[
  {
    "id": 41608648,
    "title": "Ultra high-resolution image of The Night Watch",
    "originLink": "https://www.rijksmuseum.nl/en/stories/operation-night-watch/story/ultra-high-resolution-image-of-the-night-watch",
    "originBody": "Ultra high-resolution image of The Night Watch 5 min. reading time - How did the team create this image? From the series Operation Night Watch 10/01/2022 - Rijksmuseum The new high-resolution image of The Night Watch represents a major advance in the state of the art for imaging paintings, setting records for both the resolution and the total size of the image. The sampling resolution is 5 µm (0.005 mm), meaning that each pixel covers an area of the painting that is smaller than a human red blood cell. Given the large size of The Night Watch, this results in a truly enormous image: it’s 925,000 by 775,000 pixels – 717 gigapixels – with a file size of 5.6 TB! Grid To create this huge image, the painting was photographed in a grid with 97 rows and 87 columns with our 100-megapixel Hasselblad H6D 400 MS camera. Each of these 8,439 separate photos was captured using a sophisticated laser-guided five-axis camera positioning system that can sense the precise location of the painting so that every photo is sharp – an error of even 1/8 mm in the placement of the camera would result in a useless image. New technology New technology allowed the previously-released 20 µm resolution image of The Night Watch to serve as the guide for lining up these much higher-resolution images during the process of fusing the individual captures into a single monolithic image. The technology allows each of the other types of images collected during Operation Night Watch to be precisely aligned with each other, thereby allowing all of our data to be seen in context. Physical state of the painting Why create such an incredibly huge image? With this resolution, we can very clearly see the precise physical state of the painting. Lead soap protrusions, tiny cracks, the shapes of individual paint pigment particles, past retouches, and the beautiful details of Rembrandt’s painting technique are all extraordinarily clear. This enables researchers to understand the painting’s condition in order to make the best plan for future conservation treatments. It helps us to better understand how Rembrandt painted, and it creates an exquisite 'snapshot’ of The Night Watch at this moment in its history. Ultra high resolution photo Zoom in to miniscule pigment particles The Night Watch including the missing pieces See the work as Rembrandt intended it Ultra high-resolution image of The Night Watch 5 min. reading time - How did the team create this image? Discover the high resolution image 5 min. reading time - Discoloured retouching visible Abrasion 5 min. reading time - What do we see on the high resolution image? More stories",
    "commentLink": "https://news.ycombinator.com/item?id=41608648",
    "commentBody": "Ultra high-resolution image of The Night Watch (rijksmuseum.nl)382 points by lhoff 9 hours agohidepastfavorite110 comments besttof 1 hour agoA colleague of mine made this very nice way to explore the (often) high resolution images from their collection: https://rijkscollection.net/ Highly recommended and easy to fall into a “rijkscollection hole” for a bit :) reply drng 32 minutes agoparentThis is super cool. Thanks for sharing the link reply GrumpyNl 1 hour agoparentprevWorks better than the one mentioned in the title, this one let you zoom in and out with scroll weel. reply tambourine_man 51 minutes agorootparent*scroll wheel reply wkat4242 7 hours agoprevI worked at this museum a few decades ago on a contract job, it was cool to walk around among so much history. Though I never really could appreciate the \"old masters\" from the Dutch Golden Age. Their work was part art and part record-keeping for which nowadays we have photography and video. The subject of many of these works are stuffy rich people posing for the \"family album\". Artfully done yes but boring subjects in my personal opinion. I did like some of the landscape views though. But overall I'm more into modern art where the art and the message is the only goal. One of the things special to me about the night watch is that it's huge in real life which I never really appreciated before I saw it. In contrast, the Mona Lisa at the Louvre was disappointingly tiny. reply ethbr1 4 hours agoparent> One of the things special to me about the night watch is that it's huge in real life which I never really appreciated before I saw it. Famous art that's stunningly bigger in person than I expected: - The Raft of the Medusa (Géricault) - Guernica (Picasso) - The Hallucinogenic Toreador (Dalí) Cannot recommend seeing art in person enough. Aside from the scale, it's also impossible to fully capture color or translucency in screen/page-presented imaging. And so much of the European painting mastery in the 1400s+ is the manipulation of non-opaque paint to create a desired effect. reply throwup238 39 minutes agorootparentAdd to that the Blue Boy by Thomas Gainsborough at the Pasadena Huntington and anything by Hans Holbein the Younger such as the portraits of Sir Thomas More and Thomas Cromwell at the Frick Collection. The former uses a brilliant blue paint that is simply impossible to convey via RGB display or CMYK printing color spaces and the latter look like giant printed photographs, down to the stubble on More's face, even though they were painted in the early 16th century. > And so much of the European painting mastery in the 1400s+ is the manipulation of non-opaque paint to create a desired effect. I'm sad that people don't bother with that as much today. I went on a shopping spree a while ago buying a bunch of Williamsburg and Old Holland oil paints and their colors are absolutely amazing, especially the old school heavy metal paints which come in a variety of opacities. Blending them is an art in its own right. Sadly I don't have any skill at painting so it's mostly abstract experiments with color. reply trox 1 hour agorootparentprevAside from color and translucency, an original artwork shows also the relief. It can tell much about the creation process of a painting and adds additional texture. Furthermore, some pigments were expensive and hard to work with prior to the 19th century such that artists used it very sparingly. reply SamBam 3 hours agorootparentprevAnd famous art that's much smaller in person than I expected: The Great Wave off Kanagawa by Hokusai. For such an epic image, it's only 25x37 cm / 10x14\". reply dexwiz 1 hour agorootparentprevNapoleon Crossing the Alps Is also much bigger than I expected. reply MeteorMarc 1 hour agorootparentprevAdd Birth of Venus (Botticelli) reply didntcheck 7 minutes agoparentprevWhen I visited I think I spent more time looking at the architecture of the building than the collections. It's very nice. Similar story with the Louvre I suppose - I never went in, but enjoyed walking past the pyramid exterior in the evening reply JJMcJ 5 hours agoparentprevRembrandt could put life into rich people's portraits in ways few were ever able to match. Besides the Night Watch, this one: https://commons.wikimedia.org/wiki/File:Rembrandt_-_De_Staal... known in English by various names, such as Syndics of the Drapers' Guild. These portrayals are anything but stuffy. One writer said, if you take Bach, Mozart, and Beethoven, for music, Rembrandt was more than that for painting. reply wkat4242 1 hour agorootparentYeah I just don't 'see that' in them. Like I said I'm far from an art connoisseur. So what I said is my opinion alone :) reply gyomu 6 hours agoparentprev> One of the things special to me about the night watch is that it's huge in real life which I never really appreciated before I saw it. In contrast, the Mona Lisa at the Louvre was disappointingly tiny. I had the same experience seeing a print of Hokusai’s Great Wave. For whatever reason it was built up in my mind as a huge piece, but in reality it’s the size of a standard sheet of paper. reply jimvdv 6 hours agoparentprevI agree with you on the subjects are boring rich people, if we judge it with today standards. For the time it was actually quite unique that (upper) middle class people could get their portrait done, and not just nobles. I like to think of it as part of a period of history where the merchants start to gain power from the aristocracy and that shows in what gets passed down to us. reply JJMcJ 5 hours agorootparent> (upper) middle class people It reflects a great change in Western society, which really began to flourish first in the Netherlands, where the merchant and industrial classes began to be dominant, and were growing sick of pretending it wasn't true. Mostly in Britain these days, we see the final pretenses of the nobility on display. reply ghaff 2 hours agorootparentHolland is really where the wealthy merchant class first became dominant in Europe--and was generally not subservient to the nobility as in other other countries. reply archagon 2 hours agoparentprevI was walking around the Rijksmuseum just yesterday and had the same thought. Except: Rembrandt’s paintings stood out to me among those of his peers. His subjects didn’t feel posed and his lighting and setpieces felt soft and naturalistic, not artificial. Each canvas gave the impression of an intimate peek into someone’s life. The style almost reminded me of late Romantic paintings (e.g. Peredvizhniki) that came 200 years later. reply cezart 4 hours agoparentprevI remember what I liked about Rijks upon visiting was that it was organized by decade, and had not only paintings, but various historical artifacts as well. Like state corporation sealed opium, which offered a context for the contemporary relaxed attitude of the Dutch towards drug consumption. And in general offered many windows into how the country grew up to be what it is. So yes, much history! reply scyzoryk_xyz 7 hours agoparentprevRecommend Peter Greenaway’s film „J’Accuse” about Rembrandt and that painting. It shares your criticism and argues that in it’s own time, that painting did as well. reply dclowd9901 3 hours agoparentprevFor me, it took going to Van Gogh’s museum in Amsterdam to really get it. The way they contextualize and explain his work and the actual lighting of the museum is something to experience first hand. reply AlecSchueler 3 hours agorootparentThere are several centuries between the Dutch Golden Age and Van Gogh. reply graftak 2 hours agorootparentprevVan Gogh is modern art reply kwanbix 7 hours agoparentprevWhat is so incredible is the technique they used, the level of detail and how lifelike they are. reply magicalhippo 33 minutes agorootparentSomething which is very hard, if not impossible, to get unless you look at the real deal. I'm generally not into art but my mom took me to the Rijksmuseum, and I was blown away by the details in those paintings. I spent probably 15 minutes just studying the translucent ruff in one of the paintings in amazement. The paint is three dimensional, the light interacts in ways which just aren't captured in a photo. Viewing the paintings on my screen here now they all look flat and quite dull in comparison. reply devilbunny 3 hours agoparentprevIf you want a really interesting version of the work, go to the Royal Delft factory. They made a reproduction in their famous blue tile. It's about the same size as the original. reply ErigmolCt 4 hours agoparentprevArt’s impact often depends on context reply ghaff 2 hours agoparentprevYeah, it's not really fair to associate quality with size but... Thomas Cole's huge works. Most of Rembrandt's famous works are fairly large. Etc. I admit to not being an especial admirer of the Mona Lisa but certainly larger works grab our attention more. reply hnbukkake 5 hours agoparentprevnext [3 more] [flagged] ErigmolCt 4 hours agorootparentModern art is definitely not everyone’s cup of tea yet it's designed to provoke, challenge, and sometimes irritate. reply ethbr1 4 hours agorootparentJe ne suis pas un commentaire. reply timwaagh 6 hours agoparentprevSounds like you have been to the Rijks and nowhere else. Lots of old paintings of all kinds of scenes hang in lots of museums all over this country. Not a huge museum goer but this lacks nuance. reply keepamovin 8 hours agoprevOh wow, that is so cool. I thought I was at max zoom, normal blurry tiles. Then BOOM! It came into focus and I saw tiny cracks, smallest areas of paint, no loss of clarity. It's like you're standing right up next to it. That's incredible! Wow, all I can say. That's insane, that is totally insane! I would love if there were a depthmask or something and a synthetic \"keylight\" feature you could drag around to really get an idea of the textures, the peaks and valleys. I guess we'll have that in a future version. This is incredible. reply jonasdegendt 2 hours agoparentAnother similar scan is the Ghent altarpiece[0], and you get to compare the pieces before and after a restoration. [0] https://closertovaneyck.kikirpa.be/ghentaltarpiece/#home reply bitexploder 6 hours agoparentprevEnhance, but actually :) reply tigerlily 7 hours agoparentprevYeah I noticed this too, incredible, I was thinking \"how did they do this?\". It's zoom like it should be. reply Freak_NL 8 hours agoprevAn older, lower resolution image (11206 × 9320 pixels) can be downloaded here: https://www.rijksmuseum.nl/en/search/objects?q=nachtwacht&p=... To avoid the dumb mandatory account login, just use https://bugmenot.com/view/rijksmuseum.nl . It worked just now (so be nice and leave it working). Despite the ill-advised mandatory account (really, what's up with that?), the Rijksmuseum is providing a better service than the neighbouring Van Goghmuseum, which refuses to share anything but low resolution photos of Vincent van Gogh's works. Public museums are supposed to be custodians of culture, not IP owners. reply re 8 hours agoparentWikimedia has a slightly higher-res image more easily accessible: https://commons.wikimedia.org/wiki/File:The_Nightwatch_by_Re... (14,168 × 11,528 px) reply ozim 7 hours agorootparentCool wiki has people recognition on paintings so you can click the link to see note about person in the picture! reply porphyra 3 hours agorootparentIt's not people recognition, it's just manually created tags by volunteers. Anyone can draw a box on any image and write whatever they want in it. reply Freak_NL 7 hours agorootparentprevOdd that the resolution differs. The source linked to from Wikimedia Commons is the same page at the museum's website as the one I linked to. reply mjfisher 8 hours agoparentprevI'm on mobile; I scrolled to the bottom and clicked the image of the painting and could zoom in to my heart's content - did it ask you for an account? reply Freak_NL 8 hours agorootparentYou can zoom in a lot on the 2490 × 1328 pixels offered. When you hit the download button for the full version, you get nagged. Edit: you can zoom in, and then it will offer up the painting in slices at a higher resolution. So in theory you could download those and stitch them together if you manage to hit an unscaled version. reply mistrial9 3 hours agoparentprevthe account might be a combination of \"deter abusive downloads\" and \"help, we have not enough members\" combined.. now thinking, the result of account gets sent to administration and then funders, too, as a report result. not defending the practice, but the institution has to defend and maintain, too. reply gyomu 7 hours agoprevThose 100MP digital medium format cameras are the most exciting tech in photography of the whole 21st century as far as I’m concerned. For my “serious” photography work I shoot medium/large format film, and every digital camera has left me non plussed. I may be a little obsessive about image quality, but what’s the point of dropping $5k on a setup that gives worse results than a wooden box and a sheet of film? Then I got the Fuji GFX100 (the Hassy was a little out of my range :-) and… wow. Totally different ball game. I can finally produce digital images that rival film scans. Seeing what museums have been doing with them has been super cool. reply formerly_proven 7 hours agoparentThere’s a trade off between sharpness and noise, the GFX have an intentionally lowered fill factor to, essentially, produce a sharper image. Meanwhile noise is one of the most important things when marketing mainstream cameras (next to AF), so they go for gapless microlenses etc. The reason this impacts sharpness is because a lower FF gets you closer to Shannon’s ideal point sample, while a 99% FF is like a pitch-sized box filter. reply cyberlimerence 7 hours agoprevFor anyone interested in technical aspects of this, I recommend watching Pycon talk [1] from Robert Erdmann. I bookmarked this couple of years ago. [1] https://www.youtube.com/watch?v=z_hm5oX7ZlE reply encomiast 2 hours agoparentWatching that seriously intensifies my imposter syndrome. reply ssfrr 6 hours agoprev> an error of even 1/8 mm in the placement of the camera would result in a useless image. That doesn’t make sense to me. Presumably part of the image stitching process is aligning the images to each other based on the areas they overlap, so why do they need that much precision in the camera placement? I’d think keeping the camera square to the painting would be important to minimize needing to skew the images, but that doesn’t seem to be what they’re talking about. reply gertlex 6 hours agoparentI assumed it was mostly distance from painting surface to camera that needed to be controlled for. reply KaiserPro 12 minutes agoprevI spent ages looking at this painting, and I still can't find commander vimes. reply mrs6969 5 hours agoprevI am literally standing in the museum, looking at night watch as this moment, and saw this post. Legend. reply rtaylorgarlock 22 minutes agoparentGet off yo phone!!! ;) I got to watch them do some of the scanning when I walked through the museum on a trip a couple years ago. Very cool setup. reply mmooss 1 hour agoparentprevIt's interesting that while standing in front of the painting, someone would be looking at their phone, and that they would look at a photograph of the painting. reply j4coh 31 minutes agoparentprevHacker News in one eye and the painting in the other? reply ErigmolCt 4 hours agoparentprevEnjoy the moment and soak in all the details reply mmooss 1 hour agoprevThis page is a bit better, and lets you zoom to the pixel level (they say): https://www.rijksmuseum.nl/en/stories/operation-night-watch?... reply JohnKemeny 4 hours agoprevRelated: Most detailed ever photograph of The Night Watch goes online (125 comments) https://news.ycombinator.com/item?id=23151934 Ultra High Resolution Photo of Night Watch (2022) (40 comments) https://news.ycombinator.com/item?id=29778166 reply OldGuyInTheClub 44 minutes agoprevThis is a remarkable complement to seeing a work of art in person. We can get close through zoom in ways that we couldn't at the museum without putting the piece at risk. reply Aachen 8 hours agoprevNot sure if off topic, but this German TV ad did a creative recreation of the painting that I found amusing as a Dutch person: https://www.youtube.com/watch?v=c6XQXhr7LQM reply lqet 7 hours agoparentAh, Frau Antje [0], still shaping the image most Germans have of the Dutch. [0] https://en.wikipedia.org/wiki/Frau_Antje reply charles_f 3 hours agoprevThere's something oddly satisfying in that you keep zooming in impressively close, and the image remains clean and non blurry. reply dclowd9901 3 hours agoparentThe map or whatever they use to achieve the online widget is extremely impressive. I’ve never seen such a clean implementation of a progressively loading zoom tool like that before, apart from in map applications and even they often suffer from buffering. reply ghosty141 2 hours agorootparentNow if it would just support mousewheel zooming... Thats my only problem with the viewer. reply seacourt 3 hours agorootparentprevIt was built with https://micr.io/ reply FredPret 2 hours agoprevThis is why it always pays to do your best work down to the smallest detail. You never know if, 400 years later, people are going to invent a way to examine it atom by atom. reply canjobear 1 hour agoprevIt's a pdf, you can zoom in as much as you want? https://www.usenix.org/system/files/1311_05-08_mickens.pdf reply jl6 7 hours agoprev> To create this huge image, the painting was photographed in a grid with 97 rows and 87 columns with our 100-megapixel Hasselblad H6D 400 MS camera. Looks like they had the ability to move the camera precisely to one of 97x87 grid positions. I wonder if they had any headroom in the precision of that movement. Could they have used a lower resolution but much cheaper camera and compensated by taking, say, a 200x200 grid of images instead? reply BurningFrog 43 minutes agoparentIt should be much easier to take overlapping pictures and \"seam\" them together. I assume there are software tools for that. reply buildbot 2 hours agoparentprevLower resolution yes, but one thing with the 400MS or any multishot back is that it can shift by one or 1/2 pixel to collect full RGB color info for each pixel, very important for conservation work. reply WithinReason 7 hours agoparentprevI'm sure they registered the images. https://en.wikipedia.org/wiki/Image_registration reply stavros 8 hours agoprevThis is good, but I wish they would allow for more than 1:1 zoom in. 1:1 pixels on a 4k display are too small, I'd like to be able to zoom in more than that. reply BrandoElFollito 5 hours agoprevFirst time I visited the Rijksmuseum I was of course excited to see the night watch. I found it on a side wall, 20x15 cm and was really surprised. I was expecting something more grandiose. But never mind, I love paintings from that era so I went on admiring the others. At some point I was in the middle of the central corridor and it then hit me... Wow. Before getting to the main part of the museum, there were two temporary exhibitions. One was about doll houses and the other was about the activities (work) on a 17th century ship. The latter was amazing. I was traumatized by the surgeon work, and his 5 tools... 5 tools to handle all injuries - how happy I am too live in France in the 21st century reply dralley 5 hours agoparentThe Nights Watch takes up nearly an entire wall, not sure what you saw but it wasn't the actual painting. reply BrandoElFollito 2 hours agorootparentThat's the point - As a sibling comment says - there is a small replica and then suddenly I saw the whole painting at the end of the central corridor. This was a \"wow\" moment, and an unexpected one reply I-M-S 3 hours agorootparentprevNot only does it take up the entire wall, IIRC part of it was actually cut in order for it to fit that wall. reply BrandoElFollito 2 hours agoparentprevSince it was not clear from my comment: \"At some point I was in the middle of the central corridor and it then hit me... Wow\" was when I discovered the real painting on a whole wall at the end of the central corridor. It was amazing reply lysace 1 hour agorootparentThis gives some context to the the size: https://www.bbc.com/news/world-europe-40137724 reply tnolet 3 hours agoparentprevYou saw the small replica Rembrandt made for the dude who commissioned the painting. He wanted one to hang in his home. It’s much smaller than the actual piece, which covers a whole wall. And indeed, the large one got a chunk cut off at some stage as they had to move it. This was long ago when Rembrandt was not particularly in vogue. reply ph1l337 6 hours agoprevFeels like you could make a fun game out of guessing where in the image you in the most zoomed in level. reply timwaagh 6 hours agoprevRembrandt did not work in this resolution so i think zoomed in it will just be a bunch of random noise. reply gligorot 6 hours agoparentI thought the same. But try to zoom in on the eyes, you’ll notice fascinating details. reply ErigmolCt 4 hours agoparentprevSome of the magic happens at a distance reply roughly 1 hour agorootparentThis is true for a great many things reply grugagag 6 hours agoprevFascinating to see how the paint cracked. I zoomed in around the faces of the three men on the bottom right hand side and there are light areas on their faces with few cracks and dark areas with lots of cracks, eg around the noses. I wonder what caused that. reply BurningFrog 42 minutes agoparentA next step could be to \"restore\" those cracks in the image, and get an image of how it looked when new. reply mejutoco 4 hours agoparentprevI do not know of course, but black oil painting cracks more than other colours. I think it is common to mix black colour with a bit of dark blue to avoid excessive cracking. That could be a potential explanation. reply nofunsir 5 hours agoprevreminds me of microsoft seadragon/photosynth https://en.wikipedia.org/wiki/Seadragon_Software reply tecleandor 5 hours agoparentIt's basically the same technique. Same as Google Maps too reply brookst 3 hours agorootparentThe tiled zoom thing is everywhere, and lots of museums publish high resolution images this way. There’s a handy tool to reconstruct an image at any zoom level from a url: https://dezoomify-rs.ophir.dev/ reply ikari_pl 6 hours agoprevI used to have it as a full wall wallpaper in the living room where I was growing up. reply curiousgal 8 hours agoprevTo be honest I don't understand the obsession about documenting things that are done to the painting. Going through that section of the museum I felt like the curators cared more about showcasing their efforts to store the painting than the painting itself. reply davidmr 7 hours agoparentI think it’s a way of keeping the museum’s single most popular piece of art on display whilst working it. I think most museums would remove it for a while, but so many people come specifically to see this painting that they want to keep it viewable, so they make a little show of its restoration. I dunno; I’ve been through that floor 5 or 6 times since they started work, and people always seem to love the spectacle of it. reply wrsh07 5 hours agoparentprevI always find it fascinating! Much like it is important in a museum of natural history to note \"science isn't finished, some of these things are still under research\" it's important to contextualize the painting you see today. The painting today is different than it was fifty years ago or a hundred years ago or from the day it was completed. It's common for paintings to be modified after completion, either by the creator or by the current owner. Whose version are you seeing? What are the possible versions? Anyway, the best part of a museum is you don't have to look at the things that bore you reply ErigmolCt 4 hours agorootparentYep, paintings are living artifacts that evolve over time reply roughly 1 hour agoparentprevThis particular piece of work is damn near 400 years old. When one is tasked with participating in preserving such an item so the next twenty generations can also enjoy it, it pays to take notes on what you’ve done with your small part of that chain. reply perihelions 7 hours agoparentprevI suspect there's selection effects in play: museum curators who don't aggressively make the case for more museum funding, don't end up curating the most well-funded museums. reply dewey 6 hours agoparentprevSometimes I find these things more interesting than the painting. I think it's good to also highlight what the museum is working on. Otherwise people would think it's just a room where they hang up new paintings once in a while, the restoring and research part would then be even more invisible. reply throwup238 7 hours agoparentprevPreserving and restoring an oil painting that old and large is a minor achievement, especially considering how many people have tried to destroy the painting in the last hundred years. reply shagie 3 hours agorootparentOne of the channels that I've stumbled across in my YouTube travels is Baumgartner Restoration - https://www.youtube.com/@BaumgartnerRestoration > Julian Baumgartner of Baumgartner Fine Art Restoration, a second generation studio and now the oldest in Chicago employs only the finest archival and reversible materials and techniques to conserve and restore artworks for future generations. Its really interesting seeing the removal of past restoration attempts and the modern techniques to restore a painting. If I was to pick two that touch most on the responsibility of restoration and what is and is not achievable... Scraping, Scraping, Scraping Or A Slow Descent Into Madness. The Conservation of Mathias J. Alten https://youtu.be/YOOQl0hC18U Restoring The Faceless Painting https://youtu.be/hsTkaSbMLHw https://youtu.be/rDVcgpSwnyg https://youtu.be/JWCBNL-iu5s reply andrepd 6 hours agoparentprevWhy not? It's an old work of art, if you're going to make changes to it you better do the best equivalent of `git commit` that you physically can, to preserve how it was before your change. reply diego_moita 4 hours agoprevThe Rijksmuseum is on my top 5 list of museums I've ever visited, along with the Vatican Museum, the Louvre, the Met and the Uffizzi. There are a lot more interesting works in there including Vermeer, other Rembrandt works, Pieter DeHooch, Rubens, the whole golden era of Dutch Renaissance... Since you're in Amsterdam already save some time to visit the VanGogh Museum, very close to Rijksmuseum. And since you're in Netherlands already save some time to go to Den Hag (the Hague) to visit the Maritius Huis museum and the cool M.C. Escher museum. reply dralley 1 hour agoparentKunsthistorisches Museum in Vienna is very nice. The inside is basically a palace. reply josefrichter 5 hours agoprevDid we just take down the website? reply ChrisMarshallNY 8 hours agoprevThat’s quite well-done! Much faster than most of these types of sites. reply Daub 6 hours agoprevShortly after the painting was completed it was cropped so that it would fit on the wall. See if you can guess which edge was the victim. Of the high resolution image itself... I teach painting and regularly use such images as teaching aids. I honesty belive that they have as much teaching value (or even more) than seeing the real thing. The details of paint applicationare magnificently clear in such images. reply grumple 6 hours agoparentIt was actually cropped on all 4 sides: https://news.artnet.com/art-world/operation-night-watch-1982... reply ck2 6 hours agoprevVery vaguely related to image detail but you know what similarly impressed the heck out of me: you know that first ever imaging of a black hole using telescopes across the globe and even the poles to make the signal gathering as wide as possible? well that telescope (interferometer) could also image a TENNIS BALL on the MOON (in perspective currently 5 meters is the best resolution of the moon we have and they only get like one or two photons back when they bounce a laser off that mirror the astronauts left there) So are we going to enter an era where we can get ten more times out of existing telescopes with exponentially better sensors? reply zokier 4 hours agoparentThere is fairly significant difference in radio observations and visible spectrum imaging though. You aren't going to get 5m resolution visible light image of the Moon any time soon. reply nbzso 2 hours agoprev [–] Why do you need AI for this? And how on earth this gives you any form of certainty about reality? I am amazed at the system of propaganda in the hands of Davos/Bilderberg, Club of Rome and other fake institutions. They are truly masters of the metaverse which we live in. Waiting with patience for the New World Order of our hidden \"gods\". Stochastic parrots weaponized to inject the masses to the new global system will need a new AI religion to work. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Rijksmuseum has created a record-setting ultra high-resolution image of The Night Watch, achieving a 717-gigapixel resolution and a file size of 5.6 TB.",
      "The image was captured using a 100-megapixel Hasselblad H6D 400 MS camera in a grid of 97 rows and 87 columns, with precise alignment facilitated by a laser-guided five-axis camera positioning system.",
      "This high-resolution image enables researchers to examine the painting's physical state in unprecedented detail, aiding in conservation efforts and providing deeper insights into Rembrandt's technique."
    ],
    "commentSummary": [
      "The Rijksmuseum has released an ultra high-resolution image of Rembrandt's \"The Night Watch,\" allowing viewers to zoom in and explore the painting in unprecedented detail.",
      "This new tool offers a more immersive experience, enabling users to see fine details such as tiny cracks and brush strokes that are not visible to the naked eye.",
      "The release has generated significant interest among art enthusiasts and tech-savvy individuals, highlighting the intersection of technology and art preservation."
    ],
    "points": 381,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1726909700
  },
  {
    "id": 41607059,
    "title": "I Like Makefiles",
    "originLink": "https://switowski.com/blog/i-like-makefiles/",
    "originBody": "I Like Makefiles 18 Sep 2024 4 minutes read #CLI #Tools I like makefiles. I first used a makefile more than ten years ago. Even back then, it looked like some ancient technology used by the graybeard Linux wizards. Years passed, and new build tools came and went, but I kept seeing makefiles still used here and there. I got used to them because they were part of some projects I joined. At some point, I started to like them. Today, they are often the first automation tool I use when I start a new project. The reason I like makefiles is that they often follow an unwritten convention of implementing the same set of commands to get you up and running. When I find a project I know nothing about, and I see a Makefile file inside, chances are that I can run make or make build followed by make install, and I will get this project built and set up on my computer. Or at least I will get information on other steps I need to include. I try to apply the same rule in my projects. If I open a folder with one of my old projects and run make dev, this will perform all the necessary steps to build the project and spin up a dev server. That's convenient because throughout the years, I used many different technologies, and each had different commands to build or deploy a project. I have old projects written in Jekyll, Hugo, 11ty, and all sorts of different Python web frameworks. With makefiles, when I come back to a project I haven't touched for months (or years), I don't have to remember the command to start a dev server with, let's say, Jekyll. I just run make dev, and this, in turn, fires up the corresponding Bundler commands. Even if I use tools like Docker or gulp in my project, I still use makefiles to orchestrate those tools. For example, I often write a make build command that builds all the necessary Docker images, passing additional parameters specific to a given project. My makefiles are simple. I don't use conditional statements, flags or any other fancy features. Most of the tasks (they are technically called targets, but I always call them tasks in my head) consist of one or more shell commands. I could write bash scripts with a couple of functions instead, but makefiles are easier and faster to write. Some common tasks that most of my personal projects[1] contain include: dev to start the development server build to build the project (if a build step is necessary) deploy to deploy/publish the project And that's really it. Sometimes, I include additional tasks like watch to automatically rerun the build task when I change any of the source files. But many of my projects can be managed with just two or three Make commands. This blog that you're reading right now has a simple makefile with just one target: dev:npm run dev And a more advanced project of mine uses the following makefile to run the dev server, watch for changes, build, encrypt and deploy the website: # Run dev server dev:bundle exec jekyll serve --unpublished -w --config _config.yml,_config-dev.yml --livereload # Build assets build:npm run gulp build # Watch a specific folder and process assets watch:npm run gulp watch -- --wip # Build the website locally, encrypt and deploy to Netlify server deploy:JEKYLL_ENV=production bundle exec jekyll build; \\make encrypt; etlify deploy --prod # Encrypt the \"_site\" folder encrypt:npx staticrypt _site/*.html -r -d _site In both of the above examples, I'm ignoring the existence of phony targets, which you might want to add if you have a file called dev, build, watch, deploy, or encrypt, as many kind readers on Hacker News suggested. Otherwise, this Makefile won't work as expected. GNU Make (the software that runs makefiles) is quite ubiquitous. If you're on Linux, you probably already have it installed. Even on my MacBook, I don't remember installing it explicitly. It must have come with some other tools that I installed in the past. Make is simple and doesn't require as many additional dependencies as some other build tools. This can be useful if you need a tool that will work in a restricted environment where installing additional packages is difficult or impossible for security reasons. Make will probably be already present in that environment. And if not, you can just take the commands from the makefile and run them manually in the shell. If gulp is not available on your server, you can't really take the JavaScript code and paste that into the terminal. I'm not against other build tools. I like other build tools too. I'm excited when I find a new one that is better and faster than the one I was using before. But I will still use Make to orchestrate them because it gives me a set of familiar commands to manage all sorts of different setups with different tools. By \"personal\", I mean projects where the deployment process is much simpler than production-grade stuff. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=41607059",
    "commentBody": "I Like Makefiles (switowski.com)349 points by thunderbong 16 hours agohidepastfavorite329 comments jart 13 hours agoDon't be discouraged by all the people in this thread saying you're using make wrong. One of the things that makes make a great tool is how deceptively simple it is. Yes not using .PHONY can potentially get you in trouble. But for a small project that's the sort of trap you'll fall into a year later, if at all, and even then you'll only be scratching your head for an hour. 99% of the time you don't have to care about doing things the proper way. Make lets you just hit the ground running and only imposes as much complexity as you need to keep the thing from falling apart. reply ReleaseCandidat 10 hours agoparent> One of the things that makes make a great tool is how deceptively simple it is. One of the worst things of Make is how deceptively simple it looks. Make does exactly one thing: it takes input files, some dependencies and generates _exactly_one_ output file. To have rules which don't generate output (like `install` or `all` or `clean` or all targets in the article) we need to resort to a hack, a special magic target like `.PHONY` (which hasn't been part of POSIX up to the 2017 version - IEEE Std 1003.1-2017 - https://pubs.opengroup.org/onlinepubs/9699919799/utilities/m..., only the current one - IEEE Std 1003.1-2024 - https://pubs.opengroup.org/onlinepubs/9799919799/utilities/m... includes `.PHONY`). If you want to generate more than one file (like an object file and a module or a precompiled header or ...) you are on your own to build some brittle hack to get that working. Don't forget that not every Make is GNU Make, BSD and other nix like Solaris/Illumos still exist. Don't get me wrong: Make has it's uses for sufficiently complex projects which aren't too complex yet to need some \"better\" build system. Problem is that such projects may get too complex when more code is added and they inevitably gain some sort of scripts/programs to generate Makefiles or parts of Makefiles (so, an ad hoc meta build system is created). And the problem isn't that they use it, but that they are proposing it as a solution to \"everybody\". And that their Makefile stops working as soon as there is a directory (or file) `build` (or `dev` or ...) in the project root. reply jart 10 hours agorootparentI work on a project with 4.4 million lines of code and using a single Makefile with no generated code works fine. It's really not all that difficult. reply rectang 8 hours agorootparentI don’t object to “it works for me”, but “it’s really not all that difficult” is a bad generalization. * If you need portability, Makefiles are hard. * The whitespace design of Makefiles is bad and has swallowed up countless debugging hours over the years. This design flaw isn’t intrinsic to the way Makefiles work, it’s just a lousy artifact from a superficial decision from decades ago: to change behavior based on distinctions invisible in source code. It’s mitigated by syntax highlighting but still bites people. * Makefiles are dependent on the consistency of the build environment, for example the availability and behavior of command line switches. Even if your project doesn’t need OS platform portability, this is still a pain across time and requires external tooling to manage. * There are certain subtleties to the way Makefiles behave that are addressed by `.PHONY`. I agree that these are manageable in the absence of other complexities, but they contribute towards Makefiles being more difficult than appears at first. I’m sure you’re familiar with those critiques and others. They may not bother you, but you don’t speak for everybody. reply jart 6 hours agorootparentMy Makefile is portable. It builds binaries that run on six OSes and two architectures. So I used my Makefile to build GNU Make and a GCC toolchain. Now I can run my Makefile on any of those OSes / architectures too, and it'll produce the same deterministic output, bit for bit. reply ReleaseCandidat 1 hour agorootparent> My Makefile is portable. Oh yes, in the good old tradition of \"... as long as it's some Linux on x86\". [...] on Linux 2.6+ (or WSL) using GNU Make. Sorry, it's actually AMD64 _and_ ARM64! reply whartung 5 hours agorootparentprevIf part of your build is building your own build tool in order to ensure you have the proper build tool then why not build a different “better” build tool? Part of the premise of Make is its ubiquity, but if you can’t rely on that save as a simple bootstrap (as you seem to be doing) then why not forego it for something else? reply chipdart 3 hours agorootparent> (..) then why not forego it for something else? Because blindly ditching a technology for no reason at all is not a way to fix problems. reply marci 4 hours agorootparentprevAre you suggesting this: https://xkcd.com/927/ ? reply marci 4 hours agorootparentprevYou might want to take a look at the \"actually portable executable\"* project, made by the person you're responding to. There may be tips that will make make more approchable to you, if you're still dealing with MAKEFILEs. * https://justine.lol/ape.html reply wruza 10 hours agorootparentprevProjects of much smaller sizes often have recursive convoluted makefiles. reply chipdart 9 hours agorootparent> Projects of much smaller sizes often have recursive convoluted makefiles. You name any technology and anyone can enumerate dozens of projects that use it wrong. reply wruza 8 hours agorootparentI’d walk that before talking. Take any complex makefile system and turn it into a single “not really difficult” makefile without sacrificing anything important. Wins this argument and helps those who “use it wrong”. reply flykespice 5 hours agorootparentprevSure buddy anything can be manageable once you invest enough time and sanity. Now show us the Makefile. reply ReleaseCandidat 10 hours agorootparentprevAnd I can show you thousands of \"Hello World\"s that use GNU Autotools or CMake ;) But seriously: can I take a look at it (Soource + Makefile)? reply Drawde 6 hours agorootparentThis is most likely what is being referenced: https://github.com/jart/cosmopolitan/blob/master/Makefile I like how the includes are separated and commented. Also if you weren't already familiar with their work you might be interested in giving this a read: https://justine.lol/ape.html reply anymouse123456 7 hours agorootparentprevI don't understand this statement, \"which hasn't been part of POSIX up to the 2017 version - IEEE Std 1003.1-2017.\" I've definitely been using .PHONY on various Linux and MacOS computers long before 2017. Maybe it's just me, but I've never much cared for whether or not something is specified if it happens to be present everywhere I go. reply ReleaseCandidat 4 hours agorootparent> I've definitely been using .PHONY on various Linux and MacOS computers long before 2017. Me too, and I've also used Makes which didn't (on e.g. Irix). What I wanted to express had been that you can't even rely on `.PHONY` existing, much less many other features. reply stabbles 5 hours agorootparentprev> If you want to generate more than one file A pattern like tgt: generate_many_files touch $@ is pretty common. What's the issue? reply chipdart 9 hours agorootparentprev> Make does exactly one thing: it takes input files, some dependencies and generates _exactly_one_ output file. Not true. Your dependency graph might culminate on a single final target, but nothing prevents you from adding as many targets that generate as many output files as you feel like adding and set them as dependencies of your final target. Think about it for a second. If Make was only able to output a single file, how in the world do you think it's used extensively to compile all source files of a project, generate multiple libraries, link all libraries, generate executables, and even output installers and push them to a remote repository? > To have rules which don't generate output (like `install` or `all` or `clean` or all targets in the article) we need to resort to a hack, a special magic target like `.PHONY` I don't understand what point you thought you were making. So a feature that boils down to syntactic sugar was added many years ago. So what? As you showed some gross misconceptions on what the tool does and how to use it, this point seems terribly odd. > And the problem isn't that they use it, but that they are proposing it as a solution to \"everybody\". I think you're making stuff up. No one wants Make to rule the world. I don't know where you got that from. I think the whole point is that Make excels at a very specific usecase: implement workflows comprised of interdependent steps that can be resumed and incrementally updated. Being oblivious of Make leads many among us to reinvent the wheel poorly, using scripting languages to do much of the same thing but requiring far more work. If you can do this with a dozen lines of code in a Makefile, why on earth would you be churning out hundreds of lines of any random scripting language? reply ReleaseCandidat 9 hours agorootparent> Not true. Your dependency graph might culminate on a single final target, but nothing prevents you from adding as many targets that generate as many output files as you feel like adding and set them as dependencies of your final target. Sorry, I did phrase that badly. A better version of that sentence would be A single target (a single node in the dependency graph) of Make does exactly one thing: it takes input files, some dependencies and generates _exactly_one_ output file. > I think the whole point is that Make excels at a very specific usecase [..] Excatly what I wanted to express with my post above. But the article isn't about such a case, but for something for which a single shell script (or, better, just adding the commands to the `scripts` stanza of `package.json`, which is the more common, expected way to do it) is actually better suited and way less error prone. reply chongli 7 hours agorootparentA single target (a single node in the dependency graph) of Make does exactly one thing: it takes input files, some dependencies and generates _exactly_one_ output file. Yes, but this is not particularly relevant to the user. With pattern rules it's trivial to define a large number of targets automatically, such as in the example (from the manual): objects = foo.o bar.o all: $(objects) $(objects): %.o: %.c $(CC) -c $(CFLAGS) $ `foo` -> `bar`, that is `foo` is a temporary target and won't be (re)build if `bar` already exists. Which may or may not be a problem. This temporary target (whatever the actual term is) can be \"elevated\" to a \"normal\" target with the use of some special target (which I'm too lazy to look up right now). reply oneeyedpigeon 7 hours agorootparentprev> Text after a blank line that is indented by two or more spaces is reproduced verbatim. (This is intended for code.) (from https://news.ycombinator.com/formatdoc) reply mauvehaus 6 hours agorootparentprevAmong other things, now you have to maintain a set of dummy targets. If you have a variable (possibly generated) that is basically A_MESS_OF_FILES := foo bar zot You now have to create dummy targets for bar, baz, and zot and not forget to add them. Or maybe break it into MAIN_FILE := foo SUBORDINATE_FILES := bar zot foo: baz sudo make me a foo $(SUBORDINATE_FILES): %: foo reply robinsonb5 7 hours agorootparentprev> A single target (a single node in the dependency graph) of Make does exactly one thing: it takes input files, some dependencies and generates _exactly_one_ output file. I'm still not really following the point about one output file? That might be Make's stated purpose, but a Makefile rule can certainly create extra files as a side effect (or do pretty much anything a shell user could do, from creating directories and downloading files to launching applications) One of my projects has a single makefile rule which downloads and unzips a tarball, applies a patch to it, then builds the application within, resulting in half a dozen binaries which are then used in building the rest of the project. Edit: Ah - I see what you mean now, in your subsequent comment. reply oblio 10 hours agorootparentprev> If you want to generate more than one file (like an object file and a module or a precompiled header or ...) He's not using C, though :-) > And the problem isn't that they use it, but that they are proposing it as a solution to \"everybody\". He's proposing it for the same reason I'm starting to like it, after many years in the industry: as a simple build wrapper. > And that their Makefile stops working as soon as there is a directory (or file) `build` (or `dev` or ...) in the project root. And they can fix that problem in 5 minutes, big deal :-) > Don't forget that not every Make is GNU Make, BSD and other nix like Solaris/Illumos still exist. This is a very bad reason in this day and age. 99.999999% of *NIX usage these days, probably 99.9999999999999999% for the average person, since most people won't ever get to those environments where BSD and Solaris are still used, is Linux. And even for BSD and Solaris, guess what... you add an extra step in the build instructions asking them to... install GNU Make. Heck, even back in 2005 (I think?) for Solaris one of the first things you'd do was to install the GNU userland wherever allowed because the Solaris one was so forlorn I swear I heard wooden planks creak and dust pouring down every time I had to use their version of ps. And regarding POSIX, meh. If you're a C developer (C++, Rust, I guess), knock yourself out. Most of the stuff devs use are so far removed from POSIX... Actually, not removed, but has so many non-POSIX layers on top (I mean not standardized). Ruby bundler is not standardized like awk. Python pip is not standardized like make. Etc, etc. That's the reality we're in. POSIX is very useful but only as a very low level base most people don't need to chain themselves directly to. I'd definitely not avoid a tool because it's not in the latest POSIX standard (or only in the latest POSIX standard). reply ReleaseCandidat 10 hours agorootparent> He's not using C, though :-) As said elsewhere, the use-case in the article is too simple to warrant a Makefile. So: if you aren't compiling some static language, you do not need - and certainly don't want to use - Make. > you add an extra step in the build instructions asking them to... install GNU Make. The main reason to use Make is that it is installed everywhere, as stated multiple times in other posts. If you must install something, you can also install a better alternative for your specific use-case to Make. > one of the first things you'd do was to install the GNU userland Yes, and the Unix vendors even shipped them on companion CDs or similar. > is not standardized like awk Same problem with awk (and sed and ...): some weeks ago I had problem with the SDK for some real-time Linux that works with mawk only, and not with GNU awk (most of the time it's the other way round, only working for some GNU program). reply oblio 7 hours agorootparent> As said elsewhere, the use-case in the article is too simple to warrant a Makefile. So: if you aren't compiling some static language, you do not need - and certainly don't want to use - Make. I've found that I prefer make as a command runner and most of the time I'm just running Python poetry commands or building Docker containers or running AWS infra commands. It's very useful to have a simple tool to run commands and have them depend on each other. And regarding many of the alternatives to Make, they're either more complex or have other issues: https://news.ycombinator.com/item?id=41608555 reply Brian_K_White 9 hours agorootparentprevIt's a much smaller problem to port a makefile to a different make than to deal with most of the alternatives and their requirements. reply ReleaseCandidat 9 hours agorootparentThat depends if the person who must do the porting knows Make or not and which GNU Make (it's always about GNU Make!) feature had been used. And chances are JS devs don't at all or just as little as the one who wrote the article. Don't get me wrong: I don't like Make, but I hate CMake and Autotools (and many other C++ build systems) too (and C and C++ and Fortran compilers and their vendors). reply elktown 9 hours agorootparentprev> And they can fix that problem in 5 minutes, big deal :-) Honestly, a big issue I see is that people can somehow argue with a straight face (and successfully too!) to invest weeks of work introducing a pet project to avoid a 1 hour inconvenience that happens once every blue moon. Proportionality takes a backseat very quickly to motivated reasoning. reply ReleaseCandidat 8 hours agorootparentIs this post for or against Make? And why is Make not a \"pet project to avoid a 1 hour inconvenience that happens once every blue moon\"? reply elktown 7 hours agorootparentIt's a general observation on over-engineering, \"resume driven design\", and proportionality being somewhat of a blind spot in software. But yeah, I'm not going to lie, my brain certainly patterned matched towards \"this is going to be a Bazel guy isn't it?\". So, Buck2 was close enough. Those are exactly the kind of multi-week pet projects I'm talking about that are too often introduced under vague and disproportional pretenses. Well, multi-month and dedicated specialists going forward are perhaps more accurate for those. But maybe that's the point. reply ReleaseCandidat 4 hours agorootparentBut my argument has been that _Make_ is already too complex for the given task. And talking about complex C and C++ (to be fair, the complex ones are almost always C++ ;) projects, I would not say that CMake (or Meson or ...) is less complex than Buck 2, it certainly has _way_ more magic than Buck 2. And getting Make & C++ & ccache(or whatever) & distcc (or whatever) to work _reliably_ isn't easy either ;) reply miki123211 9 hours agorootparentprev> This is a very bad reason in this day and age. 99.999999% of *NIX usage these days, probably 99.9999999999999999% for the average person, since most people won't ever get to those environments where BSD and Solaris are still used, is Linux. You have a lot of confidence. In reality, it's probably more like 30-60%, more now because of WSL. The rest is Mac OS, which uses a BSD userland and hence BSD make by default. reply oblio 7 hours agorootparentWSL basically runs GNU/Linux distributions so I fail to see the significance of that point. And for MacOS you do the same thing, you get them to use their beloved homebrew to install GNU Make. reply ReleaseCandidat 9 hours agorootparentprev> The rest is Mac OS, which uses a BSD userland and hence BSD make by default. No. Just a really old version of GNU Make make --version GNU Make 3.81 Copyright (C) 2006 reply gbuk2013 9 hours agoparentprevHere is a slightly more complex example of a Makefile I use when spinning up a new TypeScript project (but I switch out to use pnpm these days): https://github.com/borisovg/node-ts-template/blob/main/Makef... I still wouldn’t say it’s that complicated - you do need to know your way around the syntax a bit but it’s less challenging than getting all the other tooling working in the first place. :) reply orbisvicis 7 hours agorootparentI didn't know you could redefine .PHONY like that and what... all the phony targets are accumulated into a list? reply gbuk2013 7 hours agorootparentNot just .PHONY - you can do that for any target: https://www.gnu.org/software/make/manual/html_node/Multiple-... “One file can be the target of several rules. All the prerequisites mentioned in all the rules are merged into one list of prerequisites for the target.” reply chipdart 9 hours agoparentprev> Don't be discouraged by all the people in this thread saying you're using make wrong. Fully agree, and I would add that it's far better to adopt the right tool for the job, even if you are not an expert, than avoiding the criticisms from perfectionists by adopting the wrong tool for the job. Everyone needs to start from somewhere, and once the ball is rolling then incremental changes are easy to add. Great job! reply johnnyanmac 7 hours agorootparentPeople who want to call me out would be a lot more productive pointing me to some guides instead of chastising me over an ancient framework who's best documentation has been lost to time. And whose best practices are locked behing proprietary codebases. Little tips here and there are nice, but that doesn't teach me the mentality of how to achitect a makefile reply instig007 7 hours agorootparent> Little tips here and there are nice, but that doesn't teach me the mentality of how to achitect a makefile What exactly are you missing from the official manual? - https://www.gnu.org/software/make/manual/make.html - https://devdocs.io/gnu_make/ reply chipdart 2 hours agorootparentprev> People who want to call me out would be a lot more productive pointing me to some guides instead of chastising me over an ancient framework who's best documentation has been lost to time. Fully agree. Don't get discouraged, and keep it up! reply anymouse123456 7 hours agoparentprevI've been a happy make user for 20+ years across many, many projects and many languages. I've never had issues with the .PHONY task that seems to bother people so much. It's simple, readable, editable, composable and already installed everywhere. It does what it says on the tin and not much else. FWIW, I also wrap up whatever fad (or nightmare) build system people use in other projects when I need to deal with them. reply jarule 4 hours agorootparentIt's simple, readable, editable, composable I'll eat crow if wrong, but I'm guessing I know more about GNU make than you do. It is none of the four things you claim. Also, people who say \"on the tin\" need a good ass-kicking. reply matheusmoreira 9 hours agoparentprevEvery makefile recipe should produce exactly one output: $@. The makefile as a whole produces an arbitrary number of outputs since rules can depend on other rules. This leads us to a neat rule of thumb for phony targets: any recipe that does not touch $@ and only $@ should have $@ marked as phony. I find that keeping track of phony targets with a list makes things much easier. phonies := phonies += something something: ./do-something phonies += something-else something-else: something ./do-something-else # touches $@ and thus does not need to be phony create-file: ./generate-some-output > $@ .PHONY: $(phonies) reply 153957 5 hours agorootparentIt is also possible to define `.PHONY` multiple times, so you can simplify this to: .PHONY: something something: ./do-something .PHONY: something-else something-else: something ./do-something-else create-file: ./generate-some-output > $@ reply Brian_K_White 9 hours agorootparentprevI don't know if it's actually saner than just normal phonys but man I like it. What does it get you other than the ability to print the list of all phonys? reply matheusmoreira 9 hours agorootparentIt's mostly so I can immediately see which targets are phonies. Every phony has a line directly above it adding it to the list of phonies. When a makefile gets complex enough we need all the help we can get. I use phony targets so much I wrote a shell script to parse the makefile database dump into some sort of help text. It doesn't depend on that variable at all. https://github.com/matheusmoreira/.files/blob/master/~/.loca... Prints output like: phony1 phony2 dependency1 dependency2 reply kstenerud 14 hours agoprevMakefiles are terrible tech. The problem is that they're slightly less bad than most other build system we've come up with, which makes them \"useful\" in a masochistic way. Build systems tend to commit one or more of the following sins: * Too basic: Once you try to build anything beyond a toy, it quickly becomes chaos. * Too complicated: The upfront required knowledge, bureaucracy, synchronization and boilerplate is ridiculous. The build system itself takes an order of magnitude more data and memory than the build target. * No standard library (or a substandard one that does things poorly or not at all): You must define everything yourself, leading to 10000 different incompatible implementations of the same build patterns. So now no one can just dive in and know what they're doing. * Too constricting: The interface wasn't built as a simple layer upon an expert layer. So now as soon as your needs evolve, you have to migrate away. * Too much magic: The hallmark of a poorly designed system. It doesn't have to be turtles all the way down, but it should be relatively close with few exceptions. * Cryptic or inconsistent syntax. reply ristos 12 hours agoparentMy 2c: Makefiles are excellent tech, just that a lot of people haven't learned to use it properly and use it as it was intended. I'm sure I'll get pushback, that's ok. - Too basic: At least half of the software I use just uses plain makefiles and maybe a configure script. No autotools. I optionally run ./configure, and then make and make install, and it just works. I definitely wouldn't consider my setup to be a toy by any stretch of the imagination. It's built out of smaller programs that do one thing and one thing well. - Too complicated: I don't know, I think make and how it works is really easy to understand to me at least. I guess everyone's had different experiences. Not necessarily your case, but I think usually it's because they had bad experiences that they probably blamed make for, when they were trying to build some complex project that either had a bad build setup itself (not make's fault), or without the requisite knowledge. - No standard library: It's supposed to be tooling agnostic, which is what makes it universally applicable for a very wide range of tools, languages, and use cases. It's viewed as a feature, not a bug. - Too constricting: I'm not sure what you mean here, it's designed to do one thing and one thing well. The simple layer is the dependency tracking. - Too much magic: Cryptic or inconsistent syntax: See 'Too complicated' reply cryptonector 8 hours agoparentprevThe worst build systems are the ones centered on a particular programming language. Since there's N>>1 programming languages that's N>>1 build systems -- this does not scale, as the cognitive load is prohibitive. The only general-purpose build system that spans all these languages is `make` or systems that target `make` (e.g., CMake). And this sucks because `make` sucks. And `make` sucks because: - it's really difficult to use right (think recursive vs. non-recursive make) - so many incompatible variations: - Unix/POSIX make - BSD make - GNU make - `nmake` (Windows) - it's rather ugly But `make` used right is quite good. We're really lucky to have `make` for the lowest common denominator. reply zokier 4 hours agorootparentMake is very much centered around C. For example, make has built-in default rules for building C code https://pubs.opengroup.org/onlinepubs/9699919799/utilities/m... reply instig007 7 hours agorootparentprevNix is a general-purpose build system that spans all these languages. reply yencabulator 1 hour agorootparentI've never seen anyone use Nix to actually build software; it's a glorified launcher for shell scripts in a sandbox, and typically is used to start the actual build system, such as make/cargo/go build/npm/etc, with known inputs. reply zelphirkalt 6 hours agoparentprevOne or more, OK that leaves of course lots of room. I would estimate: (too basic) Makefiles are not. (too complicated) They can be, depends on what you make them to be. (standard library) Well, there is one, there are some builtin functions you can use in the makefile. (too constricting) Haven't noticed that, so I would say no. (too much magic) Hmmm I don't see it. It is very clear what is a target and a dependency and so on. Not so magical. (syntax) Yeah definitely could be better. Even a plain JSON file would be better here. reply sixthDot 13 hours agoparentprev> Once you try to build anything beyond a toy, it quickly becomes chaos. Of course the chaos is not caused by, \"very hypotheticaly\" let's say, a compiler or maybe a language without modules. How would you estimate that ? 20%, 40%, or 70%, true ? reply tomjen3 12 hours agorootparentNot OP, but its not just that C/C++ lacks modules. I think that is missing the real issue. Any complicated program probably needs a custom developed tool to build it. As a simple example, imagine a program that uses a database - you want to keep the sources as SQL and generate classes from them. Thats a custom build step. Its just that in some languages and build systems (Node, Maven), we have abstracted this away by calling them plugins and they probably come from the same group that made the library you need. No such pluginsystem exists, as far as I am aware, for makefiles. reply ReleaseCandidat 12 hours agorootparentprevGood luck writing Makefiles for Fortran, OCaml or (whenever they will really, actually work) C++ modules. There aren't many widely used build systems that can handle such dynamic dependencies without some special \"magic\" for these, the only one that I know of (with a significant number of users, so not Shake) is Buck 2 (Bazel and all C++ build systems use \"special magic\", you can't write in user rules). reply overanalytcl 5 hours agorootparent> Good luck writing Makefiles for Fortran, OCaml or (whenever they will really, actually work) C++ modules. I've successfully written Makefiles for Fortran and they worked with ifort/ifx and gfort. In my experiments I've also made GNU Cobol, GNU Modula-2 and Vishap Oberon fit within the Makefile paradigm without much trouble. You have failed to provide reasons as to why those languages in particular (or more likely any language that's not of C heritage) can't be used with Makefiles. For instance, you can definitely couple OCaml with Makefiles, just use ocamlopt and treat .cmx files as object files, generated beforehand by ocamlopt -c (like you'd do with GCC). I am not familiar with C++ modules and as such I didn't experiment with them. reply ReleaseCandidat 3 hours agorootparent> I've successfully written Makefiles for Fortran and they worked with ifort/ifx and gfort. Did the samé (I'm not sure if gfortran did exist at all at the time, I guess it had been g95), plus they worked with Absoft, PGI and Pathscale too (yes, that has been some time ago). And it was a great PITA. Not the least because at the time no Fortran compiler did generate the dependency description, so you either had to parse the Fortran sources by yourself or use makedepf90, which didn't work with all sources. > You have failed to provide reasons as to why those languages in particular [...] can't be used with Makefiles. I have obviously badly worded that. I didn't mean it is impossible, just that is a great PITA. > I am not familiar with C++ modules and as such I didn't experiment with them. They have the same problem, you don't know the name of the module that is going to be produced. reply instig007 7 hours agorootparentprev> Good luck writing Makefiles for OCaml So what's the problem exactly? https://mmottl.github.io/ocaml-makefile/ Oh look, it even builds a project faster than Dune: https://discuss.ocaml.org/t/dune-build-vs-makefile/11394 reply ReleaseCandidat 3 hours agorootparent> So what's the problem exactly? They all have the samé problem: that you don't know the name (or even the number) of modules (module files) being generated without reading the source. And as a bonus every compiler uses a sligthly different naming scheme for the generated module file (this is of course no problem for OCaml ;). As an example (using Fortran). File `test.f90`: module first contains subroutine hello () end subroutine hello end module first module second contains subroutine world () end subroutine world end module second `gfortran -c test.f90` yields the following files (2 of them are modules): -rw-r--r-- 1 roland staff 221 Sep 21 19:07 first.mod -rw-r--r-- 1 roland staff 225 Sep 21 19:07 second.mod -rw-r--r-- 1 roland staff 185 Sep 21 19:07 test.f90 -rw-r--r-- 1 roland staff 672 Sep 21 19:08 test.o reply kaba0 10 hours agorootparentprevThere are projects that generate files, depend on multiple languages, etc. If you push the job of a build tool to the compiler infrastructure, then why even have a “build tool” in the first place? Make is simply anemic for anything remotely complex, and there are countless better tools that actually solve the problem. reply imtringued 9 hours agorootparentYeah my biggest problem with make is that the compiler has to generate the header file dependencies. This means starting a C or C++ project with make from scratch is a hard problem and there is no default solution or default file for this other than to just use CMake. reply ratboy666 5 hours agoparentprevYep, terrible: I will show how Make hits every one of your complaints: (sarcasm on) in file hello.c: #includeint main(int ac, char **av) { printf(\"hello\"); return 0; } How to compile and run this? We need a build system! Download and install GNU Make. When that step is complete: Type in make hello and its done. Now, run via ./hello See, Too much magic (didn't even have a makefile or Makefile), no standard library, Too constricting, cryptic, too basic. And, because you had to install Make, too complicated. Hits every one of your objections. (sarcasm off) reply IshKebab 10 hours agoparentprevI agree. Also a lot of the replacements are focused on one language rather than being a generic \"do stuff\" tool like Make. The fact that Make can't even do subdirectories sanely is kind of ridiculous. Does anyone know of anything better than Make? There's Ninja but it's not designed to be written by hand. reply feelamee 7 hours agorootparentI think just[1] is a good generic \"do stuff\" tool [1] https://github.com/casey/just reply IshKebab 7 hours agorootparentIt's not a build system though. I mean a generic build system like Make, but without some of the terrible design decisions. reply feelamee 3 hours agorootparentthere is also gn[1]. It doesn't seem to be very popular, but feels good in my experience. [1] https://gn.googlesource.com/gn reply ReleaseCandidat 8 hours agorootparentprev> Does anyone know of anything better than Make? Xmake https://xmake.io/ for C and C++ (I haven't use that for anything serious yet) and Buck 2 https://buck2.build/ if you need a really complex build system. Both of these do caching of build artifacts and can do distributed builds (with less and more complex setup). reply IshKebab 6 hours agorootparentYeah I've been following Buck2. Definitely interesting. Xmake looks interesting too (even though I hate Lua). I wonder why it isn't more popular - I don't think I've seen a single project use it. reply john-tells-all 14 hours agoprevI adore Make. I've written one (or more) for every single task or project I've touched in the last 20 years. No smarts. It's just a collection of snippets with a few variables. \"make run\", \"make test\", \"make lint\", that kind of thing. \"make recent\" = lint then run the most recently modified script. You could do the same thing with Bash or other shells, but then you get stuck into Developer Land. Things are so much more complicated, without giving extra value. Make is just a DSL saying \"files like this, are made into files like that, by running this command or two\". That's it. This is incredibly powerful! reply rramadass 13 hours agoparent> Make is just a DSL saying \"files like this, are made into files like that, by running this command or two\". Nicely put. Decades ago i wrote a testing framework in java where you could specify your tests and their dependent classes using make-like syntax. So you could have a set of test classes which define the \"baseline suite\", then another layer of test classes which is dependent on the above and only run if the above is successful and so on. I really do not understand why folks today make everything so complicated. My advise has always been, stick to standard Unix tools and their way of doing things (tested and proven over time) unless you run into something which could absolutely not be done that way. Time is finite/limited and i prefer to spend it on System/Program Design/Modeling/Structure/Patterns etc. which are what is central to problem-solving; everything else is ancillary. reply klysm 14 hours agoparentprevSomehow every make file I’ve encountered in the wild is a lot more than “that’s it” reply wruza 10 hours agorootparentThat just rosy tinted glasses most of the historical users are wearing. It takes time and nerve to admit that you have decades of experience with a footgun that isn’t even trivial to use beyond tutorial/builtin use cases. reply maccard 8 hours agoparentprev> Make is just a DSL saying \"files like this, are made into files like that, by running this command or two\". That's it. The problem with make isn’t make - it’s that what makes calling usually doesn’t do that anymore. On my last project we had a makefile that had 4 main commands - build test frontend deploy. Build and test called through to maven, frontend called npm, and deploy called docker + aws. All of those tools do their own internal state tracking, caching, incrementalness and don’t report what they’ve done, so it’s not possible to write a molecule that says “only deploy if build has been updated” because maven/cargo/dotnet/npm/go don’t expose that information. reply elAhmo 10 hours agoparentprevLikewise! I haven't been using them in the past, but at my current position almost every repository has a Makefile. Running `make test` and knowing it will work, regardless of the stack, language, repo is a huge lifesaver. reply kccqzy 14 hours agoprevThe author is not even using the mtime-based dependency tracking. Also the targets are supposed to be PHONY but not marked as such. The author could have replaced it with a shell script that read $1 and matched on it to determine what to do. reply weinzierl 13 hours agoparent\"The author could have replaced it with a shell script that read $1 and matched on it to determine what to do.\" Or just with a simple command runner like just. https://just.systems/ reply safety1st 11 hours agorootparentThe strengths of make, in this context where it's been coaxed into serving as a task runner for small projects, are: 1) It's already installed practically everywhere 2) It reduces your cognitive load for all sorts of tasks down to just remembering one verb which you can reuse across multiple projects, even if the implementation ends up differing a bit 3) In conjunction with the similarly ubiquitous SSH and git, you have everything you need to apply the basic principles of DevOps automation and IaC There's something special about waking up one day with an idea, and being able to create a fresh git repository where the first commit is the Makefile you've had in your back pocket for years that scripts everything from environment setup to deployment to test automation to code reviews. There's zero effort beyond just copying your single file \"cookbook\" into that new repo. reply yunohn 10 hours agorootparent> It's already installed practically everywhere This always comes up, and is a sad chicken/egg problem. We can all somehow agree that Make mostly sucks, but OS maintainers aren’t interested in providing a default alternative, due to choice overload or something. reply badgersnake 9 hours agorootparentThis entire article and discussion is about why people like make. reply yunohn 3 hours agorootparentNo, a significant amount of the comments here are about how Make sucks. reply hulitu 7 hours agorootparentprevCMake runs make, ninja runs make. reply IshKebab 10 hours agorootparentprev> It's already installed practically everywhere Well, except Windows. But nobody uses that right? reply zelphirkalt 8 hours agorootparentI always find it a questionable choice, when someone, who wants to be a professional software engineer, uses Windows. If it is a choice at all. Of course they could also be working at some job, where there are silly ideas like everyone having to use Windows or so. If it is a choice, it sort of shows an \"I do not care\" attitude to software development, or being seriously uninformed about proprietary software. Usually those are the types, for whom software engineering is merely a 9 to 5 job, and not a craft they take pride in. An activity they do not really care about at other times. Which is OK to do, not a crime. If I were hiring though, I would rather look for passionate software engineers/devs, who know a lot of stuff from tinkering and exploration. Ultimately using Windows means you are not truly in control of your productive system and are at the whim of MS. It is a risk no self-respecting software engineer should take. To clarify, that is not to say, that there cannot be craftsmanship people using Windows. It is just way less likely. More likely they are \"enterprise\" software people. Even the choice to explore and use a GNU/Linux distribution betrays some kind of mentality of exploration. Wanting to know what is out there. Learning a new thing. Adapting it to ones needs. This kind of learning mindset in the long term is what sets engineers apart from others. So I would claim, that not many good software engineers use Windows to be productive. If they have to, they will likely install some VM or some means of making things work as if they were on a GNU/Linux system. WSL or whatever, to circumvent the limitations and annoyances of a Windows system. reply vouwfietsman 7 hours agorootparentThis is a silly take, have you heard about game developers? If there's craftmanship anywhere, its in game development, and they surely don't want to spend all their time working on a platform without proper tooling that their end users overwhelmingly do not use. The choice of OS has nothing to do with craftmanship or \"exploration\". I \"explored\" linux many times and am not using it currently. In fact, I'm happy to argue that most developers that care so much about the choice of OS that they are uninterested in using another one (and do not work in OS development) are probably somehow stuck in their ways and uninterested in exploration themselves. Taken even further, currently the only important OS is the browser, and nobody cares who launches it. I hope you are not somehow in charge of hiring. reply zelphirkalt 6 hours agorootparentGame developers are such an exception ... And I left room for exceptions in my explanation. But of course, if you want all the IP to leak via MS spyware phoning home, sure, let your devs work on Windows machines. reply maccard 8 hours agorootparentprevNothing is installed out of the box on windows, but anyone with a functioning development environment for a large number of programming languages will have installed wsl, msys or git bash along the way and have make installed as part of it. reply bmacho 7 hours agorootparentI don't think git bash comes with a make. reply maccard 4 hours agorootparentSo it doesn’t. TIL. reply coolgoose 9 hours agorootparentprevWsl2 :) reply oguz-ismail 10 hours agorootparentprevnmake comes with msvc reply unscaled 9 hours agorootparent1. MSVC is not installed by default on Windows. 2. nmake is with POSIX make, let alone GNU make. It doesn't even support .PHONY target, which is what you need to replace Just with make. 3. Installing Just with WinGet is simpler, faster and takes probably only 1% of the space of installing Visual Studio for nmake. reply overanalytcl 6 hours agorootparent> 1. MSVC is not installed by default on Windows. Neither is Make or GCC on Unix. reply wruza 9 hours agorootparentprevYeah, “just install msvc”. It’s easier to install msys2 when you’re that desperate. At least the tool name will be “make” out of box. reply a5c11 13 hours agorootparentprevOr just with a simple command which is guaranteed to be on most Linux systems already - make. Maybe his Makefiles aren't complex, nor they seem to follow all the best practices invented by code gurus in sandals, but it works and, what's important, it works for him. reply weinzierl 13 hours agorootparentThere was a time when people would have said the same about make. The shell is the simple command that is guaranteed to be on all Unix systems from the get go. Make is the new kid on the block. If you just want to run commands in the order written down, don't need the topological sorting feature of make and value ubiquity then a shell script is the answer. If you are not stuck in the past and you truly live by the UNIX philosophy of doing one thing and doing it well, a command runner is the answer. The command runner avoids the ton of foot guns both shell scripts (no matter which flavor) and make files have. just also brings a couple of features out of the box that would be very tedious and error prone that replicate in make and shell scripts. reply darby_nine 12 hours agorootparentRight but writing dependency management (of targets, not package management) in shell seems like a nightmare compared to just leveraging make. Why complicate things? It's dead simple to debug, the interface is dead simple, what's the downside? reply boomlinde 11 hours agorootparentRight, but the original point which started the thread is that \"The author is not even using the mtime-based dependency tracking\", in which case a plain shell script is very much a viable alternative to make. I don't particularly mind this use of make, but as an article on make it fails to exemplify what I think is its main purpose. reply darby_nine 3 hours agorootparentI don't think that really matters. Sometimes even basic shell scripts are better modeled with a makefile. reply tcfhgj 11 hours agorootparentprevIn my PS script solution, I just added a clean option+command. I rewrote my makefile in PS and don't miss anything from make and have no regrets, as it is simpler now. reply oblio 10 hours agorootparentprevWho develops just? Will it be around in 5 years? Will it be ad supported? Will the developer sell my data? Etc. I don't have any of those concerns with GNU Make. reply jjav 9 hours agorootparentprev> There was a time when people would have said the same about make. The shell is the simple command that is guaranteed to be on all Unix systems from the get go. That would've been a pretty short window of time since make first came out (according to wikipedia) in 1976. reply kristiandupont 11 hours agorootparentprev>Make is the new kid on the block. Make is from 1976. I don't think you can legitimately refer to it as that. reply cassianoleal 10 hours agorootparentThe first UNIX was announced outside of Bell Labs in 1973. In 1976, pretty much every tool was “the new kid on the block”. reply oblio 10 hours agorootparentPhew, I was so worried. So for 48 years out of Unix' 53 years of existence (90% of that time), make hasn't been the new kid on the block. Oh, let alone the fact that we're talking about stuff from 48 years ago, when their \"screen\" was a paper printout of the output. reply Ygg2 11 hours agorootparentprevYou could around 1976. Who's ever going to need make. reply bregma 8 hours agorootparentprev> There was a time when people would have said the same about make. The shell is the simple command that is guaranteed to be on all Unix systems from the get go. Make is the new kid on the block. I seem to recall it being praised very highly at the time as a great tool that saved many billable expensive CPU minutes and made a developer's job so much easier. reply movedx 11 hours agorootparentprevI believe you to be correct. I think it's important that one uses the right tool for the job, regardless of whether or not it's widely adopted or supported. reply unscaled 9 hours agorootparentprevOr just use a simple command which ALL Unix system have: sh. If you're using make a glorified task runner, why don't you just create a scripts/ directory with shell scripts to do whatever you want. This is simpler, cleaner and works everywhere. Make doesn't really add anything. I get the feeling that using make this way is an aesthetic preference that has somehow developed with time into a \"this is the one true Unix way\" cargo cult. reply deepsun 4 hours agorootparentbin/ directory. reply hulitu 13 hours agorootparentprev> but it works That it what a lot of SW developers forget: your code might be the best in the world, but , if someone is not able to build it, it is useless. reply wruza 9 hours agorootparentI remember countless times me and forum fellas debugging makefiles written under developers’ assumptions about systems. That is also what lots of developers forget or simply aren’t aware of. Make isn’t a silver bullet for builds. It isn’t even a bullet. Most software gets built from scratch and make’s deps graph makes little to zero sense in this mode. Make is a quirky poor dev tool footgun, jack of all trades master of none. reply ReleaseCandidat 12 hours agorootparentprev> it works and, what's important, it works for him. Until it doesn't. And then you really have to learn about PHONY targets, why and when there must be a tab and not spaces - good luck with an editor that doesn't treat Makefiles special and is configured to convert tabs to spaces. reply dented42 12 hours agorootparentBut those are things that he’ll learn about as he keeps using make. And why does it matter that some editors don’t know about makefiles? The one he is using handles them just fine so what’s the problem? reply ReleaseCandidat 12 hours agorootparent> And why does it matter that some editors don’t know about makefiles? Because it isn't fun checking if the whitespace at the beginning of the line is a tab or spaces. And as said, you must know when to use tabs and/or spaces in rules. For doing such a simple thing as calling some commands, Make has way too many subtle footguns which _will_ bite somebody, someday. The problem (that's not a problém at all, that's a reason to celebrate!) is that most JS devs and users aren't used to Make, compared to e.g. C programmers. To rephrase: as someone writing C, you have to use something like a Makefile, as anything else (like scripts) gets unreadable and -usable quite fast. But if you can get away with a less complex solution, you should really use that instead of Make. reply instig007 11 hours agorootparent> Because it isn't fun checking if the whitespace at the beginning of the line is a tab or spaces. And as said, you must know when to use tabs and/or spaces in rules. that's why https://editorconfig.org/ exists, so that neither you nor your teammates have to think about these things reply ReleaseCandidat 10 hours agorootparent> neither you nor your teammates have to think about these things You're better off using a Makefile linter. But you must know about the problem before being able to solve it. And error messages like Makefile:2: *** missing separator. Stop. aren't the most helpful. reply oblio 10 hours agorootparentYou'll find the issue within 2 minutes of googling the error message. reply wruza 9 hours agorootparentprevAnd as said, you must know when to use tabs and/or spaces in rules Is that Stockholm syndrome? Or an appeal to history/authority in action? What makes people believe that this is even remotely reasonable. inb kids these days, I started in the '90s and wrote my share of makefiles. Tolerating make only made sense until 2010-ish, then both hw and sw advances rendered it useless. Edit: just realized my reply to a wrong person, but let it stay here reply scrame 9 hours agorootparentprevBecause forcing devs to use tools for one specific format makes a bunch of unhappy devs. Especially if its just forcing them to use make. reply flooow 10 hours agorootparentprev`just` is great and I use it all the time. * All commands in one place, view them all with `just --list` * Stupid-simple format * Small standalone binary * Configurable (with arguments, environment variables etc) but not _too_ configurable When I see a git repo with a Makefile, I'm filled with dread. When I see a repo with a Justfile, I get warm fuzzies. Some people say it just doesn't do enough to justify existing. These people are just wrong. reply aerzen 5 hours agorootparentTo me, just is make without features I don't need. There is not a lot of benefit for me, but there is a lot of benefit for other people who need to learn the repo and have no knowledge of either make or just. Another benefit is that justfile just cannot get too complex and tangled. Simplicity at its finest. reply darby_nine 12 hours agorootparentprevWhy not just use make? I am constantly confused by people reinventing the wheel with new syntax and little benefit reply maccard 8 hours agorootparentWhen step one of using a tool is to disable the tools primary benefit (everything is phone) you’re reaching for the wrong tool. Like it or lump it, make deploy is much neater than docker build -t foo . && docker tag foo $item && docker login && docker push && helm apply I wish there was a flag for make which set it to be a command runner by default for the current makefile. reply darby_nine 3 hours agorootparentMake's clear benefit is laying out a process as a series of dependencies. reply mldbk 39 minutes agorootparentprevHonestly, it is because make is not written in Rust ;-) This is a sect and global trend to reinvent and re-implement the wheel with the Rust :D reply deepsun 4 hours agorootparentprevWhy not just use shell scripts? Why additional complexity? reply darby_nine 3 hours agorootparentShell scripts are additional complexity? I'm not sure what you mean. Adding structure reduces complexity. reply xigoi 4 hours agorootparentprevBecause they have different purposes. make is a build system, just is a command runner. reply darby_nine 3 hours agorootparentWhat's the distinction? What do just do better than make? reply xigoi 3 hours agorootparenthttps://github.com/casey/just?tab=readme-ov-file#what-are-th... reply eloisant 8 hours agorootparentprevBecause make is a huge PITA reply 112233 12 hours agorootparentprevPlease help me understand why this thing exists. Like, no snark, I like using the proper tool for a job -- when would I look at the project and think \"this is something that is better done with 'just' tool\". Instead of readme.txt and a folder with scripts reply sgarland 6 hours agorootparentIt’s gotten some syntactic sugar recently that’s made it pretty nice. Specifically, I’m thinking of its OS-specific sections: you can do something like [macos] # do a thing And without any other checks, that section will only ever run on Mac. Yes, of course you can replicate this in Make, but it isn’t nearly that easy. reply kaba0 10 hours agorootparentprevI can count on one hand the number of times a simple script or make worked out of the box. Sure, part of the reason is dependencies, but then I might as well use a build tool that is actually doing what a build tool should. Makefiles/bash scripts are hacks, and I don’t get this strange Stockholm syndrome UNIX-people have for them. reply nsonha 11 hours agorootparentprevwhy do you need a \"command runner\"? Have you heard of bash functions? Or... make? The thing is too simple to justify installing another tool, however nifty it is. reply notpushkin 13 hours agoparentprevHere’s a one-line horror story for you (from a real project I’m working on): .PHONY: $(MAKECMDGOALS) > The author could have replaced it with a shell script that read $1 Sure, but `./build.sh dev` is a bit less obvious than `make dev`. Another reason to use Make even if you don’t have any non-phony steps is that you can add those later if needed. (I agree that the author should mark {dev,build,deploy} as phony though.) reply tpoacher 12 hours agorootparentWhy is this a horror story? Under certain assumptions of how the author intends to use this, this sounds like a sensible way to define a dynamic list of phony targets to me, without having to specify them by hand. There are many reasonable scenarios why you might want to do this: determining at the point of calling make which targets to force or deactivate for safety, projects with nested or external makefiles not directly under your control, reuse of MAKECMDGOALS throughout the makefile (including propagation to submakefiles), ... reply lloeki 10 hours agorootparentConsider: .PHONY: $(MAKECMDGOALS) qux: foo: qux bar: foo Now make bar and make foo bar will disagree on whether foo is phony, which may or may not be what one wants depending on both what foo and qux do, and how bar depends on foo and qux side effects. It also very much depends on what the intent is, notably such a \"autophony\" make foo is very different from make -B foo. reply dima55 13 hours agorootparentprevI got an even better one for you: `./dev.sh`. The author is doing it wrong, and giving Make a bad name. reply jakelazaroff 13 hours agorootparentFirst of all, misusing a tool doesn’t “give it a bad name”, and second of all who cares? A tool isn’t a human being. Make’s feelings aren’t going to be hurt by this article. The author just shared something they think is cool. That takes guts to show the world, and our critiques should respect that. reply notpushkin 13 hours agorootparentprevYou’ll also want ./build.sh and ./deploy.sh then. If each is 1-2 commands, I’d argue it’s a waste to use separate files here. > giving Make a bad name How so? reply dima55 12 hours agorootparent> I’d argue it’s a waste to use separate files here Fine. Write a `make.sh` that parses the arguments; that would be better. > How so? Well, read the comments here. Do you sense that Make is a beloved tool? Most of the complaints are about some details about syntax, and those complaints are completely valid. If you use Make for its intended purpose, then it's still easily well-worth using, despite that. But if you use it as a glorified script, then all you see is the warts, without any upsides. And you then tell all your friends that \"Make sux!\" Which is a huge shame because Make is awesome. reply tpoacher 12 hours agorootparentHear hear! reply Jach 13 hours agorootparentprevThat's fine, the separate files are a benefit here. The only annoyance is clogging up the root project folder -- though some people don't seem to care about that. If they got too numerous (the 5 in OP's \"more advanced\" project would probably not be too numerous), I'd consider putting the scripts in their own folder. I might even call it 'make' just to mess with people. Then my commands are just make/build and make/deploy and so on (.sh unnecessary). But really, in OP's case, I just have no need for some of their simple wrappers. \"npm run dev\" is two characters longer than \"make dev\", pointless. reply fukawi2 13 hours agoparentprevWhile you're technically correct, what I gathered from their experience is the consistency of usage, between not only their own projects but third-party projects too. They could make technical improvements to their own Makefiles, sure. But it's more about being able to enter a project and have a consistent experience in \"getting started\". reply ReleaseCandidat 13 hours agorootparent> But it's more about being able to enter a project and have a consistent experience in \"getting started\". I'd say putting the Makefile content in `package.json` would be more consistent, especially as they are already using Gulp as the build system. reply croemer 13 hours agorootparentYou can't put comments in package.json, JSON should never have been used for something maintained by humans. reply ReleaseCandidat 12 hours agorootparentWe are not arguing whether not declaring phony targets is worse than using comments in `package.json`? But anyway, comments in a Makefile or `package.json` are not documentation anyway, that's what the `README` or `INSTALL` (or whatever) is there for (in projects like the one the Makefile is written for). reply tempodox 12 hours agoparentprevWe all were beginners at one time or another. And if you want to learn a tool, it helps to actually use it, even if your greenhorn usage is less than perfect. You can make incremental improvements as you learn, like we all do. reply ristos 12 hours agoparentprevThat's the beauty of make and shell, it's follows the UNIX principle of being simple and doing one thing and one thing well. People want it to do many other things, like be a scripting language, a dependency tracker, etc, so they're willing to pull in bloatware. New isn't necessarily better. Autoconf and automake isn't make. reply unscaled 9 hours agorootparentYes, the UNIX principle of being simple and doing one thing and one thing well. Make does dependency tracking relatively well (for 1976). But if you just want to run some commands, your shell already does that just as well, without any of the caveats that apply to make. reply trashburger 12 hours agorootparentprevIs this satire? Being a scripting language and tracking dependencies are primary features of shells and Make, respectively. reply ristos 12 hours agorootparentNot satire, sorry I didn't clarify. They want make to have a builtin scripting language rather than using shell scripts, and a dependency tracking system that more complex and less tooling agnostic rather than leveraging the appropriate tool (like `npm ci`). reply kaba0 10 hours agorootparentprevNone of them are simple, they are chock full of hacks upon hacks, “fixing” their own idiocies, and by extension, none of them are doing their one thing well. Especially bash scripts, they should be left behind.. reply ristos 8 hours agorootparentCan you be more specific what you view as hacks or idiocies? Besides the criticism of .PHONY targets, which I don't think is a hack nor particularly ugly. When I mean shell I'm referring to a family of shell languages that are used run commands, change directories, etc. Fish is a shell, for example. Babashka can be considered to be a shell. It doesn't even need to be those, someone using make could use python or javascript for the scripting part if it works better than a shell language. reply thom 10 hours agoparentprevThere isn’t even a need for a shell script. The author is already invoking three separate tools, each of which has a mechanism for invoking custom commands. reply oblio 10 hours agorootparentWhat if he wants to have a uniform environment across projects and some aren't JavaScript? reply scrame 9 hours agorootparentDear God! There is something that isn't javascript? Do you know you can it use on the server and the browser!??! reply thom 7 hours agorootparentprevThen he could just use Gulp, but I have no skin in this absurd game. reply oblio 7 hours agorootparentGulp? That JS tool that was last cool in 2018? After it which it was replaced with Grunt, which stopped being cool in 2020? And that was replaced with Webpack, ESBuild, Rome, Bun... Why would anyone voluntarily subject themselves to that kind of insanity? :-)) Better to just use the hacksaw that is Make than all these Rube Goldberg contraptions :-) reply thom 5 hours agorootparentI don’t personally care about the JS ecosystem. But OP is already using Gulp. He’s then calling Gulp from npm run. He’s then calling npm run from make. Adding make into the mix is solving nothing here. If you’re saying he should use make properly I agree! reply deepspace 12 hours agoparentprevThe author also seems not to have discovered 'make configure' and the horrors of the automake/autoconf toolset and the m4 macro language. reply tpoacher 12 hours agorootparentSure but these are completely orthogonal to make. Might as well complain about gcc. If anything, it's an argument for making better use of make's own features for configuration in the first place. reply instig007 11 hours agorootparentprevwhat are those horrors about? reply deepspace 10 hours agorootparentHave you tried writing (or even just reading) a configure.ac script? reply instig007 7 hours agorootparentsure, and there's lots of documentation around it, that's why I asked for examples of the horrors. reply bluejekyll 14 hours agoprevTechnically all of these make targets look for files by the names of the targets. Each one should really be defined as .PHONY. That said, I used to write makefiles like this all the time, but have since switched to just and justfiles in recent years which make this the default behavior, and is generally simpler to use. Things like parameters are simpler. https://github.com/casey/just reply notpushkin 13 hours agoparentI kinda like these make-ish systems, but they all have one problem: Make is already on any Linux and Mac, and is pretty easy to get on Windows as well. (It’s a real pity they don’t include it in the Git Bash!) Just using the lowest common denominator is a big argument for Make IMO. reply oblio 7 hours agorootparentOn Windows if you don't use WSL, Cygwin gets you 95% of the way there. I've been using it for decades to develop CLI tools and backbends in Python and a few other languages. You learn the quirks in about 1 month, add some tooling like apt-cyg and map C: to /c and you're off to the races. reply notpushkin 3 hours agorootparentYeah, I liked Cygwin too when I was on Windows myself! reply EE84M3i 7 hours agorootparentprevI thought `make` was not in the base install for Ubuntu, Debian or MacOS? reply kaba0 10 hours agorootparentprevYou have to handle dependencies either way to build a project - what’s one more tiny executable? This criticism might make sense for some non-vim editor because you might have to ssh into a remote location where you can’t install stuff. But if you should be able to build a project and thus install its required dependencies, then you might as well add one additional word to the install command. reply IshKebab 10 hours agoparentprevA big mistake Make has is mixing phony and file targets in the same namespace. They should be distinguishable by name, e.g. phony targets start with a : or something. Too late of course. reply a-dub 13 hours agoparentprevyeah just is really cool but it's not really commonly installed so that's kind of annoying. i feel like we're due for some kind of newfangled coreutils distribution that packages up all the most common and useful newfangled utilities (just, ripgrep, and friends) and gets them everywhere you'd want them. reply ReleaseCandidat 13 hours agorootparentBut I want please, ag and friends! The \"problem\" with this kind of package is that everybody wants something else. And the chances that they get a part of the default MacOS or Windows install (or even part of the XCode command line tools or Plattform SDK (or whatever that is called now)) is quite small. reply xelamonster 13 hours agorootparentprevI like `asdf` a lot for this, but I actually don't use it for either of those examples (though it does have plugins for them). Ripgrep is in most package repos by now and all my dev machines have a Rust toolchain installed so I can build and install `just` from source with a quick command. reply croemer 13 hours agorootparentI think parent meant to have it pre installed in most distros, not just easily installable reply xelamonster 13 hours agorootparentSure, really though I don't understand why installing a single binary which is available from several easy to use package managers somehow becomes an insurmountable barrier for people when `just` is involved. \"If it's not already on my system I can't use it\" seems like an absurd limitation to place on your projects. reply spc476 12 hours agorootparentPlease talk to security. My machine is locked down so tight I need a director (or higher) override to get anything not in the default distribution or \"blessed\" by security installed, and I can't even be the one to install it. May you never have to work at The Enterprise. It sucks! reply Thiez 10 hours agorootparentAt that point wouldn't you \"just\" download the source and compile locally? Since you presumably could compile stuff. Add a 'bin' folder in your home directory to your PATH and enjoy. reply xelamonster 12 hours agorootparentprevThat sucks for sure, I did work a giant enterprise for a few years and it was plenty painful but not that bad at least. Well maybe it was that bad, because we didn't use make either, everything had to go through Jenkins and nobody bothered with anything for local development beyond an occasional `build.sh` somewhere in the project. Simply push your code when you think it's done and wait 30 minutes to get the next set of linter errors. reply kaba0 10 hours agorootparentprevSo how do you build any project, which have countless dependencies that all have to be installed? reply a-dub 12 hours agorootparentprevoh i have no problem at all installing stuff in my own environments, i'm all about having cool new tooling -- it just starts to get a little rude to ask others to do so in order to use something you're distributing (and therefore absent coreutils-ii-electric-boogaloo installed everywhere, i'm much more likely to reach for make, unfortunately). reply xelamonster 12 hours agorootparentMaybe it's different kinds of projects then. For most of what I work with distribution would have nothing to do with the build system in the repo, only people who would ever have to deal with it are other contributors that likely have some environment setup to do regardless. reply Jach 12 hours agorootparentprevMeanwhile gradle people are like: just run these included gradlew or gradlew.bat files, they'll download the actual gradle from somewhere online, pollute some folders in your home dir, and then execute the build stuff. I notice just has some pre-built binaries that could be used for the same thing. I find it a little beyond rude what gradle normalized, but hey, it \"works\", and it removes the source of friction that's present any time you violate the principle of least surprise with your choice of build tool. reply PhilipRoman 12 hours agorootparentThe reason why Gradle needs this junk in the first place is that they aggressively change and deprecate APIs. Tried to build a 6 year old project today and of course nothing works. Gradle wrapper proved pretty useful here. Make, on the other hand, has maintained almost perfect compatibility since it's inception. reply ptx 9 hours agorootparentprevI don't understand why Gradle doesn't just provide the wrapper for download. They do provide the checksums [0], so it's not like the wrapper is customized for each repo or anything, but to download it you have to download the full distribution, extract the archive to extract the archive to extract the archive and run Gradle to run Gradle. The properties file specifying the version and checksum is great, but we shouldn't need millions of identical copies of the binary itself checked into every repo. [0] https://services.gradle.org/distributions/ reply xelamonster 12 hours agorootparentprevI have more than once considered writing a Makefile shim that would check for just, install if needed and proxy all commands to it... reply arjvik 12 hours agorootparentDo it! reply croemer 13 hours agorootparentprevLike moreutils? yamu? Yet another moreutils? reply jll29 8 hours agoprevIt funny that make evokes such fierce arguments, almost like the semi-religious vi-vs-emacs wars of old. I agree fully with the OP, in particular I find it smart that he wraps anything in a top-level makefile, even if other, more sophisticated build tools are used. The advantage is standardization, not having to remember anything and to know that if you wrote it, you will just be able to type \"make\" and it will work. Let's say a C person wants to compile a Rust project, they would not have to look up how cargo works, but could simply type \"make\" (or \"gmake\"; I don't use GNU specifics, but try to be POSIX compliant, even if it is certainly true that almost 100% of makes are gmakes). Thanks for proposing the use of the timeless \"make\" as a sort of top-level build system driver; this will probably still work in 250 years. reply zkldi 8 hours agoparentcargo is a bad example as it's universally `cargo build`. Make on its own is great but most of the time I've worked with C projects it's been cmake/autotools + global pkg installs, which you Do have to frequently look up. reply diggan 7 hours agorootparent> cargo is a bad example as it's universally `cargo build`. Except if you want to use some specific feature. Or specific log level. Or build a specific crate in a workspace. Or... reply xigoi 4 hours agorootparentHow does make solve those proBlems? reply diggan 3 hours agorootparentParent states that it's always \"cargo build\" which in 90% of the cases, is true. Except for the projects that would require something like \"cargo build --feature=wayland\" for example, in order to run. So \"cargo build\" ends up not being universal, and adding make will make it just \"make build\" regardless of what flags people use with cargo, meaning it's more universal than \"cargo build\". reply cantSpellSober 6 hours agoparentprevIt's funny such a simple title inspired a flamewar. The article itself is an insanely simple use case for make (that uses gulp in 2024?) that clearly no one read. reply fragmede 7 hours agoparentprevNot if bazel/blaze takes over before then. If you doubt that, look at Chrome vs Firefox. Or Kubernetes vs docker-compose. reply p4bl0 9 hours agoprevYes, Make is awesome. I use it for so many things. It's a great way to automate tasks. For example my personal website is built using a Makefile that calls bash scripts to rebuild the updated web pages, and I deploy it using a git push to my server and a git hook there that calls Make. However there are files that I don't want to put into the Git repository because they are blobs that may change often like PDFs of my teaching materials. It's okay, I have an \"uploads\" target in my Makefile that will upload only the modified PDFs to my server and this target is a dependency of the \"deploy\" target which does the git push so I don't even have to think about it. Also the updated PDFs for my courses materials are automatically put into my websites source tree by another Makefile that I use to manage and build my teaching materials and which let me either build the PDFs I use from my LaTeX sources or build from the same sources alternate versions of the materials for my students (without solutions to the lab sessions exercises for example) and automatically publish those to my local website version to be uploaded whenever I want to deploy the updated website. It's kind of Makefiles all the way down. I like Makefiles! =) reply duped 13 hours agoprevmake as a task runner is not too bad, but there are better alternatives today like just (as others have commented). make as a build system is ok until you hit the warts. - make/Makefiles aren't standardized, which is why automake exists. So now you're not writing Makefiles, but templates and generating the actual makefile. This doesn't matter if you own the whole toolchain, but most people don't, so this is what some folks do to guarantee their Makefiles are portable. - make cannot do any kind of dependency resolution, it assumes that whatever you need is right there. That leads to configure scripts, which like makefiles, are not standard, so you use autoconf/autoreconf to generate the configure script that runs before you can even run a target with make. - make (and adjacent tools like automake/autoconf/autorefconf) use mtime to determine if inputs are out of date. You can get into situations where building anything is impossible because inputs are out of date and running autoconf/autoreconf/automake/configure leaves them permanently out of date. (fwiw, many build systems can get away with using mtime if they can do proper dependency tracking) All in all the fundamental design flaw with make is that it's built with the unix philosophy in mind: do one thing well, which is \"rebuild targets if their inputs are out of date.\" However this is an extremely limited tool and modern build systems have to do a lot of work on top to make it useful as more than a basic task runner. reply computerfriend 12 hours agoparent> make cannot do any kind of dependency resolution dependency: ... target: dependency ... reply duped 12 hours agorootparentTracking build targets is not dependency management except with handwaving reply teo_zero 11 hours agoparentprev> make cannot do any kind of dependency resolution Ignorant's question: isn't dependency resolution the core of make? What are you referring to here? reply duped 2 hours agorootparentI'm referring to package management. Modern build systems all have some way of doing package management directly or interfacing with package managers instead of just shelling out to them, which you would have to do with make. reply imtringued 9 hours agorootparentprevPlease tell me, how exactly does make resolve header file dependencies of a .c or .cpp file? reply publicmail 4 hours agorootparentIt normally works in conjunction with GCC’s “-MMD -MP” arguments which provide .d files which then get included back into the Makefile with something like “-include $(OBJS:%.o=%.d)”. It doesn’t directly interpret any source file though, if that’s what you mean. reply teo_zero 9 hours agorootparentprevDo you really expect an answer from a self-defined \"ignorant\"? Or is this a rhetoric question and you are hiding an answer inside it? If so I don't get it. Wouldn't it better to explain it in plain words? reply Izkata 13 hours agoparentprev> make cannot do any kind of dependency resolution, it assumes that whatever you need is right there. That leads to configure scripts, which like makefiles, are not standard The ancient convention there is \"make configure\", which sets up whatever \"make [build]\" needs. reply evilotto 12 hours agoparentprevthe only thing I really miss in make is the ability to resolve mtime as something other than mtime. So I resort to using touchfiles which are gross but still work better than a lot of other things (I'm looking at you, docker build caching). reply AdamJacobMuller 14 hours agoprevAgree with the sentiment here but I've been rewriting lots of things to use Justfiles instead https://github.com/casey/just Avoids lots of weird makefileisims reply OutOfHere 14 hours agoparentIt's true, although GPT has given Makefiles a second life by helping write them, delaying their demise. reply metaltyphoon 14 hours agoparentprevSame. Bonus that the same file works on Windows too reply shepherdjerred 1 hour agoprevMany have mentioned just, but I'm a much bigger fan of Earthly [0]. It allows you to write something similar to a Makefile, but everything runs in Docker. This gets you isolated builds with parallelism and caching built-in. I've found it to be great especially for small to medium projects. For some examples, I use it to publish my personal site/blog [1] and to build a C/C++/Fortran/Rust cross-compiler targeting macOS [2]. [0]: https://earthly.dev/ [1]: https://github.com/shepherdjerred/sjer.red/blob/main/Earthfi... [2]: https://github.com/shepherdjerred/macos-cross-compiler/blob/... reply davidcalloway 12 hours agoprevI like Makefiles as well, and although many people have commented on the limitations and the fact that the author's usage of make is fairly simplistic, I think it's great to get started with the basics. Kudos to the author for writing this up and _not_ feeling the need to learn every last bit of make and do everything \"properly\" before sharing. I've worked on a team where GitLab CI pipelines replaced Makefiles, and I was asked not to commit a makefile to the project because it's a customized developer workflow. They were allergic to local testing, but I thought it was a great way to just store and share knowledge about how to build, test, clean, etc. Far easier to read the GitLab CI files (which yes of course were also necessary and served a different porpoise). reply happy_bzy 14 hours agoprevIn those cases, what author really needs is just[1] not make. [1] https://just.systems/man/en/ reply oguz-ismail 14 hours agoparentjust is just another dependency. make is available everywhere reply OutOfHere 14 hours agorootparentMake is available in a lot of places, but not everywhere. It has to explicitly be installed in containers and in some distributions. reply metaltyphoon 14 hours agorootparentprevI rather have one justfile than have one make and another nmake to support Windows reply norir 14 hours agoprevFor me, make has two fatal flaws: 1) the lack of a builtin scripting language 2) poor recursion support The problem with the lack of a scripting language is that I either have to do horrible shell contortions to do simple things like using a temporary file in a recipe or write a standalone script that doesn't live in the Makefile which is needless indirection. This is exacerbated by Make interpreting newlines in the recipe as a separate shell invocation, which I consider a poor design choice. It also requires needless forking for many small tasks which could be done more efficiently in process. The lack of proper recursion means that I either have to use recursive make, which is largely considered an anti-pattern, or I have to use a flat directory structure. What Make does have going for it is ubiquity and good performance for small projects. It is the tool most projects should probably start with and only switch to something more advanced when its scalability issues become a genuine problem. reply saurik 14 hours agoparentYou can use .ONESHELL: to switch to the shell behavior you are wanting. https://www.gnu.org/software/make/manual/html_node/One-Shell... reply spc476 12 hours agorootparentThat's for GNU make, not POSIX make [1]. For some people, they either won't, or can't, use GNU make. [1] https://pubs.opengroup.org/onlinepubs/9799919799/utilities/m... reply saurik 12 hours agorootparentWho even implements an alternative implementation to GNU make? FWIW, no one \"can't\" use GNU make... even Apple uses GNU make (hell: they even ship GNU make, lol). reply tmtvl 7 hours agorootparentBSD Make exists. Also, are you saying that no one is developing software under a contract where they aren't allowed to install software on the target machine? I'm also unsure whether GNU Make works on embedded systems. reply gcarvalho 7 hours agoprevMany responses suggesting a simple bash script instead. One reason I like make is that I can just tab-autocomplete targets. No extra setup needed. If you’re using a single entry point script (e.g. do.sh) and handling $1 you don’t get that for free. And the moment you need to make your entry point script aware of “B requires A” then you’re going to half-bake something similar to make, anyway. Here’s [1] my ~80 line version for Python projects (micromamba + uv) which I’ve been pretty happy with. [1] https://github.com/giovannipcarvalho/micromamba.mk reply deepsun 4 hours agoparent> tab-autocomplete bin/ also perfectly tab-autocompletes, don't understand what you mean. If you're having only one bin/do.sh script -- you're doing it wrong. And if you know $1 can only be either \"prod\" or \"dev\", then create scripts build-prod.sh and build-dev.sh. Simpler, installed on all systems, no quirky syntax with tabs and phonys. reply sfink 13 hours agoprevI do this a fair amount as well. It's really just a way of documenting the configuration and idiosyncratic commands in one place, which happens to be executable. I will happily create (uncommitted) Makefiles with hardcoded paths and keys and things, since otherwise that information would go in my ~/NOTES file and there's too much in there already. My default target tends to echo out things that I told myself I needed to remember when coming back to the project. As soon as I notice I'm reaching for anything more than `.PHONY` targets and dead-simple filename dependencies, I stop and do the real build work in something else (callable via a make target, of course!) I know how to do complicated stuff with make, which means that I know I will do it wrong. Repeatedly. Or possibly eventually do it right, but then have to maintain the resulting fire-breathing hairball. (But to those complaining about not marking all the non-file targets `.PHONY`: lighten up. If the correctness matters so much that you're going to be messed up by a file named `all` or `build` or whatever, you've probably already gone too far down the rabbit hole and should switch to something else.) reply ReleaseCandidat 12 hours agoparent> If the correctness matters so much that you're going to be messed up by a file named `all` or `build`... That's not the problém. _We_ know what \"dev is already up to dáte\" means, but chances are people who don't know about `.PHONY` don't. reply kitd 11 hours agoprevI don't often interact with make files so when I do, I usually need to have a reference at hand. This is the best I've found yet: https://makefiletutorial.com/ reply matheusmoreira 9 hours agoparentMore resources: https://make.mad-scientist.net https://www.cmcrossroads.com/users/john-graham-cumming https://nostarch.com/gnumake https://gmsl.jgc.org reply multani 9 hours agoprevI like Makefiles too :) I use them more or less as a command runner, not often to build new targets based on sources (sometimes still). In particular, I like: * The ubiquity: it's easily available almost everywhere I touch and if not, it's usually a package install away. * The auto completion: I often define variables with default values at the top, but they can be both easily discoverable and their values can be changed just by typing `make VAR=foobar ...` * Chaining commands: make targets can be chained with `make target1 target2 target3 ...`. They will execute in the order specified. If I run this too often, I can usually create a 'ew make target that chains them all. Make is definitely not perfect and could be simpler. My biggest griefs are: * The abscons list of built-in variable. I can only remember a few of them (\"dollar duck `$` from the root of the repo. Such a makefile is always (1) version controlled as a secret github gist (though, as personal rule, i never hardcode secrets into it), (2) committed & pushed on `make` (3) git ignored via `.git/info/exclude`. This has worked quite well for me. One downside with this approach is that the best syntax for passing parameters down to the target from the shell is to use environment variables, which is a little awkward. `NAME=value make target` is less pleasant than `make target --name=value` would have been. reply Izkata 13 hours agoparentTake out the dashes and that's a supported syntax for overriding variables: NAME := foo test: echo \"name is $(NAME)\" $ make test echo name is foo name is foo $ make test NAME=bar echo name is bar name is bar reply donatj 7 hours agoprevI use Make fairly similarly. One fairly major footgun the author is not avoiding is their targets will break if files or folders of the same name exist. You should always .PHONY any non-file targets though lest you want your build to break suddenly and confusingly when you add a folder named say \"build\" in the authors case. Make checks file modification dates by default to see if it can eliminate steps. Setting a target as a .PHONY indicates it's a \"fake target\" as in a command set to be run and not a file to be written. Makes default assumption is it's being ran to \"make a file\" so \"make foo.html\" or such. Here's a very simple example https://github.com/donatj/force-color.org/blob/dev/Makefile reply desdenova 6 hours agoprevIf you like Makefiles to run random tasks, and insist on not using the proper syntax for that, use `just` instead. It uses the same syntax, but actually for running tasks and not producing files, so you don't need .PHONY statements. reply MayeulC 8 hours agoprevI also like Makefiles, but some design choices are really dated. My latest gripe with it was its extremely poor support for filenames with spaces and special chars (;) in them: I just wanted to convert a flac library to MP3 files, it seemed well suited for the job at first glance. I don't know what tool could be a proper replacement. scons perhaps? ninja files are too verbose. just doesn't do dependencies, job control, etc. reply dented42 12 hours agoprevMake has been my favourite unix tool for years and a really useful tool to have in your pocket. It’s simple, elegant, and powerful. reply ac130kz 13 hours agoprevI do like \"just\" being suggested, but I strongly prefer using a very simple bash run script for tasks that do not require \"make\"'s extras, especially given that most modern build tools do parallelism and artifact caching internally. Inspired by: https://github.com/adriancooney/Taskfile https://death.andgravity.com/run-sh reply morningsam 12 hours agoparentI would theoretically prefer that as well, but it doesn't give you shell completion of targets/tasks for free like Make does. So for \"UX parity\", you'd also have to write a completion script and get users to source it into their shells somehow, which isn't great. reply iblaine 11 hours agoprevThe correct answer is I don’t like makefiles when they are abused. They have no state yet people try use them as such and create pain for others. reply deepsun 14 hours agoprevSame things can be said about shell scripts in a bin/ folder. reply pletnes 11 hours agoprevI agree with everything, except that I moved to just, which runs fine on win/mac/nix and is a single-file no-dependency task runner made for this use case. It irons over a lot of warts like working directory, loads dotenv files, lets you write multiline scripts, it’s just magic. reply pletnes 11 hours agoparenthttps://github.com/casey/just https://just.systems/ reply nsonha 11 hours agorootparentStop spamming the thread with petty tool people don't actually need. reply pletnes 10 hours agorootparentJust really is great, you should just give just a try. I put all the project-specific incantations in my Justfile and save my teammates lots of typing and copy-pasta. reply sethammons 6 hours agoprevHere is a quality of life one for me (if you copy pasta, remember to switch spaces to tabs). When you type `make targets`, you get a list of available targets. Kinda like `just --list`. .PHONY: targets targets: @make -qpawk -F: '/^[a-zA-Z0-9][^$#\\/\\t=]*:([^=]|$$)/ {split($$1,A,/ /);for(i in A)print A[i]}'sort reply anymouse123456 6 hours agoprevHere are some things I love about Make: * Already installed * Does not require some random runtime * Does not require some random runtime version * Same build system for (nearly) all languages * The dependency tree can be constructed incrementally from simple, composable, verifiable building blocks * Incremental builds are trivial * Task bodies are (basically) shell scripts * Isolated complexity can be moved to external shell scripts and called from a task body * Unlike fad-language-build-systems learning is amortized across decades * It's not CMake reply alex-moon 11 hours agoprevI also use make this way and have done for years. I even have the same kind of religious ritual the author has, like writing the Makefile is part of setting up the codebase and organising in my own head how the whole local dev environment is going to work. The only thing is, this isn't what make is actually for. A number of commenters have recommended Just - the one I've been using on my personal projects is Task - https://taskfile.dev/ - which is pretty great. As other commenters have said, the problem is that make is installed everywhere already. I would love to see a task runner become standard to the same extent, and will have a look at Just if that's the one people are using. reply pletnes 11 hours agoparentThing is, make is not readily available on windows. It should’ve been in git bash, in my opinion, but just fills the gap in a cross-platform way reply droelf 8 hours agorootparentPixi is native on Windows, can install a wide range of dev tools and has task running built into projects (alongside dependency management). https://pixi.sh/ reply jampekka 9 hours agorootparentprevNo dev tools are readily available on Windows. reply metaltyphoon 7 hours agorootparentOr macos reply MrVandemar 13 hours agoprevI instictively know that makefiles would make a lot of things easier, but I've never found the right tutorial that would help me understand them. I'm not a 'C' programmer, and so much seems weighted to the idea that you're generating object files and linking and producing a.out. Any good tutorials or resources for learning that show a broader applicability for makefiles? reply rramadass 13 hours agoparentSome simple examples : https://stackoverflow.com/questions/101986/what-are-the-othe... and https://ivan.sh/make/ A good detailed example; Using GNU Make to Manage the Workflow of Data Analysis Projects (pdf) here : https://www.jstatsoft.org/article/download/v094c01/1368 More generally, also take a look at how to use Unix Tools effectively, see; Unix : Concepts and Applications by Sumitabha Das and The Unix Programming Environment by Kernighan & Pike. reply hgs3 13 hours agoparentprevThere is much misunderstanding about Makefiles, what they are, and what they are not. Make is not a \"programming language build system\" as some would imply, but rather a recipe builder. With Make you provide the file you want built, the shell commands to build it, and any files the build depends on. Here's a simple example to get you started. Create a file named \"Makefile\" with the following text and an empty file alongside it named foo.txt. bar.txt: foo.txt cp foo.txt bar.txt When you run the \"make\" command in your shell it will check if the \"bar.txt\" file exists. If it does not exist OR if \"foo.txt\" has a newer timestamp, then it will rebuild \"bar.txt\" by executing the tab indented shell commands underneath. In this case, the only shell command used is the 'cp' command. In the linked article the author invokes npm, bundler, and netlify. When people use Make to compile their C code they are simply invoking the C compiler just like they would any other shell command. You might have seen something in a Makefile that looks like this: foobar.o: foobar.c gcc -c foobar.c This is just saying: \"The output file 'foobar.o' depends on the input file 'foobar.c' and to build 'foobar.o' run the shell command 'gcc -c foobar.c'\" which is conceptually the same as my previous example were we built \"bar.txt\". Since explicitly listing every .o and .c file in a Makefile is tedious, many authors opt for wildcards. I hope this helped! reply Jach 12 hours agorootparent> When you run the \"make\" command in your shell it will check if the \"bar.txt\" file exists. I got to this point and ran into an error: Makefile:2: *** missing separator. Stop. Ok I'm just giving you a hard time, and you mention right after the existence of \"tab indented\" so whatever. Still it's one of the things I detest on an aesthetic level about make, even if my editor has special syntax support for makefiles to handle this archaic requirement of actual tabs without me ever having to worry about it in practice. reply evilotto 12 hours agorootparentI have a suspicion that a lot of people who rant about makefiles using tabs also praise python for the brilliance of using whitespace for scoping. Or who love yaml for its indented block structure. Nah, who am I kidding ... no one loves yaml, it's just better than most of the alternatives. reply Jach 11 hours agorootparentI quite like python's whitespace requirements, but notably it doesn't care what your stance is on tabs vs spaces, or how many spaces, so long as you're consistent in a block. Never liked yaml, though, I don't think it's better than any of the alternatives. reply imtringued 9 hours agorootparentFor what I use YAML for, there are no alternatives for it except obscure formats like HCL. reply Izkata 12 hours agorootparentprev> Still it's one of the things I detest on an aesthetic level about make There's a little-known variable called .RECIPEPREFIX that lets you switch from tabs to anything else. Probably a bad idea to use it in anything shared with anyone else. reply spc476 12 hours agorootparentThat's for GNU make, not POSIX make [1]. For some people, they either won't, or can't, use GNU make. [1] https://pubs.opengroup.org/onlinepubs/9799919799/utilities/m... reply tpoacher 13 hours agoparentprevunironically, the info manual. It's great. the philosophical \"c\" centredness is true, but doesn't get in the way of using other languages. (there's things like indirect rules for compiling .c files auromatically for instance, but even this can be turned off) reply Izkata 13 hours agoparentprev> and so much seems weighted to the idea that you're generating object files and linking and producing a.out. Well that's kind of wrong (it's used for that but that's an extremely limited viewpoint). Here's a short introduction to get started: foo: bar baz This means \"whenever bar has been updated, create or recreate foo by running baz\". You run it with \"make foo\", and make will run \"baz\" by default in \"sh\". Here's an example in a totally different context: .PHONY: build build: node_modules node_modules: package.json yarn.lock yarn install touch node_modules With this, when you run \"make build\", it'll only do \"yarn install\" if node_modules's last-modified timestamp is older than both package.json and yarn.lock. The touch is there to mark it updated for the next time you run \"make build\", so it knows it doesn't have to do anything. Normally you wouldn't have to do that but make assumes the commands given will update the file, and \"yarn install\" won't necessarily update the directory's last-modified time. This example isn't terribly useful because \"yarn install\" is fast and doesn't do anything itself when it's up-to-date, but it should give ideas about how flexible make actually is. One of the big criticisms of how people use \"make\", and why people recommend things like \"just\" instead, is they don't bother to use that functionality (or any of the piles of stuff built on top of it like pattern matching) and would have just done: build: yarn install ...which appears to be how OP uses it. reply ivanjermakov 8 hours agoprevWhile these npm examples are easy to grasp, they do not reflect any strength of Make. With same success one could use package.json's script property, code would be pretty much identical. reply wonrax 1 hour agoparentExactly. If your project already uses npm and Node, why not just use npm scripts for trivial tasks like this? Adding an extra dependency (Make) to build a project doesn't make sense. Even if your project needs a more sophisticated build system that requires caching builds and managing conditional dependencies, turborepo and the like offer even better support for javascript codebases out of the box. Turborepo can be a workspace dependency so technically you don't even have to manually and separately install a build system to build your project. reply molszanski 7 hours agoprevHere is Makefile \"starter\" I use: https://github.com/awinecki/magicfile People call this \"self-documenting makefile\". It migrated with me from company to company, from project to projects. Through node, php, aws, docker, server management, cert updates, file processing and many many more. reply ReleaseCandidat 13 hours agoprevWhy use Make if they already use Gulp? Why not put that in `package.json`'s `script` stanza? And never ever use Make as a script runner without declaring the targets `.PHONY`, there will be a day when somebody has a directory (or file) named `build` or `dev` in their project root. reply sfink 13 hours agoparentBecause my Rust, Python, bash, Perl, etc. projects aren't impressed with the `package.json` file I wave at them. Especially before I've installed npm or any other JS runtime on my system (let alone Gulp). As the article said, it's generally installed everywhere as soon as you install any dev-related stuff. So is bash, but it's a little clunkier for very",
    "originSummary": [
      "Makefiles, despite the emergence of new build tools, remain a popular choice for project automation due to their simplicity and standardized commands.",
      "They are particularly useful for setting up and managing projects with commands like `make dev`, `make build`, and `make install`, which streamline development processes.",
      "Makefiles are versatile, requiring fewer dependencies and can be used to orchestrate various tools like Docker and gulp, making them ideal for restricted environments and diverse project setups."
    ],
    "commentSummary": [
      "The discussion revolves around the use of Makefiles, a build automation tool, highlighting its simplicity and flexibility for small projects but also its potential complexity for larger ones.",
      "Key points of contention include the necessity of `.PHONY` for non-output generating rules, portability issues, and the challenges of maintaining Makefiles across different environments.",
      "The debate underscores the balance between using Make for its ubiquity and simplicity versus adopting more complex or modern build systems for larger or more intricate projects."
    ],
    "points": 349,
    "commentCount": 329,
    "retryCount": 0,
    "time": 1726886264
  },
  {
    "id": 41609393,
    "title": "Forget ChatGPT: why researchers now run small AIs on their laptops",
    "originLink": "https://www.nature.com/articles/d41586-024-02998-y",
    "originBody": "TECHNOLOGY FEATURE 16 September 2024 Forget ChatGPT: why researchers now run small AIs on their laptops Artificial-intelligence models are typically used online, but a host of openly available tools is changing that. Here’s how to get started with local AIs. By Matthew Hutson Twitter Facebook Email Illustration: The Project Twins The website histo.fyi is a database of structures of immune-system proteins called major histocompatibility complex (MHC) molecules. It includes images, data tables and amino-acid sequences, and is run by bioinformatician Chris Thorpe, who uses artificial intelligence (AI) tools called large language models (LLMs) to convert those assets into readable summaries. But he doesn’t use ChatGPT, or any other web-based LLM. Instead, Thorpe runs the AI on his laptop. Chatbots in science: What can ChatGPT do for you? Over the past couple of years, chatbots based on LLMs have won praise for their ability to write poetry or engage in conversations. Some LLMs have hundreds of billions of parameters — the more parameters, the greater the complexity — and can be accessed only online. But two more recent trends have blossomed. First, organizations are making ‘open weights’ versions of LLMs, in which the weights and biases used to train a model are publicly available, so that users can download and run them locally, if they have the computing power. Second, technology firms are making scaled-down versions that can be run on consumer hardware — and that rival the performance of older, larger models. Researchers might use such tools to save money, protect the confidentiality of patients or corporations, or ensure reproducibility. Thorpe, who’s based in Oxford, UK, and works at the European Molecular Biology Laboratory’s European Bioinformatics Institute in Hinxton, UK, is just one of many researchers exploring what the tools can do. That trend is likely to grow, Thorpe says. As computers get faster and models become more efficient, people will increasingly have AIs running on their laptops or mobile devices for all but the most intensive needs. Scientists will finally have AI assistants at their fingertips — but the actual algorithms, not just remote access to them. Big things in small packages Several large tech firms and research institutes have released small and open-weights models over the past few years, including Google DeepMind in London; Meta in Menlo Park, California; and the Allen Institute for Artificial Intelligence in Seattle, Washington (see ‘Some small open-weights models’). (‘Small’ is relative — these models can contain some 30 billion parameters, which is large by comparison with earlier models.) Some small open-weights models Developer Model Parameters Allen Institute for AI OLMo-7B 7 billion Alibaba Qwen2-0.5B 0.5 billion Apple DCLM-Baseline-7B 7 billion Google DeepMind Gemma-2-9B 9 billion Google DeepMind CodeGemma-7B 7 billion Meta Llama 3.1-8B 8 billion Microsoft Phi-3-medium-128K-Instruct 14 billion Mistral AI Mistral-Nemo-Base-2407 12 billion Although the California tech firm OpenAI hasn’t open-weighted its current GPT models, its partner Microsoft in Redmond, Washington, has been on a spree, releasing the small language models Phi-1, Phi-1.5 and Phi-2 in 2023, then four versions of Phi-3 and three versions of Phi-3.5 this year. The Phi-3 and Phi-3.5 models have between 3.8 billion and 14 billion active parameters, and two models (Phi-3-vision and Phi-3.5-vision) handle images1. By some benchmarks, even the smallest Phi model outperforms OpenAI’s GPT-3.5 Turbo from 2023, rumoured to have 20 billion parameters. Sébastien Bubeck, Microsoft’s vice-president for generative AI, attributes Phi-3’s performance to its training data set. LLMs initially train by predicting the next ‘token’ (iota of text) in long text strings. To predict the name of the killer at the end of a murder mystery, for instance, an AI needs to ‘understand’ everything that came before, but such consequential predictions are rare in most text. To get around this problem, Microsoft used LLMs to write millions of short stories and textbooks in which one thing builds on another. The result of training on this text, Bubeck says, is a model that fits on a mobile phone but has the power of the initial 2022 version of ChatGPT. “If you are able to craft a data set that is very rich in those reasoning tokens, then the signal will be much richer,” he says. ChatGPT for science: how to talk to your data Phi-3 can also help with routing — deciding whether a query should go to a larger model. “That’s a place where Phi-3 is going to shine,” Bubeck says. Small models can also help scientists in remote regions that have little cloud connectivity. “Here in the Pacific Northwest, we have amazing places to hike, and sometimes I just don’t have network,” he says. “And maybe I want to take a picture of some flower and ask my AI some information about it.” Researchers can build on these tools to create custom applications. The Chinese e-commerce site Alibaba, for instance, has built models called Qwen with 500 million to 72 billion parameters. A biomedical scientist in New Hampshire fine-tuned the largest Qwen model using scientific data to create Turbcat-72b, which is available on the model-sharing site Hugging Face. (The researcher goes only by the name Kal’tsit on the Discord messaging platform, because AI-assisted work in science is still controversial.) Kal’tsit says she created the model to help researchers to brainstorm, proof manuscripts, prototype code and summarize published papers; the model has been downloaded thousands of times. Preserving privacy Beyond the ability to fine-tune open models for focused applications, Kal’tsit says, another advantage of local models is privacy. Sending personally identifiable data to a commercial service could run foul of data-protection regulations. “If an audit were to happen and you show them you’re using ChatGPT, the situation could become pretty nasty,” she says. Cyril Zakka, a physician who leads the health team at Hugging Face, uses local models to generate training data for other models (which are sometimes local, too). In one project, he uses them to extract diagnoses from medical reports so that another model can learn to predict those diagnoses on the basis of echocardiograms, which are used to monitor heart disease. In another, he uses the models to generate questions and answers from medical textbooks to test other models. “We are paving the way towards fully autonomous surgery,” he explains. A robot trained to answer questions would be able to communicate better with doctors. Zakka uses local models — he prefers Mistral 7B, released by the tech firm Mistral AI in Paris, or Meta’s Llama-3 70B — because they’re cheaper than subscription services such as ChatGPT Plus, and because he can fine-tune them. But privacy is also key, because he’s not allowed to send patients’ medical records to commercial AI services. Inside the maths that drives AI Johnson Thomas, an endocrinologist at the health system Mercy in Springfield, Missouri, is likewise motivated by patient privacy. Clinicians rarely have time to transcribe and summarize patient interviews, but most commercial services that use AI to do so are either too expensive or not approved to handle private medical data. So, Thomas is developing an alternative. Based on Whisper — an open-weight speech-recognition model from OpenAI — and on Gemma 2 from Google DeepMind, the system will allow physicians to transcribe conversations and convert them to medical notes, and also summarize data from medical-research participants. Privacy is also a consideration in industry. CELLama, developed at the South Korean pharmaceutical company Portrai in Seoul, exploits local LLMs such as Llama 3.1 to reduce information about a cell’s gene expression and other characteristics to a summary sentence2. It then creates a numerical representation of this sentence, which can be used to cluster cells into types. The developers highlight privacy as one advantage on their GitHub page, noting that CELLama “operates locally, ensuring no data leaks”. Putting models to good use As the LLM landscape evolves, scientists face a fast-changing menu of options. “I’m still at the tinkering, playing stage of using LLMs locally,” Thorpe says. He tried ChatGPT, but felt it was expensive, and the tone of its output wasn’t right. Now he uses Llama locally, with either 8 billion or 70 billion parameters, both of which can run on his Mac laptop. Another benefit, Thorpe says, is that local models don’t change. Commercial developers, by contrast, can update their models at any moment, leading to different outputs and forcing Thorpe to alter his prompts or templates. “In most of science, you want things that are reproducible,” he explains. “And it’s always a worry if you’re not in control of the reproducibility of what you’re generating.” For another project, Thorpe is writing code that aligns MHC molecules on the basis of their 3D structure. To develop and test his algorithms, he needs lots of diverse proteins — more than exist naturally. To design plausible new proteins, he uses ProtGPT2, an open-weights model with 738 million parameters that was trained on about 50 million sequences3. Sometimes, however, a local app won’t do. For coding, Thorpe uses the cloud-based GitHub Copilot as a partner. “It kind of feels like my arm’s chopped off when for some reason I can’t actually use Copilot,” he says. Local LLM-based coding tools do exist (such as Google DeepMind’s CodeGemma and one from California-based developers Continue), but in his experience they can’t compete with Copilot. Access points So, how do you run a local LLM? Software called Ollama (available for Mac, Windows and Linux operating systems) lets users download open models, including Llama 3.1, Phi-3, Mistral and Gemma 2, and access them through a command line. Other options include the cross-platform app GPT4All and Llamafile, which can transform LLMs into a single file that runs on any of six operating systems, with or without a graphics processing unit. NatureTech hub Sharon Machlis, a former editor at the website InfoWorld, who lives in Framingham, Massachusetts, wrote a guide to using LLMs locally, covering a dozen options. “The first thing I would suggest,” she says, “is to have the software you choose fit your level of how much you want to fiddle.” Some people prefer the ease of apps, whereas others prefer the flexibility of the command line. Whichever approach you choose, local LLMs should soon be good enough for most applications, says Stephen Hood, who heads open-source AI at the tech firm Mozilla in San Francisco. “The rate of progress on those over the past year has been astounding,” he says. As for what those applications might be, that’s for users to decide. “Don’t be afraid to get your hands dirty,” Zakka says. “You might be pleasantly surprised by the results.” Nature 633, 728-729 (2024) doi: https://doi.org/10.1038/d41586-024-02998-y References Abdin, M. et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2404.14219 (2024). Choi, H. et al. Preprint at bioRxiv https://doi.org/10.1101/2024.05.08.593094 (2024). Ferruz, N. et al. Nature Commun. 13, 4348 (2022). Article PubMed Google Scholar Download references Latest on: Technology Machine learning Computer science AI’s international research networks mapped NATURE INDEX 18 SEP 24 Rise of ChatGPT and other tools raises major questions for research NATURE INDEX 18 SEP 24 Artificial intelligence laws in the US states are feeling the weight of corporate lobbying NATURE INDEX 18 SEP 24 Jobs Assistant Professor in Molecular and Cellular Biophysics Vanderbilt University seeks an outstanding individual for a tenure-track faculty position in molecular and cellular biophysics. The candidate will ... Nashville, Tennessee Vanderbilt University - Department of Biological Sciences Associate or Senior Editor (Ecology), Nature Ecology & Evolution Job Title: Associate or Senior Editor (Ecology), Nature Ecology & Evolution Location: New York, Jersey City, Philadelphia, Beijing or Shanghai - Hy... New York City, New York (US) Springer Nature Ltd Faculty position, Department of Oncology - Division of Quality of Life and Palliative Care Memphis, Tennessee St. Jude Children's Research Hospital (St. Jude) Principal Investigator Positions at the Chinese Institutes for Medical Research, Beijing Cancer Biology, Molecular and Cellular Therapeutics, Regenerative Medicine, Immunology and Infectious Diseases, Genetics and etc... Beijing, China The Chinese Institutes for Medical Research (CIMR), Beijing Immunology PI positions – The Chinese Institutes for Medical Research CIMR is committed to building a world-class medical research hub and fostering a diverse and inclusive work environment. Beijing, China The Chinese Institutes for Medical Research (CIMR), Beijing",
    "commentLink": "https://news.ycombinator.com/item?id=41609393",
    "commentBody": "Forget ChatGPT: why researchers now run small AIs on their laptops (nature.com)282 points by rbanffy 7 hours agohidepastfavorite190 comments noman-land 2 hours agoFor anyone who hasn't tried local models because they think it's too complicated or their computer can't handle it, download a single llamafile and try it out in just moments. https://future.mozilla.org/builders/news_insights/introducin... https://github.com/Mozilla-Ocho/llamafile They even have whisperfiles now, which is the same thing but for whisper.cpp, aka real-time voice transcription. You can also take this a step further and use this exact setup for a local-only co-pilot style code autocomplete and chat using Twinny. I use this every day. It's free, private, and offline. https://github.com/twinnydotdev/twinny Local LLMs are the only future worth living in. reply vunderba 38 minutes agoparentIf you're gonna go with a VS code extension and you're aiming for privacy, then I would at least recommend using the open source fork VS Codium. https://vscodium.com/ reply unethical_ban 31 minutes agorootparentIt is true that VS Code has some non-optional telemetry, and if VS Codium works for people, that is great. However, the telemetry of VSCode is non-personal metrics, and some of the most popular extensions are only available with VSCode, not with Codium. reply wkat4242 12 minutes agorootparent> and some of the most popular extensions are only available with VSCode, not with Codium Which is an artificial restriction from MS that's really easily bypassed. Personally I don't care whether the telemetry is identifiable. I just don't want it. reply noman-land 6 minutes agorootparentHow is it bypassed? reply wkat4242 4 minutes agorootparentThere's a whitelist identifier that you can add bundle IDs to, to get access to the more sensitive APIs. Then you can download the extension file and install it manually. I don't have the exact process right now but just Google it :) kaoD 9 minutes agoparentprevWell this was my experience... User: Hey, how are you? Llama: [object Object] It's funny but I don't think I did anything wrong? reply throwup238 1 minute agorootparentYou're supposed to use the triple comma (,,,). A single comma causes an unexpected type coercion within the LLM matrix. reply _kidlike 1 hour agoparentprevOr https://ollama.com/ reply zelphirkalt 42 minutes agoparentprevMany setup rely on Nvidia GPUs, Intel stuff, Windows or other stuff, that I would rather not use, or are not very clear about how to set things up. What are some recommendations for running models locally, on decent CPUs and getting good valuable output from them? Is that llama stuff portable across CPUs and hardware vendors? And what do people use it for? reply noman-land 33 minutes agorootparentllamafile will run on all architectures because it is compiled by cosmopolitan. https://github.com/jart/cosmopolitan \"Cosmopolitan Libc makes C a build-once run-anywhere language, like Java, except it doesn't need an interpreter or virtual machine. Instead, it reconfigures stock GCC and Clang to output a POSIX-approved polyglot format that runs natively on Linux + Mac + Windows + FreeBSD + OpenBSD + NetBSD + BIOS with the best possible performance and the tiniest footprint imaginable.\" I use it just fine on a Mac M1. The only bottleneck is how much RAM you have. I use whisper for podcast transcription. I use llama for code complete and general q&a and code assistance. You can use the llava models to ingest images and describe them. reply threecheese 32 minutes agorootparentprevHave you tried a Llamafile? Not sure what platform you are using. From their readme: > … by combining llama.cpp with Cosmopolitan Libc into one framework that collapses all the complexity of LLMs down to a single-file executable (called a \"llamafile\") that runs locally on most computers, with no installation. Low cost to experiment IMO. I am personally using MacOS with an M1 chip and 64gb memory and it works perfectly, but the idea behind this project is to democratize access to generative AI and so it is at least possible that you will be able to use it. reply narrator 11 minutes agorootparentWith 64GB can you run the 70B size llama models well? reply wkat4242 11 minutes agorootparentprevNot really. I run ollama on an AMD Radeon Pro and it works great. For tooling to train models it's a bit more difficult but inference works great on AMD. My CPU is an AMD Ryzen and the OS Linux. No problem. I use OpenWebUI as frontend and it's great. I use it for everything that people use GPT for. reply wkat4242 23 minutes agoparentprevYeah I set up a local server with a strong GPU but even without that it's ok, just a lot slower. The biggest benefits for me are the uncensored models. I'm pretty kinky so the regular models tend to shut me out way too much, they all enforce this prudish victorian mentality that seems to be prevalent in the US but not where I live. Censored models are just unusable to me which includes all the hosted models. It's just so annoying. And of course the privacy. It should really be possible for the user to decide what kind of restrictions they want, not the vendor. I understand they don't want to offer violent stuff but 18+ topics should be squarely up to me. Lately I've been using grimjim's uncensored llama3.1 which works pretty well. reply heyoni 1 hour agoparentprevIsn’t there also some Firefox AI integration that’s being tested by one dev out there? I forgot the name and wonder if it got any traction. reply privacyis1mp 59 minutes agoparentprevI built Fluid app exactly with that in mind. You can run local AI on mac without really knowing what an LLM/ollama is. Plug&Play. Sorry for the blatant ad, though I do hope it's useful for some ppl reading this thread: https://getfluid.app reply twh270 6 minutes agorootparentI'm interested, but I can't find any documentation for it. Can I give it local content (documents, spreadsheets, code, etc.) and ask questions? reply toddmorey 2 hours agoprevI narrate notes to myself on my morning walks[1] and then run whisper locally to turn the audio into text... before having an LLM clean up my ramblings into organized notes and todo lists. I have it pretty much all local now, but I don't mind waiting a few extra seconds for it to process since it's once a day. I like the privacy because I was never comfortable telling my entire life to a remote AI company. [1] It feels super strange to talk to yourself, but luckily I'm out early enough that I'm often alone. Worst case, I pretend I'm talking to someone on the phone. reply vunderba 1 hour agoparentSame. My husky/pyr mix needs a lot of exercise, so I'm outside a minimum of a few hours a day. As a result I do a lot of dictation on my phone. I put together a script that takes any audio file (mp3, wav), normalizes it, runs it through ggerganov's whisper, and then cleans it up using a local LLM. This has saved me a tremendous amount of time. Even modestly sized 7b parameter models can handle syntactical/grammatical work relatively easily. Here's the gist: https://gist.github.com/scpedicini/455409fe7656d3cca8959c123... EDIT: I've always talked out loud through problems anyway, throw a BT earbud on and you'll look slightly less deranged. reply schmidtleonard 2 hours agoparentprevButton-toggled voice notes in the iPhone Notes app are a godsend for taking measurements. Rather than switching your hands between probe/equipment and notes repeatedly, which sucks badly, you can just dictate your readings and maaaaybe clean out something someone said in the background. Over the last decade, the microphones + speech recognition became Good Enough for this. Wake-word/endpoint models still aren't there yet, and they aren't really close, but the stupid on/off button in the Notes app 100% solves this problem and the workflow is now viable. I love it and I sincerely hope that \"Apple Intelligence\" won't kill the button and replace it with a sub-viable conversational model, but I probably ought to figure out local whisper sooner rather than later because it's probably inevitable. reply freetanga 1 hour agorootparentI bought an iZYREC (?) and leave the phone at home. MacWhisper and some regex (I use verbal tags) and done reply neom 35 minutes agoparentprevThis is exactly why I think the AI pins are a good idea. The Humane pin seems too big/too expensive/not quite there yet, but for exactly what you're doing, I would like some type of brooch. reply wkat4242 22 minutes agoparentprevWhat do you use to run whisper locally? I don't think ollama can do it. reply lukan 1 hour agoparentprev\"before having an LLM clean up my ramblings into organized notes and todo lists.\" Which local LLM do you use? Edit: And self talk is quite a healthy and useful thing in itself, but avoiding it in public is indeed kind of necessary, because of the stigma https://en.m.wikipedia.org/wiki/Intrapersonal_communication reply flimflamm 1 hour agorootparentThat's just meat CoT (chain of thought) - right? reply lukan 15 minutes agorootparentI do not understand? reply alyandon 2 hours agoparentprevI would be greatly interested in knowing how you set all that up if you felt like sharing the specifics. reply toddmorey 29 minutes agorootparentMy hope is to make this easy with a GH repo or at least detailed instructions. I'm on a Mac and I found the easiest way to run & use local models is Ollama as it has a rest interface: https://github.com/ollama/ollama/blob/main/docs/api.md I just have a local script that pulls the audio file from Voice Memos (after it syncs from my iPhone), runs it through openai's whisper (really the best at voice to speech; excellent results) and then makes sense of it all with a prompt that asks for organized summary notes and todos in GH flavored markdown. That final output goes into my Obsidian vault. The model I use is llama3.1 but haven't spent much time testing others. I find you don't really need the largest models since the task is to organize text rather than augment it with a lot of external knowledge. Humorously the harder part of the process was finding where the hell Voice Memos actually stores these audio files. I wish you could set the location yourself! They live deep inside ~/Library/Containers. Voice Memos has no export feature, but I found you can drag any audio recording out of the left sidebar to the desktop or a folder. So I just drag the voice memo into a folder my script watches and then it runs the automation. If anyone has another, better option for recording your voice on an iPhone, let me know! The nice thing about all this is you don't even have to start / stop the recording ever on your walk... just leave it going. Dead space and side conversations and commands to your dog are all well handled and never seem to pollute my notes. reply schainks 20 minutes agorootparentAmazing, thank you for this! reply vincvinc 2 hours agoparentprevI was thinking about making this the other day. Would you mind sharing what you used? reply yieldcrv 1 hour agoparentprevI found one that can isolate speakers, its just okay at that reply hdjjhhvvhga 2 hours agoparentprev> It feels super strange to talk to yourself I remember the first lecture in the Theory of Communication class where the professor introduced the idea that communication by definition requires at least two different participants. We objected by saying that it can perfectly be just one and the same participant (communication is not just about space but also time), and what you say is a perfect example of that. reply racked 2 hours agoparentprevWhat software did you use to set all this up? Kindof interested in giving this a shot myself. reply azeirah 2 hours agorootparentYou can use llama.cpp, it runs on almost all hardware. Whisper.cpp is similar, but unless you have a mid or high end nvidia card it will be a bit slower. Still very reasonable on modern hardware. reply bobbylarrybobby 2 hours agorootparentIf you build locally for Apple hardware (instructions in the whisper.cpp readme) then it performs quite admirably on Apple computers as well. reply navbaker 2 hours agorootparentprevDefinitely try it with Ollama, it is by far the simplest local LLM tool to get up and running with minimal fuss! reply pella 5 hours agoprevNext year, devices equipped with AMD's Strix Halo APU will be available, capable of using ~96GB of VRAM across 4 relatively fast channels from a total of 128GB unified memory, along with a 50 TOPS NPU. This could partially serve as an alternative to the MacBook Pro models with M2/M3/M4 chips, featuring 128GB or 196GB unified memory. - https://videocardz.com/newz/amd-ryzen-ai-max-395-to-feature-... reply diggan 4 hours agoparentAccording to Tom's (https://www.tomshardware.com/pc-components/cpus/amd-pushes-r...), those are supposed to be laptop CPUs, which makes me wonder what AMD has planned for us desktop users. reply MobiusHorizons 3 hours agorootparentIf I remember right, in the press conference they suggested desktop users would use a gpu because desktop uses are less power sensitive. That doesn’t address the vram limitations of discrete GPUs though. reply wkat4242 7 minutes agorootparentTrue but try to find a 96GB GPU. reply adrian_b 3 hours agorootparentprevThey are laptop CPUs for bigger laptops, like those that now use both a CPU and a discrete GPU, i.e. gaming laptops or mobile workstations. It seems that the thermal design power for Strix Halo can be configured between 55 W and 120 W, which is similar to the power used now by a combo laptop CPU + discrete GPU. reply aurareturn 4 hours agoparentprevIt will have around 250GB/s of bandwidth which makes it nearly unusable for 70b models. So the high amount of RAM doesn’t help with large models. reply pella 4 hours agorootparentBoth VRAM size and bandwidth are crucial for LLM (Large Language Model) inference. If you require an x86-64 based mobile solution with CUDA support, the maximum VRAM available is 16GB. The Strix HALO is positioned as a competitor to the RTX 4070M. \"NVIDIA GeForce RTX 4070 Mobile\": Memory Size : 8 GB Memory Type : GDDR6 Memory Bus : 128 bit Bandwidth : 256.0 GB/s \"NVIDIA GeForce RTX 4090 Mobile\" Memory Size : 16 GB Memory Type : GDDR6 Memory Bus : 256 bit Bandwidth : 576.0 GB/s reply smcleod 3 hours agorootparentprevThat’s less than half the Ultra Apple silicon chip two generations ago (800GB/s), and just over the current pro (400GB/s). reply bearjaws 3 hours agorootparentprevAnd that is nearly half of what M4 will produce (450GB/s estimated). reply throwaway314155 4 hours agorootparentprev> nearly unusable for 70b models Can Apple Silicon manage this? Would it be feasible to do with some quantization perhaps? reply pella 3 hours agorootparent- \"2 MacBooks is all you need. Llama 3.1 405B running distributed across 2 MacBooks using @exolabs_ home AI cluster\" https://x.com/AIatMeta/status/1834633042339741961 - \"Running Qwen 2.5 Math 72B distributed across 2 MacBooks. Uses @exolabs_ with the MLX backend.\" https://x.com/ac_crypto/status/1836558930585034961 reply pickettd 57 minutes agorootparentI experimented with both Exo and llama.cpp in RPC-server mode this week. Using an M3 Max and an M1 Ultra in Exo specifically I was able to get around 13 tok/s on DeepSeek 2.5 (using MLX and a 4 bit quant with a very small test prompt - so maybe 140 gigs total of model+cache). It definitely took some trial and error but the Exo community folks were super helpful/responsive with debugging/advice. reply bearjaws 3 hours agorootparentprevAny of the newer M2+ Max chips runs 400GB/s and can run 70b pretty well. It's not fast though, 3-4 token/s. You can get better performance using a good CPU + 4090 + offloading layers to GPU. However one is a laptop and the other is a desktop... reply staticman2 2 hours agorootparentApparently Mac purchasers like to talk about tokens per second without talking about Mac's atrocious time to first token. They also like to enthusiastically talk about tokens per second asking a 200 token question rather than a longer prompt. I'm not sure what the impact is on a 70b model but it seems there's a lot of exaggeration going on in this space by Mac fans. reply aurareturn 3 hours agorootparentprevYes at around 8 tokens/s. Also quite slow. reply Dalewyn 4 hours agorootparentprevFast. Large. Cheap. You may only pick two. reply meiraleal 3 hours agorootparentFor a few years ago standard the current \"small\" models like mistral and phind are fast, large and cheap. reply jstummbillig 2 hours agoparentprevAlso, next year, there will be GPT 5. I find it fascinating how much attention small models get, when at the same time the big models just get bigger and prohibitively expensive to train. No leading lab would do that if they thought it a decent chance that small models were able to compete. So who will be interested in a shitty assistant next year when you can have an amazing one, is what I wonder? Is this just the biggest cup of wishful thinking that we have ever seen? reply svnt 2 hours agorootparentI’ll flip this around a bit: If I’ve raised $1B to buy GPUs and train a “bigger model”, a major part of my competitive advantage is having $1B to spend on sufficient GPUs to train a bigger model. If, after having raised that money it becomes apparent that consumer hardware can run smaller models that are optimized and perform as well without all that money going into training them, how am I going to pivot my business to something that works, given these smaller models are released this way on purpose to undermine my efforts? It seems there are two major possibilities: one, people raising billions find a new and expensive intelligence step function that at least time-locally separates them from the pack, or two (and significantly more likely in my view) they don’t, and the improvements come from layering on different systems such as do not require acres of GPUs, while the “more data more GPUs” crowd is found to have hit a nonlinearity that in practical terms means they are generations of technology away from the next tier. reply rvnx 2 hours agorootparentMining cryptos, some \"AI\" companies already do that (knowingly or not... and not necessarily telling investors) reply svnt 2 hours agorootparentIs it still even worth the electricity to do this on a GPU? It wouldn’t surprise me if some startups were renting them out, but is anyone still mining any volume of crypto on GPUs? edit: I guess to your point if it is not knowingly then the electricity costs are not a factor either. reply jstummbillig 1 hour agorootparentprevWhat you suggest is not impossible but simply flies in the face of all currently available evidence and what all leading labs say and do. We know they are actively looking for ways to do things more efficiently. OpenAI alone did a couple of releases to that effect. Because of how easy it is to switch providers, if only one lab found a way to run a small model that competed with the big ones, it would simply win the entire space, so everyone has to be looking for that (and clearly they are, given that all of them do have smaller versions of their models) Scepticism is fine, if it's plausible. If not it's conspiratorial. reply svnt 58 minutes agorootparentThere are at least two different optimizations happening: 1) optimizing the model training 2) optimizing the model operation The $1B-spend holy grail is that it costs a lot of money to train, and almost nothing to operate, a proprietary model that benchmarks and chats better than anyone else’s. OpenAI’s optimizations fall into the latter category. The risk to the business model is in the former — if someone can train a world-beating model without lots of money, it’s a tough day for the big players. reply Larrikin 2 hours agorootparentprevWhy would anyone buy a Raspberry Pi when they can get a fully decked out Mac Pro? There are different use cases and computers are already pretty powerful. Maybe your local model won't be able to produce tests that check all the corner cases of the class you just wrote for work in your massive code base. But the small model is perfectly capable of summarizing the weather from an API call and maybe tack on a joke that can be read out to you on your speakers in the morning. reply talldayo 1 hour agorootparent> Why would anyone buy a Raspberry Pi when they can get a fully decked out Mac Pro? They want compliant Linux drivers? reply archagon 2 hours agorootparentprevIt is unwise to professionally rely on a SAAS offering that can change, increase in price, or even disappear on a whim. reply jabroni_salad 2 hours agorootparentprevOne of the reasons I run local is that the models are completely uncensored and unfiltered. If you're doing anything slightly 'risky' the only thing APIs are good for is a slew of very politely written apology letters, and the definition of 'risky' will change randomly without notice or fail to accommodate novel situations. It is also evident in the moderation that your usage is subject to human review and I don't think that should even be possible. reply Tempest1981 2 hours agorootparentprevThere is also a long time-window before most laptops are upgraded to screaming-fast 128GB AI monsters. Either way, it will be fun to watch the battle. reply wazdra 2 hours agoprevI'd like to point out that llama 3.1 is not open source[1] (I was recently made aware of that fact by [2], when it was on HN front page) While it's very nice to see a peak of interest for local, \"open-weights\" LLMs, this is an unfortunate choice of words, as it undermines the quite important differences between llama's license model and open-source. The license question does not seem to be addressed at all in the article. [1]: https://www.llama.com/llama3_1/license/ [2]: https://csvbase.com/blog/14 reply sergiotapia 6 minutes agoparentthat ship sailed 13 years ago dude. reply albertgoeswoof 3 hours agoprevI have a three year old M1 Max, 32gb RAM. Llama 8bn runs at 25 tokens/sec, that’s fast enough, and covers 80% of what I need. On my ryzen 5600h machine, I get about 10 tokens/second, which is slow enough to be annoying. If I get stuck on a problem, switch to chat gpt or phind.com and see what that gives. Sometimes, it’s not the LLM that helps, but changing the context and rewriting the question. However I cannot use the online providers for anything remotely sensitive, which is more often than you might think. Local LLMs are the future, it’s like having your own private Google running locally. reply fsmv 2 hours agoparentA small model necessarily is missing many facts. The large model is the one that has memorized the whole internet, the small one is just trained to mimic the big one. You simply cannot compress the whole internet under 10gb without throwing out a lot of information. Please be careful about what you take as fact coming from the local model output. Small models are better suited to summarization. reply albertgoeswoof 15 minutes agorootparentI don’t trust anything as fact coming out of these models. I ask it for how to structure solutions, with examples. Then I read the output and research the specifics before using anything further. I wouldn’t copy and paste from even the smartest minds, nevermind a model output. reply staticman2 2 hours agoparentprevI'm really curious what you are doing with an LLM that can be solved 80% of the time with a 8b model. reply albertgoeswoof 14 minutes agorootparentIt’s mostly how would you solve this programming problem, or reminders on syntax, scaffolding a configuration file etc. Often it’s a form of rubber duck programming, with a smarter rubber duck. reply meiraleal 3 hours agoparentprevWe need browser and OS level API (mobile) integration to the local LLM. reply Anunayj 3 hours agoprevI recently experimented with running llama-3.1-8b-instruct locally on my Consumer hardware, aka my Nvidia RTX 4060 with 8GB VRAM, as I wanted to experiment with prompting pdfs with a large context which is extremely expensive with how LLMs are priced. I was able to fit the model with decent speeds (30 tokens/seconds) and a 20k token context completely on the GPU. For summarization, the performance of these models are decent enough. However unfortunately in my use case I felt using Gemini's Free Tier with it's multimodal capabilities and much better quality output made running local LLMs not really worth it as of right now, atleast for consumers. reply mistrial9 39 minutes agoparentyou moved the goalposts when you add 'multimodal' there; another item is, no one reads PDF tables and illustrations perfectly, at any price AFAIK reply leshokunin 4 hours agoprevI like self hosting random stuff on docker. Ollama has been a great addition. I know it's not, but it feels on par with ChatGPT. It works perfectly on my 4090, but I've also seen it work perfectly on my friend's M3 laptop. It feels like an excellent alternative for when you don't need the heavy weights, but want something bespoke and private. I've integrated it with my Obsidian notes for 1) note generation 2) fuzzy search. I've used it as an assistant for mental health and medical questions. I'd much rather use it to query things about my music or photos than whatever the big players have planned. reply ekabod 18 minutes agoparentOllama is not a model, it is the sofware to run models. reply vunderba 1 hour agoparentprevThere's actually a very popular plugin for Obsidian that integrates RAG + LLM into Obsidian called Smart Connections. https://github.com/brianpetro/obsidian-smart-connections reply exe34 4 hours agoparentprevwhich model are you using? what size/quant/etc? thanks! reply smcleod 3 hours agorootparentCome join us on Reddit’s /r/localllama. Great community for local LLMs. reply wongarsu 1 hour agorootparentprevI'd be interested in other people's recommendations as well. Personally I'm mostly using openchat with q5_k_m quantization. OpenChat is imho one of the best 7B models, and while I could run bigger models at least for me they monopolize too many resources to keep them loaded all the time. reply rkwz 3 hours agorootparentprevNot the parent, but I started using Llama 3.1 8b and it's very good. I'd say it's as good as or better than GPT 3.5 based on my usage. Some benchmarks: https://ai.meta.com/blog/meta-llama-3-1/ Looking forward to try other models like Qwen and Phi in near future. reply milleramp 1 hour agorootparentI found it to not be as good in my case for code generation and suggestions. I am using a quantized version maybe that's the difference. reply axpy906 4 hours agorootparentprevAgree. Please provide more details on this setup or a link. reply deegles 4 hours agorootparentJust try a few models on your machine? It takes seconds plus however long it takes to download the model. reply exe34 1 hour agorootparentI would prefer to have some personal recommendations - I've had some success with Llama3.1-8B/8bits and Llama3.1-70B/1bit, but this is a fast moving field, so I think it's worth the details. reply NortySpock 1 hour agorootparentNew LLM Prompt: Write a reddit post as though you were a human, extolling how fast and intelligent and useful $THIS_LLM_VERSION is... Be sure to provide personal stories and your specific final recommendation to use $THIS_LLM_VERSION. reply Anthony1321 26 minutes agoprevI found this article really eye-opening! It's fascinating to see how technology is evolving and impacting our lives in unexpected ways. If you're interested in more tech insights, check out https://trendytechinfo.com for the latest updates reply Archit3ch 34 minutes agoprevOne use case I've found very convenient: partial screenshot |> minicpm-v Covers 90% of OCR needs with 10% of the effort. No API keys, scripting, or network required. reply McBainiel 4 hours agoprev> Microsoft used LLMs to write millions of short stories and textbooks in which one thing builds on another. The result of training on this text, Bubeck says, is a model that fits on a mobile phone but has the power of the initial 2022 version of ChatGPT. I thought training LLMs on content created by LLMs was ill-advised but this would suggest otherwise reply andai 4 hours agoparentLook into Microsoft's Phi papers. The whole idea here is that if you train models on higher quality data (i.e. textbooks instead of blogspam) you get higher quality results. The exact training is proprietary but they seem to use a lot of GPT-4 generated training data. On that note... I've often wondered if broad memorization of trivia is really a sensible use of precious neurons. It seems like a system trained on a narrower range of high quality inputs would be much more useful (to me) than one that memorized billions of things I have no interest in. At least at the small model scale, the general knowledge aspect seems to be very unreliable anyways -- so why not throw it out entirely? reply throwthrowuknow 4 hours agorootparentThe trivia include information about many things: grammar, vocabulary, slang, entity relationships, metaphor, among others but chiefly they also constitute models of human thought and behaviour. If all you want is a fancy technical encyclopedia then by all means chop away at the training set but if you want something you can talk to then you’ll need to keep the diversity. reply visarga 3 hours agorootparent> you’ll need to keep the diversity. You can get diverse low quality data from the web, but for diverse high quality data the organic content is exhausted. The only way is to generate it, and you can maintain a good distribution by structured randomness. For example just sample 5 random words from the dictionary and ask the model to compose a piece of text from them. It will be more diverse than web text. reply deegles 4 hours agorootparentprevYou're not just memorizing text though. Each piece of trivia is something that represents coherent parts of reality. Think of it as being highly compressed. reply snovv_crash 3 hours agorootparentprevFrom what I've seen Phi does well in benchmarks but poorly in real world scenarios. They also made some odd decisions regarding the network structure which means that the memory requirements for larger context is really high. reply sandwichmonger 3 hours agoparentprevThat's the number one way of getting mad LLM disease. Feeding LLMs to LLMs. reply kkielhofner 4 hours agoparentprevSynthetic data (data from some kind of generative AI) has been used in some form or another for quite some time[0]. The license for LLaMA 3.1 has been updated to specifically allow its use for generation of synthetic training data. Famously, there is a ToS clause from OpenAI in terms of using them for data generation for other models but it's not enforced ATM. It's pretty common/typical to look through a model card, paper, etc and see the use of an LLM or other generative AI for some form of synthetic data generation in the development process - various stages of data prep, training, evaluation, etc. Phi is another really good example but that's already covered from the article. [0] - https://www.latent.space/i/146879553/synthetic-data-is-all-y... reply mrbungie 4 hours agoparentprevI would guess correctly aligned and/or finely filtered synthetic data coming from LLMs may be good. Mode colapse theories (and simplified models used as proof of existence of said problem) assume affected LLMs are going to be trained with poor quality LLM-generated batches of text from the internet (i.e. reddit or other social networks). reply gugagore 4 hours agoparentprevGenerally (not just for LLMs) this is called student-teacher training and/or knowledge distillation. reply calf 3 hours agorootparentIt reminds me of when I take notes from a textbook then intensively review my own notes reply solardev 3 hours agorootparentAnd then when it comes time for the test, I end up hallucinating answers too. reply staticman2 2 hours agoparentprevThere's been efforts to train small LLM's on bigger LLM's. Ever since Llama came out the community was creating custom fine tunes this way using ChatGPT. reply brap 4 hours agoparentprevI think it can be a tradeoff to get to smaller models. Use larger models trained on the whole internet to produce output that would train the smaller model. reply moffkalast 4 hours agoparentprevAs others point out, it's essentially distillation of a larger model to a smaller one. But you're right, it doesn't work very well. Phi's performance is high on benchmarks but not nearly as good in actual real world usage. It is extremely overfit on a narrow range of topics in a narrow format. reply iJohnDoe 4 hours agoparentprev> Microsoft used LLMs to write millions of short stories and textbooks Millions? Where are they? Where are they used? reply HPsquared 3 hours agorootparentModel developers don't usually release training data like that. reply shahzaibmushtaq 1 hour agoprevI need to have two things of my own that work offline for privacy concerns and cost savings: 1. Local LLM AI models with GUI and command line 2. > Local LLM-based coding tools do exist (such as Google DeepMind’s CodeGemma and one from California-based developers Continue) reply dockerd 3 hours agoprevWhat spec people recommend here to run small models like Llama3.1 or mistral-nemo etc. Also is it sensible to wait for newer mac, amd, nvidia hardware releasing soon? reply freeone3000 2 hours agoparentM4s are releasing in probably a month or two; if you’re going Apple, it might be worth waiting for either those or the price drop on the older models. reply noman-land 2 hours agoparentprevYou basically need as much RAM as the size of the model. reply zozbot234 2 hours agorootparentYou actually need a lot less than that if you use the mmap option, because then only activations need to be stored in RAM, the model itself can be read from disk. reply noman-land 2 hours agorootparentCan you say a bit more about this? Based on my non-scientific personal experience on an M1 with 64gb memory, that's approximately what it seems to be. If the model is 4gb in size, loading it up and doing inference takes about 4gb of memory. I've used LM Studio and llamafiles directly and both seem to exhibit this behavior. I believe llamafiles use mmap by default based on what I've seen jart talk about. LM Studio allows you to \"GPU offload\" the model by loading it partially or completely into GPU memory, so not sure what that means. reply trash_cat 26 minutes agoprevI think it REALLY depends on your use case. Do you want to brainstorm, clear out some thoughts, search or solve complex tasks? reply aledalgrande 1 hour agoprevDoes anyone know of a local \"Siri\" implementation? Whisper + Llama (or Phi or something else), that can run shortcuts, take notes, read web pages etc.? PS: for reading web pages I know there's voices integrated in the browser/OS but those are horrible reply gardnr 12 minutes agoparentEdit: I just found this. I'll give it a try today: https://github.com/0ssamaak0/SiriLLama --- Open WebUI has a voice chat but the voices are not great. I'm sure they'd love a PR that integrates StyleTTS2. You can give it a Serper API Key and it will search the web to use as context. It connects to ollama running on a linux box with a $300 RTX 3060 with 12GB of VRAM. The 4bit quant of Llama 3.1 8B takes up a bit more than 6GB of VRAM which means it can run embedding models and STT on the card at the same time. 12GB is the minimum I'd recommend for running quantized models. The RTX 4070 Ti Super is 3x the cost but 7 times \"faster\" on matmuls. The AMD cards do inference OK but they are a constant source of frustration when trying to do anything else. I bought one and tried for 3 months before selling it. It's not worth the effort. I don't have any interest in allowing it to run shortcuts. Open WebUI has pipelines for integrating function calling. HomeAssistant has some integrations if that's the kind of thing you are thinking about. reply xenospn 1 hour agoparentprevApple intelligence? reply aledalgrande 1 hour agorootparentIt isn't clear if you can know when the task gets handed off to their servers. But yeah that'd be the closest I know. I'm not sure it would build a local knowledge base though. reply pilooch 39 minutes agoprevI run a fineruned mulmodal LLM as a spam filter (reads emails as images). Game changer. Removes all the stuff I wouldn't read anyways, not only spam. reply vessenes 3 hours agoprevAll this will be an interesting side note in the history of language models in the next eight months when roughly 1.5 billion iPhone users will get a local language model tied seamlessly to a mid-tier cloud based language model native in their OS. What I think will be interesting is seeing which of the open models stick around and for how long when we have super easy ‘good enough’ models that provide quality integration. My bet is not many, sadly. I’m sure Llama will continue to be developed, and perhaps Mistral will get additional European government support, and we’ll have at least one offering from China like Qwen, and Bytedance and Tencent will continue to compete a-la Google and co. But, I don’t know if there’s a market for ten separately trained open foundation models long term. I’d like to make sure there’s some diversity in research and implementation of these in the open access space. It’s a critical tool for humans, and it seems possible to me that leaders will be able to keep extending the gap for a while; when you’re using that gap not just to build faster AI, but do other things, the future feels pretty high volatility right now. Which is interesting! But, I’d prefer we come out of it with people all over the world having access to these. reply jannyfer 3 hours agoparent> in the next eight months when roughly 1.5 billion iPhone users will get a local language model tied seamlessly to a mid-tier cloud based language model native in their OS. Only iPhone 15 Pro or later will get Apple Intelligence, so the number will be wayyy smaller. reply visarga 3 hours agorootparentNot in EU they won't. reply darby_nine 3 hours agoparentprevI expect people will just ship with their own model where the built-in one isn't sufficient. When people describe it as a \"critical tool\" i feel like I'm missing basic information about how people use computers and interact with the world. In what way is it critical for anything? It's still just a toy at this point. reply qingdao99 30 minutes agorootparentWhen it's expected to be handling reminders, calendar events, and other device functions for millions of users, it will be considered critical. reply andai 4 hours agoprevWhat local models is everyone using? The last one I used was Llama 3.1 8B which was pretty good (I have an old laptop). Has there been any major development since then? reply esoltys 4 hours agoparentI like [mistral-nemo](https://ollama.com/library/mistral-nemo) \"A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.\" reply demarq 4 hours agoparentprevNada to be honest. I keep trying every new model, and invariably go back to llama 8b. Llama8b is the new mistral. reply moffkalast 4 hours agoparentprevQwen 2.5 has just released, with a surprising amount of sizes. The 14B and 32B look pretty promising for their size class but it's hard to tell yet. reply HexDecOctBin 4 hours agoprevMay as well ask here: what is the best way to use something like an LLM as a personal knowledge base? I have a few thousand book, papers and articles collected over the last decade. And while I have meticulously categorised them for fast lookup, it's getting harder and harder to search for the desired info, especially in categories which I might not have explored recently. I do have a 4070 (12 GB VRAM), so I thought that LLMs might be a solutions. But trying to figure out the whats and hows hase proven to be extremely complicated, what with deluge of techniques (fine-tuning, RAG, quantisation) that might not might not be obsolete, too many grifters hawking their own startups with thin wrappers, and a general sense that the \"new shiny object\" is prioritised more than actual stable solutions to real problems. reply routerl 3 hours agoparentImho opinion, and I'm no expert, but this has been working well for me: Segment the texts into chunks that make sense (i.e. into the lengths of text you'll want to find, whether this means chapters, sub-chapters, paragraphs, etc), create embeddings of each chunk, and store the resultant vectors in a vector database. Your search workflow will then be to create an embedding of your query, and perform a distance comparison (e.g. cosine similarity) which returns ranked results. This way you can now semantically search your texts. Everything I've mentioned above is fairly easily doable with existing LLM libraries like langchain or llamaindex. For reference, this is an RAG workflow. reply meonkeys 30 minutes agoparentprevhttps://khoj.dev promises this. reply dchuk 3 hours agoparentprevLook into this: https://www.anthropic.com/news/contextual-retrieval And this: https://microsoft.github.io/graphrag/ reply brap 4 hours agoprevSome companies (OpenAI, Anthropic…) base their whole business on hosted closed source models. What’s going to happen when all of this inevitably gets commoditized? This is why I’m putting my money on Google in the long run. They have the reach to make it useful and the monetization behemoth to make it profitable. reply csmpltn 4 hours agoparentThere's plenty of competition in this space already, and it'll only get accelerated with time. There's not enough \"moat\" in building proprietary LLMs - you can tell by how the leading companies in this space are basically down to fighting over patents and regulatory capture (ie. mounting legal and technical barriers to scraping, procuring hardware, locking down datasets, releasing less information to the public about how the models actually work behind the scenes, lobbying for scary-yet-vague AI regulation, etc). It's fizzling out. The current incumbents are sitting on multi-billion dollar valuations and juicy funding rounds. This buys runtime for a good couple of years, but it won't last forever. There's a limit to what can be achieved with scraped datasets and deep Markov chains. Over time, it will become difficult to judge what makes one general-purpose LLM be any better than another general-purpose LLM. A new release isn't necessarily performing better or producing better quality results, and it may even regress for many use-cases (we're already seeing this with OpenAI's latest releases). Competitors will have caught up to eachother, and there shouldn't be any major differences between Claude, ChatGPT, Gemini, etc - after-all, they should all produce near-identical answers, given identical scenarios. Pace of innovation flattens out. Eventually, the technology will become wide-spread, cheap and ubiquitous. Building a (basic, but functional) LLM will be condensed down to a course you take at university (the same way people build basic operating systems and basic compilers in school). The search for AGI will continue, until the next big hype cycle comes up in 5-10 years, rinse and repeat. You'll have products geared at lawyers, office workers, creatives, virtual assistants, support departments, etc. We're already there, and it's working great for many use-cases - but it just becomes one more tool in the toolbox, the way Visual Studio, Blender and Photoshop are. The big money is in the datasets used to build, train and evaluate the LLMs. LLMs today are only as good as the data they were trained on. The competition on good, high-quality, up-to-date and clean data will accelerate. With time, it will become more difficult, expensive (and perhaps illegal) to obtain world-scale data, clean it up, and use it to train and evaluate new models. This is the real goldmine, and the only moat such companies can really have. reply sparky_ 3 hours agorootparentThis is the best take on the generative AI fad I've yet seen. I wish I could upvote this twice. reply 101008 3 hours agorootparentI had the same impression. I have been suffering a lot lately about the future for engineers (not having work, etc), even habing anxiety when I read news about AI, but these comments make me feel better and relaxed. I even considered blocking HN. reply meiraleal 3 hours agorootparentprevAnd then the successful chatgpt wrappers with traction will become valuable than the companies creating propietary LLMs. I bet openai will start buying many AI apps to find profitable niches. reply whimsicalism 3 hours agoparentprevTheir hope is to reach AGI and effective post-scarcity for most things that we currently view as scarce. I know it sounds crazy but that is what they actually believe and is a regular theme of conversations in SF. They also think it is a flywheel and whoever wins the race in the next few years will be so far ahead in terms of iteration capability/synthetic data that they will be the runaway winner. reply throwaway314155 3 hours agoparentprevI don't have a horse in the race but wouldn't Meta be more likely to commoditize things given that they sort of already are? reply zdragnar 3 hours agorootparentSearch Gmail Docs Android Chrome (browser and Chromebooks) I don't use any Meta properties at all, but at least a dozen alphabet ones. My wife uses Facebook, but that's about it. I can see it being handy for insta filters. YMMV of course, but I suspect alphabet has much deeper reach, even if the actual overall number of people is similar. reply throwaway314155 2 hours agorootparentI was referring to the many quality open models they've released to be clear. reply pimeys 2 hours agoprevHas anybody found a good way to utilize ollama with an editor such as zed to do things like \"generate rustdoc to this method\" etc. I use ollama daily for a ton of things, but for code generation, completion and documentation 4o is still much better than any of the local models... reply statenjason 1 hour agoparentI use gen.nvim[1] with for small tasks, like “write a type definition for this JSON” . Running locally avoids the concern of sending IP or PII to third parties. [1]: https://github.com/David-Kunz/gen.nvim reply navbaker 2 hours agoparentprevThe Continue extension for VSCode is pretty good and has native connectivity to a local install of Ollama reply pimeys 1 hour agorootparentZed has also support for ollama, but all the local models I tried do not really work so well to things like \"write docs for this method\"... Also local editor autocomplete in the style of github copilot would be great, without needing to use proprietary Microsoft tooling... reply vunderba 1 hour agorootparentThere's a lot of plugins/IDEs for assistant style LLMs, but the only TAB style autocompletion ones I know of are either proprietary (Github Copilot), or you need to get an API key (Codestral). If anyone knows of a local autocomplete model I'd love to hear about it. The Continue extension (Jetbrains, VSCodium) lets you set up assistant and autocompletion independently with different API keys. reply jsemrau 1 hour agoprevThe Mistral models are not half as bad for this. reply create-username 1 hour agoprevthere's no small AI that I know of and masters ancient Greek, Latin, English, German and French and that I can run on my 18 GB macbook pro. Please correct me if I'm wrong. It would make my life slightly more comfortable reply pella 3 hours agoprevLlama 3.1 405B \"2 MacBooks is all you need. Llama 3.1 405B running distributed across 2 MacBooks using @exolabs_ home AI cluster\" https://x.com/AIatMeta/status/1834633042339741961 reply IshKebab 3 hours agoparent\"All you need is £10k of Apple laptops...\" reply nurettin 2 hours agorootparentThat is... probable, if you bought a newish m2 to replace your 5-6 year old macbook pro which is now just lying around. Or maybe you and your spouse can share cpu hours. reply svnt 2 hours agorootparentNo, you need two of the newest M3 Macbook Pros with maxed RAM, which in practice some people might have, but it is not gettable by using old hardware. And not having tried it, I’m guessing it will probably run at 1-2 tokens per second or less since the 70b model on one of these runs at 3-4, and now we are distributing the process over the network, which is best case maybe 40-80Gb/s It is possible, and that’s about the most you can say about it. reply shrubble 2 hours agoprevThe newest laptops are supposed to have 40-50 TOPS performance with the new AI/NPU features. Wondering what that will mean in practice. reply api 2 hours agoprevI use ollama through a Mac app called BoltAI quite a bit. It’s like having a smart portable sci-fi “computer assistant” for research and it’s all local. It is about the only thing I can do on my M1 Pro to spin up the fans and make the bottom of the case hot. Llama3.1, Deepseek Coder v2, and some of the Mistral models are good. ChatGPT and Claude top tier models are still better for very hard stuff. reply HPsquared 6 hours agoprevPC: Personal Chatbot reply swah 4 hours agoprevI saw this demo a few months back - and lost it, of LLM autocompletion that was a few milliseconds - it opened a how new way on how to explore it... any ideas? reply JPLeRouzic 4 hours agoparenthttps://groq.com is very fast. (this is not the same as Grok) reply nipponese 4 hours agoprevAm I the only one seeing obvious ads in llama3 results? reply Sophira 2 hours agoparentI've not yet used any local AI, so I'm curious - what are you getting? Can you share examples? reply dunefox 4 hours agoparentprevYes. reply sandspar 45 minutes agoprevWhat advantages do local models have over exterior models? Why would I run one locally if ChatGPT works well? reply binary132 3 hours agoprevI really get the feeling with these models that what we need is a very memory-first hardware architecture that is not necessarily the fastest at crunching.... that seems like it shouldn’t necessarily be a terrifically expensive product reply miguelaeh 4 hours agoprevI am betting on local AI and building offload.fyi to make it easy to implement in any app reply wslh 4 hours agoprevWhat's the current cost of building a DIY bare-bones machine setup to run the top LLaMA 3.1 models? I understand that two nodes are typically required for this. Has anyone built something similar recently, and what hardware specs would you recommend for optimal performance? Also, do you suggest waiting for any upcoming hardware releases before making a purchase? reply atemerev 4 hours agoparent405B is beyond homelab-scale. I recently obtained a 4x4090 rig, and I am comfortable running 70B and occasionally 128B-class models. For 405B, you need 8xH100 or better. A single H100 costs around $40k. reply HPsquared 2 hours agorootparentHere is someone running 405b on 12x3090 (4.5bpw). Total cost around $10k. https://www.reddit.com/r/LocalLLaMA/comments/1ej9uzh/local_l... Admittedly it's slow (3.5 token/sec) reply wslh 1 hour agorootparentApproximately, how many tokens per second would the (edited) >~ $ 40k x 8 >=~ $320k version process? Would this result in a >~32x boost in performance compared to other setups? Thanks! reply jmount 3 hours agoprevI think this is a big deal. In my opinion, many money making stable AI services are going to be deliberately of limited ability on limited domains. One doesn't want one's site help bot answering political questions. So this could really pull much of the revenue away from AI/LLMs as service. reply simion314 4 hours agoprevOpenAI APIs for GPT and Dalle have issues like non determnism, and their special prompt injection where they add stuff or modify your prompt (with no option to turn that off. Makes it impossible to do research or to debug as a developer variations of things. reply throwaway314155 3 hours agoparentWhile that's true for their ChatGPT SaaS, the API they provide doesn't impose as many restrictions. reply mrfinn 4 hours agoprevIt's kinda funny how nowadays an AI with 8 billion parameters is something \"small\". Specially when just two years back entire racks were needed to run something giving way worst performance. reply atemerev 4 hours agoparentIDK, 8B-class quantized models run pretty fast on commodity laptops, with CPU-only inference. Thanks to the people who figured out quantization and reimplemented everything in C++, instead of academic-grade Python. reply stainablesteel 1 hour agoprevi think this is laughable, the only good 8B models are the llama ones, phi is terrible, even codestral can barely code and that's 22B iirc but truthfully the 8B just aren't that great yet, they can provide some decent info if you're just investigating things but a google search is still faster reply diggan 5 hours agoprevSummary: It's cheaper, safer for handling sensitive data, easier to reproduce results (only way to be 100% sure it's reproduce even, as \"external\" models can change anytime), higher degree of customization, no internet connectivity requirements, more efficient, more flexible. reply bionhoward 4 hours agoparentNo ridiculous prohibitions on training on logs… Man, imagine being OpenAI and flushing your brand down the toilet with an explicit customer noncompete rule which totally backfires and inspires 100x more competition than it prevents reply roywiggins 4 hours agorootparentLlama's license does forbid it: \"Llama 3.1 materials or outputs cannot be used to improve or train any other large language models outside of the Llama family.\" https://llamaimodel.com/commercial-use/ reply jclulow 3 hours agorootparentI'm not sure why anybody would respect that licence term, given the whole field rests on the rapacious misappropriation of other people's intellectual property. reply ronsor 3 hours agorootparentprevMeta dropped that term, actually, and that's an unofficial website. reply sigmoid10 3 hours agorootparent>If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama 3” at the beginning of any such AI model name. The official llama 3 repo still says this, which is a different phrasing but effectively equal in meaning to what the commenter above said. reply candiddevmike 3 hours agorootparentprevIt's still present in the llama license...? https://ai.meta.com/llama/license/ Section 1.b.iv reply jerbear4328 3 hours agorootparentLlama 3.1 isn't under that license, it's under the Llama 3.1 Community License Agreement: https://www.llama.com/llama3_1/license/ reply alexander2002 4 hours agoparentprevAn AI chip on laptop devices would be amazing! reply viraptor 4 hours agorootparentIt's pretty much happening already. Apple devices have MPS. Both new Intels and Snapdragon X have some form of NPU. reply moffkalast 4 hours agorootparentIt would be great if any NPU that currently exists was any good at LLM acceleration, but they all have really bad memory bottlenecks. reply ta988 4 hours agorootparentprevThey already exist. Nvidia GPUs on laptops, M series CPUs from Apple, NPUs... reply alexander2002 3 hours agorootparentoh damn guess i am so uninformed reply aurareturn 4 hours agorootparentprevFirst NPU arrived 7 years ago in an iPhone SoC. GPUs are also “AI” chips. Local LLM community has been using Apple Silicon Mac GPUs to do inference. I’m sure Apple Intelligence uses the NPU and maybe the GPU sometimes. reply theodorthe5 4 hours agoprev [–] Local LLMs are terrible compared to Claude/ChatGPT. They are useful to use as APIs for applications: much cheaper than paying for OpenAI services, and can be fine tuned to do many useful (and less useful, even illegal) things. But for the casual user, they suck compared to the very large LLMs OpenAI/Anthropic deliver. reply maxnevermind 22 minutes agoparentUnfortunately it seems to be the truth. Also models are getting bigger, so even if local basement rig is possible right now, that might not be so in the future. Also Zuck and others might stop releasing their weights for the next gen models, then what, just hope they plateau, what if they don't? reply 78m78k7i8k 4 hours agoparentprevI don't think local LLM's are being marketed \"for the casual user\", nor do I think the casual user will care at all about running LLM's locally so I am not sure why this comparison matters. reply 123yawaworht456 2 hours agoparentprev [–] they are the only thing you can use if you don't want to or aren't allowed to hand over your data to US corporations and intelligence agencies. every single query to ChatGPT/Claude/Gemini/etc will be used for any purpose, by any party, at any time. shamelessly so, because this is the new normal. Welcome to 2024. I own nothing, have no privacy, and life has never been better. >(and less useful, even illegal) things the same illegal things you can do with Notepad, or a pencil and a piece of paper. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers are increasingly running small AI models on their laptops, moving away from web-based tools like ChatGPT.",
      "This trend is fueled by the availability of open-weight models and scaled-down versions that can operate on consumer hardware, offering benefits like cost savings, privacy, and reproducibility.",
      "Major tech firms and research institutes, including Google DeepMind, Meta, and Microsoft, have released small models with billions of parameters, enabling diverse applications in fields such as bioinformatics and healthcare."
    ],
    "commentSummary": [
      "Researchers are increasingly running small AI models locally on their laptops, highlighting the ease and privacy benefits of local models like Llamafile and Whisper for tasks such as voice transcription and code autocomplete.",
      "Local models offer significant advantages, including privacy, offline functionality, and the ability to run on various hardware architectures, making them accessible to a broader audience.",
      "The discussion includes practical tips for setting up local AI environments, such as using open-source tools like VS Codium and bypassing restrictions to manually install extensions, emphasizing the community's collaborative efforts to optimize local AI usage."
    ],
    "points": 282,
    "commentCount": 190,
    "retryCount": 0,
    "time": 1726919535
  },
  {
    "id": 41605680,
    "title": "Critical Exploit in MediaTek Wi-Fi Chipsets: Zero-Click Vulnerability",
    "originLink": "https://blog.sonicwall.com/en-us/2024/09/critical-exploit-in-mediatek-wi-fi-chipsets-zero-click-vulnerability-cve-2024-20017-threatens-routers-and-smartphones/",
    "originBody": "Critical Exploit in MediaTek Wi-Fi Chipsets: Zero-Click Vulnerability (CVE-2024-20017) Threatens Routers and Smartphones By Security News September 19, 2024 Overview The SonicWall Capture Labs threat research team became aware of the threat CVE-2024-20017, assessed its impact and developed mitigation measures for the vulnerability. CVE-2024-20017 is a critical zero-click vulnerability with a CVSS 3.0 score of 9.8, impacting MediaTek Wi-Fi chipsets MT7622/MT7915 and RTxxxx SoftAP driver bundles used in products from various manufacturers, including Ubiquiti, Xiaomi and Netgear. The affected versions include MediaTek SDK versions 7.4.0.1 and earlier, as well as OpenWrt 19.07 and 21.02. This translates to a large variety of vulnerable devices, including routers and smartphones. The flaw allows remote code execution without user interaction due to an out-of-bounds write issue. MediaTek has released patches to mitigate the vulnerability and users should update their devices immediately. While this vulnerability was published and patched back in March, only recently did a public PoC become available making exploitation more likely. Technical Overview The vulnerability resides in wappd, a network daemon included in the MediaTek MT7622/MT7915 SDK and RTxxxx SoftAP driver bundle. This service is responsible for configuring and managing wireless interfaces and access points, particularly with Hotspot 2.0 technologies. The architecture of wappd is complex, comprising the network service itself, a set of local services that interact with the device’s wireless interfaces, and communication channels between components via Unix domain sockets. Ultimately, the vulnerability is a buffer overflow as a result of a length value taken directly from attacker-controlled packet data without bounds checking and placed into a memory copy. This buffer overflow creates an out-of-bounds write. Triggering the Vulnerability The vulnerability exists in the IAPP_RcvHandlerSSB function where an attacker controlled length value is passed to the IAPP_MEM_MOVE macro as described in hyprdude’s blog and seen in Figure 1. Figure 1: Vulnerable Code sourced from hyprdude Prior to the last line which calls IAPP_MEM_MOVE, the only bounds check done is to check that the provided length does not exceed the maximum packet length of 1600 bytes. As the size of the destination struct is only 167 bytes, this results in a stack buffer overflow of up to 1433 bytes. To trigger this vulnerability an attacker must send a packet with the expected structures prepending the attack payload. These structures are referred to as the RT_IAPP_HEADER and the RT_IAPP_SEND_SECURITY_BLOCK within the code. To bypass validation checks the length of the RT_IAPP_HEADER struct needs to be small and the RT_IAPP_HEADER.Command field must be to 50. Exploitation The publicly available exploit code achieves remote code execution by using a global address table overwrite technique via a return-oriented programming (ROP) chain. This method leverages the `system()` call to execute commands, such as sending a reverse shell back to the attacker. The reverse shell is established using Bash and the existing Netcat tool on the chipset. Figure 2 illustrates how the reverse shell command is crafted and embedded within the payload to enable this exploitation tactic. Figure 2: Reverse Shell Commands SonicWall Protections To ensure SonicWall customers are prepared for any exploitation that may occur due to this vulnerability, the following signatures have been released: IPS: 20322 MediaTek MT7915 wlan Service OOB Write 1 IPS: 20323 MediaTek MT7915 wlan Service OOB Write 2 Remediation Recommendations Due to the availability of the exploit code, it is highly recommended that users upgrade to the latest version of the firmware for their respective chipset. Relevant Links https://corp.mediatek.com/product-security-bulletin/March-2024 https://blog.coffinsec.com/0day/2024/08/30/exploiting-CVE-2024-20017-four-different-ways.html https://securityonline.info/cve-2024-20017-cvss-9-8-zero-click-exploit-discovered-in-popular-wi-fi-chipsets-poc-published/ https://github.com/mellow-hype/cve-2024-20017/tree/main SECURITY NEWS The SonicWall Capture Labs Threat Research Team gathers, analyzes and vets cross-vector threat information from the SonicWall Capture Threat network, consisting of global devices and resources, including more than 1 million security sensors in nearly 200 countries and territories. The research team identifies, analyzes, and mitigates critical vulnerabilities and malware daily through in-depth research, which drives protection for all SonicWall customers. In addition to safeguarding networks globally, the research team supports the larger threat intelligence community by releasing weekly deep technical analyses of the most critical threats to small businesses, providing critical knowledge that defenders need to protect their networks. Categories: Threat intelligence Tags: Security News https://d3ik27cqx8s5ub.cloudfront.net/blog/media/uploads/sec-news-header-3.png 500 1200 Security News https://blog.sonicwall.com/wp-content/uploads/images/logo/SonicWall_Registered-Small.png Security News2024-09-19 05:57:132024-09-19 05:57:13Critical Exploit in MediaTek Wi-Fi Chipsets: Zero-Click Vulnerability (CVE-2024-20017) Threatens Routers and Smartphones",
    "commentLink": "https://news.ycombinator.com/item?id=41605680",
    "commentBody": "Critical Exploit in MediaTek Wi-Fi Chipsets: Zero-Click Vulnerability (sonicwall.com)223 points by pjf 21 hours agohidepastfavorite80 comments mmsc 12 hours agoCan the OP's link be changed to the original source, not the advertisement it currently links to? The exploit is documented https://blog.coffinsec.com/0day/2024/08/30/exploiting-CVE-20... reply armada651 10 hours agoparentI don't think that link is necessarily better just because it's the original source. The linked article gives a concise overview, while the blog post spends the first paragraph talking about moving and starting a new job. reply mmsc 10 hours agorootparentIn general, I would wager that HN prefers intellectual curiosity over overviews. Submission guidelines infer that by stating \"Please submit the original source. If a post reports on something found on another site, submit the latter.\" reply codethief 8 hours agorootparentSure, though I'd argue in the case of vulnerabilities an overview is particularly valuable. Not everyone wants to dive into the details; in my case what I'm most interested in is whether I (or anyone else at my day job) might be affected. reply freedomben 6 hours agorootparentI would agree. I would also say that when the secondary article contains a lot of value added above, the original, such as is the case here, the secondary source is better because it is easy to follow its link to the original if that's what you'd like to see. I definitely agree with the guideline around favoring original sources, but this seems like a good time to deviate. reply Retr0id 5 hours agoparentprevTheir exploit development process is interesting, and I like to think I'd have done something similar (that is, compiling an easier-to-exploit version of the application and gradually working up to the real thing) reply Namidairo 16 hours agoprevNot too surprising given what I've seen of their vendor sdk driver source code, compared to mt76. (Messy would be kind assessment) Unfortunately, there are also some running aftermarket firmware builds with the vendor driver, due to it having an edge in throughput over mt76. Mediatek and their WiSoC division luckily have a few engineers that are enthusiastic about engaging with the FOSS community, while also maintaining their own little OpenWrt fork running mt76.[1] [1] https://git01.mediatek.com/plugins/gitiles/openwrt/feeds/mtk... reply dylan604 14 hours agoparentWhy is it so much of this hardware/firmware feels so much like deploying a PoC to production? Why can't they hire someone that actually knows what they are doing? reply jdietrich 10 hours agorootparentThe consumer space is brutally competitive - you're working on tight margins and designs become obsolete very quickly. MediaTek's business is built on selling chips with the latest features at the lowest possible price. Everything has to be done at a breakneck pace that is dictated by the silicon. You start writing firmware as soon as the hardware design is finalised; it needs to be ready as soon as the chips are ready to ship. These conditions are not at all suited to good software engineering. In an ideal world, consumers would be happy to pay a premium for a device that's a generation behind in terms of features but has really good firmware. In the real world, only Apple have the kind of brand and market power to even attempt that. reply JonChesterfield 8 hours agorootparentIntel networking used to have the expensive and works traits. Not confident their current products would be as good. reply phil21 5 hours agorootparentIndeed. A friend who is more plugged into such things me told me 4-5 years ago they laid off most of the senior Intel network driver team. Basically the only edge they had. I can’t imagine things are any better these days. Inertia is a hell of a thing, but you are starting to see the cracks form. I just don’t know if there is an alternative. reply sofixa 3 hours agorootparentprevWhen? The Intel X710 series of network cards was released in 2014, and it wasn't until ~2018 that it became actually usable (end of 2018? I don't recall really, but when I stumbled upon it it had already been a public problem for more than a year, and it took a few more months for patches to come). I'm talking things like full OS crashes while doing absolutely nothing, no traffic whatsoever or even better, silently starting to drop all network traffic (relatively silently, just an error message in the logs, but otherwise no indication, the interface still shows up as fine and up in the OS). It was all a driver issue (although both Intel drivers didn't work, so not only) that was later fixed. After that, it was rock solid. But the fact that there was a high class network card sold for lots of money, on hardware compatibility lists at various vendors, which didn't work at all for pretty much everyone for more than a few years is disgusting. reply tuetuopay 1 hour agorootparentyou'd be glad to hear the e810 series is not better in this regard. at least the out-of-tree driver somewhat works, and supports more than 1 queue. reply mschuster91 7 hours agorootparentprev> You start writing firmware as soon as the hardware design is finalised; it needs to be ready as soon as the chips are ready to ship. On top of that, there's bound to be errors in the hardware design, no modern technology even comes close to being formally proven correct, it's just too damn complex/large. Only after the first tapeout of an ASIC you can actually test it and determine what you need to correct and where to correct it (microcode, EC firmware, OS or application layer). reply Rinzler89 9 hours agorootparentprev>Why can't they hire someone that actually knows what they are doing? Because those employees cost a lot of money and these commodity widgets have razor thin margins that don't enable them to pay high salaries while also making enough profit to stay in business. You can pay more to hire better people and put the extra cost in the price of the product but then HP, Lenovo, Dell, et-al aren't gonna buy your product anymore, they're gonna buy instead from your competition who's maybe worse but provides lower prices which is what's most important for them because the average end user of the laptop won't notice the difference between network cards but they do check the sticker price of the machine on Amazon/Walmart and make the purchasing decision on that and stuff like the CPU and GPU not on the network card in the spec sheet. reply ta988 13 hours agorootparentprevBecause you have to over pay all those executives and shareholders. reply layer8 5 hours agorootparentprevHardware manufacturers see software as a cost center, it’s often made as cheaply as possible. And hardware engineers aren’t necessarily good software developers. It isn’t their main expertise. reply fragmede 11 hours agorootparentprevHardware companies are bad at making software, and the corollary, software companies are bad at making hardware. reply perching_aix 10 hours agorootparentI feel like there's an opportunity for a joke here somewhere along the lines of hardware companies being really terrible at writing software, while software companies being just a normal amount of terrible at writing software. reply a_dabbler 10 hours agorootparentA few attempts with chstgpt managed it: \"Hardware companies writing software is like watching a train wreck in slow motion. Software companies? They just crash at regular speed.\" reply dist-epoch 7 hours agorootparentprevWhich is why NVIDIA is king of the world, they are good at both hardware and software. reply therein 11 hours agorootparentprevIn the middle you have Apple that is getting better at making certain kinds of hardware, worse at some hardware and definitely worse in software. reply dboreham 13 hours agorootparentprevBecause money reply molticrystal 15 hours agoparentprevIs there any news releases or other information about that program, such as their goals, how much of the feed is merged upstream, etc? reply qhwudbebd 8 hours agoprevThe wording of the headline is a bit misleading here. I followed the link thinking it might be a firmware or silicon bug as I have a couple of routers at home with mt76 wifi, but was relieved to find it's just a bug in the vendor's 'sdk' shovelware. I'm baffled that anyone even thought about using that, given there's such good mt76 support from mainline kernels with hostapd. reply vesinisa 1 hour agoparent> I'm baffled that anyone even thought about using that, given there's such good mt76 support from mainline kernels with hostapd. Not sure if you noticed but the OpenWRT 21.02.x series (based on mainline kernel 5.4 series) is affected, and these guys generally know their game when it comes to wireless on Linux. So much so that I think the mainline kernel mt76 driver is actually maintained by an OpenWRT developer. reply mbilker 20 minutes agorootparentUpstream OpenWrt does not use `wappd` so it should not be affected. reply Terretta 4 hours agoparentprev> relieved to find it's just a bug in the vendor's 'sdk' shovelware Vendors plural to worry about: “…driver bundles used in products from various manufacturers, including [but not limited to] Ubiquiti, Xiaomi and Netgear.” That said, vendors (plural) say no products use this, e.g. Ubiquiti: https://community.ui.com/questions/CVE-2024-20017/b3f1a425-d... reply qhwudbebd 2 hours agorootparentSorry, yes, my use of 'vendor' here was ambiguous. I meant Mediatek, the chipset vendor. reply hunter-gatherer 16 hours agoprevOriginal blog: https://blog.coffinsec.com/0day/2024/08/30/exploiting-CVE-20... reply userbinator 15 hours agoparentThe wappd service is primarily used to configure and coordinate the operations of wireless interfaces and access points using Hotspot 2.0 and related technologies. The structure of the application is a bit complex but it’s essentially composed of this network service, a set of local services which interact with the wireless interfaces on the device, and communication channels between the various components, using Unix domain sockets. On the bright side, it doesn't sound like this is in baseband firmware but instead in a \"value add\" service that isn't 100% necessary to the functioning of the WNIC itself. This reminds me of how some devices come with driver packages that include not just the actual driver software that's usually tiny and unobtrusive, but several orders of magnitude larger bloatware for features that 99% of users don't need nor want. Printers and GPUs are particularly guilty of this. reply dvh 10 hours agorootparent> The structure of the application is a bit complex I've done some Android development so let me translate that for you: \"layers upon layers of dog shit APIs\" reply userbinator 8 hours agorootparentI've done some Android RE and agree with you. It's basically Enterprise Java culture. reply q3k 7 hours agoprev\"No way to prevent this\" say users of only language where this regularly happens reply asveikau 15 minutes agoparentMy anecdotal experience with mediatek wifi is it's a very flakey, low quality brand. That might be more of the reason. The firmware is probably unpolished, rushed, not maintained by competent people. reply happosai 7 hours agoparentprevWell it was solved decades ago in Java yet Java apps have proven no more secure in general. It is a broader ecosystem problem that there almost no incentive to write secure code. Security is an afterthought like documentation. reply stavros 7 hours agorootparent> Java apps have proven no more secure in general Really? I think an extraordinary claim like \"eliminating a whole class of problems makes applications no more secure in general\" should also come with extraordinary evidence. reply petee 6 hours agorootparentI think Java's CVE list should say enough. Point being humans can muck anything up, regardless of safeguards reply stavros 6 hours agorootparentA CVE list says nothing. I made my own language which has no CVEs, that obviously doesn't mean it's secure. The relevant metric is \"CVEs per unit of functionality\". reply sedatk 6 hours agorootparentAlso, popularity directly affects the number of CVEs. reply Clamchop 5 hours agorootparentprevThe point of the person you're replying to is that JVM software has far fewer vulnerabilities than it would have otherwise. The number of CVEs reveals that there is a lot of Java software and that there's a strong culture of importing dependencies. But we also care about the nature of them, the normalized relative frequency of very serious flaws like RCE exploits. reply bastawhiz 6 hours agorootparentprevThis is a nonsense statement unless you note the Java runtime. Java is a language. The runtime is the software that runs the Java code. There's more than one runtime. reply anthk 21 minutes agoparentprevNot an issue with ath9k. Guess why. Hint: not Rust related. reply eqvinox 3 hours agoparentprevC, C++ and assembly are 3 languages where this regularly happens. Can we stop with the snide comments now please? They're not helpful. reply cushychicken 2 hours agoparentprev“Oh! Wow! This comment makes me want to switch over to Rust!” - nobody reply Retr0id 5 hours agoprevIs there some logic to MediaTek's naming conventions, or all their devices just MTxxxx where x is some incremented/random number? I have a device with a mt6631 wifi chip and I'd assume it's unaffected just because it's not mentioned as affected anywhere, but it's hard to tell where it might fit into the lineup. reply kam 15 hours agoprevThey say that OpenWrt 19.07 and 21.02 are affected, but as far as I can tell, official builds of OpenWrt only use the mt76 driver and not the Mediatek SDK. reply hedora 14 hours agoparentIt’s similar for Ubiquti: https://community.ui.com/questions/CVE-2024-20017/b3f1a425-d... There are vulnerable drivers for some chipsets used by UBNT hardware, but they have zero products that use those drivers. reply anthk 19 minutes agoprevThat's why we need free firmware. I'm tired of Broadcom and Ralink. reply eqvinox 5 hours agoprev> The affected versions include MediaTek SDK versions 7.4.0.1 and earlier, as well as OpenWrt 19.07 and 21.02. > The vulnerability resides in wappd, a network daemon included in the MediaTek MT7622/MT7915 SDK and RTxxxx SoftAP driver bundle. OpenWRT doesn't seem to use wappd though? reply zekica 5 hours agoparentAs a contributor to OpenWrt it makes me wonder why don't people differentiate between OpenWrt and various proprietary vendor SDKs. No one would have referenced Fedora if there was a bug in Nobara. reply caconym_ 4 hours agoparentprevCame here wondering this, as I have several Netgear APs running OpenWRT on my home network. Sounds like I'm in the clear? reply eqvinox 3 hours agorootparentIf it's a clean upstream OpenWRT, yes. For vendored OpenWRT, all bets are off. reply RedShift1 12 hours agoprevI've been buying laptops with AMD CPU's but they always come with these trash MediaTek RZ616 Wi-Fi cards, why is that? I've been replacing them with Intel Wi-Fi cards, now I have a pile of RZ616 cards ready to become future microplastics :-( reply zekica 4 hours agoparentIntel sells two versions of their WiFi cards: ones ending in 1 use CNVI protocol and work only with Intel chips. These are sold really cheap to OEMs; ones ending in 0 use standard PCIe and are sold to OEMs for ~$10 more. AMD decided to brand Mediatek's MT7921 and MT7922 as RZ608 and RZ616 to have something to sell to OEMs at the same price point as Intel's xx1 chips. reply heffer 5 hours agoparentprevLenovo grew unhappy with MediaTek as well and started soldering down Qualcomm chips for WLAN on their AMD platforms only to be burned by buggy firmware/driver interactions on Linux (which they officially sell and support). And Qualcomm stretches themselves rather thin on the mainline kernel side once a chipset generation is no longer the latest. It takes a tremendous amount of vendor pressure to make Qualcomm do anything these days. reply tuetuopay 1 hour agoparentprevHave you tried them further than \"I don't trust MediaTek\"? I've had sequentially an Intel and an AMD ThinkPad for work (I killed the first one). Turns out, the wifi is much much better on the AMD one with the MediaTek chipset than on the Intel one with the Intel chipset. On the latter, I had very frequent disconnects from the network (severals per hour) along with atrocious latency even on 5GHz. And by atrocious latency, I mean atrocious as in \"it is more than noticeable when using ssh\". The current one has been rock solid for the past two or three years. So yeah, I guess it really depends. The specific chipset I have is a MT7921 and I'm running Linux, YMMV. And it also may depend on the laptop itself. reply zokier 9 hours agoparentpreviwlwifi has its own set of problems, biggest being no AP mode (on 5 Ghz). Also intels firmware license is more restrictive than mediateks, and being fullmac the firmware does lot more of the heavy lifting; I personally prefer softmac more. There simply aren't that many great options out there, gone are the golden days of ath9k. reply 0points 6 hours agoparentprevYou get what you pay for. reply smilespray 11 hours agoparentprevYou know why. Price. reply 1oooqooq 16 hours agoprevi still cannot fathom why in this day and age where people buy any silicon that's available, these C tier vendors don't adopt the PC strategy and completely open their firmwares for open source community. reply userbinator 16 hours agoparentFCC regulations around not making it easy to transmit outside of the licensed band tend to cause this. reply 1oooqooq 3 hours agorootparentthat point is completely bogus since hardware oscillators limit the range. and even multi range devices let the driver decide the region, so even with closed source you can already offend fcc regulations (pro tip set your wifi region to cuba for extra channels) reply eqvinox 3 hours agorootparentHardware oscillators don't limit anything, PLL ranges and amplifier band characteristics do, and they have soft falloffs. Please stop making claims you have little knowledge around. (no, a PLL is not considered a \"hardware oscillator\" in any practical sense by anyone working in this area, that's \"tomatoes in a fruit salad\" category) reply vlovich123 15 hours agorootparentprevMaking the code available doesn’t necessarily mean that you can actually flash the image since it can be cryptographically locked down. Or even you support flashing but only let you do certain trusted operations from a signed image. reply fn-mote 15 hours agorootparentI feel like I'm missing something here. Honestly, if you can't update the firmware you're in the same situation... knowing that you have a critical vulnerability and unable to fix it. Enforcing trusted operations is definitely more work than they are going to do (if it's even possible to \"do this right\"). In a semi-ideal world, I would look for a vendor that permits only certain ops from a flashed image and hope that their crappy \"restriction enforcing\" code is also riddled with vulnerabilites so it's really just \"follow the rules please\". reply 1oooqooq 3 hours agorootparentprevyou managed to completely miss the point. going the pc route is fully embracing your hardware accept whatever software the user wants. not throw unbuildable source somewhere and make it impossible to use. that's the faux open source we have today when someone must comply with the gpl or something reply vlovich123 1 hour agorootparentI think you happened to miss the point about regulatory requirements that make this difficult/impossible to accomplish for the radio vendor. I think the proliferation of SDR is the only hope to change the broader regulatory culture but until that happens you're not going to see a shift. I think it's also rich calling GPL compliance faux open source. There really is no true Scotsman. reply anthk 21 minutes agorootparentprevath9k. reply usr1106 13 hours agoprevIIRC my phone uses a MediaTek chipset. And I vaguely remember the vendor has moved away from MediaTek since because of the ahem quality of those products... No idea how WiFi is done on a phone though. Is there a way to find out whether the phone is affected? I hardly ever use WiFi because I have unlimited cellular data and good coverage, but would still be good to know. reply tetris11 8 hours agoparenttermux -> \"sudo su\" and then ls /sys/module (it gives an output similar to lsmod) reply AStonesThrow 1 hour agorootparentBack in the day, shell coders would receive the \"Useless Use Of Cat\" award. https://news.ycombinator.com/item?id=23341711 Today it's giving way to \"useless use of su\" where admins aren't aware of sudo(8) options like \"-s\" or \"-i\" reply tetris11 37 minutes agorootparentSo with termux there is an actual root password set, but it differs from the phone password so it's often forgotten. The termux developers, knowing this, set it such that the default termix user can invoke sudo without a password. It might seem lazy, but its very useful reply shadowpho 17 hours agoprevExploit is hard to distinguish between a back door here. reply saagarjha 16 hours agoparentPosting claims of it being such is pretty easy, though. reply pixl97 15 hours agorootparentThere is a better middle ground here by saying the company that made it may not have known, but nation state threat actors most likely do. When you see actors at this level set up manufacturing thousands of explosive filled devices at very high production quality, inserting some compromised things like printers or routers in a company network wouldn't be and shouldn't be a surprise. reply hedora 14 hours agorootparentIf the nation state actors did intentionally backdoor it, then they would have wanted to make it look like incompetence. Here’s a link to the Simple Sabotage Field Manual from the US. It worked well in occupied Europe during WWII: https://archive.org/details/SimpleSabotageFieldManual reply justmarc 7 hours agoprevWelcome back to the 90s. reply xtanx 11 hours agoprev [–] I would like to remind people of the 2016 Adups backdoor: > According to Kryptowire, Adups engineers would have been able to collect data such as SMS messages, call logs, contact lists, geo-location data, IMSI and IMEI identifiers, and would have been able to forcibly install other apps or execute root commands on all devices. https://www.bleepingcomputer.com/news/security/android-adups... reply phh 10 hours agoparent [–] How is this relevant? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A critical zero-click vulnerability (CVE-2024-20017) with a CVSS score of 9.8 has been identified in MediaTek Wi-Fi chipsets, affecting products from Ubiquiti, Xiaomi, and Netgear.",
      "The flaw allows remote code execution without user interaction due to an out-of-bounds write issue, and MediaTek has released patches; users should update their devices immediately.",
      "SonicWall has released specific intrusion prevention system (IPS) signatures to protect against this vulnerability, and a public proof of concept (PoC) has increased the likelihood of exploitation."
    ],
    "commentSummary": [
      "A critical zero-click vulnerability has been found in MediaTek Wi-Fi chipsets, specifically in the 'wappd' service of MediaTek's SDK, affecting OpenWrt 19.07 and 21.02.",
      "Official OpenWrt builds are not impacted as they use the mt76 driver instead of the vulnerable SDK.",
      "The discovery highlights the competitive consumer electronics market's tendency to release rushed firmware, raising concerns about security."
    ],
    "points": 223,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1726867729
  },
  {
    "id": 41606530,
    "title": "Apple Shares Full iPhone 16 and iPhone 16 Pro Repair Manuals",
    "originLink": "https://www.macrumors.com/2024/09/20/iphone-16-repair-manual/",
    "originBody": "Apple Shares Full iPhone 16 and iPhone 16 Pro Repair Manuals Friday September 20, 2024 1:04 pm PDT by Juli Clover Following today's launch of the new iPhone 16 models, Apple has shared repair manuals for the iPhone 16, the iPhone 16 Plus, the iPhone 16 Pro, and the iPhone 16 Pro Max. The repair manuals provide technical instructions on replacing genuine Apple parts in the iPhone 16 models, and Apple says the information is intended for \"individual technicians\" that have the \"knowledge, experience, and tools\" that are necessary to repair electronic devices. Apple has support documents on the tools that are required for various repairs, and while the iPhone 16 tools aren't yet available on Apple's Self Service Repair website, they'll likely be added soon. Compared to prior iPhone models, the iPhone 16 and iPhone 16 Plus are easier to repair. Apple is using an electric battery removal process, and the steps for accessing a battery to replace it are outlined in a separate support document. Per Apple's instructions, a 9-volt battery and 9-volt battery clips can be applied to the iPhone 16 battery to remove the adhesive that holds it in place. Note that the simpler electricity-based battery removal process is limited to the iPhone 16 and iPhone 16 Plus. For the iPhone 16 Pro and Pro Max, Apple is using standard adhesive tabs that need to be carefully pulled to release the battery. Apple's instructions for all of the battery repairs include expensive equipment like an iPhone battery press to put a replacement battery back in place. The cost of the tools required for device repair and the cost of genuine components make self repair almost as expensive as getting a repair from an Apple retail location or an Apple Authorized Service Provider, so these manuals are really aimed at independent repair shops rather than individual consumers. Apple made other changes to the iPhone 16 models this year to improve repairability, enabling on-device configuration for the Face ID camera, allowing LiDAR Scanner repair, and offering support for swapping the TrueDepth camera modules between iPhone 16 and iPhone 16 Pro models. In addition to offering repair instructions, Apple's manuals provide some insight into the internal structure of the new iPhones that we often don't see until there are device teardowns. The iPhone 16 Pro, for example, has a metal casing for the battery, a change made for thermal reasons, and both Pro models have new casing structure that improves heat dissipation. Related Roundups: iPhone 16, iPhone 16 Pro Buyer's Guide: iPhone 16 (Buy Now), iPhone 16 Pro (Buy Now) [ 36 comments ]",
    "commentLink": "https://news.ycombinator.com/item?id=41606530",
    "commentBody": "Apple Shares Full iPhone 16 and iPhone 16 Pro Repair Manuals (macrumors.com)197 points by stalfosknight 18 hours agohidepastfavorite128 comments diggan 5 hours agoDoes the iPhone still display warnings about \"genuineness\" of replacement parts, even if they're the original/official Apple parts? That kind of ruins the whole idea of \"officially letting people repair their stuff\" as if I replace my own battery, I can no longer trust other parts won't be switched by someone else at a later point, as I'll see the same warning regardless... reply miles 18 hours agoprev> Compared to prior iPhone models, the iPhone 16 and iPhone 16 Plus are easier to repair. Apple is using an electric battery removal process, and the steps for accessing a battery to replace it are outlined in a separate support document . Per Apple's instructions, a 9-volt battery and 9-volt battery clips can be applied to the iPhone 16 battery to remove the adhesive that holds it in place. \"Easier\" is relative I guess: Here’s every tool you’ll need to replace the iPhone 16’s battery https://9to5mac.com/2024/09/20/heres-every-tool-youll-need-t... * 9-volt battery * 9-volt battery clips (923-10726) * Battery press (923-02657) * Ethanol wipes or isopropyl alcohol (IPA) wipes * Nylon probe (black stick) (922-5065) or suction cup * Safety glasses with side shields * Sand * Sand container By contrast, the Treo 650 battery replacement took a few seconds and zero tools. reply yalok 15 hours agoparentRemoving a battery attached with previous type of adhesive is torture - the elastic tab frequently tears off, and I ended up a few times having to bend the old battery a lot, to get it out (very unsafe, it starts heating). So, to me, this is a huge progress. Plus, don’t you normally have 9v battery and some connectors for it already? reply petre 10 hours agorootparentThe upcoming EU regulation regarding removable batteries is hopefully going to fix this. reply madeofpalk 9 hours agorootparentNote that the EU regulation isn't asking for 2000s Nokia style removable batteries. It's just about making sure batteries are replacable by users. These new iPhone batteries seem to be not far off being compliant, if they're not already: > A portable battery should be considered to be removable by the end-user when it can be removed with the use of commercially available tools and without requiring the use of specialised tools, unless they are provided free of charge, or proprietary tools, thermal energy or solvents to disassemble it. > Commercially available tools are considered to be tools available on the market to all end-users without the need for them to provide evidence of any proprietary rights and that can be used with no restriction, except health and safety-related restrictions. https://www.europarl.europa.eu/doceo/document/TC1-COD-2020-0... https://www.europarl.europa.eu/doceo/document/TA-9-2023-0237... reply userbinator 15 hours agorootparentprevThis is not progress, this is overengineering pretending to be progress. Batteries don't need to be glued on in the first place. reply hbbio 15 hours agorootparentNo, it helps on a lot of issues. Starting with safety (it reduces damage on drops). reply bluescrn 11 hours agorootparentA small amount of adhesive might be justifiable, but the amount used is excessive and seems there primarily to increase ‘repair friction’, in a rather dangerous way - actively increasing the chance of a battery fire when replacement attempts are made. reply madeofpalk 9 hours agorootparentApple has to do a lot of these repairs themselves in their stores by retail workers, often under warranty, so they are incentivised to make the repairs easier and cheaper. The more risk of a retail worker screwing up a repair means higher cost of them fixing it by replacing the whole phone. They're also incentivised to make devices smaller, more waterproof, and increase battery capacity to size ratio. These all push for 'just glue it together'. reply stavros 7 hours agorootparentDo they actually? I haven't had to have Apple products repaired, but whenever I hear someone giving something in, they get a new or refurbished device instead of their own. I think Apple is incentivized to trash broken devices, rather than attempt to repair them even themselves, because of the massive margin, and thus low marginal cost. It's more lucrative for them to make the devices unrepairable and have you buy a new one (or give you a new one, if it's under warranty), than make them repairable for everyone. The warranty period just incentivizes them to make the device better quality so they don't break due to manufacturing defects. reply atonse 6 hours agorootparentI’ve had my battery replaced and sent family members to have theirs replaced, all at the Apple Store. All have had smooth experiences. Without AppleCare it’s something like $59 or thereabouts. Apple’s made it very easy to go to an Apple Store to have them replace batteries. Of course, that doesn’t solve the problem for people without an Apple Store nearby, or that need other repairs. But at least where I live, I haven’t needed to replace an iPhone battery myself in at least 5-7 years or longer. reply madeofpalk 6 hours agorootparentprev> or give you a new one, if it's under warranty But it's even cheaper to give you a tenth of an iPhone if possible. It's why they have these repair guides - they're derivatives of what they use themselves in their own stores. reply Arnt 13 hours agorootparentprevThat's a really good reason, but could you elaborate on the other issues? Just curious. reply sholladay 1 hour agoparentprevThe last few are just for your safety and can obviously be skipped if you feel like being a cowboy. And to me, “zero tools” makes it sound like that’s your preferred approach. So why count them against one phone but not the other? reply cbsks 16 hours agoparentprevDon’t forget that first you need to remove the back glass, which requires: Torque driver (blue, 0.65 kgf cm) (923-0448) Torque driver (green, 0.45 kgf cm) (923-00105) Security bit (923-0247) Micro stix bit (923-01290) Nylon probe (black stick) (922-5065) ESD-safe tweezers Adhesive removal tool (923-09176) Adhesive cutter (923-01092) Ethanol wipes or isopropyl alcohol (IPA) wipes 6.1-inch repair tray (923-10712) Camera cap (923-10716) Display press (661-08916) Cut-resistant gloves. Gloves may vary by region. Heat-resistant gloves. Gloves may vary by region. Safety glasses with side shields https://support.apple.com/en-us/120638 reply threeseed 16 hours agorootparentMost of this is available in any electronics screwdriver kit. And the rest is just for safety. reply raverbashing 12 hours agorootparentprevIt's amazing how people will think this is anything out of the ordinary for a repair shop But I guess Apple caters to the people who think getting grease in their hands is beyond them. reply akerr 16 hours agoparentprevNo one is stopping you from using a Treo 650. reply m463 16 hours agorootparentI remember dropping my treo 650 while hiking. The back cover came off, the battery went flying and worst of all - my memory card was dislodged and disappeared in the woods. reply bluescrn 11 hours agorootparentBetter than dropping an iPhone, breaking the glass screen/back, then getting angry about the limited repair options… reply m463 1 hour agorootparentIf I had had an iphone, I would have just picked it back up. reply strunz 5 hours agorootparentprevHow is that better? At least the phone is still usable and no data was lost. reply Brian_K_White 15 hours agorootparentprevWas there a point to this story? I have a pixel 5a with a dead screen that runs but can't be used because the dead part is part of the motherboard not the screen. It's a known problem with this model. So it still runs, but I can't recover any pics or texts from dead people from it because I can't respond to the screen prompts to allow the USB connection. I don't see how I'm any better off. reply simonh 13 hours agorootparentCommenter was pointing out a design problem. Your phone has a different design problem. Are you arguing that a device being able to fail in one way makes it ok for it to also be able to fail in other ways? reply throwaway48540 11 hours agorootparentprevUse USB-C to connect a hub with display and mouse, copy data over internet. reply m463 1 hour agorootparentCould usb-c also connect an ethernet port and/or a flash drive? reply 2muchcoffeeman 13 hours agorootparentprevNothing solves for data loss except a half decent backup strategy. reply miles 16 hours agorootparentprevSadly, Verizon is: CDMA Network Update https://www.verizon.com/prepaid/cdma-network-update/ > Starting Dec 31,2022 we no longer support 3G/4G Non-VoLTE. To keep your service active, upgrade your phone. reply renewiltord 12 hours agorootparentYou can get a combo 5G hotspot plus power bank and tape it to the back of your device. Then you can keep using your device on WiFi alone. reply labcomputer 5 hours agorootparentBetter to use rubber bands. You don’t want to have to remove that Apple-like adhesive tape when hot-swapping batteries like a quick-draw gun slinger. reply 015a 15 hours agorootparentprevNot just Verizon; the literal FCC, who licenses and restricts what bandwidth can be used for what purpose. reply miles 14 hours agorootparentThe FCC did not mandate the transition: Why are 3G networks being phased out? https://www.fcc.gov/consumers/guides/plan-ahead-phase-out-3g... > As mobile carriers seek to upgrade their networks to use the latest technologies, they periodically shut down older services, such as 3G, to free up spectrum and infrastructure to support new services, such as 5G. Similar transitions have happened before. For example, some mobile carriers shut down their 2G networks when they upgraded their networks to support 4G services. Mobile carriers have the flexibility to choose the types of technologies and services they deploy, including when they decommission older services in favor of newer services to meet consumer demands. reply tshaddox 16 hours agorootparentprevThe same way that McDonald’s or H&R Block are preventing you from using a Treo 650. Those companies also do not provide cellular service compatible with that phone. reply miles 15 hours agorootparentThe main difference being that Verizon sold me the phone and supported it on their network until they didn't. So yes, the Treo 650 still turns on and can be used without voice or data service, but claiming that Verizon is no more preventing me from using it than McDonald's or H&R Block in this case seems disingenuous. reply sbuk 8 hours agoparentprevIn contrast, the Treo 650 battery was considerably smaller in terms of capacity and significantly bigger in terms of physical size, leading to a bulkier device that had a significantly lower resolution, colour and physically sized screen. Lets not pretend the state of the art has not moved on. reply userbinator 16 hours agoparentprevIt's almost like Apple is maliciously complying by overcomplicating the procedure, which is not surprising. reply JumpCrisscross 15 hours agorootparent> almost like Apple is maliciously complying by overcomplicating the procedure Sorry, which of a 9-volt battery, alcohol wipes, safety glasses or sand (and a container for it) screams inaccessible? (And everything there is technically optional. I doubt most Treo 650 users drained the battery before touching it, or bothered with a suction cup.) Removing the back glass takes special tools, but I'll take that over having to replace my phone every time it gets wet. reply userbinator 15 hours agorootparentNot \"inaccessible\" but totally unnecessary. Why the bloody hell do you need a 9-volt battery to replace the battery!?!? Removing the back glass takes special tools, but I'll take that over having to replace my phone every time it gets wet. Gaskets have been around for over a century. reply Kirby64 15 hours agorootparentTheir own repair guide states you can use literally any DC power supply, up to 30V. No need to waste 9Vs when you can use an off the shelf DC supply. reply sgerenser 9 hours agorootparentprevApparently the 9v battery runs enough current through the metallic case of the battery to melt and release the adhesive so you can pull the old battery out. Seems like a pretty clever solution, if not a bit over-engineered: https://www.macrumors.com/2024/06/28/new-battery-replacement... reply dumbo-octopus 12 hours agorootparentprevGaskets require pressure. Adhesive doesn't. Different solutions for different problems. reply fsflover 5 hours agorootparentprev> Sorry, which of a 9-volt battery, alcohol wipes, safety glasses or sand (and a container for it) screams inaccessible? On my Librem 5, I can replace the battery without any tools. reply karlshea 4 hours agorootparentWhat you can’t do with your Librem 5 is submerge it in water, so you keep using your phone and I’ll take the option with glue. reply lamontcg 3 hours agorootparentI've got some flashlights that can go to 100m+ in water without any glue, and they run on removable C batteries. reply diggernet 2 hours agorootparentAnd let's not forget the GoPro. reply EthicalSimilar 16 hours agoparentprevWas the Treo 650 waterproof? :) reply danieldk 10 hours agorootparentNot this again :). Just taking a random example: the Samsung S5 was IPX67 rated (up to 1m for 30min), was thinner than an iPhone 16, and had a replaceable battery. Admittedly, it has fewer mAh, but it's also older battery technology and the volume of the phone case is slightly smaller (and probably has bigger electronics). Is should be totally possible to make a good 2024 flagship with replaceable batteries, but we'd have to forgo the fancy glass back panels. reply jmull 5 hours agorootparent> Not this again :) The problem with this logic is that phones are complicated and have a lot of constraints. Phone design inherently involves numerous tradeoffs. So... of course it's possible for manufacturers like Apple and Samsung to create a thin, waterproof phone with no-tool battery replacement. But at a cost to other features. The market has shown repeatedly that few consumers value no-tool battery replacements, relative to various other features they'd have to give up to get it. People are voting with their wallets and it doesn't make sense for Apple, Samsung, etc., to build phones people don't really want. reply Wytwwww 6 hours agorootparentprev> forgo the fancy glass back panels. Plastic is just meh.. I'd rather have an unrepairable device than one made of plastic. (We can't have metal because of the wireless charging) reply ratiolat 14 hours agorootparentprevOr perhaps get Samsung Xcover Pro - removable battery and IP68 rating (and audio jack!) https://m.gsmarena.com/samsung_galaxy_xcover6_pro-11600.php reply AshamedCaptain 10 hours agorootparentprevIs gluing the battery inside the case really a requirement for waterproofing? When they remove the battery cover -- \"oh, waterproofing\" When they glue the battery amd remove all screws -- \"oh, waterproofing\" When they eventually require an approved persons blood sample to perform repair, will I also hear the \"oh, waterproofing\" thing? reply bluescrn 11 hours agorootparentprevDo people go swimming with their phones? Older devices could generally handle splashes, e.g being used in light rain. Water damage seemed far less likely than drop damage. reply throwaway48540 11 hours agorootparentYes, I do. reply danieldk 10 hours agorootparentBut that's most likely 1% of the market. They can have their own phones. The rest of us just want water resistance for accidental contact with water and easily replaceable batteries. reply davidcbc 5 hours agorootparentI remember when the iPhone first came out non-tech friends had recurring nightmares about forgetting to take their phone out of their pocket before swimming. Phones being waterproof is a huge QOL improvement reply umanwizard 9 hours agorootparentprevI think you’re in a techie bubble. I would wager more people care about better water resistance (for example, because they want to use their phone in the shower) than about easily replaceable batteries. The overwhelming majority of people will never even contemplate trying to replace their own battery no matter how easy it is (unless it’s literally 90s/2000s-style snap on). reply throwaway48540 9 hours agorootparentprevWe have our own phone, it's the iPhone. I paid the money for it because I wanted the full package. You can buy your own kind of phone that's not the full package. Many different vendors are making that. reply trompetenaccoun 4 hours agorootparentThe iPhone is not waterproof, it's IP68 rated meaning it's water resistant. Swimming with your phone is absolutely not recommended and I don't know a single person that does this (unless we count people using special waterproof cases for filming). So no, the average smartphone buyer does not swim with their phone. Manufacturers had other incentives to make changing the batteries harder and there was no pressure from customers to increase the IP rating. In fact all you hear is people ranting how much it sucks that batteries are so hard to change these days. reply throwaway48540 4 hours agorootparentIs that all you hear? I don't hear that at all. People around me didn't change the battery when it was easy either. I was always like an alien when I suggested it. reply sbuk 8 hours agorootparentprevWho is us? Likely a similar sized segment that wants to tinker and are Louis Rossman fans. reply Brian_K_White 15 hours agorootparentprevIt was water-indifferent, like a Jeep. (Kidding. I did love mine and I did not protect it, and I'm sure it got rained on many times, but I don't know if I ever literally hosed water through it. :) reply threeseed 16 hours agoparentprevThat is significantly easier than trying to remove a glued-on battery. And removable batteries require far more internal space which is why they fell out of favour. reply userbinator 15 hours agorootparentAnd removable batteries require far more internal space No they don't. Less than 1% extra volume. reply alooPotato 15 hours agorootparentthat seems not possible but i'm just guessing. where are you getting the 1% from? reply threeseed 14 hours agorootparentprevPlease provide source. Especially given that you would want to preserve some form of water resistance. Meaning you either (a) have the entire back be removable or (b) a battery injection mechanism similar to a Leica SL3. Both of which would seem to need far more than 1% extra volume. reply sbuk 8 hours agorootparentprevComparable battery capacity, materials and technology? No. reply seventytwo 15 hours agoparentprevOh, give me a break. You can still do all the same shit with iFixit tools. These are just the genuine tools aimed at repair shops. reply raverbashing 13 hours agoparentprev\"every tool you need\" sounds like basic stuff for a repair shop Nobody is going and buying the Apple 9v battery or \"Apple sand\" reply tanduv 12 hours agorootparentah yes the readily available custom \"Battery press (923-02657)\" https://cdsassets.apple.com/live/SZLF0YNV/images/tp/bucket_3... reply gruturo 10 hours agorootparentI think we just found a use for all those Juicero's destined for the landfill. reply raverbashing 12 hours agorootparentprevThis seems to be a new device, should be a matter of weeks to have a similar tool show up in Amazon, etc There are alternative devices one could use, or you know, just use the tools you have, as long as you keep the pressure smooth. Possibly a Juicero kind of device reply sgerenser 11 hours agorootparentWith a steady hand something like this would work fine: https://www.amazon.com/dp/B00409KRB4 reply fecal_henge 3 hours agorootparentHardly, needs something more like this. https://www.gasparini.com/en/the-worlds-largest-hydraulic-pr... reply vindex10 10 hours agoprevOut of curiosity, I checked whether Google Pixel has something similar, and found the list: https://xdaforums.com/t/official-google-repair-guides-for-va... reply anArbitraryOne 15 hours agoprevGood for apple. I hope consumers pressure them to be open about more things reply yieldcrv 15 hours agoparentconsumers, and the EU reply screech 5 hours agorootparentThey only do it because they have to. Not because they are that consumer friendly. And now? Another arse move, were you have to pay for not that cheap special apple-tools.consumer milking at it's best. reply CodinM 8 hours agoprevThis is great, and most of the comments here seem to either miss the days of the StarTAC _or_ would gladly enjoy a physically bigger device. So, I can now change the battery on my iPhone 12 Pro _and_ reset its status in the Settings->Battery field, which is great. I already have all of the tools apart from the Apple specific ones, which to be fair are very useful for someone that does this everyday, I don't - so I can replace them with manual alternatives. Apple did the good thing and y'all still act like children. reply labcomputer 5 hours agoparent> This is great, and most of the comments here seem to either miss the days of the StarTAC _or_ would gladly enjoy a physically bigger device. If HN had its way, iPhones would be made from through-hole components and 7400 logic. The battery would last all of approximately 13 minutes… so it would be not just possible, but actually necessary to swap the battery in seconds. Ideally, the phone would be slightly unreliable (MTBF of no more than 200 hours) so that every user could experience the joy of troubleshooting the bad component and soldering in a replacement themselves. Maybe Apple could even include a sacrificial capacitor in the power supply that is just slightly under-spec’d. That would also give users the opportunity to “soup up” their iPhones by installing a better capacitor in that one location. It would be illegal to sell them without including (in the box with the phone) a 3000 page printed service manual containing not just schematics, but a detailed theory of operations and full source listing. Nobody would actually look that these, of course, but it’s the principle of costing the manufacturer money that matters. Such a device would be not small. After all, you need to have room to work inside the engine bay^H^H iPhone and you don’t want the components packed in too tight to support easy maintenance. That space would also promote airflow from the three fans necessary to cool all those 7400 ICs. reply electriclove 5 hours agoparentprevNo good deed goes unpunished reply sandwichmonger 13 hours agoprevAll it took to easily replace the battery on my IPAQ PocketPC was another battery. reply renewiltord 12 hours agoparentSomething that every PocketPC user rapidly learned to be adept at because without the boosted Chinese batteries the thing didn't last that long. Amazing for the time, but looking back, what a primitive device. And you had to pay for OS upgrades! reply thaumasiotes 9 hours agoparentprevThis is also true of early smartphones. They were made to have easily replaceable batteries, and I assume you could buy those batteries too. But that turned out to be irrelevant because of the replacement schedule. It seems clear that the frequency with which people replace their phones is what drove the decisions to make maintaining them difficult. If nobody ever needs to maintain the phone, why would you put any effort into helping them hypothetically do so? The analog of Moore's Law for smartphones is already dying and there was a lot of news coverage a while ago of how people seem to be keeping their phones. That may drive the development of phones that can last longer than two years. reply shreddit 6 hours agorootparentMarvelous times. The battery of your galaxy s3 died? Pop that back off and just put a new one inside. reply userbinator 15 hours agoprev [–] The cost of the tools required for device repair and the cost of genuine components make self repair almost as expensive as getting a repair from an Apple retail location or an Apple Authorized Service Provider Malicious compliance accomplished. Apple's instructions for all of the battery repairs include expensive equipment like an iPhone battery press to put a replacement battery back in place. It's like they just copy-pasted their production line processes, but clearly that's not necessary. Apple is known for their... interesting attitude towards repair, even in the previous manuals that have leaked. It somewhat reminds me of German automotive engineering --- lots of special tools and fixtures when a simpler and more conventional process would work just as well. reply atonse 15 hours agoparentIn just about any other situation in life, you will have to make some investment in tools with the understanding that you can use them multiple times. For example, I bought the iFixit repair kit nearly a decade ago and I have used it for any minor work for all that time. $80 spent once and I’ve never once needed to fish for some strange bit or tool no matter what device I’ve opened. In fact, the iFixit kit will still be sufficient for this entire repair plus the 9 volt battery of course. Unless you’re saying all these are one time use tools but I didn’t see that from the parts lists. reply mosselman 11 hours agorootparentI get what you mean and I agree. I own lots of tools just because I understand that they are a good investment. What I think the parent is referring to is this: https://support.apple.com/en-us/120983 Obviously it is bullshit to suggest that a consumer would buy these tools. But it is also bullshit to suggest that you actually \"need\" a 'battery press' just because it is on the parts list. The average phone repair shop will know how to loosen some battery adhesive very well with various techniques. So I don't think they will be discouraged by the Apple documentation. Malicious compliance? Seems like it, a little bit. Still useful though. reply devjab 11 hours agorootparentI think it could also be a sort of protection from customers breaking things leading to bad press. As you point out repair shops will be capable of doing repairs just fine, but your average users will probably think twice about getting the tools. I guess I can use myself as an anecdotal example, I’m the sort of person who might try to do a repair despite never having done any sort of work on electronics since I build a radio and a weather station in the Danish equivalent of high school decades ago. I’d probably end up breaking some parts. I’m rich enough to buy the Apple tools but I’m too much of a grinch to buy them. I actually think iFixit protects me from myself as well because it’s too complicated (for me) to buy the tools I’d need. reply Y_Y 8 hours agorootparentI can see why a company would want to prevent consumers from doing things that might reflect negatively on them. On the other hand, it doesn't make sense. If a customer does something stupid and breaks their phone then they shouldn't blame Apple, a responsible journalist won't make out that it's Apple's fault, a competent reader won't get the impression it was Apple's fault from an truthful article. I know it's asking a lot for sensible action from consumers, responsible journalists, a discerning news audience, a megacorp that respects individual rights. I think ultimately this is Apple's problem and their customers and should hold them to a higher, pro-social standard. That could be through government regulation, wallet voting, or \"unauthorized\" repair. reply throw0101c 7 hours agorootparent> If a customer does something stupid and breaks their phone then they shouldn't blame Apple, a responsible journalist won't make out that it's Apple's fault, a competent reader won't get the impression it was Apple's fault from an truthful article. \"shouldn't blame\", \"responsible journalist\", \"competent reader\". I see you're an idealist. :) reply Someone 5 hours agorootparentprev> If a customer does something stupid and breaks their phone then they shouldn't blame Apple, a responsible journalist won’t make out that it’s Apples is fault, Even ignoring the existence of plenty of less responsible “journalists”, if Apple were to publish “works reasonably well most of the time in the hands of a careful person with experience” repair manuals, I think it’s certain Apple would be blamed by almost all journalists. And I don’t think adding stringent warnings to their manuals would make things better for them. On the contrary, I expect “the Internet” would burn them at the stake for adding those terms. Any complaints shouldn’t be aimed against these repair methods, but at repairing being that difficult for modern stuff. But even then, it’s hard to complain. Thing is: pouring lots of good glue inside a smartphone and making parts fit incredibly tight together makes them much stronger and makes it easier to make them water resistant. reply shalmanese 10 hours agoparentprevThe Apple provided tools are the ones used at first party Apple stores to perform authorized repairs. At some point, some bean counter tabulated the cost of building X000 machines and shipping them across the globe for a marginal increase in repair quality and deemed it a worthy tradeoff. If you want to repair phones to the equivalent quality of Apple stores, Apple makes it possible via their \"overengineered\" machines. There's nothing in Apple's ToS that forces you to make repairs this way, you're welcome to buy the Apple genuine part and use your own heat mats and press and whatever and knowingly make that tradeoff. reply 7952 8 hours agorootparentAnd good tools can reduce the skill required to make the repair. reply rjzzleep 13 hours agoparentprevI was holding out for the EU DMA third party app store, but it's clear that Apple is not on a good trajectory. The fact that they slept on Siri for so long only to then finally add \"Open\"AI to it with limited availability is, but another dot in the pattern. When MacOS was still called OSX and developers were the Macbooks greatest contributors and cheerleaders, things looked a lot different. A lot of the current framework components were copied from community components back then. I'll miss the closed loop payment card support from iOS, but for everything else, I'll just say good riddance ... reply oarsinsync 8 hours agorootparent> I'll miss the closed loop payment card support from iOS I was definitely hurting for a while for lack of this (and iMessage) on Windows. While I haven’t gotten over iMessage (and use a KVM with my iPad to resolve that), the additional friction for payments has actually been a boon for my bank account. The more friction we feel with payments, the fewer payments we make. Cash is higher friction than cards. Manual card input is higher friction than password manager autocomplete. Autocomplete is higher friction than apple pay. I haven’t found anything lower friction than apple pay. By switching back to password manager autocomplete (which is unreliable at best), I’ve found my spending has gone down, because the cost of friction in payment is higher than the value of the item. (I clearly also have too much disposable income, but that’s a whole other tangent) reply unsigner 10 hours agoparentprevIf you think something like German automotive engineering or iPhone production can be substitute by a “simpler and more convenientional process”, you probably don’t understand how it works. These things have evolved and have been optimized to within parts of a percentage; almost everything is there for a good, time tested reason. (Except for ultra-novel stuff that has been around for a year or two - there they may pay with process inefficiencies for novelty) reply iknowstuff 6 hours agorootparentdon’t glorify german automakers too much. they’re 5-10 years behind Tesla when it comes to manufacturing optimization, sw/hw integration, BEV efficiency, etc. reply diggan 5 hours agorootparentOnly thing missing from Tesla to beat them now is being better at actually building lasting cars. Hopefully Tesla eventually learns what \"tolerance\" means and that cars sometimes get wet from rain. reply threeseed 14 hours agoparentprevThe cost of the tools required to cut my lawn is far more than hiring someone to cut it. Likewise for almost every home or car repair. The whole point is that the tools are largely a once off purchase and repairing your phone is something you might do throughout your life. Therefore the initial costs should be spread over a longer period. reply makeitdouble 13 hours agorootparentDo you expect your iPhone 16 battery press tool to still be useful in 2 phone generations ? How many times do you see yourself replacing the iPhone 16's battery ? If Apple was also promising to keep the same process for the next 7 years I'd see a point to this, but this of course not the case. reply wtallis 13 hours agorootparenthttps://www.selfservicerepair.com/en-US/tool-kit-rental Considering that it's been the same battery press going back at least as far as the iPhone 12, it's probably going to continue to be the same battery press for a long time. Especially now that they've definitely been using the same battery press across at least two methods of gluing in the battery (the adhesive with pull tabs, and the new adhesive that's released electrically). reply dperrin 13 hours agorootparentprev> Do you expect your iPhone 16 battery press tool to still be useful in 2 phone generations ? How many times do you see yourself replacing the iPhone 16's battery ? Lots of my bike tools I have will take over a decade to get my money back on my stuff alone. But I get to do something I mostly enjoy. I can also help out friends/acquaintances when they need it. The same goes for this. reply makeitdouble 8 hours agorootparentWhat seems to be lost here: - Bikes will last decades with good maintenance. An iPhone wont, even if the device hardware was somewhat kept alive most standard software functionalities will be lost. - Bikes don't mandate fancy tools for regular maintenance. Regular people won't need your super pricey tools to replace brakes or tires. You can use them if you want to, but that's your hobby, not what the maker mandates. You can enjoy nice tools, but that's orthogonal to the issue here IMHO. reply wtallis 3 hours agorootparent> Bikes don't mandate fancy tools for regular maintenance. Phone batteries need replacing every few years. The kind of maintenance that a bike used daily will need on that schedule is absolutely stuff that requires fancy tools. reply brailsafe 10 hours agorootparentprev> Lots of my bike tools I have will take over a decade to get my money back That's... a bit surprising. Maybe one or two I could see, like a truing stand or some one-off equally proprietary thing for one brand of part, but what else? Edit: nvm, there seems to be plenty of Park Tools brand niche reamers and so on that are many hundreds of dollars. I would think they'd remain viable for much longer than a battery replacement press though, since you'd adapt it to a particular bike's repair needs with different bits. reply asimpletune 12 hours agorootparentprevHey do you have any recommendations on a small kit to bring for long bike trips? reply petre 10 hours agorootparentBlackburn switch tool with chain press, tire levers, patch kit, spare tube. For anything not fixable with those, you visit a shop. reply moring 5 hours agorootparentprev> Do you expect your iPhone 16 battery press tool to still be useful in 2 phone generations ? Let's assume a \"no\" for the sake of the discussion. > How many times do you see yourself replacing the iPhone 16's battery ? 100s of times (for 100s of different phones, obviously), because otherwise you _would not be buying repair tools_! The whole notion that you buy a set of high-tech tools and then use them only once or twice is the insanity that causes this whole discussion to even take place. These are tools intended for professional repair shops, not for consumers to repair their own phone. reply renewiltord 12 hours agorootparentprevMy dude, I bought a Park Tools Crank Puller CCP-44. This works on a M12 or M15 crank bolt. This is great since it worked on my Peloton and my bike. Then the other day, my friend's bike needed a CCP-22 which works on an M8 crank bolt. Oh no, why did the bike industry not all use M12. I am replacing my iPhone 13 tomorrow with an iPhone 16. Three years of use. If I were using it another three years, I might use the battery press once. This is how tools are. To have amortized utility, you need to use them multiple times. The CCP-22 was a one-time use tool. reply bluescrn 11 hours agorootparentprevReplacing a consumable part, particularly a battery, should not be a complex repair requiring specialised tools. reply shreddit 11 hours agorootparentiPhone batteries have been replaced long before Apple provided specialized tools, so you don’t need any of them. They will make your life a lot easier though. reply lijok 7 hours agorootparentprevCan you name something that Should be a complex repair? reply fsflover 5 hours agorootparentReplacing the mainboard? reply oneplane 6 hours agoparentprev> but clearly that's not necessary You are both wrong and contradicting yourself. If it isn't necessary, then complaining about the tools is moot since as you wrote yourself, it's not necessary. But, if you design a portable consumer device and you know to what tolerance you need a battery to be adhered to the case to make it not come loose, you know what pressure, movement, adhesive etc. are needed to make that happen for the form factor the battery is going to fit in. You know who doesn't know that? Pretty much everyone who isn't an engineer, and for most people who are an engineer, they might know that these are parameters that exist, but they aren't going to know every variation for every device ever produced. So now that nonsense about it being \"clearly\" is not so clear anymore. Engineering things to be safe and reliable is pretty difficult. Add in batteries and it's suddenly one or more orders of magnitudes more difficult. That much is definitely clear, because when you cut corners, just guess or think to yourself \"it is just a battery and some adhesive, how hard can this be\" you get phones self-igniting on airplanes. If we take your full line as a quote: > It's like they just copy-pasted their production line processes, but clearly that's not necessary. Do you really think there are a bunch of people using manual hand tools mass producing every aspect of a phone? Sure, there might be a bunch of steps where manual labour was effective and efficient, but it's not like you show up at the factory with your suitcase of tools and go to work at your desk. What they reproduced is the parameters. And that is exactly what you want. A repaired product should be as close to a freshly manufactured product as possible if you're going to be liable for it. reply ghodith 5 hours agorootparent> What they reproduced is the parameters. And that is exactly what you want. A repaired product should be as close to a freshly manufactured product as possible if you're going to be liable for it. This has some sense to it, mostly from the liability point of view, but this > if you design a portable consumer device and you know to what tolerance you need a battery to be adhered to the case to make it not come loose, you know what pressure, movement, adhesive etc. are needed to make that happen for the form factor the battery is going to fit in. as someone who works in this field, this is overstating the matter quite a bit. The tolerances for something like pressure in this instance are going to be wide enough that \"press firmly\" would suffice in a rework document. It's made to be very simple on purpose for manufacturing, and a lot of slop is built in so that we're not in this situation where microns or milli-newtons matter and cause a battery fire somewhere down the line. The fixtures are primarily for efficiency gains, and in that sense I would agree with the gp that press fixtures are not practically necessary in an at-home version of this process. reply oneplane 1 hour agorootparent> The fixtures are primarily for efficiency gains I agree, however, the battery example was a prime candidate for \"it is not always as simple as it seems\". LiPo batteries don't like getting squished. reply appendix-rock 13 hours agoparentprevSorry, but have you ever repaired anything? The number of things the price and complexity of a phone, that can be repaired for less than the replacement cost, when you include tools, is…very small. reply fshbbdssbbgdd 10 hours agoparentprevI was gonna say, iPhones are way more reliable than German cars, it’s not a fair comparison! Then I thought about their respective depreciation curves… reply Sakos 11 hours agoparentprevnext [8 more] [flagged] hu3 10 hours agorootparentApple tends to get tribalistic, victim blaming, passive aggressive hand-waving from a very vocal part of their customers. You see, it's hard to fault the product you paid thousand(s) of dollars because, to some, this implies in also faulting their own decision making process, which is painful. The walled garden, predatory behaviour from Apple takes this to a new height. Because once you're invested enough in Apple gadgets that mostly only integrate and depend on other Apple products, you might have to fault your decision of spending $10k+, which is just too painful to most. reply refurb 9 hours agorootparentprevWhen I buy an iPhone I know I’m not buying a phone optimized for easy user repair. I go in eyes wide open and I’m fine with it. I know the product is optimized for compactness and efficient use of space. It’s optimized for ruggedness, and water resistance. I would never do a self-repair and expect the cover to be held on with Phillips screws and a bulky o-ring. And if I choose to do a self-repair I fully expect that I’ll need to buy expensive tools I can’t use with any other brand. And I don’t expect Apple to create a manual for “how to do janky iPhone repairs at home with a multi-tool and silicone caulking” I’m well aware that if this is not the kind of phone I want, then I’ll keep looking for another phone manufacturer. It’s like buying a Porsche and complaining it’s hard to fit a car seat in or ask “why isn’t there room for hauling building materials”. If I want those things I don’t look at Porsches. reply palata 9 hours agorootparent> It’s like buying a Porsche and complaining Is it, though? How many iPhone owners can afford a Porsche? I don't think it's remotely like a Porsche: not the same volume, not the same customers. I could say \"it's like buying shoes and expecting to be able to change shoelaces\" and it wouldn't have much less value than your comparison. reply pseudosaid 8 hours agorootparenttry again. affording a porsche, volume, and same customers have nothing to do with prior poster’s statement. its about moving in next to a music club and complaining about noise reply palata 8 hours agorootparentIt's actually about moving next to a music club without knowing that there is a music club, in a world where the vast majority of (affordable) apartments are next to a music club. Would it be weird to complain about the fact that there is always noise everywhere and that you wish we improved the situation? reply refurb 7 hours agorootparentprev> I don't think it's remotely like a Porsche Ok, replace it with a $5,000 used pickup truck that is 20 years old. You don't buy an a pickup truck then complain about the ride quality or fuel economy. Or the fact it doesn't have 6 seats for passengers. reply Sakos 7 hours agorootparentprevMost users aren't consciously making this choice. It's great that you are, but every time an iPhone owner would balk at the insane repair costs, all we could do is shrug and say that's just how Apple designs their devices. So, honestly, you can bug off with this \"fuck you, I know what I'm getting into\" attitude. It's telling what kind of response I'm getting. All you care about is yourself. reply sandwichmonger 13 hours agoparentprev> It's like they just copy-pasted their production line processes, but clearly that's not necessary. If they copy-pasted their production line processes the parts would cost less than $40 total. reply madeofpalk 9 hours agorootparentTheir production line process is optimised for producing millions of devices. reply journal 14 hours agoparentprev [–] I wonder if aliens exist, what technology they have for basics like transportation. Do they just load themselves into a cannon and shoot them to the destination? Just completely different ways of doing everything. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple has released repair manuals for the iPhone 16, 16 Plus, 16 Pro, and 16 Pro Max, aimed at experienced technicians.",
      "The iPhone 16 and 16 Plus feature an easier electric battery removal process, while the Pro models still use standard adhesive tabs.",
      "Despite improvements in repairability, battery repairs require expensive equipment, making self-repair nearly as costly as professional services."
    ],
    "commentSummary": [
      "Apple has released repair manuals for the iPhone 16 and iPhone 16 Pro, outlining steps and tools needed for repairs, such as battery replacement.",
      "The repair process involves using a 9-volt battery to remove adhesive, raising concerns about the complexity and cost of required tools.",
      "Discussions highlight the EU's upcoming regulations on removable batteries and the balance between repairability and device durability, with mixed reactions from users."
    ],
    "points": 197,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1726877836
  },
  {
    "id": 41609099,
    "title": "The Collapse of Self-Worth in the Digital Age",
    "originLink": "https://thewalrus.ca/collapse-of-self-worth-in-the-digital-age/",
    "originBody": "When I was twelve, I used to roller-skate in circles for hours. I was at another new school, the odd man out, bullied by my desk mate. My problems were too complex and modern to explain. So I skated across parking lots, breezeways, and sidewalks, I listened to the vibration of my wheels on brick, I learned the names of flowers, I put deserted paths to use. I decided for myself each curve I took, and by the time I rolled home, I felt lighter. One Saturday, a friend invited me to roller-skate in the park. I can still picture her in green protective knee pads, flying past. I couldn’t catch up, I had no technique. There existed another scale to evaluate roller skating, beyond joy, and as Rollerbladers and cyclists overtook me, it eclipsed my own. Soon after, I stopped skating. Don’t let news disappear from your feed. Sign up for The Walrus newsletter and get trusted Canadian journalism straight in your inbox. Email* GDPR By checking this box I consent to the use of my information for emails from The Walrus.* Submit Years ago, I worked in the backroom of a Tower Records. Every few hours, my face-pierced, gunk-haired co-workers would line up by my workstation, waiting to clock in or out. When we typed in our staff number at 8:59 p.m., we were off time, returned to ourselves, free like smoke. There are no words to describe the opposite sensations of being at-our-job and being not-at-our-job even if we know the feeling of crossing that threshold by heart. But the most essential quality that makes a job a job is that when we are at work, we surrender the power to decide the worth of what we do. At-job is where our labour is appraised by an external meter: the market. At-job, our labour is never a means to itself but a means to money; its value can be expressed only as a number—relative, fluctuating, out of our control. At-job, because an outside eye measures us, the workplace is a place of surveillance. It’s painful to have your sense of worth extracted. For Marx, the poet of economics, when a person’s innate value is replaced with exchange value, it is as if we’ve been reduced to “a mere jelly.” Wait—Is ChatGPT Even Legal? AI Is a False God How Israel Is Using AI as a Weapon of War Not-job, or whatever name you prefer—“quitting time,” “off duty,” “downtime”—is where we restore ourselves from a mere jelly, precisely by using our internal meter to determine the criteria for success or failure. Find the best route home—not the one that optimizes cost per minute but the one that offers time enough to hear an album from start to finish. Plant a window garden, and if the plants are half dead, try again. My brother-in-law found a toy loom in his neighbour’s garbage, and nightly he weaves tiny technicolour rugs. We do these activities for the sake of doing them, and their value can’t be arrived at through an outside, top-down measure. It would be nonsensical to treat them as comparable and rank them from one to five. We can assess them only by privately and carefully attending to what they contain and, on our own, concluding their merit. And so artmaking—the cultural industries—occupies the middle of an uneasy Venn diagram. First, the value of an artwork is internal—how well does it fulfill the vision that inspired it? Second, a piece of art is its own end. Third, a piece of art is, by definition, rare, one of a kind, nonfungible. Yet the end point for the working artist is to create an object for sale. Once the art object enters the market, art’s intrinsic value is emptied out, compacted by the market’s logic of ranking, until there’s only relational worth, no interior worth. Two novelists I know publish essays one week apart; in a grim coincidence, each writer recounts their own version of the same traumatic life event. Which essay is better, a friend asks. I explain they’re different; different life circumstances likely shaped separate approaches. Yes, she says, but which one is better? Igrew up a Catholic, a faithful, an anachronism to my friends. I carried my faith until my twenties, when it finally broke. Once I couldn’t gain comfort from religion anymore, I got it from writing. Sitting and building stories, side by side with millions of other storytellers who have endeavoured since the dawn of existence to forge meaning even as reality proves endlessly senseless, is the nearest thing to what it felt like back when I was a believer. I spent my thirties writing a novel and paying the bills as low-paid part-time faculty at three different colleges. I could’ve studied law or learned to code. Instead, I manufactured sentences. Looking back, it baffles me that I had the wherewithal to commit to a project with no guaranteed financial value, as if I was under an enchantment. Working on that novel was like visiting a little town every day for four years, a place so dear and sweet. Then I sold it. As the publication date advanced, I was awash with extrinsic measures. Only twenty years ago, there was no public, complete data on book sales. Until the introduction of BookScan in the late ’90s, you just had to take an agent’s word for it. “The track record of an author was a contestable variable that was known to some, surmised by others, and always subject to exaggeration in the interests of inflating value,” says John B. Thompson in Merchants of Culture, his ethnography of contemporary publishing. This is hard to imagine, now that we are inundated with cold, beautiful stats, some publicized by trade publications or broadcast by authors themselves on all socials. How many publishers bid? How big is the print run? How many stops on the tour? How many reviews on Goodreads? How many mentions on Bookstagram, BookTok? How many bloggers on the blog tour? How exponential is the growth in follower count? Preorders? How many printings? How many languages in translation? How many views on the unboxing? How many mentions on most-anticipated lists? I was glued to my numbers like a day trader. I wanted to write my publicist to ask: Should I be worried my stats aren’t higher? The question blared constantly in my head: Did gambling years I could’ve been earning towards a house pay off? But I never did. I was too embarrassed. I had trained in the religion of art, and to pay mind to the reception of my work was to be a non-believer. During my fine arts degree, we heard again and again that the only gauge for art is your own measure, and when I started teaching writing, I’d preach the same thing. Ignore whatever publications or promotions friends gain; you’re on your own journey. It’s a purportedly anti-capitalist idea, but it repackages the artist’s concern for economic security as petty ego. My feelings—caring at all—broke code. Shame sublimated everything. And when the reception started to roll in, I’d hear good news, but gratitude lasted moments before I wanted more. A starred review from Publisher’s Weekly, but I wasn’t in “Picks of the Week.” A mention from Entertainment Weekly, but last on a click-through list. Nothing was enough. Why? What had defined my adult existence was my ability to find worth within, to build to an internal schematic, which is what artists do. Now I was a stranger to myself. I tried to fix it with box breathing videos, podcasts, reading about Anna Karenina. My partner and I were trying for another baby, but cycles kept passing, my womb couldn’t grab the egg. A kind nurse at the walk-in said: Sometimes your body is saying the time’s not right. Mine was a bad place. A few weeks after my book release, my friends and I and our little kids took a weekend vacation. They surprised me with a three-tiered cake matching my book cover, cradled on laps, from Toronto, through a five-hour traffic jam. In all the photos from that trip, I’m staring at my phone. I can hardly remember that summer. My scale of worth had torn off, like a roof in a hurricane, replaced with an external one. An external scale is a relative scale; so of course, nothing’s enough. There is no top. Then I was shortlisted for a major prize. It took me on a world tour, listed me alongside authors who are certifiable geniuses. I thought my endless accounting could stop, this had to be enough for me, I could get back to who I was. But I couldn’t. In London, I bought my two-year-old a bath toy, a little boat with a Beefeater. Today at bath time, the boat still gives me a sickly feeling, like it’s from the scene of a trauma. My centre was gone. One of at-job’s defining qualities is how efficiently output is converted into a number. In 1994, Philip Agre described this as the “capture model,” or “the deliberate reorganization of industrial work activities to allow computers to track them in real time.” Gregory Sholette, the author of Dark Matter: Art and Politics in the Age of Enterprise Culture, describes how workers in a Pennsylvania factory spent their break covering a wall of the plant with “newspaper clippings, snapshots, spent soda cans, industrial debris, trashed food containers and similar bits and pieces.” They called it “Swampwall.” It reminds me of the sculpture on a high shelf in the back of a diner where I worked, composed of unusually shaped potatoes. Its form changed with each new tuber contributed by the cook on prep shift. Such spontaneous projects are signs of life: physical evidence of the liberating fact that not all time at work can be measured or processed into productivity. Swampwall was inutile: a means to itself. It was allowed to flourish until the company was bought out by a global corporation, at which point the massive collaborative mural was “expunged.” Thirty years after Agre coined the capture model, workforce management technology can track every moment at work as a production target. Amazon’s Units Per Hour score, Uber’s and Lyft’s (constantly shrivelling) base fares, and Domino’s Pizza Tracker have made it possible to time all time, even in the break room or toilet stall. These are extreme examples, but they’re echoed across the work world, with the datafication of parts of performance that used to be too baggy or obscure to crunch and so were ours to keep. “Wellness” apps provided as health benefits by corporate management that track fob swipes for office workers; case management software that counts advice by the piece for legal workers; shares, hover rate, and time on site that measure media workers; leaderboards for tech employees, ranking who worked longest. There must exist professions that are free from capture, but I’m hard pressed to find them. Even non-remote jobs, where work cannot pursue the worker home, are dogged by digital tracking: a farmer says Instagram Story views directly correlate to farm subscriptions, a server tells me her manager won’t give her the Saturday-night money shift until she has more followers. Even religious guidance can be quantified by view counts for online church services, Yelp for spirituality. One priest told the Guardian, “you have this thing about how many followers have you . . . it hits at your gut, at your heart.” But we know all this. What we hardly talk about is how we’ve reorganized not just industrial activity but any activity to be capturable by computer, a radical expansion of what can be mined. Friendship is ground zero for the metrics of the inner world, the first unquantifiable shorn into data points: Friendster testimonials, the MySpace Top 8, friending. Likewise, the search for romance has been refigured by dating apps that sell paid-for rankings and paid access to “quality” matches. Or, if there’s an off-duty pursuit you love—giving tarot readings, polishing beach rocks—it’s a great compliment to say: “You should do that for money.” Join the passion economy, give the market final say on the value of your delights. Even engaging with art—say, encountering some uncanny reflection of yourself in a novel, or having a transformative epiphany from listening, on repeat, to the way that singer’s voice breaks over the bridge—can be spat out as a figure, on Goodreads or your Spotify year in review. And those ascetics who disavow all socials? They are still caught in the network. Acts of pure leisure—photographing a sidewalk cat with a camera app or watching a video on how to make a curry—are transmuted into data to grade how well the app or the creators’ deliverables are delivering. If we’re not being tallied, we affect the tally of others. We are all data workers. Twenty years ago, anti-capitalist activists campaigned against ads posted in public bathroom stalls: too invasive, there needs to be a limit to capital’s reach. Now, ads by the toilet are quaint. Clocking out is obsolete when, in the deep quiet of our minds, we lack the pay grade to determine worth. The internet is designed to stop us from ever switching it off. It moves at the speed of light, with constantly changing metrics, fuelled by “‘ludic loops’ or repeated cycles of uncertainty, anticipation and feedback”—in other words, it works exactly like a Jackpot 6000 slot machine. (On a basic level, social media apps like Instagram operate like phone games. They’ve replaced classics like Snake or Candy Crush, except the game is your sense of self.) The effect of gamification on artmaking has been dramatic. In Rebecca Jennings’s Vox long read on the necessity of authorly self-promotion, she interviews William Deresiewicz, whose book The Death of the Artist breaks down the harsh conditions for artists seeking an income in the digital economy. Deresiewicz used to think “selling out”—using the most sacred parts of your life and values to shill for a brand—was “evil.” Yet this economy has made it so there’s “no choice” if you want a living. The very concept of selling out, he says, “has disappeared.” A few years ago, much was made of the fact that the novelist Sally Rooney had no Twitter account—this must explain her prolific output. But the logic is back to front: it’s only top-selling authors who can afford to forgo social media. Call it Deactivation Privilege. It’s a privilege few of us can afford, if it’s the algorithm we need to impress rather than book reviewers of old. In a nightmarish dispatch in Esquire on how hard it is for authors to find readers, Kate Dwyer argues that all authors must function like influencers now, which means a fire sale on your “private” life. As internet theorist Kyle Chayka puts it to Dwyer: “Influencers get attention by exposing parts of their life that have nothing to do with the production of culture.” The self is the work, just ask Flaubert. But data collection’s ability to reduce the self to a figure—batted about by the fluctuations of its stock—is newly unbearable. There’s no way around it, and this self being sold alongside the work can be as painful for a writer of autofiction as it is for me, a writer of speculative fiction who invented an imaginary world. Itell you all this not because I think we should all be very concerned about artists, but because what happens to artists is happening to all of us. As data collection technology hollows out our inner worlds, all of us experience the working artist’s plight: our lot is to numericize and monetize the most private and personal parts of our experience. Certainly, smartphones could be too much technology for children, as Jonathan Haidt argues, and definitely, as Tim Wu says, attention is a commodity, but these ascendant theories of tech talk around the fact that something else deep inside, innermost, is being harvested too: our self-worth, or, rather, worthing. We are not giving away our value, as a puritanical grandparent might scold; we are giving away our facility to value. We’ve been cored like apples, a dependency created, hooked on the public internet to tell us the worth. Every notification ping holds the possibility we have merit. When we scroll, what are we looking for? When my eldest child was in kindergarten, she loved to make art, but she detested the assignments that tried to make math fun by asking kids to draw. If I sat her down to complete one, she would stare rebelliously at her pencil or a strand of her hair rather than submit. Then one day, while drawing a group of five ants and a group of eight ants, my kindergartener started to sketch fast. She drew ants with bulbous limbs growing out of their bodies, like chains of sausages. “Bombombom!” she cried, flapping her arms up and down. “These are their muscles.” She continued to draw and mime pumping iron, giggling to herself, delighted to have planted something in her homework that couldn’t be accounted for in the metric of correct or incorrect. She had taken drawing back. The ludic loop of the internet has automated our inner worlds: we don’t have to choose what we like, or even if we like it; the algorithm chooses for us. Take Shein, the fast fashion leviathan. While other fast fashion brands wait for high-end houses to produce designs they can replicate cheaply, Shein has completely eclipsed the runway, using AI to trawl social media for cues on what to produce next. Shein’s site operates like a casino game, using “dark patterns”—a countdown clock puts a timer on an offer, pop-ups say there’s only one item left in stock, and the scroll of outfits never ends—so you buy now, ask if you want it later. Shein’s model is dystopic: countless reports detail how it puts its workers in obscene poverty in order to sell a reprieve to consumers who are also moneyless—a saturated plush world lasting as long as the seams in one of their dresses. Yet the day to day of Shein’s target shopper is so bleak, we strain our moral character to cosplay a life of plenty. Automation isn’t forced upon us: we crave it, oblivion, thanks to the tech itself. As the ascendant apparatus of the labour market, it’s squeezed already dire working conditions to a suffocation point, until all we desire is the sweet fugue of scroll, our decision maker set to “off.” After my novel came out, whenever I met an author, I would ask, with increasing frenzy, how they managed the grisly experience of work going to market. I was comforted and horrified when everyone agreed it could be dispossessing. Then they all said the same thing: “I kept writing and I felt better.” That was the advice: keep writing. The market is the only mechanism for a piece of art to reach a pair of loving eyes. Even at a museum or library, the market had a hand in homing the item there. I didn’t understand that seeking a reader for my story meant handing over my work in the same way I sold my car on Craigslist: it’s gone from me, fully, bodily, finally. Or, as Marx says, alienated. I hated that advice to keep writing, because if I wrote another book, I’d have to go through the cycle again: slap my self on the scale like a pair of pork chops again. Now, I realize the authors I met meant something else. Yes, sell this part of your inner life but then go back in there and reinflate what’s been emptied. It’s a renewable resource. When I grasp this, all of it becomes tolerable. It’s like letting out a line, then braiding more line. I can manage, because there’ll always be more line. Iwill try to sell this essay to a publication, and if successful, the publication will try to sell it to readers. If you are reading this, it’s a commodity now, fluctuating and fungible, like so much digital dust. Thea Lim Thea Lim is an author, a culture writer, and a creative writing teacher. Her most recent novel is An Ocean of Minutes. TaggedAIartaudioessayhomepage",
    "commentLink": "https://news.ycombinator.com/item?id=41609099",
    "commentBody": "The Collapse of Self-Worth in the Digital Age (thewalrus.ca)175 points by pseudolus 8 hours agohidepastfavorite145 comments skissane 6 hours agoThe other day I was talking to a psychologist about [difficult personal situation which I am unable to discuss publicly] and she said to me “This must be really damaging to your self-worth.” And, my honest internal reaction to her statement (although I didn’t say it out loud to her) was “Self-worth, what is that?” Because I’m not sure if I have any? I don’t mean that in the negative sense that I think I am worthless or anything. It is just that in my mind “self” and “worth” are concepts which do not intersect. Maybe that’s an autistic trait. reply nostrademons 2 hours agoparentA related observation I've gotten from interacting with normies including my wife: Most people assume there is such thing as a \"self\" and that it is okay, natural, and desirable to preference yourself when interacting with other people. Indeed, for most normies, there is only various people and their selves interacting. My autistic perspective is that there is only a system of the world, various roles that people happen to occupy, and here are my options for where I can slot into it, might as well choose the best one, but it never occurred to me that the other people in the system could be individually influenced or that you could preference your self over others. At least, until I interacted with others enough to realize that that was all they were doing. Relatedly, most normies project this self-interest onto aspies and assume that when they are saying how the world is, they are pushing their own agenda, because in their worldview everybody has an agenda. As a result, the aspie tendency to make observations about broad sweeping systems comes off as arrogance, as preferencing themselves and their worldview over everybody else. But to an aspie, there is no such thing as arrogance, because arrogance implies that one preferences their self-worth over others, and there is no such thing as \"self\" or \"worth\", and it is nonsensical to talk about self-importance. reply usefulcat 14 minutes agorootparent> most normies project this self-interest onto aspies and assume that when they are saying how the world is, they are pushing their own agenda, because in their worldview everybody has an agenda. I have a theory (not claiming to have invented it, just something I often think about) that most people have a strong tendency to assume that other people think the same way they do. reply ajb 1 hour agorootparentprevWell that's an interesting theory. However, on reading it, my leading hypotheses are that people who express such disbelief in the possibility of their being selfish, either lack self-insight, or are lying. Your theory I would give a subjective probability rather lower than these. Of course, this might be my lack of insight into the aspie mind. But self-interest is highly evolutionarily favourable, and is built into our brains from very early in our evolutionary history - from the first organisms that had brains. reply ttpphd 1 hour agorootparentYeah it's your lack of insight and your projection of your own psychology onto others reply crazygringo 1 hour agorootparentprevThis is a fascinating comment that really made me think. And I'd like to present you with a different perspective. You write that \"My autistic perspective is that there is only a system of the world\". But it's important to realize that the \"system of the world\" you've developed is from your perspective exclusively. You are modeling everybody in your system, and you may be doing your best to be 100% objective and not push your own agenda. However, your model is lacking because you can model your own \"role\" far better than anybody elses \"role\" -- you know your own preferences and desires with great accuracy, but not other people's. And you'll likely unconsciously assume that other people's preferences and desires as the same as yours, until proven otherwise. You write \"the aspie tendency to make observations about broad sweeping systems comes off as arrogance\". But what if it has nothing to do with broad sweeping systems, but the fact that your observations are extrapolating too much from your own preferences/desires that you are projecting onto others without realizing it? So the perceived \"arrogance\" may not be what you describe as an inappropriate reaction that you are bringing an agenda to things. But may actually be an appropriate reaction to the fact that you're making too many assumptions in your \"observations about broad sweeping systems\" that the systems in your head are reality, when they are not. When I have conversations with really smart people who come across as arrogant, it's not usually because they are being overly objective. It's because while their logic and deductions may be 100% correct, it's their starting assumptions that are wrong, which usually assume that other people have the same preferences/desires as themselves. But sometimes it's really hard to understand just how different people can be. reply orbisvicis 4 hours agoparentprevThe flip side of detaching self-esteem from technical competency is that you can make very strong assertions without being arrogant. People of integrity with utilitarian leaning are often labeled amoral or unemotional. People with a strong drive not attached to financial gain are often termed unambitious. Your perceived worth really depends on values of society, so if you remove yourself from the equation, you only have no worth to others, not to yourself. Sequestered from others the sense of self has no meaning, so naturally you have no self-worth. Not because you are worthless, but because you have no self and place no importance on your perceived worth. Or more strongly phrased, what's the point of self-worth if you can do everything you put your mind to? I'd you can't but think you can, then that's a harmful psychological schism. Since no one person can achieve everything, self-worth is only meaningful in areas for which you lack competency For example - and this deals not with self-worth but with stress - I'm pretty inflexible in my goals. When I can't meet my goals I tend to shut down. I'm often asked if I'm stressed, to which I can only respond, \"Stress - what's that?\". And yet clearly I've suffered a harmful schism between my perception of self and reality, as indicated by my lack of stress response. reply detourdog 3 hours agorootparentMy partner considers my deliberateness as lazy. They won’t discuss things with me because they already know all about me. My willingness to share their perception of me being ineffective was quite damaging to my self esteem. I never felt successful due to lack of acknowledgment of my achievements. They asked me to leave the house about a year ago. Once I met new people and they saw me as successful and fun my self worth improved. reply orbisvicis 3 hours agorootparentYes, that's why I qualified self-worth by field of competency. I really try to view the world objectively, ignoring the opinions of others, but after N fizzled relationships... is it me or is it you? But just because you are detached from societal values doesn't imply you should be detached from people. I can't enumerate the intangibles I get by surrounding myself with others. I perform better, I have better ideas, I feel more alert. I've never been able to understand why as I never gain any objective aid. Does unexpected input promote flexible thinking? Am I inspired by differing worldviews to think outside the box. Or is it simple happiness? No idea. reply detourdog 17 minutes agorootparentThe closest conclusion is that some people are flexible and looking forward and others are rigid and looking for reasons to be dissatisfied. reply darby_nine 3 hours agorootparentprev> People of integrity with utilitarian leaning are often labeled amoral or unemotional. Utilitarianism is a moral concept. I think you just mean analytical. reply Aeolun 1 hour agorootparentMaybe, but some people think it’s weird when you say “of course I’d divert the train over that grandpa” reply __turbobrew__ 5 hours agoparentprevI feel the same. One thing that I have struggled with in relationships is that others need to feel validated, that their decisions are rational and that other people believe that they are rational. Stimulus from the outside to satisfy the inside. What makes this difficult for me is that I don’t need to feel validated, I am comfortable with my own decisions in life and do not need someone else to approve. I think this ties into self worth which seems to be related to people’s perception of their actions and the approval they see from others. If you do not need external validation then you are not concerned about peoples perception of you and therefore self worth is a foreign concept. reply Aeolun 1 hour agorootparentYou may not need anyone else to approve, but doesn’t it still cause issues when they don’t? reply TheOtherHobbes 55 minutes agorootparentYes, but that's a different problem. Although it's also tangential to the essay, which is about the mechanisation of self-image - not just sell-worth, but identity in general. Which has always been socially imposed (even on those who are sure they're different). But now it can be directed algorithmically by a very small number of actors, who have the power to apply behaviour and value modification tools that can be individually tailored for everyone in a demographic - in ways which most people aren't even aware of. \"Low self-esteem\" is just one the effects. reply lordleft 2 hours agoparentprevI am a Christian. Something I have found within my faith tradition (and something I find is lacking in the culture around me) is a sense that I am in possession of an infinitely durable source of dignity and worthiness that is not tethered to who I am or what I have done. I have found this conceit enormously consoling. reply squidgedcricket 2 hours agorootparentI'm envious of that intrinsic sense of self worth, but I don't follow how that's a consequence of being Christian. I carry guilt and shame from sins that can't be rectified, knowing that Jesus loves me doesn't help me love myself. reply Aeolun 1 hour agorootparentWell, for some people I imagine it helps to know that there is someone that loves them. reply bitwize 44 minutes agorootparentprevChristian mythology is powerful because of the idea that God loves you so much he will give you infinite chances to repent and turn away from sin for as long as you live. Carrying guilt and shame for your sins won't fix them, but being sorry for them and working to sin no more will fix your future. It was even more powerful in an era when the gods were vengeful in their retribution and/or capricious in their favor. I'm not saying I like it or agree with it, but it does really bring some form of comfort to people, which is why it spread so far and lasted for so long. reply MisterBastahrd 2 hours agorootparentprevIt's pretty funny when you consider that a huge part of Christian evangelism is the attempt to convince normal people that they aren't worth dirt and are condemned to eternal torture unless they believe in somebody that the religious can't prove exists for a feat that they can't prove ever happened. reply Apocryphon 1 hour agorootparentThat's a rather reductive take. Telling people \"they aren't worth dirt\" is a hard sell, for one thing. reply TheOtherHobbes 1 hour agorootparentNot when you're also trying to sell them your solution. \"You are a worthless sinner, a transient cloud of mortal dust, with a soul that is in danger of an eternity of torment. Unless you - you know - do what we tell you.\" As marketing pitches go, it seems pretty straightforward. reply norir 1 hour agorootparentprevYes, that is one manifestation of Christian evangelism. Modern Christianity is plagued by literalism and ignorance. Most self identifying Christians (and I would posit most skeptical rationalists) have not deeply engaged with the texts and often when they do get hung up on an overly literal interpretation. If one looks beyond the surface, there is tremendous wisdom that helps everyone live better today -- not just in some hypothetical afterlife. Indeed, I believe that if you try to follow the teachings of Jesus, your interactions with others and your life writ large will be better. That has been my personal experience and I had to be called back kicking and screaming as a committed non-believer. Of course, having said all that, I am personally dismayed by the state of Christianity in our world. Institutional Christianity has done and continues to do tremendous harm in the world so I am sympathetic to your perspective. Indeed, I am most upset because I find the way that many nominally Christian institutions behave to be directly contrary to the their own sacred texts. The hypocrisy is almost unbearable. But I would encourage everyone reading this to withhold judgment of other's faith practices and/or self-identification until you understand where they are coming from rather than lumping them in with the worst exemplars of their nominal affiliates. reply bitwize 1 hour agorootparentprevAllistic people tend to feel a need to believe in something, even (especially) if they cannot produce evidence for it. This is called \"faith\" and widely regarded as a virtue because it gives structure and purpose to an existence that largely came about by accident. Jesus is fairly middle of the road in terms of horribleness of things to have faith in. reply wpietri 4 hours agoparentprevI think there are two ways you can look at it. One is what I think of as the neurotypical way. They spend a lot of energy on social modeling, on fitting people into hierarchies of privilege. (E.g., \"respecting your elders\", older sibling vs younger, teachers vs students, the popular kids, and so many other things.) Then in the same way they're judging the worth of others, they fit themselves into the same primate status model. I think that's what the psychologist is talking about. Personally, I find that way not super useful. But another way is sort of reverse engineering. From how person X treats themself, what can we learn about how much they value their own self? Like you, I didn't have much of a concept of self-worth in the sense of \"where do I place myself in the many hierarchies most people around me are constantly aware of.\" But a therapist eventually got me to see that I did not treat myself as worth the same as the people around me. Paying attention to that has improved my life a lot. The big question for me is to what extent the latter thing is influenced by the former. For neurotypical people I gather the link is pretty strong. For me it's definitely a weaker link, but it's hard to tell the difference between \"there is no link\" and \"I don't notice the link\". reply orbisvicis 3 hours agorootparentAre you suggesting that self-worth can only be defined as the sum of your perceived worth to others? That makes the 'self' in worth an oxymoron, no? To you, it seems the best way to achieve self-improvement is to maximize your value to others, i.e. by moving up the social hierarchy. But that doesn't imply that those who don't play the game have no worth, does it? I think you are conflating a sense of happiness with a sense of worth. They are not necessarily the same. For example I occasionally find myself in conflict with an acquaintance over a miscommunication. If after explaining the underlying conditions the other individual refuses to adjust their perception of me, I couldn't care less. That's their problem, not mine, even if they continue to spread their (possibly vile) misperceptions. Now if I had sucked up to them perhaps, yes, I would have improved my life. But what I did not do was reduce my self-worth. reply wpietri 3 hours agorootparentI just described two different ways of thinking about self-worth, so I am not suggesting it \"can only be defined\" as anything. reply BadHumans 4 hours agorootparentprevNot a psychologist but I am friends with more than a few. I don't know why you assume the psychologist meant the first because they did not. Self-worth has nothing to do with social hierarchy. It is how you treat yourself and a core thing therapist work on is helping you treat yourself the way you would treat others, with compassion and respect. reply wpietri 3 hours agorootparentI thought maybe I was using the phrase wrong, but the first two definitions I find are \"the internal sense of being good enough and worthy of love and belonging from others\" and \"a feeling that you are a good person who deserves to be treated with respect\". You'll note that those are both inherently social. And both \"good enough\" and \"with respect\" are about one's position in the caste/hierarchy structure. The reason I assume what the psychologist meant is that most of them are neurotypical, and neurotypical people are deeply invested in social primate dynamics. I understand that this is hard for neurotypical people to see, but you might read things like DeWaal's \"Chimpanzee Politics\" or Johnson's \"Impro\" [1]. Plus there's my personal experience, where psychologists are very inclined to talk about self-worth in the social sense. And I think that's fine; I'm sure it works well for their neurotypical patients. [1] particularly the section on status transactions, which are vital for authentic theater performances, but are rarely articulated because it's so natural to neurotypical people reply detourdog 3 hours agorootparentprevI take comfort in seeing my experience described by others. I’m waiting on a neurological evaluation to see where on a spectrum I exist. I have been evaluated for a personality disorder of which none was found. reply wpietri 3 hours agorootparentHey, I'm glad to hear that. Some years back I came across a great online test, one created by autistic people. I can't find it now, but I strongly remember the graph, which showed a bimodal distribution and marked my place on it. [1] In the years following it was such a help, in that I could without judgement see how I and others related. It let me stop worrying so much about trying to be \"right\" and focus on being right for me. As long as I'm mentioning things that helped, I'll recommend this online alexithymia test: https://www.alexithymia.us/test-alexithymia It looks at ability to perceive one's own feelings. Once I realized that I was relatively bad at it, it was such a relief. And in the years since I've gotten a lot better, because I knew I needed to work harder at it than the average person. [1] If that rings a bell for anybody, I'd love to know! reply phkahler 4 hours agoparentprev>> \"Self-worth, what is that?” Because I’m not sure if I have any? This almost made me laugh. I know where you're at. You got a long road ahead so get started! Tell your psychologist when these things pop in your head. Have a laugh, but then reflect on it or whatever. Let me offer some alternatives to \"autistic\": Anhedonia, schizoid personality disorder, avoidant (attachment OR personality disorder). There are many things, but it's not super important to define it, lest you let it define you. reply skissane 54 minutes agorootparent> Let me offer some alternatives to \"autistic\": Anhedonia, schizoid personality disorder, avoidant (attachment OR personality disorder). You are right, except in my case I’m pretty sure it is more autistic than any of those - our children do X/Y/Z and professionals tell us “those are signs of autism” (one child diagnosed, the other not yet formally but the paediatrician is convinced she has it and on the waiting list for an assessment) and (for many but not all of them) I’m thinking “what do you mean that’s a sign of autism? I was like that when I was a kid too, some of them I am even still like that” reply daymanstep 6 hours agoparentprevI think the idea is that \"people\" tend to be attracted towards things that they think will increase their self worth and avoid things that decreases their self worth. Though from a stoic perspective the only thing that can affect your self worth are your own actions, not external events which you have no control over. reply passion__desire 5 hours agorootparentI think stoic ideas are from an era where their circumstances made them have those principles. We don't live in that era. It is possible to affect others and the associated cascading effect that can bring about a change in others action which were affecting you negatively. If Naval's idea of \"individuals having leverage holds water\" directly implies that you can change others, albiet slowly. If your reach becomes big enough that it becomes a threat that \"other actors\" need to curtail that reach through \"algorithms\" is another evidence that you were indeed having effects that they didn't like. reply HPsquared 4 hours agorootparentYou think Marcus Aurelius was unable to affect other peoples' actions? That's not the idea. The point is that you can't directly make someone else think or feel a certain way, only act on them externally. reply passion__desire 3 hours agorootparentIf causality holds, acting externally will have changes to internal assessments assuming good faith dialogue. Plus Marcus Aurelius was helpless in that it would take him lot of time and energy to give personal attention to each individual and clarify their doubts. He didn't have the technology to record his thoughts on a topic and refer people to it. reply chuckadams 3 hours agorootparent> He didn't have the technology to record his thoughts on a topic and refer people to it. He did and we’re still reading them to this day. reply passion__desire 3 hours agorootparentI could be wrong in this. But wasn't his writings only for himself and was published only later. How many people really referred to his writings? When was printing press invented? How popular was it compared to Bible? Was it possible for people to consume his writings in multimedia formats like video, audio? Were there meme-pages on tiktoks which contextualized his writings in different day-to-day scenarios so that the importance of his general ideas were imprinted on their minds? Did he have debate with others to defend his ideas watched by many, how would he respond to those counter-arguments? Would your mind change considering if his responses weren't that strong or on filmsy grounds? reply chuckadams 1 hour agorootparent> Were there meme-pages on tiktoks which contextualized his writings in different day-to-day scenarios so that the importance of his general ideas were imprinted on their minds? I honestly cannot tell whether this is satire or if I just don’t want to be on this planet anymore. reply maroonblazer 4 hours agorootparentprevAnd of course from a Buddhist perspective the self is an illusion, making all this 'self-worth' chasing akin to tilting at windmills. reply detourdog 3 hours agorootparentI don’t think the notion of self-worth is rejected by Buddha. I think the expectation of achieving it through actions is rejected. The expected results are the problem not self-worth or actions. reply svaha1728 3 hours agorootparentprevTrue, but it’s harder to run around with a begging bowl in Western cultures. Even though self worth is an illusion, right livelihood is part of the Noble Eightfold Path. reply AnimalMuppet 5 hours agorootparentprevI think most people use other people as a kind of mirror, to try to see who they are and how they fit. (Autists may do it less than others, or even none at all.) If everybody thinks well of me, then I should probably think well of myself. If everyone thinks badly of me, then I'm probably not worth very much. So goes the logic. So social media is tearing up peoples' self image, not just because of put-downs and deliberate trolling, but mostly because everyone is putting forward the best version of themself that they can, and we compare that, not to the version of ourselves that we put forward, but to the reality of ourselves, and we lose in comparison. And that's the problem with self-worth-by-comparison. There's always someone against whom you lose, in some aspects. Richest man in the world? Yeah, but that other guy has a bigger yacht, and we use yachts as measuring sticks. That's true of all of life, but social media amplifies it. We can see more people faster to compare ourselves to, and they can present a fake image more convincingly. reply hermitcrab 4 hours agorootparentAnd marketers play on this status anxiety. Just look at pretty much any car ad. The best way to innoculate yourself against this (to an extent anyway) is to learn a bit about marketing and do some marketing. Once you have seen how the sausage is made, it has less power. reply detourdog 3 hours agorootparentprevI think social media broke my partner’s self-worth. I’m not on social media beyond HN and found no way to communicate with someone so engaged in remote relationships. reply detourdog 3 hours agoparentprevI spent 56 years with that thought. Looking back for me it was family identity and trying to achieve standards of someone that died a decade before I was born. I now understand self-worth in a new way. I had to realize that there was plenty of time to slow down and be deliberate. I had to get to point where I could take the time I needed to do a task. I have no idea how others can find self worth but for me I describe it as being comfortable. reply elorant 5 hours agoparentprevHow about integrity? Do you find that relevant? Usually people with integrity have high self-esteem because they adhere to a certain set of ethics. I'm not trying in any way to pass judgement, just to give you a different perspective. reply herval 4 hours agorootparentmy understanding of integrity means that you do things that are guided by a shared moral compass - keep your word, avoid cheating, etc. Those signals seem to be all external (you do them because you don't want to violate your contract with another person, etc). I don't think that's in any way related to self-worth (which is a measurement of value to yourself, independent of others)? reply s1artibartfast 3 hours agorootparentIt doesnt have to be a shared moral compass, but simply your own. That is to say, you can practice integrity in isolation from other people. A simple mundane example would being going to the gym if you tell yourself you will. A more complex example would be acting in accordance with the values you believe in or not. If you think people that kick dogs are terrible, but you yourself go around kicking dogs, this creates a lot of cognitive dissonance and low self worth. If you promise yourself to stop, but keep breaking that promise, you realize you cant be trusted, which also impacts self worth. reply datameta 1 hour agorootparentThis definitely touches on one important aspect of difficulty in enduring an addiction one is trying to cease. A promise broken thousands of times, and yet still made again. However if it is the very cycle itself that increases the effort necessary for breaking out of it, how tied to self-worth is it for different people? When one is the subject and the researcher or the judge, defendant, and prosecutor simultaneously - it can be much more challenging to locate the anchor to which self-worth is tied. reply s1artibartfast 1 hour agorootparentHuge topic, but I totally agree. There's a massive feedback between self control and self-worth reply oliv__ 4 hours agorootparentprevIt is definitely related to self-worth, because the contract you make is not with another person, it's with yourself. And when you respect it, it increases your self respect and worth. reply herval 4 hours agorootparentI don't see how that's the case at all. A sense of obligation (how much you value someone else) has nothing to do with self worth (how much you value yourself). If this was the case at all, a great treatment for low self esteem would be to commit to stuff for others, since that'd automatically make you valuate your own self more reply fixedpointsnake 3 hours agorootparent>If this was the case at all, a great treatment for low self esteem would be to commit to stuff for others, since that'd automatically make you valuate your own self more How do you know this is not true? If your sense of obligation is seen as a value function for people, it follows that your self-worth is the value when you plug-in \"self\". Helping others and volunteering is indeed something that brings satisfaction and could help heal your sense of self-worth. If you value another person higher than yourself, by helping them you would establish a connection between their worth and your own. You potentially went from lacking any evidence of positive self-worth to having concrete first-hand evidence that you are worth something to someone. reply herval 3 hours agorootparent> How do you know this is not true? years of therapy :-) the opposite is also demonstrably false - there's people with huge self-esteem who are known for their complete disdain for others or their opinions. reply fixedpointsnake 3 hours agorootparentI can see that. And your counterexample is also pretty apt. I guess universally it may not be true, but I suspect for some it very well could be. Just depends on the value function you ascribe to (knowingly or unknowingly). It should also be said that this topic is more complex than these simple models. I've heard it described that Narcissists essentially refute the evidence rather than allow it to poke a hole in their bubble of self-worth; All of that to say, there are many moving pieces beyond just how you value things that add up to your self-worth. reply graeme 6 hours agoparentprevOne way of looking at it is how you assess yourself in things and take pride in or feel regret about that. For example, you wrote this comment. It is a well written comment. You likely have some belief roughly along the lines of \"I write reasonably well\". You have probably received feedback along those lines throughout life, and that reinforces that belief. This is perhaps mildly pleasing or at least seems correct. Now suppose instead that whenever you wrote things people replied: \"Huh?\" \"What? This makes no sense\" \"Good god what led you to think that? That's so stupid!\" And so on. And you even reread some of your own writings that you thought were good and they seem to strike you as not good. The view that your writing is not good comes to strike you as correct. You aim to improve but continue to receive negative feedback. The view that you are genuinely not good at writing seems to be correct and you come to believe you cannot ever be good at writing. That would be negative self worth. Does this example line up at all with any internal thought processed you have? Are you pleased by praise or hurt by criticism? Even to the level of thinking the judgements are correct or incorrect. (To be clear you are good at writing) reply card_zero 5 hours agorootparentThis begs the question because it starts with \"how you assess yourself\". Why assess yourself, the self, the whole person, at all? So, you're not good at writing. Perhaps you're not good at anything. In that case, you probably shouldn't attempt things except as practice. But why give yourself an overall score as a person, what are you even supposed to do with that information? You can't be anybody else, so it's useless. Work with what you've got, fuck 'em. reply graeme 13 minutes agorootparentI write imprecisely but I wad actually referring to assessing how you are at specific things. How you feel you are at specifics bleeds somehow into a general sense of self esteem. I definitely agree on not tying your sense of self worth to the opinion of others, though we are none os us totally immune. Even diogenes the cynic who scorned society reproached himself when shown he was not meeting his own standard as well as he could. He lived simply and had but a cup to drink from. Then he saw a child drinking from a stream and smashed his cup in frustration at not having thought of the child's simpler approach. Anyway my comment was aiming at illustration what high self worth or lack thereof may feel like, rather than what one should do. reply tgdude 3 hours agorootparentprevMy theory based on nothing but internal reflections A lot of people's minds are raised from a young age to make judgements and comparisons with others. Their minds are told that one must be useful to be valuable, and that simply _being_ isn't enough. Over time those bad habits of the mind are so ingrained and automatic that we assume them to be part of \"me\". \"My\" thoughts, \"my\" ideas and so we don't question their assumptions or where they came from. It takes conscious effort to be able to change those habits into something more positive, or to be able to center your mind to a point where those habits seen as just other thoughts and don't have the same \"weight\" behind them. We're an ever changing process and being able to judge and adjust is a useful skill. It's just that doing that doesn't require all of the crap we put ourselves through due to unchecked assumptions. reply herpdyderp 5 hours agoparentprevI feel this. As to why, my answer is simply that it’s a waste of time to worry about it, so why bother? reply tejohnso 2 hours agoparentprevI was confused by this part of the article: For Marx, the poet of economics, when a person’s innate value is replaced with exchange value, it is as if we’ve been reduced to “a mere jelly.” What's wrong with basing your worth on something like value provided professionally plus value provided personally, to myself and others? Are we supposed to think that we are valuable (worth something) just because we exist? reply pram 2 hours agorootparentIt might be fine if you’re a successful professional or entrepreneur and have a good salary or business. Of course if you’re an Amazon warehouse worker or a burger flipper, and your labor is completely fungible and provides low monetary remuneration, then you would judge yourself as low(er) value in comparison. So determining your self-worth via other properties makes a lot more sense huh? We can create value through more things than what we do for a job. It’s true! reply wslh 6 hours agoparentprevCompletely agree with your point, and it feels like a personal preference/trait. Do you think this tendency is related to an autistic trait because of a focus on facts over social norms? reply add-sub-mul-div 3 hours agoparentprevI'm the same way. I could dispassionately talk about either my strengths or my weaknesses. But I don't see the world as measuring myself in an overall way, nor other people. We all have so many dimensions that are always in so much flux, how could you ever reduce that to something meaningfully quantifiable? reply bbor 4 hours agoparentprevInteresting! If I said you were bad at your job and an ugly, inattentive partner, would your primary reaction be one of hurt or just one of calculating self-preservation? I would personally feel very emotional if I heard those things from someone in real life, so it’s an honest question. What emotionally drives you, if not the assessments of your peers? Why excel at work, why find a partner, why do your best to be better everyday? I wish I could say I was driven by rational assessments of my needs as a Homo Sapiens and my moral responsibilities therein, but I think I’d be lying to myself. Or, at least, it’s an eternal struggle to minimize the importance of self-worth. reply tgdude 3 hours agorootparentNot the person you're replying to but \"What emotionally drives you, if not the assessments of your peers? Why excel at work, why find a partner, why do your best to be better everyday?\" It's fun and it makes me happy. People in my life are smart people but they're just as flawed as I am, what they think of me also changes over time. Why would I build the foundation of my life and career on such shaky ground? reply delusional 6 hours agoparentprevThat sounds like a question for your psychologist, not randoms on the internet. To interact a little more with the substance. I don't think I understand what you're saying. You're clearly using the concept when you write >I don’t mean that in the negative sense that I think I am worthless or anything That's what negative self-worth is. You seem to understand it fine. self-worth is your subjective assessment of how much you are worth. The self's assessment of the \"worth\" (whatever you choose to load into that term) of the self. reply add-sub-mul-div 3 hours agorootparentYou missed the distinction between \"I don't feel I have any worth\" and \"I don't see the world in terms of quantifying worth.\" reply tomrod 6 hours agoprevA few things that inspire me in the Digital age https://en.wikipedia.org/wiki/Citizenship_in_a_Republic https://en.wikipedia.org/wiki/Invictus https://en.wikipedia.org/wiki/If%E2%80%94 reply mikewarot 16 minutes agoprevI think my self-worth was overly tied to technical skills. Now I see that it's more about impedance matching things... explaining how complex things work, using simple words, forcing technology to bend to my will, and other ways I can make reality match what I, and others want. reply nahimn 6 hours agoprevCall me unempathetic, but how would this be any different in a non-digital era? You’ll still have a market (albeit significantly smaller, IE: lets say a small locale or village). But you could have easily said the same thing in a non-digital age, with just more rudimentary metrics and a market that is a community that either values or doesn’t value your work. Much like the proverbial school playground (which could also be analogous to a market). It’s like the author is blaming technology for illustrating the truth in a highly efficient way. She may as well have complained about the printing press being problematic. reply DavidPiper 5 hours agoparent> with just more rudimentary metrics I think this is actually the answer: the word \"just\" is doing a lot more heavy lifting here than first meets the eye. The digital era has brought a lot more quantifiable data, and with that has come a much easier (and in many cases automated) comparison, value-attribution, calculation of the probability of success, etc. The article speaks to this in the second half. Previously this was largely impossible outside of government and other organisations with dedicated statistic collection. The author talks about how even sales numbers for authors were very imprecise and easy to exaggerate or fudge. You could argue that the Digital Era has turned some of \"life\" (or at some of \"art\") into one big Goodhartian[1] parody. That said, it does feel like the author has stopped believing that art has intrinsic value - and that its value might be different for different people, including the author. That is pretty much the mental step you need to take to end up in the parody to begin with. [1] https://en.wikipedia.org/wiki/Goodhart%27s_law reply herval 4 hours agoparentprev> how would this be any different in a non-digital era? You’ll still have a market (albeit significantly smaller you answered it yourself. What's harder, to become the best tennis player of your neighborhood or the best tennis player on the planet? What if you base your self on something that's fringe at a global scale, but acceptable in your local culture? What if all your human interactions are on the internet (with millions of strangers that tend to treat you badly, because people are way more rude online than in real life) vs on your local community (where people treat you better simply to avoid getting punched in the nose, but you might think they like you)? _Everything_ is different online (and that obviously impacts people's psychologies) reply rKarpinski 2 hours agorootparent> What's harder, to become the best tennis player of your neighborhood or the best tennis player on the planet? >> When I was twelve, I used to roller-skate in circles for hours [...] One Saturday, a friend invited me to roller-skate in the park. I can still picture her in green protective knee pads, flying past. I couldn’t catch up, I had no technique. There existed another scale to evaluate roller skating [...] Soon after, I stopped skating. Seems like the author struggled with comparison before the internet, like the grandfather comment said. reply wpietri 3 hours agorootparentprevThis is a great point. In one of Nassim Nicholas Taleb's books, he talks about the emotional impact of looking at one's portfolio performance. If you do it rarely, like quarterly or annually, it'll generally be a positive experience. If you do it day by day, you'll have quite a lot of negative experiences. Because we're wired for loss aversion, we'll weight those negative experiences more highly. The same facts, presented differently, have very different impacts. If I'm doing my own thing, like the author was with roller skating, my basis for comparison is me. There will be ups and downs, but more of the former, because we can't help but learn. But as you say, the bigger group I rank myself against, the more those experiences will be negative. I also think the bigger groups discourage camaraderie, because the declining chance of future interaction means smaller rewards for collaboration and support. reply wpietri 4 hours agoparentprevI think your notion of \"the truth\" is heavily influenced by exactly the context she's pointing at. Google and Meta's algorithmic rankings aren't \"the truth\", and \"the truth\" doesn't change on each regular parameter rebalancing. Those algorithms were built by a relatively small number of people from a narrow set of backgrounds who were focused on maximizing usage and revenue, and the bulk of \"the truth\" they contain is about that. One thing importantly different about our age is context collapse. With communication costs and marginal costs near zero, a global service is easier and cheaper to build than local ones with equivalent coverage. It's as if we've taken every ecosystem in the world and dumped it onto the San Francisco peninsula to fight it out to discover \"the truth\" about what plants and animals are \"best\". What's different about those small locales and villages is that each one of them naturally had its own values. Skills and tastes co-evolved. Schools of thought were born and elaborated. Communication and movement between locales gave useful exchange and inspiration, but generally weren't enough to swamp variation. In the pre-web era, the question of, say, whether Irish music was better than Spanish music would be seen as kind of a dumb question to take any more seriously than for an entertaining argument. But having put everything in Meta's global blender and reduced it to counting updoots, the ever-present metrics purport to provide \"the truth\" to questions like that. They don't, of course. But they do place a much larger burden on us to recognize that not everything that goes up and to the right is an unalloyed good. reply jakubmazanec 6 hours agoparentprev> how would this be any different in a non-digital era I would argue that the scale and the ease that comes with the digitalization and algorithmization is what makes the difference: eg. only literary critics can publish review of your book in a newspaper vs. everyone on the internet can post one. reply billiardsball 6 hours agoparentprevWhile I agree with the main point you're making, I have to note that likening school playgrounds to markets is a questionable analogy. Schools are far closer to prisons than markets - in fact, I can't come up with a single way that school playgrounds are similar to markets other than the fact that they both involve humans. reply herval 4 hours agorootparenthow are schools any close to prisons, other than both involving humans? reply Jordan_Pelt 4 hours agorootparentA chapter in Chemerinsky's casebook on Constitutional law is titled \"Speech in Authoritarian Environments: Military, Prisons, and Schools.\" reply tigen 4 hours agorootparentprevInvoluntary confinenent? reply herval 3 hours agorootparentschools imprison kids now? really? reply harimau777 3 hours agorootparentKids can't just leave schools in the middle of the day, so basically yes. reply herval 2 hours agorootparentYou can’t leave your job in the middle of the day, so basically you’re a felon too? This kind of meme nonsense belongs to reddit, not here reply Paianni 1 hour agorootparentMaybe this varies by location but at least in the UK, adults can usually leave the grounds of their place of work during a lunch break. Not true for school kids. And the difference in population density between schools and offices is usually stark. reply BelleOfTheBall 3 hours agoparentprevTechnology amplifies this, greatly. In a non-digital era our field of view was narrow, expanding either to our immediate physical surroundings or, when we went beyond them, limited by what we could read in a newspaper or see on TV. When I was little, I didn't know who was the most skilled person at my hobby or how popular it was or whether beautiful people online also happened to excel at it, while my teenage hormones wreaked havoc on both my personality and looks. Every single child in the civilized world nowadays is subjected to exactly that. You may be an aspiring dancer and there will be a million like you right there on your phone. It's hard for them to formulate self-worth when that is the case. Does that mean the internet and digital advances are bad? No, it just means we were unprepared for them in a very meaningful way. reply MichaelZuo 3 hours agorootparentHow did the child of a random peasant family 300 years ago 'formulate self-worth'? I don't see how digital technology diminishes that in relative comparison. reply s1artibartfast 2 hours agorootparentA random child peasant might be the best at sewing, strongest, wisest, or most handy in their village of 40. When compared with a pool of 40 million, that doesn't seem like much to be proud of. reply MichaelZuo 1 hour agorootparentBecause...? Of what underlying reasons? reply s1artibartfast 1 hour agorootparentThere is no fundamental force of the universal that makes it so, if that's what you're asking. There is however a tremendous amount of psychology that predisposes people to seeking the admiration and respect of others. If you want a deeper reason, it is likely due to a cultural understanding that these things are actually advantageous, and some amount of deeper evolutionary biology reply tharne 4 hours agoparentprev> She may as well have complained about the printing press being problematic. Technology isn't always fractal, but you seem to be assuming it is. The internet is not just \"a really efficient printing press\", in the same way that New York is City is not \"a little town or village made bigger\". reply AmericanChopper 2 hours agoparentprevIf you can convince yourself that your problems are caused by modern novelties, then you don’t need to address the tricky problem of what’s actually causing them (it’s probably yourself). Any sort of scapegoating like this is going to have a lot of popular appeal. reply AnimalMuppet 5 hours agoparentprevIt's not different, it's just worse. Online there are more people, interacting more shallowly (and therefore judging more superficially and less empathetically), and presenting a more fake version of their own self for you to compare yourself to, and we spend more time wading in it. It's always been going on. But it has changed, not in kind, but in degree, and it does more damage. reply yapyap 4 hours agoprevI agree, it’s also the argument that “nothing will change the big picture anyway” when you don’t go to a business with shady business practices. Or when changing eating habits in general like eating less meat or consuming less dairy. It’s not entirely about changing the industry, a part of it is just integrity. reply fHr 6 hours agoprevI have a career, nice coworkers, have few hobbys and have good friends. The only thing fucking up my selfworth honestly is the dating landscape with all this social media artificial bullshit were nobody wants to commit anymore and everything is fake. It is not worth engaging in it currently and you rather focus on yourself and education/career more it gets you further. If by chance you meet a unicorn take the chance but else just don't bother it only fucks with your selfworth. reply mettamage 4 hours agoparentFunny, I have career issues and dating is easy but that’s only because I was willing to stake my whole life on it and use everything in my power to change it (except changing my looks - I look mediocre). Happily married right now. Feel free to email me to exchange some career advice for dating advice. reply ndarray 2 hours agorootparent> I was willing to stake my whole life on it and use everything in my power to change it You made dating easy by fully revolving your life around it for some period? That sounds very interesting - mind sharing the bullet points with everyone? reply mettamage 1 hour agorootparentSure, very opinionated but here we go: dating, it’s about matching personality. Do the hexaco.org test, figure out how to identify the extreme dimensions on your profile that you identify with (openness in my case). Learn to find those women. Now be prepared to talk to 10000 women by cold approaching or through online dating. It usually takes more like 200 for a GF but you will be doing socially uncomfortable things, so having a hardcore mindset helps. Cold approaching respectfully is a small course on its own, especially since I deviate from standard advice because I test what works for me. TL;DR: approach respectfully (safe and it’s nice) yet playfully or curiously (fun and not boring). Online dating: enhance your pictures with AI as if it is the best picture of you ever taken. Test this with a few throwaway dates by telling them that your pictures were edited. Bonus points if they aren’t deterred by it (it means they can take candor). If they are, well it was a throwaway date anyway. Have throwaway dates in general. These are dates with women you genuinely like but you test certain things. All the things you test should have a screening effect (aka if they like it, it means they match you better), that’s how you make testing things ethical. Also, use an autoswiper and don’t get caught doing it. What helps: meditation, social courage (better than confidence as my confidence is quite low but my courage is crazy high), studying charisma (as it pertains to your personality). That’s a very short tl;dr. I am probably missing things. Feel free to email to chat about it. I desperately need career advice for in the EU. reply antonkar 1 hour agoprevI think self-worth is not a very useful concept. In my mind people with high “self-worth” demand or force others to do things for them - and that’s the definition of anger not “self-worth”. If you are willing to ask others to do things for you and are willing to get a no - that’s the only humane way to behave even thought some may consider it “low self-worth” reply didgetmaster 4 hours agoprevEvery user on HN has a score (up votes vs down votes) based on reactions to comments and submissions. How many of us will not be our authentic selves in a comment because of fear of how others might react? I know that I have done this. reply mikewarot 21 minutes agoparentI tend to wonder what the heck I said that caused an unexpected downvote. I tend to get grumpy when an expected one happens, there are many unwritten rules on HN. I tend to defend against imagined arguments with my statements, and against corner cases far, FAR too much. Getting those imagined arguments wrong doesn't help either. I do notice that things that are inconvenient but true tend to take a hit, then rise back over time, so that's a good thing. reply detourdog 2 hours agoparentprevHow one views karma is funny. I take comfort in my aggregate score due to implied comradely. I noticed down votes happen no matter what is stated. reply d0gsg0w00f 4 hours agoparentprevOf course. I feel more comfortable speaking my mind under a more anonymous account. Even then I'm pretty measured, I just don't want to deal with potential headaches in my professional career. I don't owe the world my true self reply rnd0 3 hours agoparentprevNot really the ideal way to approach HN, although understandable. If you're considering HN from a forum perspective, the best advice I've read is to think of 'points' as currency to spend on unpopular opinions. After a certain point, you can 'afford' to say whatever's on your mind (within reason). Reddit is the same. You rack up 'karma' so that you can afford to take some downvotes for speaking your mind. Now, the other dynamic is that some people here expect to see and work with other HN folks in the real world. THAT is more chilling, and I don't think one could be one's 'authentic self' in that instance unless you're already a 'name' who doesn't need to give a rip about others' opinions. reply jjulius 3 hours agoparentprevI genuinely don't care. I've questioned downvotes I've received before, but usually it's just because I want to know why I might have had a bad take or how I might be looking at things wrong. Respectfully, I know nobody here and you all mean nothing to my life, which is much broader than what I post here, so what do your judgements matter to me? What do these points actually mean that I should choose not to be myself because of them? The answer, to both, is nothing. reply 2grue 2 hours agoprevOne thing worth noting is that the metrics the author mentions (sales, likes etc) are clearly, as everyone would readily admit, not a true measure of value. At best, they're a proxy for what actually matters. And we know from Goodhart's law and reward hacking that optimizing a proxy is, at some point, either useless or actively counterproductive. This thought can be a real source of peace of mind. reply s1artibartfast 1 hour agoparent>And we know from Goodhart's law and reward hacking that optimizing a proxy is, at some point, either useless or actively counterproductive. I don't think that's accurate. It might be useless from the perspective of the system, but hugely advantageous from the perspective of the individual. When a co-worker hacks the metrics and gets the promotion, that might be bad for the company but it is great for them, and perhaps bad for you. The same is true for sales, search engines, and social interactions. I agree with your first point that the proxies are not true reflections, but don't see where this is peace of mind when someone loses out because of them. If anything, I think it would Foster anger that the system is rigged, basically the opposite of Peace of mind reply jakubmazanec 6 hours agoprev> a server tells me her manager won’t give her the Saturday-night money shift until she has more followers Does this really happen? Or maybe just in the USA? Why would I, as a customer, care about server's follower count? Is it somehow correlated with their performance? reply harimau777 4 hours agoparentNot exactly the same, but some friends and I have a tradition of going to a local equivalent to Hooters on Mother's Day since it's the only place that you don't need a reservation. The last time we went the server gave us all an official card with her name on it where we could go to leave feedback for her. reply toomuchtodo 5 hours agoparentprevBusinesses live and die by Google Maps reviews and social now. I am familiar with a Kentucky pizza chain that gave instructions that if each server did not meet their Google review quota, they would be let go or moved to hosting. https://ibb.co/MBcJhZW https://ibb.co/kG1bddG reply cthalupa 5 hours agoparentprevI've never heard of anything remotely like that occurring in the restaurant industry and servers - but there are industries where the reach of someone would be important for hiring or booking them for a gig. In today's world, social media followers is one of those proxies we tend to default to for reach. But a similar thing has been part of the service industry for a long time - attractive and charismatic people will often get the best shifts, even if they're not necessarily the hardest working or best at the other aspects of the job. I suspect if such a thing around the followers is happening now, it's just the a new manifestation of the same underlying cause. reply Stanley02 6 hours agoparentprevPossibly more followers relates to more customers coming by especially for her/him reply 23B1 5 hours agoparentprevIt happens everywhere in marketing, at all levels. Strippers, movie stars, ballet dancers. Anyone whose job it is to reach eyeballs and build a 'brand'. It is nigh on impossible to get a literary manager these days unless you have a sizable social media following, for example. It's absolute brain rot. reply InkCanon 5 hours agorootparentThe main hack these commodification entities makes seems to be to disrupt the traditional flow of information and valuation, then create a new marketplace it controls. For books, I imagine many centuries ago the literary class wrote and read books largely from the social and intellectual forces at the time. The smallest unit of these interactions would be a member reading a book, liking it and recommending it to his friends. The net aggregation of such would slowly produce a trend, communicating to writers that there was a higher chance of being read if you followed it. Now such exchanges have been consumed by digital marketplaces. The sheer size means writers have to make a Faustian bargain to bend to it's needs, while readers have a curated list of books for them. Any well reading writer and reader have an impossible time communicating because the main flow of information has been hijacked. reply zztop44 5 hours agorootparentprevBut that simply follows on from a shift in consumer behaviour. It’s nigh impossible to sell literary prose at any significant volume unless you have a sizeable social media following. The only exceptions are people who have a sizeable traditional media following and a specific personal reason for not being on social media. reply 23B1 2 hours agorootparent\"that simply follows on from a shift in consumer behaviour\" feels hand-washy. chicken/egg of course, but the question people – especially technologists – should be asking is: is that really helping society progress? I would argue, like this article does, that it isn't and is indeed harming society by atomizing and attempting to quantify behavior in the name of 'choice' which of course is a non-sequitur. The commercial engines should be in service of humanity and enrichment, not the other way around. A brilliant writer isn't a social media maven, nor should they be. reply happyboi4life 58 minutes agoprevHe is no scam, I tested him and he delivered a good job, he helped my son upgrade his scores at high school final year which made him graduate successfully and he gave my son free scholarship into the college,all I had to do was to settle the bills for the tools on the job,I used $500 to get a job of $50000 done all thanks to [techcrownhacker @ gmail com] he saved me from all my troubles,sharing this is how I can show gratitude in return for all he has done for me and my family. reply Log_out_ 5 hours agoprevAdd to that narrative collapse where a ton of past narratives just are refuted by reported facts and thus all romantic ideas just self destruct. The weak are not nobled by suffering . The anti imperialists are just wannabe empires. The centre does not hold. reply HPsquared 4 hours agoparentI sometimes feel we are in a similar time to the \"let a hundred flowers bloom\" period in Mao's China, where dissent was tolerated for a while. https://en.wikipedia.org/wiki/Hundred_Flowers_Campaign reply 73kl4453dz 5 hours agoprevI guess i am orthogonal to this article because: (A) i don't visit sites that have \"Followers\" which i can probably afford to do because: (B) unlike writing or waitservice, where one's market is huge numbers of people who each are only willing to pay a little, my skills interest few, but those few are willing to pay a lot. reply ryukoposting 3 hours agoparent> my skills interest few, but those few are willing to pay a lot. We share this privilege. My wife doesn't, though, and she and I have fundamentally different interactions with social media. For her, it's a necessary evil to some extent - the nature of her profession demands that she's tuned into hot trends. That's a visible connection between her profession and her social media relationships. The invisible connections are much harder to identify, and you bring up an interesting point. I can't conceive of a company looking up your instagram account just to see how many followers you have, but maybe that's a thing. I wouldn't know. reply wpietri 4 hours agoparentprevThis is an odd thing to say on a site that depends so heavily on the upvote. reply d0gsg0w00f 4 hours agorootparentIt only depends heavily on the up vote if you depend on the up vote. Sometimes it's nice just to formulate thoughts in writing. If others see it, great. If not, no big deal. reply wpietri 3 hours agorootparentBecause upvotes control visibility, you are still describing a reward function that depends on the upvote. reply s1artibartfast 3 hours agorootparentI think they explicitly said the opposite. That the writing is their reward, and comments are always visible to the author. Visibility to other humans is incidental, or at least a secondary concern. reply wpietri 3 hours agorootparentThey explicitly said, \"If others see it, great. If not, no big deal.\" So I agree they get some reward from the writing. (Like all writers do, and like we can all do just as well with a journal.) But the part I just quoted is clearly part of the reward function, and just as clearly the total reward increases with others seeing it. Which on HN is controlled by upvotes. reply s1artibartfast 3 hours agorootparentThey also said \"Sometimes it's nice just to formulate thoughts in writing\". \"Just\" implies sufficient reward. Visibility is a part of the reward function, but it is not dependent on it. reply wpietri 50 minutes agorootparentYou're using a meaning of \"depends on\" which I'm unfamiliar with. Where are you getting it from? reply InkCanon 5 hours agoprevIMO the least known, yet most powerful, driver to understand the world in the next century. The commodification of everything - including deeply personal and abstract things like attention and love - have been commoditised. Not in the abstract, rhetorical sense that Marx and other such people mean, but in the engineering sense where liquid exchanges with sub millisecond latency are trading such commodities. Everyone is constantly playing a game against a quasi omniscient, unblinking entity, trading everything from fragments of attention to romance. And humans are hopelessly outmatched as the entity has orders of magnitude more information, computational power and vast ability to restructure the market to it's advantage. reply passion__desire 4 hours agoparentI sometimes think \"algorithms\" understand me more than other people. Through my actions, they can diagnose me better than any doctor. e.g. meme therapy pages on facebook and tiktoks. I believe a constant stream of \"best matched tiktoks/reels\" to my situation\" would be equivalent in value to going to a 3-star michelin restaurant and having their best dishes. It is available to everyone. reply llm_trw 4 hours agoparentprev> The bourgeoisie cannot exist without constantly revolutionising the instruments of production, and thereby the relations of production, and with them the whole relations of society. Conservation of the old modes of production in unaltered form, was, on the contrary, the first condition of existence for all earlier industrial classes. Constant revolutionising of production, uninterrupted disturbance of all social conditions, everlasting uncertainty and agitation distinguish the bourgeois epoch from all earlier ones. All fixed, fast-frozen relations, with their train of ancient and venerable prejudices and opinions, are swept away, all new-formed ones become antiquated before they can ossify. All that is solid melts into air, all that is holy is profaned, and man is at last compelled to face with sober senses his real conditions of life, and his relations with his kind. https://www.marxists.org/archive/marx/works/1848/communist-m... >There is no remembrance of former things; neither shall there be any remembrance of things that are to come with those that shall come after. https://www.biblegateway.com/passage/?search=Ecclesiastes%20... reply zugi 1 hour agoprevThis article was a really interesting read! Probably about a 9 out of 10. reply aetherson 2 hours agoprevThis is the same core observation as Scott Alexander's famous article, Meditations on Moloch. https://slatestarcodex.com/2014/07/30/meditations-on-moloch/ reply nemo44x 6 hours agoprevWhat I love about these types of articles is it reminds me how fortunate I am to live in the wealthiest and most comfortable time Earth has ever seen and it’s very likely to get better. These types of existential panics aren’t possible when you’re sustenance farming as nearly every human that has existed was forced by nature to endure until a painful, horrifying death from disease or illness perished them. In order to access our hyper modern comfort you’re only asked to contribute something frivolous like clocking in at a record store or writing something that will entertain some people as they enjoy their morning tea. That we increasingly measure contribution, although terrifying in many ways, is also a more just way to compensate contributions. This article seems to yearn for a more political system that would allow her to indulge her creative passion at her whimsy without accountability to the reality of if anyone wants those outputs. A very self serving system indeed. But again what great fortune to be alive today in a wealthy western political zone. To be able to entertain this fantasy. reply InkCanon 4 hours agoparentDepends on what you mean by good. In Maslow's pyramid, the lowest levels are being optimised for while the highest are suffering greatly. By what are approximate proxies for high level metrics like self fulfilment and satisfaction, things are getting worse (especially in young people). Rates of suicide, depression, mental illness, self harm, addictions (drugs and alcohol), obesity, levels of social interaction, stress etc are all getting worse. Our children will live longer and be more well fed (they're very likely to be overweight), but by every upper metric they are predicted to suffer. The spiritual/philosophical progress of humanity has stagnated. Those sci-fi stories of higher life with both vast technology and purpose - like the Forerunners (Halo) - is a pipe dream. We're not even heading in the direction The Culture - a post scarcity society that's like a rich old lady trying to find charities to work with. We're heading towards (at best) a Brave New World society - pacified, materially sound but vacuous and empty. reply s1artibartfast 3 hours agorootparentIm more positive on the topic, and think what we are seeing is growing pains, and they are not equally distributed. Almost all of our historic writings wrestling with the higher levels of the pyramid came from social elites, and now vast swaths of humanity are able (or forced) to wrestle with these tough questions. Many people seem to be avoidant and running from these tough questions to their own detriment, but culture takes time to adjust. I think your points about levels of self fulfillment and satisfaction are strong evidence that the destination state of Brave New World is unstable equilibrium at best. Most people being miserable from their own existential angst is not long sustainable in an environment where the cognitive tools to overcome it exist and are available. I think culture will adapt because I think there is a lot of selection pressure favoring being well adapted. People will look at those with fulfillment and satisfaction and emulate them. Thats what I try to do. Right now just happens be a tough time for a lot of people to identify that because we are in a period of transition. I think people will have a much better sense of which one of us is correct 5 generations from now. reply carapace 1 hour agorootparentprev> The spiritual/philosophical progress of humanity has stagnated. I can't agree. It doesn't make headlines or garner clicks but progress in spiritual/philosophical progress has accelerated greatly in the last five or six decades. E.g. there's an actual algorithm for spiritual progress that was developed in 1989 from a school of psychology that is based in part on the Chomsky Transformational Grammar.[1] As Gibson said, \"The future is already here — it’s just not very evenly distributed.\" > Depends on what you mean by good. This is the essential question facing humanity. Wendel Berry phrases it as \"What are people for?\" in his essay of that name. It's a question that increases in difficulty as the intelligence of the system increases, faster, so AI won't help answer it no matter how powerful and GAI (no matter how powerful) will be in the same boat, ergo Douglas Adams was right and the Earth is a computer built to calculate the Question of Life, the Universe, and Everything. (Isn't that nice?) Interestingly, the aforementioned algorithm is based on evoking a chain of motivations from some initial \"presenting problem\" to deep and profound spiritual states, in effect answering the question to the best of the ability of the person undergoing it. Taking a step back, all human intentions form a DAG (Direct Acyclical Graph) with the upper reaches (so to speak) being perfectly compatible and harmonious. Most of the intervening motives are Yak Shaving. Another way of saying that is that most of us are wildly ridiculously wasteful. The obvious implication is that we can just \"stop doing that\" and eliminate all this wasteful activity and in effect get a huge wallop of resources and energy back \"for free\" (not to mention that high spiritual states feel really good.) All this to say that the transistor and our digital networks are, strictly speaking, a kind of side show. The actual \"information revolution\" is learning to reprogram our minds to get over our BS and live happily ever after. [1] core transformation process https://www.coretransformation.org/ reply jacamera 5 hours agoparentprevI agree generally, but I feel like we're slowly coming to realize that maximal leisure and safety might not necessarily be the recipe for a happy and fulfilling life. reply InkCanon 4 hours agorootparentBy certain angles it is even less important than spiritual, community and philosophical purpose. It's not something that you can measure directly but by many possible proxies (Amish levels of depression, the Roseto effect, various studies on happiness of underdeveloped countries), people need very few material things to be happy. Perhaps it is a kind of mass delusion or confirmation bias that happiness must correlate linearly, or even logarithmically, with disposable income. reply stonethrowaway 3 hours agoprevI haven’t read the article yet but I’m hoping to read one that attempts to elucidate a possible connection between the plummeting of self-worth of women (and the whiplash effect of their behaviour thereafter) and their exposure to social media that shows them every day, and I’ll put this in a shrewd way, the lives of other women that they will never have. A non-stop daily mental torture session for a good deal of the human race. I want to read such an article because I want someone, and someone with writing flair at that, to go for the jugular of the sick and twisted human nature that we pretend does not exist. reply bamboozled 5 hours agoprev“You’re just an LLM”… reply nbzso 2 hours agoprevJust a hint for the brave ones: The collapse of Maslow's hierarchy of needs is the goal. A global regime aimed at citizens obedience, control over movement, food, and resources is planned and in motion. There is no conspiracy or delusion in this. Seeing this requires just a drop of critical thinking. If your perception of yourself is firmly attached to the collective projections imposed by governments, ideology, religion, or any form of group thinking, you are cooked. In times like this, one must find the connection with nature and take action. Get out from the cities. Search places with enough water and option of growing your food locally. Be prepared for this activity to be outlawed under the Global Climate Emergency act, which the UN is ready to activate under the command of elite families and their servants in governments, military, and science institutions. In essence, when your money is truly worth nothing and your life is driven by debt, you are a slave for life. reply weard_beard 6 hours agoprev [–] 10$ says AI was used to partially write this. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author reflects on the contrast between intrinsic and market value, highlighting how external validation can overshadow personal worth.",
      "The relentless datafication and gamification of life have made it challenging to find intrinsic value, pushing artists to monetize their private experiences.",
      "Despite external pressures, the advice to keep creating is emphasized as a way to manage and reinflate one's inner sense of worth."
    ],
    "commentSummary": [
      "The article explores how self-worth is increasingly tied to online metrics such as followers and likes, leading to a decline in personal value.",
      "It discusses the impact of digital platforms on social comparisons and self-esteem, including perspectives from autistic individuals who may not naturally link self and worth.",
      "The piece examines the broader implications of commodifying personal attributes and the psychological effects of seeking constant online validation."
    ],
    "points": 175,
    "commentCount": 145,
    "retryCount": 0,
    "time": 1726915715
  },
  {
    "id": 41605449,
    "title": "Qualcomm Wants to Buy Intel",
    "originLink": "https://www.theverge.com/2024/9/20/24249949/intel-qualcomm-rumor-takeover-acquisition-arm-x86",
    "originBody": "Intel/ Tech/ Business Qualcomm wants to buy Intel Qualcomm wants to buy Intel / The Wall Street Journal reports Qualcomm approached Intel about a possible takeover — is this how Arm vs. x86 ends? By Richard Lawler and Sean Hollister Sep 20, 2024, 8:13 PM UTC Share this story Illustration by Alex Castro / The Verge On Friday afternoon, The Wall Street Journal reported Intel had been approached by fellow chip giant Qualcomm about a possible takeover. While any deal is described as “far from certain,” according to the paper’s unnamed sources, it would represent a tremendous fall for a company that had been the most valuable chip company in the world, based largely on its x86 processor technology that for years had triumphed over Qualcomm’s Arm chips outside of the phone space. The New York Times corroborated the report on Friday evening, adding that “Qualcomm has not yet made an official offer for Intel.” If a deal were made — and survived regulatory scrutiny — it would be a massive coup for Qualcomm, which reentered the desktop processor market this year as a part of Microsoft’s AI PC strategy after years of dominance in mobile processors. Related Windows on Arm finally has legs Intel is laying off over 15,000 employees and will stop ‘non-essential work’ Intel’s big turnaround plan includes spinning off its chipmaking business Intel, meanwhile, is arguably in its weakest position in years — while many of its businesses are still profitable, the company announced substantial cuts, shifts in strategy, and a 15-plus percent downsizing of its workforce this August after reporting a $1.6 billion loss. At the time, Intel CEO Pat Gelsinger said the company would stop all nonessential work and has since announced it will spin off its chipmaking business, a part of the company that it had long touted as a strength over rival AMD and the many fabless chipmakers that rely on entities like Taiwan’s TSMC to produce all of their actual silicon. Intel, too, recently had to partially rely on TSMC to produce its most cutting-edge chips as it continues to rebuild its own manufacturing efforts (the costs of which are responsible for most of Intel’s recent losses). And its own 18A manufacturing process reportedly ran into some recent trouble. Related Here’s how Qualcomm’s new laptop chips really stack up to Apple, Intel, and AMD While Intel’s chief rival, AMD, also had hard times over the years and had to claw its way back, gamers helped AMD every step of the way. Aside from the Nintendo Switch, whose processors are made by Nvidia, every major game console for the last decade has featured an AMD chip — and Intel reportedly lost out on a chance to change that with the future PlayStation 6. Intel also recently lost some faith with PC gamers after two generations of its flagship chips were found vulnerable to strange crashes, though Intel has since agreed to extend the warranties by multiple years and issued updates that could prevent damage. Many of Intel’s woes are about silicon leadership, not just manufacturing or profits — the company isn’t a big player in AI server chips yet as Nvidia dominates, nor even necessarily a notable small one like AMD. Even its attempts to produce its own GPUs for gamers and creators have yet to impress. And while Qualcomm, AMD, and Apple are all still smaller players in laptops, Intel has now twice overhauled how it makes flagship laptop chips to combat the growing threat of their seeming battery life and integrated graphics advantages. We’re waiting to see if its new Lunar Lake chips succeed in October and beyond. Update, September 20th: Added corroboration by the NYT. Most Popular Most Popular Qualcomm wants to buy Intel Microsoft launches a Windows app for iPhones, Macs, and Android devices Cards Against Humanity is suing SpaceX for trespassing on its ‘pristine’ property The iPhone camera is more confusing than ever Up close with Sony’s PS5 Pro — and the 30th Anniversary model Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=41605449",
    "commentBody": "Qualcomm Wants to Buy Intel (theverge.com)175 points by oco101 22 hours agohidepastfavorite97 comments GeekyBear 20 hours agoAfter a decade of being assured that the short support window for Android devices had a root cause in the lack of support device makers received from Qualcomm, they are the last company that I want to see buy Intel. reply bfrog 20 hours agoparentMeanwhile x86 has maintained backwards compat for (checks calendar) yeah decades. Literally decades. With standards up the wazoo to avoid the disaster that is the Arm ecosystem without UEFI or something like it. Every Arm SoC being a snowflake needing special attention by the OS is a huge hassle. There's a reason there's no simplified Arm installer for operating systems. reply dan-robertson 20 hours agorootparentHistorically this backwards compatibility was a competitive moat for intel – having a large supply of weird instructions with some undocumented behaviour thrown in makes it more expensive to make competing chips. reply loeg 20 hours agorootparentThe ISA isn't what makes x86 easier for operating systems to support than ARM SoCs. It's things like generic ACPICA instead of hardcoded devicetrees. reply dan-robertson 19 hours agorootparentTo be clear, I wasn’t claiming anything about operating systems. Merely that adding many weird instructions was a strategy intel used to try to make the jobs of amd and centaur and suchlike harder. reply gjsman-1000 19 hours agorootparentprevSome ARM systems (mainly servers) do support ACPI; allowing for one image to run on multiple processors and devices. However… ACPI is apparently a pretty awful thing to implement. When it doesn’t work, or mistakes are made (looking at my own 13th gen HP laptop right now - borked ACPI tables means unpatchable broken sleep on Linux), then it’s pure frustration. Device trees on the other hand are much more binary. Either everything generally works or it doesn’t at all. It’s a valid approach. reply 10000truths 19 hours agorootparentFlawed implementations of open specs can be worked around with things like quirk tables. A spec held hostage by a non-cooperating vendor cannot. In the world of ARM SoCs, bad vendors won't even provide a device tree, just a binary image compiled from a patched kernel. reply AlotOfReading 19 hours agorootparentprevYou can override broken ACPI tables. The keyword to search for is \"dsdt\". For example, if you have an HP Omen this repo has a fix: https://github.com/j0hnwang/OMEN-Transcend-16-ACPI-fix But yes, device trees are far nicer to work with. reply rldjbpin 10 hours agorootparentprevnobody is a saint on this matter. both parties act the way they do to support their business position. short-term support helps qualcomm sell more because their customers want to sell more phones to the same users. on the other hand, for the industrial partners there is an \"LTS\" model of support given (side effect being fairphone 5 https://news.ycombinator.com/item?id=37320800). long-term support through backwards compatibility helps intel because they can sell new chips with the assurance that the ancient, unmaintained industrial software continues to run in a shinier box. reply knowitnone 18 hours agorootparentprevthere's no simplified Linux installer either and every distro creates one and keeps re-inventing it reply ASalazarMX 20 hours agoparentprevIntel squandered its dominance on the CPU market for decades. Qalcomm sucking the remaining life of it would be a fitting end for a player that lost its way. Wonder if the increasing backwards compatibility became too much to bear, but IMO it never really tried to tread new grounds for risk of losing a comfortable position. reply Varloom 19 hours agorootparentQualcomm main business is mobile phone chips and 5g modems. Majority of it's revenue goes to Taiwan for TSMC as margins. Having Intel fab, will cut the middle man, and revenue will skyrocket, all while no money leaves the USA. reply geerlingguy 21 hours agoprevQualcomm still hasn't shipped any of the Snapdragon X dev kits, two months and counting. If they can't deliver on their promises (that and CoPilot PCs having very disappointing sales), how could they do anything besides further drag down Intel? Not only that, it sounds like a major customer (Apple) is close to finally ditching Qualcomm's wireless chips? (At least that's been rumored [1]) [1] https://appleinsider.com/articles/24/07/24/apple-has-reporte... reply kev009 21 hours agoparentQualcomm's RF design is best in class. This is their bread and butter and they have been consistently good at it forever. Apple purchased intel's RF baseband division,which was awful, and has been working on it in secret for years. It remains to be seen how this will go for Apple. It is attractive to Apple for cost and efficiency reasons (theoretically they can bury this all on a single SoC if they wish to) not because Qualcomm is bad. It bears in mind that just because you are good at one thing does not imply you will be good at another. For instance, Intel's networking is mediocre to bad depending on the product or various entities trying to produce MIPS and ARM products failing time and time again. reply static_motion 2 hours agorootparent> Intel's networking is mediocre to bad depending on the product That's an interesting statement, as their Wi-Fi cards are some of the best on the market and common laptop-purchasing wisdom says to buy anything with an Intel Wi-Fi adapter and avoid everything else. reply MichaelNolan 18 hours agorootparentprev> Apple … has been working on it in secret for years. I keep hoping Apple will release a MacBook with a 5G chipset. The rumors are saying their in house one will ready in 2026 at the earliest. It sure seems like a long road given they bought the intel RF division in 2019. reply snitty 18 hours agorootparentMy understanding is that Intel's chips weren't great and making power efficient 5g chips is wildly difficult. Thus ends my understanding of these issues, though. reply ikekkdcjkfke 13 hours agorootparentAnd Qualcomm patents. I just don't see how you can patent anything related to complying with a radio spec, there has to be limited ways to comply reply wmf 21 hours agoparentprevCompanies should do what they say they're going to do, but these dev kits are an example of something that's relevant to HN but not to Qualcomm's business. reply cowmix 20 hours agorootparentA large part of the success of this new platform is how fast devs can adapt / fix their apps to work natively. Apple, for instance, provided dev mules for OSX ARM --- and their rollout of Apple Silicon was smoother than anyone could have hoped. Windows ARM -- still borked in SOOO many ways -- and its 10+ years old now. reply com2kid 20 hours agorootparent> Windows ARM -- still borked in SOOO many ways -- and its 10+ years old now. Internally, nearly 20 years. It was kept alive for a long time by a single individual as a side project. When I first got out of college I actually helped update tests that were being used for it (I maintained the ARM compiler test harness, and it was being used for some Windows on ARM stuff as well). Microsoft has never went fully in on arm, whereas Apple was willing to burn bridges and start brand new. reply tonyedgecombe 5 hours agorootparentTo be fair to Microsoft Apple did have the advantage of controlling the stack from top to bottom. reply freehorse 21 hours agorootparentprevHow can you expect your products to have good software support by developers then? reply wmf 20 hours agorootparentSnapdragon laptops have been available for a few weeks already. Although laptops cost more than this dev kit they're also more usable as a daily driver. If Qualcomm wants real adoption they'd send them out for free, not require devs to pay. reply geerlingguy 5 hours agorootparentDev shops would rather not have piles of laptops with batteries to maintain sitting in their racks/shelves for build and remote testing. For individual devs laptops are fine, usually, but there's also no solid \"reference\" platform, since all the laptops are targeting different consumer lines. That's a bit beside the point though, the Dev kits should've come out months before the consumer products were launched... and failing that at the same time. Qualcomm and Arrow said the units would ship \"tomorrow\" in July... and it took over a month (after accepting many orders) before they even updated the stock to a more realistic timeframe, late September. reply cowmix 21 hours agoparentprevWhen this platform was heralded as the “AI” desktop, I pre-ordered both the dev kit and a laptop. Like many of you, I’ve experienced a months-long delay in the delivery of the dev kits. Although I STILL don't have my devkit, I received my laptop pretty much on time. -- and I quickly discovered that despite Windows on ARM (WOA) being over a decade old, the support for open-source tooling is as complete as Swiss cheese. Key Python modules are missing, and even the Git command-line (git bash) client isn’t functional yet! I mean, forget about basic open-source development, let alone performing AI inference work on your new Snapdragon laptop. After some digging, I’ve learned that just four overworked developers in Prague make up the core team unclogging this tooling dependency log-jam. Gah! For what it’s worth, WSL2 (Linux on Windows) actually runs quite impressively on the Snapdragon X. reply nine_k 20 hours agorootparentWhy run Windows on ARM, when Linux on ARM is so.much more mature? Or are you buildings a Windows-specific product? reply cowmix 19 hours agorootparentMy plan was to run the laptop Windows and the 'devkit' Linux. This first batch of laptops (AFAICT) can NOT dual boot. My thinking is this, if Windows ARM is a success -- there will be more units out there that can ALSO run Linux too. If Windows ARM is a failure, then Linux will suffer too. reply andrewmcwatters 19 hours agorootparentprev> After some digging, I’ve learned that just four overworked developers in Prague make up the core team unclogging this tooling dependency log-jam. Gah! What an embarrassment. So basically, it’s not a serious product. reply pas 19 hours agoparentprev... Intel wasn't able to ship on EUV, so they would be in great company. reply kev009 21 hours agoprevThe headline seems intentionally bombastic and false. The text specifies that they are interested in lines of business, for example consumer computing, not the entire entity. reply MattGrommes 21 hours agoprevQualcomm consists of at least 4 lawyers for every engineer. If this happens expect a lot more lawsuits making anything involving hardware much more expensive for everybody. reply jjtheblunt 18 hours agoparentNot plausible. I worked at Qualcomm for several years in engineering and office of the chief scientist, and that would be insane inversion of division headcounts. reply Narhem 21 hours agoparentprevQualcomm makes lots of their money by holding a monopoly on wireless chip patents. They use lawyers to bully other companies out of the space. You can compare this with the patent wars of the companies in Silicon Valley which came to halt when the orgs realized they were effectively giving money to lawyers instead of innovating. Qualcomm doesn’t really have real competition in Southern California. It’s cheaper for them to bully smaller companies with lawyers than employ more engineers (not sure if it’s possible to employ more engineers in the wireless space regardless). You could also argue Qualcomms success is related to the other companies which reside around them. They have effectively built an “office moat” with their wireless patents. reply slt2021 20 hours agorootparentso Qualcomm is basically Oracle? reply Narhem 15 hours agorootparentSort of, in Northern California there was a lot of office space. In San Diego most of the land zoned for office space is owned by the Jacob brothers (although not directly connected to Qualcomm anymore). Imagine if Google and Apple had to rent their campuses from Oracle. reply refulgentis 20 hours agoparentprevNo way, 4x?! reply 015a 21 hours agoprevWasn't there some clause in the Intel/AMD x86-64 cross-licensing deal which voids it if either company changes ownership? I have some recollection of that being a thing. But to be fair, Qualcomm might not care. reply electronbeam 20 hours agoparentDeals can be amended for $$ The idea is to keep the number of x86 suppliers low, but enough patents are expired already you could probably make an x86_64 avx2 era cpu without asking reply Woodi 7 hours agoprevHow are MS and Qualcomm finances related ? They look like good budies with no visible reason. And if they are close then MS can have their own cpus. Win4ever !!!11 :) But it's possible plain silicon is outdated and Intel stuff was splitted for a reason, eg. \"photonics\" part goes to \"datacenter\" division, fabs can be spun off and re-named any second. But it's USoA ! - if you win military contract you live OK for few years and do not sell out suddenly like Sun - they did it just instantly after loosing military contract. Or maybe they (Sun managers) was preparing it 2 years ahead when dey bought Mysql :) Just theoretising :) reply ChrisArchitect 21 hours agoprev[dupe] Some more discussion: https://news.ycombinator.com/item?id=41604817 reply rrrrrrrrrrrryan 13 hours agoprevStrange timing - Qualcomm just announced hundreds of layoffs literally yesterday: https://techcrunch.com/2024/09/19/chipmaker-qualcomm-lays-of... reply dzonga 20 hours agoprevselling intel to Qualcomm, would the equivalent of selling yourself to a vampire instead of selling blood. the reason Apple ended up making their own wireless chips is due to Qualcomm reply fidotron 20 hours agoprevI wonder if Intel still have a suitable Arm architecture license that would be transferable? It seems unlikely without Arm approval, but a bundle could offset some of the cost Qualcomm might be thinking about, as that lawsuit might be getting too expensive, even for them. For context: https://www.reuters.com/technology/arm-qualcomm-legal-battle... reply onepointsixC 20 hours agoprevA sale isn’t going to happen. Intel has had a rough quarter but their lunar lake launch looks promising, beating Qualcomm’s offerings on battery life and performance. reply snitty 18 hours agoparentThe issue long term is that Lunar Lake is built on TSMC, so Intel is netting a fraction per chip of what they'd make if they made it themselves. Intel is currently investing $7B a quarter into getting their foundries competitive again, and it's not clear yet that they'll be able to really do so at scale. And even if they do, it's not clear whether those foundries can effectively serve customers that aren't Intel. The reason people trust TSMC to make their chips is because TSMC isn't making a competing chip. If I come to an Intel Foundry with my design and work with them to spin up some new capability to get the features to work right, there isn't much of anything that stops Intel Chips from using that new capability to compete with me in a year. reply gunalx 21 hours agoprevI really don't see it. Maybe a merger of some sort but still. Has Qualcomm anything to gain from taking intel, and likewise intel from being merged in? reply lagadu 20 hours agoparentThey might just split and sell off the cpu and gpu divisions and keep the rest of Intel's (very wide) portfolio. Of particular interest might be the Intel foundries. reply LarsDu88 21 hours agoprevThis would not be good for either party. And Qualcomm would only do this to kill x86 prematurely which I'm sure AMD will pick up on. reply icar 9 hours agoprevThis is potentially bad news for open source kernels. They do lots of work there. reply rldjbpin 10 hours agoprevas outlandish as it sounds, there might be a better chance of this happening than the arm deal. the only reason being that both companies are registered in the same country, where the other parties would have a harder time blocking things. reply knowitnone 18 hours agoprevI want Apple to buy Intel so they can own their fabs. Instead of a billion going to TSMC, spend the billion to fix the fabs and the next several billion is profit. reply incognition 18 hours agoparentUS gov already put 20Bn into Intels fabs. reply nadist 15 hours agoprevWhy Qualcomm’s Approach to Intel for Taking Over Semiconductor Industry Answer: Read this post: https://speadinfo.com/qualcomms-approach-to-intel-implicatio... reply tippytippytango 20 hours agoprevApple, c’mon. Get in there and just buy intel. Get the foundry working, spin out x86 into its own legacy, fabless business unit. Make your own chips, it’s only 100B!! reply osnium123 15 hours agoparentIt’s very hard to get semiconductor factories to work. In almost all cases, it’s better to let the professionals at TSMC to do the fab than to rely on newbies to the foundry industry. reply monocasa 12 hours agoparentprevThey already made the investment playing king maker for TSMC; I doubt they want to start that over for no real reason. reply WheelsAtLarge 21 hours agoprevIntel has focused on manufacturing efficiency for years now. Their innovation abilities have been lacking. A combination of Qualcomm and Intel will be a powerhouse. Intel as an entity will disappear but Qualcomm will be the stronger for it. I doubt Intel will go for it but I hope it happens. reply qwytw 21 hours agoparent> Their innovation abilities have been lacking And Qualcomm's haven't? What did they really design besides the Snapdragon X Elite in the last 10+ years? > but I hope it happens So more industry concentration and even less competition would somehow be a good thing? reply mewse-hn 21 hours agorootparent> What did they really design besides the Snapdragon X Elite in the last 10+ years? The Snapdragon 8 series have been the flagship SoCs for non-Apple phones for years, that's why people had high hopes for the Snapdragon X on PCs reply silisili 20 hours agorootparentSnapdragons have mostly been reference ARM designs with Adreno bolted on being the big selling point, since it blows away Mali or whatever the reference design is today. They got that from AMD of all places, hence the name being an anagram of Radeon. reply qwytw 21 hours agorootparentprev> The Snapdragon 8 series They \"just\" are just using standard ARM cores which is hardly comparable to what Intel and Apple are doing (and they have been lagging behind Apple by a few years since forever; arguably Intel is closer to the M series than Qualcomm is to A series). > Snapdragon X I thought it's because they finally designed their own core, since ARM has been completely ignoring the laptop market and couldn't really offer anything? reply packetlost 21 hours agorootparentprev> What did they really design besides the Snapdragon X Elite in the last 10+ years? Lots of 5G modems reply Wytwwww 20 hours agorootparentIt's pretty easy to design the best 5G modems when you're are almost (effectively) the only company that's legally allowed to make them. They are basically a monopoly... Intel would be doing much better as well if they didn't have to share x86 with AMD and could sue ARM into oblivion... reply jprd 20 hours agorootparentI imagine Apple would disagree. reply Wytwwww 5 hours agorootparentWe'll never really know though, will we? Or are you claiming that the endless patent related lawsuits (going both ways) had no impact on Intel's/Apple's efforts? reply ginko 20 hours agoprevQualcomm should be broken up. reply Narhem 21 hours agoprevIs there a patent or something Qualcomm is trying to get from Intel? Seems like an odd acquisition otherwise. As far as chip manufacturing they target different markets, I’d bet most IP isn’t transferable between the orgs anyway, especially since Apple bought Intels modem patents already. reply klyrs 21 hours agoparentA... single patent? No, there are licensing agreements for that. reply osnium123 21 hours agoprevChina will block this deal from ever happening. reply ectospheno 21 hours agoparentHow exactly would a foreign power block two US companies from merging when it has no significant stock holding in either? reply mlyle 21 hours agorootparentSame way Europe exercises outsized antitrust influence in the US: threatening to fine US entities, and if necessary deny them access to the European market. It would be tricky, though, because my sense is that China still needs Intel/Qualcomm more than they need China. At the same time, it would be pretty deadly to be denied access to that market and your products subject to excess tariffs if imported by others. reply shortsunblack 21 hours agorootparentTo the poster, Europe does not exercise outsized antitrust influence in the US. Many of these companies have their tax residency in the EU. There is no need to \"deny\" anything. If the company gets fined and it refuses to pay the fine, EU seizes the money in one of many bank accounts in Europe. reply mlyle 21 hours agorootparentEurope absolutely exercises significant antitrust influence upon US firms. In practice, yes, as you point out: US firms must have assets in Europe to compete effectively. But even if they didn't, Europe could deny access to the European market. So there is no reason to try and minimize surface in Europe. e.g. Apple has to comply with European antitrust rulings about app store access, even if Apple were to just sell their product to third party distributors in Europe and not have any presence in Europe. reply overstay8930 20 hours agorootparent> Europe could deny access to the European market Not really, Apple would just get customers the same way they’re sold in any other part of the world that doesn’t officially have iPhones (i.e. Russia), the EU doesn’t have the authority to seize shipments purely based on a violation of the DMA. reply onepointsixC 21 hours agorootparentprevSame way China blocked Intel from acquiring Tower Semiconductor. reply wahern 20 hours agorootparentTIL: https://www.reuters.com/technology/intel-walk-away-54-bln-ac... reply osnium123 21 hours agorootparentprevChinese regulators have a say because both companies do business in China. reply sidkshatriya 21 hours agorootparentprevMajor markets like EU and China often do have a major influence on mergers even though the companies merging could be based in the US and most shareholding could be US. I think it goes like this: The major market regulator could say directly/indirectly: Hey you can merge in the US ... but good luck operating in our geography in a frictionless manner if we are against your merger. As a regulator we can make life hell for you if you don't obey our anticompetitive laws. Since you derive a high percentage of your revenue/profits you must listen to us ! It all depends on the percentage of sales in the foreign geography. With EU/China it can be quite high -- especially for tech companies. So yes, foreign powers can and often do block companies from merging. reply ASalazarMX 20 hours agorootparentAnd I think it ends like this: CEO to board: We want to acquire $LESSER_COMPANY, but China opposes. We can go through, but we'll lose easy access to that market, and in net terms our valuation will drop. Board: Forget it. reply ivewonyoung 21 hours agorootparentprevLike how UK blocked Microsoft's purchase of Blizzard. reply electronbeam 21 hours agoparentprevWhat would motivate this reply Arn_Thor 21 hours agorootparentWhat’s bad for US semiconductor manufacturing (I.e. a poor and cash starved Intel) buys China more time to catch up to frontier fab tech. If Qualcomm buys Intel, the optimistic scenario (for the US) is a stronger domestic player. reply qwytw 21 hours agorootparent> stronger domestic player Is it though? Qualcomm is more likely to just strip Intel for parts than to turn it around and we'll just end up with more market concentration and less competition. reply Arn_Thor 13 hours agorootparentI did say “the optimistic scenario”, not “the only/likely scenario” reply electronbeam 20 hours agorootparentprevIf that were true they’d denial-of-service every possible merger reply kube-system 20 hours agorootparentGeopolitically you can't ixnay everything, you've got to pick your battles. reply osnium123 21 hours agorootparentprevYes. This would also give more volume for the existing Intel factories. reply pwython 21 hours agorootparentprev\"So, why is Qualcomm under so much pressure? China is one significant reason. China is one of their critical worldwide markets and both the US and China governments seem to be increasing pressure on various US companies, including Apple, Qualcomm and Google.\" https://www.rcrwireless.com/20230911/uncategorized/kagan-how... reply quantum_state 21 hours agorootparentprevFrom some perspective, governments of the big markets are like gangsters… reply jovial_cavalier 21 hours agoprev>Is this how arm vs x86 ends? No... Intel isn't the only one that makes x86 processors and Qualcomm isn't the only one that makes arm. Separately, Intel should sell them the flagging chips business and keep the fledgling foundries business separate. reply qwytw 21 hours agoparent> flagging chips business Is it \"flagging\", though? Intel still seems to be pretty good at designing chips and their next gen laptop chips (made at TSMC) are allegedly more power efficient than the Snapdragon Elite (of course remains to be seen). It's the foundry that's dragging down. reply zeusk 20 hours agoparentprevIf you look at the financial statements, it's quite the opposite however? Their chips made on TSMC process are doing quite well and IFS has failed to secure worthwhile external customers and is losing money in their expansion hand over first. reply onepointsixC 20 hours agorootparentIFS has just announced Amazon as a customer with a design on 18A. Microsoft is also expected to tape out one design. They’re not going to challenge TSMC this decade, but becoming the #2 fab 2030 is achievable. reply jovial_cavalier 12 hours agorootparentprevThe federal government is clearly ok with supporting a TSMC transition to the states. Something tells me though that they are willing to throw a lot of money at Intel if Intel is willing to fill the same niche that TSMC does currently. That the chips currently produce more return than foundries is expected - it’s an established business. The foundries require much more up front investment. However the chips side of business has recently begun to show some cracks. The foundries side of the business is in a different phase of life. It currently needs some TLC but has the potential to be totally ascendant at some point in the future. Assuming snapdragon is more interested in a chips business than a foundries business… it would just make sense to split them. There is tension with both under one roof as it is. reply xmly 20 hours agoprevgood joke reply melling 21 hours agoprev [–] Yeah, Intel is really cheap. It would be a steal. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Qualcomm is considering acquiring Intel, as reported by The Wall Street Journal, though no official offer has been made and the deal is uncertain.",
      "If successful, this acquisition would significantly impact the chip industry, with Qualcomm reentering the desktop processor market, aligning with Microsoft's AI PC strategy.",
      "Intel is currently in a weakened state, facing substantial financial losses, layoffs, and competition from AMD and Nvidia, with regulatory scrutiny expected to be a major challenge for the deal."
    ],
    "commentSummary": [
      "Qualcomm is reportedly interested in acquiring parts of Intel, not the entire company, which has sparked significant discussion in the tech community.",
      "The potential acquisition is seen as a strategic move for Qualcomm to leverage Intel's manufacturing capabilities and reduce reliance on TSMC (Taiwan Semiconductor Manufacturing Company).",
      "Concerns have been raised about the compatibility and integration of Qualcomm's ARM-based technology with Intel's x86 architecture, as well as the potential impact on open-source development and market competition."
    ],
    "points": 175,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1726865844
  },
  {
    "id": 41608350,
    "title": "Kamal Proxy – A minimal HTTP proxy for zero-downtime deployments",
    "originLink": "https://github.com/basecamp/kamal-proxy",
    "originBody": "Kamal Proxy - A minimal HTTP proxy for zero-downtime deployments What it does Kamal Proxy is a tiny HTTP proxy, designed to make it easy to coordinate zero-downtime deployments. By running your web applications behind Kamal Proxy, you can deploy changes to them without interruping any of the traffic that's in progress. No particular cooperation from an application is required for this to work. Kamal Proxy is designed to work as part of Kamal, which provides a complete deployment experience including container packaging and provisioning. However, Kamal Proxy could also be used standalone or as part of other deployment tooling. A quick overview To run an instance of the proxy, use the kamal-proxy run command. There's no configuration file, but there are some options you can specify if the defaults aren't right for your application. For example, to run the proxy on a port other than 80 (the default) you could: kamal-proxy run --http-port 8080 Run kamal-proxy help run to see the full list of options. To route traffic through the proxy to a web application, you deploy instances of the application to the proxy. Deploying an instance makes it available to the proxy, and replaces the instance it was using before (if any). Use the format hostname:port when specifying the instance to deploy. For example: kamal-proxy deploy service1 --target web-1:3000 This will instruct the proxy to register web-1:3000 to receive traffic under the service name service1. It will immediately begin running HTTP health checks to ensure it's reachable and working and, as soon as those health checks succeed, will start routing traffic to it. If the instance fails to become healthy within a reasonable time, the deploy command will stop the deployment and return a non-zero exit code, allowing deployment scripts to handle the failure appropriately. Each deployment takes over all the traffic from the previously deployed instance. As soon as Kamal Proxy determines that the new instance is healthy, it will route all new traffic to that instance. The deploy command also waits for traffic to drain from the old instance before returning. This means it's safe to remove the old instance as soon as deploy returns successfully, without interrupting any in-flight requests. Because traffic is only routed to a new instance once it's healthy, and traffic is drained completely from old instances before they are removed, deployments take place with zero downtime. Host-based routing Host-based routing allows you to run multiple applications on the same server, using a single instance of Kamal Proxy to route traffic to all of them. When deploying an instance, you can specify a host that it should serve traffic for: kamal-proxy deploy service1 --target web-1:3000 --host app1.example.com When deployed in this way, the instance will only receive traffic for the specified host. By deploying multiple instances, each with their own host, you can run multiple applications on the same server without port conflicts. Only one service at a time can route a specific host: kamal-proxy deploy service1 --target web-1:3000 --host app1.example.com kamal-proxy deploy service2 --target web-2:3000 --host app1.example.com # returns \"Error: host is used by another service\" kamal-proxy remove service1 kamal-proxy deploy service2 --target web-2:3000 --host app1.example.com # suceeds Automatic TLS Kamal Proxy can automatically obtain and renew TLS certificates for your applications. To enable this, add the --tls flag when deploying an instance: kamal-proxy deploy service1 --target web-1:3000 --host app1.example.com --tls Specifying run options with environment variables In some environments, like when running a Docker container, it can be convenient to specify run options using environment variables. This avoids having to update the CMD in the Dockerfile to change the options. To support this, kamal-proxy run will read each of its options from environment variables if they are set. For example, setting the HTTP port can be done with either: kamal-proxy run --http-port 8080 or: HTTP_PORT=8080 kamal-proxy run If any of the environment variables conflict with something else in your environment, you can prefix them with KAMAL_PROXY_ to disambiguate them. For example: KAMAL_PROXY_HTTP_PORT=8080 kamal-proxy run Building To build Kamal Proxy locally, if you have a working Go environment you can: make Alternatively, build as a Docker container: make docker Trying it out See the example folder for a Docker Compose setup that you can use to try out the proxy commands.",
    "commentLink": "https://news.ycombinator.com/item?id=41608350",
    "commentBody": "Kamal Proxy – A minimal HTTP proxy for zero-downtime deployments (github.com/basecamp)162 points by norbert1990 11 hours agohidepastfavorite74 comments 000ooo000 8 hours agoStrange choice of language for the actions: >To route traffic through the proxy to a web application, you *deploy* instances of the application to the proxy. *Deploying* an instance makes it available to the proxy, and replaces the instance it was using before (if any). >e.g. `kamal-proxy deploy service1 --target web-1:3000` 'Deploy' is a fairly overloaded term already. Fun conversations ahead. Is the app deployed? Yes? No I mean is it deployed to the proxy? Hmm our Kamal proxy script is gonna need some changes and a redeployment so that it deploys the deployed apps to the proxy correctly. Unsure why they couldn't have picked something like 'bind', or 'intercept', or even just 'proxy'... why 'deploy'.. reply nahimn 5 hours agoparent“Yo dawg, i heard you like deployments, so we deployed a deployment in your deployment so your deployment can deploy” -Xzibit reply irundebian 7 hours agoparentprevDeployed = running and registered to the proxy. reply vorticalbox 4 hours agorootparentThen wouldn’t “register” be a better term? reply rad_gruchalski 2 hours agorootparentIt’s registered but is it deployed? reply 8organicbits 5 hours agoparentprevIf your ingress traffic comes from a proxy, what would deploy mean other than that traffic from the proxy is now flowing to the new app instance? reply viraptor 4 hours agoprevIt's an interesting choice to make this a whole app, when the zero-downtime deployments can be achieved with other servers trivially these days. For example any app+web proxy which supports Unix sockets can do zero-downtime by moving the file. It's atomic and you can send the warm-up requests with curl. Building a whole system with registration feels like an overkill. reply dewey 42 minutes agoparentThat's just a small part of Kamal (https://kamal-deploy.org), their deployment tool they build and used to move from the cloud to their own hardware, saving millions (https://basecamp.com/cloud-exit). reply simonw 1 hour agoprevDoes this implement the “traffic pausing” pattern? That’s where you have a proxy which effectively pauses traffic for a few seconds - incoming requests appear to take a couple of seconds longer than usual, but are still completed after that short delay. During those couple of seconds you can run a blocking infrastructure change - could be a small database migration, or could be something a little more complex as long as you can get it finished in less than about 5 seconds. reply ignoramous 20 minutes agoparenttbh, sounds like \"living dangerously\" pattern to me. reply shafyy 7 hours agoprevAlso exciting that Kamal 2 (currently RC https://github.com/basecamp/kamal/releases) will support auto-SSL and make it easy to run multiple apps on one server with Kamal. reply blue_pants 7 hours agoprevCan someone briefly explain how ZDD works in general? I guess both versions of the app must be running simultaneously, with new traffic being routed to the new version of the app. But what about DB migrations? Assuming the app uses a single database, and the new version of the app introduces changes to the DB schema, the new app version would modify the schema during startup via a migration script. However, the previous version of the app still expects the old schema. How is that handled? reply diggan 6 hours agoparentFirst step is to decouple migrations from deploys, you want manual control over when the migrations run, contrary to many frameworks default of running migrations when you deploy the code. Secondly, each code version has to work with the current schema and the schema after a future migration, making all code effectively backwards compatible. Your deploys end up being something like: - Deploy new code that works with current and future schema - Verify everything still works - Run migrations - Verify everything still works - Clean up the acquired technical debt (the code that worked with the schema that no longer exists) at some point, or run out of runway and it won't be an issue reply wejick 6 hours agorootparentThis is very good explanation, no judgment and simply educational. Appreciated Though I'm still surprised that some people run DB alteration on application start up. Never saw one in real life. reply whartung 1 hour agorootparentWe do this. It has worked very well for us. There's a couple of fundamental rules to follow. First, don't put something that will have insane impact into the application deploy changes. 99% of the DB changes are very cheap, and very minor. If the deploy is going to be very expensive, then just don't do it, we'll do it out of band. This has not been a problem in practice with our 20ish person team. Second, it was kind of like double entry accounting. Once you committed the change, you can not go back and \"fix it\". If you did something really wrong (i.e. see above), then sure, but if not, you commit a correcting entry instead. Because you don't know who has recently downloaded your commit, and run it against their database. The changes are a list of incremental steps that the system applies in order, if they had not been applied before. So, they are treated as, essentially, append only. And it has worked really well for us, keeping the diverse developers who deploy again local databases in sync with little drama. I've incorporated the same concept in my GUI programs that stand up their own DB. It's a very simple system. reply e_y_ 1 hour agorootparentprevAt my company, DB migrations on startup was a flag that was enabled for local development and disabled for production deploys. Some teams had it enabled for staging/pre-production deploys, and a few teams had it turned on for production deploys (although those teams only had infrequent, minor changes like adding a new column). Personally I found the idea of having multiple instances running the same schema update job at the same time (even if locks would keep it from running in practice) to be concerning so I always had it disabled for deploys. reply miki123211 5 hours agorootparentprevIt makes things somewhat easier if your app is smallish and your workflow is something like e.g. Github Actions automatically deploying all commits on main to Fly or Render. reply diggan 5 hours agorootparentprev> Though I'm still surprised that some people run DB alteration on application start up I think I've seen it more commonly in the Golang ecosystem, for some reason. Also not sure how common it is nowadays, but seen lots of deployments (contained in Ansible scripts, Makefiles, Bash scripts or whatever) where the migration+deploy is run directly in sequence automatically for each deploy, rather than as discrete steps. Edit: Maybe it's more of an educational problem than something else, where learning resources either don't specify when to actually run migrations or straight up recommend people to run migrations on application startup (one example: https://articles.wesionary.team/integrating-migration-tool-i...) reply shipp02 1 hour agorootparentprevSo if you add any constraints/data, you can't rely on them being there until version n+2 or you need to have 2 paths 1 for the old date, 1 for new? reply simonw 1 hour agorootparentEffectively yes. Zero downtime deployments with database migrations are fiddly. reply svvvy 3 hours agorootparentprevI thought it was correct to run the DB migrations for the new code first, then deploy the new code. While making sure that the DB schema is backwards compatible with both versions of the code that will be running during the deployment. So maybe there's something I'm missing about running DB migrations after the new code has been deployed - could you explain? reply ffsm8 3 hours agorootparentI'm not the person you've asked, but I've worked in devops before. It kinda doesn't matter which you do first. And if you squint a little, it's effectively the same thing, because the migration will likely only become available via a deployment too So yeah, the only things that's important is that the DB migration can't cause an incompatibility with any currently deployed version of the code - and if it would, you'll have to split the change so it doesn't. It'll force another deploy for the change you want to do, but it's what you're forced to do if maintenance windows aren't an option. Which is kinda a given for most b2c products reply jacobsimon 5 hours agorootparentprevThis is the way reply andrejguran 7 hours agoparentprevMigrations have to be backwards compatible so the DB schema can serve both versions of the app. It's an extra price to pay for having ZDD or rolling deployments and something to keep in mind. But it's generally done by all the larger companies reply gsanderson 6 hours agoparentprevI haven't tried it but it looks like Xata has come up with a neat solution to DB migrations (at least for postgres). There can be two versions of the app running. https://xata.io/blog/multi-version-schema-migrations reply efortis 7 hours agoparentprevYes, both versions must be running at some point. The load balancer starts accepting connections on Server2 and stops accepting new connections on Server1. Then, Server1 disconnects when all of its connections are closed. It could be different Servers or multiple Workers on one server. During that window, as the other comments said, migrations have to be backwards compatible. reply stephenr 6 hours agoparentprevOthers have described the how part if you do need truly zero downtime deployments, but I think it's worth pointing out that for most organisations, and most migrations, the amount of downtime due to a db migration is virtually indistinguishable from zero, particularly if you have a regional audience, and can aim for \"quiet\" hours to perform deployments. reply diggan 5 hours agorootparent> the amount of downtime due to a db migration is virtually indistinguishable from zero Besides, once you've run a service for a while that has acquired enough data for migrations to take a while, you realize that there are in fact two different types of migrations. \"Schema migrations\" which are generally fast and \"Data migrations\" that depending on the amount of data can take seconds or days. Or you can do the \"data migrations\" when needed (on the fly) instead of processing all the data. Can get gnarly quickly though. Splitting those also allows you to reduce maintenance downtime if you don't have zero-downtime deployments already. reply sgarland 3 hours agorootparentSchema migrations can be quite lengthy, mostly if you made a mistake earlier. Some things that come to mind are changing a column’s type, or extending VARCHAR length (with caveats; under certain circumstances it’s instant). reply lukevp 2 hours agorootparentNot OP, but I would consider this a data migration as well. Anything that requires an operation on every row in a table would qualify. Really changing the column type is just a built in form of a data migration. reply stephenr 5 hours agorootparentprevVery much so, we handle these very differently for $client. Schema migrations are versioned in git with the app, with up/down (or forward/reverse) migration scripts and are applied automatically during deployment of the associated code change to a given environment. SQL Data migrations are stored in git so we have a record but are never applied automatically, always manually. The other thing we've used along these lines, is having one or more low priority job(s) added to a queue, to apply some kind of change to records. These are essentially still data migrations, but they're written as part of the application code base (as a Job) rather than in SQL. reply jakjak123 1 hour agorootparentprevMost are not affected by db migrations in the sense that migrations are run before the service starts the web server during boot. the database might block traffic for other already running connections though,in which case you have a problem with your database design. reply kh_hk 7 hours agoprevI don't understand how to use this, maybe I am missing something. Following the example, it starts 4 replicas of a 'web' service. You can create a service by running a deploy to one of the replicas, let's say example-web-1. What does the other 3 replicas do? Now, let's say I update 'web'. Let's assume I want to do a zero-downtime deployment. That means I should be able to run a build command on the 'web' service, start this service somehow (maybe by adding an extra replica), and then run a deploy against the new target? If I run a `docker compose up --build --force-recreate web` this will bring down the old replica, turning everything moot. Instructions unclear, can anyone chime in and help me understand? reply sisk 3 hours agoparentFor the first part of your question about the other replicas, docker will load balance between all of the replicas either with a VIP or by returning multiple IPs in the DNS request[0]. I didn't check if this proxy balances across multiple records returned in a DNS request but, at least in the case of VIP-based load balancing, should work like you would expect. For the second part about updating the service, I'm a little less clear. I guess the expectation would be to bring up a differently-named service within the same network, and then `kamal-proxy deploy` it? So maybe the expectation is for service names to include a version number? Keeping the old version hot makes sense if you want to quickly be able to route back to it. [0]: https://docs.docker.com/reference/compose-file/deploy/#endpo... reply thelastparadise 6 hours agoparentprevWhy would I not just do k8s rollout restart deployment? Or just switch my DNS or router between two backends? reply joeatwork 6 hours agorootparentI think this is part of a lighter weight Kubernetes alternative. reply ianpurton 6 hours agorootparentLighter than the existing light weight kubernetes alternatives i.e. k3s :) reply diggan 6 hours agorootparentOr, hear me out: Kubernetes alternatives that don't involve any parts of Kubernetes at all :) reply ozgune 3 hours agorootparentprevI think the parent project, Kamal, positions itself as a simpler alternative to K8s when deploying web apps. They have a question on this on their website: https://kamal-deploy.org \"Why not just run Capistrano, Kubernetes or Docker Swarm? ... Docker Swarm is much simpler than Kubernetes, but it’s still built on the same declarative model that uses state reconciliation. Kamal is intentionally designed around imperative commands, like Capistrano. Ultimately, there are a myriad of ways to deploy web apps, but this is the toolkit we’ve used at 37signals to bring HEY and all our other formerly cloud-hosted applications home to our own hardware.\" reply jgalt212 6 hours agorootparentprevYou still need some warm-up routine to run for the newly online server before the hand-off occurs. I'm not a k8s expert, but the above described events can be easily handled by a bash or fab script. reply ahoka 5 hours agorootparentWhat events do you mean? If the app needs a warm up, then it can use its readiness probe to ask for some delay until it gets request routed to it. reply jgalt212 5 hours agorootparentGET requests to pages that fill caches or those that make apache start up more than n processes. reply thelastparadise 3 hours agorootparentprevThis is a health/readiness probe in k8s. It's already solved quite solidly. reply ksajadi 3 hours agoprevThis primarily exists to take care of a fundamental issue in Docker Swarm (Kamal's orchestrator of choice) where replacing containers of a service disrupts traffic. We had the same problem (when building JAMStack servers at Cloud 66) and used Caddy instead of writing our own proxy and also looked at Traefik which would have been just as suitable. I don't know why Kamal chose Swarm over k8s or k3s (simplicity perhaps?) but then, complexity needs a home, you can push it around but cannot hide it, hence a home grown proxy. I have not tried Kamal proxy to know, but I am highly skeptical of something like this, because I am pretty sure I will be chasing it for support for anything from WebSockets to SSE, to HTTP/3 to various types of compression and encryption. reply hipadev23 2 hours agoparentI feel like you’re conflating the orchestration with proxying. There’s no reason they couldn’t be using caddy or traefik or envoy for the proxy (just like k8s ends up using them as an ingress controller), while still using docker. reply ksajadi 1 hour agorootparentDocker is the container engine. Swarm is the orchestration, the same as Kubernetes. The concept of \"Service\" in k8s takes care of a lot of the proxying, while still using Docker (not anymore tho). In Swarm, services exist but only take care of container lifecycle and not traffic. While networking is left to the containers, Swarm services always get in the way, causing issues that will require a proxy. In k8s for example, you can use Docker and won't need a proxy for ZDD (while you might want one for Ingress and other uses) reply jauntywundrkind 2 hours agoparentprevKamal feels built around the premise that \"Kubernetes is too complicated\" (after Basecamp got burned by some hired help), and from that justification it goes out and recreates a sizable chunk of the things Kubernetes does. Your list of things a reverse proxy might do is a good example to me of how I expect this to go: what starts out as an ambition to be simple inevitably has to grow & grow more of complexity it sought to avoid. Part of me strongly thinks we need competition & need other things trying to create broad ideally extensible ways or running systems. But a huge part of me sees Kamal & thinks, man, this is a lot of work being done only to have to keep walking backwards into the complexity they were trying to avoid. Usually second system syndrome is the first system being simple the second being overly complicated, and on the tin the case is inverse, but man, the competency of Kube & it's flexibility/adaptability as being a framework for Desired State Management really shows through for me. reply ksajadi 56 minutes agorootparentI agree with you and at the risk of self-promotion, that's why we built Cloud 66 (which takes care of Day-1 (build and deploy) as well as Day-2 (scale and maintenance) part of infrastructure. As we all can see there is a lot to this than just wrapping code in a Dockerfile and pushing it out to a Swarm cluster. reply ahdfyasdf 6 hours agoprevIs there a way to configure timeouts? https://github.com/basecamp/kamal-proxy/blob/main/internal/s... https://github.com/basecamp/kamal-proxy/blob/main/internal/s... https://blog.cloudflare.com/exposing-go-on-the-internet/ reply ianpurton 6 hours agoprevDHH in the past has said \"This setup helped us dodge the complexity of Kubernetes\" But this looks like somehow a re-invention of what Kubernetes provides. Kubernetes has come a long way in terms of ease of deployment on bare metal. reply wejick 6 hours agoparentNo downtime deployment is always there long before kube. It does look as simple as ever been, not like kube for sure. reply moondev 6 hours agoprevDoes this handle a host reboot? reply risyachka 6 hours agoprevDid they mention anywhere why they decided to write their own proxy instead of using Traefik or something else battle tested? reply yla92 6 hours agoparentThey were actually using Traefik until this \"v2.0.0\" (pre-release right now) version. There are some context about why they switched and decided to roll their own, from the PR. https://github.com/basecamp/kamal/pull/940 reply oDot 8 hours agoprevDHH mentioned they built it to move from the cloud to bare metal. He glorifies the simplicity but I can't help thinking they are a special use case of predictable, non-huge load. Uber, for example, moved to the cloud. I feel like in the span between them there are far more companies for which Kamal is not enough. I hope I'm wrong, though. It'll be nice for many companies to be have the choice of exiting the cloud. reply martinald 7 hours agoparentI don't think that's the real point. The real point is that 'big 3' cloud providers are so overpriced that you could run hugely over provisioned infra 24/7 for your load (to cope with any spikes) and still save a fortune. The other thing is that cloud hardware is generally very very slow and many engineers don't seem to appreciate how bad it is. Slow single thread performance because of using the most parallel CPUs possible (which are the cheapest per W for the hyperscalers), very poor IO speeds, etc. So often a lot of this devops/infra work is solved by just using much faster hardware. If you have a fairly IO heavy workload then switching from slow storage to PCIe4 7gbyte/sec NVMe drives is going to solve so many problems. If your app can't do much work in parallel then CPUs with much faster single threading performance can have huge gains. reply sgarland 3 hours agorootparent> The other thing is that cloud hardware is generally very very slow and many engineers don't seem to appreciate how bad it is. This. Mostly disk latency, for me. People who have only ever known DBaaS have no idea how absurdly fast they can be when you don’t have compute and disk split by network hops, and your disks are NVMe. Of course, it doesn’t matter, because the 10x latency hit is overshadowed by the miasma of everything else in a modern stack. My favorite is introducing a caching layer because you can’t write performant SQL, and your DB would struggle to deliver it anyway. reply igortg 3 hours agorootparentprevI'm using a managed Postgres instance in a well known provider and holy shit, I couldn't believe how slow it is. For small datasets I couldn't notice, but when one of the tables reached 100K rows, queries started to take 5-10 seconds (the same query takes 0.5-0.6 in my standard i5 Dell laptop). I wasn't expecting blasting speed on the lowest tear, but 10x slower is bonkers. reply miki123211 5 hours agorootparentprevYou can always buy some servers to handle your base load, and then get extra cloud instances when needed. If you're running an ecommerce store for example, you could buy some extra capacity from AWS for Christmas and Black Friday, and rely on your own servers exclusively for the rest of the year. reply jsheard 5 hours agorootparentprevIt's sad that what should have been a huge efficiency win, amortizing hardware costs across many customers, ended up often being more expensive than just buying big servers and letting them idle most of the time. Not to say the efficiency isn't there, but the cloud providers are pocketing the savings. reply toomuchtodo 5 hours agorootparentIf you want a compute co-op, build a co-op (think VCs building their own GPU compute clusters for portfolio companies). Public cloud was always about using marketing and the illusion of need for dev velocity (which is real, hypergrowth startups and such, just not nearly as prevalent as the zeitgeist would have you believe) to justify the eye watering profit margin. Most businesses have fairly predictable interactive workload patterns, and their batch jobs are not high priority and can be managed as such (with the usual scheduling and bin packing orchestration). Wikipedia is one of the top 10 visited sites on the internet, and they run in their own datacenter, for example. The FedNow instant payment system the Federal Reserve recently went live with still runs on a mainframe. Bank of America was saving $2B a year running their own internal cloud (although I have heard they are making an attempt to try to move to a public cloud). My hot take is public cloud was an artifact of ZIRP and cheap money, where speed and scale were paramount, cost being an afterthought (Russ Hanneman pre-revenue bit here, \"get big fast and sell\"; great fit for cloud). With that macro over, and profitability over growth being the go forward MO, the equation might change. Too early to tell imho. Public cloud margins are compute customer opportunities. reply miki123211 4 hours agorootparentWikipedia is often brought up in these discussions, but it's a really bad example. To a vast majority of Wikipedia users who are not logged in, all it needs to do is show (potentially pre-rendered) article pages with no dynamic, per-user content. Those pages are easy to cache or even offload to a CDN. FOr all the users care, it could be a giant key-value store, mapping article slugs to HTML pages. This simplicity allows them to keep costs down, and the low costs mean that they don't have to be a business and care about time-on-page, personalized article recommendations or advertising. Other kinds of apps (like social media or messaging) have very different usage patterns and can't use this kind of structure. reply toomuchtodo 4 hours agorootparent> Other kinds of apps (like social media or messaging) have very different usage patterns and can't use this kind of structure. Reddit can’t turn a profit, Signal is in financial peril. Meta runs their own data centers. WhatsApp could handle ~3M open TCP connections per server, running the operation with under 300 servers [1] and serving ~200M users. StackOverflow was running their Q&A platform off of 9 on prem servers as of 2022 [2]. Can you make a profitable business out of the expensive complex machine? That is rare, based on the evidence. If you’re not a business, you’re better off on Hetzner (or some other dedicated server provider) boxes with backups. If you’re down you’re down, you’ll be back up shortly. Downtime is cheaper than five 9s or whatever. I’m not saying “cloud bad,” I’m saying cloud where it makes sense. And those use cases are the exception, not the rule. If you're not scaling to an event where you can dump these cloud costs on someone else (acquisition event), or pay for them yourself (either donations, profitability, or wealthy benefactor), then it's pointless. It's techno performance art or fancy make work, depending on your perspective. [1] https://news.ycombinator.com/item?id=33710911 [2] https://www.datacenterdynamics.com/en/news/stack-overflow-st... reply olieidel 8 hours agoparentprev> I feel like in the span between them there are far more companies for which Kamal is not enough. I feel like this is a bias in the HN bubble: In the real world, 99% of companies with any sort of web servers (cloud or otherwise) are running very boring, constant, non-Uber workloads. reply ksec 5 hours agorootparentNot just HN but overall the whole internet. Because all the news and article, tech achievements are pumped out from Uber and other big tech companies. I am pretty sure Uber belongs to the 1% of the internet companies in terms of scale. 37Signals isn't exactly small either. They spend $3M a year on infrastructure in 2019. Likely a lot higher now. The whole Tech cycle needs to stop having a top down approach where everyone are doing what Big tech are using. Instead we should try to push the simplest tool from low end all the way to 95% mark. reply nchmy 5 hours agorootparentThey spend considerably less on infra now - this was the entire point of moving off cloud. DHH has written and spoken lots about it, providing real numbers. They bought their own servers and the savings paid for it all in like 6 months. Now its just money in the bank til they replace the hardware in 5 years. Cloud is a scam for the vast majority of companies. reply toberoni 7 hours agoparentprevI feel Uber is the outlier here. For every unicorn company there are 1000s of companies that don't need to scale to millions of users. And due to the insane markup of many cloud services it can make sense to just use beefier servers 24/7 to deal with the peaks. From my experience crazy traffic outliers that need sophisticated auto-scaling rarely happens outside of VC-fueled growth trajectories. reply appendix-rock 8 hours agoparentprevYou can’t talk about typical cases and then bring up Uber. reply pinkgolem 8 hours agoparentprevI mean most B2B company have a pretty predictable load when providing services to employees.. I can get weeks advance notice before we have a load increase through new users reply mt42or 6 hours agoprevNIH. Nothing else to add. reply elktown 6 hours agoparentMeanwhile in the glorious land of \"never invented here\": https://news.ycombinator.com/item?id=38526780 reply rohvitun 5 hours agoprevAya reply 0xblinq 5 hours agoprev [–] 3 years from now they'll have invented their own AWS. NIH syndrome in full swing. reply bdcravens 4 hours agoparent [–] It's a matter of cost, not NIH syndrome. In Basecamp's case, saving $2.4M a year isn't something to ignore. https://basecamp.com/cloud-exit Of course, it's fair to say that rebuilding the components that the industry uses for hosting on bare metal is NIH syndrome. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Kamal Proxy is a lightweight HTTP proxy designed for zero-downtime deployments, allowing web applications to update without interrupting ongoing traffic.",
      "It can be used standalone or as part of Kamal, which includes container packaging and provisioning for a complete deployment experience.",
      "Key features include customizable run options, host-based routing, automatic TLS certificates, and environment variable configuration."
    ],
    "commentSummary": [
      "Kamal Proxy is a minimal HTTP proxy designed for zero-downtime deployments, part of Basecamp's Kamal deployment tool, which aims to simplify deployments compared to Kubernetes.",
      "It supports features like auto-SSL and running multiple apps on one server, with users discussing strategies for handling database migrations during zero-downtime deployments.",
      "The discussion includes why Basecamp built their own proxy instead of using existing solutions and the broader trend of companies moving away from cloud services due to high costs and performance issues."
    ],
    "points": 162,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1726905341
  },
  {
    "id": 41606772,
    "title": "Working in the office 5 days/week to build company culture is a myth: PwC report",
    "originLink": "https://www.msn.com/en-us/money/other/working-in-the-office-5-days-a-week-to-build-company-culture-is-a-myth-pwc-report-says/ar-AA1qU17L",
    "originBody": "Working in the office 5 days a week to build company culture is a myth, PwC report says{ \"@context\": \"http://schema.org\", \"@type\": \"Article\", \"headline\": \"Working in the office 5 days a week to build company culture is a myth, PwC report says\", \"image\": \"https://img-s-msn-com.akamaized.net/tenant/amp/entityid/AA1qU17u.img?w=2048&h=1365&m=4&q=88\", \"author\": \"Orianna Rosa Royle\", \"publisher\": { \"name\": \"Fortune\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://img-s-msn-com.akamaized.net/tenant/amp/entityid/AA1b1OSR.img\" } }, \"url\": \"http://www.msn.com/en-us/money/other/working-in-the-office-5-days-a-week-to-build-company-culture-is-a-myth-pwc-report-says/ar-AA1qU17L?apiversion=v2&noservercache=1&domshim=1&renderwebcomponents=1&wcseo=1&batchservertelemetry=1&noservertelemetry=1\", \"datePublished\": \"2024-09-20T12:08:03Z\", \"dateModified\": \"2024-09-20T21:39:19Z\", \"description\": \"Amazon has become the latest firm to end working from home in the name of company culture—a PwC reports suggests it could have the opposite effect.\", \"articleBody\": \"Amazon has become the latest major company to order staff to return to the office five days a week, effectively ending working from home at the $1.99 trillion tech giant. Just like JPMorgan, Boots, and Goldman Sachs’ bosses, Amazon’s CEO Andy Jassy cited strengthening company culture as one of the main reasons behind raising its in-office requirements from three days to full-time. However, a new report from PwC suggests that the move to full-time in-office work could have a different effect than the one intended. The Big Four accounting firm conducted 13 months of research and surveyed over 20,000 business leaders, chief human resources officers and workers for its new Workforce Radar Report—and it found that hybrid workers feel more included and productive than those who sit at their company’s desk five days a week.“While many companies are pushing for return to office, it turns out that hybrid workers demonstrate the highest levels of satisfaction,” the report highlights. The researchers found that over three-quarters of hybrid workers feel like they belong, compared to 74% of fully on-site workers and 68% of remote workers. Similarly, 74% of hybrid workers are engaged—this drops to 72% for in-office workers and 63% for remote workers. These may seem like marginal differences, but they have a ripple effect on company culture: A staggering 90% of hybrid employees reported that the culture at their firm promotes community, collaboration, inclusion and belonging.“The idea that being on-site all day every day is necessary to establish and sustain a strong culture is a myth,” the report concludes. “Don’t be afraid to offer flexible options for fear of diminishing it.”Why two extra in-office days can chip away at company culture It might seem counterintuitive to build company culture by encouraging workers to spend more time away from each other—not less. However, in reality, when leaders enforce five days a week in the office they often end up overlooking engagement activities like team away days and after-work drinks.“The office becomes a crutch—engagement, recognition, and connection all happen by default,” Daan Van Rossum, author of the Future Work newsletter and founder of FlexOS tells Fortune.“When companies move to a hybrid schedule, they start implementing more purposeful efforts to replace this engagement. In the process, the hybrid experience actually leads to improved engagements with more touch points.” Plus, nobody likes commuting to sit in a noisy office to do the same job they could have done at home—especially not introverts. Experts tell Fortune that only having to do it a few days a week helps workers make the most of their in-office days and maximize collaboration.Then there’s having to deal with personality clashes on a daily basis, rather than in microdoses. “Being forced to work in the office exposes you to people who may share very different values to you,” says Amrit Sandhar CEO of the employment engagement firm &Evolve. “Over time, this can feel exhausting.”‘Don’t look back, look ahead,’ PwC says Despite Amazon’s call for workers to return to “the way we were before the onset of COVID”, PwC’s research highlights that workers simply won&#39;t return to the old days of working. “We’ve seen that return-to-office mandates have, in many cases, failed,” the report says before adding “the business-as-usual paradigm to which some business leaders want to return doesn’t exist anymore.” “Employees didn’t miss those long, stressful commutes, and they got used to the flexibility in scheduling, parenting, caregiving and so on that working remotely gave them,” it continues. “They were not keen on going into the office without a compelling reason.” Indeed, the biggest reason company culture dips when employees are forced to collaborate daily is precisely because it’s been forced.RTO’s and measures that monitor attendance, including tracking badge swiping, sends out the message that worker’s presence is more important to the company than output or meaningful collaboration.It’s “as if the number of employees was the point and not what those employees were doing once inside,” the report outlines,\" the report notes.For &Evolve CEO Sandhar, the problem for employees is that a full return to office can feel like surveillance. “Having the autonomy to make your own decisions does more to enhance a culture of valuing employees, rather than diminishing it,” Sandhar concludes. “No one wants to be controlled in a rigid environment, so forcing people back to workplaces can feel like control.\" \"It’s likely that this theme is followed through in day-to-day work experiences—rather than providing employees with a sense of autonomy and freedom, that sense of control such as being micro-managed, is likely to chip away at whatever vision the organization has for its culture, and lead to disengagement.” This story was originally featured on Fortune.com\" }window.isSSREnabled=true;window.markTTSR=function wi(e){window._pageTimings||(window._pageTimings={}),window._pageTimings[`${e}-Init`]=Math.round(performance.now()),window.requestAnimationFrame((()=>{window._pageTimings[`${e}-RAF`]=Math.round(performance.now()),window.setTimeout((()=>{window._pageTimings[e]=Math.round(performance.now()),window._pageTimings.TTSR=Math.max(window._pageTimings.TTSR||0,window._pageTimings[e])}))}))}try {window._pageTimings = window._pageTimings || {};window._pageTimings[\"TTJSStart\"] = Math.round(performance.now());} catch (e) {console.error(\"Error in adding TTJSStart marker\");}function getCookieConsentRequired(){return false;}window._ssrServiceEntryUrl=\"/bundles/v1/views/latest/SSR-service-entry.a43029b098f75e2757ed.js\";window[\"_webWorkerBundle\"] = \"/bundles/v1/views/latest/web-worker.2ebba1ae1b36072b3c9c.js\";window[\"_authCookieName\"] = \"ace\";!function(){\"use strict\";var e={},t={};function n(o){var i=t[o];if(void 0!==i)return i.exports;var r=t[o]={exports:{}};return e[o](r,r.exports,n),r.exports}function o(){return!(\"undefined\"==typeof window||!window.document||!window.document.createElement||window.isRenderServiceEnv)}n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(e){if(\"object\"==typeof window)return window}}(),function(){if(void 0!==n){const e=n.e,t={};n.e=function(o){return e(o).catch((function(e){const i=t.hasOwnProperty(o)?t[o]:2;if(ii.delete(e),has:e=>i.has(e)});const s=\"__RequestDataInstance__\";class c{constructor(e,t){if(this.url=new URL(e.href),this.innerHeight=e.innerHeight,this.devicePixelRatio=e.devicePixelRatio,this.canUseCssGrid=e.canUseCssGrid,this.requestId=e.requestId,this.cookie=e.cookie,this.referer=e.referer,this.userAgent=e.userAgent,this.clientData=e.clientData,this.oneServiceHeaders=function(e){try{if(e)return JSON.parse(e)}catch(e){}}(e.oneServiceHeaders)||{},this.isPssrMode=t,t){const e=a(\"OSATE\",this.cookie),t=!!e&&\"1\"===e,n=a(\"OSAT\",this.cookie);if(t&&n||!e&&!n)return this.msalAuthReady=!0,void(n&&(this.oneServiceHeaders.Authorization=`Bearer ${n}`));this.msalAuthReady=!1,this.pssrRejectedReason=e&&!t?\"interactiveLogin\":e&&!n?\"missOSAT\":\"missOSATE\"}}static getInstance(){const e=r.get(s);return e||(n.g.TEST_ENV?d({href:\"http://localhost:8080/\",innerHeight:768,devicePixelRatio:1,canUseCssGrid:!1,requestId:\"0\",cookie:\"\",userAgent:\"\",referer:\"\",oneServiceHeaders:\"\"}):d({href:\"http://localhost:8080/\",innerHeight:0,devicePixelRatio:0,canUseCssGrid:!1,requestId:\"0\",cookie:\"\",userAgent:\"\",referer:\"\",oneServiceHeaders:\"\"}))}static resetInstance(e){const t=new c(e,arguments.length>1&&void 0!==arguments[1]&&arguments[1]);return r.set(s,t),t}}function a(e,t){if(t&&e){const n=new RegExp(\"\\\\b\"+e+\"\\\\s*=\\\\s*([^;]*)\",\"i\").exec(t);return n&&n.length>1?n[1]:null}return null}const d=c.resetInstance,l=()=>c.getInstance();var u,p;!function(e){e.Anon=\"ANON\",e.AppAnon=\"APP_ANON\",e.Unknown=\"\"}(u||(u={})),function(e){e.AnonCookieExists=\"ace\",e.AppAnonCookieExists=\"aace\"}(p||(p={}));const h=\"child\";function g(e,t){if(e){const n=new RegExp(\"\\\\b\"+t+\"\\\\s*=\\\\s*([^;]*)\",\"i\").exec(e);return n&&n.length>1?n[1]:null}return null}function w(e){try{if(e)return JSON.parse(e)}catch(e){}return null}const m=\"prg-pr2-\",f=\"prg-pr2-only\",b=\"prg-pw-\",v=\"prg-\",y=\"prg-1sw-\",k=\"prg1flights\";function S(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:v;const n=t.length,o=[];return e&&e.length&&(e.forEach((e=>{const i=e&&e.trim();i&&i.length>=n&&i.substring(0,n).toLowerCase()===t&&o.push(i.toLowerCase())})),o.sort()),o}function C(){let e=arguments.length>0&&void 0!==arguments[0]&&arguments[0],t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:\"\";const n=A(\"info\"),o=((n&&n.match(/f:\\s*([^;]+)/i)||[])[1]||\"\").split(\",\"),i=function(e){let t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:\"\",n=v;return arguments.length>1&&void 0!==arguments[1]&&arguments[1]?n=b:e&&e.includes(k)&&(t.toLocaleLowerCase()===\"windowsShell\".toLocaleLowerCase()||t.toLocaleLowerCase()===\"windowsShellV2\".toLocaleLowerCase())?n=y:e&&e.includes(f)&&t.toLocaleLowerCase()===\"winWidgets\".toLocaleLowerCase()&&(n=m),n}(o,e,t);return{all:o,prg:S(o,i)}}function A(e){var t;return document.head.dataset[e]||(null===(t=_())||void 0===t?void 0:t.dataset[e])}let I;function _(){return void 0===I&&(I=document.getElementsByClassName(\"peregrine-widget-settings\")[0]||null),I}const q=\"Authorization\";var O,E;!function(e){e.Presentation=\"presentation\"}(O||(O={})),function(e){e.Unknown=\"Unknown\",e.Portrait=\"Portrait\",e.Landscape=\"Landscape\"}(E||(E={}));var L,x;!function(e){e[e.Undefined=0]=\"Undefined\",e[e.Basic=1]=\"Basic\",e[e.Advanced=2]=\"Advanced\",e[e.Premium=3]=\"Premium\"}(L||(L={})),function(e){e.Init=\"init\",e.Config=\"config\",e.Targeted=\"targeted\",e.Sticky=\"sticky\",e.NoSticky=\"no_sticky\",e.Admin=\"admin\",e.Forced=\"forced\",e.Manual=\"manual\"}(x||(x={}));new Set([\"winp0dash\",\"winp1taskbar\",\"winp1taskbarent\",\"winp1taskbardirect\",\"winp1taskbardirectent\",\"winp1taskbarent\",\"winp2juntaskbar\",\"winp2juntaskbarent\",\"winp2\",\"winp2ent\",\"winp2fp\",\"winp2fpent\",\"winp2fptaskbar\",\"winp2fptaskbarent\",\"winp2fptaskbarhover\",\"winp2fptaskbarhoverent\",\"winp2widget\",\"winp2widgetent\"]);const T=new Set([\"finance-app-win\",\"weather-app-win\",\"winpstoreapp\"]),P=(new Set([\"msedgdhp\",\"msedgdhphdr\",\"msedgntphdr\",\"msedgntp\",\"msedgdhp\",\"entnewsntp\"]),new Set([\"hpmsn\"]));new Set([\"chromentpnews\"]);function N(){var e;return o()?w(document.head.dataset.clientSettings||(null===(e=document.getElementsByClassName(\"peregrine-widget-settings\")[0])||void 0===e?void 0:e.getAttribute(\"data-client-settings\"))):null}const R=\"feed/personalize/settings\";function W(e,t){const n=t.replace(/[[\\]]/g,\"\\\\$&\"),o=new RegExp(\"[?&]\"+n+\"(=([^&#]*)|&|#|$)\").exec(e);if(!o)return null;const i=o[2];return decodeURIComponent(i.replace(/\\+/g,\" \"))||\"\"}let M=new class{constructor(){o()?(this.isDebugEnabled=(W(window.location.href,\"debug\")||\"\").toLowerCase()in{1:1,true:1}||\"vp\"===(W(window.location.href,\"reqsrc\")||\"\").toLowerCase(),this.isDebugEnabled&&!n.g.TEST_ENV&&(window.webpackRequire=n)):this.isDebugEnabled=!1}getLoggingService(){return null}isDebug(){return this.isDebugEnabled}setDebug(e){this.isDebugEnabled=e}setTags(e){}log(e,t){this.isDebug()&&console.info(e)}logError(e){console.error(e)}logCallback(e){this.isDebug()&&console.info(e())}logObjects(){this.isDebug()&&console.log(...arguments)}logSingleObject(e,t){this.isDebug()&&console.log(e)}};class D{get supported(){return!!this.storage}constructor(e){this.storage=e}getItem(e){if(this.supported)return this.storage.getItem(e)}getObject(e,t){const n=this.getItem(e);if(null!=n){const e=w(n);if(null!=e)return e}return t}key(e){if(this.supported&&e>=0)return this.storage.key(e)}keys(){return this.supported?Object.keys(this.storage):[]}setObject(e,t){void 0!==t?this.setItem(e,JSON.stringify(t)):this.removeItem(e)}removeObject(e){const t=this.removeItem(e);if(null!=t)return w(t)}setItem(e,t){let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(this.supported)try{if(!t)throw\"Attempted to store null/undefined value: \"+t;this.storage.setItem(e,t)}catch(e){if(!n)throw e;M.logError(e)}else if(!n)throw new Error(\"WebStorage not supported\")}get length(){if(this.supported)return this.storage.length}removeItem(e){if(this.supported){const t=this.getItem(e);return this.storage.removeItem(e),t}}clear(){this.supported&&this.storage.clear()}removeSubstringKeys(e){if(!this.supported||!e)return;const t=[];for(let n=0;nnew Date?\"valid\":n}function V(){return B=B||void 0===B&&F.getObject(U)||null,B}function z(e){try{return decodeURIComponent(e)}catch(e){}}function K(e,t){if(!e)return null;if(t)return X(e);if(!j){const e=\"undefined\"!=typeof document&&document.cookie.split(\"; \");j={};const t=e&&e.length;for(let n=0;n{n=!1,t=void 0}},o}!function(e){e.NotAvailable=\"notAvailable\",e.Expired=\"expired\",e.Valid=\"valid\"}($||($={}));const Z=\"currentaccount\",ee=Y((()=>{const e=K(Z,!0),t=e&&w(e),{login_hint:n}=t||{};return n&&t})),te=Y((()=>!!K(Z)));function ne(){try{return localStorage}catch(e){return null}}const oe=\"__PageExperimentInstance__\";function ie(e){r.set(oe,e)}function re(e){if(!r.has(oe)&&o()){var t;ie(se(document.head.dataset.info||(null===(t=window.document.getElementsByClassName(\"peregrine-widget-settings\")[0])||void 0===t||null===(t=t.dataset)||void 0===t?void 0:t.info)||\"\"))}const n=r.get(oe);return n&&n.has(e)}function se(e){const t=(e||\"\").replace(/(^f:|;.*$)/g,\"\").split(\",\");return new Set(t)}const ce=\"uxlogin\",ae=\"uxlogout\",de=\"uxedit\",le=\"useRedirect\",ue=\"uxswitch\",pe=Y((()=>{const e=ne();return e&&\"1\"===e.getItem(ce)||o()&&location.search&&location.search.includes(`${ce}=1`)}));const he=Y((()=>{const e=function(){try{return sessionStorage}catch(e){return null}}();return e&&!!e.getItem(ae)}));const ge=Y((()=>{const e=ne();return e&&\"1\"===e.getItem(de)}));const we=Y((()=>{const e=ne(),t=!re(\"prg-noredirect\")&&e&&\"1\"===e.getItem(le)&&function(){const e=document.head.dataset.clientSettings||\"\",{browser:t}=w(e)||{},{browserType:n=\"\"}=t||{};return!!/safari/i.test(n)}();return t}));const me=Y((()=>{const e=ne();return e&&\"1\"===e.getItem(ue)}));const fe={bingHomepageMobile:\"binghomepagemobile\",outlookMobile:\"OnOOutlookMobile\",officeMobile:\"OnOOfficeMobile\",sapphire:\"OnOStartApp\",skype:\"OnOSkype\",freFullPage:\"emmxFre\",winWeatherApp:\"weather-app-win\",xiaomiApp:\"xmweather-\",launcher:\"launcherntp\",launcherInterests:\"launcherInterests\",moto:\"moto\",swiftKey:\"swiftKey\",winMoneyApp:\"finance-app-win\"},be=\"superappdhp\",ve=[\"ios\",\"android\",\"ipados\"];let ye=We(\"ocid\").toLowerCase(),ke=We(\"chpltfm\"),[Se,Ce]=ke.toLowerCase().split(\"-\");function Ae(){if(!o()||Pe())return!1;const e=window.sapphireWebViewBridge,t=window.webkit,n=e&&e.send,i=t&&t.messageHandlers&&t.messageHandlers.send&&t.messageHandlers.send.postMessage;return!(!n&&!i)}function Ie(){const e=ke&&\"outlook\"===Se&&ve.includes(Ce),t=ye===fe.outlookMobile.toLowerCase();return e||t}function _e(){const e=ke&&[\"office\",\"union\"].includes(Se)&&ve.includes(Ce),t=ye===fe.officeMobile.toLowerCase();return e||t}function qe(){const e=ye===fe.skype.toLowerCase()||\"skype\"===Se;return o()&&(null===(t=window.skypeWebviewBridge)||void 0===t?void 0:t.isSkype)||e;var t}function Oe(){return!(!o()||Pe())&&(Ae()||ye===fe.sapphire.toLowerCase()||ye===be.toLowerCase())}function Ee(){const e=We(\"edge\");return(\"emmx\"===Se||\"1\"===e)&&Oe()||\"mmx\"===ye}function Le(){return\"3rdp\"===Se||ye.startsWith(fe.xiaomiApp)||xe()||Pe()}function xe(){return ye===fe.moto.toLowerCase()}function Te(){return[fe.launcher.toLowerCase(),fe.launcherInterests.toLowerCase()].includes(ye)&&Oe()}function Pe(){return ye.toLowerCase()===fe.swiftKey.toLowerCase()||\"swiftkey\"==Se.toLowerCase()}function Ne(){return-1!==location.href.indexOf(\"huawei\")||-1!==location.href.indexOf(\"airfind\")||-1!==location.href.indexOf(\"aloha\")||\"vivo\"===ye&&Le()}const Re=Y((()=>Ae()||qe()||_e()||Ie()||Oe()||ye===fe.winWeatherApp.toLowerCase()||ye===fe.winMoneyApp.toLowerCase()||Ee()||Le()||Te()||Pe()||Ne()||-1!==location.href.indexOf(\"metaos=true\")));function We(e){try{return new URL(location.href).searchParams.get(e)||\"\"}catch(e){return\"\"}}const Me=N()||{},De={newsAndInterests:1,windowsNewsbar:1,windowsNewsPlus:1,winWidgets:1,windowsShell:1,windowsShellV2:1,distribution:1,superApp:1,channeldesktop:1,channelmobile:1,edgeMobile:1},Fe={edgeChromium:1},Ue={winWidgets:1},$e={edgeChromium:1,shopping:1,newsAndInterests:1,windowsNewsbar:1,windowsNewsPlus:1,winWidgets:1,windowsShell:1,windowsShellV2:1},Be={edgeChromium:1,channeldesktop:1,channelmobile:1,cgHomePage:1,distribution:1,newsAndInterests:1,windowsNewsbar:1,windowsNewsPlus:1,winWidgets:1,windowsShell:1,windowsShellV2:1,superApp:1,edgeMobile:1},He=Y((()=>Me&&Me.apptype)),je=(Y((()=>o()&&He()in Ue)),Y((()=>o()&&!Re()&&!(He()in De)&&!Ve()&&!ze()))),Je=Y((()=>je()&&!(He()in Fe)));Y((()=>je()&&(!(He()in Fe)||!te()))),Y((()=>je()&&!Ye()&&!Ze()));const Ge=Y((()=>Ze()||Ye())),Ve=Y((()=>{const e=!(He()in $e)&&et()&&function(){const e=new URLSearchParams(o()?window.location.search:l().url.search).get(\"ocid\")||(null==Me?void 0:Me.ocid),t=null==e?void 0:e.toLowerCase(),n=!P.has(t)&&\"homePage\"!==He()&&!T.has(t);return n}()&&!function(){const e=\"windows\"==He()&&\"windowshp\"==(Me&&Me.pagetype)&&(o()?window.location.href:l().url.href).includes(`${R}`);return e}();return e})),ze=()=>o()&&document&&document.body&&document.body.getElementsByClassName(\"peregrine-widgets\").length>0,Ke=Y((()=>re(\"prg-1s-workid\")||re(\"prg-1s-sm-workid\")||re(\"prg-1s-twid\")||re(\"prg-1s-mm-wid-t\")));Y((()=>He()in Be||Ve()));function Xe(){const e=H;if(e)return M.log(\"dual-auth: tryGet1SAuthToken returned live tokens\"),e;const t=J();if(t)return M.log(`dual-auth: tryGet1SAuthToken returned cached tokens that are ${G()}`),t;M.log(\"dual-auth: tryGet1SAuthToken returned empty tokens\")}function Qe(e){const t={};return e&&(t[q]=`Bearer ${e}`),t}function Ye(){return Je()&&pe()||function(){const e=Je()&&!tt()&&!pe()&&ee()&&(\"expired\"==G()||ge());return e}()||he()||we()}function Ze(){return Je()&&!ee()&&me()}const et=Y((()=>{const{browser:e}=Me,{browserType:t=\"\",detected:n={}}=e||{},{browserType:o=\"\"}=n||{};return/edgeChromium/i.test(t)||/edgeAndroid/i.test(t)||/edgeiOS/i.test(t)||/edge/i.test(o)}));Y((()=>Je()&&!Ve()));const tt=()=>{const e=ee();if(!e)return!1;let t;switch(e.account_type){case\"MSA\":t=Je()?\"ace\":\"aace\";break;case\"AAD\":t=Je()?\"WID\":\"APP_WID\";break;default:return!1}return!!K(t)};const nt=\"__core_auth_authHeaderPromiseParts__\",ot=()=>r.get(nt);var it;!function(){let e;const t={promise:new Promise((t=>e=t)),resolver:e};r.set(nt,t)}(),function(e){e.GLOBAL=\"GLOBAL\",e.GCC_MODERATE=\"GCC_MODERATE\"}(it||(it={}));let rt;function st(){return rt}function ct(e){const t=e||st()||\"al_app_anon\",n=at();return!!(n&&([\"anon\",\"al_anon\"].includes(t)&&n.AL_ANON||[\"app_anon\",\"al_app_anon\"].includes(t)&&n.AL_APP_ANON))}const at=Y((()=>w(K(\"AL_STATE\",!0))));const dt=()=>{const e=K(\"elt\",!0),t=e&&w(e),{access_token:n,account_type:o,login_hint:i,region_scope:r}=t||{};return(n||o||i||r)&&t};function lt(e){if(\"undefined\"==typeof window||!window.document||window.isRenderServiceEnv)return;const t=window.trustedTypes;if(!t||!t.createPolicy)return;const n=window.trustedTypePolicies||(window.trustedTypePolicies={});if(n[e])return n[e];const o=t.createPolicy(e,{createScriptURL:e=>{const t=function(e){const t=[\"ntp.msn.com\",\"ntp.msn.cn\",\"assets.msn.cn\",\"assets.msn.com\",\"www.bing.com\",\"assets2.msn.com\",\"assets2.msn.cn\",\"www.clarity.ms\",\"int.msn.com\",\"r.bing.com\",\"business.bing.com\",\"staticview.msn.com\",\"staticview.msn.cn\",document.location.hostname];let n;try{n=new URL(e,location.origin)}catch(e){return\"about:blank#error\"}if(n.hostname&&!t.includes(n.hostname))return\"about:blank#error\";return e}(e);return t}});return n[e]=o,o}class ut{constructor(){this.childCookieName=\"child\",this.domInfo=document.head.dataset.info,this.command=\"command\"}init(e,t){this.clientSettings=e||N(),this.authCookieName=st()||window._authCookieName||\"\",this.bundle=window._webWorkerBundle||\"\",t=t||{};const n=this.buildWebWorkerUrl(this.clientSettings,t),o=this.clientSettings.apptype;let i;if(o&&\"edgeChromium\"===o){if(i=this.tryGetWebWorkerUsingPolicy(n,\"webWorkerUrlPolicy\"),!i)return void console.warn(\"Error in creating trusted types compliant web worker\")}else i=window.webWorker=new Worker(n,{name:\"web-worker\"});const r=window.webWorkerMessages=[];i.onmessage=e=>{r.push(e.data)};const s=window.webWorkerErrors=[];if(i.onerror=e=>{s.push(e)},window.chrome){const e=this.getAudienceMode(this.clientSettings),t=window.webWorker&&window.chrome&&window.chrome.ntpSettingsPrivate;t&&\"function\"==typeof t.getPref&&(t.getPref(\"ntp.news_feed_display\",(e=>{e&&e.value&&i.postMessage({id:\"FeedDisplaySetting\",type:this.command,payload:e.value})})),t.getPref(\"ntp.layout_mode\",(e=>{e&&e.value&&i.postMessage({id:\"LayoutModeSetting\",type:this.command,payload:e.value})})),t.getPref(\"new_device_fre.has_user_seen_new_fre\",(e=>{e&&i.postMessage({id:\"NewUserFre\",type:this.command,payload:e.value})}))),t&&\"function\"==typeof t.getConfigData&&t.getConfigData((e=>{const t=(null==e?void 0:e.enabledFeatures)||[];t.includes(\"msReducedNtpAdsForNewDevice_stage1\")&&i.postMessage({id:\"EdgeNoAds\",type:this.command,payload:!0}),t.includes(\"msReducedNtpAdsForNewDevice_stage2\")&&i.postMessage({id:\"EdgeReduceAds\",type:this.command,payload:!0})})),\"enterprise\"===e&&this.clientSettings.browser&&\"edgeChromium\"===this.clientSettings.browser.browserType&&parseInt(this.clientSettings.browser.version)>=87?this.getEnterpriseAccessToken().then((e=>{e&&i.postMessage({id:\"AuthTokenEnterprise\",type:this.command,payload:Qe(e)})})):i.postMessage({id:\"AuthTokenEnterprise\",type:this.command,payload:{}}),\"adult\"!==e&&\"enterprise\"!==e||this.sendPageConfiguration()}Ge()&&ot().promise.then((e=>i.postMessage({id:\"AuthHeaders\",type:this.command,payload:e||{}})))}buildWebWorkerUrl(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:null,t=arguments.length>1?arguments[1]:void 0;return(this.bundle||\"\")+\"#\"+this.qsFromDocument()+this.qsFromCookies()+this.qsFromDataAttribute(e||this.clientSettings)+this.qsMisc(t)+this.qsAuth(e||this.clientSettings)+this.qsFromUrl()+this.qsFromServiceWorker()+this.qsSelectedPivot()+this.qsXboxXToken()}qs(e,t,n){return e?t+\"=\"+(n||e)+\"&\":\"\"}qsFromDocument(){var e;return this.qs(document.getElementsByTagName(\"html\")[0].getAttribute(\"lang\"),\"lang\")+this.qs(encodeURIComponent(window.location.href),\"adsReferer\")+this.qs(null===(e=o()?window.devicePixelRatio:l().devicePixelRatio)||void 0===e?void 0:e.toString(),\"devicePixelRatio\")}qsFromServiceWorker(){if(!navigator.serviceWorker||!navigator.serviceWorker.controller)return\"\";const e=navigator.serviceWorker.controller;if(\"activated\"!==e.state||!e.scriptURL)return\"\";if(e.scriptURL){const t=new URL(e.scriptURL).searchParams.toString();return t?\"&\"+t+\"&\":\"\"}return\"\"}qsFromCookies(){return this.qs(Q(this.authCookieName),\"aace\",\"1\")+this.qs(Q(\"muid\"),\"muid\")+this.qs(Q(this.childCookieName),\"child\")+this.qs(Q(\"cbypass\"),\"cbypass\")+this.qs(this.cookieBannerConsentRequired().toString(),\"ccRequired\")+this.qs(Q(\"ecasession\"),\"ecasession\")+this.qs(Q(\"TOptOut\"),\"browserOptOut\")}qsFromDataAttributeHelper(e,t){return e&&e.featureFlags&&e.featureFlags.wpoEnabled&&t&&!t.includes(\"remwpofltww\")&&(t=this.appendWpoFlightsToInfo(e,t)),this.qs(e.pcsInfo&&e.pcsInfo.env||e.env,\"env\")+this.qs(e.aid,\"aid\")+this.qs(e.apptype,\"apptype\")+this.qs(e.pagetype,\"pagetype\")+this.qs(e.audienceMode,\"audienceMode\")+this.qs(e.configIndexDocId,\"configIndexDocId\")+this.qs(e.deviceFormFactor,\"deviceFormFactor\")+this.qs(e.domain,\"domain\")+this.qs(e.configRootUrl,\"configRootUrl\")+this.qs(this.getHighestCbid(e.cbid,e.apptype),\"cbid\")+this.qs(e.ocid,\"ocid\")+this.qs(e.os,\"os\")+this.qs(JSON.stringify(e.locale),\"locale\")+this.qs(e.geo_lat,\"lat\")+this.qs(e.geo_long,\"long\")+this.qs(JSON.stringify(e.featureFlags),\"featureFlags\")+this.qs(JSON.stringify(e.browser),\"browser\")+this.qs(JSON.stringify(e.servicesEndpoints),\"servicesEndpoints\")+this.qs(e.bundleInfo&&e.bundleInfo.v||\"\",\"buildVersion\")+this.qs(t,\"dataInfo\")}qsSelectedPivot(){const e=\"selectedPivot\";let t;try{const n=ne();n&&(t=n.getItem(e))}catch(e){console.warn(\"Error getting pivot id from local storage. \"+e)}return this.qs(t,e)}qsXboxXToken(){const e=\"xboxXTokenId\";let t;try{const n=ne();n&&(t=n.getItem(e))}catch(e){console.warn(\"Error getting xbox XToken from local storage. \"+e)}return this.qs(t,e)}appendWpoFlightsToInfo(e,t){const n=this.getWpoFlightsFromLocalStorage(e);if(n&&n.length){const e=\";\",o=t.split(e);for(let e=0;ee.id))}}catch(e){console.warn(`Error getting wpo flights from ls for ${o} Error:${e}`)}return null}getHighestCbid(e,t){try{const n=ne(),o=parseInt(e),i=parseInt(JSON.parse(n.getItem(`cbid_${t||\"\"}`)));let r;if(o&&!isNaN(o)&&i&&!isNaN(i)&&(r=o>i?o:i),r=r||o||i,r)return window._cbid=r.toString()}catch(e){console.warn(\"Error getting highest CBID\"+e)}}getAudienceMode(e){ne();const t=K(this.authCookieName),n=K(this.childCookieName),{child:o}=t&&JSON.parse(t)||{};return t&&(1===o||n)?\"kids\":e.audienceMode||\"adult\"}sendPageConfiguration(){const e=this.clientSettings.pagetype;if(e){const t=K&&K(\"pglt-edgeChromium-\"+e);if(t)return void window.webWorker.postMessage({id:\"PageConfiguration\",type:this.command,payload:t})}window.webWorker.postMessage({id:\"PageConfiguration\",type:this.command,payload:!1})}qsFromDataAttribute(e){let t,n;const o=ne();if(o){const i=JSON.parse(o.getItem(\"shd_\"+e.pagetype)||null);i&&i.clientSettings&&i.info&&(t={...i.clientSettings,bundleInfo:e.bundleInfo},n=i.info)}return(!t||!n||e.pageGenTime>=t.pageGenTime)&&(t=e,n=this.domInfo),t.audienceMode=this.getAudienceMode(t),this.qsFromDataAttributeHelper(t,n)}qsFromUrl(){const e=location.search;return e&&e.length?e.substring(1,e.length):\"\"}getEnterpriseAccessToken(){return new Promise((e=>{window.chrome&&window.chrome.authPrivate?window.chrome.authPrivate.getPrimaryAccountInfo((t=>{t&&\"GCC_MODERATE\"!==t.region_scope||e(\"\");const n={account_id:t.account_id,account_type:t.account_type,client_id:\"d7b530a4-7680-4c23-a8bf-c52c121d2e87\",scope_or_resource:\"https://enterprisenews.microsoft.com\"};window.chrome.authPrivate.acquireAccessTokenSilently(n,(t=>{t.is_success?e(t.access_token):e(\"\")}))})):e(\"\")}))}qsMisc(e){return this.qs(performance.timeOrigin&&performance.timeOrigin.toString(),\"mainTimeOrigin\")+this.qs(e.disableWWConfig&&\"1\",\"disableWWConfig\")}qsAuth(e){let t=this.qs(Je()&&\"1\",\"disableWWAuth\");if(!Ge()){const e=Je()&&Xe()||{};t+=this.qs(encodeURIComponent(JSON.stringify(Qe(e.accessToken))),\"authHeaders\")}return t+=this.qs(ct()&&\"1\",\"isAccountLinked\"),t+=this.qs(this.shouldFetchAppAnonCookie()&&\"1\",\"fetchAppAnonCookie\"),t}getSingleColRequest(e){const t=\"render_single_column\",n=\"feed_layout\";let o,i;for(const r of e){const{key:e,value:s}=r;e===t?o=s:e===n&&(i=s)}return\"boolean\"==typeof o?o:!!i&&(i&&\"single\"===i.type)}cookieBannerConsentRequired(){const e=null!=document.getElementById(\"consent-banner-container\")&&null===K(\"MSCC\"),t=null!=document.getElementById(\"onetrust-sdk\")&&null===K(\"eupubconsent-v2\");return e||t}shouldFetchAppAnonCookie(){if(!et())return!0;const e=(()=>{const e=dt();if(e){if(\"MSA\"===e.account_type)return\"MSA\";if(\"AAD\"===e.account_type)return\"AAD\"}})();return\"MSA\"===e||X(\"lt\")?!X(\"aace\"):\"AAD\"===e?ct()&&!Ke():void 0}tryGetWebWorkerUsingPolicy(e,t){const n=lt(t);if(n){const t=n.createScriptURL(e),o=t&&t.toString();if(!o||\"about:blank#error\"===o)return;window.webWorker=new Worker(t,{name:\"web-worker\"})}else window.webWorker=new Worker(e,{name:\"web-worker\"});return window.webWorker}getFlightList(){return((/^f:\\s*([^;]+)/i.exec(this.domInfo)||{})[1]||\"\").toLowerCase().split(\",\")}}let pt,ht,gt;function wt(){if(ht)return ht;const e=document.head.getAttribute(\"data-info\");return ht=((/f:\\s*([^;]+)/i.exec(e)||{})[1]||\"\").toLowerCase(),ht}var mt;!function(e){e[e.Alert=0]=\"Alert\",e[e.Deprecated=1]=\"Deprecated\",e[e.HighImpact=2]=\"HighImpact\",e[e.Critical=3]=\"Critical\"}(mt||(mt={}));const ft=new class{constructor(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:20;this.maxLength=e,this.list=[]}push(e){this.list.push(e),this.list.length>this.maxLength&&this.list.shift()}get data(){return this.list}};function bt(e,t,n){let i=arguments.length>3&&void 0!==arguments[3]?arguments[3]:mt.Alert,r=arguments.length>4?arguments[4]:void 0,s=arguments.length>5?arguments[5]:void 0;try{const c=function(){if(!pt){const e=document.head.getAttribute(\"data-client-settings\");e&&(pt=JSON.parse(e))}return pt}(),a=wt(),d=function(e){if(e){const{pcsInfo:t,pageGenTime:n}=e,o=new Date(n).getTime(),i=!t||[\"prod\",\"prod-ssr\",\"prod-ssrntp\"].includes(t.env);gt=i?\"browser.events.data.msn.com\":\"events-sandbox.data.msn.com\";return{cors:\"true\",\"content-type\":\"application/x-json-stream\",\"client-id\":\"NO_AUTH\",\"client-version\":\"1DS-Web-JS-2.2.2\",apikey:i?\"0ded60c75e44443aa3484c42c1c43fe8-9fc57d3f-fdac-4bcf-b927-75eafe60192e-7279\":\"f8857dedc6f54ca8962cfb713e01e7d7-e9250191-fe0b-446f-95ae-07516262f98c-7028\",\"upload-time\":o,w:\"0\",anoncknm:\"app_anon\"}}return null}(c);let l=\"\";d&&d.apikey&&\"\"!==d.apikey&&(l=function(e){if(e){const t=e.indexOf(\"-\");if(t>0)return e.substring(0,t)}return\"\"}(d.apikey));const u=function(e,t,n,i,r,s){let c=arguments.length>6&&void 0!==arguments[6]?arguments[6]:mt.Alert;if(n){r=r||{};const{apptype:d,audienceMode:l,pagetype:u,pageGenTime:p,bundleInfo:h,deviceFormFactor:g=\"\",fd_muid:w,os:m}=n;r.pageGenTime=p,r.build=h&&h.v,r.appType=d;const f=function(e,t,n){const i=n&&\"phone\"===n.toLowerCase(),r=t&&\"enterprise\"===t;let s=\"Edge\";Pe()?s=\"swiftKey\":xe()?s=\"moto\":Te()?s=\"Launcher\":function(){var e;if(!o())return!1;const t=document.head.dataset.clientSettings||(null===(e=document.getElementsByClassName(\"peregrine-widget-settings\")[0])||void 0===e?void 0:e.getAttribute(\"data-client-settings\"));if(t){const e=JSON.parse(t);return e&&e.pagetype&&\"bingHomepageMobile\"===e.pagetype}return!1}()&&(s=\"bingHomepageMobile\");const c={bingHomepage:\"binghomepage\",mmx:\"emmx\",edge:\"spartan\",edgeChromium:r?\"entnews\":\"anaheim\",hybrid:\"spartan\",hub:i?\"prime_mobile\":\"prime\",microsoftNews:\"msnews\",office:\"entnews\",views:i?\"prime_mobile\":\"prime\",homePage:i?\"prime_mobile\":\"prime\",windowsShell:\"windowsshell\",edgeMobile:s};return c[e]}(d,l,g),b=f||d,v=document.getElementsByTagName(\"html\")[0].getAttribute(\"lang\");let y,k=\"\",S=\"muid\";try{if(\"edgeChromium\"===d&&\"object\"==typeof window&&window.location&&window.location.search){const e=new URLSearchParams(window.location.search);y=e.has(\"startpage\")?\"msedgdhp\":\"msedgntp\",\"enterprise\"===l?y=\"entnewsntp\":\"xbox\"===m&&(y=\"xboxntp\")}window&&window.getCookieConsentRequired&&\"function\"==typeof window.getCookieConsentRequired&&window.getCookieConsentRequired()||(k=K(\"muid\"))}catch{}k||(k=n.aid,S=\"aid\");const C={name:\"MS.News.Web.AppError\",time:p,ver:\"4.0\",iKey:`o:${i}`,data:{baseData:{},baseType:\"MS.News.Web.Base\",page:{name:\"default\",product:b,type:yt(u),content:vt[d]??{category:\"standaloneError\"},ocid:y},browser:{clientId:k,clientIdType:S},flight:{id:s},request:{activityId:n.aid,requestId:n.aid,afdMuid:w},locale:{mkt:v},extSchema:{id:e,severity:c,pb:r,message:t}}};var a;if(k&&\"muid\"===S)C.ext={...null==C?void 0:C.ext,user:{...null==C||null===(a=C.ext)||void 0===a?void 0:a.user,localId:`t:${k}`}};return C.data.flight.tmpl=\"\",\"object\"==typeof window&&(window.isSSREnabled&&(C.data.flight.tmpl+=\";ssr-enabled:1\"),window.isSSRCompleted&&(C.data.flight.tmpl+=\";ssr-completed:1\")),C?JSON.stringify(C):null}return null}(t,e,c,l,n,a,i);if(s&&!function(e){if(null==e)return!1;return e.startsWith(\"1\")||e.startsWith(\"2\")||e.startsWith(\"3\")||e.startsWith(\"4\")}(null==c?void 0:c.aid))return console.error(u),void console.error(`This App error Id: ${t} will not be sent due to app error sampling!`);if(d&&u){console.error(u),ft.push(u);const e=\"https://\"+gt+\"/OneCollector/1.0\"+function(e){return\"?\"+Object.keys(e).map((function(t){return t+\"=\"+encodeURIComponent(e[t])})).join(\"&\")}(d);if(r&&!performance.getEntriesByType(\"visibility-state\").some((e=>\"visible\"===e.name))){const t=()=>{\"visible\"===document.visibilityState&&navigator.sendBeacon(e,u)};document.addEventListener(\"visibilitychange\",t,{once:!0})}else navigator.sendBeacon(e,u)}}catch{}}const vt={homePage:{vertical:\"homepage\",category:\"\",id:\"\",domainId:\"13041\",title:\"undefined\"!=typeof document?document.title:\"\"}};function yt(e){let t=e;switch(e){case\"windowsshellhp\":t=\"dhp\";break;case\"video\":t=\"watch\";break;case\"EdgeMobile\":t=Ee()?\"ntp\":\"dhp\"}return t}function kt(e){return new Promise((t=>setTimeout(t,e)))}const St=\"https://acdn.adnxs.com/ast/ast.js\";async function Ct(){window.apntag=window.apntag||{anq:[]},window.apntag.anq=window.apntag.anq||[],await async function(e){if(!o())return;const{src:t,id:n,async:i=!0,container:r=document.head,retryNumber:s=0,retryDelayMs:c=0}=e;try{await async function(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:3,n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:100;if(tfunction(e,t,n){let i=!(arguments.length>3&&void 0!==arguments[3])||arguments[3];return new Promise(((r,s)=>{const c=function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:void 0,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:void 0,n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2],i=arguments.length>3&&void 0!==arguments[3]?arguments[3]:\"anonymous\",r=arguments.length>4?arguments[4]:void 0,s=arguments.length>5?arguments[5]:void 0;if(!o())throw new Error(\"createScriptTag is unsupported server side. This call should be wrapped in canUseDOM().\");const c=document.createElement(\"script\");return t&&(c.id=t),r&&c.setAttribute(\"class\",r),s&&(c.onload=s),c.type=\"text/javascript\",e&&(window&&window.trustedTypesURLValidation?c.src=window.trustedTypesURLValidation(e,\"scriptSrcUrlPolicy\"):c.src=e,c.async=n,c.crossOrigin=i),window&&window.NONCE_ID&&(c.nonce=window.NONCE_ID),c}(e,t,i,void 0,void 0,(()=>r(!0)));c.crossOrigin=void 0,c.onerror=s,n.prepend(c)}))}(t,n,r,i)),s,c)}catch(e){bt(\"Failed to download Static JS\",11103,{scriptId:n,scriptSrc:t},mt.Alert)}}({id:\"display-sdk\",src:St,container:document.head,retryDelayMs:200,retryNumber:5})}const At=N();function It(e){return((null==At?void 0:At.ocid)||\"\").toLowerCase().includes(e.toLowerCase())}function _t(){return It(\"superappdhp\")}function qt(){const e=It(\"skype\")&&(t=\"prg-rr-skype-rv\",!((null===(n=document.head)||void 0===n||null===(n=n.getAttribute(\"data-info\"))||void 0===n?void 0:n.split(\";\").find((e=>e.startsWith(\"f:\"))))||\"\").includes(t))&&!(It(\"msnaroverlay\")||\"phone\"===(null==At?void 0:At.deviceFormFactor));var t,n;const o=It(\"skype\")&&\"video\"===(null==At?void 0:At.pagetype);_t()||(It(\"skype\")||It(\"office\")||It(\"outlook\"))&&!e&&!o||Ct()}const Ot=\"experience\",Et=Object.create(null),Lt=2;function xt(){const e=[\"vendors\",\"microsoft\",\"common\"];window.addEventListener(\"load\",(()=>{window._pageTimings.navType=Pt(),window.requestIdleCallback((()=>{const t=function(e){const t=window.viewsWebpackChunks;if(!t)return[...e,Ot];if(t.length>e.length)return[];t.forEach((t=>e.splice(e.indexOf(t[0][0]),1))),t.push===Array.prototype.push&&e.push(Ot);return e}(e);t.length&&t.forEach((e=>Tt(e)))}))}),{once:!0})}function Tt(e,t){if(!(t=t||Array.from(document.scripts).find((t=>t.src.indexOf(`/${e}.`)>-1))))return;const n=function(e,t){const n=document.createElement(\"script\");return n.type=\"text/javascript\",n.crossOrigin=\"anonymous\",n.src=e,n.onerror=()=>{let e=Et[t]||0;Et[t]=++e,e{delete Et[t];const e=window._pageTimings.retriedBundles;window._pageTimings.retriedBundles=e?`${e},${t}`:t,Nt()},n}((o=t.src,Et[e],o),e);var o;setTimeout((()=>t.replaceWith(n)),100)}function Pt(){const[e={}]=performance.getEntriesByType(\"navigation\");return e.type}function Nt(){if(!Object.values(Et).every((e=>e===Lt)))return;let e=\"\";Object.keys(Et).forEach((t=>{e=e?`${e},${t}`:t,delete Et[t]})),e&&bt(`Error when loading bundle(s): ${e}`,20202,{timeElapsed:Math.round(performance.now()),navType:Pt()})}function Rt(e,t){if(e instanceof Error){const n=e.toString();return t&&e.stack?`${n}, Stack: ${e.stack}`:n}return\"string\"==typeof e?e:function(e){try{return void 0===e?\"undefined\":JSON.stringify(e)}catch{return\"\"}}(e)}\"undefined\"!=typeof window&&window.document&&window.document.createElement&&(window._pageTimings=window._pageTimings||{},window.requestIdleCallback=window.requestIdleCallback||window.setTimeout);!function(){xt();const e=new Set([\"Script error.\",\"ResizeObserver loop limit exceeded\",\"ResizeObserver loop completed with undelivered notifications.\"]);window.onerror=function(t,n,o,i,r){if(e.has(t))return;bt(\"JS Exception\",20203,{source:n,line:o,message:t,stack:Rt(r,!0),currentUrl:window.location.href,column:i},mt.Alert)}}();const{all:Wt}=C(),Mt=null==Wt?void 0:Wt.includes(\"icrscall-views\");Mt&&qt();const{inlineCRS:Dt}=function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};const t={};try{var n,i,r;if(!o())return t;const s=w(A(\"clientSettings\"))||{};let c=null===(n=s.apptype)||void 0===n?void 0:n.toLocaleLowerCase();const a=e.crsOptions||{};\"winwidgets\"!==c&&\"homepage\"!==c||(a.crsPrefetch=!0),null!==(i=s.widgetAttributes)&&void 0!==i&&null!==(i=i.clientPerf)&&void 0!==i&&i.earlyCRSCall&&(a.crsPrefetch=!0),\"edgechromium\"===c&&(c=\"edgechr\");const d=!(null===(r=s.widgetAttributes)||void 0===r||null===(r=r.clientPerf)||void 0===r||!r.shouldUsePWOnlyFlights),{all:u,prg:p}=C(d,c);(a.crsPrefetch||u.includes(\"inlinecrscall\")||u.includes(`icrscall-${c}`))&&(!function(e,t,n){const i=function(e,t){let n;const o=decodeURIComponent(g(e,\"aace\")),{child:i}=o&&w(o)||{},r=g(e,h);return n=o&&(1===i||r)?\"kids\":t.audienceMode||\"adult\",n}(document.cookie,e),{apptype:r,browser:s,domain:c,bundleInfo:{v:a}={},deviceFormFactor:d,locale:{content:u,display:p}={},ocid:m,os:f,pagetype:b,servicesEndpoints:{crs:v}={}}=e,y=(_()?o()?window.location.hostname:l().url.hostname:c)||\"\";if(!r||!a||!v)return;const k={audienceMode:i,browser:s,deviceFormFactor:d,domain:y,locale:{content:u,display:p},ocid:m,os:f,platform:\"web\",pageType:b,pageExperiments:t},S=new URLSearchParams([[\"expType\",\"AppConfig\"],[\"expInstance\",\"default\"],[\"apptype\",r],[\"v\",a]]),C=`${v.path}${v.v}/config/?${S}&targetScope=${JSON.stringify(k)}`,A=e=>{const t=new URL(`https://${e}${C}`),{timeout:o=5e3}=n;let i;if(window.crsRequestData={url:t.toString()},o>=0){const e=new AbortController;setTimeout((()=>e.abort()),o),i=e.signal}return fetch(t,{signal:i})},I=\"msn.\"+(function(e,t){return!!(e.hostname.toLowerCase().endsWith(\".cn\")||t.isChinaCompliance||t.useCnDomain||e.search.match(/item=revip:cn/))}(location,e)?\"cn\":\"com\"),{domain:q}=v,O=q&&new URL(q).host;let E=A(O||`assets.${I}`);n.disableFallback||(E=E.catch((()=>{const{hostname:e}=location,t=e.match(/\\.msn\\.(com|cn)$/)?e:`www.${I}`;return A(t)})));window.crsPromise=E}(s,p,a),t.inlineCRS=!0)}catch(e){console.error(\"InlineHeadCommon: \"+e)}return t}();var Ft;Ft={disableWWConfig:Dt},_t()||(new ut).init(void 0,Ft),Mt||qt()}();if(matchMedia(\"(prefers-color-scheme:dark)\").matches){const s=document.createElement(\"style\");document.head.appendChild(s);s.type=\"text/css\";s.appendChild(document.createTextNode(\"body{background:#242424}\"));}const _clientSettings = JSON.parse(document.head.dataset.clientSettings);function appendProtocolQSP(url){return url;}function appendScript(url,async,defer,fetchPriority=\"\"){let s=document.createElement(\"script\");s.type=\"text/javascript\";const akmurl=appendProtocolQSP(url);const w=window.trustedTypesURLValidation;s.src=w?w(akmurl,\"bundleUrlPolicy\"):akmurl;if(async){s.async=true;}if(defer){s.defer=true;}if(fetchPriority){s.fetchPriority=fetchPriority;}s.onerror=()=>onErrorHandler(s.src);s.nonce=\"\";s.crossOrigin=\"anonymous\";document.body.appendChild(s);}:root{--base-layer-luminance: 1;--neutral-layer-1: #ffffff;--fill-color: #ffffff;--neutral-foreground-rest: #2b2b2b;--neutral-stroke-rest-delta: 25;--neutral-stroke-hover-delta: 40;--neutral-stroke-active-delta: 16;--neutral-stroke-focus-delta: 25;--neutral-stroke-rest: #bebebe;--neutral-stroke-hover: #979797;--neutral-stroke-active: #d5d5d5;--neutral-stroke-focus: #bebebe;--type-ramp-base-font-size: 14px;--type-ramp-base-line-height: 20px;--direction: ltr;--base-height-multiplier: 8;--base-horizontal-spacing-multiplier: 3;--density: 0;--design-unit: 4;--stroke-width: 1;--focus-stroke-width: 2;--neutral-stroke-divider-rest-delta: 8;--neutral-stroke-divider-rest: #eaeaea;--body-font: Segoe UI, Segoe UI Midlevel, sans-serif;--accent-foreground-rest-delta: 0;--accent-foreground-hover-delta: 6;--accent-foreground-active-delta: -4;--accent-foreground-focus-delta: 0;--accent-foreground-rest: #0078d4;--accent-foreground-hover: #0066b4;--accent-foreground-active: #1181d7;--accent-foreground-focus: #0078d4;--neutral-fill-rest-delta: 7;--neutral-fill-hover-delta: 10;--neutral-fill-active-delta: 5;--neutral-fill-focus-delta: 0;--accent-fill-rest-delta: 0;--accent-fill-hover-delta: 4;--accent-fill-active-delta: -5;--accent-fill-focus-delta: 0;--accent-fill-rest: #0078d4;--accent-fill-hover: #006cbe;--accent-fill-active: #1683d8;--accent-fill-focus: #0078d4;--neutral-fill-rest: #ededed;--neutral-fill-hover: #e5e5e5;--neutral-fill-active: #f2f2f2;--neutral-fill-focus: #ffffff;--control-corner-radius: 4;--layer-corner-radius: 4;--disabled-opacity: 0.3;--type-ramp-minus-2-font-size: 10px;--type-ramp-minus-2-line-height: 14px;--type-ramp-minus-1-font-size: 12px;--type-ramp-minus-1-line-height: 16px;--type-ramp-plus-1-font-size: 16px;--type-ramp-plus-1-line-height: 22px;--type-ramp-plus-2-font-size: 20px;--type-ramp-plus-2-line-height: 28px;--type-ramp-plus-3-font-size: 24px;--type-ramp-plus-3-line-height: 32px;--type-ramp-plus-4-font-size: 28px;--type-ramp-plus-4-line-height: 36px;--type-ramp-plus-5-font-size: 32px;--type-ramp-plus-5-line-height: 40px;--type-ramp-plus-6-font-size: 40px;--type-ramp-plus-6-line-height: 52px;--neutral-fill-input-rest-delta: 0;--neutral-fill-input-hover-delta: 0;--neutral-fill-input-active-delta: 0;--neutral-fill-input-focus-delta: 0;--neutral-fill-layer-rest-delta: 3;--neutral-fill-stealth-rest-delta: 0;--neutral-fill-stealth-hover-delta: 5;--neutral-fill-stealth-active-delta: 3;--neutral-fill-stealth-focus-delta: 0;--neutral-fill-strong-rest-delta: 0;--neutral-fill-strong-hover-delta: 8;--neutral-fill-strong-active-delta: -5;--neutral-fill-strong-focus-delta: 0;} :host { --fill-color: #ffffff;--neutral-layer-1: #ffffff;--neutral-foreground-rest: #2b2b2b;--neutral-stroke-rest: #bebebe;--neutral-stroke-hover: #979797;--neutral-stroke-active: #d5d5d5;--neutral-stroke-focus: #bebebe;--neutral-stroke-divider-rest: #eaeaea;--accent-foreground-rest: #0078d4;--accent-foreground-hover: #0066b4;--accent-foreground-active: #1181d7;--accent-foreground-focus: #0078d4;--accent-fill-rest: #0078d4;--accent-fill-hover: #006cbe;--accent-fill-active: #1683d8;--accent-fill-focus: #0078d4;--neutral-fill-rest: #ededed;--neutral-fill-hover: #e5e5e5;--neutral-fill-active: #f2f2f2;--neutral-fill-focus: #ffffff;--base-layer-luminance: 1;--direction: ltr; } :host { background-color: var(--fill-color); color: var(--neutral-foreground-rest); }:host([hidden]){display:none}:host{display:block} body{margin:0}.page-wrapper{position:relative;background:var(--fill-color,#FFFFFF)}.consumption-content{position:relative;max-width:100vw}.mcp-article-page-container{position:relative;padding:0 16px}.mcp-gallery-page-container{margin-top:12px}.breaking-news-banner{font-family:var(--body-font)}.consumption-error-page{margin:10px;font-size:20px;font-family:var(--body-font)}.consumption-error-page h1{font-size:25px}.content-body{display:flex;flex-direction:column;align-items:center}.content-with-RR{display:flex;width:100%;justify-content:space-evenly}.views-right-rail-container{border-inline-start:none;padding-bottom:16px} .header{width:100%;min-height:49px;position:sticky;top:0;z-index:500;background-color:var(--fill-color,#FFFFFF)} .consumption-header{position:relative;z-index:300;width:100%}.views-header-container{position:relative;padding:16px 16px 0;background:transparent;font-family:var(--body-font)}.views-header-container-full-width{padding:0}.views-header-gradient{height:157px;top:0px;margin-top:0px;width:100%;position:absolute}views-header-wc{--mobile-follow-btn-radius:2px} .banner-container{display:flex;justify-content:center;background:var(--fill-color,#FFFFFF)}.banner-ad{display:flex;justify-content:center}.banner-ad.loading-state{position:absolute}.banner-placeholder{display:flex;justify-content:center;align-items:center;height:71px;width:320px;background:rgba(0,0,0,0.06);color:var(--neutral-stroke-rest,rgba(255,255,255,0.55));font-size:12px} .end-of-content-block{margin:16px;clear:both}.end-of-content-block-leadgen{margin:0;width:100%}.mcp-feed-container{width:100vw;padding-bottom:20px;background:#f7f7f7}.mcp-feed-container h2{font-weight:700;text-align:center;padding-top:20px;font-family:var(--body-font)} .comment-container{position:absolute} .actiontray-container{position:absolute;z-index:301;background:var(--fill-color,#FFFFFF)} @media (min-width: 656px) and (max-width: 979px){.consumption-content,.consumption-header,.end-of-content-block,.end-of-content-block-leadgen{width:612px}}@media (min-width: 980px) and (max-width: 1303px){.consumption-content,.consumption-header,.end-of-content-block,.end-of-content-block-leadgen{width:534px}.views-right-rail-container{border-inline-start:1px solid rgba(255,255,255,0.06);min-width:332px}}@media (min-width: 1304px){.consumption-content,.consumption-header,.end-of-content-block,.end-of-content-block-leadgen{width:720px}}@media (min-width: 980px){.views-right-rail-container{border-inline-start:1px solid rgba(255,255,255,0.06);min-width:332px}} :host{--common-header-logo-color:#737373;--subtle-background-hover:rgba(0,0,0,3.92%);--hover-color:#1679c3}:host(:focus){outline:none}.root-container{display:grid;width:auto;margin:auto}.top-container{align-items:center;display:grid;grid-column:1;grid-gap:10px;padding:10px 0;margin:0 24px;justify-content:space-between}.logo{cursor:pointer;grid-row:1;grid-column:1;width:fit-content}.logo a{display:flex;outline:none;text-decoration:none;color:inherit}.logo a:focus-visible{border:calc(var(--stroke-width) * 1px) solid var(--focus-stroke-outer);margin:-1px -1px 0px;box-shadow:0 0 0 calc((var(--focus-stroke-width) - var(--stroke-width)) * 1px) var(--focus-stroke-outer)}.logo-container{display:flex;outline:none;flex-direction:row;align-items:center;font-size:24px;direction:ltr}.logo-container.with-hover:hover .start-logo,.logo-container.with-hover:hover .logo-text{display:none}.logo-container.with-hover:hover .logo-hover-text,.logo-container.with-hover:hover .start-logo-hover-text,.logo-container.with-hover:hover .start-logo-hover-icon{display:inline-block}.logo-container.with-hover:hover .MSNButterflyLogo{background-color:var(--hover-color)}.logo-container.with-hover:hover .logo-hover-text{color:var(--hover-color)}.logo-container.with-hover .logo-hide{display:none}.microsoftStartLogo{width:133px;height:20px;background:var(--start-logo-light) no-repeat center}.MSNButterflyLogo{background-color:var(--neutral-foreground-rest);height:32px;width:24px;display:block;mask:var(--butterfly-logo) center no-repeat;-webkit-mask:var(--butterfly-logo) center no-repeat}.logo-text,.logo-hover-text{margin:-3px 0px 0px 4px;font-weight:600}.start-logo-hover-text{min-width:108px;font-size:16px;font-weight:600;line-height:32px;margin-left:5px;color:#737373}.start-logo-hover-icon{width:20px;text-align:center}.start-logo-hover-icon img{min-width:16px;filter:drop-shadow(#737373 0px 1000px);transform:translateY(-1000px)}.start-logo-hover-icon,.start-logo-hover-text,.logo-hover-text{display:none}.hamburger-logo{height:20px;margin-inline-end:0;margin-inline-start:24px}.skip-to-link{border:1px solid var(--neutral-stroke-rest);border-radius:100px;clip:rect(0,0,0,0);color:var(--neutral-foreground-rest);fill:var(--neutral-foreground-rest);position:fixed}.skip-to-link:active,.skip-to-link:focus{clip:auto}.search-container{height:46px;justify-self:center}.me-control-container{width:100%;min-width:44px}.me-control-container.sign-in-btn{min-width:88px}.contextual-nav-container{flex-grow:1;width:calc(100% - 44px)}.nav-container{align-items:center;display:flex;flex-direction:row;height:46px;grid-column:1;justify-self:center;transition:all 0.2s ease 0s;width:inherit}.secondary-nav-container{align-items:center;flex-direction:row;grid-column:1;width:inherit;margin:auto;padding:6px 0}.hamburger-menu-container{margin-inline-start:-8px;margin-inline-end:12px;min-width:32px;position:relative;z-index:500}.user-pref-container{display:flex;flex-direction:row;align-items:center;justify-content:flex-end;grid-row:1;grid-column:3;justify-self:end;min-height:48px;gap:0px}.edit-icon{display:flex}.settings-link{border:1px solid var(--neutral-stroke-rest);border-radius:100px;padding:0;margin:0;margin-inline-start:15px;font-size:var(--type-ramp-base-font-size);line-height:var(--type-ramp-base-line-height);cursor:pointer;color:var(--neutral-foreground-rest);fill:var(--neutral-foreground-rest);height:30px}.responsive-btn .settings-link{margin-inline-start:10px}.show-streaks{margin-inline-start:5px}.wide-endsection::part(end){width:inherit;display:block}.end{margin-inline-start:15px;display:flex;min-width:max-content}fluent-anchor::part(start){display:flex;margin-inline-end:11px}@media (max-width:415px){.top-container{margin:0 16px}}@media (max-width:955px){.top-container{grid-row:1;grid-template:repeat(2,auto) / repeat(3,1fr)}.search-container{grid-row:2;grid-column:1 / 4}.nav-container{grid-row:3;width:612px}.secondary-nav-container{grid-row:4;width:612px}.settings-link{margin-inline-start:5px}}@media (max-width:643px){.root-container{grid-template-rows:repeat(3,auto)}.top-container{grid-row:1;grid-template:repeat(2,auto) / repeat(3,1fr);width:inherit}.search-container{grid-row:2;grid-column:1 / 4}.secondary-nav-container{width:343px}.nav-container{width:343px}}@media (max-width:343px){.root-container{display:block}.nav-container{width:100%}}@media (min-width:644px) and (max-width:955px){.root-container{grid-template-rows:repeat(3,auto)}}@media (min-width:956px){.root-container{grid-template-rows:repeat(2,auto)}.top-container{grid-row:1;grid-template:auto / repeat(3,1fr)}.search-container{grid-row:1;grid-column:1 / 4}.nav-container{grid-row:2;margin:auto;width:924px}.secondary-nav-container{grid-row:3;width:924px}}@media (min-width:1268px){.nav-container,.secondary-nav-container{width:1236px}}@media (min-width:1580px){.nav-container{width:1548px}}cs-common-settings-dialog{position:relative}cs-common-settings-dialog::part(content){overflow:visible}cs-market-settings::part(title){font-size:var(--type-ramp-base-font-size);line-height:var(--type-ramp-base-line-height);font-weight:400;padding-top:12px;padding-bottom:0}cs-market-settings::part(content){padding-bottom:12px}.overflow{visibility:hidden;position:absolute}.header-divider{margin-bottom:0}cs-common-settings-dialog.layout-no-border::part(heading){border-bottom:0px}.nav-divider{margin:0} .mobile-container{grid-gap:0;padding:0}.mobile-logo{margin-inline-start:32px}} .user-pref-container welcome-greeting-light::part(wea-popup-area),.user-pref-container welcome-greeting::part(wea-popup-area),.user-pref-container welcome-greeting::part(visible-hovercard-container){left:initial;right:0}:host { --fill-color: #ffffff;--neutral-layer-1: #ffffff;--neutral-foreground-rest: #2b2b2b;--neutral-stroke-rest: #bebebe;--neutral-stroke-hover: #979797;--neutral-stroke-active: #d5d5d5;--neutral-stroke-focus: #bebebe;--neutral-stroke-divider-rest: #eaeaea;--accent-foreground-rest: #0078d4;--accent-foreground-hover: #0066b4;--accent-foreground-active: #1181d7;--accent-foreground-focus: #0078d4;--accent-fill-rest: #0078d4;--accent-fill-hover: #006cbe;--accent-fill-active: #1683d8;--accent-fill-focus: #0078d4;--neutral-fill-rest: #ededed;--neutral-fill-hover: #e5e5e5;--neutral-fill-active: #f2f2f2;--neutral-fill-focus: #ffffff; } :host { background-color: var(--fill-color); color: var(--neutral-foreground-rest); }:host([hidden]){display:none}:host{display:block} .providerInfo{display:flex;font-family:'Segoe UI','SegoeUI','Roboto',sans-serif,margin-bottom:2px}.providerContainer{align-items:center;display:inherit}.newProviderBannerContainer.largeClickArea .providerContainer::after{content:\"\";position:absolute;top:0;right:0;bottom:0;left:0}.providerInfoWrapper,.providerLogoAndName,.providerLogoAndName a{display:flex;align-items:center}.providerLogoAndName{flex:0 1 fit-content}.providerLogoAndName:hover .providerName,.providerLogoAndName:hover .providerNameLarge,.providerLogoAndName:hover .providerNameSm{text-decoration:underline}.providerLogoAndNameNoHover:hover .providerName,.providerLogoAndNameNoHover:hover .providerNameLarge,.providerLogoAndNameNoHover:hover .providerNameSm{text-decoration:none}.providerInfoWrapper{margin:0 auto}.providerLogo{height:36px;min-width:36px}.providerLogoLarge{height:48px;min-width:48px}.providerLogo img,.providerLogoLarge img,.providerLogoSm img{border-radius:4px;display:block;margin-right:auto;background-color:#fff;-ms-high-contrast-adjust:none}.providerName{color:var(--neutral-foreground-rest);font-size:18px;line-height:36px;margin:0 10px;word-break:keep-all}.providerNameLarge{color:var(--neutral-foreground-rest);font-size:20px;font-weight:600;line-height:48px;margin:0px 16px 0px 10px;word-break:keep-all}.providerNameSm{color:var(--neutral-foreground-rest);font-size:16px;line-height:22px;margin:0 8px;font-weight:600;word-break:keep-all}.providerInfoWrapper a:link,.providerInfoWrapper a:visited,.providerInfoWrapper a:hover,.providerInfoWrapper a:active{color:white;text-decoration:none;cursor:pointer}.viewsInfo{font-family:'Segoe UI','SegoeUI','Roboto',sans-serif;text-align:center;width:100%}.viewsInfo.viewsInfoInnerPadding{padding:0 16px;width:auto}.viewsHeader h1{color:var(--neutral-foreground-rest);font-family:'Eb Garamond';font-size:36px;font-weight:normal;line-height:40px;margin:0;max-height:160px;overflow:hidden;padding-bottom:3px;padding-top:10px}.readTimeInfo{color:#13A10E;background:url(https://assets.msn.com/staticsb/statics/latest/views/icons/ClockLight.svg) no-repeat;background-position:left center;padding-inline-start:24px;margin-inline-start:4px;pointer-events:none;display:inline-block}@media (min-width: 980px) and (max-width: 1303px){.viewsHeaderText{font-size:28px}}.viewsAttributionDivider{border-bottom:1px solid #878787}.viewsAttribution{color:var(--neutral-foreground-rest);margin:17px auto 20px;min-height:19px;padding-bottom:20px}.viewsAttributionNewRR{display:inline-block}.viewsAuthors{overflow:hidden;white-space:normal;line-height:22px}.viewsOpinionWord{font-weight:bold;font-style:italic}.content{text-align:center}.noContent .viewsAttribution{border-bottom:1px black}.profilePromotionContainer{position:relative;z-index:700}.gradientBackground .providerInfoWrapper{margin:0}.gradientBackground .providerName{font-size:var(--type-ramp-plus-1-font-size);font-weight:600}.gradientBackground .viewsHeader h1{font-size:var(--type-ramp-plus-5-font-size);;font-weight:600;padding-top:12px;padding-bottom:0px}.gradientBackground .viewsInfo,.gradientBackground .viewsHeader h1,.gradientBackground .viewsAttribution{text-align:start}.gradientBackground .content{text-align:start}.gradientBackground .providerName,.gradientBackground .providerNameLarge,.gradientBackground .viewsHeader h1,.gradientBackground .viewsAttribution,.gradientBackground .providerInfoWrapper a:link,.gradientBackground .providerInfoWrapper a:visited,.gradientBackground .providerInfoWrapper a:hover,.gradientBackground .providerInfoWrapper a:active{color:var(--neutral-foreground-rest)}.gradientBackground .viewsAttribution{margin-left:0;margin-right:0;opacity:0.8;margin-top:14px;margin-bottom:22px;padding-bottom:0px}.newProviderBannerContainer{padding:20px;border-radius:8px;outline:1px solid rgba(0,0,0,0.06)}.newProviderBannerContainer.fullWidth{border-radius:0;outline:none;padding:16px}.newProviderBannerContainer.fullProviderLogo:not(.fullWidth){padding:24px}.newProviderBannerContainer.largeClickArea{position:relative}.newProviderBannerBgLight{background-color:#fafafa}.newProviderBannerBgDark{background-color:#1f1f1f}.providerLogoSm{height:32px;width:32px}.providerLogoFull{height:24px}.providerLogoFull img{min-width:24px;height:24px;object-fit:contain}.providerInfoSpace{gap:8px}.providerInfoSpace.fullProviderLogo{gap:16px}.providerInfoSpace>:last-child{flex:1 0 250px}.disabled{pointer-events:none}@media (max-width: 655px){.gradientBackground .viewsHeader h1{font-size:24px;padding-top:8px;padding-bottom:3px}.gradientBackground .viewsAttribution{margin-top:0px;margin-bottom:2px}.viewsHeader h1{max-height:320px;font-size:24px;line-height:28px}.viewsAttribution{font-size:14px;line-height:20px;padding-bottom:0}.providerInfoWrapper{margin:0}.providerName{line-height:24px;overflow:hidden;max-width:calc(100vw - 90px - 32px - 36px - 20px)}.providerNameWithNoLogo{max-width:calc(100vw - 90px - 32px)}.providerNameLarge{line-height:28px;overflow:hidden;max-width:calc(100vw - 90px - 32px - 48px - 20px)}.providerInfoSpace>:last-child{flex:1 0 60px}}@media (min-width: 980px) and (max-width: 1303px){.viewsInfo{width:100%;margin:0 auto}.viewsAttributionLowerCol{width:100%}}@media (min-width: 1304px){.providerInfoWrapper:not(.providerInfoFullSizeWidth){margin:0}.viewsInfo:not(.viewsInfoFullWidth){text-align:initial;width:85%}.viewsInfo:not(.viewsInfoFullWidth,.viewsInset){width:100%}.viewsAttribution:not(.viewsAttributionFullWidth){margin:17px 0 20px}.viewsAttributionLowerCol{width:100%}}@media (hover:hover) and (pointer:fine){.tooltip{position:relative;margin-left:4px;margin-right:4px}.tooltip:before,.tooltip:after{display:block;opacity:0;pointer-events:none;position:absolute;transition:all .15s ease-in-out}.tooltip:after{content:'';height:0;width:0;top:0;right:0}.tooltip:before{background:#fff;border-radius:4px;color:#333;content:attr(data-content);font-size:14px;padding:6px 10px;top:-4px;white-space:nowrap;line-height:normal;box-shadow:0 0 2px 0 #0000001F,0 4px 8px 0 #00000024}.tooltip:hover:after,.tooltip:hover:before,.tooltip:focus-visible:after,.tooltip:focus-visible:before{opacity:1;transform:translate3d(0,0,0)}.tooltip.dateTooltipOnBelow:before{left:50%;transform:translateX(-50%);top:100%}}@media (prefers-color-scheme:dark) and (hover:hover) and (pointer:fine){.tooltip:before{background:#333;color:#fff;box-shadow:0 0 2px 0 #0000003D,0 4px 8px 0 #00000047}}@media screen and (prefers-contrast:more){.tooltip:before{color:highlight}}@media screen and (-ms-high-contrast:white-on-black){.providerLogoFull img{filter:invert(1)}} @media (hover:hover) and (pointer:fine){.tooltip:before{left:35px}.tooltip:before,.tooltip:after{transform:translate3d(3px,0,0)}}Fortune Working in the office 5 days a week to build company culture is a myth, PwC report says Story by Orianna Rosa Royle• 1d .image-ttvr-marker{position:absolute}:root{--article-back-to-feed-forced-color:#ededed}:host{background-color:#FFFFFF;color:var(--neutral-foreground-rest)}.article-cont-read-container{bottom:0;position:absolute;width:100%;z-index:98;left:0;background:linear-gradient(0deg,#FFFFFF 0%,rgba(255,255,255,0) 100%)}.article-cont-read-button-container{align-items:center;display:flex;flex-direction:column;height:120px;justify-content:flex-end}.article-cont-read-button-container .article-cont-read-button{border-radius:20px;font-size:16px;height:40px;box-shadow:0px 1.6px 3.6px rgb(0 0 0 / 13%),0px 0px 2.9px rgb(0 0 0 / 11%);width:250px}.article-cont-read-button span,.article-cont-read-button-on-app span,.article-cont-read-button-on-page span{display:flex}.article-cont-read-button-on-app span{margin-top:2px}fluent-button::part(control){padding:0 28px}.article-cont-read-button-on-app{width:auto;height:36px;font-size:14px;line-height:18px;font-weight:600;min-width:calc(min(270px,100vw));box-shadow:0px 1.6px 3.6px rgb(0 0 0 / 13%),0px 0px 2.9px rgb(0 0 0 / 11%);border-radius:20px}.article-cont-read-button-on-page{width:auto;height:36px;font-size:14px;line-height:18px;font-weight:400;margin-top:14px;border:1px solid var(--accent-fill-rest);background:transparent;border-radius:20px}.article-cont-read-button-on-app span img{height:23px;width:23px;margin-top:-2px}@media (forced-colors:active) and (prefers-color-scheme:light){.article-cont-read-button:hover span img{filter:invert(1)}}.article-cont-read-button-container-on-app{background:linear-gradient(0deg,#FFFFFF 40%,rgba(255,255,255,0) 100%)}.article-video-slot:empty,.article-slideshow-slot:empty{background:#F2F2F2;min-height:350px;display:block;animation:pulseLight 0.5s alternate infinite;animation-timing-function:linear;border-radius:6px}.article-video-slot{min-height:432px}.article-image-slot:empty{background:#F2F2F2;min-height:400px;display:block;animation:pulseLight 0.5s alternate infinite;animation-timing-function:linear;border-radius:6px}.related-video-large-title-slot{margin-bottom:48px;margin-top:16px;aspect-ratio:auto}.intra-article-relatedcontent,.related-video-loading-state{min-height:475px};.article-video-slot-adjust{clear:both;margin-bottom:79px};.article-vid-title-auto-embed::before{content:'';position:absolute;width:48px;height:4px;left:4px;right:0;margin-top:-14px;background:var(--accent-fill-rest)}.article-vid-auto-embed::after{content:'';position:absolute;width:48px;height:4px;left:0;right:0;bottom:-24px;background:var(--accent-fill-rest)}.intra-article-ad-half,.bopcfqq{float:inline-start;margin-block-end:28px}.intra-article-module{position:relative;z-index:97}.intra-article-ad-half-placeholder,.irfgkmk{min-height:toPx(300);min-width:toPx(300);background:rgba(0,0,0,0.06)}.intra-article-ad-full,.bvzdxia{margin-bottom:32px;margin-top:32px;padding:0px;background:rgba(0,0,0,0.06);min-height:toPx(140);border-radius:calc(var(--layer-corner-radius) * 1px)}.intra-article-ad-full.left-image-intra-ad,.bvzdxia.left-image-intra-ad{border-radius:12px;box-shadow:none}.intra-article-ad-full.full-bleed-image-intra-ad,.bvzdxia.full-bleed-image-intra-ad{border-radius:12px;box-shadow:none}.intra-article-ad-full.intra-article-carousel,.bvzdxia.intra-article-carousel{background:none}.article-social-slot{display:flex;justify-content:center}.article-back-to-feed-button{border-radius:20px;font-size:12px;height:32px;margin-bottom:3px;color:#242424;background-color:var(--article-back-to-feed-forced-color)}.article-back-to-feed-button-container{display:flex;justify-content:center;padding-top:16px}.article-back-to-feed-link{text-decoration:none}.article-back-to-feed-button span{display:flex}.ad-slot-placeholder{height:140px;max-height:140px;background:rgba(0,0,0,0.06);padding:0px 2px;margin-bottom:32px;margin-top:32px;border-radius:calc(var(--layer-corner-radius) * 1px)}.ad-slot-placeholder.collapsed{transition:max-height 0.15s;max-height:0;overflow:hidden;margin:0;padding:0}.ad-slot-placeholder.left-image-intra-ad{border-radius:12px}.ad-slot-placeholder.full-bleed-image-intra-ad{margin-left:auto;margin-right:auto;border-radius:12px}@media (max-width:600px){.full-bleed-image-intra-ad{width:fit-content}}.ad-slot-placeholder.intra-article-carousel{background:none}.continue-reading-slot{visibility:visible !important;height:auto !important;opcaity:1 !important}.upnext-card-static{position:relative;height:auto;padding:0px}@media (max-width: 655px){.article-video-slot{min-height:222px}.article-video-title-at-bottom{min-height:269px}.intra-article-relatedcontent,.related-video-loading-state{min-height:231px}.intra-article-ad-full,.bvzdxia{margin-bottom:16px;margin-top:0;min-height:toPx(140);width:100%;background:rgba(0,0,0,0.06);padding-left:0px;padding-right:0px}.ad-slot-placeholder{padding:0}.article-reader-container{margin-top:24px}.article-social-slot{max-width:calc(100dvw - 16px)}}@media (min-width: 656px) and (max-width: 979px){.article-video-slot{min-height:334px}.article-video-title-at-bottom{min-height:390px}.intra-article-relatedcontent,.related-video-loading-state{min-height:387px}}@media (min-width: 980px) and (max-width: 1303px){.article-video-slot{min-height:334px}.article-video-title-at-bottom{min-height:390px}.intra-article-relatedcontent,.related-video-loading-state{min-height:387px}}.full-bleed-image-intra-ad .card-full-size{height:144px !important;box-shadow:none}:host{background-color:var(--fill-color);color:var(--neutral-foreground-rest);font-family:var(--body-font);overflow-x:hidden}.article-body{font-size:17px;text-align:start;line-height:26px;margin:0px}.article-body a{text-decoration:none;color:var(--accent-foreground-rest);pointer:cursor;overflow-wrap:break-word}.article-body a:hover,a:focus{text-decoration:underline}.article-body h1{font-size:1.17em;margin-block:1em}.$article-body-a-udl a{text-decoration:underline}.article-sub-heading{font-family:'Eb Garamond';font-size:30px;font-weight:500;margin-top:48px;margin-bottom:24px;line-height:38px;position:relative}.article-sub-heading::before{content:'';position:absolute;width:48px;height:4px;left:0;right:0;top:-14px;background:var(--accent-fill-rest)}.article-sub-heading:has(.continue-read-break){position:static;margin-top:0}.article-sub-heading:has(.continue-read-break)::before{content:none}.article-clear-div{clear:both;display:block}.article-body p:empty,div:empty{display:none}.vjs-text-track-display div{display:block}.dropcap-element-slot{font-size:64px;font-family:'Eb Garamond';font-style:normal;line-height:52px;float:left;margin-inline-end:8px}.dropcap-special-letter{font-size:80px;line-height:60px}.BuyNowButton{font-size:14px;line-height:28px;background-color:#0070c6;color:#fff !important;display:inline-block;font-style:normal;font-weight:normal;padding-left:15px;padding-right:15px;text-decoration:none}.article-body a.internal-link{text-decoration:underline;text-decoration-style:dotted;cursor:pointer;outline-offset:-1px}.article-body a{outline-offset:-1px}.dropcap-special-letter-q{margin-inline-end:16px}li{font-size:18px;line-height:28px;margin:10px 0}.vl_disclosure{font-style:italic;font-size:15px;line-height:22px;font-weight:normal}.article-source{font-style:italic;font-size:14px}.article-body p{margin:0 0 16px 0}.article-body p:last-child:{margin-bottom:0}.article-body-ex-gap p{margin:0 0 24px 0}.webtoon-gallery p{margin:0px}div.article-image-hidden:{display:none}a.article-image-hidden:{display:none}.article-video-slot{box-sizing:content-box !important;margin-bottom:32px;position:relative;--video-border-radius:6px;min-height:432px;width:100%}.article-video-title-at-bottom{min-height:478px}.related-video-large-title-slot{margin-bottom:48px;margin-top:16px;aspect-ratio:auto}.related-video-loading-state{min-height:475px};.article-video-slot-adjust{clear:both;margin-bottom:79px};.article-vid-title-auto-embed::before{content:'';position:absolute;width:48px;height:4px;left:4px;right:0;margin-top:-14px;background:var(--accent-fill-rest)}.article-vid-auto-embed::after{content:'';position:absolute;width:48px;height:4px;left:0;right:0;bottom:-24px;background:var(--accent-fill-rest)}.hide-overflown-content,.hide-overflown-content ~ *{visibility:hidden;height:0}.intra-article-module{position:relative;z-index:97}.top-span-increased-height{height:68px};.top-span-v2-ux{height:78px};.article-slideshow-slot{padding-bottom:36px}.article-slideshow-slot a{color:#126d91}.article-slideshow-slot a[title=Ad]{color:white;padding-top:2px}.article-list-slot ul{padding-inline-start:20px}.instagram-slot iframe{width:500px;max-width:500px}.blockquote-text{font-family:'Eb Garamond';font-size:28px;font-weight:500;line-height:130%;margin-bottom:48px;margin-left:78px;margin-right:78px}.blockquote-text::before{content:open-quote;width:28px;height:25px;display:flex;color:var(--accent-fill-rest);font-size:60px;font-family:'Eb Garamond'}table{border-spacing:0;overflow:auto}table .article-table-iframe_container{width:100%}table .article-table-iframe_container td{padding:0}table tbody tr:nth-child(odd){background:var(--neutral-fill-rest)}table td{padding:0 10px}table:has(.article-image-slot){width:100%}.intra-article-module,.intra-article-module-bottom-slot{marign-inline-end:28px}.intra-article-disableSpaceFix{float:inline-start}.fb_iframe_widget{background:#fff}div.common-slot-placeholder{background:#F2F2F2;min-height:350px;display:block;animation:pulseLight 0.5s alternate infinite;animation-timing-function:linear;border-radius:6px;margin-bottom:48px}.article-slideshow-slot{padding-bottom:36px}div.intra-article-ad-placeholder{display:block;min-height:140px;background:rgba(0,0,0,0.06);padding:0px 2px;margin-bottom:32px;margin-top:32px;border-radius:calc(var(--layer-corner-radius) * 1px)}.intra-article-module,.intra-article-module-bottom-slot{width:100%}.continue-read-break{opacity:1;position:static}@media (max-width: 655px){.dropcap-special-letter{font-size:50px;line-height:42px}.article-video-slot{min-height:222px}.article-video-title-at-bottom{min-height:269px}.related-video-loading-state{min-height:231px}div.common-slot-placeholder{min-height:250px;margin-bottom:16px}}@media (min-width: 656px) and (max-width: 979px){.article-video-slot{min-height:334px}.article-video-title-at-bottom{min-height:390px}.related-video-loading-state{min-height:387px}}@media (min-width: 980px) and (max-width: 1303px){.article-video-slot{min-height:334px}.article-video-title-at-bottom{min-height:390px}.related-video-loading-state{min-height:387px}}Amazon has become the latest major company to order staff to return to the office five days a week, effectively ending working from home at the $1.99 trillion tech giant. Just like JPMorgan, Boots, and Goldman Sachs’ bosses, Amazon’s CEO Andy Jassy cited strengthening company culture as one of the main reasons behind raising its in-office requirements from three days to full-time. However, a new report from PwC suggests that the move to full-time in-office work could have a different effect than the one intended. The Big Four accounting firm conducted 13 months of research and surveyed over 20,000 business leaders, chief human resources officers and workers for its new Workforce Radar Report—and it found that hybrid workers feel more included and productive than those who sit at their company’s desk five days a week.“While many companies are pushing for return to office, it turns out that hybrid workers demonstrate the highest levels of satisfaction,” the report highlights. The researchers found that over three-quarters of hybrid workers feel like they belong, compared to 74% of fully on-site workers and 68% of remote workers. Similarly, 74% of hybrid workers are engaged—this drops to 72% for in-office workers and 63% for remote workers. These may seem like marginal differences, but they have a ripple effect on company culture: A staggering 90% of hybrid employees reported that the culture at their firm promotes community, collaboration, inclusion and belonging.“The idea that being on-site all day every day is necessary to establish and sustain a strong culture is a myth,” the report concludes. “Don’t be afraid to offer flexible options for fear of diminishing it.”Why two extra in-office days can chip away at company culture It might seem counterintuitive to build company culture by encouraging workers to spend more time away from each other—not less. However, in reality, when leaders enforce five days a week in the office they often end up overlooking engagement activities like team away days and after-work drinks.“The office becomes a crutch—engagement, recognition, and connection all happen by default,” Daan Van Rossum, author of the Future Work newsletter and founder of FlexOS tells Fortune. “When companies move to a hybrid schedule, they start implementing more purposeful efforts to replace this engagement. In the process, the hybrid experience actually leads to improved engagements with more touch points.” Plus, nobody likes commuting to sit in a noisy office to do the same job they could have done at home—especially not introverts. Experts tell Fortune that only having to do it a few days a week helps workers make the most of their in-office days and maximize collaboration.Then there’s having to deal with personality clashes on a daily basis, rather than in microdoses. “Being forced to work in the office exposes you to people who may share very different values to you,” says Amrit Sandhar CEO of the employment engagement firm &Evolve. “Over time, this can feel exhausting.”‘Don’t look back, look ahead,’ PwC says Despite Amazon’s call for workers to return to “the way we were before the onset of COVID”, PwC’s research highlights that workers simply won't return to the old days of working. “We’ve seen that return-to-office mandates have, in many cases, failed,” the report says before adding “the business-as-usual paradigm to which some business leaders want to return doesn’t exist anymore.” “Employees didn’t miss those long, stressful commutes, and they got used to the flexibility in scheduling, parenting, caregiving and so on that working remotely gave them,” it continues. “They were not keen on going into the office without a compelling reason.” Indeed, the biggest reason company culture dips when employees are forced to collaborate daily is precisely because it’s been forced.RTO’s and measures that monitor attendance, including tracking badge swiping, sends out the message that worker’s presence is more important to the company than output or meaningful collaboration.It’s “as if the number of employees was the point and not what those employees were doing once inside,” the report outlines,\" the report notes.For &Evolve CEO Sandhar, the problem for employees is that a full return to office can feel like surveillance. “Having the autonomy to make your own decisions does more to enhance a culture of valuing employees, rather than diminishing it,” Sandhar concludes. “No one wants to be controlled in a rigid environment, so forcing people back to workplaces can feel like control.\" \"It’s likely that this theme is followed through in day-to-day work experiences—rather than providing employees with a sense of autonomy and freedom, that sense of control such as being micro-managed, is likely to chip away at whatever vision the organization has for its culture, and lead to disengagement.” This story was originally featured on Fortune.com :host{background-color:var(--fill-color);color:var(--neutral-foreground-rest)}.article-image-container{position:relative;margin:20px 0 36px 0;width:100%;height:100%}.article-image-height-wrapper{width:inherit;display:block;appearance:none;padding:0;border:none;background:transparent}.article-image-height-wrapper-new{position:relative;overflow:hidden;border-radius:6px}.article-image{width:inherit;border-radius:6px 6px 0px 0px}.article-image-new{position:absolute;top:0;bottom:0;left:50%;transform:translateX(-50%);vertical-align:middle;object-fit:contain;width:auto !important;max-width:100%;border-radius:0px !important}.article-image.clickable{width:inherit;cursor:pointer}.image-attribution{padding-bottom:4px;opacity:65%}.image-caption{font-weight:normal}.image-caption *{all:unset}.image-caption-container{display:flex;font-size:12px;line-height:16px;padding:12px 16px 8px 24px;flex-flow:column}.articlewc-image-caption-container:{margin-top:0px}.article-image-container.image-small{width:270px}.article-image-container:first-child{margin-top:0}.image-caption-container:before{position:absolute;display:inline-block;height:11px;width:11px;border-left:1px solid #0078D4;border-bottom:1px solid #0078D4;left:-1px;content:\"\"}.articlewc-image-caption-container:before{left:0px}.image-caption-container.image-webtoon{display:none}.article-image-container.image-webtoon{margin:0px}.article-image.article-image-ux-impr{border-radius:6px;background:#F2F2F2}.article-sub-heading{font-family:'Eb Garamond' font-size:30px;font-weight:500;margin-top:48px;margin-bottom:24px;line-height:38px;position:relative}.article-sub-heading::before{content:'';position:absolute;width:48px;height:4px;left:0;right:0;top:-14px;background:var(--accent-fill-rest)}.image-webtoon .article-image{border-radius:0px;display:block}.image-webtoon .article-image-height-wrapper-new{margin-bottom:unset;border-radius:unset}.in-table.image-webtoon .article-image-new{position:relative;margin-top:5px} @keyframes zoom-in{from{left:var(--initial-x);top:var(--initial-y);transform:unset;-webkit-transform:unset;-moz-transform:unset}to{left:var(--final-x);top:var(--final-y);transform:scale(var(--scale-ratio));-webkit-transform:scale(var(--scale-ratio));-moz-transform:scale(var(--scale-ratio))}}@keyframes zoom-out{from{position:fixed;left:var(--final-x);top:var(--final-y);z-index:1000;transform:scale(var(--scale-ratio));-webkit-transform:scale(var(--scale-ratio));-moz-transform:scale(var(--scale-ratio))}to{position:fixed;left:var(--initial-x);top:var(--initial-y);z-index:1000;transform:unset;-webkit-transform:unset;-moz-transform:unset}}.image-fullscreen-overlay{position:fixed;top:0;left:0;width:100vw;height:100vh;z-index:1000;scroll-behavior:smooth;background:rgba(255,255,255,0.8);backdrop-filter:blur(25px);-webkit-backdrop-filter:blur(25px)}.article-image-height-wrapper.expandable{cursor:zoom-in}.article-image-height-wrapper.expandable.fullscreen{cursor:zoom-out}.article-image.expandable{transform-origin:0 0;-webkit-transform-origin:0 0}.article-image.expandable.fullscreen{position:fixed;max-width:100%;background:transparent;z-index:1000;animation:300ms cubic-bezier(0.5,0,0.25,1) zoom-in forwards;-webkit-animation:300ms cubic-bezier(0.5,0,0.25,1) zoom-in forwards;-moz-animation:300ms cubic-bezier(0.5,0,0.25,1) zoom-in forwards;width:var(--initial-width) !important;border-radius:6px !important}.article-image.expandable.close-fullscreen{animation:200ms cubic-bezier(0.5,0,0.2,1) zoom-out;-webkit-animation:200ms cubic-bezier(0.5,0,0.2,1) zoom-out;-moz-animation:200ms cubic-bezier(0.5,0,0.2,1) zoom-out;width:var(--initial-width) !important}@media (max-width: -1px){.article-image-height-wrapper.expandable.fullscreen{max-width:NaNpx}}@media (min-width: 1304px){.article-image-height-wrapper.expandable.fullscreen{max-width:1304px}}@media (min-width: 980px) and (max-width: 1303px){.article-image-height-wrapper.expandable.fullscreen{max-width:980px}}@media (min-width: 656px) and (max-width: 979px){.article-image-height-wrapper.expandable.fullscreen{max-width:656px}}@media (max-width: 655px){.article-image-height-wrapper.expandable.fullscreen{max-width:0px}} .article-image-progressive{height:100%;filter:blur(10px)}.inherit-cursor{cursor:inherit} @media (max-width: 655px){.article-image{border-radius:6px;background:#F2F2F2}.article-image-container{margin:16px 0px}} .article-image-container.image-small{float:left;margin-right:24px}Amazon has become the latest firm to end working from home in the name of company culture—a PwC reports suggests it could have the opposite effect. © Kevin Winter—Getty Imageswindow.isSSRCompleted=true;",
    "commentLink": "https://news.ycombinator.com/item?id=41606772",
    "commentBody": "Working in the office 5 days/week to build company culture is a myth: PwC report (msn.com)156 points by ivewonyoung 17 hours agohidepastfavorite200 comments flappyeagle 17 hours agoNo “research” that PwC publishes is worth the digital ink it’s printed on. It’s motivated by whoever is paying them to get the result that they want Whether you are into remote work or not, this is meaningless reply abadpoli 13 hours agoparentYou’re completely off the mark here. I’ve worked at a Big4 company before on reports like this. These reports aren’t paid for by other companies at all. They’re internally funded and done by the internal research teams. The motivations behind them are numerous: marketing, having artifacts to help rank at the top of stuff like Gartner reports, and even because believe it or not the people that work there sometimes genuinely enjoy researching and publishing reports. Reports like this are the consulting company equivalent of a tech company’s engineering blog bragging about their new scalable infrastructure or whatever. If you see a report published by a company that says “PwC did research for us”, then yes, it has likely been influenced by that company. But a report like this that is entirely PwC branded is not that. reply lq9AJ8yrfs 3 hours agorootparentI saw examples of both your and GP's experiences in my experience at a big4. Unfortunately none I saw (sample size: a handful in detail and perhaps dozens to a skim) followed even basic statistical practices I had learned in undergraduate studies at a well-respected university. There were clues that at least some of the parties involved knew better, but the imperative to publish completely overwhelmed any instinct for academic rigor. It was dressed up as \"eminence\", which came after sold work and delivered work (in order) in annual reviews. Statistical rigor would probably help eminence, but there were faster paths to eminence. reply emeril 3 hours agorootparentprevyeah, at least PwC + Deloitte are more legitimate than say KPMG or E&Y IMO this is judging by all the time I spend reading their \"handbooks\" professionally reply GeoAtreides 14 hours agoparentprevSurely if a thief argues against stealing we will not dismiss their argument just because they're a thief; might call them a hypocrite, sure, but the validity of an argument is independent by the moral standing of whoever is presenting it. That's to say, one should read the report, especially the methodology, before fully dismissing it. reply aubanel 12 hours agorootparentI disagree with this: when an actor is known for repeated bad faith, it's often a net gain of time to dismiss what they say by default. Only if someone else presents the same argument should you take the time to analyze it. reply GeoAtreides 11 hours agorootparentSaying you disagree with my original comment might be inaccurate, seeing how it didn't touch on repeated bad faith; not that habitual lying would make any difference to the validity of any argument (which depends only on being logical sound). I do agree ignoring known and mercenary liars does save time. reply SuperNinKenDo 4 hours agorootparentGiven the history of PwC and the Big 4, whether you specifically mention repeated bad faith or not, it's relevant to whether your analogy holds. reply rockskon 17 hours agoparentprevBy that logic, the interesting question is who paid them? There's no shortage of money and headlines promoting working in the office. Who had the money to pay for the other side? reply EarthBlues 16 hours agorootparentI do not think that it is obvious that WFH is a superior bargain for software workers in developed countries. An aspect of WFO/WFH debate that seems to have been missed among US commentators is that a remote workforce is easier to extend or supplant with offshore or nearshore staff. Brazilian and Mexican SWE contractors, in particular, have advanced considerably in quality from just a decade ago, and can now easily replace US-based workers at half the cost without compromising quality. I have seen this trend taking form in my own, remote-first work experience, and market research corroborates the trend, with US nearshore job offers increasing steadily quarter over quarter since the pandemic, in contrast to onshore positions which have notoriously been more volatile. Europe is also seeing increased nearshoring, particularly to Turkey and Egypt. Yes, there are many nice things about WFH and I prefer it for my part, but it’s not clear to me that this debate is the obvious slam dunk that one perceives reading comments on HN. reply datadrivenangel 15 hours agorootparentActual quality nearshoring engineers are ~2/3rds the cost when you factor in the added complexities of direct employment or pay an agency to do that for you. Still a good deal, but not quite as obviously a good deal. reply phil21 4 hours agorootparentIt’s not even remotely close to 2/3rds if you at at all competent at nearshore/offshore hiring. 1/3 at best for top 20% talent. Once you get into rockstar quality it’s more or less parity since they have all the options anyone in the U.S. has. Most companies are not doing interesting enough work to justify hiring such people. This is from my personal direct experience over at least dozens if not a hundred directly managed employees over the past decade. Outsourcing isn’t your fathers offshoring any more if done right. Watching most marginal US tech workers be so short sighted by giving up the only competitive advantage they have left has been interesting to watch. reply rob74 13 hours agorootparentprevTrust me, this argument has not been missed. I have read several WFO vs WFH threads on HN over the years, and it came up every single time. reply CalRobert 11 hours agorootparentprevNot sure why you’re getting downvoted. I’ve been remote from Europe for U.S. companies for seven years and being cheaper than people in the US was part of my appeal. In the last year or so I notice more of them have finally figured out lots of South Americans are just as smart and in the same time zone too, so I’m starting work for a local company soon (at noticeably lower pay than the American ones) reply Retric 16 hours agorootparentprevFast growing as a percentage but still small on an absolute scale. India became such a hub for outsourcing because of the huge pool of English speakers in a single country. Large scale near sourcing to South America has less of a time zone issue but only 1/3 the population of which fewer people speak English + a large number of countries with their own tax codes etc. It’s easy to lump Brazilian and Mexican developers together but cheaper to only operate in one of those countries at which point the talent pool just isn’t that huge. Really it’s Middle America and Canada that are the ‘threats’ except unlike India there isn’t the vast talent pool. Silicon Valley alone has roughly the population of Montana + North Dakota + South Dakota but those states have nowhere near the number of software developers and they want first world compensation. It’s easy to confuse geographic area for talent but those expensive costal cities are huge: https://www.visualcapitalist.com/maps-extremes-us-population... reply otteromkram 13 hours agorootparentprev> contractors > without compromising quality Pick one. I recently reviewed some offshore code in Python (pandas) that was something like: s = pd.Series(...) for i, d in enumerate(s): if i == 12: value = df.loc[i][5] break Yes, that's right; instead of checking the dimensions and selecting the value by index, they added an iterator. An entire app full of this. Know who reviewed the code? That's right, more offshore workers. Can't really blame 'em though. When you're a contractor, you have no obligation to write anything of quality since you might never see the code again. So, now we'll spend more time unf*cking the code which is a latent expense the company didn't anticipate. ¯\\_(ツ)_/¯ reply meiraleal 9 hours agorootparentOh, do you think you are a better programmer than all brazilians? Care to share your github? reply rowanG077 3 hours agorootparentWhat kind of bad faith argument is this? They poster did not make any claim about the quality of Brazilian software devs. They made a statement about offshore contractors. reply bbarnett 13 hours agorootparentprev> contractors > without compromising quality Pick one. -- Nonsense. I've seen companies with only full time employees that have horrid work cultures, which churn out junk, junk, junk. It's not about contractors. And hiring out of country isn't necessarily a 'contractor' thing regardless. You can full-time employee people worldwide, too. reply marcosdumay 16 hours agorootparentprevMicrosoft? reply luxuryballs 16 hours agorootparentprevPerhaps in this case they are being “paid” by the desire to retain their home offices. reply MBCook 17 hours agoparentprevThat’s why I’m surprised to see this. If this was bunk shouldn’t this reinforce what their clients what to hear? reply hammock 16 hours agorootparentThis is an internally-funded whitepaper. PWC accounting and consulting practices are both historically built on lots of travel to client sites. If they can make excuses to travel less, they can charge the same amount for their services but save a LOT of money on flights, hotels and meals. (just an idea) reply jdlshore 16 hours agorootparentEh, unlikely. Consultancies usually charge for travel and expenses (or a per diem). reply Attummm 10 hours agorootparentThe thought process is as follows: The customer seems satisfied with the overall cost.But if travel expenses could be lowered, we have the opportunity to reduce the bill slightly while increasing our profit margin. Thus, it can be construed as a win-win. reply jdlshore 34 minutes agorootparentSpeaking as a former consultant, just… no. The chain of events here is fantasy-land: 1. Consulting company writes article supportive of remote work 2. But they’re lying because remote work increases their profit margins 3. Because it decreases travel expenses, so they charge more for the same work, but the overall package is less. This is ridiculous conspiracy thinking. The potential profit is miniscule, the reputational cost of being caught is high, and it completely misunderstands how these things are sold. Prices are negotiated exclusive of expenses (typically) and buyers often don’t even see expenses. They often go straight to Accounts Payable and might even come out of an entirely different budget. reply bluGill 16 hours agorootparentprevWhich clients? Generally you should have several with different interests. reply notepad0x90 16 hours agoparentprevwouldn't that be against their own business interest? If an executive or the board of some company want PwC's services, knowing that they'll only help confirm a biased opinion, then is it worth spending millions on them? Their main income is a direct result of their capability to advise in the best interests of their clients. If action taken upon their advise, and that action leads to a loss of profit, then their clients paid them to lose money. There aren't enough companies that pay consultants money just for the sake of their ego to sustain a company the size of PwC. reply rtpg 16 hours agorootparent> wouldn't that be against their own business interest? If an executive or the board of some company want PwC's services, knowing that they'll only help confirm a biased opinion, then is it worth spending millions on them? Generally speaking the point is you hire the consultants to get to where you want. You want RTO? Then you hire them and they will tell you how to get there/how to justify it to everyone/how to roll it out. This is in some sense cynical, but in some sense also just logical. You want something to happen, then you pay somebody to do it! The idea that you would hire some external vendors to... decide for you... might make sense in theory but if you're the one defining the strategy surely you will have made some decision. These are business people, not sociologists. Though you talk to many sociologists and they'll tell you they're often hired in to justify an existing decision as well... With fuzzy stuff like this you can make an argument in any direction anyways. It's all pretty self-fulfilling. reply notepad0x90 15 hours agorootparentYou're not wrong, but their value is that they do this for other companies as well and they have staff that have experience and competence in the subject matter. So, in their attempt to support the executive, they find out that they're heading in the wrong direction, they must advice them accordingly and help them come up with a strategy back-pedal. In other words, if they have new information, they will share that information to see if their clients will have a change of mind. If the new information (or lack of one) won't change minds, then they will submit their advise in support of what they were hired to do to begin with. These consulting firms, on their own, have had an enormous impact in shaping entire economies and countries. They are not mere props, supporting the ego of an executive. A lot of times, it is the entire board that hires them, or their report by the CEO is presented to the board, not one person. if the outcome is undesirable, they will lose credibility the next time they're used at that company. reply rtpg 6 hours agorootparentI appreciate this theory, but even from the top 4 management consultant firms I've found that many consultants I've met have fairly surface level understanding of things. My super cynical take is that the people who end up in management consulting are people who really liked making powerpoint slides in school. Just a continuation of what they did all through high school and university. Homework, as a job. I only half believe this reply Spooky23 16 hours agorootparentprevBasically, they will take your idea and make it a plan, without regard to the organization. You’re paying for validation and cutting through your own management and organization problems. reply notepad0x90 15 hours agorootparentYeah, validation, but not rubber-stamping. The validation may or may not succeed. reply andsoitis 16 hours agorootparentprev> Generally speaking the point is you hire the consultants to get to where you want. So in this case all those companies are paying PwC to conclude that hybrid > in office > full remote? reply s1artibartfast 15 hours agorootparentI'm sure if you get into the details, the answer will be \"It depends...\", which is obviously true, but also self serving for a consultant. To figure out which is true for your company, they would come in, spend a few months studying you workplace. This would include several interviews with leaders and SMEs that the leaders hand select. Given this study process, it wont be surprising that the consultants end echoing back the opinions of leadership. I been through many consultant assisted re-orgs. Somehow, the leaders who brought them in always end up with promotions and more control. On one hand, yes, this usually gets oversold to the rank and file with promises of efficiency. On the other hand, the consultants are doing what they are paid for: finding a way to get their stakeholders what they want, when the stakeholders dont have a roadmap to get it. reply benjaminwootton 13 hours agorootparentprevThe vast majority of consulting engagements work like this. Someone wants something to happen and brings in a consultant to explain why it’s a good idea to cover themselves and sell to the rest of the business. A consultancy wouldn’t last very long if they disagree on strategy with the person signing the cheques. reply notepad0x90 12 hours agorootparentBut if the strategy fails, won't they lose credibility? How is it sustainable, if all they do is rubber-stamp? reply mejutoco 12 hours agorootparentCheck Ernst & young for wirecard or PwC for Evergrande for the latest examples, both banned temporarily or fined for their involvement. reply notepad0x90 9 hours agorootparentThanks, law aside, it is so hard for me to imagine why they continue to be in business. Maybe, it is because whoever is being convinced by them of something believes they would need to pull the same \"trick\" some day. reply robertlagrant 4 hours agorootparentprevAnother one is McKinsey for their role in the opioid crisis. reply electronbeam 16 hours agorootparentprevThey’re paid to support the CEOs hunch, so it’s credible to take action on the hunch reply sokoloff 16 hours agorootparentprevThat’s ignoring the blame-shifting or blame-sharing cover that hiring an outside expert advisor brings. “We used the advice from top experts.” reply paulcole 13 hours agoparentprevEver noticed that HN commenters are quick to point out the tiniest errors in articles that they disagree with yet won’t even give the slightest effort to consider the validity of something they’re agree with? Office work is good = “There’s an issue with the sample size and methodology plus I think this was written by a marketing person” Remote work is good = “Go off king” reply disgruntledphd2 13 hours agorootparentThis is all of humanity, not just HN commentators. reply paulcole 4 hours agorootparentGo off king! reply anonymysz 15 hours agoprevApologies for the throwaway account, I just don't want this comment 'on the record': I've worked nearly 100% from home for the last 5 years. My wife works for a major bank, and has had a hybrid work arrangement for all that time. Employees there are required to spend 5 days in the office per fortnight. This requirement is strictly enforced. For whatever reason, we spend about 3 months of every year battling viruses that spread around her office. Every few months someone brings COVID to her office. Flu season is an absolute nightmare. I'm not sure what has changed in the wider human biome, but I don't remember respiratory viruses being something I needed to worry about prior to COVID. I'd only ever had the flu once before in my life, but now we get it multiple times a year. I've had three severe respiratory viruses this year alone, and I'm a healthy, fit, non-immunocompromised person. Anecdotally, other people are experiencing the same thing. Is this something that affects other people here? reply majke 15 hours agoparentWhat is your location? Here, in Poland, its mostly the kids bringing infections home. I usually expect at least one week off due to sickness in early fall, and then proper flu (taking around 10 days) in december/january. This is the norm. I remember a stark contrast with the UK where, I guess due to the climate, the kids dont get sick at all. reply BrandoElFollito 5 hours agorootparentSame in France. The threshold for \"can go to school\" is \"no fever\". So you have plenty of kids sneezing and coughing as if they were in a final stage of tuberculosis and there was no tomorrow. They are taught to sneeze in their arm (and they do it) but not coughing. So they cough as of it was the olympics again and there would be a prize for the one who coughs further. For the record, I have two children (now students) so they did their share of spreading diseases (to home and at school) reply repiret 14 hours agorootparentprevIn my experience in the US, it's about the same. Although it seems my house doesn't get quite as sick as yours; I don't think I've ever been sick enough long enough to take 10 days off work. reply laserlight 13 hours agoparentprevIt amazes me that our culture normalizes people spreading their infections. reply tharne 4 hours agorootparentWhat we normalize is not stopping the world every time someone gets the sniffles. reply nickpp 10 hours agorootparentprevI thought getting regularly infected with seasonal diseases keeps your immune system up-to-date against the pathogen mutations and avoids a rarer but more dangerous episode down the road. But I guess without a serious study my belief is hard to check & validate. reply sunaookami 3 hours agorootparentIt's the opposite - regularly getting infected weakens your immune system. It's not something that constantly needs to be \"trained\". reply stackghost 15 hours agoparentprevI get a flu and COVID vaccine once a year and essentially never get anything worse than a cold. I have young school-aged kids, and kids are cesspools of filth and pestilence. reply jiggawatts 15 hours agoparentprevYes, there was a “rebound” effect where isolation during COVID reduced herd immunity to other respiratory viruses. When people were brought back into close contact, the result was a “speed run” through three years of viruses in the space of one year. It’s a bit more complex than that but you get the gist. reply laserlight 13 hours agorootparent> isolation during COVID reduced herd immunity to other respiratory viruses. Do you have any reference explaining this? As far as I know, respiratory viruses (flu, RSV, COVID-19) are difficult to develop immunity for because of their rapid evolution cycles. The quote above sounds counter-intuitive to me. reply jiggawatts 12 hours agorootparentDisclaimer: I'm not even remotely a biologist, and this is a vague memory of some announcements (albeit by reputable scientific sources) a year ago. This gist of what I remember was that viruses mutate some \"percentage\" of their envelope on average annually. It's somewhat predictable for more than six months into the future, which is how flu vaccines are developed and manufactured ahead of time before the next flu season. This unpredictability accumulates, so after three years the viruses become nearly totally unpredictable, not just to vaccine manufacturers, but also to the human immune system. If you get the flu every year, each year the new flu might only be 30% different than the one before, so you have some lingering immunity to it. If you catch it every year, you'll generally get mild cases and refresh your immunity each time. If you skip three years, the viruses become 66% different, which might be sufficient to evade your immune system and give you a really bad case. There were also a few coincidental compounding issues recently, such as particularly virulent strains of viruses such as RSV going around. reply s1artibartfast 15 hours agorootparentprevI think the majority of the effect is that and confirmation bias, but I think there could be some marginal effect. Im hybrid with days in office counted and a performance consideration. When I have used by WFH days then get sick, I 100% take it to the office. I dont know if this surpasses the effect of fewer people in the office, but I think it is possible. reply aulin 14 hours agoparentprevIt's not flu. I just assume anything respiratory I get from the office or when travelling is COVID and so far antigenic tests always confirmed it. I take flu vaccine shots once a year and didn't get flu since years. Same for COVID but they don't work, I still get it at least twice a year with bad symptoms reply Elinvynia 12 hours agoparentprevIf you had COVID (which you definitely had) you are now immune compromised. You can test your cell immunity to see this (CD3+CD4- and such). I had AIDS-level immunity after my infection. This resolves within 2 years, usually, unless you are constantly getting reinfected due to this lower immunity and not taking any precautions like masking. reply karaterobot 17 hours agoprevIt seems a consultancy has done some research, only to discover that the thing their customers want to be true is true. What a novel development! I'd always understood that hybrid was the worst of both worlds. Companies still have to pay for real estate, can't recruit from neighboring cities, let alone internationally, and employees have to commute to work and live in expensive cities, just so they can work in loud offices and be micromanaged. reply tyre 15 hours agoparentWe do hybrid with people in office 3-4 days of the week (and no one on Friday.) Feels like a good balance. As a manager, it’s absolutely not to micromanage. People build bonds in person in a different way than online. I can give reasons why that helps with work, but also it’s nice to be around humans? As for commute, NYC subways are great (or at least as good as America’s got…). We have people in NJ, Manhattan, Brooklyn, and Queens. Everything has trade-offs, but it’s common for engineers to mention in the interview process that they’re looking for something at least partly in person. reply cassianoleal 4 hours agorootparent> it’s common for engineers to mention in the interview process that they’re looking for something at least partly in person. How can you tell this is not because they're aware of present market or even your specific company's preference / conditions and are simply trying to increase their appealing to you and increase their chance of getting an offer? It's common interviewing advice and CV writing to market yourself according to the job you're aiming for - that includes \"company culture\", \"company values\" and whatever other crap helps bias the interviewers in your favour. > it’s nice to be around humans Yep. That's what family and friends are for. Work is mainly to make money. Until the time comes when work stops being a requirement for survival, that will always be its primary function. Making friends can be a nice side-effect sometimes, but is not a requirement. reply tyre 4 hours agorootparent> How can you tell this is not because they're aware of present market or even your specific company's preference / conditions and are simply trying to increase their appealing to you and increase their chance of getting an offer? Hybrid is a requirement so it’s more about expectation setting, and most people who say that bring it up themselves when asked what they’re looking for. Maybe they’re all lying, but it’s a pretty low priority thing to lie about and after interviewing hundreds of people, you do get pretty good at parsing out how genuine someone is. Not perfect! But pretty good. > Yep. That's what family and friends are for. Work is mainly to make money. Until the time comes when work stops being a requirement for survival, that will always be its primary function. Agree to disagree. If I’m going to spend a third of my life doing a thing, I’m damn sure going to screen for people I respect and want to spend time with. Work means a lot more than the money to me; impact and people are higher up there. reply viraptor 14 hours agorootparentprev> People build bonds in person in a different way than online I see you said \"different\", which I agree with. But you didn't say \"better\" - and I do feel that's a mix that goes both sides. reply mcdeltat 13 hours agorootparentFrom my own experience at a hybrid company, sample size 1, there's a marked difference in interaction between me and other devs when we meet in person. It's basically like I don't exist and am worth 0 to them before we've met in person. After meeting, even if only briefly, it's completely different: suddenly they respond to messages, are willing to help with questions, even willing to prioritise work if I ask. Now this could be for various reasons - company culture, me being a juniorish dev, confirmation bias. But it really seems like humans don't account value to something/someone unless they interact with it in person. Personally I am completely for remote work, although I think it's wrong to say it's strictly better for everyone in every scenario. reply viraptor 13 hours agorootparentHave you tried to schedule some video calls for lunch / banter / get to know each other time? They can do magic too. In my experience it's about taking the time to actually chat to each other when you both have time, not about the presence. reply mcdeltat 9 hours agorootparentVideo calls do work to some extent, true. Just having some form of rapport with someone helps. However, I think there are some downsides to calls: - The person has to agree to the call (tricky if they are already not responding and/or are more senior and/or dislike meetings) - Calls often seem to have a lesser effect than in-person - Plus other standard downsides of Zoom, etc. In comparison, taking 2 minutes to walk up to someone's desk and directly address them is almost like a magic trick. Instant \"pay attention to me now\", thanks to the evolved human brain. Now maybe overall not worth the tradeoff for in-office vs remote, but still I appreciate in-office's effectiveness here. reply brailsafe 10 hours agorootparentprevThis sounds like the difference between going to a singles event to try and meet people instead of just bumping into them where you already are. One is much more likely to actually form something substantial, even though I'm hugely in-favor of focusing social time on doing this outside of work, it's important to have some level of that with coworkers I'd think. reply tyre 4 hours agorootparentprevI didn’t say ‘better’ because it’s a value judgement and personal preference. Some people are very anxious about working in person and remote work is the balance they’re looking for. We work a certain way and make trade-offs as best we can. It doesn’t work for everyone, and that’s okay—non-judgmentally in a completely values-independent way. In my opinion, startups are really hard if you take them seriously and an in-person aspect helps. There are exceptions, especially if you’re working with people you already know irl (like during COVID) or have worked with a while. It’s not black and white. reply piva00 6 hours agorootparentprev> People build bonds in person in a different way than online. I can give reasons why that helps with work, but also it’s nice to be around humans? Completely agree on people building bonds in person, hard disagree that you need people in the office most of the time for it. My employer went full onboard with allowing people to choose if they want to work from home or in an office, we're very distributed (offices in many European cities, in the USA, Japan, etc.) with thousands of employees, having 1-2 events a year where we gather together has already been enough to develop bonds that we maintain remotely. I live in a city with amazing public transportation but I would not choose to commute again for work in my life, I prefer working from home and doing the odd trip to the office when I feel like it, it's enough to personally connect with people while also avoiding a lot of the pitfalls of office life. reply dmitrygr 14 hours agorootparentprev> As a manager [...] it’s nice to be around humans? And there is your bias reply tyre 3 minutes agorootparentwhen your ellipsis elides whole sentences and glues together phrases that weren’t tied together in text, that’s bad faith reply repeekad 17 hours agoparentprevHybrid is exclusively a stop gap to stop people for quitting, no company wants to pay for an office downtown used 2-3 out of 7 days a week, S.F. vacancy rates seem to be leveling off and not improving around 40% (officially) reply iancmceachern 4 hours agoparentprevYou have it wrong. First, think about from which viewpoint your above comment comes from. Each sentence above could be amended to add \"... , from the companies standpoint\". From the employees standpoint it's better on each account. If you understand that companies, and their performance, is just an aggregate of thr employees and their performance. Empower the employees, that's the only company culture worth pursuing. Your points: - \"Companies still have to pay for real estate\" - they need this anyway. If you do it properly you don't have extra space, you have twice as many hybrid people working out of the same space you had half as many full time on site folks. This is the choice of management in how they efficiently use their space. Even explicitly remote first companies I know of have offices and meet regularly in person. This is a red herring. - \"cant recruit from neighboring cities\" - Being hybrid actually helps with this, no? I personally know lots of people who work further from the office than they would if they didn't have a hybrid schedule. - your last point is exactly why people want hybrid or remote positions. I don't see an argument for your point there, only one against it. - reply martin_henk 17 hours agoprevWFH only is low quality experience for me. No close bonding. Just zoom calls. Hybrid/Flex is the way to go I think. No need to be in the office 100%, but show up if you can. reply tivert 15 hours agoparent> WFH only is low quality experience for me. No close bonding. Just zoom calls. Hybrid/Flex is the way to go I think. No need to be in the office 100%, but show up if you can. I think Hybrid/Flex is the way to go, rather than pure WFH, because of the social connections, BUT it's also utterly pointless with distributed teams. I'm on a distributed team. I have to fucking go into the office to fucking zoom from there. Plus it's a hoteling setup, so it's even hard to know who's around and my workspace is always a little off. reply globular-toast 10 hours agorootparentBefore COVID I worked for a big multinational that got rid of all meeting rooms because they realised people could just have meetings at their desks. It was all open plan \"hot desking\". You'd literally have people transporting their sack of meat upwards of 50 miles each day just to sit at a desk on Skype all day talking to people on the other side the room. I found it particularly amusing when two people on the same call would be sitting right next to each other but talking through the screen. There might be real reasons to be physically present at some places, but I think it's important to remember many places are like the above and the reasons for coming in are complete bullshit. For most people in an office coming in is mainly because they don't have an office at home. That's not a problem for most geeks who generally have a far better setup at home. reply the_gorilla 16 hours agoparentprevI can't imagine having to get close bonding from the office. One of the benefits of WFH is that you spend less time around people who happen to get paid at the same job as you, less time commuting, and more time with people you choose to be around. reply cflewis 16 hours agorootparentI think most people would see not interacting with coworkers in any sociable way aside from video conferences as a net negative. You can’t form relationships that way. I think this is a key difference between people in this conundrum: some WFH advocates just see no value in building relationships with workers past the screen. It just doesn’t matter to them at some core level, and they don’t understand why it does for others (who I think are the silent majority). You have more time for relationships outside of work, but for myself I find it much easier to work with others if I have some meaningful concept of who they are, and they have some meaningful concept of who I am. reply 46Bit 16 hours agorootparent> You can’t form relationships that way Plenty of millenials and Gen Z can We've been building groups online ever since we were little reply strken 13 hours agorootparentI am a millennial with a few online friend groups and they're not really the same thing. For me those online relationships are loose and impermanent. People are continuously entering the group and continuously leaving never to be seen again. There's some level of trust and stability from meeting in-person that I can never seem to achieve online. reply lloeki 13 hours agorootparent> For me those online relationships are loose and impermanent I hate to break the news but most relationships simply are loose and impermanent, we just don't usually notice how brittle they actually are. As for trust, is it really reasonable to trust someone more or less just because they've been in front of you vs not? And I mean that both ways: too trusting of people in front of us and not enough of people away. reply brailsafe 10 hours agorootparent> I hate to break the news but most relationships simply are loose and impermanent, we just don't usually notice how brittle they actually are. Probably true at some level, but I'd wager that's much more common for Gen Z and younger millennials for a variety of reasons, as well as among people who just aren't really authentic, suburbanites, and people who just don't invest in friendship building. However, that's a bit of a silly comparison, online relationships have some value, maybe a lot maybe a little, but they aren't an equal substitute for a friend in meatspace reply strken 11 hours agorootparentprevI'm aware that most relationships aren't going to last forever, but the friends I have online are notably less cohesive than the friends I used to work with. I make no claims to reasonableness. We are not creatures of pure reason and our friendships are never totally rational. All I claim is that there's something which ties offline friends to me and I to them, particularly if we've worked together, which is not present for any of my online friend groups. reply benfortuna 15 hours agorootparentprevIt may seem normal, if you have never experienced regular in-person relationships (in a work environment). Even after WFH for a long time I think everyone becomes used to it, but there is definitely something missing. reply viraptor 13 hours agorootparentYou really don't need them to be in person. Have you scheduled any lunchtime catch-ups? Got any regular group calls around interests? Just random banter? My work group online is a better experience than I've ever had in the office. It may vary for other people and environments, but \"something missing\" is not a given just because of remote contact. reply lazide 16 hours agorootparentprevIn the same way junk food is equivalent to a healthy meal (IMO). There is a reason some mental health issues have been skyrocketing, and this is a big part of it. reply davidcbc 16 hours agorootparentGonna need a citation on this one. Not mental health issues increasing, but online communities being the cause reply lazide 16 hours agorootparentNot ‘online communities being the cause’, rather ‘lack of genuine in person community and physical connection’ being the cause. Same as junk food isn’t necessarily the cause of health issues - rather lack of enough healthy, not processed to the tits food is the cause. Replacing most/all food intake with junk food is going to be bad. Doing it periodically with enough of the ‘real thing’ to compensate? No issues. The issue is not enough of the real deal. Which is possible until something breaks because of the alternative, but not necessary. If you put someone in a capsule in say Antarctica, and they only communicated with other people via video chat - would anyone be surprised if they went crazy? Hell, I think we’d all be surprised if they didn’t. The challenge right now is a lot of people (including many people here) are de facto in that pod in a way that they can’t see, because theoretically they could walk outside and have conversations, etc. They just won’t actually do it, because there are less visible factors pushing them away - factors that in many cases they aren’t allowed to see or acknowledge. reply redserk 15 hours agorootparentMy coworkers are extremely wonderful people, however we aren’t friends. It is possible, and in many places quite easy, to make local friends without relying on coworkers. Facebook Groups has been very helpful to find local groups. reply lazide 13 hours agorootparentIt’s also entirely possible, even in the worst ‘food deserts’ to drive to a grocery store and make home cooked food. It’s also pretty easy to demonstrate how there is a direct relationship between how hard that is to do, and obesity and bad health outcomes. reply icehawk 14 hours agorootparentprevOh that started way before COVID. reply aaomidi 16 hours agorootparentprevVery boomer to quickly consider social circles made by young people equivalent to “junk” with no actual evidence other than feelings and random correlations. reply s1artibartfast 16 hours agorootparentWhat about an unprecedented epidemic of loneliness? reply aaomidi 16 hours agorootparentAgain, correlation not causation. reply s1artibartfast 15 hours agorootparentThere is a vast body of literature that shows that in person interactions is not just correlated with happiness, but an effective intervention for loneliness and depression. In full disclosure, even setting aside the research, I have way too much anecdotal evidence from what I have seen and experienced to be convinced otherwise. reply rockskon 15 hours agorootparentDoes that body of literature say anything about in-person interactions in the workplace? Specifically the workplace. An environment that a breathtaking number of people do not want to think one second about outside of work. reply s1artibartfast 13 hours agorootparentI havent read about workplace interactions as an intervention for loneliness and depression. It would be hard to run a RCT on that, so you would only be left with correlation. I dont know why making it a more pleasant experience during work has anything to do with how much someone thinks about it outside of work. reply rockskon 12 hours agorootparentBecause not all human interaction is equal. It equivocates talking to a cashier at the grocery store to hanging out with friends. reply lazide 13 hours agorootparentprevAs any stats course would tell you, while correlation != causation it definitely implies there is something to consider there. And causation almost always causes correlation. So do you have any alternative theories? reply aaomidi 5 hours agorootparentYes. Climate catastrophe for one. Inability to see the point of life when you can’t even afford to move out. Etc. there’s hundreds of theories. reply lazide 16 hours agorootparentprevSure dude. Too bad I’m waaaaay too young to be a boomer. I’ve just been online and doing this for long enough to have had to recognize the effects and deal with them. reply hanniabu 16 hours agorootparentprevThey think they can, but it's not the same reply the_gorilla 16 hours agorootparentIt's primarily some mental blocker in the old that prevents them from connecting things online to their real-life counterparts. It's like being illiterate and insisting that no one else can read those strange symbols. I'll offer in advance that younger people need to learn to separate the two sometimes. reply ywvcbk 4 hours agorootparentOr they (some at least) might have a better frame of reference and “the young” people simple don’t know and can’t comprehend what they are losing. My interpretation is on no way less generous than yours. reply the_gorilla 16 hours agorootparentprevMost people can establish friendships that are primarily online. It's not \"screen relationships\" vs \"in-person relationships\" for me, it's \"real relationships\" vs \"work relationships\". The main thing is that I don't want anything to do with coworkers and would choose not to engage with them even in the office. They're not friends, or family, they're coworkers and they come and go with the money. I had one coworker I liked to talk to, but he got a better job somewhere else and that was the end of that relationship. I would prefer to put more time and energy into actual relationships and not at work relationships. reply ywvcbk 4 hours agorootparent> coworkers and would choose not to engage with them even in the office Which is fine and understandable. Some people actually enjoy their work and like spending time with their colleagues (due to shared interests, worldviews etc.). > actual relationships and not at work relationships. I don’t see a difference or rather why can’t there be a significant overlap between the two (after this has always been the case for most people). Why wouldn’t you choose to work with the people you enjoy spending time with if you have the option? reply s1artibartfast 16 hours agorootparentprevplease understand you are not representative everyone. Most of my friends are coworkers from various points in my career. I also don't care for screen relationships either. reply the_gorilla 16 hours agorootparentWho else am I representing? Of course I'm representing myself, these are opinions and I'm saying them, which makes them my opinions. The difference is whether you get to force people to go into the office because that's how you make friends. reply s1artibartfast 16 hours agorootparentFor some reason I read your post as a negation or dismissive of the parent post. I dont think there will be a clear solution to the issue. I think companies will sort into those with in person cultures and remote cultures, but there will always be some dissatisfied minority in each one. The burden will then be on the employee. If you dont want to work in person, dont accept an offer with that in the job requirements. Inversely, if you want in person relations, dont join a remote company. reply the_gorilla 15 hours agorootparentThat's fair. I was also trying not to be dismissive. It's just a fundamental difference in how we see work relationships. reply s1artibartfast 15 hours agorootparentThat part was kind of interesting to me. I have both \"real relationships\", and \"work relationships\", but they are not at all mutually exclusive categories. For me, the more the two categories overlap, the happier I am. I like working with people who I deeply enjoy and trust. People I can laugh with and be honest with. Afterall, the reality of a 40 hour week is that I spend as much time with these people as my wife and family. Life is too short for me to spend 40 hours without real connection. reply brailsafe 9 hours agorootparentprev> Most of my friends are coworkers from various points in my career That's fine, but contextually it would seem more important what you'd prefer to be the case, not what is the case. As in, would you prefer to have a greater portion of your social circle made up of friends outside the places you've worked, or are you happier having most of those come from work? reply s1artibartfast 4 hours agorootparentI like it However, the way I view it, having fewer friends from work doesn't imply making more friends outside of work. I spend 40ish hours the the workplace either way. If I categorically avoid seeking or making friends there, that doesn't mean I'm spending more time looking elsewhere. It just means I'm spending 40hrs/week friendless. reply ImPostingOnHN 3 hours agorootparent> I spend 40ish hours the the workplace either way Lets call it 45 or 50 due to commuting. With that said, that seems to be begging the question. If you weren't spending all that time in the workplace or commuting (in other words, if you worked remote), you'd have more time and opportunity to make friends outside of work. reply hdjjhhvvhga 4 hours agorootparentprev> others (who I think are the silent majority). [citation needed] But let's go to the core of the argument: there are 2 groups, one values social interaction/bonding at work and the other doesn't. One group likes to have a mental image of who other people are, the other group prefers to focus on their work instead. The first group may even find it more difficult to work with someone with \"no bond\" whereas the other group just goes on with their work. The problem is, each of these groups project their vision of co-operation to the rest, that's why the first group insists on hybrid and the other group on remote-only (fortunately nobody except some PHBs insists on RTO...). If you can't understand the other group, it's difficult to have an agreement. My take is: work for the company that is in accord with your values and the ways you prefer to work. For me it's remote-only and there is no way I would change it, no matter what. reply ImPostingOnHN 3 hours agorootparentEven if a company prefers RTW (or partial RTW aka forced hybrid), that doesn't mean every employee does. The mismatch is not ideal. Additionally, this limits the company to either hiring from a fraction of the available talent pool, or enduring the strife you speak of. As a company grows, these issues compound and become more likely to present. That's why the ideal solution here is a choice arrangement (aka flex), where the people who want RTO or Partial RTO can self-organize in the office, and the others can self-organize elsewhere. reply aulin 12 hours agorootparentprevThe days I'm forced at the office I need to wake up two hours earlier, get home at least one hour later, usually two because for some reason I always end up doing overtime at the office. When I get home I still have chores to do which I usually do in the pauses when WFH. Little energy I have left I spend with the wife. People bond at the office because they have no personal time and energy left to bond outside. reply cebert 5 hours agorootparent> People bond at the office because they have no personal time and energy left to bond outside. That’s the goal. You are bound to the company. They want control and for your social life to be centered around work. reply jay-barronville 15 hours agoparentprevI’m with you on this. As much as I love the many benefits of working from home—especially as a dad—I find the work experience to be hit or miss, often feeling low-quality. You used to be able to have real connections with colleagues, but now almost everything feels superficial at best, whether through Zoom calls or disconnected Slack messages. One thing I especially miss is whiteboarding sessions with a couple of smart colleagues as we work on a difficult problem; you simply can’t replicate that feeling digitally. reply kkfx 12 hours agorootparentPersonally I found just issues with people not understanding WFH, those with no home office room, those who work on a laptop, always at the same desk of course and so on. There are MANY, but that's not a WFH problem, is simply a problem of training people to something they do not already know even if they practiced it for some years. The only who really do not work well from home anyway are those with \"home issues\" (familiar, of mere available space etc) and well, using the office as a way to leave their personal issue aside is not a good thing, nor the purpose of work. Beside that I think it's totally absurd in 2024 wasting enormous resources to build big buildings used for less than 12h/day, to commute between them in order to consume services (from transportation to ready made food), get exposed to physical ads (shop windows, mega-screens and so on), participating in rituals pushing people to consume fast fashion and fast tech, augmenting the enormous pile of polluting rubbish we produce just to save the giants of capitalism who can't live without the big city Barnum circus... People just need to learn and stop consider the home the place to sleep, a whole home used for just few activities, whole buildings used for just few hours, only to keep people pastured in old rituals is really untenable. reply ywvcbk 4 hours agorootparent> People just need to learn and stop consider Why? > of training people to something they do not already know even if they practiced it for some years You seem to be talking about something almost completely different than the comment you replied to was. reply kkfx 5 minutes agorootparentPeople need to learn because resources are scarce, so we can't keep up using big buildings for less than 12h/day, to move between them, just to consume services (from collective transport to ready made food, wasting money in fast fashion, fast tech status symbols (like wearable smart devices) in the meantime only to get exposed to physical ads (from shop windows to maxi-screens). Long story short we can't keep up the office as we can't keep up the paper model behind the office. You find the experience as a hit and miss because most companies and most people as well are not really ready to WFH, so the paradigm they follow it's dysfunctional. Zoom and Slack are good example of this dysfunctionality: chats are somewhat good in certain context to quickly ping someone, to pass a link, nothing more, chatrooms and co are waste of time from another era where VoIP was not possible for mere bandwidth availability. Video is almost never needed, screen sharing it's often needed, ability to sketch quickly is often needed and actually we do not have that much good tools for that but most people do not even realize they use bad tools and paradigm because they can't organize themselves for effective remote work. Even if individually we might find a good way to work until the entire company have found a way anything is hard. reply MisterBastahrd 16 hours agoparentprevI'm not a child. I don't go into the workplace looking for bonding. You know what bonding gets you when you get laid off? A cone of silence because you're immediately forgotten. If you are attempting to create a social life through your work environment, you're doing it wrong. This isn't to say you can't have friends at work, or create professional networks, but you should never pretend that your co-workers will automatically be your friends or want to be your friends, especially in competitive environments. It isn't good for you and it isn't good for them. reply s1artibartfast 15 hours agorootparentMy experience is the opposite. I bond with coworkers and get frequent job offers, career tips, and solid life advice. We go hang out, go camping together, and watch each others kids, It would be moronic to think coworkers will \"automatically\" be friends, but I have found work to be a great place to hunt for friends. Also, it helps to have friends in a competitive environment. It can be very good for you and good for them. You just have to collaborate to boost competitive advantage against everyone else. Friends that will talk you up to leadership (with justification) is basically a workplace hack. Not to mention being friends with your boss or boss's boss. Extremely good for you. reply iancmceachern 2 hours agorootparentI do all these things too, but I don't need to be required go into an office to do so. reply s1artibartfast 1 hour agorootparentMy post was in response to someone claiming that not only is it pointless to make friends with coworkers, it is actively disadvantageous. In contacts of the greater discussion, I'm sure some people like you are capable of developing these relationships virtually, while others are not. That said, I wouldn't trust a virtual coworker a thousand miles away to watch my kids, haha. reply zero-sharp 4 hours agorootparentprevI've had it both ways. As soon I leave, I don't hear from those people ever again. On the other hand, I've also made good friends that have lasted for years beyond the employment. The tone of your post doesn't really help. You're not a child so you don't go looking for bonding in the workplace? Haha okay Mr Adult. Jesus Christ. Forming relationships is such a basic human thing. reply scarface_74 16 hours agoparentprevSo for context so I don’t get accused of being antisocial, my job for three and half years until last year was (at least post Covid) flying across the country talking to directors and CxOs and ground level employees working in the consulting department (full time) for the $BigTech company based in Seattle. Before that, I spent years talking to the “business” as an in house architect for two companies. My coworkers are not my family nor are they people I need to “bond” with. We work well together, I do a lot of mentoring and teaching, etc. I’ve had 9 and hopefully soon 10 jobs in almost 30 years. Whatever bad things I can say about Amazon (and I have a shit ton of bad things to say about Amazon), they did teach me how to work in a mostly remote culture. The department I worked in was remote before Covid and is still exempt from the RTO requirements. I’m no longer working there Currently I keep in touch with 3 people I’ve ever worked with and one of those three is my wife. I don’t want to live in a high cost city and in fact I moved to Florida partially because it was a state tax free state and low cost of living. reply s1artibartfast 15 hours agorootparentThe money question is did you work in person with your wife, and do you think you would have started the relationship if you were remote coworkers? Maybe I'm small minded, but I struggle to imagine people starting office romances from remote relations. reply scarface_74 13 hours agorootparentThat statement might not have been clear. I meant in my entire career there are only three people I keep in touch with that I have ever worked with (well actually five) including my wife. I started dating my now wife in 2011. I wasn’t working remotely then. But at 37, I had an active social life and a large friend group of people outside of work from the gym. I was in the fitness industry as a part time fitness instructor. In an alternate universe I might have gotten serious about someone in my friend group. You would be amazed at how easy it is to meet woman as one of the few straight male fitness instructors. My wife and I moved to a new city last year. I make it a point to fly back to our former city around once a quarter to hang out with a group of friends to play cards, we travel together with another friend group at Least once a year and we end up in each others city for random reasons throughout the year and I go to my childhood home/where I went to college to go to football ganes with some of my college friends reply paulcole 13 hours agorootparentprev> My coworkers are not my family nor are they people I need to “bond” with. We work well together, I do a lot of mentoring and teaching, etc. What you’re describing is literally bonding. reply scarface_74 7 hours agorootparentIsn’t that the job of every “senior” developer? That’s part of the leveling guidelines at every tech company that I’ve seen. reply paulcole 4 hours agorootparentYes, isn’t it fun to learn that the thing you thought you didn’t want/need to do at work you’ve been doing this whole time? If you disagree, I’d love to know what you think bonding is. reply scarface_74 2 hours agorootparentSo doesn’t that go against not being able to “bond” remotely? reply paulcole 32 minutes agorootparentDid I ever say a team couldn’t bond remotely? You’re the one who felt as though “bonding” with people at work was beneath you — seemingly regardless of whether it was in person or remote — and who then went on to describe things you do at work which were excellent examples of bonding. reply bitwize 16 hours agoparentprevBonding too closely with your coworkers will only make it easier for them to find a spot to drive the knife into. I have a meetup I attend weekly to bond over tech stuff with more friend type people. Coworkers are coworkers. Nothing against them but they're not my buddies. reply ywvcbk 4 hours agorootparentHaving no other option besides willingly working in such an extremely toxic environment seems like the bigger issue, though. Although I’m kind of curious. Presumably those other people you interact with also have jobs? Would you still feel the same way about your coworkers if you worked with them instead? reply 7speter 12 hours agorootparentprevTell this to Kirk, Spock, and McCoy reply downrightmike 16 hours agoparentprevnext [3 more] [flagged] s1artibartfast 16 hours agorootparentThere is nothing wrong with bonding with your coworkers and enjoying your work. Why would anyone want to work a job with people they dont enjoy? Thats miserable. reply lazide 16 hours agorootparentprevYou might as well tell poor people to ‘stop being poor and get rich, so sad’. Sure, but how? reply gamesbrainiac 17 hours agoprev\"Office culture\" has been used as an excuse for many bad decisions for a very long time. reply majke 15 hours agoparentDont forget the invention of open spaces! Open spaces is proven to be inferior to closed offices, but cost cutting and flexibility in layout is most important for HR. reply downrightmike 16 hours agoparentprevCube Crawls is a thing because of culture reply MichaelRo 6 hours agoprevTo the people complaining that they can't make friends or maintain meaningful relationships outside of physical in-office contact, I wanna point them out to the classic SNL sketch \"Romano Tours\" where Adam Sandler is a tourism operator selling trips to Italy: https://www.youtube.com/watch?v=TbwlC2B-BIg Me, I have zero issues bonding with remote colleagues but then again, I have local friends too with whom I connect mostly through telecommunication tools (like WhatsApp) and occasionally meet in person. Quoting from the SNL sketch, and I can't stress this enough: \"If you are sad where you are and then you get on a plane to Italy, in Italy there will be the same sad you from before\". reply ergonaught 16 hours agoprevWeird comments. There's no major reported difference between hybrid and fully on-site in most categories, and the only thing they say about culture is that \"[90%] of hybrid employees say that the culture at their companies promotes community, collaboration, inclusion and belonging\", without bothering to report the statistic for fully on-site or fully remote employees. That may support their assertion (\"the idea that being on-site all day every day is necessary to establish and sustain a strong culture is a myth\"), but if the other categories were in the same range as with the other questions, it's not really saying much more than that. reply animal531 7 hours agoprevWhat employers haven't considered is that when employees come in they spend the majority of time with each other. That means that the culture they are building is one of us against them, where them is management and the upper echelons of the company (or any other group that doesn't seem to have their best interest at heart). Once they start working from home they are effectively isolated and not spending that time with each other. So I think we could argue that working from home is working in favour of the employer. reply tapanjk 13 hours agoprevResearch shows us statistical trends but what we care about most is what works for us. That can be answered only by an individual. In my case, I went from \"hurray! work from home!\" at the start of Covid lock-downs, to \"I miss the white-board discussions at office\". Fortunately, the tech industry is now large and diverse enough that you can find work that suites your style. reply iancmceachern 2 hours agoparentThis is the key thing for employers to understand. Be flexible to your employees style, and you'll have an easier time attracting and keeping happier , better performing employees. The employees desired style is what matters, not the employers. reply sahmeepee 9 hours agoprev> These may seem like marginal differences Yep. A difference of a couple of percentage points even with a sample size of 20,000 is tiny. While they have taken it to mean that hybrid is \"better\" than in-office, I would take it to mean that they are essentially identical across the breadth of the survey. For your particular industry or organisation there may be significant variation, but you'll need to find that out for yourself. reply quacked 15 hours agoprevI think that the adverse effects of allowing WFH pale in comparison to moving up and down the spectrum of strengths of companies within certain industries, to the point that WFH/WFO seems like an irrelevant question unless you're currently at the top of your industry. Here's my example: I work in the space industry, and the difference between SpaceX and every other vehicle provider is insane. The speed, quality, and average performance of people working within SpaceX's culture cannot be matched by anyone else flying hardware right now. It's gotten to the point where spaceflight feels like an elaborate farce, or a welfare program for aerospace engineers, unless you're working on a SpaceX project. I'm almost embarrassed to collect a paycheck, and I'm considered a pretty decently high performer at my own company. So--if my company chooses to WFH or WFO, so what? Maybe we lose some competitive edge with WFH, but all the employees are happier and retention may improve a little bit. We're not going to compete with SpaceX. We don't have the intellect or experience necessary. My guess is that this is how it is across all industries. There are a handful of companies competing for the \"top dog\" spot, and at these companies, WFO/WFH is a relevant question. If WFH causes you to lose competitive edge, it's worth disallowing in favor of retaining only hustlers who are willing to go for the throat and win, and thus either make insane profits or redefine technological standards for the next decade. For most of the other companies within the industry, it doesn't matter. You're not the top dog, and your corporation doesn't have the DNA necessary to get there. By definition 50% of employees are worse than the median quality employee, and yet they're still employed; how many are at your company? You may as well allow WFH to keep your people happy. Sure, you can eke out another 5-30% performance increase by mandating them back to the office, and that's the equivalent of 0.001% performance increase at whatever company is currently truly dominating in your industry. Is it worth it? Life is short. reply 3np 12 hours agoparentI find the fundamental argument flawed. Your employer is allegedly not competing with SpaceX in either case. Howver, you're still supposedly competing with other companies in your bracket? In that context, it could give a competitive advantage. reply thfuran 15 hours agoparentprevThe notion that a 25% across the board performance increase isn't a huge deal just because supposedly there's some other company in the same industry that's better makes no sense. reply quacked 15 hours agorootparentSorry, I should have been more specific in my last paragraph. I was talking about individual employee performance, not an across-the-board increase in performance across all company metrics like gross profit, etc. A 25% across the board performance increase would be a huge deal. If you know someone at work that doesn't really make much of a difference, there's not rally a big difference between him at 100% capacity and him at 70% capacity. Either way, he's not affecting the company baseline that much. That's how I feel about most companies I've interacted with so far. From what I've seen, your average lifer at a subcontractor for NASA doing 30% better on his monthly performance is about the same as your average SpaceXer doing 0.001% better. reply Swizec 16 hours agoprevLet me share some recent experience. After 4 years of working remote, I changed to a new company that’s doing hybrid. Didn’t need to move or anything like that. The commute sucks just by virtue of existing. You can’t beat the ease of swapping laptops. But I feel more onboarded into the company after a month of hybrid than I did after maybe 6 months at the old job. Kvetching over lunch is just nice. Getting to hear about the aches and pains of the codebase and various processes without having to schedule a bunch of 1on1s and polluting my calendar with meetings is fantastic. It’s my job to solve these things systemically and just being around to see the painpoints when people drop their guard and don’t feel like their words are recorded forever in a slack channel is freaking amazing. It makes my job so much easier and less forced. The best part is that my day is no longer full of meetings because we can resolve a lot of things informally. And yes I absolutely need the flexibility to work from home when it’s time for heads down focus mode. reply candiddevmike 16 hours agoparentIt sounds like you started very recently. I would be interested in your thoughts after working like this for 6 months - a year. reply downrightmike 16 hours agoparentprevnext [2 more] [flagged] lazide 16 hours agorootparentSo, apparently, do you? reply joeyagreco 2 hours agoprev> The researchers found that over three-quarters of hybrid workers feel like they belong, compared to 74% of fully on-site workers lol reply j45 17 hours agoprevDefault being remote or in person doesn't create company culture. Businesses in person or remote both have to learn to do what it takes to build company culture. reply blackeyeblitzar 17 hours agoprevEveryone knows this is a way to do a silent layoff without a WARN notice or severance. Or a way to discriminate against older workers with families, who don’t live in city cores. Everything else used to justify RTO policies is a lie. reply thelastparadise 17 hours agoprevI definitely trust PwC on matters of \"culture.\" reply SV_BubbleTime 17 hours agoprev [–] Let me know when you can find a work from home advocate that admits it is rife with abuse. Until then, it all sounds like game to me. reply bayarearefugee 17 hours agoparentYou'll have to define \"abuse\" more concretely for this to have any meaning. Do most knowledge workers actually put in exactly 8 full hours a day on work tasks working from home? Probably not. But as a professional software developer since the 1990s the same was true for myself and virtually everyone I know when I worked in an office. The way I 'waste' time is different now, but arguably a lot better (eg. me taking a nap just before or just after lunch improves my productivity in the afternoon a lot more than me fucking off on slashdot for extended periods of time in the office did back in the day.) (And whether or not this not-directly-related-to-work downtime is really a time waste is extremely debatable, as switching thorny problems into the subconscious realm often turns out to be one of the best ways to actually power through them in my experience). reply s1artibartfast 16 hours agorootparentWhat I consider abuse is Lying and padding resources. I know people who will turn an 8 hour task into a 40 hour task, and spend the difference hiking. The other grift is when people have multiple projects to tell each manager that the workload is high on the other project right now. reply redserk 15 hours agorootparentThis isn’t an exclusively remote problem though. This happens in-office, too. It’s extremely easy to pad a week with meetings, “I’m checking/organizing my email”, “I need to double check code from last week”, etc… if you wanted to burn time. Sure the remote worker has the opportunity to go hiking, but the business impact is the same. Except at least the hiking wastes less time from others. reply s1artibartfast 15 hours agorootparentSure, lots of people waste time in office too. I just see it a lot worse from friends and colleges who are hybrid and WFH. reply kkfx 11 hours agorootparentThis is not a WFH peculiarity, is the bad company architecture to blame, where workers can extort time and managers can't see that. It might happen more FH than FO simply because managers tend to have little to know digital culture, but it's still not a work mode peculiarity. The real issue is that with the big RTO push to save cities, who can't be saved BTW, avoided baking a WFH culture where people take WFH seriously providing an office room per worker at home, with all things needed there, instead of working with craptops anywhere. It's not different from people who take a photo of an email and send it via WA because they do still not having a clue about CC/BCC. reply surgical_fire 9 hours agorootparentprevFunny you say that. When I worked in the office, I knew people that turned 8 hour tasks into 40 hour tasks, and spent the difference slacking off. The most productive teams I worked with (only one exception) were mostly remote. And yes, I also worked with remote teams that were slacking off. What predicates performance in my experience is not if people are remote or not. It is the quality of the people hired and low churn. reply ImPostingOnHN 2 hours agorootparentprevWhy can't your managers or engineering leaders tell who is underperforming their potential? That seems just as much of a problem for in-office folks, if not moreso - the in-office folks can get away with simply looking busy. reply icehawk 14 hours agorootparentprev> The other grift is when people have multiple projects to tell each manager that the workload is high on the other project right now. Your problems might be a little bit more deeply rooted, WFH abuse sounds like a symptom and not a cause. reply iends 17 hours agoparentprevYou’re right. I work far more from home on average. When I worked from the office I had so many interruptions. We’d also take Wednesday and Friday afternoons to play table-top games. There was table tennis and foosball. Sometimes we’d all go out for lunch and it’d take 60-90 minutes. Now I just eat lunch at my computer, I don’t table top game or socialize. I just code and zoom all day. The abuse is real, but since I’m not commuting it’s a great trade off of an hour wasted each way. reply robwwilliams 17 hours agorootparentWhat a great response. And don‘t forget the 2 x 30 minute commute you converted to a solid hour of real work. Yes, the abuse is real ;-) reply s1artibartfast 17 hours agorootparentprevIt might be workplace specific. Most co-workers I know who went remote are 25-50% as productive as before. Only one or two did I suspect had a 2nd job. The rest were spending most of their workdays hiking, sailing, or playing with their kids. Given the choice, I would rather lead an in office team 10/10 times. reply theideaofcoffee 16 hours agorootparentAnecdote for exactly the other side: most co-workers I know who went remote are 50% more productive as before. They don’t have to waste their preciously short lives in pointless office drama, pointless meetings that could have been emails, waste their lives in cars commuting to a soulless office complex because an incompetent manager thinks that butts-in-seat time makes for a productive workforce. I would 100% lead a full remote team, and have done so, and will never go back. reply s1artibartfast 16 hours agorootparentIm not surprised. Like I said, it is likely workplace specific based on culture, management, and hiring. Some places you can coast for years working 10 hours a week. reply mozman 16 hours agorootparentprevI’ve been remote for 15 years with the past 3 in a C level position. People fuck around in the office and they do at home too. Expectation management and selling yourself (aka know how to communicate) is all that matters. I can sleep in, tell people the truth, and life is good. Offices are fucking expensive. It burdens IT and it’s a performance art at the end of the day anyway. The boards job is to fire me. As long as I maintain my roadmap and provide evidence of progress nobody who matters has time to think about this shit. Manage your SG&A expenses, risk, and be nice to legal. You’ll be fine. reply s1artibartfast 16 hours agorootparentThats cool for you, but I'm speaking from my experience about my team's productivity falling off a cliff. Employees that do work when in the office, but wont even respond to email for days when working from home. reply surgical_fire 9 hours agorootparentThat may me a problem of the pepple tou hired, and how you lead them, instead of being a problem of where they work. Perhaps they were slacking in the office as well, and only giving you the impression that they were busy. You might only be annoyed now because the mask is off. reply iends 14 hours agorootparentprevIf an employee cannot handle working from home then they can voluntarily return to office or be terminated for cause. I’ve been leading a team remotely since March 2020 and have had no issues with my team that wasn’t cleared up after a quick conversation in our 1-1. reply s1artibartfast 13 hours agorootparentThat's great for you. I have had good experiences long ago in other companies. In my current company, team leads have little input on employee performance, and nobody has been fired or PIPed in my 100 person department in the last 10 years. The primary thing that incentivized worker performance was social pressure and visibility from an in person environment. You might say this is a management problem, and yes, I would agree. However, it still stands that this management problem is greatly exacerbated by hybrid and remote work. reply icehawk 14 hours agorootparentprevPlease understand you are not representative of everyone. I'm aware of several teams that were heavily remote far before COVID. They worked fine then, and they work fine now. reply s1artibartfast 13 hours agorootparentMy very first sentence was saying I think it is different in different workplaces.... Thats my whole point. reply FridgeSeal 17 hours agoparentprevWhat are you calling abuse? Not showing up for “company mandated fun time o’clock”? Also, I hate to break it to you, but the employees that are “genuinely” abusing WFH and shirking work responsibilities are doing the same thing in the office. reply hunter-gatherer 16 hours agorootparentAbsolutely. In the WFH vs Office debate people seem to forget that time-abusing employees always exsisted. I've had coworkers at past jobs who sat next to me and nobody on the team coukd come up with anything they did for a year besides browse the internet and plan vacations. reply shahbaby 17 hours agoparentprevDo you want to optimize for output or optics? If output, WFH. If optics, WFO. If WFH is being abused then that's a problem with who you're hiring. reply mozman 16 hours agorootparentComment of the year. Know your requirements and find a match. reply crummy 17 hours agoparentprevLike, people working multiple jobs at once? Doesn't that become pretty obvious? reply mozman 16 hours agorootparentI worked 4 remote jobs for years long ago - I found companies with the same problem and resold my work. I probably put in 60-80 hours a week but made a killing. Now ADP and similar companies offer monitoring services to HR to weed these people out. You might get caught now. reply icehawk 16 hours agoparentprevDefine \"abuse\" first. Bonus if you add how its different from all the abuse you can do when working in an office. (I'll define \"abuse\" in my post after you define yours, I asked you first.) reply MBCook 17 hours agoparentprevYeah, some will. If managers are incapable of detecting and handling that unless people are in their eye line in the office then the company has bigger problems. I don’t think remote work encourages bad behavior on the whole. It may make what was already happening more obvious. But I think there are few good workers who become bad when remote. And they can stay in the office. I think it makes poor management more obvious and bad techniques like enforcing butts-in-seats as a your main way of enforcing productivity. I’ve seen plenty of people in my career do nothing or next to nothing all day. But they were in their seat clicking on things or typing so they must be “productive”. Instead of forcing everyone to be in office, how about getting better managers through training or hiring? reply WoodenChair 17 hours agoparentprev [–] Anecdotally, knowing many people that work from home, there is definitely abuse of the system. And those people are ruining it for the rest of us! But it would be absurd to think that if everyone was really more productive at-home, companies would still be forcing everyone back in the office. I don't believe the conspiracies—most workplaces really do want collaboration and productivity. The only bit I'll buy is that some are doing it as a form of soft layoffs. But I find it hard to believe that every company doing this would act against its own interests in terms of productivity. reply the_gorilla 16 hours agorootparentI've seen many absurd things working for various companies. I've personally seen millions of dollars set on fire over pet projects and egos, or petty power struggles. A sufficient justification to force people back into the office is because it gratifies a powerful man's ego to see people working in front of him. reply 000ooo000 17 hours agorootparentprev>But I find it hard to believe that every company doing this would act against its own interests in terms of productivity Ever worked in a company? reply WoodenChair 17 hours agorootparentAre you actually going to reply to the substance of my comment or just do a Reddit like sarcastic reply? Yes, I am middle age and have worked for multiple companies. I've seen co-workers, friends, and even family abuse the opportunity to work from home. Which as I said is unfortunate because it hurts everyone who uses it wisely. Show me or gp that there is no evidence of abuse. Anecdotally, I've seen it. reply surgical_fire 9 hours agorootparent> I am middle age and have worked for multiple companies Same. If you never saw a company kill teams productivity with completely retarded decisions and petty office drama, I think you might just have been lucky. reply ITB 16 hours agorootparentprevI think your original comment is entirely valid. It seems anyone who questions WFH gets downvoted. I’m involved with several startups, and I can assure you there are clear advantages to people spending time together. WFH or not, people meeting and strategizing in person is incredibly valuable. I also think people need to make a better distinction between remote and distributed teams. Because of the above opportunity to meet in person from time to time, and also because of the additional communication challenges of a distributed team with multiple time zones. There’s an assumption that all work can be perfectly partitioned and assigned to people. But more often than not, innovation requires constant exploration of ideas and negotiation of resources being invested. more cohesive teams are way more likely to innovate. reply icehawk 16 hours agorootparentprev>The only bit I'll buy is that some are doing it as a form of soft layoffs. But I find it hard to believe that every company doing this would act against its own interests in terms of productivity. WRT \"in terms of productivity\"? Companies do this all the time, and especially when they lay people off. (soft or otherwise) There's absolutely a loss of productivity as everyone has to sort out the logistical changes while doing the things they normally do. reply M2Ys4U 16 hours agorootparentprev [–] >But it would be absurd to think that if everyone was really more productive at-home, companies would still be forcing everyone back in the office. To draw this conclusion one would have to assume that companies are measuring productivity correctly. Or that the companies are acting rationally, and we know that's never always true. reply WoodenChair 16 hours agorootparent [–] > To draw this conclusion one would have to assume that companies are measuring productivity correctly. Or that the companies are acting rationally, and we know that's never always true. So is your argument that bad management is so endemic that every company is making a poor decision about its productivity when it orders workers back to the office? I think some companies want to do soft layoffs. And I think others actually see real benefits of working in-person. We don't have to take such extreme positions. Doubtless some people are more productive at-home, some people are less productive at-home. And some companies are actually seeing productivity decreases when their teams are remote. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A PwC report challenges the belief that a full-time office presence is essential for building company culture, contrary to mandates from companies like Amazon, JPMorgan, and Goldman Sachs.",
      "The research, involving over 20,000 business leaders and workers, indicates that hybrid workers feel more included, productive, and engaged than those working in the office full-time.",
      "Forcing employees back to the office can reduce their sense of autonomy and potentially harm company culture, as it may feel like surveillance."
    ],
    "commentSummary": [
      "A PwC report challenges the notion that working in the office 5 days a week is essential for building company culture, sparking debate on the effectiveness of remote versus in-office work.",
      "Opinions on the report's legitimacy vary, with some suggesting bias and others defending its findings, highlighting the subjective nature of the remote work debate.",
      "The discussion includes perspectives on hybrid work as a balanced approach, the potential for higher productivity and better work-life balance with remote work, and the role of management in preventing abuse and maintaining productivity."
    ],
    "points": 156,
    "commentCount": 200,
    "retryCount": 0,
    "time": 1726881553
  },
  {
    "id": 41607887,
    "title": "Porsche's idea for a six-stroke internal combustion engine",
    "originLink": "https://www.motor1.com/news/734156/porsche-six-stroke-combustion-engine/",
    "originBody": "NEWS REVIEWS FEATURES USA / GLOBAL Home Porsche News Porsche's Idea for a Six-Stroke Internal Combustion Engine Looks Brilliant The patented engine design features a special crankshaft that adds more power and compression strokes to the cycle. Porsche Sep 17, 2024 at 1:00pm ET By: Christopher Smith Porsche has patented a six-stroke internal combustion engine design. It uses a special crankshaft to create extra compression and power strokes per cycle. Nearly all combustion-powered vehicles use a four-stroke engine design. Porsche has revealed a strange (and possibly brilliant) idea for a six-stroke combustion engine. If you don't know the fundamentals of an internal combustion engine, we'll try to keep this simple. If you do know how engines work ... we'll still try and keep it simple. With very few exceptions every combustion-powered car uses a four-stroke engine: intake, compression, power, and exhaust. The intake stroke is where air and fuel come into the cylinder. Compression is when the piston pushes that mixture to the top of the cylinder. The mixture is ignited, shoving the piston back down for the power stroke. Exhaust is the final step, pushing the remaining gas out of the cylinder. Porsche Porsche designers reckon they can add another compression and power stroke to this process. Documents filed with the US Patent and Trademark Office specifically describe this as \"six individual strokes that can be divided into two three-stroke sequences.\" The added steps would occur between the traditional power and exhaust stroke. The first sequence, then, would be intake-compression-power, followed by compression-power-exhaust. To do this, Porsche's patent shows a crankshaft spinning on a ring with two concentric circles—an annulus. This alternates the center point of rotation, effectively lowering the piston's travel (bottom dead center) slightly for the added strokes. That in turn changes the compression, since the piston isn't traveling as far up (top-dead-center) in the cylinder. And that also means this engine has two top and bottom dead centers. Why all the complexity? In short, this design has the potential to generate more power with better efficiency. In a typical engine, only one stroke in four actually makes power. This changes the formula to one stroke in three, and it also burns up the mixture more thoroughly. Of course, the downside is added complexity. Whether the gains are enough to justify the design remains to be seen. As with many patents, it's possible this could never see the light of day. It's certainly an interesting idea, but perhaps more importantly, it suggests Porsche is working very hard at finding ways to keep combustion engines alive amid the push for electric power. More Cool Patents: Ferrari Wants to Build an Upside-Down, Hydrogen, Twin-Supercharged Inline-Six Mazda Patent Shows Another Rotary Sports Car, This Time A PHEV With AWD Get the best news, reviews, columns, and more delivered straight to your inbox, daily. SIGN UP For more information, read our Privacy Policy and Terms of Use. Source: US Patent and Trademark Office via AutoGuide ++ Share on Facebook Share on Twitter Share on LinkedIn Share on Flipboard Pin It Share on Reddit Share on WhatsApp Send to email Join the conversation Got a tip for us? Email: tips@motor1.com TRENDING Nissan: Regulations Killed the GT-R Evoluto 'Virtually Bent' the F355’s Chassis to Find Weak Spots, Then Cured Them With Carbon Ford Calls Chinese Car Companies an 'Existential Threat' The Porsche Cayman GT4 RS Manthey Is on Another Level What Could Make the Honda Civic Better? A Hatch and a Hybrid LATEST ARTICLES 1:00PM This Abandoned Corvette Looks Amazing After Its First Wash in 15 Years 10:00AM The 10 Best Cheap Cars to Buy: Consumer Reports 2:00PM We're Driving the 2024 Ford Ranger. Ask Us Anything 12:00PM What Makes a McLaren, A McLaren? 11:15AM Ineos Grenadier Production Halted, Possibly Until 2025 10:47AM Ford Recalls 144,000 Mavericks Over Freezing Rear Cameras 9:00AM The Best New SUVs Coming Out in 2025 About this article MakePorsche CategoryPatents and Trademarks ++ Share on Facebook Share on Twitter Share on LinkedIn Share on Flipboard Pin It Share on Reddit Share on WhatsApp Send to email",
    "commentLink": "https://news.ycombinator.com/item?id=41607887",
    "commentBody": "Porsche's idea for a six-stroke internal combustion engine (motor1.com)145 points by tempestn 12 hours agohidepastfavorite289 comments markhahn 2 hours agoI always wonder about premises in cases like this. a pair of compression-power cycles is a nice way to question the usual. has anyone gone through all the features of a conventional engine and asked: what if there's an alternative? for instance, what if we're just interested in range-extension? can we transform the motion created by combustion into electrical power in a clever way? cylinder-solenoid coils? are poppet valves so great? suppose we have some other mechanism to create the motion (solenoids?) or rotating valves? something electromagnetic appeals because it gives complete control over timing (rather than a crankshaft). rotary engines are appealing, for the same contrarian reasons. but they seem to either have practical problems (wankel) or don't seem to be making it to market (peanut-shaped rotors, etc). if 6 cycles makes sense (presumably in combustion physics), does it make any sense to burn in one chamber, then move those products to another chamber for some further (potentially different) cycle? would it help if you could ignite from more than a single place? multiple plugs sounds like a bit of a pain, but could you generate an annular spark? would you want to control the location-timing of the combustion front? does rotating-detonation have any meaning in this context? are there ways to reconsider the materials engineering of engines? make them dramatically cheaper, lighter? one of the best EV arguments is simplicity, but how much of current IC engineering is based on assumptions that can be broken? reply 01HNNWZ0MV43FF 1 hour agoparentCylinder solenoids are indeed a thing https://en.m.wikipedia.org/wiki/Free-piston_engine And the Prius uses the Atkinson cycle which is slightly different from the traditional Otto cycle somehow, although I couldn't find a good explanation https://en.m.wikipedia.org/wiki/Atkinson_cycle reply vvillena 34 minutes agorootparentAccording to that Wikipedia article, the modern Atkinson cycle trades time spent creating power in the compression phase for time spent extracting energy during the expansion phase, gaining efficiency at the cost of total power. reply dotancohen 10 minutes agorootparent> creating power in the compression phase Other than the potential energy of either the heat or the compressed gas, what \"power\" is created in the compression phase of a normal four stroke ICE? reply bestham 4 minutes agorootparentPumping losses are greater in a Otto cycle engine due to the increased resistance in compressing the whole stroke. In an Atkinson-cycle you get a longer power stoke than compression stroke and thus are more efficient. There is always another cylinder that get does i take during the short time when valve is open during compression in another cylinder. Thus that cylinder get some free assistance during intake. reply ryukoposting 53 minutes agoparentprevRotating valves exist - look up \"spool valves.\" Much like rotary engines, seal wear is the main drawback. Poppet valves also have the upside that they inherently spread air-fuel mixture out laterally. reply mpol 1 hour agoparentprevSince hybrids are now so popular, rethinking how a gasoline engine fits into that picture might be worth it. Question is whether the market will shift again from hybrids to full EV, and if so, when. Designing and building a new engine might cost 5 billion, without a real certainty you will earn it back. reply kevin_thibedeau 28 minutes agorootparentBEVs will never take over in North America. There are too many people spread out or living in places with no hope of charging infrastructure. People are going to balk when they find out they can only charge from a 20A circuit if even that is available. reply jjtheblunt 11 minutes agorootparentwho/what can only charge from a 20A circuit? reply DriftRegion 1 hour agorootparentprevThe Obrist Zero Vibration Generator is exactly that: reimagining ICE tailored to series hybrid application. https://m.youtube.com/watch?v=0s5Du7qrPoM https://www.obrist.at/powertrain/components/ reply ithkuil 2 hours agoparentprevIirc Koenigsegg is using camless valves (using solenoids?) reply Too 22 minutes agorootparentFreevalve yes. They’ve got loads of other cool thinking outside of the box solutions. Like the Lightspeed transmission with 7 clutches, providing instant shifting between any gears and ability to slip freely between them. reply cr125rider 2 hours agorootparentprevChristian and his team over there are actually pushing boundaries and doing really cool stuff. reply magicalhippo 6 hours agoprevI'm a programmer, not an engine guy. From the description in the article, they do one intake stroke, two pairs of compression-power strokes, followed by an exhaust stroke. Also, it seems the initial compression-power strokes are done with the piston moving lower, ie both lower top dead center and bottom dead center, hence would have lower compression, and the second moving higher so with higher compression. From my understanding of more fuel means less compression is tolerated before knocking[1], and vice versa. So do I understand it correctly that their idea then to make the first power stroke rather rich with lower compression ratio to eliminate knock, and the second at a higher compression ratio to burn the remaining unburnt fuel? Or the other way around, ie lean with high compression first? If so, it seems like an evolution of variable compression ratio engines[2]. edit: my morning-brain is having issues with thinking about how air-fuel ratio change in rich-burning vs lean-burning scenarios. So perhaps they aim for a good stoichiometric ratio and rely on the exhaust gasses to avoid knock when increasing compression the second time around? [1]: https://en.wikipedia.org/wiki/Engine_knocking [2]: https://en.wikipedia.org/wiki/Variable_compression_ratio reply i_am_jl 5 hours agoparent>more fuel means less compression is tolerated before knocking Generally, no. Knocking happens from pre-detonation, that's usually caused by heat from compression causing the fuel/air mix to ignite before it's triggered by spark. To avoid this engines will run a fuel/air mix that is not stoicheometrically ideal, to make the mixture less likely to ignite early. It is safer to run a fuel rich mix than to run a fuel lean mix as it keeps combustion chamber pressures low (unburned fuel takes heat out of the exhaust). It is more economical and more ecologically friendly to run fuel lean since you paid for that unburned fuel and it's kinda gross. In general, more fuel than ideal means more resistance to knock. But these things are complex. EDIT: Knocking happens from pre-detonation. Knocking can also happen from predestination, like in the case of turbocharged Subarus. reply hatsunearu 45 minutes agorootparent>Knocking happens from pre-detonation, that's usually caused by heat from compression causing the fuel/air mix to ignite before it's triggered by spark. No, it usually happens because the normal flamefront from the spark causes a rise in pressure that triggers compression-ignition in other parts of the cylinder. It's not solely from the compression, usually. That scenario is rare primarily because as you reduce the knock margin, you'd hit knock from what I said before you get to the state where it's so bad it ignites from compression alone. https://www.researchgate.net/figure/In-cylinder-pressure-tra... Look at this picture; this is a typical waveform of cylinder pressure vs. crank angle. The spark happens 28 degrees before TDC, so basically the left edge of each of the graphs. As the flamefront consumes the air-fuel mixture inside the cylinder after the spark, the cylinder pressure gradually rises. During knock events, the cylinder pressure as risen by the normal combustion process gets to a point where it starts igniting the fuel elsewhere in the cylinder, away from the gradually expanding flamefront. This causes rapid combustion which causes the pressure to rise suddenly, which causes damage to the engine (if severe enough) reply MrDunham 5 hours agorootparentprevFriendly correction for others because your auto correct failed you... \"predestination\", pretty sure parent meant \"pre-detonation\" A.k.a. Autoignition aka \"it goes boom before you planned on it\" Only adding this as it's a pretty crucial word for understanding the comment. reply rpmisms 5 hours agorootparentThat fuel is totally depraved, and has no hope of salvation. reply i_am_jl 4 hours agorootparentprevAh you're right, thank you! reply magicalhippo 5 hours agorootparentprevAh yes, read that engine knock article the wrong way around, guess my mind was drawing on intuition from things like gun powder where more stuff crammed more tightly together is worse. So then, if they're doing a non-ideal initial burn it would have to be a lean lower-compression burn, followed up by a higher-compression secondary burn? reply hatsunearu 35 minutes agorootparentPretty sure it's just a way to get more expansion from the same air charge. It's a similar idea to the Atkinson cycle. You have dissimilar compression and expansion strokes. In normal engines, there's a limit to compression ratio because if it's too high, it causes knocking. But a bigger expansion ratio lets you extract more energy out of the combusted gas, which leads to higher efficiency. The original Atkinson cycle idea was to use some complex linkage to get dissimilar compression and expansion strokes, but the way it's implemented in things like the Prius is to have a high compression engine, but mess with the intake valve timing such that you only use a small part of that compression during the intake phase so you effectively handicap your compression ratio to avoid knock, while still retaining the full stroke during the expansion phase. reply i_am_jl 3 hours agorootparentprevI am sure that I don't understand the specifics of what Porsche is doing, but the article says the cycle is intake-compression-power-compression-power-exhaust. For that to be the case they have to be burning the same fuel and air mixture twice, and I don't understand the chemistry there enough to even speculate how it works. I think it's safe to assume that the second stroke is burning incomplete combustion products left over from the first stroke. I think that the second compression stroke would have to be higher compression than the first in order to get more complete combustion of what's left behind. reply Ccecil 3 hours agorootparentThat actually makes a lot of sense merging with some comments from above. Second stroke goes lower and there are some ports to add air which are not accessed during the initial compression stroke...so the stroke is longer (higher compression) and more air is added to help with the reburn. Kinda sounds like combining the idea of the Miller cycle with a variable compression/stroke setup (see Nissan). There are a lot of ideas out there that create gains individually...glad to see them being combined more and more in modern engines. (ex. VRIS, VVT, DFI,) I personally think there is still another few decades of playing around with ICE to be done...not sure it will be viable for the market...but the research will lead to a lot more interesting engineering. reply magicalhippo 1 hour agorootparentAh, extra ports, that makes a lot more sense. I had missed that detail. So the first phase is like a regular 4-stroke engine, and the second phase is more like 2-stroke engine, where extra air (and possibly fuel) is introduced into the cylinder, like a 2-stroke, through ports located below the position of the piston during the bottom dead center of the first phase. So I guess you have something like intake (high), compression (high), power (high->low), compression (low), exhaust (low->high), where in parenthesis is the adjustable piston height? So a richer higher-compression first phase, followed by a leaner lower-compression second phase? reply Ccecil 1 hour agorootparentI suspect the second stroke is higher compression. First stroke has the compression \"shape\" controlled by the boost (miller cycle). With direct injection they could even be injecting more fuel into the cylinder for the second stroke...but I suspect the second stroke F/A ratio is determined by the first stroke remnants combined with the extra air allowed in at the bottom of the second stroke. All of this with the cam variators, timing control, boost control and fuel setup that VW already runs would be fairly easy to control with the proper sensors and code. Just speculating at this point...but it makes sense to me. reply lloeki 3 hours agorootparentprev> Miller cycle Heh all along I'm wondering, what kind of thermodynamic cycle is it? By six strokes, does it mean there are actually distinct new phases to the PV graphs compared to Atkinson/Carnot/Miller? Or is it just masquerading a well-known cycle underneath six strokes, only some parts are being optimised? reply Ccecil 2 hours agorootparentNot sure myself... Seems to be like it is a Miller cycle (or could be) on the initial 4 strokes. Which would allow you to control the timing of the \"Compression max point\" in the stroke by varying the boost. That may also vary the amount of spent fuel remaining for the second \"scavenge\" stroke...which if I am reading comments above correctly it pulls air from ports lower in the cylinder which would help clear the cylinder for the next 4 stroke cycle. Seems to me more focused on reburning/scavenging to make a \"cleaner\" burn than anything else though. *Not an engineer...just a shadetree mechanic who reads too deep into engine papers. reply adrian_b 4 hours agoprevThe number of the Porsche patent application: 20240301817 (which can be used on various sites, e.g. https://pat2pdf.org/ to retrieve the document) reply jtxt 3 hours agoparenthttps://ppubs.uspto.gov/dirsearch-public/print/downloadBasic... o reply kopirgan 4 hours agoprevKnow nothing about automobile engineering but somehow this feels like WordPerfect releasing a fantastic version for DOS when Windows was already capturing all the market. Btw they did that. Rest is history. reply randerson 4 hours agoparentI actually miss DOS. I could focus on one task for hours without being distracted by other apps trying to get my attention. There was something raw and engaging about being a few layers closer to the hardware without all the bloat. And, while I'd never go back to mechanical hard drives, you could hear the computer 'thinking'. It was more visceral. I think it's a great analogy. It's all the little quirks and flaws that make ICE cars feel like they have a 'soul'. The more you have to engage with it, the more it feels human and machine are having a conversation. Many car enthusiasts seek out manual gearboxes (despite being slower to 60mph than a modern auto), because it's _fun_ and gives you some mechanical sympathy. I own an EV, which is a fantastic daily, and a 911 for weekends. I've never felt like taking the EV out for no reason other than to enjoy a mountain pass. It's too heavy in the corners and too sterile. reply kopirgan 3 hours agorootparentInteresting perspective from someone that owns both. I only know ICE unless as passenger. Yes manual steering and gearbox are fun. The response is linear predictable. But if I were a kid entering college I'll be scared to choose automotive engineering as it's mostly likely to be wasted knowledge.. reply ryukoposting 33 minutes agoparentprevThere's another block of folks who look at Tesla like a company who's making great software in 1994, but it only runs on Windows NT. Yes, EVs are better in a lot of ways, but in 2024 there are severe barriers that make EVs impractical for a lot of people. Throughout most of the world, charging infrastructure just isn't good enough. Because cities have better charging infra than rural areas, EVs are at their best as commuter cars. Ironically, there was already an alternative to commuter cars that's hypothetically even better than EVs - public transit - and it also suffers due to lack of investment in infrastructure development. reply bayindirh 4 hours agoparentprevConsidering that people still love vinyl, automatic watches, fountain pens and hand made notebooks, I think this will still has its niche, and more interesting designs will follow to push the internal combustion engines forward. On the other hand of the spectrum, Hyundai Ioniq 5 N can emulate a sports car with an internal combustion engine and 8 speed sequential gearbox. reply mrangle 4 hours agorootparentICE aren't niche, and there isn't strong evidence that they will be. There doesn't seem to be a public Come-to-Jesus moment for electric cars on the horizon. There seems to be solid reasoning behind that reluctance. That is, it will be difficult to displace. If phasing out ICE comes down to regulations only, then the antique interface analogy doesn't fit. >On the other hand of the spectrum, Hyundai Ioniq 5 N can emulate a sports car with an internal combustion engine and 8 speed sequential gearbox. Sports cars are sometimes for vanity, which is a role that a Hyundai wouldn't fill. But you referenced driving characteristics. Fair enough, and so no need to talk about the vanity attraction of a future all electric Porsche. When not for vanity only, sports cars are for people who like a driving experience. Sports car culture is strongly critical of any deviation from an ideal experience even with better ICE cars. Therefore, it tends to detest \"the other end of the spectrum\" the most. Sports cars are integrating electric motors, but mostly as horsepower and torque supplements for ICE. The most well regarded sports cars for the common man err toward being ultra-light weight, relatively low power, and high rpm with a manual gearbox. With the rest being as analog as possible. With incremental deviations from that ideal only as preferred for specific owner comfort. None of that criteria speaks to an appropriate / desired role for an electric motor. reply freeone3000 4 hours agorootparent>need to talk about the vanity attraction of a future all electric Porsche Future? You can buy an all-electric Porsche Taycan since 2019 - and it’s faster than the 911. Sports cars are fully adopting electrification, due to the huge torque numbers and high scalability. M-B and BMW of course, but also Porsche and soon to be Ferrari. It’s not just hybrids used for a boost (as in the decade past): those are full electrics. reply brookst 3 hours agorootparentUnfortunately, EV’s are incredibly heavy, so “faster” is scoped to straight lines. As soon as you try to wrestle the 2300kg Taycan through twisties, the 1500kg 911 gets faster. And it’s still massively overweight compared to proper sports cars (the 911 is a GT). reply okdood64 1 hour agorootparentprevSerious [sports] car enthusiasts don't care about faster; especially not in a straight line. I by no means fall into this category completely, but I much rather have a moderately slower car than an electric, with real engine/intake/exhaust noise that handles well on turns. reply mrangle 3 hours agorootparentprevWhat's your point? Future or now, my point about vanity remains. I don't see any point in arguing over my use of the word \"future\" here. Again, \"faster\" isn't the most important metric for car enthusiasts. Which is what I described in my post. I know that's disappointing to people who would like it to be in order to claim total justification for electric motors. To say that \"sports cars are fully adopting electrification\" seems to want to imply that sports cars are moving mostly to full electrification. This isn't remotely true. Their customer base wouldn't stand for it. reply grvdrm 1 hour agorootparentUnder-appreciated point you make: not about speed. I own a 2014 Boxster S. 315 hp, 266 lb/ft torque, 6-speed. It is NOT the fastest car out there. In fact I own an automatic 3-series BMW that's faster in a straight line every time. But the Boxster is under 3k pounds curb weight. It is laser-precise on the road. And it sounds glorious, especially with sport exhaust. I'm often driving between 20-35 mph in second gear because it's the best day-to-day way to hear the engine's sounds. Otherwise, there are people out there buying 911s/etc. because they can rather than because they care, and those people don't care that Porsche is moving sports cars to hybrid, or less interested in putting manuals in their cars. But lots of us still want the pure sports experience. reply jasonwatkinspdx 3 hours agorootparentprevYour understanding is very out of date. All electric sports cars have been a thing for a while, even at the supercar level. Car enthusiasts have in fact embraced electrics. Go on youtube and look up drag race comparison races and a Model S Plaid is very frequently the one people want to beat. Double clutched automatics have taken over h pattern manuals. reply mrangle 3 hours agorootparentMy understanding isn't out of date whatsoever. Your understanding is wishcasting. You citing the appearance of the Model S Plaid at \"youtube drag races\" reflects more of your lack of understanding than my own. The double clutch vs manual debate isn't relevant to the one under discussion. Its an interface argument that is more niched than ICE vs electric, is specialized to each particular car, and each particular use and driving preference. reply _DeadFred_ 1 hour agoparentprevI think it's more like every keyboard player still lusting after physical analog synths even though everyone has perfectly usable, deterministic, digitally perfect software synths for a fraction of the price (or even the amazing free version of Vital). We are talking hobbyists not corporate office software users. ICE is boring to my monkey brain looking to be entertained/engaged even if vastly superior. reply kleiba 3 hours agoparentprevIn Germany, where Porsche is from, sales of EVs are on the decline. The same is true for other countries in Europe as well. https://www.teslarati.com/electric-vehicle-eu-sales-drop/ reply kopirgan 3 hours agorootparentThink this is short term. Again no expert just based on what I see. Solid batteries give it few years then the maths will be very different reply kleiba 2 hours agorootparentNot sure. Germans are traditionally both tech xenophobes and cheapskates - two arguments against EVs. reply generic92034 2 hours agorootparentRegarding \"cheapskates\" - I think you have to see the EV prices in combination with typical German salaries. If a middle-class EV costs more than the yearly gross income of an average German software dev, you know there is not much room for rising sales figures. Especially figuring in the large difference between gross and net income here. Add the lacking charging infrastructure and the current decline in sales is no surprise at all. reply meiraleal 3 hours agorootparentprevIn August. After a 38% tariff. reply kleiba 2 hours agorootparentCheck out the diagram at the top of this article: https://www.autozeitung.de/assets/styles/article_image/publi... As you can see, compared to 2023, the sales numbers have been worse month for month in 2024, not just in August. reply typon 2 hours agorootparentprevThis says less about consumer habits than about thr quality of EVs being sold in Europe and North America reply kleiba 2 hours agorootparentThe main reason is price. reply teo_zero 10 hours agoprevBut isn't the chamber full of exhaust at the beginning of the second power stroke? What will burn? reply labcomputer 4 hours agoparentIt looks like the patent describes a weird hybrid of a conventional 4-stroke and a uniflow 2-stroke. At the bottom of the first power stroke, the cylinder drops lower to expose scavenging ports. That both forces air in (at the bottom of the cylinder) and helps push exhaust out (through conventional exhaust valves). Uniflow 2-strokes tend to have high thermal efficiency, but poor emissions, especially particulates. So the idea here might be to gain some efficiency without another emissions-gate. reply OJFord 10 hours agoparentprevIt says the goal is efficiency, so I assume the point is that if the first does say 80% combustion, then another cycle eeks out another 80% of 80% or whatever. (Brb, have an 8 stroke engine to patent.) reply UniverseHacker 5 hours agorootparentWhen Porsche says efficiency they mean making more power, not using less fuel. Porsche has a long track record of adopting efficiency tech and using them to make fast cars that still use a ton of fuel. reply OJFord 2 hours agorootparentI'm not sure if that's supposed to be a correction, but if so I don't understand. The description of the two new strokes is as an addition to the normal four, one of which is the fuel intake, I was not assuming that decreases in quantity or anything. It could do, it's just orthogonal to the point - if you don't entirely combust whatever amount you inject in the first ignition , then the idea here aiui is to compress & ignite again to, yes, get some more 'power' out of it. reply dahart 5 hours agorootparentprevWhat’s the difference between getting more power out of a given quantity of fuel, versus using less fuel to achieve a given power output? reply Ccecil 3 hours agorootparentMy assumption is there is a context difference. Efficiency in fuel/mile. Efficiency in Power output/liter of displacement. But that is just my assumption. You can have a relatively small engine and force a ton of air/fuel into it under boost and get tons of HP but it tends to lose the ability to maintain fuel economy. Tune it for fuel economy and it tends to lose power. It is very difficult to have both in the same package for many reasons. Adding mods like discussed in the article start to allow for the overlap to be wider. reply serial_dev 4 hours agorootparentprevExtremely simplified, when you drive a sports car, you want to maximize power (work per unit of time), period, you don’t care about fuel. If you want to go the farthest with a full tank, you want to maximize the total work (proportionate to traveled distance) for unit of fuel. Reality for practically all cars are between these two extremes, you want to enjoy driving, go relatively fast, and still not wasting 25 liters for traveling 100 km. reply dahart 4 hours agorootparentThose are correct statements that don’t answer the question I asked. You’ve explained the difference between not caring about efficiency and caring about efficiency. The article said the 6-stroke design is for “efficiency” and did not say Porsche doesn’t care about fuel and only wants to maximize power, contrary to the comment I replied to above. If an engine gives you more power with a fixed quantify of fuel, then it must also give you less fuel for a given quantity of power, right? There’s no such thing as more efficient only for power. BTW Porsche makes consumer vehicles, not just race cars. And they make engines for other types of vehicles, not just cars. It’s ridiculous to claim that Porsche doesn’t care about fuel just because they happen to make some race cars. Nothing in the article suggests this design is for a race car, nor would it; a patent is designed to be broadly applicable and if a 6-stroke design is shown to be more efficient than a 4-stroke design for fuel-efficient consumer cars, you can bet Porsche will be happy to sell you the engines or license the design. reply OJFord 6 hours agorootparentprevOops, another 80% of the remaining 20%, I mean. (But also I would guess it is probably less efficient than the initial one, not another 80%.) reply yobbo 6 hours agoparentprevMight also be possible to inject water into the compressed exhaust gasses. The water evaporates and creates the second power stroke, while lowering the temperature of the exhaust gasses. There's quite a lot of energy left in the exhaust gasses otherwise. This is not a new idea but it creates mechanical complexity and higher requirements on the materials of the piston and cylinder. reply Ccecil 3 hours agorootparent6 stroke diesel prototype. https://www.autoweek.com/news/a2063201/inside-bruce-crowers-... edit: Description before link reply Maledictus 6 hours agoparentprevGiven there is enough oxygen left, they can inject more fuel. reply Szpadel 6 hours agoparentprevthat might be something similar to why diesels mix exhaust gas with fresh air, but that might be just to purify exhaust gases better - I'm no expert reply Neil44 8 hours agoparentprevMaybe the first cycle is super lean, leaving lots of O2 and CO still to burn. reply nikanj 8 hours agoparentprevYou can overfill the cylinder for the first stroke, because the unspent fuel will burn on the next cycle instead of being exhausted. High power engines exhaust a lot of unburned fuel, because you can't guarantee an exact 100% fill for the cylinder, and there's more power to be had at 120% fill than 80% fill. Oversimplified of course. reply ethbr1 6 hours agorootparentBut you don't have an additional intake stroke, so there's limited oxygen in the second power stroke to burn. It sounds more like they're running fuel-lean, then possibly adding more fuel before the second compression stroke. reply HPsquared 6 hours agoparentprevSounds like EGR reply rikthevik 4 hours agoprevIf I remember correctly the engine in the Mazda Millenia was a Miller cycle engine and did something unconventional like this. Props to Mazda for trying new stuff. https://en.wikipedia.org/wiki/Mazda_Millenia reply Ccecil 4 hours agoparentMiller cycle (IIRC) was a bit different. It simply kept the intake valves open longer and used boost from it's supercharger to emulate a valve \"closing\". It used a boost level that was significantly higher than a supercharged Otto cycle would use. Volkswagen is doing something similar with the EA888.3B (B cycle) motor that is in the most recent Tiguans. https://www.motortrend.com/features/inside-volkswagen-ea888-... This six stroke is doing something a bit more complex...more impressive, IMHO. Similar in complexity to what Nissan is doing with their variable compression engine they are currently using https://www.nissan-global.com/EN/INNOVATION/TECHNOLOGY/ARCHI... There was also a 6 stroke diesel a while back that injected water into the empty cylinder after the exhaust stroke to gain an extra compression stroke from waste heat...Bruce Crower (Crower Cams fame) built one years ago but I never heard anything more about it. https://www.autoweek.com/news/a2063201/inside-bruce-crowers-... reply jeffbee 2 hours agoparentprevMazda has marketed half a dozen weird engines, all of which were crap. It has been a remarkable streak of stubborn, unwanted innovation from such a small carmaker. reply bell-cot 6 hours agoprevNot to say that modern IC engines are any sort of \"simple\"...but there appear to be a lot more high-precision moving parts, under load, in their clever new crankshaft assembly. (Vs. traditional 4-cycle IC engines.) Obviously, Porche's target market isn't likely to care about that. But for possible down-market uses of this technology - are there any mechanical engineers in the house, to comment? reply UniverseHacker 5 hours agoparentI drive a 24 year old Porsche and the engine already has a lot more mechanical complexity than most other engines- especially for their variable camshaft timing system. You’re right that Porsche and their target customers don’t care much. I’d say German engineering culture in general is to make things work better (when new), and not worry about complexity. reply nolan879 3 hours agorootparentI Do Cars did a tear down of the M96 engine from a 986 Boxster S. Seeing inside of them, it makes sense how these motors cost $20K to teardown, rebuild and remedy Porsche's cost cutting in their first water cooled engine. I would own another 986 or 996 in a heartbeat. Tear down video: https://youtu.be/qrkALiq5hTU?si=0OmBKYcim-cflJEy reply brookst 3 hours agoparentprevNot a ME but many modern ICE’s have tons of mechanical complexity in the valvetrain to handle varying timing and lift. From multiple valves driven by different camshafts to various ways to switch between camshafts or rotate cams, this stuff gets crazy. Presumably Porsche’s design still has all of that, too. reply osigurdson 3 hours agoprevI'm sure one day we will look back and laugh about how we used to install actual mobile power plants in our cars. But as of today, they are still objectively better on many criteria. reply martinky24 3 hours agoparentWhat makes you so sure? We don't look back and laugh at people using horses for transportation. We understand that's what they had at the time. reply osigurdson 2 hours agorootparentIt personally strikes me as a little humorous, particularly from the perspective of a human in (say) 2150. But, not in a mean spirited way. Of course, much respect for the pioneers of the ICE engine and on-going improvements today. reply everyone 9 hours agoprevI'd love to see a decent effort with some funding behind it to make a rotary vane engine.. https://youtu.be/UPFFXBAe5mc?feature=shared reply outside1234 1 hour agoprevReading these articles is like when you used to read articles in 2005 about people \"innovating\" on OS/2. The market has moved to electric (see China) and Porsche would be well served on investing there versus on OS/2 (nee ICE engines). reply m463 1 hour agoparentThere was quite an overlap between the use of bows and guns. Bows were reliable, accurate, quiet, would work in the rain and reloaded quickly. Guns took a long time to exceed them. I think there is still quite a bit of time left for internal combustion engines. Long distance is a big one, as is racing. Fuel is lightweight and can still be added quickly. reply akira2501 32 minutes agoparentprev> The market has moved to electric It has not. > (see China) Understand the impact of subsidies. > versus on OS/2 (nee ICE engines). There is no roadmap to an electric plane in your lifetime. ICE engines are going to be here longer than you are. reply bluGill 1 hour agoparentprevWhile you are onithe right track, the market may not be moving quite that fast and to a small amount of ice inventment may be needed. reply _DeadFred_ 1 hour agoparentprevIt's all in a weird state. Tweaking ICE engines was a fun hobby with lots of 'consume' products to dream about. Electric just out of the box stomps ICE, but not in the same 'acquiring things and putting together a puzzle' type way that feeds our monkey brains. That said Porsche is dead and the super wide Audi's pretending to be Porsches just because they have a Porsche skin are lame. reply kopirgan 4 hours agoprevKnow nothing about automobile engineering but somehow this feels like Wordpress releasing a version for DOS when windows already captured most of the PC market. reply ttwwmm 12 hours agoprev [204 more] [flagged] Szpadel 10 hours agoparentI think many of people here live in US and assume that it everywhere things look similar. in my European country electric infrastructure is not ready for electric cars and with current energy prices this makes no financial sense. I own plug in hybrid and I live in apartment meaning I have no possibility to charge at home. near me (about 10 min of walking there is public single phase allow charger with 2 plugs machine maxing 3.5kW total (for both)) and if I'm lucky I manage to find free slot one a week, but usually that's one every two weeks. This is only \"free\" charger in reasonable distance from me, and free means that I can use it when I have ticket for public transportation with I need to have anyways. there are some normal chargers, but they cost 2x for slow charging or 3x for fast charging than energy prices here + you have to pay per minute of taking parking spot. in summary when I calculated how much does it cost $/km it is very similar to gasoline but it's much trickier to recharge. I would love to own fully electric but without also owning house with solar this makes no sense right now. reply thebruce87m 8 hours agorootparent> in my European country electric infrastructure is not ready for electric cars and with current energy prices this makes no financial sense. I’m always amazed by statements like this. People who have lived through the rollout of the internet to the masses can’t see how we’d modify existing structure to accommodate needs. We had phone lines initially, 20 years later now we have 5G, fibre, satellites. They just dug up every street in my European town to put fibre internet in. We already had broadband in the town. They dug up every street to give us fibre that most people won’t even need over their existing connection. People look at this and say “how will we charge electric cars” when we are already surrounded by electricity. reply tirant 7 hours agorootparentNot even that: We have managed to extract some dense and flammable black oil hundreds of meters deep under the ocean soil, transport it thousands of km again to remote countries where it is processed and again transported and distributed daily to many storage locations where any layman can again transfer it to their own vehicles. That is a feat. Electricity infrastructure already exists everywhere where we have civilization. And production is usually just hundreds or less km away. For lots of people it even happens at their own home. The average amount of km every European drives is around 40km. For a standard EV that is around 7,2 kWh of energy per day, which can be charged in 2-3 hours from a normal 240V Schuko plug at home. If there’s no option to charge at home, a weekly load on newer EVs with around 500km range would be more than enough, taking around 3-4 hours on standard 22kW chargers or less than 2 hours in a DC 50kW chargers. EVs are a real option for lots of people. Main blocker for most of people according to most statistics is lack of knowledge on how to operate, and as a consequence fear of the unknown. reply Szpadel 6 hours agorootparentThat would be fine for second car. I need form time to time visit family that is about 270km one way mosty on a highway The 500km mark is calculated in ideal conditions, flat street at 60km/h or something like that in more realistic highway conditions it's about 300km That's why plugins have so much sense even when they cost about the same that EV would. I can drive fully electric on everyday short distances (my car is rated for 33km of electric range, but I'm able to squeeze about 40 by careful eco driving) That usually covers my city driving and at the same time I have 700-1000km backup of ICE/hybrid range In my observation hybrid approach have about 30% better fuel efficiency than pure petrol. I agree that having 22kW charger nearby that is affordable could solve all of that. But for many people that is not the case and if so they are so expensive. In my case petrol costs me about $0.12/km and electric from supercharger would be $0.14/km (assuming 19kW/100km). Why would I want to switch to more expensive car to pay more per driven distance? Of course everything changes when you have solar installation and you have basically free electricity (of course I omit here installation cost that would make if free after about 10years or so) then driving basically everyday for free and paying for fast charging few times a year when doing very long distance trip makes is fine. But again, not everyone can have solar installation. I have high hopes for new sodium batteries, give them 2 more generations and we could afford ev cars with 1000km of range in cars less expensive than ice and that would be very compelling. reply whatevaa 7 hours agorootparentprev500km range cars are still stupidly expensive right now, and the fact that paid chargers are quite expensive compared to home charging remains. This may change in the future, but we live in the present. People aren't that stupid as you think, you are just ignorant. EV preachers take the easy route and ignore issues which are inconvenient to them, or focus only on home charging. reply tirant 6 hours agorootparentThe average price of a new car sold in Germany for private individuals is 42.000€. Around $47,000 in USA. Prices are even higher if you include company or fleet purchases. A RWD Model 3 and Model Y with around 500km range is even less than that right now in their available stock. How can that be stupidly expensive if it’s even below average price? Street chargers are more expensive than home charging, that is right. However it is still cheaper than gas and diesel by 50-70% by average in mainland Europe. You can look up facts instead of calling me ignorant. reply jvanderbot 7 hours agorootparentprevWe have two massive parallel energy transfer systems in the world. And they are massive. Consider the electrical grid for all it's behemoth costs and regulations, at least it exists and is a known monster. Doubling it is a technical challenge but not an unknown. Akin to laying fiber. On one side of the grid you could even just change how the energy is produced at a few thousand plants and have zero change on consumer side if you wanted (for a given territory the size of a USA avg state) Now imagine we came along and said we were going to add to the electrical grid a parallel system of transporting explosive liquid to billions of distributed combustion engines. Not only that, the explosive liquid is only available in a few politically charged areas in the world. Add all the tankers refineries pipes truck lines holding tanks pumps etc so that most people live at most a few miles from a holding tank that will distribute hundreds of gallons to them for their own micro power plant that only exists for their commute. It seems crazy. We built both systems incrementally over time so we're in a massive sunk cost analysis whenever we think about replacing gas cars. reply acchow 8 hours agorootparentprevNaturally, the infrastructure arises when the demand is there. But nobody was ever hesitant to buy a 4G-only phone for fear of insufficient 4G service because every 4G phone also worked on 3G. The same is not true of electric cars. People have real mileage anxiety because they can’t use petrol when a charger isn’t available reply ajb 7 hours agorootparentprevThe big question is not whether the infrastructure will be built, but whether it will owned by rent-seekers who charge everyone without their own driveway a massive premium to charge their vehicles. reply albertopv 7 hours agorootparentprevTransferring so much electricity to everywhere in an efficient way is more expensive and harder though, not counting electricity generation, only partially clean, noy enough, very expensive. reply fragmede 7 hours agorootparentprevYou need to acknowledge that in Europe, the high price of electricity mean consumers aren't getting any discounts in money/distance ($/mile being inappropriate in the EU), if not being charged more to charge an electric vehicle over an ICE vehicle. Saving the environment is nobel, but affording it is another thing. If price hikes in the US continue, there are places in the US where that may soon be the case, too. Then we're screwed. Thankfully hybrids aren't going anywhere. They may be more complicated, but they have the most critical feature, regenerative braking. instead of shedding the energy spent slowing down as pollution and heat, it gets saved and is used to move the vehicle later. reply thebruce87m 6 hours agorootparentI can’t acknowledge anything without numbers to compare. In the UK some suppliers give cheaper rates at night (1/4 the rate). Is that not the case elsewhere? What is the cost per mile in Europe vs ICE? reply xcv123 8 hours agorootparentprevYes we already have power cables but they do not generate power. The issue is clean power generation capacity. reply thebruce87m 7 hours agorootparentThere is a reason that I pay 7p/kWh (3p/mile) to charge at night compared to 30p/kWh during the day. There is already spare capacity at night. In my country it’s mostly wind. reply xcv123 7 hours agorootparentIs your country 100% EV now? reply thebruce87m 7 hours agorootparentNo, but even if it was demand would still be lower than 2002: > The highest peak electricity demand in the UK in recent years was 62GW in 2002. Since then, the nation’s peak demand has fallen by roughly 16% due to improvements in energy efficiency. > Even if we all switched to EVs overnight, we estimate demand would only increase by around 10%. So we’d still be using less power as a nation than we did in 2002, and this is well within the range the grid can capably handle. https://www.nationalgrid.com/stories/journey-to-net-zero/ele... reply jbgt 8 hours agorootparentprevIf I may disagree: I live in a flat with a fully electric vehicle. It's just a matter of planning around charging a bit. Once you think ahead (and it's not dramatic), you don't notice anymore. I find plug in hybrids make no sense: electrification is the future. reply wkat4242 8 hours agorootparentI would really hate having to keep going out to move my car to a charger, then go out again to move it back to another parking spot when it's done (usually you pay a fine if you keep it at the charger). It's just so much work and it means I can't sit down and relax. And parking spaces in my area are extremely limited at night so every time I move I'd have to worry about finding one again (and a free charger too obviously which also seem to be extremely rare, I just know of one about 1km away) For me a car is about convenience and having to worry about it and going out and doing things with it when I'm not even using it is the opposite of convenient. It should just be there and ready when I need it and forgotten when I don't. And it should be as cheap and low-maintenance as possible. If every parking space had a charger and I could just plug it in and forget about it when I park it, it'd be acceptable (in fact better than an ICE because at times I also had to remember to refuel it for an early trip). What adds to this also is that I don't really have a 'routine', my life is really ad-hoc. So for me it would be a huge dealbreaker to have an EV until chargers are everywhere. But right now I have the perfect solution anyway which is not even having or needing a car at all :) My city has amazing public transport and 20 euros gets me an unlimited monthly travel pass. I don't even like spending money on cars, and when I did still own one it was usually a 1500-2000 euro old banger meaning that EVs are out of my price range anyway (even used ones will never be that cheap). reply rafaelmn 8 hours agorootparentprev> I find plug in hybrids make no sense: electrification is the future. Those of us living in the present prefer to be more flexible and have the best of both worlds (electric commute, ICE for long trips) reply JSR_FDED 8 hours agorootparentprevReally, just a bit of “planning around charging”? The less predictable your life is the less this is an option. The most extreme example of this is Hertz having to walk away from billions invested in 100,000 Teslas. Short term renters simply don’t tolerate the uncertainties that EVs bring. reply whatevaa 6 hours agorootparentMaybe this person neighbours a Tesla supercharger, who knows reply mgkimsal 8 hours agorootparentprev> I find plug in hybrids make no sense: electrification is the future. They make sense depending on how far away that future is, and I think that 'future' is way different for different parts of the world. reply threeseed 8 hours agorootparentprevI assume you live in the US. Because in almost every other country there is a real lack of decent charging infrastructure. Especially outside of densely populated areas. reply oezi 8 hours agorootparentI think Europe and China aren't doing so bad on charging infrastructure. reply threeseed 7 hours agorootparentIt's not bad but it's nowhere near what the US has. Not just availability of chargers but also the average kW rate. reply oezi 2 hours agorootparentThe following link says 5x more chargers in EU than US (and twice as many EVs): https://apricum-group.com/ev-charging-infrastructure-race-in... Rapid charging is now widely available (certainly could be more) reply freedomben 6 hours agorootparentprev> I think many of people here live in US and assume that it everywhere things look similar. Most of the US is in the same boat as you. Where I live, there is nowhere to charge and there are fairly great distances between things. You rarely ever see electric cars here, because they are only useful for people who have invested quite a bit in their house to build a charging station. The people here on hn that have the attitude you mention, are people who live in large populated regions, especially congregated around the coasts. I'm all for electrifying, and I think it is very necessary, but people do love to put the cart way in front of the horse. reply njarboe 6 hours agorootparentFor now electric cars make sense for those who have a way to charge at their home. Otherwise not so great. reply tuna74 6 hours agorootparentprevMy condo (sort of) put chargers for every parking spot and garage in the whole \"complex\". Wasn't that expensive and raises the value of the units more than the cost per charger. reply bn-l 9 hours agorootparentprevSo spend on that infrastructure instead, or mandate it. Honestly, spending resources on anything ICEs (outside of special industries) in this day and age is just wrong. reply LightBug1 8 hours agorootparentThey're called sports cars for a reason. They're driven far less than everyday cars. They're fantastic fun. I'm an enviromentalist and I'd still buy an ICE Porsche. No EV has come close ... only the Hyundai ioniq 5 N seems to be looking interesting. reply hnaccount_rng 7 hours agorootparentWhat are you looking for? Typically when people say sports car they mean high-torque (and thereby acceleration)... which ... you will not get an ICE motor that gives you similar performance to an electric drive reply tonyedgecombe 8 hours agorootparentprev>I'm an enviromentalist and I'd still buy an ICE Porsche. Environmentalists walk or ride bikes. reply tirant 7 hours agorootparentAccording to whom? I am also an environmentalist and burn some fuel from time to time, I’m not a zealot. For me to operate and support the environment the best I can I also need to be mentally fit and have life enjoinment. And riding my old classic carbureted Triumph helps me with that. reply tonyedgecombe 5 hours agorootparentAccording to my dictionary an environmentalist is \"a person who is interested in or studies the environment and who tries to protect it from being damaged by human activities\". I can't see how buying a Porsche (or any car really) is compatible with that. reply chromatin 7 hours agorootparentprevhttps://en.wikipedia.org/wiki/No_true_Scotsman reply tonyedgecombe 5 hours agorootparentWhere do you draw the line then? If I spend the week jetting across the Atlantic but then take a reusable bag to the supermarket am I an environmentalist? reply LightBug1 5 hours agorootparenthttps://www.logicallyfallacious.com/logicalfallacies/Appeal-... https://en.wikipedia.org/wiki/Reductio_ad_absurdum reply _factor 8 hours agorootparentprev“I’m a vegetarian, but I’d still scarf down a steak.” reply tirant 7 hours agorootparentprevFortunately the definition of wrong is very personal. We still have at least 40-50 years of ICE vehicles running on the Earth. Making them even more efficient and cheap is a valuable feat for many millions. reply Szpadel 6 hours agorootparentprevI like to compare this to SSD vs HDD. Why even invest in HDD development when SSDs are so much better? Right now almost all consumers use SSDs because they are affordable for space most people need. And I remember how we were transitioning where I bought 128gb drive for cost of 4TB HDD to have super snappy system and I had to manage system much more to not burn writes too quickly (ram drives for logs and temp etc) That where I believe we are right now with EVs. But they are more affordable and have better range every year. But to go further with this comparison we still develop new generations of HDDs even after all that time, even when everyone believe that at some point in the future we will abandon that technology completely reply lb1lf 11 hours agoparentprevPresumably as Porsche is not about efficient transportation; it is about emotion, driving as an experience, making a statement, etc. Chances are there will be - or, at least, Porsche appears to bet there will be - a sufficient number of well-heeled enthusiasts who prefer the sound, smell and vibration of an ICE to the quiet performance of an electric motor. I am really curious to see how this pans out in a generation or two - I suspect nostalgia plays a significant role in the 'performance cars should have ICEs' mindset, so what happens when the generation who grew up on EVs enter middle age and have the disposable income for a performance car? As for the rest of us, if, say, 1 in 1000 cars in a couple of decades' time burns fossil fuels, it is hardly going to be a nuisance. I'd be more worried (If I owned an ICE-powered vehicle) about the infrastructure needed to get the fossil fuels I needed in the tank. (Glancing anxiously over at my 1949 S1 Land Rover (1.6l I4 petrol) and 1981 Land Cruiser 42 (3.4l I4 diesel) - neither of which are performance cars by any stretch of the imagination! The 3B engine in the Land Cruiser will merrily chug along on just about anything vaguely combustible you pour in the tank, though.) reply sheepdestroyer 11 hours agorootparent\"it is about emotion, driving as an experience, making a statement, etc\" These kind of motivations need to be heavily vilified and taxed. reply lb1lf 11 hours agorootparentOh, they will be, no worries - doubly so as the fuel they need goes from being a necessary evil of civilization to a niche product for enthusiasts. There's going to be along tail of ICE vehicles out there, though - say, I have a tractor on my farm (which is not being worked commercially, it is basically an expensive and time-consuming hobby seeing as my wife's family has tilled this plot of land since the dark ages. Literally.) Anyway, even if diesel prices soar to $25/l (That's $100/gal for the metrically challenged), with our current usage, it will still be cheaper to buy diesel at that cost than to invest in a new, EV tractor. Hence I can't see that tractor going anywhere anytime soon. reply chii 9 hours agorootparentprevWhy should something be heavily vilified and taxed simply because you personally dislike it? reply bn-l 8 hours agorootparentBecause it’s a dead-end tech that’s holding us back and destroying the place where we all live. If you insist on pursuing it for your own pleasure, knowing what we all know now, you may be a villain. reply mensetmanusman 7 hours agorootparentMost emissions come from grid energy. The greens cancelled nuclear and were the real villains in the end. reply chii 8 hours agorootparentprevbut the grandparent poster's problem isn't with ICE itself, but with \"emotion, driving as an experience\" etc. In other words, the grandparent poster's argument is merely that people shouldn't be basing decisions on their (flawed) emotional attachment to driving as an experience, and would like such to be shamed and vilified. This is a moralistic argument. It has nothing to do with environmentalism, but is using climate change as a platform to push a personal agenda or world view. In other words, it's a woke argument. reply xattt 9 hours agorootparentprevIt’s not vilified. It’s the recovery of environmental costs associated with ICEs as well as subsidizing a lower barrier to entry for EVs. reply pkphilip 8 hours agorootparentprevThere are plenty of place around the world where there isn't sufficient capacity in the grid even for powering the houses.. let alone EVs reply _factor 8 hours agorootparentprevIt’s amazing how much the luxury car industry has advertised what is essentially a shaking and noise complement to going fast. The “smooth” revs. The “visceral” feel. The “connection” to the road. It’s just a few metal cylinders going up and down real fast, tuned to sound nice in a packaged product. It’s like brainwashing. Impressive really. I like going to the track when I can, but that’s a bit different. The statement is the work you’ve put into your car, not how “exclusive” your statement is. reply wiseowise 11 hours agorootparentprevThey’re already heavily taxed. But why vilified? reply aziaziazi 9 hours agorootparentA tax can help common infrastructures (building road) or limit something (cigarettes price) Some people think the ICE car tax(es) already in place are not sufficient compared to the damage they cause. To reduce usage, both tax and public opinion (shaming, vilifying) works great. It’s regrettable the efforts only focus on a car motors, the most dangerous property of a car is it’s weight (and emissions depends on that weight). reply wiseowise 2 hours agorootparentThe cars in question are luxury cars that are dime a dozen. I understand heavily taxing ordinary ICE cars, but why luxury ones? reply jprete 8 hours agorootparentprevVilifying things doesn't accomplish anything because the negative opinion becomes a cultural split. People don't care about the hatred of their out-group and lots of people will seek it out to burnish their status with the in-group. reply Tade0 9 hours agorootparentprevThere's a huge overlap between people who seek out such experiences and those who drive recklessly and modify their exhausts. reply mistercheph 8 hours agorootparentOr maybe you're not particularly interested in cars or the people who are interested in cars and you can't tell the difference / assume all of \"those\" people are the same, reply Tade0 8 hours agorootparentI actually am - mostly the engineering part of them though and to me, to paraphrase a prominent local motoring journalist, motoring is dead - EVs are a transition propulsion method and the end state is no motoring as we currently understand it and maintaining a car broadly considered lame. The only people I've met who are car enthusiasts and don't go for the reckless/loud style of driving are people like my friend - a car mechanic who managed to form a single, road worthy Nissan Z31 from two defunct halves. reply inglor_cz 11 hours agorootparentprevYour proposals are a bit contradictory. If you heavily tax something, it becomes a money source for the government; does it then make sense to try to extinguish said activity through artificial social aversion? Also, whatever is vilified by the government, but stays legal or at least feasible, will act as a magnet for the counterculture. If you really hate something, just propose an outright ban. It would work for Porsches; no one can secretly grow a Porsche in his backyard. reply pokerface_86 10 hours agorootparentprevyou must be european reply svara 8 hours agorootparentprevIf you're into that exotic hobby long after ICE cars are obsolete, you'll need to purchase synthetic fuel for it or purchase the equivalent amount of CO2 direct air capture. Sounds okay to me. reply cbeach 11 hours agorootparentprev> performance cars should have ICEs After driving a fast Tesla, stepping into an ICE performance car feels like going back to a steam engine. A very slow, unresponsive steam engine. When avid petrolheads try a fast EV and feel the instant torque it will be an epiphany for them. Only the most stubborn will stick with fossil cars. reply smileysteve 7 hours agorootparentI was at a local track in my older bmw (non M), a Tesla 3 (on lowered springs) was next to me in the grid. It did not compete with the corvettes or Porsches but with the Miatas and Me, it just weighed too much in the turns to carry the speed. reply wiseowise 11 hours agorootparentprevInstant torque is cool, but goddamn it is soulless. reply 082349872349872 10 hours agorootparentIf you want instant torque with soul, try a horse. (the catch: this instant is often one which you weren't expecting, and if you are lucky it will mostly have been applied along the craniocaudal axis) reply teo_zero 10 hours agorootparentprevI'm sure they said the same when they saw the first photographs and compared them with paintings. reply stonogo 10 hours agorootparentyou might note that people still paint. reply lowdownbutter 10 hours agorootparentWell that also needs to be heavily vilified and taxed. reply nasmorn 7 hours agorootparentSince paintings are mostly made by someone they are under income tax which is at least in my country the highest percentage of all taxes. Plus VAT like everything else. A painter this easily pays 60% tax on what you have to pay reply avereveard 9 hours agorootparentprevAh yes we should all live in a gray bloc working in a gray factory until the end of time Vilifying a d taxing any luxury not only is a ludicrous position, it will only make inequality worse. Its not like the 0.1% will renoujce to it, and taxes don't matter to them. reply wiseowise 2 hours agorootparentThe parent comment you're replying to is a sarcasm. reply smileysteve 7 hours agorootparentprevYet the manual transmission is dying out too. reply prds_lost 10 hours agorootparentprevWhich ICE performance car? I went from an SRT8 back in 2012 to a Tesla Model 3 performance + mods and now most recently went back to ICE via an AMG GT63. I can confidently say that the GT63 is leagues more fun than the Tesla ever was. Sure the Tesla was quick off the line, but outside of that it felt sterile and the ride was quite uninspired. reply foobazgt 8 hours agorootparentModel 3's didn't exist in 2012. Did you mean something else? I've driven an M3P for 5 years, and I wouldn't call it sterile. I haven't been cross shopping it with vehicles that cost 3x as much and get only 15% of the mileage, so maybe I'm missing something? But weighing 10% more and taking 20% longer to reach 60 doesn't seem particularly compelling. reply mistercheph 8 hours agorootparentIf it's the only car you've driven for five years of course it doesn't feel sterile, it's a wonderful car, and nothing against it, but it is not a fun car / \"enthusiast\" car insofar as enthusiast refers to the thin slice of all automotive enthusiasts that are primarily interested in handling that is dynamic and communicative reply avereveard 9 hours agorootparentprevThats a very common misconception. Being very fast doesnt necessarily make a car very fun. reply lb1lf 10 hours agorootparentprev-True, but I believe (at least this holds true for me) - the tinkerability, if you like, of ICEs is much higher. You can work on them at home, install all sorts of modifications and upgrades - real or perceived - to a much larger extent than you (currently) can on your EV. It all depends on what makes cars interesting to you, of course - but a lot of the car enthusiast clientele would merrily start tinkering with their pride and joy even if all is well, just because they can. reply smileysteve 7 hours agorootparentYou can still modify an electric car's suspension with shocks, springs, anti roll bars, and performance tires. reply pixxel 7 hours agorootparentprevYou’re wasting your time if you have to explain passion and soul, of which these types have neither. reply apelapan 9 hours agorootparentprevEh, no? One car being objectively faster than another does not mean that it is more fun. And even if one vehicle is more fun in some total measure, it will not be fun in all the same ways as all other fun vehicles. There are plenty of people who alternate between something like a modern Mercedes V8 and a classic Mini. There is no question of which one is faster, but it might be the slow one that gives the most joy of sporty driving reply nottorp 9 hours agorootparentprevI suppose you don't know how to drive a manual either :) reply foobazgt 9 hours agorootparentI drove stick for a couple of decades. They were fun at the time (except in traffic jams), but I'm no Luddite. I can enjoy being thrown back into my seat with roller coaster acceleration without the accompaniment of vroom vroom noises, exhaust, and shifting. reply nottorp 7 hours agorootparentI did test drive a Tesla and i agree any electric motor does the throwing back better than combustion. But being thrown back is not the fun part. The fun part is controlling the car around those bends on a mountain road. Anyone can floor it in a straight line. reply xcv123 7 hours agorootparentprevThe fastest EV is nothing compared to the fastest ICE. Top fuel dragsters accelerate from 0 to 100 mph in 0.8 seconds. I have tried fast EV and it's boring. I prefer my loud meth injected turbo diesel BMW (520 lb/ft at the wheels, pulls like a Tesla but is not boring to me) reply xenospn 7 hours agorootparentprev911 Turbos are faster than Teslas. Regardless, EVs feel like driving a washing machine. Some people are just not into that, or expect more. I’ve never been excited by any EV I sat down in, no matter how quickly they go 0-60. reply dahart 4 hours agorootparent> 911 Turbos are faster than Teslas. Nope. Tesla Model S Plaid beats the 911 Turbo S. https://en.wikipedia.org/wiki/List_of_fastest_production_car... reply JumpCrisscross 11 hours agoparentprev> Why waste R&D on a dead end? \"There were an estimated 20 million horses in March 1915 in the United States\" [1]. In 2023? Almost 7 million. Internal combustion engines will be around for a long time, in part because they're beautiful. [1] https://en.wikipedia.org/wiki/Horses_in_the_United_States#St... [2] https://www.ppfas.com/pdf-docs/b-finance/cigar-butt.pdf reply 082349872349872 10 hours agorootparentPedantry: they aren't the same horses, though: in 1915 they were mostly draft horses, with a few sport/pleasure; in 2023 they're mostly sport/pleasure, with a few draft. reply ManuelKiessling 10 hours agorootparentPorsche engines are clearly sport/pleasure engines, not draft engines. reply 082349872349872 9 hours agorootparentTrue; looks like their last tractor was over 50 years ago. Other marques are still extant: https://www.ferraritractor.com https://www.lamborghini-tractors.com/en-eu/ reply AlecSchueler 9 hours agorootparentprevWere horses ever actively restricted by governments around the world or did we move away from them because better solutions were available? reply tonyedgecombe 8 hours agorootparentThey were causing environmental problems, cities were clogged up with horse shit. I don't know whether that influenced the transition though. reply lostlogin 10 hours agorootparentprevSomething that surprised me: the German army in WW2 was mainly horse drawn. According to the below link, 80% of its transport was via horse. https://www.zdnet.com/article/the-wwii-german-army-was-80-ho... reply 082349872349872 9 hours agorootparentI believe this has a lot to do with the failure of the Maginot line: it had been constructed to slow down an army with horse-drawn logistics (after all, germany had no oil) but the germans did their end run with mechanised units (having researched synfuels in the meantime). From 1940 to 1943, one can readily explain Axis strategy as an insatiable quest for more oil: no barrels, no Blitz. EDIT: Lagniappe: https://en.wikipedia.org/wiki/Joe_Medicine_Crow#World_War_II reply jabl 8 hours agorootparentprevReflecting the interwar German society more broadly; agricultural mechanization was decades behind the US and UK. The industrial and economic \"miracle\" of the pre war Nazi regime wasn't spent on tractors either, but on weapons. reply Tade0 10 hours agorootparentprevIn the meantime the human population quadrupled, so the actual demand for horses is not half, but closer to 1/8 of what it was. They'll go the way of horses in the sense that you'll be hardly seeing any in cities. reply GavinMcG 9 hours agorootparentYou seem to be using “actual” in a way that suggests proportion to population is a key part of the definition. But although a company that served a market for 20 million might well have failed as the market shrank, it also had the opportunity to survive and continue serving an actual market with an actual demand for 7 million actual horses. reply 082349872349872 9 hours agorootparentHermès' original market for tack (harnesses and bridles for driving) shrank considerably, but they've found new markets since. Lagniappe: https://www.youtube.com/watch?v=qs-IKeRyRAg reply night862 11 hours agoparentprevI definitely think this is a valid question. ICE engines and every type of motor have a Power Curve. Many things go into the power curve including the construction, configuration, fuel and general type of motor. The power curve graphs engine power output as it relates to engine rotational speed. Every one of these different motors and transmissions out there including electric as well, have differing characteristic curves which effect the handling of the vehicle to great effect. This affects their possible applications. Controlling the power and efficiency curves of motors is the entire story of the very well known \"VTECH\" Variable Valve Timing technologies, super and turbo chargers or other forced air intake, and even the way that Tesla electric motors arrange the magnets in their electric vehicle motors. Inventing a new way to operate an ICE is good for many things. This patent looks cumbersome, and a bit complex. In my mind the long-term future of ICE for vehicles is in more specialized use cases as fossil fuels reach the long tail. Certain highly reliable, or certain types of safety prohibiting voltage or battery chemistry, types of standby vehicles, certain rugged vehicles, industrial equipment, small motors, these might be better off being petro-like. It might be better to make corn-gasoline than to stash a huge battery onto your lawnmower all year in this case. But please do not discount: They are enjoyable. Even during the end-of-days, we can process something like biofuels into \"gasoline replacements\". We can use waste vegetable oil to create biodiesel today. Small volumes are pretty easy and although we desperately must reduce fossil fuels to near zero (I dont need to have a blast gunning my Porche in bumper-to-bumper traffic, for example...) I don't see a reason why people can't still buy a motorcycle, even if we are living under sci-fi-like domed cities. All else, people would simply make them themselves in their own garages. For Fun. reply andrewxdiamond 12 hours agoparentprevI think it’s likely we have passed the peak of R/D spend on ICE, but the momentum of research that has gone into it for decades is going to continue to produce innovations for a while longer. reply usrusr 7 hours agorootparentThe engineers are still there (where else would they go!) and with ICE development moving far out of strategic focus, there might actually be more room now for trying odd curveball approaches than back when ICE refinement was still a cornerstone of success. reply Merad 11 hours agoparentprevICE aren't going to go away any time soon. Battery tech has a long way to go before it can compete with the energy density of liquid dinosaurs, and if it did there are plenty of cases where ICE has major advantages over battery power. But even if say 90% of the vehicles on earth could be replaced with electric, should we really stop innovating on that last 10%? reply snthd 7 hours agorootparenthttps://www.thoughtco.com/does-oil-come-from-dinosaurs-10920... >Tiny Bacteria, Not Huge Dinosaurs, Formed Oil >try to grasp the concept of deep geologic time, a talent possessed by very few people. Try to wrap your mind around the enormity of the figures: bacteria and single-celled organisms were the dominant forms of life on earth for a whopping two and a half to three billion years, a virtually incomprehensible stretch of time when measured against human civilization, which is only about 10,000 years old, and even against the reign of the dinosaurs, which lasted \"only\" about 165 million years. That's a lot of bacteria, a lot of time, and a lot of oil. reply cesarb 7 hours agoparentprev> Seems pretty dumb considering it still consumes fossil fuels. Internal combustion engines are not limited to fossil fuels. For instance, here in Brazil it's common to use sugarcane ethanol as an alternative fuel, and many cars come from factory ready to use either gasoline or ethanol (the keyword to look for is \"flex fuel\"). reply freetanga 11 hours agoparentprevBecause large manufacturers are seeing EVs are struggling to take off. Might as well spread your bets. They are also working heavily on efuel, that works with ICE engines, does not pollute much, but currently would cost 2x at the pump. If they could shave that cost in half we could keep most ICEs (plus an add on catalyst). Distribution networks could be kept. Batteries and their chemicals would not be needed. So if either pays off, it’s a fortune. reply freetanga 10 hours agorootparentJust to clarify: I had to do an analysis on car markets across 10 countries in EU, Mexico and Latin America recently. If 100% of new vehicles were electric today, it would take 15 to 20 years to renew the whole fleet in those countries. But today the share of new EVs were between 0 to 5%. While there is a subyacent network effect (when 30% of gas stations close, it will be so hard to refuel that adoption will spike, which in turn will kill more gas stations), it still seems way off. The people I interviewed at car makers seemed to see EV as a new segment more than a new paradigm. Your mileage might vary. reply uluyol 10 hours agoparentprevICEs have a lot of advantages that make them much better suited than EVs to certain tasks (extreme climates and remote locations, for example). They will likely stick around for niche use cases (at least) for quite a long time. As for why Porsche is spending money on ICE engines...well, no one really buys a Porsche because it's a \"practical car\", do they? reply magicalhippo 5 hours agorootparentHere in Oslo, Norway, they got BEV busses for a lot of lines, and last year was a really cold winter. Somehow, the ones in power were shocked to discover the range dropped to half due to the electric heaters needing a lot of power, causing a public transportation chaos as busses queued up for charging instead of transporting passengers. Now the busses are getting retrofitted with fuel-based heaters, which will be running on bio-diesel... better than nothing I guess. reply TacticalCoder 10 hours agorootparentprev> As for why Porsche is spending money on ICE engines...well, no one really buys a Porsche because it's a \"practical car\", do they? I don't know. I find my daily very practical: a Porsche Panamera MY2013, now nearing 125 000 miles and 12 y/o. Still under extended manufacturer warranty. I have it since more than five years now (bought it used). Next car is another used Panamera (probably a 2020 or something). Very sweet, comfy, luxurious ride and yet if you push the pedal to the metal, way funnier to drive than, say, a Mercedes Class S. reply tecleandor 9 hours agorootparentWell, in a big city I wouldn't call a 5m long car that drinks 12 or more liters \"practical\". Also it's an expensive and high maintenance car even when functioning properly (I'd be scared of the bill for changing those tires or doing Porsche's maintenance...) But, you said it: sweet, comfy, luxurious, funnier. Valid reasons, of course, but different :-) reply tehlike 12 hours agoparentprevBecause it will continue to sell - it's a porsche. Something i wouldn't do, but i understand. reply left-struck 11 hours agoparentprevCould be any of several reasons. As a backup plan, for niche uses, or just uses that you are not considering such as small aircraft. Could be a stop gap as well, it seems like there was a quick uptake in EVs but they aren’t replacing petrol engines just yet, even in cars. reply frankgrimesjr 11 hours agoparentprevFor sports cars, internal combustion engines still provide a weight advantage over battery packs. reply mistercheph 8 hours agorootparentI'm really looking forward to when someone manages to bring to market an EV that is light and efficient with small battery pack and lowish range that is purely about driving pleasure and fun, but E/V early adopters still want massive ten thousand pound luxury tanks, for now! reply usrusr 7 hours agorootparentUnsurprisingly, Lotus (aka the maker of the car that Tesla converted into their Roadster) did not completely abandon their lightweight sports car identity in the BEV age. The Evija advertised with a range of 315 km. reply larodi 11 hours agoparentprevSeems brilliant to me, as this is true marvel of engineering, contrary to electrocars which... well are not. They could've been should the engine be powered by fuelcells, but in their present form the electrocar movement took off only because of green deal, climate change, etc, which forced governments to allow business create the needed infrastructure of batteries and powerstations. Besides, you'd be surprised at the amount of work geard towards hydrogen-based economy which is still about burning stuff to turn wheels (even when electricity is produced) as they did in 1890s... reply pineaux 11 hours agorootparentYeah. It looks pretty cool. There is something satisfying about a complex mechanical mechanism. However, in its current form it is not the future. Combustion engines in general are not going to be the future. Hydrogen is a joke. Not energy dense enough. And expensive to make (energy wise). It also needs a vast network of gas stations. reply larodi 7 hours agorootparentWho says so? Batteries been available for hundred years yet we keep talking about fusion being the future, and we have fission. Which both drive turbines, which are a thing from the past? No? What is joke and when? To me is a joke that we keep heating water to move magnets to get energy in 2024 with all the quantum entanglement, Johnson teleportation, lasers, etc. engineering magic happening around. And we do what? heat water to move magnets to squeez energy? So why is then hydrogen bad, just because it goes boom? Plenty of things we use daily go boom now and then, not meaning recent news, but in general. I also invite you to take a look at the Tokamak and then try to argue again that metal tubes wired around to pump liquids and do some combustion is not the future. reply 00N8 20 minutes agorootparentCompressed hydrogen has low enough density that even at dangerously high pressures it would take up most of the luggage & passenger areas of a car, just to get mediocre range. Liquid hydrogen has better density (roughly 2x), but it's still not great, & you're dealing with a cryogenic propellant that's constantly venting explosive gas (so it runs out of gas when parked for ~3 days, can't be parked in an enclosed space, & still doesn't deliver impressive range). I'm all for using hydrogen for things in general, but as I understand it there are hard physical limits that make it a poor choice for personal motor vehicles as we understand them today (size, safety & range in particular). Methanol or renewable synthetic gasoline seem like they'll always outperform hydrogen for ICE cars. Hydrogen airships would be cool though & we could make them safe with modern technology (both preventing fires & making the passenger compartment an under-slung survival cell with parachutes & giant airbags so a catastrophic loss is survivable) reply rsynnott 6 hours agoparentprevEven the most ambitious petrol car bans are normally targeting 2035 or so, new cars only. That’s probably time for one more engine generation iteration. reply abenga 12 hours agoparentprevWhy is it a dead end? I agree that it should be, but it seems the entire world is walking back the commitment to a full transition to EVs. reply thebruce87m 11 hours agorootparenthttps://www.edie.net/one-in-seven-new-cars-worldwide-is-now-... > One in seven new car sales worldwide is now electric > Year-to-date figures show that nearly 10.6 million EVs have been sold globally as of July 2024, marking a 16.3% increase compared to the same period last year. What constitutes “walking back”? reply abenga 9 hours agorootparent> What constitutes “walking back”? Legally+Economically. After seeing how badly their cars were doing against Chinese EVs, it seems the EV-only mandates in the EU and US are on shaky ground; only heavy tariffs on imported cars seem to be giving local producers a chance. I do not think they will figure it out by the 2030-ish targets, and bet that European and American companies will be allowed to sell combustion engine cars after that. > marking a 16.3% increase compared to the same period last year This is buoyed a lot by growth in China (which is not nothing, I guess), but growth in Europe and the Americas is slowing. reply rsynnott 6 hours agorootparentThe EU and California targets are 2035; not aware of any large market going for a full ban earlier than that? reply abenga 6 hours agorootparentEven 2035 is not happening. Politically, parties that are not enthusiastic about climate change mitigation are gaining ground in Europe. A lot of western car companies are scaling back their ev targets, and announcing more ICE car research (like this article we are discussing). Those bans will keep getting postponed/cancelled the closer they come. reply freetanga 10 hours agorootparentprevI need to check that source but I guess is heavily per country. In Southern Europe is under 5%, south from the US border between 0 to 1 % (source: per country Car Makers Association report) Globally, if I recall, you need 20 years to replace the whole global fleet (so around 5% replacement rate). Let’s assume this global number is fixed (installed base is growing, as more vehicles are being created that destroyed, actually) 1/7 is 15%. 15% of 5% is 0,75%. So this means that every year 0,75% of global installed base is replaced by EVs. So 130y for full replacement? Yes, the 1/7 weight will increase. But so will expand base. Even if suddenly today every car made now on the planet is EV only, iCE will still be around for 20-30 years. reply freetanga 10 hours agorootparentNot to pile on, but the linked article you presented backs their data on New AutoMotive’s Global Electric Vehicle Tracker, a UK based group fostering EV adoption. NAGEVT site only shows data for UK, is unclear which data sources they used, and whether they added PHEV (still ICEs) into the mix. Don’t mean to come obnoxious, I just spent a few weeks researching the topic for 10 countries and my data differs a lot from theirs. reply yardstick 10 hours agorootparentprevNot the op but in NZ there has been a significant drop in demand due to removal of subsidies and introduction of a distance-based tax (road user charge) for EVs. https://www.1news.co.nz/2024/09/15/car-importers-stuck-with-... “In 2023 January through August, one in four new light passenger vehicles sold were EVs. Fast forward to 2024 and EVs make up just one in 11 new light passenger vehicles sold” reply thebruce87m 10 hours agorootparentThe original claim was for the “entire world” which is why I specifically used global stats. reply yardstick 10 hours agorootparentMaybe not every single country, sure. In the EU sales are sluggish. https://www.forbes.com/sites/neilwinton/2024/05/19/europes-w... “Current sales of EVs in Europe have stalled at just over 2 million a year, as early adopters and corporate purchasing peaked out. Schmidt Automotive Research said during the opening third of 2024 Western Europe EV market share stalled at 14.4% compared with the same period last year, according to its provisional data.” reply thebruce87m 9 hours agorootparentIt’s fascinating the language used around EV sales. You have used “sluggish” where others might have used “stable”. In the past we had articles saying sales were “slowing” when in fact they were growing, just not growing as fast. There are other regions that are growing, but I see little relevance to the original point unless people expected continued growth in every region with no blips? reply 000ooo000 11 hours agoparentprevNot all ICEs must consume fossil fuels. The tech could bring alternative fuels closer to viability. reply hnlmorg 11 hours agoparentprevElectric cars still consume fossil fuels. It’s just earlier in the pipeline. People love to talk about combustion engines being pollutants but few people talk about just how damaging it is mining for rare metals to make those batteries. Nor how harmful for the environment it is disposing of old batters (we haven’t yet solved the problem of recycling them either). And that’s before you even touch on problems at the power grid. Eg many addresses don’t have high quality enough power infrastructure to cope with everyone switching to electric, so that would need to be upgraded. And a lot of countries still rely on fossil fuels to generate electricity to begin with. Electric cars are likely the future. But they’re not as green as people like to think. In fact for some use cases they can actually work out both more expensive to run and less environmentally friendly once you total up their carbon footprint in full. And I think it’s going to be a long time before they become the greener option of everyone. Whereas in the meantime, research into combustion engines can provide benefits for more than just petrol-fuelled sports cars. reply thebruce87m 11 hours agorootparentMany of the myths you are posting are busted here: https://www.carbonbrief.org/factcheck-21-misleading-myths-ab... The national grid in the UK was so fed up with people making claims about the grid capacity it also busted some too: https://www.nationalgrid.com/stories/journey-to-net-zero/ele... > The highest peak electricity demand in the UK in recent years was 62GW in 2002. Since then, the nation’s peak demand has fallen by roughly 16% due to improvements in energy efficiency. > Even if we all switched to EVs overnight, we estimate demand would only increase by around 10%. So we’d still be using less power as a nation than we did in 2002, and this is well within the range the grid can capably handle. > Nevertheless, at National Grid we’re working with the distribution networks, government, the regulator and industry to provide the green energy infrastructure around Britain – the wires, the connections to charge points – to support the needs of a decarbonised transport network into the future. > In the US, the grid is equally capable of handling more EVs on the roads – by the time 80% of the US owns an EV, this will only translate into a 10-15% increase in electricity consumption.1 reply citrin_ru 10 hours agorootparent> Since then, the nation’s peak demand has fallen by roughly 16% due to improvements in energy efficiency. Energy demand in UK declining because prices went up which accelerated deindustrialisation and forced people's to use less energy e. g. heat homes less . reply JohnVideogames 9 hours agorootparentBritish homes are almost exclusively heated by natural gas boilers; home heating is only a small part of the grid electrical load. reply citrin_ru 9 hours agorootparentYes, gas heating is common (thought in apartment buildings electric heating is more likely) but electricity and gas prices are correlated for now. reply timthorn 9 hours agorootparentprevWhat that article doesn't address is the concurrent shift to electrify domestic heating. A worked analysis of the impacts can be found here: https://www.carboncommentary.com/blog/2022/2/11/how-much-ele... reply thebruce87m 9 hours agorootparentIt will take a long, long time to move to heat pumps for heating the majority of homes. Last time I looked into it I would have to change all the pipes and radiators in my home. Same with my mums house. The savings would have to be massive to offset the disruption and cost. reply hnlmorg 7 hours agorootparentNew houses are being built though. And at an alarming rate in some districts too reply thebruce87m 7 hours agorootparentNew houses also have solar, which can offset some of their new load. reply hnlmorg 6 hours agorootparentSure, but I don’t think it’s as common as you think. For example, none of the new houses in any of the new estates around me were built with solar panels. reply hnlmorg 8 hours agorootparentprev> Many of the myths you are posting are busted That article addresses literally just one of my claims reply thebruce87m 2 hours agorootparentnext [1 more] > That article addresses literally just one of my claims There are two articles in my post that address multiple points. Luckily I don’t even have to argue with you about this as ChatGPT can do it for me. AI content below: ### Analysis of the Original Post and Response The original post critiques the environmental benefits of electric vehicles (EVs), raising several concerns about their production, usage, and infrastructure. The response refutes many of these claims by providing evidence from reputable sources, notably *Carbon Brief* and *National Grid UK*, which seek to correct common misconceptions about EVs. Let’s analyze both the original post's claims and how the response addresses each point. --- ### Claim 1: *Electric cars still consume fossil fuels—just earlier in the pipeline* - *Original Post's Argument*: The claim suggests that although electric vehicles do not burn fossil fuels directly, the electricity used to charge them is often generated from fossil fuels, thus shifting pollution earlier in the supply chain. - *Response*: The *Carbon Brief article* acknowledges that while electricity generation still relies on fossil fuels in some countries, the grid is becoming increasingly decarbonized over time. As renewable energy sources like wind, solar, and hydroelectricity grow, the carbon footprint of charging EVs is steadily shrinking. > \"Many countries, including the UK, have rapidly decarbonised their electricity grids, meaning that electric cars are already far cleaner than petrol and diesel cars, and will get cleaner as the grid continues to decarbonise.\" - **Carbon Brief** This directly counters the argument, indicating that while fossil fuels are still used, the grid is moving towards cleaner energy sources, making EVs progressively greener over time. --- ### Claim 2: *Mining for rare metals in batteries is damaging to the environment* - *Original Post's Argument*: The post highlights the environmental impact of mining rare metals like lithium, cobalt, and nickel, which are essential for EV batteries. It also points out the issue of disposal and recycling, suggesting that current methods for handling old batteries are inadequate. - *Response*: The *Carbon Brief article* acknowledges the environmental costs of mining but contextualizes it within the broader benefits of EVs. It points out that while battery production has a higher upfront environmental cost, EVs offset this over their lifetime by producing lower emissions during use compared to internal combustion engine (ICE) vehicles. > \"The manufacturing of electric cars can indeed produce higher emissions, particularly due to the energy required to produce the battery. However, this is offset by the much lower emissions during the use of the vehicle.\" - **Carbon Brief** Additionally, the article touches on battery recycling, noting that significant research is being undertaken to improve recycling technologies and reduce the environmental impact of batteries. For example, companies are exploring second-life applications for batteries and refining recycling techniques. > \"Recycling of electric vehicle batteries is improving, and the EU has introduced regulations requiring EV batteries to be recycled.\" - **Carbon Brief** --- ### Claim 3: *Power grids are not equipped to handle widespread EV adoption* - *Original Post's Argument*: This claim suggests that the infrastructure for delivering electricity to homes is insufficient to support a mass switch to EVs, particularly in areas with outdated grid systems. The post implies that upgrading the grid would be costly and challenging, and that many countries still rely on fossil fuels to generate electricity. - *Response*: The *National Grid UK article* directly addresses the concern about grid capacity, arguing that the national grid is more than capable of handling an increased demand from EVs. In fact, the article notes that peak electricity demand in the UK has been falling due to improvements in energy efficiency, and that even with widespread EV adoption, the grid would still use less electricity than it did in 2002. > \"Even if we all switched to EVs overnight, we estimate demand would only increase by around 10%. So we’d still be using less power as a nation than we did in 2002, and this is well within the range the grid can capably handle.\" - **National Grid UK** In the U.S., a similar analysis found that the grid could easily support the growth of EVs. > \"By the time 80% of the US owns an EV, this will only translate into a 10-15% increase in electricity consumption.\" - **National Grid UK** This contradicts the claim that grids are ill-prepared and highlights that grid modernization efforts are already in progress to support EV infrastructure. --- ### Claim 4: *Electric cars are more expensive to run and may not always be greener over their full lifecycle* - *Original Post's Argument*: The post suggests that, in certain cases, EVs may be both more expensive to run and less environmentally friendly when considering the total carbon footprint, including production, use, and disposal. - *Response*: The *Carbon Brief article* provides a nuanced answer, recognizing that while the upfront cost of EVs can be higher due to the price of batteries, the total cost of ownership is often lower in the long run. This is due to reduced fuel and maintenance costs, as well as government subsidies and incentives for EV owners. > \"Electric cars can have higher upfront costs, mainly due to the cost of the battery. However, they are cheaper to run, due to lower fuel and maintenance costs.\" - **Carbon Brief** On the environmental side, the article argues that EVs have a lower lifecycle carbon footprint compared to ICE vehicles, especially as grids become cleaner. > \"Studies consistently show that, even when powered by relatively carbon-intensive grids, electric cars still emit less CO2 over their lifetime than conventional cars.\" - **Carbon Brief** --- ### Conclusion: The original post raises several valid concerns about the environmental and infrastructural challenges of EV adoption, but the response effectively refutes most of these claims with data and evidence from *Carbon Brief* and *National Grid UK*. The articles emphasize the following: - EVs are becoming greener as grids decarbonize. - The environmental costs of battery production are offset over the vehicle's lifetime. - Power grids in countries like the UK and the U.S. are equipped to handle increased EV demand. - EVs offer lower running costs and, overall, a smaller carbon footprint compared to internal combustion vehicles. The original poster's assertion that the reply only addresses one of their claims is inaccurate. Both the *Carbon Brief* and *National Grid UK* articles provide detailed information that counters multiple points raised in the original argument. reply mjamesaustin 11 hours agorootparentprevMost of these claims are dramatically false. - Electric motors use low amounts of rare earth metals, or in some cases none. Batteries generally don't use them either. - Automotive batteries have been demonstrated as >90% recyclable, far better than most materials we consider \"recyclable\" today such as plastic - The electric grid has sufficient capacity to onboard millions of electric cars without issue, and will not be impacted by the transition to EVs - Cost of ownership of EVs is significantly lower than ICE vehicles in their class, as is the lifetime carbon footprint reply azherebtsov 11 hours agorootparentI’m particularly interested in last three points. Do you mind to share some data? For instance, where one recycle a Tesla battery, say in Poland? I do not see to many chargers around and I believe the grid is not very modern here. Why do you think it can easily handle millions EVs? What about peak hours like nights? Do you think it will be env friendly, provided that majority of electricity in Poland is produced on coal plants. EV initial cost is about 5-10 times more than a slightly used ICE in a perfect condition. Such a car car drive decades and will never reach in total spendings even initial cost of EV. I’m not counting charging that may not be very cheap. As electricity prices went up in recent years and may go up more. When you stating something, think beyond your household. Your statements are not universal. reply Symbiote 8 hours agorootparentThe battery can be recycled in Germany if Poland doesn't have the facility. reply hnlmorg 8 hours agorootparentprev> Electric motors use low amounts of rare earth metals, or in some cases none. Batteries generally don't use them either. Maybe the term “rare” was poorly chosen. But mining those metals is the damaging part, not the abundance of the metals. So my point is still correct. > Automotive batteries have been demonstrated as >90% recyclable, far better than most materials we consider \"recyclable\" today such as plastic Citation needed. EV batteries are reused in some less demanding domains for a couple of years after their life in EVs. but everything that I’ve read thus far has said recycling the batteries themselves, after their life is done, is still very much in its infancy. > The electric grid has sufficient capacity to onboard millions of electric cars without issue, and will not be impacted by the transition to EVs Most country’s electric grids do. That’s the bloody point of a grid. It’s the last mile that’s the issue in some counties. However I will concede that this point is a little unfair because it’s non-specific. > Cost of ownership of EVs is significantly lower than ICE vehicles in their class, as is the lifetime carbon footprint That depends. If you’re an occasional driver and charging your car at home, then the cost of electricity in some countries is going to be greater than the cost of petrol. However if you travel lots and are heavily use charge stations at super markets et al, then EVs will be much cheaper to run than ICE. It’s definitely not as clear cut as a lot of EV marketing claims. (I’ve been adding up the costs just this month because was looking at buying an EV and it just wasn’t a cost saving for my particular requirements). reply quintushoratius 6 hours agorootparent>> Automotive batteries have been demonstrated as >90% recyclable, far better than most materials we consider \"recyclable\" today such as plastic > Citation needed It shouldn't be hard to believe. The components of a battery aren't consumed during operation. An old battery weighs the same as a new battery, and the chemical reactions taking place inside are reversible (because that's how a chemical battery works). I believe, though, the the previous commenter erred. 90% of _batteries_ are recycled. According to Wikipedia, lithium battery recycling can see up to 96% recovery rate of material. Lead acid is over 98% of the lead. https://en.m.wikipedia.org/wiki/Battery_recycling > However if you travel lots and are heavily use charge stations at super markets et al, then EVs will be much cheaper to run than ICE. This statement is confusing to me. In my experience in the US, charging away from home is far more expensive. Charging at home has added an average of $10/month to our electric bill. Charging at a commercial charger can easily cost $5-$10 _per charge_. At home I have a 3kw charger, one of the lowest options you can buy. I prefer to take time and keep the battery cool. Heat ki",
    "originSummary": [
      "Porsche has patented a six-stroke internal combustion engine that adds extra compression and power strokes, aiming to increase power and efficiency.",
      "The design involves a special crankshaft with two concentric circles, altering the piston's travel and compression, but adds complexity.",
      "Implementation of this design remains uncertain, making it a notable but speculative innovation in automotive engineering."
    ],
    "commentSummary": [
      "Porsche's six-stroke internal combustion engine concept has initiated discussions on reimagining traditional engine designs.",
      "The design features two pairs of compression-power cycles, which could enhance combustion efficiency.",
      "While electric vehicles (EVs) are becoming more popular, internal combustion engines (ICEs) may still have niche applications and ongoing innovations, with debates also considering the environmental impact of EVs."
    ],
    "points": 145,
    "commentCount": 289,
    "retryCount": 0,
    "time": 1726899301
  },
  {
    "id": 41605774,
    "title": "How do archivists package things? The battle of the boxes",
    "originLink": "https://peelarchivesblog.com/2024/09/10/how-do-archivists-package-things-the-battle-of-the-boxes/",
    "originBody": "How do archivists package things? The battle of the boxes September 10, 2024 · by Region of Peel Archives · in Archives FAQs and Facts, Case studies: Archivists at work. · It’s been a while since we posted one of our articles pulling back the curtains on archival work. To make up for that, here’s a special edition of our popular Archives FAQs and Facts series. For the first time, we’ll compare how archivists in two countries do things a little differently to achieve a common goal. We’ll show you how archivists package (or “house”) the most common types of physical documents for long-term storage. That is, we’ll talk boxes and files. And we’ll benefit from the experience of special contributors from the United Kingdom to compare how archivists in Canada and the UK commonly do their packaging. To make the comparison easier, we’ll even coin some technical terms that may spark discussions among archives fans and followers. This post came about for a few reasons: Stereotype busting: Fictional archives are often a far cry from reality, leading to misunderstandings when new users approach real archives. Public curiosity: Individuals and organizations often contact archivists and conservators (experts on repairing and stabilizing physical media) for advice about properly packaging their own valuable records. Perhaps some of our practices, revealed here, can be adapted for home and office use. Professional curiosity: I (Samantha at the Region of Peel Archives) have long noticed some intriguing differences in packaging practices between archives in the UK and North America. The comparison is a great way to showcase basic preservation measures. This box carried by Archivist Barbie on social media is instantly identifiable as a archival box by archives aficionados in North America – perhaps not so much in other countries. (Courtesy of a twitter thread by the State Archives of North Carolina.) Why is this post called the “battle of the boxes”? First, this is a tongue-in-cheek reference to the friendly rivalry between the two common packaging styles we’ll look at. Archivists themselves sometimes take their local practices for granted. We hope that archival workers, as well as interested members of the public, might learn something about one another. Second, and more seriously, archivists wage war every day against the universe’s tendency to disorder. Boxes and files are just some of the weapons in our arsenal, as we strive to keep the self-knowledge of humanity safe for as long as possible. It’s only fitting, then, to begin by thanking the members of the worldwide community of preservationists who helped me with this post. I’m particularly grateful to Lizzy Baker and Rachel Gill (Archives Lead and Archivist respectively at the Tyne & Wear Archives in Newcastle, UK) and Shirley Jones (Head of Conservation at the West Yorkshire Archive Service, Wakefield, UK) for sharing their experience. Thank you also to Penelope Bertrand and Elise Rowsome, collection managers at Library and Archives Canada, and Professor Don Spanner at Western University for helpful conversations. Archival scenery and stereotypes Our exploration of archival packaging begins with popping a pervasive stereotype. In fact, this balloon of expectation rapidly deflates for anyone visiting archival storage for the first time. Archives in movies and TV often have a certain look: a jumble of odd-sized papers and books (mostly books), crammed on shelves and tables. This stuff looks varied and intriguing. (It’s much easier for stuff to look intriguing when it’s exposed.) Scenes from the films The Ring and Nostradamus. Horror comes in many forms. Of course, real archival shelves do host traditionally vintage-looking items, especially large tomes. (These shelves are frequently employed as photo op backdrops when archives appear in the news.) But the archivist’s guilty secret is that most stocked archival shelves look comparatively bland: row on row of identical boxes, filled with identical files, and cryptically labelled. Left: Region of Peel Archives in Brampton, Ontario, Canada. Right: Tyne & Wear Archives in Newcastle, UK. (Courtesy of Tyne & Wear Archives.) Why would regularly sized boxes and files be the norm, especially if archival records run the gamut from maps, postcards, and reports, to letters, handbills, and photographs? In a way this question has answered itself, but let’s unpack it – pun intended. Some “handy” terminology The first thing that often surprises people is that archivists overwhelmingly use paper or cardboard packaging. Paper containers are lightweight and breathable. (Trapped stagnant environments are attractive to pests and mould). Archival containers are specially treated to be acid-free. They may also contain a buffering agent that helps neutralize incoming acid from old documents. Low-acid packaging considerably slows the breakdown of naturally acidic records, especially in the controlled climate of archival vaults. For all these reasons (and more), archivists don’t put records and photos in ring-binders or albums. In this post, we’ll concentrate on the most common sizes and combinations of archival boxes and files. We’re going to refer to these containers as standard packaging. In this common archival packaging method, records are placed in standard archival files which are then packed in standard archival boxes. In fact, archival boxes and files are so very standard that archives users instantly spot them when they occasionally appear in the media. One of the key advantages of standard packaging is that it universally accommodates a particular range of common document sizes. For convenience, we’ll call this range hand-scale documents: documents that can be held comfortably between our hands held not too far apart. This range of sizes arguably encompasses the majority of paper and photographic records produced in daily life over the centuries, both before and after paper sizes were standardized. Files of hand-scale documents at the Region of Peel Archives. From top left clockwise: early 19th century, mid-late 19th century, late 20th twentieth century, early 20th century. The benefits of uniformity The aim of standard packaging is to enclose groups of records – in this case, hand-scale documents of variable sizes – in smaller packages (files) housed in larger packages (boxes). Why? It’s easier to control records packed up this way. And for archivists dealing with lots of rare or evidential records, control means stability, and stability means accessibility over the long term. After all, in the world of archives, “long-term” means hundreds of years, and “lots” means thousands of boxes containing millions of individual records. How standard packaging maximizes control Preservation control: Separating large volumes of documents into small groups (like files) minimizes overcrowding and damage from creasing and curvature. Shielding documents protects them from dust, fading, errant mould spores, and modest amounts of liquid, like the blood, sweat, and tears of archivists. Stabilizing documents reduces damage from unnecessary movement, shifting, and handling. Same-sized items stored next to one another (including boxes) are generally more stable than oddly sized items. Administrative control: Storing boxes efficiently maximizes our use of shelf space. Archivists are always chasing ways to optimize every costly square centimeter of real estate, and standard packaging wastes the least amount. Locating boxes on kilometres of shelving is easier if the boxes can be assigned precise, documented positions. Compact sequentially shelved boxes are documented more readily than jumbled formless piles. Manufacturing standard-sized boxes and folders is more efficient for vendors. This means better prices for archivists, since archival-quality packaging is already expensive. (To stabilize particularly fragile or oddly shaped records, conservators sometimes design and construct custom-fitted containers.) Intellectual control: Understanding archival records requires structure. Archivists analyze and organize records, and then synthesize structured overviews called archival descriptions or finding aids. Physical packaging is linked to these descriptions so that people know what they’re looking at. For example, a physical folder of records will correspond to a “file” in an archival description. Labelling records is easier with regular, clean packaging. File labels link records to their descriptions. How we do it What does all this actually look like on the inside? We’re going to show you, and from two different perspectives. We’ll demonstrate how standard packaging works in Canada, representing North America The UK, also representing some European countries As we’ll see, there is one major difference between the two styles. We’ll dub the North American style vertical standard packaging and the UK style horizontal (flat) standard packaging. I have found that verticalist and horizontalist practitioners feel strongly about the strengths of their own style. It will be interesting to see if this blog post generates any discussion, heated or otherwise. And of course there are a variety of exceptions to what follows. (We’ll return to those at the end of this post.) So, let’s look at each style with respect to two major categories: Basic supplies: the physical containers we rely on Basic configuration: how we use those supplies properly CANADA: VERTICAL standard packaging The following shows how we do common packaging at the Region of Peel Archives. This style tends to be fairly standard across North America, down to the physical dimensions of our supplies. (Just to make sure of this, I reached out to several Canadian experts who corroborated my sense that this packaging style is indeed routine across the continent.) A. Basic supplies Standard packaging in North America relies on two basic physical containers: the file folder and the box. These supplies are supplemented by various enclosures and separators for use within files. The classic North American archival folder (also called a file or a file folder) is a folded piece of acid-free card. The open edge of the file sports a tab or raised edge on which we write information about the file contents, keyed to the archival description. The typical North American archival file folder. Boxes for hand-scale documents generally come in two main styles: The Hollinger Box: In the North American archives world, the Hollinger Box is iconic. The term Hollinger is to boxes as Kleenex is to tissues: used for any narrow upright file box, even ones not actually made by the Hollinger company. These smaller filing boxes are lighter to carry and easier to store. The Bankers Box: A commonly used larger box is the classic “Bankers Box.” This size of box is typically used in records centres of large organizations. An acid-free version is used for modern administrative records in archives. This box can be, let’s say, taxing on staff in the large volumes in which it tends to accumulate. Typical North American archival boxes including (from the left) a Bankers box, a Hollinger box, and a half-Bankers box, a new favourite of the Region of Peel Archives. Some brands have removable lids while others have hinged lids. Like a glove It’s worth pointing out the close fit between the North American file and box. The two supplies are complementary, based on dimensions that evolved with record keeping itself. In organizational office cultures, paper sizes were standardized differently in North America than in the rest of the world. In North America, two popular sizes of hand-scale paper exist: “letter-size” paper (8.5 X 11 inches) and “legal-size” paper (8.5 X 14 inches). At the Region of Peel Archives (and in many other North America archives) we predominantly use the larger legal-size folders designed to accommodate legal-size paper, along with boxes designed to accommodate both. Both Hollinger and Bankers boxes come in lengths constructed for legal-size files. Naturally, many of our records predate this standard sizing; but we find that legal-size folders and boxes accommodate the majority of hand-scale documents created over many centuries. This includes all the examples of hand-scale documents depicted above. Last on our list of basic supplies are a variety of enclosures that we use within files to help further stabilize documents. Enclosures, such as paper or archivally acceptable plastic sleeves, tend to have at least one open edge. This reduces the level at which the record will stew in its own juices (in a “microclimate”) without exposure to air circulation. Some examples of enclosures and sleeves, both paper and archival plastic. B. Basic configuration Now let’s look at how these supplies work together. As we’ve already hinted, at the Region of Peel Archives, as in most of North America, we package hand-scale files vertically in boxes. Files are held in place by the files on either side of them. A Hollinger box of records at the Region of Peel Archives. To protect documents filed this way, we need to package them properly. Here are some guidelines we observe. Packing files ready for boxes File folders should not be overfilled and should be adjusted appropriately. File folders should be manually squared off at the bottom to accommodate the thickness of the records within them. North American file folders generally come with an array of pre-scored lines for this purpose. Left and centre: Correctly foldering a thick file of modern office paper, and a slim file of 19th century documents. Right: Incorrectly foldering some 19th century deeds. Packing files into boxes Boxes should be filled snuggly with files, but not overfilled. Both overfilling and underfilling a box could cause damage like folding or curvature. Archivists test the ideal fit by eye, as well as by ease of access to files, and ability to fit an average hand down the back of a filled box. A vertically filled box seen from above. This box includes file folders containing documents of varying sizes, and also an unpublished bound volume (book). When bound volumes are stored in boxes, we make sure their spines are facing downwards so that the weight of the text block (the pages) does not pull away from the spine. Any space left in a partially filled box needs to be braced so that files don’t slump over. Slumping and curvature are the primary risks of this packaging style. Some archivists tip the box on its side while filling it, and ask archives researchers to do the same when removing files for use. Filling the gap At the Region of Peel Archives, we have invented our own way of filling gaps in boxes with an accordioned piece of polyethylene foam (Ethafoam) sheeting affectionately known (by us) as a “puff.” We find that the folded sheeting adjusts itself nicely as the gap is gradually filled with files. This method is more efficient and less wasteful for us than constructing custom-fitted cardboard spacers, especially as we try eventually to use every inch of space in every box. The “puffs” can be reused many times, unlike custom-made spacers. Controlling documents inside files Archival files commonly contain more than one document. There are a couple of reasons for this. Foldering every individual document would add a lot of bulk. More importantly, records are often closely related to other records: storing them together as a set maintains and clarifies these conceptual chunks. Inside files we use a variety of means to stabilize individual documents and protect them from one another. This is where “house style” may vary between North American archival institutions. Below are some ways we use to stabilize documents at the Region of Peel Archives. Keeping multipage documents together. A single document may have multiple parts, such as a multipage report, or letters with envelopes or inserts. To clarify which pages belong together in a file of multipage documents we may use a piece of buffered acid-free paper as a thin sub-folder for each. Generally speaking, we try to associate pages without altering or damaging them. The pages of this document are kept together inside a file by using a folded piece of bond paper. Of course, some documents come to us clipped together with staples or paperclips. These are removed if corroded, but archivists decide whether to remove all non-archival clips on a case-by-case basis. (Whether and how to replace them with archival paperclips is controversial even among archivists!) Stabilizing fragile records. A particularly fragile document may be placed in an enclosure, such as a polyester sleeve. (Again, only some plastics are suitable for archival use.) This sleeve allows researchers to view and handle the record safely. Similarly, photographic records like slides, negatives, and photographs may require extra protection to ensure the photographic emulsion (image layer) is not damaged. We may also use acid-free tissue paper to protect photographs or other easily scratched surfaces. Left: Photographs interleaved with archival tissue paper to protect image surfaces. Right: A fragile 19th-century letter protected by a Mylar sleeve and backed by a stiff piece of card. Stabilizing small records. Small records like tiny cards and notes, or photographic slides, can shift around and be overlooked more easily within standard files, particularly when housed with larger associated records (think of snapshots enclosed in a letter). We may put smaller items in sleeves to bring them closer to standard document sizes. Small photographs and slides are inserted into paper or polypropylene sleeves to stabilize and keep them in order within larger files. (Large collections of same-sized photographs may be stored in other ways.) Packing boxes on shelves Both Hollinger and Bankers boxes are shelved in a single layer on shelves specially designed for their height. As boxes aren’t piled on top of on another, this allows for quick retrieval and return. Removing a North American archival box from its shelf. Hollinger-style boxes sometimes come with a pullstring or handle to aid retrieval. The labelling on boxes gives no detailed information about the records within. Rather an identification code links the box to information about its contents and allows archivists to find the box on kilometres of shelving. UNITED KINGDOM: HORIZONTAL (FLAT) standard packaging The following is my synthesis of common UK practice, based on chats and visits with UK archivists over the years. Most recently, Lizzy, Rachel, and Shirley (mentioned in the introduction) have been my primary informants. Many thanks to all three for the accompanying photographs. Photographs from both their practices are included: in the UK, box and file dimensions vary more considerably between archival institutions than they do in North America. A. Basic supplies Archivists in the UK also rely on boxes and folders for basic archival packaging of hand-scale documents. However, the exact dimensions of these supplies are not as interrelated or universal as in North America. Rather, archives order boxes that work with their shelving or document types. Then an archivist or conservator will buy or make folders to fit within the footprint of their boxes. Files are physically constructed in a variety of ways. Sometimes archivists use folders made of acid-free card. These are somewhat similar to standard North American file folders, but unlike the latter, they generally include flaps which enclose records or two or more sides. For thinner sets of records, archivists may use large acid-free envelopes as a file. In lieu of a file folder, they may also wrap sets of records in acid-free paper or tissue. Left: Four-flap folder as used in the West Yorkshire Archive Service. (Courtesy and copyright of the West Yorkshire Archive Service, Conservation). Right: Two-flap folder in use at the Tyne & Wear Archives. (Courtesy of the Tyne & Wear Archives.) Boxes for documents in the UK tend to be shallow, with removeable lids. Left: An empty archive box as used by the Tyne & Wear Archives, who have them specially made to fit their shelves. (Courtesy of Tyne & Wear Archives). Right: Archive boxes on the shelves at the West Yorkshire Archives. (Courtesy and copyright of West Yorkshire Archive Service, Wakefield.) Other UK supplies: As we’ll see shortly, in the UK cotton tape is widely used to tie each file shut. And, as in North America, archivists use various ways of stabilizing and associating records within files, including sleeves and paperclips. Basic packaging supplies and enclosures at the West Yorkshire Archive Service (left) and Tyne & Wear Archives (right). Copyright and courtesy of these archives. B. Basic configuration Most archivists in the UK store hand-scale document files flat, by stacking them in shallow boxes. A filled box at the Tyne & Wear Archives showing two ways of packaging files, in folders and in large envelopes (see below). (Courtesy of the Tyne & Wear Archives.) Packing files ready for boxes In UK document boxes, files are commonly tied shut with archival tape made of cotton or linen. The knot or bow is placed to the side of the file. The archival citation is printed on the file so that the file can be identified and retrieved. Closed and tied four-flap folder at the West Yorkshire Archive Service. (Courtesy and copyright of the West Yorkshire Archive Service, Conservation). Packing folders into boxes Files are laid flat in their boxes, one on top of the next, until the box is full. Left: Stack of thicker tied folders ready for stacking into their box. (Courtesy and copyright of West Yorkshire Archive Service.) Right: Loaded box of stacked files. (Courtesy and copyright of The John Goodchild Archive and Antiquarian Collection, West Yorkshire Archive Service, Wakefield.) For this orientation to work, files must be packaged correctly. Folders should not be overfilled past their capacity, to avoid curvature of records. The edges of records should be well clear of the edges of the folder before folding the flaps. The knot of the archival tape should be tied to the side so that a file on top doesn’t experience a pressure point from a knot on the file beneath it. Files should be stacked for maximum stability and protection: for instance, archivists place heavier, thicker, or broader files near the bottom of the stack to avoid damage to lighter, smaller packages. Boxes should be filled to the top so that the box lid does not buckle inwards from the weight of boxes placed on top of it. Controlling records inside files As in North America, UK archives each have their own in-house preferences for controlling documents within files. The range of practices on both sides of the pond are very similar. That said, one uniquely British way of controlling documents within files includes the use of the “treasury tag,” particularly for modern government files. The treasury tag is piece of cord with wide ends. Holes are punched through one corner of a stack of related papers or enclosures, and the cord loosely passed through them. The tag ensures that papers from a file can’t become disassociated while being used. You can learn more about how tags are applied in guidance from the UK’s National Archives. (Not all UK archives apply new tags, but many will receive records that have been tagged before reaching the archives.) Left: Treasury tags of different lengths made to accommodate different file thicknesses. Right: A file bound with a treasure tag. (Courtesy of the National Archives Preparation of Records.) Storing boxes on shelves Flat storage boxes in the UK are stacked one on top of the other within the limits of shelf depth and the pressure from accumulated weight. Left: Double stacks of boxes at the Tyne & Wear Archives. (Courtesy of the Tyne & Wear Archives.) Right: Triple stacks of boxes at the West Yorkshire Archives. (Courtesy and Copyright of the West Yorkshire Archive Service, Wakefield.) Face-off: advantages and disadvantages Over the years, I’ve met archivists from both sides of the Atlantic who were initially surprised by one another’s packaging style. The generous professionals I spoke with for this post have risen to the challenge of direct comparison. Below is our collective sense of the relative strengths and weaknesses of our differing styles. We agree that both methods have advantages and disadvantages, and that both work as long as guidelines are observed. Vertical packaging Advantages: Documents are gently suspended and don’t move around very much even if the box is jostled. The only edge of a document touching its file (the bottom edge) is protected from damage or folding by this sandwiching of the entire document. Records are free from the weight of other records being stored on top of them. Stacks of larger items do not curve around stacks of smaller items. (Curvature of larger items around other items can be a problem with items stored on top of each other.) One file can be easily removed without having to rummage through a pile, cutting down on handling. Boxes are not weighed down by other boxes on top of them. One box can be removed from a shelf without removing other boxes, again cutting down on handling. By reading the top file tabs you can see at a glance what the box contains. The open-at-the-top file allows for healthy air circulation. Packaging, retrieving, and replacing documents, files, and boxes is relatively quick. Disadvantages: Improperly filled files and boxes can cause curvature and damaged edges. Contents of files can spill out if a box is dropped or tipped and the lid comes off. When boxes are being filed, or when files are removed for use, the remaining files can slump. Horizontal (flat) packaging Advantages Flat storage ensures gravity is acting on documents equally at all points. This helps protect documents from problems with slumping and curvature. As long as documents and boxes are not overweighted, horizontal storage helps to flatten and immobilize documents. Folders that completely encase documents and are tied shut offer the maximum protection from dust and light, and from being dropped and scattered. Folders of varying dimensions can be stacked in a single box. Disadvantages Sometimes multiple folders need to be removed from boxes to search for and remove a particular folder. This does increase overall handling of records. Sometimes boxes may need to be moved to access other boxes below them. Archives researchers don’t always retie tape appropriately, so archivists need to monitor tying. Tape that is too tight or improperly tied can cause notching in the sides of records or files. Packaging, retrieving, and replacing documents and their containers can be a little more time-consuming. Wrapping up Interestingly, the horizontal versus vertical division we’ve looked at is echoed in archives around the world. Glimpses of archival storage on social media posts suggest that across Europe, for example, both methods are followed. What is the basis for what seems a big difference (to archivists anyway)? The answer likely lies in the emphasis on vertical storage in the US after the invention of the filing cabinet in the late 19th century – and the fact that many North American archives put down formal roots after this time. Media historian Craig Robertson has written a fascinating article and now a book on vertical storage and its role in office culture for those who wish to pursue a direct link to the archival Hollinger box. Aside from habits rooted in history, our preferences may be philosophical and aesthetic. British conservator Sheila Jones muses that there is a sense in which flat storage visually symbolizes a shift in status. Horizontal storage, she suggested in our conversation, gives us a sense that archival records are special: their active “work is done, and they may now lie down in their boxes” safely and behind the scenes. Perhaps, in the end, we should focus on the ultimate goal of preservation. In that spirit, we return to some commonalities. Here are some caveats all our archivists wanted you to know. Top: Shallow flat boxes stacked one on top of the other at the Region of Peel Archives in Canada. Bottom: Shallow flat boxes turned on their sides in a single layer at the National Archives of the UK. (Courtesy of the National Archives.) First, our packaging methods do overlap. For instance, the Region of Peel Archives routinely stores records in flat shallow boxes if those records are larger than our legal-size folders. We’ll even wrap certain types of records in acid-free paper, and tie wrappers shut with linen tape! And in the UK, a few archives routinely store some records vertically. An interesting example is the National Archives in the UK, which uses a hybrid method: shallow boxes are loaded flat, and then tied up with tape and turned on their sides in storage. Second and relatedly, remember that our show-and-tell above applies to one very common range of sizes and types of records, that is, hand-scale documents. Most institutional archives collect records that fall outside this range. Indeed, the sheer variety and fragility of the world’s documentary heritage means archivists and conservators must show considerable ingenuity and skill in storing it safely. This blog post has not been able to cover a host of packaging challenges, including audiovisual and digital records (those are, as they say, another story). Below you’ll find a slideshow gallery of other packaging configurations. Finally, what we’ve represented above is an ideal. The volume of valuable records increases exponentially and relentlessly year by year, at pace with the human activity that produces it. Archivists and conservators often need to triage repackaging: they must balance availability for public access against packaging perfection. For example, sometimes archivists must decide if the original containers in which files arrive might suffice for the short or the long term. In such cases, we will remove the most destructive elements of original packaging – such as plastics that eventually turn to gluey slime, or rusting paperclips – while retaining relatively stable original containers. Archival work is, if nothing else, a compromise with reality. While we can’t halt time, we can slow down its effects using the humble ammunition of packaging. Bonus gallery of packaging picks We conclude with a casual stroll through archival packaging examples from both the UK and Canada that go beyond what we’ve explored above. (We feel morally obligated to point out that all boxes depicted below have lids, even when not shown!) We hope this slideshow demonstrates our unity of purpose and practice on both sides of the Atlantic. A photograph album in a flat box at the Region of Peel Archives. Highly acidic albums and scrapbooks are a challenge if photos and other documents are glued into them, or pages feature important annotations. Often the best we can do is store the album as is, perhaps interleaving pages with acid-free tissue. Vellum (parchment) deed with wax seal, stabilized in a flat box at the Region of Peel Archives. Custom-made box for a particularly fragile bound volume at the West Yorkshire Archive Service. (Anne Lister Diary No. 12, 1829 – 1830. SH:7/ML/E/12. Photograph courtesy and copyright of West Yorkshire Archive Service, Calderdale.) A custom-made large shallow box constructed to house an illuminated vellum (parchment) document with an attached seal, shown on the conservation desk at the West Yorkshire Archive Service. (Inspeximus of Henry VIII, WYL100/HX/A/42, West Yorkshire Archive Service, Leeds. Photograph courtesy and copyright of the West Yorkshire Archive Service.) Large shelves for very large maps at the West Yorkshire Archive Service. (Courtesy and copyright of the West Yorkshire Archive Service, Wakefield.) Hanging storage for framed items and historic signage at Tyne & Wear Archives. (Courtesy of the Tyne & Wear Archives.) Storage of large flat maps and plans at the Tyne & Wear Archives. (Courtesy of the Tyne & Wear Archives.) Photographs housed in uniformly sized sleeves in a divided box. (Courtesy and copyright of the West Yorkshire Archive Service, Conservation.) Rolled plans in special slots at the Tyne & Wear Archives. (Courtesy of the Tyne & Wear Archives.) Specialty storage for large rolled plans at the West Yorkshire Archive Service. (Courtesy and copyright of the West Yorkshire Archive Service, Wakefield.) Glass negatives stored vertically in a box at the Tyne & Wear Archives. (Courtesy of the Tyne & Wear Archives.) Loose and bundled turn-of-the-century tickets from an abandoned railway route stored in a flat box at the Region of Peel Archives. Awkwardly shaped and oversize records in a flat box at the Region of Peel Archives. Wrapped ledgers at the Region of Peel Archives. A growing collection of postcards housed in sleeves in a complementary box, at the Region of Peel Archives. Map cabinets at the Region of Peel Archives, housing broadsides (posters), maps, plans, and other large-scale documents. Fragile and compromised glass plate negative stabilized in a custom enclosure made by a conservator, at the Region of Peel Archives. Posted by Samantha Thompson, Senior Archivist Share this: Twitter Facebook Like Loading...",
    "commentLink": "https://news.ycombinator.com/item?id=41605774",
    "commentBody": "How do archivists package things? The battle of the boxes (peelarchivesblog.com)141 points by bookofjoe 21 hours agohidepastfavorite23 comments jonathanlydall 49 minutes agoIt’s good to look at physical systems like this to understand how we’ve landed up with software systems which we take for granted. For example, when I was about 10 in the early 90s, I remember being taught how to use our school library which at that time had no computers. The books were sorted on the shelves by author’s surname, so finding them when you knew the author was easy, but what about when you only knew the book title? Well the solution to that was to look through little cards in a series of draws sorted by book title, when you found the card it had the author name and you could then easily find the book. These cards were called index cards. Software databases model this system. A clustered index is essentially equivalent to the books being on the shelves in a particular order, while a normal index is like the index cards. Email was essentially modelled off of real world postal systems. And when I read this article talking about standard box sizes, I think of cluster sizes on computer disk drives. Software is a lot less abstract than a lot of people may realise. reply ttyprintk 18 hours agoprevFor the (over?) engineered storage box for Isaac Newton’s death mask, Adam Savage chose aluminum and an archival-quality polymer. https://m.youtube.com/watch?v=5Ukv0sPsePY reply freosam 9 hours agoprevThe horizontal vs vertical storage thing is interesting. I've often wondered what the rationale is with the difference, and it seems that it's cultural to a large extent. One thing that wasn't covered, that sometimes matters for non-institutional collections, is that cardboard is thicker than plastic, and can add quite a lot to the number of boxes required for a given collection. Polyester or polypropylene sleeves (open at the top, and stored vertically, i.e. to allow gas exchange) can be as cheap and sometimes are a better option, at ~0.08 mm vs ~0.5 mm. reply interestica 2 hours agoprevAnyone have a good indexing system/software? A spreadsheet works, but I want to scan a code on a box and see what’s inside. reply tokai 1 hour agoparentAll cataloguing/indexing software I know of is kinda bad. Especially if you're not running a whole library/collection. The open source projects in this space are under cooked[0], and the enterprise ones are expensive and labyrinthine in use. Maybe Access or some other DBMS would be better suited. [0] https://itsfoss.com/open-source-library-management-software/ reply brianzelip 8 hours agoprevArchivesSpace has a \"space calculator\" feature to help with this! https://github.com/archivesspace/archivesspace reply jhardy54 17 hours agoprevI went into this hoping for a solution to my “how do I store all of my random stuff from having lots of projects and interested”, and while I enjoyed the article, I am shocked to report that my question was not answered. Strange to imagine that I don’t have the exact same needs as archivists. reply mattlondon 14 hours agoparentRecently I had this question myself when trying to organise DIY materials and spare parts (for which I have a lot somehow). I ended up buying modular \"Ivar\" shelves from IKEA where shelves can be positioned every 2 cm or so as you need them, and you buy as many or as few shelves and uprights as you need. I got about twenty ~25 litre clear plastic storage boxes (with lids! Very important to prevent dust etc) and set the shelf height to suit those. Things are roughly grouped into boxes with a big label on the outside (\"screws\", \"adhesives\", \"electrical\" etc). I built a small ~1.25x0.5m worktop at hip-height next to the shelves so when I pull out a box I have somewhere right there to put it and reach in to get things (otherwise you need to put it on the floor which is a pain). Below the worktop I have some metal drawers that I use for hand tool storage. Finally I have a small plastic open tote toolbox with a handle so I can load that up with bits and pieces and hand tools etc when I am at the shelves/mothership, and easily carry them to wherever I need them. The wooden shelves are handy as you can cut and modify them very easily so you can trivially customise them to the space you have. They're fairly sturdy when assembled and attached to a wall. Not very sophisticated really but seems to be working well so far. Previously I was hunting through random cardboard boxes or bags (\"I know I've got a foobar whatsit here somewhere!\"), typically stacked on top of each other. I think not having things stacked has been the most useful thing - i.e. if you need box X, you just pull it out right away and don't have to unstack 4 precariously-balanced other boxes first only to find that the thing you thought was in the box at the bottom actually was not there and now you need to stack everything back up again then look in another battered Amazon cardboard box in another half-collapsed tower of boxes etc. Good luck. reply neilv 2 hours agorootparent> I ended up buying modular \"Ivar\" shelves from IKEA where shelves can be positioned every 2 cm or so as you need them, This shelf density solution is what I needed for my small-parts plastic shoeboxes setup. The $1 plastic shoeboxes are a great size (though presumably not so electrostatic-safe nor archival-safe), and you can also stack them 2-high on normal bookshelves, but there's still a lot of wasted space. Which means they take up more floor space, and more visual space. reply larsrc 10 hours agorootparentprevGood system! Clear plastic boxes and labels are essential. For projects in progress, I have started using ziplock bags that I label, so I don't have small collections of stuff constituting a project lying all over. reply datadrivenangel 6 hours agorootparentGet a couple of stacking plastic tubs, and use them for in-progress projects! reply Terr_ 9 hours agorootparentprev> (with lids! Very important to prevent dust etc) Yeah, in the long run \"stackable boxes with lids that go onto an independent shelf\" are much better than various \"stackable plastic drawers\". The latter are not resilient against the wear of time, ex: * Frame for a drawer breaks? Now you have a weird tub with a handle on only one end. * Drawer bottom breaks? Now you have an empty spot you can't fill. * Need to move or reorganize content? You either have to put everything on the floor or your stack ability depends on the squishiness of the contents. reply internet101010 13 hours agoparentprevIt really depends. For anything involving paper or metal the answer is mylar. Keep things out of direct sunlight at 70F and 30-50% humidity. reply AstroJetson 16 hours agoparentprevMe too. But I will pass on that transparent packing (boxes, bags, etc) is the way to go. I’ll pack smaller items in zip lock bags (they come in sizes from “snack” to 4 gallon) then they go in transparent boxes. It’s amazing that being able to see into makes finding thing so much faster. I’ve also found that hanging jewelry bags are awesome for parts storage in a closet. Good luck on your journey. reply _VX3r 14 hours agorootparentDepends on what you are packaging. I have learned that books shouldn’t be packaged in plastic since they need to ‘breath’ reply freosam 9 hours agorootparentTrue. Although the ziploc bags can just be left a bit open, that's quite sufficient. The good thing about food-safe bags is that they're usually polypropylene and so good for archival use (and much cheaper than anything from a preservation-supplies shop). reply ninalanyon 3 hours agorootparent> food-safe bags is that they're usually polypropylene Interesting. I've just checked the biggest grocery site in Norway (oda.com) and two out of the three bags they sell were low density polythene, the third was polypropylene. Not a big sample I'll admit. I'm pretty sure that the very thin bags provided in supermarkets here for fresh loose produce are also polythene. reply tomjen3 12 hours agorootparentprevThe article specifically says not to do that, as traps moisture. reply creer 9 hours agorootparent> moisture Exactly. So are there breathable dust-proof ziplock bag equivalents? At a reasonable price? And I find one obvious direction: produce bags. 10\" square, 25c each. reply tdeck 12 hours agoprev> File folders should be manually squared off at the bottom to accommodate the thickness of the records within them. North American file folders generally come with an array of pre-scored lines for this purpose. This is a revelation for me! Somehow I never noticed what these were for and have been using folders wrong my entire life. reply neilv 2 hours agoparentInterestingly, even the file folders that hang from hooks at the top have this scoring. Canonical: https://www.tops-products.com/pendaflexr-surehookr-reinforce... Better photo: https://www.amazon.com/dp/B01MXC0YS5/ (Source: Nerd young child with Pendaflex filing system.) reply tdeck 18 minutes agorootparentThis is in fact the kind I've been using wrong at home :) reply TheRealPomax 1 hour agoprev [–] Please don't mess with the default font size. I can't read something this small without having to hack it back to 16px. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article compares archival packaging methods in Canada and the UK, highlighting differences in how physical documents are stored for long-term preservation.",
      "North American archivists typically use vertical packaging with file folders and boxes like the Hollinger Box, while UK archivists prefer horizontal packaging with folders and shallow boxes.",
      "Each method has its pros and cons: vertical packaging offers easy retrieval and good air circulation but risks curvature, while horizontal packaging provides even weight distribution but requires more handling."
    ],
    "commentSummary": [
      "Archivists use various methods to package items, often drawing parallels to software systems, such as comparing physical library index cards to software databases.",
      "Practical storage solutions include modular shelves, clear plastic boxes, and materials like polyester sleeves, which can be more efficient than traditional cardboard.",
      "Proper storage conditions for paper and metal involve using mylar, controlled temperature, and humidity, with transparent packing aiding in quick identification."
    ],
    "points": 141,
    "commentCount": 23,
    "retryCount": 0,
    "time": 1726868735
  },
  {
    "id": 41606493,
    "title": "CISA boss: Makers of insecure software are the real cyber villains",
    "originLink": "https://www.theregister.com/2024/09/20/cisa_sloppy_vendors_cybercrime_villains/",
    "originBody": "Software 47 CISA boss: Makers of insecure software are the real cyber villains 47 Write better code, urges Jen Easterly. And while you're at it, give crime gangs horrible names like 'Evil Ferret' Jessica Lyons Fri 20 Sep 2024 // 00:33 UTC Software suppliers who ship buggy, insecure code are the true baddies in the cyber crime story, Jen Easterly, boss of the US government's Cybersecurity and Infrastructure Security Agency, has argued. \"The truth is: Technology vendors are the characters who are building problems\" into their products, which then \"open the doors for villains to attack their victims,\" declared Easterly during a Wednesday keynote address at Mandiant's mWise conference. Easterly also implored the audience to stop \"glamorizing\" crime gangs with fancy poetic names. How about \"Scrawny Nuisance\" or \"Evil Ferret,\" Easterly suggested. Even calling security holes \"software vulnerabilities\" is too lenient, she added. This phrase \"really diffuses responsibility. We should call them 'product defects,'\" Easterly said. And instead of automatically blaming victims for failing to patch their products quickly enough, \"why don't we ask: Why does software require so many urgent patches? The truth is: We need to demand more of technology vendors.\" Why does software require so many urgent patches? We need to demand more of vendors While everyone in the audience at the annual infosec conference has job security, Easterly joked, it's also the industry's role to make it more difficult for miscreants to compromise systems in the first place. \"Despite a multi-billion-dollar cyber security industry, we still have a multi-trillion-dollar software quality issue leading to a multi-trillion-dollar global cyber crime issue,\" Easterly lamented. While no one would buy a car or board an airplane \"entirely at your own risk,\" we do that every day with the software that underpins America's critical infrastructure, she added. \"Unfortunately we have fallen prey to the myth of techno exceptionalism,\" Easterly opined. \"We don't have a cyber security problem – we have a software quality problem. We don't need more security products – we need more secure products.\" CISA director: US is 'not afraid' to shout about Big Tech's security failings CISA boss: Secure code is the 'only way to make ransomware a shocking anomaly' 68 tech names sign CISA's secure-by-design pledge 'Four horsemen of cyber' look back on 2008 DoD IT breach that led to US Cyber Command This is a drum Easterly has been beating since she took the helm of the US cyber defense agency. She tends to bang it louder at industry events, such as the annual RSA Conference where she told attendees secure code \"is the only way we can make ransomware and cyber attacks a shocking anomaly.\" Naturally, if writing flawless code was super easy, it would be done without fail. Some developers are clearly careless or clueless, leading to vulnerabilities and other bugs, and sometimes skilled humans with the best intentions simply make mistakes. In any case, Easterly isn't happy with the current defect rate. Also at RSAC, nearly 70 big names – including AWS, Microsoft, Google, Cisco, and IBM – signed CISA's Secure by Design pledge – a commitment to \"make a good-faith effort to work towards\" seven secure-software goals within a year, and be able to measurably show their progress. At mWise, Easterly revealed that number has grown to nearly 200 vendors. But the pledge remains voluntary, so software companies who fail to follow its guidelines – such as increasing the use of multi-factor authentication across their products and reducing default passwords – aren't going to be slapped down if they ignore it. Google says replacing C/C++ in firmware with Rust is easy READ MORE Easterly wants that to change. She suggested technology buyers use their procurement power to pressure software vendors, by asking suppliers if they have signed the pledge – and, hopefully, done more than just put ink to paper in terms of building secure-by-design [PDF] products. To this end, CISA just published guidance that organizations buying software can use, and questions they should ask manufacturers, to better understand if they are prioritizing security in the product development life cycle. \"Use your voice, take an active role, use your purchasing power to advance secure by design, by demanding it,\" Easterly urged. And then cross your fingers and pray that more and more vendors really do begin to take things like pre-release software testing and secure code to heart. ® Whitepaper: Top 5 Tips For Navigating Your SASE Journey Share More about Cybersecurity and Infrastructure Security Agency Security More like these × More about Cybersecurity and Infrastructure Security Agency Security Narrower topics 2FA Advanced persistent threat Application Delivery Controller Authentication BEC Black Hat BSides Bug Bounty CHERI CISO Common Vulnerability Scoring System Cybercrime Cybersecurity Cybersecurity Information Sharing Act Data Breach Data Protection Data Theft DDoS DEF CON Digital certificate Encryption Exploit Firewall Hacker Hacking Hacktivism Identity Theft Incident response Infosec Kenna Security NCSAM NCSC Palo Alto Networks Password Phishing Quantum key distribution Ransomware Remote Access Trojan REvil RSA Conference Spamming Spyware Surveillance TLS Trojan Trusted Platform Module Vulnerability Wannacry Zero trust Broader topics Federal government of the United States More about Share 47 COMMENTS More about Cybersecurity and Infrastructure Security Agency Security More like these × More about Cybersecurity and Infrastructure Security Agency Security Narrower topics 2FA Advanced persistent threat Application Delivery Controller Authentication BEC Black Hat BSides Bug Bounty CHERI CISO Common Vulnerability Scoring System Cybercrime Cybersecurity Cybersecurity Information Sharing Act Data Breach Data Protection Data Theft DDoS DEF CON Digital certificate Encryption Exploit Firewall Hacker Hacking Hacktivism Identity Theft Incident response Infosec Kenna Security NCSAM NCSC Palo Alto Networks Password Phishing Quantum key distribution Ransomware Remote Access Trojan REvil RSA Conference Spamming Spyware Surveillance TLS Trojan Trusted Platform Module Vulnerability Wannacry Zero trust Broader topics Federal government of the United States TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=41606493",
    "commentBody": "CISA boss: Makers of insecure software are the real cyber villains (theregister.com)129 points by tsujamin 18 hours agohidepastfavorite118 comments hn_throwaway_99 17 hours agoAt this point, I have to wonder what is even the point of missives like this. There are only two things that will solve the software quality problem: 1. Economic incentives. It's all just mindless blather unless you're actually talking about ways that software vendors will be held liable for bugs in their products. If you're not talking about that, what you're saying is basically \"ok pretty please\" useless. 2. Reducing the complexity of making products secure in the first place. Making truly secure software products is incredibly hard in this day and age, which is one reason why demanding software product liability is so scary. Professional structural engineers, for example, are used to taking liability for their designs and buildings. But with software security the complexity is nearly infinitely higher, and making it secure is much harder to guarantee. The other thing that people often ignore, or at least don't want to admit, is that the \"move fast and break things\" ethos has been phenomenally successful from a business perspective. The US software industry grew exponentially faster than anyplace else in the world, even places like India that doubled down on things like the \"Software Capability Maturity Model\" in the early 00s, and honestly have little to show for it. reply gwd 10 hours agoparent> At this point, I have to wonder what is even the point of missives like this. ...It's all just mindless blather unless you're actually talking about ways that software vendors will be held liable for bugs in their products. I think that liability for bugs is exactly where she's going with this. I'm not an expert, but it sounds from a few things I've heard on some Lawfare podcasts (e.g,. [1][2]) like the idea of software liability has been discussed for quite a while now in government policy circles. This sort of public statement may be laying the groundwork for building the political will to make it happen. [1] https://www.youtube.com/watch?v=9UneL5-Q98E&pp=ygUQbGF3ZmFyZ... [2] https://www.youtube.com/watch?v=zyNft-IZm_A&pp=ygUQbGF3ZmFyZ... EDIT: > Making truly secure software products is incredibly hard in this day and age, which is one reason why demanding software product liability is so scary. Loads of companies already are liable for bugs software that runs on their products: this includes cars, airplanes, and I would presume medical devices, and so on. The response has been what's called \"safety certification\": as an industry, you define a process which, if followed, you can in court say \"we were reasonably careful\", and then you hire an evaluator to confirm that you have followed that process. These processes don't prevent all bugs, naturally, but they certainly go a long way towards reducing them. Liability for companies who don't follow appropriate standard processes would essentially prevent cloud companies cutting security to get an edge in time-to-market or cost. reply jand 9 hours agorootparent> The response has been what's called \"safety certification\": This is the most scary part for me. Certifications are mostly bureaucratic sugar and on the other hand very expensive. This seems like a sure way to strangle your startup culture. If customers require certifications worth millions, nobody can bootstrap a small business without outside capital. reply Eddy_Viscosity2 5 hours agorootparentAssuming the level of certification will be proportionate the potential risk/harm, then this is actually totally ok. Like would you want to fly in a plane built but a bootstrap startup that had no certifications? Or go in a submarine to extremely deep ocean tours of the titanic? Or put in a heart device? Or transfer all of your savings to a startup's financial software that had no proof of being resilient to even the most basic of attacks? For me, its a hard no. The idea of risk/harm based certification and liability is overdue. reply mike_hearn 2 hours agorootparentProblem is that it's rarely proportional. There's a different thread on HN about the UK Foundations essay. It gives the example of the builders of a nuclear reactor being required to install hundreds of underwater megaphones to scare away fish that might otherwise be sucked into the reactor and, um, cooked. Yet cooking fish is clearly normal behavior that the government doesn't try to restrict otherwise. This type of thing crops up all over the place where government certification gets involved. Not at first, but the ratchet only moves in one direction. After enough decades have passed you end up with silliness like that, nobody empowered to stop it and a stagnating or sliding economy. > Like would you want to fly in a plane built but a bootstrap startup that had no certifications? If plenty of other people had flown in it without problems, sure? How do you think commercial aviation got started? Plane makers were startups once. But comparing software and planes (or buildings) is a false equivalence. The vast majority of all software doesn't hurt anyone if it has bugs or even if it gets hacked. It's annoying, and potentially you lose money, but nobody dies. reply Eddy_Viscosity2 43 minutes agorootparentCommercial aviation was regulated because planes were killing people, and when it came in, air travel became the safest form of transportation. That isn't a coincidence. If the vast majority of software doesn't hurt anyone if it has bugs, then it won't require any certifications. If you heard me arguing for that, then you heard wrong. I am advocating for risk/harm based certification/liability. reply generic92034 9 hours agorootparentprev> The response has been what's called \"safety certification\": as an industry, you define a process which, if followed, you can in court say \"we were reasonably careful\", and then you hire an evaluator to confirm that you have followed that process. Can you still call it \"liability\" when all you have to do is performing some kind of compliance theater to get rid of it? reply telgareith 8 hours agorootparentAsk Boeing. reply Buttons840 14 hours agoparentprevA third option is to empower security researchers and hope the good guys find the security holes before the bad guys. Currently, we threaten the good guys, security researchers, with jail way to quickly. If someone presses F12 and finds a bunch of SSNs in the raw HTML of the State's web page the Governor personally threatens to send them to jail[0]. The good security researchers tip-tow around, timidly asking permission to run pentests while the bad guys do whatever they want. Protect security researchers, change the laws to empower them and give them the benefit of the doubt. I think a big reason we don't do this is it would be a burden and an embarrassment to wealthy companies. It's literally a matter of national security and we current sacrifice national security for the convenience of companies. As you say, security is hard, and putting liability for security issues on anyone is probably unreasonable. The problem is companies can have their cake and eat it too. The companies get full control over their software, and they get to pick and choose who tests the security of their systems, while at the same time having no liability. The companies are basically saying \"trust me to take care of the security, also, I accept no liability\"; that doesn't inspire confidence. If the liability is too much to bear, then the companies should give up control over who can test the security of their systems. [0]: https://techcrunch.com/2021/10/15/f12-isnt-hacking-missouri-... reply ezoe 12 hours agorootparentIt suggest that insecure software should be simply called defective product. So the security audit should be called QA. A product, which don't spend a lot on QA, profit more. Unless there will be a catastrophic incident. Also, why haven't those so called security researchers jointly criticized EDR yet? A third-party closed source kernel driver which behave like, practically same as malware. reply Thorrez 11 hours agorootparentSoftware has tons of bugs. A fraction of those bugs are security vulnerabilities. Any type of bug can be considered a defect, and thus can be considered to make the product defective. By using the terminology \"defective\" instead of \"vulnerable\" we lose the distinction between security bugs and other bugs. I don't think we want to lose that distinction. reply irundebian 10 hours agorootparentSecurity-related product defect, or simply security defect. reply acdha 5 hours agorootparentprev> Also, why haven't those so called security researchers jointly criticized EDR yet? A third-party closed source kernel driver which behave like, practically same as malware. They have been, for years. Some very prominent voices in the security community have been criticizing the level of engineering prowess in security tools for ages - Travis Ormandy ripped into the AV industry for Project Zero over a decade ago and found critical problems in things like FireEye, too. The problem is that without penalties, the companies just keep repeating the cycle of “trust us” without improving their levels of craft or transparency. reply specialist 5 hours agorootparentprevEmpathic agreement. Source: Was QA/test manager for a bit. Also, recovering election integrity activist. TIL: The conversation is just easier when bugs, security holes, fraud, abuse, chicanery, etc are treated as kinds of defects. Phrases like \"fraud\" and \"exploit\" are insta-stop conversation killers. Politicians and managers (director level and above) simply can't hear or see or reason about those kinds of things. (I can't even guess why not. CYA? Somebody else's problem? Hear no evil...?) QA/Test rarely receives its needed attention and resources. Now less than ever. But advocating for \"fixing bugs\" isn't a total bust. reply thayne 15 hours agoparentprev> But with software security the complexity is nearly infinitely higher, and making it secure is much harder to guarantee. The other thing is that software, especially software connected software (which these days is most software) has to have a much higher level of security than most other industries. When a structural engineer builds a bridge they don't have to worry abou a large number of criminals from all over the world trying to find weaknesses that can be exploited to cause the bridge to collapse. But software engineers do have to worry about hackers, including state sponsored ones, constantly trying to find and exploit weaknesses in their software. I think it absurd to put blame on software engineers for failing to make flawless software instead of the bad actors that exploit bugs that would never be noticed during normal operation, or the law enforcement that is ineffective at stopping such exploitation. Now, I do think that there should be more liability if you don't take security seriously. But there is a delicate balance there. If a single simple mistake has the potential to cause devastating liability, that will seriously slow down the software industry and substantially increase the cost. reply Veserv 14 hours agorootparentIf a single simple mistake has the potential to cause devastating harm, then that is precisely the standard that should be demanded. If you do not want that, then you can work on something where your mistakes do not cause immense harm to others or reduce the scope of the system so such harm is not possible. Imagine a mechanical engineer making industrial robots complaining about how unfair it is that they are held liable for single simple mistakes like pulping people. What injustice! If you want to make single simple mistakes, you can work on tasks where that is unlikely to cause harm like, I don't know, designing door handles? Nothing wrong with making door handles, the stakes are just different. \"But I want to work on problems that can kill people (or other devastating harm), but not be responsible for killing them (or other devastating harm)\" is a utterly insane position that has taken hold in software and has no place in serious, critical systems. If you can not make systems fit for the level of harm they can cause, then you have no place making such systems. That is irrespective of whether anybody in the entire world can do so; systems inadequate to minimize harm to the degree necessary to provide a positive societal cost-benefit over their lifetime are unfit for usage. reply Phiwise_ 8 hours agorootparent>Imagine a mechanical engineer making industrial robots complaining about how unfair it is that they are held liable for single simple mistakes like pulping people. What injustice! If you want to make single simple mistakes, you can work on tasks where that is unlikely to cause harm like, I don't know, designing door handles? Nothing wrong with making door handles, the stakes are just different. This is a truly absurd comparison. In the first place, yes, it is in fact much easier to make physical products safer, as anyone who's ever seen a warning line or safety guard could tell you. The manufacturers of CNC mills don't accept liability by making it impossible to run a bad batch that could kill someone, they just put the whole machine in a steel box and call it a day. The software consumers want has no equivalent solution. What's worse, in the second place, these engineers aren't actually held responsible for the equivalent of most software breaches already. There pretty much is zero liability for tampering or misuse, thus the instruction booklet of 70% legal warnings that still comes with everything you buy even in this age of cutting costs. Arguing software should be held to the same standard as physical products, when software has no equivalent of safety lockouts, is just to argue it should include acceptable use sections in its terms and conditions, which is no real improvement to people's security in the face of malicious actors. reply thayne 10 hours agorootparentprevDo you think that the mechanical engineer should be held liable if, say a criminal breaks into the factory, and sabatoges the robot to damage property or injure workers, because they didn't make the robot sufficiently resistent to sabatoge attempts? reply Veserv 1 hour agorootparentIs that something that you expect to be attempted against a significant fraction of the products over the average expected lifetime of the product? If so, they should absolutely be required to make a product fit for its usage context that is robust against common and standard failure modes, like adversarial attacks. That such requirements are relatively uncommon in physical products due to their usage contexts being fundamentally airgapped from global adversaries is a blessing. Their requirements are lower and thus their expected standards can be lower as well. What a concept! You seem to think that standards are some kind of absolute line held in common across all product categories. Therefore it is unfair to hold some products to higher standards. That is nonsense. Expected standards are in relation to the problem and usage context and should minimize harm to the degree necessary to result in positive societal cost-benefit over their lifetime. That is how literally everything else works and principle that can be applied in general. “But my problems are different, can I ignore them and inflict devastating harm on other humans” is asinine. If you are causing net harm to society, you should not get to do it. Period. reply thayne 22 minutes agorootparentOk let's try another analogy. Should a carmaker be held liable if someone breaks into a car and steals something valuable (laptop, purse, etc.)? Or how about if someone drives irresponsibly (possibly under the influence) and injures or kills someone in an accident? Those are things that a significant number of cars will encounter. And yes if you don't put any effort into making your car secure or safe, there should be liability, but if you put in significant effort to security it is unreasonable to make you liable because there is more you could have done, because there is always more you could have done, and there are significant tradeoffs between security and safety and useability and cost. Maybe you could build a car that is impossible to break into, but it would be a huge pain to use, and most people wouldn't be able to afford it. > inflict devastating harm on other humans I'm not sure where you are getting this \"devastating harm\" from. The context of this is the large number of \"urgent\" security patches and vulnerability disclosures. The thing is, the vast majority of those are probably not as bad as the terminology might lead you a layperson to believe. Often in order to exploit a vulnerability in the real world, you have to chain multiple vulnerabilities together. A lot of \"critical\" vulnerabilities in software I need to patch, I read through the description and determine that it would be extremely difficult, or impossible to exploit the way I use the software, but the patch is still necessary because some (probably small) set of users might be using it in a way that could be exploited. reply pjmlp 3 hours agorootparentprevMoving goalposts posts, if the person was injured by something the mechanical engineer has made themselves, contrary to software industry, they are indeed liable, and can even be the target of a lawsuit. And since a mechanical engineer is a proper engineer, not someone that call themselves engineer after a six weeks bootcamp, additionally they can lose the professional title. reply mike_hearn 2 hours agorootparentThe goalpost isn't moving. Comparisons to physical products are wrong because the question is not \"are there safety critical bugs\" but \"can this product survive a sustained assault by highly trained adversaries\", which is a standard no other product is held to. reply pjmlp 1 hour agorootparentSilly me thinking there is something like product recalls, food poisoning, and use cases that insurances don't cover due to high liability in hazardous goods. Anything that brings proper engineering practices into computing, and liability for malpractices, gets my vote. reply tastyfreeze 14 hours agorootparentprevLock makers aren't liable for making pickable locks. Punish the bad actors and leave the liability to civil suits. reply soerxpso 1 hour agoparentprev> Professional structural engineers, for example, are used to taking liability for their designs and buildings. But with software security the complexity is nearly infinitely higher, and making it secure is much harder to guarantee. I'm not sure about your claim that structural engineering is less complex, but there's another (arguably much more significant) difference: structural safety is against an indifferent adversary (the weather, and physics); software security is against a malicious adversary. If someone with resources wants to take down a building (with exception for certain expensive military installations), no amount of structural engineering is going to stop them. Software that isn't vulnerable to cyberattacks should be compared to a bunker that isn't vulnerable to coordinated artillery strikes, not to your average building. reply EnigmaFlare 14 hours agoparentprevI agree with her about blaming developers, not hackers. Though not to the point of liability for all developers, but maybe for a few specialist professionals who take on that responsibility and are paid appropriately for it. Hackers are essentially a force of nature that will always exist and always be unstoppable by law enforcement because they can be in whatever country doesn't enforce such laws. You wouldn't blame the wind for destroying a bridge - it's up to the engineer to expect the wind and make it secure against that. Viewing them this way makes it clear that the people responsible for hacks are the developers in the same way developers are responsible for non-security bugs. Blame is only useful if you can actually use it to alter people's behavior - which you can't for international hackers, or the wind. Banging this drum could be effective if it leads to a culture change. We already see criticism of developers of software that has obvious vulnerabilities all the time on HN, so there's already some sense that developers shouldn't be extremely negligent/incompetent around security. You can't guarantee security 100% of course, but you can have a general awareness that it's wrong to make the stupid decisions that developers keep making and are generally known to be bad practice. reply pmontra 12 hours agorootparentDevelopers build insecure software in part because themselves and in part because of the decisions made by their managers up to the CEO. So when you write \"developers\" we must read \"software development companies\". reply gljiva 10 hours agorootparentprev> I agree with her about blaming developers, not hackers. They are clearly called \"villains\". Wind isn't a person capable of controlling their actions. There is no intention to do harm. They aren't senseless animals either. Yes, it's developers' fault if a product isn't secure enough, but it's also not wrong to put blame on those actively exploiting that. Let's not stop blaming those who do wrong --- and that kind of hackers is doing wrong, not just the developers \"making stupid decisions\". Those aren't mutually exclusive reply acdha 5 hours agorootparent> They are clearly called \"villains\". As readers of the article know, they are not: > The truth is: Technology vendors are the characters who are building problems\" into their products, which then \"open the doors for villains to attack their victims,\" She’s talking about companies, not individual developers, and she didn’t call them villains but rather creators of the problems actual villains exploit. The company focus is important: it’s always easy to find who committed a problematic line of code - say a kernel driver which doesn’t validate all 21 of its arguments properly - but the person who typed that in doesn’t work alone. The company sets their incentives, provides training (or not), and most importantly should be pairing the initial author of that code with reviewers, testers, and quality tools. When a company makes a $50 toaster, they don’t just ask the designer whether they think it’s safe, they actually test it in a variety of ways to get that UL certification, and we have far fewer fires than we had a hundred years ago. One key to understanding this is to remember CISA’s scope and mission. They’re looking at a world where every week has new ransomware attacks shutting down important businesses, even things like hospitals, industrial espionage is on the rise and the industry has largely tried to stay in the cheaper reactive mode of shipping patches after problems are discovered rather than reducing the rate of creating them. This is fundamentally not a technical issue but an economic one and she’s trying to change the incentive structure to get out of the cycle which really isn’t working. reply juunpp 17 hours agoparentprev3. Legal incentives. When somebody dies or fails to receive critical care at a hospital because a Windows XP machined got owned, somebody should probably face something along the lines of criminal negligence. reply rockskon 16 hours agorootparentWill Microsoft face liability if someone dies or fails to receive critical care because some infrastructure system auto-rebooted to apply an update and lost state/data relating to that patient's care? reply mkoubaa 16 hours agorootparentThat's the only thing that'll Microsoft chill with the reboots reply juunpp 1 hour agorootparentprevYes. reply pasc1878 5 hours agorootparentprevThe person who should be fined etc is the person who said put the machine on the internet. reply impossiblefork 10 hours agoparentprevYou can also choose to avoid complexity. Often a shorter computer program that is easy to understand can do exactly what a more complicated program can. We can simplify interfaces between systems and ensure that their specifications are short, readable and implementable without allocation, buffers or other things that can be implemented incorrectly. We can ensure that program code is stored separately from data. Now that LLMs are getting better we could probably have them go through all our code and introduce invariants, etc. to make sure it does exactly what it's supposed to, and if it can't then a human expert can intervene for the function for which the LLM can't find the proof. I think hardware manufacturers could help too. More isolation, Harvard architectures etc. would be quite appealing. reply pjmlp 3 hours agoparentprevMaking vendors understand that the time of EULAS waving responsibility is coming to pass, and like in any other grown up industry, liability is coming. reply croes 5 hours agoparentprevDo get laws to punish certain behavior that behavior must be considered as a bad thing in terms of being morally wrong. So it's a first step to liability. reply transpute 17 hours agoparentprev> economic incentives EU CRA incoming, https://ubuntu.com/blog/the-cyber-resilience-act-what-it-mea... How will the US version differ? reply lelanthran 7 hours agoparentprev> ways that software vendors will be held liable for bugs in their products. And if we do that then the state of software would grind to a halt, because there is no software that is completely bug-free.[1] Like it or not, the market has spoken on what it considers acceptable risk for general software. Software where human lives are at risk is already highly regulated, which is why so few human lives are lost to bugs, compared to lives lost to other engineering defects. [1] I feel we are already at a point where the market has, economically anyway, hit the point of diminishing returns for investment into reliability in software. reply nradov 1 hour agoparentprevIf customers want their software vendors to take liability for software defects (including security vulnerabilities) then they can just negotiate that into licensing agreements. We don't need the federal government to get involved with new laws or regulations. reply jiggawatts 12 hours agoparentprev> Making truly secure software products is incredibly hard in this day and age I politely disagree. Writing secure software is easier than ever. For example, there are several mainstream and full-featured web frameworks that use managed virtual machine runtimes. Node.js and ASP.NET come to mind, but there are many other examples. These are largely immune to memory safety attacks and the like that plague older languages. Most popular languages also have a wide variety of ORMs available that prevent SQL injection by default. Don't like heavyweight ORMs? No problem! There's like a dozen micro-ORMs like Dapper that do nothing to your SQL other than block injection vulnerabilities and eliminate the boilerplate. Similarly, web templating frameworks like Razor pages prevent script injection by default. Cloud app platforms, containerisation, or even just virtual machines make it trivial to provide hardware enforced isolation on a per-app basis instead of relying on the much weaker process isolation within an OS with shared app hosting. TLS 1.3 has essentially eliminated cryptographic vulnerabilities in network protocols. You no longer have to \"think\" about this concern in normal circumstances. What's even better is that back end protocols have also uniformly adopted TLS 1.3. Even Microsoft Windows has started using for the wire protocol of Microsoft SQL Server and for the SMB file sharing protocol! Most modern queues, databases, and the like use at least TLS 1.2 or the equivalent. It's now safe to have SQL[1] and SMB shares[2] exposed to the Internet. Try telling that to someone in sec-ops twenty years ago! Most modern PaaS platforms such as cloud-native databases have very fine-grained RBAC, built-in auditing, read-only modes, and other security features on by default. Developers are spoiled with features such as SAS tokens that can be used to trivially generate signed URLs with the absolute bare minimum access required. Speaking of PaaS platforms like Azure App Service, these have completely eliminated the OS management aspect of security. Developers never again need to think about operating system security updates or OS-level configuration. Just deploy your code and go. Etc... You have to be deliberately making bad choices to end up writing insecure software in 2025. Purposefully staring at the smörgåsbord of secure options and saying: \"I really don't care about security, I'm doing something else instead... just because.\" I know that might be controversial, but seriously, the writing has been on the wall for nearly a decade now for large swaths of the IT industry. If you're picking C or even C++ in 2025 for anything you're almost certainly making a mistake. Rust is available now even in the Linux kernel, and I hear the Windows kernel is not far behind. Don't like Rust? Use Go. Don't like Go? Use .NET 9. Seriously! It's open-source, supports AOT compilation, works on Linux just fine, and is within spitting distance of C++ for performance! [1] https://learn.microsoft.com/en-us/azure/azure-sql/database/n... [2] https://learn.microsoft.com/en-us/azure/storage/files/files-... reply mgh95 12 hours agorootparentIt's important to remember that even `npgsql` can have issues (see https://github.com/npgsql/npgsql/security/advisories/GHSA-x9...). In your world, would the developer of a piece of software exploted by a vulnerability such as this be liable? reply pjmlp 3 hours agorootparentThat is why other industries have bill of materials and supplier chain validation. Bill of materials is already a reality in high critical computing deployments. reply jiggawatts 11 hours agorootparentprevThe point is that secure software is easier to write, not that it's impossible to have security vulnerabilities. Your specific example is a good one: interacting with Postgres is one of those things I said people choose despite it being riddled with security issues due to its age and choice of implementation language. Postgres is written in C and uses a complicated and bespoke network protocol. This is the root cause of that vulnerability. If Postgres was a modern RDBMS platform, it would use something like gRPC and there wouldn't be any need to hand-craft the code to perform binary encoding of its packet format. The security issue stems from a choice to use a legacy protocol, which in turn stems from the use of an old system written in C. Collectively, we need to start saying \"no\" to this legacy. Meanwhile, I just saw a video clip of an auditorium full of Linux kernel developers berating the one guy trying to fix their security issues by switching to Rust saying that Rust will be a second class citizen for the foreseeable future. reply mgh95 11 hours agorootparent> Your specific example is a good one: interacting with Postgres is one of those things I said people choose despite it being riddled with security issues due to its age and choice of implementation language. Ah there is the issue: protocol level bugs are language independent; even memory safe languages have issues. One example in the .net sphere is f* which is used to verify programs. I recommend you look at what the concepts of protocol safety actually look like. > The security issue stems from a choice to use a legacy protocol, which in turn stems from the use of an old system written in C. This defect in particular occurs in the c# portion of the stack, not in postgres. This could have occurred in rust if similar programming practices were used. > If Postgres was a modern RDBMS platform, it would use something like gRPC and there wouldn't be any need to hand-craft the code to perform binary encoding of its packet format. There is no guarantee a borked client implementation would be defect free. This is a much harder problem than I think you think it is. Without resorting to a very different paradigm for programming (which, frankly, I don't think you have exposure to based upon your comments) I'm not sure it can be accomplished without rendering most commercial software non-viable. > Meanwhile, I just saw a video clip of an auditorium full of Linux kernel developers berating the one guy trying to fix their security issues by switching to Rust saying that Rust will be a second class citizen for the foreseeable future. Yeah, I mean start your own OS in rust from scratch. There is a very real issue that RIIR isn't always an improvement. Rewriting a linux implementation from scratch in rust if it's a \"must have right now\" fix is probably better. reply ptx 8 hours agorootparent> protocol level bugs are language independent; even memory safe languages have issues. [...] This defect in particular occurs in the c# portion of the stack, not in postgres. This could have occurred in rust if similar programming practices were used. But it couldn't have occurred in Python, for example, and Swift also (says Wikipedia) doesn't allow integer overflow by default. So it's possible for languages to solve this safety problem as well, and some languages are safer than others by default. C# apparently has a \"checked\" keyword [0] to enable overflow checking, which presumably would have prevented this as well. Java uses unsafe addition by default but, since version 8, has the \"addExact\" static method [1] which makes it inconvenient but at least possible to write safe code. [0] https://learn.microsoft.com/en-us/dotnet/csharp/language-ref... [1] https://docs.oracle.com/en/java/javase/19/docs/api/java.base... reply jiggawatts 9 hours agorootparentprevThe counter to any argument is the lived experience of anyone that developed Internet-facing apps in the 90s. Both PHP and ASP were riddled with security landmines. Developers had to be eternally vigilant, constantly making sure they were manually escaping HTML and JS safely. This is long before automatic and robust escaping such as provided by IHtmlString or modern JSON serializers. Speaking of serialisation: I wrote several, by hand, because I had to. Believe me, XML was a welcome breath of fresh air because I no longer had to figure out security-critical quoting and escaping rules by trial and error. I started in an era where there were export-grade cipher suites known to be compromised by the NSA and likely others. I worked with SAML 1.0 which is one of the worst security protocols invented by man, outdone only by SAML 2.0. I was - again — forced to implement both, manually, because “those were the times”. We are now spoiled for choice and choose poorly despite that. reply AStonesThrow 12 hours agorootparentprevThis is a strange and myopic understanding of \"application security\". You seem quite focused on vulnerabilities that could threaten underlying platforms or connected databases, but you're ignoring things like (off the top of my head) authentication, access control, SaaS integrations, supply chains, and user/admin configuration. Sure, write secure software where nobody signs in or changes a setting, or connects to Google Drive, and you have no dependencies... Truly mythical stuff in 2024. reply jiggawatts 11 hours agorootparentI wanted to avoid writing war & peace, but to be fair, some gaps remain at the highest levels of abstraction. For example, SPA apps are a security pit of doom because developers can easily forget that the code they're writing will be running on untrusted endpoints. Similarly, high-level integration with third-party APIs via protocols such as OIDC have a lot of sharp edges and haven't yet settled on the equivalent of TLS 1.3 where the only possible configurations are all secure. reply AStonesThrow 11 hours agorootparentStill too narrow. Even I assumed \"application security\" when this isn't the point of the comments. We're talking about the gamut, from the very infrastructure AWS or VMWare is writing, mainframe OS, Kubernetes, to embedded and constrained systems like IoT, routers, security appliances, switches, medical devices, automobiles. You don't just tell all those devs to throw them on Rust and a VM, whether it's 2024 or December 31, 1999. reply boomboomsubban 18 hours agoprevThat is rich coming from a former NSA Tailored Access Operations agent. She had no problems paying companies to release insecure software, including some that have signed the \"secure by design\" pledge. reply davisr 18 hours agoparentThat is important context, but I still agree with what she's said in this article. It's also rich that Cisco especially -- a company known for hard-coding backdoors into their products for decades -- is \"taking a pledge\" to do better. reply boomboomsubban 11 hours agorootparentI agree that software should often get more tests to improve security. I don't think supporting companies that sign a meaningless pledge improves anything and I question her motives in trying to shame people who use companies that have not signed this pledge. reply keepamovin 13 hours agoparentprevI see it as the opposite: as ex Deputy Head of TAO Easterly is no retard. And there's a difference between defective software that leads to vulns exploited by crime gangs and NOBUS backdoors that the good guys use to keep you safe. Sounds bullshit, right? That's how far the public discourse on cyber has diverged from the reality, which is part of the issue. Easterly's push for renaming cyber actors and flaws is smart. Bad quality comes from mindset, attitude. And names are important, as programmers should know! :) I would prefer it if she had a GitHub profile tho. Always cool if you do that. reply rockskon 16 hours agoparentprevSo I'm aware she worked for the NSA but this is the first I'm hearing of her working for TAO. I had thought she worked at the NSA's IAD (defensive side) pre-merge of the offensive and defensive sides. reply croes 5 hours agoparentprevI think NSA and CISA have different objectives. reply tonetegeatinst 11 hours agoprevI think sometimes cyber is still seen as an unnecessary cost. Plenty of places do bare minimum for security, and most of the time its after an incident that budgets suddenly get raised. Software, hardware, policy, and employee training are all things one must focus on. You can't just start making rdx or fireworks without the proper paperwork, permits, licenses, fees, and a lawyer around to navigate everything. You run a business without investing anything into IT and cybersecurity, you just make it easier for an incident to occur. And remember, just because your product isn't IT or cyber security dosnt mean its losing money, it a cost of doing buissness in our regulated market. You mishandle HIPPA, PII or sensitive info, and the customers realize you didn't take basic steps to stop this, you open yourself to a lawsuit. Think about it like this, investing in it every day means your lowering that risk, however much you think is reasonable to pay for, and every day its paying for itself. reply transpute 17 hours agoprevDan Geer on prioritizing MTRR over MTBF (2022): Metrics as Policy Driver: Do we steer by Mean Time Between Failure (MTBF) or Mean Time To Repair (MTTR) in Cybersecurity? Choosing Mean Time Between Failure (MTBF) as the core driver of cybersecurity assumes that vulnerabilities are sparse, not dense. If they are sparse, then the treasure spent finding them is well-spent so long as we are not deploying new vulnerabilities faster than we are eliminating old ones. If they are dense, then any treasure spent finding them is more than wasted; it is disinformation. Suppose we cannot answer whether vulnerabilities are sparse or dense. In that case, a Mean Time To Repair (MTTR) of zero (instant recovery) is more consistent with planning for maximal damage scenarios. The lesson under these circumstances is that the paramount security engineering design goal becomes no silent failure – not no failure but no silent failure – one cannot mitigate what one does not recognize is happening. reply gavinhoward 13 hours agoprevI don't quite agree, but I do somewhat agree. We need to professionalize and actually accept liability for our work. [1] [1]: https://gavinhoward.com/2024/06/a-plan-for-professionalism/ reply z3phyr 8 hours agoprevUsing the same logic, one can argue \"SOFTWARE IS PROVIDED AS IS\". It should be up-to the user to choose the correct software based on their security policy. I write software for fun and skillz, making computers do extraordinary things. If I start following regulation, then there is no fun for me and no software that does extraordinary things. No ma'am Doom or Second Reality would not have been possible with this attitude. reply croes 5 hours agoparentThe users choice affects third parties, so it's not that simple. I bet you won't recommend to install your software on essential systems. >No ma'am Doom or Second Reality would not have been possible with this attitude. Same is true for many kinds of malware. reply z3phyr 3 hours agorootparent> Same is true for many kinds of malware. Would you prefer to not have Doom at all? (Over only some malware not existing) reply hermannj314 5 hours agoprevI strongly disagree. If someone puts cyanide in the coffee pot, we don't blame the engineer that designed the coffee pot for not making it cyanide proof. Criminals are the criminals, not a developer that didn't code defensively enough. The fact that a government official is blaming developers for crimes they don't commit is fascist level rhetoric. reply croes 5 hours agoparentWe did blame KIA for their shitty car locks. And your example is a bad one, because you would blame the engineer if sells it as cyanide proof. Software vendors claim their software is safe that only high sophisticated criminals could break their software. In reality it's often just script kiddies. reply DangitBobby 2 hours agorootparentIn the comment you responded to, they made no mention of the engineer expecting or claiming it is cyanide proof, so your response makes no sense. Is the person in the article saying only people who claim that software is free of defects should be held liable? I would never make a claim that software I've written is 100% secure, and I would stop writing software if I were held criminally liable for defects. reply userbinator 17 hours agoprevWhy does software require so many urgent patches? Conspiracy theory: creating new bugs they can always fix later is a good source of continued employment. Of course there's also the counterargument that insecurity is freedom: if it weren't for some insecurity, the population would be digitally enslaved even more by companies who prioritise their own interests. Stallman's infamous \"Right to Read\" is a good reminder of that dystopia. This also ties in with right-to-repair. The optimum amount of cybercrime is nonzero. reply tonetegeatinst 11 hours agoparentOne thing is software development is not really focused on secure software. If Microsoft ca n manage to call recall a engineered product feature where those M$ folks get payed stacks to work on, can't even both securing the data in rest, and then decide its a great idea to use cloud computing for recall, then I can totally see \"security software engineer\" becoming a separate field. Securing software seems to be hard especially in web development. You got to worry about regular development, then all these crazy different exploit methods like xss,SQL injection, data sanitation, etc....and then you got to get this site working for multiple browsers, you need to jugle all of this. And if an api or some 3rd party tool gets compromised how do you prevent that except a crystal ball a bottle of burbon and a clairvoyant chant? Also their was iirc people submitting bogus CVE reports to increase the mental drain on people and overwhelm the human link which is always the weakest. reply wredue 15 hours agoparentprevIt isn't malice dude. Unfortunately, it really is just that 70% of developers are utterly incompetent. reply tsimionescu 12 hours agorootparentIf having security vulnerabilities in code you wrote or reviewed is a sign of incompetence, then there has probably never been a competent developer in the history of the industry. reply wredue 3 hours agorootparentI wouldn’t say that. There are some not obvious things like timing attacks that you probably shouldn’t feel bad about. If you’re still writing sql injections though, yeah, you’re terrible. reply LinuxBender 10 hours agoparentprevConspiracy theory: creating new bugs they can always fix later is a good source of continued employment. That is absolutely a thing [1]. There are hardware devices that can be fixed illegally fixed using 3rd party software for this reason. The people making a work around to the scam then get sued. The video is worth a watch in my opinion. It was created 3 years ago and the problem is still ongoing. [1] - https://www.youtube.com/watch?v=SrDEtSlqJC4 [video][29min] reply dhx 15 hours agoprevWhere does CISA/NIST recommend (for software developers) or require (for government agencies integrating software) specific software/operating system hardening controls? * Where do they require software developers to provide and enforce seccomp-bfp rules to ensure software is sandboxed from making syscalls it doesn't need to? For example, where is the standard that says software should be restricted from using the 'ptrace' syscall on Linux if the software is not in the category of [debugging tool, reverse engineering tool, ...]? * Where do they require government agencies using Kubernetes to use a \"restricted\" pod security standard? Or what configuration do they require or recommend for systemd units to sandbox services? Better yet, how much government funding is spent on sharing improved application hardening configuration upstream to open source projects that the government then relies upon (either directly or indirectly via their SaaS/PaaS suppliers)? * Where do they provide a recommended Kconfig for compiling a Linux kernel with recommended hardening configuration applied? * Where do they require reproducible software builds and what distributed ledger (or even central database) do they point people to for cryptographic checksums from multiple independent parties confirming they all reproduced the build exactly? * Where do they require source code repositories being built to have 100% inspectable, explainable and reproducible data? As xz-utils showed, how would a software developer need to show that test images, test archives, magic constants and other binary data in a source code repository came to be and are not hiding something nefarious up the sleeve. * Where do they require proprietary software suppliers to have source code repositories kept in escrow with another company/organisation which can reproduce software builds, making supply chain hacks harder to accomplish? * ... (similar for SaaS, PaaS, proprietary software, Android, iOS, Windows, etc) All that the Application Security and Development STIG Ver 6 Rel 1[1] and NIST SP 800-53 Rev 5[2] offer up is vague statements of \"Application hardening should be considered\" which results in approximately nothing being done. [1] https://dl.dod.cyber.mil/wp-content/uploads/stigs/zip/U_ASD_... [2] https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.S... reply tptacek 14 hours agoparentI don't know how meaningful those countermeasures really are. Like, you're basically looking at the space of Linux kernel LPEs, and you're bringing it down to maybe 1/3rd the \"native\" rate of vulnerabilities --- but how does that change your planning and the promises you can make to customers? reply jart 14 hours agoparentprevGovernment agencies have got their hands tied just getting people to use MFA and not click on phishing emails. You're decades ahead of the herd if you're thinking about coding in SECCOMP-BPF. reply dhx 11 hours agorootparentSoftware sandboxing has a relatively good cost-to-benefit ratio at reducing the consequences of software bugs, which is why it's already implemented in a lot of software we all use every day. For example, it exists in Android apps, iOS apps, Flatpak apps (Linux), Firefox[1][2], Chromium browsers[3][4][5], SELinux-enabled distributions such as Fedora and Hardened Gentoo[6], OpenSSH (Linux)[7], postfix's multi-process architecture with use of ACLs, etc. Kubernetes, Docker and systemd folk will be familiar with the idea of sandboxing for containers too, and they're able to do so using much higher level controls, e.g. turn on Kubernetes \"Restricted pod security standard\" for much stricter sandboxing defaults. Even if containerisation and daemon sandboxing aren't used, architects will understand the concept of sandboxing by just specifying more servers, each one ideally performing a separate job with the number of external interfaces minimised as much as possible. In all of these situations, the use of more granular controls such as detailed seccomp-bpf filters is most useful to mitigate the risks introduced by (ironically) security agent software that is typically installed alongside a database server daemon, web server daemon, etc within the same container. Tweaking some Kubernetes, Docker or systemd config is _much_ cheaper and quicker to implement rather than waiting to rewrite software in a safer language such as Rust (a noble end goal). Even if software were rewritten in Rust, you'd _still_ want to implement some form of external sandboxing (e.g. systemd-nspawn applying seccomp-bpf filters based on some basic systemd service configuration) to mitigate supply chain attacks against Rust software which cause the software to perform functions it shouldn't be doing. [1] Firefox Linux: https://searchfox.org/mozilla-central/source/security/sandbo... [2] Firefox Windows: https://searchfox.org/mozilla-central/source/security/sandbo... [3] Chromium multi-platform: https://github.com/chromium/chromium/blob/main/docs/security... [4] Chromium Linux: https://chromium.googlesource.com/chromium/src/+/0e94f26e8/d... (seemingly Linux sandboxing is experiencing significant changes as this document or one similar to it does not appear to exist anymore) [5] Chromium Windows: https://chromium.googlesource.com/chromium/src/+/HEAD/docs/d... [6] https://gitweb.gentoo.org/proj/hardened-refpolicy.git/tree/p... [7] https://github.com/openssh/openssh-portable/blob/master/sand... reply jdougan 16 hours agoprevI dunno. \"Evil Ferret\" and \"Scrawny Nuisance\" sound pretty good in our irony filled world. reply swiftcoder 9 hours agoprevWas about half way through before I realised that the article was not, in fact, satirical. Half-expected to see harddrive in the URL. reply forgetfreeman 13 hours agoprevPretty sure I'm going to burn some karma on this one but what the hell. To the best of my knowledge there is no evidence in over four decades of commercial software development that supports the assertion that software can be truly secure. So to my mind this suggests the primary villains are the individuals and organizations that have pushed software into increasingly sensitive areas of our lives and vital institutions. reply gavinhoward 13 hours agoparentAs someone who thinks the industry should professionalize, I actually agree with you somewhat; we pushed too far, too fast, and into territory that software has no business being in. reply irundebian 10 hours agoparentprevWhat's the definition of truly secure? reply forgetfreeman 9 hours agorootparentThe world economy is hemorrhaging over a trillion dollars USD annually to cybercrime, so however you choose to define it it clearly doesn't exist. reply sublinear 17 hours agoprevWhat a joke. This role deserves way better, but I understand it's only been around since 2018. reply acdha 17 hours agoparentThis seems like a very pat dismissal of what upon reading further seems like a very reasonable critique of the industry. Vendors have been seriously exploiting their ability to decline responsibility for their product development decisions and that often has significant negatives for affected users. reply consumer451 16 hours agoprevI used to be an IT guy at a structural and civil engineering firm. Those were real professional engineers with stamps and liability. As long as \"SWEs\" do not have stamps and legal liability, they are not real (professional) engineers, IMHO. My point is that I believe to earn the title of \"Software Engineer,\" you should have a stamp and legal liability. We done effed up. This breach of standards might be the great filter. edit: Thanks to the conversation down-thread, the possibly obvious solution is a Software Professional Engineer, with a stamp. This means full-stack is actually full effing stack, not any bullshit. This means that ~1% to ~5% of SWE would be SWPE, as it is in other engineering domains. A SWPE would need to sign off on anything actually important. What is important? Well we figured that out in other engineering domains. It's time for software to catch the f up. reply bruce511 15 hours agoparentOk, so overnight all programmers stop calling themselves Engineers. [1] What problem does that solve? I fix bugs all day, but I don't call myself a Software Doctor. Frankly whether software people call themselves engineers or not matters to pretty much no-one (except actual engineers who have stamps and liabilities.) Creating a bunch of requirements and liability won't suddenly result in more programmers getting certified and taking on liability. It'll just mean they stop using that title. I'm not sure that achieves anything useful. We'd still have the exact same software coming from the exact same people. [1] for the record I think 'software engineer' is a daft title anyway, and I don't use it. I don't have an engineering degree. On the other hand I have a science degree and I don't go around calling myself a data scientist either. reply consumer451 15 hours agorootparentThat's fine, it just means that devs without stamps can't sign off on anything actually important. In real engineering, there is a difference between an Engineer and a Professional Engineer. The latter has a stamp. I realize that this is the nearly the opposite of our current environment. It is a lot more regulation, a lot more professional standards... but it worked for civil and structural, and those standards were written in blood. Maybe what I am asking for is a PE for SWE, those people have stamps, and it would be really hard to get a SW PE. Anything deemed critical (like security), by regulation, would require a SW PE stamp. [0] Software did in-fact eat the world. Why shouldn't it have any legal/professional liability like civil and structural engineering? [0] In this case, full stack, actually means full freaking stack = SWPE reply bruce511 15 hours agorootparent>> That's fine, it just means that devs without stamps can't sign off on anything actually important For some definition of important. But let's follow your thought through. Who decides what is important? You? Me? The developer? Yhe end-user? Some regulatory body? Is Tetris important? OpenSSL? Notepad ++? My side project on github? If my OSS project becomes important, and I now need to find one of your expensive engineers to sign off on it, take liability for it, do you think I can afford her? How could they be remotely sure that the code is OK? How would they begin to determine if its safe or not? >> Software did in-fact eat the world. Why shouldn't it have any legal/professional liability like civil and structural engineering? Because those professions have shown us why that model doesn't scale. How many bridges, dams etc are built by engineers every year? How does that compare to the millions of software projects started every year? In the last 30 years we've pretty much written all the code, on all the platforms, in use today. Linux, Windows, the web, phones, it's all less than 35 years old. What civil engineering projects have been completed in the same time scale? A handful of new skyscrapers? You are basically suggesting we throw away all software ever written and rebuild the world based on individual's prepared to take on responsibility and legal liability for any bugs they create along the way? I would suggest that not only would this be impossible, not only would it be meaningless, but it would take centuries to get to where we are right now. With just as many bugs as there are now. But, yay, we can bankrupt an engineer every time a bug is exploited. reply consumer451 15 hours agorootparentThis has all been done before in mechanical, structural, and civil engineering. People die and then regulatory and industry standards fix the problems. We do not need to re-invent the concepts of train engine, bridge, and dam standards again. I mean, I guess we actually do. The issue is that software has not yet killed enough people for those lessons to be learned. We are now at that cliff's edge [0], [1]. Another problem might be that software influence is on a far more hockey-stick-ish growth curve than what we dealt with in mechanical, civil, and structural engineering. Meanwhile, our tolerance for professional and governmental standards seems to be diminishing. [0] https://news.ycombinator.com/item?id=39918245 [1] https://news.ycombinator.com/item?id=24513820 ... https://hn.algolia.com/?q=hospital+ransomware reply tsimionescu 12 hours agorootparentNo, the world's infrastructure has never been rebuilt from scratch to higher standards, not in the last few thousand years. We have always built on what already exists, grandfathered in anything that seemed ok, or was important enough even if not ok, etc. We often live in buildings that far predate any building code, or even the state that emitted that code. We still use bits of infrastructure here and there that are much older than any modern state at all (though, to be fair, if a bridge has been around for the last thousand years, the risk it goes down tomorrow because it doesn't respect certain best practices is not exactly huge). reply dhx 14 hours agorootparentprevThere have long been multiple forms of professional software engineering in aerospace, rail, medical instrumentation and national security industries such as ISO 26262, DO-178B/DO-178C/ED-12C, IEC-61508, EN-50128, FDA-1997-D-0029 and CC EAL/PP. DO-178B DAL A (software whose failure would result in a plane crashing) was estimated at [1] to be writable at 3 SLOC/day for a newbie and 12 SLOC/day for a professional software engineer with experience writing code to this standard. Writing software to DO-178B standards was estimated in [1] to double project costs. DO-178C (newer standard from 2012) is much more onerous and costly. I pick DO-178 deliberately because the level of engineering effort required in security terms is probably closest to that applied to seL4, which is stated to have cost ~USD$500/SLOC (adjusted for inflation to 2024).[2] This is a level higher than CC EAL7 as CC EAL7 only requires formal verification of design, not the actual implementation.[3] DO-178C goes as far as requiring every software tool used to verify software automatically has been formally verified otherwise one must rely upon manual (human) verification. Naturally, formally verified systems such as flight computer software and seL4 are deliberately quite small. Scaling of costs to much larger software projects would most likely be prohibitive as complexity of a code base and fault tree (all possible ways the software could fail) would obviously not scale in a friendly way. [1] https://web.archive.org/web/20131030061433/http://www.euroco... [2] https://en.wikipedia.org/wiki/L4_microkernel_family#High_ass... [3] https://cgi.cse.unsw.edu.au/~cs9242/21/lectures/10a-sel4.pdf reply consumer451 14 hours agorootparentWith much humility, may I ask, have you been exposed to the world of PEs with stamps and liability? Do you see the need for anything like this in the software world, in the future? reply dhx 13 hours agorootparentProfessional engineers have been stamping and signing off on safety-critical software for decades, particularly in aviation, space, rail and medical instrumentation sectors. Whilst less likely to be regulated under a \"professional association\" scheme, there has also been two decades of similar stamping of security-critical software under the Common Criteria EAL scheme. The question is whether formal software engineering practices (and associated costs) expand to other sectors in the future. I think yes, but at a very slow pace mainly due to high costs. CrowdStrike's buggy driver bricking Windows computers around the world is estimated to have caused worldwide damages of some USB$10bn+.[1] There will be cheaper ways seen to limit a buggy driver bricking Windows computers in the future other than requiring every Windows driver be built to seL4-like (~USD$500/SLOC) standards. If formal software engineering practices are implemented more as years go by, it'll be the simplest/easiest software touched first, with the highest consequences of failure, such as Internet nameservers. [1] https://en.wikipedia.org/wiki/2024_CrowdStrike_incident reply olliej 13 hours agorootparentprevAs a header, there's clearly a liability problem in modern software, which I'll get to later. [pre-posting comment: I've moved the semi-rant portion to the bottom, because I realized I should start with the more direct issues first, lest the ranty-ness cause you to not read the less ranty portion :D ]Now getting to the point: there is a real problem in that companies can advertise products to do a certain thing, and they can then have a license agreement that says \"we're not liable if it fails to do what we said it would do\", but generally despite those licenses (which to be clear are a requirement for open source to exist as a concept), the law has found companies are liable for unreasonable losses. So the question is just how liable should a company be for a bug in their software (or hardware I guess depending on where you place the hardware vs firmware vs software lines) that can be exploited, and your position is that in addition to liability bought about by their own actions (Again despite the \"we have no liability\" EULAs plenty of companies have ended up with penalties for bugs in their software causing a variety of different awful outcomes. But you're going a step further, you're now saying, in addition to liability for your errors, you're also liable for other people causing failures due to those errors, either accidentally or intentionally. I am curious, and I would be interested in the responses from your Real Engineer coworkers. Who is responsible if a bridge collapses when a ship crashes into it? An engineer can reasonably predict that that would happen, and should design to defend against it. Let's say an engineer designs a building, and a person is able to bomb the building and cause it to collapse with only a small amount of fertilizer? What happens to the liability if the only reason the plot succeeded was because they were able to break past a security barrier? Because here is the thing: we are not talking about liability if a product/project/construction fails to do what it says it will do (despite EULAs, companies generally lose in court when their product causes harm even if there's a license precluding liability). The question is who is liable if a product fails to stand up to a malicious actor. At its heart, the problem we're discussing is not about liability for \"the engine failed in normal use\", it's \"the engine failed after someone poured sugar into the gas tank\", not \"the wall collapsed in the wind\" it's \"the wall collapsed after someone intentionally drove their truck into it\", not \"the plane crashed when landing due to the tires bursting\", it's \"the plane crashed when landing as someone had slashed the tires\". What you're saying, is that a Professional Engineer signing off on a design is saying not only \"this product will work as intended\", they're saying \"this product will even under active attack outside of its design window\". That's an argument that seems to go either way: I don't recall ever hearing about a lawsuit against door manufacturers due to burglars being able to break through the doors or locks, but on the other hand Kia is being sued due to the triviality of stealing their cars - and even then the liability claims seem to be due to the cost of handling the increased theft, not the damage from any individual theft. [begin ranty portion: this is all I think fairly reasonable, but it's much more opinionated than the now initial point and I'm loathe to just discard it] I'm curious what/who you think is signing off and what they are signing off on. * First off, software is more complex than more or less any physical product, the only solution is to reduce that complexity down to something manageable to the same extent that, say, a car is. How many parts total are in your car? Cool that's how many expressions or statements your program can have. And because it's not governed by direct physical laws and similar interactions, then that's still more complex than a car. * Second: no more open source code in commercial products - you can't use it in a product, because doing so requires sign off by your magical software engineers who can understand products more complex, again, than any single physical product * Third: no more free development in what open source remains - signing off on a review now makes you legally liable for it. You might say that's great, I say that means no one is going to maintain anything for zero compensation and infinite liability. * Fourth: no more learn development through open source contributions, as a variation of the above now every newbie that submits a change brings liability, so you're not accepting any changes from anyone you don't know, and who you don't have reason to believe is competent. * Fifth: OSS licenses are out - they all explicitly state that there's no warrantee or fitness for purpose, but you've just said the engineer that signs off on them is liable for it, which necessarily means that your desire for liability trumps the license. * Sixth: Free development tools are out - miscompilation is now a legal liability issue, so now a GCC bug opens whoever signed off on that code to liability. The world you're describing is one in which the model of all software dev including free dev, now comes with liability that matches designing cars, or constructing buildings, both of which are much less complex and much more predictable than even the modest OSS projects, and those fields all come with significant cost based barriers to entry. The reason there are more software developers than there are civil or mechanical engineers is not because one is easier than the other, it's because you cannot learn civil or mechanical engineering (or most other engineering disciplines) as cheaply as software. The reason that software generally pays more than those positions is because the employer is taking on a bunch of the financial, legal, and insurance expenses required to do anything - the capital expenditure required to start a new civil or mechanical engineering companies is vastly higher than for software, and the basic overhead for just existing is higher, which means employers don't have to compete with employees deciding to start their own companies. A bunch of this is straight up capital costs, and capital, but the bulk of it is being able to have sufficient liability protection before even the smallest project is started. At that point, the company has insurance to cover the costs so the owners are generally fine, but the engineer that missed something - not the people forcing time crunches, short cuts, etc - is the one that will end up in jail. You've just said the same should apply to software: essentially the same as today company screws up and pays fine/settlement, but now with lowered pay rates for the developers, and they can go to jail. All because you have decided to apply a liability model that is appropriate to an industry where the things that are signed off on have entirely self contained and mostly static behavior to a different industry where _by design_ the state changes constantly, so there is not, and cannot be, any equivalent \"safety\" model or system. Even in the industries that you're talking about, where analysis is overwhelmingly simpler and predictable, products and construction fails. Yet now you're saying the software development could just be the same as that. When developing a building, you can be paranoid in your design, and make it more expensive by overbuilding - civil engineering price competition is basically just a matter of \"how much can I strip down the materials of construction\" without it collapsing (noting that you can exactly model the entire behavior of anything in your design). Again, the software your new standards require are the same as that required by the space and airline industry that people routinely already berate for being \"over priced\". You've made a point that there are engineers, and professional engineers, and the latter are the only ones who sign off on things. So it sounds like you're saying patches can only be reviewed by specific employees, who taken on liability for those changes, so now an OSS project has to employ a Professional Engineer to review contributions, who becomes liable for any errors they miss (out of curiosity, if a bug requires two different patches working together to create a vulnerability, which reviewer is now legally liable?). Those professional engineers now have to sign off on all software, so who is signing off on linux? You want to decode images, hopefully you can find someone to sign off on that. Actually it's probably best to have a smaller in-house product, or a commercial product from another company who have had their own professional engineers sign off, and who have sufficient insurance. Remember that you need to remind your employees not to contribute to any OSS projects, and don't release anything as OSS, because you would be liable if it goes wrong in a different product that you don't make money from now (remember your own Professional Engineers have signed off on the safety of the code you released, if they were wrong, you're now liable if someone downstream relied on that sign off). This misses a few core details: Physical engineering is *not* software engineering (which yes as a title \"software engineer\" is not accurate in many/most cases as it does imply a degree of rigour absent in most software projects). Physical engineering does not employ the same degree of reuse and intermingling as occurs in software - the closest I can really think of is engine swaps in cars, but that's only really doable because the engine is essentially the most complex part of the car anyway (at least in an ICE), and even then the interaction with the rest of the car is extremely constrained, predictable, and can be physically minimized. For civil/structural engineering it's even more extreme: large construction (e.g. the complex cases) are not simply made by mashing together arbitrary parts of other projects - buildings fall into basically two categories: the exact same building with different dimensions and a different paint job, or entirely custom. Physical engineering has basically an entirely predictable state from which to work. The overwhelming majority of any physical design is completely static, the things that are dynamic have a functionally finite and directly modelable set of states and behaviors, and those states and behaviors vary in predictable ways in response to constrained options. Despite this, many (if not most, though quantifying this would be hard because there's also vastly more static engineering than dynamic) failures are in the dynamic parts of these projects than the static portions. Software is definitionally free of more or less any static behavior, the entire reason for software is the desire for constantly changing state. A lot of failures in physical engineering are the result of failing to correctly model dynamic behavior, and again, software is entirely the part that Real Engineers have a tendency to model incorrectly. reply tonetegeatinst 11 hours agoparentprevI think in certain ways you are right we need those standards and stamps. But also colleges need to be very clear and cease calling it software engineering as it implies the expertise of a engineer. Computer engineering degrees can sorta get away with this, as they have quite a bit of engineering classes they have to take, but I'm not sure if they can call themselves engineers legally. Its hard because I believe certain titles are protected like doctor,lawyer,engineer, but last I check it varied by state in the USA at least. Sidenote: sometimes people get a computer science degree, or they he a cyber security degree, or even cyber warefare degree. All three seem to be used interchangeably in the security field. On one hand I get how formal protected titles help uphold standards and create trust, but it also enforces the grip colleges have and it creates more barriers to entering a field. Some States require 3 years experience to get a state permit for certain activities, when a federal permit is already needed so in that case its just more paperwork. Imagine if we started demanding people who build houses, not the guy who designed it but the builders to be an engineer, so we can trust their work and ability to do a job in the correct manner? At what point do we sacrifice and pass legislation in the name of reliability and safety? reply petre 13 hours agoparentprev> As long as \"SWEs\" do not have stamps and legal liability, they are not real (professional) engineers When and if that happens I’ll move to carpentry. Good luck. Tech is already full of *it. The only thing short of making it even worse is stamps and a mafia-like org issuing the stamps and asking for contributions in return, like it happens in the fields of medical care, law, book keeping, architecture or civil engineering. The companies should certify the products which require certification instead and get liability insurance. reply Animats 17 hours agoprevA previous head of cyber security was fired when he said something like that. reply Log_out_ 4 hours agoprevSo in a hypothetical world were the paranoid reign supreme and all software ia safe and unuseable cause usage is not a protection goal, do they declare a revolution in the name of useability and economic speed to overthrow the evil protectors? reply waihtis 14 hours agoprevI hope she starts the crackdown with easily the biggest impact offender here, Microsoft reply notepad0x90 17 hours agoprev\"Technology vendors are the characters who are building problems...\" there are vendors building problems into their products? isn't that a crime? What a silly take. \"House developers that build weak doors into houses are the real problem, not the burglars\" Security is an age-old problem, it is not a new concept. What is different with information security is the complexities and power dynamics changed drastically. I mean, really! She should know better, the #1 attack vector for initial access is still phishing or social-engineering of some kind. Not a specific vulnerability in some software. reply __turbobrew__ 4 hours agoparentPhysical analogs do not apply to computer security because the burglar usually does not live in a place where the judicial system can punish them. How are you going to bring some random hackers from Russia, China, India, etc to justice if those countries will not extradite them for their crimes? When the judiciary is no longer effective the only option is to design secure systems or balkanize the internet so only those who can be punished under the judiciary can access the computer system (which is nearly impossible with proxies). reply EnigmaFlare 14 hours agoparentprevSearch MasterLock on Youtube and you'll see that people blame the lock maker rather than the burglars. That's because the lock maker is notorious for making insecure locks. Social engineering is also something that can be protected against by developers, at least to some extent. Yubikey type 2FA is more resistant to that than user-input TOTP codes, for example. Nothing's bulletproof but it could certainly be improved in many cases. Wasn't some company that experienced a credential stuffing attack recently sued for not requiring 2FA for its users? We don't tolerate house builders who says \"well there's always some way water might get into the framing, so we don't need to bother installing flashings properly - a bit of caulking will do instead.\" reply notepad0x90 12 hours agorootparentEven a yubikey isn't resistant to cookie-theft, plenty of TOTP code theft phishing kits exist. Users literally enable third-party apk side-loading and install malicious apks on their android phones because of social engineering. It's not new to security. If you wear a high-vis vest and carry a clipboard, you'd be let into even government buildings. that also is social engineering. For your house builders statement, that is not a fair analogy. Is there evidence of a trend where software devs are saying \"well there's always some other way of getting hacked so let's not bother doing things properly\"? You masterlock analogy is on-point though, because of \"threat model\", the purpose of most doors and locks in the US for residential use is as a lightweight deterrent. Burglars can just break the glass window and walk by the no-fence perimeter. You can and probably should get a more secure lock, but it is as strong as the door frame and windows, and whatever alarm system you're using. In other words, for that analogy (and for what the CISA boss is saying) to be valid, there needs to be evidence that burglars give up and go home when the lock is secure. I would even go further and ask for a proper root cause analysis. Do the builders of masterlock know how insecure their lock is? If they are indeed making it weak because of cost, then are they really to blame? they're a business after all. Where is the regulation for proper secure locks. As a government agency, CISA shouldn't be blaming vendors, that's a cheap cop-out. They should be lobbying for regulation and laws, and then enforcing them. So, in the end, even if the CISA boss is right, ultimately she shouldn't be blaming vendors but explaining what she's been doing to pass regulations. reply acdha 15 hours agoparentprevHint: she didn’t say that they were building them intentionally. Skimping is building a problem even if nobody had a Jira ticket saying they had to leave out bounds checking. reply notepad0x90 12 hours agorootparentMost vulnerabilities are not there because someone was deliberately negligent either. I see no evidence of a trend where vendors are \"skimping\" on anything. reply acdha 6 hours agorootparentYou don’t think the vendors who are years behind on dependency updates are skimping? Not the ones who are still struggling to ship patches on a better than quarterly cadence? Not the ones who still ship 90s-style C code to enterprise customers who are paying high prices for their security products? Not the ones still writing new code in memory unsafe languages? Not the ones who still tell customers to disable SELinux? Not the ones who still refuse to use the sandboxing features in modern operating systems? Companies love the idea that you can’t hold the liable for any defect they didn’t intentionally build in, but software is the extreme outlier where they were able to avoid consumer safety regulations and thus the expense of hiring people who can even tell when something is risky. Shift the cost back to the supplier would restore the market feedback mechanism which is currently missing, greatly improving the health of the industry. reply rawgabbit 17 hours agoprev>\"We don't have a cyber security problem – we have a software quality problem. We don't need more security products – we need more secure products.\" Uhmmm. The foundation of a lot of the modern economy is built on Windows and the Crowdstrike fiasco has shown, Windows requires a security software to save it from itself by running at the kernel level. If we truly want secure products, we should shutdown all Windows machines? reply lmz 17 hours agoparentCrowdstrike also provided software for Linux systems. It's something you install to satisfy auditors and they would demand the same of any OS unless such functionality was built-in. reply bruce511 15 hours agoparentprevBut wait... most Windows runs on Intel, we should shutdown Intel. Of course shutting down Windows will have zero effect on security. There are plenty of exploits for Linux, and most data leaks are hosted on Linux (but have nothing to do with the OS.) To the degree to which software is a failure at the coding level (like every SQL injection, phishing, social engineering , sim swap, php, attack ever) the OS is irrelevant.) In truth most all security has to do with people; programmers and users. The OS might be a nice scapegoat but it's not the root of the problem. Blaming it though helps to deflect attention away from ourselves though. reply idle_zealot 16 hours agoparentprevRunning the world's critical infrastructure on verified, small, readable codebases rather than a diaspora of unvetted closed-source programs sprinkled across Windows and Linux systems sounds like a good start. reply Aerroon 16 hours agorootparentIt wont matter unless the code is written specifically for the hardware and will only run on that hardware. reply jdougan 16 hours agorootparentprevWe would need to dump POSIX/WinXX and replace/upgrade them with something better, probably using an object-capabilities approach. WASI, Capsicum. etc. reply davisr 17 hours agoparentprev> If we truly want secure products, we should shutdown all Windows machines? There should be a period where you put a question mark. reply juunpp 17 hours agoparentprevPrecisely. reply johndhi 15 hours agoprev [–] What she is asking for is a radical economic restructuring of a free market. It's attitudes like hers that lead to the federal government having the worst software imaginable. It just doesn't work unless everyone agrees to do it .. so good luck reply acdha 15 hours agoparent [–] Liability for a defective product is a “radical restructuring”? It’s something we have in almost every other category of business - not perfectly but pervasive enough that software is really conspicuous as an outlier. reply EnigmaFlare 14 hours agorootparent [–] Absolutely. OEM car parts suppliers have liability not just for their own product but whatever consequences happen downstream, like the cost of recalls, etc. And that makes sense becase liability is on the companies that are in the best position to ensure their product is correct. Vendors of engineering software used by car makers, on the other hand, have no such liability. It's software so its the user's responsibility. reply usrusr 8 hours agorootparent [–] Now imagine software priced like OEM car parts. We'd all be running some heirloom version of Turbo Pascal. reply acdha 6 hours agorootparent [–] You know that some of the most profitable companies in the world are software companies, right? Putting more resources into security and robustness wouldn’t mean a copy of Excel costs $5,000, it’d mean that Microsoft’s profit margins go down slightly and they ship new features slightly slower. The incredible leverage of software engineering would still mean that they’re amortizing those costs across a billion users. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jen Easterly, head of CISA, criticized software suppliers for shipping insecure code, labeling them as key contributors to cybercrime.",
      "Speaking at Mandiant's mWise conference, she urged vendors to improve code quality and suggested renaming \"software vulnerabilities\" to \"product defects\" to emphasize responsibility.",
      "Despite a multi-billion-dollar cybersecurity industry, software quality issues persist; Easterly has been advocating for secure code, with nearly 200 vendors signing CISA's Secure by Design pledge."
    ],
    "commentSummary": [
      "The head of CISA (Cybersecurity and Infrastructure Security Agency) has labeled makers of insecure software as the primary culprits in cybersecurity issues.",
      "The discussion highlights the complexity of creating secure software and suggests that economic incentives and liability for software vendors could drive improvements in software quality.",
      "The debate includes comparisons to other industries, such as structural engineering, where professionals are held liable for their designs, and suggests that similar accountability could be applied to software developers."
    ],
    "points": 129,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1726877157
  },
  {
    "id": 41610619,
    "title": "Omega-3 intake counteracts symptoms of anxiety and depression in mice",
    "originLink": "https://www.psypost.org/omega-3-fatty-acid-intake-counteracts-symptoms-of-stress-induced-anxiety-and-depression-in-mice/",
    "originBody": "Depression Can your personality predict depression across your lifespan? September 11, 2024 A study found that neuroticism and introversion are significant predictors of depression across the lifespan, with anxiety showing strong links to these traits in adulthood, while physical health factors like BMI also contribute to depression risk. Read more",
    "commentLink": "https://news.ycombinator.com/item?id=41610619",
    "commentBody": "Omega-3 intake counteracts symptoms of anxiety and depression in mice (psypost.org)128 points by geox 3 hours agohidepastfavorite56 comments voytec 1 hour ago\"Omega-3\" is as vague and underdescriptive term as \"marihuana\". We're better educated now and we can focus on the eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA) where it comes to Omega 3. And we can point at THC, CBN, CBD or any of the 100+ other cannabinoids present in cannabis flowers when we describe \"marihuana\", \"weed\" or \"pot\". And there's a reason behind my local pharmacy offering THC-focused or CBD-focused pot, and my local supplements store offering DHA-focused and EPA-focused softgels, produced by the same company and under the same brand. How these these products act varies by active substances content and the person. Both \"Omega 3\" and \"marihuana\" are dumbed-down terms, meaningless when it comes to studies or papers. There's a ton of studies on nih.gov about EPA's potential as an antidepressant, misaligned with DHA-related articles on the subject, that I could link. But I can say - purely anecdotally - that I removed psypost.org feed from my RSS reader a few months after ChatGPT became public. reply derefr 41 minutes agoparentI think there's a practical distinction between the kind of term \"omega-3\" is vs the kind of term \"marijuana\" is. You mostly only find THC/CBD/etc in marijuana; and we mostly only consume marijuana to get those particular active ingredients into our bodies. So you can forget about \"marijuana\" as a category for describing those compounds, and just speak of the compounds themselves — measure marijuana strains by the presence of those active ingredients; extract and purify one particular active ingredient and sell it; etc. Doing this doesn't lose you anything; in fact, it's a pure win, as the use of precise language gives people a tool to leverage to more precisely ask for the effect they're looking for, and gives suppliers a tool to more precisely describe what they're selling. While the omega-3 constituent compounds can be treated this way, they are not solely a thing we extract or synthesize to put in precise-molarity-per-dose pills; they're also a thing found in food. Many different foods; with most of them being foods people eat for reasons beyond just getting omega-3s in their bodies. In other words, the \"omega-3\" constituent compounds are nutrients. And many of these omega-3-containing foods — fish, for example — aren't carefully cultivated species that have known ratios of the omega-3 constituent compounds that could be put on the label of the food-product. Rather, the ratio of those constituent compounds is pretty much random per individual food item. One salmon at the grocery store has omega-3 fats which happen to be high in DHA, while the next salmon beside it in the same cooler display is higher in EPA. All you can in general about a food product — all a supplier can say, and all a food shopper can generally expect to look for — is a food that is \"high in omega-3s.\" As long as people are interested in optimizing their health in a loose manner by eating \"healthful foods\" — rather than taking specifically-formulated supplements — I don't expect they'll let go of the generic categorical term \"omega-3.\" When it comes to food, \"it contains omega-3s\" is almost always the tightest bound you can put on the \"nutritional value\" of a given food. reply sheepdestroyer 27 minutes agorootparentI'm not so sure about the incertitude you speak of about omega-3s content and ratios in food. In fact while in Japan I used to specifically select my bags of dried sardine (niboshi) and mackerel cans by the content of EPA and DHA, clearly indicated in mg per 100g. Every such products had these indications with content varying by brands. I guess that if you master your process you can ensure and advertise a consistent quality. reply derefr 3 minutes agorootparentProcessed fish products are a bit different from a large chunk of a single fish, in terms of the promises that can be made. The problem with large, wild-caught fish, is that different fish are going to be living in slightly-different regions, or migrating through given regions at slightly-different times, and so eating different things; and so will have more or less of any given nutrient coming from those things in their bodies. If you're catching large swarms/schools of fish, all the fish in a given school will be mostly identical in their nutritional content. And so, if you're doing some bulk operation like canning or drying, and you're doing it on fish that swarm/school and get caught as whole swarms/schools (such as sardine or mackerel) — then, for each catch delivered to your plant by a fishing vessel, you can take a few samples from that catch to get a sense of the average nutritional micronutrient values of that catch; and then you can store these catches separately, titrating together the different catches into each processed mixture, to achieve the desired nutritional values. (Same thing that e.g. orange juice companies do with the truckloads of oranges they buy.) But if you're just buying e.g. one salmon, then it came through an entirely different logistics pipeline to get to you — either an \"independent\" one where a small-time fisherman sold some fish directly to a local fishmonger, who then sold it directly to a local grocer, a few fish at a time; or a \"big chain\" one where a stream of flash-frozen fish from fishing vessels is being just-in-time streamed out to various grocery stores. (This lack of a fan-in processing step for raw large fish is also why grocers end up with so much mislabelled fish; the guy who works at a fish processing plant near the fishery will recognize all the fish that fishery tends to pull out of the water; but there's no similar expert in the meat department of a random grocery store in Idaho — in fact, that person might not even know which fishery the fish they're receiving came from!) reply orblivion 55 minutes agoparentprevI had been under the mistaken impression that DHA was only interesting inasmuch as your body inefficiently turns it into EPA. reply adrian_b 0 minutes agorootparentDHA and EPA have distinct roles in the body. Both are necessary for most people, especially for males and for older people, because the inter-conversions between the various omega-3 fatty acids are done inefficiently by humans. reply Trasmatta 2 hours agoprevOmega 3 supplements are such a wild world. So many low quality ones, so many different types, and so many claims about them being miracle cures (that then don't hold out for later studies). I have severe chronic dry eye, and omega 3 is one of the first things they recommend. But then in the past 5 or 6 years there have been a bunch of studies that have shown there is no actual measurable effect. Some people still claim that it helps, but that you have to get it by actually eating Omega 3 rich fish, and not just taking the supplements. Either way, I've never seen any difference with my dry eye symptoms. reply someothherguyy 1 hour agoparentIt is suspect to me that a lot of waste products end up being pushed as supplements. Also, there is a swath of junk science associated with nutrition publishing. reply lambdaba 1 hour agorootparentIt's used in TBI, see Dr. Barry Sears for some accounts. \"Fish oil helped save our son\" https://edition.cnn.com/2012/10/19/health/fish-oil-brain-inj... > Most of the studies about omega-3 for traumatic brain injury are in animals, but they indicate potential for healing the human brain. > After a trauma, the brain tends to swell, and the connections between some nerve cells can become damaged, while other cells simply die. > National Institutes of Health research suggests that omega-3 fatty acids may inhibit cell death and could be instrumental for reconnecting damaged neurons. > Another recent study revealed genes that are activated to contain massive damage – especially inflammation – when the brain is injured. What activates those genes: omega-3. > “We have strong data that suggest omega-3 will activate good proteins to cope with brain damage and turn off proteins that cause neuroinflammation,” said Dr. Nicolas Bazan, director of the Neuroscience Center of Excellence at LSU Health in New Orleans and author of the study. > Bailes consulted with a fish oil expert and eventually decided that administering 20 grams a day of omega-3 fish oil through a feeding tube might repair the myelin sheath. (For comparison: A typical supplemental dose for someone with an uninjured brain is about 2 grams a day.) reply hirvi74 55 minutes agorootparentTBI is a wide spectrum of conditions akin to \"trauma\" in psychological terms. I skimmed the article, but I didn't see any specific medical terms used. The reason I am curious is because I do not think that mTBI like concussions cause brain swelling since the injury is more of a functional injury than a structural injury. I like to play ice hockey as an adult, and I try to do everything reasonably possible to protect my brain from vulcanized rubber disks flying at my head at 0Omega 3 supplements are such a wild world. That's why you should get it from seafood and grass fed mammals (if you need long chain PUFAs) or flaxseed (if you need short chain PUFAs). There are probably many other things in seafood that co-exist with Omega 3 for a reason. reply tharmas 1 hour agoparentprevHave you tried Lutein? reply readyplayernull 31 minutes agoprevOn a related topic, I started taking bacopa monnieri and it improved my memory and mental endurance, anxiety and depression. I gave it to my wife and had the totally opposite effect: https://en.wikipedia.org/wiki/Bacopa_monnieri Also I can drink coffee and won't make any effect on my sleep, while my wife gets the boost. So I wouldn't be surprised the effects of all those supplements depend on your body chemistry. reply huuhee3 10 minutes agoparentYep, what works for one may not work for others. Some people find L-theanine supplement helpful for reducing anxiety. For me it kind of worked, but then gave bad anxiety as a withdrawal symptom. reply meindnoch 12 minutes agoprevWhen I took omega-3 in addition to duloxetine, I got terrible brain zaps the whole day. reply cultofmetatron 2 hours agoprevThis definitely tracks with my experience. I take a tablespoon of cod liver oil daily and I've definitely noticed a complete flatlining of my emotional states. My baseline is just basic calmness and a general lack of \"monkeymind\"/intrusive thoughts. Really amazed its not a more prescribed treatment. reply alberth 2 hours agoparentCod is super high in Vitamin D, and considered by many historians the secret food that contributed to Vikings success. https://goodness-exchange.com/cod-vikings-vitamin-d/ reply Trasmatta 1 hour agoparentprevDoes it also flatline your positive emotions? Ashwaganda does a similar thing for a lot of people, it tamps down their anxiety and depression but suppresses most other emotions as well (it's called anhedonia). reply cultofmetatron 1 hour agorootparentI would definitely say so. I can still feel happy if I choose to but I don't feel upward swings. Overall I'd say its a net positive as I generally feel content. reply mmanfrin 1 hour agoprevWell this is a great thing to see an hour after buying some O3s. I have read though that omega-3 supplements seem to not do much, it's the intake of them dietarily that show results, I'm not sure how current that research is. I've also read that algae as a source might be better? There's a tremendous amount of conflicting information. reply oarfish 16 minutes agoparent> I have read though that omega-3 supplements seem to not do much afaik that holds for a lot (if not most) commonly used supplements (vitamins are another popular offender). Sometimes the reason is the food matrix effect, where just isolated nutrients are not as beneficial as when they are consumed along with other nutrients. Sometimes (like vitamin D or testosterone), the biomarker is reflective of health status, not predictive. From this [1] podcast (2y old at this point) I too seem to remember that it doesn't do much most of the time, that supplements are generally untrustworthy w.r.t. dosing and purity and there's also a slight chance of giving yourself afib. 1. https://soundcloud.com/user-344313169/episode-193-fish-oil reply reubensutton 1 hour agoprevThere’s never been a better time to be a mouse reply atombender 1 hour agoparentOr a worse one. I know you're making a joke, but it doesn't seem that funny to me — U.S. biomedical industry goes through about 111 million lab mice and rats each year [1]. Most mice lives are short and cruel, living their entire lives in tiny plastic boxes and often subjected to horrific experiences. We should be more thankful. [1] https://www.science.org/content/article/how-many-mice-and-ra... reply hirvi74 49 minutes agorootparentI really appreciate that someone else shares my opinions on this matter. It pains me what we do to animals for our benefit. I suppose there is no other alternative, but that does not diminish the cruelty. I feel the same way about the \"Boar's Head\" meat recall in the US due to Listeria. 7 million pounds of meat must be disposed of. That means every single one of the pigs died for that meat died for absolutely nothing at all. It just doesn't sit right with me. reply PlattypusRex 1 hour agoparentprevThis isn't funny. Mice and rats are among the most intelligent animals used in research, and despite ethical regulations on their treatment, they are often ignored. Many of these animals endure severe procedures without adequate pain relief, or none at all. This widespread suffering also causes mental health problems for many of the researchers who have to regularly euthanize them. These animals sacrifice their lives to improve human health and well-being. At the very least, we should show respect for their pain and suffering, which is undertaken for our benefit. reply CatWChainsaw 1 hour agoparentprevRight up until any given study with mice concludes... reply elromulous 3 hours agoprevThis is exciting, and should be fairly easy to confirm/deny with a double blind trial in humans. There's no money to be had in it for big pharma, so funding it would have to come from some other means, but it should be possible. reply nosefurhairdo 2 hours agoparentThere's already research on Omega 3 supplementation's impact on depression + anxiety symptoms in humans. The results generally show that Omega 3 is roughly as effective as antidepressants, and can be safely taken in addition to antidepressants for an even greater effect. reply petesergeant 1 hour agorootparentExamine.com summary: > Fish oil supplementation has been noted to be comparable to pharmaceutical drugs (fluoxetine) in majorly depressed persons, but this may be the only cohort that experiences a reduction of depression. There is insufficient evidence to support a reduction of depressive symptoms in persons with minor depression (ie. not diagnosed major depressive disorder) They give it a B for the research and an effect size of moderate improvement. Also a B with a small effect size for anxiety. https://examine.com/supplements/fish-oil I’d note it also says: > A meta-analysis of 35 small, randomized trials found that fish oil can slightly improve depression when compared to control. However, this improvement may be too small to be noticeable. Also, adding fish oil to antidepressant medication seems to be more beneficial than antidepressant treatment alone In short, sounds like it’s worth throwing in as an adjunct treatment for people wanting to take a kitchen sink approach reply RobotToaster 2 hours agoparentprev> There's no money to be had in it for big pharma IIRC there's several patent medicine versions of omega 3 such as icosapent ethyl reply setting1855 3 hours agoparentprevas opposed to supplement studies in mice, which are notoriously lucrative reply elromulous 2 hours agorootparentAs the other comment mentions, mouse studies are hilariously cheap compared to human trials. We're talking many orders of magnitude. A couple grad students can do a mouse study at costs very much approaching zero. reply brokensegue 2 hours agorootparentprevthey are cheaper reply dukeofdoom 10 minutes agoprevJust instruct your personal chef to include more organic eggs, to cure your ego problem, and you'll feel better. Great news for anyone that partied with P.diddy reply maxresdefault 2 hours agoprevThis is great, but why does Omega 3 paradoxically reduce expression of dopamine receptors and dopamine release reply daniel_reetz 2 hours agoparentThe dopamine hypothesis is being heavily debated and revisited these days. Might be worth checking any assumptions against current research. reply FollowingTheDao 2 hours agoparentprevBecause it makes the dopamine more effective. reply FollowingTheDao 3 hours agoprevHigh long chain omega 3 and low omega 6 not only revered my hyperlipidemia (through stimulating reverse cholesterol transport) but notably reduces my symptoms of schizoaffective disorder bipolar type (I am assuming this is through assisting with carecholamine receptor function). I came to the conclusion this diet would help me by noting the polymorphisms in my FADS1 and FADS2 genes linked to needing more long chain PUFAs. You might also be interested that omega 3 is responsible for making natural Cannabinoid (endocannabinoids) in humans which could be linked to the lower anxiety. reply wtetzner 1 hour agoparentYeah, I've noticed too much Omega 6 is at least one big cause of my anxiety. Avoiding vegetable oils (and foods that are cooked with it) and just using coconut oil, tallow, butter etc. reduces my anxiety to the point that it's barely noticeable. reply dukeofdoom 42 minutes agoprevAnxiety and depressions are feelings. Eating feels good for a little bit. But I could easily make the claim that getting a dog and taking it for regular walks in nature counteracts symptoms of anxiety and depression. Doing Yoga with synchronized breathing...same Taking a hot bath, a sauna session, a massage, sex, a long list. Why do so many people think eating something is the path to good health. Also knowing how things are encoded in neural networks, where it's a combination of variables that sets off some behaviour. Why do Scientists still persist on trying to isolate a single variable. reply nightmonkey 2 hours agoprev [–] Does caffeine intake nullify this effect? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A study indicates that personality traits such as neuroticism and introversion are significant predictors of depression throughout one's lifespan.",
      "Anxiety is strongly associated with these personality traits, particularly in adulthood.",
      "Physical health factors, including Body Mass Index (BMI), also play a role in the risk of developing depression."
    ],
    "commentSummary": [
      "Omega-3 intake has been shown to counteract symptoms of anxiety and depression in mice, but the term \"Omega-3\" is broad and includes specific compounds like EPA and DHA.",
      "The effectiveness of Omega-3 supplements is debated, with some studies indicating benefits for conditions such as traumatic brain injury and depression, while others show minimal impact.",
      "Nutrient levels of Omega-3 can vary in different foods, making it challenging to determine precise intake from whole foods compared to processed products or supplements."
    ],
    "points": 128,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1726932693
  },
  {
    "id": 41609670,
    "title": "Scaling up linear programming with PDLP",
    "originLink": "https://research.google/blog/scaling-up-linear-programming-with-pdlp/",
    "originBody": "Home Blog Scaling up linear programming with PDLP September 20, 2024 Haihao Lu, Research Scientist, Google Research, and Assistant Professor, MIT, and David Applegate, Principal Scientist, Google Research This post describes the award winning product called PDLP, a new first-order method based solver for large-scale linear programming. Quick links Paper Share Copy link × Classic linear programming (LP) problems are one of the most foundational problems in computer science and operations research. With extensive applications across numerous sectors of the global economy, such as manufacturing, networking, and other fields, LP has been the cornerstone of mathematical programming and has significantly influenced the development of today’s sophisticated modeling and algorithmic frameworks for data-driven decision making. If there's something to optimize, there's a good chance LP is involved. Since the late 1940s, LP solving has evolved significantly, with the simplex method by Dantzig and various interior-point methods being the most prevalent techniques. Today's advanced commercial LP solvers utilize these methods but face challenges in scaling to very large instances due to computational demands. In response to this limitation, first-order methods (FOMs) have gained traction for large-scale LP problems. With the above in mind, we introduce our solver PDLP (Primal-dual hybrid gradient enhanced for LP), a new FOM–based LP solver that significantly scales up our LP solving capabilities. Utilizing matrix-vector multiplication rather than matrix factorization, PDLP requires less memory and is more compatible with modern computational technologies like GPUs and distributed systems, offering a scalable alternative that mitigates the memory and computational inefficiencies of traditional LP methods. PDLP is open-sourced in Google’s OR-Tools. This project has been in development since 2018 [1, 2, 3], and we are proud to announce that it was co-awarded the prestigious Beale — Orchard-Hays Prize at the International Symposium of Mathematical Programming in July 2024. This accolade is one of the highest honors in the field of computational optimization, awarded every three years by the Mathematical Optimization Society. LP and first-order methods for LP Scaling the methods used in today’s state of the art LP solvers presents significant challenges. The primary computational limitations for both methods relate to matrix factorization required for solving linear equations, introducing two key challenges as problem sizes grow: Memory overflows: LP solvers that use the simplex method (such as Google's GLOP) employ LU factorization, and solvers that use the interior point method use Cholesky factorization. For both these methods the resulting factorization uses considerably more memory than the LP instance itself. Hardware-related challenges: Both methods face difficulties leveraging modern computing architectures, such as GPUs or distributed systems, because the sparse matrix factorization step usually requires highly sequential operations. Given the above limitations associated with traditional LP methods, FOMs have emerged as a promising alternative for tackling large-scale LP problems. Unlike methods that rely on matrix factorization, FOMs utilize gradient information to iteratively update their solutions, with the primary computational requirement being matrix-vector multiplication. This distinction means that FOMs require only the storage of the LP instance itself, without needing additional memory to store factorized forms. Additionally, advances in FOMs for machine learning and deep learning have enhanced their scalability, making them highly efficient on modern computing platforms such as GPUs and distributed computing. This scalability and reduced memory dependency make FOMs particularly suitable for large and complex LP tasks where traditional methods may falter. Restarted primal-dual hybrid gradient for LP Primal-dual hybrid gradient (PDHG) is widely recognized for its application in image processing. When applied to LP, PDHG's primary computational demand involves matrix-vector multiplication, eliminating the need for matrix factorizations. This makes PDHG particularly efficient for large-scale computational tasks, but it is not reliable in solving LP. For example, in a benchmark of 383 instances, PDHG can only solve 113 instances to moderate accuracy. To enhance PDHG’s reliability in solving LP problems, we have developed a modified approach called restarted PDHG. This version uses a two-loop structure where PDHG is run until a restarting condition is triggered, after which the average of the PDHG iterations is computed. The algorithm then restarts from this average point. This approach is visualized below where the trajectory of the standard PDHG is depicted with a blue line, the average iteration with a red line, and the restarted PDHG with a green line. Notably, the restarted PDHG shows a quicker convergence to the optimal solution, marked by a star on the plot. play silent looping video pause silent looping video The convergence behaviors of the PDHG and restarted PDHG, where the x-axis is the current solution of the LP, and the y-axis is the current solution of the dual LP. The intuition behind this faster convergence is that by restarting from the computed average at the end of each spiral phase, the restarted PDHG effectively shortens the path to convergence. This strategy leverages the cyclical nature of the PDHG spirals to expedite the solution process. We show in our research that this restarting technique can significantly speed up the convergence behaviors of PDHG for LP both in theory and in practice. This establishes restarted PDHG as a highly efficient and theoretically sound method for tackling LP challenges, reinforcing its utility and effectiveness in computational optimization. PDLP We designed PDLP as a software package that can solve linear programming problems efficiently. The core algorithm of PDLP is based on the restarted PDHG, which we have enhanced significantly through five improvements: Presolving: This process simplifies the LP problem before solving. It involves detecting inconsistent bounds, detecting duplicate rows, tightening bounds, etc. These steps reduce complexity and improve the efficiency of the solver. Preconditioning: A preconditioner in PDLP rescales variables and constraints within the LP instance. This adjustment helps speed up the algorithm by optimizing the numerical condition of the problem, thereby enhancing convergence rates. Infeasibility detection: In real-world scenarios, LP problems may often be infeasible or unbounded. Our approach utilizes the iterates of PDHG, which encodes information about the problem's feasibility and boundedness, allowing for detection without extra computational effort. The theory of this method is detailed in our SIAM Journal paper. Adaptive restarts: This technique involves strategically deciding when to optimally restart the PDHG algorithm to enhance its efficiency, particularly speeding up the convergence to a high-accuracy solution. Adaptive step-size: We introduced an adaptive method for selecting the step-size in the PDHG, which significantly reduces the need for manual tuning. This approach adjusts the step-size dynamically based on the problem's characteristics and the algorithm's performance, promoting faster convergence. PDLP is open-sourced as part of Google’s OR-Tools, an open-source software suite for optimization. The solver is easy to use and it has interfaces in Python, C++, Java and C#. More details and examples on how to use PDLP can be found in the OR-Tools documentation. Applications Scaling up and speeding up LP enables new applications — here, we briefly mention three: Data center network traffic engineering (blog post, paper): Google's data centers rely on dynamically optimized traffic engineering for high-performance efficiency. The challenge of optimizing network traffic routing is periodically addressed as a large-scale LP problem. Previously, solving this large problem fast enough was not possible, leading to the development of partition heuristics. These heuristics decomposed the problem into many smaller-scale LPs that could be solved concurrently, albeit at the cost of optimality. With the introduction of PDLP, we can now efficiently optimize traffic routing across an entire data center network, effectively saving a significant amount of machine resources across the network. This solution has been deployed in Google's production environment since May 2023. Container shipping optimization (blog post): The world's shipping supply chain relies on optimizing the order in which vessels visit ports and the placement of containers on those vessels. Due to the extreme scale of real-world instances, a direct solution often is intractable. Consequently, various heuristic approaches have been proposed to enhance efficiency and practicality in solving this complex optimization problem. The problem can be formulated as a type of optimization problem called a massive integer two-layer multi-commodity flow problem. PDLP enables solving the linear relaxation of this formulation, quantifying the quality of the heuristics. Traveling salesman problem: The traveling salesman problem (TSP) poses a classic question: given a list of cities and their distances, what's the shortest route that visits every city once and returns to the starting point? This problem is notoriously challenging, holding significant importance in theoretical computer science and operations research. PDLP has demonstrated its power by solving real-world TSP lower bound LP instances of immense scale, encompassing up to 12 billion non-zero entries in the constraint matrix. This capability far surpasses the capacity of even the most advanced commercial solvers available today, showcasing PDLP's potential for tackling large-scale LP challenges. Broader impacts Since its initial release, PDLP has attracted significant interest, leading to further enhancements. Here are some notable developments: cuPDLP.jl is an open-sourced GPU implementation of PDLP, written in Julia. The commercial solver company, Cardinal Optimizer, has incorporated PDLP into their software in Version 7.1 in January 2024. The open-source solver, HiGHS, has incorporated a version of PDLP in their software in V1.7.0 in March 2024. In addition, the academic community has continued to explore and expand upon the theoretical foundations of PDLP. Recent studies have focused on areas such as new analysis on PDHG, condition number theory, trajectory-based analysis, extensions to quadratic programming and semi-definite programming, etc. These efforts not only deepen the understanding of PDLP's underlying mechanics but also explore its potential applications to more complex problems. These developments reflect PDLP's significant impact on the field of optimization, bridging the gap between theoretical research and practical application. As PDLP continues to evolve, its influence is expected to grow, pushing the boundaries of what can be achieved in computational optimization. Acknowledgments We are grateful to our co-authors Mateo Diaz, Oliver Hinder, Miles Lubin, and Warren Schudy for their exceptional support and contributions. We would also like to thank our managers, Vahab Mirrokni, Jon Orwant and Aaron Archer, and our collaborators in the Data Center Networking team, the Algorithm team and the Operations Research team. Labels: Algorithms & Theory Machine Intelligence Quick links Paper Share Copy link × Other posts of interest September 19, 2024 Open Buildings 2.5D Temporal dataset tracks building changes across the Global South Climate & Sustainability · Machine Intelligence September 16, 2024 Neptune: The long orbit to benchmarking long video understanding Machine Intelligence · Machine Perception August 21, 2024 Speculative RAG: Enhancing retrieval augmented generation through drafting Generative AI · Machine Intelligence · Natural Language Processing",
    "commentLink": "https://news.ycombinator.com/item?id=41609670",
    "commentBody": "Scaling up linear programming with PDLP (research.google)114 points by bookofjoe 4 hours agohidepastfavorite24 comments DominikPeters 1 hour agoIn this post, I don’t see any comparisons of their solver to other LP solvers on benchmarks, so it’s difficult to know how useful this is. reply thxg 57 minutes agoparentI think it's partially excusable. Most LP solvers target large-scale instances, but instances that still fit in RAM. Think single-digit millions of variables and constraints, maybe a billion nonzeros at most. PDLP is not designed for this type of instances and gets trounced by the best solvers at this game [1]: more than 15x slower (shifted geometric mean) while being 100x less accurate (1e-4 tolerances when other solvers work with 1e-6). PDLP is targeted at instances for which factorizations won't fit in memory. I think their idea for now is to give acceptable solutions for gigantic instances when other solvers crash. [1] https://plato.asu.edu/ftp/lpfeas.html reply dsfcxv 6 minutes agoparentprevTheir numerical results with GPUs, compared to Gurobi, are quite impressive [1]. In my opinion (unless I'm missing something), the key benefits of their algorithms lie in the ability to leverage GPUs and the fact that there’s no need to store factorization in memory. However, if the goal is to solve a small problem on a CPU that fits comfortably in memory, there may be no need to use this approach. [1] https://arxiv.org/pdf/2311.12180 reply sfpotter 1 hour agoparentprevThey link to three of their papers that have more details. reply optcoder 1 hour agorootparentThe three linked papers seem to be old, but the broader impact section mentioned cupdlp, which is more recent and has interesting numerical comparisons with commercial solvers: https://arxiv.org/abs/2311.12180, https://arxiv.org/pdf/2312.14832. It is CPU vs GPU, though, not sure how fair it is. reply raidicy 3 hours agoprevSlightly off topic but does anybody have any resources for learning linear programming for business applications? reply cashsterling 2 hours agoparentI'd recommend getting the 9th or 10th edition of Introduction to Operations Research by Hillier and Lieberman. 9th: https://www.amazon.com/dp/0077298349 You can search for the 10th edition. Both are available used for less than 50 USD in good condition. The book covers a lot more than linear programming. A solution manual for both editions can be found on the internet. A good \"free-pdf\" optimization book, to support the above is, Algorithms for Optimization by Kochenderfer & Wheeler ( https://algorithmsbook.com/optimization/ ). It has a chapter on constrained linear optimization with Julia code and is a good secondary resource. Kochenderfer, Wheeler, and colleagues also have two other free optimization books that are a little more advanced. It is exceptionally cool that they make the high quality PDF freely available; more authors in the technical space are making their books freely available as pdf and I applaud them for it. reply bookofjoe 2 hours agorootparent10th edition free here: https://s23.middlebury.edu/MATH0318A/Hillier10th.pdf reply armanboyaci 2 hours agoparentprevI recommend this blog: https://yetanothermathprogrammingconsultant.blogspot.com/?m=... Not all posts are business related but you can learn many practical tricks hard to find in books. reply tomas789 39 minutes agorootparentI add my +1 to this. I often come across this blog posts while working as a OR professional. reply chris_nielsen 1 hour agoparentprevApplied Mathematical Programming https://web.mit.edu/15.053/www/AMP.htm reply l33t7332273 45 minutes agorootparentI second this blog. The comparison of (10 different!) MIP modeling techniques for piecewise linear functions is more complete than any I’ve seen. reply nickpeterson 2 hours agoparentprevI really wish I could find solid websites/blogs/resources for operations research, mathematical planning, linear programming, etc aimed at regular software engineers. I feel like a lot of the really crazy parts of codebases often derive from inexperience with these kinds of tools. reply polivier 23 minutes agorootparentI write blog posts about constraint programming from time to time. I always include a step-by-step description of the model, which makes it fairly easy to understand. Hopefully this can be of help for you: https://pedtsr.ca reply colelyman 38 minutes agorootparentprevHave you seen http://www.hakank.org/ ? Mostly about constraint programming, but definitely in the realm of operations research. reply nuclearnice3 2 hours agoparentprevIf you can get your hands on informs publications. https://www.informs.org/Publications reply currymj 2 hours agoparentprevthe Mosek Modeling Cookbook is really good for seeing the \"tricks\" of how to reformulate things as convex programs. https://docs.mosek.com/modeling-cookbook/index.html Not for total beginners though but great 201 level resource. reply jeffbee 2 hours agoparentprevYou could just grab or-tools and work through their example problems, then extend it to something in your area of interest. The Python APIs are easy to experiment with. reply bee_rider 4 hours agoprevI often see the sentiment, essentially, a method is much better because it uses only matvecs (rather than factorize and solve). This always confuses me, because the game for numerical linear algebra folks is inventing funky preconditioners to fit their problem, right? Unless people are subtly saying “our new method converges incredibly quickly…” ILU0 is practically free, right? reply sfpotter 1 hour agoparentThere are tons of different games to play. Designing a preconditioner that's specific to the problem being solved can help you beat incomplete LU, often by a substantial (even asymptotic) margin. If you have a problem that's small enough to factorize and solve, that's great. That probably is the best approach. This doesn't scale in parallel. For really big problems, iterative methods are the only game in town. It's all about knowing the range of methods that are applicable to your problem and the regimes in which they operate best. There's no one-size-fits-all solution. reply bee_rider 1 hour agorootparentI agree, I was just using ilu as sort of a well known example. Also, because ILU0 has no fill-in, it should (IMO) be considered the “baseline” in the sense that not trying it should be justified somehow. reply pkhuong 3 hours agoparentprevPreconditioning would only apply to approaches like interior point methods, where the condition number quickly approaches infinity. reply bee_rider 1 hour agorootparentI wonder if they put any matrices in the suitesparse collection, or anything like that. It would be a nice fun weekend project to just play around with them. reply Shopper 2 hours agoprev [–] Daudi reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PDLP is a new first-order method-based solver for large-scale linear programming (LP), addressing scalability issues faced by traditional LP solvers like the simplex and interior-point methods.",
      "PDLP uses matrix-vector multiplication, requiring less memory and being more compatible with modern computational technologies such as GPUs and distributed systems, and is open-sourced in Google’s OR-Tools.",
      "PDLP includes enhancements like presolving, preconditioning, infeasibility detection, adaptive restarts, and adaptive step-size, making it efficient for applications in data center network traffic engineering, container shipping optimization, and solving large-scale Traveling Salesman Problems."
    ],
    "commentSummary": [
      "Google Research's PDLP (Primal-Dual Hybrid Gradient for Linear Programming) aims to handle large-scale linear programming instances that exceed memory capacity, using GPUs instead of RAM.",
      "PDLP is slower and less accurate than top commercial solvers for smaller problems but is notable for its ability to manage very large instances without memory storage.",
      "The discussion includes various resources for learning linear programming and operations research, such as textbooks, blogs, and academic papers."
    ],
    "points": 115,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1726923431
  },
  {
    "id": 41607166,
    "title": "Inside Annapurna Interactive's Mass Walkout",
    "originLink": "https://www.ign.com/articles/what-the-heck-has-been-going-on-at-annapurna-interactive-an-investigation",
    "originBody": "ALAN WAKE 2 BLADE RUNNER 2033: LABYRINTH COCOON CONTROL 2 LORELEI AND THE LASER EYES Inside Annapurna Interactive's Mass Walkout: Internal Politics, the Surprise Remedy Deal, and Why It All Happened The entire staff of one of gaming's most beloved publishers walked off the job in early September. Why? BY REBEKAH VALENTINE UPDATED: SEP 21, 2024 12:09 AM POSTED: SEP 20, 2024 9:00 PM Last week, Bloomberg reported that 25 people comprising the entire staff of Annapurna Interactive walked out the door in a group resignation. But while some of the circumstances around their departure emerged in the reporting, one pressing question was left unanswered: why? Having spoken to multiple individuals close to the situation who requested anonymity due to fear of reprisal, as well as an Annapurna spokesperson, IGN has pieced together a somewhat complex answer. Disagreements over the direction of the Interactive division, chaotic departures, communication breakdowns, and a perceived lack of leadership transparency at Annapurna Interactive led to a staff walk-out that has left 25 individuals jobless, Annapurna leaders scrambling, and numerous developers concerned about their contracts with the publisher. A Company Divided Though the collapse of Annapurna Interactive as we once knew it started earlier this year, its roots lie in the company's historical leadership structure. Annapurna Interactive was initially conceived as the gaming division of Annapurna Pictures, which was founded by film producer and billionaire Megan Ellison in 2011. Annapurna Interactive itself was spun up in 2016, tapping a staff of industry veterans including including former Sony creative director Nathan Gary, former Sony executive producer Deborah Mars, and former Sony producer Hector Sanchez for leadership roles. Sanchez left the company in 2019, and former Capybara Games co-founder Nathan Vella was brought on that same year. The film side of Annapurna's business has undergone well-publicized struggles. In 2018, it was bleeding enough money to prompt Ellison's father, multi-billionaire Larry Ellison, to step in. By 2019, Annapurna Pictures was reportedly teetering on bankruptcy, and in the ensuing years its film and TV output slowed significantly. Variety reports that Ellison disappeared from public life in 2019 almost entirely, leaving her business to largely run itself during the height of the pandemic. She reemerged in 2021, only to name Gary president over all of Annapurna, with Mars and Vella stepping into co-head roles at the Interactive division. IGN's Twenty Questions - Guess the game! 💡 Hint To start: ...try asking a question that can be answered with a \"Yes\" or \"No\". 000/250 Ask In the ensuing years, Annapurna Interactive continued to grow, releasing financial and critical successes such as Stray, Outer Wilds, What Remains of Edith Finch, and Cocoon. While the company claims the Annapurna Pictures side of the business hasn't struggled in recent years, saying that film and TV were more profitable than Annapurna Interactive in 2023, a spokesperson alleged to IGN that Gary was a less-than-ideal steward of Annapurna Pictures. Under his guidance, they claim, resources were pulled away from film and TV, key executives were pushed out, and the company was largely refocused on gaming. Annapurna tells IGN that Gary also elevated co-founder James Masi to chief administrative officer, a role the spokesperson suggested was unnnecessary at a company of Annapurna's size. Notably, Annapurna did launch an animation division under Gary's tenure that released the critically-acclaimed film Nimona just last year, though an Annapurna spokesperson reached out to IGN post-publication with the assertion that it was Ellison who brought in Nimona as the foundation for what would become the animation division. IGN has reached out to Gary, but he declined to comment. IGN understands that opinions of Ellison within Annapurna Interactive prior to 2024 varied from indifference to latent mistrust given previous reports on her behavior toward employees. Anonymous sources I spoke to all cited a strong fear of reprisal from Ellison in particular, given her resources, history, and reach. A few people referenced creative or compensation disagreements during their time at Annapurna that contributed to a general feeling Ellison would not keep promises. Multiple sources we spoke to described Ellison as a largely hands-off leader and rarely present in the gaming division, an attitude that for years suited many of Annapurna Interactive's employees just fine. Ad-Verset effects This was the state of things at the start of 2024 according to our sources. Prior to March of this year, work at Annapurna Interactive was business-as-usual, they say, until employees were suddenly informed mid-month that James Masi had been unexpectedly let go. An Annapurna spokesperson confirmed Masi was made redundant, saying that Ellison had chosen to step back in at the company and oversee the Annapurna Pictures side again in an effort to re-invest in the film and TV side of the business. As a part of this, Annapurna claims that Ellison reinstalled Gary as head of Interactive, and deemed Masi's role unnnecessary. However, at this time, Gary also left the company. Annapurna claims he left of his own accord in response to Masi's firing and his change in role. But sources say employees were told in the following days by leadership within Annapurna Interactive that Gary had been fired along with Masi. The belief that two of their leaders had been fired seemingly out of the blue sparked confusion and fury, and a handful of individuals quit in protest, including at least one other Interactive leader. The sudden resignation of multiple key individuals came as a shock to the company, and IGN understands that Ellison held a video call with Annapurna employees to discuss what had happened and find a way to move forward. On the call, Ellison allegedly expressed a desire to keep the entire group, including those who had been fired or resigned, together. In the following days, all the departed staff returned, including Gary and Masi, and discussions began for a potential spin-off of the company that would allow Gary and Ellison to achieve their respective visions with minimal disruptions to partner developers. Roughly, the plan was for Gary and the Annapurna Interactive staff to become a new company called Verset, with ownership split between Annapurna and Verset's leaders. Verset would oversee all currently existing signed Annapurna Interactive projects, with revenue split between itself and Annapurna proper in Annapurna's favor. It would also be free to sign its own, independent deals. Developers IGN spoke to report being made aware the spin-off was happening in the following months, and were reassured their contracts would be fulfilled. While employees understood such a venture would take time to get off the ground, in the ensuing months a number of events occurred that made some skeptical of Ellison's commitment to parting with Annapurna Interactive. In early summer, sources tell us that employees discovered Hector Sanchez had been quietly rehired back at Annapurna by Ellison and was working on gaming projects without the knowledge of the rest of the Interactive staff. The news wasn't made official until August that Sanchez had been appointed president of interactive and new media at Annapurna — a title that seemed to some as potentially at odds with Vella and Mars' roles at Interactive. Annapurna, for its part, claims that talks between Ellison and Sanchez began as far back as February for Ellison to fund a new venture Sanchez was planning after departing Epic Games. As talks continued, Annapurna says Sanchez began negotiating with Remedy Entertainment for a deal related to film and TV spin-offs of its properties. However, when spin-off negotiations began to crystalize at Annapurna Interactive, Ellison offered Sanchez a position at Annapurna. The intent, per the spokesperson, was for Verset to become the company's indie arm, and for Sanchez to lead efforts in the AAA and AA-gaming space, including transmedia properties. Which is how, months later, Annapurna announced it was partnering with Remedy Entertainment on film, TV, and other projects including funding support for Control 2. The press release, which IGN received, referenced both Sanchez and Ellison. But it doesn't reference Annapurna Interactive at all, and IGN understands Annapurna Interactive employees were only informed the deal was happening that morning. Employees, unaware of Ellison's plans or the status of the spin-off, were confused, concerned, and frustrated about the direction of the company and the future of its Interactive division, Verset or no Verset. While all this was going on, sources say that discussions with Ellison regarding the spin-off appeared to have stalled out, and in August Annapurna officially terminated discussion. Annapurna claims this was due to Gary's lack of response to requests for feedback on legal drafts, and in a statement sent to IGN after publication, doubled down. \"Any implication Annapurna was backtracking on the deal is false. We agreed to high-level deal terms and signed a term sheet in early April, which makes it all the more surprising that we never got a response.\" Meanwhile, multiple people told IGN that in those final months, they began to see signs of Ellison exercising greater involvement over Annapurna Interactive's deals, projects, and budgets in a way that began to make them further uncomfortable with the direction the company was taking overall. All of this came to a head at the end of August when all 25 Annapurna Interactive employees including Gary, Vella, Masi, and Mars signed a joint resignation letter. The group gave two weeks notice and departed the company together on September 6 leaving Ellison, Sanchez, and newly-hired chief strategy officer Paul Doyle working on a semblance of Annapurna's gaming efforts. Sources tell IGN that up to the letter being sent and after, the group asked Ellison to work with them on other possible solutions such as the aforementioned spin-off, but did not receive any interest. IGN also understands that despite the two-week notice, partner developers did not learn about the sudden exodus of all their Annapurna contacts until a day or two before it occurred. Annapurna claims they didn't have enough time to collect developer contact information to alert them sooner, while Annapurna Interactive sources say they received no guidance from the company during that period as to who should tell developers, when, and how. An Annapurna spokesperson reached out post-publication to IGN to deny this, claiming it had active and open conversations with the Interactive staff on how to communicate the news. Annapurna Aftermath While IGN couldn't glean any details on the future of the 25 departed employees, there are some indications that the group is collectively working on some new venture together. A website for Verset appears to be online at the time of publication with a PR alias, which IGN reached out to for comment. IGN was also unable to find any posts or other discussions from the departed members indicating they were looking for employment elsewhere. Whatever their future plans, IGN understands that the group did not have a ready-made venture waiting when they left, as some have speculated. If they build anything new, it will be largely from scratch. Meanwhile, at Annapurna itself, efforts are underway to right the ship. Multiple developers I spoke to expressed a mixture of frustration and confusion at the sudden departure, but several told me they felt confident in Sanchez's ability to honor existing obligations. Several individuals with projects at different stages of development told me about meetings they'd had with Sanchez in the wake of the event that they reported had reassured them. Sanchez has previously stated his intention both to backfill roles as well as work with outside agencies to fulfill Annapurna's contractual obligations, and IGN has confirmed this process is ongoing. Earlier this week, Annapurna also posted an open role for a QA Manager overseeing \"multiple external QA teams.\" The unusual situation appears to have impacted a few specific projects in unique ways. On August 30, iam8bit shared an announcement that an upcoming PlayStation 5 physical edition of Outer Wilds: Archaeologist Edition had suffered from a manufacturing error, and did not include the Echoes of the Eye expansion as expected. Annapurna Interactive at the time said it would \"continue to investigate\", while iam8bit offered either a digital DLC voucher to impacted customers, or a replacement corrected physical copy. Both Annapurna and Interactive sources have told IGN that this issue is unrelated to the resignations, and Annapurna reassured that it will not be impacted by the upheaval at the company. Then there's Blade Runner. Last summer, Annapurna Interactive announced it would be developing its first in-house game, based on the Blade Runner franchise, titled Blade Runner 2033: Labyrinth. However, game director Chelsea Hash appears to be one of the 25 individuals who resigned, per LinkedIn, and IGN understands that all other full-time members of the development team joined her. Annapurna has told IGN that development on Blade Runner 2033 will continue despite the departure of its entire team. An Annapurna spokesperson also shared the following statement when asked for further comment: \"The whole situation is a baffler, but now we're focused on moving forward. We've had really great conversations with an overwhelming majority of our existing development teams and are grateful for their partnership. If our inbox is any indication, a ton of developers continue to want to be a part of what we're building, and we look forward to seeing their pitches. We've also had an influx of quality job applicants and are excited to build a team passionate about our mission to tell original stories that aren't being told elsewhere. P.S. We're hiring.\" The whole situation is a baffler. “ Annapurna has splintered into two groups, both of which are now working to pick up the pieces. The remains of Annapurna Interactive (or perhaps a future Verset) consist of 25 individuals who felt strongly enough about perceived mismanagement, poor communication, apparent spontaneous layoffs of leaders, and one another that they were willing to give up paychecks and stability at a time of overwhelming industry job and funding uncertainty. When IGN approached its sources close to this group about Annapurna's version of events (specifically, the conflicting information around Masi and Gary's alleged resignation/firing and the collapse of spin-off discussions), they reacted with skepticism, but did not feel they could safely provide more specific details. Meanwhile, at Annapurna, a tiny leadership team is struggling to ensure that around 40 projects have the support they need, while the company's partners have been left at various stages of development and uncertainty as to what comes next. Annapurna Interactive as we once knew it — a beloved publisher of critically-acclaimed, unique, beloved indie games — is no more. What, if anything, will rise to take its place? Rebekah Valentine is a senior reporter for IGN. Got a story tip? Send it to rvalentine@ign.com. Additional statements from an Annapurna spokesperson were added to this piece post-publication. Recommends 'Nobody Could Do Gotham': An Oral History of the Fox Show That Reinvented Batman 19 ISSUE NO. 1 PS5's 30th Anniversary Range Is Why We Should Always Question Nostalgia Never Let Go Ending Explained 8 The Inside Story on Annapurna Interactive's Mass Walkout 158 Nintendo vs. Palworld: 'Killer Patent' May Be About the Mechanic of Catching Pokémon 105 Anime Vanguards Codes (September 2024) 1 Dress to Impress Codes (September 2024) 15 The Correct Watch Order of the X-Men Movies Explained 162",
    "commentLink": "https://news.ycombinator.com/item?id=41607166",
    "commentBody": "Inside Annapurna Interactive's Mass Walkout (ign.com)106 points by samfriedman 15 hours agohidepastfavorite70 comments ChrisArchitect 14 hours agoRelated: Entire Annapurna Game Team Resigns https://news.ycombinator.com/item?id=41526074 Entire staff of game publisher Annapurna Interactive has reportedly resigned https://news.ycombinator.com/item?id=41528266 reply redundantly 15 hours agoprevI confused Annapurna Interactive with Annapurna Labs. Was lost for a bit there reading the article. reply hilux 13 hours agoparentAnnapurna is a mountain in Nepal. \"Only\" tenth highest in the world, but with the highest or second-highest fatality rate. That makes it an attractive company name, I guess. reply jajko 10 hours agorootparentIt also has one of most spectacular multi week hikes in the world, if not #1, the Annapurna circuit. I was lucky to take it in 2008 with almost nobody on the trail (just right after monsoons in second half of September), took 16 days, around 220km IIRC. No roads built back then although we saw few CAT bulldozers dropped in the impossible places in gorges, I guess part of construction. From absolute tropical jungle, through all other climates to frozen high altitude icy desert and all the way back, top point is 5,400m high. Also at one point hinduism of lowlands switches to buddhism. Wild marihuana growing everywhere. Muktinath just after(before) the highest pass is an important sacred and pilgrimage place for 3 different religions. There is one point in Kali Gandaki gorge where you look left and there is absolute tibetan-style desert with 0 plants, just rocks and dirt, going up to Manang region. You look right and its typical tropical jungle. And in between, in span of maybe 7-8km the whole gradient happens continuously on Annapurna western slopes. Another nice spot is IIRC Marpha where you are looking at almost 5000m pretty much vertical drop on cca top of Dhaulaghiri, its sister 8000m+ peak. Even after 2 weeks of constant exposition to himalayan giants I just stood there in awe. A life changing experience for me due to various factors and also people met. It has a special place in my heart. reply hilux 2 hours agorootparentThat sounds ... amazing! I've never heard it described in this way, and I'm actually tempted. reply jajko 6 minutes agorootparentLife is too short to be left with regrets, and such adventures are easier done when younger for many reasons. That being said, please manage your expectations, what was a wonderful profound experience for me may end up being a very different one for you. The best is to go there 1) physically prepared for long hikes with some backpack (not heavy but not just daily pack neither); 2) have your mind open to handle everything that comes your way as part of adventure, both good and bad. Plus careful with hygiene, mostly using hands sanitizer before touching any food, eating just freshly cooked meals. Some folks I know ended up with subpar experience due to catching nasty stomach infection which made it too hard to enjoy it. Using consistently hand sanitizer before any meal may be common stuff now but in 2008 I raised some eyebrows. But 3 months of backpacking in India and this and no stomach issue with such approach, that's pretty rare. reply devoutsalsa 5 hours agorootparentprevIn 5 days I’m flying to Nepal to trek up to Everest Base Camp. I’d also love to do the Annapurna Circuit. Maybe next year :) reply jajko 5 hours agorootparentI've done that one too, cca 7 years ago. A bit harder variant - 3 passes trek, where EBC is a side hike. Amazing place and whole region too, can't get enough of that country. Other valleys are much more remote seeing only fraction of the main EBC route traffic. If you are not part of organized group (I went solo), without prior booking it may be tricky to find accommodation in Gorak Shep, sort of bottleneck since everybody goes to EBC for morning views on the top of the world and its the closest place. I ended up convincing one of owners of guest house there to let me sleep with them in dining room, it was -10C outside and my sleeping bag would not be up to that, even in dining room it was just above freezing. Wishing you good weather for nice views from Kala Patthar and on Ama Dablam! reply blackeyeblitzar 12 hours agorootparentprevFrom Wikipedia: > The mountain is named after Annapurna, the Hindu goddess of food and nourishment, who is said to reside there. The name Annapurna is derived from the Sanskrit-language words purna (\"filled\") and anna (\"food\"), and can be translated as \"everlasting food\".[8] Many streams descending from the slopes of the Annapurna Massif provide water for the agricultural fields and pastures located at lower elevations.[9] reply Sateeshm 10 hours agorootparentIt is also a popular name in the south indian boomer generation reply dudeinjapan 12 hours agorootparentprevMaybe we should just look at it and not try to climb it then. reply hinkley 13 hours agorootparentprev> highest or second-highest fatality Dramatic irony? reply seanhunter 11 hours agoprevFor reference, Annapurna Interactive are not a mainstream publisher churning out franchise type games, but tend to publish unique games, some of them considered classics: - Outer Wilds - Stray - Lorelei and the laser eyes - Cocoon - Neon White ...and others. https://store.steampowered.com/publisher/annapurnainteractiv... This is a real shame for anyone who cares about really distinctive, original games. reply orlp 10 hours agoparent> This is a real shame for anyone who cares about really distinctive, original games. Is it though? Annapurna is a publisher, they didn't make those games. The games you listed were made by: - Outer Wilds: Mobius Digital - Stray: BlueTwelve Studio - Lorelei and the laser eyes: Simogo - Cocoon: Geometric Interactive - Neon White: Angel Matrix I'm confident that creators of distinctive, original games will be able to find other publishers. reply karlgkk 10 hours agorootparent> I'm confident that creators of distinctive, original games will be able to find other publishers. That confidence is (partially) misplaced. It's a really rough world out there for indie devs, and if you do find a publisher, many times they are not interested in paying as much attention to your title as you are. reply seanhunter 6 hours agorootparentExactly, and if you take stray or Neon White as examples, part of the reason they got so much attention was the reputation that Annapurna Interactive had gained by publishing a game as excellent as Outer Wilds. So people took much more notice of those games than they might have done coming from some random indie publisher with no track record. As excellent as those games are, this might have made a very big diference to the traction they got at the start. reply ekianjo 10 hours agorootparentprevthey can self publish then. thats always an option. reply karlgkk 6 hours agorootparentThat’s definitely one of those easier said than done things, that someone who’s never tried to do will think. Publishers have benefits, there’s a reason some game, developers, and other type of media creators will eagerly sign away a big chunk of their revenue. Not true for every media or genre, but there’s a reason these companies exist. reply ekianjo 6 hours agorootparentof course publishers bring benefits. but lets not pretend self publishing is not a thing in the games industry. reply october8140 9 hours agorootparentprevThey can’t self fund. reply ekianjo 9 hours agorootparentmost publishers dont give money without seeing a fairly elaborate demo. And making games is something that can be done with zero budget unlike endeavors in many other industries. reply delusional 8 hours agorootparentOuter Wilds was in production for 7 years. Practically nobody can fund years of full time work without some external funding. I don't have any proof, but I am confident that Annapurna poured some amount of money into that venture during those 7 years. reply keyringlight 7 hours agorootparentIIRC Mobius Digital got their initial money for Outer Wilds from crowdfunding after a much more limited demo of what they wanted to achieve, then I assume Annapurna came on the scene later when they went professional and needed support for multi-platform. The period where kickstarter was more commonly accepted for game projects is over, and fewer projects could get a similar starting point that lets them spend years on their first release reply ekianjo 6 hours agorootparentprevmost games dont take 7 years to make so making a point with outliers is not a very good argument. reply delusional 5 hours agorootparentMost games aren't good. If we don't talk about the outliers we'll be stuck talking about roblox maps, king.com games, or assassins creed. Everything Annapurna interactive funded was an outlier, that's what made it interesting. reply jccalhoun 6 hours agorootparentprevI didn't know that. I was wondering how a team of only 25 people could make so many games. This makes it less of a big deal in my mind. Sure the publisher can be important in providing funding for games and which games get a marketing budget but they still aren't making them. reply Wowfunhappy 6 hours agorootparentprevSimogo is fully owned by Annapurna. Edit: Oops, no they're not, see below. But it sounds like they're contractually prevented from working with a different publisher. reply jccalhoun 6 hours agorootparentIt looks like they just have an exclusive publishing agreement with them not owned by them? At least that's what wikipedia says? https://en.wikipedia.org/wiki/Simogo reply Wowfunhappy 5 hours agorootparentOh, you know what, I'm completely wrong. I was remembering this blog post from years ago: https://simogo.com/2020/03/10/simogo-annapurna-interactive-t... In my memory, this post said \"we're now owned by Annapurna.\" But I read it again and it literally says the opposite. But yeah, it sounds like they can't just switch publishers. reply johnnyanmac 11 hours agoparentprevDepending on the team that walked, this may be a new opportunity. Sounds like this all happened because they worked pretty well without Ellis, but Ellis would come in every now and then and cause a tornado of everything when she did. There's a name for this kind of manager, but it eludes me. Shame they lose the name, but the talent is what makes the company. And I'm sure any small dev will be following them closely on their next venture. reply sitharus 10 hours agorootparentYou’re thinking of seagull management, managers who “flew in, made a lot of noise, dumped on everyone from a great height, then flew out again, leaving others to deal with the consequences”. That’s the polite version anyway. https://en.wikipedia.org/wiki/Seagull_management reply rwmj 10 hours agorootparentprevJoel Spolsky called it \"hit and run management\": https://www.joelonsoftware.com/2000/03/23/command-and-conque... reply plg94 4 hours agorootparentprevhttps://en.wikipedia.org/wiki/Seagull_management : come in, make a lot of noise, shit everywhere, leave reply UberFly 11 hours agorootparentprevAs if the \"team\" will exit together and re-establish as one unit on the other side for the greater good. reply johnnyanmac 10 hours agorootparentHow would you define \"the greater good\" for a publisher of small-medium scaled games? reply Veserv 10 hours agorootparentprevAh yes, the Elon Musk special. The term you are looking for is “pigeon CEO”[1]: “he comes, shits all over us, and goes”. [1] https://electrek.co/2024/04/22/elon-musk-pigeon-ceo-former-t... reply Sakos 11 hours agorootparentprevThe benefit was the funding that could be provided by Ellis, no? How are they going to make up for that if they make a new company? reply johnnyanmac 10 hours agorootparentThat's definitely the million dollar question (literally). I don't imagine they got all 25 or so staff to resign at once if they didn't have a plan for funding. But there weren't any details about that aspect from what I read (could have missed it). I imagine well get more developments over the coming weeks on that. reply plg94 4 hours agoparentprev> considered classics Great games, but all you listed are barely 3 years old – can something that young already be considered a \"classic\"? reply ekianjo 10 hours agoparentprevStray is not made by Annapurna. Its just published by them. Devs can easily find other alternatives. Publishers are everywhere. reply lofaszvanitt 9 hours agoprevIn gamedev, the only thing that matters, once you have a good product is how to market it. Without exposure your project is dead, no matter the quality of it. Who will talk/write about it and for how much? Yters have steep pricing, and they usually don't give a F about your project. That's why Steam and other similar platforms are a trap. And yet, people protect/love them, like it's the 8th wonder of the world. reply dxuh 8 hours agoparentI feel like actually good indie games have a much better chance, because from all I have heard the Steam algorithm actually wants to make money. It will show the game to a bunch of people regardless of the number of wishlists and if it does well, it will show it to more people. Of course Steam can not even do half the work for you, but it's still a much better system than just needing to have enough followers on Twitter or knowing someone at Kotaku or IGN. reply lofaszvanitt 2 hours agorootparentHow, when you have to compete with thousands of games from the same genre? How do you find the outlier amongst the mediocre stuff? Who takes the time to fish out the real gems among the moissanites? reply saghm 5 hours agoparentprev> That's why Steam and other similar platforms are a trap. And yet, people protect/love them, like it's the 8th wonder of the world. Sure, if you only look at it from the developer perspective. From the player perspective, having a centralized location to find and download games is massive. I'm not saying there aren't tradeoffs, but looking at it from only one side ignores all of the reasons that it's successful in the first place. reply antimemetics 9 hours agoparentprevI’ll take YouTube and Steam over the olden days of game „journalism“ any day. reply penguin_booze 7 hours agoprevI'm wondering what the genesis of the name Annapurna is. Clearly, it has some Indian grains to it. Pun intended? Maybe. reply titanomachy 6 hours agoparentIt’s the name of a famous mountain in Nepal which is a popular tourism and climbing destination. reply thoroughburro 6 hours agorootparentThis is the often given answer, but is wrong. The company, the mountain, and many other things are named after a Hindu god of food and nourishment. Don’t just mimic the first search result (check its etymology section, instead). reply btown 14 hours agoprev\"You need to think of [Megan] Ellison the way you think of a lawnmower. You don't anthropomorphize your lawnmower, the lawnmower just mows the lawn, you stick your hand in there and it'll chop it off, the end. You don't think 'oh, the lawnmower hates me' -- lawnmower doesn't give a shit about you, lawnmower can't hate you. Don't anthropomorphize the lawnmower. Don't fall into that trap about [Annapurna].\" -- Bryan Cantrill, on the sins of the father, heavily editorialized. https://youtu.be/-zRN7XLCRhc?t=38m24s reply lofaszvanitt 9 hours agoparentHe talks about Larry Ellison. reply mkl 9 hours agorootparentYes, btown says that: \"Bryan Cantrill, on the sins of the father, heavily editorialized\". reply shakow 9 hours agorootparentprevThat's why GP used brackets to denote the adaptation of the quote by changin the first name. reply kelnos 9 hours agorootparentThat's not how I read it. Often square braces are used to denote a -- factual -- addition to a quote, in order to add information that would otherwise make the quote incomplete or confusing to the reader. Square braces around \"Megan\" doesn't mean \"it was actually someone else and I'm taking creative license with the quote\". reply diziet_sma 7 hours agorootparentIf I didn't know the original quote I would have thought the same. It would be clearer to use L̶A̶R̶R̶Y̶ Megan Ellison reply karaterobot 5 hours agorootparentprevTrue, but they did link to the exact timestamp of the original quote, so it's not like they were hiding anything. The only people apt to be tricked are people who make firm conclusions without checking the sources first. reply kelipso 5 hours agorootparentYou don't need to form firm conclusions about anything to get tricked by an internet comment. Pretty much everyone reading this would get the wrong idea about the quote. No one is going to the source and forming \"firm conclusions\" for some random quote. reply karaterobot 9 minutes agorootparentTo be candid, I couldn't think of a polite way to say you'd have to be credulous to believe that's what the quote was about, and irresponsible not to double-check before posting about it. reply ethbr1 13 hours agoparentprevThis seems like a very different scenario -- don't be part of a hobby business that's so low on the owner's radar that they aren't willing to bleed for it. Because if they lose interest and everything explodes, you're out of a job and they're still a billionaire. reply johnnyanmac 11 hours agorootparentso, 99.9% of all businesses? passion or not a company will gut your position if it means more money or power. But yes, this is an extreme scenario. it sounds (crudely put) like Ellison saw the pandemic, completely retreated for a while, then decided to come back and act like nothing changed and just shuffeled stuff around. Then when her labor decided they wanted to be more independent she sat on her hands. I'm just proud the workers for a rare time could actually just completely walk away. Many issues would be solved almmost overnight if workers could coordinate a joint walkout like this. reply karlgkk 10 hours agorootparentThe majority of businesses are not run by billionaires, in fact most of them are more tightly affiliated than the average large tech company. It's not nearly as simple as you're making it out, and those of us that work in large capital industries (tech), have a very different relationship than the majority of workers in America. I'm not saying we don't share problems, or that they don't have their own issues that are worse, just that your view doesn't reflect the total of reality reply johnnyanmac 10 hours agorootparentwell I can't prove my notions, so I suppose we're at a stalemate. All I can say is I never felt like much more than a number in any company I worked at that had more than, say, 10 people on staff. So whether they want to tank the company or are pouring 110% of their soul into the product doesn't seem to trickle down here to my level. reply ethbr1 3 hours agorootparentThe relevant metric would be corporate value/market cap to owner net worth. reply neonsunset 13 hours agoprevUnfortunately, with the recent coverage of Concord by IGN, I don't think any content they put out is to be taken seriously. reply hinkley 13 hours agoprevDoesn’t seem to be much information here except maybe that the next “Runic Games” but with Annapurna alumni will be called Verset. reply johnnyanmac 11 hours agoparentyou get the entire story of a collapse of one of the most renowned indie game publishers over the course of ~7 years, a situation where the entire staff of the house walks out, and all you get out of this is \"well we'll see what game they release next\"? It's a long story, so if you are just wondering about their portfolio, you can skip to the end section \"The Future of Annapura\". reply cen4 13 hours agoprev [–] There is more content being produced than there are eyeballs and time to consume it all. How does such an industry survive? In a very round about manner that has nothing to do with what they say they do. General rule in life - Don't focus on the drama a fucked up environment produces. reply johnnyanmac 11 hours agoparent> How does such an industry survive? In a very round about manner that has nothing to do with what they say they do. can you clarify this statement? I just read this as \"companies need to make money, creatives aren't focused on money\". > Don't focus on the drama a fucked up environment produces. if you somehow found this politics/drama free work environment with no creative tensions whatsoever, congratulations I suppose. Most of us don't have that and are only one whim away from those \"dramas\" costing us our jobs, even if we keep our heads down. reply rcxdude 11 hours agoparentprev> There is more content being produced than there are eyeballs and time to consume it all. There's more than one person can ever experience. But the ratio of production to consumption is still quite small. Most stuff put on the internet is going to be seen by someone (e.g. 95% of youtube videos have at least one view). reply cen4 9 hours agorootparentCheck out the UN report on the Attention Economy. They quote a study that says less than 0.05% of content is consumed by humans. Obviously it has to come from the UN cause such reports wont come from within the entertainment industry which lives in a disconnected la la land. reply Sakos 11 hours agoparentprev [–] This affects gamers who want indie games worth playing. So, yeah, why wouldn't we care? They're at least partly responsible for a handful of the best games I've played in the past decade. Their loss can't be understated. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In early September, the entire staff of Annapurna Interactive resigned due to internal disagreements, chaotic leadership changes, and communication breakdowns.",
      "Key figures like Nathan Gary and James Masi were abruptly let go, leading to confusion and further resignations, and a proposed spin-off company, Verset, fell through, worsening tensions.",
      "By the end of August, all 25 employees, including key leaders, resigned, leaving Annapurna Interactive scrambling to manage ongoing projects, though they are working to stabilize operations and fulfill commitments."
    ],
    "commentSummary": [
      "Annapurna Interactive, known for unique games like \"Outer Wilds\" and \"Stray,\" faced a mass staff resignation due to management issues with Megan Ellison.",
      "Despite Annapurna's reputation, the independent developers behind its games may seek new publishers, underscoring the challenges indie developers face in funding and marketing.",
      "The situation brings attention to the difficulties within the indie game development community, even when associated with a well-known publisher."
    ],
    "points": 106,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1726887927
  }
]
