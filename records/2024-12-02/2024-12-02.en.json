[
  {
    "id": 42296067,
    "title": "Intel announces retirement of Pat Gelsinger",
    "originLink": "https://www.intel.com/content/www/us/en/newsroom/news/intel-ceo-news-dec-2024.html",
    "originBody": "News December 2, 2024 Contact Intel PR Follow Intel Newsroom on social: More Corporate News SANTA CLARA, Calif., Dec. 2, 2024 – Intel Corporation (NASDAQ: INTC) today announced that CEO Pat Gelsinger retired from the company after a distinguished 40-plus-year career and has stepped down from the board of directors, effective Dec. 1, 2024. Intel has named two senior leaders, David Zinsner and Michelle (MJ) Johnston Holthaus, as interim co-chief executive officers while the board of directors conducts a search for a new CEO. Zinsner is executive vice president and chief financial officer, and Holthaus has been appointed to the newly created position of CEO of Intel Products, a group that encompasses the company’s Client Computing Group (CCG), Data Center and AI Group (DCAI) and Network and Edge Group (NEX). Frank Yeary, independent chair of the board of Intel, will become interim executive chair during the period of transition. Intel Foundry leadership structure remains unchanged. The board has formed a search committee and will work diligently and expeditiously to find a permanent successor to Gelsinger. Yeary said, “On behalf of the board, I want to thank Pat for his many years of service and dedication to Intel across a long career in technology leadership. Pat spent his formative years at Intel, then returned at a critical time for the company in 2021. As a leader, Pat helped launch and revitalize process manufacturing by investing in state-of-the-art semiconductor manufacturing, while working tirelessly to drive innovation throughout the company.” Yeary continued, “While we have made significant progress in regaining manufacturing competitiveness and building the capabilities to be a world-class foundry, we know that we have much more work to do at the company and are committed to restoring investor confidence. As a board, we know first and foremost that we must put our product group at the center of all we do. Our customers demand this from us, and we will deliver for them. With MJ’s permanent elevation to CEO of Intel Products along with her interim co-CEO role of Intel, we are ensuring the product group will have the resources needed to deliver for our customers. Ultimately, returning to process leadership is central to product leadership, and we will remain focused on that mission while driving greater efficiency and improved profitability.” Yeary concluded, “With Dave and MJ’s leadership, we will continue to act with urgency on our priorities: simplifying and strengthening our product portfolio and advancing our manufacturing and foundry capabilities while optimizing our operating expenses and capital. We are working to create a leaner, simpler, more agile Intel.” Gelsinger said, “Leading Intel has been the honor of my lifetime – this group of people is among the best and the brightest in the business, and I’m honored to call each and every one a colleague. Today is, of course, bittersweet as this company has been my life for the bulk of my working career. I can look back with pride at all that we have accomplished together. It has been a challenging year for all of us as we have made tough but necessary decisions to position Intel for the current market dynamics. I am forever grateful for the many colleagues around the world who I have worked with as part of the Intel family.” Throughout Gelsinger’s tenure at Intel across a variety of roles, he has driven significant innovation and advanced not only the business but the broader global technology industry. A highly respected leader and skilled technologist, he has played an instrumental role in focusing on innovation while also creating a sense of urgency throughout the organization. Gelsinger began his career in 1979 at Intel, growing at the company to eventually become its first chief technology officer. Zinsner and Holthaus said, “We are grateful for Pat’s commitment to Intel over these many years as well as his leadership. We will redouble our commitment to Intel Products and meeting customer needs. With our product and process leadership progressing, we will be focused on driving returns on foundry investments.” Zinsner has more than 25 years of financial and operational experience in semiconductors, manufacturing and the technology industry. He joined Intel in January 2022 from Micron Technology Inc., where he was executive vice president and CFO. Zinsner served in a variety of other leadership roles earlier in his career, including president and chief operating officer at Affirmed Networks and senior vice president of finance and CFO at Analog Devices. Holthaus is a proven general manager and leader who began her career with Intel nearly three decades ago. Prior to being named CEO of Intel Products, she was executive vice president and general manager of CCG. Holthaus has held a variety of management and leadership roles at Intel, including chief revenue officer and general manager of the Sales and Marketing Group, and lead of global CCG sales. Forward-Looking Statements This release contains forward-looking statements that involve a number of risks and uncertainties. Words such as \"accelerate\", \"achieve\", \"aim\", \"ambitions\", \"anticipate\", \"believe\", \"committed\", \"continue\", \"could\", \"designed\", \"estimate\", \"expect\", \"forecast\", \"future\", \"goals\", \"grow\", \"guidance\", \"intend\", \"likely\", \"may\", \"might\", \"milestones\", \"next generation\", \"objective\", \"on track\", \"opportunity\", \"outlook\", \"pending\", \"plan\", \"position\", \"possible\", \"potential\", \"predict\", \"progress\", \"ramp\", \"roadmap\", \"seek\", \"should\", \"strive\", \"targets\", \"to be\", \"upcoming\", \"will\", \"would\", and variations of such words and similar expressions are intended to identify such forward-looking statements, which may include statements regarding: our business plans and strategy and anticipated benefits there from; projections of our future performance; projected costs and yield trends; future cash requirements, the availability, uses, sufficiency, and cost of capital resources, and sources of funding, including for future capital and R&D investments and for returns to stockholders, such as stock repurchases and dividends, and credit ratings expectations; future products, services, and technologies, and the expected goals, timeline, ramps, progress, availability, production, regulation, and benefits of such products, services, and technologies, including future process nodes and packaging technology, product roadmaps, schedules, future product architectures, expectations regarding process performance, per-watt parity, and metrics, and expectations regarding product and process leadership; investment plans and impacts of investment plans, including in the U.S. and abroad; internal and external manufacturing plans, including future internal manufacturing volumes, manufacturing expansion plans and the financing therefor, and external foundry usage; future production capacity and product supply; supply expectations, including regarding constraints, limitations, pricing, and industry shortages; plans and goals related to Intel's foundry business, including with respect to anticipated customers, future manufacturing capacity and service, technology, and IP offerings; plans and goals related to Intel’s product business; expected timing and impact of acquisitions, divestitures, and other significant transactions, including the sale of our NAND memory business; expected completion and impacts of restructuring activities and cost-saving or efficiency initiatives; future social and environmental performance goals, measures, strategies, and results; our anticipated growth, future market share, and trends in our businesses and operations; projected growth and trends in markets relevant to our businesses; anticipated trends and impacts related to industry component, substrate, and foundry capacity utilization, shortages, and constraints; expectations regarding government incentives; future technology trends and developments, such as AI; future macro environmental and economic conditions; geopolitical tensions and conflicts and their potential impact on our business; tax- and accounting-related expectations; expectations regarding our relationships with certain sanctioned parties; and other characterizations of future events or circumstances. Such statements involve many risks and uncertainties that could cause our actual results to differ materially from those expressed or implied, including those associated with: the high level of competition and rapid technological change in our industry; the significant long-term and inherently risky investments we are making in R&D and manufacturing facilities that may not realize a favorable return; the complexities and uncertainties in developing and implementing new semiconductor products and manufacturing process technologies; our ability to time and scale our capital investments appropriately and successfully secure favorable alternative financing arrangements and government grants; implementing new business strategies and investing in new businesses and technologies; changes in demand for our products; macroeconomic conditions and geopolitical tensions and conflicts, including geopolitical and trade tensions between the US and China, the impacts of Russia's war on Ukraine, tensions and conflict affecting Israel and the Middle East, and rising tensions between mainland China and Taiwan; the evolving market for products with AI capabilities; our complex global supply chain, including from disruptions, delays, trade tensions and conflicts, or shortages; product defects, errata and other product issues, particularly as we develop next-generation products and implement next-generation manufacturing process technologies; potential security vulnerabilities in our products; increasing and evolving cybersecurity threats and privacy risks; IP risks including related litigation and regulatory proceedings; the need to attract, retain, and motivate key talent; disruptions to our business, including our retention efforts and relationships with customers, distributors and suppliers, due to the changes in our senior management; strategic transactions and investments; sales-related risks, including customer concentration and the use of distributors and other third parties; our significantly reduced return of capital in recent years; our debt obligations and our ability to access sources of capital; complex and evolving laws and regulations across many jurisdictions; fluctuations in currency exchange rates; changes in our effective tax rate; catastrophic events; environmental, health, safety, and product regulations; our initiatives and new legal requirements with respect to corporate responsibility matters; and other risks and uncertainties described in this release, our 2023 Form 10-K, and our other filings with the SEC. Given these risks and uncertainties, readers are cautioned not to place undue reliance on such forward-looking statements. Readers are urged to carefully review and consider the various disclosures made in this release and in other documents we file from time to time with the SEC that disclose risks and uncertainties that may affect our business. The forward-looking statements in this release are based on management's expectations as of the date of this release, unless an earlier date is specified, including expectations based on third-party information and projections that management believes to be reputable. We do not undertake, and expressly disclaim any duty, to update such statements, whether as a result of new information, new developments, or otherwise, except to the extent that disclosure may be required by law. Tags Corporate About Intel Intel (Nasdaq: INTC) is an industry leader, creating world-changing technology that enables global progress and enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and manufacturing of semiconductors to help address our customers’ greatest challenges. By embedding intelligence in the cloud, network, edge and every kind of computing device, we unleash the potential of data to transform business and society for the better. To learn more about Intel’s innovations, go to newsroom.intel.com and intel.com. © Intel Corporation. Intel, the Intel logo and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.",
    "commentLink": "https://news.ycombinator.com/item?id=42296067",
    "commentBody": "Intel announces retirement of Pat Gelsinger (intel.com)573 points by tybulewicz 5 hours agohidepastfavorite418 comments etempleton 4 hours agoI feel like this is a mistake. Pat's strategy is aggressive but what the company needs. Intel's stock is jumping at this announcement, but I look at it as a bad signal for Intel 18a. If 18a was looking to be a smash hit then I don't think Pat gets retired. If 18a is a success then it is an even more short-sighted decision by the board. What this likely means is two-fold: 1. Intel 18a is being delayed further and/or there are significant issues that will hamstring performance. 2. Pat is/was unwilling to split the foundry and design business / be part of a M&A but the board wants to do one or the other. If 18a is not ready I think the best case scenario for Intel is a merger with AMD. The US Govt would probably co-sign on it for national security concerns overriding the fact that it creates an absolute monopoly on x86 processors. The moat of the two companies together would give the new combined company plenty of time to ramp up their fabs. reply phkahler 4 hours agoparent>> If 18a is not ready I think the best case scenario for Intel is a merger with AMD. Aside from the x86 monopoly that would create, I don't think Intel has much of value to AMD at this point other than the fabs (which aren't delivering). IMHO if Intel is failing, let them fail and others will buy the pieces in bankruptcy. This would probably benefit several other companies that could use 22nm and up fab capacity and someone could even pick up the x86 and graphics businesses. BTW I think at this point the graphics business is more valuable. Even though Intel is in 3rd place there are many players in the SoC world that can use a good GPU. You can build a SoC with Intel, ARM, or RISC-V but they all need a GPU. reply paulpan 3 hours agorootparentCertainly feels like preempting news that Intel 18A is delayed. Restoring Intel's foundry lead starting with 18A was central to Pat's vision and he essentially staked his job on it. 18A is supposed to enter production next year but recent rumors is that it's broken. reply DebtDeflation 3 hours agorootparentThe original \"5 Nodes in 4 Years\" roadmap released in mid 2021 had 18A entering production 2H 2024. So it's already \"delayed\". The updated roadmap has it coming in Q3 2025 but I don't think anyone ever believed that. This after 20A was canceled, Intel 4 is only used for the Compute Tile in Meteor Lake, Intel 3 only ever made it into a couple of server chips, and Intel 7 was just renamed 10nm. reply Lammy 1 hour agorootparenthttps://www.intel.com/content/www/us/en/newsroom/opinion/con... I have next to zero knowledge of semiconductor fabrication, but “Continued Momentum” does sound like the kind of corporate PR-speak that means “people haven't heard from us in a while and there's not much to show”. I also would never have realized the 20A process was canceled were it not for your comment since this press release has one of the most generous euphemisms I've ever heard for canceling a project: “One of the benefits of our early success on Intel 18A is that it enables us to shift engineering resources from Intel 20A earlier than expected as we near completion of our five-nodes-in-four-years plan.” reply lstodd 1 minute agorootparentprev> Certainly feels like preempting news that Intel 18A is delayed. I think at this point no one believes Intel can deliver. So news or not.. reply sitkack 10 minutes agorootparentprevI could see Broadcom picking up x86. reply xbar 4 hours agorootparentprevI'd like to address the aside for completeness' sake. An x86 monopoly in the late 80s was a thing, but not now. Today, there are sufficient competitive chip architectures with cross-compatible operating systems and virtualization that x86 does not represent control of the computing market in a manner that should prevent such a merger: ARM licensees, including the special case of Apple Silicon, Snapdragon, NVIDIA SOCs, RISC-V... Windows, MacOS and Linux all run competitively on multiple non-x86 architectures. reply burnte 3 hours agorootparent> An x86 monopoly in the late 80s was a thing, but not now. Incorrect, we have an even greater lack of x86 vendors now than we did in the 80s. In the 80s you had Intel, and they licensed to AMD, Harris, NEC, TI, Chips & Technologies, and in the 90s we had IBM, Cyrix, VIA, National Semi, NexGen, and for a hot minute Transmeta. Even more smaller vendors. Today making mass market x86 chips we have: Intel, AMD, and a handful of small embedded vendors selling designs from the Pentium days. I believe what you meant was that x86 is not a monopoly thanks to other ISAs, but x86 itself is even more of a monopoly than ever. reply bluGill 1 hour agorootparentI believe in the 80s all those vendors were making the same intel design in their own fab. I don't think any of them did the design on their own. In the 90s some of them had their own designs. reply burnte 5 minutes agorootparentSome were straight second sources but they all had the license to do what NEC, AMD, and OKI did, which is alter the design and sell these variants. They all started doing that with the 8086. There were variants of the 8086, 8088, and 80186, I'm unaware of variants of the 80188, or 80286 although there were still multiple manufacturers, I had a Harris 286 at 20MHz myself. Then with the 386 there were more custom variants of the 386 and 486. In the Pentium days Intel wouldn't license the Pentium design, but there were compatible competitors as AMD also began 100% custom designs that were only ISA compatible and pin compatible with the K5 and K6 lines. Octoth0rpe 3 hours agorootparentprev> An x86 monopoly in the late 80s was a thing, but not now. I think you're off by 20 years on this. In the 80s and early 90s we had reasonable competition from 68k, powerpc, and arm on desktops; and tons of competition in the server space (mips, sparc, power, alpha, pa-risc, edit: and vax!). It wasn't till the early 2000s that both the desktop/server space coalesced around x86. reply bane 3 hours agorootparentThank you for saying this. It's clear that processors are going through something really interesting right now after an extended dwindling and choke point onto x86. This x86 dominance has lasted entire careers, but from a longer perspective we're simply seeing another cycle in ecosystem diversity, specialized functions spinning out of and back into unified packages, and a continued downward push from commoditization forces that are affecting the entire product chain from fab to ISA licensing. We're not quite at the wild-west of the late 80s and 90s, but something's in the air. It seems almost like the forces that are pushing against these long-term trends are focused more on trying to figure out how to saturate existing compute on the high-end, and using that to justify drives away from diversity and vertical integrated cost/price reduction. But there are, long-term, not as many users who need to host this technology as there are users of things like phones and computers who need the benefits the long-term trends provide. Intel has acted somewhat as a rock in a river, and the rest of the world is finding ways around them after having been dammed up for a bit. reply throw0101a 2 hours agorootparentprev> It wasn't till the early 2000s that both the desktop/server space coalesced around x86. A lot of companies killed off their in-house architectures and hopped on the Itanium bandwagon. The main two exceptions were Sun and IBM. reply mort96 3 hours agorootparentprevI believe the \"x86 monopoly\" was meant to refere to how only Intel and AMD are legally allowed to make x86 chips due to patents. X86 is currently a duopoly, and if Intel and AMD were to merge, that would become a monopoly. reply chillpenguin 2 hours agorootparentThis is how I interpreted it as well. The others seem to be arguing due to a misunderstanding of what was said/meant. reply Wytwwww 3 hours agorootparentprevDidn't AMD start making x86 chips in 1982? reply mort96 3 hours agorootparentThat seems correct from some quick Wikipedia reading, but I don't understand what it has to do with anything? reply Wytwwww 2 hours agorootparentThe existence of an imaginary x86 monopoly in the 80s? reply mort96 2 hours agorootparentOh, but xbar's interpretation of the phrase \"x86 monopoly\" is clearly the x86 architecture having a monopoly in the instruction set market. Under that interpretation, I don't really think it's relevant how many companies made x86 chips. I don't think xbar is necessarily wrong, I just think they're interpreting words to mean something they weren't intended so they're not making an effective argument reply Wytwwww 2 hours agorootparentDid x86 have a monopoly in the 80s to begin with? If there is any period when that was true it would be the 2000s or early 2010s. > intended so they're not making an effective argument To be fair I'm really struggling to somehow connect the \"x86 monopoly in the late 80s\" with the remainder of their comment (which certainly makes sense). reply bluGill 1 hour agorootparentx86 didn't have a monopoly, but IBM PC clones were clearly what everyone was talking about and there the monopoly existed. There are lots of different also ran processors, some with good market share in some niche, but overall x86 was clearly on the volume winners track by 1985. reply Wytwwww 33 minutes agorootparent> but overall x86 was clearly on the volume winners track by 1985. By that standard if we exclude mobile x86 has a much stronger monopoly these days than in 1985. Unless we exclude low end PCs like Apple II and Commodore 64. In 1990 x86 had ~80%, Apple ~7%, Amiga ~4% (with the remainder going to lowend or niche PCs) so again not that different than today. reply layer8 3 hours agorootparentprevX86 is more than just the ISA. What’s at stake is the relatively open PC architecture and hardware ecosystem. It was a fluke of history that made it happen, and it would be sad to lose it. reply tadfisher 1 hour agorootparentPCI-e is the culmination of that ecosystem, and like PCI before it, is available on all architectures to anyone who pays PCI-SIG. reply michaelt 24 minutes agorootparentPCIe is great, yes. Sadly with the rise of laptops with soldered-in-everything, and the popularity of android/iphone/tablet devices, I share some of layer8's worries about the future of the relatively open PC architecture and hardware ecosystem. reply seabass-labrax 3 hours agorootparentprevI was only just today looking for low-power x86 machines to run FreePBX, which does not yet have an ARM64 port. Whilst the consumer computing space is now perfectly served by ARM and will soon be joined by RISC-V, if a widely-used piece of free and open source server software is still x86-only, you can bet that there are thousands of bespoke business solutions that are locked to the ISA. A monopoly would hasten migration away from these programs, but would nonetheless be a lucrative situation for Intel-AMD in the meantime. reply saltminer 3 hours agorootparentThe fact that C++ development has been effectively hijacked by the \"no ABI breakage, ever\"/backwards compatibility at all costs crowd certainly speaks to this. https://herecomesthemoon.net/2024/11/two-factions-of-cpp/ There are a lot of pre-compiled binaries floating about that are depended on by lots of enterprise software whose source code is long gone, and these are effectively locked to x86_64 chips until the cost of interoperability becomes greater than reverse engineering their non-trivial functionality. reply ryao 2 hours agorootparentThey had ABI breakage when C++11 support was implemented in GCC 5 and that was extremely painful. Honestly, I still wish that they had avoided it. reply etempleton 3 hours agorootparentprevThis is all very true and why I think a merger between AMD and Intel is even possible. Nvidia and Intel is also a possible merger, but I actually think there is more regulatory concern with NVIDIA and how big and dominant they are becoming. reply Wytwwww 3 hours agorootparentprev> An x86 monopoly in the late 80s was a thing, but not now And then in the 2000s after AMD64 pretty much destroyed all competing architectures and then in the 2010s Intel itself effectively was almost a monopoly (outside of mobile) with AMD being on the verge of bankruptcy. reply cogman10 4 hours agorootparentprevYeah, what both companies would need to be competitive in the GPU sector is a cuda killer. That's perhaps the one benefit of merging Antel can more easily standardize something. reply physicsguy 4 hours agorootparentYou don't get a CUDA killer without the software infrastructure. Intel finally seem to have got their act together a bit with OneAPI but they've languished for years in this area. reply gpapilion 4 hours agorootparentThey weren’t interested in creating an open solution. Both intel and AMD have been somewhat short sighted and looked to recreate their own cuda, and the mistrust of each other has prevented them from a solution for both of them. reply pbalcer 2 hours agorootparentDisclaimer: I work on this stuff for Intel At least for Intel, that is just not true. Intel's DPC++ is as open as it gets. It implements a Khronos standard (SYCL), most of the development is happening in public on GitHub, it's permissively licensed, it has a viable backend infrastructure (with implementations for both CUDA and HIP). There's also now a UXL foundation with the goal of creating an \"open standard accelerator software ecosystem\". reply J_Shelby_J 2 hours agorootparentNeat. Now release 48gb GPUs to the hobbyist devs and we’ll use intel for LLMs! reply pixelpoet 1 hour agorootparentI have to say this is blatant moving of goalposts: the claim being responded to was that Intel weren't interested in developing an open standard. Your point is valid of course, but please let's not muddle up the threads as if they aren't getting anything right. I'm no Intel apologist, but as decades-long OpenCL supporter, in all my commercial software, we URGENTLY need a third player here, and one that people actually support. A770 kicked ass in OpenCL, comparable to an RTX 3070, and the 16GB variant was very affordable - something Nvidia to this day refuses to do because of its AI business. reply redandblack 39 minutes agorootparentprevApple is your savior if you are looking at it as a CPU/GPU/NPU package for consumer/hobbyists. I decided that I have to start looking at Apple's AI docs reply stonogo 1 hour agorootparentprevThis is all great, but how can we trust this will be supported next year? After Xeon Phi, Omnipath, and a host of other killed projects, Intel is approaching Google levels of mean time to deprecation. reply ur-whale 2 hours agorootparentprevActual links to the github would be much appreciated, as well a half-page tuto on how to get this up an running on a simple Linux+Intel setup. reply mepian 1 hour agorootparentThis link should cover everything: https://www.intel.com/content/www/us/en/developer/tools/onea... reply bn-l 9 minutes agorootparentprevWhat’s happening with intel wino? That seemed like their cuda ish effort. reply quotemstr 3 hours agorootparentprevThere are already packages that let people run CUDA programs unmodified on other GPUs: see https://news.ycombinator.com/item?id=40970560 For whatever reason, people just delete these tools from their minds, then claim Nvidia still has a monopoly on CUDA. reply dzdt 2 hours agorootparentAnd which of these have the level of support that would let a company put a multi-million dollar project on top of? reply equestria 2 hours agorootparentWe have trillions of dollars riding on one-person open-source projects. This is not the barrier for \"serious businesses\" that it used to be. reply stonogo 2 hours agorootparentprevThose packages only really perform with low-precision work. For scientific computing, using anything but CUDA is a painful workflow. DOE has been deploying AMD and Intel alternatives in their leadership class machines and it's been a pretty bad speedbump. reply jcranmer 2 hours agorootparentprevThere's already a panoply of CUDA alternatives, and even several CUDA-to-non-Nvidia-GPU alternatives (which aren't supported by the hardware vendors and are in some sense riskier). To my knowledge (this isn't really my space), many of the higher-level frameworks already support these CUDA alternatives. And yet still the popcorn gallery says \"there no [realistic] alternative to CUDA.\" Methinks the real issue is that CUDA is the best software solution for Nvidia GPUs, and the alternative hardware vendors aren't seen as viable competitor for hardware reasons, and people attribute the failure to software failures. reply jdewerd 1 hour agorootparent> There's already a panoply of CUDA alternatives Is there? 10 years ago, I burned about 6 months of project time slogging through AMD / OpenCL bugs before realizing that I was being an absolute idiot and that the green tax was far cheaper than the time I was wasting. If you asked AMD, they would tell you that OpenCL was ready for new applications and support was right around the corner for old applications. This was incorrect on both counts. Disastrously so, if you trusted them. I learned not to trust them. Over the years, they kept making the same false promises and failing to deliver, year after year, generation after generation of grad students and HPC experts, filling the industry with once-burned-twice-shy received wisdom. When NVDA pumped and AMD didn't, presumably AMD could no longer deny the inadequacy of their offerings and launched an effort to fix their shit. Eventually I am sure it will bear fruit. But is their shit actually fixed? Keeping in mind that they have proven time and time and time and time again that they cannot be trusted to answer this question themselves? 80% margins won't last forever, but the trust deficit that needs to be crossed first shouldn't be understated. reply bn-l 5 minutes agorootparentThis is absolutely it. You pay the premium not to have to deal with the BS. quotemstr 1 hour agorootparentprev> alternative hardware vendors aren't seen as viable competitor for hardware reasons, and people attribute the failure to software failures. It certainly seems like there's a \"nobody ever got fired for buying nvidia\" dynamic going on. We've seen this mentality repeatedly in other areas of the industry: that's why the phrase is a snowclone. Eventually, someone is going to use non-nvidia GPU accelerators and get a big enough cost or performance win that industry attitudes will change. reply izacus 3 hours agorootparentprevWhy would you want this kind of increased monopolization? That is, CPU companies also owning the GPU market? reply nemomarx 1 hour agorootparentis it a lot more competitive for Nvidia to just keep winning? I feel like you want two roughly good choices for GPU compute and AMD needs a shot in the arm for that somewhere. reply bionhoward 3 hours agorootparentprevWGSL seems like a nice standard everyone could get behind reply epolanski 2 hours agoparentprev> The moat of the two companies together would give the new combined company plenty of time to ramp up their fabs. Doubt. Neither of the companies is particularly competitive on either processor or GPU architecture nor fabrication. A merger of those entities looks like nothing but a recipe for further x86 stagnation and an even quicker death for the entities involved imho. In particular I cannot see what's good in it for AMD. The fabs have no use/clear path forward. Their processors/gpu either match our outmatch the Intel offering. A Broadcom/Apple takeover of Intel sounds much more reasonable. reply danieldk 41 minutes agorootparentA Broadcom/Apple takeover of Intel sounds much more reasonable. Out of curiosity, what would make Intel interesting to Apple? Apple already acquired Intel's modem business and they have their own CPU and GPU. reply mylies43 30 minutes agorootparentMaybe for the fabs? It might be attractive for Apple to move production state side via Intels fabs but on the other hand I don't think Intels fabs can do what Apple wants reply orenlindsey 3 hours agoparentprevA lot of people on this thread are underestimating how much of a hold Intel has on the chips industry. In my experience, Intel is synonymous with computer chip for the average person. Most people wouldn't be able to tell you what AMD does differently, they'd just say they're a knockoff Intel. Technologically, both companies are neck and neck. But for the average person, it's not even close. reply hedora 3 hours agorootparentMarketing campaigns only go so far. They’ve been riding the “Intel Inside” slogan for 25 years. In the mean time, AMD/ARM already won phones, table and game consoles. Server purchasing decisions aren’t made by everyday people. Intel’s roadmap in that space slipped year for year for at least 10 of the last 15 years. That leaves Intel with the fraction of the non-mac laptop market that’s made up of people that haven’t been paying attention for the last ten years, and don’t ask anyone who has. reply alternatex 1 hour agorootparent>In the mean time, AMD/ARM already won phones, table and game consoles. Don't forget laptops. Intel has been terrible on laptops due to their lack of efficiency. AMD has been wiping the floor with them for years now. 2024 is the first year that Intel has released a laptop chip that can compete in efficiency. I hope Intel continues to invest in this category and remain neck and neck with AMD if we have any hope of having Windows laptops with decent battery lide. reply orenlindsey 57 minutes agorootparentprevRemember, most people don't care as much as you or I. If they're going to buy a laptop to do taxes or web browsing or something, they will probably be mentally biased towards an Intel-based chip. Because it's been marketed for so long, AMD comparatively seems like a super new brand. reply ghaff 34 minutes agorootparentIf they don't buy a Mac, they'll be biased to whatever BestBuy sells them. reply kbelder 27 minutes agorootparentprev>That leaves Intel with the fraction of the non-mac laptop market that’s made up of people that haven’t been paying attention for the last ten years, and don’t ask anyone who has. Evidently, that leaves Intel the majority of the market. reply etempleton 3 hours agorootparentprevPeople miss this. A lot of people will only buy Intel. Businesses and IT departments rarely buy AMD, not just out of brand loyalty, but because of the software and hardware features Intel deploys that are catered to the business market. reply gdwatson 1 hour agorootparentThis is in large part an OEM issue. Dell or HP will definitely have an Intel version of the machine you are looking for, but AMD versions are hit and miss. I think this is partly because big OEMs doubt (used to doubt?) AMD’s ability to consistently deliver product in the kind of volume they need. Partly it’s because of Intel’s historically anticompetitive business practices. reply twoodfin 2 hours agorootparentprevIntel’s board is (or should be!) in exactly the right position to assess whether this dam is springing leaks. (It is.) reply menaerus 3 hours agorootparentprevLast report I read it was ~80% (Intel) vs ~20% (AMD) for PC market. And ~75% (Intel) vs ~25% (AMD) for data center servers. reply Wytwwww 3 hours agorootparent> And ~75% (Intel) vs ~25% (AMD) for data center servers. IIRC their data center CPU revenue was about even this quarter so this is a bit deceptive (i.e. you can buy 1 large CPU instead of several cheaper ones). reply menaerus 2 hours agorootparentFrom https://www.tomshardware.com/pc-components/cpus/amds-desktop... \"When it comes to servers, AMD's share totaled 24.2%\" and \"Intel, of course, retained its volume lead with a 75.8% unit market share.\" reply mandevil 2 hours agorootparentprevI think data center revenue was in AMD's favor because AMD is second (obviously far behind NVidia) and Intel is third in AI accelerators, which both companies include in their data center numbers. So that pushes things in AMD's favor. I think on data center CPU's alone Intel is still ahead. reply hedora 3 hours agorootparentprevMarket share is often measured in install base. reply wtallis 13 minutes agorootparentThose two terms are related but definitely are never interchangeable. Market share is the portion of new sales a company is getting. Install base is the portion of existing in-use products that were from that company. Install base is essentially market share integrated over time, less systems that are discarded or otherwise taken out of service. If market share never changes, install base will approach the same proportions but it's a lagging indicator. reply Wytwwww 2 hours agorootparentprevSure, but if the point is showing how Intel isn't really in such a bad spot as one might think just looking at the install base would be pretty deceiving and semi-meaningless. reply tsunamifury 3 hours agorootparentprevOf a market that is dying between two growing at the edges. Mobile and server clear trump personal compute. This the markets devaluing intel. reply Wytwwww 2 hours agorootparentTo be fair it's not like there is that much profits in mobile either. ARM CPUs are almost a commodity and the margins aren't that great. reply bryanlarsen 2 hours agorootparentprevData center revenue is not just CPU. It includes MI300 et al. So that's why data center revenue can be roughly equivalent between AMD & Intel while CPU revenue is still predominantly Intel. reply hedora 3 hours agorootparentprevFor PC’s that can’t be right. For overall consumer, Windows is at 25.75%, Linux is 1.43% and MacOS is at 5.53%. Ignoring ChromeOS, and assuming 100% of windows and linux is x86 (decreasingly true - the only win11 I’ve ever seen is an arm VM on my mac) and 100% of Mac is arm (it will be moving forward), that puts arm at 20% of the PC market. Interpolation from your numbers puts intel at 64% (with a ceiling of 80% of PC; 25% of consumer computing devices unless windows makes a comeback). https://gs.statcounter.com/os-market-share reply layer8 3 hours agorootparentThere is a common usage of “PC” that excludes Macs, Chromebooks, and the like. It means the x86-based PC platform descendant from IBM PC compatibles, with BIOS and all. reply steveBK123 22 minutes agoparentprevOne argument I've hard in favor of the split is this: If you are AMD/NVDA/other top player, do you want to send your IP to an Intel owned fab for production? At least in theory, a fully independent, split/spun out standalone fab removes this concern. That said - what does Intel have to offer the top players here? Their fabs are being state of the art. And what's the standalone value of post-spin fabless Intel if their chip designs are as behind as their fabs? This certainly presents a conundrum for US policy since we need fabs domestically for national security reasons, but the domestically owned ones are behind. reply tonyhart7 3 hours agoparentprevUS government wouldn't let intel down, this is matter of national security (only grown semiconductor fabs left on US soil) and edge of US tech dominance reply badsandwitch 2 hours agorootparentWhen that happens typically the company starts optimizing for sucking money from the government. From the point of view of the consumer Intel would be finished. reply wing-_-nuts 3 hours agorootparentprevThat's the bet I made after the crash last summer. I think the USG only really cares about the fabs, as we've shown the ability to design better chips than intel's here. Time will tell if I'm right. reply vlan0 3 hours agoparentprev>I feel like this is a mistake. Pat's strategy is aggressive but what the company needs. He's also 63. Has plenty of money to survive the rest of his life. Has eight grandchildren. There's so much more to life than business. What's to say he doesn't want to simply enjoy life with more connection and community to loved ones around him? reply shermantanktop 3 hours agorootparentThat would be a healthy, balanced and long-term-oriented approach. But those who get to the level of CEO are subjected to intense forces that select against those traits. I don’t know much about this guy but it’s reasonable to assume that any C-level exec will hold on to the position for dear life until they are forced out. reply vlan0 2 hours agorootparentHere's more info on Pat. He is not your average CEO. He wrote this book almost 20 years ago. Why do you resort to conversation about assumptions? The Juggling Act: Bringing Balance to Your Faith, Family, and Work https://www.amazon.com/Juggling-Act-Bringing-Balance-Family/... reply jillyboel 1 hour agorootparentPlenty of sociopaths have written bullshit books for fame and money. It doesn't mean anything. Besides, the guy is clearly delusional with all the mentions of faith and god reply steelframe 2 hours agorootparentprev> any C-level exec will hold on to the position for dear life until they are forced out I don't know. Frank Slootman's retirement from Snowflake earlier this year was certainly not celebrated by any significant stakeholders. I'd imagine at some point someone like Frank realizes that they are worth more than Tim Cook, they consider that they're in their mid-60s, and they decide the remaining time they have on earth might be better spent in other ways. Every person in the workforce, no matter how ambitious or how senior, is forced into the calculus of money and power vs. good years remaining. I expect the rational ones will select the balance point for themselves. reply ghaff 42 minutes agorootparentThere are certainly some indications this was something that was not long in the planning. On the other hand, when (solid) financial security is not a subject on the table, it's a lot easier for many folks to let go--especially given that they can probably do as many board or advisor gigs in the industry as they have an appetite for. Or just go on to a new chapter. reply etempleton 3 hours agorootparentprevTrue CEO retirements are announced in advance and do not lead to a strange co-interim CEO situation. reply zarzavat 3 hours agorootparentIt's possible he's sick. Who could know? reply cogman10 4 hours agoparentprev> The moat of the two companies together would give the new combined company plenty of time to ramp up their fabs. I'm not convinced of this. Fabs are incredibly expensive businesses. Intel has failed to keep up and AMD spun off their fabs to use TSMC. There is also ARM knocking at the door for general computing. It's already gaining traction in previously x86 dominated markets. The model for US based fabbing has to include selling large portions of capacity to third party ASIC manufacturers, otherwise I see it as doomed to failure. reply JamesLeonis 3 hours agorootparent> There is also ARM knocking at the door for general computing. It's already gaining traction in previously x86 dominated markets. I know anecdotes aren't data, but I was talking with a colleague about chips recently and he noticed that converting all of his cloud JVM deployments to ARM machines both improved performance and lowered costs. The costs might not even be the chips themselves, but less power and thermal requirements that lowers the OpEx spend. reply cogman10 2 hours agorootparentYeah, my company is gearing up to do the same. We primarily use the JVM so doing the arm switcharoo only makes sense. reply etempleton 4 hours agorootparentprevThey would have at least 5 years to figure it out before ARM becomes viable on desktop assuming there continues to be movement in that direction. There is so little incentive to move away from x86 right now. The latest Intel mobile processors address the efficiency issues and prove that x86 can be efficient enough for laptops. IT departments are not going to stop buying x86 processors until they absolutely are forced to. Gamers are not going to switch unless performance is actually better. There just isn't the incentive to switch. reply cogman10 4 hours agorootparent> There is so little incentive to move away from x86 right now. > IT departments are not going to stop buying x86 processors until they absolutely are forced to. IT departments are buying arm laptops, Apple's. And there is an incentive to switch, cost. If you are in AWS, you can save a pretty penny by adopting graviton processors. Further, the only thing stopping handhelds from being arm machines is poor x86 emulation. A solvable problem with a small bit of hardware. (Only non-existent because current ARM vendors can't be bothered to add it and ARM hasn't standardized it). Really the only reason arm is lagging is because the likes of Qualcomm have tunnel vision on what markets they want to address. reply n144q 3 hours agorootparentLooks like your post is talking about two things -- corporate purchased laptops and AWS instances, which are quite different. About corporate laptops, do you have evidence to show that companies are switching to Macbooks from HP/Dell/ThinkPads? reply cogman10 2 hours agorootparent> corporate purchased laptops and AWS instances, which are quite different. They are similar. Particularly because developing on a corporate hardware with an ARM processor is a surefire way to figure out if the software you write will have issues with ARM in AWS. That's pretty much the entire reason x86 took off in the server market in the first place. > About corporate laptops, do you have evidence to show that companies are switching to Macbooks from HP/Dell/ThinkPads? Nope. Mostly just anecdotal. My company offers devs the option of either an x86 machine or a mac. reply n144q 1 hour agorootparentLots of companies do that, and I wouldn't call it an x86/ARM choice but rather the same old Windows/Mac choice. For Windows, only x86 makes sense for companies with lots of legacy software, and the only choice for Mac is ARM. reply Wytwwww 3 hours agorootparentprev> handhelds from being arm machines is poor x86 emulation Also Qualcomm's GPUs are pretty shit (compared to Intel's and AMDs or Apple's) reply maeil 4 hours agorootparentprev> IT departments are not going to stop buying x86 processors until they absolutely are forced to. Plenty of them are buying Macbooks. It's definitely a small percentage of the worldwide market, but not entirely insignificant either. reply etempleton 3 hours agorootparentYes, but that is because users demand it. And they do so begrudgingly. Users are not going to demand an ARM Windows laptop. reply cogman10 2 hours agorootparentBut will IT departments buy them if they are 100, 200, or $400 cheaper than a competing x86 machine? That's the question that remains to be seen. reply bluGill 52 minutes agorootparentCan those users get all the software they need? Many users who want a mac are told no because some weird software they need doesn't run on it. Others only get a mac because some executive demanded IT port that software to mac. So long as companies have any x86 only software they won't let people switch. Often \"art\" departments get a specific exception and they get to avoid all the jobs that require x86 only software just to run their mac. Of course these days more and more of that is moving the the cloud and all IT needs to a web browser that works. Thus making their job easier. reply cogman10 36 minutes agorootparent> Of course these days more and more of that is moving the the cloud and all IT needs to a web browser that works. Thus making their job easier. This was the point I was going to make. While not completely dead, the days of desktop applications are quickly coming to a close. Almost everything is SAAS now or just electron apps which are highly portable. Even if it's not saas or electron, the only two languages I'd do a desktop app in now-a-days is C# or Java. Both of which are fairly portable. reply rvba 2 hours agorootparentprevIf you need to use Excel at work, you need x86 since Excel for Mac is a gutted toy (MS wants your company yo buy / subscribe to windows too). And google sheets in my opinion is not good for complicated stuff - the constant lag.. reply lotsofpulp 2 hours agorootparentI would bet 95%+ of people who use Excel are not affected by any difference between Excel for macOS versus Windows. reply anthonyskipper 1 hour agorootparentI work in a large enterprise company, have both windows and mac machines, and excel works equally great in both, but more and more excel runs in a browser. We mostly email links to spreadsheets running in cloud. So it really doesn't matter what your OS is any more from an excel perspective, as long as your computer can run a modern browser you are good. reply mcintyre1994 3 hours agorootparentprev> They would have at least 5 years to figure it out before ARM becomes viable on desktop assuming there continues to be movement in that direction. What's this based on? Surely the proportion of desktops that need to be more powerful than anything Apple is doing on ARM is very small. And surely Apple isn't 5 years ahead? reply etempleton 3 hours agorootparentIt is less about the development of ARM on desktop and more about software support. Most apps on Windows are still emulated. Some will not work at all. Games are kind of a mess on ARM. A ton of security software that IT departments require are only going to work x86. Businesses run legacy custom applications designed for x86. Some major applications still run on emulation only and are therefore slower on ARM. Apple can force the transition. It is not so straightforward on Windows/Linux. reply grecy 2 hours agorootparentprev> There is so little incentive to move away from x86 right now Massively lower power consumption and way less waste heat to dispose of. Literally the two biggest concerns of every data centre on earth. reply tucnak 4 hours agorootparentprevHonestly, I wouldn't put it behind IBM to turn it around with POWER revival. They'd been doing some cool stuff recently with their NorthPole accelerator[1], and using 12nm process while at it, indicating there's much room for improvement. It could eventually become a relatively open, if not super affordable platform. There's precedent with OpenPOWER! And not to mention RISC-V, of course, championed by Jim Keller et al (Tenstorrent) but it's yet to blossom, all the while pppc64el is already there where it matters. I say, diversity rules! [1]: https://research.ibm.com/blog/northpole-llm-inference-result... reply Grazester 3 hours agoparentprevIs Intel really in such a dire situation that they need to merge with AMD? AMD has been in troubling situation in the past(some thanks to Intel illegal dealings) yet they managed to survive and they were nowhere near Intel's size. reply izacus 3 hours agorootparentMore importantly, AMD troubles made them refocus and improve their products to the levels we're seeing today. Giving life support to dinosaurs isn't how you create a competitive economy. reply csdreamer7 4 hours agoparentprevPretty much fair game for speculation. The only way this is not bad for the tech industry was if he resigned due to medical or age reasons. That would not be unexpected. Doubtful that is the issue with Intel's track record. Curious when we will know if 18A is competitive or not. > If 18a is not ready I think the best case scenario for Intel is a merger with AMD. The US Govt would probably co-sign on it for national security concerns overriding the fact that it creates an absolute monopoly on x86 processors. The moat of the two companies together would give the new combined company plenty of time to ramp up their fabs. No way other countries would allow that. If A-tel (Amd-inTEL) can not sell to the EU the merger will not happen. reply sokoloff 3 hours agorootparentThis does not seem orderly/planned enough to be simple age-related. reply ghaff 3 hours agorootparentI tend to agree. He's not outside the window where someone might choose to retire. But no named permanent successor? Retiring immediately? Tend to speak to a fairly sudden decision. reply KETHERCORTEX 3 hours agorootparentprev> If A-tel (Amd-inTEL) can not sell to the EU the merger will not happen. What the EU gonna do then? Stop buying computers? Perform rapid continental ARM transition for mythical amount of money? reply mattlondon 3 hours agorootparentJust stop buying new intel chips, and continue buying Arm chips. Its not like every single existing x86 CPU would need to be taken away and destroyed. Apple has made it fairly obvious, even if it was not already with smartphones and chromebooks, that Arm is a viable, realistic, and battle-tested alternative for general purpose computing. Windows 11 even runs on Arm already. It would not happen \"tomorrow\" - this would be years in court if nothing else. This would give Dell/HP/Lenovo/whoever plenty of time to start building Arm laptops & servers etc for the European market. And who knows what RISC-V will look like in a few more years? The EU has done a bunch of stupid anti-consumer shit in tech already (hello cookie warnings that everyone now ignores), so I would not be surprised if this happened. reply Wytwwww 3 hours agorootparentprevHard to imagine it ever coming to that but presumably massive fines? > Perform rapid continental ARM transition for mythical amount of money? And what is Intel + AMD going to do? Not sell CPUs in Europe? reply csdreamer7 3 hours agorootparentprev> What the EU gonna do then? Seize or terminate their patents and copyrights. Issue arrest warrants for criminal evasion. Compulsory licensing of x86 to a European design firm immunized by EU law. > Perform rapid continental ARM transition Yes. Windows is on ARM. Apple is on ARM. AWS and Ampere make decent ARM servers. You have decent x86 user-space compatibility on ARM laptops. That is all users want. I doubt it will cost 'mythical amounts of money'. Most users use a web browser and an office suite. I doubt they will know a difference for a while. reply ghaff 4 hours agorootparentprevPeople who are presumably very well-off financially can retire a tad on the early side for all sorts of reasons or a combination thereof. Certainly he has made some significant course corrections at Intel but the most charitable thing one can say is that they will take a long time to play out. As you say, a merger with AMD seems like a non-starter for a variety of reasons. reply dv_dt 2 hours agoparentprevIf a breakup is in the works for Intel, merger of the foundry side with Global Foundries would make more sense than AMD. Intel's foundries, even in the state they're in would likely be a step up for GF. And given the political sensitiveness, GF already has DoD contracts for producing chips. reply 7speter 16 minutes agorootparentDidn't the US government just give Intel $7b on the condition they don't spin off the foundry business? https://finance.yahoo.com/news/intels-7-86-billion-subsidy-0... reply windexh8er 2 hours agoparentprevNope. Look at what Pat did to VMware. He's doing the exact same thing at Intel. He came in, muddied the waters by hiring way too many people to do way too many things and none of them got done appropriately. Pat is a huge part of the problem. I had the unfortunate pleasure of watching him not understand, at all, VMware's core competency. It was a nightmare of misunderstanding and waste in that company under his leadership. Intel turned into even more of a laughing stock under Gelsinger. I say: good riddance. He burned time, capital and people at both VMware and Intel. He's a cancer as a CEO. reply jamiek88 1 hour agorootparentWhen he came back to Intel I was saying that all this ‘finally an engineer’ in charge stuff was misunderstanding Pat Gelsinger and VMWare was front and center in my thinking that. reply thomasjudge 53 minutes agorootparentprevDo you have more details or a reference for the VMware activity? The wikipedia VMware article is pretty brief reply patrickmoo 2 hours agoparentprevFriends that work at intel said gelsinger and the board have done EVERYTHING wrong in the past four years. From blowing key customer accounts to borderline malfeasance with payouts. It’s also the board that needs to go too for enabling. The merger with amd sounds like the right path. reply nelsoch 1 hour agorootparentMy friends at Intel are essentially saying the same thing. This includes the ones that got laid off- and those that got 'transitioned' to Solidigm. reply UncleOxidant 1 hour agoparentprev> I think the best case scenario for Intel is a merger with AMD I think a merger with Nvidia would be more likely given the antitrust issues that a merger with AMD would bring up. reply ckozlowski 4 hours agoparentprevGiven the push of ARM designs into the desktop and server space, that monopoly doesn't seem to me as much of a danger as it might have a decade ago. I imagine any anti-competitive behavior in x86 would only accelerate that trend. Not that monopolies shouldn't be a concern at all, but my thought is that it's not quite that large of a danger. reply KeplerBoy 4 hours agoparentprevWhy would the US govt allow a merge with AMD? Sure they won't allow Intel to be bought by a foreign company, but surely everyone would much rather see Intel being bought by literally any other company than AMD and Nvidia. reply bee_rider 4 hours agorootparentNvidia makes a lot more sense than AMD; it is better for the market (preserving some competition in x86), and at least Intel does something Nvidia doesn’t. reply jacoblambda 4 hours agorootparentWhat do they do that Nvidia doesn't (and that Nvidia would care about)? They already do networking, photonics, GPUs, high speed interconnects, and CPUs. They are planning on selling their FPGAs (the Altera acquisition) to Lattice. The only things left are their fab ops, thunderbolt/usbc, wifi, and ble. Their fab ops would take over a decade of heavy investment to catch up to TSMC or Samsung and idk if even Nvidia is ambitious enough to take that on. Wifi and BLE could be good additions if they wanted to branch out their mellanox portfolio to wireless. Thunderbolt/USB C also might be worthwhile. But that IP is probably going to be cheaper to buy piecemeal so idk if it's worth it to buy the whole company outright. reply bee_rider 3 hours agorootparentI mean, ARM designs have had some wins lately, but x86 still does quite well in single-thread performance, right? Excluding Apple, because they are magic—Amazon, Ampere, these ARM CPUs make a reasonable pitch for applications that use lots of cores well, but that isn’t every application. reply TheBigSalad 4 hours agorootparentprevx86 CPUs. reply mort96 3 hours agorootparentYeah I wonder if maybe the x86 license is the most valuable art of Intel at this point... reply bee_rider 2 hours agorootparentX86 cores remain pretty good at branchy, lightly threaded codes, right? reply adventured 4 hours agorootparentprevChina and the EU would never allow an Nvidia Intel merger, not under any scenario the US would find acceptable. They'll barely allow Nvidia to acquire anybody at this point, no matter how small. See recent EU response to Run:ai. Intel would be considered 100x worse. reply FredPret 4 hours agorootparentWhy would China and the EU have input on a US merger? reply Cumpiler69 3 hours agorootparentBecause Intel, AMD, etc have offices in EU and China, for sales, distribution and also R&D. If you intend to operate in those markets you need to comply with local regulations. reply adra 3 hours agorootparentprevThe same reason as anything else. If the merger goes ahead with opposition from foreign markets, those markets can impose import tariffs or outright bans. Smaller markets may be ones these combined companies are willing to piss off, but not Europe. Their opposition is defacto a deal killer. reply corimaith 3 hours agorootparentThey literally don't any serious homegrown alternative though, they'd be effectively forfeiting the AI race reply Wytwwww 3 hours agorootparentprev> Why Well they obviously would.. Also EU has promised various significant subsidies to Intel. They obviously has fabs in Ireland and building one in Germany and perhaps even Poland.. reply maeil 3 hours agorootparentprevIf this is a rhetorical question, just make your point instead. If not, look up e.g. Microsoft 's purchase of Activision, both US companies. reply tiahura 3 hours agorootparentprevDo they want to sell pentiums in China or EU? reply jacoblambda 4 hours agorootparentprevMaybe they'll sell Intel to Northrop Grumman /hj reply Cumpiler69 4 hours agorootparentBoeing should buy Intel the way they bought McDonald Douglass. It's gonna be a success, trust me. reply jacoblambda 4 hours agorootparentHey I mean two negatives make a positive right? Can't possibly go any worse than it already is. reply hedora 3 hours agorootparentprevProbably should add GMC in there. I heard they’re building a iOS / android replacement. Think of the vertical integration! I’m picturing a boot-looping cargo plane full of hummers dropping like a stone — doesn’t get much more vertically integrated than that. Think of all the layers they can eliminate. reply etempleton 4 hours agorootparentprevWhat other US companies are equipped and interested in running a giant chip design/fab? NVIDIA and AMD are likely the only two candidates. reply hypercube33 3 hours agorootparentMicron Technology is the only one that comes to mind, but they are more on the memory side of things - the last time they were on level with Intel was in the 90s when they both made DRAM but intel pivoted to processors and networking reply esskay 4 hours agorootparentprevI do not at all think it will happen, nor does it make any sense at all but the rumours of Apple seemingly being interested in buying out Intel dont seem to be going away. I can see them wanting certain parts of the business (GPU mainly) but on a whole it doesn't make a lot of sense. I don't see Intel as a single entity being valuable to any US business really. You're essentially buying last years fall line, theres very little use for Intel's fabs without a huge amount being spent on them to get them up to modern standards. It'll all come down to IP and people that'll be the true value. reply etempleton 3 hours agorootparentApple is interesting. They certainly have the money and I think the idea of fabricating their own chips appeals to Apple, but at then end of the day I don't really think it makes sense. Apple won't want to fab for others or design chips for others. The only way it happens is if it is kept separate kind of like the Beats acquisition. Apple lends some chip designs to Intel and Apple starts fabricating their chips on Intel fabs, but otherwise the companies operate independently. reply bluGill 44 minutes agorootparentprevFord, GM... The big automakers got burned with the chip shortage after COVID (this is their fault, but still they got burned) reply KeplerBoy 4 hours agorootparentprevThere are also options like Texas Instruments or Microchip. Of course far more unlikely than either nvidia or amd, but definitely options. reply JonChesterfield 3 hours agorootparentprevApple is the obvious one. Essentially the only place with both the capital to do it and the extreme vertical integration enthusiasm. AMD hopefully still remembers running a fab. reply Wytwwww 3 hours agorootparentThe only thing that Apple might find even remotely useful are Intel's fabs. The rest of the business would have to be sold ton someone else or closed down (which would never be approved by the government). Even then there is zero indication that Apple would ever want to do their own manufacturing. reply raverbashing 4 hours agorootparentprevThat would be a hard call One of the reasons where Intel \"let\" AMD compete in the x86 space is US Gov requirements for being able to source chips from two vendors at least reply MPSimmons 4 hours agoparentprev>the best case scenario for Intel is a merger with AMD Oh man, the risk in that is extreme. We are moving away from x86 in general, but wow, that's... a big jump in risk. reply chasil 3 hours agorootparentAnd really, AMD spun off Global Foundries. AMD doesn't want to run a fab. Does this mean that Intel's fabs should split for Global Foundries, and the Intel design team should go to AMD? reply monooso 2 hours agoparentprev> ...then I don't think Pat gets retired. This implies that he was pushed out, rather than chose to retire. I can't see anything in the article to suggest this, do you have another source? reply solarkraft 4 hours agoparentprev> I think the best case scenario for Intel is a merger with AMD (…) it creates an absolute monopoly on x86 processors If this happens, couldn’t they force them giving out licenses as a condition? The licensing thing has been such an impediment to competition that it seems like it’s about time anyway. reply ethbr1 4 hours agorootparentIt's a good point, and yes, afaik any redress can be requested as a condition for blessing a merger. reply alganet 4 hours agoparentprevSure, let's create a monopoly around one of the most valuable commodities in the world. What could go wrong? reply steelframe 2 hours agorootparentI think Apple Silicon has shown us that x86 doesn't have the monopoly potential it once had. reply alganet 2 hours agorootparentApple Silicon was designed to be efficient at emulating x86-64. If you take that away, it becomes irrelevant (like many other ARM-based processors that struggle to be a good product because of compatibility). Apple has a promising path for x86 liberation, but it is not there yet. reply alganet 39 minutes agorootparent> One of the key reasons why Rosetta 2 provides such a high level of translation efficiency is the support of x86-64 memory ordering in the Apple M1 SOC. https://www.sciencedirect.com/science/article/pii/S138376212... reply JonChesterfield 3 hours agoparentprevHell no, I don't want the Intel management structures coming over here. Qualcomm is welcome to them. reply bell-cot 4 hours agoparentprev> ...the best case scenario for Intel is a merger with AMD... Why would AMD want a merger? They aren't a charity, and certainly don't need the distraction. reply etempleton 4 hours agorootparentWell, for at least a time they would have the entire x86 market. That is not nothing. Also AMD may want to get back into the fab business. Without competition in x86 why not use Intel's fabs? reply esskay 4 hours agorootparentThey dont need to merge with intel to get the entire x86 market, they'll be getting that anyway if Intel folds. Even if Intel gets bought out, it'll be in pieces. Nobody wants to enter the x86 market, but there may be smaller segmenrs of the business that can help an ARM based business, or someone looking to get into GPU's. reply Wytwwww 3 hours agorootparent> they'll be getting that anyway if Intel folds. Why would Intel \"fold\"? Their revenue is still 2x higher than AMDs... I mean obviously they are not doing great but its silly to say something like that at this point. reply toast0 2 hours agorootparentprev> Nobody wants to enter the x86 market If the ISA patent licenses opened up, that might not be the case. When the topic comes up, it's more about Intel shutting down license transfers, so naturally companies have avoided x86. reply tadfisher 1 hour agorootparentprevThey would technically have no market, because the Intel-AMD X86 license is non-transferable and expires if one party goes out of business. reply KeplerBoy 4 hours agorootparentprevAnd everyone would rush to migrate away from x86. Having a sole supplier for CPUs is a bad strategy. reply KETHERCORTEX 3 hours agorootparentYet everyone is okay with a de-facto single supplier of CUDA capable AI GPUs. reply hedora 3 hours agorootparentThis is only true if “everyone” excludes all of the hyperscalers and the client device manufacturers. reply Wytwwww 3 hours agorootparentprevPerhaps not. But (unlike in this hypothetical situation) nobody besides AMD and Intel can do much about that... reply bell-cot 3 hours agorootparentprevOutside of Nvidia, nobody is okay with that. reply osnium123 3 hours agoparentprevThe mistake Pat Gelsinger made was that he put his faith in the American manufacturing workforce. Very sad. reply LittleTimothy 5 hours agoprevDifficult to see how this is anything other than a failure. I had high hopes when Gelsinger returned, but it seems that BK had done too much damage and Gelsinger didn't really get a grip on things. One of the things I heard that really highlighted the failure of this recovery was that BK had let Intel balloon in all sorts of ways that needed to be pared back and refocused, but head count under Gelsinger didn't just stay static but continued to significantly grow. It's no good giving the same politically poisonous, under-delivering middle management more and more resources to fail at the same job. They really need to clear house in a spectacular way but I'm not sure who could even do that at this point. reply alecco 7 minutes agoparentI would argue there were many good things but not well delivered. The Optane persistent memory should've been revolutionary for databases but Intel just put it out and expected people to do all the software. I'm seeing the same thing now with Intel QAT / IAA / DSA. Only niche software support. Only AWS seems to have it and those \"bare metal\" machines don't even have local NVMe. About 10 years ago Intel Research was publishing a lot of great research but no software for the users. Contrast it with Nvidia and their amazing software stack and support for their hardware. reply this_user 5 hours agoparentprevThey have made too many bad choices, and everyone else has been eating their lunch for the last few years. They are a non-factor in the mobile space where ARM architectures dominate. They are a non-factor in the GPU market where NVDA dominates ahead of AMD. They were focused heavily on data centres and desktop/laptop CPUs where ARM is also increasingly making inroads with more efficient designs that deliver comparable performance. They are still struggling with their fab processes, and even they don't have enough money to make the investment needed to catch back up to TSMC. There is a good reason that even Global Foundries has given up on the bleeding edge some time ago. They are in a deep hole, and it is difficult to see a future where they can restore their former glory in the foreseeable future. reply kllrnohj 4 hours agorootparent> where ARM is also increasingly making inroads with more efficient designs that deliver comparable performance. ARM isn't doing any such thing. Apple & Qualcomm are, though. ARM itself if anything looks weak. Their mobile cores have stagnated, their laptop attempts complete failures, and while there's hints of life in data center it also seems mediocre overall. reply mort96 3 hours agorootparentThis feels a bit pedantic. ARM-the-CPU-architecture is increasingly making inroads with more efficient designs that deliver comparable performance to Intel's x86 chips, thanks to Apple and Qualcomm. ARM-the-holding-company is not doing that, they just make mediocre designs and own IP. reply kllrnohj 2 hours agorootparentThe post I was replying to had 3 other companies mentioned or referred to (INTC, AMD, and NVDA), it seems odd that they'd suddenly have meant ARM-the-ISA instead of ARM-the-company when ISA wasn't otherwise being discussed at all. But even if they meant ARM-the-ISA, that'd still put it in a fragile position when the 2 clear market leaders in the ARM-the-ISA space have no particular allegiance to ARM-the-ISA (Apple having changed ISAs several times already, and QCOM both being active in RISC-V and also being sued by ARM-the-company) reply rwmj 4 hours agorootparentprevDon't forget their \"brilliant\" strategy of suing their own customers. reply dagmx 4 hours agorootparentBeing a customer shouldn’t protect a company from lawsuits. ARM feels they have merit here , just like Qualcomm did when they sued Apple. It’s not that rare in the corporate setting to have suits between companies with otherwise amicable relationships. reply rwmj 2 hours agorootparentThe optics can still be terrible. Qualcomm (or more accurately, Nuvia, the company they acquired) produced some stunning chips with almost unheard of battery life for a laptop, and Arm are suing them to use their own inferior designs. They even tried to have end user laptops recalled and destroyed! There's no world where this looks good for Arm. reply dagmx 2 hours agorootparentThere’s a very clear bias and idolization in this comment of yours which is based on a misunderstanding of the case at hand. ARM aren’t trying to force Qualcomm to use ARMs cores. They’re trying to force them to update the licensing royalties to make up for the (as ARM sees it) licensing term violations of Nuvia who had a design license for specific use cases. The part you reference (using ARM designs) is the fallback if Qualcomm lose their design license. The destruction of the chips is standard practice to request in cases of license and IP infringement . reply kllrnohj 1 hour agorootparentQualcomm already had a design license prior to the acquisition of Nuvia. They were doing custom cores back in the original Kryo days which were an in-house custom ARMv8.0-A design. reply bluGill 22 minutes agorootparentThe terms of that license was specific to what they can do. ARM is claiming it doesn't cover some of the things they are doing. I'm not enough of a lawyer to figure out who is right. reply Workaccount2 4 hours agorootparentprevThe US government wants then making SOTA semiconductors. Biden wanted it and I highly suspect Trump will want it too. reply phkahler 4 hours agorootparentTraditionally if you wanted SOTA semiconductors you'd go to IBM for the technology and then build your own fab. I'm not sure how true that is today but I wouldn't be surprised if it is. reply my123 3 hours agorootparentThat's what Rapidus is doing reply leoc 4 hours agorootparentprevI have to wonder if part of this is Intel wanting a Trumpier CEO so it can retain the favour of the US government, while Trump associates Gelsinger with the CHIPS act which he reflexively hates as a Biden thing. reply kranke155 4 hours agorootparentprevApple saw the writing on the wall and bailed years ago, built their own chips and denied them a large customer. It’s a really bad sign when a customer decides it can out innovate you by internally copying your entire business and production line. reply jayd16 2 hours agorootparentThey haven't made their own fabs. Not yet, anyway. And historically, wasn't this juts an extension of their fight with Samsung in the mobile space more than a rejection of Intel? reply sofixa 4 hours agorootparentprev> It’s a really bad sign when a customer decides it can out innovate you by internally copying your entire business and production line. Not necessarily, at scale, especially Apple's scale, vertical integration can make a ton of sense - for control, innovation, price, risk hedging. reply tiahura 3 hours agorootparentprevlast few years. It's pretty much two decades at this point. reply bell-cot 4 hours agoparentprevOh, yes. They spent too many years as the obvious #1, with a license to print money...when, longer-term, staying there required that Intel remain top-of-league in two profoundly difficult and fast-moving technologies (10e9+-transistor CPU design, and manufacturing such chips). Meanwhile, the natural rot of any large org - people getting promoted for their ladder-climbing skills, and making decisions for their own short-term benefit - were slow eating away at Intel's ability to stay at the top of those leagues. reply rwmj 4 hours agoparentprevIntel needed to be split in two as well, which Gelsinger only half-heartedly did. He split the company into two functions - foundry and design, but didn't take that to its logical conclusion and split up the company completely. reply Wytwwww 3 hours agorootparent> Intel needed to be split in two as well Wouldn't that just pretty much guarantee that the foundry business would fail since Intel wouldn't have any incentives to shift most of their manufacturing to TSMC? The same thing happened with AMD/Global Foundries.. reply mastax 2 hours agorootparentAMD has a big wafer supply agreement with GlobalFoundries, and has since the spinoff. It was exclusive until the seventh WSA in 2019 which allowed AMD to purchase 7nm and beyond from other suppliers (without paying penalties) which was the only reasonable resolution after GloFo cancelled their 7nm fab (which may have been the best thing to happen to AMD). But AMD increased their GloFo orders in May and December 2021 during the chip crunch to $2.1B total through 2025. If you look at the first WSA amendment from March 2011 it includes AMD agreeing to pay an additional $430M if they get some (redacted) node in production in time. Anyway, whatever woes GloFo is facing you can’t blame them on AMD. They had an exclusivity deal for a decade which only got broken when it was no longer tenable and AMD still buys a ton of their wafers. I suppose AMD may have bought more wafers if their products didn’t suck for most of that time but it had nothing to do with shifting production to TSMC which only happened after GloFo gave up. reply jpalawaga 2 hours agorootparentright. so glofo couldn't keep up abandoned the bleeding edge. what's the evidence that intel foundaries, divorced from design, wouldn't suffer the same fate? reply paulpan 4 hours agorootparentprevAgree with OP that Intel was probably too deep into its downward spiral. While it seems Pat tried to make changes, including expanding into GPUs, it either wasn't enough or too much for the Intel board. Splitting Intel is necessary but probably infeasible at this point in the game. Simple fact is that Intel Foundry Services has nothing to offer against the likes of TSMC and Samsung - perhaps only cheaper prices and even then it's unproven to fab any non-Intel chips. So the only way to keep it afloat is by continuing to fab Intel's own designs, until 18A node becomes viable/ready. reply mnau 3 hours agorootparentHe failed on GPU. The product was substandard. That means either he knew and allowed it to happen, which is bad, or he didn't know and allowed GPU division to squander the resources, which is even worse. Either way, it was an adventure Intel couldn't afford. There is a similar story in other areas. reply paulpan 3 hours agorootparentDisagree on \"failed on GPU\" as it depends on the goal. Sure Intel GPUs are inferior to both Nvidia and AMD flagship offerings, but they're competitive at a price-to-performance ratio. I'd argue for a 1st gen product, it was quite successful at opening up the market and enabling for cross-selling opportunities with its CPUs. That all said, I suspect the original intent was to fabricate the GPUs on IFS instead of TSMC in order to soak up idle capacity. But plans changed along the way (for likely performance reasons) and added to the IFS's poor perception. reply 7speter 10 minutes agorootparentThe issue with the GPUs is that their transistor to performance ratio is poor. The A770 has as many transistors as about a 3070ti but only performs as well as a 3060 (3060ti on a good day). So with that, they are outsourcing production of these chips to TSMC and using nearly cutting edge processes (battlemage is being announced tomorrow and will use either TSMC 5 or 4), and the dies are pretty large. That means they are paying for dies the size of 3080s and retaling them at prices of 3060s. reply timschmidt 3 hours agorootparentprev> He failed on GPU. The product was substandard. I will never understand this line of reasoning. Why would anyone expect an initial offering to match or best similar offerings from the industry leader? Isn't it understood that leadership requires several revisions to get right? reply mnau 3 hours agorootparentOh, poor multi billion company. We should buy its product with poor value, just to make it feel better. Intel had money and decades of integrated GPU experience. Any new entrant to the market must justify the value to the buyer. Intel didn't. He could sell them cheap to try to make a position in the market, though I think that would be a poor strategy (didn't have financials to make it work). reply timschmidt 2 hours agorootparentI think you misunderstood me. I wasn't calling for people to purchase a sub-par product, rather for management and investors to be less fickle and ADHD when it comes to engineering efforts one should reasonably expect to take several product cycles. Honestly, even with their iGPU experience, Arc was a pretty impressive first dGPU since the i740. The pace of their driver improvement and their linux support have both been impressive. They've offered some niche features like https://en.wikipedia.org/wiki/Intel_Graphics_Technology#Grap... which Nvidia limits to their professional series. I don't care if they have to do the development at a loss for half a dozen cycles, having a quality GPU is a requirement for any top-tier chip supplier these days. They should bite the bullet, attempt to recoup what they can in sales, but keep iterating toward larger wins. I'm still upset with them for cancelling the larrabee uarch, as I think it would be ideal for many ML workloads. Who needs CUDA when it's just a few thousand x86 threads? I'm sure it looked unfavorable on some balance sheet, but it enabled unique workloads. reply mnau 2 hours agorootparent> I don't care if they have to do the development at a loss for half a dozen cycles, And here is the problem. You are discussing a dream scenario with unlimited money. This thread is about how CEO of Intel has retired/was kicked out (far more likely) for business failures. In real world, Intel was in a bad shape (see margins, stock price ect) and couldn't afford to squander resources. Intel couldn't commit and thus it should adjust strategy. It didn't. Money was wasted that Intel couldn't afford to waste. reply timschmidt 1 hour agorootparentWell, seeing as GPU is important across all client segments, in workstation and datacenter, in console where AMD has been dominant, and in emerging markets like automotive self-driving, not having one means exiting the industry in a different way. I brought up Intel's insane chiplet [non-]strategy elsewhere in the thread as an example where it's clear to me that Intel screwed up. AMD made one chiplet and binned it across their entire product spectrum. Intel made dozens of chiplets, sometimes mirror images of otherwise identical chiplets, which provides none of the yield and binning benefits of AMD's strategy. Having a GPU in house is a no-brainer, whatever the cost. Many other decisions going on at Intel were not. I don't know of another chip manufacturer that makes as many unique dies as Intel, or has as many SKUs. A dGPU is only two or three of those and opens up worlds of possibility across the product line. reply dralley 3 hours agorootparentprevIt's worth mentioning that IIRC the team responsible for the Arc GPU drivers was located in Russia, and after the invasion of Ukraine they had to deal with relocating the entire team to the EU and lost several engineers in the process. The drivers were the primary reason for the absolute failure of Arc. Intel deserves a lot of blame but they also got hit by some really shit circumstances outside of their control. reply mnau 3 hours agorootparentHe was CEO. Chief executing officer. It's literally his job to execute, i.e. fix that stuff/ensure it doesn't happen. Get them out of Russia, poach new devs, have a backup team, delay the product (i.e. no HVM until good drivers are in sight). That's literally his job. This only reinforces my previous point. He had good ideas, but couldn't execute. reply jpadkins 1 hour agorootparentCEO stands for Chief Executive Officer. a chief executive officer, the highest-ranking person in a company or other institution, ultimately responsible for making managerial decisions. Maybe you mean COO? reply rvba 2 hours agorootparentprevExecutive, not executing. On a side note getting people in russia write your drivers sounds a bit insane. Yea lower cost and probably ok quality, but the risks... reply Wytwwww 3 hours agorootparentprev> shit circumstances outside of their control. They chose to outsource the development of their core products to a country like Russia to save costs. How was that outside of their control? It's not like it was the most stable or reliable country to do business in even before 2022... reply kgeist 3 hours agorootparentRussia is reliable when it comes to software engineering. I've met a few guys from Intel Russia, bright folks. The politics, though... reply Wytwwww 2 hours agorootparentIndividual Russian software developers might be reliable but that's hardly the point. They should've just moved them to US or even Germany or something like that if they were serious about entering the GPU market, though... e.g. There are plenty of talented engineers in China as well but it would be severely idiotic for any western company to move their core R&D there. Same applied to Russia. reply kgeist 2 hours agorootparentWell, Intel Russia opened in 2000 back when USA and Russia were on good terms, and Putin was relatively unknown. Sure it was a mistake in hindsight... reply Wytwwww 2 hours agorootparentI doubt they began working on ARC/XE drivers back in 2000. If the entire driver team being in Russia (i.e. Intel trying to save money) was truly the main reason why ARC failed on launch they really only have themselves to blame... reply kergonath 30 minutes agorootparentprevIt had been obvious for quite a while even before 2022. There were the Chechen wars, and Georgia in 2008, and Crimea in 2014. All the journalists and opposition politicians killed over the years, and the constant concentration of power in the hands of Putin. The Ukraine invasion was difficult to predict, but Russia was a dangerous place long before that. It’s a CEO’s job to have a strategic vision, there must have been contingency plans. reply lucianbr 1 hour agorootparentprevI think if you're CEO of Intel, some foresight might be in order. Or else the ability to find a solution fast when things turn impredictibly sour. What did he get a $16mil salary for? reply Wytwwww 3 hours agorootparentprev> He failed on GPU. The product was substandard. Weren't they pretty good (price/performance) after Intel fixed the drivers during the first year or so after release? The real failure was taking so long to ship the next gen.. reply BirAdam 4 hours agorootparentprevThey legally cannot split due to the CHIPS Act. reply rwmj 4 hours agorootparentNo idea if that's true or not, but the CHIPS Act didn't exist when he started as CEO. reply ethbr1 4 hours agorootparentprevNot entirely true. It just requires that Intel retain a 51% stake in any split foundry company. reply CoastalCoder 3 hours agorootparentI wonder about the details of this. For example, could \"Intel A\" continue to own the foundries, split off \"Intel B\" as the owner of the product lines, and then do some rebranding shenanigans so that the CPUs are still branded as \"Intel\"? reply twoodfin 2 hours agoparentprevThey really need to clear house in a spectacular way but I'm not sure who could even do that at this point. An alien from Vega looking at our constellation of tech companies and their leadership might point at an obvious answer… reply tonetegeatinst 2 hours agoparentprevWho/what is BK? Are they the previous person who held Pat Gelsingers position? reply _huayra_ 1 hour agorootparentBrian Krzanich, the CEO before Pat reply paxys 19 minutes agoprevThe CEO of a public company is a glorified cheerleader. Doing the right things is half the job, convincing people you are doing the right things is the other half. Considering Intel's share price dropped 61% under Gelsinger's tenure, no matter the work he did towards the first half, it's pretty clear he thoroughly failed at the second. They desperately need someone who will drive back investor confidence in the company, and fast. reply hintymad 4 minutes agoparent> The CEO of a public company is a glorified cheerleader. I find this view limited. If we looked at the most successful tech CEOs, they all personally drove the direction of products. Steve Jobs is an obvious example. Elon pushes the products of his companies so much that he even became a fellow of the National Academy of Engineering. Jeff Bezos was widely credited as the uber product manager in Amazon. Andrew Grove pushed Intel to sell their memory business and go all in on CPUs. Walton Jr. was famous for pushing IBM to go all in on electronic computers and later the mainframes. The list can go on and on. In contrary, we can see how mere cheerleading can lead to the demise of companies. Jeff Immelt as described in the book Lights Out can be such an example. reply nine_k 9 minutes agoparentprevIntel has no GPU. Intel has no mobile processors / SOCs. These seem to be the drivers of growth nowadays. And their CPUs have hard time competing with AMD now. I'm not sure that the 3 years which Geslinger had were enough to turn the tide. reply dralley 13 minutes agoparentprevJanuary 18, 2022 - \"Intel CEO says AMD is in the rearview mirror and 'never again will they be in the windshield'\" https://www.pcgamer.com/intel-ceo-says-amd-is-in-the-rearvie... reply highwaylights 4 hours agoprevThe market seems to think this is great news. I disagree strongly here, but I can see why traders and potentially the board thought this was the right move. A lot of hate for Pat Gelsinger on Reddit and YouTube from armchair experts who don't really grasp the situation Intel were in or what was needed to turn the ship around, so if he was pushed it seems like it might be to pacify the (not particularly informed) YouTube/gamer community and bump the share price. That's all speculation, though. I'd be interested to see who Intel intends to get to run the company in his place, as that would signal which audience they're trying to keep happy here (if any). reply Aromasin 4 hours agoparentAgreed. My career at Intel coincided with Pat's, although I jumped ship a little earlier. Admittedly this means I probably have a little bias, but based on my hundreds of conversations with Intel vets I do think his business strategy was the right decision. He came on board a company years behind on process, packaging, and architecture technology after years of mismanagement by a largely nontechnical staff, which favoured buybacks and acquisitions over core business practice. He had two routes with the capital available following a cash injection from COVID-19 and the rapid digitization of the workplace - compete with AMD/NVIDIA, or compete with TSMC/Samsung. The only sensible option that would capture the kind of capital needed to turn the ship around would be to become a company critical to the national security of the US, during a time of geopolitical stability, onshoring chip manufacture and receiving support from the government in doing so. He could either compete with competitors at home or those abroad, but not both simultaneously. The thesis makes sense; you've lost the advantage to NVIDIA/AMD, so pivot to become a partner rather than a rival. I don't think it's a coincidence that just a week after Intel finally received the grant from the government, he announced his departure. The CHIPS act was a seminal moment in his career. It makes sense he'd want to see that through till completion. He's 63; now is as good a time as ever to hand over the reins, in this case to a very capable duo of MJ and Zisner (who were always two of the most impressive EVPs of the bunch in my book). reply paulpan 12 minutes agorootparentPat was seemed to understand the criticality of fabrication process lead in today's day and age. Hence his push and decision to invest in IFS, plus to win over the government funding to sustain the effort. In short, a bad or subpar chip design/architecture can be masked by having the chip fabricated on a leading edge node but not the inverse. Hence everyone is vying for capacity on TSMC's newest nodes - especially Apple in trying to secure all capacity for themselves. reply 7speter 2 minutes agorootparentprevI'm not in the industry but from what I gather, i agree with you 100%. Bloomberg published an article on the matter though, but it seems they are reporting that Gelsinger was pushed out by a frustrated board because of \"slow progress.\" This is a real head scratcher to me, even as someone looking in: https://www.bloomberg.com/news/articles/2024-12-02/intel-ceo... reply mrweasel 4 hours agoparentprevThe market isn't really the greatest indicator of anything. Gelsinger has spend three years trying to turn Intel around and the company is only now starting to turn the wheel. It will be at least another three years before we see any results. The market doesn't have a three year patience, three months maybe. I hold no opinion on Pat Gelsinger, but changing the CEO in the middle of ensuring that Intel remains relevant in the long term, seems like a bad move. Probably his plan for \"fixing\" Intel is to slow for the market and the board. Let's see who takes over, if it's not an engineer, then things just became more dangerous for Intel. The interim people are an administrator and a sales person, that does not bode well. reply moh_maya 3 hours agorootparentIIRC, Lisa Su and her team took nearly decade to orchestrate the turn-around at AMD, and they are still a distant second player in GPUs. Expecting Pat Gelsinger to turn around Intel (or any company in an industry with such long development and tech lead times), and replacing him in 3 years - given that he is an engineer with extensive domain and leadership experience - seems - reactive, as opposed to thoughtful and directed. Wonder if they will approach Lisa Su to take the job now :D reply paxys 17 minutes agorootparentprev> and the company is only now starting to turn the wheel How is it starting to turn the wheel? reply etempleton 4 hours agoparentprevBoth of the Co-CEOs have a finance background. I think that is rather telling. They are trying to appeal to Wallstreet and potentially have people that are equipped to manage an M&A deal. reply bee_rider 3 hours agorootparentThe guy that got them into this situation started as an engineer. Swan was a money guy, but he did better than Krzanich. So, I think it is just hard to guess based on somebody’s background how they’ll do. However, IMO: they need somebody like Lisa Su, somebody with more of an R&D-engineering background. Gelsinger was a step in the right direction, but he got a masters degree in the 80’s and did technically hardcore stuff in the late 80’s, early 90’s. That was when stuff just started to explode. Su finished her PhD in the 90’s, and did technically interesting stuff through the computer revolution. It appears to have made a world of difference. reply rsanek 4 hours agoparentprevwhy would the board care about \"pacifying the YouTube/gamer community\"? seems like a very unlikely reason for a CEO to be fired. reply dragontamer 4 hours agorootparentIf anything, the streams are reversed. I'd expect Intel marketing and Public Relations to be paying YouTube Influencers to have a particular opinion, the one most favorable to the board. reply sgerenser 1 hour agoprevLatest story from Bloomberg confirms \"He was given the option to retire or be removed, and chose to announce the end of his career at Intel\" https://finance.yahoo.com/news/intel-ceo-gelsinger-leaves-ch... reply kjellsbells 4 hours agoprevSad to see Gelsinger go, but despite the churn, I don't think Intel is doomed. At worst, per their most recent Q3 results, I see $150Bn in assets and $4Bn in outstanding stock, and I see the US Gov (both incoming and outgoing) gearing up for a long war against China where Intel would be an asset of national importance. My back of envelope calculation says Intel should be about $35 a share (150/4). If they stumble when they report Q4, I think the US Gov will twist some arms to make sure that fresh ideas make it onto the board of directors, and perhaps even make Qualcomm buy them. I do think that Intel need to ditch some of their experiments like Mobileye. Its great (essential) to try and \"rebalance their portfolio\" away from server/pc chips by trying new businesses, but Mobileye hasnt grown very much. reply cedws 40 minutes agoparentInterested in your opinion, since TSMC has fabs in the US now, are Intel still relevant even in the context of a Chinese invasion of Taiwan? reply nxobject 26 minutes agorootparentI imagine it takes a lot behind the scenes - especially priceless professional experience concentrated at HQ - to know how to set up new sites, set the future direction at all levels of the organization/timeframes, etc. etc. etc. What happens to the fabs long-term if leadership from Taiwan is decapitated? reply noahbp 19 minutes agorootparentprevTSMC's US fabs cannot produce enough to replace all that they produce in Taiwan, nor are the US fabs producing a leading-edge node. reply nabla9 4 hours agoprevGelsinger retires before Intel Foundry spin is ready. This means trouble. Intel invested tens of billions into A20 and A18 nodes, but it has not paid off yet. News about yield seemed promising. Massive investments have been made. If somebody buys Intel foundries now, they pay one dollar and take debt + subsidies. Intel can write off the debt but don't get potential rewards. Foundry is the most interesting part of Intel. It's where everything is happening despite all the risk. reply consp 3 hours agoparent> Massive investments have been made. If somebody buys Intel foundries now, they pay one dollar and take debt + subsidies. Intel can write off the debt but don't get potential rewards. You are describing the start of the Vulture capitalist playbook for short term profits and dividents right there, take subsidies and loans and sell everything to pay dividents (or rent back to yourself via a shell company) then let the remaining stuff die and sell of for an aditional small profit. Don't know how it works here but it sure sounds like it. reply nabla9 1 hour agorootparentI'm describing massive investment to fundamental manufacturing infrastructure. Deprecating that capital expenditure takes long time. Exact opposite of vulture capitalism and short determinism. > Don't know how it works here but it sure sounds like it. Thank you for being honest and saying that you are describing how things you don't understand sound for you. reply bcantrill 2 hours agoprevI am amazed -- stunned -- how many people here seem to think that Gelsinger was the right person for the job, but wronged by the people who pre-dated him (BK?!) or the CHIPS act (?!) or other conditions somehow out of his control. Gelsinger was -- emphatically -- the wrong person for the job: someone who had been at Intel during its glory days and who obviously believed in his heart that he -- and he alone! -- could return the company to its past. That people fed into this messianic complex by viewing him as \"the return of the engineer\" was further problematic. To be clear: when Gelsinger arrived in 2021, the company was in deep crisis. It needed a leader who could restore it to technical leadership, but could do so by making some tough changes (namely, the immediate elimination of the dividend and a very significant reduction in head count). In contrast, what Gelsinger did was the worst of all paths: allowed for a dividend to be paid out for far (FAR!) too long and never got into into really cutting the middle management undergrowth. Worst of all, things that WERE innovative at Intel (e.g., Tofino) were sloppily killed, destroying the trust that Intel desperately needs if it is to survive. No one should count Intel out (AMD's resurrection shows what's possible here!), but Intel under Gelsinger was an unmitigated disaster -- and a predictable one. reply hedgehog 1 hour agoparentI don't think you're wrong but the overarching structure of the chip business is very different from times gone by. It's not even clear what \"technical leadership\" should mean. When Intel was the leading edge volume leader just on their own processor line, that gave them a scale advantage they no longer have and that won't come back. They built a culture and organization around thick margins and manufacturing leadership, what we're seeing now looks like everyone from investors to employees searching for anyone who will tell them a happy story of a return to at least the margins part. Without a cohesive version of what the next iteration of a healthy Intel should look like all the cost cutting in the world won't save them. reply nunez 2 hours agoparentprevInterestingly, people were bullish about Gelsinger at VMware too. Many still talk about the glory days with him at the helm despite decisions that IMO significantly harmed the company reply baq 1 hour agoparentprevlet's not pretend BK did any good for the company though, you sound like he did an ok job reply bcantrill 43 minutes agorootparentI definitely have some issues with BK, but it's more that there is another entire CEO between BK and Gelsinger (Bob Swan!) -- and I think it's strange to blame BK more than Swan? reply B1FF_PSUVM 57 minutes agorootparentprevI had forgotten who \"BK\" was , so I dove into https://en.wikipedia.org/wiki/Intel \"On May 2, 2013, Executive Vice President and COO Brian Krzanich was elected as Intel's sixth CEO [...]\" The next paragraph is ominous ... 'As of May 2013, Intel's board of directors consists of Andy Bryant, John Donahoe, Frank Yeary, Ambassador Charlene Barshefsky, Susan Decker, Reed Hundt, Paul Otellini, James Plummer, David Pottruck, and David Yoffie and Creative director will.i.am. The board was described by former Financial Times journalist Tom Foremski as \"an exemplary example of corporate governance of the highest order\" and received a rating of ten from GovernanceMetrics International, a form of recognition that has only been awarded to twenty-one other corporate boards worldwide.' reply jarbus 5 hours agoprevNot an insider, but this doesn't seem good. I more or less agreed with every call Intel's made over the past few years, and was bullish on 18A. I liked that Pat was an engineer. His interim replacements don't seem to have that expertise. Intel wasn't doing great to start, but Pat put it on the path, potentially, towards greatness. Now even that is in major jeopardy. reply mepian 4 hours agoparent>His interim replacements don't seem to have that expertise. MJ has a pretty good reputation inside the company. reply ksec 4 hours agoprevI am perhaps one of Pat's biggest fan. Was in tears when I knew he is back at Intel [1]; >\"Notable absent from that list is he fired Pat Gelsinger. Please just bring him back as CEO.\" - 2012 on HN, when Paul Otellini Retired. >\"The only one who may have a slim chance to completely transform Intel is Pat Gelsinger, if Andy Grove saved Intel last time, it will be his apprentice to save Intel again. Unfortunately given what Intel has done to Pat during his last tenure, I am not sure if he is willing to pick up the job, especially the board's Chairman is Bryant, not sure how well they go together. But we know Pat still loves Intel, and I know a lot of us miss Pat.\" [2] - June, 2018 I am sad to read this. As I wrote [2] only a few hours ago about how the $8B from Chip ACT is barely anything if US / Intel wants to compete with TSMC. Unfortunately there were lot of things that didn't go to plan. Or from my reading of it was out of his control. Cutting Dividends was a No No from Board until late. Big Cut of headcount wasn't allowed until too late. Basically he was tasked to turn the ship around rapidly, not allow to rock the ship too much all while it has leaky water at the bottom. And I will again, like I have already wrote in [1], point the finger at the board. My reading of this is that it is a little too late to save Intel, both as foundry and chip making. Having Pat \"retired\" would likely mean the board is now planning to sell Intel off since Pat would likely be the biggest opponents to this idea. At less than $100B I am sure there are plenty of interested buyers for various part of Intel. Broadcomm could be one. Qualcomm or may be even AMD. I am just not sure who will take the Foundry or if the Foundry will be a separate entity. I dont want Pat and Intel to end like this. But the world is unforgiving and cruel. I have been watching TSMC grow and cheer leading them in 2010 before 99.99% of the internet even heard of its name and I know their game far too well. So I know competing against TSMC is a task that is borderline impossible in many aspect. But I would still wanted to see Pat bring Intel back to leading edge again. The once almightily Intel..... Farewell and Goodbye Pat. Thank You for everything you have done. [1] https://news.ycombinator.com/item?id=25765443 [2] https://news.ycombinator.com/item?id=42293541 reply not_your_vase 4 hours agoprevUnfortunately not surprising, looking at the past year or so. When he took over, I remember the enthusiasm and optimism, not only in business, but in hacker circles also. It's a pity it didn't work out. I wonder if it is even possible to save Intel (not trying to imply that \"if Gelsinger can't do it, than no one can\", just wondering if Intel is just doomed, regardless of what their management does). reply ridruejo 5 hours agoprevI got to meet and interact with Pat a few times while he was the CEO of VMware. I really liked him and his approach to things. He has done the best he could with the hand that was dealt to him. reply hitekker 2 hours agoprevA few years, Pat said Intel had internally rebuilt their \"Grovian execution Engine\". I found those words empty, a far cry from Andy Grove's hard decision to dump memory in favor of microprocessors. Andy Grove's decisions made Intel, Intel, not \"execution\". It's unfortunate the situation is beyond almost anyone's grasp but I wonder if Pat should have talked less, and done more. reply acheong08 5 hours agoprevIs this not a bit too short a time for results to show yet? Turning a ship too many times would just result in it spinning in circles around the same position reply n144q 4 hours agoparentPart of me is wondering if in an imaginary world, these same people are on AMD's board, would Lisu Su have already been fired a few years ago? reply mrandish 1 hour agoparentprev>Is this not a bit too short a time for results to show yet? Pat so suddenly getting \"retired\" like this isn't based on the success or failure of the new foundry nodes. You're correct that they weren't supposed to ship yet anyway. With this news most are expecting they'll be (further) delayed soon, but the real cause of today's action is strategic. Things at Intel are so far gone that there's now no choice but to look at splitting up the company and/or selling/merging key parts off. Pat didn't want to preside over breaking up Intel, he wanted to save the company by shipping the new nodes. This was always a long shot plan which would require the existing businesses to do well and new things like GPUs to contribute while massively cutting costs in other areas. Those things didn't work out. The GPUs were late and under performed forcing them to be sold near break even. The market for desktop and laptop CPUs was much softer than expected for macro economic reasons and, worse, there were massive, delayed death field failures of the last two gens of desktop CPUs. Competitors like AMD generally took more share from Intel faster than expected in other markets like data center. The big layoffs announced last Summer should have been done in 2021. Those things combined caused time and money to run out sooner than the new nodes could show up to save the day. This is reality finally being acknowledged. Frankly, this should have happened last Summer or even earlier. Now the value has eroded further making it even harder to save what's left. reply BadHumans 4 hours agoprevPat returned as CEO in 2021. I don't think that 3 years is enough time to clean out the internal rot that has led Intel to where it is today and a lot of problems have been in motion for a while. Short-term this might be better for Intel but this move lacks long term vision. Intel also just got a big hand-out from the government that would've kept them moving in the right direction regardless of what CEO is leading the pack. reply tiahura 2 hours agoparentHow long would you give him to run it into the ground? reply BadHumans 1 hour agorootparentI'm not sure but from the outside it seems his biggest sin was not clearing out the executives that let the rot get to this point to begin with. reply neverartful 4 hours agoprevIntel has had some big failures (or missed opportunities) over the years. Just going from memory - Pentium 4 architecture, not recognizing market opportunities with ARM, Itanium, AMD beating them to 64 bits on x86, AMD beating them to chiplets and high number of PCIe lanes with EPYC, poor battery life (generally) on laptops. The innovations from Apple with Apple Silicon (ARM) and AMD with EPYC make Intel look like they're completely lost. That's before we even touch on what RISC-V might do to them. It seems like the company has a long history of complacency and hubris. reply honkycat 3 minutes agoprevThe Intel story is very simple folks: they spent all of their cash on stock purchases instead of upgrading equipment. reply RandyOrion 3 hours agoprevFrom a customer's perspective: NO, I don't like the BIG-little core architecture of CPUs on desktop platforms, and I don't enjoy the quality issues of the 13-14th gen CPUs; I also don't like the lack of software support for middling performing GPUs. I used to like intel's SSD, but the division was sold. reply hedora 3 hours agoparentThey were just rebranded micron ssd’s the last time I checked. reply wtallis 1 minute agorootparentThey weren't. Intel and Micron used to co-develop the flash memory but had different strategies for SSD controllers, and Intel did a full generation of flash memory after breaking up with Micron and before selling their SSD business to Hynix. reply 627467 4 hours agoprevThese large troubled incumbents seems to have infinite lives to linger on towards a slow death path destroying value in their journey. Like Boeing, why does intel hangs around taking up space, opportunities, resources, one failed attempt after another failed attempt instead of making space for newer ideas, organizations? at this point is so clear these public companies are just milking the moat their predecessors built around them and the good will (or is it naive will) of their new investors who continue to pour money buying out the ones jumping ship reply chang1 4 hours agoparentReminds me of Jobs quoting Gil Amelio at D5[1]: > Gil was a nice guy, but he had a saying. He said, 'Apple is like a ship with a hole in the bottom, leaking water, and my job is to get the ship pointed in the right direction.' [1]: https://youtu.be/wvhW8cp15tk?t=1263 reply chvid 4 hours agoprevGelsinger seemed well connected to Washington / the democratic administration in particular and the CHIPS act seemed to be designed to save/bail out Intel. Perhaps this is a fall out from the election. reply fodkodrasz 4 hours agoparentIMO Intel is far more of a strategic asset to allow such short-sighted policy from whatever administration. The upcoming administration surely knows that, and as far as I know Intel has never made strong steps that would alienate the coming administration from them. Also getting it back on its feet is actually inline with the narrative, it is easy to give it such spin, even if some personal",
    "originSummary": [
      "Intel's CEO Pat Gelsinger announced his retirement after over 40 years, effective December 1, 2024, with David Zinsner and Michelle Johnston Holthaus serving as interim co-CEOs.",
      "The board is committed to maintaining product leadership and efficiency, with Frank Yeary as interim executive chair during the search for a new CEO.",
      "Intel remains a leader in semiconductor technology, inspired by Moore’s Law, with a focus on innovation and leadership continuity."
    ],
    "commentSummary": [
      "Intel CEO Pat Gelsinger's retirement has prompted speculation about the company's future, with opinions divided on whether his aggressive strategy was beneficial or indicative of underlying issues.",
      "Discussions have emerged about potential mergers, including a controversial suggestion of merging with AMD, which raises concerns about creating a monopoly.",
      "The announcement has elicited mixed reactions, with some viewing it as a mistake and others considering it a necessary change for Intel's progress."
    ],
    "points": 573,
    "commentCount": 418,
    "retryCount": 0,
    "time": 1733146662
  },
  {
    "id": 42290448,
    "title": "What Will Enter the Public Domain in 2025?",
    "originLink": "https://publicdomainreview.org/features/entering-the-public-domain/2025/",
    "originBody": "Support PDR Essays Collections Explore Shop About Blog EXPLORE OUR PRINTS STORE Archival inks on premium art paper. Unframed or framed ready to hang. All profits back into the project. SHOP NOW FREE SHIPPING To US, UK, EU, Canada, and Australia. What Will Enter the Public Domain in 2025? A Festive Countdown At the start of each year, on January 1st, a new crop of works enter the public domain and become free to enjoy, share, and reuse for any purpose. Due to differing copyright laws around the world, there is no one single public domain — and here we focus on three of the most prominent. Newly entering the public domain in 2025 will be: works by people who died in 1954, for countries with a copyright term of “life plus 70 years” (e.g. UK, Russia, most of EU and South America); works by people who died in 1974, for countries with a term of “life plus 50 years” (e.g. New Zealand, and most of Africa and Asia); films and books (incl. artworks featured) published in 1929 for the United States. In our advent-style calendar below, find our top pick of what lies in store for 2025. Each day, as we move through December, we’ll open a new window to reveal our highlights! By public domain day on January 1st they will all be unveiled — look out for a special blogpost from us on that day. (And, of course, if you want to dive straight in and explore the vast swathe of new entrants for yourself, just visit the links above). 3 22 10 6 17 29 23 27 28 31 12 2 Graham Greene’s The Man Within 4 1 Virginia Woolf’s A Room of One's Own 8 11 14 18 26 16 21 5 13 24 30 25 9 7 20 15 19 Check out John Mark Ockerbloom’s own “Public Domain Day Countdown” on Mastodon, and summarised in his blogpost. Read more about what makes the public domain so important in Communia’s Public Domain Manifesto. Wondering if “bad things happen to works when they enter the public domain”? Wonder no more. The Public Domain Review is registered in the UK as a Community Interest Company (#11386184), a category of company which exists primarily to benefit a community or with a view to pursuing a social purpose, with all profits having to be used for this purpose. We rely on donations from readers. Please consider supporting the project and becoming a Friend of the PDR. About Masthead Contact Submissions Explore PDR Index Sources Further Reading What is the Public Domain? Rights Labelling Reusing Material Privacy Policy Terms and Conditions Essays Collections Conjectures Curator’s Choice Blog Shop PDR Press Prints Shop FAQ Returns Policy Subscribe to Our Newsletter Like many sites, we use cookies — small text files placed on your device used to gather info about how the site is being used and provide basic functionality (such as the shopping cart). This use is minimal and doesn’t identify you as an individual. You can set your browser to not accept cookies, however, be aware that some key aspects of our site may not function as a result. More Info Accept and Close",
    "commentLink": "https://news.ycombinator.com/item?id=42290448",
    "commentBody": "What Will Enter the Public Domain in 2025? (publicdomainreview.org)338 points by Tomte 22 hours agohidepastfavorite276 comments GeneticGenesis 20 hours agoI know it's not public domain per-say, but for me, the thing that's most exciting is that in 2025, the last remaining patents on the h.264 (AVC) video codec will expire [1]. Now if only HEVC wasn't such a hot patent / licensing mess. [1] https://meta.wikimedia.org/wiki/Have_the_patents_for_H.264_M... reply TeMPOraL 2 hours agoparent> Now if only HEVC wasn't such a hot patent / licensing mess. Somehow I suspect HEVC suddenly became a thing in the past few years precisely because AVC patents are expiring. reply harshreality 33 minutes agorootparentEncoding efficiency for a given perceptual quality is very important when you pay for bandwidth or disk space. Otherwise there would have been no effort to create vp9 and av1, as everyone on that side of the codec wars would've stuck with vp8. reply iterance 19 hours agoparentprevJust thought you might want to know - it's \"per se\" not \"per say\"/variations thereof. reply asveikau 19 hours agorootparentPer se is latin for \"for itself\". reply gweinberg 22 minutes agorootparentMaybe word for word, but \"per se\" means \"as such\". reply casta 19 hours agorootparentprevI'd say \"by itself\". reply kspacewalk2 3 hours agorootparentIf you translate it literally, \"per\" is closer to \"for\". If you don't translate it literally, I'd vote for \"in itself\". \"In itself\" (viewed in its essential qualities; considered separately from other things[0]) has a different meaning than \"by itself\" (alone/unaided). And to me it's clear that \"per se\" pretty much universally means the former. [0] https://www.google.com/search?q=in+itself reply grepLeigh 35 minutes agorootparentA less literal translation like \"essentially\" or \"in essence\" is deployed by master Latin translators like Robert Fagles. I've even seen \"in a vacuum\" which does a better job at communicating the original intent than a string of cryptic prepositions. reply asveikau 19 hours agorootparentprevThat's another valid translation for the same preposition. And there are many definitions of English \"for\" as well. This would fit the one used in the phrase \"if not for this, ...\" In other words, for itself = by virtue of itself, through the existence of itself. Also note in terms of Indo European roots, per is a cognate with English for. reply dhosek 17 hours agorootparentPrepositions are some of the least translatable bits of language. For that matter, even without translation they tend to get slippery within a language, especially over time (one that springs to mind is the whole “quarter of” referring to a time which I first encountered some 50 years ago and still don’t know if it’s quarter to or quarter after).¹ ⸻ 1. Cue some dude to tell me in 3…2…1² 2. And this knowledge will promptly disappear from my brain five minutes later, sort of like the guy I knew in my 20s whose name was either Jack or Chad and to this date, I still am not sure, but I do know that every single time I called him by name, I got it wrong and it totally wasn’t on purpose even though he didn’t believe me. reply TeMPOraL 2 hours agorootparent> quarter of Still can't beat stuff like \"bi-weekly\" which can mean \"every two weeks\" or \"twice a week\" or probably some other thing as well. reply lcnPylGDnU4H9OF 1 hour agorootparentAs is often the case, Randall Munroe has already delivered: https://xkcd.com/1602/. Perhaps the joke in this context would be if it said \"bi-weekly\". \"You should come to our Linguistics Club's bi-weekly meeting. Membership is open to anyone who can figure out how often we meet.\" (I mean, you have a 50-50 shot. I wonder if there's any personality insights one could learn from such a selection.) reply zelphirkalt 9 hours agorootparentprevThat's my cue! I once had a Spanish teacher, who also had problems remembering what that kind of time specification stands for and I came up with maybe a trick to remember. We do the same thing in German, so I guess it translates: Lets say you have 11:00. That's easy. But what about 11:15? We would say \"quarter 12\", so I guess the English version is \"quarter of 12\". How to memorize, that this is 11:15? Well, you can imagine a round clock and the minute pointer has moved _quarter of its way to 12_. So you only have a quarter of that hour \"already done\". 10:30? We say \"half 11\". So I guess English is \"half of 11\", meaning that the minute pointer has moved half the way to 11. Maybe this will help. (Actually I personally usually don't use those ways of specifying the time, neither in English nor in German. I just say the 24h format as it is written: \"11:15\" is \"eleven fifteen\", 13:35 is \"thirteen thirtee five\" not 1pm something.) reply card_zero 4 hours agorootparentWhatever \"quarter of 12\" means in English, whether it's 11:45 (quarter to 12) or 12:15 (quarter past 12), it definitely isn't 11:15. We do fraction of an hour forward or backward relative to the hour mentioned, not fraction of an hour elapsed in approach to hour mentioned. I recently encountered a German asking for the English phrase equivalent to bis unter, looking for a phrase like \"up to below\". There isn't one in common use. We just don't count things in equivalent ways. reply monkpit 1 hour agorootparentIsn’t _bis unter_ akin to “just under”, or “right up to”? I feel like either of those could work depending on the context and are common in English. reply jcranmer 1 hour agorootparentprev> Lets say you have 11:00. That's easy. But what about 11:15? We would say \"quarter 12\", so I guess the English version is \"quarter of 12\". The English terms would be: 11:15 -> quarter after 11, quarter past 11 (both pretty rare, tbh) 11:30 -> half past 11 (this is the only form that is moderately common) 11:45 -> quarter of 12, quarter before 12 (also pretty rare) reply tags2k 2 hours agorootparentprevThis definitely doesn't translate - if you say \"half 11\" to a British person you are getting them at 11:30, not 10:30. reply Melatonic 3 hours agorootparentprevThis is not great advice - the only way I have heard it in English would be \"quarter past 11\" to mean 11:15. Most people would just say \"eleven fifteen\". reply zelphirkalt 1 hour agorootparentIf no one says it, did the post above mine just make things up and I am trying to explain their invented things? reply daveguy 23 minutes agorootparentYou were just wrong. They explicitly gave the understood options as 15 before or 15 after. These are the options everyone uses in English -- not 45 before or a quarter of the hour before. In English no one says quarter of 12 to mean 11:15. You just explained it completely different from the ways it is interpreted in English. I understand the logic and how it might come about. Maybe it's very common in German, but it is not used that way in English. If you referred to 11:15 in that way to a native English speaker you would be misinterpreted. reply dghf 4 hours agorootparentprevTok Pisin, a.k.a. New Guinea Pidgin, has exactly two prepositions: bilong, which means \"of\" or \"from\" in a possessive or attributive sense; and long, which means everything else. reply philistine 17 hours agoparentprevThat's incredible. With MP3 already completely patent-free as well, we have an extraordinary free set of audio and video codecs for the next couple of decade, at least until HEVC becomes free. reply yaomtc 12 hours agorootparentLet's not forget Opus. Not technically patent-free but it practically is. https://en.wikipedia.org/wiki/Opus_(audio_format)#Patent_cla... Also Vorbis has always been patent free. reply bobmcnamara 3 hours agoparentprevA tale as old as video codecs. reply walrus01 19 hours agoparentprevOne of the primary reasons why AV1 exists is because HEVC is such a hot mess. reply TiredOfLife 6 hours agorootparentThere are already two patent pools for av1 that want rent. reply nabakin 2 hours agorootparentDo you think AOM is going to start charging royalties? reply TiredOfLife 1 hour agorootparentNo, Sisvel and Avanci are. reply nabakin 1 hour agorootparentHow does that work? They force AOM to pay up and then I guess either AOM passes the royalty burden onto AV1 users or they take the hit and pay it themselves? reply TiredOfLife 25 minutes agorootparentPatent pools want money from companies and end users. reply a1o 17 hours agoparentprevIn the link it seems the last patent in US go as long as 2027? If the patents really expire in 2025, is there an already open source library written either in C or C++ one could use for reading h.264? reply mtlynch 16 hours agorootparentCisco published their implementation under BSD license: https://github.com/cisco/openh264/ reply extraduder_ire 3 hours agorootparentThey also make a reproducible build of it for firefox in order to shield mozilla from patent suits. reply gavinsyancey 2 hours agorootparentprevx264 has been around forever, and it's FOSS. reply Amorymeltzer 20 hours agoprevIt's advent calendar-style, somight be more informative. reply leoc 19 hours agoparentRed Harvest is in there? You can be sure that number of film and TV writers, and perhaps directors and producers, have set their alarm clocks for that one. https://crimereads.com/the-strange-cinematic-afterlife-of-re... reply cxr 19 hours agorootparentYou may be interested to know that the story for The Maltese Falcon, not listed, will also be entering public domain because it was published in monthly installments in Black Mask magazine before it was published as a novel. (Any changes between the publication in Black Mask and those made for the novelization, if significant enough, will of course still be under copyright and so the novelization proper still won't be entering public domain for another 13 months.) We have had difficulty getting our hands on these issues, though (or scans of them). It's interesting that AMC just launched a series featuring Sam Spade this year, the year before the character goes into the public domain… reply qingcharles 15 hours agorootparentWhen you say, \"we\" who do you mean? Are the magazines particularly rare? reply cxr 15 hours agorootparentThose of us trying to get them scanned in and online. Black Mask is pretty rare. Brooks Hefner at James Madison University oversaw the acquisition/accession (I think) of their Black Mask collection, of which they have quite a bit in comparison to other places, but it's still incomplete. The Ransom Center in Austin specializes in rare and collectible items, but only has one or two issues. The Library of Congress may well have a complete collection, but what they do have isn't digitized because the pages are brittle and are literally falling apart (even moreso than other/older printed material). reply blacksqr 5 hours agorootparentGodspeed to all working on this. reply qingcharles 2 hours agorootparentprevI'm guessing you're already on the Pulp Scans group, right? reply GeoAtreides 9 hours agoparentprevNo notable wroters, except one: Pär Lagerkvist reply clarkmoody 13 hours agoprevPrevious civilizations were able to own their cultural myths. Modern civilization's cultural myths are controlled by giant faceless corporations with legions of lawyers. No one can tell a new story about Han, Luke, and Leia without permission from the House of Mouse. reply narski 2 hours agoparentAt least in the case of the Maya, literacy was carefully guarded so that a small class of priests could exercise precisely this kind of control. In fact, this is believed to be one of the reasons why modern Mayan languages are written in the Latin alphabet, even though there's a complete Mayan script that was the most developed writing system in the Americas until the conquest. reply criddell 1 hour agoparentprev> No one can tell a new story about Han, Luke, and Leia without permission from the House of Mouse. Disney is surprisingly friendly to Star Wars fan fiction. https://www.nytimes.com/2021/01/07/movies/star-wars-fan-film... reply sigio 47 minutes agorootparentUntil they are not... it's still permission, instead of 'always allowed'. I'm quite convinced they will not like it if you make a star-wars themed porn-parody (with the character names as-is) and try marketing it ;) reply s1artibartfast 1 hour agoparentprevIn previous civilizations, it was not uncommon to be killed, disemboweled, or crucified for telling myth. I dont think that distant kings, state religions, or crusading armies were more faceless. reply asimpletune 6 hours agoparentprevI agree that the way things were along time ago was more natural and definitely more creative. Things today though are way different though. A huge difference is that media wasn’t an industry back then. Most people couldn’t even read. The retelling of myths would mostly happen in the form of poetry or drama performed publicly, and the performance schedule was tightly controlled in the form of contests and festivals to honor various things across the calendar. There’s not really analogue to that now. We are truly in uncharted territory, and that was the case since the printing press. Throw in the invention of the internet and it’s a giant mess. I’m optimistic though that we can resolve it and pave a way forward. reply Retric 2 hours agorootparentStorytelling was both a profession and an industry in antiquity. Just as rock concerts and bar bands exist today, amphitheaters weren’t the only way people would watch performances. reply immibis 3 hours agoparentprevOr a Tor onion service. reply cxr 19 hours agoprevUnfortunately, public domain isn't everything. There are lots of works that we know of and that we should be able to share with one another, but it's hard to come by copies. To give one example: The novel Red Harvest from 1929 is listed as entering the public domain next month. But the thing is that prior to being published as a novel, it was serialized in Black Mask magazine, and since all installments were published pre-1929, they're all already in the public domain. The trouble, though, is that despite being public domain, actually getting your hands on these issues, whether in real life or figuratively as scans is something that poses a challenge—we simply don't have easy access to this material. And that goes for lots of other stuff that we know about but don't have copies at hand. reply d3VwsX 7 hours agoparentAlso all the sites that post scans of old books or magazines but claim various usage restrictions or/and copyrights on the scans. That effectively keeps public domain works copyrighted, for most purposes, for most of us. Most libraries for instance have some EULA that says you can only use scans you order from them for non-commercial purposes, even if they scan from a public domain book. I do not know if that can be enforced in general, but it would not be fun to have to find out in court. reply RobotToaster 1 hour agorootparentAs far as copyright is concerned, \"slavish reproductions\" do not pass the threshold of originality https://en.wikipedia.org/wiki/Threshold_of_originality#Repro... reply aspenmayer 19 hours agoparentprevIf it’s in the public domain, you can make legal derivative works even without the source material, can’t you? Or can you? reply nemomarx 19 hours agorootparentIf the serialized one is in the public domain and a book isn't, then your work might be seen as derivative of the book anyway. For instance the recent Wicked movie (and book, and play, etc) can't use ruby red slippers, because that was a detail only in the relativley more recent movie, so even though the original wizard of oz book has been in the public domain a while, some details are siloed off. I think the Holmes stories had disputes like this for a while too, where if you mentioned certain side characters it might infringe a more recent adaptation or etc. reply recursivecaveat 18 hours agorootparentThe Holmes situation was even sillier than that: they were arguing that certain personality traits of Sherlock were still under copyright: https://www.denofgeek.com/movies/conan-doyle-estate-sues-net... reply aspenmayer 18 hours agorootparentprevI can see how having the public domain source material might be necessary to defend yourself against claims of infringement of the non-public domain works. That makes sense to me. reply timpark 20 hours agoprevIn May 1998, before the Sonny Bono Copyright Term Extension Act went into effect, there was an amusing Tom the Dancing Bug comic regarding characters falling out of copyright. https://www.gocomics.com/tomthedancingbug/1998/05/17 In that vein, similar to the Mickey Mouse and Winnie the Pooh horror movies that have been released in recent years, Popeye will be entering the public domain next year and people are working on a horror movie based on that. reply saghm 20 hours agoparentEven as someone who isn't really into horror much as a genre, it's hard not to appreciate how one of the first instincts we have as a society when freeing some IP is \"We should make a creepy version of this!\". reply Swizec 20 hours agorootparent> it's hard not to appreciate how one of the first instincts we have as a society when freeing some IP is \"We should make a creepy version of this!\" Almost certainly rule 34 happens first and just gets less attention in public. There were “bear” posters of Pooh all over SOMA (in SF) almost immediately after the copyright ended. reply ben_w 19 hours agorootparentRule 34 happens well before copyright expires. Clopfic, Kirk/Spock, or ask any pile of furries about Robin Hood or Nick Wilde. reply registeredcorn 19 hours agorootparentprevWhat is Soma? I see there is a video game (link)[https://en.wikipedia.org/wiki/Soma_(video_game)], but have no idea what it means in this context. Are you saying there is some kind of add-on for that? reply hollerith 19 hours agorootparentSouth of Market neighborhood of San Francisco. reply registeredcorn 19 hours agorootparentAh, my bad. I think I saw that in one of the results but went off on some other route because I was focused on \"horror\" stuff. Thanks. :) reply fred_is_fred 18 hours agoparentprevI am curious how the long tail and changing of media consumption habits has devalued many of these characters. Mickey is Mickey and there's an entire company built on it of course, but my kids have literally no clue who Popeye is. They have never seen a Popeye cartoon and probably never will. When I was younger we didn't have nearly as much choice (2-3 channels with limited times for cartoons) - rather than Netflix or Youtube which effectively offers unlimited/fragmented options. Additionally the main plot line of Popeye is effectively Popeye protecting Olive Oil from being assaulted by Bluto - not exactly modern cartoon material. reply timpark 14 hours agorootparentI think some things were a product of their time, and weren't popular/profitable enough to keep marketing or update or modernize. (whether they didn't maintain peoples' attention, or had issues like the Popeye one you mentioned) Another not-as-popular character that's entering the public domain in 2026, for example, is Betty Boop. On the other hand, Superman and Batman enter the public domain in 2034 and 2035 respectively, so that should be interesting. Though like Mickey Mouse/Steamboat Willie, I expect that it's only the original version/costume that goes public domain. reply tzs 16 hours agoprevAs is usual there is quite a bit of discussion here on copyright reform, which is mostly just suggestions to change the term of copyright. I think it would be interesting to consider other reforms. Note that copyright is not just a single right. It is a bundle of rights. In the US those are the copying right, the derivative work right, the distribution right, the performance right, the display right, and some others. The bundle of rights might be different in other jurisdictions but in most it is similar. In the rest of this comment I'll only be consideringd US copyright. First, I don't see why all of those rights should all have the same term. I see no reason to believe that the optimal term for say the copying right and the optimal term for the derivative work right would be the same. Second, how about adding more compulsory licenses? US copyright law already has some compulsory licenses (also called mechanical licenses), such as for cover songs. Briefly, a federal agency called the Copyright Royalty Board sets the terms and rates for these licenses, and anyone can obtain the license according to those rates and terms, regardless of whether or not the copyright owner wants to license the work to them. For example suppose we made it so that the copying and distribution rights have a three phase lifetime instead of the current two phases (which are an exclusive phase tied to the author's lifetime followed by public domain). The three phase lifetime could be (1) an exclusive phase of a fixed number of years, followed by (2) a compulsory license phase, followed by (3) public domain. The derivative work right is the hard one. On the one hand a short term allows others to play in an author's universe. I've seen some really good and really well written fan fiction that is not currently technically legal, especially crossover fan fiction that merges the story universes of different authors. Encouraging this would be good. On the other hand some things would be ruined if they became public domain too quickly. I'm quite pleased that Bill Watterson still gets to decide who can make \"Calvin & Hobbes\" derivative works. If copyright was only 14 or 21 years (terms people often suggest), I've no doubt that every character from \"Calvin & Hobbes\" would have started appearing in ads as soon as the copyright expired. reply retrac 3 hours agoprevThanks to the retroactive extension here in Canada, nothing. reply swyx 20 hours agoprevhandpicked selection of notables: - Frida Kahlo - Henri Matisse - Alan Turing i guess the Chrysler Building is public domain now? what can you do with a buidling? reply Rebelgecko 19 hours agoparentKind of a niche, but the open world Spiderman video games stopped including the Chrysler Building due to licensing issues after it was bought by new owners in ~2019 reply jabroni_salad 3 hours agoparentprevThere was some crank that once C&D'd a minecraft server for reproducing copyrighted buildings. I dont think he had the guts to take it to court though. https://www.youtube.com/watch?v=9y2IiZvg1xQ reply diggan 20 hours agoparentprev> i guess the Chrysler Building is public domain now? what can you do with a buidling? I guess maybe the design of the building, if anything? If that will become public domain, expect it to appear in Macau in a year or less, and probably other places :) reply shiroiushi 14 hours agoparentprevYou wouldn't download a building. reply poulpy123 5 hours agorootparentI would if I could ! but my 3D printer is a bit to small reply GeoAtreides 9 hours agoparentprevAlso the writer Pär Lagerkvist, nobel award winner reply nottorp 10 hours agoprevMost are still books in that list. But there is no readable version of it, just photos :) Next year it will be a link to a tiktok movie? reply distantsounds 19 hours agoprevI'd love to know! Too bad this website only gives a small selection of what those items are, despite the entire lists already being published elsewhere! Very useful, thanks Hacker News! reply aaronbrethorst 20 hours agoprevI, for one, am looking forward to A Farewell to Arms and Zombies, inevitably coming out soon. reply gweinberg 20 hours agoparentI guess putting a book on a trash can to trap your possessed severed hand is considered fair use. reply hoseja 6 hours agoprevOh hey look Crowley. It's fascinating to me that like 95% of what anglos see as \"occult\" or \"magic\" is directly descended from that very recent grifter. Also see vampires and Stoker (though that's not as intentional). reply m3kw9 17 hours agoprevHow do people typically make money from these new things reply aucisson_masque 19 hours agoprev70 years feels so long for movie or book. I get it’s important to protect the right of authors and companies but damn it’s 3 generations. Something my grand grand father may have seen, that’s insane. 25 year, a single generation would make sense. I’d argue that by then all the money would have been made and you would allow new generation to grow up with the greatest art from previous generation, it would be like a virtuous circle. Next generation would improve based on previous one And so on. reply ronsor 19 hours agoparentI'm going to designate this as the obligatory \"copyright is far too long\" subthread. In my opinion, the ideal length (if we are to have copyright) is between 10 and 15 years, at least if a work is already monetarily successful. If a work has yet to be monetarily successful, then we can allow up to 25 years for it in particular. reply pclmulqdq 16 hours agorootparentEuropean patents have an interesting fee structure that increases exponentially (IIRC doubling every ~3 years), with a 20 year limit. I think this (plus mandatory registration if you want to enforce) is a great solution to copyrights, too, even if you let it run unbounded. If the registration fee is $1000, a registration 30 years later is $1000000 and if you want 60 years you have to pay $1000000000 over the life of the copyright. Any excess revenue could, theoretically, be redistribute as grants for the arts. reply silvestrov 5 hours agorootparentEuropean Patent Office fee list: https://my.epoline.org/epoline-portal/classic/epoline.Schedu... has 4 pages of prices reply pclmulqdq 3 hours agorootparentLooks like I was wrong about which patent office I'm thinking of, thanks for posting. Seems like it's the US that has doubling fees, doubling every 4 years. reply devsda 16 hours agorootparentprevI think it should be proportionate (& increasing exponentially) to the revenue(not profit because hollywood accounting). Big coporations like Disney can hold on to the material and pay the sum with profits from other ip if it's fixed. reply Wowfunhappy 5 hours agorootparent> Big coporations like Disney can hold on to the material and pay the sum with profits from other ip if it's fixed. I think that's okay, Disney has lots of money but they aren't stupid, i.e. they won't spend money unless they expect a return on investment. What you really want to prevent are orphan works which are copyrighted but no one can get a copy of. reply toast0 11 hours agorootparentprevAssuming fee based renewal, proportionate to revenue is problematic for works that fall out of publishing. There's lots of works from when I was young that nobody is interested in publishing, even though I would like to see them again. They make zero revenue now (and probably didn't make much before), so I suspect the rights holders would abandon them if they had to pay any fee, especially an escalating fee. I'm personally less worried about works that remain in print. reply lcnPylGDnU4H9OF 4 hours agorootparent> I suspect the rights holders would abandon them if they had to pay any fee Wouldn’t that just put the works in the public domain, allowing others to legally publish the works? What would be the downside? reply toast0 3 hours agorootparentYes, that's the upside. That's why people want a meaningful fee. If it's revenue based and there's no revenue, then the rightsholder will renew to protect their options. reply zarzavat 8 hours agorootparentprevCouldn't you print it yourself? If it's that obscure and that important to you to own a hard copy. reply bluGill 4 hours agorootparentPrint them how? Am I supposed to type up all the pages of that falling apart book - book scanning helps but still requires extensive effort? What about the page that got ripped out and is missing, how do I recreate that? While it is likely legal for me to go through all that effort for myself, it isn't legal for me to share the fruits of my effort with someone else who wants a copy so they need to do it themself. Even if they have a worn out copy themself it isn't clear that I can print them a new copy. If I want to share this work with someone else who might want it I'm stuck - I can only do the above for personal use. reply pclmulqdq 16 hours agorootparentprevWhy complicate things and allow for even more creative accounting? Elevating flat fees give you a term that is inversely proportional to revenue. With flat exponential fees, term is logarithmic in revenue. reply shiroiushi 16 hours agorootparentprevI completely disagree. The previous comment's idea is better: just charge fees. The government shouldn't be worried about exactly how profitable something is, because you can argue that too many ways (see Hollywood accounting). It's too easy for large corporations to invent \"creative accounting\" tricks, and just basing things on revenue penalizes any company that invests larger amounts of money in ventures that have lower profit margins (e.g. doing high-quality movie production with physical models, on-location shooting, etc. instead of just using some crappy AI to generate everything). With exponentially increasing fees, the copyright holder can decide for themselves if it's economically worthwhile to pay the renewal fees or release it into the public domain. If the cost to extend copyright another 5 years after 50+ years is $1B, for instance, very few copyright holders will bother with that unless it's a highly profitable property. reply wtallis 15 hours agorootparentThe problem with \"just charge fees\" is that it wastes an excellent opportunity to make the copyright system more useful for individual authors and artists, and instead strengthens the status quo with even more incentives to consolidate copyright powers under mega-corporations. Long before the fees got high enough to incentivize a corporation to abandon a work's copyright to the public domain, they would force authors to sell out to a corporation in return for a share of future earnings, rather than accept an immediate loss of all royalties. reply pclmulqdq 15 hours agorootparentPresumably you could set up \"just charge fees\" to have a grace period of 3-5 years to register your copyright (and pay any back fees) if you happen to want to enforce the copyright after that time. It's also possible that you could set up a smaller fee for a news article, etc. There are lots of implementation details that make \"just charge fees\" work. reply benj111 3 hours agorootparentBack fees? So I use a 4 year old work, that I assume is public domain, and then the creator, pays his back fees and sues me? alternatively, a creator creates a work, gets 5 years of protection, and never pays the fees that he owes. reply pclmulqdq 25 minutes agorootparentYes, exactly that model. In practice, you would treat that 4-year-old work as \"under copyright\" the same way everything written is under copyright today. This sort of thing gives you a chance to market your creative work with protection and avoid paying fees until you know that it is valuable. That would then allow the fees to be relatively high since only people with valuable IP would pay. The alternative is that everyone's blog enters the public domain immediately upon writing unless they want to pay $XXX per article, which also seems wrong to me. reply benj111 3 hours agorootparentprevI fail to understand the problem. Assuming some nominal or zero starting fee. The author can choose to sell his rights at the beginning, or can choose to keep the rights. If x years later, the renewal comes up and isn't worth it, then he doesn't have to pay it. the value to the author isn't worth the value to the public. If a mega corp comes along and buys it, they have taken on a risk that it will be worth more in the future, and the author has gained some extra income. If the mega corp think its worth buying the rights, theres probably a good business case for the property, so the author should be able to get the money somehow. If not, the mega corp is giving the author a nice bonus. reply deprecative 7 hours agorootparentprevReturn ownership of IP to humans only. You'll solve basically all of the issues currently faced that way. reply michaelt 5 hours agorootparentHow would that apply in the case of, say, Peter Jackson's 2003, $281 million \"The Lord of the Rings\" film series? Does Jackson own the IP? Do actors own part of the IP for every scene they're in? What does Jackson offer to investors, to get the backing he needs to hire loads of horse riders or whatever? Do we do it Star Citizen style, giving Jackson a few hundred million upfront with no obligation to deliver anything? reply tanewishly 3 hours agorootparentYou make an excellent point. Companies could eg. have 25yrs to make a profit. On the other hand: if an artist produces something that slumbers in anonymity for decades before it suddenly explodes into popularity and becomes part of the cultural canon, then I'd want the artist to reap whatever benefits possible. That is: if anyone is making big bucks off of that, it first and foremost should be the artist, for as long as they're alive. reply benj111 2 hours agorootparentInteresting. I'm against long term copyright, because things become part of the 'cultural canon' Why should I pay George Lucas because I want to say \"use the force luke\"* 'Cultural Canon' shouldn't be owned by anyone, because it, by definition belongs to everyone. *Yes I know thats a misquote. reply andrepd 7 hours agorootparentprev> The government shouldn't be worried about exactly how profitable something is, because you can argue that too many ways (see Hollywood accounting). It's too easy for large corporations to invent \"creative accounting\" tricks This is also the core reason why tax systems should be simplified simplified simplified. reply modeless 18 hours agorootparentprevHas anyone done a study on when copyrighted works make their money? I'm guessing that the vast majority of the income is made in the first decade after creation. Timeless classics are by far the exception. And for those, does it really benefit society for their fortunate authors to sit back and get rich resting on their laurels? The creation that copyright was supposed to incentivize already happened. At that point society benefits when works go into the public domain so derivatives can flourish. reply ronsor 18 hours agorootparent> does it really benefit society for their fortunate authors to sit back and get rich resting on their laurels? In my opinion, it doesn't. In creative and entertainment industries, the idea of practically indefinite royalties has been normalized, but no other industries has this*. For example, it would be strange to continue paying a construction company after your home has been built. *As far as I can remember. I'm open to correction here. reply johnmaguire 17 hours agorootparentI don't understand the analogy. We don't continue to pay authors after we buy their book. We do pay construction companies again if we want a second house - even if the design is the same. Probably floorplans would be a closer comparison - and I believe they are licensed IP? reply shiroiushi 14 hours agorootparent>We do pay construction companies again if we want a second house - even if the design is the same. You might pay the construction company again for the identical 2nd house, but you're not going to pay the architect again. reply eesmith 8 hours agorootparentWhy not? The US, for example, recognizes a copyright in architecture. https://en.wikipedia.org/wiki/Copyright_in_architecture_in_t... reply Juliate 9 hours agorootparentprevDépends. If the 2nd house location requires review by an architect because of ground issues or regulation. If contractual provisions require an architect fee. If small adjustments that may have structural impacts are needed. You won’t pay the same amount, but still something. reply tmtvl 9 hours agorootparentprevI believe the analogy is as follows: Imagine that you've paid the construction company after it finished building your house. You then go and live in it. One year later you get an invoice because you're living in the house they built. That's what doesn't happen and what (I think) GP means with indefinite royalties: the person who owns the house has to keep paying the company which built the house. The problem with that analogy is of course that royalties are based off profits, but there are ways to consider a home to have its own sense of profit (like the Belgian legal term 'cadastral income': > Cadastral income is not an actual income. It is a notional value that we determine for an immovable property (building or land). This corresponds to the average annual net rental income you would receive in 1975 for your leased out property. ). reply s1artibartfast 1 hour agorootparentnot only that, is that the analogy includes a surprise invoice. Royalties are contract law. A closer analogy would be rent. Why do we allow a builder to collect rent a year after building a house. reply Mindwipe 7 hours agorootparentprev> We don't continue to pay authors after we buy their book. Libraries certainly do in basically every country in the world apart from the US. reply paxys 2 hours agorootparentprevThat isn't the right analogy. To start, royalties have nothing to do with copyright. They are simply an agreement between an author and a publisher. I give you the exclusive right to publish my book, and I get a cut of every sale. Royalties extend far beyond creative fields. Any deal where someone gets a percentage share of the sale of a product or service on an ongoing basis is a \"royalty\". E.g. in manufacturing or even software. reply benj111 2 hours agorootparentprevI'd go slightly further. A rich author can retire, and not write any more books. From an encouraging creativity POV, copyright length should be set at about the amount of time it takes to create a followup. But, as open source software, and most authors and musicians demonstrate. People will create without any financial incentive. So ultimately copyright is there to allow an industry that can actually find and distribute these works. For the record, I'm not suggesting that creators should be decently rewarded for their works. reply lionkor 18 hours agorootparentprevany subscription service that doesn't deliver new value each day or month, like the Adobe suite(s) reply ronsor 18 hours agorootparentIn the case of Adobe, you're paying for continuous updates and their added cloud services (or other functionality which requires their servers). Also Adobe's subscription pricing (per-month) is significantly less than the retail cost for a one-time purchase of their software without future updates. Meanwhile one does not pay for continuous updates to a particular novel or a movie. Even if they do pay for new installments in a series, they do so separately. reply bluGill 4 hours agorootparentAdobe presumably also fixes bugs, so hopefully you are getting something better over time. I've in the past wore out a favorite book and since it was in print bought a new copy - and found the same typos that were in the previous copy. reply drdeca 18 hours agorootparentprev> Also Adobe's subscription pricing (per-month) is significantly less than the retail cost for a one-time purchase of their software without future updates. And the per-second pricing is even less! reply benj111 2 hours agorootparentprevI'm sure thats what they say you're paying for. I suspect you're just paying a monopoly tax. reply pclmulqdq 14 hours agorootparentprevMy guess is that authors have relatively long-term returns on their books, while movies are known to generally make almost all their money in the first year. reply Mindwipe 7 hours agorootparent> while movies are known to generally make almost all their money in the first year. That hasn't been true for fifteen years. reply mcosta 2 hours agorootparentAfter the DVD and TV licenses, what are the sources of income? Toys? reply s1artibartfast 1 hour agorootparentstreaming. Netflix has 40B in annual revenue, and is just one player among many. reply pclmulqdq 50 minutes agorootparentStreaming revenue has no public attribution of fees to specific content, as far as I can tell. It is very much possible that a large fraction of that revenue is driven by (and attributed to) new content, the same way DVD sales were. reply s1artibartfast 20 minutes agorootparentIm not sure I understand your point. I thought the question is where movie studios derive their profit. I dont think any streaming service would be viable with a catalog of only original productions... and require some additional breakthroughs on a par with the original GPT and stable diffusion breakthroughs ... You say this like Stable Diffusion isn't a 2022 technology. And not early 2022, but quite late (August). ChatGPT is younger. I mean sure we need more breakthroughs, but we've barely even seen a new hardware generation since those things came out and the researchers are really only getting started with the new capabilities of generative tech. If we don't get more breakthroughs in short order then that would be a stunning halt of progress, a breaking stop the likes of which we have almost never before seen. More breakthroughs are a given. reply ravenstine 16 hours agorootparentprevNot sure why you're downvoted. This is one of the most objectively true things said here. CGI was pretty crappy for at least the first few decades of its existence. Even aspects of the animation in Toy Story really show that film's age. I remember realizing that in the early 2000's. Most people either forgot or didn't even experience the early days of CGI and would consider much of it to be nightmare fuel today. AI is pretty clearly advancing orders of magnitude faster than CGI has. Just because it sucks now doesn't mean it's going to suck in another 5 years. reply lukan 10 hours agorootparent\"Just because it sucks now doesn't mean it's going to suck in another 5 years.\" We will see. Some flaws might be baked in, like LLM's halucinating. That won't go away, unless we invent a new tech. So here with generating videos, will morphing objects for example ever go away? I am sceptical with the current approach. reply fwip 16 hours agorootparentprevAn interesting example. It may be because I consider myself a fan of animation (moreso than the average person), but the video has obvious garbage less than fifteen seconds in, with the spaceships (?) morphing and sludging around the pyramid. reply zuminator 10 hours agorootparentSure, that's why I said casual viewer and not careful viewer. But getting back to your original point, would you say it was foul and unpleasant? That's really what I'm claiming, that we're fairly quickly advancing beyond the old days of those nightmare Nekobuses and vomit-inducing clips of Will Smith devouring spaghetti, and into territory where at least some people can find the product genuinely enjoyable. Of course nothing will ever be perfect. AI aside, after all these years it's still often jarring when computer physics is shoehorned into cartoons/anime that's designed to look like traditional hand drawn animation. reply vel0city 5 hours agorootparent> would you say it was foul and unpleasant? If I were to watch 90+ minutes of that with dubbed voices on top of it, absolutely. There's practically zero cohesion between any of those shots. No real action, no real narrative. It's a collection of non-cohesive stills that were stretched, not any bit of a story at all. reply ronsor 17 hours agorootparentprevIt is not inherently, but people are very effective at using it to produce foul, unpleasant output, which is a temporary problem. Like almost all things, people will not actually care how they're made if the final product is good. reply mlindner 16 hours agorootparentprevAgreed. Everyone raves about generative AI but I've yet to see a single generative AI video that is \"enjoyable\" to watch in a way beyond the way tech demos are enjoyable to watch. reply underlipton 17 hours agorootparentprevDisagree. I've seen things done with generative AI which I've wanted to see visualized for decades, but which were too difficult and time-consuming to do traditionally. I've also seen beautiful things that were either impossible to produce or impossible to even conceive of through traditional workflows. Those make up an infinitesimally small portion of the total output, which is largely a deluge of crap, certainly. But, generally, rarity makes something more valuable and beautiful by comparison. reply bbddg 16 hours agorootparentCare to share your examples? reply echelon 17 hours agorootparentprevAs someone who spends 100 hours a week working in this space, it's so weird seeing such pervasive negative attitudes everywhere I look. I know the work I'm doing is valuable and that this field is the future. I'm sure it'll click for more folks soon. reply jorvi 16 hours agorootparentTo me the magic is in generating things that would have had too much right issues or would not have been economically viable. A series that is a variant of the stories of “The Wire”, but taking place in the Harry Potter universe? Coming right up. Obscure prog rock band from the 90s put out one album? Now its two. I can understand people their apprehension, feeling like art is losing something essential without the human touch behind it, but I saw an article a few days ago where people thought generated Shakespeare was better than actual Shakespeare. Until it was revealed which was generated. If AI can generate me another, better Illmatic, I’m all here for it. reply Freak_NL 8 hours agorootparentThe conclusion to draw there is that Shakespeare honestly doesn't make for very good reading today unless you are also delving into the historical context or considering the major impact of his works on humanity's culture from the arts to language. LLM generated output has none of that. 'Ah ha! LLMs are better than Shakespeare!' is a meaningless statement. Besides, no one reads Shakespeare for pleasure; there is no need to generate more. ;) reply vundercind 4 hours agorootparentI bet you could get a random person off the street to fairly-consistently pick (curated) AI works over an amalgamated top-5 of great jazz recordings selected by jazz super-fans. > Besides, no one reads Shakespeare for pleasure Exactly (kind of). Lots of rewarding works take effort to learn to appreciate, for a bunch of reasons that may include (as in Shakespeare's case) that they're old and their context and vernacular is not ours. Lots of people (I'd say a large majority, in my experience) dismiss entire genres and forms of art that they weren't heavily exposed to as children, often going so far as to judge them bad, simply because it would take some time and effort to learn how to enjoy them and to be able to discern what's good or remarkable about a given work. What proportion of the population has ever in their lives enjoyed the experience of reading a Shakespeare play? Even once? It's gotta be tiny. Of course you can get them to choose AI junk over Shakespeare, it's not a kind of thing they understood or enjoyed to begin with, in most cases. reply happymellon 11 hours agorootparentprev> I know the work I'm doing is valuable and that this field is the future. I'm sure it'll click for more folks soon. Maybe because not everyone shares your opinion? Having an LLM generate art isn't necessarily a net benefit for society. Computers were supposed to improve our lives but instead of robots to perform dangerous menial work it's taking the creativity out of humanity. Hey guys you no longer have to do fun things, tech bros have that covered. Now get back down the mine. reply slyall 15 hours agorootparentprevThere seems to be some people who really hate Generative AI and will call it out and complain wherever they see it. eg somebody uses it to illustrate an article there will always be somebody who complains. So anything that enough people see will generate at least one complaint. reply sumtechguy 4 hours agorootparentThe thing is GenAI will be just like CGI. When it is bad it will look bad and has 'that look'. But when it is good enough you will not even know. The creativity has already gone sideways for most of this. I can with a few simple sentences create an acceptable picture (in some cases a short film). With a AI pipeline I can make some pretty cool scenes. Instead of having to know how to properly draw an s curve with a nice gradient bit of layered colors over it and 14 meticulously created layers. I tell the program to do it for me. It does an acceptable job in a fraction of the time. People can complain all they want but the rest of us are already using these tools and will continue to do so until something better comes along. reply fwip 2 hours agorootparentprevMost of the people who seem to be fans of AI-generated art are fans of AI, not of art. Maybe it'll get to the point where it's good enough to have on as background television - not everything needs to be great, after all - but what's the point of that? We already have far more high-quality television shows and movies than most people can ever watch. reply ronsor 17 hours agorootparentprevPeople repeat \"generative AI is all evil garbage\" because that's what the media (which is very afraid of AI, might I add) has told them. It's also funny to see AI turn people who normally dislike copyright into die-hard copyright lovers. reply vundercind 4 hours agorootparentSo far, it's really bad at actually replacing human labor in pro-social ways while being a supercharger for various antisocial jobs, like scam artist or astroturfer. The main \"beneficial\" use for it today is replacing wasteful labor that probably didn't need to be done in the first place—which is why an AI version is fine, because it didn't matter to begin with. My wife and I both work in the field, I on the tech side, her on the creative side, and she's been in it since the earliest days of industry trying to adapt these tools. There's a lot (like, holy shit, so much) of effort and money going into it, but so far it's only marginally helpful for non-evil jobs. reply sakjur 9 hours agorootparentprevPeople might care for culture. I’m very much in favor of a reformed copyright that strengthens indie artists, conservationists, and remixers and weakens Disney et al. It’s also a matter of fact that we have the copyright we have that’s prohibitive to people and favors corporations. It’s upsetting to see how a bunch of Silicon Valley companies stomps right across those lines with impunity, while people like Aaron Swartz are persecuted and threatened with decade long prison sentences for crimes that in my mind ought to be much less upsetting. If copyright was fair, training of AI intended for non-personal use ought to be a sufficient commercial activity to require a license. That would stiffle the development of AI, which is what I’d argue happens to human creators under our current system. If we had a 25 year copyright, we could easily make useful AI trained on the sum of human creation until 1999, _and_ have badass human made remixes of 80s and 90s songs — we wouldn’t have to do legal gymnastics to allow the development of useful AI, as it’d have access to quite substantial training material from the 1900s, and unlock relatively modern training material year-by-year. So yes, I dislike AI for infringing on copyright and I dislike copyright (in its current state). reply fwip 16 hours agorootparentprevI didn't say that? It is foul and unpleasant to behold, based on my experience of viewing it repeatedly over the last 5+ years. It would be nice if you didn't assume that anybody who doesn't share your opinion is mindlessly regurgitating slop. reply ronsor 16 hours agorootparentI was speaking in response to most of the general \"pervasive negative attitudes\" mentioned, not you specifically. Although I'm curious where you viewed generative AI content repeatedly 5 years ago; it was effectively non-existent outside research circles then. reply fwip 15 hours agorootparentIf I recall correctly, that was about the time that Google started demoing its generative AI \"deepmind\" - a particular demo of a frog comes to mind. The commonality of AI content has certainly increased since then, I didn't mean to imply it was commonplace back then. reply furyofantares 18 hours agorootparentprevHm, also at 10 years out how likely is the author to be able to convince someone to do a TV adaptation, knowing that by the time they're done someone else will be able to release their own versions (sans royalty even)? reply seabass-labrax 17 hours agorootparentApparently the TV series Game of Thrones cost just under $600m to produce[1], and George R. R. Martin earnt something like $100m from royalties as its original author[2]. Although access to the author for advice and publicity must be valuable, that is nonetheless a very large proportion of the profits that I'm sure many studios would rather not have to share! [1]: https://movies.stackexchange.com/a/100996 [2]: https://www.dailymail.co.uk/tvshowbiz/article-6182197 reply ronsor 18 hours agorootparentprevMany things are only popular for a relatively short period of time. If someone wants to wait until the copyright period is over to do an adaptation, the source material may no longer be that popular, and the adaptation, even without having paid a single royalty, will be unprofitable. reply longdustytrail 15 hours agorootparentOk but you’re gonna have a hard time convincing me that (morally) HBO should have been able to make game of thrones without cutting a check to the guy who created it reply ronsor 15 hours agorootparentI'm not saying it'd be good for HBO to do that, only that they could. Regardless of whether or not they should, edge cases like GoT's success should not control the outcome for everyone and everything else. reply Mindwipe 7 hours agorootparentGoT isn't an edge case - it's actually quite quick for an adaptation. reply Mindwipe 7 hours agorootparentprevThe reality is that almost no adaptations happen until ten years after a book is published. Virtually never. reply thisislife2 17 hours agorootparentprevThat can be fixed by limiting copyright to a certain duration per medium. You write a book - you have copyright on paper based books for 15 years. You publish it as Ebooks for desktop and mobile devices - get 15 years on that medium. Convert to visuals on Television &/or Films - 15 years on that medium. Virtual reality - another 15 years and so on ... reply johnmaguire 17 hours agorootparentAre you suggesting that if you release only a book, anyone could take the story and produce a film based on it, because you didn't publish a film? reply thisislife2 16 hours agorootparentNo. I am suggesting that short copyright terms should be tied to the medium of delivery. If someone writes a book, copyright will begin when they publish the book and the 10-15 years copyright expiration would only be applicable for paper book medium. After the copyright for the paper book medium expires, anybody can republish it. But, only the original copyright owner can \"recreate\" the work again in another medium - like Games, TV/ Films, Virtual reality etc. Even if that happens after the expiration of copyright on the first medium it originally appeared on. With the Game of Thrones example, with short copyrights, Martin would lose the copyright on the books (the first original medium it was published on) in 10-15 years. But he would retain the copyright on his work for other medium. So if 20 years down the lane, HBO wanted to recreate his work for TV, they would still have to get his permissions to do so. Once Martin's gives HBO the rights to his work for TV, HBO would own it only for the 10-15 years, and after that, anybody could use it freely too, but only for paper book and TV medium. This means if Meta or Apple want to recreate Game of Thrones as a virtual reality show, they would again have to approach Martin to get his permission. If they do, then they own the copyright to his work, on virtual reality medium, till it expires in 10-15 years. In this kind of system, the original author would continue to retain the future rights for any new future medium of delivery too. reply tmtvl 8 hours agorootparentWhat would happen then if someone wants to make a movie about a book published a few centuries earlier? Would they have to do deep archaeology to find the heirs of the author, the heirs of the heirs, the heirs of the heirs of the heirs,... and then get permission of the hundred-odd heir^Nths? reply echelon 18 hours agorootparentprevThis is a remarkably salient point. It can take a long time for certain works to find their wings or true market potential, especially books and music. Some examples of music: \"Take On Me\", \"Running Up That Hill\", \"Bohemian Rhapsody\", or even bands, like Neutral Milk Hotel reply jhbadger 17 hours agorootparentYou may have a point with a cult band like Neutral Milk Hotel, but songs like Take On Me and Running Up That Hill were incredibly popular when they were new -- it's just that both got a second wave of popularity decades later when they were used on soundtracks of films/shows. reply echelon 12 hours agorootparent> Running Up That Hill were incredibly popular when they were new It was far more popular in the recent revival! reply gosub100 18 hours agorootparentprev> does that mean they would have been able to make it without licensing it from the author? yes and it's possible someone could have done it even better, had they not been required to convince investors to purchase copyright. GoT was a masterpiece, don't get me wrong, but it's a fallacy to think it couldn't have been better, or that other book adaptations could have been as good or better, without copyright being in the way. It's a minor issue in the grand scheme, but my pet peeve is with \"synch licenses\" (not sure if that's even the right term), but where sitcoms can't go to home video because of stupid disputes about shitty songs that happened to be included. Did anyone watch \"Married With Children\" because of Frank Sinatra's song \"Love and Marriage\" in the intro? It's a catchy song, and I'm sure it lured people in who might have otherwise changed the channel, so yes it has value. But it should only be a tiny fraction of the royalties for a full performance of the song. doubly so for home video releases. Would anyone buy even 1 season of MWC just to hear the Sinatra song? I say no. And therefore should not be required to pay any royalties. I am watching \"Murphy Brown\" reruns from pirateflix because apparently it never went to home video because of license disputes about the 60's soul songs in the intro. They add character to the show, for sure. But they're not why I watch the show. I watch it for the story and the acting. In this case, actors (who worked extremely hard over 10 seasons of that show!) are being wrongfully deprived of royalties because record execs can't be reasonable about how much 10 seconds of a 60 -year-old song is worth. reply plopz 17 hours agorootparentYeah, thats a big problem with shows that work really well with the music, like Scrubs. I'm glad we have piracy to be able to keep the original works with the intended tracks intact. reply theshrike79 6 hours agorootparentprevI'd go with something like the Sony Spiderman deal. Unless they release new IP with that character every X years, the rights revert back go Marvel (Now Disney). Same with books. If you have an ongoing series for 20 years, the first books shouldn't enter public domain. But a book series or TV show with no new content for 15+ years? Public domain. reply HideousKojima 3 hours agorootparentThat gets really murky really fast. Is 10 Cloverfield Lane a sequel to Cloverfield? They decided to throw the Cloverfield name onto it shortly before release for marketing reasons, the actual movie has nothing to do with the events and story of the original Cloverfield. Is the video game Nier a sequel to Drakengard? Technically yes, but the connection is vague and distant. And there's also Drakengard 2 which is the sequel to a different ending for the original game. How would you count Fear the Walking Dead, the spin-off series of The Walking Dead, itself and adaptation of a comic book series. Do the shows continue to get copyright protection so long as the comics are still being published? Or vice versa? reply benfortuna 16 hours agorootparentprevIf this was the case would we have enough interesting content (movies, music, etc.) to reduce demand for streaming services, etc.? I think those indirect impacts probably incentivize more lobby groups to keep the status quo. reply vundercind 5 hours agorootparentprevIt should definitely be short enough for creatives to engage directly with their influences from childhood and adolescence in middle age, and to take on and use the earlier works of their contemporaries in their later years. So, probably not more than twenty years. Fifteen would be better. reply Animats 14 hours agorootparentprevAll the TRIPS agreement requires is 50 years. That can be from first publication, regardless of when the author dies. The US should go with 50 years from first publication. It doesn't have significant financial effect for rights holders. Revenue on content over 50 years old is tiny. Maybe if you're still alive, the sole author, and own the rights yourself, you could apply for an extension for the rest of your life. But no more than that. Someone with Trumpworld connections could push this, as a way of getting back at Hollywood. reply Mindwipe 7 hours agorootparent> All the TRIPS agreement requires is 50 years. That can be from first publication, regardless of when the author dies. TRIPS requires fifty years for features, but not for various bits of copyright that go into making a feature, where it requires life + 50. There are vanishingly small amounts of films that would be genuinely clear under the TRIPS terms, they'd basically just be performing arts pieces with no script or planning or music. reply liontwist 16 hours agorootparentprevWhat it sounds like you’re saying is “this stuff is too old to be interesting” which is kind of the point. reply ronsor 16 hours agorootparentThe point of copyright is to promote the creation of new works. It has nothing to do with the popularity of the work under copyright or after entering the public domain. Mickey Mouse was still relevant at the time of entering the public domain, ninety-five lobbied years later. reply liontwist 3 hours agorootparentYep. But if it still commands economic value for its creator and you cut that off, then it is reducing that incentive to create and cultivate in the first place. reply deprecative 7 hours agorootparentprevTo add it's only Steamboat Willie Mickey that is public domain. The actual character as we think of him isn't. The creator has been dead for nearly 80 years. It's absolutely insane that any of those creations are still privately owned. reply benj111 3 hours agorootparentprevSurely the point of copyright is to allow a person to have sole rights to commercialise a thing before it goes to the public domain. If so, if they havent done that in the first 10/15 years, why should they get an extra 10/15 years? Further, another issue of very long copyrights is preserving things you don't really have a right to preserve. that successful videogame may still be around in 25 years. the one that wasn't so popular has much less chance of surviving. And then you have the added complication of what was successful? a fixed term means you know when something is in the public domain. personally, I think there should be registration and fees attached. if you want copyright protection for the first 5 years, pay a nominal fee. if you want more than that, pay exponentially more for each year. If companies want to pay that tax, they can. if it isn't worth it, then it can go in the public domain. Either way, at least you have a register of what is in, and out of copyright. reply aucisson_masque 18 hours agorootparentprevI mean if everyone agree but a few big companies, how is it possible that it has not yet been changed by politicians. I’m not saying specifically USA but also Europe, I can’t see common people fighting over the right of author to hold intellectual properties for 70 years. Haven’t been trials to shorten it somewhere, either USA or any other developed countries where it’s actually enforced ? reply ronsor 18 hours agorootparentThere are a few reasons why copyright terms aren't shortened: * International treaties make it difficult without buy-in from everyone, or at least the most important countries. The USA is probably the only country that could afford to unilaterally make such a change. * Politicians are largely beholden to big companies now. * The average person is distracted with other societal woes, and politicians and companies work hard to keep it that way. * Some people have been convinced that excessive copyright is a moral good through propaganda. reply genghisjahn 17 hours agorootparentI’m not sure where I stand on this, but is it possible that some people have been convinced that copyright is a moral evil by propaganda? I pause when I hear this line of argument, “I’m a free thinker and I have objective truth. Others are weak minded victims of propaganda.” reply ronsor 16 hours agorootparentOf course. There's all kinds of propaganda (and depending on where you stand, some propaganda is good and other propaganda is bad), and not one person is immune. But for the sake of this discussion, there are some extra factors to consider: * Those promoting copyright expansion or the status quo have significant amounts of money; those criticizing it mostly do not (counting groups with real principles, anyway) * It is suspicious for people who have no personal interest in extended copyright to excessively favor it. reply lmm 18 hours agorootparentprevThe benefits of a shorter term are diffuse. The benefits of a longer term are concentrated. This is a case where representative democracy often breaks down - policy A would be better for almost everyone, but not by enough for them to switch their vote on, whereas policy B is better by enough for a few people to make them single-issue voters. So we get policy B. reply mrkstu 17 hours agorootparentSingle issue /donors/ reply Aloisius 19 hours agoparentprevThe length should be no more than would be required to maximize the creation of works and not a moment longer. Long terms prevent the creation of derivative works which at an extreme could be reducing the number of works created as well as disincentivizing creators from creating new works if they've been especially successful early in their careers and decided to coast. reply thrance 17 hours agorootparentThen I'd say the ideal length is probably zero. I don't buy that derivative works negatively impact revenue that much. reply chipotle_coyote 14 hours agorootparentDerivative works aren't really the primary problem copyright law was created to address. In the early days of novel publishing, it wasn't uncommon for a popular book an author had sold to a publisher on a royalty basis to just be reprinted by other publishers who kept all the money for themselves. It wasn't unheard of for the majority of an author's books to be published by people who weren't giving any money to them until the law stepped in. I think people have a tendency to focus on corporations and super-successful individual creators as the primary beneficiaries of copyright, and I get it, but George R.R. Martin should not be your yardstick: the long tail applies here. A lot of authors have books that might keep bringing in a thousand dollars or less a year in royalties over a couple decades; if those authors are able to put out a book a year, that \"back catalogue\" might end up being most of their writing income. And the rise of ebooks has probably created more authors in that boat, not less. The original US copyright act in 1790 set the term at 14 years, with a near-automatic extension of another 14 years granted upon request. I'd be happy enough going back to that, but I don't think I'd want to see less, honestly. reply stephen_g 18 hours agoparentprevAbsolutely. There's also absolutely zero sense that if there's any term based on the lifetime of the authors, that it should extend a single day after their death. I'd definitely prefer a 20-30 year fixed term, but if it was going to be based on lifetime then it should only be until the death of the author. reply devsda 16 hours agorootparentThe problem with that is some ridiculous edge cases. Young book author agrees to a profit share agreement with publisher and works most of the time. Has an accidental death at launch party, his work is in public domain now, and the publisher & author's family are in trouble ? Or it simply makes publishers reluctant to work with old authors and be biased towards healthy young writers. reply criddell 5 hours agorootparentI don't have particularly strong feelings on this particular issue, but I do take issue with worrying about edge cases. By definition, edge cases are infrequent and unlikely occurrences. Trying to account for every possibility is a way of making sure nothing ever changes. It's a form of perfect being the enemy of good. For this particular scenario, I would tell publishers and authors to take out a life or accidental death insurance policy. reply stephen_g 13 hours agorootparentprevYes that's why it shouldn't have any bearing at all on lifetime, and why I prefer a fixed term of 20-30 years. reply coldtea 5 hours agoparentprev>I'd argue that by then all the money would have been made Not by a long shot. They're still milking famous books, movies, songs, from 50 years back and more. But I'd argue that by 25 years all the money being made for the original owners should have been forced to stop. Similar as with patents. Once concern is when a creator isn't making money (e.g. from a book), and the work takes off after the 25 years (say, it becomes viral). reply themaninthedark 4 hours agorootparentJust for argument, if we set the limit at 25 years. The Fellowship of the Ring, published 29 July 1954, would have been out of copyright by 1981. (https://en.wikipedia.org/wiki/The_Fellowship_of_the_Ring) I would say that the bulk of the (for lack of a better term) fandom, occurred after the 1980. Frodo Lives!(https://en.wikipedia.org/wiki/Frodo_Lives!) notwithstanding. I would also argue that corporations would have no qualms of waiting 25 years to capitalize and format shift a work of art, where as the 50 year term limit makes it more difficult for them to play off of nostalgia alone. reply ElectricalTears 5 hours agorootparentprevWe'd just end up with Seinfeld and Friends reruns on every channel. reply poulpy123 4 hours agoparentprevWhile I agree with you, I'm struggling to find a good argument why intellectual property should be treated differently from physical property reply oneplane 4 hours agorootparentReasoning about it from the perspective of something physical, one could argue that intellectual property doesn't exist at all, and we should only consider the books, movie media etc. to be property. If we were to take the contents of a book or a movie for example, and copy it, you still have the physical source, and nothing is lost, your property is still yours. It's just that there is more of it due to the additional copy. That copy isn't yours, and from the moment it was created it still isn't yours. So in that line of thinking, the property that was created is not the same as the property it was copied from, which means two different properties exist. We can make this even trickier, because if we were to reason about the physical property and the intellectual property separately, the story in a book, and the physical book itself would be two different properties. So when you create a copy, that book that started out blank was definitely not part of the property of someone else. So does the act of adding intellectual property now suddenly transfer the physical property to the source of the intellectual property? In the current laws and practises around the world we have made all sorts of rules about this, but just reasoning about it before falling back on established practice already shows that it doesn't always turn out to be as easy as it seems. reply csdreamer7 4 hours agorootparentprev> While I agree with you, I'm struggling to find a good argument why intellectual property should be treated differently from physical property 1) First reason, it is not physical property. 2) Second, many of the creators who lobbied for longer copyright terms benefited from a rich public domain when they did not have anything. In the case of Walt Disney, he made a film series based on Alice in Wonderland, which never had copyright protection in the US, and it's copyright expired in the UK in 1907. https://en.wikipedia.org/wiki/Walt_Disney#Early_career:_1920... reply a57721 1 hour agorootparentDisney also used \"Le Sacre du printemps\" in \"Fantasia\" after meeting Stravinsky, who was offended by the idea. However, Disney informed him that he would use it anyway since it was not protected in the US. Stravinsky was paid pennies. reply andrewclunn 4 hours agorootparentprev\"Hey, I like that table, I'm going to commission somebody to build one for myself!\" Totally legal. No loss to first individual. The fact is that intellectual property IS treated differently, and was justified as being required to incentivize invention and creation NOT because there was any natural right to the product of one's thoughts to not be copied or expanded upon. Now that it arguably gets in the way of innovation and creation, what justification is there for these ADDITIONAL legal restrictions. reply fsckboy 4 hours agorootparent>NOT because there was any natural right to the product of one's thoughts to not be copied or expanded upon grandma not sharing her best recipes, and children calling each other \"copycat!\" is an argument for a natural right. It is innately how humans feel, and there is a supporting argument that we don't want grandma's secret innovations dying with her, to incent sharing. reply adamc 19 hours agoparentprevI think that's too short. Lifetime of the author or a min. 50 years would make sense to me. Mark Twain, for example, was worried about providing for surviving daughters. I think that's reasonable. The crazy long Disney thing, though, is what it is because of lobbying muscle. reply lolinder 19 hours agorootparent> Mark Twain, for example, was worried about providing for surviving daughters. I think that's reasonable. I don't think it's reasonable to expect to be able to continue to make money after you are dead. Earn enough during your lifetime to provide for your surviving daughters, sure, but I don't like the idea of someone being able to posthumously put a gag on people's ability to express themselves just so that their kids can get a nice inheritance. reply seizethecheese 18 hours agorootparentPerhaps not, but the point of copyright is to provide incentive for creating work. Since earning beyond the grave is an incentive, there’s an argument to protect it (within limit). Another example is Grant’s autobiography, which he wrote as he was dying of throat cancer. No way he would have done that without copyright. reply lolinder 18 hours agorootparent> No way he would have done that without copyright. Why not? For centuries people wrote books so that they would have a legacy and be remembered. They wrote because they felt it was the right thing to do, or because they wanted to control the narrative around their lives. Do you have any specific reason to believe that Grant wrote his autobiography to provide for his successors, rather than just to have written it? I understand the theory about incentivizing people to create, but honestly I'm not convinced that what we get from that deal is worth it. Too often it feels like extended copyright creates a similar set of incentives to advertising—sure, we get more works, but the best works would have been written even with a much shorter copyright because the author had something they wanted to say. The works that are being incentivized by long copyrights are the ones that we could do without. reply Quillbert182 17 hours agorootparentGrant wrote his autobiography pretty much entirely out of desperation to provide for his family after his death. He had lost everything he had in a Ponzi scheme and was heavily in debt as he was dying from cancer, and the autobiography was his last chance to make money for his family. reply lolinder 17 hours agorootparentI stand corrected. That said, at the time of Grant's writing copyright in the US was 28 years (with an optional extension for another 14 if the author lived long enough), which means that OP's proposal of 25 years would likely have been sufficient to motivate Grant. reply seizethecheese 17 hours agorootparentYes. This is the correct synthesis. Copyright is good for one generation and bad beyond that. reply Aloisius 17 hours agorootparentprevBoth Clemens and Grant wrote when copyright was 28 years + optional 14 year extension. I'd much prefer that to the current life + 70 years or 95 years after publication. reply WillDaSilva 18 hours agorootparentprev> No way he would have done that without copyright. Was his creation of his autobiography primarily motivated by money? I would assume not. reply Quillbert182 17 hours agorootparentIt actually was, he was almost entirely destitute when he died and the autobiography was his last desperate at providing for his family. reply WillDaSilva 17 hours agorootparentI stand corrected. Thanks for sharing that info. reply Brybry 18 hours agorootparentprevThe problem that I see is not the Mark Twains but the vast majority of other authors. Their works go out of print but they're still copyrighted so people can't legally reproduce them (for profit or otherwise). My grandfather was a published author with some success but he's dead and his stuff is no longer in print. No one in my family is going to see revenue from his work. No one outside of his generation (when he was successful) will ever have a chance to read his books as they're impossible to find now. Most books written in the 20th century are basically gone from public availability.[1][2] [1] https://www.theatlantic.com/technology/archive/2012/03/the-m... [2] https://www.law.berkeley.edu/files/How_Copyright_Keeps_Works... reply xienze 17 hours agorootparent> No one outside of his generation (when he was successful) will ever have a chance to read his books as they're impossible to find now. Does the family not have rights to those works? A copy of the books? Scan them and release them copyright free if you want the world to see them. reply modeless 18 hours agorootparentprevAuthors can provide for their children the same way the rest of us do. With the money they made while they were working. reply lolinder 18 hours agorootparentI agree, with the caveat that a literary work can be seen as an asset that has value which pays out over time. It takes a large investment upfront and then pays out slowly. So having a limited ability to pass on that asset if you die prematurely seems only fair—you'd have the same option if you were building something physical—but we shouldn't use inheritance as an argument for longer copyright. The question of how long someone should be allowed to earn money from a work should be orthogonal from the question of whether that right to earn money should be inheritable. This is the flaw in life plus 70—it assumes that copyright should last indefinitely during one's lifetime and then provide for successors. I'd rather see a flat rate for how long we're comfortable locking up a work in copyright, successors or otherwise. reply modeless 18 hours agorootparent> It takes a large investment upfront and then pays out slowly Does it really? Sure, timeless classics pay out over a long period of time but they are by far the exception. I'll wager that the vast majority of copyrighted works make the vast majority of their money in the first decade. So why do we need essentially perpetual copyright? (Essentially perpetual because almost none of the works created in my lifetime will ever pass out of copyright before I die) reply lolinder 18 hours agorootparent> the vast majority of their money in the first decade. So why do we need essentially perpetual copyright? I agree. I think OP's proposal of a fixed term of 25 years is more than reasonable. All I'm saying is that it should be inheritable and not based on when the author dies. reply adamc 16 hours agorootparentprevI don't see why written things can't be an asset while other creations can. It just discriminates against writers. reply modeless 14 hours agorootparentWe have property rights for physical objects because physical objects are scarce. Only a limited number of people can use any given object and we need some way of deciding who gets to use it. On the other hand copies of writing are not scarce. We can give copies to everyone who wants one for practically free. Property rights for copies of writing are therefore artificial. Creating artificial scarcity where none exists has real costs that in many cases outweigh the benefits. reply seizethecheese 18 hours agorootparentprevAll of us working in development of any kind create value well into the future with our work. The only question is whether you monetize immediately with wages or with ownership reply rrrrrrrrrrrryan 17 hours agorootparentprevDrug parents are only 10 - 20 years, and it works great. The creator gets to make enough money to finance their next drug, and the public gets cheaper generics after a decade or two. There's no reason any other IP should be longer. reply shiroiushi 14 hours agorootparentprev>Mark Twain, for example, was worried about providing for surviving daughters. I think that's reasonable. Why is that reasonable? Why should someone's daughters get a free ride instead of having to work for a living like everyone else? reply poulpy123 5 hours agorootparentAre you in favor of forbidding inheritance to everyone or just for authors ? reply ronsor 19 hours agorootparentprevI think that's still too long when copyright was originally supposed to be a compromise between society and the author, not a indefinite guarantee for an author. I understand the concern of providing for family, but keep in mind that the average person works continuously to provide for their family and has to be responsible enough to save money. It is not society's responsibility to ensure that; and alternatively if it is to be society's responsibility, there are better mechanisms than copyright. reply bigstrat2003 17 hours agorootparentprevAgreed. I think lifetime of the author, or moderate fixed term in the case of untimely death or corporate copyright, is perfectly reasonable. 10-20 years is way too short, it does not give enough consideration to the author's rights. reply bdangubic 18 hours agorootparentprevhis daughters were in their mid to late 20’s - wtf does he need to support them from his grave reply lolinder 18 hours agorootparentTo be fair to him, this was an era where their ability to support themselves was limited—if not in practice at least by strong cultural stigma. The same cannot be said of most people today, and I therefore agree that it's a bad argument for long copyright terms in a modern context. reply globular-toast 5 hours agoparentprevWhere are you getting 70 years from? In the UK etc it's lifetime plus 70 years. These will be works whose authors died before my retired father was born. It says authored 1929 for the US which seems to indicate 95 years? I've lost track of how long these ridiculous lengths are now. reply HWR_14 4 hours agorootparentFor works produced by a company's employees and owned by the (immortal) company, its 95 years. For works produced by a mortal human it's that author's life + 70 years. That's how it is in the US and most of Europe. reply theshrike79 6 hours agoparentprevThe most important factor should be that if something isn't commercially available (not used, new), it should fall to Public Domain faster. If you're not selling your game published in 2005, it should be free for everyone to grab - you clearly don't care about it anymore. If you did, you'd let people pay money for it. reply nemo44x 15 hours agoparentprevI disagree entirely in that I believe government shouldn’t determine this but rather the market. The rights to an intellectual property should be transferable/sold in perpetuity and at some point the work will become less and less valuable as newer works outcompete for attention. This will differ depending on how great the work was as judged by the market but it’s still the commanding force. Any particular time for transfer from market control to public domain is arbitrary. If it’s worth anything it should be traceable not stolen. Everything will eventually have too little demand to be defensible and control will be let go and at that point it becomes public domain because it became public domain. reply panja 11 hours agorootparentIf you want the market to determine then why do you need IP at all? That's just government intervention... reply nemo44x 3 hours agorootparentThe government exists to protect the property of its citizens. You can’t have a reliable market without the threat of violence (prison, fines, etc) and judiciary from an authority and government is suitable for that purpose. It’s their main purpose in fact. reply globular-toast 4 hours agorootparentprevSo no copyright then? How would \"the market\" prevent publishers freeloading the works of authors? Suggest you look up why copyright was invented (and all the other thousands of ways simplistic markets don't work the way we'd like them to). reply nemo44x 3 hours agorootparentI’m saying copyright in perpetuity enforced by the government with the threat of fines or prison for stealing a copyright holders property. From there the market can take over as it’s now free to operate without fears of piracy etc. reply cle 20 hours agoprevHere's the full list, base64-encoded (since HN doesn't support spoilers...) QSBSb29tIG9mIE9uZSdzIE93biBieSBWaXJnaW5pYSBXb29sZgpUaGUgTWFuIFdpdGhpbiBieSBHcmFoYW0gR3JlZW5lCkhlbnJpIE1hdGlzc2UKVGhlIFNvdW5kIGFuZCB0aGUgRnVyeSBieSBXaWxsaWFtIEZhdWxrbmVyCkZyaWRhIEthaGxvCk1hZ2ljayBpbiBUaGVvcnkgYW5kIFByYWN0aWNlIGJ5IEFsZWlzdGVyIENyb3dsZXkKRHVrZSBFbGxpbmd0b24KTWFnbmlmaWNlbnQgT2JzZXNzaW9uIGJ5IExsb3lkIEMuIERvdWdsYXMKQ2hhcmxlcyBJdmVzCkEgRmFyZXdlbGwgdG8gQXJtcyBieSBFcm5lc3QgSGVtaW5nd2F5ClRoZWEgdm9uIEhhcmJvdQpCZXJsaW4gQWxleGFuZGVycGxhdHogYnkgQWxmcmVkIETDtmJsaW4KUm9iZXJ0IENhcGEKTGF1Z2hpbmcgQm95IGJ5IE9saXZlciBMYSBGYXJnZQpDb2xldHRlCkxvb2sgSG9tZXdhcmQsIEFuZ2VsIGJ5IFRob21hcyBXb2xmZQpBdWd1c3RlIEx1bWnDqHJlClByb2Nlc3MgYW5kIFJlYWxpdHkgYnkgQWxmcmVkIE5vcnRoIFdoaXRlaGVhZApEYXVnaHRlciBvZiBFYXJ0aCBieSBBZ25lcyBTbWVkbGV5CkFkb2xwaCBHb3R0bGllYgpHb29kLUJ5ZSB0byBBbGwgVGhhdCBieSBSb2JlcnQgR3JhdmVzClRoZSBTdG9yeSBvZiBNeSBFeHBlcmltZW50cyB3aXRoIFRydXRoIGJ5IE1haGF0bWEgR2FuZGhpCkxpbCBHcmVlbgpBbGwgUXVpZXQgb24gdGhlIFdlc3Rlcm4gRnJvbnQgKHRyYW5zbGF0aW9uKSBieSBFcmljaCBNYXJpYSBSZW1hcnF1ZQpUaGUgR29vZCBDb21wYW5pb25zIGJ5IEouIEIuIFByaWVzdGxleQpIdWRzb24gUml2ZXIgQnJhY2tldGVkIGJ5IEVkaXRoIFdoYXJ0b24KVGhlIE1hcmFjb3QgRGVlcCBieSBBcnRodXIgQ29uYW4gRG95bGUKQW5uZSBTZXh0b24KTGVzIEVuZmFudHMgVGVycmlibGVzIGJ5IEplYW4gQ29jdGVhdQpEb2Rzd29ydGggYnkgU2luY2xhaXIgTGV3aXMKRG9uYWxkIEdvaW5lcwo= reply remram 19 hours agoparentStick a data: prefix in front and it'll decode in web browsers: data:text/plain;base64,QSBSb29tIG9mIE9uZSdzIE93biBieSBWaXJnaW5pYSBXb29sZgpUaGUgTWFuIFdpdGhpbiBieSBHcmFoYW0gR3JlZW5lCkhlbnJpIE1hdGlzc2UKVGhlIFNvdW5kIGFuZCB0aGUgRnVyeSBieSBXaWxsaWFtIEZhdWxrbmVyCkZyaWRhIEthaGxvCk1hZ2ljayBpbiBUaGVvcnkgYW5kIFByYWN0aWNlIGJ5IEFsZWlzdGVyIENyb3dsZXkKRHVrZSBFbGxpbmd0b24KTWFnbmlmaWNlbnQgT2JzZXNzaW9uIGJ5IExsb3lkIEMuIERvdWdsYXMKQ2hhcmxlcyBJdmVzCkEgRmFyZXdlbGwgdG8gQXJtcyBieSBFcm5lc3QgSGVtaW5nd2F5ClRoZWEgdm9uIEhhcmJvdQpCZXJsaW4gQWxleGFuZGVycGxhdHogYnkgQWxmcmVkIETDtmJsaW4KUm9iZXJ0IENhcGEKTGF1Z2hpbmcgQm95IGJ5IE9saXZlciBMYSBGYXJnZQpDb2xldHRlCkxvb2sgSG9tZXdhcmQsIEFuZ2VsIGJ5IFRob21hcyBXb2xmZQpBdWd1c3RlIEx1bWnDqHJlClByb2Nlc3MgYW5kIFJlYWxpdHkgYnkgQWxmcmVkIE5vcnRoIFdoaXRlaGVhZApEYXVnaHRlciBvZiBFYXJ0aCBieSBBZ25lcyBTbWVkbGV5CkFkb2xwaCBHb3R0bGllYgpHb29kLUJ5ZSB0byBBbGwgVGhhdCBieSBSb2JlcnQgR3JhdmVzClRoZSBTdG9yeSBvZiBNeSBFeHBlcmltZW50cyB3aXRoIFRydXRoIGJ5IE1haGF0bWEgR2FuZGhpCkxpbCBHcmVlbgpBbGwgUXVpZXQgb24gdGhlIFdlc3Rlcm4gRnJvbnQgKHRyYW5zbGF0aW9uKSBieSBFcmljaCBNYXJpYSBSZW1hcnF1ZQpUaGUgR29vZCBDb21wYW5pb25zIGJ5IEouIEIuIFByaWVzdGxleQpIdWRzb24gUml2ZXIgQnJhY2tldGVkIGJ5IEVkaXRoIFdoYXJ0b24KVGhlIE1hcmFjb3QgRGVlcCBieSBBcnRodXIgQ29uYW4gRG95bGUKQW5uZSBTZXh0b24KTGVzIEVuZmFudHMgVGVycmlibGVzIGJ5IEplYW4gQ29jdGVhdQpEb2Rzd29ydGggYnkgU2luY2xhaXIgTGV3aXMKRG9uYWxkIEdvaW5lcwo= reply mdaniel 19 hours agorootparentIn the spirit of nerd-sniping, it turns out that one needs to specify the encoding of those bytes, too, since it defaults to charset=us-ascii for some horrible reason https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/dat... data:text/plain;charset=utf-8;base64,QSBSb29tIG9mIE9uZSdzIE93biBieSBWaXJnaW5pYSBXb29sZgpUaGUgTWFuIFdpdGhpbiBieSBHcmFoYW0gR3JlZW5lCkhlbnJpIE1hdGlzc2UKVGhlIFNvdW5kIGFuZCB0aGUgRnVyeSBieSBXaWxsaWFtIEZhdWxrbmVyCkZyaWRhIEthaGxvCk1hZ2ljayBpbiBUaGVvcnkgYW5kIFByYWN0aWNlIGJ5IEFsZWlzdGVyIENyb3dsZXkKRHVrZSBFbGxpbmd0b24KTWFnbmlmaWNlbnQgT2JzZXNzaW9uIGJ5IExsb3lkIEMuIERvdWdsYXMKQ2hhcmxlcyBJdmVzCkEgRmFyZXdlbGwgdG8gQXJtcyBieSBFcm5lc3QgSGVtaW5nd2F5ClRoZWEgdm9uIEhhcmJvdQpCZXJsaW4gQWxleGFuZGVycGxhdHogYnkgQWxmcmVkIETDtmJsaW4KUm9iZXJ0IENhcGEKTGF1Z2hpbmcgQm95IGJ5IE9saXZlciBMYSBGYXJnZQpDb2xldHRlCkxvb2sgSG9tZXdhcmQsIEFuZ2VsIGJ5IFRob21hcyBXb2xmZQpBdWd1c3RlIEx1bWnDqHJlClByb2Nlc3MgYW5kIFJlYWxpdHkgYnkgQWxmcmVkIE5vcnRoIFdoaXRlaGVhZApEYXVnaHRlciBvZiBFYXJ0aCBieSBBZ25lcyBTbWVkbGV5CkFkb2xwaCBHb3R0bGllYgpHb29kLUJ5ZSB0byBBbGwgVGhhdCBieSBSb2JlcnQgR3JhdmVzClRoZSBTdG9yeSBvZiBNeSBFeHBlcmltZW50cyB3aXRoIFRydXRoIGJ5IE1haGF0bWEgR2FuZGhpCkxpbCBHcmVlbgpBbGwgUXVpZXQgb24gdGhlIFdlc3Rlcm4gRnJvbnQgKHRyYW5zbGF0aW9uKSBieSBFcmljaCBNYXJpYSBSZW1hcnF1ZQpUaGUgR29vZCBDb21wYW5pb25zIGJ5IEouIEIuIFByaWVzdGxleQpIdWRzb24gUml2ZXIgQnJhY2tldGVkIGJ5IEVkaXRoIFdoYXJ0b24KVGhlIE1hcmFjb3QgRGVlcCBieSBBcnRodXIgQ29uYW4gRG95bGUKQW5uZSBTZXh0b24KTGVzIEVuZmFudHMgVGVycmlibGVzIGJ5IEplYW4gQ29jdGVhdQpEb2Rzd29ydGggYnkgU2luY2xhaXIgTGV3aXMKRG9uYWxkIEdvaW5lcwo= reply HPsquared 18 hours agorootparentprevWow, that's amazing. I've recently been having some mild trouble with base64. Just put it in the address bar, of course!! reply ahmedfromtunis 17 hours agorootparentprevThis does NOT work in Android/tablet version of Chrome (unless I'm doing something wrong). Gonna try it tomorrow on my computer. reply remram 3 hours agorootparentIt works for me on Chrome 131.0.6778.81 on Android 14. Are you sure you copy/pasted right? reply sexy_seedbox 7 hours agorootparentprevWorks fine in Kiwi Browser on Android. reply __rito__ 13 hours agorootparentprevJust worked in Android Firefox. reply zamadatix 20 hours agoparentprevFor those on the lazy side: A pastebin link option https://pastebin.com/raw/9wfPfzT3 reply saghm 20 hours agorootparentAt the risk of the above comment being edited and making this stop working, here's an alternative lazy version: curl https://news.ycombinator.com/item?id=42291112grep 'QSBSb[^ Copyright, at least in the U.S., is automatic. It is now, but back then a work was public domain if released without a valid copyright notice. Charade, a 1963 film, entered the public domain immediately on release. https://en.wikipedia.org/wiki/Charade_(1963_film)#Public-dom... reply ndriscoll 18 hours agorootparentprevIt wasn't automatic in the past, which notably led to Night of the Living Dead accidentally becoming public domain on release. reply crossroadsguy 17 hours agorootparentprevHaving known of the man I would say copyright would have been, if at all, at somewhere in the bottom of bottom list of battles he was (and had to be) fighting. reply qingcharles 15 hours agoparentprevSome amazing works of literature in that list. reply doublerabbit 20 hours agoparentprevAnd if you can't read base64, a base16 encoded version below. 4120526F6F6D206F66204F6E652773204F776E2062792056697267696E696120576F6F6C660A546865204D616E2057697468696E2062792047726168616D20477265656E650A48656E7269204D6174697373650A54686520536F756E6420616E642074686520467572792062792057696C6C69616D204661756C6B6E65720A4672696461204B61686C6F0A4D616769636B20696E205468656F727920616E6420507261637469636520627920416C6569737465722043726F776C65790A44756B6520456C6C696E67746F6E0A4D61676E69666963656E74204F6273657373696F6E206279204C6C6F796420432E20446F75676C61730A436861726C657320497665730A41204661726577656C6C20746F2041726D732062792045726E6573742048656D696E677761790A5468656120766F6E20486172626F750A4265726C696E20416C6578616E646572706C61747A20627920416C667265642044F6626C696E0A526F6265727420436170610A4C61756768696E6720426F79206279204F6C69766572204C612046617267650A436F6C657474650A4C6F6F6B20486F6D65776172642C20416E67656C2062792054686F6D617320576F6C66650A41756775737465204C756D69E872650A50726F6365737320616E64205265616C69747920627920416C66726564204E6F727468205768697465686561640A4461756768746572206F662045617274682062792041676E657320536D65646C65790A41646F6C706820476F74746C6965620A476F6F642D42796520746F20416C6C205468617420627920526F62657274204772617665730A5468652053746F7279206F66204D79204578706572696D656E74732077697468205472757468206279204D616861746D612047616E6468690A4C696C20477265656E0A416C6C205175696574206F6E20746865205765737465726E2046726F6E7420287472616E736C6174696F6E29206279204572696368204D617269612052656D61727175650A54686520476F6F6420436F6D70616E696F6E73206279204A2E20422E205072696573746C65790A487564736F6E20526976657220427261636B657465642062792045646974682057686172746F6E0A546865204D617261636F7420446565702062792041727468757220436F6E616E20446F796C650A416E6E6520536578746F6E0A4C657320456E66616E7473205465727269626C6573206279204A65616E20436F63746561750A446F6473776F7274682062792053696E636C616972204C657769730A446F6E616C6420476F696E65730A reply clarkdale 19 hours agorootparentMaybe more helpful would be base 256 so it renders in ascii. reply MichaelZuo 19 hours agoprev [–] I think practically a lot more things are in the public domain because they were distributed via the internet to certain countries that until recently had a much lower bar than the Berne Convention standard and entered the public domain there. Edit: or had at least one person make at least one copy there. If the law allowed everyone to make one copy for private use. Or perhaps I’m misunderstanding ? e.g. Ethiopia which only shifted to a quasi Berne standard midway through 2004. reply hoppyhoppy2 19 hours agoparentSomething going into the public domain in one country doesn't mean that it automatically enters the public domain in every country. It may make it easier to find a free copy on the internet, but it is still often under copyright in other countries. The article is about works that are going into the public domain legally, not just practically. reply MichaelZuo 18 hours agorootparentBut they would be legally in the public domain for anyone in Ethiopia? Ethiopian citizens, residents, etc., can clearly own a copy for each work. reply gbear605 19 hours agoparentprevIt depends on your purpose. If you’re an American who wants to make movies for Americans, that are derivative of one of these works, this matters a lot. reply MichaelZuo 18 hours agorootparentAnd if you’re a filmmaker who intends to establish their business in Ethiopia…? reply Calavar 18 hours agorootparentThen you can establish your business in Ethiopia, but good luck selling it in the US! reply MichaelZuo 4 hours agorootparentThat’s not an issue for the filmmaker who sticks to online distribution, there are branches of well known payment processors in Addis Ababa. They won’t be making blockbuster movies anytime soon, but I imagine their business would be greater than zero? reply Mindwipe 21 minutes agorootparent> That’s not an issue for the filmmaker who sticks to online distribution, there are branches of well known payment processors in Addis Ababa. Yes it is once they distribute to people online outside of Ethiopia. And the payment providers will enforce that. reply pbhjpbhj 19 hours agoparentprev [–] It would still be infringing to bring a copy into some countries from Ethiopia. UK doesn't have fair use, for example. reply MichaelZuo 18 hours agorootparent [–] Yes, but on the internet someone in the UK can just get a connection to some server in Ethiopia hosting this vast treasure trove of works. So maybe not de jure, but probably de facto. Edit: And maybe it is de jure too for those on a ship on the high seas? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On January 1, 2025, new works will enter the public domain, including those by creators who died in 1954 and 1974, and U.S. films/books from 1929.",
      "The Public Domain Review, a UK Community Interest Company, uses profits for community benefit and relies on reader donations.",
      "Readers are encouraged to explore resources like John Mark Ockerbloom’s “Public Domain Day Countdown” and Communia’s Manifesto for more information on the public domain."
    ],
    "commentSummary": [
      "In 2025, the expiration of the last patents on the h.264 (AVC) video codec will make it freely available, potentially increasing accessibility and reducing costs for users.",
      "The HEVC codec remains entangled in patent complexities, highlighting ongoing challenges in video codec accessibility.",
      "Discussions on copyright reform are ongoing, with proposals for shorter terms and more compulsory licenses to balance creators' rights with public access, addressing concerns that current long terms primarily benefit corporations."
    ],
    "points": 339,
    "commentCount": 276,
    "retryCount": 0,
    "time": 1733084222
  },
  {
    "id": 42290996,
    "title": "How to Study Mathematics (2017)",
    "originLink": "https://www.math.uh.edu/~dblecher/pf2.html",
    "originBody": "How to Study Mathematics Lawrence Neff Stout, Department of Mathematics, Illinois Wesleyan University, Bloomington, Il 61702-2900 This essay describes a number of strategies for studying college level mathematics. It has sections entitled How is college mathematics different? What should you do with a definition Theorems, Propositions, Lemmas, and Corollaries Fitting the subject together How to make sense of a proof Developing technique A few final suggestions Return to L.N. Stout's home page. How is college mathematics different from high school math? In high school mathematics much of your time was spent learning algorithms and manipulative techniques which you were expected to be able to apply in certain well-defined situations. This limitation of material and expectations for your performance has probably led you to develop study habits which were appropriate for high school mathematics but may be insufficient for college mathematics. This can be a source of much frustration for you and for your instructors. My object in writing this essay is to help ease this frustration by describing some study strategies which may help you channel your abilities and energies in a productive direction. The first major difference between high school mathematics and college mathematics is the amount of emphasis on what the student would call theory---the precise statement of definitions and theorems and the logical processes by which those theorems are established. To the mathematician this material, together with examples showing why the definitions chosen are the correct ones and how the theorems can be put to practical use, is the essence of mathematics. A course description using the term ``rigorous'' indicates that considerable care will be taken in the statement of definitions and theorems and that proofs will be given for the theorems rather than just plausibility arguments. If your approach is to go straight to the problems with only cursory reading of the ``theory'' this aspect of college math will cause difficulties for you. The second difference between college mathematics and high school mathematics comes in the approach to technique and application problems. In high school you studied one technique at a time---a problem set or unit might deal, for instance, with solution of quadratic equations by factoring or by use of the quadratic formula, but it wouldn't teach both and ask you to decide which was the better approach for particular problems. To be sure, you learn individual techniques well in this approach, but you are unlikely to learn how to attack a problem for which you are not told what technique to use or which is not exactly like other applications you have seen. College mathematics will offer many techniques which can be applied for a particular type of problem---individual problems may have many possible approaches, some of which work better than others. Part of the task of working such a problem lies in choosing the appropriate technique. This requires study habits which develop judgment as well as technical competence. We will take up the problem of how to study mathematics by considering specific aspects individually. First we will consider definitions---first because they form the foundation for any part of mathematics and are essential for understanding theorems. Then we'll take up theorems, lemmas, propositions, and corollaries and how to study the way the subject fits together. The subject of proofs, how to decipher them and why we need them, comes next. And finally, we will discuss development of judgment in problem solving. To contents What should you do with a definition? A definition in mathematics is a precise statement delineating and naming a concept by relating it to previously defined concepts or such undefined concepts as ``number'' or ``set.'' Careful definitions are necessary so that we know exactly what we are talking about. Unfortunately, for many of the concepts in undergraduate mathematics the definition is rather difficult to understand, so often at low levels an intuitive feeling for the meaning of a term is all that is given or required. This intuitive feeling, while necessary, is not sufficient at the college level. This means that you need to grapple with and master the formal statement of definitions and their meanings. How do you do it? Step 1. Make sure you understand what the definition says. This sounds obvious, but it can cause some difficulties, particularly for definitions with complicated logical structure (like the definition of the limit of a function at a point in its domain). Definitions are not a good place to practice your speed reading. In general there are no wasted words or extraneous symbols in established definitions and the easily overlooked small words like and, or, if ... then, for all, and there is are your clues to the logical structure of the definition. First determine what general class of things is being talked about: the definition of a polynomial describes a particular kind of algebraic expression; the definition of a continuous function specifies a kind of function; the definition of a basis for a vector space specifies a kind of set of vectors. Next decipher the logical structure of the definition. What do you have to do to show that a member of your general class of things satisfies the definition: what do you have to do to show that an expression is a polynomial, or a function is continuous, or a set of vectors is a basis. Step 2. Determine the scope of the definition with examples. Most definitions have standard examples that go with them. While these are useful, they may lead you to expect that all examples look like the standard example. To understand a definition you should make up your own examples: find three examples that do satisfy the definition but which are as different as possible from each other; find two examples of items in the general class described by the definition which do not satisfy it. Prove that your five examples do what you think they do---such proofs are usually short, follow the structure of the definition quite closely, and help immensely in understanding the definition. These examples should be neatly written up so that you can refer to them later. Your own examples will have more meaning for you than mine or the book's when it comes time to review. Step 3. Memorize the exact wording of the definition. This step may sound petty, but the use of definitions demands knowledge of exactly what they say. For this reason you can count on being asked for the statement of any definition on an exam. The importance of precise wording should have been made clear by your examples in step 2 and it certainly is essential in the proof of theorems. Solid knowledge of definitions is more than a third of the battle. Time spent gaining such knowledge is not wasted. To contents Theorems, Propositions, Lemmas, and Corollaries Occasionally definitions are useful in and of themselves, but usually we need to relate them to each other and to general problems before they can be made to work for us. This is the role of theory. The relative importance and the intended use of statements which are then proved is hinted at by the names they are given. Theorems are usually important results which show how to make concepts solve problems or give major insights into the workings of the subject. They often have involved and deep proofs. Propositions give smaller results, often relating different definitions to each other or giving alternate forms of the definition. Proofs of propositions are usually less complex than the proofs of theorems. Lemmas are technical results used in the proofs of theorems. Often it is found that the same trick is used several times in one proof or in the proof of several theorems. When this happens the trick is isolated in a lemma so that its proof will not have to be repeated every time it is used. This often makes the proofs of theorems shorter and, one hopes, more lucid. Corollaries are immediate consequences of theorems either giving special cases or highlighting the interest and importance of the theorem. If the author or instructor has been careful (not all authors and instructors are) with the use of these labels, they will help you figure out what is important in the subject. The steps to understanding and mastering a theorem follow the same lines as the steps to understanding a definition. Step 1. Make sure you understand what the theorem says. Part of this is a vocabulary problem. Theorems use terms which have been given precise meanings by definitions. So you may need to review the definitions to understand the words in a theorem. Next you need to understand the logical structure of the theorem: what are the hypotheses and what are the conclusions? If you have several hypotheses, must they all be satisfied (that is, do they have an and between them) or will it suffice to have only some of them (an or between them)? In most cases theorems require that all of their hypotheses be satisfied. A theorem tells you nothing about a situation which does not satisfy the hypothesis. The hypothesis tells you what you must show in order for the theorem to apply to a particular case. The conclusions tell you what the theorem tells you about each case. Step 2. Determine how the theorem is used. This involves finding examples of problems for which the theorem gives a technique for finding the answer. Make up your own problems and show how the theorem helps with them. Again writing this down will help solidify the theorem in your mind and make it easier to review. Step 3. Find out what the hypotheses are doing there. This is a little tricky and is probably more important in advanced courses than in beginning courses. What you do is find examples (either your own or someone else's) to show that if individual hypotheses are omitted the conclusion can be false. For instance, in calculus many theorems have a hypothesis that the functions involved be continuous; why does the theorem fail if this hypothesis is left out? Usually an example will make this clearer than an examination of how the hypothesis was used in the proof will. A catalog of such examples can be very useful. See for instance the books Counterexamples in Analysis and Counterexamples in Topology. In some cases a hypothesis is included just because it makes an otherwise complicated proof easy. This means that you may not be able to find examples which illustrate that each hypothesis is essential. Step 4. Memorize the statement of the theorem. If you are going to use a theorem you need to know exactly what it says. Pay particular attention to hypotheses. We will take up proofs later, but for now let me note that it is not a good idea to try to memorize the proof of a theorem. What you want to do is understand the proof well enough that you can prove the theorem yourself. To contents Fitting the subject together Mathematics is not a collection of miscellaneous techniques but rather a way of thinking---a unified subject. Part of the task of studying mathematics is getting the various definitions and theorems properly related to each other. This is particularly important at the end of a course, but it will help you make sense of the content and organization of a subject if you keep the overall organization in mind as you go along. There are two techniques I know of which help with this process: working backwards and definition-theorem outlines. Step 1. Working backwards In general there is very little difficulty recognizing a major result when you get to it. A good way of seeing how a subject works is to examine the proof of a major result and see what previous results were used in it. Then trace these results back to earlier results used to prove them. Eventually you will work your way back to definitions (unless there are theorems given without proof---in calculus, for instance, the proof of the intermediate value theorem is often omitted because it requires a deeper understanding of the real numbers than is usually available at the beginning of calculus 1). This information can be put into a sort of genealogy chart for results which helps you see at a glance how the results fit together. It helps to have descriptive names for your theorems and lemmas. Such a chart might look sort of like this: Mean Value Theorem Rolle's Theorem Candidate Lemma Meaning of the sign of the derivative Definition of derivative Definition of max and min Existence of max and min for continuous functions on [a, b] Definition of max and min Definition of closed interval Least upper bound axiom Definition of continuity With such a road map through the theory you should be able to tell how you got where you are, if not where you are headed. Step 2. Make a definition-theorem outline. After you have worked backwards to the definitions for each of your major theorems in a section you should have a good idea of which results are needed before others can be proved. Some definitions will not make sense until certain theorems are proved (dimension of a vector space is an example: you can't give a number a name until you know you are talking about a unique number, and that requires a theorem). A definition theorem outline is an arrangement of the results in an order so that each result is introduced before it is needed in a proof. It should contain the precise statements of all definitions and theorems and a sketch of the proof of each theorem. A sketch of a proof will show which earlier results were used and how they were combined. It will usually omit calculations simplifying forms of expressions and routine checks that hypotheses are satisfied. This outline is both a good way to start a review and a useful thing to have to refer to. To contents How to make sense of a proof College level mathematics demands that the student work through (or at least sit through) many proofs. This is often unpopular, I think largely because it is hard work to follow a proof, hard work of an unfamiliar kind. Proof is largely absent or at most optional in high school math; it is neither absent nor optional in college mathematics. Step 1. Make sure you know what the theorem says. If you have the hypotheses mixed up with the conclusions you will not know what assumptions may be made nor will you know what conclusion you are trying to reach. Step 2. Make a general outline of the proof. This is what you would do in a definition theorem outline. See what the previous results used are and find out what the basic strategy of the proof is. On this pass through omit the details, else you miss the direction of the road by too close examination of the bricks in the pavement. Most theorems have the form of implications: if the hypotheses are true, then the conclusion follows. The easiest structure for a proof to use is to assume the hypotheses and combine them, using previous results, to reach the conclusion through a chain of implications. Some proofs use other strategies: contrapositive argument, reductio ad absurdum, mathematical induction, perhaps even Zorn's lemma (a form of the axiom of choice). The more complicated kinds of proofs will need to be discussed in class. Step 3. Fill in all of the details. Once you understand the strategy of the proof concentrate on its tactics. Almost all expositions of proofs in undergraduate mathematics textbooks (and all expositions at higher levels) leave out many routine steps. An expression will be simplified without showing exactly how to get from one line to the next. Fill in these details. A theorem will be quoted and applied without explicitly checking all of its hypotheses. Check them. Some parts of the proof will be outlined with the details left to the reader. Put in those details. When you finish you should know why each step follows from what came before. You may not see how anyone could have thought to do the proof that way, but you should be able to see that it is correct. Why bother with proofs at all? For the mathematics major this question is easy to answer---a large portion of mathematics consists of proofs. The mathematician enjoys the logical puzzle which must be solved to find a proof and obtains aesthetic satisfaction from elegance in proofs. The student who wants to major in mathematics should do so because of ability in deciphering and producing proofs and enjoyment derived from proof well done. The major should also have skill in solving problems and finding applications as well. But many of you will say ``I'm not a math major; I want applications so that I can use tools from mathematics in my field'' or ``I'm just taking this course because it's a requirement in my major and I sort of liked math in high school.'' Why should you learn about proof? The applications you meet in other fields are not likely to look exactly like the math textbook applications, which are chosen for their appeal to a traditional audience (largely engineers) and for their representative character. Other applications work similarly, though not exactly the same way. This means that you need to learn how to apply the concepts in your math courses to situations not discussed in those courses. (There is no way that a course could discuss every possible known application: about 500 papers appear every two weeks with applications, and those are just the applications published in the ``mathematical'' literature!) To do so you need the best possible understanding of the mathematics you want to apply. Certainly this means that you need to know the hypotheses of theorems so that you don't apply them where they won't work. It is helpful to know the proof so that you can see how to circumvent the failed hypothesis if necessary. One of the major pitfalls of applied mathematics, particularly as practiced by nonmathematicians, is the danger of conveniently overlooking the assumptions of a mathematical model. (Mathematicians trying to do applied mathematics are more likely to fall into the trap of making models which have no relationship to reality.) Many applications consist of recognizing the definition of a mathematical concept phrased in the terms of another discipline---the more familiar you are with the definition, the more likely you are to be able to recognize the disguised version elsewhere. The nuances of definitions are made most clear in the proofs of propositions relating definitions and pointing out unexpected equivalent variants, some of which may look more like a situation in another discipline than the precise form used in your math class. Arguments for theory as an aid to application rest on an obvious premise: it is much easier to apply something you understand thoroughly. This is, however, a better argument for care in learning the statements of theorems than it is for spending time understanding proofs. The best justification for the inclusion of proof in math classes is more philosophical: Proof is the ultimate test of validity in mathematics. Once one accepts the logical processes involved in a proof no further observation or change in fashion will change the validity of a mathematical result. No other discipline has such an immutable criterion for validity. The major benefit derived from an education is the ability to think clearly and make considered judgment. Each discipline should teach a body of material, appropriate modes of thought in dealing with that material, and a means for determining the validity of the conclusions reached. A chemistry curriculum with no lab work would be seriously deficient since experiment is the test of validity in science. Similarly mathematics without proof is severely deficient, indeed it is not mathematics. To contents Developing technique About a third to a half of any math course deals with technique---the process of making theorems work for you in specific situations rather than in the general setting in which they are usually stated. Sometimes this is fairly easy: many proofs give explicit constructions which you follow for the special case. In these situations the only problems are with algebraic and trigonometric manipulations and keeping track of where you are in the process. In other situations (technique of integration is a good example) there are lots of approaches which might apply to a given problem and several tricks which might be used to make the problem more tractable. For these you need to develop judgment. Step 1. Read through the theorems and examples. Some students make the whole process of learning how to do problems more difficult by acting like it had no connection with the other material in the course. Often problems follow a pattern which is given explicitly in the proof of the major theorem they follow. Knowing the general pattern in advance is easier than trying to find it by trial and error. Step 2. Work enough problems to master the technique. At this stage you should work enough problems so that the single technique which the problems illustrate is firmly in your mind. Since you have ultimate responsibility for your education, you should take the initiative to work enough problems for your own practice needs. This may well be more problems than are assigned to be turned in. Step 3. Work a few problems in as many different ways as possible. Too often the practice obtained in step 2 leads the student to think that there is only one approach to each problem. Sometimes one approach is easy and another is complicated, but often several different attacks will work equally well. Complicated approaches give the student practice in solving problems which take more than one step and more than one technique. Step 4. Make yourself a set of randomly chosen problems. One difficulty with learning many techniques to solve a particular kind of problem is that you have to figure out which technique to use before you can get to work on a solution. This is exacerbated by the tendency for problems to be grouped so that the appropriate technique to use is the one which immediately preceded the problem set. Putting two or three problems from each of the problem sets in a chapter on technique on 3 by 5 cards and then shuffling the cards will give you a set of problems on which to practice deciding which technique to use. To contents A few final suggestions Mathematical prose has a very low redundancy rate and mathematics is a very cumulative subject. Pay close attention as you read---once introduced, a concept is rarely repeated and it will be assumed later. Allow yourself adequate time to read the book before starting the problems. Few students write fast enough to get complete and readable notes in class. For this reason it is useful to go back over your class notes shortly after each class and make a complete, clean copy with all of the definitions and theorems clearly stated. This practice will also help you identify parts you don't understand so you can ask your professor about them in a timely fashion. Do not let yourself fall behind. Mathematics requires precision, habits of clear thought, and practice. Cramming for an exam will not only fail to produce the desired result on the exam, it will also reinforce a bad habit---that of trying to do mathematics by memorization rather than understanding. A good night's sleep and a clear head will serve you better than last minute memorization. To contents lstout@sun.iwu.edu",
    "commentLink": "https://news.ycombinator.com/item?id=42290996",
    "commentBody": "How to Study Mathematics (2017) (uh.edu)215 points by ayoisaiah 21 hours agohidepastfavorite84 comments vouaobrasil 8 hours agoPhD in math here with several published papers. And my recommendation is a metaprinciple: enjoy mathematics. Benjamin Finegold said similarly that the secret to chess is to enjoy every move. Personally, I had no trouble in mathematics, ever. And I think the reason for that is that I really enjoy just doing it, writing symbols down, learning about new theories, and even inventing my own. Not everyone will enjoy mathematics at first sight. But I think at least 50% of that can be explained by the lack of obvious paths to enjoy mathematics. Obviously, most mathematics taught in high-school is not taught as it should be: a cool artistic logical pursuit that has all kinds of fun in it. So my advice is to really find a mentor who already has found that path and let them show you how to enjoy math. Believe me, I've tutored a lot of people, many of which initially disliked math and found it difficult. But after a few tutoring sessions, I could see a little sparkle in their eye that said, \"hey, this might be cool\". So before you apply logic, studying, and other tedious \"productivity\" measures to your math learning, make sure you find a way to enjoy it first. reply kzz102 15 minutes agoparentOne of the biggest problem of maths education is that they are taught by people who dislike it. They think of maths as eating bitter medicine or training of a complex, rigid skill. The way maths is taught by them is clumsy and authoritarian, and this makes the students either passive or rebellious. On a side note, recently the government of Manitoba in Canada removed requirement for maths teachers to take university maths courses. This is being pushed strongly by the education departments of university, which shows how much these maths teachers hate maths. reply ajb117 1 hour agoparentprevI get what you're saying, but I think confidence is actually more important than enjoyment. A lot of people confuse enjoying [x] with enjoying being good at [x]. This is why so many students switch subjects later on in life; when a field suddenly doesn't come naturally to them, they seek to play to their strengths elsewhere. Problems occur when they quit too early, and building confidence early on is important for stopping this. In my experience, when you think you're bad at something, it's almost impossible to enjoy doing it, which makes preliminary mastery actually the first step to enjoyment and therefore downstream success. reply jackschultz 2 hours agoparentprevGreat comment, and applies to any activity we do. Seems like in western culture we're told to not have enjoyment, to work hard and grind, and that any bit of enjoyment means you're not trying hard enough and lacking. Much better to have the attitude of allowing the happiness to come from the actions. reply sourcepluck 6 hours agoparentprevAny tips or anecdotes for us (well, me) about the fun things you'd do with students to encourage that joy? I am not a PhD, but have done a fair bit of tutoring young people in maths. I feel similarly to you there, and am always on the lookout for new ways to foster that feeling of mathematics being fun and wonderful. It can be hard. The feeling a lot of young people pick up - of maths being roughly akin to pointless abject suffering - is so strongly rooted in young people sometimes, and can be strongly connected to feelings of inadequacy and shame and so on. reply sourcepluck 6 hours agorootparentWrote that reply then clicked on your page to see if there were a blog or anything where you elaborate already on these topics - I've already read a couple of articles from your substack! Oops, hah, I did not realise I was writing to the person behind that. I'd filed it away in my head as something to sink more time into when I've the emotional space to do so, but essentially I find it quite courageous to go against the grain in a world where it's perhaps harder and harder to do so. Everyone has a hot take, of course, but the hot takes are usually very much within the bounds of acceptable discourse. Anyway, more power to you, in your endeavours. reply andrepd 6 hours agoparentprev> Believe me, I've tutored a lot of people, many of which initially disliked math and found it difficult. But after a few tutoring sessions, I could see a little sparkle in their eye that said, \"hey, this might be cool\". Exactly the same for me. Honestly, the satisfaction of seeing that \"sparkle\" in the eye of an initially unmotivated or discouraged student is probably among the most fulfilling moments of my professional career. reply gowld 3 hours agoparentprevThis post is quite rude and dismissive to people who enjoy math but struggle with it. reply js8 3 hours agorootparentWhy? Can you elaborate what you find so offensive? reply zaik 34 minutes agorootparentI don't think it's offensive, but it may be frustrating advice for people who cannot enjoy math at all. reply litoE 17 hours agoprevMy secret sauce as an undergraduate for all my math courses was solving problems. Solve all the problems at the end of the chapters in the textbook. Find other textbooks in the library and solve all the problems at the end of their chapters. In graduate school that was expanded: take every chapter of the textbook and rewrite it, filling in all the intermediate steps of every proof, those where the author writes \"it follows that ...\" or \"from which it's obvious that ...\" reply Ntrails 5 hours agoparent> take every chapter of the textbook and rewrite it, filling in all the intermediate steps of every proof, those where the author writes \"it follows that ...\" or \"from which it's obvious that ...\" I recently-ish had a read through some of my old fundamental/pure maths notes from Uni, including plenty of proofs. The damned things are littered with steps which were \"obvious\" to my smug self-satisfied 20 year old self but impenetrable to me reading without much context 20 years later. Git. reply schneems 5 hours agoparentprevI’ll add: go to the TA study sessions. And try to answer questions in class. Even if you have nothing to ask, use that time to do homework, listen to other people’s questions. Yes, you’ll learn more. Also, TAs will recognize you put in the effort so if you’re arguing for partial points or you’re really close to a cutoff grade they will be more likely to bump you up versus someone they’ve not seen or noticed all semester. reply InkCanon 14 hours agoparentprevI did this but faced enormous problems with transfer/recalling definitions/etc. what helped me improve several fold in approximately exponential value 1) Drill/spaced repetition basic definitions. The Cornell note taking method is convenient to do this while taking notes. 2) Keep a diary of thoughts, things you couldn't solve or did solve. Especially identifying problems, what works or doesn't, why something went wrong. Metacognitive thinking was really useful for transferring problems to solving new ones. 3) A study group involving a lot of us explaining to each other. reply Tainnor 2 hours agoparentprevAn undergraduate text like Pugh's Real Mathematical Analysis can have over 500 exercises. Trying to do them all on top of your regular workload seems excessive, unless maybe it's an area you really want to specialise in. I also tend to find certain kinds of exercises boring because they appear terribly unmotivated. On the other hand, I second the suggestion to engage more deeply with the subject material itself: Modify assumptions and see what happens. Can certain proofs be simplified? Try to reconstruct proofs by only memorising certain key details. Try to draw a mental map of a subject and how the different theorems and definitions relate together. Try to implement some proofs in an automated theorem prover, if that's your thing. reply bluGill 2 hours agorootparent500 exercises over 10 weeks is 50 a week, or 10 a day. This leaves you weekends free, and you get the final weeks (typically 5) of the semester left for all those projects that are due at the end of the semester but you can't start early. 500 a day is too much, but you should be able to do 50-100 problems a day in your study time. And others report that when you do this you get good at doing math and so it takes less time. Sadly I didn't do that. I graduated and do okay, but I encourage everyone to do better than me. As I get close to retirement I need a few people who are still working to build things (and medical treatments) that makes my life better (and in turn take some of that money I saved up over the years for your own life) reply max_ 13 hours agoparentprevBrute force exposure to as many problems and solutions as possible is the same way AIs learn. That strategy in my opinion is not optimal for humans. What we need to do is develop math resources that can help students learn things analytically & conceptually. Like how they learn biology. reply conjectures 9 hours agorootparentIt's not quite like that. Maths content is the ne plus ultra of conceptual. It's totally possible to slog through a chapter of a maths text and feel like we got it. But it turns out our 'understanding' was a facade. We can't apply the concepts in a new situation. Exposing ourselves to feedback via problem sets reveals this. reply funcDropShadow 7 hours agorootparentprev> Like how they learn biology. How do they learn biology? reply bluGill 1 hour agorootparentAn lot of brute force memorizing details that everyone else would look up. Biology feeds to medical fields where you have to have details you rarely used memories because you might be doing surgery on someone who is slightly different from typical and have to figure out what that thing in the way is - can you safely move/remove it or not. (I'm not a doctor, real doctors can come up with plenty of much more likely real world examples of situations where you don't have time to look up some detail) reply g9yuayon 12 hours agoparentprev> Solve all the problems at the end of the chapters in the textbook It really depends on the textbook, isn't it? I find it impossible to solve all the problems in CLRS, for example. Our professor assigned one of the problems about universal hashing, and it took me hours to get the key insights to find the correct proof. I can't imagine how one can solve all the problems given so many competing priorities, except for a few truly talented. reply liontwist 3 hours agorootparentDid you do the few hundred problems before that build up the techniques and theorems? > given so many competing priorities, Undoubtedly, the best time to do this was when you were young. The second best time is now. Pick a book and work on a problem or 2 every day. It will likely take 6 months or so but you will learn the material. This is an incredible way to level up in a technical area. Imagine how much knowledge is in CLRS. reply funcDropShadow 6 hours agorootparentprevThe point is that you gradually get better at it. And by gradually I mean exponentially. If you start with a problem at then end of the book, it'll be very difficult. reply fastasucan 11 hours agoparentprevI agree with this. In the end it is beneficial with repetition for learning anything, and I think subjects with calculations are partly a craft where you need to get it in your fingers. I teach a subject, it not maths but a (different) engineering subject and what I see is that the students don't buy text books any more (for several understandable reasons), but one of the things they miss out on is a lot of practice problems. This becomes evident in the types of errors they do at the exam, and an apparent lack of system for laying out their problem and solution in an understandable matter for themselves (and me). reply badpun 10 hours agorootparentMore often than the \"at the end of the chapter\" math problems are not about calculations, but are essentialy mini-problems requiring mini proofs. It's not about 1-1 applying the knowledge that was in the chapter, but rather about creative thinking. Some problems can take hours or days to solve. There isn't much to \"get in your fingers\". reply bluGill 1 hour agorootparentYou need to get creativity in your fingers. You can get good at solving problems. As those who did much better than me in school proved. reply sourcepluck 6 hours agoparentprevSomeone correct me if I'm misremembering, but I think Donald Knuth wrote somewhere that when he'd be assigned the odd-numbered problems, he'd always do them, and do the even-numbered problems as well. reply raincole 13 hours agoparentprevMy issue during my undergraduate days was that the textbook didn't come with solutions, and I really don't know how to unstuck myself when I got stuck (except asking the TA in the office hours). reply deskr 5 hours agorootparentYep, I hated that. Spending an hour on a problem and get a solution you can't verify. It very much affected my motivation on starting on the problem knowing that I wouldn't know if I was right or wrong. reply abdullahkhalids 13 hours agorootparentprevAsk your peers. As an ex-prof, one of the most prominent sources of missed potential I see in undergrads is not working with their peers. Like serious 2-6 hours sessions of collaborative problem solving. reply deskr 5 hours agorootparentYou're right, but it depends on your peers. If you're the only one motivated to do extra work, your peers are unlikely to want to spend hours on verifying the solution. Guess it shows the importance of good peers. reply Koshkin 2 hours agoparentprevSolving problems (and trying to invent proofs) is indeed the way to enjoy math. reply sage76 5 hours agoparentprevThis is not applicable to every book. Try doing this with Casella and Berger and see how difficult it becomes. Some books can take many many months to finish off like this, and most courses only cover a small percentage of the book. reply Abecid 10 hours agoparentprevpo-shen loh? reply processunknown 16 hours agoparentprev“The proof is left as an exercise …” reply lordnacho 8 hours agoprevThe first thing you have to get used to when moving from school to uni is being utterly lost and defeated. At the end of high school, I could do everything. I finished my IB exams with huge amount of time to spare, the only thing holding me back was being able to write fast enough. It had been months since I saw a regular curriculum question that I didn't know how to do. Any marks I lost were just trivial errors. When I got to university, there would be question sheets where I would look at the questions and wonder what it had to do with the lectures I had just been in. As in \"I went to this lecture, and I'm supposed to use the information to answer these questions, but I don't even know what the questions mean\". The learning happens when you are doing this frustrating head-bashing. You read, you read more, you fill a notebook with useless derivations, and eventually you things start to take shape. This could take the entire week's worth of time, just sitting there fumbling about. The difference is that in uni, the amount of material is so vast you cannot explain it to someone in the time that you have. The students have to pick up some key ideas, and then fill in all the details themselves by pouring hours into it on their own. reply funcDropShadow 6 hours agoparent> The first thing you have to get used to when moving from school to uni is being utterly lost and defeated. Very well said, at the university where I studied, there was a pre-semester math repetition course. It was a week long and started with addition of natural numbers. After two days, everything I learned in 13 years of school had been repeated. That was a brutal resetting of expectations. But it made everybody clear, that this is not school anymore. That a different kind of work and focus would be necessary. reply adamddev1 37 minutes agoprev> \"A good way of seeing how a subject works is to examine the proof of a major result and see what previous results were used in it. Then trace these results back to earlier results used to prove them. Eventually you will work your way back to definitions\" I find the parallels between proofs and programs to be fascinating. We could write an analogous thing for programming: \"A good way of seeing how a sort of program works is to examine one of the popular programs/libraries and see what functions were used in it. Then look inside of those functions and see what functions are used inside of those. Eventually you will work your way back to the lower level primitives.\" reply sourcepluck 5 hours agoprev> The Germans have aptly called Sitzfleisch the ability to spend endless hours at a desk, doing gruesome work. Sitzfleisch is considered by mathematicians to be a better gauge of success than any of the attractive definitions of talent with which psychologists regale us from time to time. Stan Ulam, however, was able to get by without any Sitzfleisch whatsoever. After his bout with encephalitis, he came to lean on his unimpaired imagination for his ideas, and on the Sitzfleisch of others for technical support. The beauty of his insights and the promise of his proposals kept him amply supplied with young collaborators, willing to lend (and risking to waste) their time. Taken from Gian-Carlo Rota in The Lost Cafe, a quote I found here http://www.romanpress.com/Rota/Rota.php reply deskr 5 hours agoparentArticle on Sitzfleisch: https://www.bbc.com/worklife/article/20180903-to-have-sitzfl... reply jayhoon 18 hours agoprevInterestingly, this guide states that the intuitive understanding of maths is only suitable at the school level but not for the university. In his recently published book \"Mathematica: A Secret World of Intuition and Curiosity\", David Bessis argues that the intuition is the \"secret\" of understanding maths at all levels. Not sure what conclusion to draw from here, but my (rather dated) experience with university maths tells me that the intuition is a powerful tool in developing the understanding of the subject. reply chongli 14 hours agoparentThis seems like a contradiction but I don’t think it is. What it’s really saying is that experience is a precondition for intuition. When a high school student looks at a high school math problem they’re drawing on all of their experience in K-12 math to get intuition for how to solve the problem. When they leave high school to study math in undergrad they struggle because their experience is no longer sufficient. They’re faced with a lot more abstract problems and the demands for rigour are much higher. The problems also tend to operate at higher levels on Bloom’s taxonomy [1] than high school math, something with which the average high school student would have little or no experience. It is this unfamiliar territory where intuition is hard to come by. After gaining more experience (later undergrad and into grad school and beyond) the intuition starts to come back. But it’s fundamentally a different kind of intuition. In high school math it was often a visual/geometric intuition that teachers were trying to build. In higher math it’s an intuition for abstractions and for the tools you need to attack problems. This is really no different from a programmer looking at a problem and saying “I need a hash map and then this problem is trivial.” [1] https://en.wikipedia.org/wiki/Bloom's_taxonomy reply 609venezia 18 hours agoparentprevPossible harmonization of the two ideas: the intuition that we go into math at high school level can help serve us at that level of math. We have some idea of geometry-like objects and 2d-calculus like curves from our everyday life At university level the objects become more abstract, so the intuition we use in normal daily life may no longer apply. New kinds of intuition may develop but it takes work, including lots of time spent with the formal processes and calculations along with reflection on that time, and the active creation of new metaphors to drive the intuition. For example, I still remember a professor using \"Ice-9\" (from _Cat's Cradle_) as a metaphor for how proving some local property of a holomorphic function on the complex plane made that property true for its global behavior reply tseid 17 hours agoparentprevI think he's saying here that intuition is sufficient for high school math, but not sufficient for college. That's not to say that it isn't necessary, only that it isn't sufficient. reply Tazerenix 14 hours agoparentprevThis is related to Terence Tao's notion of the stages of mathematical rigor. As Tao puts it, the value of intuition becomes much higher in the post-rigorous stage once you have sufficiently developed your technical skills. https://terrytao.wordpress.com/career-advice/theres-more-to-... reply agumonkey 9 hours agoparentprevThat topic fascinates me. As a kid, a lot of topics felt intuitive, and college became a pit of darkness. Makes me wonder what in our brains makes something feel natural, obvious, with that feeling of playfulness and certainty .. while some times you're drowning in a blur. reply alganet 17 hours agoparentprevI read the book. To me, what it says is \"intuition can be honed and it is powerful, but hard to pass along to others\". Just that. Bessis actually mentions examples of how intuition and technique complement each other nicely. reply Syzygies 13 hours agoprevThis article is missing the \"meta\" in studying mathematics. Creative introspection into how one learns begins to really pay off partway through college. One's relationship to convention becomes as important as one's relationship to technique. Understanding the \"whole\" of something involves understanding the biases that shape the presentation you're seeing. You'll probably want to shed them. This applies whether one wants to change math or just learn it. A passive stance, trying to do what others want, is a recipe for frustration. reply youoy 13 hours agoprevIn my case I realized I achieved a whole new level when I tried to do all of the proofs by myself. I used to read the whole subject a few times and after that, I would go one definition/proof at a time and first to recall the statement and then try to do the proof by myself. By doing that you indirectly achieve all of what the article says. A nice thing I realised is that once I did that, almost all of the exercises that were complex before for me, turned out to be straightforward. It was like a cheat code where I almost did not need to do any exercises. I used to teach at the uni at several levels, and every year I would ask if anyone tried to recall the proofs of the theorems at home. and no one did. They were always shocked when I told then they should do it. reply fastasucan 11 hours agoparent>I used to teach at the uni at several levels, and every year I would ask if anyone tried to recall the proofs of the theorems at home. and no one did. They were always shocked when I told then they should do it. I try to do this as well. If you combine this with understanding the definitions of the various units you more or less will have the textbook in your head, some assembly required. reply dang 17 hours agoprevRelated: How to Study Mathematics (2017) - https://news.ycombinator.com/item?id=26524876 - March 2021 (73 comments) How to Study Mathematics (2017) - https://news.ycombinator.com/item?id=16392698 - Feb 2018 (148 comments) Bonuses: How to Study Mathematics? - https://news.ycombinator.com/item?id=23074249 - May 2020 (31 comments) How to self-study mathematics from the undergrad through graduate level? - https://news.ycombinator.com/item?id=18939913 - Jan 2019 (227 comments) How to self-learn math? - https://news.ycombinator.com/item?id=16562173 - March 2018 (211 comments) Others? reply nioj 16 hours agoparentAnother bonus: How to self-learn math? - https://news.ycombinator.com/item?id=16562173 - March 2018 (211 comments) reply dang 15 hours agorootparentThanks! Added above. reply yzydserd 4 hours agoprevI’ve been very pleasantly surprised by my recent experience, having signed up for MathAcademy.com after reading about it on HN. Now in my 50’s, I wanted to relearn high school maths from 35 years ago and I scooted through their Foundations series (now half way through Foundations 3, rapidly accruing like 9000 xp in 9 weeks, 2 hours a day). Planning in 2025 to do 1-3 university level courses with them at a slower pace. It’s suited my way of learning as an autodidact: enjoyable; measurable; adaptable level of hardness; no hitting of a “wall” or “unmet dependencies”; thorough explanation of problems I didn’t solve. Perhaps my biggest realisation was that one can learn without needing to document many notes to revise/memorise, because experience and spaced repetition suffices. I’m taking a Xmas hiatus which will be the real test of baked learning. reply 0xRusty 15 hours agoprev> Do not let yourself fall behind. That hit home. I'm afraid I was one of those lazy math undergrads who struggled with a few of the first year topics, didn't get help or put the hours in and never really recovered. I will maintain I think the teaching was very poor in places (lots of \"just trust me\" handwashing and \"this is obvious so I'll leave it to you to complete\" which for an 18 year old frankly sucks). A system that lets you get 30% in \"analysis 1\" and then just marches you straight into \"analysis 2\" next semester and expects you to just pull your socks up isn't much of a system to me. Honestly I'm afraid my time at university doing maths was miserable. I should have done something more applied like engineering or CS probably. Someone once told me \"If you like biology at school, do psychology at university. If you like chemistry, do biology. If you like physics, do chemistry. If you like maths, do physics and if you like philosophy, do maths\". I should have listened. reply vector_spaces 14 hours agoparentI agree that the system is garbage. School ruins math -- but I just want to share that nothing stops you from revisiting math later as a fully actualized adult, without the bullshit time constraints and grade pressure. Math isn't well suited to time pressure -- in my view math is about spending lots of time with ideas and playing with them and taking risks and making dumb guesses, it's moonshot through and through. At its core is play, and time pressure doesn't serve anyone well. Anyway, if you were interested in it, you can always revisit it, and even try taking a class again if you ever want to. There are lots of great books out there for self-learners, and lots of communities of folks learning together reply jll29 15 hours agoparentprev> \"If you like biology at school, do psychology at university. If you like chemistry, do biology. If you like physics, do chemistry. If you like maths, do physics and if you like philosophy, do maths\" This is good advice if the objective is to do well regarding grade results. If you want to get down to the bottom of things, to understand everything and to solve fundamental questions of science, you might well want to invert the advice: If you liked biology, study chemistry, for the processes of life are (electro-)chemical processes. If you liked chemistry, study physics, for the processes of molecules and atoms, their formation and reactions, are physical processes. ... reply jgwil2 15 hours agorootparentThe way I heard this phrased back in the day was, \"Biology is really chemistry, chemistry is really physics, physics is really math.\" Never heard psychology or philosophy added to the chain. reply xanderlewis 5 hours agoparentprev> If you like maths, do physics and if you like philosophy, do maths. Seems like I made the right decision! (I did maths.) reply cod1r 14 hours agoprevSometimes it is weird to see a webpage from my university on here. Usually UH doesn't get a lot of attention for its STEM. At least from my anecdotal experience. reply selimthegrim 6 hours agoparentUH is famous at least in the high temperature superconductivity world. reply ChaitanyaSai 16 hours agoprev>Step 3. Memorize the exact wording of the definition. Huh. Any mathematicians who want share their own opinions and experiences about this? This pretty much goes completely against my experience with other grad school level neuroscience/ML You don't want to be so familiar with stuff as to make it second nature but NOT from memorization. That, at least an other areas, leads to surface level recognition Does the author mean internalize and not memorize? reply kmill 13 hours agoparentThe author really does mean memorize. To engage with pure mathematics, you must know the definitions, since the definitions are the bedrock of the subject. If you don't know the axioms of a topology, how can you check for yourself whether something forms a topological space? Or without knowing the exact definition of continuous, how can you know whether a proof of continuity is correct? Without knowing the definitions, you can't really know mathematics. To be clear, this does not mean memorizing all the theorems. Getting to know the theorems (and solving problems) is what helps you internalize the subject. Math is the art of what's certain, and knowing exactly what the objects of the subject are is necessary for that. Theorems are derived from the definitions, but definitions can't be derived. In my experience with a math (undergrad and PhD), I realized I had to know definitions to feel competent at all. In my teaching, it's hard to convince students to actually memorize any definitions — so many times students carry around misconceptions (like that \"linearly independent\" just means that no vector is a scale multiple of any other vector), but if they just had it memorized, they might realize that the misconception doesn't hold up. Math is weird in that the definitions are actually the exact truth (by definition! tautologically so), so it does take some time to get used to the fact that they're essential. reply xanderlewis 5 hours agorootparent> the definitions are actually the exact truth (by definition! tautologically so) It’s easy to forget that non-math people find this — the idea that the definition is its own ‘model’ rather than an approximation of something more ‘real’ — somewhat hard to stomach. Outside of pure mathematics the idea is that mathematics is a tool for (usually lossy) modelling of reality, not a collection of already perfectly well-motivated objects to be studied in their own right. reply gowld 3 hours agorootparentMore generally and symmetrically: When you are studying science and technology, and the math theorem doesn't match experiment, the theory is probably wrong (or incomplete, missing factors), so you can discard it or try to improve it. When you are studying math, and the intuition doesn't match the theorem, the intuition is probably wrong (or incomplete). reply kmill 1 hour agorootparentThings get a bit messier once you're doing research mathematics — definitions don't just come from nothing, and a good definition is one that serves its theorems. Definitions can be \"wrong\" (they might be generalizable, they might have unexpected pathological examples, etc.), and it's the result of lots of hard work by lots of mathematicians throughout history that we have the definitions we enjoy the use of today. But yeah, while studying math, I think it's similar to learning programming — don't blame the compiler for your mistakes, it's a well-tested piece of software. reply vector_spaces 14 hours agoparentprevThe reasoning is that, similar to memorization of times tables, being able to recall a definition or a theorem and its context / assumptions \"automatically\" without needing to use your brain frees you to worry about higher order activities. Being able to apply a theorem over and over again ultimately builds mastery and internalization. Counterintuitively, mathematicians like being \"brain-off\" as much as possible -- you want to be able to read a phrase like \"closed convex subset of a Hilbert space\" and effortlessly think to yourself \"oh! there's a unique norm minimizer\" -- if you have to piece that together from scratch every time, you're going to have a hard time -- reading papers and learning new fields becomes a dreadful slog, similar to how math in general becomes a slog for kids who don't memorize their times tables. reply Skeime 9 hours agoparentprevI think they mean memorize. Most students simply lack the mathematical experience to internalize a definition correctly without simply memorizing it. They will forget an \"only if\", or accidentally swap two quantifiers around, or conflate two variables that need to be kept separate, etc. This is okay! They're students on the way to gain that experience. At some point you can and will go over to internalizing, instead. But as advice to students just starting out, memorizing is the way to go. reply jyounker 3 hours agoparentprevMathematical definitions are precise. If you miss one part of a definition, then you cannot actually understand what the definition actually means. You have to internalize the meaning also, but you must know the definition precisely. reply Koshkin 53 minutes agorootparentIt is also useful to know all alternative definitions, if any exist (they often do). reply wisty 16 hours agoparentprevThe author also said to internalise it, but the simple fact is that in a proof-y math test you can't just say \"well it should converge if everything is well behaved\", the edge cases matter and definitions set the boundaries. reply majikaja 11 hours agoparentprevMaybe not the exact wording but the exact meaning... reply PandaRider 18 hours agoprevI appreciated the article for emphasising memorising definitions and statement of theorems... But not for proofs. For proofs, a general outline would be sufficient. reply xanderlewis 17 hours agoparentFor proofs, I find it a good idea to memorise (or at least implicitly retain) the reason a result is true. So, yes, an outline, but minus any of the implementation details of the proof. I kind of think every book in the definition-theorem-proof style should really be definition-theorem-reason-proof. The reason part being essentially a one or two line natural language summary of ‘why the proof works’ — something that is almost always possible and is enlightening and conducive to efficient memorisation, but that for some reason is very rarely written down explicitly. reply jll29 15 hours agorootparent> definition-theorem-reason-proof Along the lines of your own argument: even better would be reason0-definition-reason1-theorem-reason2-proof reply gowld 3 hours agorootparentprevI think a better word is \"motivation\" -- why we chose this option at this juncture instead of many other options. Yes, it's a \"reason\", but \"reason\" already means something else. The \"Reason\" as result is true is that it follows from the previously established axioms via logical reasoning. reply youoy 13 hours agoparentprevIt depends if you want to be able to prove new things by yourself or not. If you want to do it, then you definitely need to understand /recall all of the whys of every section of the proof. They are all there for a reason. If you don't, you just want the intuition of why the whole theorem is true. reply abdullahkhalids 13 hours agoparentprevYou should definitely memorize most of the \"basic\" (and short) proofs in some field you are super interested in. The intermediate and advanced proofs, only the outline is sufficient. reply cubefox 8 hours agoprevDoes someone have experiences with using o1-preview for mathematics? A while ago I tried to use GPT-4o and Claude Sonnet for certain algebraic questions related to probability theory. The models did help significantly (at least relative to my rather limited ability), though they also often produced wrong results and struggled to make progress on harder questions. reply Koshkin 2 hours agoprev\"Young man, in mathematics you don't understand things. You just get used to them.\" - John Von Neumann reply richrichie 11 hours agoprevThe biggest obstacle for me was (and is) fear or failure when it comes to solving exercises. reply revskill 14 hours agoprev [–] I consider college math is all about abstraction. reply jyounker 3 hours agoparent [–] It's about relationships. Abstraction is the tool for understanding those relationships. reply Koshkin 2 hours agorootparent [–] That is why we have category theory and homological algebra. https://en.wikipedia.org/wiki/Abstract_nonsense reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The essay by Lawrence Neff Stout provides strategies for studying college-level mathematics, emphasizing the need for a deeper understanding of theory, definitions, and proofs compared to high school math.- Key strategies include understanding precise definitions, memorizing and comprehending theorems and proofs, and using techniques like working backwards to see how concepts interconnect.- The essay highlights the importance of practicing problems to develop techniques and judgment, and stresses that understanding is more crucial than memorization in college mathematics."
    ],
    "commentSummary": [
      "Enjoyment in studying mathematics can enhance understanding and lead to success, as many students struggle with the subject due to unengaging teaching methods.- Having a mentor to reveal the enjoyable aspects of math and building confidence early can prevent students from giving up prematurely.- Effective strategies include engaging deeply with material, solving problems, and understanding definitions and proofs, balancing intuition with rigorous study for mastery."
    ],
    "points": 215,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1733089851
  },
  {
    "id": 42291386,
    "title": "Gene behind orange fur in cats",
    "originLink": "https://www.science.org/content/article/gene-behind-orange-fur-cats-found-last",
    "originBody": "www.science.org Verifying you are human. This may take a few seconds. www.science.org 8ebd91c51f102f4b",
    "commentLink": "https://news.ycombinator.com/item?id=42291386",
    "commentBody": "Gene behind orange fur in cats (science.org)211 points by rbanffy 20 hours agohidepastfavorite117 comments fsckboy 4 hours agohumans share the vast majority of our genes with other mammals; are a cat's genes for fur the same genes humans have for body hair? does this orange gene for cats bear any resemblance to ginger genes in humans? reply aithrowawaycomm 3 hours agoparentNo, cats are a bit different from other mammals: In most mammals, including humans, red hair is caused by mutations in one cell surface protein, Mc1r, that determines whether skin cells called melanocytes produce a dark pigment or a lighter red-yellow pigment in skin or hair. Mutations that make Mc1r less active cause melanocytes to get “stuck” producing the light pigment. But the gene encoding Mc1r didn’t seem explain where cats’ orange fur came from. It isn’t located on the X chromosome in cats or any other species—and most orange cats don’t have Mc1r mutations. “It’s been a genetic mystery, a conundrum,” says Greg Barsh, a geneticist at Stanford University. reply FollowingTheDao 6 hours agoprevHa! Oh my, this is exactly what I am studying. MC1R is a G-Protein Coupled receptor (GOPCR) and the ARHGAP36 gene/enzyme is a GTPase activator for the Rho-type GTPases by converting them to an inactive GDP-bound state. Meaning it breaks down GTP (Guanosine Triphosphate is responsible for providing energy to the receptor.) If there is too little or too much GTP it will effect the sensitivity of the MC1 Receptor and will change the color of the hair accordingly. BTW, humans have these same genes so the same will apply to humans. I am studying these GPCRs and how they play a role in Mood and Chronic Illness like Long COVID and ME/CFS. Too little GTP and all of the 850+ G-Protein Coupled receptor will be effected. These include Serotonin, Dopamine, and Epinepherine Receptors. You can see the whole list of them here: https://www.guidetopharmacology.org/GRAC/ReceptorFamiliesFor... I has an enzyme deficiency (PNP) that makes me have way too much GTP and it has me disabled but getting better after this discovery I made about Receptor Energy. I too much GTP also leads to grey hair. Also, GLP1 is a GPCR! This might have been the most serendipitous or God given thing for me to read today because I have been pretty suicidal and thinking I was wrong about my idea, but this literally is another piece I can put in my paper that proves that if the serotonin receptor does not have enough GTP, it does not matter how much serotonin you have, you will not get cellular activation via cAMP. Adding here that cats had more Arghap36 RNA in orange regions. More Arghap36 means less GTP. Less GTP means less energy for the MC1R receptor which means less black/brown pigment is produced. reply waterpowder 8 hours agoprevThe (orange) cat has your pipe https://www.youtube.com/watch?v=NAh9oLs67Cw reply RobRivera 55 minutes agoparentThis has opened my 3rd eye, engaged a critical reassessment of how I choose to engage in this world, and left me asking myself how I can leave my mark on this world commensurate to the truth the pipe strip has enlightened my world reply quantadev 13 hours agoprevThere's no such thing as a \"gene for a trait\". Just because some gene perfectly correlates with some trait doesn't mean that's all the gene does. There will be 1000s of things that gene does, and merely one of them just happens to be hair color. reply superb_dev 13 hours agoparentThis is covered at the end the article: > No one previously knew Arhgap36 could affect skin or hair coloration—it is involved in many aspects of embryonic development, and major mutations that affect its function throughout the body would probably kill the animal, Barsh says. But because the deletion mutation appears to only affect Arhgap36 function in melanocytes, cats with the mutation are not only healthy, but also cute. reply quantadev 59 minutes agorootparentIt's more clear to just say \"There's no such thing as a gene for a trait\", especially since the title of the article says the opposite, and is wrong. That's why I corrected it. reply ourmandave 6 hours agorootparentprevit is involved in many aspects of embryonic development Ugh, you don't find this out by not looking at a lot of cute orange (and non-orange) cat embryos under a microscope. =( reply FollowingTheDao 5 hours agorootparentprevThat is just them making sht up. Arhgap36 does not just effect the MC1R receptor. It is GTPase activator for the Rho-type GTPases. And there are a lot of them: https://en.wikipedia.org/wiki/Rho_family_of_GTPases These polymorphisms not diminish GTP totally, it just lowers it. reply m00x 11 hours agoparentprevOr it could do nothing, it could do a single thing, or it can do several things. Sometimes it just influences color, but often not. They already addressed it in the article, so I'm not sure what the purpose of this comment is. reply Sharlin 9 hours agorootparentTrying to sound smart by nitpicking on article titles without actually reading the article is more or less the national sport of Hacker News. reply quantadev 3 hours agorootparentActually 'nitpicking' is when you assume someone didn't read an article whenever their post references the title of the article. Clarifying something in an article in a more clear way than the article did is fine, and especially when even the actual title itself is misleading as well. reply latexr 2 hours agorootparent> Clarifying something in an article in a more clear way than the article did is fine, and especially when even the actual title itself is misleading as well. That is very true. However, to myself (and presumably the other commenters) it didn’t look like that’s what you were doing. Though I’ll certainly give you the benefit of the doubt and trust that was your intention. In the future, it might help if you also quote the specific part of the article that does the clarification, so it’s clear you’re aware of it and are providing additional context or better wording. reply quantadev 57 minutes agorootparentNo, because both of these things are true at the same time: 1) I WAS correcting the TITLE. 2) I DID read the article. reply gweinberg 16 minutes agoparentprevIt's vastly more misleading to say \"correlates with\" as if the correlation weren't causal. reply amelius 5 hours agoparentprevYes. For example, in humans, being red haired correlates with fear of dentists. https://pmc.ncbi.nlm.nih.gov/articles/PMC2740987/ reply AceyMan 2 hours agorootparentCould it be because red hair people require additional anesthesia for equivalent effect, which perhaps they don't get at their dentist, resulting in more pain: Ergo, they're scared of visiting their dentist. reply EasyMark 4 hours agorootparentprevCould that also be that dentists are meaner to red haired people too? Subconsciously of course. reply polishdude20 13 hours agoparentprevSo is it more like there's some secondary \"thing\" that is for hair color and it looks at that gene to determine the color but there can also be another secondary \"thing\" such as hair length that also uses that same gene to determine that? reply hobofan 11 hours agorootparent> it looks at that gene I think it may help to not think of genes as lookup tables. It's rather that genes act as blueprints for compound (protein/RNA) factories (which can also be potentially turned off/on). So the compounds that is produced may interact with the compound(s) that end up resulting in the hair color, and it may also interact with the compound that results in the hair length. The problem is that any compound may in theory interact with any other compound, and there currently exists no way to 100% determine that they won't (for most compounds), which leaves open a huge space of possible chains of interactions. In practice there are many interactions for every biological compound, which is the reason why medicine is so hard to develop and usually has a risk of side-effects. reply XorNot 9 hours agorootparentIt's also worth remembering that a cell isn't a well structured environment - it's essentially a bag of chemicals, with some slightly smaller bags of other chemicals inside it. While there's a lot of mechanisms which are adding order and structure to what happens, it's all still just a big concentrated aqueous solution of everything in the cell, diffusion processes and mixing and all. So statements like a gene being \"switched on\" are very much an abstraction: whereas switching on a data line in a chip puts a very nice neat little voltage potential somewhere, switching on a gene basically just means the concentration of some \"chemical\" (protein) starts increasing and getting mixed into the cell (or ejected out of it by interacting with a bunch of other floating around things). reply timschmidt 7 hours agorootparent> While there's a lot of mechanisms which are adding order and structure to what happens, it's all still just a big concentrated aqueous solution of everything in the cell, diffusion processes and mixing and all. Global variables, blegh. /s reply quantadev 13 hours agorootparentprevA rough analogy is like if you flipped some bits in a computer program's executable code and then looked at what happens when you run that code, and notice a specific feature now no longer works for example. Yeah you found a way to break something, but you don't know what else you broke, or what the full effects were of flipping those bits. reply astrange 11 hours agorootparentAlso, the results with the current methods could be fine for imprecise purposes. Say, if you had some cat embryos and wanted to know which one was orange for your designer kitten. But in the future it might not be good enough for later more precise interventions; if you just start editing cats to make them orange now you're going to find out what the side effects are. reply willis936 9 hours agorootparentprevI think with modern computer systems the effects would be relatively silo'd. I have the sense that each gene are more multi-use and interconnected than execution memory. reply quantadev 3 hours agorootparentThe more complex a system is the more reuse of code there generally is. (i.e. multiple different functions making use of some shared function) That means if you randomly alter something, it's more likely to have multiple effects rather than a single effect. BTW, most times in software parlance \"silo'd\" means \"not interconnected\" rather than \"interconnected\". reply willis936 31 minutes agorootparent>BTW, most times in software parlance \"silo'd\" means \"not interconnected\" rather than \"interconnected\". And that's how I meant it. Software doesn't reuse execution bits as much as genes. reply FollowingTheDao 5 hours agorootparentprevNo. ARHGAP36[1] stimulates GTP catabolism. And since MC1R is a g protein coupled receptor and uses GTP as an energy source, changes in the levels GTP will change the response of MC1R[2] to MSH and ACTH. It will react less strongly when there is lower GTP, and more strongly when there is higher GTP[3]. [1]https://www.uniprot.org/uniprotkb/Q6ZRI8/entry [2] https://www.uniprot.org/uniprotkb/Q865E5/entry [3]https://www.nature.com/scitable/content/ne0000/ne0000/ne0000... reply astrange 11 hours agoparentprevGenetics solve causation issues by putting \"correlation does not equal causation\" at the start of their paper and then writing the rest of it as if it did equal causation. I don't fault them much, it's kind of hard to do an experiment here, but don't believe their results too hard. reply smackeyacky 8 hours agorootparentThe fun thing is that they drift. So you might breed orange cats by selecting cats that are orange but over time the correlation to the genome might become weaker. It happens quite a bit in livestock genetics. reply quantadev 3 hours agorootparentYeah, not even DNA is fully in control. Michael Levin's planarian worms experiments show that he can use an electromagnetic field for a short time in a certain way, and create a two-headed worm (normally it's one of course) that will persist to all it's offspring even though there's been ZERO alterations to DNA. So there is still much about the emergent complexity that we do not know. reply asdff 1 hour agorootparentEven weirder conceptually than that: you have the same dna in all your cells. So why is your brain your brain, your eyes your eyes, and so on? Then you learn that not all genes are expressed all the time. Then you learn even among those expressed genes that they could have been spliced differently and resulted in completely different protein structures. No magnets needed for this example of phenotypes extending beyond mere dna sequence, just a mirror to look at yourself. reply quantadev 48 minutes agorootparentThat was correct and made sense until the magnets and mirrors stuff which would require a bit of mental telepathy to understand what you meant. reply FollowingTheDao 5 hours agorootparentprevBut they are literally showing the mechanism, so the reason for causation is there. reply pilif 14 hours agoprevTangentially: interesting how we are looking for this for 60 years and then manage to have two separate teams come up with the answer independently and practically at the same time reply dj_mc_merlin 13 hours agoparenthttps://en.wikipedia.org/wiki/Multiple_discovery reply downboots 11 hours agorootparentinefficiency reply dekhn 1 hour agorootparentmodern science isn't intended to minimize inefficiency- it's intended to maximize the rate of stochastic discovery. reply ffsm8 4 hours agorootparentprevCollaboration comes with it's own challenges and inefficiencies reply toast0 1 hour agorootparentprevIndependent concurrent discovery might be better than independent confirmation. reply micromacrofoot 49 minutes agorootparentprevsurvivorship bias reply atrus 13 hours agoparentprevIf it's affordable and practical for one person/team to do it, it's probably affordable for two or more groups to do it. reply benatkin 7 hours agoparentprevThe rate of research could have sped up thanks to technological advancements. reply FollowingTheDao 6 hours agoparentprevThe reason this happened G Protein Coupled Receptors, which is what MC1R is, are being heavily investigated for drug development. https://www.nature.com/articles/s41392-024-01803-6 reply hinkley 1 hour agorootparentWell it makes sense since orange boys act like they’re on drugs. reply bitwize 14 hours agoparentprevIt orange cat genes when it's orange-cat-gene time. reply telgareith 14 hours agorootparentThey got the braincell reply dkarbayev 8 hours agoprevSounds like a solid candidate for Ig Nobel Prize award. reply lenerdenator 3 hours agoparentWhen you consider that most of human discovery eventually relates back to wanting to take care of cats, I'd say it's more Nobel worthy. reply runeks 8 hours agoprevI didn't even know we were looking for that gene reply NoRagrets 13 hours agoprevSomewhat related: https://vgl.ucdavis.edu/species/cat The Cat genetics lab at UC Davis has been cracking the feline code for many years now. reply trebligdivad 5 hours agoparentI loved reading about cat coat genetics; they seem to be 'right we've got it!' and someone then goes 'ok then what about this furball....' reply astrange 10 hours agoparentprevWhat's the point of a genetics test for a coat pattern? Can't you just look at the cat? (Same question about covid/flu tests - a lot of people act like taking a test is part of a treatment regimen, but it kinda isn't. But those are still useful since you might be an asymptomatic spreader.) reply TeMPOraL 6 hours agorootparent> What's the point of a genetics test for a coat pattern? Can't you just look at the cat? When you're trying to understand a complex system, it's best to start with things you can actually see directly. Doubly so, if you're going to try and change something. Coding analogy: there's a reason \"Hello World\" is about printing stuff to console, and the very first thing you do when writing to a new target, or reconfiguring some application, or testing unfamiliar commands, etc. is something where observable output directly reflects the changes to inputs you make. Otherwise, you don't know whether you're doing things right, or doing anything in the first place. WRT. COVID/flu tests. You or your kid come to the doctor with a running nose and some cough. The test is useful to tell you whether you're dealing with one of the \"heavy hitters\" like the flu or COVID or RSV, or just a bog standard ${random kindergarten viral infection} that's treated by nose cleaning + anti-cough medicine + pretending you're not sick anymore, or whether this stuff is bacterial and maybe you need another swab to pick the right antibiotics. reply Etheryte 10 hours agorootparentprevThe point is trying to understand how genetics works. Traits we can easily check for are easier to test against than say personality traits which are more subjective. reply astrange 7 hours agorootparentA $60 customer genetics test doesn't seem like it's for educational purposes. https://vgl.ucdavis.edu/test/charcoal-pattern-bengals reply toast0 1 hour agorootparentprev> Same question about covid/flu tests - a lot of people act like taking a test is part of a treatment regimen, but it kinda isn't. First line treatment for covid/cold/flu is fluids, rest, and at least a bit of isolation. But if you need more than that, it's useful to test for what you might have, as further treatments vary. reply pvaldes 6 hours agorootparentprevSometimes in animals the same color can be result of several different genes working in different paths to achieve a similar result. This info is relevant for breeding purposes. Chicken color genetics is evil, for example but understanding it allowed to have self-sexing chicken. This saves millions of dollars to the farms. Some color patterns in cats carry healthy problems. Breeding for white cats with blue eyes for example is discouraged, because they born deaf. reply catlikesshrimp 8 hours agorootparentprevOne of the most visible features of a cat: its hair. Imagine a startup for cloning your cat, except you can change some features. It can start with hair color. After some rounds it might add size, eye color, hair or no hair, chubbiness. You name it. It may start in any BRICS country, and then expand! Tourist trips to cat cloning resorts. Check our CATLOG! reply pnutjam 3 hours agorootparentThat's how we get Cheshire cats... reply bloopernova 2 hours agorootparentIs that a Windup Girl reference? https://en.wikipedia.org/wiki/The_Windup_Girl (Cheshires are a thing in that book, bio-engineered cats that can change fur colour like a chameleon/octopus/squid to exactly match their surroundings. They breed and devastate the world's population of small animals such that there's a bounty for every one killed. I'm trying to remember where I read a great critique of the book, especially the cheshires, but I'm not finding it) Edited to add: found it: https://www.nyrsf.com/2015/10/eric-schaller-the-problem-with... reply pnutjam 1 hour agorootparentYes! reply xeonmc 14 hours agoprevTangentially related, but https://old.reddit.com/r/OneOrangeBraincell/ is a subreddit dedicated to specifically orange-furred cats. I was expecting quite something else when I first read the subreddit name. reply aitchnyu 13 hours agoparentDo orange cats act/be dumber than the rest or is it Internet legend? reply yabones 5 hours agorootparentIt's largely folklore, but there is a kernel of truth to it. Orange cats have a pretty \"silly\" attitude, kind of like golden retriever dogs. Black/tux cats are usually docile and friendly. Female tortiseshell/tabby cats are lunatics. But there's also quite a bit of individual variation. I'd say it varies more from cat to cat than the \"breed\" would imply. reply phaedrus 3 hours agorootparentCan confirm female tortiseshell cats are, yes- lunatic is the best way to describe their personality. My family had several and they were all a combination of smart and crazy, mostly the latter. As a teenager I found it amusing to teach one of them to ride on my shoulder like a pirate's parrot. An unintended consequence is that she would also do that to house guests, who weren't expecting it, making a five foot vertical leap from ground level from behind to perch on their shoulder. reply EasyMark 4 hours agorootparentprevThis describes my black cat exactly; she runs and hides at the door bell, but when it all calms down, she inevitably comes out to see who it is and within half hour is trying to climb in their lap and is begging for attention from the new person. I had an orange cat once and he pretty much thought he was the family dog. I’ve never seen a friendlier cat, didn’t matter if it was another cat, a dog, a person, or a squirrel; anyone who would play he was ready to go. reply flkiwi 2 hours agorootparentWe have an almost pure-black void that screams bloody murder at everything--pick her up, screaming; walk in the room, screaming; give her food, screaming--but cuddles more aggressively than any cat I've ever heard of and purrs nonstop (when she isn't screaming). She hisses as a primary communications mechanism. I hear that cat hiss more in a day than every hiss from every other cat I've ever had combined. She also panic poops when the other cat gets within 5 feet. She is pure, hate-filled joy. reply toast0 1 hour agorootparent> She is pure, hate-filled joy. Cats are full of love and murder; all in different amounts. Sounds like yours has lots of bothcats with the mutation are not only healthy, but also cute. The latter of which being the likely reason that the trait has become so widespread in the artificial selection environment of the domestic cat population. reply zanderwohl 2 hours agoparentThe history of domestic cat coats is really fascinating. It wasn't until photography and cartoon artists like Louis Wain that breeding cats for coat color and pattern really took off. Then pedigree cat shows emerged and cats exploded in variety. We've only been aggressively selectively breeding cats for about 150 years, so there's a lot of potential still. reply TheSpiceIsLife 6 hours agoparentprevI understand your use of artificial in this context, However, it’s not like humans are en mass terminating the non-orange siblings, to the best of my knowledge. reply Sharlin 37 minutes agorootparentThere's nothing about selection, natural or artificial, that implies killing. Orange cats spread around the world because the coat color was considered pretty and a novelty. I've read that ancient Egyptians already prized orange cats and likely intentionally bred more of them (which may or may not have involved culling). reply hansonkd 5 hours agorootparentprev> it’s not like humans are en mass terminating the non-orange siblings, Black cats for instance are disproportionately more likely to be euthanized: > According to the National Library of Medicine, of all cats in shelters, black cats have the highest rate of euthanasia at a rate of 74.6%, and the lowest rate of adoption at 10% of any cat. [1] Not to many breeders out there breeding black cats. Many people who feed feral cats want to feed the cute ones, so there is at least some bias towards propagating certain colors of cats. (This is one of the reasons I picked the black cat at the shelter and one of the best decisions I ever made. Such a kind and loving cat) [1] https://www.msj.edu/news/2024/02/the-power-of-black-cats.htm.... reply tumsfestival 4 hours agorootparentWhat the hell, my favorite cats are the black ones. Even my cat is black and I chose her from her litter specifically because of her color. Is this all because of stupid superstition? reply fiedzia 1 hour agorootparentMost reasonable explanation I've heard is that black cats typically don't look good on pictures. Or maybe they are just overlooked by people visiting animal shelters? reply Brybry 5 hours agorootparentprevBut we do en masse sterilize cats, including feral cats. reply yapyap 8 hours agoprev [–] does this also make them stupid? reply lenerdenator 3 hours agoparent [–] Kitteh is not stupid. Kitteh just doesn't care. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Recent studies indicate that the Arhgap36 gene, which influences GTP levels and MC1R receptor activity, may be responsible for orange fur in cats, rather than the Mc1r gene linked to red hair in humans.",
      "This discovery underscores the complexity of genetics, where a single gene can impact multiple traits, and is part of broader research efforts to understand genetic mechanisms.",
      "Understanding these genetic factors has potential implications for breeding and health, contributing to the popularity of orange cats, often noted for their playful behavior."
    ],
    "points": 211,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1733093992
  },
  {
    "id": 42292294,
    "title": "’Brain rot‘ named Oxford Word of the Year 2024",
    "originLink": "https://corp.oup.com/news/brain-rot-named-oxford-word-of-the-year-2024/",
    "originBody": "Home » Press Office » ‘Brain rot’ named Oxford Word of the Year 2024 ‘Brain rot’ named Oxford Word of the Year 2024 Language and Literacy 2 December 2024 2 min read Following a public vote in which more than 37,000 people had their say, we’re pleased to announce that the Oxford Word of the Year for 2024 is ‘brain rot’. Our language experts created a shortlist of six words to reflect the moods and conversations that have helped shape the past year. After two weeks of public voting and widespread conversation, our experts came together to consider the public’s input, voting results, and our language data, before declaring ‘brain rot’ as the definitive Word of the Year for 2024. Why ‘brain rot’? ‘Brain rot’ is defined as “the supposed deterioration of a person’s mental or intellectual state, especially viewed as the result of overconsumption of material (now particularly online content) considered to be trivial or unchallenging. Also: something characterized as likely to lead to such deterioration”. Our experts noticed that ‘brain rot’ gained new prominence this year as a term used to capture concerns about the impact of consuming excessive amounts of low-quality online content, especially on social media. The term increased in usage frequency by 230% between 2023 and 2024. The first recorded use of ‘brain rot’ was found in 1854 in Henry David Thoreau’s book Walden, which reports his experiences of living a simple lifestyle in the natural world. As part of his conclusions, Thoreau criticizes society’s tendency to devalue complex ideas, or those that can be interpreted in multiple ways, in favour of simple ones, and sees this as indicative of a general decline in mental and intellectual effort: “While England endeavours to cure the potato rot, will not any endeavour to cure the brain-rot – which prevails so much more widely and fatally?” The term has taken on new significance in the digital age, especially over the past 12 months. Initially gaining traction on social media platform—particularly on TikTok among Gen Z and Gen Alpha communities—’brain rot’ is now seeing more widespread use, such as in mainstream journalism, amidst societal concerns about the negative impact of overconsuming online content. In 2024, ‘brain rot’ is used to describe both the cause and effect of this, referring to low-quality, low-value content found on social media and the internet, as well as the subsequent negative impact that consuming this type of content is perceived to have on an individual or society. It has also been used more specifically and consistently in reference to online culture. Often used in a humorous or self-deprecating manner by online communities, it is strongly associated with certain types of content—including creator Alexey Gerasimov’s viral Skibidi Toilet video series, featuring humanoid toilets, and user-generated ‘only in Ohio’ memes, which reference bizarre incidents in the state. This content has given rise to emerging ‘brain rot language’—such as ‘skibidi’, meaning something nonsensical, and ‘Ohio’, meaning something embarrassing or weird—which reflects a growing trend of words originating in viral online culture before spreading offline into the ‘real world’. Now also gaining momentum is a broader, more serious conversation about the potential negative impact that excessively consuming this content might have on mental health, particularly in children and young people. Earlier this year, a mental health centre in the US even published advice online about how to recognize and avoid ‘brain rot’. Speaking about this year’s selection process and the 2024 winner, Casper Grathwohl, President of Oxford Languages, said: “It’s been insightful and deeply moving to see language lovers all over the world participate and help us select the Oxford Word of the Year 2024. “Looking back at the Oxford Word of the Year over the past two decades, you can see society’s growing preoccupation with how our virtual lives are evolving, the way internet culture is permeating so much of who we are and what we talk about. Last year’s winning word, ‘rizz’, was an interesting example of how language is increasingly formed, shaped, and shared within online communities. ‘Brain rot’ speaks to one of the perceived dangers of virtual life, and how we are using our free time. It feels like a rightful next chapter in the cultural conversation about humanity and technology. It’s not surprising that so many voters embraced the term, endorsing it as our choice this year. “I find it fascinating that the term ‘brain rot’ has been adopted by Gen Z and Gen Alpha, those communities largely responsible for the use and creation of the digital content the term refers to. These communities have amplified the expression through social media channels, the very place said to cause ‘brain rot’. It demonstrates a somewhat cheeky self-awareness in the younger generations about the harmful impact of social media that they’ve inherited.” Find out more about our Word of the Year shortlist for 2024, and discover our winners from the past two decades as we celebrate 20 years of Oxford Word of the Year. Related articles Your Oxford Story Your Oxford Story: Casper Grathwohl 30 October 2024 Casper shares his career story, how Oxford Languages has changed, and insights on Oxford Word of the Year. Read more 4 min read Expert in the Spotlight Oxford Word of the Year: the evolving role of a lexicographer 22 October 2024 Fiona McPherson shares her view as a lexicographer on Oxford Word of the Year. Read more 3 min read Language and Literacy What does Oxford Word of the Year mean to us? 30 November 2023 Oxford Word of the Year is an increasingly anticipated part of the calendar. Find out more about our process of Read more 4 min read",
    "commentLink": "https://news.ycombinator.com/item?id=42292294",
    "commentBody": "’Brain rot‘ named Oxford Word of the Year 2024 (oup.com)192 points by ChrisArchitect 17 hours agohidepastfavorite148 comments 0xDEAFBEAD 12 hours ago>‘Brain rot’ is defined as “the supposed deterioration of a person’s mental or intellectual state, especially viewed as the result of overconsumption of material (now particularly online content) considered to be trivial or unchallenging. Also: something characterized as likely to lead to such deterioration”. Trivial / unchallenging is not the problem, in my opinion. Sometimes it's good to relax and give yourself a break. Staring at the wall and letting your mind wander is trivial and unchallenging, but it's not going to rot your brain. The problem is overstimulating shortform content that wrecks your attention span. But I don't believe super-longform content is necessarily the solution, either. If you force yourself to think about the same thing for too long, it effectively becomes work, and generates the same sort of mental fatigue. That's sort of pointless if you're trying to relax and recover. I think it's best to aim for a sweet spot of \"unchallenging and also understimulating\". reply viraptor 9 hours agoparentI don't think you necessarily disagree with the definition. You mentioned \"Sometimes\", while they explicitly said \"overconsumption\". Most things are ok in moderation, including stupid relaxing. reply SebastianKra 8 hours agorootparentI think what the definition fails to capture is the addiction-like behavior that this specific kind of content can induce. You feel yourself getting more tired and frustrated by the minute, yet you don't stop. reply lupusreal 8 hours agorootparentIts a definition, not an encyclopedia article or something. The definition of alcoholism probably doesn't go into the ways people fall into alcoholism either. reply automatic6131 9 hours agorootparentprev>Most things are ok in moderation, including stupid relaxing For real. Sometimes, I think that the binary nature of language, things are either good or bad, there is no single word to describe n-shape dose responses except in aphorisms like \"too much of a good thing\", means that it's too easy to autopilot into thing A is either good for you, or bad for you. One beer with friends is good for you but ten is not. The optimal amount is somewhere between (it's slightly less than 2). reply vasco 11 hours agoparentprevThe word doesn't have to be right, it's just what it means. > I think it's best to aim for a sweet spot of \"unchallenging and also understimulating\". Press 'X' for Doubt. reply 0xDEAFBEAD 11 hours agorootparentWhat do you do to relax? reply asyx 9 hours agorootparentNot the person you replied to but most of the things I do in my free time are somewhat challenging and stimulating. Programming on something that is very new to me and outside of what I do in my job, a challenging video game, hobbies that do involve my computer (got into 3d modeling a while ago) and hobbies that don’t involve my computer (leather working as of quite recently). I also used to learn languages for fun but I figured out really quickly that I like linguistics more than languages so I can be a real smart ass about grammar in various languages but couldn’t order a beer in most of them. I get really annoyed and in a really bad mood if I just watched a movie after work and went to bed. Or just hung out with people. I don’t feel like I got that itch taken care of if I don’t get an hour or so at night for the side projects. Oh and none of this is monetized. I can and do drop projects as soon as they get boring. reply ukFxqnLa2sBSBf6 9 hours agoparentprevIs there actually any evidence that an “attention span” can be affected in any way? I’m not knowledgeable about any psychological studies on this topic. My instinct says that young people consume short form content because that’s their preference. I’m sure they’re also still capable of like watching a movie. reply indigo945 7 hours agorootparentYes, there is. (Mind you, I'm not a psychologist, this is just from a quick online search.) There's a study from 2012 linking social media usage to reduced academic performance and a lowered attention span [1]. A recent study has shown that excessive TikTok use in particular is linked to short-term memory deterioration [2]. I assume this is a somewhat trendy field of research in psychology, so you can likely find more studies, should you want to. [1]: Paul, Baker et al (2012): https://www.sciencedirect.com/science/article/abs/pii/S07475... [2]: Sha, Dong (2021): https://www.mdpi.com/1660-4601/18/16/8820 reply yungporko 6 hours agorootparentprevi've met countless people who aren't capable of watching a movie, they just sit there looking at their phone the whole time then claim the movie was stupid and/or boring and that they \"didn't get it\". if you ask them what a good movie is, they say something they liked from before they became phone addicts because they literally haven't watched a movie since. i also personally struggle a bit with older movies (80s/early 90s) because they tend to have a slower pace and i instinctively want to start multitasking every time there's a lull in the action. i actually watched die hard for the first time last night and found it kinda slow and boring despite it being one of the most iconic action movies of all time. there may or may not be a scientific study to suggest it can happen (although i assume there is) but based on my own experiences i am 100% certain that it is happening. reply KptMarchewa 5 hours agorootparentI've personally found newer slightly slower movies like Villeneuve ones are really good. reply NalNezumi 5 hours agorootparentprevPosted a while ago on HN: https://slimemoldtimemold.com/2023/07/24/your-mystery-have-a... A good litterature survey, easy to follow. Better than reading one or two isolated studies, since it does ask the crucial question of \"well what is (and how do you measure) attention span anyway?\" Less rigorous but similar conclusion in video if you prefer https://youtu.be/aDfeOvUZ7Kk?feature=shared reply smolder 6 hours agorootparentprevI'm not personally aware of academic study of this (and haven't looked, admittedly,) but my experience and anecdata from others I've known says that yes it can be very affected --our attention spans are malleable and affected by our environment. When I've been going hard on (overstimulated by) low-attention span content like social media, video games, certain video content, it has a pronounced impact on my ability to focus on things longer term. The best example is reading a book. I end up having to reread sentences and paragraphs frequently as my mind wanders from what the words are conveying. It takes a while to get into the groove of following the thread of the writing. Spending some time in a less stimulating environment like going camping, practicing meditation, or just 'unplugging' seems to be restorative to that ability to focus. Other people have shared similar experiences with me many times as this subject comes up. Again, my experience, but this capacity for focus applies to many things beyond just reading a book, like tackling programming problems or having a deep and nuanced conversation, effectively communicating complex ideas and being able to receive the same. reply fxwin 9 hours agorootparentprevAll i've really got is personal anecdotes, but i've noticed that excessive scrolling (~consuming short form/attention grabbing content) in the evening/morning correlates negatively with my ability to focus on longer, less dopamine-rewarding tasks. This usually goes away after a few days of reducing said behaviors, so it seems at least mostly temporary (the same might not be true in teenagers or younger) reply nunodonato 9 hours agorootparentprevthey watch a movie AND their instagram/tiktok feed at the same time reply Temporary_31337 6 hours agorootparentNo such thing as multitasking. You just context switching frequently so any movie with any depth to the story will be hard to follow this way reply thaumasiotes 8 hours agorootparentprevYes, I'm not so sure that they are capable of watching a movie. I tried to watch TV with my sister and failed because she spent the time looking at her phone. reply NiloCK 6 hours agorootparentprevBeing in the same room that a movie is displayed movie is not attending to it. See, eg, https://www.cbc.ca/news/entertainment/cellphones-in-theatres... In any case, if watching a movie is the counterexample it is probably in the category of \"damning with faint evidence\". This is still passive media which carries on whether or not the watcher is attending. Reading a book for an hour or two is different, because the pages don't turn themselves. People in a position to notice changes in these capabilities have noticed them. https://www.theatlantic.com/magazine/archive/2024/11/the-eli... reply triceratops 6 hours agorootparentprevBe right back, doing some research on this. reply blueflow 7 hours agoparentprevIt does not matter what the actual effect of this type of content is - it makes people feel in a specific way, so they started using \"brainrot\" to describe what they are feeling. reply downWidOutaFite 10 hours agoparentprevJust do what we've done since the dawn of time: hang out with friends and family, prepare and eat food together, maybe play card or board games. It feels good and it's good for you. reply theclansman 10 hours agorootparentThat's actually not that easy to do when people are expected to be living alone at a certain age. Finding a partner could be good for you but that's also not so simple. I think most people would prefer hanging out with others rather than scrolling tik tok (you might enjoy alone activities but tik tok scrolling comes out of boredom), but sometimes they have no choice. It's like the way we live is pointing us specifically in that direction, at least by default. reply InsideOutSanta 10 hours agorootparentYou don't need to be in a relationship to hang out with people. I think the issue most people encounter is that hanging out with friends suddenly gets a lot more difficult after education ends. The people you meet at work are less likely to become close friends than the people you meet at school, and as people get older, they have more responsibilities that take up their spare time, and fewer options to just spontaneously hang out. In addition, traditionally, men tend to rely on their partner for non-professional social networking. The percentage of people who live with a spouse or partner seems to be decreasing, so men are becoming more responsible for maintaining their own circle of friends. This requires effort. It's not like when you're 18 and you just naturally hang out, you have to put work into maintaining friendships and finding ways to meet. You have to plan things and invite people. You also have to make new friends as old ones disappear from your life over time. This is difficult, it requires conscious effort. I didn't realize that for a long time, and the longer I didn't do it, the harder it became to reestablish old friendships and make new ones. But it's absolutely possible to do it, if you put in the effort. And it's very worthwhile. It's much more enjoyable and fulfilling and relaxing to cook with somebody, go for a walk, or play Mario Kart in person, than to scroll Instagram or play an online shooter. reply HappMacDonald 9 hours agorootparentprev> I think most people would prefer hanging out with others rather than scrolling tik tok However both introversion and the hedgehog's dilemma can complicate this presumption. I think I'd rather say that most people would prefer hanging out with others in some fashion where they are safe from constant drama .. but the reality is that getting along with other people — including close friends and perhaps especially family members — takes a lot of effort and emotional labor. With our current ubercapitalist society (definitely in the US but it affects the rest of the globe as well) and poor social understanding of mental health, many of us simply lack the resources to constantly expend said emotional labor and connections often fall away as a result. Thus the mass of terminally online people. reply thaumasiotes 8 hours agorootparent> I think I'd rather say that most people would prefer hanging out with others in some fashion where they are safe from constant drama .. but the reality is that getting along with other people — including close friends and perhaps especially family members — takes a lot of effort and emotional labor. Counterpoint: https://www.youtube.com/watch?v=AA0PwmQMVG8 reply libertine 8 hours agoparentprevI think you're referring to another phenomenon that leads mostly to apathy. \"Brain rot\" as it's described leads to a change in perception, and I wonder if it also changes beliefs. \"Brain rot\" seems to be linked with the repeated consumption of, for example, unchallenged propaganda. It encourages people to think they made some huge insightful breakthroughs, and that they have some new undisputed understanding of reality, when in reality they just have a superficial knowledge of someone else's idea. Which sometimes is amplified by someone repeating someone else's ideas. Yet they refuse to either acknowledge or question it. For example, unlike general perception, conspiracy theories aren't that complex, they're much simpler than reality and more entertaining - as an anecdote it's much simpler to think the moon landing was done in a Hollywood studio than to understand the endeavor taken to achieve this milestone for Humanity. This leads to people dismissing reality in favor of fantasy, leading to algorithmic thinking, which is just a lazy cognitive process. If you challenge this, you'll be ignored or dismissed. All for the aesthetic of being an intellectual. reply DonsDiscountGas 16 hours agoprevAm I crazy or should it be \"brainrot\" (no space)? I'm actually not bothered by the idea of word of the year having spaces, I just thought this particular term had no space. reply casey2 15 hours agoparentIt's definitely \"brainrot\" scrolling through the thumbnails on [1] I see \"brain rot\" exactly 0 times. [1] https://www.tiktok.com/tag/brainrot reply someothherguyy 13 hours agoparentprevBoth are used according to wiktionary: https://en.wiktionary.org/wiki/brain_rot Also, OUP and others have picked a number of words of the year having spaces: https://en.wikipedia.org/wiki/Word_of_the_year For example, \"goblin mode\" (2022). There are hundreds of words in the English language that contain spaces. reply panarchy 14 hours agoparentprevspelling brainrot as \"brain rot\" is brainrot reply InsideOutSanta 10 hours agoparentprevGoogle Trends seems to indicate that it's overwhelmingly written as \"brainrot\", no space. reply xvector 15 hours agoparentprevIt should absolutely be \"brainrot\"! reply chii 14 hours agorootparentexactly! \"Brain rot\" is when you have gangrene in the brain. Brainrot is when you have tiktok in the brain. reply xnx 16 hours agoprevI'm glad to see this word getting wider exposure. I hope people will expand it's use to describe all kinds of content beyond the narrow/extreme aesthetic it refers to in short form video. Once you start looking, you recognize brainrot content everywhere: daytime TV, talk radio, reality TV, cable news, Facebook AI slop, etc. reply xyst 15 hours agoparentOn the topic of brain rot, content creators with longer form videos often include a video playing in the background which has no relevancy to the narrated content. The video itself isn’t necessarily brain rot content but rather a side effect of a majority of people’s short attention spans. In some cases, the content can include relevant video content but split viewed with something irrelevant (ie, random subway surfers gameplay). It’s quite annoying and distracting for me. reply cwillu 15 hours agorootparentA lot of those are actually spam accounts that repost popular content with an extra video in order to still have enough apparent originality to get paid. reply xnx 15 hours agorootparentprevYes. I think the extra/unrelated content may also have roots in trying to defeat copyright content detection. reply Uehreka 14 hours agorootparentprevThere was an AI news/analysis channel that I had to unfollow because of this. The guy did a good job distilling down the news, but half of the length of each video would be random AI-adjacent videos (like footage of someone testing a drone or a self driving car) that had nothing to do with what he was talking about, and I found it difficult to focus when I couldn’t be sure if the stuff on screen was supposed to be relevant or not (he’d often cut to charts or samples of a video output). Lately in slop-land I’ve also seen shorts where the creator will take an old boomer joke (one of those “and then the student stumped the teacher” kind of stories) and have an AI voice read it while the video is cuts from “satisfying animations”-genre videos. reply kiba 15 hours agorootparentprevI don't see this in my youtube feed. reply Gigachad 15 hours agorootparentIt’s done on YouTube shorts a lot. Usually the audio is someone talking, and the video is Minecraft parkour or similar. reply cedws 15 hours agoparentprevPolitical brainrot is arguably the most sinister because it trains people to abandon logic and base all of their beliefs on feelings. And there is a huge incentive for companies to inject this into discourse to polarise and swing people one way or another. reply carimura 15 hours agoparentprevit ends up being a zero-sum game for depleting attention spans.... reply tourmalinetaco 12 hours agoparentprevI‘ve been using brainrot to describe content since about 2016, but apparently its been used as early as 2003 online to describe shows like The Bachelor. Brainrot is everywhere, even among what one may consider “high culture”. The play Hamilton is a prime example of this. Theater is regularly considered high culture, however due to a combination of poor historical accuracy, oversimplification of the events involved, and, in my opinion, very commercialized sounding music, leads to what is very effectively “high culture brainrot”. reply t-writescode 11 hours agorootparentDo you have a similar opinion of Jesus Christ Superstar? Or the Technicolor Dreamcast? And if not, what makes them different? reply tourmalinetaco 1 hour agorootparentI am not familiar with either, so this was hardly the “gotcha” you may have expected. If you have disagreements with what I posted I urge you to actually focus on said post rather than whataboutism. reply Spivak 15 hours agoparentprevCan't forget video games, video game streamers, variety podcasters, pornography, sports tv and sports commentary, edutainment, documentaries, cartoons, comics, fanfic, romance novels, action movies, Vince Flynn books, political masterbation such as Last Week Tonight and Sean Hannity, news farms like NYT and WSJ, or smoking weed. You chose a cross-section of content that that is universally derided by the average HN user; if you're not willing to use the front facing camera you're just taking potshots at your outgroup. reply anon-3988 15 hours agorootparentYou lump a lot of things into this bucket. I am curious what you consider as not a brainrot? reply Spivak 13 hours agorootparentI mean I came up with the list by going through the kinds of mindless passive entertainment that has little to no to negative intellectual or artistic value that either I or my coworkers (who are mostly Dads hence Vince Flynn) enjoy that I think intersects with HN types. It's easy to call out the kinds of low-value activities that you don't personally enjoy or partake in. It's a lot harder and more uncomfortable to see them in yourself and have some empathy and not judge others too much for their brain-off vices. Only when I die and see the big stats board do I want to know how many hours I've wasted on Slay the Spire. And while I love it dearly I can't really delude myself that it's anything other than Candy Crush for nerds. reply Kelteseth 12 hours agorootparentYou do know that there exist more games than just candy crush? There are many games that I personally would order in the same magnitude of culture importance than musical symphonies. Portal 2, World of Warcraft, The last of us, Breath of the wild, just to name a few. reply Spivak 4 hours agorootparentIt's always video games that strikes a nerve with people on the internet. Yes I've played and beaten all the games you've mentioned. Sure artistic video genres exist just like artistic porn exists but we're not meaningfully engaging with the activity as it really exists by pretending that's what I'll find if I go to xvideos. I've sunk 16k+ hours into WoW in my lifetime, you named perhaps the most brainrot game that has ever existed in the history of video games. It's makes Tiktok look like long form essays, doing meth instead would be considered harm reduction. It's easy to identify when the activity is low status to your in-group. The Big Bang Theory has significant cultural importance but I bet you'd agree it's still brainrot. reply gambiting 9 hours agorootparentprev>> by going through the kinds of mindless passive entertainment that has little to no to negative intellectual or artistic value Why are video games on that list then? reply Gigachad 15 hours agorootparentprevVideo games are probably the least bad here considering most of the popular ones are like 100 hour stories and are usually played in 1 hour chunks. Not the typical short form brain rot. reply smolder 5 hours agorootparentBeing long doesn't necessarily mean they aren't bad for attention spans. I recall seeing a reviewer complain about a story driven game recently where the characters were constantly restating plot points to make sure you didn't forget. And the more important thing in games than narrative, I think, is how much the gameplay itself makes you think long term (or not), and the pace of things. Do you have to keep a goal/strategy in mind or just constantly react to what's immediately in front of you? reply xnx 15 hours agorootparentprevI meant to include color matching and gatcha games in my list. reply teractiveodular 15 hours agorootparentprevDon't forget link aggregator sites run by venture capitalists to advertise their investments. reply Spivak 14 hours agorootparentOf course, how could I be so forgetful?! Probably the brainrot of being on here so long. reply lazycouchpotato 14 hours agoprevI wouldn't stress too much about 'brain rot' being the \"word\" of the year. The Oxford Word of the Year 2015 was the 'Face with Tears of Joy' emoji https://languages.oup.com/word-of-the-year/2015/ reply gniv 10 hours agoparentThat emoji is everywhere now. It didn't go away, so they were right to highlight it. reply theclansman 10 hours agoparentprevI've noticed that younger fellows like to correct people while using the crying emoji, and point disturbing stuff with the skull one. Tears of joy is probably not ironic enough for them. reply someothherguyy 13 hours agoparentprevWikipedia article with many previous words of the year from various publishers: https://en.wikipedia.org/wiki/Word_of_the_year reply airstrike 15 hours agoprevIf you're unfamiliar with the term, I recommend this hilarious intro: https://www.youtube.com/watch?v=49l39sSrGqk Don't worry, once you catch your breath there's also a version for Spider-Man 2 which is somehow even funnier reply lmpdev 8 hours agoprevSometimes I feel like going the way of David Foster Wallace No TV (or phone/computer) in the house, to safeguard my monkey brain from hijacking my time But then I recall his suicide (albeit unrelated to stimulation) and think, maybe there’s a happy middle reply joshdavham 15 hours agoprevI'm seeing some confusion in the comments as to whether 'Brain rot' is one word or in fact two words. 'Brain rot', in the context of this slang term, can be considered one word. Despite that you can technically split 'Brain rot' into 'brain' and 'rot' (which are both words), 'brain rot' isn't actually the sum of its parts and has a more idiomatic meaning than just 'brain' + 'rot'. reply fenomas 14 hours agoparentThat's not what the comments are about - they're noting that the cohort popularly using this term spells it \"brainrot\", so it's odd that Oxford picked a different spelling. reply joshdavham 11 hours agorootparentThere were other comments. I know which one you’re referring to though. reply airstrike 2 hours agoparentprevIt's still one two-word expression. reply MathMonkeyMan 15 hours agoparentprevlike \"common law\" reply Rebelgecko 16 hours agoprevAn interesting generational battle- I'm surprised to see slop made the shortlist, I believe it came from a term popularized on 4chan reply itishappy 15 hours agoparentYou sniped me. The internet seems to agree with you that the current usage of \"slop\" was popularized on 4chan in 2022, but it's been around way longer. I've always known slop as the food scraps served to pigs. reply Gigachad 15 hours agorootparentIt’s kind of the same usage. Calling something “AI slop” is describing it like food scraps for pigs. Cheap, distasteful, mass produced. reply itishappy 14 hours agorootparentAlso backlash in motors and gears. Imprecise engineering. I'm liking the word \"slop\" more and more! reply duskwuff 12 hours agorootparentAs someone on Mastodon observed [1], it's also got all sorts of great resonances with words like \"slime\", \"sleaze\", \"sludge\", \"slug\", \"slither\", \"slouch\", etc. The word itself implies a distasteful, slippery (another sl- word!) nature. [1]: https://transmom.love/@elilla/113578996568287769 reply blueflow 8 hours agoparentprevCurrent youth culture is massively influenced by imageboard culture. reply cam_l 5 hours agoprevThere you go, I just learned a new term.. autological. https://en.m.wiktionary.org/wiki/autological reply metalman 8 hours agoprevI would bet that at Oxford the word that is most important, but never spoken out loud is \"relevance\" which they are grasping at A good part of this thread just ignores there \"naming\" and gets on with having fun with language, without them. reply kmarc 12 hours agoprevFunnily, I started using this word when I'm referring to social media, the addicts of daily TikTok and Instagram scrolling users. Maybe I'm exposed to this brain rot, since I am using this word lately a lot, and possibly, I picked up unconsciously from other sources. Maybe. Possibly. Too much internet for me, too. reply sirsinsalot 8 hours agoprevI think it is excellent that vocabulary describing the negative impact of mental junk food is in common use. It shows that as a society we are quickly recognising the damage it can do and are creating cultural pressures against it. reply openrisk 9 hours agoprevEnshittification enabled by kakistocracy induces brain rot. There you have it, all \"words of the year 2024\" in one sentence [1],[2]. Enjoy. [1] https://www.economist.com/culture/2024/11/29/the-economists-... [2] https://www.macquariedictionary.com.au/woty-2024/ reply ilrwbwrkhv 17 hours agoprevHaha. It's a lovely word. I was also hoping for skibidi would make the cut. reply hamburga 1 hour agoparentTo be fair, “skibidi” is considered part of the brain rot vocabulary, according my tween kids. reply pixelatedindex 15 hours agoparentprevTo this day I fail to understand the word “skibidi” reply hhhAndrew 15 hours agorootparentI got an LLM to explain it to me. That sort of thing is definitely in their wheelhouse. reply atemerev 9 hours agorootparentprevThis is just a greeting and an invitation. “I speak brainrot, and if you do too, let’s do it!”. TCP SYN. reply AlienRobot 7 hours agorootparentprevBy the time I understood what yeet meant nobody was using it anymore. reply ilrwbwrkhv 13 hours agorootparentprevIt's basically like chama. reply deadbabe 16 hours agoprevWill brainrot morph into a politically correct term over time similar to how shellshocked became post traumatic stress disorder? reply teractiveodular 15 hours agoparent\"Cerebral decomposition disorder\" does have a certain ring to it. reply deadbabe 4 hours agorootparentAh CDD, more misdiagnosed BS. reply add-sub-mul-div 16 hours agoparentprevWho can tell? Perhaps language will stop evolving for the first time in history. reply AlienRobot 7 hours agoparentprevAttention deficit disorder? No, that's already taken... reply siva7 6 hours agorootparentTough times for ADD people. It's like we're doing all we can to make their lives more miserable in this modern world, alongside the healthy. reply morkalork 16 hours agoparentprevBrain rot isn't pc already? I may be in some trouble.. reply amelius 8 hours agoprevEnshittification should have been the word of the year. reply Abekkus 5 hours agoparentEnshittification was an indictment of for-profit social media, but they're already irrelevant as they've been out-competed by state-sponsored social media like tiktok and twitter. reply Waterluvian 17 hours agoprevI know that they don’t literally mean a single word, but it got me thinking: is a space ever a valid character in a word? I’m guessing no, because that’s fundamentally how languages separate tokens However, while I’m no language person, I have wallowed in unicode enough times to recognize that the “rules” of language are so wild and varying and riddled with exceptions that basically anything is possible. reply teractiveodular 17 hours agoparentYes: an open compound has two words separated by a space, but still has a unique meaning. Ice cream, common sense, never mind, etc. FWIW, there are plenty of languages that don't use any spaces at all in their writing (Japanese, Chinese, Thai), and the use of spacing is quite fluid even in English. Is it a cellphone or a cell phone? reply joshdavham 15 hours agoparentprevSo technically in linguistics, the spoken language is considered the ground truth while the way the language is written is just a crude representation of the spoken language. What this implies is that a space being used between two tokens doesn't actually make it two words. You can still consider that to be one word. (Also, if you wanna go even more meta, there isn't even a widely agreed upon definition for what a word even is in linguistics!) reply qiqitori 16 hours agoparentprevAre you allowed to use \"se\" in Scrabble because it exists as a single word in \"per se\"? reply vitus 14 hours agorootparentNo, \"se\" here is Latin in the same way that \"exempli\" and \"gratia\" are. You similarly can't use \"kung\" or \"fu\". That said, you may not like this answer, but ultimately whatever's in the official dictionary (now known as Collins Scrabble Words / CSW) is what goes. \"mein\" as in \"chow mein\" is not valid (nor is \"chowmein\"), but \"lomein\" and \"wonton\" are. reply llm_trw 16 hours agoparentprevLike always scheme has a solution to the problem. Symbols that contain space separated word are enclosed in ||, |this is a symbol|, that while the naked space is still the symbol separator token. Now if we can just remove all other punctuation for s-expressions we'd finally have the start of a language fit for the digital age. reply proc0 15 hours agoparentprevSpace is arguably the absence of a character. It is only a character for creating encoding standards in computing systems (*or actually since the typewriters). reply gniv 10 hours agoparentprev> is a space ever a valid character in a word? No, because then how do you define a word? I suspect grammar has another term(!?) for the concept you're looking for. reply garciansmith 16 hours agoparentprevThe practice of placing spaces between words is connected to writing, so not some fundamental part of language. There are early Latin texts that don't have word dividers at all, for instance, and some languages used different symbols like dots. reply gpvos 15 hours agoparentprevIn English, yes. In many other languages, no. reply TheCraiggers 17 hours agoparentprevAt least in English (I don't know other languages well enough to speak with any any kind of knowledge) whenever you want a term containing 2+ words meant to be taken together, you use a hyphen. Compound words, I believe they are called. From what I can tell, they should have spelled it brain-rot. reply FabHK 14 hours agorootparentOne minute on Wikipedia would have disabused you of that wrong notion: > If the joining of the words or signs is orthographically represented with a hyphen, the result is a hyphenated compound (e.g., must-have, hunter-gatherer). If they are joined without an intervening space, it is a closed compound (e.g., footpath, blackbird). If they are joined with a space (e.g. school bus, high school, lowest common denominator), then the result – at least in English – may be an open compound. https://en.wikipedia.org/wiki/Compound_(linguistics) reply karaokeyoga 16 hours agorootparentprevA general heuristic is: single word is the noun form (e.g. \"I made a backup\"), two words is the verb form (e.g. \"back up your data\"), and hyphenated words is the adjectival form (e.g. \"the back-up copy is over there\"). reply gruez 17 hours agorootparentprev>whenever you want a term containing 2+ words meant to be taken together, you use a hyphen. Compound words, I believe they are called. So the shopping festival after thanksgiving should be called \"black-friday\"? reply TheCraiggers 16 hours agorootparent> So the shopping festival after thanksgiving should be called \"black-friday\"? I would say this is a prime example, yes. reply gre 16 hours agorootparentPrime-day? reply snvzz 17 hours agorootparentprevOr just brainrot. It'll win at the end. Hitting that spacebar is effort. reply TheCraiggers 16 hours agorootparentYes, many words that started out as two words separated by a hyphen merge to become one as language evolves. Notebook is a good example of this; a more recent example would be email. reply xanderlewis 16 hours agorootparentIn German it happens even more quickly. reply teractiveodular 15 hours agorootparentI thought you only used hyphens in German in exceptional circumstances like proper names in compounds? Karl-Marx-Allee etc. reply xanderlewis 7 hours agorootparentWell, what I meant was that concepts made up of several words almost immediately become one word in German (without hyphens). reply teractiveodular 17 hours agoprevReally? Macquarie got it right by choosing \"enshittification\" instead. https://www.theguardian.com/science/2024/nov/26/enshittifica... reply bee_rider 16 hours agoparentEnahittificaton is the word of the last decade or so. But didn’t it become a named thing a couple years ago? reply johnzim 15 hours agorootparentI remember reading it in 2014ish. The Cory Doctorow use is the newer one. reply BLKNSLVR 16 hours agoparentprevDidn't get much attention: https://news.ycombinator.com/item?id=42244076 reply anon-3988 15 hours agoparentprevDifferent concept. Enshittification implies an original state that is better. Meanwhile, most of the stuff that we consider brainrot are actually pretty well made. reply th0ma5 13 hours agoprevIt is funny that certain niche circles complain about the hive mind virus or whatever and that is by this definition a kind of brainrot to be complaining about that! Too funny. reply snvzz 17 hours agoprevI favor \"bed rot\". These days, why even get out of bed. reply bdangubic 16 hours agoprevthats two words reply itishappy 15 hours agoparentWell, it should be, but you've made it one. \"Thats\" actually stands for \"that\" and \"is\" so \"that's\" would be the correct form. You're missing an apostrophe. reply morkalork 16 hours agoparentprevA portemanteau? reply JasserInicide 17 hours agoprevamidst societal concerns about the negative impact of overconsuming online content. Yeah sure there's concern but it's only about what the other side is consuming. We're not applying the concern holistically and probably won't be able to because that would mean threatening the companies that host said brain rot and we just can't have the shareholders mad. reply Retr0id 16 hours agoparentI've never personally seen it used in reference to \"sides\". reply Rebelgecko 16 hours agorootparentAgreed, it's not really a political term (if that's what GP is implying). Translating into millennial/Gen x speak it's the sort of meme that you'd see in YouTube Poops a decade ago (or today, I guess) reply joshdavham 15 hours agoparentprevThis comment is actually the first time I've heard this term used politically. reply bongodongobob 15 hours agoparentprevI don't think you understand the term. Actually, I'm sure you don't. reply TrackerFF 17 hours agoprevCan I coin some new relevant word(s) for 2025: Billionaire's disease / billionaireitis / ventureitis Basically same as Nobel disease / nobelitis, but applied to wealthy (often tech) billionaires. Where after a certain amount of success in a quite narrow field, they start to push broad and wide-reaching ideas, which go far, far beyond their expertise. I'm not going to name names, but there are plenty of such billionaires out there, some getting far too much media time. Some even showing signs of advanced brain rot due to the media they consume. reply prawn 16 hours agoparentI assume the principle operates at lower levels too, and is just more noticeable with extremes of riches and fame? The person that first made me wonder about all this was Clive Palmer. https://en.wikipedia.org/wiki/Clive_Palmer reply BLKNSLVR 16 hours agoparentprevThe Prince Effect \"Prince has been living in Prince-world for a number of years now\" (possibly misphrasing of a sentence from Kevin Smith's recollection of his experience working for Prince). Sycophantitis. For which the treatment is a Messianectomy. reply alganet 12 hours agoparentprevMoney-infused pseudopolymathitis. reply triyambakam 16 hours agoparentprevGates is definitely one reply threeseed 16 hours agorootparentGates comes from a highly technical, engineering background and has spent over a decade largely full time on his philanthropic work. So I would definitely not put him in the same category as most other billionaires. reply mmmBacon 16 hours agorootparentnext [3 more] [flagged] threeseed 16 hours agorootparentNot that it has any relevance to his expertise. He wasn’t on the island at all: https://www.reuters.com/article/world/false-claim-bill-gates... reply Der_Einzige 15 hours agorootparentprevSo was Trump, and trump clearly knew about Epstein's \"preference for girls, especially of the young kind\" circa the 2001 vanity fair article where he was quoted saying that. reply m463 15 hours agoparentprevI've thought of something similar, maybe call it \"techbro poverty\" It's sort of like chesterton's fence + branding + fashion + stupid optimization giving us dystopian product design that any normal engineer understands is stupid. There are lots of examples of this: - remote controls with terrible play pause skip buttons, but big oversized dedicated buttons for some product they want you to use. - cars with no dashboard, no dedicated buttons, no turn signal stalks and everything on the touchscreen - laptops with no ports and terrible flat, unsculpted keyboards. - phones with no dedicated buttons, or buttons you can only dedicate to payment systems I'm sure there are tons more of these. reply vundercind 16 hours agoparentprevI've heard \"engineer brain\" for a similar phenomenon, and since the main examples now are tech billionaires... reply blitzar 6 hours agorootparentI have to push back against the misuse and abuse of the word \"engineer\". reply xeonmc 14 hours agoparentprevMusknitis reply ekianjo 15 hours agoparentprev> far beyond their expertise if COVID19 taught us anything is that 'experts' have no clue what they are talking about reply threeseed 14 hours agorootparentAnd the best thing about comments like this is that because ‘experts’ in this situation includes thousands of people it only takes a mistake from a handful for this statement to be seen as accurate by the lay person. It’s like arguing that ‘experts’ in IT have no clue what they are doing because of a mistake made by IBM failing to deliver a government contract. reply ekianjo 2 hours agorootparentNo, it's not like one expert was wrong, they were just literally wrong over and over about everything - it was pseudo science galore all the way. But people have short memories. reply Animats 14 hours agoprev [–] Oh, not the brain damage from COVID.[1] [1] https://nap.nationalacademies.org/read/27756/chapter/1 reply musha68k 8 hours agoparent [–] Why are you bringing up facts? We collectively decided not to speak about these issues again. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Oxford Word of the Year for 2024 is \"brain rot,\" selected through a public vote with over 37,000 participants, highlighting its growing relevance.- \"Brain rot\" describes the mental decline from consuming trivial online content, with a 230% increase in usage from 2023 to 2024, reflecting concerns about digital consumption's impact on mental health.- Originally coined by Henry David Thoreau in 1854, the term has gained popularity on platforms like TikTok, sparking discussions about the effects of excessive online content, particularly among Gen Z and Gen Alpha."
    ],
    "commentSummary": [
      "\"Brain rot\" has been selected as Oxford Word of the Year 2024, referring to the decline in mental state from excessive consumption of trivial online content.- The term underscores worries about shrinking attention spans and the addictive qualities of digital media, such as social media, TV, and video games.- Conversations around \"brain rot\" emphasize the need for moderation to balance relaxation and avoid overstimulation."
    ],
    "points": 192,
    "commentCount": 148,
    "retryCount": 0,
    "time": 1733103573
  },
  {
    "id": 42290861,
    "title": "Programming the C64 with Visual Studio Code",
    "originLink": "https://retrogamecoders.com/c64-visual-studio-code/",
    "originBody": "Programming the C64 with Visual Studio Code Program the Commodore 64 in BASIC, Assembly or C using Microsoft Visual Studio Code on Mac, Windows and Linux! C64 programmers usually turn to CBM PRG Studio, and I even suggest it as a go-to (heh) in my C64 BASIC Programming course series, but it is only natively compiled for Windows. Commodore 64 BASIC Programming Series Part 1: Introduction to Commodore BASIC Part 1.5: Installing CBM Prg Studio on Mac/Linux Part 2: Commodore BASIC Commands GOSUB and FOR Loops Part 3: If/Then, Game Logic and Cursor Movement Part 4: The Magic of POKE Part 5: C64 DOS Commands Part 6: Working with Data Files Part 7: Programming C64 with Visual Studio Code Part 8: C64 BASIC String Manipulation Part 9: C64 Text Adventure Game You can, with some effort, get CBM PRG Studio for Mac and Linux using Wine, but that is a lot of time and disk space to expend to get an IDE for a vintage computer working. Ideally we would be able to use a familiar, modern, cross-platform IDE. Surprise! You can code for the C64, for free, using Microsoft Visual Studio Code! The Commodore 64 Programming Extension – VS64 VS64 is a Visual Studio Code extension for C64 programmers by Rolandshacks on Github. It has syntax highlighting, tools and integrations for BASIC, Assembly and C programming. At the time of writing it is on version 2.5.3 but the best way to install it is through Visual Studio Code itself. Simply open VS Code and open the Extensions Marketplace to search for “vs64” and you should fine it right away. vs64 extension in visual studio code extensions marketplace Once you have found the extension you just need to click the blue Install button. Click the blue install button If you want to know more about the extension you can browse the page that is available on clicking the extension card. This is all the same information available on the Github page linked earlier. vs64 extension welcome page in visual studio code Setting up VS64 The vs64 setup wizard There are three parts to using VS64: Installing the required tools (if you are going to be compiling and debugging). Configuring the settings to point to your compilers and assemblers. Project settings, to tell VSCode how to understand your new project. After installing the extension you have an opportunity to go through a setup wizard, but you can always change your settings using the little cog icon on the extensions page within Visual Studio. Once the wizard is done you can create a project, and later vs64 extension project creation options 6502 Assembler VS64 supports both ACME and Kick assemblers. ACME can be installed on Linux using sudo apt install acme and binaries are available on Sourceforge. Download KickAssembler from theweb.dk/KickAssembler. KickAssembler requires Java. The recommended file extension is .asm C/C++ Compilers for the C64 VS64 supports both the CC65 6502 C Compiler and LLVM-MOS C/C++ Compiler Download CC65 from cc65.github.io Install CC65 on Ubuntu/Debian/RaspberryPi with sudo apt install cc65 Download and install LLVM from github.com/llvm-mos/llvm-mos-sdk VICE C64 Emulator and 6502 Debugger VS64 supports debugging using the VICE Commodore 64 emulator. Install from Sourceforge. or on Ubuntu/Debian/Raspberry Pi: sudo apt install vice Commodore BASIC Integration VS64 supports BASIC natively without anything required to be installed but it has some extra tricks and tools that make the experience better than coding on the C64 itself, outside of the obvious benefit of 80+ column text and syntax highlighting! Supports Commodore BASIC V2 and Simon’s BASIC (TSBneo) Upper/Lower Case Character Sets Auto-Numbering Code crunching Removal of spaces Line number re-ordering Removal of REM statements My favourite feature, however is being able to use labels instead of line numbers. For example: label1: A$=\"HELLO\" PRINT A$ GOTO label1 Will be converted on the Commodore 64 to: 1 A$=\"HELLO\" 2 PRINT A$ 3 GOTO 1 VS64 can bundle binary resources to be referenced by your code (requires Python 3): SpritePadPro and SpritePad 1.8.1 file format (.spd) CharPad64Pro file format (.ctm) SpriteMate file format (.spm) SID file format (.sid) Raw binary data (.raw) PETSCII Control Characters Like in CBM PRG Studio, PETSCII control characters are supported in BASIC via special strings: PRINT \"{clr}HELLO, {green}WORLD{$21}{lightblue}\" The following mnemonics are available: {space}, {return}, {shift-return}, {clr}, {clear}, {home}, {del}, {inst}, {run/stop}, {cursor right}, {crsr right}, {cursor left}, {crsr left}, {cursor down}, {crsr down}, {cursor up}, {crsr dup}, {uppercase}, {upper}, {cset1}, {lowercase}, {lower}, {cset0}, {black}, {blk}, {white}, {wht}, {red}, {cyan}, {cyn}, {purple}, {pur}, {green}, {grn}, {blue}, {blu}, {yellow}, {yel}, {orange}, {brown}, {pink}, {light-red}, {gray1}, {darkgrey}, {grey}, {lightgreen}, {lightblue}, {grey3}, {lightgrey}, {rvs on}, {rvs off}, {f1}, {f3}, {f5}, {f7}, {f2}, {f4}, {f6}, {f8}, {ctrl-c}, {ctrl-e}, {ctrl-h}, {ctrl-i}, {ctrl-m}, {ctrl-n}, {ctrl-r}, {ctrl-s}, {ctrl-t}, {ctrl-q}, {ctrl-1}, {ctrl-2}, {ctrl-3}, {ctrl-4}, {ctrl-5}, {ctrl-6}, {ctrl-7}, {ctrl-8}, {ctrl-9}, {ctrl-0}, {ctrl-/}, {c=1}, {c=2}, {c=3}, {c=4}, {c=5}, {c=6}, {c=7}, {c=8} C64 Assembly Example with KickAssembler In a new VSCode window, open your folder where the project will live then select Show All Commands (Shift-CMD/Ctrl-P) and type vs64. A list of vs64 options will appear like so: Now select your chosen project. I will create a KickAssembler project. In the Src folder you will find an example main.asm which you can use or clear to start afresh. I will add the code found below … // // Hellorld // *=$0801 .byte $0c,$08,$b5,$07,$9e,$20,$32,$30,$36,$32,$00,$00,$00 jmp main hellotext: .text \"hello, commodore 64!!\" .byte $0 .const ofs = 10 main: ldy #0 lda #$00 sta $d020 sta $d021 tax lda #$20 loop: sta $0400,x sta $0500,x sta $0600,x sta $0700,x dex bne loop hello: lda hellotext,y beq !loop+ sta $400+ofs,y lda #1 sta $d800+ofs,y iny jmp hello !loop: rts Saving the file will automatically run KickAssembler, you simply need to select Run followed by Run and Debug, or click the debug button in the left toolbar, then click the green arrow at the top, to run your new program in Vice! Running a 6502 assembly project in vs64 and vice Category: ProgrammingTag: 6502, assembly, basic programming, Commodore 64 (C64), Retro C/C++ Programming Previous Post:How to Setup Your Raspberry Pi 5, 4 or Pi400 with PiMiga Amiga Emulator + Games Bundle Next Post:C64 BASIC Tutorial: Using String Manipulation to Write a Text Adventure",
    "commentLink": "https://news.ycombinator.com/item?id=42290861",
    "commentBody": "Programming the C64 with Visual Studio Code (retrogamecoders.com)180 points by rbanffy 21 hours agohidepastfavorite32 comments FLT8 20 hours agoI credit the C64 that I had as a kid and magazines like COMPUTE! / Compute's Gazette for my career in software. I taught myself 6510 assembler and started writing some simple demo-like things on that machine, and got hooked on the feeling of creativity that it unlocked. Funnily enough I'd been thinking that it's about time I tried (again, as an older person) to write a game or a demo for the old 64. It's absolutely amazing what people are able to get out of these 40+ year old machines now, and I love that there's still a vibrant scene. In addition to the tools specified in the article, I would also recommend \"retro debugger\", it's an amazing tool for single stepping through code and seeing what's going on, even letting you follow the raster down the screen to see what code is executing on given scaliness. Also, there are some really good youtubers out there helping to demystify how various games/demos work.. Martin Piper comes to mind as a good example. reply breput 20 hours agoparentI credit the BASIC and machine language byte code type-in programs for reinforcing my attention to detail and being able to track down software problems. Kids these days[0] will never know the \"pleasure\" of spending hours typing in some cheesy BASIC game only to have to track down any number of syntax errors! [0] Get off my lawn! reply lstodd 16 hours agorootparentA9 LDA# A0 LDY# your lawn may stay. reply selcuka 15 hours agorootparentIt's amazing that I still remember some opcodes like the ones you posted (and others, such as 0xAD for LDA$, 0x78 for SEI, 0x58 for CLI) after all these years. Brains are weird. reply giamma 9 hours agorootparentI don't remember the numeric codes unfortunately, but BEQ, BNE, JMP, JSR, ROL, ASL... reply Lance_ET_Compte 18 hours agoparentprevI had a C64 as well. My school had a programming class and we all shared a TRS80 (I think). I remember writing a program to find prime numbers and thinking about various optimizations. Mine was fastest, and I was proud. Then the boy that wrote directly in assembly ran his... That was the moment I decided to get good. :-) reply agentdrek 6 hours agoparentprevI still keep the Reference Guide on my desk to remind me of my roots and how far things have come and yet how much they remain the same. reply ddingus 17 hours agoparentprevDefinitely try as an older person! I have similar experiences and sentiments myself. One difference is I was into Apple and Atari computers, but that does not seem to matter all that much. As a younger person, I did demos and explored the tech plenty without actually building finished applications and or games. Learned a ton! And had major league fun. Great times filled with bits of understanding I draw on all the time. And YES! Good grief, the pixels are dancing in ways nobody would have predicted back then. When I hop on the machines today I find them simpler than I remember and fun to program. reply nurettin 14 hours agoparentprevPOKE 53280, 0 reply Manuel_D 21 hours agoprevRetro game coders achieve some pretty astounding results sometimes. One of my favorite examples is one who optimized super Mario 64 to such a degree that it runs with much better framerate on the original hardware. Also, multiplayer was added: https://www.youtube.com/watch?v=t_rzYnXEQlE reply bitwize 12 hours agoparenthttps://www.youtube.com/watch?v=QB3oHdSjfCE Dragon's Lair. DRAGON'S fucking LAIR. On TI-99/4A. The original laserdisc FMV version, albeit heavily bitcrushed, on a 16-bit home computer from 1981. reply pipes 9 hours agoparentprevKaze? Yeah he's being doing insane things, like getting mario sunshine levels to run on his modded engine. Sounds like he's accomplished much much more too: https://m.youtube.com/watch?v=Drame-4ufso&pp=ygUEa2F6ZQ%3D%3... reply tbensky 20 hours agoprevOne of my favorite retro projects in this real-time TRS-80 (Model I assembler and emulator that assembles and runs Z80, literally with each key press. Mind boggling how today's CPUs can emulate and entire 8-bit computer dev-process all between key presses in a browser. https://www.teamten.com/lawrence/projects/assembly-language-....\" The author even says \"How about: With every keystroke in the IDE’s code editor, we assemble the whole program, reset a virtual TRS-80 to a known state, install the program, and run it?? reply rob74 11 hours agoprev> My favourite feature, however is being able to use labels instead of line numbers. I guess that's less of a feature and more of a necessity? Actually, this was one of the questions on my mind when I started reading the article. If you remember how those old BASIC dialects worked, you basically decided the line numbers for each line by yourself. The common strategy was to increment by 10, then you had some space to insert additional lines if needed without having to renumber everything (including the destinations of GOTOs and GOSUBs). Subroutines got line numbers in the 1000+ range so they didn't interfere with your main program (example here: https://www.c64-wiki.com/wiki/GOSUB). Of course this system would be extremely confusing in an IDE that shows the actual line numbers of the source code by default. reply codeflo 10 hours agoparentFrom a nostalgia viewpoint, adding labels feels a bit \"impure\". You might as well want to introduce other syntactic sugar, but at some point, you're no longer writing C64 BASIC. But what if you had an IDE with convenient/native editing support for BASIC line numbers? It could auto-number on pressing enter, and it could have refactoring/renumbering support so that you can't miss a GOTO somewhere. But you'd always see and edit plain BASIC code. It feels like that might be fun. reply rob74 8 hours agorootparentThat would be possible actually, but it would probably require support from the IDE for letting language plugins filter which lines are shown in the editor (which I don't think VS Code supports currently). Then the line numbers shown in the editor's gutter would be the line numbers defined in your code, and if you typed a new line starting with a line number, it could insert it at the location specified by the line number. reply larodi 11 hours agoprevIs not worth learning 6502 much better and even relevant since 6502s are still printed, rather than God-forsaken BASIC? Besides as I tried as a child and all the magic is hidden behind POKE/PEEK commands, the basic itself is unlike Apple II basic or DOS basic. And with all the peek/pokes it implies going on the assembly level. I did go on the assembly level for x86 when I was 14 and never regretted it. One of the top coder ppl that I knew started x86 assembly when he was 10-12 and only got into the Java bandwagon when he got in university, meanwhile mixing C and Asm. I have many other reasons to believe such knowledge is not too hard for children to understand. reply nubinetwork 11 hours agoparentIf you know x86, then 6502 should be easy... on the other hand, you'd have to be interested in making homebrew for legacy systems because there's nothing really new that uses it. reply richrichardsson 9 hours agorootparentI know my way around m68k assembler, so in \"theory\" 6502 should be easy, but damn it's actually quite tricky. * way fewer registers to play with (understatement) * You want to mul/div? lol... * Logical Shift Left? Why do you want to do that? that's all I've come up against after a couple of hours last night, I'm sure there are a load more \"problems\" I'll face. reply nubinetwork 7 hours agorootparentYou know 68k, but 6502 is hard? I always figured it would be the other way around... doesn't the 68k have various MMU/\"protected mode\"-like stuff? Edit: i could also be confusing it with PPC. reply richrichardsson 6 hours agorootparentMy 68k knowledge is from Amiga 500 days, no MMU to worry about there. reply tom_ 8 hours agorootparentprevLogical shift left is covered by ASL. reply richrichardsson 6 hours agorootparentAh, thanks, had missed that somehow! reply chillingeffect 6 hours agorootparentprevTrue. Ppl nostalgize aboutbthe 6502 but it's a PITA. Still i believe the answer toyour question is use a Macro Assembler. To collect dozens of library routines like sort, copies, fills, yoyos, etc. And work at that slightly higher level. I wish did that at the beginning of my career :) but nowadays theyre easy to find. reply rbanffy 20 hours agoprevWhat I love about this is that it should be relatively trivial to extend it to, at least, all emulators supported by VICE. reply ryandrake 17 hours agoprevCool project, looks like they support a number of assemblers and compilers, too. It’s not clear if it also supports the ca65 macro assembler as a standalone assembler as part of their cc65 support, or if it only supports the c compiler. reply flohofwoe 8 hours agoprevShameless plug: I cobbled together a simple VSCode extension for (so far) KC85, C64 and CPC - only for assembly coding/debugging though. The 'special sauce' is that the assembler and emulators compiled to WASM and directly integrated into the extension (e.g. the emulator is running in a VSCode tab, everything is properly sandboxed): https://marketplace.visualstudio.com/items?itemName=floooh.v... ...and it even works in the VSCode browser version, e.g. you can go here and press 'dot' to start into VSCode: https://github.com/floooh/kcide-sample-kc854 It *is* really quite amazing how productive 8-bit assembly coding feels with modern tooling (e.g. when you have a tight edit-build-debug loop, and having a debugger that lets you inspect the state of the entire hardware, not just CPU registers). Here's the accompanying blog post: https://floooh.github.io/2023/12/31/vscode-wasm-wasi.html reply wiz21c 7 hours agoparenthey flooooh thanks for your 6502 cycle accurate emulation :-) signed:some emulator author :-) reply karmakaze 16 hours agoprevAmazing to see this alive and kicking in this day and age. I grew up programming the 8-bit Atari and it was such a great time to experience. Information was scarce without internet, so magazines and word-of-mouth was so important. Once you collected enough information, you could pretty much hold a mental model of the entire machine in your head, and the only limitations were the number of cycles you had per second or the effort you were willing to put in. I firmly believe this is how I developed the mindset of stepping outside of the problem and readily looking for out-of-the-box solutions that pays dividends to this day. Programming was a mix of BASIC and machine language routines (not even assembly as you hand-assembled to machine bytes) similar to how someone might use Python with C calls but at lower levels of both. Later on I tried compiled languages like Pascal but it ran like a dog, not tuned for the slow floppy or memory constraints of these machines. Getting a macro assembler was next-level, where you could write everything to be as fast or small as possible. Self-modifying code for wasn't only common practice, it was quite necessary for performance. Other great pastimes were reverse-engineering games and copy protection in an arms-race. Most were pretty simple and it was always exciting to see completely new techniques or super-complicated multi-pass self-modifying or otherwise obfuscated code. The filesystem on the floppy disks were also easily understood with sector chaining and a free sector bitmap, so you could write low-level routines like undelete or a defragger. Modifying the floppy drive with custom sector sequencing and RPM tuning could improve timings to get higher throughput and finding that you can add SRAM to buffer an entire track really sped things up. The simplest and cheapest thing was to punch a hole in floppy disks and write to the other side (of a single-sided floppy). Having this level of understanding made you believe that you could do pretty much anything and everything (just not always fast enough) and makes you program without boundaries except the hardware. Even then it was simple enough to modify parts of the hardware like ROMs or extend it with helpers plugged into the bus or PIO (parallel I/O joystick) ports much like the GPIO on a Raspberry Pi. The graphics system on the Atari was really something else, with 'display lists' of data that defined memory addresses and display modes (character or raster) that could be defined for each (character or raster) row, along with color palette lookup tables that you could modify to change all the colors using those values. Even the serial SIO daisy chain on the Atari was the basis of what is now how USB enumeration and command/reply works. Shout-out to Bill Budge's Pinball Construction Set (PCS) originally for the Apple ][ and also on Atari. Of course being a kid I mostly made video games and tools for making video games like sprite editors, character/tileset editors, font editors, and level editors. PCS blew my mind and later got me into making programs that build applications, following the likes of Visual Basic (but running on OS/2 PM). reply bitwize 14 hours agoparent> Having this level of understanding made you believe that you could do pretty much anything and everything (just not always fast enough) and makes you program without boundaries except the hardware. Even then it was simple enough to modify parts of the hardware like ROMs or extend it with helpers plugged into the bus or PIO (parallel I/O joystick) ports much like the GPIO on a Raspberry Pi. Some of my favorite bits of The 8 Bit Guy's channel involve when he explains how the NES and SNES controllers work, and to demonstrate he goes \"I wrote a little program on my C64 to poll the NES controller through the user port...\" and proceeds to run it and what do you know, the C64 can read NES controller input, it just needs to be taught how. I'm still a bit miffed about him dremeling out the screws on that rare IBM prototype, but he's still done some really cool stuff. reply jansan 11 hours agoprev [–] Maybe this is a good place to ask a question regarding the hardware that is required to transfer code from and to your C64. I have a C64 and a 1541 sitting in my basement in a box and I sometimes take it out to check if it still works (it does). After watching one particularly insane demo [1] I decided that I need to run them on my own machine for full experience. The question is what hardware is necessary to run them. After a little research it seems that something like a Kung Fu Flash cardridge and an SD card is all that I need to put on my Christmas wish list. Can anyone with more insight tell me if this is the right way to go? [1] https://www.youtube.com/watch?v=KPb6y44HagI reply FLT8 9 hours agoparent [–] Kung Fu flash is probably all you need, with a few caveats (eg. With KFF the drive is \"emulated\" by intercepting kernal vectors rather than acting as a 1541 on the serial bus, so some software that eg. uses fast loaders or relies on the disk drive for offloading computation won't work). If you want to get fancy you could go for something like an Ultimate II+ and a usb key, which will get you a bunch of extra functionally like network connectivity, extra SID support, pretty solid compatibility, REU emulation etc (but UII+ will also cost a lot more). Given you've got a real 1541, maybe you could just copy files/disk images across to the real thing if KFF doesn't work for a particular program I guess? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The VS64 extension allows programming the Commodore 64 using Visual Studio Code on Mac, Windows, and Linux, providing a modern, cross-platform alternative to the Windows-only CBM PRG Studio.",
      "VS64 supports BASIC, Assembly, and C programming with syntax highlighting, tools, and features like auto-numbering and label usage, enhancing the development experience.",
      "It integrates with ACME and Kick assemblers, CC65 and LLVM-MOS C/C++ compilers, and the VICE C64 emulator for debugging, making it a comprehensive tool for C64 development."
    ],
    "commentSummary": [
      "The discussion emphasizes the nostalgia and continued interest in retro computing, particularly programming the Commodore 64 (C64) using Visual Studio Code.",
      "Participants share experiences with vintage systems, crediting them for inspiring their software development careers, and discuss tools like \"retro debugger\" and YouTube tutorials for learning about old games and demos.",
      "The conversation highlights the challenges and joys of programming in assembly language and BASIC, and mentions modern tools and extensions that enhance the retro coding experience."
    ],
    "points": 180,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1733088578
  }
]
