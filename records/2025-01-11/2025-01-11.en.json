[
  {
    "id": 42663231,
    "title": "Be Aware of the Makefile Effect",
    "originLink": "https://blog.yossarian.net/2025/01/10/Be-aware-of-the-Makefile-effect",
    "originBody": "ENOSUCHBLOG Programming, philosophy, pedaling. Home Tags Series Favorites Archive Main Site TILs Be aware of the Makefile effect Jan 10, 2025 Tags: programming I’m not aware of a perfect1 term for this, so I’m making one up: the Makefile effect2. The Makefile effect boils down to this: Tools of a certain complexity or routine unfamiliarity are not run de novo, but are instead copy-pasted and tweaked from previous known-good examples. You see this effect frequently with engineers of all stripes and skill/experience levels, with Make being a common example3: A task (one of a common shape) needs completing. A very similar (or even identical) task has been done before. Make (or another tool susceptible to this effect) is the correct or “best” (given expedience, path dependencies, whatever) tool for the task. Instead of writing a Makefile, the engineer copies a previous (sometimes very large and complicated4) Makefile from a previous instance of the task and tweaks it until it works in the new context. On one level, this is a perfectly good (even ideal) engineering response at the point of solution: applying a working example is often the parsimonious thing to do, and runs a lesser (in theory) risk of introducing bugs, since most of the work is unchanged. However, at the point of design, this suggests a tool design (or tool application5) that is flawed: the tool (or system) is too complicated (or annoying) to use from scratch. Instead of using it to solve a problem from scratch, users repeatedly copy a known-good solution and accrete changes over time. Once you notice it, you start to see this pattern all over the place. Beyond Make: CI/CD configurations like GitHub Actions and GitLab CI/CD, where users copy their YAML spaghetti from the last working setup and tweak it (often with repeated re-runs) until it works again; Linter and formatter configurations, where a basic set of rules gets copied between projects and strengthened/loosened as needed for local conditions; Build systems themselves, where everything non-trivial begins to resemble the previous build system. Does this matter? In many cases, perhaps not. However, I think it’s worth thinking about, especially when designing tools and systems: Tools and systems that enable this pattern often have less-than-ideal diagnostics or debugging support: the user has to run the tool repeatedly, often with long delays, to get back relatively small amounts of information. Think about CI/CD setups, where users diagnose their copy-pasted CI/CD by doing print-style debugging over the network with a layer of intermediating VM orchestration. Ridiculous! Tools that enable this pattern often discourage broad learning: a few mavens know the tool well enough to configure it, and others copy it with just enough knowledge to do targeted tweaks. This is sometimes inevitable, but often not: dependency graphs are an inherent complexity of build systems, but remembering the difference between $< and $^ in Make is not. Tools that enable this pattern are harder to use securely: security actions typically require deep knowledge of the why behind a piece of behavior. Systems that are subject to the Makefile effect are also often ones that enable confusion between code and data (or any kind of in-band signalling more generally), in large part because functional solutions are not always secure ones. Consider, for example, about template injection in GitHub Actions. In general, I think well-designed tools (and systems) should aim to minimize this effect. This can be hard to do in a fully general manner, but some things I think about when designing a new tool: Does it need to be configurable? Does it need syntax of its own? As a corollary: can it reuse familiar syntax or idioms from other tools/CLIs? Do I end up copy-pasting my use of it around? If so, are others likely to do the same? The Makefile effect resembles other phenomena, like cargo culting, normalization of deviance, “write-only language,” &c. I’ll argue in this post that it’s a little different from each of these, insofar as it’s not inherently ineffective or bad and concerns the outcome of specific designs. ↩ Also note: the title is “be aware,” not “beware.” The Makefile effect is not inherently bad! It’s something to be aware of when designing tools and systems. ↩ Make is just an example, and not a universal one: different groups of people master different tools. The larger observation is that there are classes of tools/systems that are (more) susceptible to this, and classes that are (relatively) less susceptible to it. ↩ I’ve heard people joke about their “heritage” Makefiles, i.e. Makefiles that were passed down to them by senior engineers, professors, &c. The implication is that these forebearers also inherited the Makefile, and have been passing it down with small tweaks since time immemorial. ↩ Complex tools are a necessity; they can’t always be avoided. However, the occurrence of the Makefile effect in a simple application suggests that the tool is too complicated for that application. ↩ Previously",
    "commentLink": "https://news.ycombinator.com/item?id=42663231",
    "commentBody": "Be Aware of the Makefile Effect (yossarian.net)237 points by thunderbong 14 hours agohidepastfavorite202 comments myhf 8 minutes ago\"A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.\" – John Gall (1975) Systemantics: How Systems Really Work and How They Fail https://en.wikipedia.org/wiki/John_Gall_(author)#Gall's_law reply Scubabear68 3 hours agoprevThe best term for this is Cargo Cult Development. Cargo Cults arose in the Pacific during World War II, where native islanders would see miraculous planes bringing food, alcohol and goods to the islands and then vanishing into the blue. The islanders copied what they saw the soldiers doing, praying that their bamboo planes and coconut gadgets would impress the gods and restart the flow of cargo to the area. The issue of course is the islanders did not understand the science behind planes, Wallis talkies, guns, etc. Likewise, cargo cult devs see what is possible, but do not understand first principles, so they mimic what they see their high priests of technology doing, hoping they can copy their success. Hence the practice of copying, pasting, trying, fiddling, googling, tugging, pulling and tweaking hoping that this time it will be just right enough to kind of work. Badly, and only with certain data on a Tuesday evening. reply lolinder 2 hours agoparentI don't think of this as being cargo cult development. Cargo culting has more to do with mimicking practices that have worked before without understanding that they only worked within a broader context that is now missing. It's about going through motions or rituals that are actually ineffective on their own in the hopes that you'll get the results that other companies got who also happened to perform those same motions or rituals. What OP is describing isn't like this because the thing being copied—the code—actually is effectual in its own right. You can test it and decide whether it works or not. The distinction matters because the symptoms of what OP calls the Makefile effect are different than the symptoms of cargo culting, so treating them as the same thing will make diagnosis harder. With cargo culting you're wasting time doing things that actually don't work out of superstition. With the Makefile effect things will work, provably so, but the code will become gradually harder and harder to maintain as vestigial bits get copied. reply themanmaran 51 minutes agorootparentI would almost call this the \"boilerplate effect\". Where people copy the giant boilerplate projects for React, K8, Terraform, etc. and go from there. Those boilerplates are ideal for mid to large scale projects. And it's likely you'll need them someday. But in the early stages of development it's going to impart a lot of architecture decisions that really aren't necessary. reply peterldowns 33 minutes agorootparentThat's a great phrase. A perfect example of what you're talking about is actually built-in to the `helm` tool for creating re-usable kubernetes configs: `helm create myapp` creates the most convoluted junk I've ever seen in my life. As a new `helm` user I was extremely confused, as I had been expecting a minimal example that I could start from. Instead, I got a maximal example. Thankfully a more experienced engineer on the infra team confirmed that it was mostly unnecessary boilerplate and I could remove it. Something to consider for anyone else building tools — boilerplate has costs! reply godelski 23 minutes agorootparentprev> only worked within a broader context that is now missing > because the thing being copied—the code—actually is effectual in its own right. I don't understand how the second disproves the former. In fact, a cargo cult works because there's the appearance of a casual linkage. It appears things work. But as we know with code, just because it compiles and runs doesn't mean \"it works\". It's not a binary thing. Personal I find that belief is at the root of a lot of cargo cult development. Where many programmers glue things together say \"it works\" because they passed some test cases but in reality code shouldn't be a Lovecraftian monster made of spaghetti and duct tape. Just because your wooden plane glides doesn't mean it's AC an actual plane reply NomDePlum 51 minutes agorootparentprevSeeing this exact effect where I am currently working. Main available CI/CD tool is a customised and centrally managed Jenkins fleet. It's pretty much impossible to avoid using and seldom needs changed - until it does. Some attempts have been made at centralised libraries and patterns - but even that requires knowledge and study that most won't know is available or be given time to acquire. So when the inevitable tweak or change is made it's made in the easiest, cheapest way - which is usually copying an existing example, which itself was copied from somewhere else. I see exactly the same in other teams repositories. Easiest path taken to patch what already exists as the cost/benefit just isn't perceived to be there to worth prioritising. reply woodruffw 2 hours agoparentprev(Author of the post.) This is mentioned in footnote 1. Concretely, I don’t think this is exactly the same thing as cargo culting, because cargo culting implies a lack of understanding. It’s possible to understand a system well and still largely subsist on copy-pasting, because that’s what the system’s innate complexity incentivizes. That was the underlying point of the post. reply cle 2 hours agoparentprevFor me, there are many cases where I copy-paste stuff I've written in the past b/c some tool is a pain-in-the-ass and I can't afford the mental context switch. I usually do understand what's happening under the hood, but it's still cognitively heavy to switch into that \"mode\" so I avoid it when possible. Tools that fall into this category are usually ops-y things with enormous complexity but are not \"core\" to the problem I'm solving, like CI/CD, k8s, Docker, etc. For Make specifically, I usually just avoid it at this point b/c I find it hard to avoid the context switch. It has nothing to do with miraculous incantations--I know the tradeoff I'm making. But it still runs the risk of turning into the Makefile Effect. reply sgarland 1 hour agorootparentIt’s always hoped (but rarely shown to be true) that by making templates, teams will put thought into their K8s deployments etc. instead of just copy/pasting. Alas, no – even when the only things the devs have to do is add resource requests and limits, those are invariably copy/pasted. If the app gets OOMkilled, they bump up memory limit until it doesn’t. If it’s never OOMkilled, it’s probably never touched, even if it’s heavily over-provisioned (though that would only matter for the request, of course). This has spawned a cottage industry of right-sizing tooling, which does what a dev team could and should have done to begin with: profiling their code to see resource requirements. At this point, I feel like continuing to make things easier is detrimental. I certainly don’t think devs need to know how to administer K8s, but I do firmly believe one should know how to profile one’s code, and to make reasonable decisions based on that effort. reply remus 2 hours agorootparentprevI agree, and I think the key distinction is in understanding. In a cargo cult there's a lack of understanding, whereas I'll often copy and paste code/config I understand to get something done. Usually this is for something I don't do very often (configuring nginx, writing some slightly complicated shell script etc.) I could spend an hour reading docs and writing the thing from scratch but that's likely gonna be wasted time because there's a good chance Im not going to look at that thing again for a few years. reply raverbashing 1 hour agorootparentprevPretty much this And of course every one of those tools has to have their own special language/syntax that makes sense nowhere else (think of all the tools beyond make, like autotools, etc) I don't care about make. I don't care learning about make beyond what's needed for my job Sure, it's a great tool, but I literally have 10 other things that deserve more of my attention than having my makefile work as needed So yeah I'll copy/paste and be done with it reply qwery 2 hours agoparentprevThe article does kind of mention this in footnote '1', for what it's worth: > The Makefile effect resembles other phenomena, like cargo culting, normalization of deviance, “write-only language,” &c. I’ll argue in this post that it’s a little different from each of these, insofar as it’s not inherently ineffective or bad and concerns the outcome of specific designs. reply Learner100 7 minutes agoprevA design philosophy called \"Progressive Disclosure\" tries to tackle this problem, where a tool is supposed to present a minimal set of functionality initially to allow a user to be productive without being an expert and progressively \"reveal\" more complex features as the user becomes more familiar with the tool and attempts to do more complex things. I've heard the programming language Swift followed this philosophy during development, though I've never written any Swift code to know how well it worked out. https://en.wikipedia.org/wiki/Progressive_disclosure reply mianos 11 hours agoprevI have an alternate theory: about 10% of developers can actually start something from scratch because they truly understand how things work (not that they always do it, but they could if needed). Another 40% can get the daily job done by copying and pasting code from local sources, Stack Overflow, GitHub, or an LLM—while kinda knowing what’s going on. That leaves 50% who don’t really know much beyond a few LeetCode puzzles and have no real grasp of what they’re copying and pasting. Given that distribution, I’d guess that well over 50% of Makefiles are just random chunks of copied and pasted code that kinda work. If they’re lifted from something that already works, job done—next ticket. I’m not blaming the tools themselves. Makefiles are well-known and not too verbose for smaller projects. They can be a bad choice for a 10,000-file monster—though I’ve seen some cleanly written Makefiles even for huge projects. Personally, it wouldn’t be my first choice. That said, I like Makefiles and have been using them on and off for at least 30 years. reply huijzer 9 hours agoparent> That leaves 50% who don’t really know much beyond a few LeetCode puzzles and have no real grasp of what they’re copying and pasting. Small nuance: I think people often don’t know because they don’t have the time to figure it out. There are only so many battles you can fight during a day. For example if I’m a C++ programmer working on a ticket, how many layers of the stack should I know? For example, should I know how the CPU registers are called? And what should an AI researcher working always in Jupyter know? I completely encourage anyone to learn as much about the tools and stack as possible, but there is only so much time. reply kragen 8 hours agorootparentIf you spend 80% of your time (and mental energy) applying the knowledge you already have and 20% learning new things, you will very quickly be able to win more battles per day than someone who spends 1% of their time learning new things. Specifically for the examples at hand: - at 20%, you will be able to write a Makefile from scratch within the first day of picking up the manual, rather than two or three weeks if you only invest 1%. - if you don't know what the CPU registers are, the debugger won't be able to tell you why your C++ program dumped core, which will typically enable you to resolve the ticket in a few minutes (because most segfaults are stupid problems that are easy to fix when you see what the problem is, though the memorable ones are much hairier.) Without knowing how to use the disassembly in the debugger, you're often stuck debugging by printf or even binary search, incrementally tweaking the program until it stops crashing, incurring a dog-slow C++ build after every tweak. As often as not, a fix thus empirically derived will merely conceal the symptom of the bug, so you end up fixing it two or three times, taking several hours each time. Sometimes the source-level debugger works well enough that you can just print out C++-level variable values, but often it doesn't, especially in release builds. And for performance regression tickets, reading disassembly is even more valuable. (In C#, managed C++, or Python, the story is of course different. Until the Python interpreter is segfaulting.) How long does it take to learn enough assembly to use the debugger effectively on C and C++ programs? Tens of hours, I think, not hundreds. At 20% you get there after a few dozen day-long debugging sessions, maybe a month or two. At 1% you may take years. What's disturbing is how many programmers never get there. What's wrong with them? I don't understand it. reply remus 2 hours agorootparentYou make it sound easy, but I think it's hard to know where to invest your learning time. For example, I could put some energy into getting better at shell scripting but realistically I don't write enough of it that it'll stick so for me I don't think it'd be a good use of time. Perhaps in learning more shell scripting I have a breakthrough and realise I can do lots of things I couldn't before and overnight can do 10% more, but again it's not obvious in advance that this will happen. reply kragen 1 hour agorootparentI agree. And there's no infallible algorithm. I think there are some good heuristics, though: - invest more of your time in learning more about the things you are currently finding useful than in things that sound like they could potentially be useful - invest more of your time in learning skills that have been useful for a long time (C, Make) than in skills of more recent vintage (MobX, Kubernetes), because of the Lindy Effect - invest more of your time in skills that are broadly applicable (algorithms, software design, Python, JavaScript) rather than narrowly applicable (PARI/GP, Interactive Brokers) - invest your time in learning to use free software (FreeCAD, Godot, Postgres) rather than proprietary software (SolidWorks, Unity, Oracle), because sooner or later you will lose access to the proprietary stuff. - be willing to try things that may not turn out to be useful, and give them up if they don't - spend some time every day thinking about what you've been doing. Pull up a level and put it in perspective reply BurningFrog 48 minutes agorootparentprevOne simple approach that the second, or at least third, time you deal with something, you invest time to learn it decently well. Then each time you come back to it, go a bit deeper. This algorithm makes you learn the things you'll need quite well without having to understand and/or predict the future. reply icameron 3 hours agorootparentprevThat’s an insightful comment, but there is a whole universe of programmers who never have to directly work in C/C++ and are productive in safe languages that can’t segfault usually. Admittedly we are a little jealous of those elite bitcrashers who unlock the unbridled power of the computer with C++… but yeah a lot of day jobs pay the bills with C#, JavaScript, or Python and are considered programmers by the rest of the industry reply kragen 2 hours agorootparentYeah, I write most things in Python or JavaScript because it's much more practical. reply silveraxe93 8 hours agorootparentprevThis is the 40% that OP mentioned. But there's a proportion on people/engineers that are just clueless and are incapable of understanding code. I don't know the proportion so can't comment on the 50% number, but hey definitely exist. If you never worked with them, you should count yourself lucky. reply silver_silver 8 hours agorootparentprevWe can’t really call the field engineering if this is the standard. A fundamental understanding of what one’s code actually makes the machine do is necessary to write quality code regardless of how high up the abstraction stack it is reply kragen 7 hours agorootparentSteam engines predate the understanding of not just the crystalline structure of steel but even the basics of thermodynamics by quite a few decades. reply silver_silver 6 hours agorootparentYes and they’re far less efficient and require far more maintenance than an equivalent electric or even diesel engine, where equivalent power is even possible reply kragen 2 hours agorootparentSteam engines currently power most of the world's electrical grid. The main reason for this is that, completely contrary to what you said, they are more efficient and more reliable than diesel engines. (Electric motors of course are not a heat engine at all and so are not comparable.) Steam engines used to be very inefficient, in part because the underlying thermodynamic principles were not understood, but also because learning to build safe ones (largely a question of metallurgy) took a long time. Does that mean that designing them before those principles were known was \"not engineering\"? That seems like obvious nonsense to me. reply BlueTemplar 6 hours agorootparentprevWhy do you assume that the same doesn't apply to electric and diesel engines ? reply cudgy 8 hours agorootparentprevSure if you are doing embedded programming in C. How does one do this in web development though where there are hundreds of dependencies that get updated monthly and still add functionality and keep their job? reply silver_silver 6 hours agorootparentThe current state of web development is unfortunately a perfect example of this quality crisis. The tangle of dependencies either directly causes or quickly multiplies the inefficiency and fragility we’ve all come to expect from the web. The solution is unrealistic because it involves design choices which are either not trendy enough or precluded by the platform reply kragen 7 hours agorootparentprevMaybe switch to less frequently updated dependencies and rewrite the easy ones in-house? reply pdimitar 6 hours agorootparentYes, and I should overrule half the business decisions of the company while I am at it. Oh, and I'll push back on \"we need the next feature next week\" and I'll calmly respond \"we need to do excellent engineering practices in this company\". And everybody will clap and will listen to me, and I will get promoted. ...Get real, dude. Your comments come across a bit tone-deaf. I am glad you are in a privileged position but you seem to have fell for the filter bubble effect and are unaware to how most programmers out there have to work if they want to pay the bills. reply kragen 1 hour agorootparentI know a lot of people have terrible jobs at profoundly dysfunctional companies. I've had those too. That situation doesn't improve unless you, as they say, have the serenity to accept the things you cannot change, the courage to change the things you can, and the wisdom to know the difference. Not everyone has a position where they have the autonomy to spend a lot of effort on paying down technical debt, but some people do, and almost every programmer has a little. I think it's important to keep in view both your personal incentive system (which your boss may be lying to you about) and the interests of the company. reply pdimitar 22 minutes agorootparentThe serenity in question boils down to \"I'll never make enough money to live peacefully and being able to take a two years sabbatical so let's just accept I'll be on the hamster wheel for life and I can never do anything about it\". No. I'll let my body wither and get spent before my spirit breaks. I refuse to just \"accept\" things. There's always something you can do. BTW is that not what HN usually preaches? \"Change your job to a better one\" and all that generic motivational drivel [that's severely disconnected from reality]? Not throwing shade at you here in particular, just being a bit snarky for a minute. :) RE: your final point, I lost the desire to keep view of both my personal and my company's incentive systems. Most \"incentive systems\" are basically \"fall in line or GTFO\". Before you ask, I am working super hard to change my bubble and get a bit closer to yours. To say it's not easy would be so understated so as to compare the description of a lightning hit on you and you enduring the said lightning hit. But as said above, I am never giving up. But... it's extremely difficult, man. Locality and your own marketing matter a lot, and when you have been focused on technical skills all your life and marketing is as foreign to you as are the musical notes of an alien civilization... it's difficult. reply ori_b 3 hours agorootparentprevYes, sometimes things are unfixably broken, and it's impossible to build anything good. For everything else, there's MasterCard. reply pdimitar 20 minutes agorootparentAny golden MasterCards with $50M one-time limit you could offer for free? I can think of a few things to fix with those. RE: unfixably broken, well, not necessarily in concept but de facto you are sadly correct. Most people resist even the provably good changes. reply ajross 3 hours agorootparentprev> I completely encourage anyone to learn as much about the tools and stack as possible, but there is only so much time. That seems like a weird way to think about this. I mean, sure, there's no time today to learn make to complete your C++ ticket or whatever. But yesterday? Last month? Last job? Basically, I think this matches the upthread contention perfectly. If you're a working C++ programmer who's failed to learn the Normal Stable of Related Tools (make, bash, python, yada yada) across a ~decade of education and experience, you probably never will. You're in that 50% of developers who can't start stuff from scratch. It's not a problem of time, but of curiosity. reply Joker_vD 3 hours agorootparent> I mean, sure, there's no time today to learn make to complete your C++ ticket or whatever. But yesterday? Last month? Last job? That seems like a weird way to think about this. Of course there was no time in the past to learn this stuff, if you still haven't learned it by the present moment. And even if there were, trying to figure out whether there perhaps was some free time in the past is largely pointless, as opposed to trying to schedule things in the future: you can't change the past anyhow, but the future is somewhat more malleable. reply ajross 2 hours agorootparentTo be clear: I'm not suggesting a time machine, and I'm not listing any particular set of skills everyone must have. I'm saying that excusing the lack of core job skills by citing immediate time pressure is a smell. It tells me that that someone probably won't ever learn weird stuff. And in software development, people who don't learn weird stuff end up in that 50% bucket posited upthread. reply oweiler 6 hours agorootparentprevThat's why suggestions like RTFM! are stupid. I just don't have time to read every reference documentation of every tool I use. reply kccqzy 1 hour agorootparentI feel like your working environment might be to blame: maybe your boss is too deadline-driven so that you have no time to learn; or maybe there is too much pressure to fix a certain number of tickets. I encourage you to find a better workplace that doesn't punish people who take the time to learn and improve themselves. This also keeps your skills up to date and is helpful in times of layoffs like right now. reply aulin 42 minutes agorootparentprevyou don't have the time because you spend it bruteforcing solutions by trial and error instead of reading the manual and doing them right the first time reply prerok 58 minutes agorootparentprevSeriously? Yes, you should read the docs of every API you use and every tool you use. I mean, it's sort of ok if you read somewhere how to use it and you use it in the same way, but I, for one, always check the docs and more often even the implementation to see what I can expect. reply adrian_b 8 hours agoparentprevActually it is trivial to write a very simple Makefile for a 10,000 file project, despite the fact that almost all Makefiles that I have ever seen in open-source projects are ridiculously complicated, far more complicated than a good Makefile would be. In my opinion, it is a mistake almost always when you see in a Makefile an individual rule for making a single file. Normally, there should be only generic building rules that should be used for building any file of a given type. A Makefile should almost never contain lists of source files or of their dependencies. It should contain only a list with the directories where the source files are located. Make should search the source directories, find the source files, classify them by type, create their dependency lists and invoke appropriate building rules. At least with GNU make, this is very simple and described in its user manual. If you write a Makefile like this, it does not matter whether a project has 1 file or 10,000 files, the effort in creating or modifying the Makefile is equally negligible. Moreover, there is no need to update the Makefile whenever source files are created, renamed, moved or deleted. reply mianos 6 hours agorootparentIf everything in your tree is similar, yes. I agree that's going to be a very small Makefile. While this is true, for much larger projects, that have lived for a long time, you will have many parts, all with slight differences. For example, over time the language flavour of the day comes and goes. Structure changes in new code. Often different subtrees are there for different platforms or environments. The Linux kernel is a good, maybe extreme, but clear example. There are hundreds of Makefiles. reply adrian_b 3 hours agorootparentDifferent platforms and environments are handled easily by Make \"variables\" (actually constants), which have platform-specific definitions, and which are sequestered into a platform-specific Makefile that contains only definitions. Then the Makefiles that build a target file, e.g. executable or library, include the appropriate platform-specific Makefile, to get all the platform-specific definitions. Most of my work is directed towards embedded computers with various architectures and operating systems, so multi-platform projects are the norm, not the exception. A Makefile contains 3 kinds of lines: definitions, rules and targets (typical targets may be \"all\", \"release\", \"debug\", \"clean\" and so on). I prefer to keep these in separate files. If you parametrize your rules and targets with enough Make variables to allow their customization for any environment and project, you must almost never touch the Makefiles with rules and targets. For each platform/environment, you write a file with appropriate definitions, like the names of the tools and their command-line options. The simplest way to build a complex project is to build it in a directory with a subdirectory for each file that must be created. In the parent directory you put a Makefile that is the same for all projects, which just invokes all the Makefiles from the subdirectories that it finds below, passing any CLI options. In the subdirectory for each generated file, you just put a minimal Makefile, with only a few lines, which includes the Makefiles with generic rules and targets and the Makefile with platform-specific definitions, adding the only information that is special for the generated file, i.e. what kind of file it is, e.g. executable, static library, dynamic library etc., a list of directories where to search for source files, the strings that should be passed to compilers for their include directory list and their preprocessor definition list, and optionally and infrequently you may override some Make definitions, e.g. for providing some special tool options, e.g. when you generate from a single source multiple object files. reply Loic 9 hours agoparentprevI like Makefiles, but just for me. Each time I create a new personal project, I add a Makefile at the root, even if the only target is the most basic of the corresponding language. This is because I can't remember all the variations of all the languages and frameworks build \"sequences\". But \"$ make\" is easy. reply 1aqp 8 hours agorootparentI'd say: you are absolutely using the right tool. :-) reply choeger 8 hours agorootparentprevYou're probably using the wrong tool and should consider a simple plain shell script (or a handful of them) for your tasks. test.sh, build.sh, etc. reply poincaredisk 2 hours agorootparentI disagree. Make is - at it's simplest form - exactly a \"simple plain shell script\" for your tasks, with some very nice bonus features like dependency resolution. Not the parent, bit I usually start with a two line makefile and add new commands/variables/rules when necessary. reply nrclark 1 hour agorootparentprev(not the parent) Make is - at its core - a tool for expressing and running short shell-scripts (\"recipes\", in Make parlance) with optional dependency relationships between each other. Why would I want to spread out my build logic across a bunch of shell scripts that I have to stitch together, when Make is a nicely integrated solution to this exact problem? reply f1shy 9 hours agoparentprevI would just change the percentages, but is about as true as it gets. reply mianos 6 hours agorootparentI’d be curious to hear your ratio. It really varies. In some small teams with talented people, there are hardly any “fake” developers. But in larger companies, they can make up a huge chunk. Where I am now, it’s easily over 50%, and most of the real developers have already left. PS: The fakes aren’t always juniors. Sometimes you have junior folks who are actually really good—they just haven’t had time yet to discover what they don’t know. It’s often absolutely clear that certain juniors will be very good just from a small contribution. reply f1shy 5 hours agorootparentMy personal experience: - 5% geniuses. This are people who are passionate about what they do, they are always up to date. Typically humble, not loud people. - 15% good, can do it properly. Not passionate, but at least have a strong sense of responsibility. Want to do “the right thing” or do it right. Sometimes average intelligence, but really committed. - 80% I would not hire. People who talk a lot, and know very little. Probably do the work just because they need the money. That applies for doctors, contractors, developers, taxi drivers, just about anything and everything. Those felt percentages had been consistent across 5 countries, 3 continents and 1/2 a century of life PS: results are corrected for seniority. Even in the apprentice level I could tell who was in each category. reply sebazzz 11 hours agoparentprev> That leaves 50% who don’t really know much beyond a few LeetCode puzzles and have no real grasp of what they’re copying and pasting. Who likely wouldn't have a job if it weren't for LLMs. reply raziel2p 10 hours agorootparentpretty sure we've made this complaint about a subset of developers since way before chatgpt and the like. reply f1shy 9 hours agorootparentAnd that happens not only with developers, but in any profession, which gives me shivers when I go to the doctor! reply getnormality 2 hours agoprevAnother factor is frequency of use. I use LaTeX to do big write-ups on the order of once per year or less. LaTeX at the level I use it is not a hard tool, but I generally start a new document by copy-pasting a previous document because there is a lot of detail about how to use it that I'm never going to remember given that I only use it for a few weeks once a year. reply folmar 9 minutes agoparentI use it more often and also start with copy-paste header, that includes: * all packages needed for my language (fontenc, babel, local typography package) * typical graphicx/fancyhdr/hyperref/geometry packages that are almost always needed * a set of useful symbol and name definitions for my field If you are not writing math or pure text in English only LaTeX is batteries not included. reply Linux-Fan 1 hour agoparentprevI usually try to avoid the \"makefile effect\" by learning the technolgoy I use reasonably frequently (like e.g. Makefiles, Shell Scripts, ...). However, despite the fact that I used to use LaTeX very much, I always copy-pasted from a template. It is even worse with beamer presentations and TikZ pictures where I would copy-paste from a previous presentation or picture rather than a template. For TikZ I am pretty sure that the tool is inherently complex and I just haven't spent enough time to learn it properly. For LaTeX I have certainly spent enough time on learning it so I wonder whether it might be something different. In my opinion it could very well be a matter of “(in)sane defaults”. Good tools should come with good defaults. However, LaTeX is not a good tool wrt. this metric, because basically all my documents start something like ~~~ \\documentclass[paper=a4, DIV9, 12pt, abstracton, headings=normal, captions=tableheading]{scrartcl} \\usepackage[T1]{fontenc} \\usepackage[utf8]{inputenc} \\usepackage[english,german,ngerman]{babel} \\usepackage[english,german]{fancyref} % ... \\usepackage{microtype} \\usepackage{hyperref} ~~~ Most of this is to get some basic non-ASCII support that is needed for my native tongue or enable some sane defaults (A4 paper, microtype...) which in a modern tool like e.g. pandoc/markdown may not be needed... Hence the purpose of copy-pasing the stuff around is often to get good defaults which a better tool might give you right out of the box (then without copy/paste). reply kccqzy 50 minutes agorootparentCopy-pasting itself is not bad per se. What's bad is copy-pasting without understanding the why and how. For LaTeX I also copy-paste a whole lot from older files, but I don't feel bad because (a) I wrote these files before, (b) I know exactly what each line is doing, (c) I understand why each line is needed in the new doc. I wrote a relatively large amount of TikZ code earlier in my life (basically used it as a substitute for Illustrator) and for this library in particular, I think it just has so much syntax to remember that I cannot keep it all in my brain for ever. So I gladly copy from my old TikZ code. reply mattbillenstein 11 hours agoprevI think Makefile is maybe the wrong analogy - the problem with most people and makefiles is they write so few of them, the general idea of what make does is at hand, but the muscle memory of how to do it from scratch is not. But, point taken - I've seen so much code copy-pasta'd from the web, there will be like a bunch of dead stuff in it that's actually not used. A good practice here is to keep deleting stuff until you break it, then put whatever that was back... And delete as much as possible - certainly everything you're not using at the moment. reply sumanthvepa 11 hours agoparentThis is exactly the problem I face with many tools, Makefiles, KVM setups, docker configurations, CI/CD pipelines. My solution so far has been to create a separate repository with all my notes, shell script example programs etc, for these tool, libraries or frameworks. Every time I have to use these tools, I refer to my notes to refresh my memory, and if I learn something new in the process, I update the notes. I can even point an LLM at it now and ask it questions. The repository is personal, and contains info on tools that are publicly available. I keep organisation specific knowledge in a similar but separate repo, which I discard when my tenure with a client or employer ends. reply parasti 10 hours agorootparentWhat if your client comes back? On a more practical note, what structure, formats and tools do you use that enable you to feed it to an LLM? reply sumanthvepa 10 hours agorootparentI'm usually contractually obligated to destroy all client IP that I may posses at the end of an engagement. My contracts usually specify that I will retain engagement specific information for a period of six months beyond the end of the contract. If they come back within that time, then I'll have prior context. Otherwise it's gone. Occasionally, a client does come back after a year or two, but most of the knowledge would have been obsolete and outdated anyway. As for LLMs. I have a couple of python scripts that concatenate files in the repo into a context that I pass to Google's Gemini API or Google AI studio, mostly the latter. It can get expensive in some situations. I don't usually load the whole repository. And I keep the chat context around so I can keep asking question around the same topic. reply mcdeltat 12 hours agoprevAt my work I've noticed another contributing factor: tools/systems that devs need to interact with at some point, but otherwise provide little perceived value to learn day-to-day. Example is build system and CI configuration. We absolutely need these but devs don't think they should be expected to deal with them day to day. CI is perceived as a system that should be \"set and forget\", like yeah we need it but really I have to learn all this just to build the app? Devs expect it to \"just work\" and if there are complexities then another team (AKA my role) deals with that. As a result, any time devs interact with the system, there's a high motivation to copy from the last working setup and move on with their day to the \"real\" work. The best solution I see is meet the devs halfway. Provide them with tooling that is appropriate simple/complex for the task, provide documentation, minimise belief in \"magic\". Tools like Make kinda fail here because they are too complex and black-box-like. reply john-tells-all 2 hours agoparentStrong agree. The best workflow I've seen uses CICD as a very thin wrapper around in-tree scripts or make files. If a Dev can run some/all of the \"cicd\" stuff locally, they can see, control, and understand it. It helps tremendously to have a sense of ownership and calm, vs \"cicd is something else, la la la\". (This doesn't always work. We had a team of two devs, who had thin-wrapper CICD, who pretended it was an alien process and refused to touch it. Weird.) reply peterldowns 30 minutes agorootparent+1. The only CI tool that I've seen really organize around this principle is Buildkite, which I've used and enjoyed. I'm currently using Github Actions and it's fine but Buildkite is literally sooooo good for the reasons you've mentioned. reply nicoburns 11 hours agoparentprevFor me the big problems with CI setups tend to be: - They're often slow - They're often proprietary - They're often dealing with secrets which limits who can work on them - You generally can't run them locally So the feedback cycle for working on them is incredibly long. And working on them is therefore a massive pain. reply sharkjacobs 11 hours agorootparent> You generally can't run them locally I recognize that this is such a disincentive for me taking the initiative to fiddle with and learn about anything like this reply vintermann 2 hours agorootparentSame goes for anything \"enterprisey\". Last time I set up a big project, I made a commitment that \"we should be able to check out and build this whole thing, for as long as humanly possible\". reply pipes 8 hours agorootparentprevThe local part is my big problem too. I used azure Dev ops in work. I find clicking through the UI to be a miserable experience, Id love to have it running locally so I could view inputs and outputs on the file system. Also yaml is an awful choice, no one I know enjoys working with it. The white space issues just get worse and worse longer your files get. reply tempodox 4 hours agorootparentprev> You generally can't run them locally GitLab CI gives you local runners. You can completely self-host CI. reply Joker_vD 3 hours agorootparentWell, yes, but aren't those runners have different configuration than the runners that are actually deployed and used by your company's CI/CD? reply tempodox 2 hours agorootparentWat? Our company's CI is GitLab CI, self-hosted. What other runners would we need? reply Joker_vD 2 hours agorootparentThe local ones, mentioned in the original comment that we've been discussing in this thread? The local runner, executing on the developer's machine. reply tempodox 1 hour agorootparentOf course it makes no difference whether that runner runs on our local CI host or on my dev machine. reply Plasmoid 2 hours agorootparentprevIt's not self hosting. It's can I run the build from my local command line and get the same results. reply tempodox 2 hours agorootparentHow is running those runners on our own hosts (locally) not self-hosting? reply mdaniel 1 hour agorootparentSpoken like someone who has not tried what you are describing. There are two moving parts to your response: a locally hosted runner awaits jobs from GitLab itself, which doesn't help running _locally_, and the other part is that -- back when it existed! -- trying $(gitlab-runner exec) was not a full fledged implementation of the GitLab CI concepts, making it the uncanny valley of \"run something locally.\" However, as of v16 there is no more exec https://gitlab.com/gitlab-org/gitlab/-/issues/385235 which I guess is good and bad. Good in that it not longer sets improper expectations that it could have plausibly done anything, and bad in that now it joins GitHub Actions[1] in not having any _local_ test strategy aside from \"boot up gitlab/gitlab-ce && echo good luck\" 1: yes, I'm acutely aware of the 3(?) implementations/forks of nektos/act that claim to do GHA but, again, readme is not software and I can tell you with the utmost certainty they do not do as advertised reply exitb 11 hours agoparentprevThe office coffee machine is not „set and forget”, but you wouldn’t expect the entire responsibility for it’s maintenance to be evenly distributed between all people that use it. Similarly, CI needs ownership and having it fall on the last developer that attempted to use it is not an efficient way of working. reply tempodox 4 hours agoparentprevIf you think `make` is “too complex and black-box-like” then you haven't seen `cmake`. reply vintermann 2 hours agorootparentIf you think cmake is a good example of more complex than make, then you haven't seen automake/autoconf. The first thing I thought of. You can find tons of tons of configure scripts that check if you're running ancient versions of Unix, checks that a byte is 8 bits wide, and a ton of other pointless checks. They don't do anything with all that information, don't think for a moment that you can actually build the app on Irix, but the checks for it have been passed along for decades likes junk DNA. reply tempodox 2 hours agorootparentFirstly, automake/autoconf is not `make`, but a different piece of software, and secondly, that you know all those details about it is because it is not black-box-like. reply vintermann 2 hours agorootparentI never said it was. It's a script to generate a script to generate a makefile, more or less. If it wasn't black box like, why do people keep blindly copying tests which check things which haven't been relevant for decades and in any case would require a ton of manual effort to actually use for something? reply internet_points 9 hours agoparentprevYeah, I think this is the real issue. Too many different tool types that need to interact, so you don't get a chance to get deep knowledge in any of them. If only every piece of software/CI/build/webapp/phone-app/OS was fully implemented in GNU make ;-) There's a tension between using the best tool for the job vs adding yet another tool/dependency. reply layer8 46 minutes agoprevThere are certainly a lot of tools that are more complicated than necessary, but Make as a tool isn’t a good example of that, IMO. With modern tooling, more often than not the complexity problem is compounded by insufficient documentation, the existing documentation being predominantly cookbook-style and not explaining the conceptual models needed to reason about how the tool works, nor providing a detailed and precise enough specification of the tool. That isn’t the case for Make, which is well-documented and not difficult to get a good grasp on, if one only takes the time to actually read the documentation. The cookbook orientation mentioned above in turn leads to a culture that underemphasizes the importance of learning and understanding the tools that one is using, and of having thorough documentation that facilitates that. Or maybe the direction of causation is the other way around. In any case, I see the problem more in too little time being spent in creating comprehensive and up-to-date documentation on tooling (and designing the tooling to be amenable to that in the first place), and in too little resources being allocated to teaching and learning the necessary tooling. reply rednafi 2 hours agoprevThis also happens with tools you have to use but don’t get much payoff from—like internal tooling. At work, we have a shitty in-house feature flag service. It breaks all the time and is super finicky. Learning it properly doesn’t really help me, so I mostly copy and paste my way through it. Another example is jq. I use it occasionally, and ChatGPT handles the syntax pretty well. For me, learning it properly just isn’t worth the time or effort. reply linuxftw 2 hours agoparentMakefile syntax is also well understood by ChatGPT. If you want to know a suitable way for doing some task, ChatGPT can do it. It can also explain what another Makefile is doing. Here's an example of a (similar) prompt I used recently: \"Write me a makefile that executes a script inside a docker container. I want the script to be someprefix- that calls /app/.sh inside the container.\" I don't have to care about Makefile syntax anymore for the most part. reply rednafi 2 hours agorootparentExactly. I used to go for Just and Go Task as Make alternatives, but not anymore. LLMs are great with Make syntax, and Make is everywhere. reply yuppiepuppie 10 hours agoprevThis is “Copy-Pasta Driven Development” [0] and it’s not even related to makefiles. It’s related to the entire industry copying code from here to there without even knowing what they are copying. TBH I think copilot has made this even worse, as we are blindly accepting chucks of code into our code bases. [0] https://andrew.grahamyooll.com/blog/copy-pasta-driven-develo... reply pdimitar 5 hours agoparentBlame the business people. I tried becoming an expert in `make` probably at least 7 times in a row, was never given time to work with it daily until I fully memorized it. At one point I simply gave up; you can never build the muscle memory and it becomes a cryptic arcane knowledge you have to relearn from scratch every time you need it. So I moved to simpler tools. The loss of deep work is not the good programmers' fault. It's the fault of the business people. reply markfsharp 8 hours agoparentprevCould not agree more! reply jcarrano 5 hours agoprevI wouldn't say this is necessarily a bad thing. I wrote my first version of a Makefile with automatic dependencies and out-of-tree builds 10+ years ago and I have been copying and improving it since. I do try to remove unneeded stuff when possible. The advantage is that one can go in and modify any aspect of build process easily, provided one takes care to remove cruft so that the Makefile does not become huge. This is very important for embedded projects. For me, the advantages have surpassed the drawbacks (which I admit are quite a few). You could, in theory, abstract much of this common functionality away in a library (whether for Make or any other software), however properly encapsulating the functionality is additional work, and Make does not have great built-in support for modularization. In this sense I would not say Make is overly complex but rather the opposite, too simple. Imagine how it would be if in C global variables were visible across translation units. So, in a way, the \"Makefile effect\" is in part due to the nature of the problem being solved and part due to limitations in Make. reply rini17 1 hour agoparentCan you imagine the makefile was made by someone else and you are now suddenly confronted with the result of 10 years of tuning. reply jcarrano 10 minutes agorootparentI am that someone else because I seldom edit the makefiles and I forget things. That's why I try to trim unused targets and recipes and I try to keep it documented. In the end it is no different from any code that's suffered from 10 years of tuning and it can get ugly. Maybe Make is even somewhat worse in this respect, but then again it does not need to be changed often. reply earnestinger 5 hours agoprev> However, the occurrence of the Makefile effect in a simple application suggests that the tool is too complicated for that application. I interpret it in a bit of different way. Makefile is relatively simple and unopinionated like a brick. Also makefile defines/reflects project’s structure. From simple blocks one can build any shape one want. Total freedom. Problem is, make doesn’t impose best practice and doesn’t steer you clear of common pitfalls of project structuring and building and publishing. One example for illustration: Out of source builds is rather good idea, but not imposed by make. So makefile is not enough, one needs all the life-lessons of using make, so inherited makefiles are better than written from scratch. reply o11c 2 hours agoparentTo expand the illustration: * using out-of-source builds is a good idea * using fully automatic dependencies is a good idea * never committing generated files is a good idea (avoid hysteresis) It is fundamentally very difficult to get all three of these at once; automatic dependencies often require generating files ahead of time, but generating files often involves needing to know the dependencies or at least their paths ahead of time. These days the trend seems to be to commit \"generated-in-place\" files, which avoids some of the historical problems with the last (at the cost of introducing others). I don't claim this is optimal. reply dare944 1 hour agoprev> However, the occurrence of the Makefile effect in a simple application suggests that the tool is too complicated for that application. The author's overall point is fine (specifically, that one should consider developer cut-and-paste behavior as an indicator of unnecessary complexity in a tool). However, when discussing the designer's perspective, I think the author should have taken a broader view of complexity. Much of the complexity in Makefiles stems from their generality; essentially, the set of problems to which a Makefile can be a solution. Substantively reducing this complexity necessarily means eliminating some of those use cases. In the case of make, this is clearly possible. Make as a Unix tool has been around for a looong time, and one can look at the early versions for an idea of how simple it could be. But the rub is, simplifying make doesn't necessarily reduce complexity. Once armed with a simpler, but more limited make, developers are now tasked not only with knowing the easier Makefile syntax, but also knowing when make isn't an appropriate solution, and when and how to use whatever tool exists to fill the gap. Compounding this is the fact documentation and shared knowledge regarding which tool is appropriate for which problem is much harder to come by than documentation for the tool itself. This can easily lead to the tool choice equivalent of developer cut-and-paste behavior: \"so-and-so uses build tool X so I must use it too\", \"if your doing (general description of problem) the only build tool you ever need is Y\", \"I used Z before, so I'm just going to make it work again\". Essentially you can think of make as one \"verb\" in a sprawling and uncoordinated domain-specific language that targets building things. Developers need some level of proficiency across this language to succeed at their work. But trading complexity that must be mastered in one tool for complexity that must be mastered across tools can very easily increase overall complexity and promote its own kind of \"Makefile Effect\", just at a different level. EDIT: Some might prefer the term \"Cargo Culting\" rather than \"Makefile Effect\" here. I suggest they are the same behavior just in different contexts. reply fargle 2 hours agoprevcalling this \"Makefile\" effect is a terrible disservice. one could as easily call it \"PHP\" effect, \"YAML\" effect, etc. pick whichever language you'd personally like to denigrate. there is nothing that makes makefiles inherently more or less susceptible to this. if it's more common, it's because people don't want to take the time doing more solid engineering and clean design for something like a ci/cd config or a makefile, being viewed as ancillary or less important. and so they often don't want to learn the language, so monkey-see-monkey-do. as sibling comments state, this is better called cargo cult or maybe copy-pasta. and i've seen it with any language c, c++, python, scripts, config files, anything. i even see it in chat gpt answers because it's regurgitating someone else copy pasta. if it's a problem, it's a problem of discipline. reply cassepipe 1 hour agoparentThe reason why it seems to apply to makefiles in particular is because most people think life is too short to bother learning and understanding makefiles so it seems to happen there more than anywhere else. Also no matter how complicated and subtle you think your makefile is, true experts will tell you it's wrong and you instead copy their apparently over-engineered, hard to understand makefile reply rednafi 2 hours agoparentprevBut it sure makes it more memorable. I would’ve called it GitHub Actions effect. reply donatj 12 hours agoprevIf I had a nickel for every time I have seen a Makefile straight up copied from other projects and modified to \"work\" while leaving completely unrelated unnecessary build steps and targets in place. It's a major pet peeve of mine. reply bboygravity 11 hours agoparentHow do you know what is and isn't related if nothing is documented? Trial and error? Well have fun with that :p reply marcosdumay 2 hours agorootparentHum... You know, a makefile is documentation. That's why you should probably never copy one (except for a single line here or there). There's space for commenting a few stuff, but your target names and variables should explain most of what is going there. Anyway, the article and most people here seem to be talking about those autotools generated files. Or hand-built ones that look the same way. But either way, it's a bad solution caused by forcing a problem to be solved by a tool that wasn't aimed at solving it. We have some older languages without the concept of a \"project\" that need a lot of hand-holding for compiling, but despite make being intentionally created for that hand-holding, it's clearly not the best tool for that one task. reply donatj 11 hours agorootparentprevExactly. Bonus points if the person who started the project moved on and you have to be the one to build and maintain it. reply rmgk 6 hours agorootparentprevYou find the first part in your stack that is documented (e.g., make is documented, even if your makefile is not) and use that documentation to understand the undocumented part. You then write down your findings for the next person. If you don’t have enough time, write down whatever pieces you understood, and write down what parts “seem to work, but you don’t understand“ to help make progress towards better documentation. If you put the documentation as comments into the file, this can make copy&pasting working examples into a reasonably solid process. reply svilen_dobrev 3 hours agoprevApart of not knowing / unable to start from scratch, this is about frequent, and infrequent use. The latter also means not-that-important-in-overall-landscape, and brings forgetting and perceiving it as less \"ROI\", so one (hopefuly) finds and copy-pastes that-last-working thing into.. whatever Frankenstein. https://utcc.utoronto.ca/~cks/space/blog/programming/Frequen... reply teeray 3 hours agoprevI see this often on our codebase. It was mostly written by ex-C# developers who were new to writing Go, and there’s many ham-handed C#-isms in there. At some point, someone took a guess at how something should be, then subsequent changes were done by copy-paste. Years down the road, another copy-paste job happens, and when I point out that the patterns within are not good (like, can actually be buggy), I get a confused response, because that is what was there. There is an implicit assumption that the code written espouses best-practices, but that is far from the truth. reply nwallin 23 minutes agoparentHappens to us at my day job too. The codebase is primarily C++. My most recent horror story is that I was stepping through some code in an object that was statically initialized and a variable that was initialized as `static const double foo = nan;` had a value of 0 in it. This was very surprising to me. I look at how we defined nan and it turns out that nan is a singleton that was initialized in a DLL somewhere. My variable was being initialized before the singleton nan was initialized. I asked around, and someone with access to the previous version control system (we migrated to git in 2016) discovered that this was part of the original commit to that VCS back sometime in 2003-2006 or something. We think that was probably from before our C++ compiler was updated to support C++98 and `numeric_limits` was added. So of course I moved this over so that accessing our special nan singleton is just a static constexpr call to `std::numeric_limits::quiet_NaN()`. But our special nan singleton is used all over the place in our codebase. So of course I have to check to see nobody's doing something weird with it. Of course they are. There are about a hundred lines of code that boil down to `if (foo == special_nan_singleton) { /* ...handle nan / }` which of course...isn't how nan works. This is a no-op and the compiler just straight up compiles it out of binary. This happens a lot*. Including fundamental infrastructure, like the special JSON serializer somebody reinvented. reply rednafi 2 hours agoparentprevIDK I feel like Go suffers from this a lot. I have seen a lot of Gava, Guby, and G# over the last few years. It happens in Python a lot as well. Some people just love to write Java in Python and the new type hints make it even easier. reply neonsunset 2 hours agoparentprevI can imagine these developers being frustrated with highly verbose and often unsound semantics of Go. reply parasti 11 hours agoprevI have made conscious effort in the past to never copy/paste the initial fleshing-out of a Makefile or a PHP class, or HTML boilerplate, or whatever. Like, for years I stuck to that. Then I stopped making that effort because there is no upside. Or rather, there is no downside to copy+paste+modify. It's faster and you save your brain power for things that actually matter. reply tpoacher 8 hours agoparentThere's a subtle difference between a nice template and a fully-working implementation that you then modify though. (e.g. in that they were designed with different goals in mind, so the former is likely to have stopped at the point where it was general enough, to save you time, but not too specific to create footguns). Bonus points if your template explicitly has fail patterns that prevent your code from silently failing. reply dgan 10 hours agoprevOkey but to me, copying - pasting working code (even with sone extra unused bits) really looks no more different than inheriting a library - provided base class, and then extending it to one's needs. That's literally the basis of all software. There is no need to invent \"a Makefile effect/syndrome\" Yes that's an indication that a code sharing mechanism is needed but not implemented. Copying pasting solves that. You don't expect people to rewrite http client for every project which interacts with APIs, so you? reply worldsayshi 10 hours agoparentI think this is a good point. As somewhat of a tangent I have vaguely been thinking of the difference between copy pasting and explicitly extending for a bit. It seems that in many cases, adapting copy pasted code has some benefits over importing and adjusting some library code. https://ui.shadcn.com/ is an example of going the copy paste direction. It seems to me this is preferable when tweaking the exact behaviour is more important than keeping up to date with upstream or adhering to an exact standard. If you customize the behaviour a lot the extra abstraction layer only gets in the way. This insight might be a bit mundane. But I remember myself bending over backwards a bit too much trying to reuse when copy pasting is fine. reply heisenbit 8 hours agoparentprevWell, I expect people to understand http clients and if things don't work to be sufficiently knowledgeable to recognize when they have a performance problem and figure out why they have it. For that one needs language, library and networking skills which to a degree most developers have because they do it every day. At issue however are niche skills. We are dealing with the long tail of a distribution and heuristics which work most of the time might not - the author mentions e.g. security. The way I look at this is risk i.e. security, bus factor, disruptions due to software moving from state \"works and is not understood\" to \"broken and is not understood\" and last but not least ability to predict behavior of this niche technology when it is going to be pushed into an larger project. reply ruuda 12 hours agoprevI think LaTeX is the poster child of this. Nobody writes a LaTeX preamble from scratch, you always copy your previous document and tweak it. reply sandbach 10 hours agoparentDon't do that! If you're always using the same preamble, you should turn it into a .sty file. Then the preamble of new documents is just \\usepackage{myessay} reply aragilar 6 hours agorootparentI did that, then I needed to tweak things so I added options, then I needed to use the package somewhere that needed to be self-contained, so I started copy-pasting ;). I've done similar things with makefiles, tox configs, linter settings (all of which started from an initial version I wrote from scratch). I suspect the real reason this effect exists is because there's copy-pasting is the best way to solve the problem, due to a varying mix of: there being no way of managing the dependencies, needing to avoid (unmanaged) dependencies (i.e. vendoring is the same, only we have a tool managing it), the file (or its contents) needing to exist there specifically (e.g. the various CI locations) and no real agreement on what template/templating tool to use (and a template is just as likely to include useless junk). Copy-pasting is viewed as a one-time cost, and the thing copy-pasted isn't expected to change all that much. reply ttyprintk 1 hour agorootparentDefinitely true for linting. Nothing unlike a linter solves the problems we want the linter to solve. reply __mharrison__ 11 hours agoparentprevI use Typst now instead and wrote the \"preamble\" from scratch. (Because Typst is that much less annoying than LaTeX) reply BlueTemplar 6 hours agoparentprevI guess that there's a very important difference between copying something that you understand (or at least the details of which, like syntax, you can easily remember - here comments become important), and copying something that not only you do not understand, but you were not the one that made it in the first place, and you never understood it ! reply janalsncm 8 hours agoprev> Does it need syntax of its own? As a corollary: can it reuse familiar syntax or idioms from other tools/CLIs? I’m with the author here 100%. Stop inventing new syntaxes and formats for things that don’t need it. It’s not clever, it’s a PITA when it doesn’t work as expected at 3:30 on a Friday. reply mongol 11 hours agoprevIs not this a very generic phenomenon? I would argue it applies broadly. For example budgeting, you usually start from last year's budget and tweak that, rather than start from scratch. Or when you write an application letter, or a ServiceNow ticket, or whatever. Now I regret that I have brought in ServiceNow in the discussion, it kills the good mood.... reply Over2Chars 10 hours agoparentThere's also \"zero based budgeting\" (ZBB) that starts from zero and says \"justify everything\". reply KineticLensman 10 hours agorootparentWhich in my experience sometimes involves copying last years justifications reply Over2Chars 10 hours agorootparentIt might! But as I understand it and I am not an accountant (IANAA?), for non-ZBB budgets last years budget is usually used as a starting point and increases are justified. \"Here's why I need more money to do the same things as last year, plus more money if you want me to do anything extra\". I'd be curious what our man Le Cost Cutter Elon Musk does for budgeting? reply szarnyasg 12 hours agoprevI have observed the Makefile effect many times for LaTeX documents. Most researchers I worked with had a LaTeX file full of macros that they have been carrying from project to project for years. These were often inherited from more senior researchers, and were hammered into heavily-modified forks of article templates used in their field or thesis templates used at their institution. reply wjholden 11 hours agoparentThis is a great example of an instance of this \"Makefile effect\" with a possible solution: use Markdown and Pandoc where possible. This won't work in every situation, but sometimes one can compose a basic Beamer presentation or LaTeX paper quickly using largely simple TeX and the same Markdown syntax you already know from GitHub and Reddit. reply kergonath 11 hours agorootparent> use Markdown and Pandoc where possible. That won’t solve any problem that LaTeX macros solve. Boilerplate in LaTeX has 2 purposes. The first is to factor frequently-used complex notations. To do this in Markdown you’d need to bolt on a macro preprocessor on top of Markdown. The second one is to fine-tune typography and layout details (tables are a big offender). This is something that simply cannot be done in Markdown. A table is a table and if you don’t like the style (which is most of the time inadequate) then there is no solution. reply gardenerik 10 hours agorootparentprevA much better solution would be to use Typst, but that still might not work in all situations. reply elcapitan 8 hours agoprevI guess this is an effect of declarative programming and layered abstractions. The declarative syntax and abstraction are an answer to code being repetitive and long and hard to follow, but this then creates its own issues by making it harder to reason (especially for beginners or occasional users) about what is actually going on. The price for learning how to get it right just becomes much higher with every layer of abstraction inbetween, because you always have to learn what's going on underneath the \"cushions\" anyway. For me typical examples are Terraform configurations with their abstracted configuration syntax, which just mimicks some other configuration (e.g. AWS) and executes it in an environment where I don't necessarily have access to. Of course I'm not going to run endless experiments by reading documentation, assembling my own config and running it in painful slow CI pipelines until it works. I'll rather copy it from another project where it works and then go back to work on things that are actually relevant and specific for the business. reply PaulKeeble 6 hours agoprevI end up doing the copy paste thing quite a lot with build tools, it was very common in Ant, Maven and then in Scala build tool. When your projects all have the same fundamental top level layout and you are doing the same actions over and over you solve the problem once then you copy and paste it and remove the bits that don't apply. These types of tools there isn't much you do differently they don't give you much in the way of abstractions its just a list of actions which are very similar between projects. Since you typically with them are working in declarations rather than the usual programming primitives it often fundamentally falls down to \"does my project need this build feature or not?\". reply nayuki 11 hours agoprevI see this effect in Java Maven pom.xml files. It's hard to get a straightforward answer on why each build step is needed, what each attribute means, what parts are optional or mandatory, etc. There seems to be a culture of copying these XML files and tweaking a few things without truly understanding what the whole file means. I briefly looked at Ant and Gradle, and their ecosystems don't look any better. The build configuration files seem to have too much unexplainable magic in them. reply imoverclocked 10 hours agoparent> I briefly looked at …Gradle… The build configuration files seem to have too much unexplainable magic in them. This is largely due to the use of groovy. When the Kotlin DSL is used instead, it can usually be introspected by (eg) IntelliJ. Otherwise, it’s pretty opaque. reply brabel 10 hours agorootparentBullshit. Groovy can be introspected just as well as Kotlin. And the magic in kts files is still there: configure { named(\"main\") { java.srcDir(\"src/core/java\") } } Unless you know this, there's zero way you will come up with this by typing `configure` and using just auto-completion. Might as well use Groovy and a String for the name of the thing you're configuring. Good tooling would be able to auto-complete from there whether it's Groovy or Kotlin (or Java etc). reply imoverclocked 8 hours agorootparentThat wasn’t my experience a few years ago with a large groovy-dsl project. Since groovy will take a look in several different namespaces to automatically resolve things in a script, editors I tried had no hope of telling me what anything was. Also, groovy allows modification of private instance variables which leads to … un-fun situations. I converted tens of thousands of lines of groovy to Kotlin. A lot of those lines were automated. Too many were not automatable for myriad reasons. As far as the magic in Kotlin, I can easily click through all keywords and jump to the implementation in IJ. Groovy (at the time and in the project I was in) was utterly hopeless in this regard. reply brabel 8 hours agorootparentGroovy closure delegates' type can be declared, giving as much information as with Kotlin. The reason you couldn't follow the code was that the people who wrote those things either didn't declare types, or IntelliJ wasn't using the type declarations (I believe Groovy support in Gradle files is less good than in general Groovy files, where the IDE does support this). You're correct that some plugins will resolve things dynamically and those cannot be resolved by the IDE. But that's not the fault of the language, if you're going to rewrite in Kotlin with types, you could just as well add types to your Groovy declarations for the same result. reply fiddlerwoaroof 11 hours agoparentprevImo, the only solution is to avoid boilerplate generators and the parent poms projects like spring boot use for things like pom files: you can look at the boilerplate to get ideas for what might be necessary, but, if you’re starting a project, write the pom yourself. It’s a pain the first couple times, but it gets easier to know what you need. reply mongol 10 hours agoparentprevJava would really benefit from a fresh take on the build story. Maven is definitely a tool that suffers from this phenomenon. reply rockwotj 11 hours agoparentprevHonestly for Java I really like Bazel. You should give it a shot. I have a project with a self contained jvm and jars from maven central. Its more explicit than the other options but way less magical IMO. reply dave333 1 hour agoprevThe traditional Unix man page or list of options output with --help is often a firehose of details that most devs will never use. Sometimes there are a few examples shown of common use cases which is a good place to start learning the tool. reply sgarland 1 hour agoparentSure, but IME even when the tool in question is incredibly well-documented (like Django, or some other popular library), and has plenty of examples, most still don’t read the docs. I don’t know how to deal with that mentality. I don’t mind showing someone how I came to an answer, but I also expect them to remember that for the next time, and do some searching of their own. reply shusaku 8 hours agoprevI always write my makefiles from scratch. At some point in the process, I will google “make automatic variables”, because they’re a pain to memorize. reply kragen 8 hours agoparentYeah, I've always been mystified by the idea that writing a new Makefile is some kind of wizardly mystery. Make has its design flaws, for sure, but how hard is it really to write this? CFLAGS = -std=gnu99 -Wall all: foo clean: $(RM) foo *.o foo: foo_main.o foolib.o $(CC) $(CFLAGS) $(LDFLAGS) $^ -o $@ (Except with tabs, which HN doesn't allow.) I haven't tested what I just typed above, but I'm reasonably sure that if I biffed it in a way that makes it nonfunctional, it will be obvious how to correct the problem. I mean, not that you can't do better than that (I'm pretty sure anyone experienced can see some problems!), or that there aren't tricky and annoying tradeoffs, but it just doesn't seem like a big activation barrier the way people sometimes make it out to be? Maybe those people just need to spend an afternoon once in their life working through a basic make tutorial? Maybe not the first time they work on a project using make, but, maybe, after the fifth or sixth project when they realize that this somewhat primitive inference engine is going to be something they interact with daily for years? At some point you're getting into \"lead a horse to water\" or \"teach a man to fish\" territory. There's a limit to how much you can empower someone who's sabotaging themself. There's a slightly less minimal example in https://www.gnu.org/software/make/manual/html_node/Simple-Ma... with a full explanation. You can read it in a few minutes, but of course you have to experiment to actually learn it. The whole GNU Make 4.4.1 manual in PDF form is only 229 pages, so you can read it after dinner one night, or on your commute on the train over the course of a few days. And then you'll know the complete rules of the game. reply pantalaimon 8 hours agorootparentAuto-generated Makefiles that CMake and Autotools produce really leave a bad impression on how complex Makefiles need to be. reply mdaniel 48 minutes agorootparentGood news, you can change the output then! And for as much as you might not like its generated Makefiles, I assert that $(cmake -G Ninja) is 100,000,000x more \"wtf\" than -G Makefiles I disagree about the autotools ones, I find them very sane although autotools itself can die in a rotting dumpster out back. And take m4 with it. reply kragen 5 minutes agorootparentYeah, m4 is powerful in its way, and has an extraordinary strength-to-weight ratio (check out the implementation in Software Tools in Pascal) but ultimately I think it turned out to be a mistake. Make, by contrast, turned out to be a good idea despite its flaws. reply o11c 2 hours agorootparentprevSpecifically, any generated makefile that refuses to take advantage of GNU make is necessarily going to be horrible. BSD make is ... viable I guess, but only really worth it if you're already in the ecosystem - and even then I can't guarantee you won't hit one of its silly limitations. reply kragen 7 hours agorootparentprevYeah, I guess trying to read the output of gcc -O -S would make assembly language seem pretty overwhelming, too. reply pantalaimon 8 hours agoparentprevit always feels like those are from an era where teletypes would print out the computer output on paper and brevity was key to save ink and paper. reply bluocms 11 hours agoprevThis only happens because people treat build code at a lower standard than app code. IMO you should treat all code with the same rigour. From build scripts to app code to test code. Why write hacks in build tools when you wouldn’t do in your app code. We build tool code with the same quality as the app code. That’s why most tooling we use are written in typescript: type safety, code reuse… reply ido 11 hours agoparentI would argue the main reason is that Make is just bad. There are easier to use alternatives such as scons or rake that don't have this effect applied to them. reply teo_zero 11 hours agoprev> the tool (or system) is too complicated (or annoying) to use from scratch. Or boring: some systems require boilerplate with no added value. It's normal to copy & paste from previous works. Makefiles are a good example. Every makefile author must write their own functionally identical \"clean\" target. Shouldn't there be an implicit default? C is not immune, either. How many bits of interesting information do you spot in the following excerpt? #includeint main(int argc, char **argv) { printf(\"Hello\"); return 0; } The printf alone is the real payload, the rest conveys no information. (Suggestion for compiler authors: since the programs that include stdio.h outnumber those that don't, wouldn't it be saner for a compiler to automatically do it for us, and accept a flag to not do it in those rare cases where we want to deviate?) reply lucianbr 11 hours agoparent> since the programs that include stdio.h outnumber those that don't I don't think that is true. There is a lot of embedded systems C out there, plus there are a lot of files in most projects, and include is per file not per project. The project might use stdio in a few files, and not use it in many others. reply chikere232 9 hours agoparentprev> Makefiles are a good example. Every makefile author must write their own functionally identical \"clean\" target. Shouldn't there be an implicit default? At some point you have to give the system something to go on, and the part where it starts deleting files seems like a good one where not to guess. It's plenty implicit in other places. You can for example, without a Makefile even, just do `make foo` and it will do its best to figure out how to do that. If there's a foo.c you'll get a `foo` executable from that with the default settings. reply skirge 11 hours agoparentprevmore implicit behaviors more surprises, like security bugs because default functionality or conversions happen reply brabel 10 hours agoparentprev> The printf alone is the real payload, the rest conveys no information. What are you talking about? Every line is important. #includeThis means you need IO in your program. C is a general purpose language , it shouldn't include that unless asked for. You could claim it should include stuff by default, but that would go completely against what C stands for. Code shouldn't have to depend on knowing which flags you need to use to compile successfully (at least not in general like this). int main(int argc, char** argv) Every program requires a main function. Scripting languages pretend they don't, but they just wrap all top-level code in one. Having that be explicit, again, is important for a low level language like C. By the way, the C standard lets you declare it in a simplified manner: int main(void) Let's ignore the braces as you could just place them on the same line. printf(\"Hello\"); You could just use `puts` here, but apart from that, yeah that's the main payload, cool. return 0; The C standard actually makes this line optional. Funny but I guess it addresses your complaint that \"common stuff\" perhaps should not be spelled out all the time? So, here is the actual minimalist Hello world: #includeint main(void) { puts(\"Hello world\"); } reply teo_zero 1 hour agorootparentThank you, but this thread was not about writing good code, but rather how often one ends up acritically copying existing \"legacy\" parts without even attempting to understand it. I probably used the wrong words: \"conveys no information\" was meant as \"is less meaningful than the printf\". Just like switching on the PC every morning is essential, but if you ask me what my job's about, I wouldn't mention it. In the same vein, I'm convinced that the printf is the part that expresses the goal of the program. The rest, the #include, the main(), even with the optimizations that you suggested, is just boilerplate, the part that is usually copied and pasted, not because it's not useful and not because it's too difficult to get right, as the original article claims, but because it's boring. reply pantalaimon 8 hours agoparentprevMy main frustration with make has usually been with too much (surprising) implicit default behavior, not the lack thereof. reply oguz-ismail 11 hours agoparentprev> wouldn't it be saner for a compiler to automatically do it for us no reply dirkt 11 hours agoprevSame with programming: You just copy some old code and modify it, if you have something lying around. Same with frameworks (Angular, Spring Boot, ...). The tools even come with templates to generate new boilerplate for people who don't have existing ones somewhere. reply choeger 8 hours agoprevMakefiles have an even more interesting issue: They lost their main purpose. In many, many projects that I've seen, they only consist of phony targets. No dependency tracking is used whatsoever. How many Makefiles are there that just Wrap npm, pip, or some other tool like that? A Makefile is supposed to be the build system, not trigger it. reply klysm 8 hours agoparentOkay but make is a shitty build system. What it does have going for it is you can nearly universally expect it to be already installed or easy to install. That makes it a good way to name commands shorter in a portable way, with some dependencies maybe thrown in. It’s used for the same reason we write shell scripts reply mdaniel 41 minutes agorootparent> It’s used for the same reason we write shell scripts Only worse since it also uses $ for its variables leading to \"thing:\\t env FRED=$$HOME/thing some-command -p $$AWS_PROFILE $(OTHER_THING) -d $$(date +%s)\" level of squinting So for those using it as a task runner from the 60s, without dependency tracking, now it's just laziness over a shell script that has \"dependencies\" in an imperative and shellcheck-able way reply torstenvl 10 hours agoprevCopy+tweak happens IRL all the time. There's no reason everyone who bakes should have to reinvent biscuits from scratch. There's no reason chip manufacturers should have to reinvent N-type P-type sandwiches from scratch. The existence of adaptations of previous success does not suggest that baking, or physics, or Make, is overly complicated. reply Taniwha 10 hours agoprevI think this is completely normal for tools that you program seldomly. I write makefiles a couple of times a year, I've been using make for more than 40 years now, I use it every day, but I seldomly program it, and when I want something more than simple dependancies I often clone something that already works. reply camgunz 7 hours agoprevThis is pretty thought provoking. I think the issue is \"80% of the use of this complicated tool is for very simple ends\". From there you get a lot of \"I can't be bothered to learn git/make/sed/helm/jenkins, all I'm doing is X 15 minutes a year\". My guess is SWEs hate ceilings, so we don't want to use tools that have them, even though they'd be far more fit for purpose. We also don't want to build tools with ceilings: why limit your potential userbase/market? reply Havoc 8 hours agoprevTo me it seems fine that a tool that is both complexity and versatile needs a config file that is beyond memorization. So I think this line of reasoning has limitations. I could see it with say CLI tools though. Like if I need to reference my notes for a CLI command then that may well indicate a failure in tool design. >repeatedly copy a known-good solution and accrete changes over time. Alternative phrasing would be that it evolves. Arguably there is a positive trajectory there reply kazinator 13 hours agoprevAt first I couldn't understand what this article is saying. Then, .SECONDEXPANSION: kicked in! reply matsemann 1 hour agoprevWhy do some tools have this problem, and others not? I think it's convention over configuration. Makefile can do anything, so every project is different and needs different configurations, and everything must be configured. Which means that when I use a tool like that, it's sooo many decisions to make, that I just copy something that I know works. If instead it was some sane defaults, it would be pretty apparent where it deviates. And instead of thinking of hundred things and which to choose, I either don't think about them, or think \"do I have a reason to configure this instead of using defaults?\" reply tpoacher 8 hours agoprev> However, at the point of design, this suggests a tool design (or tool application) that is flawed: the tool (or system) is too complicated (or annoying) to use from scratch. As someone who teaches and sees college-level students ask chatgpt what's 1 + 1, I disagree that it has anything to do with complexity or annoyance. Humans be humans; that's mostly it. reply knallfrosch 43 minutes agoprevYeah I'm just not wasting my life (or professional time) learning Groovy, Maven, DotNET project files, DotNET 4.8, Gradle, Azure DevOps, Grafana, Prometheus, Docker, Docker compose, Kubernetes, Jenkins etc et all. I need those things once at project setup. I copy-paste and change a bit. Why copy-paste? It's a proven strategy with a high success rate despite little effort behind it. I also don't want to learn templating for every little DSL I need to write one file per project with. But if you love doing it \"the right way\", you're welcome to do that work for me. reply m463 7 hours agoprevThere's another similar concept that sort of goes along with this. Sometimes it's better to duplicate code rather than make a special routine to do it. Sometimes it's not only easier to copy/paste, but it is better than adding a level of abstraction reply rpcope1 12 hours agoprevI feel this way every time with webpack, npm and babel. reply baobun 12 hours agoprevGood points in general. On the other hand, there are cases where (beneficial/desired) verbosity prompts copy-paste and tweaking - not due to complexity but from some form of scale or size of the input. In many cases this is a sign of something that should be dynamic data (put it in a db instead of conf) but that's not always the case and worth the tradeoff in the moment. reply leoc 12 hours agoprevOld IBM mainframe scripting in JCL https://en.wikipedia.org/wiki/Job_Control_Language (so \"OS JCL\" now, I suppose) used to have a terrible reputation for this, but I've never actually touched the stuff myself. reply teddyh 9 hours agoprev“Just modify someone else’s Borgmon code and you’re good to go.” — Broccoli Manreply g7r 10 hours agoprevWe use Cargo Cult to refer to this phenomenon: https://en.wikipedia.org/wiki/Cargo_cult_programming reply gavindean90 3 hours agoprevNo one tell them about Cobal reply agnishom 9 hours agoprevWhat is a counterexample? I cannot think of a single tool which is complex enough but does not show the makefile effect reply vandahm 8 hours agoprevHonestly, my .zshrc file started out as a .kshrc file that was passed down to me by an older developer about 20 years ago, when I was still in university. I've added and removed a lot of things over the years, but there are still a few parts of it that I don't totally understand, simply because they work and I've never had a reason to think about them. The guy I got it from, in turn, got his from someone else. In the old days, I had a .fvwm2rc config file that I got from my boss in the university computing center. I had no idea how it worked! And neither did he -- he got it from a professor when he was in university. reply lazystar 12 hours agoprevamazon's internal build tool experiences this same phenomena. engineers are hired based on their leetcode ability; which means the average engineer has gaps in their infrastructure and config tool knowledge/skillset. until the industrys hiring practices shift, this trend will continue. reply marxisttemp 10 hours agoparentAs an undergrad, I did group projects with people who quite literally could not compile and run any actual project on their system outside of a pre-packaged classwork assignment, who essentially could not code at all outside of data structure and algorithm problem sets, who got Google internships the next semester. But they were definitely brighter than I when it came to such problem sets. I suppose we need both sorts of engineer to make great things reply Papirola 11 hours agoprevI call it the yoghurt effect. reply fargle 2 hours agoparenti'll bite. care to expand? is this a riff on copy-pasta or spaghetti code or what? reply keepamovin 3 hours agoprevIn other news I like the coherence of your online digital identity and websites. Reminds me of the early internet. Auras of class, cred, erudition, intelligence, mystery, imagination. Thank you. reply fithisux 12 hours agoprevAlias, cmake effect. reply chikere232 9 hours agoparentLet's keep \"cmake effect\" for \"trying your best to not have to touch the language and repeatedly looking for something else whenever you do\" It's a very microsoft feeling pile of crap reply bitwize 10 hours agoprevA better name for this might be the JCL effect, as even experienced mainframe sysprogs copypasta the JCL it takes to build their COBOL programs from a known-good example and then mutatis the mutandis, rather than attempt to build a mental model of how JCL works from the impenetrable documentation and write JCL de novo. It's no big deal to me to write a small Makefile from scratch. My editor (Emacs) even knows to always use tabs when I hit TAB in a Makefile, removing the confusion of whether I inserted tabs (correct) or spaces (horribly incorrect) on the lines with the commands to build a particular target. reply travisgriggs 12 hours agoprevIt’s like a variant of Pournelles Law. reply fargle 2 hours agoparenti think you got the wrong eponymous law, pournelle's iron law of bureaucracy (which i see happening all the time, btw.) has nothing to do with this issue. reply Over2Chars 12 hours agoprevA clever point, worth discussing reply smitty1e 11 hours agoprevWhat is the counter-example? We recycle known good stuff to avoid getting bogged down and introducing fresh flaws. The admonition to know what we're doing and act deliberately applies to so much in life, but flies in the face of Milton Friedman's point in \"I, Pencil\" => https://youtu.be/67tHtpac5ws?si=yhheE1Y5ELfjWXs- reply malkia 11 hours agoprevor ahem... CMake reply dvh 9 hours agoprevMost of my simple C projects have make.sh instead that has something like: clear gcc some options -o foo && ./foo reply chikere232 9 hours agoparentYou might benefit from make, as you wouldn't need a full rebuild every time, or have to spell out every step. reply bigpingo 2 hours agorootparentFor C and C++* projects under ~100k lines I wouldnt bother with incremental builds - I have a 70k C project with a single translation unit that builds in under 1s on my machine. * C++ requires some discipline to not explode build times, but it can be done if you dont go nuts with templates and standard headers. reply __mharrison__ 11 hours agoprevSetup.py, cron, makefile, bash scripts, GitHub actions, and devcontainers all had this effect... Until AI came around. Now AI does a great job of getting you 90-100% of the way there. reply davidguetta 5 hours agoprev [–] Is it not a problem which is basically COMPLETELY SOLVED by LLMs ? The reason this happens is because Makefiles (or CI/CD pipelines / linters config, bash scripts) are more or less \"complete language\" on their own, that are not worth learning when you can do ... exactly what the author says (copy/pasting/modifying until it works) 99% of the time. But LLMs in general know the language so if you ask \"write a minimal Makefile that does this\" or even \"please simplify the Makefile that i copy/pasted/modified\", my experience is that they do that very well actually. reply uludag 2 hours agoparent [–] Completely solved? I'd say exacerbated beyond recognition. We have tools to let us get by so much farther without understanding anything, so it probably becomes less of a problem in more cases. But it basically guarantees that all but the most curious will not understand how the system actually works. Everything becomes magical copy/pasting from the most advanced information retrieval system with LLMs. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The \"Makefile effect\" describes the practice of engineers copying and modifying existing complex configurations, like Makefiles, instead of creating new ones from scratch. - This approach is prevalent in various systems, such as Continuous Integration/Continuous Deployment (CI/CD) configurations and build systems, and can indicate overly complex or poorly designed tools. - While not inherently negative, the Makefile effect can impede learning, complicate security, and lead to inefficient debugging, highlighting the need for thoughtful tool design that considers configurability and ease of use."
    ],
    "commentSummary": [
      "The \"Makefile Effect\" describes developers' tendency to copy and modify code or configurations without fully understanding them, often due to tool complexity or infrequent use.",
      "This phenomenon is widespread across programming and configuration tasks, where developers use known solutions to save time and effort, potentially leading to difficult-to-maintain code.",
      "The discussion emphasizes the need to balance using templates and existing solutions with gaining a deeper understanding of the tools and systems involved."
    ],
    "points": 238,
    "commentCount": 203,
    "retryCount": 0,
    "time": 1736568792
  },
  {
    "id": 42661185,
    "title": "Portals and Quake",
    "originLink": "https://30fps.net/pages/pvs-portals-and-quake/",
    "originBody": "30fps.net - Computer Graphics & Programming with Pekka Väänänen Portals and Quake January 10th, 2025 This is the first installment in the “Demystifying the PVS” series. Portals and Quake Coarse base visibility Fine visibility via clipping Portal flow brings it all together (to be published) Precomputed visibility in Quake’s first level. The camera location is shown in red. Ever wanted to know how exactly did Quake’s precomputed visibility work? I did, so I wrote vis.py, a reimplementation of their algorithm in Python. This guide has all the information you need to understand vis, the tool used by Quake, Half-Life and Source Engine games. During the development of Quake, overdraw became a concern. It means the same pixel getting written many times during the rendering of a frame. Only the last color stays visible and the earlier writes go to waste. This is bad if your game is software rendered and already pushing the mid 90’s PCs to their limits. How to reduce overdraw? Let’s begin with a very high-level overview of the solution landscape. Portal culling helps with overdraw In 3D games, it’s a good idea to reduce the number of drawn objects. Frustum culling is one fundamental method for this, in which objects confirmed to be outside the virtual camera’s view are skipped during rendering. This can be done for example with object bounding boxes or bounding spheres. Frustum culling still leaves some performance on the table. Many objects may still be within the field of view of the camera even if they don’t contribute any pixels to the final image. This is not a performance catastrophe if everything is rendered from front to back. GPU’s early-z testing will help here. Still, in large worlds it would be faster to never submit these objects for rendering in the first place. Occlusion culling is a process where you discard objects that you deem to lie behind other objects in the scene. Its purpose is to discard as many occluded objects as possible. It’s not strictly needed, since you’ll get the correct image thanks to the z-buffer anyway. There are a few ways to do this such as the hierarchical z-buffer, occlusion queries, portal culling, and potentially visible sets (PVS). In this article I talk about the last two: portals and the PVS. In portal culling, the world is divided into spaces where the virtual camera can move around and the openings between them. The spaces are called cells, viewcells, zones, clusters or sectors, and the openings portals. This is a useful split especially in architectural models with cleanly separated rooms connected by doorways or windows. It also works for mostly-indoor video game levels :) The floorplan of our example level with three hand-placed portals shown. Cells have the color of their entry portal. In this case also the cell where the camera lies is visible. Portal rendering starts from the camera’s cell. The game renders everything inside that cell, and then recursively looks into portals leading away from that first cell to find out what else to draw. It renders all objects in every cell and then examines the cell’s portals. If a portal doesn’t line up with another one on screen, it won’t be visited. Each successive portal shrinks the visible screen area smaller and smaller until the whole portal is clipped away. A straightforward way to test portals for visibility is to intersect their screenspace bounding boxes. Those are shown in white in the picture below. If two bounding boxes overlap, we can see through the respective portals. More accurate tests can be performed with 3D clipping or per-pixel operations. This is how three portals could look in game. Portal openings are shown as colored polygons and their screenspace bounding boxes are in white. Objects have dashed bounding boxes. The star object is culled because it doesn’t overlap with the red portal. The Quake engine uses portals but only during map preparation time. At runtime, the portals are nowhere to be seen. This technique is a variant of Seth Teller’s PVS method presented in his 1992 dissertation that only worked with axis-aligned walls. Portals of a Quake map disappear Often portals are placed by hand by a level designer. Quake’s bsp map compilation tool places portals automatically, which is nice, but unfortunately it creates a lot of them! Quake’s first map viewed in the TrenchBroom map editor with portals shown in red. As you can see, not just doorways act as portals. You see, in Quake the cells are very small. But no portals are tested at runtime. Instead, each cell gets a precomputed list of other cells that can been seen from it. This is the Potentially Visible Set (PVS) for that cell. In Quake, a cell is a small convex volume of space, so a single room will usually get split into multiple cells. These cells correspond to leaves of a binary space partitioning (BSP) tree. The BSP tree was used to divide the map into cells and portals. For us, the exact method is irrelevant though. But BSP does make it easy to find the cell the camera is in at runtime. Since we have now entered the Quake territory in our discussion, I’ll start calling a cell a leaf. Leaf is the term used in all source code, level editors, error messages, and other resources on Quake. The meaning stays exactly the same though, it’s just a convex cell connected to other cells via portals. This is how leaves look in our example level: The example map divided to convex leaves. Leaf colors are random. The portals appear in between leaves, as expected: Portals placed automatically by the bsp tool. This map is pretty much 2D but everything discussed works just fine in 3D too. Leaf indices are shown in white. Nothing would’ve stopped them from grouping multiple leaves to form larger cells with fewer portals in between. In fact, this is exactly what they did for Quake 2 with its “clusters” of leaves. With larger clusters of leaves, you do get more overdraw. Also, a cluster made of convex leaves may not be convex itself any more. But even in that case you can still act as if it still is, and assume the portals inside can be seen from anywhere in the cluster. It’s less accurate but works. High-level overview of vis The Quake map tool vis takes in portals generated by another tool, bsp, precomputes a leaf-to-leaf visibility matrix, and writes the matrix back to the compiled map file. This article series describes how vis functions. We know that leaves can see each other only through portals. So we don’t even need to know how exactly the leaves look like, only how they are connected together. At its most basic level, vis does two recursive depth-first traversals, followed by a quick resolve pass before writing the visibility results back to a compiled map file. Three steps: Base visibility. Estimate a coarse leaf-to-portal visibility. Full visibility. Refine the coarse results via portal clipping. Resolve. Combine the refined portal-to-leaf results to the final leaf-to-leaf visibility. For a quick visual overview, I can recommend Matthew Earl’s great video on Quake’s PVS. Portals have a direction In a portal system, the cells and portals are structured as a cell-and-portal graph. Quake’s map tooling follows this pattern and connects leaves with portals, even though this structure isn’t present at runtime. Leafs are connected by portals: Leaves (nodes) connected by portals (edges) in a cell-and-portal graph. Each portal is a 3D polygon. They are written by bsp to a plain text file with a version code, the number of leaves and portals, followed by one portal per line. Like this: PRT1 11 12 4 0 1 (880 -224 -8 ) (880 -272 -8 ) (880 -272 72 ) (880 -224 72 ) 4 1 2 (832 -224 -8 ) (832 -272 -8 ) (832 -272 72 ) (832 -224 72 ) 4 2 4 (768 -272 -8 ) (768 -320 -8 ) (768 -320 72 ) (768 -272 72 ) 4 2 3 (768 -112 72 ) (768 -112 -8 ) (768 -160 -8 ) (768 -160 72 ) 4 3 5 (720 -112 72 ) (720 -112 -8 ) (720 -160 -8 ) (720 -160 72 ) 4 4 5 (720 -272 -8 ) (720 -320 -8 ) (720 -320 72 ) (720 -272 72 ) 4 5 6 (640 -224 -8 ) (640 -288 -8 ) (640 -288 72 ) (640 -224 72 ) 4 6 7 (592 -224 -8 ) (592 -288 -8 ) (592 -288 72 ) (592 -224 72 ) 4 7 10 (384 -304 -8 ) (384 -368 -8 ) (384 -368 72 ) (384 -304 72 ) 4 7 8 (384 -112 -8 ) (384 -176 -8 ) (384 -176 72 ) (384 -112 72 ) 4 8 9 (240 -176 -8 ) (336 -176 -8 ) (336 -176 72 ) (240 -176 72 ) 4 9 10 (240 -304 -8 ) (336 -304 -8 ) (336 -304 72 ) (240 -304 72 ) Each portal is a loop of 3D points: ┌ the number of points │ ▽ x y z x y z x y z x y z 4 0 1 (880 -224 -8 ) (880 -272 -8 ) (880 -272 72 ) (880 -224 72 ) △ △ └─┴─ the two leaves the portal is in between Since portals are interfaces between convex leaves, the polygons are also convex. In 3D, a portal looks like this: Each portal is stored as a convex polygon. Conceptually, each portal is a two way opening. You can see through it in both directions. However, it’s convenient to make the portals directed. This way we can keep track on what’s visible in different directions. We give each portal a normal vector, the direction the portal can be seen through. Now a single input portal becomes two directed portals: Each input portal is split into a so called forward (red) and backward (yellow) portal before processing. There’s a small gap here for demonstration purposes but actually they overlap. The arrows show the directions the portals can be seen through. Therefore the graph will now have directed edges instead: Each portal is represented by two edges in the graph. The earlier forward and backwards portal edges are highlighted with red and gold, respectively. The graph in code Now is the time to present the main data structures of vis.py, the Portal and Leaf classes: class Portal: winding: list[np.ndarray] # polygon's 3D points leaf: int # the leaf this portal leads to plane: Plane # plane normal points to destination leaf ... # (other class attributes omitted) class Leaf: portals: list[int] # indices of portals leading away from this leaf Note that a leaf stores only indices of portals leading away from that leaf. The graph is stored in two global arrays called portals and leaves with objects of the respective types. Since the graph is accessed both via indices and direct object references, I came up with the following naming convention: pi is the index of a portal, Pi is the actual object Pi = portals[pi], and li is the index of a leaf, Li is the actual object Li = leaves[li]. Our goal is to compute which nodes can reach each other in this graph while honoring the 3D visibility relations between portals associated with each edge. But what on earth are those “visibility relations”? In the next part we’ll use the graph for some quick checks. I’m also thinking of writing a book. Sign up here if you’re interested. Mastodon Bluesky YouTube Feed 30fps.net",
    "commentLink": "https://news.ycombinator.com/item?id=42661185",
    "commentBody": "Portals and Quake (30fps.net)183 points by ibobev 20 hours agohidepastfavorite48 comments leoc 11 hours agoEdit: ugh, I missed dalant979's comment which already covers this https://news.ycombinator.com/item?id=42662830 reply hgs3 16 hours agoprevSome additional points of interest: Quake level designers could use \"hint\" brushes to help the BSP compiler determine where to create cells. Starting with Quake II, designers were able to place \"area portals\" which are portals programmers could toggle at runtime (think disabling a doorway portal when the door is closed). reply rollcat 5 hours agoparentI remember discovering all these optimizations The Hard Way when making maps for id tech 3 games as a kid. I always tried to build grand and detailed spaces, and the BSP/vis would choke on all the geometry. reply swayvil 2 hours agorootparentMe too. Giant open space. A baron, way off barely visible, throws a firebolt. Takes like 30sec to get to me. What doom mapmaking needs is higher level tools. Like a tunnel kit. Just plug together pipes and fittings. Or crank out mazes generatively. reply rollcat 1 hour agorootparent> What doom mapmaking needs is higher level tools. Like a tunnel kit. Just plug together pipes and fittings. Or crank out mazes generatively. You're describing the contemporary Unreal Engine, which was \"solid by default, carve out the space\" kind of a thing. GtkRadiant (the id tech 3/4 editor) had all the basic CSG operations built in, so you could theoretically carve out spaces, but Radiant's CSG was widely regarded as \"evil\" - I never found out why. reply paulryanrogers 18 hours agoprevInteresting. I recall Prey being the first game to hype portals for rendering, though I think theirs may have been for drawing not just culling? Does anyone know how it worked? Dark Forces apparently also used portals for culling, IIRC. reply phire 17 hours agoparentNo. Prey was one of the first games to hype portals and non-euclidean maps for gameplay. Portals for rendering date back to the dawn of 3d graphics and many early FPS engines used the concept. But these portals were only there as a rendering optimisation. While you could abuse them to make non-euclidean maps, the tools were intended to make proper maps, and the portals would be invisible to players. reply smolder 15 hours agorootparentMarathon 2 by Bungie also had weird non-euclidean maps with overlapping sections in the late 90s I think. I miss the customizability of that game. Early bungie works were up there with early blizzard or early valve. They sold a tool for fun instead of casting a bait for profit. reply Terr_ 14 hours agorootparentI recall that some of the developer commentary for either Portal or Portal 2 mentioned that they began level-development by using a special version of the namesake non-euclidean Portals to connect different chambers and hallways that were being developed in parallel. [Per distinct loading-screen level.] Then, later in development, they ran a tool to stitch together all that geometry in a more-normal way, and they only had one case of impossible-overlap, which continued to use the non-euclidean link. _________ Edit: Found it: > [...] When we started the project making any big structural change in a level or the order of levels would lead to hours or even days of busy work trying to reconnect things and make sure they lined up again. If we ever wanted to ship something the size of Portal with the finely tuned balance we desired then we needed a way to be able to make big changes to the layout of the game without paying the cost of making everything line up again. We needed a way to bend space. We needed to think with portals. Using portals to connect different areas in the world we could make any type of impossible space work out. You could look through a hallway into the next room but the hallway might be on the other side of the map and the room you are looking into might be in a completely different orientation. We could seamlessly insert an elevator, a huge expansive vista, a room that was bigger on the inside than the outside, or even create an infinite fall by connecting a shaft back into itself. Soon every connection between any space was a portal. We would even switch them on the fly. Even a simple door worked like the cartoons - just a facade painted on a wall that seamlessly opened somewhere else entirely. Once the game settled down we were able to finalize our path and remove all of the world portals. There's only one impossible space left in the whole game - see if you can figure out where it is. -- \"World Portals\". Portal 2 developer commentary reply brobdingnagians 6 hours agorootparentFor those wondering where the impossible space is: https://gaming.stackexchange.com/questions/20601/where-is-th... reply endgame 12 hours agorootparentprevIIRC Duke3D also had a map where you had to walk a circular path twice to get back to where you started. reply Synaesthesia 5 hours agorootparentDuke3D used teleportation to simulate height, for instance in the beginning of the game when you drop down from the rooftop you actually teleport to a other part of the map. reply chamomeal 14 hours agorootparentprevDo you know anything about Prey 2017’s “looking glass”? Do you know if it involves any special techniques? I know nothing about this stuff, but when I saw in-game I was really impressed. You can even break the screens into shards, and the little pieces still maintain the effect when they’re lying on the floor or flying through the air. reply adastra22 14 hours agorootparentI've never played the game, but your description doesn't sound very hard to implement. Keep in mind there is no physical camera when rendering, so tricks like this become rather easy to implement. You just setup the camera appropriately and render the perspective being shown by the portal, transformed into view space, using an alpha mask to make sure it only draws pixels where the glass fragment is. reply keyringlight 6 hours agorootparentIsn't the more common usage of this in skyboxes? You'd have one area where the gameplay is taking place, then another area in the map that shows the larger scale area surrounding where the player is and the portal makes it appear that the gameplay area is within the other. The designer needs to confine the player so they can't break the illusion (where Prey lets Morgan Yu shatter that 4th wall), or you can do things like put the player on a speeding train traveling through a scenery and falling off is fatal reply corysama 17 hours agoparentprev> Dark Forces apparently also used portals for culling, IIRC. DF used BSP for culling. But, I made a mod level long ago playing with features and made a stairway in the middle of a room that you could only see from in front of it. It was some kind of free-standing portal with no surrounding support. Fun fact: the creator of the DF engine told me he based it on a reverse engineering of a beta release of DOOM. Apparently, the final release of DOOM cut a lot of engine features to gain speed. But, DF shipped with them and maybe a few more. reply paulryanrogers 14 hours agorootparent> Fun fact: the creator of the DF engine told me he based it on a reverse engineering of a beta release of DOOM. Can you provide a source? As I understand it DF is based on the (pre release?) Tie Fighter engine, and DF began production before Doom released. That's why for example DF has some non-textured 3D models like the turrets and mouse droids. I follow The Force Engine (reverse engineered fan port) and Dev Game Club (hosts worked at LA). Supposedly DF devs did see Doom (perhaps even a pre release version), but already had their own tech working by that point. > Apparently, the final release of DOOM cut a lot of engine features to gain speed. Doubt. There's a lot of interviews with the ID guys, and I don't recall anyone saying they cut significant technical features. They made Doom in only 13 months, minus almost a month to port Wolf3D to SNES. Maybe Carmack -- or the released alphas and betas -- can clarify? Or perhaps you are confusing some of the Doom porting efforts, which did cut down levels and sometimes features significantly. reply adastra22 14 hours agoparentprevAs mentioned by others, Descent used portal rendering all the way back in 1995. It's a simple trick that only requires an alpha mask, which even the earliest hardware supported. But certainly the game Portal (2007) hyped using portals for rendering prior to Prey. reply usea 13 hours agorootparent> But certainly the game Portal (2007) hyped using portals for rendering prior to Prey. They were referring to Prey (2006). reply Tempat 11 hours agorootparentAlso worth noting that Prey (2006) started development in 1995. reply keyringlight 6 hours agorootparentThere are a handful of videos from E3 (97 or 98) showing off the original 3DRealms development of Prey. One of the things that didn't survive into Human Head's version on the Doom3 engine was rotating portals, or if they did they were rare that I don't remember them. reply bananaboy 11 hours agorootparentprevOriginal Descent was software rendered though. reply adastra22 9 hours agorootparent2D graphics cards had alpha masking. reply rep_lodsb 7 hours agorootparentBut not standard VGA. reply to11mtm 17 hours agoparentprevI think Build used some form of portals as well... primarily for water/underwater transitions, but there was at least one level in Duke3d that used it for some non-euclidian geometry. reply d3VwsX 17 hours agorootparentPretty sure water was just a teleporter? You may be thinking about the mirror effect? I think that was a kind of a portal effect. You had to make a large empty section behind each mirror that was used by the engine to render a mirrored copy of things in front of the mirror. You could make some weird impossible geometries by just superimposing sections. Two sections could occupy the exact same coordinates, but movement and rendering were done across edges shared by sections and did not care about if there was some other section in the same space. As long as there was never a way to see those sections at the same time. reply Sharlin 9 hours agorootparentA teleporter is just a portal that you can’t see through. > You could make some weird impossible geometries by just superimposing sections. Two sections could occupy the exact same coordinates, but movement and rendering were done across edges shared by sections and did not care about if there was some other section in the same space. As long as there was never a way to see those sections at the same time. This is exactly what portals are. reply CyberDildonics 17 hours agorootparentprevPretty sure water was just a teleporter? You may be thinking about the mirror effect? That's what they just said? They didn't mention the mirror effect? The mirror effect was done with duplicate geometry? It occupied the overlapping space of what was behind it? The build engine also didn't have vertical levels. reply paulryanrogers 14 hours agorootparentShadow Warrior (Build) did have some room over room stuff. But IIRC it still leveraged offset sectors like Duke's water. FWIW some modern Doom ports appear to have this as well, at least with levels that support it. reply smitelli 17 hours agorootparentprevE2L11 “Lunatic Fringe.” The map has an outer ring containing 720 degrees of hallway. Surprisingly disorienting even when you know that’s what’s going on. reply bluedino 15 hours agoparentprevDescent used portals reply jofla_net 14 hours agoprevI remember the analogue to this in unreal engine 1, zones i believe, same concept though, good years. reply markus_zhang 12 hours agoparentDoes anyone know why Unreal Engine 1 runs pretty smooth but with far superior graphics? Or maybe it is not that superior as I thought? I remember went into a netbar in 1998 when it came out and was COMPLETELY blown away -- and that PC didn't even have a good graphics card so it's choppy. reply dahart 3 hours agorootparentI’d be curious if you can find some examples of superior UE1 graphics compared to the best of what you can find in UE5. If your question is based on your memory from 98, I would assume the answer is that UE1 graphics is not superior to today’s, but things that look good today are less surprising than they were then. I just googled UE1 images, and everything I saw in several pages of results looks rather dated to my eyes. If you’re thinking more about the smoothness & framerate of the game, then that’s entirely subjective. UE1 did not prevent choppiness or make things fast, and neither does UE5. That’s entirely at the discretion of the game developer. If you use fewer polygons and use simple shaders, the game will be fast and smooth. If you use more than the hardware & engine can handle, then your game will be choppy. There are a lot of advancements in real-time lighting today, which means that the look doesn’t necessarily change, but is computed at run time. Most of the nice lighting in UE1 was baked into textures or vertex data, and could not change during the game. Today with shaders and global illumination algorithms running in real-time, the lighting can change in response to a moving sun, moving objects & characters, changing materials, etc. reply markus_zhang 2 hours agorootparentEr, I was thinking about UE1 versus Quake 2, definitely not UE5. reply chickenzzzzu 12 hours agorootparentprevI'm thinking it's likely the number of supported colors and integer vs floating point precision. You'll notice that quake textures are very similar to PS1 textures in that they are pixelated and use a limited number of colors, whereas N64 textures have more of a smooth gradient. Likely also there are differences in the lighting systems as well. This is why I think people compare Quake II or even Quake III Arena to UE1. The OG quake really was a hack just to be the first that did 6 degrees of freedom textured 3D graphics on a PC, which I think they were the first for those exact constraints. My history is a little fuzzy, SEGA certainly had them beat by multiple years on arcade boards but those were all custom, and other games that had 6 degrees of freedom were not textured. It was a busy time ! reply whaleofatw2022 6 hours agorootparentDescent had textures, although they were arguably not-even-ps1 quality and there were probably some things still flat shaded. reply Sharlin 9 hours agorootparentprevThe original Unreal had great graphics for the time but did require a very beefy machine. reply markus_zhang 5 hours agorootparentAh I wish they open sourced the UE1 engine. I know some of the OldUnreal community members get to see the source code and made patches for it as well as migrated the UT editor to it, which is way better than the original VB one. reply andrewmcwatters 12 hours agorootparentprevNewer temporal rendering techniques result in lower quality frames and output when the prevailing art style of the game is not one that is photorealistic. There's a lot of \"smearing\" as a result. It's not just you, it's how newer rendering techniques in the Unreal Engine end up being perceived by most people. reply dalant979 16 hours agoprevSome related content from Youtube channel \"Matt's Ramblings\": Quake's PVS: A hidden gem of rendering optimization: https://www.youtube.com/watch?v=IfCRHSIg6zo How Quake's software renderer ELIMINATES overdraw: https://www.youtube.com/watch?v=zdXsHWHxeBY I added portals into software Quake: https://www.youtube.com/watch?v=kF-7Jd37gYk reply leoc 9 hours agoparent(I missed your comment earlier, sorry!) His real name may be \"Matthew Earl\", it seems https://mastodon.cloud/@mattsramblings . I stuck his three Quake-rendering videos together in a playlist: https://www.youtube.com/playlist?list=PLVJ2vTa3CVnRRsJvvcdR3... . He also has a video on Quake 1 and 2's lighting https://www.youtube.com/watch?v=bxientPsqRg among his videos https://www.youtube.com/@MattsRamblings . reply dalant979 6 hours agorootparentthanks for the additional links, apparently i have more to watch! :) reply Jyaif 8 hours agoprevIs this technique still used in modern engines to determine which part of a level to render? reply dahart 3 hours agoparentSimilar techniques are used, UE5 has Precomputed Visibility Volumes and Cull Distance Volumes, among other render optimizations. https://dev.epicgames.com/documentation/en-us/unreal-engine/... https://dev.epicgames.com/documentation/en-us/unreal-engine/... reply fourseventy 16 hours agoprevThis makes me really want to play quake 3 with a portal gun reply egypturnash 13 hours agoparenthttps://www.moddb.com/mods/quake-portal-gun/ reply smolder 15 hours agoprev [–] Acknowledge splitgate 2 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article is the first in the \"Demystifying the PVS\" series, focusing on Quake's precomputed visibility system to address overdraw, where pixels are rendered multiple times. - Quake uses portal culling and potentially visible sets (PVS) by dividing the world into cells and portals, precomputing visibility to optimize rendering. - The article introduces the Portal and Leaf classes in vis.py, a Python reimplementation of Quake's visibility algorithm, highlighting the use of a leaf-to-leaf visibility matrix."
    ],
    "commentSummary": [
      "Quake level designers utilized \"hint\" brushes and \"area portals\" to optimize game performance, a technique that has been foundational in 3D graphics. - Games like Prey and Portal brought portals into the spotlight for gameplay mechanics, though the concept has been used in rendering since early 3D graphics. - Unreal Engine 1 employed zones for optimization, and contemporary engines like Unreal Engine 5 continue to use similar methods to enhance rendering efficiency."
    ],
    "points": 183,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1736549293
  },
  {
    "id": 42660335,
    "title": "Phi-4 Bug Fixes",
    "originLink": "https://unsloth.ai/blog/phi4",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"unsloth.ai\",cType: 'managed',cRay: '900728f72dc41177',cH: 'ERLi2ZcDleVBnHd2ITwKPw9gB6fvC.NYk1yhA9522So-1736622134-1.2.1.1-6tvMUkHOe45l0Jw.qGtiCMf7XSaEbpFCoppV0rl97OQKp.CeIAAG3RGGrvyTH2Ji',cUPMDTk: \"\\/blog\\/phi4?__cf_chl_tk=LSOWfNeJSpPUGrugP9vF8_yNeQAi7r4b_e0FI8IPkvA-1736622134-1.0.1.1-Cakqz5EajJCfrr7sGBRGsmpfZhH8CaPkPMq6bqsrTvg\",cFPWv: 'b',cITimeS: '1736622134',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/blog\\/phi4?__cf_chl_f_tk=LSOWfNeJSpPUGrugP9vF8_yNeQAi7r4b_e0FI8IPkvA-1736622134-1.0.1.1-Cakqz5EajJCfrr7sGBRGsmpfZhH8CaPkPMq6bqsrTvg\",md: \"HRjjo9oIIOse3mPmV.QxeASCCSSMBTbvFDXmqvf2IoI-1736622134-1.2.1.1-9Le3VARwbxBwL7AGJPqLxO8_Kej4YeTGGoRUlmRQ4nMVhDffN0habwdirft0rDIRneAmPDxCtUYzSg4T7qiezDNmoTOUbFGEpjXR79TWwgkt75yldlGZslc5N2GE0PIMccw58zZLNdtPWubxfGmreBk8XPId4AzFrN1cU5bymO.C_VwK_I3n9vKqecdWeCkHh4m2i_LNm70k0_p2xq2B3OH9mDAlzyxVGd_YN6PrUamcaE6IycPa3jysNa2E0ZXvq3iDdxpxtS2567_bXGhaAqM2s8xPHSwyGvH1CfecPZICnto_MdhvscINw_wLYvbC7.amTOOSN3mOCitB2DrIYYMoSvsNPzUCGvtj5W3WaB9SEelRJv2qPEMRZxu7wY_uuQ6AY4WHFLCmIWB7dVgb2OECwUvT5wmqrz_wOmZRYa4DNkMDko75ArCUOsjHN8lg.b1zAoll2nyCgF2TAWhA._mAIHFR0pdVdF08aiFFYgib2GtJ85tcfdYoVp9GqMMLFihJ_qWmP8aSrjPNGsKV58keK2EN43lu7P7_4aTINl8P3I1rGQba3uXgewtnGiMJaCqWTYLiMysv7rXN3BW26j_b4irOkKYBmBhj6HV_ZiaqtGpwalqBZMpA1x2BG2_eqhhLWg9BdXXebwIKqFb7rj3FAdErOF7mOkW1sMteU2yu76_25KceKGXFxNRGL2ZmEotGQjBfNUahNjezMtKEdxOmKX44uujaiQztMF8ZyyIqg5meg1JZVZmUPq.Nc7RCUPw7VFHW6ZtnNJSrzIu9AQ8A5Y3SFD5domo6c19Zzkh.4TAQ8TBBTA.szz5F_S4ugFxhfy1hKLvfUzkarMS6erPgKuF6lDTjud8aqgaLCNks8SNUSjPgmZRykiIpaUgKik4TiuSXpXJSQMm9bFtzyscnZtaoz92xG7RM5M_Rq_79VYgbMaPkDQ9hB2IkAKbCtNuWYKncaViUcWiti4tMbBeLSo8C_aGRDWP7NHKDFoK_h9QAUzTkIRyiJh05M3pkXjJJd0C4nYONdZlTft242TzwBtVQF8LWHWpX1qNrtM1sdDnl4z2wrTMWsclC.Y_uJpCKg7mYUohhqnE6cSAs13JMuSF.rNrhxsXByNcnvJ_V84s2KrzRQ099cT02ZvC.3xFBnUKMfm7qEgApMEUodN_zN8AwkQvCHKVxoYhn6e9xrOuInFFlw53cZLSwhOR3VUrKGrxlsTc81DAdTAac8AvtDRrObL2IFKyLUzZ1Uyh3m5SQg_w8cgwMy4KEPW7eyh88jVp0oc85m5kGdliami_cV3uNGaM7igAbqUtyH1nwIf6y418p2H64DsJNEJaafL7572y4HmUHRHQpP_oGRvCRdRaPOQzQSyyE_QDyjWt2iZ9nc_MMwQnWac78p_1taiaPVCxONtkB0ydJ1pj20v0d2xyuJirJBAMB_euNJPngGQrUlkX9YLqr3H5qldxKtZSJd8ZooW3XrVtLesPuwntj9RXjOojrI7HaPk8_pGQ7dGIFod96xfHrlPU5Zpo3invBmedGDJ6ldYvH0t0qe.ww4Tff3RfsX3l4yesBpwvf7dLttmPpA7as.MU0a9VRkAZEMDdwdawXLiM2gEvrGfWklOAjkRYHQAhxB3TnK1vv2ffFkpkkpjvMeC5G0GbgL7rZMd1Aq7fPEHUlV7o.gcTjjsfDo9bU.GUQhSQq7_NaLtYryz2jT_bKSsXpuSUdw1OpY2s03ilRlFJvrfIv_qMWtUpjNnkyzsjN2c_IKsQseHZFewrl_X3ukRbQGO8eeUlnjrf8eF43ltt_YNA29aYgkmdzMsGzjYaOwNmENiRqtgIPbR.brygBLqBW5Wg9n8fZ6t2JLUov90e9FuH3qGr81yJ6BSDlaGtjc7d0CJAIhNkkU5_FCcfTy5_VjIUZF.jnshhNR8bJWFfqOGUpg.1v64O4oPl6gMwHSywXQtqsIf_IBib7IQNFekD80boDlPDS.s1BBcPeqoFGS6RlvYq.O1nMnszKtCnMV6o7Y1O6ZVH5Sf3EU9MZYe9bh20aSh9QWz7gdqG1_njOrkbzcG9tm8n12kYGC1aoZXbMawDT4uOusb9bGZVITgJQ.gq5b7_E6b.R7GRkg4bzYAJU28rGUmGhP3z1FxIx1iiqeZ8ymj4WiXXqLO_8SOXZISftaY9ULNnuTgGfHEViOEv31AUvBb364fzSxNvKccPY4nCRsjfLm9G4iPFAq5oFWuTP8k0ktX0hE4nm_6VNXdgcYjCti6eDPUhLgZ6u94tYQmpz..2O3H3Qm4G4b3Z_1XQLKyQnLk.Li4V0T8kygz05bZCAHYTmqUvRsdE28SH2.xp1Z0UL9cUQB.FsNcV0TDfpSy0rYvoFFCrFLF2mKzjr08g1JYhQSh8PeI7f1L4uCm4\",mdrd: \"vtIjhs0PyERnwjbfHA5.SlbM.03Mkke07GdMrWSG_hs-1736622134-1.2.1.1-A_20w5AgILnMznMiEPJg7dH9gCPtGbKKTjTVucAVXYKiAR5cC4IZMefQFgWilTDsy.ZqzjCmPhketHl4NR8z0NlrZjUFtvyndQMdRo6VXS0863sLalXkaTKxmxramRuzja5OkMG4pBIHfPG335P.oQRQyA4aqDk1HWM8o1dBc4iZNT8TN8ORkrNdO6eniEehbMmiY01nxe2p4UVBAwE3Fnzy007GHk3pjko1_nzs8CnDa3NsHGMRCdpsVakEA8Za4qGGVTHu29IgzfkiF_lewH9.ui.5oKXIz3md_a_cF4fIMWOkVzdfPaVnLmUGQ4gcdiH1FQj7NplkyYkRanK0.rqhSgcK8Qb.C.FpMuS8H5VbXBKaoPR2IhcMHWPYANhou7i06GvoBCWFbMVlbvDENeowO_ZlTXixN3SlucCBN_lU3u5qX2jl8RZf7f5hzUGE1KJ3VBeUB_OHpp6D104tE.QiqK_AgvzUbUfp0UsK2J0fDYRF7dvIxBWn5jtWzQclKpFlbdbu_7Yz6XzeNqtnwsESQrmcKx4KDy3e6fw2N2VK9jPygBk2IHzCoQPCetHPtQZb0FDSOvjpg9Dls4mS2idTeQw6hdYX9kD7YVxgXoU4Y1T78Ux7L4ZS4DfJzXKio4FDn_MdYJFqrMInDp4pE.NXaGG3z8sjDE5ojwScvYBzruC22vQkip7ZRyDIsvm7bJgZvwvGvaGWM1dUZyU04evPipnd0VBpIzjLaSN8YaBJDXaIrRpInMTvoHrO1_SwDBgJk2iPXyVKzkxZf9H.fw1zWUOHSI.tyhM6FBiewNWlT1b7L6sWcoRqsFbneuUDzAMIDtfS9eNE497TWiVdkFid80iPvaHAsLmO2__itmfN37gXaPVNP7HFS8Uokzn99WwX4oQbp8ygMke3rABS_Zj5k7MZyOFzLyOawJ9cSc.huug0Z6SDI.vuRMmCMz4dUUD3QAuygORJG6CcQUmVEWTQ4i7AvzPmME7vOTmlr6f530B01Ces9fLhuL0SI5EmeKLbEBSO9g2VVEifsJFkBQoSw7uxz4dqfPXIY5A0ck4oWp_Cuz4IEAY_Wob38ZWx4U2HsdwXhi474wY6CgykRHVpm273P2HYRr3v1TdchcxRs.aeksFhG6_ubCAEtF.NNPjDMyzZVWISAcrm9891yNID4p31WW_vJvLluFlwT_XwOqeT6pCIEMR21eSFwh_cu5VChI94wquv.v2rhiJUIfPU3zXK8MzVo5T251lK8jxK6KQXGSveT1CR8lB0e1.KGAbENg6_FR1LpUnKTNwEwT..UeGVNW9gP1Hbxq4rtarEl9euBg_TGVBtgbWseD06Dq1x8oGi2_CjhQ4A58bTcX5ty0di2UOBR_7mRp8t2DHrMWeGnTFOeQTg0mBbPJXYnMVp7q.sY0wtpNsHZ1kqBjoKf9HKklcc.n29klrJCgEz7KQjRaY9nvneX5iUWJbYFHfawqL84t9Wn.PMkkdAPNBVQ0.WzR2dwKpbyhwy9hyQgqGYc6nbZh8pNQ7uu.GsefS5ZArOgcpOhXSJSSRfjPtCtjiSVmZnAdSrBQUFMNH0xXSCo4eRalP2zaiLRL4tpEuxP6plc71iobvk_CoYVqIKiAPk_D8Xc8oW0Y34PPLybnVoMrqL57XWWzLFTzqEhM8BNFJWZeTrVY7.tNQKFxOeXoB071ppygzEiFz3VVQLKAGxV0jdLipubyLDj3xNzuCM.9A0zoWUDhDPNd9cNom3K7ZwkNBh9NDn3LCLFOdpwTueCBiFJ7sIkmr4tEN1XjdyNTC8HsweFK8YI2A626XD_dnxeoshAChFNXbqHpZWr6HOEULDMiPxcM8AD9DQiqaOp5CzFLRGjqQV.hwnnufzDbG5jAV8ehvyEfLD9.CGHm9E0Z373xnB4QSuQ3SWHGS5MGSZatDP1gFSWNHxtCoRyBhAvEwBRgm6E9JU0Eqax1vFvkC5HlUrKcbkMTkYxfLeD2bj5HgOzi3Xe7QDpp1EKp0C77draGDOSAXBrBc0TRh45n_1k1HWLnQeVnfj5.CnqfDdgYs2xqnHOsBwZyqVf5AQh40.LTXiLtD4nxw6nEyvUxnobXLnAU1xahpe9TQlPMoBcqb3TR0jtpwtjxP34Vh.Z.UVnGyEcMT2wVpOctbKbsQydxdhPzZDywX48FkmPzMYhFuPwMdufVl4w.AYp1DIJxd6YBqBDUfUgwtcwLIs18.OlmBaedhMSkQXDFdydwkhVCkKrmVfwvGVDiBhcqPS6MhGj.sLqEVJzfdc0ajKRX5eyW_1SNHE75v3eE1e5.NR7rKFlU8jRKY8xNKfwDb_YHdLkq.Z0LFT5dBnhGd9JFbOi58Jd3bBv55v\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=900728f72dc41177';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/blog\\/phi4?__cf_chl_rt_tk=LSOWfNeJSpPUGrugP9vF8_yNeQAi7r4b_e0FI8IPkvA-1736622134-1.0.1.1-Cakqz5EajJCfrr7sGBRGsmpfZhH8CaPkPMq6bqsrTvg\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42660335",
    "commentBody": "Phi-4 Bug Fixes (unsloth.ai)176 points by danielhanchen 21 hours agohidepastfavorite50 comments danielhanchen 21 hours agoHey HN family! I found a few bugs for Phi-4 - Microsoft's latest MIT licensed LLM to be on par with GPT-4o mini 1. End of sentence should benot2. Chat template should not auto add an assistant prompt 3. Padding token should not be EOS butI also converted Phi-4 to Llama-arch. I uploaded GGUFs, 4bit quants, dynamic quants and all fixes to https://huggingface.co/unsloth I also made a Colab notebook to finetune Phi-4 on a free GPU: https://colab.research.google.com/github/unslothai/notebooks... reply CGamesPlay 18 hours agoparent> We converted Phi-4 to Llama’s architecture for better accuracy and easier use. What does this mean? When I think about \"model architecture\", I think about the number of weights in each layer, the organization of the layers, etc. And AFAIK, it's untenable to \"port\" a model from one to the other without effectively retraining it. So what does it actually mean to \"convert to Llama's architecture\"? reply danielhanchen 17 hours agorootparentOh Phi-4's architecture is inspired from Llama itself, except they merged the attention matrices into 1 large matrix for better FLOP utilization, and the gate/up matrices in the MLP. Phi-3 use to use sliding window attention, but they got rid of that in Phi-4. So, you can \"Mistral-fy\" Phi-3 and convert it to Mistral arch (by unmerging the merges), and now you can \"Llama-fy\" Phi-4 to Llama arch. The reason why accuracy increases in finetuning is because during LoRA finetuning, you learn only 1 A matrix for merged QKV, whilst unmerging it creates 3 A matrices - this allows the model to have more freedom to learn new features. reply behnamoh 4 hours agorootparentI know some of those words... Man, do you recommend any blog/book/etc. that teaches me how to know this stuff? Most books are either too low level or too high level. reply Sn0wCoder 18 hours agorootparentprevWould guess GGUF so you can run on llama.cpp, LM Studio, etc..., but OP can hopefully clarity further for you. reply danielhanchen 17 hours agorootparentYep converting to Llama arch definitely makes accessibility much better - also many fast LLM serving libraries normally support Llama, so it makes it easier to port and use! reply sunaookami 19 hours agoparentprevWasn't Phi-3 also bugged/is still bugged? Seems like Microsoft just doesn't care. >to be on par with GPT-4o mini Phi is known to overfit benchmarks. It's way, way worse then that. reply danielhanchen 18 hours agorootparentPhi-3 should be fixed as well - but yes there were bugs as well! https://x.com/danielhanchen/status/1782853167572832650 Phi-3's sliding window should be 2048 and not 2047, and they also had chat template issues - I uploaded correct versions to https://huggingface.co/unsloth/Phi-3.5-mini-instruct reply throwaway314155 19 hours agorootparentprevAnecdotally, I've been experimenting with Phi-4 the past hour or so (so, yeah, not very comprehensive) and it's certainly a strong model. Definitely better than the previous Phi models. reply danielhanchen 18 hours agorootparentYep Phi-4 definitely is better than Phi-3.5! reply simonw 19 hours agoparentprevHuh! That may explain why I kept on getting visibleoutput when I tried running a Phi-4 GGUF file using llama.cpp. reply danielhanchen 18 hours agorootparentOh yes exactly! I trimmed it out now :) The better chat template should be: {% for message in messages %}{% if (message['role'] == 'system') %}{{'system' + message['content'] + ''}}{% elif (message['role'] == 'user') %}{{'user' + message['content'] + ''}}{% elif (message['role'] == 'assistant') %}{{'assistant' + message['content'] + ''}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'assistant' }}{% endif %} reply sroussey 17 hours agoparentprevCan you convert to ONNX so I can try in web browser? reply sroussey 17 hours agorootparentWould like to update this: https://huggingface.co/spaces/webml-community/phi-3.5-webgpu reply danielhanchen 17 hours agorootparentprevOh I can probs try doing this! reply danielhanchen 16 hours agoprevUpdate: The Phi-4 team is actively working on adding all our fixes into the original model! https://huggingface.co/microsoft/phi-4/discussions/21 reply RandyOrion 5 hours agoprevHi. It's nice to see these fixes. I got a question after checking results on the open LLM leaderboard[1]. Comparing the result of NyxKrage/Microsoft_Phi-4 and microsoft/phi-4 or unsloth/phi-4, I can see fixing both the tokenizer and chat template causes the performance of both IFEval and BBH to increase. However, the performance on MATH, GPQA and MUSR degrades A LOT. Is there any explanation on why this is happening? [1] https://huggingface.co/spaces/open-llm-leaderboard/open_llm_... reply t1amat 19 hours agoprevDaniel’s fixes to Phi-4 make it the best scoring Phi-4 on HF’s Open LLM Leaderboard. Great job on that. Unsloth is a masterpiece, keep up the great work! reply danielhanchen 18 hours agoparentThanks a lot! reply dorian-graph 6 hours agoprevThese seem like amazingly egregious mistakes MS made? Or is it not as bad as it seems? I suppose, I'm curious how these kinds of mistakes happen for a model release. reply NooneAtAll3 15 hours agoprevApplication Error TypeError: m(...).findLast is not a function at L (https://unsloth.ai/assets/root-DexjOeLv.js:1:340) at ia (https://unsloth.ai/assets/components-D38fXVcE.js:7:30549) at Ac (https://unsloth.ai/assets/components-D38fXVcE.js:7:98661) at Am (https://unsloth.ai/assets/components-D38fXVcE.js:7:94250) at o0 (https://unsloth.ai/assets/components-D38fXVcE.js:7:93401) at ha (https://unsloth.ai/assets/components-D38fXVcE.js:7:93212) at Mm (https://unsloth.ai/assets/components-D38fXVcE.js:7:90555) at Om (https://unsloth.ai/assets/components-D38fXVcE.js:7:89963) at MessagePort.M (https://unsloth.ai/assets/components-D38fXVcE.js:1:11235 reply danielhanchen 15 hours agoparentSorry are there some issues with our website? reply NooneAtAll3 11 hours agorootparentyep, it appears for a second - then displays only this :( reply danielhanchen 10 hours agorootparentOh no :( Do you know which device / platform? reply excerionsforte 16 hours agoprevAvailable on Ollama already: https://ollama.com/vanilj/phi-4-unsloth reply tandr 11 hours agoparentlooking at \"original\" Phi4 on ollama, it looks like they have fixed parameters issue for im_start/end reply danielhanchen 16 hours agoparentprevOh fabulous! :) reply sinuhe69 4 hours agoprevHow big is GPT4o-mini? Some sources say it's 8b big, but I guess they have different models with different sizes. But if GPT4o-mini is just 8b, I don't see the point of a \"distilled\" model, which requires a much bigger network but still not on par with the original. Because it's open source? reply greensh 7 hours agoprevMicrosoft developed and trained Phi-4. How can there be bugs in their official implementation? Does this mean they trained und evaluated it on their own completly different code and then ported it to the huggingface library for compatibility? reply c1b 5 hours agoprevdaniel youre a legend, thanks for all you do! one question, I see perf comparisons here are done on an L4, but isn't this SKU very rare? Im used to T4 at that tier reply lostmsu 19 hours agoprevThe benchmark results of the model before and after the \"fixes\" do not match numbers reported in the model card: https://huggingface.co/microsoft/phi-4 According to Microsoft MATH score should be 80.4, while both original and the \"fixed\" models as run by unsloth only score just over 12.3. So either Microsoft made a few huge mistakes, or unsloth was not able to run their model correctly. reply danielhanchen 18 hours agoparentOh yes I found this to be a bit strange - I uploaded our versions and Microsoft's own version to Hugging Face's public LLM leaderboard - https://huggingface.co/spaces/open-llm-leaderboard/open_llm_... You can see Microsoft's own original Phi-3 scores 12.31% - I'm unsure why. My fixes at least pushes it to 20%. It's possible because HF's benchmark does \"Scoring: Exact match: Was the solution generated correct and in the expected format\" which might be the issue reply m3kw9 3 hours agoprevBut fixing a model is the first I’ve heard of. reply adultSwim 18 hours agoprevAre there alternatives to unsloth? I would love to use it but the open/free version only handles one GPU, and it's unclear how much the paid version would cost. I have some limited access to multiple older NVidia cards and would love to make better use of them while I'm still learning. My budget for learning/projects is rather modest. Hopefully they succeed. At work I could make a strong case for going with them as they allow keeping data local only, instead of relying on an API. reply danielhanchen 17 hours agoparentMulti GPU support is definitely coming to Unsloth OSS! Our goal was to release it this month, but unsure on exact timelines - maybe next month!! reply adultSwim 14 hours agorootparentThank you! reply danielhanchen 10 hours agorootparentI'll ping you when it comes along! reply make3 18 hours agoprev\"Yes it improves performance!\" proceeds to show the most unconvincing stats ever you can probably blow on your GPU and get a similar performance change reply danielhanchen 17 hours agoparentI uploaded our fixed versions to https://huggingface.co/spaces/open-llm-leaderboard/open_llm_... which show the difference in scores. I agree it's not super convincing, so I provided anecdotal evidence as well - I'll work with the Phi-4 team to upstream these fixes! PS for further credibility, we also fixed 8 bugs in Gemma 1 - see https://x.com/danielhanchen/status/1765446273661075609 , multiple bugs in Llama, Mistral, Qwen and other models reply refulgentis 18 hours agoparentprevI'm sorry, I don't understand what you mean. I checked the original article again too. As it stands, my understanding is you are claiming: - blowing on a GPU (which I take to mean doing roughly nothing) - gets roughly the same perf change - as moving from fp16 to q4 reply danielhanchen 17 hours agorootparentAre you referring to the finetuning part? The multiple bug fixes are separate from the finetuning sections - Unsloth itself makes finetuning 2x faster and use 70% less memory - the bug fixes are totally detached from finetuning - ie you can take the fixed version we uploaded at https://huggingface.co/unsloth/phi-4, and use it in any framework or inference engine. Apologies I'm confused on the comment sorry. If you're questioning the credibility of the bug fixes - we fixed 8 bugs in Gemma https://x.com/danielhanchen/status/1765446273661075609, multiple bugs in Llama, Mistral, Qwen, a gradient accumulation bug https://x.com/danielhanchen/status/1846235913443262891 and much more reply grumpopotamus 14 hours agorootparent2x faster than what? reply danielhanchen 14 hours agorootparentOh 2x faster and uses >70% less memory than Hugging Face + Flash Attention 2! I did a CUDA / GPU Mode talk about it here: https://www.youtube.com/watch?v=hfb_AIhDYnA Also to the PyTorch team here: https://www.youtube.com/watch?v=MQwryfkydc0 and the PyTorch Conference here: https://www.youtube.com/watch?v=PdtKkc5jB4g reply kouteiheika 7 hours agorootparent> Oh 2x faster and uses >70% less memory than Hugging Face + Flash Attention 2! Is this doing the same type of fine-tuning, or are you comparing full bf16 fine-tuning in HF with 4-bit QLoRA in Unsloth (in which case it's not really an apples-to-apples comparison)? If it's the latter then do you have a comparison of the former? reply danielhanchen 16 hours agorootparentprevUpdate - the Phi-4 team is working on adding all our fixes to the original model! https://huggingface.co/microsoft/phi-4/discussions/21 reply TZubiri 18 hours agoprevAh yes, drawing ASCII art, the de facto benchmark for evaluating LLM quality. reply danielhanchen 18 hours agoparentAnecdotal evidence was provided to show some Redditors tested it out - but I do agree it's not correct to show that as an example - so I uploaded our fixed versions to Hugging Face's public LLM leaderboard here: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_... - this shows the fixes do in fact work! reply wsintra2022 16 hours agoprev [–] >Reddit comments show our fixes make Phi-4 inference much better I’d like to try ‘Reddit comments show my fixes make app better’ in my next review reply danielhanchen 16 hours agoparentFixed versions are also independently scored by Hugging Face's Open LLM Leaderboard: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_... The Reddit LocalLlama community is actually pretty cool - tonnes of research actually comes from the community - for example kaiokendev's linear RoPE scaling, YaRN, NTK Aware RoPE Scaling, many LLM benchmarks - many researchers use LocalLlama to share research and discuss on new stuff. I know a lot of AI researchers use the \"LocalLlama vibe check\" which essentially is an anecdotal approach to LLM evaluation - ie instead of relying on Chat LMsys or LLM benchmarks, 3rd party crowd sourced vibe checks sometimes do much better. reply danielhanchen 16 hours agoparentprev [–] As an update - the Phi-4 team is actively working on incorporating all fixes! See https://huggingface.co/microsoft/phi-4/discussions/21 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Daniel Hanchen discovered and addressed bugs in Microsoft's Phi-4 Large Language Model (LLM), such as sentence-ending issues and chat template prompts. - He converted Phi-4 to the Llama architecture, enhancing performance, and shared the fixes on Hugging Face, a platform for machine learning models. - A Colab notebook for finetuning Phi-4 is available, and while the fixes improved performance on some benchmarks, they decreased it on others; the Phi-4 team is working to integrate these improvements into the original model."
    ],
    "points": 176,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1736543841
  },
  {
    "id": 42661432,
    "title": "Very Wrong Math",
    "originLink": "https://www.charlespetzold.com/blog/2025/01/Very-Wrong-Math.html",
    "originBody": "Very Wrong Math January 10, 2025 New York, N.Y. The difference between misinformation and disinformation is the difference between ignorance and malice. Trolling is somewhat different, incorporating an element of provocation and narcissism. But what the hell is this? This was posted by a design and construction company that Facebook thinks I should follow. A cursory glance at their other posts reveals nothing quite as egregiously wrong. So I’m confused. Is it supposed to be a joke? Of course, me being me, I was curious exactly how wrong it was. Let me ignore the “flight time” part of the problem and focus on the length of those two arcs. The length of a circular arc is proportional to the subtending angle and the radius of the circle, specifically: length of arc = 2 · π · angle 360° · radius The angle for the two arcs in the illustration is the same, but in calculating the length of those arcs, the radius of the earth has to be taken into account as well as the distance above the surface of the earth. If the radius of the earth is R, then this illustration implies that: 𝑅 + 33,000′ = 4 · ( 𝑅 + 5,000′ ) The radius of the earth would therefore be: 𝑅 = 4,333′ That’s a distance in feet! That would make the circumference of the earth a bit over 5 miles, which is considerably less than the actual circumference. The mean radius of the earth is actually 3,459 miles or over 18 million feet. Setting R to that value, the ratio of the length of the outer arc to the inner arc is therefore: 𝑅 + 33,000′ 𝑅 + 5,000′ = 1.0015 In other words, the outer arc is less than 1% longer than the inner arc, but the flight time at that altitude would likely be less because of the decreased air resistance.",
    "commentLink": "https://news.ycombinator.com/item?id=42661432",
    "commentBody": "Very Wrong Math (charlespetzold.com)170 points by breadbox 17 hours agohidepastfavorite91 comments seanhunter 12 hours agoI have seen a very similar (incorrect) argument used to justify the idea of a flat earth. A builder on youtube made the argument (with a similar out of scale drawing of the earth) that if he drops a plumb bob and makes a right angle so he has a straight horizontal line and then goes across that line for a bit and drops another plumb bob, the two lines he has dropped are parallel, \"proving\" that the surface of the earth must be parallel to the horizontal line and therefore flat and not curved. If the earth's surface was actually curved he argued then the two lines he has dropped should tilt slightly inward towards each other. Which of course they do. The earth is just much much much bigger than in the diagram so the effect is within the margin of error for the measurement he was taking. As a meta point, our intuition often fails us hilariously when we are dealing with stuff that is out of the scale we have commonly seen in our lives. We joke about LLMs hallucinating but I'm not convinced we are so superior when we are outside our personal \"training data\". reply userbinator 8 hours agoparentThe earth is just much much much bigger than in the diagram so the effect is within the margin of error for the measurement he was taking. It's actually measurable on a human scale: https://www.mathscinotes.com/2017/01/effect-of-earths-curvat... 1 5/8\" difference over 693', or slightly less than 1 part in 5 thousand --- definitely measurable on a smaller scale with accurate machinists' tools. reply pvillano 2 hours agorootparentOne can also watch a boat leaving shore descend \"under\" the horizon with a telescope reply sigmoid10 2 hours agorootparentYou don't even need a boat or a telescope. Just watch the sun set on the ocean while lying down at the beach just in front of the water. The moment it disappears completely, stand up. You'll see part of it again. If you measure the time it takes disappear completely again and know your own height, you can even get a rough estimate of earth's radius. reply munch117 11 hours agoparentprevAh, but would they actually be parallel on a flat earth? Say the earth is disc-shaped. Then the center of gravity is only directly beneath you if you're standing at the exact center. You get ever-so-slightly not parallel lines, just like on a round earth. The fun part of a disc-shaped earth comes as you move towards the sides, and gravity, still pointing towards the center, makes you stand at an increasingly acute angle to the surface. The ground beneath you will then appear like one big endless mountainside, with an increasingly steep slope the further away from the center that you get. reply tempestn 10 hours agorootparentDepends what causes things to stick to the flat Earth. IIRC flat earthers have various explanations for gravity, including the disc continuously accelerating upward; in that case you'd experience the same force everywhere on it. reply munch117 8 hours agorootparentIf this mysterious disc-accelerating force also accelerated the people and things on the surface, we'd all be weightless. I guess it must be a pushing force from below. So, who's doing the pushing? I'm thinking a big turtle. reply nkrisc 8 hours agorootparentThey mean it is actually accelerating constantly. My math might be wrong, but if we were accelerating at 9.8m/s/s for at least 4000 years (roughly as long as we have continuously recorded history and the minimum time “gravity” has been observed) then we ought to currently be traveling through space at over 1,000,000,000,000m/s. Now I’m no physicist, but I reckon that might end up violating causality. reply jbeninger 7 hours agorootparentNah, when you move that fast, further acceleration stops increasing speed and starts squishing time instead, so you asymptotically approach C. So I guess what I'm saying is I see absolutely no problem with the flat earth arguments? reply plagiarist 4 hours agorootparentWe should see this as all the celestial bodies traveling \"down\" at relativistic speeds by now. Unless maybe they are also experiencing 1 G in the same direction as us in addition to whatever other accelerations. reply nkrisc 5 hours agorootparentprevCan’t argue with that, I guess. reply bryanrasmussen 7 hours agorootparentprevwait, is the flat earth theory going to make me immortal? reply lukan 6 hours agorootparentOnly if you truly believe in it. Then you create a belive field, shaping your reality in any form you desire. reply empath75 4 hours agorootparentprevSome of them think that. The problem with trying to \"explain\" this is that fundamentally, flat-earthers, to the extent that they could be said to have a coherent world view at all, are usually a kind of occasionalist[1]. They don't _believe_ in natural laws or cause and effect. For the most part, they believe that god is in complete control of all events, and things go down because god wants them to go down. There's no required explanation for _anything_. The sun moves across the sky because god wants it to, and he could stop it or make it go backwards if he wanted it to, etc. Indeed, that a flat earth is incompatible with physics is part of the appeal of believing in it to begin with. They _want_ to overthrow Newton, because a clockwork universe is incompatible with their belief system. It's also sort of immune to any kind of argumentation. The result of any experiment is simply that god wants it that way, that they're predictable and testable doesn't _prove_ anything, because you can do an experiment a million times, and god could still cause it to fail any time he wants to. God just doesn't want to argue with Netwon right now, for his own reasons, you see. 1: https://en.wikipedia.org/wiki/Occasionalism reply mlyle 3 hours agorootparenthttps://www.xkcd.com/2440/ reply lukan 7 hours agorootparentprevThe theological argument I recently heard is, the creator just made up and down. And things move down. But it is not gravity. reply mlyle 2 hours agorootparent> And things move down. It's not a bad way to look at it for a start. Things move down because it is their nature to move downwards. And this kind of empirical law is what we rely upon for most thought. It takes a lot of work to get to a theory that makes more general predictions. And even after having that, 98% of the time my thought is effectively just \"things move down.\" Another 1.5% it's \"things move down at 9.8 m/s/s\". It's an extreme edge/special case when I'm thinking \"massive things are attracted to each other, with a force proportional to their masses and inversely proportional to the square of their distances\". And even with \"massive things are attracted to each other, with a force proportional to their masses and inversely proportional to the square of their distances\" ... if you ask me why, it's because \"uh, they just do that?\" reply lukan 14 minutes agorootparent\"with a force proportional to their masses and inversely proportional to the square of their distances\" ... if you ask me why, it's because \"uh, they just do that?\"\" To be fair, that is somewhat the current scientific consensus on gravity. It just is. We can meassure it and determine a general constant and calculate with it (and even though some people claim to have understood way more, it is highly debated terrain as far as I know) And in general, I was actually arguing with flat earthers recently a lot, I even met a flat earther in real life. It is an interesting intellectual challenge debating them. Basically rebasing all the physical theory I have. (Main summary is, they have a high ego, but lack understanding of everything and make up for it with make believe.) If I found a school one day, one of the lessons will be the teacher telling the students: \"The earth is flat! Proof me otherwise.\" Or more advanced, model a flat earth on a computer. Flat earthers try that for real - it gets weird very quickly, so much that I could not believe anyone taking it serious and it all is just satire. But they are for real (but with a very different concept of reality). reply mlyle 7 minutes agorootparent> To be fair, that is somewhat the current scientific consensus on gravity. It just is. We can meassure it and determine a general constant and calculate with it (and even though some people claim to have understood way more, it is highly debated terrain as far as I know) Sure, and if we come up with some fancy unified theory, and ask \"why\" once more, the answer will still be \"uh, because they do?.\" > But they are for real (but with a very different concept of reality). We think ourselves so advanced. I wonder what big counterfactual scientists believed in the 1900s and 2000s will be laughed at a few hundred years from now. And, of course, some of that will be libel; e.g. that we thought the world was flat \"just like Christopher Columbus's compatriots\" [who didn't]. cratermoon 4 hours agorootparentprev\"A wizard did it\" reply f1shy 8 hours agorootparentprevWe can give them points for creativity. reply thombat 10 hours agorootparentprevStandard flat-earther response is to scornfully deny the existence of gravity. It's all density/buoyancy you see... Gravity is a hoax promulgated by the notorious cabalist Newton, in service to his Illuminati/Papal masters, etc, etc. reply mp05 5 hours agorootparentWhy do we still talk about these people? The more we stand in awe of their calculated ignorance, the more satisfied they are. I feel like there are better things to do with my time than be as fascinated by it as some people. reply Someone 5 hours agorootparentprev> and gravity, still pointing towards the center, makes you stand at an increasingly acute angle to the surface. The ground beneath you will then appear like one big endless mountainside That’s why you never hear of people who went to the edge of that dis: they slid down that mountainside, and dropped off :-) Alternatively, you can postulate that disc to be arbitrarily thick. That will decrease the deviations. If that’s not enough to make them immeasurable, postulate that the stuff “deeper down” has higher density. In the limit, just postulate that there’s an enormous black hole millions of light years below the center of the earth. Flat-earthers probably won’t accept Newton’s theory of gravity, however, so you can make up anything. reply phkahler 6 hours agorootparentprevEven physicists have a hard time with disks and gravity. I can't tell you how many times I've seen them use the shell theorem on galaxies (does not apply). The only dark matter is in their head ;-) reply jimmaswell 10 hours agorootparentprevI'm considering what flat-surfaced shape you could construct with equal gravitational pull at all points. Maybe something where the center is thin as a point, the edges have a lot of depth, and they curve towards the center either convex or concave. Might run some calculus to figure it out. reply t_mann 9 hours agorootparentThat way you should be able design a disc-shaped earth with constant strength of the gravitational force on the whole surface. But it would still have a center of mass (likely lying outside the shape you're describing, in the void beneath the center point), and the direction of the force should still be pointing towards that center, no? So the problem the GP has described, that you're starting to tilt as you move towards the edge, should remain in principle. reply benterris 8 hours agorootparentI believe the strength of gravitational force would not be constant either, as your center of mass would still have a fixed location, so every point on the disc have different distances to that center of mass (in addition to not being orthogonal to the surface). But maybe it might be approximated with an infinitely long cylinder, so the center of mass is infinitely far away below the surface ? reply somat 9 hours agorootparentprevyes, we call it a sphere. I am just joking with you, I know what you mean, however the fruit was hanging too low not to pick. reply rendaw 6 hours agorootparentprevDoesn't the flat earth extend infinitely in all directions? reply jerf 3 hours agoparentprev\"We joke about LLMs hallucinating but I'm not convinced we are so superior when we are outside our personal \"training data\".\" In all seriousness one of the things about LLMs that most impress me is how close they get to human-style hallucination of facts. Previous generations of things were often egregiously and obviously wrong. Modern LLMs are much more plausible. It's also why they are correspondingly more dangerous in a lot of ways, but it really is a legitimate advance in the field. I observe that when humans fix this problem, we do not fix it by massive hypertrophy of our language centers, which is the rough equivalent of \"just make the LLM bigger and hope it becomes accurate\". We do other things. I await some AI equivalent of those \"other things\" with interest; I think that generation of AI will actually be capable of most of the things we are foolishly trying to press hypertrophied language centers into doing today. reply ben_w 7 hours agoparentprev> We joke about LLMs hallucinating but I'm not convinced we are so superior when we are outside our personal \"training data\". Every time I see the phrase \"common sense\", I expect to see an example of the human failing you describe. reply BalinKing 16 hours agoprevRelated Wikipedia article: https://en.wikipedia.org/wiki/String_girdling_Earth#Implicat.... The takeaway is that the extra length of the arc is likely much smaller than one would intuitively expect. The problem is usually framed like so: If you wrapped a rope around the earth, how much more rope would you need to add so that it would be 1 meter above the ground at all points? The answer is only 2π meters! reply travisjungroth 1 hour agoparent> The takeaway is that the extra length of the arc is likely much smaller than one would intuitively expect. Maybe it’s because I’m a pilot and we never account for altitude when measuring distance, my intuition puts the difference at “effectively zero”. I also have it internalized that the earth’s atmosphere is very thin. reply nayuki 11 hours agoparentprevAnd the text about the airplane problem was added on 2024-11-26: https://en.wikipedia.org/w/index.php?title=String_girdling_E... reply gsf_emergency 12 hours agoparentprevThis could be why dimensional analysis is one of the few things from physics class that can't be drilled enough.. Without forcefully dumping the geometric \"intuition\", this would still feel counterintuitive to me! reply shortrounddev2 15 hours agoparentprev(2pi * (n + 1)) - (2pi * n) -> 2pi * (n + 1 - n) -> 2pi * 1 -> 2pi If I remember my algebra correctly. Someone else check my work I'm a dropout reply freeopinion 14 hours agorootparentFor convenience, we set τ=2pi. :-) x = τ(r+1) - τr = τ(r+1-r) = τ(1) = τ reply shortrounddev2 12 hours agorootparentHow do you pronounce that symbol? reply layer8 36 minutes agorootparenthttps://tauday.com/tau-manifesto https://en.wikipedia.org/wiki/Tau reply Tistron 12 hours agorootparentprevTau. cf vihart: https://www.youtube.com/watch?v=jG7vhMMXagQ&t=0s reply freeopinion 6 hours agorootparentprevhttps://doc.rust-lang.org/stable/std/f64/consts/constant.TAU... https://docs.python.org/3/library/math.html#math.tau https://www.google.com/search?q=tau*1 https://docs.oracle.com/en/java/javase/21/docs/api/java.base... https://learn.microsoft.com/en-us/dotnet/api/system.math.tau... reply kurthr 15 hours agoparentprevThe only issue I see with this is that as a classic physics trope, we've approximated the earth as a sphere. If, instead we approximate it as a fractal... then the distance is infinite, or at least highly dependent on the thickness of the rope! The error in the original is assuming that the radius is proportional to the height above the earth (Earthradius=0?). reply seanhunter 12 hours agorootparentWe actually model the earth as a very large spherical cow. This is approximately the same for most purposes but ends up being more convenient. P.S. Not a physicist, but my child is studying maths and physics at Uni at present, so I have it on good authority that this is still going on. They told me in their first week one of their classes had a worked example where the lecturer used the phrase \"Assume the penguin's beak is a cone\". reply kergonath 8 hours agorootparent> I have it on good authority that this is still going on Do you mean making simplifying assumptions to make a problem tractable? Of course it’s still going on. It has to be, otherwise you just cannot do anything. > Assume the penguin's beak is a cone It is impossible to consider the true shape of a penguin’s beak for several reasons: - you’d need to go all the way down to the electron clouds of the atoms of the beak, at which point the very concept of shape is shaky - every penguin has a different beak so even if you describe perfectly one of them, it does not necessarily make your calculation more realistic in general. There is a spectrum of approximations one can make, but a cone is a sensible shape at a first order. It’s also simple enough that students can actually do it without years of experience and very advanced tools. What do you think they should do instead? reply seanhunter 8 hours agorootparentBet you’re fun at parties as they say. I totally understand why simplifying assumptions are helpful in modelling and definitely don’t need you to explain that. It also is a bit ridiculous if you think literally about it which makes it something that is fun to laugh about as here. reply kergonath 7 hours agorootparentYes, sure, I get the jokes. I just found it puzzling that someone would think it stopped. And I don’t talk about work at parties anyway :) reply eminent101 5 hours agorootparentBut nobody in this thread thought it (simplifying assumptions) stopped. You seem to be making an assumption that someone thought that and then posting long explanations that nobody asked for. I read the \"P.S.\" of grand-grand-parent comment as good humor. Nothing there implied that they really thought that simplifying assumptions would/should stop. Imagine a world where every bit of humor is interpreted literally and then refuted pedantically! What kind of a world would that be? reply davrosthedalek 9 hours agorootparentprevA spherical cow /in vacuum/ reply Dylan16807 15 hours agorootparentprev> infinite, or at least highly dependent on the thickness of the rope The latter. But that's only if it's not somewhat taut. Some tension brings it closer to a circle and makes the actual thickness pretty unimportant. But I like the idea overall. It means that lifting up the string makes it smoother and it actually needs less length. How's that for being unintuitive? reply kurthr 15 hours agorootparentExactly, if you're only 1cm off the surface you follow every nook and cranny. If you're 10km off the surface only Everest is a blip. reply aardvark179 10 hours agorootparentprevJust because your initial fractal path is infinite does not imply that a line offset from it is also infinite (even for an infinitely thin rope), at least if the offset version is not self intersecting. reply csours 1 hour agoprevReminds me of this classic: https://www.politifact.com/factchecks/2020/mar/06/msnbc/bad-... “Bloomberg spent $500 million on ads. The U.S. population is 327 million. He could have given each American $1 million and still have money left over.” https://www.youtube.com/watch?v=6egeUxIEQnM reply bruce511 12 hours agoprevEven if the math of the arc length was correct (and you don't need to be a math professor to figure out it isn't) there's another logic misstep. Implied in the caption is that the speed is the same at all heights (given that an increase in distance is implied as an increase in time.) This is again obvious nonsense - speed is a function of thrust versus drag, and it's safe to say that both of those are affected by air density. It becomes even less true once one gets to space. There height is a function of speed which means that to \"catch up\" something in front of you, you need to slow down. reply mastermedo 11 hours agoparent> It becomes even less true once one gets to space. There height is a function of speed which means that to \"catch up\" something in front of you, you need to slow down. Can you expand on this? My brain is not connecting the dots. reply db48x 11 hours agorootparentHe is talking about orbital mechanics, rather than free space. When you are in an orbit, the shape of the orbit is determined by your speed. At every distance from the center of the object you are orbiting (such as the Earth), there is a speed that makes your orbit a circle. If you are going at any other speed then your orbit will be an ellipse instead. Too fast and your orbit rises higher above the Earth. Too slow and it dips back down closer to it. If you try to “catch up” with an object ahead of you in your orbit by speeding up you will only turn your orbit into an ellipse that gets further away from the Earth, and thus further away from the object you were trying to catch. Instead of catching it you’ll go up and over it. As Niven wrote, “forward is up, up is back, back is down, and down is forward”. It’s rather counterintuitive at first. Playing KSP can help you get a feel for it, especially once you start docking multiple craft together. reply davrosthedalek 9 hours agorootparentJust to point out here what's different between \"space\" and \"not space\": \"Space\" assumes no \"height control\",i.e. ways to exert force \"down or up\" along the earth-object direction. That's obviously not true for a plane. If you can exert force in that direction, you can change speed and keep the shape of the trajectory around earth constant. reply Sharlin 9 hours agorootparentprevIt’s even worse than that. By speeding up you end up actually getting further behind your target because in your new higher orbit you actually move slower on average, and as your average orbital radius gets longer, so does the circumference, so you end up on a \"detour\" trajectory compared to your target! Whereas if you slow down, you drop to a lower, shorter, higher-speed orbit. reply nyc111 9 hours agorootparentprevThis is called Kepler's Third Law, right? Radius^1.5 :: Period reply f1shy 8 hours agoparentprevYou are literally going way and beyond what the target audience of that post (the original with bad math) was for. reply chrismorgan 10 hours agoprevI think the funny thing about this article is this numeric error (though not so egregious as the one that caused the article!): > The mean radius of the earth is actually 3,459 miles or over 18 million feet. That’s off by 500 miles; the correct figure is 3,959 miles. That makes it almost 21 million feet, and yields a ratio of about 1.0013378, even smaller than the quoted 1.0015. reply jodrellblank 1 hour agoprevEarth in the picture is scaled to roughly 300,000 feet per pixel; Earth's surface and both flying altitudes would be the same pixel if drawn to scale. (~42M feet diameter shown in ~134 pixels). reply quantified 37 minutes agoprevHoly crap is one drop of stupid consuming a lot of mental energy. This is after the drop of stupid that was Terence Howard's \"1 x 1 = 2\" physics rant fell on everyone's head. Individual drops quickening into rain would drown us all, apparently. Do serious people like Charles Petzold (here) and in other venues address this stupid out of fear that the stupid spreads, or because they just can't stand someone being wrong somewhere like a cognitive itch that must be scratched? If one troll flooded the zone with 30 of these over a month, mayhem would ensue. Absent knowing the true origin for this diagram, we don't even know if was stupid or malicious. reply n4r9 8 hours agoprevDon't know if anyone mentioned this yet, but presumably the flight path does not follow a normal vector to gain height, but generally something more diagonal in the direction of travel. reply 1970-01-01 2 hours agoprevAll models are wrong, but some are useful. ⇒ Some models are wrong and useless. reply lynguist 7 hours agoprevI would do it like this: Approximate Earth as a flat line. (The 5000ft path is close enough that it is also represented by the flat line. This is the 5000ft path.) Then make the 33000ft path which is a slightly looser line on top of this line. This new path is not 4 times longer. Just a little bit raised, because 33000 ft is “nothing” compared to Earth. (To become 4x longer we would go deep into outer space and back.) reply prmph 9 hours agoprevOne question I've always had with this: How does the rotation of the earth affect an airplane's flight time, if any? And how does this change with altitude? reply f1shy 8 hours agoparentSimple answer: Zero. Because the planes move inside the atmosphere, which moves with the earth. A more nuisance would be that earth rotating generate all sorts of things in the atmosphere, including winds and Coriolis effect on the winds, and you can account for that considering the winds. Btw a flight from Chile to France and back, will have a leg significantly shorter (up to 2 hs in a 13hs flight) and which leg it is, depends on the time of the year. reply prmph 6 hours agorootparentInteresting to know about the Coriolis effect. I get that what really matters is the relative motion, but it still seems to me that there might be a gravitational/inertial effects at play, even if tiny. Consider this thought experiment: Planes cannot really fly into space, but assume they can. At a certain altitude, it cannot be said the the plane is moving perfectly in step with the gravity of the earth. At infinite altitude, that certainly cannot be the case. So that tells me there is some deviation due to the inertia of the plane, even at low altitudes. Like I said, the effect might be tiny, but would be interesting to learn more about it nonetheless. reply f1shy 6 hours agorootparentI meant coriolis effect on the wind. Not sure if noticeable in a plane. reply Derbasti 7 hours agoparentprevIn general, the air moves with the ground, so the earth's rotation does not affect airplanes. However, rotation of the earth imparts a coriolis force on the air, which results in jetstream winds. Aircraft routes are optimized to use/avoid jetstreams for shorter travel times. reply kergonath 8 hours agoparentprevIt does not really, at least not directly. What matters is relative velocity compared to the starting and final locations, and relative to the air around the aircraft. It just happens that there are very powerful atmospheric currents that go west to east (those are due to the earth’s rotation, among others phenomena). So, when flying towards the east, catching these currents can significantly reduce flying time. When flying towards the west, we want to avoid them by flying below or elsewhere. reply prmph 6 hours agorootparentThanks for this explanation; quite interesting. But it still seems to me that there might be a gravitational/inertial effects at play as well. At a (hypothetical) infinite altitude, it can no longer be said the the plane is moving perfectly in lock-step with the gravity/rotational acceleration of the earth. This implies the inertia of the plane relative to the rotation of the earth still has an effect at lower altitudes. The effect might be tiny, but would be interesting to learn more about it nonetheless. reply svilen_dobrev 2 hours agoprevCharles Petzold.. His c++ book stood on the shelf behind me. 30y ago. heh :) reply simplicio 7 hours agoprevSeems intuitively obvious. On a flat Earth the two distances would be the same, and while the Earth isn't flat, its close enough to approximate a flat surface for most purposes, so you'd expect the differences in the two arcs to be ~0 reply syntex 10 hours agoprevjust 2piR and then extra h change the result very little fraction. How is that counter-intuitive :) reply wittjeff 7 hours agoprev\"the high you go\" reinforces my initial assumption that this is self-filtering clickbait. reply kubb 13 hours agoprevCurious, does the air being thinner affect flight time? reply jhanschoo 12 hours agoparentThere's no straightforward answer because there are many factors that affect flight time and fuel economy, including the aerodynamics of the plane and the engine technology. I hazard a guess that for commercial airplanes these are chosen primarily for reasons of fuel economy per seat and then that determines the model's designated cruising altitude. For a particular model, flying above the model's cruising altitude should lead to lower fuel efficiency. reply aja12 12 hours agoparentprevFrom what I've learned reading AdmiralCloudberg's plane crashes analysis [1]: altitude heavily matters in fuel consumption. Jet planes use a lot less fuel at a higher altitude, up to the point that a plane on the verge of running out of fuel at a medium altitude might manage to squeeze in 50 or 100 more miles of flight by climbing 5000 feet, even accounting for the increased fuel consumption during climb. I guess that correlates with speed as well. Turbofan engines, on the other hand, are more fuel efficient than jet engines at lower altitudes, hence they remain common for interstate transit. The difference seems to be directly caused by the effect of air \"thickness\" on the engines. [1] https://admiralcloudberg.medium.com/ reply danieldk 4 hours agorootparentI’m confused, isn’t turbofan a type of jet engine? https://en.m.wikipedia.org/wiki/Turbofan Did you mean turboprop? reply kergonath 8 hours agoparentprevYes, because friction depends on the air density. You can think of this as the molecules in the air colliding with a moving object and pushing it backwards, thus slowing it. If there are fewer molecules, there is less friction and the object can move faster with the same thrust. Thrust itself decreases because there are also fewer molecules to push against, so it can get quite complicated if you want to account for everything. But overall it is easier to fly faster higher up in the atmosphere. Also, atmospheric currents are important. There is a useful discussion here: https://aviation.stackexchange.com/questions/57209/how-does-... reply seanhunter 12 hours agoparentprevPresumably this would affect drag significantly. Here are the equations of motion of an aircraft https://eaglepubs.erau.edu/introductiontoaerospaceflightvehi... ...and indeed it does. Here is a discussion https://aviation.stackexchange.com/questions/24641/what-is-t... reply Waterluvian 13 hours agoparentprevI think in a way that’s why planes fly so much further up than you’d think they’d need to. They want more consistent and minimal atmospheric conditions. Less air means less energy means less turbulence, I think? If you’re talking about friction… oooh that’s an interesting one. Intuitively yes. But is it also negligible? reply juresotosek 1 hour agoprevhaha crazy reply teo_zero 12 hours agoprevAnd of course pi = 22/7! ;) reply necovek 10 hours agoparentThat's actually a lot closer to the actual value of pi than the implied difference in the article vs the 0.15% actual difference in path length. As in, the illustration would be less wrong if it only used 22/7 for Pi and correctly portrayed dimensions of Earth and flight heights. reply nejsjsjsbsb 13 hours agoprevAre we talking spacetime? reply fargle 2 hours agoprevi've seen that exact image posted semi-regularly on various reddit and facebook groups. (it's one of 500 things i hate about those sites. but for some communities that's where the information lives, that's where marketplace lives, etc., so i'm stuck with it) these kind of things are intentionally wrong \"puzzles\" that are designed to get hundreds of people mad and post rebuttals to \"drive engagement\" or whatever. the pictures of a wheel with sledgehammers and chains and jacks with lugnuts plainly still in place and a post \"how can i get this off it's stuck and i've tried everything\". sigh... it's just another form of trolling. sure enough, notice the sibling comments here. how many nice people took the time to patiently explain the fallacy(ies) for the 1000th time. then the pedants who correct the grammar/math/etc. in the 98% correct explanations. then the \"true believers\"/trolls who don't get it and argue back. and so on. https://xkcd.com/386/ reply pessimizer 8 hours agoprevDoes \"Remember the high you go the further it'll have to travel\" really need to be debunked? Did the \"design and construction firm\" spell \"drill\" with one \"l\"? reply mppm 3 hours agoprev [–] xkcd.com/386 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A design and construction company made a math error on Facebook, incorrectly calculating the Earth's radius as 4,333 feet, leading to a vastly underestimated circumference of just over 5 miles.",
      "The error stemmed from calculating arc lengths without using the Earth's actual radius, which is approximately 3,959 miles.",
      "The post highlighted that the outer arc was less than 1% longer than the inner arc, suggesting a potential reduction in flight time due to decreased air resistance."
    ],
    "commentSummary": [
      "A YouTube builder incorrectly claimed that plumb lines would be parallel on a flat Earth, misunderstanding the Earth's curvature, which is negligible in such measurements. - The discussion highlighted the failure of intuition in understanding large-scale concepts and addressed flat Earth theories, gravity misconceptions, and Earth's rotation effects on flight. - The conversation humorously examined the persistence of incorrect beliefs and the difficulty of explaining scientific concepts to those who reject established science."
    ],
    "points": 170,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1736550656
  },
  {
    "id": 42660942,
    "title": "Building Bauble",
    "originLink": "https://ianthehenry.com/posts/bauble/building-bauble/",
    "originBody": "January 10, 2025 Building Bauble I made something that I think is pretty neat, and I want to tell you about it. This is a little hot air balloon made out of alternating layers of brass and bronze that stack together with these angled facets: It’s 3D printed, sort of, but it really is solid metal – it’s not a metallic filament. It’s made by “lost wax casting,” where you 3D print a model out of resin, then pack it in plaster, and then once the plaster dries you melt out the resin and fill the void with molten– You know what? This is neat, but this actually isn’t what I wanted to tell you about. Get cute with me Neither is that, but we’re getting closer. That’s the 3D model that the balloon is cast from. I didn’t actually cast the balloon – I paid someone else to do that for me – but I did make the 3D model. And it’s an interesting 3D model. It’s not a triangle mesh, like most 3D shapes you encounter. It has no faces; it has no vertices. Instead, it’s made entirely out of math: this balloon is a pure function of 3D space. Here, take a look: (def thickness 25) (def sections 12) (def angle (pi * 0.25)) (def lobe-intensity 1) (def bezel 1) (def bronziness 1.5) (def branzino false) (ball [(100 / (lobe-intensity + 1)) 100 100]union :r 50 (cylinder y 25 50move [0 -100 0])scale y (ss p.y -100 100 1 0.8)intersect :r bezel (plane yshell thicknesscolor (gl/if (mod $i.y 2= 0) (pow default-3d-color bronziness) default-3d-color)tile: $i [0 thickness 0]rotate z (remap- parity * angle)gl/let [parity (mod $i 2)] _)radial: $i y sectionsmove y 40 ) There’s the source code to that hot air balloon. Mess around with it. Edit some constants. Pull up the autocomplete with ctrl-space, and see what else it can do. This is called Bauble, and this is what I wanted to tell you about. Bauble is a tool – toy? – that I wrote in 2022, because I wanted to make pictures with math on my computer. And not just simple geometrical things like that. I wanted to make pictures like this: (defn fork [shape f1 f2] (union (f1 shape) (f2 shape))) (defn spoon [shape f1 f2] (union shape (f1 shape) (f2 shape))) (defn cel [shape rgb] (shade shape :f (fn [light] (gl/do (var cel-shadow (step 0.8 light.brightness + 1 / 2)) (var cel-shading (dot light.direction normal * light.colorquantize 2 * cel-shadow)) (var regular-shading (dot light.direction normal * light.color * cel-shadow)) (var b (mix cel-shading regular-shading 0.5 + 0.5)) (b * rgb))))) (setdyn *lights* [(light/directional 1 [-2 -2 -1] 1024 :shadow 0.25)]) (def ear (cone y 40 153 :r 12morph 0.15 (sphere 46move y 64)union :r 13 (cylinder y 26 30move y -10)scale z 0.5)) (def ears (earmove x 134rotate z (tau * -0.01)mirror x)) (def body (ball [1 0.75 0.5 * 100]union :r 72 (ball [0.58 (.84 * 0.75) 0.5 * 250]move y -156))) (defn body-color [$] (cel $ (hsv (4 / 6) 0.1 0.3))) (def decoration (rect [32 10] :r 10rotate (q.x * 0.044 - pi)rotate pi)) (def top-decorations (fork decoration (fn [$] ($rotate -0.21)) (fn [$] ($scale 0.9move x 71rotate -0.30))move x 40mirror x)) (def bottom-decorations (spoon decoration (fn [$] ($scale 0.95move x 76 y 0rotate -0.14)) (fn [$] ($scale 0.9rotate -0.59move x 140 y -37))mirror x)) (def decorations (union top-decorations (bottom-decorationsmove y -80))) (def tummy-patch (box 110 :r 64morph 0.70 (sphere 118)move z 60 y -137cel (hsl (/ 69 255) 0.10 0.65))) (def body-and-ears (union :r 6 body (earsscale 0.42move y 81)body-colorunion-color (subtract tummy-patch (decorationsextrude z infscale 0.5move y -46)))) (def eyes ( sphere 14union :r 4 (box [14 0 1]move z 4)cel 10union-color (sphere 5move z 14cel 0.05)scale z 0.5rotate x -0.37 y 0.34move [52 28 43]mirror x)) (def arms (box [15 100 (ss p.y -100 50 25 40)] :r 15rotate z (p.y * 0.002)rotate z 0.30move x 131 y -122mirror xbody-color)) (def whiskers (union (line [0 0 0] [85 0 0] 1.5 0.5rotate z 0.03move [0 6 0]) (line [0 0 0] [82 0 0] 1.5 0.5rotate z -0.03) (line [0 0 0] [87 0 0] 1.5 0.5rotate z -0.10move [0 -6 3])move [60 0 41]mirror xcolor 0.1)) (def floor (ground -300cel (hsv 0.7 0.1 0.04))) (def nostrils (cylinder z 2.5 3move [7 1 4]mirror xcolor [0 0 0])) (def nose (ball 11subtract :r 2 (cylinder z 10 10move x 11 y -12mirror x :r 1)scale z 0.6 y 0.7 x 1.5rotate x -0.5cel (hsv 0 0.0 0.02)union-color nostrilsscale 1.20move z 48 y 30)) (def lilypad (cylinder y (5 * sin (theta + 1 * 3) * cos (theta - 1 * 2) + 40) 0.3move y (sin (theta * 4) * cos (theta * 2 + (length p.xz / 8 + 18)) * dot p.xz * 0.005 + (ss p.z 0 40 0 -10))slow 0.7union :r 5 (cylinder y 1 10 :r 1move y 10rotate x 0.2 z (sin (p.y / 4) * 0.1))move y 77cel (hsv (2 / 6) 0.9 0.5)gl/let [theta (atan2 p.xz)] _)) (union eyes (union body-and-ears arms :r (10 - (distance [(abs p.x) p.y p.z] [120 -78 -1] / 10)max 0)expound (perlin p 15) 0.05 2) whiskers floor nose lilypadtint white (fresnel 5 * 0.3)) I had just discovered signed distance functions, and I was enamored by the power that they give you to sculpt space with simple mathematical expressions. Signed distance functions – SDFs – are amazing, and if this is the first time you’re hearing about them, you should probably drop everything you’re doing today and watch this twenty-five minute video of Inigo Quilez using signed distance functions to create an animation instead. Yes, twenty-five minutes is a lot of minutes. It’s worth it. I know you didn’t actually watch the video, but the overall gist is that someone very smart and very good at math describes an animation he created out of pure functions of time and space. But the description is pretty high-level: he says things like “we’ll define three circles that we spin as we move down the parameterization of the curve,” which is a beautiful way to think about the effect he uses to create the braids in that video – but how do you actually do that? Well, you write several hundred lines of something called GLSL, plumbing arguments around and looking up how to construct rotation matrices and forgetting that matrices are column-major in GLSL and trying to remember what you stuck in the w component of this vector and, well, you can do it, and lots of people have, but not without losing some of the mathematical elegance of the original, intuitive presentation. Because I really just want to write “gimme three circles extruded along a bezier curve, and rotate them by an angle that varies with the current position along the curve,” you know? (circle 10color (hsv ($i + 3 / 6) 0.6 1)radial: $i 3 5rotate ($t * tau * 4 + t)bezier: $t [-100 0 100] [0 100 0] [100 0 -100] :to (osc t 5 0 1ss 0.1 1)) (You can click to pause any of the animations on this page.) So: Bauble. I wrote Bauble to solve this impedance mismatch, so that I could play around with the SDFs the way that I wanted to play around with SDFs: in a functional, expression-oriented programming language. SDFs are signed distance functions, remember, and primitive operations on SDFs like rotation or translation are literal function composition. You can write a function that takes an SDF and an angle and returns a new SDF – a new function – for the rotated shape. But you can’t write that in GLSL! GLSL doesn’t have first-class functions, so you actually have to do this composition by hand: if you want to rotate a shape, you have to rotate its input coordinate first, then pass the newly-rotated point in space to the SDF. Which, like, is fine, but it’s friction, and that’s the very simplest sort of operation – once you get into more interesting higher-order operations like instanced tiling, the friction stops feeling fine. (gl/def apothem (osc t 15 5 10)) (circle (oss t 7 (apothem * 0.5) (apothem * 2 / sqrt 3))shade (hsv (hash $i + (t / 10)) 0.75 0.8)with-lights (light/point 1 (P + normal))move x (mod $i.y 2 * apothem)tile: $i [(apothem * 2) (apothem * sqrt 3)] :oversample truerevolve zrotate x (t / 20) z (t / 5)intersect (cylinder x 150 20 :r 20) :r 2) (set camera (camera/perspective [403 0 0] :fov 45)) Bauble is not just a higher-level language, though. It’s more accurate to say that I started working on Bauble because I was frustrated with the speed at which I was able to write shaders using SDFs. Not just the verbose manual composition, but the verbose manual composition: it’s hard to compose a detailed scene in pure code! I didn’t only want to make abstract “shadery” looking things. I wanted to be able to make characters too, and that requires a degree of precise and subjective control: I wanted to be able to drag things around, edit shapes interactively, see my shader update live, look at it from different angles… but instead I was over here backspacing over a 1.4, typing 1.5, recompiling, and deciding if it looks better or worse. # \"manta raymarching\" (gl/def bg (ok/mix [0.1 0.1 0.25] sky (ray.direction.yremap+))) (triangle [(ss (q.y * gl/if ( (hash ($i + 100) * (14 / length ($i * [2 1]))) 0.5) (distance q (hash2 $i * 4) - (ss (hash $i) 0 1 1 1 * (ss normal.y 0.5 1))) 1000))tile: $i [4 4] :oversample trueshade whiteextrude y inf)morph (shade r3 gray) :distance 0 :color (1 - occlusion :dist 40ss 0.4 0.5 + (dot normal -ymax 0) )union (ball 2move [61 -3 32]mirror zshade [0 0 0] :g 20 :s 1)rotate x (ss p.z 200 0 1 0 * osc t 3 pi/4 -pi/4)pivot [0 0 10]rotate x (ss p.z -200 0 1 0 * osc (t + (sin t * 0.1)) 3 -pi/4 pi/4)pivot [0 0 -10]rotate z (p.x / 800 * osc t 3 -1 1 + osc t 3 -0.1 0.1)pivot [(oss t 6 -100 100) 0 0]rotate x (osc t 6 -0.1 0.1) y (osc t 12 -0.2 0.2)bound (ball 140move x -20) 20move (hash3 $i * 400 + [(osc t 3 -10 10) (oss t 6 -40 40) 0])gl/let [t (hash $i * 10 + t)] _tile: $i (vec3 700) :limit [50000 8 10] :oversample truemove x (t * 150)map-color (fn [c] (mix c bg (depth / 5000pow 2clamp 0 1)))slow 0.8) (set background-color bg) And the camera, gosh – modeling in 3D with a fixed camera is hard. And – while I realize this sounds really dumb – I think the math to calculate a perspective matrix and position a camera where you want pointed in the direction that you want is actually much harder than any the math related to the actual SDFs that you’re trying to render. So I whipped up a little hack that would basically just concatenate strings of GLSL for me, and put them in a little window with a moving camera. It took me a few days to get it working: I decided that I wanted to use Janet as my “high level” language, because I’d had a positive experience with the language before, and I knew that it could at least in theory run inside the browser. I had never used WebAssembly before, and I had barely used Janet at this point, and even following existing examples I had quite a time getting it to work. Nevermind that it had been almost a decade since I’d done web development seriously, and my sole experience with WebGL at that point was making a few visuals for an old blog post. But I got something working, eventually. Here’s the very first demo I ever recorded of the thing that would become Bauble: Notice the dark, oversaturated colors. I didn’t know I had to do my own gamma correction! This was like my third ever shader. I had no idea what I was doing. But even though this was extremely crude – it was literally GLSL string concatenation, of a few fixed primitive shapes, with no dynamic expressions of any kind – it was already so much better than writing GLSL by hand. Even just being able to type [1 2 3] instead of vec3(1.0, 2.0, 3.0) was worth the time I’d spent on it. And it was fun. There’s something so viscerally satisfying about making something you can touch and play with and see in real time like this. I was having fun working on this little toy, so I kept working on it. I implemented an orbital camera. I switched the editor to CodeMirror, and learned how to write a Janet grammar for it, so that I could directly manipulate the parsed AST to edit values with my mouse (ctrl-click and drag on any number!). With CodeMirror came TypeScript, which I had never used before, and some cruel prank called “rollup,” and I got to experience firsthand the hell of the modern JavaScript ecosystem. I wrote a UI, and decided to try something called SolidJS, which I’ve mildly regretted ever since. (union (revolve shape y radiusmove y (atan2+ p.xz / tau * sep + (round (p.y / sep) * sep)) ) (revolve shape y radiusmove y (atan2+ p.xz / tau * sep + (round (p.y / sep) - 1 * sep)) )let [shape (circle 2shade (hsv (hash $i + hash $j + (t * 0.1)) 0.7 1) :s 1 :g 10with-lights (light/ambient 1 normal)tile: $i [10 10] :limit 4radial: $j 5 50rotate (t / 3))] _gl/let [radius (osc p.y 1000 50 200) sep 146] _rotate y t) Everything was very new and exciting, and I learned a lot about Wasm and Janet and OpenGL and SDFs and procedural art in general. And I kept growing the capabilities of Bauble’s… compiler? Would we call it a compiler? It was still, at this point, a glorified string concatenator. But I taught it how to concatenate real fancy-like; I added support for custom dynamic expressions so that you could write things like “rotate space around the y-axis by an angle that varies with the current y coordinate:” (star 100 50extrude y 100rotate y (osc t 3ss 0.1 0.9 * pi/2 * p.y / 100 + (0.5 * t))slow 0.5) Eventually I even implemented animations, and complex surface-blending operations, and higher-order bounding operations to improve rendering performance, and domain repetition, and, and… And finally my crowning achievement: custom dynamic lighting, with raymarched soft shadows, which you could specify on a shape-by-shape basis, and whose properties could vary over time and space to produce complex, interesting effects. (def light-count 6) (defn light [i] (gl/def at (rotate [0 (osc t 5 20 200) 60] y (i / light-count * tau + t))) (light/point (hsv (i / light-count) 1 1) at :shadow 0.25 :brightness (100 / (dot P atabspow (osc t 3 0.7 1.2))min 2))) (octahedron 20 :r 5rotate x (t + $i) y (t + $i) z (t + $i)shade (hsv $i 0.6 0.5)union (ground -40shade gray)with-lights ;(seq [i :range [0 light-count]] (light i))gl/let [$i (hash $i)] _tile: $i [80 0 80]) It was the most complicated feature of Bauble, one that stretched its string concatenator to the absolute limits, one that had to be special-cased in the typechecker in order to generate correct code, and one that would still occasionally generate invalid GLSL if you looked at it wrong. It was also the last “must-have” feature. Once lighting was done, Bauble was “finished.” I wrote some token documentation, and a little tutorial, and I announced Bauble to the world. I forced myself to stop hacking on it for a little while, because I had more important things to do, and I went outside for the first time in two months. (cylinder y 100 20 :r 20union (cone y 30 50 :r 5) :r 50expound (fbm 5 simplex [p.x p.z (distance p [0 50 0] - (t * 20) + (atan2+ p.xz / pi * -150) )] [50 50 20]) 3shade (mix blue sky 0.5) :g 30 :s 1slow 0.5tint sky (fresnel 5)) (set camera (camera/perspective [-180 100 0])) I didn’t set it aside for long. But when I returned to it, when I looked back over what I had wrought in this furious coding binge, I found… You know that scene in Raiders of the Lost Ark where they open up the roof of the Well of Souls, and they drop a torch down there, and the ground is just a solid mass of writhing snakes? That was basically the codebase that I had produced. (morph 0.88 (ball 40move x 10rotate y (t * 2)move y (osc t 20 -150 150)) (hexagon :r 5 10revolve x (80 + (40 * hash [$i $j]))shade (hsv (hash ($i + $j) * 0.04 - 0.03) 1 0.8) :g 15 :s 0.2radial: $j y 20 :oversample true :sample-from -1)rotate y (p.y / 40sin * (mod $i 2 * 2 - 1))rotate y (mod $i 2 * tau)radial: $i y 2 :oversample true :sample-from -1rotate y (t / 10)with-lights (light/directional white [-1 -2 0normalize] 300 :shadow 0.1) (light/ambient 0.25 normal)slow 0.5) See, the string concatenation never just went away. The whole core “compiler” was still based on this fragile web of carefully crafted, hardcoded GLSL primitives. There was never, at any point, an abstract syntax tree. There was a sort of weird builder-like imperative “code printer” thing that sort of implicitly tracked an AST and like knew what was in scope at some times, but, like, if you ever wrote a function with a local variable called p you’d break everything, because p is, obviously, the name of a dynamic variable that– You know what? I don’t need to explain it. I’m sure you can believe me when I say that it was awful code. (color r2 (teal * 0.1 * (fbm 8 :f (fn [q] (rotate (q * 2) (pi * sin (t / 100)))) (fn [q] (cos q.x + sin q.y /)) q (osc t 20 30 90)))) (set aa-grid-size 2) But it wasn’t just the code. It was also a bad product. It was too limiting: Bauble was a tool for making shaders with SDFs, but it didn’t give you any way to actually write your own signed distance functions. You just couldn’t write arbitrary shader code. You could write some custom expressions, using a limited subset of the functions available to you in real GLSL, but there was no way that you, as a Bauble user, could have implemented any of the provided built-ins. There was no “escape hatch” to pure GLSL. So you were limited to the built-ins, and there just wasn’t that much built-into it. It was missing so many things: I wanted 2D SDFs, and extrusions into 3D space. I wanted to be able to distort normals without altering distance fields. I wanted to be able to define custom material shaders that could use Bauble’s native shadow casting – you could define custom colors of course, but the only light-aware material in all of Bauble was a simple Blinn-Phong shader. And that fact was, of course, hardcoded. (defn strip [axis q] (revolve (trapezoid (mix lo hi h) (mix hi lo h) 20 :r 2rotate (h * pi) tgl/let [lo 0 hi 10 h (atan2+ q / tau)] _) axis 100)) (union (strip y p.xzmove x -50shade sky) (strip z p.xymove x 50shade orange)rotate z (t / 3) y (t / 2)tint purple (fresnel 5 * 0.5)) Maybe more than anything else, I wanted to add 3D mesh export – I wanted to be able to export Bauble shapes into OBJ files or STL files or whatever the right one is today, because I wanted to 3D print my Baubles. But I also wanted to add custom camera support, and anti-aliasing, and video export… But I had stretched my strings to the breaking point. Even I couldn’t understand what I’d written, and I knew that, if I wanted to keep growing Bauble, I would have to rewrite the core compiler from scratch. (gl/let [t1 (t / 4) t2 (ss (fract t1) 0.2 1 (floor t1) (ceil t1))] (defn nudge [i] (hash3 i - 0.5normalizerotate y (osc t2 4 -0.5 0.5) z (osc t2 3 -0.5 0.5) x (osc t2 2 -0.5 0.5))) (intersect :r (s * 20 + 1) (plane [+1 +1 +1 + nudge 0normalize] 80) (plane [+1 +1 -1 + nudge 1normalize] 80) (plane [+1 -1 +1 + nudge 2normalize] 80) (plane [+1 -1 -1 + nudge 3normalize] 80) (plane [-1 +1 +1 + nudge 4normalize] 80) (plane [-1 +1 -1 + nudge 5normalize] 80) (plane [-1 -1 +1 + nudge 6normalize] 80) (plane [-1 -1 -1 + nudge 7normalize] 80)expound (perlin p (20 * s + 30)) (20 * s) 20shade (ok/hcl (t2 * 0.4) 0.4 0.6)with-lights (light/ambient 1 normal)gl/let [s (osc t2 1 0 1)] _rotate [1 -1 -1normalize] (t / 10)tint normal+ (fresnel 3)tint white (fresnel 0.5 * 0.3)map-color (fn [c] (c * (mix 0.1 1 (dot normal [-1 1 1normalize]max 0)))))) Fortunately though, over the course of Bauble’s development, I had produced a comprehensive suite of test scripts with reference images that demonstrated all of the edge cases and problems that I had faced and already fixed and… No, of course not. I can’t even type that with straight fingers. There were no tests. Actually, worse: there was one test. And it was failing. I tried to fix it, when I finally noticed it was broken. I tried to reverse engineer my own code, untangle my spaghetti mess to figure out how it had ever worked in the first place, but eventually I gave up. It just wasn’t worth it. There was nothing worth salvaging, and the thought of starting over from scratch after all of the work I’d already done was so discouraging that I just stopped working on Bauble altogether. And that’s the story of Bauble. It’s a sad story, a story of a codebase collapsing under its own weight, of a prototype trying to grow into a product, and finding that the old aphorism still holds true. (defn half-hour [offset] (capsule y 300 (ss p.y 0 300 0 100) (ss p.y 0 300 80 100)expound (perlin [0 (t + 2log * 300 + offset) 0 + p] 50) 20 10 )) (defn cel [shape color1 color2] (color shape (mix color1 color2 (fresnel 5quantize 3))tint (vec3 -0.3) (fresnel 1quantize 3))) (union :r 10 (half-hour 0move y 4cel sky white) (gl/with [p [1 -1 1 * p]] (half-hour 1000move y 0cel orange red))scale [1 0.5 1]) (set camera (camera/perspective [0 100 400])) Two years passed. (color r2 (ok/mix (ok/hcl (length qss 0 150 0.2 0.4 + (t / 30)) (length q / 200ss 0 1 0.3 0.1) 0.9) (vec3 0.1) (fbm 3 perlin (normalize [q 10] * (ss t 0 20 0 20 + t)) [(vec2 (length qsqrt)) 6] + (length q / 150)))rotate (t / 20)) I used Bauble on and off, but found myself increasingly annoyed by its limitations. Occasionally I would even try adding new features, but I could barely type through my hazmat suit. I kept meaning to write a blog post about Bauble, about everything that I’d learned – how to embed Janet into a website and make an interactive art project that doesn’t use JavaScript – but I never got around to it. Meanwhile I wrote a book about Janet, and dedicated a chapter to my embedding experience, but it never even mentions Bauble. Despite being the most interesting side project that I’ve ever worked on, I haven’t written anything about it until now. (gl/def sun-dir [0 -0.5 -1normalize]) (defn broad-height [xz] (perlin xz 10000 * 2000)) (gl/def sky-color (mix [1 0.5 1] [0 0 0] (ray.direction.y * 2) * 0.5mix (hsv (1 / 6) 0.5 1) (dot ray.direction (- sun-dir)clamp 0 1))) (plane y (perlin p.xz 600 * 200 + broad-height P.xz)expound (osc p.y 30) 10shade (hsv (0.6 / 6) 0.6 0.5 + (normal.yzx * 0.1)) :g (normal.y * 10) :s 0.1with-lights (light/directional 1 sun-dir 500 :shadow 0.5) (light/ambient (hsv 0.3 0.3 0.5) normal)map-color (fn [c] (mix c sky-color (1 - exp (* -0.0001 depth))pow [1.5 2 1.5]))slow 0.9) (set background-color sky-color) (gl/def camera-xz [(t * -500) 0]) (set camera (camera/perspective [camera-xz.x (broad-height camera-xz + 500) camera-xz.y] :dir [-1 -0.5 -0.10normalize]camera/tilt (osc t 20 -0.1 0.1)camera/pan (osc t 30 -0.1 0.1))) I’m writing about it now because I recently took a few months off work, for the usual reason, and after some weeks of sleep deprivation and exhaustion and elation I found that I had a few cycles to spend on a side project. But not a hard side project. Not something that required concentration, or prolonged stretches of focus – luxuries that I have temporarily foresworn. I needed something that I could, almost literally, do in my sleep. (torus y 100 50expound (fbm 6 simplex p [100 50 100]) (osc t 10ss 0.25 0.75mix 1 (simplex+ (p + 1000 + (t * 10)) 300ss 0.2 1 * 15 + 1) _) 16shade 0.8 :g 40 :s 0.5rotate y (t / 2.5)slow 0.4tint [1 0.5 0.5] (fresnel 0.25 * 0.1)tint white (fresnel 5 * 0.1)) (set camera (camera/perspective [0 250 300rotate y (t / 2)])) So I rewrote Bauble. Or rather, I didn’t rewrite Bauble – instead, I did all of the boring things that I never bothered with the first time around. I wrote a GLSL AST library, with a little pretty-printer. I wrote a typed expression-oriented language that adds “first-class” functions to GLSL. I wrote a Janet DSL for constructing programs in this high-level language, and I added Janet wrappers for (almost) all of GLSL’s built-in functions. I made a command-line interface to Bauble, using vanilla OpenGL instead of WebGL, so that I could finally write a real test suite. I wrote a real test suite. (def mouth (ball 50subtract (ball 200move y 200)move z 100shade black)) (defn fork [shape f1 f2] (union (f1 shape) (f2 shape))) (def teeth (box :r 2 [5 10 2]rotate x 0.30move y -40fork (fn [$] ($move x 6)) (fn [$] ($rotate x -0.04move x -6 z -1))shade white :g 10 :s 1 )) # :ambient 0.5 (def eye-center [39 265 41]) (def eyelid (ball 40move eye-centersubtract :r 10 (ball 30move eye-centermove [0 -10 16]))) (def eye-color (shade r3 white :g 10 :s 1 # :ambient 0.3union (ball 10move z 30shade black :g 10 :s 1))) (def eyeball (ball 35shade white :g 10 :s 1union-color (ball 10move [0 0 30]shade black :g 10 :s 1)rotate x 0.34 y (sin tss 0 0.1 * 0.2 - 0.1)move eye-center)) (def neck (cylinder y 30 100move y 100rotate z (p.y * -0.001)rotate x (p.y * 0.0010)move y 85union :r 10 eyelid)) (def head (ball 100scale y 0.9)) (def feet (box [20 30 20] :r 15union :r 15 (box :r 10 [10 20 30]fork (fn [$] ($rotate y 0.1move x 8)) (fn [$] ($rotate y -0.1move x -11))rotate x 0.015 y 0.37move [15 -31 22])move [50 -82 0]rotate y 0.12subtract :r 5 (plane y -110)mirror x)) (union (union :r 60 head neckunion :r 10 feetshade green :s 0.5 :g 6tint white (fresnel 15 * 0.5)subtract :s 2 mouth) eyeball (teethmove z 80) (ground -110shade gray)scale 0.5) I worked bottom-up this time, building one boring primitive at a time and stacking them on top of each other. It was not the joyful exploratory everything-is-new interactive process of building Bauble for the first time, but it was still rewarding: I could see where it was going, and how to get there. It was delayed gratification this time, knowing that if I just got through the slog of the rewrite, I would be rewarded with something that I could be proud of. (union :r 10 (cone y 120 200) (cone y 100 150move [80 0 -44]) (cone y 89 137move [-110 0 -9])expound (fbm :f 2.2 5 perlin [2 1 2 * p] 80) 20 40slow 0.5shade (normal+rotate y 1.34pow 2)move y (osc t 10ss 0 0.8 -230 -50)union :s 20 (plane y (osc (perlin [1 2 * p.xz] 200 + (t * 0.5)) 1 0 10)shade sky :g 20 :s 1tint (fresnel 3)) ) And I am, now. This is a new Bauble, and it is, across every axis, a better Bauble. You’ve already seen it of course, but let me give you a quick tour of what you can do with it now. You can edit complicated shaders without lag: Bauble uses “web workers” now, so that all of the Janet evaluation and compilation and rendering takes place off of the UI thread. This… this doesn’t actually seem to work very well in Chrome or Safari, at least on my “Apple Silicon” MacBook – recompilation is pretty stuttery, taking around 100ms with both the OpenGL and Metal backends. But it’s buttery smooth in Firefox. Weird. WebGL rendering performance is also just universally better in Firefox – if any of the examples on this page are dipping below 60fps, maybe try switching? You can export 3D models, and you can 3D print them: That shape on the left is called a “gyroid,” and it was the first Bauble that I ever 3D printed (or, well, had someone cast in bronze for me). There’s no gyroid primitive in Bauble, but you can create custom shapes by writing out an implicit function directly: (def gyroid (shape/3d (gl/with [p (p / 15)] (dot (cos p) (sin p.yzx)) + 1 * 10))) (intersect :r 2.5 gyroid (ball 145)) # Er okay so this is not a \"real\" gyroid; # it's more like a half-filled gyroid, # because it's hard to print thin walls. # The real deal looks like this: # (def gyroid (shape/3d (gl/with [p (p / 15)] # (dot (cos p) (sin p.yzx)) * 10))) # # (intersect :r 2.5 (gyroidshell 1) (ball 145)) I don’t have a 3D printer, and this feature is pretty new, so I haven’t really explored this very much yet. Also Bauble’s mesh export is… primitive, to say the least. It’s just marching cubes, which means you have to generate pretty large models if you want to preserve fine details. I realize that there are many better algorithms for triangulating SDFs, and Bauble should probably use one of them. But… I can only code for a few minutes a week now, and I spent my whole budget on the next feature. You can embed Bauble on other pages. Not the way that I’ve been doing – the crimes I committed to Bauble’s build system in order to embed the editor here are not really replicable. But you can export your shaders to GLSL, and embed them on any page to add interactive 3D examples in a few lines of code. No one even needs to know you’re using Bauble: How do planet work? Innard Since Bauble pre-compiles the shader, the actual “bauble.js” that you have to embed is just a single 8kb pure-JS file.1 You don’t need to include the Janet compiler or WebAssembly or anything fancy like that – in fact, you don’t even have to use the Bauble library at all. You can construct the graphics context and compile the shader and draw it yourself, if you’d like. There’s a biannual event called the “lisp game jam,” and I think it would be fun to use Bauble to render the graphics for a game. Janet has pretty good bindings to Raylib, and you could use that to handle the input and sounds, but render all the graphics with Bauble. Here, click on this, and then move around with WASD: Obviously that’s not… a game, exactly. There’s no hit detection, and fire probably shouldn’t cast shadows. But, you know, that’s 30 lines of Bauble code plus 40 lines of JS for the event handling? Imagine what you could do if you weren’t furiously trying to finish the blog post you started writing months ago. One of my favorite new features of Bauble is that you can edit vectors interactively. Not just the ctrl-click-and-drag on scalars that I mentioned already, but actual dragging vectors around in 3D. Here: put your cursor inside the [50 100 150], and then open quad view with alt-q. You should see crosshairs, and then you can cmd- or ctrl-click and drag one of the orthographic viewports to edit the vector with your mouse. Try it on the [0 0 0] too, to move the box around! (box [50 100 150]move [0 0 0]) So that’s everything you can do with Bauble. Except… it’s not, is it? I just listed all of the things that I can do with Bauble. Because I know some things about SDFs, and I know Janet, and I understand this weird DSL that I’ve created. But you don’t – yet. How could you? Which brings me to the last, biggest, and most important new feature of Bauble: This is https://bauble.studio/help. I wrote a giant reference page with hundreds of interactive examples of every primitive and operation and thing that you can possibly do with Bauble. And it’s available right in the editor, any time you trigger autocomplete: the reference page and completions are both generated from the docstrings of the actual Janet functions. And in case the docstrings aren’t sufficient, there is a little source link next to every single definition that will take you straight to the code. The documentation isn’t perfect: some some small helpers are missing examples; the “escape hatch” to writing raw GLSL isn’t really described at all, and it doesn’t include any of the functions that Bauble lifts directly from GLSL. And I fully realize that a reference like this is no substitute for a decent tutorial. Bauble still needs a proper tutorial, and one day I’ll write it. One day I’ll write the Book of Bauble, and explain SDFs and procedural noise and periodic distortions of space and all the tricks that I’ve learned, and how you can apply them to Bauble. One day. Let’s say… eighteen years from now, just to be on the safe side. This is a little misleading, because the compiled shaders themselves are like 5-15kb each. I could minify their source, which would help a bit, but even without doing that, a single Bauble and the player library clocks in around 2% the size of embedding p5.js. (Comparing minified, uncompressed sizes, which is, again, misleading.) ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=42660942",
    "commentBody": "Building Bauble (ianthehenry.com)164 points by ianthehenry 20 hours agohidepastfavorite26 comments peterkos 12 hours agoAfter reading this, I am now of the belief that software is, actually, a good thing. And that programming can be enjoyable. Seriously, this does such a good job of capturing the feeling of MAGIC that code is capable of -- both in its process and in its output. Textbook \"craft\". It's hard to experience that sometimes when surrounded by dependency hell, environments, build systems, certain dynamic programming languages, and the modern web ecosystem. reply troad 17 hours agoprevThis is so cool. I love this so much. I read the article on my RSS, and then immediately hopped over to HN just so I could upvote. > Fortunately though, over the course of Bauble’s development, I had produced a comprehensive suite of test scripts with reference images that demonstrated all of the edge cases and problems that I had faced and already fixed and… No, of course not. I can’t even type that with straight fingers. There were no tests. Intensely relatable. reply xrd 15 hours agoparentWhat RSS reader do you use, may I ask? Mine (readrops for Android) doesn't render the images. It is probably quicker to just review the XML but I'm committed to this comment now. reply troad 13 hours agorootparentNor did mine, interestingly. I'm not entirely sure why, the HTML for the first image seems to render fine on its own. It's a very longtag, seemingly optimised to return different sizes at different resolutions. It looks like something generated by a web framework. I still hand-write mytags like it's 1999, so I'll leave it to the frontend wizards to explain what the problem here is. Re my choice of reader, I host FreshRSS[0] on a home server, using the official Docker image.[1] It comes with pretty good in-built webpage change tracking too, for websites that refuse to offer RSS. I don't feel confident enough to expose it to the Internet, though I imagine you could use something like Tailscale to tunnel home securely for it. [0] https://freshrss.org/; https://github.com/FreshRSS/FreshRSS/ [1] https://hub.docker.com/r/freshrss/freshrss/ reply xrd 11 hours agorootparentFunny, I'm using FreshRSS as my feed aggregator as well, but then reading it using the android app. One of us should file a bug. reply troad 7 hours agorootparentThat is amusing! I'd think that your Android app would be using a different rendering engine to my desktop browser though? There must be something non-standard about those images. reply Multiplayer 3 hours agoprevBeen here for 15 years. This is the most interesting project and writeup I have ever seen here. reply skalarproduktr 2 hours agoprevCongrats, Ian, on the amazing side project and the thorough and thoroughly enjoyable writeup. You really got me with the tests for the original Bauble, I had a good laugh after that paragraph! I’m now also somewhat inclined to have a deeper look into Janet, it looks both useful and interesting. reply xnx 17 hours agoprevWow. Very impressive work and a very readable and interesting summary. You should be immensely proud. Personally, it is reassuring to know there are still people out there capable of doing such complicated and quality work. After seeing my 1000th $19/month thin wrapper over uncredited AI, I had doubts. reply uxcolumbo 6 hours agoprevThis is awesome and inspiring. Thanks for the write up. I like the look and style of the game art and making it interactive, somewhat makes me think of the game 'The last night' (still in development). Your post made me curious about Janet. Will explore it more and how it can be used for graphics and art. reply agentkilo 10 hours agoprevThis article is really inspiring! I have a few dozen half-baked projects, that's meant for no one else but me, to play with, or to \"learn new things\" from, but none of them ever became so complete and polished as author's Bauble. I can feel the pure enjoyment of crafting from the article. I hope more people can find out about it, and share OP's joy. I should really finish what I started. It may take, say, eighteen years, but I should finish them XD reply tempodox 2 hours agoprevFuck Apple, I'm only getting “TypeError: undefined is not an object (evaluating 'renderer.recompileShader')” in Safari. It works in Firefox. reply ianthehenry 2 hours agoparenthuh, that's no good. it works fine for me on safari 18.1. would you share the line that it's raising on? no other errors before that one? reply tempodox 1 hour agorootparentNo usable line, only a URL: https://ianthehenry.com/posts/bauble/building-bauble/bauble.... After that, is shows line number 1 and a column number in the millions, so seems to be megabytes of code in that JS file. For each code box, I get two errors with the same URL, just different column numbers: - TypeError: undefined is not an object (evaluating 'renderer.draw') - Unhandled Promise Rejection: Error: failed to create webgl2 context This is Safari 18.2 on Ventura (Intel), which seems to be a buggy release all around. Even the scroll bars are broken. reply ianthehenry 1 hour agorootparentokay, that's helpful. the \"undefined is not an object\" bit is definitely a function of the \"failed to create webgl2 context\" error -- it tries to reference it unconditionally. no idea why it can't create the graphics context in the first place, though (and not much to do without one) reply spiralganglion 2 hours agoparentprevWorked on my iPhone. reply tempodox 2 hours agorootparentWhat iOS version? I'm on macOS 13 (Ventura). reply FragenAntworten 20 hours agoprevThis is both a great demo of what Bauble can do and an engaging description of its development. I'm impressed by both! reply medhir 9 hours agoprevAbsolutely incredible, the power of persistence can yield such cool results. I bookmarked to play around with the editor sometime, I’ve always wanted to learn GLSL and this feels like a more gentle intro into the world of shaders. reply flir 4 hours agoprevThat was fantastic on, like, eight different levels. reply atombender 8 hours agoprevThe die cast bronze sculptures are very cool. Where does one go to get them cast from 3D models? Sounds very expensive to do as a one-off (the author is a developer at Jane Street, which probably explains why they could afford it). reply ianthehenry 4 hours agoparentI ordered them from sculpteo — the only service I could find that would do it. Very happy with everything I’ve gotten from them. It is very very expensive if you want to make anything large, but for small jewelry-scale stuff it’s not too crazy. The balloon is seven custom parts so that was pricey ($250 total?), but the other two models were around $50 each. reply atombender 2 hours agorootparentThat's surprisingly cheap for a one-off. Thanks, I will have to check them out. reply keeptrying 15 hours agoprevBeautiful reply dinkumthinkum 17 hours agoprevThis is very nice. I remember first watching Inigo Quilez videos and they are extremely impressive but this, obviously, gives a little more insight into making something work. Well done. reply mrayycombi 17 hours agoprev [–] 3d latex reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bauble is a 3D modeling and rendering tool that utilizes signed distance functions (SDFs) to create complex shapes and animations. - Initially a basic system for generating GLSL shaders, Bauble has been rewritten to include features like 3D model export, interactive editing, and custom lighting. - The tool now supports web embedding, interactive vector editing, and offers comprehensive documentation, making it accessible for creative projects and users interested in 3D design."
    ],
    "commentSummary": [
      "Building Bauble, a project featured on ianthehenry.com, has generated excitement by showcasing the magic and enjoyment of programming, particularly in coding and graphics.",
      "The project is praised for its creativity and detailed writeup, inspiring readers to revisit unfinished projects and explore new technologies such as Janet (a programming language) and GLSL (OpenGL Shading Language).",
      "Despite some technical issues on Safari, the project is celebrated for its impressive execution and engaging development story, sparking discussions about personal projects and the use of RSS readers."
    ],
    "points": 164,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1736547735
  },
  {
    "id": 42664400,
    "title": "Nearly all binary searches and mergesorts are broken (2006)",
    "originLink": "https://research.google/blog/extra-extra-read-all-about-it-nearly-all-binary-searches-and-mergesorts-are-broken/",
    "originBody": "Home Blog Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken June 2, 2006 Posted by Joshua Bloch, Software Engineer Quick links Share Copy link × I remember vividly Jon Bentley's first Algorithms lecture at CMU, where he asked all of us incoming Ph.D. students to write a binary search, and then dissected one of our implementations in front of the class. Of course it was broken, as were most of our implementations. This made a real impression on me, as did the treatment of this material in his wonderful Programming Pearls (Addison-Wesley, 1986; Second Edition, 2000). The key lesson was to carefully consider the invariants in your programs. Fast forward to 2006. I was shocked to learn that the binary search program that Bentley proved correct and subsequently tested in Chapter 5 of Programming Pearls contains a bug. Once I tell you what it is, you will understand why it escaped detection for two decades. Lest you think I'm picking on Bentley, let me tell you how I discovered the bug: The version of binary search that I wrote for the JDK contained the same bug. It was reported to Sun recently when it broke someone's program, after lying in wait for nine years or so. So what's the bug? Here's a standard binary search, in Java. (It's one that I wrote for the java.util.Arrays): 1: public static int binarySearch(int[] a, int key) { 2: int low = 0; 3: int high = a.length - 1; 4: 5: while (lowkey) 12: high = mid - 1; 13: else 14: return mid; // key found 15: } 16: return -(low + 1); // key not found. 17: } The bug is in this line: 6: int mid =(low + high) / 2; In Programming Pearls Bentley says that the analogous line \"sets m to the average of l and u, truncated down to the nearest integer.\" On the face of it, this assertion might appear correct, but it fails for large values of the int variables low and high. Specifically, it fails if the sum of low and high is greater than the maximum positive int value (231 - 1). The sum overflows to a negative value, and the value stays negative when divided by two. In C this causes an array index out of bounds with unpredictable results. In Java, it throws ArrayIndexOutOfBoundsException. This bug can manifest itself for arrays whose length (in elements) is 230 or greater (roughly a billion elements). This was inconceivable back in the '80s, when Programming Pearls was written, but it is common these days at Google and other places. In Programming Pearls, Bentley says \"While the first binary search was published in 1946, the first binary search that works correctly for all values of n did not appear until 1962.\" The truth is, very few correct versions have ever been published, at least in mainstream programming languages. So what's the best way to fix the bug? Here's one way: 6: int mid = low + ((high - low) / 2); Probably faster, and arguably as clear is: 6: int mid = (low + high) >>> 1; In C and C++ (where you don't have the >>> operator), you can do this: 6: mid = ((unsigned int)low + (unsigned int)high)) >> 1; And now we know the binary search is bug-free, right? Well, we strongly suspect so, but we don't know. It is not sufficient merely to prove a program correct; you have to test it too. Moreover, to be really certain that a program is correct, you have to test it for all possible input values, but this is seldom feasible. With concurrent programs, it's even worse: You have to test for all internal states, which is, for all practical purposes, impossible. The binary-search bug applies equally to mergesort, and to other divide-and-conquer algorithms. If you have any code that implements one of these algorithms, fix it now before it blows up. The general lesson that I take away from this bug is humility: It is hard to write even the smallest piece of code correctly, and our whole world runs on big, complex pieces of code. We programmers need all the help we can get, and we should never assume otherwise. Careful design is great. Testing is great. Formal methods are great. Code reviews are great. Static analysis is great. But none of these things alone are sufficient to eliminate bugs: They will always be with us. A bug can exist for half a century despite our best efforts to exterminate it. We must program carefully, defensively, and remain ever vigilant. Update 17 Feb 2008: Thanks to Antoine Trux, Principal Member of Engineering Staff at Nokia Research Center Finland for pointing out that the original proposed fix for C and C++ (Line 6), was not guaranteed to work by the relevant C99 standard (INTERNATIONAL STANDARD - ISO/IEC - 9899 - Second edition - 1999-12-01, 3.4.3.3), which says that if you add two signed quantities and get an overflow, the result is undefined. The older C Standard, C89/90, and the C++ Standard are both identical to C99 in this respect. Now that we've made this change, we know that the program is correct;) Resources Programming Pearls - Highly recommended. Get a copy today! The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri discussing a related problem - The problem is a bit more general but perhaps less interesting: the average of two numbers of arbitrary sign. The paper does not discuss performance, and its solution is not fast enough for use in the inner loop of a mergesort. Quick links Share Copy link ×",
    "commentLink": "https://news.ycombinator.com/item?id=42664400",
    "commentBody": "Nearly all binary searches and mergesorts are broken (2006) (research.google)142 points by thunderbong 10 hours agohidepastfavorite135 comments jsnell 7 hours ago(2006) Past discussions: https://news.ycombinator.com/item?id=33492122 https://news.ycombinator.com/item?id=14906429 https://news.ycombinator.com/item?id=12147703 https://news.ycombinator.com/item?id=9857392 https://news.ycombinator.com/item?id=6799336 https://news.ycombinator.com/item?id=1130463 https://news.ycombinator.com/item?id=621557 reply motorest 7 hours agoparent> (2006) In the very least,this should feature in the title. In fact, previous submissions from over a decade ago also feature year in the title. Otherwise it conveys the idea that this is some major epiphany. reply brandall10 2 hours agorootparentConsidering how often this happens, I'm surprised HN doesn't have a feature to flag and index a historically resubmitted article, whether externally by users or internally by the admin team. Then it could have a bot create links to past submissions like the OP did and use the best arrived at title for resubmissions. It does have the ability to catch multiple submissions of new articles but that probably has a time window of a day or so. reply kurthr 1 hour agorootparentOne problem would be that you can't just check the URL, you'd have to check the content. Not only are there many URLs that could point to the same content, but content on a page can obviously change. I suppose you could compare against wayback, and not sure I'd try to compare with an LLM or RAG. reply xmcqdpt2 5 hours agorootparentprevAlso, I didn't know about this specific bug but I spotted it almost immediately while reading the code. This is not because I'm some kind of 10x programmer but because Integer.MAX_VALUE bugs while processing arrays are actually fairly common in java programs in 2025, I fixed one a few weeks back and it's something I look for in code reviews. I guess it would have been surprising in 2006? reply brabel 5 hours agorootparentIn 2006, if you tried to create an array with 2^30 elements you would just get an OutOfMemoryError almost anywhere. reply masklinn 4 hours agorootparentAMD's 64b CPU were released in 2003, Intel followed up in 2004, and in 2006 launched their first 64b laptop chip (with Core 2). By then the ability to reasonably allocate more than 1GB in one shot was becoming quite widespread (the mid 2007 MBPs could be upgraded to 6GB RAM). And obviously Google would have been dealing with servers, where it'd be even older as they'd probably have been using PAE and each process would be able to allocate and address a full 4GB. reply brabel 1 hour agorootparentNot sure if you're a troll... but for general info, in 2006 most desktop computers (laptops were still not so common) had something like 100MB of RAM if you were lucky. Maybe Google already had some huge machines with 1GB+ but that was not at all a common thing outside supercomputers. reply aspenmayer 1 hour agorootparentI think you’re off by a decade or so. Common consumer laptops had 2-4 GB of RAM in 2006. https://everymac.com/systems/apple/macbook_pro/specs/macbook... reply brabel 59 minutes agorootparentJust like today you can buy machines with 128GB of RAM... but that doesn't mean that's what people were using... a lot of people buy machines with 4GB today (just checked the most popular website in my country, lots of the offers only have 4GB: https://www.elgiganten.se/datorer-kontor/datorer/laptop). I remember pretty clearly that if you had anywhere above 512MB of RAM in 2006 you had very much top-of-the-line. reply aspenmayer 48 minutes agorootparent> I remember pretty clearly that if you had anywhere above 512MB of RAM in 2006 you had very much top-of-the-line. That’s a different claim than your original statement of having 100MB max. reply saalweachter 40 minutes agorootparentprevEh, 2006 was right on the cusp of the 32-bit/64-bit flip. 32-bit was still pretty common, but my first task at my first job that year was migrating a search app to compile as a 64-bit binary, for servers that had (I believe) 8GB. They were good-sized servers for the time, but not particularly exotic, commodity hardware at a startup. reply Smithalicious 1 hour agorootparentprevYou think Windows Vista ran on 100MB of RAM? I wish! reply brabel 58 minutes agorootparentI was still on Windows 98 :D, that could easily run on 64MB. reply joshka 7 hours agoprevThe problem with this article's name is that we're in an era where actually checking whether \"nearly all sorts are broken\" against all publicly available implementations of binary searches would be almost feasible, but that's not actually what the article is claiming. > Moreover, to be really certain that a program is correct, you have to test it for all possible input values, but this is seldom feasible. This is incorrect. Generally it's just a little excessive to try to solve the halting problem in a library's unit tests ;). You don't have to test a program for all possible inputs, only for all materially unique state transitions. In a binary search, each variable can be zero, one, some(*), max, overflowed. The combination of these is not infinite as the values of some that cause different behavior is much more reasonable when grouped into chunks of the same behavior. reply PaulKeeble 6 hours agoparentIt was certainly possible to run the algorithm on 16GB of array before the moment when it happened but did the original developer have that sort of space on their desktop at the time? Possibly not. If a unit test only runs on a server and not on the laptops of the developers then its not going to be written, whereas ideally someone should write a test that is tagged to only run on the server but that is a lot of extra work if that isn't a common thing on a project. Even now I would be quite wary of producing a max size input test for an array of ints and especially objects, that is going to raise some questions due to being a slow test and a highly resource consuming one. If I was worrying about the same in aerospace programming however then no question that test would get written and we would ensure it got run on the hardware that it was designed to run on. In typical business software its less common to run potentially very long running tests and simulations of the machines states everyone wants to go faster than that especially for the sake of a max input test. reply xmcqdpt2 5 hours agorootparentThe correct way to test that code is to write a version that doesn't take an actual array but an index -> int function, then you wouldn't need to instantiate the array at all. reply miohtama 5 hours agorootparentprevAlso you can have checked math, like in Rust, and automatically just crash when you overflow a variable. In this case, it's not a bug (cannot get incorrect result) but an unsupported input. reply poincaredisk 6 hours agorootparentprevSmall nitpick: you don't need 16GiB, 2^31 bytes is \"just\" 2GiB. Doesn't contradict your point though. reply penteract 4 hours agorootparentEach element of the input array is at least 4 bytes, bringing it to 8GiB. reply javcasas 16 minutes agorootparentNot necessary. But, still, my work laptop from 2020 has 32Gb of memory. So, not that implausible. reply rileymat2 2 hours agoparentprev> In a binary search, each variable can be zero, one, some(*), max, overflowed. The combination of these is not infinite as the values of some that cause different behavior is much more reasonable when grouped into chunks of the same behavior. You are presuming that the algorithm is correct in the first place. Contrived example. binary_search(array, key) { if (key == 453) explode_world() //Do correct binary search. } So you also need to prove explode_world is not called or something similar but less contrived is not in there. reply MaxikCZ 7 hours agoparentprevAre you quantizing information? reply LiamPowell 4 hours agoprevThis bug, and many others, can be detected with a trivial amount of formal verification. I really think formal verification should see much wider use for things that see as much use as standard libraries, even if it's just for the trivial things like overflow and out-of-bounds access. In the below code we can see a formal verification tool (GNATProve) detect both the original error and the out-of-bounds access that it causes. Doing the arithmetic using a larger type clears both reported errors without the need for any additional annotations for GNATProve. function Search (A : A_Type; Target : Integer) return Integer is Left : Integer := A'First; Right : Integer := A'Last; begin while LeftTarget then Right := Mid - 1; end if; end; end loop; end Search; GNATProve output: Phase 1 of 2: generation of Global contracts ... Phase 2 of 2: flow analysis and proof ... wrapper.adb:12:36: medium: overflow check might fail, cannot prove lower bound for Left + Right 12Mid : Integer := (Left + Right) / 2;~~~~~~^~~~~~~~ reason for check: result of addition must fit in a 32-bits machine integer wrapper.adb:12:45: info: division check proved wrapper.adb:14:19: medium: array index check might fail 14if A (Mid) = Target then^~~ reason for check: value must be a valid index into the array reply jheriko 4 hours agoparentcan also be spotted with experience. these kinds of problems are present in very many standard treatments of algorithms and don't survive their first contact with real world use in some cases. \"needs a carry bit or a wider type\" is common for arithmetic operations that actually use the range. reply djoldman 1 hour agoprev> The bug is in this line: > In Programming Pearls Bentley says that the analogous line \"sets m to the average of l and u, truncated down to the nearest integer.\" On the face of it, this assertion might appear correct, but it fails for large values of the int variables low and high. Specifically, it fails if the sum of low and high is greater than the maximum positive int value (231 - 1). The sum overflows to a negative value, and the value stays negative when divided by two. In C this causes an array index out of bounds with unpredictable results. In Java, it throws ArrayIndexOutOfBoundsException. At some point we have to draw an arbitrary line. Even an \"arbitrary precision\" calculation is bounded by system memory. \"bug\" is not well-defined, or perhaps \"bug\" is more of a continuous label as opposed to discrete. reply bhouston 6 hours agoprevIt makes the case that we need Math.mean or Math.avg function that we can use in these cases rather than than reinventing it. Basically we should favor using built in functions for as much as possible because those should be reliable and tested more than ad hoc code we write. And compilers should optimize those built in functions well so there is no extra cost in using them. reply sltkr 3 hours agoparentC++ added std::midpoint() to the standard library: https://en.cppreference.com/w/cpp/numeric/midpoint Another fun case, besides integer overflow, is negative values: in Java and C/C++ (i + j)/2 will round towards j, but i + (j - i)/2 will round towards i instead. Sometimes the difference matters. reply ape4 5 hours agoparentprevI was thinking the same. And the code would be clearer too. reply heinrichhartman 7 hours agoprevint mid =(low + high) / 2; > Specifically, it fails if the sum of low and high is greater than the maximum positive int value (2^31 - 1). I would really challenge calling this kind of effects \"bug\" or \"breakage\". It's like calling Newtons law of gravity broken, because it's not accurate at predicting how galaxies move. Things are engieered and tested for a certain scale. Knowing which tools to use at which sacle is part of the craft of engineering. reply wat10000 2 hours agoparentSometimes they’re engineered and tested for a certain scale. More often they’re engineered and tested for an arbitrary scale. The limits aren’t considered, behavior at the edges isn’t accounted for, and it’s assumed it will be good enough for real world inputs. The use of `int` tends to be a dead giveaway. There are some cases where it’s clearly correct: where the spec says so (like argv), where you’re starting from a smaller type and it’s impossible for the calculations to overflow in an int (like adding two uint8), that sort of thing. And there are cases where it’s subtly correct, because you know the range of the value is sufficiently limited, either by mechanics or by spec. But most of the time, int gets chosen because it’s the apparent default and it’s easy to type. No analysis has been done to see if it’s correct or if you want to declare your code to only support inputs in a certain range. It’s really clear if you’ve written a binary search (or anything else that works on general arrays) in C and you use int as the index type. There’s pretty much no scenario where that makes sense. In theory you could analyze the entire program and prove that over-large arrays are never passed in, and keep doing it to make sure it stays that way, but that’s not realistic. If the programmer actually took one second to think about the appropriate data types, they’d use size_t rather than int. You can still have this bug with size_t, of course. But it won’t be “this falls apart with arrays over 1G elements on 64-bit systems that can easily handle them.” If you declare that you wrote the obvious midpoint calculation with size_t because you didn’t intend to support byte arrays larger than half the address space, it’s at least plausible. reply tehjoker 52 minutes agorootparenti write c++, but i had to teach myself and always wondered why others use imprecise types. portability is one possibility, but then you can't know if your datastructure will break for a given input reply wat10000 30 minutes agorootparentHistory and tradition at this point. Bit-sized integers and the other “meaningful” integer types like size_t weren’t added to the languages themselves until C99 and C++11. A lot of us learned those languages before that, and lots of code still exists from that time, or at least code bases that have evolved from that time. I think it actually comes from the opposite of portability. Access to different kinds of systems wasn’t common then. If you were learning and working on a system where int is 32 bits and pointers are 32 bits, and other possibilities are just vague mentions in whatever books you’re learning from, it’s very easy to get into the habit of thinking that int is the right type for a 32-bit quantity and for something that can hold a pointer. reply alanfranz 5 hours agoparentprev> certain scale Make it explicit. If the array is too large, throw an IllegalArgumentException. Document the limit. Then I agree with you. Otherwise, if an allowed input crashes the program at runtime with a random exception, I respectfully disagree. reply brabel 5 hours agorootparentThen you should absolutely stay away from C :) reply alanfranz 26 minutes agorootparentI do. But the example was Java. reply jldugger 2 hours agorootparentprevWords to live by! reply javcasas 49 minutes agoparentprev> Certain scale Or just put the algorithm in a 16-bit microcontroller, put some table that needs to be looked up (think precomputed sine table), put that table near the end of the memory range, and just make the mistake to call binary search specifying the start and end memory positions of the table. reply perching_aix 5 hours agoparentprevThese implementations are definitely broken when the specification goes like \"you just pass in your array here and it will perform binary search on it for your value.\" Yes, you could constrain this spec, but come on... It's such a blatant example for a bug, that I struggle to believe how can anyone even remotely conceive and pivot to the idea that \"nuh-uh, this is a bad spec not a bad implementation!\". reply ajuc 6 hours agoparentprevThe problem is that it's not that much harder to make it work for all the valid inputs. Not doing that is not good enough. Another example is summing lots of floats naively instead of using Kahan's algorithm. It's like we had a theory of gravity that doesn't work on Mars because we have unnecessary division by (m-Mars' mass) in our laws :) It wouldn't be good physics. reply xigoi 4 hours agorootparentIs it worth making the algorithm slower just to have it work on extreme edge cases? reply javcasas 45 minutes agorootparentIs it worth to make the algorithm faster at the cost of throwing surprise OutOfBounds exceptions in some extreme edge cases? Maybe, but only if you - and only you,and not an attacker can control the case you are in. reply xigoi 42 minutes agorootparentIf an attacker can somehow make sure that there is an array with 2³⁰ elements, you have worse problems than a binary search crashing. reply javcasas 18 minutes agorootparentWhy do you think this algorithm only applies to arrays? Why do you think this algorithm doesn't apply to this sine lookup table that the compiler placed at the end of the memory in the microcontroller? reply tempodox 3 hours agorootparentprev+1 for mentioning Kahan. reply croemer 4 hours agorootparentprevNice example with the 1/(m-m_mars)! reply secondcoming 6 hours agoparentprevI would disagree. The inputs to these functions are user controlled and so can be forced to break, whereas humans cannot change how gravity works. reply dzaima 4 hours agorootparentBut in what realistic scenario would a user be able to put in ≥2^31 while not being able to put in 2^32-1 (which probably breaks many more things from innocent increments or similar) or 2^100? reply wat10000 2 hours agorootparent2^32-1 is almost always a possibility when 2^32 is, but there are many cases where those are possible but 2^100 is not. Basically anything where the value is a count of something rather than raw input fits the bill. How many characters, lines, or files do you support? 2^32 is a totally feasible number in many contexts. 2^100 is physically impossible, there isn’t enough matter. reply dzaima 1 hour agorootparentIf you accept 2^32, then code using 32-bit ints is definitely broken on it and thus the OP question of the issue on half that is irrelevant. Which is my point - widening the acceptable input range from 2^31 to 2^32 (or in the case of signed integers, from 2^30 to 2^31; give or take 1 of course) just \"fixes\" one small case of the actual core issue of nearly any arithmetic anywhere being wrong if you don't explicitly constrain input sizes. reply wat10000 41 minutes agorootparentI agree on there not being much difference between 2^30/31/32. But it’s not “nearly any arithmetic.” If your size is an actual data size, then 2^64 is fine. reply dzaima 8 minutes agorootparentRight, with 64-bit ints things are a lot nicer. Though you can still run into some issues on \"generate this much data\" tasks as opposed to \"operate over existing data of this size\". Retric 4 hours agorootparentprevAnd further this all assumes they used int vs. long. It can be “wrong” in that it only works for arrays of under 2^63 elements without that ever being a possibility. Production code is often filled with edge case bugs that simply never come up. Works for 100x the expected use case is generally good enough if you’re approaching the limits where 2^31 is a potential issue then you are also approaching the case where 2^32 definitely will be. reply sillysaurusx 4 hours agorootparentprevHacking, of course. Overflows are one of the primary ways that hackers gain control of systems. reply Retric 2 hours agorootparentThat’s irrelevant for majority of software. reply sillysaurusx 2 hours agorootparentThis mindset is why hackers are able to exploit most systems. reply Retric 50 minutes agorootparentWhich isn’t a problem as exploiting most software is meaningless. Wow someone can hack software they already have root access to the machine for whatever will we do. It’s management and developers not treating software where it is meaningful for someone to hack as a different category that’s an actual problem. reply javcasas 25 minutes agorootparentSo a coworker sends me this CAD file via email, I open it and my computer gets controlled by a botnet, and the file immediately sends itself to all my outlook contacts. Nah, that sounds impossible. I'm sure it has never ever happened. reply borntyping 7 hours agoprevNearly all binary searches and mergesorts are broken in languages with bounded integers. reply perching_aix 5 hours agoparentAnd on machines with finite memory, right? Which would be every actual computer ever built? reply socksy 4 hours agorootparentWell I would posit that it would be hard to get to this code in a language with unbounded integers where (low n + high n) causes an OOM error, because in order to run this code, you first need an array n units wide. You could argue that the array itself could take up most of the space leaving no room for the indices, but that's hardly a fault with the algorithm, as now you've got a computer that basically can't do anything due to overloading. Whereas overflowing a 32 bit integer is a much more likely occurrence that arguably the algorithm should account for. reply chowells 40 minutes agorootparentWhy does everyone talk about binary search in terms of arrays? Binary search works with any monotonic function. Looking up a value in a sorted array is just a special case of a monotonic function. reply dietr1ch 3 hours agoparentprevDoing any math with bounded integers considered harmful. At some point during my undergrad I realized this and tried to be really careful when implementing algorithms, but it's stupidly hard to do in a tidy way, at least in old C. It's not practical and people just rather close their eyes and live in blissful avoidance. reply xigoi 10 hours agoprevSo previously it worked for arrays up to length 2³⁰ and now it works for arrays up to length 2³¹. Is it really that much of a difference? Wouldn’t it be better to change the indexing type to a 64-bit integer? reply ddxxdd 7 hours agoparentThe difference is that we know for a fact that the proper implementation works for integers up to 2^31, whereas the improper implementation deceived us into thinking that the code would work in situations where the code actually doesn't work. I find it valuable to understand when my code will crash. reply foldr 7 hours agoparentprevArticle was written in 2006 when 32-bit architectures were dominant. I suppose the issue is moot now on 64-bit architectures, as the difference between 2^62 and 2^63 isn't relevant. (2^62 bytes is 4611686 terrabytes.) reply im3w1l 6 hours agorootparentSpelling it out like that sure gives some perspective - It's a frighteningly low number! They sell 2tb microsd cards nowadays. I bet you could fit 2^64 bytes in a shoebox. So I think uint64 might eventually be insufficient as a size type. Edit: Well not quite, more like a small closet. reply tliltocatl 6 hours agorootparentAlmost makes you think RISC-V was right with 128-bit extension. On the other hand, exabyte-scale memories might one day be possible, but would it still be useful to maintain single-address-space illusion for these? reply snvzz 5 hours agorootparentRV128I is not an extension, but a (non-ratified) base ISA. Independent from RV64I, which in turn is independent from (and predates) RV32I. reply coldtea 5 hours agorootparentprev>Spelling it out like that sure gives some perspective - It's a frighteningly low number! Yeah, it's very common for computers to have byte addressable 4 exabytes of storage... reply im3w1l 1 hour agorootparentWell I used to think 64 bits would be enough forever basically, I guess that's why I was a little shocked that it actually might become insufficient at some point even if its far off. reply xigoi 1 hour agorootparentI do not look forward to the future where common software requires exabytes of memory. reply eesmith 9 hours agoparentprevThe use of: int high = a.length - 1; tells us that a.length-1 is supposed to fit into an int, so there is no need to handle 2³¹ or larger. reply rob_c 7 hours agorootparentYep. No story here, feel free to move on... reply MattPalmer1086 7 hours agoparentprevNot really. Arrays are limited to an int in size. You would be using more memory for the calculation and have to cast the value down to a 32 bit value to use as an array index. Or you could just write the code so it isn't vulnerable to integer overflow. reply touisteur 7 hours agorootparentWhich is sad (array size limited to an int) and has always annoyed me, coming back from Ada where the index of an array is just another discrete type (including boolean, enum type, and ranged integers). reply rob_c 7 hours agorootparentprevIdeally the index should support int64 with int being a synonym on a 64bit platform by default. If not yes frankly you're into such unexpected behaviour territory that you should check your whole codebase rather than rely on stuff working just because it compiled. And we all know how everyone loves writing and understanding integration tests... (I personally do but most problems in the industry wouldn't be a thing if more people stopped to write them) reply bobmcnamara 7 hours agorootparentsize_t, you mean? reply rob_c 6 hours agorootparentWell yes by inference reply hedora 2 hours agorootparentJava doesn’t have a type that corresponds to size_t. It only has signed integer types, so the closest match is ssizet_t (but even then you need to figure out how many bits that is on your architecture). reply Ameo 6 hours agoprevA cynical takeaway from this is that it's hard to write good code and it doesn't really matter if you do or not. Most code at every company I've worked at and project I've built is littered with technical incorrectness and buggy half-measure implementations. It's human nature or time pressure or something like that, but the application continues providing business value despite that. reply SoftTalker 2 hours agoparentBecause it's sort of like premature optimization. If your business case will never be dealing with billion-element arrays, it's a waste of time to make sure your code can handle them. reply 3ple_alpha 6 hours agoprevNo they're not. If you're using an array with length over a billion in Java, your code stinks already before you start using binary search. reply coldtea 5 hours agoparentThat's not only wrong in itself, but totally orthogonal. A binary search implementation should still work, regardless of the array length, or have the limitation documented. And of course an array \"with length over a billion\" can be totally valid, depending on the use case, your tradeoffs, available memory, etc. It could even be the optimal data structure for some use cases. reply poincaredisk 6 hours agoparentprevI'm not a Java programmer, but how would you load of a 1GB file into memory? I assume read returns some kind of an array. Also big arrays being (supposedly) a coffeee smell doesn't mean that code handling them improperly is not buggy. reply aardvark179 4 hours agorootparentIf you really needed it in memory you’d use one of the file APIs that will map it and present a direct byte buffer view over that memory. Those APIs use long as their offset unlike the 32 ints used by arrays, and would avoid having to copy the data into some other object. There has been some discussion over the years about how arrays could be changed in the JVM to support longer lengths, but doing so without breaking existing code and while providing truly useful functionality without providing obvious footguns isn’t as easy as you might think. reply angus_gh 6 hours agorootparentprevJava's arrays use a signed 32-bit int as their length, so the longest they can be is about 2 billion elements. If your code has arrays over a billion elements, then it will fall over the moment someone inputs slightly larger data reply danvonk 6 hours agoparentprevRelational databases often require searching and sorting gigabytes of data to answer queries (sometimes larger than RAM if e.g. k-way merge sort is used) so it doesn't seem that far-fetched, especially given that there are database systems written in Java. reply tehjoker 50 minutes agoparentprevnot doing much scientific programming eh? reply sonu27 7 hours agoprevTitle needs updating with the year 2006 reply usr1106 6 hours agoparentI often think AI is mostly crap, wasting a lot of energy for very questionable benefits. But could/should this repetitive task of reminding submitters to follow the submission guidelines and add the year to submissions of old articles be automated? reply pdimitar 6 hours agorootparentI would agree, though why would you need AI for that is an open question. reply sonu27 6 hours agorootparentA simple crawler would have been able to detect it’s from 2006. Perhaps a reminder should be added if the year is not recent reply Too 6 hours agorootparentEven simpler, just check if the url or title has been submitted before. That would also take care of all the duplicate entries that pop up once per day for a week after a viral story is emerging. In this instance, the url is slightly different from previous submissions so some more clever fuzzy matching or using only the title would be needed. reply usr1106 5 hours agorootparentYes, I have always wondered why the simple duplicate checker within the same couple of days does not exist. Or does it exist and the duplicates are actually sligt variations of the URL. reply usr1106 5 hours agorootparentprevWhat algorithm would you suggest to find the year in an arbitrary submission? Of course AI is not a very clearly defined term, more difficult problems certainly exist. I was just thinking of the case the submission contains several dates or none at all and still several hints a human would take into consideration get checked. Of course some minimal implementation without AI techniques could already handle many cases. My AI suggestion was not death-serious ;) reply coder543 3 hours agorootparentGoogle's research blog does not seem to provide this, but many blogs include the Open Graph metadata[0] around when the article was published or modified: article:published_time - datetime - When the article was first published. article:modified_time - datetime - When the article was last changed. For example, I pulled up a random article on another website, and found thesetags in the : For pages that contain this metadata, it would be a cheaper/faster implementation than using an LLM, but using an LLM as a fallback could easily provide you with the publication date of this Google article. [0]: https://ogp.me/ reply coldtea 5 hours agorootparentprev>What algorithm would you suggest to find the year in an arbitrary submission? In the submission title, a simple regex for the presence of a date with a standard format (e.g. %Y) would suffice. Matching it to the article might or might not be possible, but that would already be enough (assuming having the date is a good thing, which I'm not certain at all) reply pdimitar 5 hours agorootparentprevAs another comment suggested, you can scan for previous submissions by URL -- Algolia is very helpful with that. Outside that, no clue, been a long time since I last wrote crawlers, admittedly. Though it can't be too difficult to crowd-source origin date parsers per domain? But hey, if any LLM's free tier can achieve it, then why not. My point was that many people worked on that particular problem historically. It would be a shame if we can't use any of their hard work. reply coldtea 5 hours agorootparentprevI think adding the year is mostly crap. What exactly information would it give, except perhaps the false impression that this article is \"antiquated information\", when it pretty much holds true, and describes a perrenial issue? reply gmfawcett 3 hours agorootparentIt gives a cue about how many times I've probably seen the article before. Quite useful, IMO. I read this particular article when it came out in 2006... it's convenient to know we're not discussing a novel finding on the same topic. reply nsxwolf 2 hours agoprevWrite this in a Leetcode interview and I suspect the average interviewer will fail you and not believe your reasoning. reply cvoss 2 hours agoprev> It is not sufficient merely to prove a program correct; you have to test it too. Well... If your proof made the (false) assumption that int is an unbounded integral type, then you didn't prove the program is correct at all. What you proved was than an algorithm in some ideal universe is correct. But your program is a different beast that lives in a specific programming language. reply nritchie 6 hours agoprevIsn't the larger point of this article that errors like this can sneak in and remain dormant for years? Even if this example is old, isn't this lesson still relevant? Expecting that we are now immune from this class of errors because it is not 2025 and not 2006 is hubris. Hubris rarely ends well. reply Dwedit 4 hours agoprevAt which point does this article talk about Merge Sort? Anyway... Merge sort doesn't even need to be recursive in the first place. It's always taught that way in CS classes, but it can just as easily be written with nested for loops. On the outermost for loop, you double the sublist size until you exceed the whole size. reply bediger4000 2 hours agoparentWikipedia gives an example merge sort that does nearly exactly what you describe. That algorithm has some weird performance drops at 2^n+1 sorted elements. https://bruceediger.com/posts/mergesort-investigation-8/ reply kunley 4 hours agoprevI think most of the language implementers know about it. Here is a relevant line from Go, even with a comment that it's about overflow: https://github.com/golang/go/blob/19e9231/src/sort/search.go... reply mac3n 3 hours agoprevwhile it's unlikely you'd allocate a memory array[2^30] on a 32-bit machine, I also use binary search on mmap'd files. Specifically, text files. Binary search doesn't need the exact midpoint, just somewhere nearby, so I pick the midpoint and look around for line boundaries. On normal sorted text files, this works fairly well. I was able to search a list of a billion IP addresses while touching only a few pages. https://gitlab.com/mac3n/ksip reply whatever1 4 hours agoprevOk let me say it. The implementation of ints in computers is plain stupid. I am not sure why we have been trying to convince ourselves otherwise for decades. So many programs are wrong because of this and they are just ticking bombs. reply toast0 1 hour agoparentI think you'd have better responses if you said the implentation of ints in most programming languages is plain stupid. Processors are doing a reasonable job, but there's certainly things they could do differently. Some languages have pretty specific integer types. And then you have C. Java at least makes all the integer widths consistently sized on all platforms, but neither provides good ways to express and manage overflow or carry. And the architects of Java decided that developers aren't qualified to use unsigned types, which makes a lot of tasks much harder. reply Y_Y 2 hours agoparentprevWhat would you have done? I don't disagree, but I'm not sure what would have been better either. reply whatever1 2 hours agorootparentInt as a type serves different purposes. Counting, indexing, rounding floats, exact arithmetic etc. These have completely different requirements and edge cases. It was a mistake trying to tackle all of these with a singular type. reply Y_Y 1 hour agorootparentNot to mention bools, bitarrays, error codes... reply truefossil 10 hours agoprevsize_t is 64 bits on 64-bit CPUs reply anonnon 1 hour agoprevWhat's funny is that the safer alternative: int mid = low + ((high - low) / 2); is probably what most of us originally came up with before we saw the shorter, more elegant, but overflow-prone approach. reply RaftPeople 13 minutes agoparentI wrote one before I read the article to see if I would hit the bug and yep, I wrote it the safer way. For me, that is the most readable way to do it because it lines up both conceptually and visually with how I'm thinking of the problem. reply djmips 5 hours agoprevIf you did a lot of stuff on 8 bit systems you ran into this very early and often. reply rep_lodsb 5 hours agoparentNot in assemly, where it's trivial to shift right with carry. reply thisisnotauser 2 hours agoprevNotably, this is impossible in well-designed languages. reply RaftPeople 9 minutes agoparentDo you mean that the language would either halt or throw an exception due to overflow on \"high+low\"? reply akam4n4n 6 hours agoprevmaybe dumb, but what happens when low and high > unitMAX/2 shift left is worse in that case right? reply poincaredisk 6 hours agoparentIt's not possible, because low and high are of type int, which is always lesser than uint_nax/2 reply beyondCritics 6 hours agoprev> mid = ((unsigned int)low + (unsigned int)high)) >> 1; Also not correct if the sum overflows. reply poincaredisk 6 hours agoparentIt can't overflow - max_int+max_intThis is an obvious, common category of bug that's been well known for decades. TFA is actually the article which made it common knowledge, it's from 2006. reply jansan 5 hours agoprevNot true for Javascript. The maximum array length is 2^32-1, but the maximum safe integer value for numbers is 2^53 – 1. reply makach 6 hours agoprevWait a second. Is it really your bug if the fix is to make it work around the hardware's limits? reply cdogl 4 hours agoparentMy personal experience is yes. Hardware provides a substrate of reality, and software developers are asked to implement aspirations. reply secondcoming 6 hours agoprevC++20 introduced std::midpoint() [0] Interestingly, calculating the midpoint of doubles seems quite complex (according to gcc at least) [1] [0] https://en.cppreference.com/w/cpp/numeric/midpoint [1] https://godbolt.org/z/G1P611o7Y reply bediger4000 4 hours agoprevI'm puzzled by the inclusion of merge sort. Merge sort is referred to by the title, and in passing in the text, but no examples are given. reply moffkalast 6 hours agoprevI'll make sure to consider this the next time I'm working with a billion length array lmao. reply phplovesong 5 hours agoprev [–] Meh. Thats not the algo but int type used. I did not like this title, it was a scamm by ai. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 2006, Joshua Bloch identified a bug in the binary search algorithm, initially noted in Jon Bentley's \"Programming Pearls,\" where an overflow occurs when 'low' and 'high' exceed the maximum int value.",
      "This bug, affecting arrays with lengths of 2^30 or more, was rare in the 1980s but is more common today, also impacting mergesort and other divide-and-conquer algorithms.",
      "Bloch recommends fixing the issue by calculating the midpoint as `int mid = low + ((high - low) / 2);`, highlighting the challenges of writing error-free code and the necessity of thorough testing."
    ],
    "commentSummary": [
      "Many binary searches and mergesorts are prone to integer overflow issues, especially in older implementations, as highlighted in a 2006 article.\"",
      "The article emphasizes the importance of considering edge cases and hardware limitations in programming, which remains relevant today.\"",
      "Developers are advised to use built-in functions or larger integer types to prevent such bugs, underscoring the need for caution with data types and input sizes.\""
    ],
    "points": 143,
    "commentCount": 136,
    "retryCount": 0,
    "time": 1736585205
  },
  {
    "id": 42665367,
    "title": "Track your devices via Apple FindMy network in Go/TinyGo",
    "originLink": "https://github.com/hybridgroup/go-haystack",
    "originBody": "go-haystack Go Haystack lets you track personal Bluetooth devices via Apple's massive \"Find My\" network. It uses OpenHaystack together with Macless-Haystack to help you setup a custom FindMy network with tools written in Go/TinyGo. No Apple hardware required! Build Your Own Beacon This package provides firmware written using TinyGo and the TinyGo Bluetooth package. As a result, any of the following hardware devices should work: Adafruit Bluefruit boards using nRF SoftDevice - https://github.com/tinygo-org/bluetooth?tab=readme-ov-file#adafruit-bluefruit-boards BBC Microbit using nRF SoftDevice - https://github.com/tinygo-org/bluetooth?tab=readme-ov-file#bbc-microbit Other Nordic Semi SoftDevice boards - https://github.com/tinygo-org/bluetooth?tab=readme-ov-file#flashing-the-softdevice-on-other-boards Boards using the NINA-FW with an ESP32 co-processor - https://github.com/tinygo-org/bluetooth?tab=readme-ov-file#esp32-nina Boards such as the RP2040 Pico-W using the CYW43439 co-processor - https://github.com/tinygo-org/bluetooth?tab=readme-ov-file#cyw43439-rp2040-w You can also run the beacon code on any Linux that has Bluetooth hardware, such as a Raspberry Pi or other embedded system. The beacon code is located in the firmware directory. TinyScan Go Haystack also includes TinyScan, a hardware scanner for local devices. TinyScan runs on several different microcontrollers boards with Bluetooth and miniature displays, such as those made by Adafruit and Pimoroni The TinyScan code is located in the tinyscan directory in this repository. How to install Apple ID You must have an Apple-ID with 2FA enabled. Only sms/text message as second factor is supported! anisette-v3-server Start anisette-v3-server docker network create mh-network docker run -d --restart always --name anisette -p 6969:6969 --volume anisette-v3_data:/home/Alcoholic/.config/anisette-v3 --network mh-network dadoum/anisette-v3-server macless-haystack Start and set up your Macless Haystack endpoint in interactive mode: docker run -it --restart unless-stopped --name macless-haystack -p 6176:6176 --volume mh_data:/app/endpoint/data --network mh-network christld/macless-haystack You will be asked for your Apple-ID, password and your 2FA. If you see serving at port 6176 over HTTP you have all set up correctly Hit ctrl-C to exit the process once it has been configured. Restart the macless-haystack server docker restart macless-haystack See https://github.com/dchristl/macless-haystack/blob/main/README.md#server-setup for the original instructions. go-haystack Install the go-haystack command line tool go install github.com/hybridgroup/go-haystack/cmd/haystack@latest How to use Scanning for local devices haystack scan Should return any local devices within range: $ haystack scanCE:8B:AD:5F:8A:02 -53 ce8bad5f8a0271538ff5afda87498cb067e9a020d6e4167801d55d83 - battery full FE:B0:67:9B:9A:5C -55 feb0679b9a5c55b1141c5cc6c8f65224ae9bc6bc2d998ccf5c56a02d - battery full CE:8B:AD:5F:8A:02 -53 ce8bad5f8a0271538ff5afda87498cb067e9a020d6e4167801d55d83 - battery full CE:8B:AD:5F:8A:02 -53 ce8bad5f8a0271538ff5afda87498cb067e9a020d6e4167801d55d83 - battery full FE:B0:67:9B:9A:5C -56 feb0679b9a5c55b1141c5cc6c8f65224ae9bc6bc2d998ccf5c56a02d - battery full CE:8B:AD:5F:8A:02 -53 ce8bad5f8a0271538ff5afda87498cb067e9a020d6e4167801d55d83 - battery full FE:B0:67:9B:9A:5C -56 feb0679b9a5c55b1141c5cc6c8f65224ae9bc6bc2d998ccf5c56a02d - battery full CE:8B:AD:5F:8A:02 -53 ce8bad5f8a0271538ff5afda87498cb067e9a020d6e4167801d55d83 - battery full Adding a new device Generate keys for a device haystack keys DEVICENAME The keys will be saved in a file named DEVICENAME.keys and the configuration file for Haystack will be saved in DEVICENAME.json. Replace \"DEVICENAME\" with whatever you want to name the actual device. Flash the hardware with the TinyGo target and the name of your device. For example: haystack flash DEVICENAME nano-rp2040 This will use TinyGo to compile the firmware using your keys, and then flash it to the device. See https://tinygo.org/getting-started/overview/ for more information about TinyGo. Upload the JSON file for that device to your running instance of macless-haystack using the web UI. Point your web browser to https://dchristl.github.io/macless-haystack/ which is a single-page web application that only reads/writes local data. Click on the link for \"Accessories\", then on the \"+\" button. Choose the DEVICENAME.json file for your device. That's it, your device is now setup. Objects in your data may be closer than they appear Eventually, if your device is in range of any iPhone, they will appear in your Macless-Haystack data in the web UI. Note that it might take a while for the first data to show up. Have fun, be good!",
    "commentLink": "https://news.ycombinator.com/item?id=42665367",
    "commentBody": "Track your devices via Apple FindMy network in Go/TinyGo (github.com/hybridgroup)139 points by deadprogram 6 hours agohidepastfavorite32 comments a12k 4 hours agoIncredibly cool. I am constantly amazed by these efforts and the find my network is a really impressive thing. What’s stopped me from using anything like this ever is the fact that I’m confident that at some point Apple will either embrace this sort of piggybacking on the network and open it up more officially, or it will ban any Apple ID that has ever been associated with such things. Right now they know about this and are not commenting either way. Hope in the future either Apple supports this more officially, or there is a way to use it with no direct link to my Apple ID or account. Until then, I am a spectator in these things. reply malmeloo 2 hours agoparentThe process of requesting locations for a certain tag is not tied to any Apple Account. In the instructions in the README, when logging into macless haystack, you can just use a burner account. reply jcutrell 2 hours agoparentprevI think there is some positive effect potential for Apple to let this slide. The broader this network is, the more adoption it receives. P2P as a super-structure has always been a bigger than vendor problem; adoption by any means is likely an allowable tradeoff, especially since Apple doesn't have to do the work here. Eventually they will capitalize more on the mesh density, rather than crushing the adoption now. reply malmeloo 2 hours agorootparentExcept that custom tags like these do not require an Apple device in order to use them, so the size of the network is not increased. They only increase the load on the network. FindMy is not a P2P/mesh network; all these tags do is broadcast keys which are picked up by iDevices, which then upload those reports to Apple. reply wkat4242 25 minutes agoprevOoooh it looks like you can now use it without a Mac? That would be amazing. Because Apple trackers cost only a few euro (aftermarket ones of course) reply 1vuio0pswjnm7 1 hour agoprevhttps://media.ccc.de/v/38c3-find-my-101 reply bloopernova 2 hours agoprevI hope in the future we can determine the position of beacons to within a cubic metre or less. My wife has ADD and she loses items often. Tiles aren't very loud and are flaky, we don't have an iPhone to use Airtags. I'm too exhausted to try to master the math needed to locate Bluetooth beacons, but I wish I could. I'd love for there to be a \"just add 4 small Bluetooth boards\" kind of software project, but it doesn't seem to scratch that itch for most open source devs. reply crazysim 2 hours agoparentMy partner and I are also Android only and honestly, I'm thinking about simply picking up a smashed up iPhone 11 or 12 mini and exclusively relegating it for precision finding needs with AirTags. I don't think there will be equivalent alternatives, at least not ones with ultra wideband precision location functionality, availability, acceptable price, and robustness. On the other hand there does seem to be some UWB support in some Android that might work with Tile's UWB: https://www.zdnet.com/article/how-to-enable-uwb-on-android-a... reply mook 31 minutes agorootparentMy understanding is that the Motorola trackers have UWB but phone support isn't widespread yet. I haven't looked deep enough to see if that satisfied your other criteria though. reply willseth 35 minutes agoparentprevAirTags are generally loud enough to solve that problem. I wish they put the same speaker in the ATV remote. reply shermantanktop 42 minutes agoparentprevI’m also a misplacer-of-items. I hot glued an AirTag to the inside of my glasses cases but was frustrated at the poor signal. Turns out most cases are made of thin metal. Switched to a case with a cardboard core and it works great now. reply lolinder 2 hours agoparentprevI also have ADHD, am also constantly losing things, and I've had success with the Pebblebee tags. I have a tag on my keys and a card in my wallet. The noise is loud enough that I can hear it on a different floor in an old home with thick brick walls. reply tgsovlerkhgsel 2 hours agoparentprevIf you know that the item is inside your apartment, you only need the UWB part, which is quite orthogonal to this project. reply stackskipton 2 hours agoparentprevThat’s AirTags with modernish iPhone now. reply nobunaga 3 hours agoprev [–] Is it just me or this whole find my network capability is a security nightmare? I mean I understand its usefulness but can the [insert authority here] just request apple to tell them where this person is even without cellular coverage? Ive decided to move away from the apple ecosystem either way because of this but it just seems to me to be a surveillance nightmare. reply wkat4242 23 minutes agoparentNo they can't. Apple doesn't know who has which tag. It's built with privacy in mind. I know Apple listen touts privacy while having ulterior motives but I looked at the technical design specs and this is pretty great I doubt Samsung and Google have gone to such lengths with their trackers. reply andy_xor_andrew 12 minutes agorootparentApple always seems to design services the way a privacy-obsessed nerd would, (if you forced said privacy nerd to design a P2P tracking network). It's like, \"oh, you want all your photos to be searchable, like 'dogs' or 'Eiffel tower'? Fine, we'll create an on-device embedding of each photo, use homomorphic encryption so you can share it with us and we can match it to its contents without even knowing what they are, then we'll send that back to your device for storage. Oh, and we'll use a relay so we don't even see your IP address while doing this, not that it matters since we can't decrypt the content anyway.\" It's pretty wild, like they could have easily skipped all this and only a fraction of a fraction of a fraction of users would even know or care. In fact, I was pretty annoyed that the news story from the above example was \"Apple is looking at all your photos and violating your privacy\", since they spent so much effort doing it the right way, in a way that respects your privacy, it makes it less likely they will bother going through the effort again https://www.theregister.com/2025/01/03/apple_enhanced_visual... reply rgovostes 3 hours agoparentprevhttps://support.apple.com/guide/security/find-my-security-se... reply nobunaga 3 hours agorootparentInteresting thanks. I understand that its designed to be anonymous, but I guess it requires faith in Apple not complying to any forceful request from a security authority in the US to not modify it in secret. reply Terretta 1 hour agorootparentWhich mobile phone maker do you have more faith in? Which telco? Apple have done work, and published tools for researchers, to make it so they can't \"modify it in secret\". The tools for security research community help verify that and \"keep them honest\". For instance, this is partly what the prompts about new devices or log in on other devices are about, there's a key exchange happening, and you get told. You can also exchange keys with Messages contacts to verify you're talking to them. You can turn on iCloud Advanced Security and Apple don't get even your backup keys. Also see the new Lockdown Mode. Granted, Apple can change their minds and become anti-privacy or pro data-brokers and ad-tech, but some of these proofs would break so folks would know. Anyway, if the government wants to know where you are, they can just ask the Chinese who've been watching Americans' cell phone identifiers move around. In seriousness, the telcos already sell* this position data to data-brokers and law enforcement have portals to just watch you scurry around, even without a warrant. * Sometimes telcos share your location data in ways that aren't \"selling\" so they can say they don't sell it. But the data goes and telcos derive value in exchange. reply lathiat 2 hours agorootparentprevIf you want to learn more about how this all works in video form, there was a talk at 38c3: https://www.youtube.com/watch?v=nWQcgZfxkOM&pp=ygUMMzhjMyBma... reply CubsFan1060 2 hours agorootparentprevOf course that can be said for nearly anything you own. iPhone, android, tablet, anything that is Bluetooth (for instance, your car), etc. reply rgovostes 2 hours agorootparentprevCryptographers who design these systems do consider the threat of a malicious future iteration of the company and thus try to reduce the trust in a centralized authority. Apple did fight in court to not have to crack the San Bernardino shooter’s phone, which probably didn’t garner much sympathy with the general public, specifically against government power to compel them to make changes to subvert security. They also publish a Transparency Report about government requests they’ve received and how many they’ve responded to. reply oefrha 2 hours agorootparent> Cryptographers who design these systems do consider the threat of a malicious future iteration of the company and thus try to reduce the trust in a centralized authority. It’s no use. All the opaqueness to Apple relies on > This private key pair and the secret are never sent to Apple and are synced only among the user’s other devices in an end-to-end encrypted manner using iCloud Keychain. Which is trivial to compromise from Apple. They do their best to minimize trackability from third parties though. reply Terretta 1 hour agorootparent> Which is trivial to compromise from Apple. Explain this? Since both Apple and security researchers have worked on provable trust. reply givinguflac 2 hours agorootparentprevGiven apples outright refusal to help the FBI previously I have more faith than other companies that they’ll do the right thing. But nothing’s perfect. reply tgsovlerkhgsel 2 hours agoparentprevThe system is designed specifically to make this impossible. Your tag doesn't know its position, it simply broadcasts its own, rotating public key. Since the key changes randomly (in a way that you as the legitimate owner can predict), a third party can't easily follow the tag. Other devices see that key, and share their position, encrypted with your tag's public key. That makes it relatively hard to get the data, essentially impossible without forcing Apple to re-design the system and push malicious updates, which is generally considered as something that goes beyond what normal subpoenas can do. reply nixpulvis 43 minutes agorootparentApple could be subpoenaed to look at the account holder's registered tags still, no? reply HALtheWise 10 minutes agorootparentNo, because Apple doesn't have the private key of the account holder, and so can't see which rotating codes are associated with that account holder since it's all encrypted. reply crims0n 34 minutes agorootparentprevIf the US government is subpoenaing Apple on your behalf, you probably have bigger problems. reply malmeloo 2 hours agoparentprevI think it's worth mentioning that FindMy consists of two distinct \"networks\"; there's the one where other Apple devices find your stuff, and another where your devices upload their locations straight to Apple. The FindMy app combines these two networks to show the most recent location. As far as I can tell this project only uses the former network, which would require an explicit backdoor due to the way it is designed. But if you're trying to defend against government agencies, that latter network is probably more of your concern. reply reaperducer 1 hour agoparentprev [–] Is it just me or this whole find my network capability is a security nightmare? Settings → your_name → Find My → device → toggle off If you don't trust that this will really disable the feature, then you are going to have to think hard about every electronic device you own. Do you trust the firmware in your Android phone? What about the non-open-source modem chip? What about the SIM card, which runs Java? Are there microphones you haven't noticed built in to your TV remote? (Many have them.) Your toaster likely has a chip in it more powerful than a networked DOS-era computer. (Mine does.) How do you know it's not joining a nearby wifi network and sending out information? Ever since the China/iCloud thing, I don't fully trust Apple. But among big tech companies, it's certainly the one that I trust the most. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Go Haystack enables tracking of personal Bluetooth devices via Apple's \"Find My\" network without requiring Apple hardware, utilizing OpenHaystack and Macless-Haystack with Go/TinyGo tools.",
      "Users can create their own beacons using TinyGo firmware on hardware like Adafruit Bluefruit boards and BBC Microbit, with setup requiring an Apple ID with two-factor authentication (2FA).",
      "The process involves installing the go-haystack tool, using commands like `haystack scan` to locate devices, generating keys, flashing the device, and uploading a JSON file to macless-haystack for device visibility in the web UI."
    ],
    "commentSummary": [
      "The Apple FindMy network can be utilized with Go/TinyGo to track devices, offering a way to expand the network without needing an Apple account.",
      "Privacy concerns are mitigated by Apple's design, and the company has a strong reputation for maintaining user privacy, although some users remain wary of potential surveillance.",
      "The FindMy network operates in two parts: Apple devices locating items and user devices uploading location data to Apple, with an option for users to disable the feature if desired."
    ],
    "points": 139,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1736597674
  },
  {
    "id": 42665222,
    "title": "The State of Vim",
    "originLink": "https://lwn.net/SubscriberLink/1002342/a8d8a17f30968b93/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us User: Password:| Subscribe / Log in / New account The state of Vim [LWN subscriber-only content] Welcome to LWN.net The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net! January 10, 2025 This article was contributed by Murukesh Mohanan The death of Bram Moolenaar, Vim founder and benevolent dictator for life (BDFL), in 2023 sent a shock through the community, and raised concern about the future of the project. At VimConf 2024 in November, current Vim maintainer Christian Brabandt delivered a keynote on \"the new Vim project\" that detailed how the community has reorganized itself to continue maintaining Vim and what the future looks like. Vim after Bram Brabandt began with his history with Vim: he has been involved in Vim since 2006, and said his first commit to the project was made in the 7.0/7.1 days (sometime around 2006). He started by contributing small patches and fixes, and then contributed larger features such as the gn and gN commands, which combine searching and visual-mode selection, improved cryptographic support using libsodium, maintained the Vim AppImage, and more. He said he became less active in the project around 2022 due to personal and work-related reasons. That changed in August 2023, when Moolenaar passed away. Moolenaar had been the maintainer of Vim for more than 30 years; while he had added Brabandt and Ken Takata as co-maintainers of Vim in the years before, most development still flowed through him. With his death, a considerable amount of knowledge was lost—but Brabandt and others stepped up to keep the project alive. Moolenaar was the only owner of the Vim GitHub organization at the time, so only his account could change certain settings. Initially, contributors tried to use the GitHub deceased user policy to add owners to the organization. That was quite an involved process, and it soon became apparent that the end result would be the deactivation of Moolenaar's account. Having Moolenaar's account be accessible by his family was important, so they abandoned that approach, and instead the family granted access to it as needed for organizational changes. Charles Campbell (known as \"Dr Chip\"), a Vim contributor for more than 25 years also decided to retire soon after Moolenaar's death. His departure was followed by an expansion of the team of maintainers, as Yegappan Lakshmanan joined it, with Dominique Pellé, Doug Kearns, and GitHub users \"glepnir\", \"mattn\", and \"zeertzjq\" joining soon after. More than just the source code He stressed that maintaining Vim is not just about the source code. There are quite a few other things to be managed, such as the Vim web site, FTP server, security disclosures, Vim communities on other sites such as Reddit and Stack Exchange, and more. Vim's site needed work. The design, and most of the code, had been unchanged for quite a while—until 2023, it was based on PHP 5. In recent times, there had been a few occasions where the web site was unstable, and so he started looking for a new host in 2024. The move involved an upgrade to PHP 8, for which some of the code had to be rewritten. Brabandt thanked Mark Schöchlin, who stepped up to take care of all this. He acknowledged that the design has been pretty much unchanged since 2001, doesn't look modern, and can be scary to new users. There has been some work on redesigning it, but the first attempt hasn't been that successful. He prioritizes consistency and does not wish to scare away longtime users. DNS was also troublesome—the vim.org domain was managed by Stefan Zehl, but Moolenaar also owned a number of other domains such as vim8.org, vim9.org, etc. Thankfully, SSL certificates were already managed using Let's Encrypt, so Brabandt had no problems there. Several email addresses, such as bram@vim.org, bugs@vim.org, etc., were forwarded to Moolenaar's personal email; those have since been updated to point to Brabandt's address instead. The FTP server was hosted by NLUUG, but he decided to retire it and says that he hasn't received any complaints so far. ICCF Holland As readers might know, Vim is charityware, and the charity of choice is ICCF Holland, founded by Moolenaar. Brabandt said that the ICCF is very much alive, and plans to reorganize and restructure itself. Quite a few users started donating after Moolenaar's passing, and in 2023, it raised about €90,000. The project plans to continue to work with ICCF and doesn't want to change ICCF's association with Vim. He noted that there is no sponsorship for the maintainers, all of whom are working for free. Traditionally, all money raised has been given to the ICCF and he has no plans to change that. Brabandt said he earns enough from his job that he doesn't need assistance to work on Vim, so he's happy to let all donations go to ICCF. As an incentive to donate, Moolenaar had allowed people who donated to ICCF to vote on Vim feature requests. Donors to the ICCF could link to their Vim.org account when donating, and then vote on features. This is one aspect that he no longer sees a need for, now that issues and enhancements are discussed on GitHub, and so has decided to shut this down. Linking the accounts and donations was also not easy for Brabandt—he was not sure how Moolenaar did this in the past. Communication channels He also talked about the community centered around the Vim mailing lists, which are hosted on Google Groups. In May 2024, he received an automated message from Google informing him that all content from the vim-dev list had been blocked due to spam or malware. This caused a fair bit of trouble, and while it was restored in around a day or so, he still does not know what the exact problem was. There has been some consideration of self-hosting the list, but one drawback is that everyone would have to sign up again. The mailing list is no longer that active now, with more of the community conversations happening on Reddit or Stack Exchange. Security reporting had to be addressed as well. A couple of years ago, people were reporting issues on the Huntr platform. There were quite a few open issues which have since been taken care of. Huntr was acquired by another company in 2023, which refocused it entirely on AI and shut down general open-source vulnerability reporting. Now, Vim is accepting security reports via email or GitHub, and publishing vulnerabilities via GitHub security advisories. There is a private mailing list for as-yet unpublished security issues, and emails are forwarded to all maintainers. Brabandt has started adding a [security] tag to commit messages for marking security fixes, and such commits are announced on the oss-security list (the most recent being from October) and to maintainers of distribution packages. Maintenance mode Brabandt then showed the contribution graph, to demonstrate that development did not stop after Moolenaar passed away. There was a slowdown as Moolenaar's health deteriorated, and then a spike as he cleaned up the open pull requests (PRs). Version 9.1, dedicated to Moolenaar, was released on January 2, 2024—about four months after his passing. The 9.1 release included improvements to virtual text (which enables completion suggestions and such to appear in the editing area, while not being part of the actual text), smooth scrolling, and OpenVMS support. After 9.1, he started adding more potentially controversial changes, such as support for the XDG base directory specification. Now Vim does not need to litter your top-level home directory: ~/.vimrc or ~/.vim/vimrc still work, but $XDG_CONFIG_HOME/vim/vimrc will now work if neither of the above are present. Another such change is Wayland support. It is not complete yet, and he says he is not sure whether remaining problems with clipboard support are Vim bugs or Wayland ones. As he went through the backlog of PRs, he started developing a policy for merging PRs, prioritizing the need to test things well. Tests are now running with continuous integration (CI). He said that it's also important to have good documentation. Vim has interfaces to quite a few languages, including Python 2 and 3, Ruby, Lua, Tcl, and MzScheme. But Brabandt isn't sure which of these are really needed these days. For example, Python 2, Tcl, and MzScheme (which does not build with the latest version of the language) might need to be retired to reduce the maintenance burden. Other areas to improve include the GUI (GTK 4 has been around for a while, but Vim does not use it yet), support for advanced terminal features, and better spell checking (which has largely remained unchanged since Vim 7). Support for the tree-sitter parser generator is wished-for, but it is controversial, and he does not see it coming to Vim soon. He knows there have been some significant changes in Neovim, but he's not sure how many of those can come to Vim. There have been small changes in Vim, but for major changes, you need community support. He does not want to make backward-incompatible changes and is quite hesitant to merge changes that might break things. He said he has to keep in the mind the whole picture, especially the expectations of users, when dealing with PRs. Currently, he said that Vim is more-or-less in maintenance mode. He said he has created an internal repository to keep track of stakeholders and to ensure that if something were to happen to him, other maintainers could pick up where he left off. Brabandt recommended that those new to the project start by making small contributions and becoming familiar with the codebase. He had some pointers for developers. He said it is important to use a defensive style with C to ensure that new bugs aren't being introduced. One should use Coverity, a static-analysis tool, to scan for defects. Some parts of the Vim codebase are complex, he said, and need to be refactored into more manageable units if possible. Maintaining Vim is a full-time job, he said, and it is not only about maintaining the code, but also the community—managing expectations and listening to users' needs. He has to understand the community: what does it want Vim to be? An IDE? Bug-for-bug compatibility with old Vim? How can we make Vim9 script, the new Vim scripting language, more widely used? How can we ensure that the Vim community remains healthy? He ended his talk by thanking all the Vim contributors and then took a few questions. Questions One audience member asked about the difference between Vim and Neovim's maintenance model. Since most PRs are still merged by Brabandt, would that make him the new BDFL for Vim? Brabandt emphatically denied being a BDFL. Currently, he merges most changes because the version number has to be incremented with each change, so multiple people merging can introduce conflicts. However, when he was on vacation, he handed over the main maintainership to Lakshmanan. He emphasized that it's a community project, and he listens to the community before making decisions. It just happens that at this time the other maintainers don't want to merge changes themselves and instead defer to Brabandt, which is fine with him. Another member of the audience wondered about language barriers, since there are many Japanese members of the Vim community as well as many languages in Europe, etc. Brabandt answered that, as an international project, the primary language for working on Vim is English. He also noted that it is easier these days to collaborate across languages thanks to ChatGPT and translation tools, but it still happens that some users do not communicate in English well, and that makes it harder to understand their needs. The rest of VimConf 2024 VimConf was first held in 2013 by the Japanese Vim user group vim-jp. Since then, the group has organized it every year, until 2020 when VimConf was canceled due to COVID. After a hiatus, it resumed in 2023 with a scaled-down version. The full-fledged edition returned to Akihabara, Tokyo on November 23, 2024. Even though most of the organizers and attendees are Japanese, VimConf strives to be welcoming to all. Presentation materials are expected to be in English, and live translation is provided in both Japanese and English for keynotes and regular presentations, except for lightning talks. PDFs for the talks are available on VimConf's website, and all of the talks are now on YouTube. Index entries for this article GuestArticles Mohanan, Murukesh to post comments Thank you Posted Jan 10, 2025 23:33 UTC (Fri) by decaffeinated (subscriber, #4787) [Link] I wondered about the state of the project following Bram's passing. Thanks for this informative update. The ongoing excellence of LWN reporting is why I am a subscriber. Controversial? Posted Jan 11, 2025 4:08 UTC (Sat) by intelfx (subscriber, #130118) [Link] (2 responses) > Support for the tree-sitter parser generator is wished-for, but it is controversial, and he does not see it coming to Vim soon. Is there any reason *why* a better syntax support engine would be \"controversial\"? Controversial? Posted Jan 11, 2025 7:01 UTC (Sat) by kmeyer (subscriber, #50720) [Link] (1 responses) Some discussion here I guess: https://github.com/vim/vim/issues/9087 . Controversial? Posted Jan 11, 2025 10:26 UTC (Sat) by intelfx (subscriber, #130118) [Link] Oh... I see. The amount of disingenuous arguing in that discussion is just painful to read. Copyright © 2025, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=42665222",
    "commentBody": "The State of Vim (lwn.net)138 points by yla92 7 hours agohidepastfavorite33 comments jgb1984 5 hours agoAs a long time (20+ years) vim user the passing of Bram came as a shock. But it has become clear to me that the project is in safe hands, and I've seen slow but steady progress, continuing the tradition of stability that Bram always safeguarded for 3 decades. I do try out neovim from time to time, but I don't care for Lua (vimscript is easier to read and less verbose for .vimrc), I don't need an LSP and I found treesitter often buggy and slow. So I'm sticking with vim, here's to another few decades more, thank you to all maintainers! reply sundarurfriend 5 hours agoparentYou can pretty much ignore Lua and still use neovim, but I see your point. Lua is a big part of the appeal of neovim to me, and I'd probably have stuck to Vim without it. (Another big part is its embeddability - I can have a real neovim instance processing my commands from VS Code when I need to use that). I agree with you about treesitter too: it's not really better than the old regex way in terms of (non-)bugginess in my experience, and I've been hearing more and more reports of it being much slower than it's claimed to be. reply yosef123 3 hours agoparentprevStrange, this is the first time hearing Treesitter is slow. From experience, Treesitter has been incredibly fast for me (when running its core functions). I’m not discrediting your observation, but it might be something else in the pipeline. reply Aachen 2 hours agoparentprevLSP=? Web search comes up with language server protocol (among many other things, this seemed the most fitting), do you mean language server? reply sundarurfriend 1 hour agorootparentYep, LSP is Language Server Protocol, and NeoVim has (I believe) built-in support for it - as in, it can talk to a language server using LSP and use that for, for eg., \"Go to definition\" type shortcuts that are language-aware and more robust than pure syntax-based attempts. reply JetSetIlly 6 hours agoprevI'm happy with this \"maintenance mode\" tag. Personally speaking, so long as it continues to get security fixes and the build system is kept working, I'm a happy user. reply skydhash 5 hours agoparentI think so too. Any new features can be implemented as a plugin and one they're proven useful/stable enough, they can get refactored in the main project a la emacs. reply em-bee 13 minutes agoprevi don't understand the issues with the github account. they tried to supersede the ownership of the organization by declaring the owner dead which would have deactivated the account. but if the family has full access to the account, couldn't they just assign a new owner to the organization? reply ctenb 6 hours agoprevVim in maintainance mode makes a lot of sense to me. For larger features and refactors it makes more sense to turn to neovim, which has already undergone a lot of modernization work, at times breaking with old vim. reply mitch-crn 5 hours agoprevI started with vi, a long time ago. I do not remember when I switched, but it was most likely when I start using Linux. I will stay with it until I die at this point. http://crn.hopto.org/unix/#vim reply 1vuio0pswjnm7 1 hour agoprevOn Linux, I can compile a static vim 4.6 (1997 mar 13) with size of 514K If I compile static vim 9.1 (2024 jan 02) I get size of 3.7M When I compile a static nvi, I get a size of 575K. And the resulting binary segfaults On NetBSD I use nvi but for Linux I am using vim 4.6 reply amelius 5 hours agoprevBig fan of Vim. One thing I wish they fixed is the UX around opening a file twice. reply sundarurfriend 1 hour agoparentWhat specifically do you need fixed? reply amelius 1 hour agorootparentWell, if I open the same file that I already opened in another terminal, then it shows me the PID of the Vim process and a question about removing the lock file. What I normally do is kill the PID, and then open the file again and tell Vim to remove the lock file. This can be automated. But even better would be to allow me to take over the session in case there were any unsaved changes. reply henearkr 5 hours agoparentprevFwiw it's useful in split mode to view and edit simultaneously different parts of the same text. Not only in split mode, as you can also just switch between two buffers (or more) focused on different parts of the text. reply genieyclo 6 hours agoprevomg, I’ve used vim everyday for more than 15 years and did not know Bram died. RIP reply dredmorbius 5 hours agoparentDiscussed at the time on HN, August 2023:And yeah, that one hit really hard. reply wonger_ 3 hours agorootparentSeventh most upvoted HN post of all time, under Steven Hawking's and Steve Job's death. https://hn.algolia.com/ reply blame-troi 6 hours agoprev [–] Bram's passing and the split into basically two code bases (Vim, Neovim) and three customization languages pushed me over to Emacs. I switched back and forth for years, as many others do, but I didn't want to deal with having to use one or the other depending on scripting language. [edit: grammar] reply whiplash451 5 hours agoparentCurious to know whether you considered Visual Studio? I used vim happily for 15 years. When deciding to move on to something else, VSCode won over emacs. reply devjab 5 hours agorootparentI'm not the person you asked but I chose Emacs over VSC because it's just a better fit for a lot of things for me. I do think the telemetry Microsoft harvests through VSC is an issue to consider. While it is \"just\" metadata and no file content, they're getting your entire project structure down to file extensions. I don't see why I would want Microsoft to know what I'm working on. Anyway, the key point for me was ORG mode and that plugins for Go and C++ suck(ed?) in VSC. There are other things, the intellisense is slow, the vim plugin is terrible, the constant Microsoft product pushes are annoying, there is no Magit and so on. I think it's important to say that I don't dislike VSC as such at this point. Because I probably made it sound like I think it's terrible. I don't I think it's ok. I didn't mind using it for Typescript as an example. Over all I think it's average at best. I get why people use it, it's easy to setup. It's easy to share configurations and so on. I probably would have gone from vim to neovim if it wasn't for doom emacs though. I think the major advantage both emacs and vim have though is that they're always good. A lot of VSC users are now switching to Zed and that hamsterwheel will go on and on. With vim or emacs you'll never really have to change anything. reply skydhash 5 hours agorootparentPretty much the reasons I don't care about VSC. Sensible telemetry is not an issue for me, but pushing electron while there are much more performant solutions is one. I'm much more amenable to the Emacs and Vim's plugin/package solutions than VSC. And they're extraordinary stable. reply 8bitbeep 3 hours agorootparentprevI’m also unease about the open-source-but-not-really VSCode situation. I don’t know how useful an editor you can build from the available source, which is enough for me to not consider it seriously. I’ve been bitten before. reply sabellito 4 hours agorootparentprevWithout telemetry: https://vscodium.com/ reply pletnes 5 hours agorootparentprevVim (used to be?) insanely much faster on large files, it’s already installed everywhere, it has built-in highlights for all languages under the sun, configuration is easier to sync around (in my opinion). That said, I use both, and jetbrains. reply vkazanov 5 hours agorootparentprevWhy did it have to win? Emacs is fundamentally different, vscode is only convenient if there is no intention to change things. reply jkaplowitz 5 hours agorootparentprevTangent: Despite the misleadingly similar naming, Visual Studio and VSCode are two completely different MS products. I too use VSCode for most of my programming these days, but since I don't currently program for Win32 or UWP or .NET, I haven't seen a reason to install Visual Studio even though I have a Windows machine and they have a zero-cost Community Edition. reply blame-troi 3 hours agorootparentprevI'm familiar with Visual Studio (in my professional persona) and Code. Too many dials and knobs and displays. Telemetry is not a huge issue for me, but I understand why it bothers some people. I had been jumping back and forth between Vim and Emacs for several years in my hobbyist persona for years. When in hobbyist mode, I prefer to avoid anything that reminds me of my professional mode. VS and Code are noisy and intrusive. Emacs and Vim (and Neovim) are not. reply beretguy 5 hours agorootparentprevCan’t run vscode in that black box thingy. reply TMWNN 4 hours agoparentprev>Bram's passing and the split into basically two code bases (Vim, Neovim) and three customization languages pushed me over to Emacs. The resulting increase in height, strength, intelligence, and sexual appeal is no doubt a nice bonus. reply deskr 5 hours agoparentprev [–] Ah, someone has arrived with matches and kindling for the classic Emacs/Vi flame war. But I'm pouring water on it right away. I'm glad you found peace with Emacs. reply emptysongglass 5 hours agorootparentYes to all perusers about to make some holy war comment or crack a long-dead joke about editors, please don't. Most of us out there are happy you've found peace or joy with your editor of choice. Keep the holy war dead and let people decide for themselves what their editor should be. I use Emacs but I have many colleagues who use Neovim and we all are very supportive of each other. reply blame-troi 3 hours agorootparentprev [–] I certainly did not intend to ignite any flamewar. IMO both Emacs and (Neo)Vi(m) are fine editors. I wish more people would accept other people's right to have a personal preference. My choice of Emacs was based on what I see as a fragmenting of (Neo)Vi(m) ecosystem. When writing extensions, I was uncomfortable with a \"which of the three languages do I use\" decision. The only common language is old Vimscript, and both forks are moving on from it and neither seems willing to support the other fork's preferred language. That's their right, to be sure. It's just a deal breaker for me. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Following the passing of Vim founder Bram Moolenaar in 2023, the Vim community has reorganized to ensure the project's continued development, with Christian Brabandt as the current maintainer. - At VimConf 2024, it was discussed that the team has expanded, focusing on updating the Vim website, managing security, and enhancing communication channels. - Vim remains in maintenance mode, prioritizing community needs and integrating new features carefully, while continuing to support ICCF Holland, a charity established by Moolenaar."
    ],
    "commentSummary": [
      "Long-time Vim users are reflecting on the editor's future following the passing of its creator, Bram Moolenaar, with some content with its current stability and others exploring alternatives like Neovim for modern features.",
      "Neovim is gaining attention for its support of Lua scripting and modern enhancements, while some users are considering Emacs due to perceived fragmentation between Vim and Neovim.",
      "Discussions among users include topics like Treesitter's performance, Language Server Protocol (LSP) support, and comparisons with Visual Studio Code (VSCode), highlighting the flexibility and stability of these editors."
    ],
    "points": 138,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1736595779
  },
  {
    "id": 42659061,
    "title": "Flattening ASTs and other compiler data structures (2023)",
    "originLink": "https://www.cs.cornell.edu/~asampson/blog/flattening.html",
    "originBody": "Normal and flattened ASTs for the expression a * b + c. Arenas, a.k.a. regions, are everywhere in modern language implementations. One form of arenas is both super simple and surprisingly effective for compilers and compiler-like things. Maybe because of its simplicity, I haven’t seen the basic technique in many compiler courses—or anywhere else in a CS curriculum for that matter. This post is an introduction to the idea and its many virtues. Arenas or regions mean many different things to different people, so I’m going to call the specific flavor I’m interested in here data structure flattening. Flattening uses an arena that only holds one type, so it’s actually just a plain array, and you can use array indices where you would otherwise need pointers. We’ll focus here on flattening abstract syntax trees (ASTs), but the idea applies to any pointer-laden data structure. To learn about flattening, we’ll build a basic interpreter twice: first the normal way and then the flat way. Follow along with the code in this repository, where you can compare and contrast the two branches. The key thing to notice is that the changes are pretty small, but we’ll see that they make a microbenchmark go 2.4× faster. Besides performance, flattening also brings some ergonomics advantages that I’ll outline. A Normal AST Let’s start with the textbook way to represent an AST. Imagine the world’s simplest language of arithmetic expressions, where all you can do is apply the four basic binary arithmetic operators to literal integers. Some “programs” you can write in this language include 42, 0 + 14 * 3, and (100 - 16) / 2. Maybe the clearest way to write the AST for this language would be as an ML type declaration: type binop = AddSubMulDiv type expr = Binary of binop * expr * exprLiteral of int But for this post, we’ll use Rust instead. Here are the equivalent types in Rust: enum BinOp { Add, Sub, Mul, Div } enum Expr { Binary(BinOp, Box, Box), Literal(i64), } If you’re not a committed Rustacean, Box may look a little weird, but that’s just Rust for “a plain ol’ pointer to an Expr.” In C, we’d write Expr* to mean morally the same thing; in Java or Python or OCaml, it would just be Expr because everything is a reference by default.1 With the AST in hand, we can write all the textbook parts of a language implementation, like a parser, a pretty-printer, and an interpreter. All of them are thoroughly unremarkable. The whole interpreter is just one method on Expr: fn interp(&self) -> i64 { match self { Expr::Binary(op, lhs, rhs) => { let lhs = lhs.interp(); let rhs = rhs.interp(); match op { BinOp::Add => lhs.wrapping_add(rhs), BinOp::Sub => lhs.wrapping_sub(rhs), BinOp::Mul => lhs.wrapping_mul(rhs), BinOp::Div => lhs.checked_div(rhs).unwrap_or(0), } } Expr::Literal(num) => *num, } } My language has keep-on-truckin’ semantics; every expression eventually evaluates to an i64, even if it’s not the number you wanted.2 For extra credit, I also wrote a little random program generator. It’s also not all that interesting to look at; it just uses a recursively-increasing probability of generating a literal so it eventually terminates. Using fixed PRNG seeds, the random generator enables some easy microbenchmarking. By generating and then immediately evaluating an expression, we can measure the performance of AST manipulation without the I/O costs of parsing and pretty-printing. You can check out the relevant repo and try it out: $ echo '(29 * 3) - 9 * 5'cargo run $ cargo run gen_interp # Generate and immediately evaluate a random program. Flattening the AST The flattening idea has two pieces: Instead of allocating Expr objects willy-nilly on the heap, we’ll pack them into a single, contiguous array. Instead of referring to children via pointers, Exprs will refer to their children using their indices in that array. Let’s look back at the doodle from the top of the post. We want to use a single Expr array to hold all our AST nodes. These nodes still need to point to each other; they’ll now do that by referring to “earlier” slots in that array. Plain old integers will take the place of pointers. If that plan sounds simple, it is—it’s probably even simpler than you’re thinking. The main thing we need is an array of Exprs. I’ll use Rust’s newtype idiom to declare our arena type, ExprPool, as a shorthand for an Expr vector: struct ExprPool(Vec); To keep things fancy, we’ll also give a name to the plain old integers we’ll use to index into an ExprPool: struct ExprRef(u32); The idea is that, everywhere we previously used a pointer to an Expr (i.e., Box or sometimes &Expr), we’ll use an ExprRef instead. ExprRefs are just 32-bit unsigned integers, but by giving them this special name, we’ll avoid confusing them with other u32s. Most importantly, we need to change the definition of Expr itself: enum Expr { - Binary(BinOp, Box, Box), + Binary(BinOp, ExprRef, ExprRef), Literal(i64), } Next, we need to add utilities to ExprPool to create Exprs (allocation) and look them up (dereferencing). In my implementation, these little functions are called add and get, and their implementations are extremely boring. To use them, we need to look over our code and find every place where we create new Exprs or follow a pointer to an Expr. For example, our parse function used to be a method on Expr, but we’ll make it a method on ExprPool instead: -fn parse(tree: Pair) -> Self { +fn parse(&mut self, tree: Pair) -> ExprRef { And where we used to return a newly allocated Expr directly, we’ll now wrap that in self.add() to return an ExprRef instead. Here’s the match case for constructing a literal expression: Rule::number => { let num = tree.as_str().parse().unwrap(); - Expr::Literal(num) + self.add(Expr::Literal(num)) } Our interpreter gets the same treatment. It also becomes an ExprPool method, and we have to add self.get() to go from an ExprRef to an Expr we can pattern-match on: -fn interp(&self) -> i64 { +fn interp(&self, expr: ExprRef) -> i64 { - match self { + match self.get(expr) { That’s about it. I think it’s pretty cool how few changes are required—see for yourself in the complete diff. You replace Box with ExprRef, insert add and get calls in the obvious places, and you’ve got a flattened version of your code. Neat! But Why? Flattened ASTs come with a bunch of benefits. The classic ones most people cite are all about performance: Locality. Allocating normal pointer-based Exprs runs the risk of fragmentation. Flattened Exprs are packed together in a contiguous region of memory, which is good for spatial locality. Your data caches will work better because Exprs are more likely to share a cache line, and even simple prefetchers will do a better job of predicting which Exprs to load before you need them. A sufficiently smart memory allocator might achieve the same thing, especially if you allocate the whole AST up front and never add to it, but using a dense array removes all uncertainty. Smaller references. Normal data structures use pointers for references; on modern architectures, those are always 64 bits. After flattening, you can use smaller integers—if you’re pretty sure you’ll never need more than 4,294,967,295 AST nodes, you can get by with 32-bit references, like we did in our example. That’s a 50% space savings for all your references, which could amount to a substantial overall memory reduction in pointer-heavy data structures like ASTs. Smaller memory footprints mean less bandwidth pressure and even better spatial locality. And you might save even more if you can get away with 16- or even 8-bit references for especially small data structures. Cheap allocation. In flatland, there is no need for a call to malloc every time you create a new AST node. Instead, provided you pre-allocate enough memory to hold everything, allocation can entail just bumping the tail pointer to make room for one more Expr. Again, a really fast malloc might be hard to compete with—but you basically can’t beat bump allocation on sheer simplicity. Cheap deallocation. Our flattening setup assumes you never need to free individual Exprs. That’s probably true for many, although not all, language implementations: you might build up new subtrees all the time, but you don’t need to reclaim space from many old ones. ASTs tend to “die together,” i.e., it suffices to deallocate the entire AST all at once. While freeing a normal AST entails traversing all the pointers to free each Expr individually, you can deallocate a flattened AST in one fell swoop by just freeing the whole ExprPool. I think it’s interesting that many introductions to arena allocation tend to focus on cheap deallocation (#4) as the main reason to do it. The Wikipedia page, for example, doesn’t (yet!) mention locality (#1 or #2) at all. You can make an argument that #4 might be the least important for a compiler setting—since ASTs tend to persist all the way to the end of compilation, you might not need to free them at all. Beyond performance, there are also ergonomic advantages: Easier lifetimes. In the same way that it’s easier for your computer to free a flattened AST all at once, it’s also easier for humans to think about memory management at the granularity of an entire AST. An AST with n nodes has just one lifetime instead of n for the programmer to think about. This simplification is quadruply true in Rust, where lifetimes are not just in the programmer’s head but in the code itself. Passing around a u32 is way less fiddly than carefully managing lifetimes for all your &Exprs: your code can rely instead on the much simpler lifetime of the ExprPool. I suspect this is why the technique is so popular in Rust projects. As a Rust partisan, however, I’ll argue that the same simplicity advantage applies in C++ or any other language with manual memory management—it’s just latent instead of explicit. Convenient deduplication. A flat array of Exprs can make it fun and easy to implement hash consing or even simpler techniques to avoid duplicating identical expressions. For example, if we notice that we are using Literal expressions for the first 128 nonnegative integers a lot, we could reserve the first 128 slots in our ExprPool just for those. Then, when someone needs the integer literal expression 42, our ExprPool don’t need to construct a new Expr at all—we can just produce ExprRef(42) instead. This kind of game is possible with a normal pointer-based representation too, but it probably requires some kind of auxiliary data structure. Performance Results Since we have two implementations of the same language, let’s measure those performance advantages. For a microbenchmark, I randomly generated a program with about 100 million AST nodes and fed it directly into the interpreter (the parser and pretty printer are not involved). This benchmark is not very realistic: all it does is generate and then immediately run one enormous program. Some caveats include: I reserved enough space in the Vec to hold the whole program; in the real world, sizing your arena requires more guesswork. I expect this microbenchmark to over-emphasize the performance advantages of cheap allocation and deallocation, because it does very little other work. I expect it to under-emphasize the impact of locality, because the program is so big that only a tiny fraction of it will fit the CPU cache at a time. Still, maybe we can learn something. I used Hyperfine to compare the average running time over 10 executions on my laptop.3 Here’s a graph of the running times (please ignore the “extra-flat” bar; we’ll cover that next). The plot’s error bars show the standard deviation over the 10 runs. In this experiment, the normal version took 3.1 seconds and the flattened version took 1.3 seconds—a 2.4× speedup. Not bad for such a straightforward code change! Of that 2.4× performance advantage, I was curious to know how much comes from each of the four potential advantages I mentioned above. Unfortunately, I don’t know how to isolate most of these effects—but #4, cheaper deallocation, is especially enticing to isolate. Since our interpreter is so simple, it seems silly that we’re spending any time on freeing our Exprs after execution finishes—the program is about to shut down anyway, so leaking that memory is completely harmless. So let’s build versions of both of our interpreters that skip deallocation altogether4 and see how much time they save. Unsurprisingly, the “no-free” version of the flattened interpreter takes about the same amount of time as the standard version, suggesting that it doesn’t spend much time on deallocation anyway. For the normal interpreter, however, skipping deallocation takes the running time from 3.1 to 1.9 seconds—it was spending around 38% of its time just on freeing memory! Even comparing the “no-free” versions head-to-head, however, the flattened interpreter is still 1.5× faster than the normal one. So even if you don’t care about deallocation, the other performance ingredients, like locality and cheap allocation, still have measurable effects. Bonus: Exploiting the Flat Representation So far, flattening has happened entirely “under the hood”: arenas and integer offsets serve as drop-in replacements for normal allocation and pointers. What could we do if we broke this abstraction layer—if we exploited stuff about the flattened representation that isn’t true about normal AST style? The idea is to build a third kind of interpreter that exploits an extra fact about ExprPools that arises from the way we built it up. Because Exprs are immutable, we have to construct trees of them “bottom-up”: we have to create all child Exprs before we can construct their parent. If we build the expression a * b, a and b must appear earlier in their ExprPool than the * that refers to them. Let’s bring that doodle back again: visually, you can imagine that reference arrows always go backward in the array, and data always flows forward. Let’s write a new interpreter that exploits this invariant. Instead of starting at the root of the tree and recursively evaluating each child, we can start at the beginning of the ExprPool and scan from left to right. This iteration is guaranteed to visit parents after children, so we can be sure that the results for subexpressions will be ready when we need them. Here’s the whole thing: fn flat_interp(self, root: ExprRef) -> i64 { let mut state: Vec = vec![0; self.0.len()]; for (i, expr) in self.0.into_iter().enumerate() { let res = match expr { Expr::Binary(op, lhs, rhs) => { let lhs = state[lhs.0 as usize]; let rhs = state[rhs.0 as usize]; match op { BinOp::Add => lhs.wrapping_add(rhs), BinOp::Sub => lhs.wrapping_sub(rhs), BinOp::Mul => lhs.wrapping_mul(rhs), BinOp::Div => lhs.checked_div(rhs).unwrap_or(0), } } Expr::Literal(num) => num, }; state[i] = res; } state[root.0 as usize] } We use a dense state table to hold one result value per Expr. The state[i] = res line fills this vector up whenever we finish an expression. Critically, there’s no recursion—binary expressions can get the value of their subexpressions by looking them up directly in state. At the end, when state is completely full of results, all we need to do is return the one corresponding to the requested expression, root. This “extra-flat” interpreter has two potential performance advantages over the recursive interpreter: there’s no stack bookkeeping for the recursive calls, and the linear traversal of the ExprPool could be good for locality. On the other hand, it has to randomly access a really big state vector, which could be bad for locality. To see if it wins overall, let’s return to our bar chart from earlier. The extra-flat interpreter takes 1.2 seconds, compared to 1.3 seconds for the recursive interpreter for the flat AST. That’s marginal compared to how much better flattening does on its own than the pointer-based version, but an 8.2% performance improvement ain’t nothing. My favorite observation about this technique, due to a Reddit comment by Bob Nystrom, is that it essentially reinvents the idea of a bytecode interpreter. The Expr structs are bytecode instructions, and they contain variable references encoded as u32s. You could make this interpreter even better by swapping out our simple state table for some kind of stack, and then it would really be no different from a bytecode interpreter you might design from first principles. I just think it’s pretty nifty that “merely” changing our AST data structure led us directly from the land of tree walking to the land of bytecode. Further Reading I asked on Mastodon a while back for pointers to other writing about data structure flattening, and folks really came through (thanks, everybody!). Here are some other places it came up in a compilers context: Mike Pall attributes some of LuaJIT’s performance to its “linear, pointer-free IR.” It’s pointer-free because it’s flattened. Concordantly, a blog post explaining the performance of the Sorbet type-checker for Ruby extols the virtues of using packed arrays and replacing 64-bit pointers with 32-bit indices. The Oil shell project has a big collection of links all about “compact AST representation,” much of which boils down to flattening. Beyond just language implementation, similar concepts show up in other performance-oriented domains. I admit that I understand this stuff less, especially the things from the world of video games: A line of work from Purdue and Indiana is about compiling programs to operate directly on serialized data. Gibbon in particular is pretty much a translator from “normal”-looking code to flattened implementations. Flattening-like ideas appear a lot in data-oriented design, a broadly defined concept that I only partially understand. For example, Andrew Kelley argues in a talk on the topic for using indices in place of pointers. Check out this overview of arena libraries in Rust and its discussion of the ergonomics of arena-related lifetimes. Here’s a post comparing handles vs. pointers in game development that advocates for packing homogeneously typed objects into arrays and using indices to refer to them. Similar ideas show up in entity-component systems (ECS), a big idea from game development that I also don’t completely understand. This post covers many of the same locality-related themes as we did above. After I published this post, many people pointed me toward a post from last year by Inanna Malick that shows the same technique applied to same kind of toy “calculator” language implemented in Rust. That post also uses recursion schemes, an elegant idea from the Haskell world that helps abstract over different concrete representations. I highly recommend checking that post out. In Rust, using Expr there would mean that we want to include other Exprs inline inside the Expr struct, without any pointers, which isn’t what we want. The Rust compiler doesn’t even let us do that—it would make Exprs infinitely large! ↩ The totality-at-all-costs approach uses Rust’s wrapping integer arithmetic functions and abuses checked division to boldly assert that dividing by zero yields zero. ↩ A MacBook Pro with an M1 Max (10 cores, 3.2 GHz) and 32 GB of main memory running macOS 13.3.1 and Rust 1.69.0. ↩ I added a feature flag that enables calls to Rust’s std::mem::forget. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=42659061",
    "commentBody": "Flattening ASTs and other compiler data structures (2023) (cornell.edu)137 points by aw1621107 23 hours agohidepastfavorite36 comments jgrowl 12 hours agoI thought a reddit comment on this article had an interesting point: https://www.reddit.com/r/rust/comments/1d3b356/my_new_favori... [–]Timzhy0 3 points 7 months ago Btw I think one can go a step further than the author, there is no need to keep two explicit ExprRef baked in a binary node (lhs, rhs). You can exploit locality, basically the AST, seen it the LISP way, is just an arbitrarily nestable list, where elements are atoms or other lists. Hence all you need to know is where each list ends (and if it's an atom you can assume it spans one node) and actually one bit to know if it is the last entry in the list is quite ergonomic as well (because then you can distinguish whether moving next slot in the AST means there is a sibling). Basically it's easier to keep it sync while constructing and takes up less memory per node. I pay 40 bits per node, stored interleaved for best cache locality (some unaligned accesses but I think it's still worthwhile), 8 bits for the tag, 32 for the data, if data is bigger, 32 is an index into some auxiliary segment (basically a ptr). reply catgary 26 minutes agoparentAn arbitrarily nestable list is a tree, no? reply dmagyari 21 hours agoprev\"Instead of allocating Expr objects willy-nilly on the heap, we’ll pack them into a single, contiguous array.\" Zig compiler pipeline (AST, Zir, Air, Sema) does exactly this on all layers. Not only contiguous, but instead of array-of-structs it is struct-of-arrays, so walking the tree is even more cache friendly. For AST see: https://github.com/ziglang/zig/blob/master/lib/std/zig/Ast.z... reply gritzko 11 hours agoparentI work on a C dialect where everything is flattened. JSON and other trees in particular. Binary heaps are flat, merge sort and iterator heaps are absolutely great, can build LSM databases with that. Stacks, circular buffers, hash maps, etc, all flat. Templated output (PHP like) is done by a flat data structure. https://github.com/gritzko/librdx/blob/master/abc/B.md Apart from locality and lifetimes, these flat data structures improve composability. When every data structure is a flat buffer, you can mmap them or zip them or send them by the network, all by the same routine. They are uniform like bricks, in a sense. reply rurban 3 hours agorootparentI worked in a language where all datastructures were \"flattened\", could be trivially serialized to disk, and read in again. Called print and read. The language was called lisp. All flat, just parens. Some of my compilers export the AST as lisp trees. Much smaller and more readable than json, and it can be executed. Uniform like bricks reply vanderZwan 23 minutes agorootparent> All flat, just parens. So not flat then. Prefix is not postfix. Forth, and most concatenative languages, are much closer to actually bein, flat. Lisp is trivial to flatten, but that's not the same thing. reply agumonkey 18 hours agoparentprevMakes me wonder if people in APL/J/K community have not been influenced or influencing this kind of technique. IIRC Aaron Hsu does tree processing through arrays (but i'm not skilled enough to analyze his code) reply gsf_emergency 11 hours agorootparentDo you have a link to such an example of Aaron's code? Thank you in advance! reply 082349872349872 7 hours agorootparenthttps://scholarworks.iu.edu/dspace/items/3ab772c9-92c9-4f59-... Iverson's 1962 book also mentions tree representations, see pp45-62: https://archive.org/details/aprogramminglanguage1962/page/n6... reply agumonkey 7 hours agorootparentprevCan't remember where exactly but he did demo his code in talks/conferences with links. reply torginus 7 hours agoprevI guess Rust's contribution to high performance programming is that its borrow checker is so loathsome that it pushes people into using ideas like ECS or arenas just to not have to bother with it. reply Agraillo 7 hours agoprevAbout 10 years ago working with AST trees I (re)invented a flat structure representing trees in a flat array. It reminds me of what is described here but not exactly. In my case I needed only two indices per node: total sub-region length of all the children and sub-children and parent index (so no need to have indices of all children). Total sub-length basically can be interpreted as the index of the next sibling. With such a structure it's easy/cheap to execute FindFirstChild/FindNextSibling. Later the theory behind such structures was revealed as \"Nested set model\" [1]. The article seems to not mention the internal representation, but I think that the implementation should use something like my solution, so fixed number of references per node [1] https://en.wikipedia.org/wiki/Nested_set_model reply kazinator 20 hours agoprev> Instead of allocating Expr objects willy-nilly on the heap, we’ll pack them into a single, contiguous array. This happens naturally if you bump-allocate them in a garbage-collected run-time, particularly under a copying collector. Free lists also tend to co-locate because they are produced during sweep phases of GC which run through heaps in order of address. Don't make me bring out the L word for the billionth time. > A flat array of Exprs can make it fun and easy to implement hash consing OK, it's not a case of L-ignorance, just willful neglect. reply layer8 10 minutes agoparentAnd if you don’t need more than 32 GB of heap space, the JVM also gives you the ability to reduce reference sizes to 32 bits, with compressed references. (Due to alignment requirements, the lower 3 bits of a pointer are zero and hence do not need to be stored, which effectively gives you a 35-bit address space.) Of course, the presence of object headers counteracts this to a certain extent. reply samps 20 hours agoparentprevFWIW I did acknowledge this in the article: > A sufficiently smart memory allocator might achieve the same thing, especially if you allocate the whole AST up front and never add to it > Again, a really fast malloc might be hard to compete with—but you basically can’t beat bump allocation on sheer simplicity. reply Tarean 4 hours agoprevTwee (an equational theorem prover in Haskell used by quickspec) has an interesting take on this. Terms are slices of arrays, but you get a normal interface including pattern matching via synonyms. It can also be nice to use phantom types of your references (array offsets), so if you project them into flat view types you can do so type safely Requires the language to have something equivalent to pattern synonyms to be as invisible as twee, though. In twee a TermList is a slice of a bytearray (two ints for offset/length plus a pointer). And a term is an int for the function symbol and an unpacked TermList for the arguments. The pattern match synonyms load a flat representation from the array into a view type, and the allocation of the view type cancels out with the pattern matching so everything remains allocation free. https://hackage.haskell.org/package/twee-lib-2.4.2/docs/Twee... reply userbinator 10 hours agoprevRediscovering techniques that were somewhat well-known in the 70s and 80s. See also: https://en.wikipedia.org/wiki/Binary_heap reply torginus 7 hours agoparentYup, and chances are if you're using a good C++ stl implementation, most containers already use packed storage internally. It doesn't even have a heap data structure, it uses an std::vector, with a set of helper functions. reply Taniwha 10 hours agoparentprevheh - I built compilers this back in the 70s because the machine I was working on didn't really do pointers as a 1st class data structure (B6700 algol), it's not really surprising finding someone doing something similar in another language that makes pointers difficult to deal with reply emptysea 21 hours agoprevRust-analyzer uses a similar technique for parsing https://github.com/rust-lang/rust-analyzer/blob/master/crate... which then gets fed into https://github.com/rust-analyzer/rowan (lossless syntax tree) reply bradrn 18 hours agoparentThere is a longer overview of it here: https://github.com/rust-lang/rust-analyzer/blob/master/docs/... reply cardanome 21 hours agoprevAmazing article, very good advice to keep your data structures flat. Adding to that, it also makes editing the AST vastly more efficient. I have discovered that principle on my own when I worked on an editor that directly operated on the AST instead of text. I found manipulating the tree-style AST so painful, constantly traversing the tree and all. Once I made it flat, my life was a hell lot easier. You can just directly index any part of AST in linear time. reply ww520 21 hours agoprevThis is a fantastic idea. AST works well in an array based allocation block since it has no need for freeing individual nodes. It’s an add-only allocation. reply JonChesterfield 9 hours agoparentWhat about transforming the AST after it is built, or deriving a new tree which partly aliases the original in persistent structure fashion? reply ndesaulniers 20 hours agoprevCool! Carbon is doing exactly this. I had asked leads if there was a paper on this approach, but they didn't have anything for me. I'll send them this post! reply wrsh07 2 hours agoparentChandler discusses it in this video though! https://youtu.be/ZI198eFghJk You get some traversals for free with this layout (preorder, reverse post order). Can search for subtrees with string searching algorithms or more complex things with regex. reply benatkin 19 hours agoparentprevZig uses a MultiArrayList which sounds similar https://mitchellh.com/zig/parser reply dang 13 hours agoprevRelated: Flattening ASTs and other compiler data structures (2023) - https://news.ycombinator.com/item?id=42181603 - Nov 2024 (2 comments) Flattening ASTs and other compiler data structures - https://news.ycombinator.com/item?id=36559346 - July 2023 (81 comments) reply hencq 18 hours agoprevAs the article mentions, this makes it quite similar to a bytecode vm. I think the traditional wisdom is that an AST walker is easy to write, but for speed you'd want a bytecode interpreter. It'd be interesting to see how close the performance gets with this flattened AST. In practice I think there are more differences. E.g. AST interpreters tend to pass environments around while bytecode interpreters often store these on a stack (though I guess there's nothing stopping you from doing this with an AST either). I wonder if there's some goldilocks zone for ease of implementation with decent performance. reply kazinator 18 hours agoparentIf you instead flatten the expression tree into RPN, then you can execute it like that, with a stack machine. I seem to recall that the Red Dragon Book (Compilers: Principles, Techniques and Tools, Aho, Sethi, Ullman [1988]) describes a technique whereby intermediate code is represented in RPN, and transformations are performed by pattern matches on it. reply finnh 17 hours agorootparentThe sample flat program in the post is exactly RPN, no? reply samps 16 hours agorootparentI think it would be more like RPN if it used a stack, and operands were specified as relative offsets (i.e., stack offsets). In the version I wrote, operands are still represented as absolute offsets in the expression table. reply efitz 6 hours agoprevDid the author just reinvent Forth? reply pfdietz 4 hours agoprevOne advantage to this is the ease with which it handles ephemeral annotations. Suppose you want to attach additional information to some of the nodes of the AST. Different algorithms on the AST will attach different information; you don't necessarily need them all at the same time or know ahead of time what you'll need. With nodes, you have to have some sort of node/value hash table, or hang a key/value map off each node. But with this flattened representation, each datum gets its own flat array as well, which can be independently allocated and deallocated. One other thing I noticed about this flat representation is that it throws static typing into a cocked hat! All you have to refer to other nodes is indices. All different kinds of nodes are stored in the same array. reply jonstewart 5 hours agoprev [–] You can also be smart about memory with lexing for great winnage. Have a struct for your tokens that has an enum for your token type and either pointer or indices or a string_view (or a &str but lol lotsa luck with the borrow checker). You can then have a vector of your token structs for fast allocation and iteration and you have a slice for the token back into the original input, no substring copying. reply uecker 3 hours agoparent [–] Yes, the C FE I write (in C) does exactly this and othen utputs in one pass a flattened intermediate code. I did not see at as AST because it does semantic analysis during parsing, but it sitll has all the information. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses data structure flattening, a technique for compilers that optimizes abstract syntax trees (ASTs) by using a single array with indices instead of pointers. - This method enhances performance by improving memory locality, reducing reference size, and simplifying memory management, resulting in a 2.4× speedup in a microbenchmark. - The technique is notably used in Rust projects and offers ergonomic benefits, such as easier memory management, and is compared to bytecode interpreters for further performance improvements."
    ],
    "commentSummary": [
      "The discussion focuses on optimizing Abstract Syntax Trees (ASTs) by storing them in flat, contiguous arrays to improve performance and memory efficiency.",
      "This method enhances cache locality and simplifies memory management, drawing comparisons to techniques in languages like Lisp and the Zig compiler.",
      "The conversation also references historical methods and similar techniques in other programming languages, emphasizing the benefits of flat data structures for composability and ease of manipulation."
    ],
    "points": 137,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1736537030
  },
  {
    "id": 42666572,
    "title": "De-smarting the Marshall Uxbridge Bluetooth speaker",
    "originLink": "https://tomscii.sig7.se/2025/01/De-smarting-the-Marshall-Uxbridge",
    "originBody": "",
    "commentLink": "https://news.ycombinator.com/item?id=42666572",
    "commentBody": "De-smarting the Marshall Uxbridge Bluetooth speaker (sig7.se)128 points by fanf2 3 hours agohidepastfavorite34 comments mysteria 1 hour agoIf you're going through all the effort to design a PCB have you thought about driving the I2S input digitally? I skimmed through the AD85050 datasheet and it has internal DSP functionality which would have been already tuned for the drivers and box by Marshall. The reason powered boxes sound decent despite their relatively cheap hardware is because of the extensive processing they have in the background to compensate for any hardware defiencies. As the AD85050 has a stereo I2S input there's a possibility for the actual crossover to be either done on the amp chip itself (with the same signal driving both channels) or done on the Amlogic SOC. The latter would be ugly as you would need another DSP chip on your board to do the crossover functionality, or perhaps you could program the AD85050 via I2C to add the appropiate low and high pass filters. A two channel A/D converter would work on the front end, as you could drive both channels with a single analog input to get a stereo I2S out with duplicate channels to drive the amp. A USB input would be much messier if you want true stereo using two speakers unless you plan on doing routing on the software side. With SPDIF you probably could get away with splitting the signal and using a SPDIF to I2S converter chip in each speaker, but you would still need some way to separate out the left and right channels. The AD85050 has mixing functionality via I2C which may help with that. And of course, all this might be more work than desigining an amp in the first place, and it really depends if you want to explore the analog or digital side of things. reply f1shy 51 minutes agoparent> The reason powered boxes sound decent despite their relatively cheap hardware is because of the extensive processing they have in the background to compensate for any hardware defiencies. I will not argue that that could be one ingredient, but a couple of months ago I did a toy for my kids, I bought decent speakers, placed them in a cheap plastic box, and was absolutely amazed bybthe sound quality. The amplifier is a sub 1 dollar class D bought in a Raspberry Pi shop. No processing at all. If the box is sturdy and sealed, and the speaker is good, is incredible what you can do. reply mysteria 3 minutes agorootparentMost cheap amp ICs perform well when they're outputting less than a watt, with distortion barely audible. Try connecting the same amp board to your main HiFi system if you have one and do some listening tests against the original HiFi amp. Then turn it up and it's a completely different story. As always the speakers are the crucial part and having decent speakers will make a big difference. What a DSP can do however is correct bad speakers to some degree. A typical cheap computer speaker has a muddy midrange, can't reproduce past 13 kHz or so, and has little bass due to the small driver. With DSP the manufacturer would typically low pass the amp input, smooth out the nonlinear frequency response, lift the bass a bit, and apply limiting to prevent the user from blowing up his system. The speakers are still limited by physics but the manufacturer is in this case able to opt for cheaper drivers to get similar sound quality. reply acchow 47 minutes agorootparentprevI'm interested in which speaker and amp those were. Also, the plastic box :) reply bayindirh 44 minutes agorootparentNot the OP, but if you have a little budget, HifiBerry's AMP2 [0] sounds great. After my dad gave his Hi-Fi stack to me (due to having no space at home), I built a small system with this and connected to a set of passive 2.1 Kenwood Hi-Fi speakers for him. They sound amazing, plus HiFiBerry OS is superb for connectivity. I just want to note that software is built with collaboration of Bang & Olufsen. Both hardware and software oozes quality. [0]: https://www.hifiberry.com/shop/boards/dealing-with-blocked-p... reply zxcvgm 33 minutes agoparentprevI have the same thoughts about the approach, and I'm actually working (on the back burner) a similar thing. It's a harman kardon \"smart\" speaker with a similar design where the brains are on a separate daughterboard and that's now fried. I've already figured out the control signals and have designed a new daugterboard with an ESP32 to drive the I2S output. I just need to figure out how to downmix the audio to mono and to DSP the L/R channels into tweeter/bass outputs, or to find some code already out there that does this. Any help/pointers here would be appreciated! reply bayindirh 52 minutes agoparentprevMarshall speakers, from my experience, has a brand sound signature, and that tuning is not very optimal for every genre of music. Replacing the DSP with a simpler amplifier may allow to get more detailed sound from the drivers and the box themselves and may create a more pleasant listening experience. From what I have seen, the drivers seem pretty full-size for that box, and any disturbing sound characteristic can be tuned with a simple equalizer. A more dynamic approach might create audibly weird sound profile if done wrong. Modern DSPs are magic, but I still prefer an audio pipeline where things show their deficiencies and not hide things real-time. reply AstroNoise58 49 minutes agoparentprevI assume you mean AD85050 (rather than AD8255). And yes, the last paragraph before \"Going all-in\" is about the idea of driving the I2S. But the I2C config sent to the ESMT chip would have had to be reverse-engineered as well... reply mysteria 24 minutes agorootparentFixed, thanks. Somehow the title of the datasheet pdf is AD8255 despite the chip being an AD85050. reply gavinuhran 2 hours agoprevI have this speaker and cannot believe how annoying the smart features are. I'll be talking on the phone in my apartment and the speaker will think I'm trying to prompt it. \"SORRY. YOUR DEVICE IS NOT CONNECTED TO THE INTERNET. PLEASE CHECK YOUR BLUETOOTH SETTINGS AND TRY AGAIN.\" (at max volume!) It's unbelievable. I'm not an EE, but would love to know how I can disable these incredible unsmart features. reply seba_dos1 1 hour agoparentThere's an article that tells you that. I believe it's linked somewhere above this comment. reply bonzini 57 minutes agorootparentThe article removes all the smart features (not just the annoying ones) and requires pretty serious knowledge of analog electronics. Probably it doesn't fit the bill for the parent comment. reply dheera 32 minutes agorootparentI mean, it's a speaker in a box, so you could also just snip the speaker wires, ditch the circuit board, solder some extension wires, and plug it into an external audio amplifier box of your choice. If you go that route you don't really need much EE knowledge. (This is also only if you already have this box and want to reuse it. Otherwise I would just go to your next neighborhood garage sale and pick up some good speakers for $10) reply emidoots 1 hour agoparentprevReturn that crap and buy something like Audio Pro speakers instead reply tlhunter 1 hour agoprevLately I've been wondering if there's a way to do this to Smart TVs. Personally, I like the name \"stupify\" better ;) reply rotifer 44 minutes agoparentA year or so ago I bought a Hisense 65U88KM, which comes with Google TV. During the setup procedure it asked me if I wanted to enable the \"smart\" features, such as Google TV, the camera and microphone, or connect it to a network. I said no to all of them, and that was that. Now it just acts as a dumb screen for my Apple TV box. reply 05 52 minutes agoprevThere’s a project to load OpenWRT onto LinkPlay A31 [0], might be easier than basically replacing the insides.. [0] https://github.com/hn/linkplay-a31 reply Mathnerd314 2 hours agoprevI'm curious about price - sure, the speakers were free ($240 value), but I don't think printing up a PCB is cheap, and those are some pretty big capacitors. reply jdietrich 1 hour agoparentJLCPCB have changed the game. Five 2-layer PCBs of up to 100x100mm cost just $3.50 including global shipping. Things get more expensive if you stray from their standard specs, but you're still looking at just a few dollars per board. https://jlcpcb.com/ The biggest electrolytic caps in this circuit cost $3.29 each in qty 1, but they're fancy \"audio-grade\" Nichicon caps; a standard-grade capacitor of that size would cost you $1.68 if you want a Japanese brand, or as little as $0.36 if you can settle for a Chinese brand. reply f1shy 47 minutes agorootparentAnd you can even order all or the SMD components soldered for less money that you can buy solder and other consumables for soldering never mind the time. reply stavros 2 hours agoparentprevA PCB like that would cost around $1 each, if you got 10 or so, so it's not expensive at all. I don't know how much assembly costs, but I'd be surprised if the total was over $20. reply MrBuddyCasino 2 hours agoprevThis looks an EE‘s approach who hasn’t had a lot of exposure to speaker design. You need to consider Thiele/Small parameters of the chassis, enclosure volume, baffle design and a million other factors to design a proper crossover. You can’t just ltspice your crossover and call it a day. VituixCad would be a more appropriate solution. And then you actually have to measure! Replacing and amp and „smart“ crap is easy if there is an analogue crossover you can reverse-engineer, if it was just some DSP things get difficult quickly (unless its just a single broadband chassis, but even then…). And no, you can’t use pre-built „standard“ crossovers or some calculator on a website either. But other than that, nice that he saved some hardware. reply AstroNoise58 38 minutes agoparentDo not underestimate audio circuit tuning based on listening tests. Good ears and patience can substitute a lot of lab equipment dollars, especially for a hobbyist. reply ipsum2 2 hours agoparentprevYou seem to be very knowledgeable about this subject. I have some Google Nest Audio speakers that sound fantastic, but have the same problems as bluetooth speakers, and lack a 3.5mm input jack to convert into normal speakers. Do you have any recommendations on how to do this, for someone with minimal audio knowledge and some basic EE? reply liminalsunset 1 hour agorootparentThe Google Nest Audio speakers are kind of a special case. They only sound good because they use a sealed, extremely rigid cast aluminum sealed enclosure with a high excursion driver. The performance of these speakers with a regular crossover and amp will be poor, due to the low efficiency of the enclosure/small driver. To get around this, Google put in the TI TAS5825M smart audio amp. By measuring the speaker parameters through V/I measurement and a model, it drives the speaker in a closed loop way with far more power than it would actually be able to handle nornally to compensate for the resistance from the enclosure air pressure, and throttles to maintain the coil at a safe temperature. The chip also does DSP to compress the audio signal, cutting the peaks off the bass as needed when the volume is turned up so volume is maintained at the cost of bass. One way to explore could be to just feed I2S audio from an I2S ADC i.e. PCM1808 to the digital input of the amplifier. The processing is internal to the amp so theoretically you won't lose the tuning. However this may turn out to be a relatively annoying reverse engineering project with fine magnet wire involved. Note: I2S is different from I2C - the amp will likely have both. You will likely need to keep the original system around to program the amp over I2C (or capture the transaction and replay it) - otherwise you will likely get no audio. The \"raw\" audio performance of this device (just an amplifier connected directly to the internal speaker and dsp on the computer) is impressive, kicking out bass down to 40Hz. It will, however, not last long like that. Reports online are that these blow speakers easily even when used with the default amplifier. I would recommend that if 3.5mm input is desired, to replace them altogether with the IK Multimedia iLoud Micro Monitors. These have sound quality just as good as the Google at similar size, with the same DSP tricks, but have regular inputs and no smart features. reply ipsum2 1 hour agorootparentThanks for the comprehensive answer, I'll look into the I2S audio solution. You're right that iLoud Micros sound similar, they're 3x the price (The Nest Audios were sold at $50/each on sale). Definitely worth it, I just like tinkering with things. reply encom 2 hours agoparentprev>You can’t just ltspice your crossover and call it a day. I'm sure that's true, but how important is that really for a set of crappy plastic speakers? reply mgaunard 2 hours agoprev [–] what exactly is the problem with smart speakers? Most people just want to stream Spotify reply ipsum2 2 hours agoparentIt's mentioned in the article, but briefly 1) latency (300ms+) and 2) random voices at max volume while you're trying to listen to music. reply netsharc 2 hours agoparentprev [–] \"We are delighted to announce an update to our smart speaker line: we are sunsetting the online services for this model. Your devices will be disabled in March 2025. They will no longer work, and please bring them to an electronics recycling center near you. You are eligible for a voucher to get a discount when you upgrade to a newer model.\" reply mynameisvlad 2 hours agorootparent [–] This is pretty much word for word what has already happened at least once. Sonos at least reversed their decision to disable the devices when they sunset S1. They just made them incompatible with their current system but allowed people to download the old apps. Unfortunately, devices that were already marked for trade-in before they made the decision are still completely bricked. reply f1shy 43 minutes agorootparentI bought a Yamaha CD-N301, came with a web radio, where you could setup a user, and make an index of URIs with webradios. After that you could select the station from a menu in the front. 3 months after purchase, the service started costing 3 dollars per month. I only use the CD function now. I contacted Yamaha, the answer was “it still works, we never promised it would be forever free” PoS reply lelandfe 1 hour agorootparentprev [–] \"Sustainability is non-negotiable!\" was my favorite platitude from their website at that time. As Sonos needlessly condemned 6 year old speakers to the landfill. reply exe34 1 hour agorootparent [–] corporations are functionally psychopaths. they will say whatever you need to hear to spend money and they will do whatever it takes to maximise profit, even when it goes contrary to what they have said. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The discussion focuses on modifying the Marshall Uxbridge Bluetooth speaker to eliminate its \"smart\" features, involving technical aspects like designing a Printed Circuit Board (PCB) for digital I2S input.",
      "Users debate the role of Digital Signal Processing (DSP) in achieving good sound quality, with some emphasizing its necessity to compensate for hardware limitations.",
      "The conversation also covers the challenges of modifying smart devices and the affordability of modern PCB production and components."
    ],
    "points": 119,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1736610122
  },
  {
    "id": 42660377,
    "title": "OpenAI's bot crushed this seven-person company's web site 'like a DDoS attack'",
    "originLink": "https://techcrunch.com/2025/01/10/how-openais-bot-crushed-this-seven-person-companys-web-site-like-a-ddos-attack/",
    "originBody": "On Saturday, Triplegangers CEO Oleksandr Tomchuk was alerted that his company’s e-commerce site was down. It looked to be some kind of distributed denial-of-service attack. He soon discovered the culprit was a bot from OpenAI that was relentlessly attempting to scrape his entire, enormous site. “We have over 65,000 products, each product has a page,” Tomchuk told TechCrunch. “Each page has at least three photos.” OpenAI was sending “tens of thousands” of server requests trying to download all of it, hundreds of thousands of photos, along with their detailed descriptions. “OpenAI used 600 IPs to scrape data, and we are still analyzing logs from last week, perhaps it’s way more,” he said of the IP addresses the bot used to attempt to consume his site. “Their crawlers were crushing our site,” he said “It was basically a DDoS attack.” Triplegangers’ website is its business. The seven-employee company has spent over a decade assembling what it calls the largest database of “human digital doubles” on the web, meaning 3D image files scanned from actual human models. It sells the 3D object files, as well as photos — everything from hands to hair, skin, and full bodies — to 3D artists, video game makers, anyone who needs to digitally recreate authentic human characteristics. Tomchuk’s team, based in Ukraine but also licensed in the U.S. out of Tampa, Florida, has a terms of service page on its site that forbids bots from taking its images without permission. But that alone did nothing. Websites must use a properly configured robot.txt file with tags specifically telling OpenAI’s bot, GPTBot, to leave the site alone. (OpenAI also has a couple of other bots, ChatGPT-User and OAI-SearchBot, that have their own tags, according to its information page on its crawlers.) Robot.txt, otherwise known as the Robots Exclusion Protocol, was created to tell search engine sites what not to crawl as they index the web. OpenAI says on its informational page that it honors such files when configured with its own set of do-not-crawl tags, though it also warns that it can take its bots up to 24 hours to recognize an updated robot.txt file. As Tomchuk experienced, if a site isn’t properly using robot.txt, OpenAI and others take that to mean they can scrape to their hearts’ content. It’s not an opt-in system. To add insult to injury, not only was Triplegangers knocked offline by OpenAI’s bot during U.S. business hours, but Tomchuk expects a jacked-up AWS bill thanks to all of the CPU and downloading activity from the bot. Robot.txt also isn’t a failsafe. AI companies voluntarily comply with it. Another AI startup, Perplexity, pretty famously got called out last summer by a Wired investigation when some evidence implied Perplexity wasn’t honoring it. EACH OF THESE IS A PRODUCT, WITH A PRODUCT PAGE THAT INCLUDES MULTIPLE MORE PHOTOS. USED BY PERMISSION. IMAGE CREDITS:TRIPLEGANGERS (OPENS IN A NEW WINDOW) Can’t know for certain what was taken By Wednesday, after days of OpenAI’s bot returning, Triplegangers had a properly configured robot.txt file in place, and also a Cloudflare account set up to block its GPTBot and several other bots he discovered, like Barkrowler (an SEO crawler) and Bytespider (TokTok’s crawler). Tomchuk is also hopeful he’s blocked crawlers from other AI model companies. On Thursday morning, the site didn’t crash, he said. But Tomchuk still has no reasonable way to find out exactly what OpenAI successfully took or to get that material removed. He’s found no way to contact OpenAI and ask. OpenAI did not respond to TechCrunch’s request for comment. And OpenAI has so far failed to deliver its long-promised opt-out tool, as TechCrunch recently reported. This is an especially tricky issue for Triplegangers. “We’re in a business where the rights are kind of a serious issue, because we scan actual people,” he said. With laws like Europe’s GDPR, “they cannot just take a photo of anyone on the web and use it.” Triplegangers’ website was also an especially delicious find for AI crawlers. Multibillion-dollar-valued startups, like Scale AI, have been created where humans painstakingly tag images to train AI. Triplegangers’ site contains photos tagged in detail: ethnicity, age, tattoos versus scars, all body types, and so on. The irony is that the OpenAI bot’s greediness is what alerted Triplegangers to how exposed it was. Had it scraped more gently, Tomchuk never would have known, he said. “It’s scary because there seems to be a loophole that these companies are using to crawl data by saying “you can opt out if you update your robot.txt with our tags,” says Tomchuk, but that puts the onus on the business owner to understand how to block them. TRIPLEGANGERS’ SERVER LOGS SHOWED HOW RUTHELESSLY AN OPENAI BOT WAS ACCESSING THE SITE, FROM HUNDREDS OF IP ADDRESSES. USED BY PERMISSION. He wants other small online businesses to know that the only way to discover if an AI bot is taking a website’s copyrighted belongings is to actively look. He’s certainly not alone in being terrorized by them. Owners of other websites recently told Business Insider how OpenAI bots crashed their sites and ran up their AWS bills. The problem grew magnitudes in 2024. New research from digital advertising company DoubleVerify found that AI crawlers and scrapers caused an 86% increase in “general invalid traffic” in 2024 — that is, traffic that doesn’t come from a real user. Still, “most sites remain clueless that they were scraped by these bots,” warns Tomchuk. “Now we have to daily monitor log activity to spot these bots.” When you think about it, the whole model operates a bit like a mafia shakedown: The AI bots will take what they want unless you have protection. “They should be asking permission, not just scraping data,” Tomchuk says. TechCrunch has an AI-focused newsletter! Sign up here to get it in your inbox every Wednesday. Topics AI, Exclusive, GPTBot, OpenAI, website crawler Julie Bort View Bio Most Popular Mark Zuckerberg defends Meta’s latest pivot in three-hour Joe Rogan interview Maxwell Zeff How OpenAI’s bot crushed this seven-person company’s website ‘like a DDoS attack’ Julie Bort How to delete Facebook, Instagram, and Threads Rebecca Bellan Meta eliminates DEI programs Dominic-Madori Davis Gumloop, founded in a bedroom in Vancouver, lets users automate tasks with drag-and-drop modules Kyle Wiggers India’s digital payments strategy is cutting out Visa and Mastercard Manish Singh Mark Zuckerberg gave Meta’s Llama team the OK to train on copyrighted works, filing claims Kyle Wiggers Newsletters See More Subscribe for the industry’s biggest tech news TechCrunch Daily News Every weekday and Sunday, you can get the best of TechCrunch’s coverage. Add TechCrunch Daily News to your subscription choices TechCrunch AI TechCrunch's AI experts cover the latest news in the fast-moving field. Add TechCrunch AI to your subscription choices TechCrunch Space Every Monday, gets you up to speed on the latest advances in aerospace. Add TechCrunch Space to your subscription choices Startups Weekly Startups are the core of TechCrunch, so get our best coverage delivered weekly. Add Startups Weekly to your subscription choices No newsletters selected. Subscribe By submitting your email, you agree to our Terms and Privacy Notice. Related Social Everything you missed at CES 2025 Cody Corrall 1 hour ago Transportation I got soaked driving the Arc Sport electric boat Kirsten Korosec 1 hour ago AI Nvidia’s AI empire: A look at its top startup investments Marina Temkin 4 hours ago Latest in AI See More AI Nvidia’s AI empire: A look at its top startup investments Marina Temkin 4 hours ago AI Microsoft accuses group of developing tool to abuse its AI service in new lawsuit Kyle Wiggers 20 hours ago AI New OpenAI job listings reveal the company’s robotics plans Kyle Wiggers 22 hours ago",
    "commentLink": "https://news.ycombinator.com/item?id=42660377",
    "commentBody": "OpenAI's bot crushed this seven-person company's web site 'like a DDoS attack' (techcrunch.com)111 points by vednig 22 hours agohidepastfavorite102 comments dang 19 hours agoRecent and related: AI companies cause most of traffic on forums - https://news.ycombinator.com/item?id=42549624 - Dec 2024 (438 comments) ericholscher 21 hours agoprevThis keeps happening -- we wrote about multiple AI bots that were hammering us over at Read the Docs for >10TB of traffic: https://about.readthedocs.com/blog/2024/07/ai-crawlers-abuse... They really are trying to burn all their goodwill to the ground with this stuff. reply PaulHoule 21 hours agoparentIn the early 2000s I was working at a place that Google wanted to crawl so bad that they gave us a hotline number to crawl if their crawler was giving us problems. We were told at that time that the \"robots.txt\" enforcement was the one thing they had that wasn't fully distributed, it's a devilishly difficult thing to implement. It boggles my mind that people with the kind of budget that some of these people have are struggling to implement crawling right 20 years later tough. It's nice those folks got a rebate. One of the problems why people are testy today is that you pay by the GB w/ cloud providers; about 10 years ago I kicked out the sinosphere crawlers like Baidu because they were generating like 40% of the traffic on my site crawling over and over again and not sending even a single referrer. reply jgalt212 16 hours agorootparentI've found Googlebot has gotten a bit wonky lately. 10X the usual crawl rate and - they don't respect the Crawl-Delay directive - google search console reports 429s as 500s https://developers.google.com/search/docs/crawling-indexing/... reply TuringNYC 21 hours agoparentprevSerious question - if robots.txt are not being honored, is there a risk that there is a class action from tens of thousands of small sites against both the companies doing the crawling and individual directors/officers of these companies? Seems there would be some recourse if this is done at at large enough scale. reply krapp 21 hours agorootparentNo. robots.txt is not in any way a legally binding contract, no one is obligated to care about it. reply vasco 20 hours agorootparentIf I have a \"no publicity\" sign in my mailbox and you dump 500 lbs of flyers and magazines by my door every week for a month and cause me to lose money dealing with all the trash, I think I'd have a reasonable ground to sue even if there's no contract saying you need to respect my wish. End of the day the claim is someone's action caused someone else undue financial burden in an way that is not easily prevented beforehand, so I wouldn't say it's a 100% clear case but I'm also not sure a judge wouldn't entertain it. reply krapp 20 hours agorootparentI don't think you can sue over what amounts to an implied gentleman's agreement that one side never even agreed to and win but if you do, let us know. reply boredatoms 20 hours agorootparentYou can sue whenever anyone harms you reply dredmorbius 10 hours agorootparentYou can sue whenever. The suit itself is the mechanism for determining whether the harm existed. And yes, of course, this presents much opportunity for abuse. reply krapp 20 hours agorootparentprevI didn't say no one could sue, anyone can sue anyone for anything if they have the time and the money. I said I didn't think someone could sue over non-compliance with robots.txt and win. If it were possible, someone would have done it by now. It hasn't happened because robots.txt has absolutely no legal weight whatsoever. It's entirely voluntary, which means it's perfectly legal not to volunteer. But if you or anyone else wants to waste their time tilting at legal windmills, have fun ¯\\_(ツ)_/¯. reply vasco 19 hours agorootparentYou don't even need to mention robots.txt, there's plenty of people that have been sued for crawling and had to stop it and pay damages, just lookup \"crawling lawsuits\". reply macintux 19 hours agorootparentprevYour verbs, “sue” and “win”, are separated by ~16 words of flowery language. It’s not surprising that people gave up partway through and reacted to just the first verb. reply echoangle 19 hours agorootparentThe „well everyone can sue anyone for anything“ is a non-serious gotcha answer anyways. If someone asks „can I sue XY because of YZ“, the always mean „and have a chance of winning“. Just suing someone without any chance of winning isn’t very interesting. reply krapp 17 hours agorootparentprev\"flowery language?\" It's just a regular sentence with regular words in it. reply ericmcer 20 hours agorootparentprevYou can sue over literally anything, the parent comment could sue you if they could demonstrate your reply damaged them in some way. reply jdenning 20 hours agorootparentprevWe need a way to apply a click-through \"user agreement\" to crawlers reply huntoa 20 hours agoparentprevDid I read it right that you pay 62,5$/TB? reply Uptrenda 20 hours agoparentprevHey man, I wanted to say good job on read the docs. I use it for my Python project and find it an absolute pleasure to use. Write my stuff in restructured text. Make lots of pretty diagrams (lol), slowly making my docs easier to use. Good stuff. Edit 1: I'm surprised by the bandwidth costs. I use hetzner and OVH and the bandwidth is free. Though you manage the bare metal server yourself. Would readthedocs ever consider switching to self-managed hosting to save costs on cloud hosting? reply exe34 21 hours agoparentprevcan you feed them gibberish? reply blibble 21 hours agorootparenthere's a nice project to automate this: https://marcusb.org/hacks/quixotic.html couple of lines in your nginx/apache config and off you go my content rich sites provide this \"high quality\" data to the parasites reply Groxx 21 hours agorootparentprevLLMs poisoned by https://git-man-page-generator.lokaltog.net/ -like content would be a hilarious end result, please do! reply jcpham2 21 hours agorootparentprevThis would be my elegant solution, something like an endless recursion with a gzip bomb at the end if I can identify your crawler and it’s that abusive. Would it be possible to feed an abusing crawler nothing but my own locally-hosted LLM gibberish? But then again if you’re in the cloud egress bandwidth is going to cost for playing this game. Better to just deny the OpenAI crawler and send them an invoice for the money and time they’ve wasted. Interesting form of data warfare against competitors and non competitors alike. The winner will have the longest runway reply actsasbuffoon 21 hours agorootparentIt wouldn’t even necessarily need to be a real GZip bomb. Just something containing a few hundred kb of seemingly new and unique text that’s highly compressible and keeps providing “links” to additional dynamically generated gibberish that can be crawled. The idea is to serve a vast amount of poisoned training data as cheaply as possible. Heck, maybe you could even make a plugin for NGINX to recognize abusive AI bots and do this. If enough people install it then you could provide some very strong disincentives. reply GaggiX 20 hours agorootparentprevThe dataset is curated, very likely with a previously trained model, so gibberish is not going to do anything. reply exe34 20 hours agorootparenthow would a previously trained model know that Elon doesn't smoke old socks? reply GaggiX 20 hours agorootparentAn easy way is to give the model the URL of the page so it can value the content based on the reputation of the source, of course the model doesn't know future events, but gibberish is gibberish, and that's quite easy to filter, even without knowing the source. reply exe34 10 hours agorootparent> gibberish is gibberish most insightful, thank you! also, stay away from linkedin, you sweet summer child. reply GaggiX 5 hours agorootparentI don't understand why you are so aggressive ahah, gibberish is easy to recognize I'm sorry, you don't need to be mad about it ahah reply jcgrillo 20 hours agoparentprevnext [7 more] [flagged] jsheard 19 hours agorootparentJudging by how often these scrapers keep pulling the same pages over and over again I think they're just hoping that more data will magically come into existence if they check enough times. Like those vuln scanners which ping your server for Wordpress exploits constantly just in case your not-Wordpress site turned into a Wordpress site since they last looked 5 minutes ago. reply KTibow 19 hours agorootparentprevI personally predict this won't be as bad as it sounds since training on synthetic data usually goes well (see Phi) reply spacecadet 3 hours agorootparentWhile Phi is a good example of this technique, Phi as a model is very anemic. It was recently part of a CTF hosted by Microsoft, where other models were also included- I assume MS was looking to test performance of Phi against the competition... Phi performed the worst. Its outputs easier to predict, quicker to construct injection attacks and jailbreak. All models utilized the same defenses. As I have also trained and fine-tuned models using synthetic data, I have seen this approach increase determinism and increase predictability. Some might see this as a good thing- but I think it depends. On one hand this opens the model to several adversarial attacks such as jailbreaking, extraction, etc, on the other hand some consumers may prefer less random outputs. reply spacecadet 19 hours agorootparentprevnext [–]“OpenAI used 600 IPs to scrape data, and we are still analyzing logs from last week, perhaps it’s way more,” he said of the IP addresses the bot used to attempt to consume his site. The IP addresses in the screenshot are all owned by Cloudflare, meaning that their server logs are only recording the IPs of Cloudflare's reverse proxy, not the real client IPs. Also, the logs don't show any timestamps and there doesn't seem to be any mention of the request rate in the whole article. I'm not trying to defend OpenAI but as someone who scrapes data I think it's unfair to throw around terms \"like DDOS attack\" without providing basic request rate metrics. This seems to be purely based on the use of multiple IPs, which was actually caused by their own server configuration and has nothing to do with OpenAI. reply mvdtnz 20 hours agoparentWhy should web store operators have to be so sophisticated to use the exact right technical language in order to have a legitimate grievance? How about this: these folks put up a website in order to serve customers, not for OpenAI to scoop up all their data for their own benefit. In my opinion data should only be made available to \"AI\" companies on an opt-in basis, but given today's reality OpenAI should at least be polite about how they harvest data. reply griomnib 21 hours agoprevI’ve been a web developer for decades as well as doing scraping, indexing, and analyzing million of sites. Just follow the golden rule: don’t ever load any site more aggressively than you would want yours to be. This isn’t hard stuff, and these AI companies have grossly inefficient and obnoxious scrapers. As a site owner those pisses me off as a matter of decency on the web, but as an engineer doing distributed data collection I’m offended by how shitty and inefficient their crawlers are. reply PaulHoule 21 hours agoparentI worked at one place where it probably cost us 100x (in CPU) more to serve content the way we were doing it as opposed to the way most people would do it. We could afford it because it was still cheap, but we deferred the cost reduction work for half a decade and went on a war against webcrawlers instead. (hint: who introduced the robots.txt standard?) reply mingabunga 20 hours agoparentprevWe've had to block a lot of these bots as they slowed our technical forum to a crawl, but new ones appear every now and again. Amazons was the worst reply griomnib 18 hours agorootparentI really wonder if these dogshit scrapers are wholly built by LLM. Nobody competent codes like this. reply add-sub-mul-div 17 hours agoparentprevThese people think they're on the verge of the most important invention in modern history. Etiquette means nothing to them. They would probably consider an impediment to their work a harm to the human race. reply griomnib 13 hours agorootparentYeah but it’s just shit engineering. They re-crawl entire sites basically continuously absent any updates or changes. How hard is it to cache a fucking sitemap for a week? It’s a waste of bandwidth and CPU on their end as well, “the bitter lesson” isn’t “keep duplicating the same training data”. I’m glad DeepSeek is showing how inefficient and dogshit most of the frontier model engineer is - how much VC is getting burned literally redownloading a copy of the entire web daily when like They would probably consider an impediment to their work a harm to the human race. They do. Marc Andreeson said as much in his \"techno-optimist manifesto,\" that any hesitation or slowdown in AI development or adoption is equivalent to mass murder. reply add-sub-mul-div 16 hours agorootparentI want to believe he's bullshitting to hype it up for profit because at least that's not as bad as if it was sincere. reply mikro2nd 5 hours agorootparentIt's not just for profit, it's to save some future, mythical version of humankind! https://netzpolitik.org/2023/longtermism-an-odd-and-peculiar... reply krapp 16 hours agorootparentprevI also want to believe AI hype is being fueled just by grifters and not an accelerationist messiah cult (and also grifters.) But they do seem really committed to the bit. reply jonas21 20 hours agoprevIt's \"robots.txt\", not \"robot.txt\". I'm not just nitpicking -- it's a clear signal the journalist has no idea what they're talking about. That and the fact that they're using a log file with the timestamps omitted as evidence of \"how ruthelessly an OpenAI bot was accessing the site\" makes the claims in the article a bit suspect. OpenAI isn't necessarily in the clear here, but this is a low-quality article that doesn't provide much signal either way. reply ted_bunny 14 hours agoparentThe best way to tell a journalist doesn't know their subject matter: check if they're a journalist. reply peterldowns 19 hours agoparentprevWell said, I agree with you. reply ted_bunny 14 hours agorootparentI wish they'd add a little arrow you could click. reply Thoreandan 19 hours agoparentprevHear hear. Poor article going out the door for publication with zero editorial checking. reply joelkoen 19 hours agorootparentHaha yeah just noticed they call Bytespider \"TokTok's crawler\" too reply spwa4 19 hours agoprevIt's funny how history repeats. The web originally grew because it was a way to get \"an API\" into a company. You could get information, without a phone call. Then, with forms and credit cards and eventually with actual API's, you could get information, you could get companies to do stuff via an API. For a short while this was possible. Now everybody calls this abuse. And a lot of it is abuse, to be fair. Now that has been mostly blocked. Every website tries really hard to block bots (and mostly fail because Google funds their crawler millions of dollars while companies raise a stink over paying a single SWE), but it's still at the point that automated interactions with companies (through third-party services for example) are not really possible. I cannot give my credit card info to a company and have it order my favorite foods to my home every day, for example. What AI promises, in a way, is to re-enable this. Because AI bots are unblockable (they're more human than humans as far as these tests are concerned). For companies, and for users. And that would be a way to ... put API's into people and companies again. Back to step 1. reply afavour 19 hours agoparentI see it as different history repeating: VC capital inserting itself as the middleman between people and things they want. If all of our interactions with external web sites now go through ChatGPT that gives OpenAI a phenomenal amount of power. Just like Google did with search. reply spwa4 8 hours agorootparentWell, it's not just that. Every company insists on doing things differently and usually in annoying ways. Having a way to deal with companies while avoiding their internal policies (e.g. upselling, \"retention team\", ...) would be very nice. Yes, VCs want this because it's an opportunity for a double-sided marketplace, but I still want it too. I wonder to what extent is what these FANG businesses want with AI can be described as just \"an API into businesses that don't want to provide an API\". reply Hilift 19 hours agoprevPeople who have published books recently on Amazon have noticed that immediately there are fraud knockoff copies with the title slightly changed. These are created by AI, and are competing with humans. A person this happened to was recently interviewed about their experience on BBC. reply PaulHoule 21 hours agoprevFirst time I heard this story it was '98 or so and the perp was somebody in the overfunded CS department and the victim somebody in the underfunded math department on the other side of a short and fat pipe. (Probably running Apache httpd on a SGI workstation without enough ram to even run Win '95) In years of running webcrawlers I've had very little trouble, I've had more trouble in the last year than in the past 25. (Wrote my first crawler in '99, funny my crawlers have gotten simpler over time not more complex) In one case I found a site got terribly slow although I was hitting it at much less than 1 request per second. Careful observation showed the wheels were coming off the site and it had nothing to do with me. There's another site that I've probably crawled in it's entirety at least ten times over the past twenty years. I have a crawl from two years ago, my plan was to feed it into a BERT-based system not for training but to discover content that is like the content that I like. I thought I'd get a fresh copy w/ httrack (polite, respects robots.txt, ...) and they blocked both my home IP addresses in 10 minutes. (Granted I don't think the past 2 years of this site was as good as the past, so I will just load what I have into my semantic search & tagging system and use that instead) I was angry about how unfair the Google Economy was in 2013, in lines with what this blogger has been saying ever since http://www.seobook.com/blog (I can say it's a strange way to market an expensive SEO community but...) and it drives me up the wall that people looking in the rear view mirror are getting upset about it now. Back in '98 I was excited about \"personal webcrawlers\" that could be your own web agent. On one hand LLMs could give so much utility in terms of classification, extraction, clustering and otherwise drinking from that firehose but the fear that somebody is stealing their precious creativity is going to close the door forever... And entrench a completely unfair Google Economy. It makes me sad. ---- Oddly those stupid ReCAPTCHAs and Cloudflare CAPTCHAs torment me all the time as a human but I haven't once had them get in the way of a crawling project. reply vzaliva 20 hours agoprevFrom the article: \"As Tomchuk experienced, if a site isn’t properly using robot.txt, OpenAI and others take that to mean they can scrape to their hearts’ content.\" The takeaway: check your robots.txt. The question of how much load requests robots can reasonably generate when allowed is a separate matter. reply krapp 20 hours agoparentAlso probably consider blocking them with .htaccess or your server's equivalent, such as here: https://ethanmarcotte.com/wrote/blockin-bots/ All this effort is futile because AI bots will simply send false user agents, but it's something. reply Sesse__ 20 hours agorootparentI took my most bothered page IPv6-only, the AI bots vanished in the course of a couple days :-) (Hardly any complaints from actual users yet. Not zero, though.) reply methou 19 hours agoprevI used to have some problem with some Chinese crawlers, first I told them no with robots.txt, then I see a swarm of of non-bot user-agents from cloud providers in China, so I blocked their ASN, and then I see another rise of IPs from some Chinese ISP, so I eventually I have to block the entire country_code = cn and show them a robots.txt reply tonetegeatinst 18 hours agoprevWhat options exist if you want to handle this traffic and you own your hardware on prem? It seems that any router or switch over 100G is extremely expensive, and often requires some paid for OS. The pro move would be to not block these bots. Well I guess block them if you truly can't handle their throughput request (would an ASN blacklist work?) Or if you want to force them to slow down, start sending data, but only a random % of responses are sent (so say ignore 85% of the traffic they spam you with, and reply to the others at a super low rate or you could purposely send bad data) Or perhaps reachout to your peering partners and talk about traffic shaping these requests. reply andrethegiant 20 hours agoprevI'm working on fixing this exact problem[1]. Crawlers are gonna keep crawling no matter what, so a solution to meet them where they are is to create a centralized platform that builds in an edge TTL cache, respects robots.txt and retry-after headers out of the box, etc. If there is a convenient and affordable solution that plays nicely with websites, the hope is that devs will gravitate towards the well-behaved solution. [1] https://crawlspace.dev reply readyplayernull 19 hours agoprev> has a terms of service page on its site that forbids bots from taking its images without permission. But that alone did nothing. It's time to level up in this arms race. Let's stop delivering html documents, use animated rendering of information that is positioned in a scene so that the user has to move elements around for it to be recognizable, like a full site captcha. It doesn't need to be overly complex for the user that can intuitively navigate even a 3D world, but will take x1000 more processing for OpenAI. Feel free to come up with your creative designs to make automation more difficult. reply liamwire 19 hours agoparentSounds entirely at odds with any accessibility requirements. reply readyplayernull 19 hours agorootparentAre 3D videogames at odds with accessibility? reply atleastoptimal 21 hours agoprevStuff like this will happen to all websites soon due to AI agents let loose on the web reply JohnMakin 21 hours agoparenthttps://cyberpunk.fandom.com/wiki/Blackwall reply philipwhiuk 18 hours agoprevI had the same problem with my club's website. reply OutOfHere 20 hours agoprevSites should learn to use HTTP error 429 to slow down bots to a reasonable pace. If the bots are coming from a subnet, apply it to the subnet, not to the individual IP. No other action is needed. reply Sesse__ 20 hours agoparentI've seen _plenty_ of user agents that respond to 429 by immediately trying again. Like, literally immediately; full hammer. I had to eventually automatically blackhole IP addresses that got 429 too often. reply jcgrillo 20 hours agorootparentIt seems like it should be pretty cheap to detect violations of Retry-After on a 429 and just automatically blackhole that IP for idk 1hr. It could also be an interesting dataset for exposing the IPs those shady \"anonymous scraping\" comp intel companies use.. reply OutOfHere 19 hours agorootparentprevThat is just what a bot does by default. It will almost always give up after a few retries. The point of 429 is that you will not be using up your limited bandwidth sending the actual response, which will save you at least 99% of your bandwidth quota. It is not to find IPs to block, especially if the requestor gives up after a few requests. The IPs that you actually need to block are the ones that are actually DoSing you without stopping even after a few retries, and even then only temporarily. reply Sesse__ 21 minutes agorootparent> That is just what a bot does by default. It will almost always give up after a few retries. I've had them go on for hours. (This is not mainly bots, but various crap desktop applications.) > The IPs that you actually need to block are the ones that are actually DoSing you without stopping even after a few retries, and even then only temporarily. Yup. They only get blocked until next reboot :-) reply ThrowawayTestr 21 hours agoprevHas anyone been successfully sued for excess hosting costs due to scraping? reply neom 21 hours agoparenthttps://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn makes it clear scraping publicly available data is generally not a CFAA violation. Certainly it would have to be a civil matter, but I doubt it would work (ianal) reply ericholscher 21 hours agorootparentWe did get $7k out of one of the AI companies based on the massive bandwidth costs they caused us. https://about.readthedocs.com/blog/2024/07/ai-crawlers-abuse... reply neom 21 hours agorootparentwow GOOD JOB!!! Were they relatively decent about it, is that why? I feel like normal businesses that are not super shady should be able to accept this kind of conversation and deal with the mistake and issue they causes for you. Good job perusing it tho, that's fantastic. (ps, big fan of your product, great work on that too!) reply 1oooqooq 20 hours agoprevis there a place with a list of aws servers these companies can block? reply peterldowns 20 hours agoparentOpenAI publishes a list of IP prefixes that its scraper uses, as well as the user-agents: https://platform.openai.com/docs/bots It does not document how the bot will respond to things like 429 response codes. It does not document how frequently their bots may crawl a given domain/url. It does not document any way to get in touch about abusive crawler practices. reply layer8 18 hours agorootparentFrom https://pod.geraspora.de/posts/17342163 (discussed in https://news.ycombinator.com/item?id=42549624): “If you try to block them by User Agent string, they’ll just switch to a non-bot UA string (no, really).” reply more_corn 20 hours agoprevCloudflare bot detection? https://developers.cloudflare.com/bots/plans/free/ reply 1oooqooq 20 hours agoparentthey are problably hosting the bots reply ldehaan 21 hours agoprevFail2ban would have been all they needed. It's what I have, works great. reply peebee67 21 hours agoprevGreedy and relentless OpenAI's scraping may be, but that his web-based startup didn't have a rudimentary robots.txt in place seems inexcusably naive. Correctly configuring this file has been one of the most basic steps of web design for living memory and doesn't speak highly of the technical acumen of this company. >“We’re in a business where the rights are kind of a serious issue, because we scan actual people,” he said. With laws like Europe’s GDPR, “they cannot just take a photo of anyone on the web and use it.” Yes, and protecting that data was your responsibility, Tomchuck. You dropped the ball and are now trying to blame the other players. reply mystified5016 20 hours agoparentOpenAI will happily ignore robots.txt Or is that still my fault somehow? Maybe we should stop blaming people for \"letting\" themselves get destroyed and maybe put some blame on the people actively choosing to behave in a way that harms everyone else? But then again, they have so much money so we should all just bend over and take it, right? reply peebee67 19 hours agorootparentIf they ignore a properly configured robots.txt and the licence also explicitly denies them use, then I'd guess they have a viable civil action to extract compensation. But that isn't the case here at all, and while there's reports of them doing so, they certainly claim to respect the convention. As for bending over, if you serve files and they request files, then you send them files, what exactly is the problem? That you didn't implement any kind of rate limiting? It's a web-based company and these things are just the basics. reply peterldowns 21 hours agoprev [–] I have little sympathy for the company in this article. If you put your content on the web, and don't require authentication to access it, it's going to be crawled and scraped. Most of the time you're happy about this — you want search providers to index your content. It's one thing if a company ignores robots.txt and causes serious interference with the service, like Perplexity was, but the details here don't really add up: this company didn't have a robots.txt in place, and although the article mentions tens/hundreds of thousands of requests, they don't say anything about them being made unreasonably quickly. The default-public accessibility of information on the internet is a net-good for the technology ecosystem. Want to host things online? Learn how. EDIT: They're a very media-heavy website. Here's one of the product pages from their catalog: https://triplegangers.com/browse/scans/full-body/sara-liang-.... Each of the body-pose images is displayed at about 35x70px but is served as a 500x1000px image. It now seems like they have some cloudflare caching in place at least. I stand by my belief that unless we get some evidence that they were being scraped particularly aggressively, this is on them, and this is being blown out of proportion for publicity. reply swatcoder 21 hours agoparent> I have little sympathy for the company in this article. If you put your content on the web, and don't require authentication to access it, it's going to be crawled and scraped. Most of the time you're happy about this — you want search providers to index your content. If I stock a Little Free Library at the end of my driveway, it's because I want people in the community to peruse and swap the books in a way that's intuitive to pretty much everyone who might encounter it. I shouldn't need to post a sign outside of it saying \"Please don't just take all of these at once\", and it'd be completely reasonable for me to feel frustrated if someone did misuse it -- regardless of whether the sign was posted or not. reply dghlsakjg 21 hours agoparentprevThere is nothing inherently illegal about filling a small store to occupancy capacity with all of your friends and never buying anything. Just because something is technically possible and not illegal does NOT make it the right thing to do. reply riffraff 20 hours agorootparentas the saying goes \"it's not illegal\" is a very low bar for morality. reply agmater 21 hours agoparentprevFrom the Wayback Machine [0] it seems they had a normal \"open\" set-up. They wanted to be indexed, but it's probably a fair concern that OpenAI isn't going to respect their image license. The article describes the robot.txt [sic] now \"properly configured\", but their solution was to block everything except Google, Bing, Yahoo, DuckDuckGo. That seems to be the smart thing these days, but it's a shame for any new search engines. [0] https://web.archive.org/web/20221206134212/https://www.tripl... reply peterldowns 20 hours agorootparentThe argument about image/content licensing is, I think, distinct from the one about how scrapers should behave. I completely agree that big companies running scrapers should be good citizens — but people hosting content on the web need to do their part, too. Again, without any details on the timing, we have no idea if OpenAI made 100k requests in ten seconds or if they did it over the course of a day. Publicly publishing information for others to access and then complaining that ~1 rps takes your site down is not sympathetic. I don't know what the actual numbers and rates are because they weren't reported, but the fact that they weren't reported leads me to assume they're just trying to get some publicity. reply dghlsakjg 18 hours agorootparent> Publicly publishing information for others to access and then complaining that ~1 rps takes your site down is not sympathetic. I don't know what the actual numbers and rates are because they weren't reported, but the fact that they weren't reported leads me to assume they're just trying to get some publicity. They publicly published the site for their customers to browse, with the side benefit that curious people could also use the site in moderation since it wasn't affecting them in any real way. OpenAI isn't their customer, and their use is affecting them in terms of hosting costs and lost revenue from downtime. The obvious next step is to gate that data behind a login, and now we (the entire world) all have slightly less information at our fingertips because OpenAI did what they do. The point is that OpenAI, or anyone doing massive scraping ops should know better by now. Sure, the small company that doesn't do web design had a single file misconfigured, but that shouldn't be a 4 or 5 figure mistake. OpenAI knows what bandwidth costs. There should be a mechanism that says, hey, we have asked for many gigabytes or terrabytes of data from a single domain scrape, that is a problem. reply icehawk 19 hours agoparentprev> although the article mentions tens/hundreds of thousands of requests, they don't say anything about them being made unreasonably quickly. It's the first sentence of the article. On Saturday, Triplegangers CEO Oleksandr Tomchuk was alerted that his company’s e-commerce site was down. If a scraper is making enough requests to take someone else's website down, the scraper's requests are being made unreasonably quickly. reply JohnMakin 21 hours agoparentprevrobots.txt as of right now is a complete honor system, so I think it's reasonable to make a conclusion that you shouldn't rely on it protecting you because odds are overwhelming that scraping behavior will become worse in the near to mid term future reply nitwit005 20 hours agoparentprevLet us flip this around: If your crawler regularly knocks websites offline, you've clearly done something wrong. There's no chance every single website in existence is going to have a flawless setup. That's guaranteed simply from the number of websites, and how old some of them are. reply fzeroracer 21 hours agoparentprev> If you put your content on the web, and don't require authentication to access it, it's going to be crawled and scraped. Most of the time you're happy about this — you want search providers to index your content > The default-public accessibility of information on the internet is a net-good for the technology ecosystem. Want to host things online? Learn how. These two statements are at odds, I hope you realize. You say public accessibility of information is a good thing, while blaming someone for being effectively DDOS'd as a result of having said information public. reply hd4 21 hours agorootparentThey're not at odds. \"default-public accessibility of information\" doesn't necessarily translate into \"default-public accessibility of content\" ie. media. Content should be served behind an authentication layer. The clickbaity hysteria here is missing out how this sort of scraping has been possible long before AI agents showed up a couple of years back. reply macintux 19 hours agorootparentOf course it was possible, but the incentives have changed. Now anyone can use the accumulated knowledge of the world to build something new, so more independent actors are doing so, often very badly. reply j45 21 hours agoparentprev [–] It's less about sympathy and more about understanding that they might not be experts in things tech, relied on hired help that seemed to be good at what they did, and the most basic thing (setup a free cloudflare account or something) was missed. Learning how, is sometimes actually learning who's going to get you online in a good way. In this case when you have non-tech people building Wordpress sites, it's about what they can understand and do, and teh rate of learning doesn't always keep up relative to client work. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Triplegangers' e-commerce site experienced a DDoS-like attack due to OpenAI's bot making tens of thousands of requests to scrape over 65,000 product pages. - The site lacked a properly configured robots.txt file to block such bots, leading to increased AWS costs and concerns about data security. - This incident underscores the importance of AI companies obtaining permission before data scraping and serves as a warning for small businesses to monitor for AI bot activity."
    ],
    "commentSummary": [
      "OpenAI's bot caused excessive traffic on a small company's website, similar to a Distributed Denial of Service (DDoS) attack, highlighting issues with AI bots overwhelming websites.",
      "Companies like Read the Docs have experienced similar challenges with AI bots generating massive traffic, despite using \"robots.txt\" to manage web crawlers.",
      "The incident underscores the ongoing debate on balancing web accessibility with protection against aggressive data scraping by AI companies, as some bots ignore \"robots.txt,\" leading to potential legal and financial issues for small site owners."
    ],
    "points": 111,
    "commentCount": 102,
    "retryCount": 0,
    "time": 1736544077
  }
]
