[
  {
    "id": 42344002,
    "title": "My second year without a job",
    "originLink": "https://shilin.ca/my-second-year-without-job/",
    "originBody": "my second year without a job 06 Dec, 2024 Two years ago, I quit my job. I was divorced, and I had $80K on my bank account. Today, I am still without a job, still divorced, and, not surprisingly, out of money. I wish I had written about my first year of unemployment, so I can compare the regress progress. At the very least, I can do a short overview of this year, so I’ll have something to compare in the next one. Tangible Money At first, I was sad that I had spent all the money so quickly. If you think about it, $80K in two years is $40K a year, or $3,300 a month. It's a considerable amount, and I used to think that I overspent. That my groceries could have been cheaper. That I could have eaten out or traveled less. Then a few months ago, I went on NomadList and found out that the average living cost in Montreal is $3,750/mo. Considering that I spent the first year in Ottawa/Toronto, where the cost of living is even higher, my spending habits turned out not that bad after all. I’m not just average — I’m slightly better! So I said goodbye to my money and made peace with the fact that I would be broke for a while. At the end of 2022, when I quit my job to work on my projects, I made resolutions: to make $1M in revenue in 2025. Well, that's not really happening… But rest assured, I do everything possible to reach that goal rather sooner than later. (the picture of the money left in my bank account) Work Since January, I worked on four projects with four other founders. First, we launched Guitartonic with Armaan, and it was fulfilling to build something for fun. Then we tried to get a fintech product going with Nima, but failed to agree on a common vision. After that, I took a month off to try to resurrect Wonderbook. I reworked the entire app, redesigning screens and introducing a couple of important features. But as much as I would rather not admit, Wonderbook was a dying project, and none of my changes helped. So I watched the traffic plummet, made a loud sigh, and went on with life. In April, Zane reached out with an idea of a product he wanted to build. We spent two weeks putting an MVP together, and shortly after, we landed the first two customers. Blymp was born. Zane had to step down from the operations in July, but I kept going. Of the four projects I worked on, blymp is the only one generating money — about $600/month — and the one I plan to continue next year. Yay! Housing This year I stayed in the same place, an old victorian-style house in downtown Montreal, with 3 other roommates. But the house is not by any means a usual co-living situation. The person who launched it is also an entrepreneur, and he started this project for other entrepreneurs to collaborate and inspire each other. When I signed the lease last year, I already had an offer for another place, a one-bedroom apartment in my favorite neighborhood of the city. Plus, the price was similar, so I wouldn't have saved a lot of money by living with other people. But collaboration is the seed for innovation, so I decided to give it a try. These days, we do a lot of things together, and it has a positive impact on my mental health. Once a week, we play board games, cook food, or do coworking sessions. Living with others is not always easy, but it is fun. I got very lucky with these folks, and I hope we keep the good vibes going forward in the new year. Numbers-wise, my rent is $1250 for a room, which is between one-third and one-half of my monthly budget. I could find something cheaper, but I chose sanity over an extra month of runway. Intangible Music In June, I started playing piano. I have always loved music, and the fact that I still play guitar after 18 years only confirms it. While the guitar entered my life through my desire to play songs for friends, the piano was an accidental discovery. My roommate, who just moved in with us this spring, brought his piano with him. It stood untouched in our living room for a month. One evening, while watching Amélie with Yann Tiersen's beautiful music, I felt an inexplicable urge to learn that song. So I did. It took me about a month to sync the movements of both hands — remember, I had never played piano before — but it came together and sounded decent. After that, I went on to learn Sleeping Lotus by Joep Beving and Ascent (Day 1) by Ludovico Einaudi. Approaching the end of this year, I am 90% done with my fifth piece, Lumières by Alexandra Stréliski, and it’s both the hardest and most beautiful song I’ve learned so far. It took me two months just to finish reading it, and it will likely take another two to get comfortable playing it. Guitartonic, the app that I built back in January, also reignited my passion for guitar. It helped me learn a few positions of a minor pentatonic scale, so now I can improvise along when playing with other people. It's not great by any means, but it's progress nonetheless. Sports Oh boy, did I do sports this year! At the end of 2023, I joined a climbing team to train for provincial and national bouldering competitions in Canada. The training stopped suddenly when our coach got injured and couldn't come to the gym anymore. So the beginning of 2024 was rough, as my team continued to train without him. It simply wasn't the same thing anymore. But we decided to keep up with the schedule our coach had set for us, and it kept us motivated for a few months. By summer, bouldering moved to the back of my priorities, as I started going outdoor more often. I introduced lead climbing and went on climbing trips. Nima and I bought gear of our own -- a rope, quickdraws, and peripherals. While I avoid spending a lot of money on new hobbies, this felt justified. Now, looking back, I don't regret that we did it. Along with climbing, we started to train for a triathlon around June. The triathlon was scheduled for September, so we had about three months to prepare. Swimming terrified me the most, but turned out the most pleasurable exercise of all three. Biking was never an issue for me — I always liked it. But running was a different story. I have a love/hate relationship with running, dating back to 2020. I was bored during Covid, started going for short runs in the evenings, and because of my poor technique, I killed my knees in only 3 months. It took me two years to fully recover. These days, running feels much better, and I have my climbing sessions to thank for it — my butt got much stronger, so my knees don't have to work as hard. Gut Unlike with sports, the relationship with my stomach is rather tough. For the last two years, it's been giving me a hard time. I have only two possible explanations. Stress from not having a stable income, or a side effect from a year-long course of Accutane — the timing aligns well with this. The end of this year was the time I decided to make changes. I went on a FODMAP diet and finally saw improvements. For the first time in months, I had no symptoms. No gas and no bloating. I still remember the first evening of my diet, as I felt like a happy man, all because my stomach was also happy. My diet is not over, and I am slowly reintroducing food groups to my meals to find out what causes intolerances. Being on a diet without lactose, gluten, and many other foods is hard. But having a healthy gut is certainly a step up from not having one. For better or worse, it forced me to cook more. And knowing me, that's not a small win. Overall At first, I thought I would only write about my projects, as any struggling entrepreneur should. But this year was about much more than just projects. Without music, sports, and friends, I wouldn't be able to keep up with work, especially unpaid. No matter the angle under which I look at it, it was a great year — fulfilling, with a lot of love and care. It's exciting to look ahead. I am always intrigued by what comes next. For the past few years, my rule has been simple: keep the good things, cut the bad things, and acquire more of the good things. But this year… I don't really have bad things. Minor inconveniences, maybe. Hiccups. But nothing that I am looking to part with. Here's to a promising year 2025. My third year without a job. A year when I give more than I receive. A year of patience. And a year of an even deeper connection with myself. Cheers! This post blew up on Hacker News, which got me by surprise. I wrote a little “thank you” note here. #entrepreneurship #money 126",
    "commentLink": "https://news.ycombinator.com/item?id=42344002",
    "commentBody": "My second year without a job (shilin.ca)381 points by true_pk 22 hours agohidepastfavorite567 comments hn_throwaway_99 14 hours agoAm I missing something about where this guy is getting money? He posted his bank account, which is essentially empty, and it seems his only current income is $600/mo, which is less than half of his rent. So how is this guy surviving currently? I don't mean to be one of those people that shout \"privilege\" at every turn on the Internet, but most people with no savings and barely any income would be freaking out unless they had some family or support network to lean on, which I noticed any discussion of is suspiciously absent. reply bawolff 13 hours agoparentDidn't he say he started with $80,000, spent it, and is now out of money? Presumably he now needs to either get a job asap or make some hard choices. But it sounds like the post is supposed to be a retrospective so its not surprising he isn't really talking about the future. > I don't mean to be one of those people that shout \"privilege He literally had enough money to blow $80,000 on 2 years of unemployment. Of course he is privleged. Most people in the computer industry are. Most posters to hn are. The average person lives paycheque to paycheque and certainly doesn't have 80k just lying around in their bank account. reply hn_throwaway_99 2 hours agorootparent> But it sounds like the post is supposed to be a retrospective so its not surprising he isn't really talking about the future. No, my specific point is that he does not sound like he needs to make hard choices, and he is alluding to continuing working on his own projects in the future. He writes: > I made resolutions: to make $1M in revenue in 2025. Well, that's not really happening… But rest assured, I do everything possible to reach that goal rather sooner than later. and > blymp is the only one generating money — about $600/month — and the one I plan to continue next year. Yay! and finishes with > Here's to a promising year 2025. My third year without a job. A year when I give more than I receive. A year of patience. And a year of an even deeper connection with myself. Cheers! And sure, people that make a high income job are privileged, but I was using it in the sense that you frequently see it used online, specifically that he has a backup pool of money/support somewhere, most likely family, that he conspicuously leaves out of his post. reply maronato 1 hour agorootparentprev> Here's to a promising year 2025. My third year without a job. A year when I give more than I receive. A year of patience. And a year of an even deeper connection with myself. He does talk about the future and seems to imply he’ll continue to not have a job. If that’s the case, I’m with GP in wondering how he’s going to make it past January. reply osrec 13 hours agorootparentprevI read somewhere that most people in the US are definitely not paycheck to paycheck. The article I read claimed it's around 15-20% of the working population. reply camdenreslink 6 hours agorootparentI saw the same stat, and felt like their definition was too restrictive (basically you had to spend 100% of your income on your bills). Usually a small amount of extra income gets spent and those people are also living paycheck to paycheck for all intents and purposes. reply varjag 5 hours agorootparentWell that's what paycheck to paycheck means: you barely have the ends meet for essentials. Not that you're zero after topping 401(k) and your yoga classes. reply bawolff 1 hour agorootparentIf losing your job means you won't be able to make rent this month, i'd consider that paycheque to paycheque. Like that is literally what the phrase is saying. Whether that is because you simply dont have much money, or you spent it all on something frivilous is immaterial. reply alsetmusic 1 hour agorootparentYes, absolutely. I took a job where the company (a major one) effed up HR in some way where I and another person, who got hired at the same time, didn't get paid for around six weeks. He got evicted from his apartment. I had the means to keep mine. His life was ruined. Mine kept on. I believe his situation is / was far more common than mine. reply detourdog 28 minutes agorootparentMy first job in NYC paid very little. I did something to my foot that required surgery and about 6 weeks on crutches. I ended up spending most of my paycheck at that time to getting car service from Brooklyn to Manhattan for work. During that time I was just spending whatever was needed to maintain the status quo. Only years later did I realize the privilege I was experiencing. During that time I remember colleagues remarking in disbelief that I was taking car service to work. My thinking was the only alternative was the subway and no way thought I was experiencing privilege. I was just trying to keep my job. reply hombre_fatal 5 hours agorootparentprevWell, you threw 401k in there for some reason. But living paycheck to paycheck to most people definitely does include spending one’s money on non-bill things like lotto tickets, consumer goods, restaurants, and things beyond their means. reply varjag 5 hours agorootparentThing is it's absolutely possible to scale up your spending to any income level. And this is indeed what happens with many people, this is how you get these NYT pieces on households struggling on $500k. If you go by this the whole expression is kinda meaningless. reply ghaff 4 hours agorootparentMaybe not any income level. Well, you can buy Twitter I suppose. But, yes, you can fly Netjets or whatever, buy supercars, have a bunch of personal staff, and eat out at Michelin-starred restaurants on a regular basis. And burn through certainly multiple $100Ks or even 6 figures annually pretty quickly. reply FredPret 3 hours agorootparentThere was once a billionaire in Brazil who blew it all. Instead of Netjets you can own your own jet(s). Instead of restaurants you can buy them or have the chef come to your house. At the extreme end, you can start a space program. The sky’s the limit! reply EVa5I7bHFq9mnYK 2 hours agorootparentEating from the hand of the same chef all my life? No, thanks. reply ghaff 3 hours agorootparentprevYeah. I sometimes feel like I'm being a bit extravagant but it's pretty small-scale in the scheme of things. No interest in multiple homes, yachts, jets, etc. Don't even eat out all that much except when I'm relatively modestly traveling. reply detourdog 26 minutes agorootparentprevWe definitely live in a time when one can create their own inflation. Khaki's in the USA can range from sub $50 to $900+. One can decide what is the correct price for our pants. reply bryanrasmussen 2 hours agorootparentprevI bet paycheck to paycheck like most things has a range of meanings circling around the same essential thing, which is that if you do not get paid next month you cannot afford to live the same next month as you did this one. You might be able to make ends meet by cutting the 401K and yoga classes, but if that's what you need to cut to eat the next month, what do you cut to eat the month after that? This guy was obviously not living anywhere near paycheck to paycheck. reply shubb 2 hours agorootparentMandatory spending isn't stable either. In my 20s I had low income, but at the end of the month I'd have a bit of money left over. Then the next month my car I needed to get to work would break down or I'd need to get a new shirt to look nice enough for the dress code rules or something. So over time I went slowly into debt. By this definition I was not living paycheque to paycheque, but my situation was definitely not good, compared up someone who made better money but spent it each month on nonessential. reply jghn 56 minutes agorootparentprev> you barely have the ends meet for essentials This exact debate showed up on another thread here the other day. While I agree with you, I was surprised to learn that many people view it to mean that tdon't have anything extra *after* they've done all of their socking away of money each money. Which is weird to me, but hey. reply ericd 2 hours agorootparentprevThe problem with that is there are also a large number of people who make a large amount of money and still save very little. So we should maybe put more work into figuring out what is the fault of economic structure, and what is the fault of poor decision making. Perhaps define some metric that’s a measure of what percentage of people would be unlikely to be able to do anything other than live paycheck to paycheck to paycheck in their current setup and where they live. reply truckerbill 5 hours agorootparentprevnext [–]Some financial challenges, such as a job loss, require more financial resources than would an unexpected $400 expense. One common measure of financial resiliency is whether people have savings sufficient to cover three months of expenses if they lost their primary source of income. In 2023, 54 percent of adults said they had set aside money for three months of expenses in an emergency savings or “rainy day” fund—unchanged from 2022 but down from a high of 59 percent of adults in 2021. reply ghaff 2 hours agorootparentprevYep. In the US, I would of course pay for an unexpected car repair (or indeed most any expense) with a credit card. It doesn't mean it won't be paid off at the end of the month in essentially all cases. So one of those meaningless statistics. reply dingnuts 3 hours agorootparentprevYOU would, but a lot of people with credit cards don't even understand that it's a loan, or what compound interest is. They put the expense on the card and then pay the minimum payment, either until it's paid off with insane interest, or they keep racking up debt until they reach their limit and get a worse card, until they're trapped in a cycle of poverty seriously, everyone in this thread should watch Financial Audit and see how people outside the silicon valley bubble really live. That show has extreme examples to be sure but there are so many people like this reply Aeolun 2 hours agorootparentI find it hard to sympathize with people that have enough money to pay off their card bills but don’t actually do so because they can’t be bothered to read. reply ghaff 3 hours agorootparentprevAs I recall, there was also some disagreement in discussions over what \"having cash\" meant. A lot of people have ready access to assets that aren't necessarily literally cash. reply eek2121 3 hours agorootparentprevWe have no savings right now (I am not working) and could afford a $400 emergency, but we live paycheck to paycheck. Income is good, the extra just gets spent on various things due to having multiple disabled people in the house. Eventually we hope to rebuild the savings. reply Aeolun 2 hours agorootparentprevThat’s a weird stat. If I spend all my $10k/month so that I end up with $0, I’d be included in that stat, which is clearly insane. reply bawolff 12 hours agorootparentprevhttps://www.cnbc.com/2024/04/09/most-of-americans-are-living... had 65% but the methodoly seems kind of terrible, your number might be better. Either way, its a significant portion, and even those with more generally don't have the ability to quit their job for 2 years. reply fwipsy 12 hours agorootparentprevMaybe you got it from Noahpinion: https://www.noahpinion.blog/p/paycheck-to-paycheck-and-five-... reply bronco21016 3 hours agorootparentprevI’ve always understood paycheck to paycheck to describe households that don’t have a buffer. Meaning, if the paychecks suddenly stop, or a sudden large expense, there’s nothing to bridge the gap other than social programs. By this definition, I would imagine a very large number of US households fit the mould. reply browningstreet 2 hours agorootparentprevRecent Noahpinion on this… https://www.noahpinion.blog/p/paycheck-to-paycheck-and-five-... reply readams 3 hours agorootparentprevIt\"s definitely not true https://www.noahpinion.blog/p/paycheck-to-paycheck-and-five-... reply mplewis 54 minutes agorootparentprev50% of Americans don’t have an emergency fund. That’s paycheck to paycheck. reply wigster 8 hours agorootparentprevover a third in the uk reply OtomotO 3 hours agorootparentprevThe average person in the US maybe. And the third world. In Western Europe? Not so much. I mean, yes, there are poor people, but the middle class mostly doesn't live paycheck to paycheck. reply rsynnott 3 hours agorootparentMost Western European countries do have higher savings rates than the US, but, disregarding Switzerland, Ireland and the Nordics (ie small wealthy countries with very high cost of living), it’s not a _dramatic_ difference. People may be less likely to be living paycheck to paycheck, but being able to just take two years off not working, unless you had very low living costs, would still be very unusual. reply jajko 1 hour agorootparentA lot of people adjust their lifestyle to paycheck, so little is left afterwards. Concept of savings is quite foreign to them, they want to enjoy life now. So paycheck to paycheck, while sporting newest highend phone or other gizmos, nice package of vacations through the year or just look at fashion clothes they wear and how often they buy new ones. I mean I would fit in this category too for maybe past decade, little cash left over after paycheck but I did like exotic 5 vacations per year and invested rest into mortgage for mountain apartment for rentals. reply turnsout 4 hours agorootparentprevTo me, $80k sounds like a very small amount of \"F-You\" money. I'm not criticizing the OP at all, but if I quit my job with 80k in the bank, I would immediately start researching the most painful B2B problem I could solve quickly using AI. Not to generate an income stream, but to build an asset (the business) to sell. That's the quickest path to $1M. Software developers are too caught up on salary (or 73 different \"income streams\" that all make $0), and rarely think about building a valuable business. reply johann8384 4 hours agorootparentIf you can just do that, then why are you not quitting your job and why don't you have $800k in the bank? It's easy, just build a million dollar asset and sell it. Pssh, I am on my third one this week. reply rwmj 1 hour agorootparentI posted an idea for a proven business a few days ago based on an area why my employer has given up on (too \"small\" for a multi-billion dollar company) and yet customers still seem to like and want[1]. I'm pretty sure this would be worth a few million at least, and I'd even be qualified to do it. Why don't I do it? Because it would be dull as ditchwater and I'd hate every minute of it. There's more to life than that. The point is that solving dull business problems like that might be lucrative, but not many of us are motivated enough to do them. [1] https://news.ycombinator.com/item?id=42305336 reply turnsout 2 hours agorootparentprevSomething can be both conceptually simple yet difficult to execute. (The key to losing weight is \"just\" to eat less and exercise, piano is \"just a bunch of keys,\" etc) Personally I have a mortgage and a family, so I'm not eager to burn through savings to build a company 0-1, but if I had to, I described exactly what I would do. reply ptsneves 1 hour agorootparentHehehe, you just described most normal people: a mortgage a family, cannot just wake up and say I am going all-in “this great idea”. Your idea is therefore only reachable for early age people or singles. I am trying to revive a project I believe strongly which is kind of ready to sale, but I cannot get a damn 4 hours to seat down and look at it without being interrupted by life’s chores or just being too tired Anyway I have similar fantasies to yours. It feels good to not be alone, both in circumstances as in fantasy. reply turnsout 23 minutes agorootparentI've been there! I recently released an app that has been 99% ready for sale for a full year, and the only way I got it over the finish line was to wake up every morning at 5am to get an hour or two of work in before my kid wakes up. :) I'd love to do something bigger, but this will have to scratch that entrepreneurial itch for now! reply ghaff 4 hours agorootparentprevI dunno. That seems a pretty quick path to a 74th income stream that makes $0. Especially at the moment, I'm not seeing a lot of money trees around. Not that I needed them, but when I semi-retired a number of possible revenue streams sort of evaporated--not that I looked too hard. But if you deliberately quit, that may be the thing to do. A more conservative person would probably counsel getting at least a part-time job that pays a salary though. reply turnsout 4 hours agorootparentTo be specific, I would quit the other 73 income streams and stop any extracurricular activities until I shipped something. Even though AI feels overhyped (it sort of is), we're still early in the game, and there's plenty of money to be made. Businesses still have a ton of time-consuming, expensive pain points. You just have to pick an industry and go talk to insiders to figure out what to build. reply FredPret 3 hours agorootparentI’m in business technology consulting. You’re right about tons of pain points and opportunity. But the limiting factor is not AI or any kind of tech, it’s getting these businesses to trust you with fitting into their existing systems and giving you their time. reply turnsout 2 hours agorootparent100%. Another reason to start by talking to customers. The product has to be more than useful—it has to fit into their current workflow. Most B2B startups underinvest in sales (which is not the same as sending 100,000 cold emails). reply ghaff 4 hours agorootparentprevI won't argue if I had a bunch of other activities consuming most of my bandwidth that weren't bringing in any money. If you really need cash though, I might consider lower risk, lower reward options on at least a part-time basis as well though. Doing a startup isn't exactly a high ROI activity in general--especially in a max-hype area. reply turnsout 2 hours agorootparentVery true—for a software developer, part-time consulting is probably the quickest path to making some money. It's a lower-risk, but much lower-leverage option than a 0-1 startup. Building a startup has a high risk of going to zero, but if you've chosen the right customer, the potential upside is dramatically higher than any other option. reply gdilla 3 hours agorootparentprevHe's privileged sure, not because he had 80K in the bank to burn through, but because he knows it's not the end of him when it's burnt through. He COULD get a job and steady income if he wanted, in at least that 80K range (probably double really). So meh. Good for him though for scratching his itches though. Any dude who can code can be a wage slave if they really want to. reply MarcelOlsz 14 hours agoparentprevI'm in a similar boat. -$800 in my checking and about ~$1000 left on my CC. Also from Canada. Morgage payment coming up in 5 days will put me in the negative, and another one in January as well. Should not have bought a house. I'm basically screwed, but I figure the wheels of the system or whatever will move slower than me finding work. reply m00x 10 hours agorootparentOverspent on the house? Not uncommon in Canada tbh. The country is in a really bad state real-estate wise. reply MarcelOlsz 3 hours agorootparentA little bit. It was a great idea at the time when I had work. It's been listed for sale for 2 months now which is to be expected with the rate frenzy. reply venk12 7 hours agorootparentprevsimilar boat 7 years ago. Luckily, I found a data science job that barely covered by rent and family expenses for 12hrs a day. In addition I had to freelance for a 2 to 3 hours everyday to make ends meet - basically sleep, work, repeat - rough patch - you will get through it. Hold tight, wishing you good luck! reply Tade0 2 hours agorootparentprevWhat is going on with the IT sector in Canada? I'm a contractor and last year my agency said there would be layoffs, which caused quite a scare, but in the end only contractors residing in Canada were affected. reply SvenL 11 hours agorootparentprevWould it be possible to sell the house? Kind of get rid of the mortgage? reply MarcelOlsz 3 hours agorootparentHave had it listed for months now, I expect it will sell when rates cool, in which case I'm gravy. reply stocknoob 13 hours agorootparentprevRent out/airbnb a room in your house? reply tomcam 10 hours agorootparentprevDamn, brother. But if anyone can pull a rabbit out this hat, it’s you. Your main problem will be getting your giant brain through the door for a job interview. reply MarcelOlsz 3 hours agorootparentThanks man! reply Zambyte 12 hours agorootparentprevWhat do you mean by \"~$1000 left on my CC\"? Do you mean like $1k until you max out your CC? $1k left to pay off? reply MarcelOlsz 12 hours agorootparentYeah $1k until max out, it's blocked from online payments but I can still use it irl. reply justsomehnguy 11 hours agorootparentSo it's really -$200. And CAD to boot. reply reshlo 9 hours agorootparentThey have negative $800 in their checking account. reply justsomehnguy 7 hours agorootparentYikes, totally missed that. That makes it -$1800CAD. Thanks for noticing. reply svnt 6 hours agorootparentHow are you doing your math? If they have $1k left in available credit, and they were making tech wages, that means on just that one credit card they are probably $9k to $24k at least in the negative already. The $1k of remaining available credit is not yet debt, but the negative bank balance is. It is also likely they have more than one credit card, and maybe also some student loans. reply BehindBlueEyes 3 hours agorootparent> How are you doing your math? As a european, I also didn't realize what CC stood for and was tripped up by the math same as the person you're replying to. Not everyone lives in a world/country where living on credit is the norm (at least not yet). reply Aeolun 2 hours agorootparentI don’t think living on credit is the norm even in countries that use a credit card regularly. The limit on my card is fairly high, but if I don’t pay it off every month my provider gets pissy. reply lionkor 5 hours agorootparentprevThey are simply not counting the CC since it's not available money reply carlosjobim 1 hour agorootparentprev> Should not have bought a house. I'm basically screwed Sell your house. You can't afford it. reply MarcelOlsz 25 minutes agorootparentYeah it's been listed for ages and I've dropped the price significantly. We're all collectively waiting on the coming rate announcements. reply rendang 8 hours agorootparentprevWhy did you buy a house while having such small savings? reply TeMPOraL 7 hours agorootparentThey probably spent all their savings on the down payment, as approximately everyone on the planet who wants a place to live but can't pay for it all out-of-pocket does? Investment shenanigans excluded, if you have enough savings to pay off your mortgage in case you lose your job, there's no point in getting a mortgage in the first place. reply technothrasher 6 hours agorootparent> Investment shenanigans excluded You're poisoning the well there, as that is the major reason to get a mortgage even when you have enough cash to buy a house outright. It's not \"shenanigans\", it almost always makes financial sense to keep your cash invested elsewhere when mortgage rates are low. reply NewJazz 5 hours agorootparentAlso mortgage interest is tax deductible. reply TexanFeller 1 hour agorootparentStandard exemption in the US for a married couple is around $30k. If your mortgage is so massive that the mortgage tax deduction is better than the standard deduction then in most cases you bought a far too luxurious house or in a far too “prestigious” location(SF, NYC, etc.). If you’re not already a millionaire you shouldn’t buy such property. reply NewJazz 1 hour agorootparentStandard deduction for single people is 15k. SALT can get you to 10k, and anything past 5k in mortgage interest (at 5% rates and 100k balance, you're already at the point) is gain over the std deduction. 22.5% on those dollars for many people. More depednnding on state tax. reply ghaff 1 hour agorootparentprevAs someone else commented, things can of course always change, but as things stand today, it takes a very large mortgage and/or very significant other deductibles to get over the standard deductible at this point in the US. reply goodJobWalrus 4 hours agorootparentprevNot in Canada reply ghaff 4 hours agorootparentAnd, for many people in the US without a lot of deductibles, not in the US either after tax law changes a few years ago. reply NewJazz 3 hours agorootparentYeah they're going to keep SALT capped farther than they said they would, huh? reply ghaff 2 hours agorootparentNo idea. I wouldn't count on anything with a new administration/congress. reply NewJazz 2 hours agorootparentNew? It is the same crowd as 2017. I mean they dropped the McCains, Cheneys, and Paul Ryan. But the rest are still there. reply ghaff 2 hours agorootparentAnd who knows what the crowd will do in 2025 given new cabinet etc.? Deductions don't really affect me much going forward. I care but not planning around specific policies going forward. reply NewJazz 2 hours agorootparentYeah I'm not saying it is a determinant. But the new/old head of state is the only one in the admin who matters legislatively. I'm merely musing on the pressure to balance a budget and give away all of our money. I hear he isn't so good with that. Go big or go home tho. reply Ferret7446 7 hours agorootparentprevNo emergency fund, I suppose. Sometimes I wonder how people manage to browse the Web without encountering \"common\" pieces of wisdom. Maybe we should bundle up a bunch of useful YouTube/TikToks as a supplemental education package for students. reply TeMPOraL 5 hours agorootparentNah, people know those \"common\" pieces of wisdom. What people bringing them up often miss is, they're also unachievable for most people. Life isn't a MMORPG where you can check out of progression at any moment and spend some time grinding to build up savings. Life has a clock to it that doesn't stop, and most can't afford falling behind it much. reply Turskarama 6 hours agorootparentprevYou're kind of damned if you do, damned if you don't. If you save up an emergency fund instead of paying a downpayment then you're spending money on rent that you could have in retirement, and it's not like rent is consistently cheaper than mortgage payments anyway so if you can save up when renting you can save up when owning. So the gamble is, do you spend 2 years saving up for 6 months of income as a buffer and send tens of thousands of dollars down the drain in the meantime, or do you roll the dice and hope nothing bad happens in the next 5 or so years? The people who end up in the best position will be those who take the second option, and most of the time it will work out. reply ghaff 4 hours agorootparentThe short answer is probably everything in moderation if you're reasonably young. Maybe don't put every penny you can get your hands on into a down payment on a house, especially if you're also a bit uncertain about future income streams and life situation. But maybe you also don't really need a year or two comfortable emergency fund. At some point, it probably makes sense to buy a place if you can if only for the stability as you get older. reply zeroCalories 3 hours agorootparentprevThe problem is that many people view home ownership as a cultural / political statement beyond all else. They desperately need to stop being a rentoid and become a landchad. The reality is that renting is often a better financial decision than buying, especially short term. Plus, in this economy there is no guarantee that you're going to be employed in one city your whole life. reply away271828 2 hours agorootparentMaybe. It's very situational. Beyond the spreadsheets for a given location, there are times in your life when you want to be able to pickup and move fairly easily and there are times when you want to be able to put down roots and be in a pretty stable situation that lets you tailor things. reply s1artibartfast 1 hour agorootparentprevSure. but what isn't cultural? Being cultural does not mean it is irrational. It would probably be a better financial decision if people lived cells and ate nutrient paste. reply MarcelOlsz 3 hours agorootparentprevEmergency fund is useless when expenses are 5k/mo, and that's just sitting in my room working in 15C with my food costs and expenses as low as can be. Burned through 40k already. reply MarcelOlsz 3 hours agorootparentprevBingo. I should have waited another 10y on the house. reply axus 2 hours agorootparentI'm gonna predict that in a 10-year trend, the house price inflation will have continued. Unless you could save faster than the bubble, you still wouldn't be getting/keeping a house in 10 years. reply MarcelOlsz 24 minutes agorootparentI'd have been WAY better off if I stayed at my parents and invested it all into the S&P or something similar, or even bitcoin (inb4 it's going to crash). Then kept building/working/saving, and in 10y I'll have been in a much better position. reply lottin 5 hours agorootparentprevIf that's the case, the savings are not 'spent' but are simply invested in the house. They should be able to sell the house for the net value (market value minus mortgage outstanding) which should be roughly equal to the down payment. reply NewJazz 5 hours agorootparentHigh transaction fees on sales. Typically at least 5% in US, not sure about Canada. Also don't forget taxes, insurance, mortgage interest needs to get paid while house is in your hands. Most mortgages have lots of interest during the first few years. reply MarcelOlsz 3 hours agorootparent5% for me + various breakage fees and other minor costs. reply NewJazz 3 hours agorootparentCan you imagine if buyers and sellers only had to pay, say 1% each? The whole market would become a lot more liquid. But nah, brokers are essential and have your interests at heart. reply s1artibartfast 1 hour agorootparentThey absolutely aren't required and fees are negotiable. reply MarcelOlsz 23 minutes agorootparentYeah sure but then nobody will ever see your property for sale unless they drive by it. Purplebricks also shut down in Canada. reply NewJazz 23 minutes agorootparentprevLol yeah. Biggest joke ever. reply vunderba 13 hours agoparentprevI called it out in an different thread. There's a safety net here that's simply not being mentioned. Nothing wrong with that of course, but let's not pretend that somebody with a monthly rent of ~1200 and a whopping ~60 CAD in savings would somehow magically be stress free. You'd be homeless inside a few months in the US. reply ornornor 8 hours agorootparent> stress free But he’s not. He’s specifically spelling it out by saying his gut and stomach issues are very likely tied to the stress of his no stable income. reply hn_throwaway_99 2 hours agorootparentHard disagree. In his post he's talking about continuing on in his own projects, and about 2025 being essentially more of the same, except this time with no money and no significant stable income. There is other support somewhere he is simply not talking about, because he certainly doesn't mention being worried about paying his rent going forward. reply ornornor 8 hours agoparentprev> I don't mean to be one of those people that shout \"privilege\" at every turn on the Internet, but most people with no savings and barely any income would be freaking out unless they had some family or support network to lean on, which I noticed any discussion of is suspiciously absent. I understand the sentiment but I don’t understand why point it out. It’s not like the blog post in question is putting anyone down, complaining about anything, or being obnoxious. There will always be someone more privileged than you are. Why constantly remind yourself (and others) about it if the more privileged person isn’t harming anyone? reply hn_throwaway_99 1 hour agorootparentI point it out because he's leaving out a very important part of his story. He's come to the end of 2024 with no money left and no stable income, yet he talks about continuing on more of the same in 2025, so he's got to have some other significant support from somewhere, and that's critical to his story. In my mind the better analogy is fitness influencers that go into detail about their diet and exercise regimen, and then conveniently leave out their weekly testosterone shots. I don't judge him at all for being privileged or what he's done. I judge him for leaving out what I think is the most important part of the story: how he could go on this journey and come to a point where he's got no money left but he isn't freaking out about being homeless or getting food. reply vasco 8 hours agorootparentprevYeah this just sounds like my mom saying \"finish your food because there's starving kids in africa\". reply throwup238 13 hours agoparentprevIt’s the privilege of renter’s protections. Even with nonpayment the landlord has to go through a Tribunal administratif du logement hearing to evict them and that usually takes at least a few months unless the tenant is trashing the property on their way out. The process can get extended every time the tenant pays rent so if someone is genuinely trying to pay as soon as they can, eviction takes a while. reply btbuildem 5 hours agorootparentSounds like he rents a room in a house/apartment, so it's probably one of his roomies that has the lease, not him. If that's the case, protections wont apply here. reply Thorrez 4 hours agorootparentWhy wouldn't the protections apply? They can evict him without going through the proper process? Couldn't he make some sort of claim against them if they do that? reply Ferret7446 7 hours agorootparentprevRenter's protections is one of those ideas that sound good on paper. Having to keep a non-paying rentee is disastrous for a landlord who is also trying to pay their mortgage. Sure, some landlords are large real estate companies, but all you're doing is forcing out small landlords and ensuring that large companies own all of the rental properties. (Like all regulation, it is at a fundamental level pro-monopolistic, favoring large companies that can handle the overhead of complying with the regulation and punishing small players who cannot.) reply svnt 6 hours agorootparentIf you lose the mortgage on your investment property, you lose your investment property. Risk/reward and all that. If a renter is evicted, the consequences for their life are much more severe. Not all regulation is pro-monopolistic. The accumulation of general regulation and restriction is often supported by incumbents but that doesn’t support the sweeping conclusion you’ve reached. reply lukas099 4 hours agorootparentThe consequences for renters is also severe if not enough housing is supplied because it doesn't make financial sense to do so. reply patrick451 24 minutes agorootparentprevIf I own rental property, I should not be personally responsible for subsidizing your bad financial decisions. reply carlosjobim 1 hour agorootparentprevThe landlord deserves zero empathy in these cases. It is his job to select a reliable tenant, that's usually his entire job. Anybody else who fails completely with their responsibilities at their job can expect to be fired and lose their income, or worse. There's an extremely large surplus of honest, tidy and reliable tenants, so it's not a problem for a responsible landlord to avoid going to court, if they just put in the minimum effort at doing their job. And if they can't handle that, they should sell the real estate and not be a landlord. reply Havoc 5 hours agoparentprevDefinitely feels like there is some sort of unmentioned safety net at play. Or guy is just fearless. reply EVa5I7bHFq9mnYK 2 hours agoparentprevThere is no problem surviving for a young healthy man in Montreal. One can always uber, deliveroo, tend bars etc, and it pays pretty well. Nothing to freak out about. reply hn_throwaway_99 1 hour agorootparentFair enough, but then talking about this going forward would be a critical part of his story, because he talks about continuing on in 2025 developing his projects and essentially more of the same, which becomes considerably more difficult if he's going to spend 30-40 hours a week driving for Uber. reply d_burfoot 1 hour agoprevThe problem with the \"try a lot of projects, see what works\" is that you lose out on exponential growth every time a project fails. A lot of people would be better off doing \"intra-preneurship\", meaning trying to innovate and strategize within their current company to make the job more fun, gain more autonomy, and advance their career prospects. reply asdev 47 minutes agoparentthis is literally impossible at most companies due to politics, incentive alignments, OKRs etc reply ghaff 1 hour agoparentprevAt the end of the day, it's a bet. You'll probably make zero (or negative after expenses). You may make some passable income. You might make a lot. Whereas working for a big company, you have a fairly reliable shot at bringing in a decent income with some stability. Nothing is guaranteed of course. reply itake 1 hour agoparentprevFair, but the exponential growth function can be much more aggressive for a side project than your career. In a career, you max raises at 20% per year over a 5 year period? But side projects can have revenue increase 20% MRR. Also, within companies, you need everyone’s unanimous permission to do anything.. if a problem is big enough, the team would do it themselves. If the problem is too small, the team won’t integrate. reply fellowmartian 1 hour agoparentprevI’m sorry, but what exponential growth are you talking about? No company will ever pay you that much as a regular salaried employee. reply itake 59 minutes agorootparentCareer growth. This includes job title, responsibilities, and compensation. Companies can promote you every 1-2 years, but you need to start that clock. reply coldtea 43 minutes agorootparentThat's not exponential growth. It's not even linear-with-good-slope. And it's not even \"give 1x work -same as before- and take x1.5 money\" but more like \"give 1.5x work and take x1.5 money\" at best (and usually less): more responsibilities, more stress, and more work time. reply fellowmartian 52 minutes agorootparentprevIt’s not exponential though, at most you’re looking at 2-3 orders of magnitude growth unless you’re a founder. reply coldtea 48 minutes agoparentprev>meaning trying to innovate and strategize within their current company to make the job more fun, gain more autonomy, and advance their career prospects Assuming that's in an environment where it's possible (no shitty office politics, etc), the monetary returns from that ain't gonna be competing to a succesful project. Nor is the freedom and satisfaction ever going to be the same. And of course all progress in that front, is a round of layoffs or a change in management away from being nulled. So, it's a safer path (as much as being an employee is safe these days), but not achieving the same thing. reply declan_roberts 21 hours agoprevIf anyone else finds themself in this position, I highly recommend you sell what you can, pack the rest up into a storage unit, and travel, 4 or 6 months in each location. $80k can easily last you 4+ years of very comfortable living in much of the world. Enjoy the food, really try and learn some of the local language, and enjoy yourself. You can do it, and there won't be another window. reply bane 15 hours agoparentMy wife and I always say that the most dangerous show on television is \"House Hunters International\". \"I need a 3 bedroom apartment with a full kitchen in the village center, with at least parking for one car, I have a budget of $300/mo\". \"Here's at least three options\" It also bring up a kind of discomfort between us, as tech workers, and regular people. The quantity of money we can earn, even at the lower end in the U.S., is unfathomable. It makes us want to go places and spend somewhat frivolously to support local businesses. We were recently in Portugal. There's a ton of trendy food spots with prices near what we'd pay in HCOL U.S., but there's a ton of really local, mom 'n pop places, with absolutely incredibly prices, and they're incredibly appreciative of your business. One place we frequented was maybe a 30 second walk from a very trendy tourist district, but served local food at very local prices. Ubers were lined up to drop people to go overpay for mediocre food in the district, literally on the street next to where we were eating. If they would have just taken a moment, they could have come and had a great relaxed meal and support some locals for almost next to nothing. reply Simon_ORourke 10 hours agorootparent> We were recently in Portugal. There's a ton of trendy food spots with prices near what we'd pay in HCOL U.S., but there's a ton of really local, mom 'n pop places, with absolutely incredibly prices, and they're incredibly appreciative of your business. I hear that - last time I was in Lisbon I got breakfast in one of those kind of places and had to ask the cashier if they made a mistake with the change and had given me back too much. Like literally, they could have charged double I would have accepted it as a fair price for a nice meal. reply Der_Einzige 1 hour agorootparentprevThe trivial ability to use even a small fraction of this wealth to improve a huge amount of lives is exactly what the “effective altruist” crowd should be creaming themselves over. The fact that I can buy a house in Thailand for something insanely cheap also means that I can make sure that the local school has whatever they need, for basically just a haircut. You can take the local orphanage and make sure hundreds of kids are comfortable for hundreds of USD a month. reply carlosjobim 31 minutes agorootparentUSD and EUR are in for a massive correction in foreign exchange value. It's not like a dollar earned in reality is worth ten times more than what a person in Thailand earns. So enjoy it while it lasts. The law of supply and demand means that as the supply of USD and EUR to other countries increases, their value will decrease. Especially the EUR, which has nothing to back it up. At least the USD has the world police to back up the currency value. reply secondcoming 3 hours agorootparentprevThat's great, unless you're a young person in Portugal being priced out of accomodation due to wealthy foreigners. reply bane 1 hour agorootparentYeah, we were sensitive to that fact. It's unfortunate. We came across more than a couple business owners who had side hustles as property speculators. One owner basically said that he's buying whatever he can get his hands on since there's a guaranteed return. As tourists we're of course part of the problem, but we're also a lot of the economy. I think Lisbon is also somewhat in a problem like San Francisco, constrained geography, and paradise for the climate, all creating very high demand from people who have the money. People with money will simply outpay people with less to live in more desirable areas. It doesn't help that Portugal also has very cheap/easy long-term/permanent visa programs which doesn't help. They're almost one of the easiest ones globally that also gets you into the EU. https://www.globalcitizensolutions.com/portugal-residency-vi... reply szundi 12 hours agorootparentprevI second this, I had a seabass with potatoes for 5.5 eurs some years ago... not the nicest environment but it was clean and the food was delicious. reply cactusplant7374 9 hours agorootparentprevI have only met poor retirees when living abroad. The sort of people that have never been abroad before but love the low prices (and spend most of their time talking about it). I don’t think the House Hunters demographic has much money. reply eps 9 hours agorootparentprev> One place we frequented Got a name? reply bane 1 hour agorootparentO Golfinho, literally right outside of LxFactory. You'd have to work hard to spend more than 12 euros a person. Simple, local food. Lots of specials (soup, sandwich, fries, beer for like 9-10 euro). Run by an older couple who spoke no English and treated us like we were their niece and nephew. I think both of us got in and out of there for under 20 euro combined. For a bit higher-end, we liked Cantinho do Sol near the Marques De Pombal circle. I think we spent maybe 20-25 euros a person there, were stuffed to the gills, and nearly drown in our drinks. Employees were lovely Brazilians in the front of house and the owner, a local I think, was the chef. reply eps 1 hour agorootparentWoah, looks very nice, thanks. Noted for the future visit. > For a bit higher-end You've been to the TimeOut Market? Very busy, touristy, but good selection of local specialties. Same price range, more or less. reply bane 44 minutes agorootparentNo, but nice looking recommendation for our next time there! reply insane_dreamer 21 hours agoparentprevMy brother and his wife did this; quit their jobs and spent a year in Europe \"slow living\" (still did a bit of consulting work, maybe 10 hours/week), but with careful planning were able to keep their expenses waaay down (not sure of exact figure but much less than if living in the US). Month-long AirBNBs in non-tourist areas are inexpensive, so they'd spend 1 month in each place. A lot of walking, hiking, trains, just living. Not Paris, London or Zurich of course, but places like Albania, Montenegro, Portugal, Greece, Turkey, France of course. (Part of the reason for going to those countries is you can only spend so long in the Schengen Zone.) So you don't have to go as far as Vietnam or Ecuador to do this. Obviously the OP has no children (and my brothers' kids are grown up) - if you have kids this changes the equation completely (though if they're very young then it can still work). reply Loughla 21 hours agorootparentI have a cousin and his wife who did this with kids. They sold everything, bought an RV, and traveled north and Central America. They were homeschooling during that time. Their kids were 5 and 7. It was not good. I cannot stress enough how bad of an idea this is unless your children are under 3. Children need structure and permanency. They need friends, not just acquaintances. My cousin did everything else right, but his kids still have issues with relationships because 5 of their very formative years were spent without actual friends. It's really sad to think about. For him, it was a wildly selfish move that negatively impacted his children. Don't be that guy. reply pinkmuffinere 14 hours agorootparentI totally agree this is bad for the kids. Fwiw please try to encourage the kids to break out of their shell and socialize in college/high school/when they can. I moved a ton growing up, went to four different high schools, etc, and had a really hard time being “normal” with other people. In grad school I got a TA position that forced me to do 1:1 meetings with ~300 new students every semester, and that experience taught me how to be more normal. I’m still weird, but at least can fit in with Bay Area engineers. All this to say — I didn’t have their experience, but I think they can ameliorate it reply hi_hi 13 hours agorootparentWeird doesn't exist. You are not weird. You are not normal. This may be different to your situation, but when i was younger I wanted people to like me, and if they didn't I'd blame myself for being weird. Now I'm older, I don't give a shit if people like me or not, and I've stopped thinking of myself as weird. reply kortilla 12 hours agorootparentWeird definitely exists. If you can’t form social connections well with pretty much everyone you run into, you’re weird. You can pontificate about the proper terminology but that’s the one society understands. reply Earw0rm 9 hours agorootparentI don't entirely buy that, I'd suggest there's a fair sized chunk of sampling bias in that statement. Simply because the sample of people we encounter socially is skewed towards the pro-social, and the ones we notice and remember, doubly so. There's a lot more quiet, reserved, low-key people out there than we realise, and most of them aren't weird. (Also plenty of pro-social, extrovert types who are weird AF, but that's a different topic.) reply pineaux 7 hours agorootparentI am very weird and quite capable of making friends with people. I am not super extrovert, I need concentrated down time. And will ignore people when I am doing the introvert phase. But overal people find me sociable. Weird just means not normal. It's basically a codified statistical concept. reply Earw0rm 6 hours agorootparentThere's whether or not you can make friends at all, and how choosy you are about who to make friends with, which aren't really the same thing. As a moderate introvert (handle social situations OK but need the down time to balance), I just find some people aren't worth the effort, and like to save my energy for those that are. But the weird/normal axis, I'm a little less comfortable with (similarly \"neurotypical\"/\"neurodivergent\"). Fundamentally I dislike the idea of letting the most boring people claim normal, in a similar way to how LGBTQ and nonwhite folks don't like it when cishet or white people claim normal. Most of the people I most enjoy spending time with are ADD, ADHD or ASD, and all the better for it. It's not like these are even disabilities, they're just different ways of being. I'm OK with the labels themselves as a broad, shorthand way of understanding personality types / ways of seeing the world, but I don't buy that no-label people are more normal. reply pinkmuffinere 12 hours agorootparentprevI agree to some extent, and I am getting better at just being comfortable in my own skin, but I think social conventions matter too. Eg I shouldn’t act overly surprised when someone says they’re a morning person, but I should/can do that when they say they’re pregnant. If you get too many conventions wrong, people are uncomfortable lol Edit: I realized I didn’t respond as directly as I’d like to. I think I do want people to like me, and that’s ok. I think it’s also ok to not care reply hi_hi 7 hours agorootparentThank you for the considered and insightful response. This is obviously a deeply person topic and each individual will have their own take on how they feel. I think the key point in all of this, which you and others highlighted, is weird vs not weird is very much a consequence of social conventions. It's also important, generally speaking, to fit into these conventions to facilitate social cohesion. There are obviously extremes, which are outside of my considerations here. I should have put more effort into my original comment but I was in a rush at the time. This bit might not apply to you, I don't know you, these are just my own poor articulations. Feeling like you are weird, or don't fit in, or make people uncomfortable, basically comes down to peoples reflections of their judgement on you. This is inescapable and to judge is human nature. But being on the wrong side of it, for long enough, can lead to a very negative mental state. Having the ability to realise you are not responsible for other peoples feelings is important. And also realising these feelings are largely dictated by the society you find yourself in is also important. These things can be changed, social circles need not be permanent, and should probably be changed if leading to a negative mental state, brought about because you feel you don't fit in. I stand by my original point that there is no such thing as weird or normal, anymore so than some cultures or societies can appear weird or normal, which is highly relative. Otherwise intelligent and conscientious people should not believe themselves to be less than they are because they are at odds with their current time and place. As Yuval Noah Harari would say, society is a fiction. reply baq 9 hours agorootparentprevIf most people aren’t like you, you are not normal by definition. Good for you that you learned to cope with that. After the ‘don’t give a shit’ stage there usually is ‘sit back and observe’ stage to understand what exactly you don’t give a shit about. reply hi_hi 7 hours agorootparentDefine \"most people\"? Clearly an absurd question in the context of this discussion. It's like saying \"Most people on earth are not like you\". How do you define that? reply vasco 8 hours agorootparentprevThis comment only has a slight amount of ground to stand on if you actually have a bunch of friends, otherwise it's just \"hell is other people\". reply bowsamic 11 hours agorootparentprevSounds like you developed a bad coping mechanism imo, you absolutely should and still do care if people like you reply WizardClickBoy 10 hours agorootparentI actually laughed out loud at how different this is to the usual \"be yourself and to hell with what people think\" advice. Can you elaborate on why it's so important to care if people like you? reply OldOneEye 10 hours agorootparentNot OP, but we humans are social animals. As much as we may want to pretend we live alone just fine, it's not the common case. Sure, some people enjoy solitude and don't have to care about what others think, but most of us enjoy company, and this comes with caring about others and what they think of you. You surely care about what your partner thinks about you. Your parents perhaps? Your friends? It's part of the emotional connection. You can be laid back and easy going, but you're still going to care if your loved ones strongly go against your core beliefs and ways of living, right? reply CalRobert 5 hours agorootparentprevWell, people are generally social animals. Also, whether we like it or not we depend on other people. If you want to get hired, reproduce, sell stuff, or just not be a hermit, it matters what people think of you. Sometimes that means changing who you are. Sometimes it means finding people who are more like you (I know that I hate living in most rural areas based on the people I've met in them, for instance). Maybe a combination. reply Spooky23 2 hours agorootparentprevYou’re the common protagonist in every scene of your personal story. If everyone prefers not do deal with you, perhaps they are all toxic, terrible people. Or… there is a common element. There’s a difference between defining yourself based on the expectations of others and being such an individual that nobody can relate. reply bowsamic 9 hours agorootparentprevIt’s basic human psychology reply otikik 8 hours agorootparentprevAt the very least your parents need to tolerate you, because you depend on them for living during your early years. So early on it is a simple survival necessity. It turns out that this necessity never truly goes away. Aside from merely surviving (e.g. you need your doctor to at least tolerate you) interacting with other human is what makes life more than just surviving. At least it’s like that for most people. Even hermits and sociopaths need to be liked by at least one person, which is their own selves. Since the number must be at least one, it might as well be 2 or 3. reply bowsamic 6 hours agorootparent> At the very least your parents need to tolerate you, because you depend on them for living during your early years. So early on it is a simple survival necessity. Yep, and even a slight degradation of that trust that your parents that are necessary for your survival will protect you can have devastating, life-long psychological effects. And indeed everything can be traced back to that. Perhaps controversial but I think this is the origin of most religion: baby is protected by infinitely powerful parents, child has shocking and painful revelation that their parents are not infinitely powerful and have all kinds of insecurities and weakness, therefore a forever infallible representative (e.g. God) is constructed to fill in that gap. But I think that's just one way to fill the gap, and people engage in all kinds of strange, obsessive behaviours to try and reclaim that illusion of eternal protection and safety. reply rvba 4 hours agorootparentprevThe \"be yourself\" crap generally doesnt work. Good advice would be \"be somebody else\". The self help books that teach various tricks... basically make one be someone else. reply bowsamic 3 hours agorootparentYeah, most of the advice is really bad, because they want to avoid the harsh truth: that things aren't necessarily going to work out. You can't sell a self-help book that teaches \"you need people to love you but you might be left alone forever\". People want a guaranteed solution but that simply doesn't exist. However, a lot of people, like the person I originally replied to, choose to remain alone, and that's often because they are scared of rejection or of being left alone. It's kind of ironic, like a contradiction. Longing for connection, but being so scared of rejection that you force the rejection to happen yourself, so that it doesn't happen to you involuntarily, but by forcing that rejection through self-isolation you basically guarantee your doom rather than opening the possibility for flourishing. reply em-bee 18 hours agorootparentprevi used to think that moving frequently when i was young was the cause of why i didn't make any friends, but i realized that there was much more to it. if the kids have relationship issues, then i suspect it wasn't the frequent travel that caused that but it may also have been relationship issues with the parents as well. staying in one place and going to school may have mitigated the issue or it may have not. we can't say. my parents went through a divorce when we moved, and we didn't get any help from family or community while my parents tried to sort out their lives. there wasn't much, if anything they could have done better. the things that happened were more or less unavoidable. obviously i don't know your cousin, but before you blame him, consider that there may have been other factors that you can't see, that were beyond their control. the worst thing in my experience is relatives who think they know what i am doing wrong as a parent, without understanding the whole picture. (friends too, but once friends do that, they are no longer friends). try not to be that person. reply 0x1ceb00da 9 hours agorootparentHomeschooled kids are overfitted to their training data (parents). reply rikafurude21 21 hours agorootparentprevI dont really see the issue here? Obviously the parents are the stable anchor in that situation, and the kids probably had a great time traveling and exploring the world with their parents. I would honestly prefer that compared to the usual childhood experience you get nowadays, which is sitting around watching youtube brainrot right after being subjected to public school brainrot for 8 hours. reply geraldwhen 20 hours agorootparentKids need structure and permanency. This is child abuse, and I know people who have done it. Their kids are a mess, and it breaks my heart. reply UniverseHacker 12 hours agorootparentI am into sailing and know a bunch of people that grew up “cruising the world” on sailboats with their parents. They’re almost all well adjusted, successful, and have unusually good social skills from learning how to make friends with people from other cultures. I think the structure and permanence that kids need is emotionally mature and available parents- the physical part is not the important part. Also, kids can have mental health problems and developmental disorders that have nothing to do with parenting- and it’s pretty awful to blame parents for their kids problems unless they intentionally abused them. reply withinboredom 8 hours agorootparentHol up. Sailing culture is VERY different from RV culture. I lived on a sailboat for years, going up and down the US coast, startup hopping. The sailing culture is very friendly, especially to kids (who are treated more like small adults than children). The people I met along the way are life-long friends. If I needed a ride to an auto store to pick up new house batteries, engine parts, or even to a Walmart, there were always people to give me a ride. RV culture is much more lonely and expects people to be more self-sufficient. If you need help, there may or may not be people to help you; for sure, the whole campsite won’t jump to help, unlike a marina. At least, that was my experience. reply jhives 2 hours agorootparentI don’t think that’s true of RV culture at all. I currently live on the road with my partner, and while we don’t yet have kids, we’ve met dozens of families and many more dozens of kids. All were more than capable of socializing and making friends, both with adults and other kids, and could often be found running around whatever campground or forest we were parked at. Certainly you can live a very isolated life on the road, but just like when living in a house if you put yourself out there there is a welcoming community that will respond. reply geraldwhen 8 hours agorootparentprevIt’s funny you mention sailing. The couple people I know who grew up on a boat sailing like you describe no longer talk to their parents and long for a normal childhood they didn’t have. reply borski 11 hours agorootparentprevThis is precisely the first thing I thought of. You can absolutely provide structure and strong friendships without physical proximity. reply zrobotics 15 hours agorootparentprevIt's presumptuous to claim child abuse, since we fundamentally know very little about the situation. I wouldn't say it's abuse, but it's certainly depriving the kids from learning how to develop socially. They aren't learning how to maintain friendships, and are being implicitly taught that such connections are disposable. I've had the misfortune to see actual child abuse, from the story presented in the OP it doesn't rise even close to that level. Let's please reserve words/phrases like that for situations that warrant it. It may not be an ideal parenting strategy, but claiming it's abuse cheapens the word. Are the children being fed properly? Are they being physically/sexually harmed? I've unfortunately had to intervene in a situation with my niece that involved the above 3. The parenting method in the OP may not be ideal, but plenty of people have had childhoods like that My mom grew up moving every 5-8 months, her dad was a contractor for the TVA. There are still people who follow around contracting work. Please don't minimize that actual harm caused by child abuse by cheapening the term. reply Aeolun 13 hours agorootparentWhile one is undoubtedly worse than the other, I don’t think it’s a good idea to say it’s not abuse because it’s not as bad as the worst possible variety. I happen to agree this probably doesn’t rise to the level of child abuse, but there’s a large range between there and unfed/physically/sexually harmed. We shouldn’t cheapen it, but we shouldn’t make it too expensive either. reply borski 11 hours agorootparentYou’re right. But this is decidedly not abuse; it’s a parenting style that many are unaccustomed to, and perhaps doesn’t work as well in the US, I have no idea. But it’s not abuse. You’re correct that physical/sexual harm and malnutrition are not the only things that constitute abuse either, though. reply SoftTalker 20 hours agorootparentprevYes I know a woman who grew up this way, moving every couple of years, nothing ever permanent. As an adult, she has lived a life of self-sabotage, quitting jobs, moving and starting over, in and out of relationships with self-destructive men, never saving any money, never really planning for the future. reply raddan 15 hours agorootparentMy family moved nearly every year (and sometimes twice a year) until I was in high school. I loved the changes. I liked that when I went someplace new, the only thing my peers knew about me was what I told them. I made many friends over the years and I’d like to think that I am happy. So YMMV. reply rvba 4 hours agorootparentWith how many of those friends do you still talk? *Liking facebook photos does not count. reply BehindBlueEyes 3 hours agorootparentHow many childhood friend does anyone still talk to beyond liking photos on facebook? reply borski 11 hours agorootparentprevThis is correlation. Plenty of people are brought up this way and are completely fine, and plenty of people are just like this and stayed in one place their entire lives. reply hi_hi 13 hours agorootparentprevYou just described a good friend of mine. He's one of the smartest and most capable people I know, but could never get out of his own way. reply LouisSayers 12 hours agorootparentprevAnd I went to 7 different schools growing up, have since lived in several countries, have a master's degree and money in the bank. Correlation != Causation. People are different and respond differently to various situations. For me personally, having been in lots of places means I always have people to visit and in many cases a place to stay. reply zrobotics 15 hours agorootparentprevI mean, the adult life you describe doesn't sound different from my brother, and growing up we moved once. Edit: my brother grew up into a child abusing POS by neglecting his kids, but let's looks at statistics VS anecdotes since individuals from all backgrounds can be garbage people. reply Invictus0 4 hours agorootparentprevI know a woman who grew up in the typical way, in a stable loving household. As an adult, she has lived a life of self-sabotage, quitting jobs, moving and starting over, in and out of relationships with self-destructive men, never saving any money, never really planning for the future. reply galangalalgol 20 hours agorootparentprevHumans have been nomads for most of our existence. I think if the traveling group had been larger so that there were consistent friendships of various ages that it might have gone differently. Carnival workers are a modern example in NA, they have traveling homeschools as they go to each ren-fair or whatever. So there are permanent friends, but the structure is pretty lacking. reply Invictus0 4 hours agorootparentprevPlease don’t trivialize child abuse like this. reply stogot 15 hours agorootparentprevThis is not child abuse. It’s an anecdote of two kids. I’m sure the parents were very loving. Another pair of kids could turn out perfectly fine doing this. reply jmcgough 15 hours agorootparentIn the above scenario, they were depriving their kids of any kind of meaningful friendship with peers during a critical development period. This is neglect, and neglect is abuse in children. If you care about your children, why would you risk permanently impairing their ability to form healthy relationships? reply piuantiderp 12 hours agorootparentI'm not sure the average school peers are any better. Unless we're talking private education. Do consider that for the better part of history, even in school, kids were not separated into years. That they need permanency, sure but that's the parents and the living situation reply insane_dreamer 20 hours agorootparentprev> Kids need structure and permanency. This is child abuse Sorry, but this is BS. Structure, yes. Permanency, no. And certainly not child abuse. I know just as many examples of people with this experience, for whom it was amazingly positive and contributed to the successful people they are today. reply lenkite 16 hours agorootparentThere is an extraordinary amount of scientific evidence that frequent moving during childhood severely impacts child psychology. I am not even sure how you can say it is BS while throwing up anecdotal experience. reply hilux 12 hours agorootparentMost of your \"evidence\" has to do with kids moving between foster homes, or families running out in the middle of the night because they don't have money to make rent. That is nothing like what we are discussing here. reply lenkite 11 hours agorootparentNo, this is extensively tested and include educated, higher social-class and stable families. Make a consultation with a practicing psychologist if you doubt the dozens of studies carried out across nations in both the West and the East. \"Even AFTER accounting for family background and achievement at the end of kindergarten, mobile students had significantly lower reading and math achievement tests scores in seventh grade.\" \"Frequent relocation was associated with higher rates of all measures of child dysfunction; 23% of children who moved frequently had repeated a grade vs 12% of children who never or infrequently moved. Eighteen percent of children who moved frequently had four or more behavioral problems vs 7% of children who never or infrequently moved. Use of logistic regression to control for potential confounding covariates demonstrated that children who moved frequently were 77% more likely to be reported to have four or more behavioral problems\" reply hilux 10 hours agorootparentIf you're going to quote studies, you should cite then. Then we can pick apart what \"mobile\" means to your ivory tower researchers, which is almost definitely not \"traveling the world as a healthy, happy, family unit.\" Diplomats travel with their families. Employees of multi-nationals travel. US military travel. At least that last group (i.e. their kids) I know does better than average. reply 317070 4 hours agorootparentNot OP, but he's quoting this article: https://pubmed.ncbi.nlm.nih.gov/7689659/ reply interloxia 10 hours agorootparentprevThe study of Third Culture Kids and the trouble they have is probably a good example of research on the topic that doesn't focus on foster kids. A specific research paper isn't necessary in my opinion for this site and topic, but Wikipedia is a simple starting point to find some. https://en.m.wikipedia.org/wiki/Third_culture_kid reply Glyptodon 14 hours agorootparentprevAt the same time I doubt anyone would use that kind of thing as evidence for forcibly settling nomadic cultures. I'm somewhat curious because I'm sure they try to do things like divorce socioeconomic factors, abuse, poverty, and other negatives from such a conclusion. But as someone who moved 7 times through 7th grade and attended at least 6 different schools through that interval, I'm generally quite grateful to not have been dulled and stultified by living in one place my whole life. To the point that I've contemplated planning at least one or two significant moves so my own children don't end up excessively influenced by whatever locality specific tints and delusions color a place. (Maybe another way to put it is its easier to boil frogs that have always lived in the same pot.) But I also do think it's nice for kids to have a solid friend group for a good part of childhood, and so forth. I suspect there's got to be a lot more complexity to this. (And I will say I do think there's a connection between moving a lot and loneliness, but I view loneliness as distinct from generalized depression. But totally not scientific.) reply stogot 15 hours agorootparentprevThe original OP is an anecdote. Could you link the extraordinary data? I want to see how they can empirically test this reply lenkite 15 hours agorootparentHonestly, you can very easily find this yourself. Multiple independent studies carried out across both organizations and nations have shown this. https://pmc.ncbi.nlm.nih.gov/articles/PMC4866579/ https://www.sciencedirect.com/science/article/pii/S135382921... For the layman: https://www.nytimes.com/2024/07/17/health/moving-childhood-d... https://www.psychiatrist.com/news/moving-even-once-in-childh... https://www.verywellmind.com/moving-depression-and-your-chil... reply borski 11 hours agorootparentLiterally the first three proposed explanations in the discussion of the findings in the meta-study you linked are correlational. It is only the fourth proposed explanation that suggests a possible causal relationship. reply LouisSayers 12 hours agorootparentprevAnd how much of this is just correlation and not due to mobility per se? From the first link you provided: > Firstly, increased risk for onset of mental disorders between mid-adolescence and early middle age could be a consequence of serious and enduring difficulties within families, rather than being a direct result of residential mobility. Relocation occurs more commonly amongst single parent and step families and those from lower socioeconomic background reply lenkite 11 hours agorootparentIt cannot be dismissed as mere \"correlation\". These studies have been carried out on people of middle-class background too. Esp children of defense service personnel who have experienced frequent mobility. And studies carried out by different nations AND different cultures as well! Mental well-being is not mathematics - you cannot proof a definite cause with utterly no ambiguity. Unfortunately, you took one single \"could be\" sentence out of an entire gamut of data confirming mobility-related mental health issues in children and completely ignored the conclusions section in that same paper, so I think you have already severely hardened your position and are unlikely to be convinced by anything I offer. I would suggest simply talking to a practicing psychologist about this - you would probably be far more convinced than a HN commenter. Actually, this is where I first found out the same - I didn't know about this until a consultation with a psychologist. (You can also ask your AI buddies - ChatGPT also confirmed it with several case studies offered.) reply addicted 14 hours agorootparentprevRV living is very different from living in an actual home (or even Air Bnb) and moving around. AirBnB’s and homes are located in areas and designed for living like normal social human beings. RVs are parked in areas that are not designed to sustain long term living. reply rvba 4 hours agorootparentSocializing kids is about actual interactions with other kids. Something that nomadic life takes away from them. In many cases they simply dont have any friends at all. Or are always the incomers. Of course you can live in a place with lots of kids of your own age and still be lonly, or the kids can be dicks, but in my opinion there is benefit in socialization at age 4-10. Kids could go out ans play together. The nomads cant. IMO permanent group of friends and place, repetition, predictability are a foundation for growth. Then you can sprinkle one off things on top. Not a life of unpredictable mess when you are on your own. Also if you dont speak the local language how can one even socialize reply tbrownaw 15 hours agorootparentprevShouldn't something that unavoidably severe show up pretty clearly at a population level for military kids? I'd think there'd be some high-profile lawsuits by now. reply lumost 15 hours agorootparentMilitary kids hang out with other military kids for protracted periods (6+ months). Living in an ev in a foreign country where you don’t speak the language is different. reply zrobotics 15 hours agorootparentprevI mean, 'military brat' is definitely a stereotype. I'm not a sociologist so not familiar with relevant research, but I would be shocked if it hadn't been studied. One thing I would point out is that military kids are in a far different scenario than the OP's cousins kids. Military kids will grow up moving from base to base, but the schools around bases are fundamentally different. Those are schools where teachers are used to student turnover, and the students are as well. OP talked about kids that were in a more nomadic situation, where they could only form briefinto a storage unit I've heard some terrible things about how vulnerable those are to theft / squatters. reply pluc 11 hours agoparentprevI am currently doing this, AMA! reply em-bee 3 hours agorootparenti did this more than 20 years ago. i was between jobs. my choice was to go back home or go travel. so i sent my stuff home to my grandparents and went to travel... reply mock-possum 14 hours agoparentprevSorry, but then what - money will just be magically waiting for you when you’re ready to come home? Having spent every penny to your name? God it must be nice to be in a position where you can afford to just be like “ok my partner and I are just going to travel for four years, and after that, idk, things will work out somehow.” reply ornornor 8 hours agorootparentSame as it would be nice to win the lottery. Not everyone is so lucky and others will be jealous they can’t do it too. reply StressedDev 14 hours agorootparentprevSome people don't mind living paycheck to paycheck. It has a downside but if it works for someone, I don't think judging it really helps. reply thfuran 11 hours agorootparentI suspect there's very little overlap between that group and people who have $80k sitting around. reply 123yawaworht456 20 hours agoparentprevnext [10 more] [flagged] senkora 15 hours agorootparentThe food in different places is truly different, in surprising ways. It really is worth it to travel for food. I’m not sure what factors make it that way, but my experience tells me that it is true. reply raddan 15 hours agorootparentprevFor me, it’s less about the food (but to be clear, still about the food) and more about the cultural experience. Going someplace different, seeing how other people live, and eating with them can be a real joy. Your mileage may vary, of course. But people often find ways to connect over our shared need to eat. I think fondly about a meal I shared in rural China. I was the only American in the group. Despite our many differences, we all learned a lot from each other. I have similar strong memories about having breakfast at Shaw’s hiker hostel in Maine (the one featured in Bryson’s A Walk in the Woods). And many others. I hope to have many more experiences like that in my life. reply eszed 15 hours agorootparentprevYou may not care, and that's fine, but > every ingredient imaginable available globally is utterly false. My wife and I have been traveling in Europe for the last couple of weeks. We've had maybe three or four meals in restaurants, and mostly gone to grocery stores and cooked in our Airbnbs. The array of (only) locally-available breads, pastries, meats, cheeses, wines, yoghurts - heck, even breakfast cereals - is incredible, and of fantastic quality. We just traveled when you can watch a youtube video for every dish in existence Do you watch a youtube video when you get hungry? reply grogenaut 15 hours agorootparentOften. I watch them while I'm cooking in the background. Max Miller is pretty fun. reply bloqs 15 hours agorootparentprevThey dont mean the food. They think they do, but they dont. What they mean is the sense of cultural inhabitation. Eating and living a certain way for tourism is deeply rewarding on a human level. Its about people, not things. reply sampullman 14 hours agorootparentI think that can be true, but sometimes it's just about the food. There are a few specific places I go just for the food. There are plenty of dishes which can't be easily replicated outside their host country, either due to quality ingredient access or skill. reply j7ake 15 hours agorootparentprevThe new generation in America has lost ability to cook. reply raddan 15 hours agorootparentPeople have been saying this forever. reply mancerayder 19 hours agoprevThe problem in the US is healthcare. In my state the health insurance marketplace only has 'bronze' plans that are in-network only (with mediocre options) and high deductibles, it costs 6-800 for individual and close to 1500-2k for a family. The United States has rigged the employment market to forever keep you dependent on a corporate employer until retirement. Now these same companies are taking away remote work, forcing people close to expensive and increasingly annoying cities like New York City, where the median one bedroom is one million to buy. So, for giving your time to a company, your reward is never being able to be fully independent without huge risk. Healthcare is the Company Store Voucher of the modern era. Has anyone else considered leaving the United States long before retirement age? reply m00x 10 hours agoparentIn Canada you just die before you get a family doctor :) Last time I went to the ER, the wait was 10h for kidney stones. Thankfully I passed out for 4h in the waiting room from the pain and the stones passed by the time I saw a doctor, so all they did was send me home. My family doctor died 10 years ago and I'm still on a list. reply HWR_14 7 hours agorootparentIn the US, 70-80% of the time they just send people with unpassed kidney stones home from the ER. With painkillers and a giant bill. reply nubinetwork 8 hours agorootparentprevIf you're from Ontario, you might want to call the health care connect people... I was on a list for roughly a year and a half, then out of the blue I got 2 referrals in 2 months (the first one never called me)... while the one I got isn't a typical family doctor, just having someone to help me with my needs is a start... reply newsclues 4 hours agorootparentThen you merely have unreasonably long waits for specialists and testing, and then more waiting for treatment! reply psb217 1 hour agorootparentIt's super cool when you have to go through a chain of several specialists while dialing in on the problem. Especially when the default diagnosis is typically: \"it's probably nothing, wait six weeks and make another appointment if it's still an issue...\", lol. reply jrochkind1 4 hours agorootparentprevCan't find a primary care doctor in the US either, so we're even. reply FredPret 2 hours agorootparentprevHave you tried calling doctors directly? Sometimes there are information gaps between their capacity and the provincial bureaucracy. reply gadders 1 hour agorootparentprevSurprised you weren't offered MAID. reply iancmceachern 12 hours agoparentprevBeing your own employer is the only way to take charge of this. Platforms like Gusto, etc. make it so even small businesses can offer benefits and insurance like bigger companies do. I went though various big and small employers plans, several states' exchanges, and none were nearly as good as doing the legwork to get benefits setup for my own company. Now I choose. reply mancerayder 1 hour agorootparentIn some states the insurance group policy won't be available for a one person LLC, and there's no option direct from the health insurance companies, so all that remains is the state marketplace. I tried. I ended up signing up with an agency that basically put me as a W2 employee and took a fee, so I could use their so-so but better networked group insurance. Is Gusto for stuff like that? Single member LLCs? reply iancmceachern 58 minutes agorootparentYes, that's exactly what's it's for, also bigger companies. For smaller ones they sort of bundle you in with other small companies for the insurance, is my understanding. You may not have as many options, but they're better in the same ways an employer plan is, because they are one. reply wrs 41 minutes agorootparentprevThis may vary by state, but in Washington my broker tells me that a one-person LLC can work, if it elects to be taxed as a corporation so it can issue that person a W-2. Which accountants hate because of the potential extra taxes, but as she says, \"my job is to get you health insurance, not reduce your taxes\". reply pcl 10 hours agorootparentprevHow do the costs pencil out on that, if you are signing up with Gusto etc. only for access to the health care plans? Presumably you need some minimum payroll or something? reply iancmceachern 10 hours agorootparentYes, even of you are a one person company you need to pay yourself through some sort of payroll service so that you stay compliant with all the regulations and pay things like disability, various taxes, etc. So you already need them, or similar, to handle that stuff. The minimum payroll is defined by the minimum wage in your local area, keep in mind you can also choose how many hours you work. reply le-mark 5 hours agorootparentThis sounds like one hell of a hack! Can you go into a bit more detail about how you have it set up? reply iancmceachern 2 hours agorootparentSomeday I'll write a blog post about it. Feel free to reach out to me, contact info is in my bio. I'll be happy to share anything you'd be interested in. reply the__alchemist 1 hour agorootparentThis seems like an incredibly useful piece of information to anyone who doesn't have trad employment in the US. I found on Gusto's site info about how you can write off your insurance costs if you meet certain criteria. Is the benefit this, or something else? reply iancmceachern 56 minutes agorootparentYes, theyre deductable for any employer, this is no different. Lots of benefits including this. It gives you the same power and security a employer plan does, because it is. In my experience the marketplaces and insurance from them can be a minefield needing navigation. reply keyserj 12 hours agoparentprevI'm in Illinois, my bronze marketplace plan was $247/mo this year, and the coverage was good (for what I used at least). 2025 plans look to be going up $50/mo or so which is annoying. Still a good chunk to have to pay, but nowhere near as bad as your situation. Makes me curious how much marketplace costs vary per state? In any case, as another person pointed out, the ACA plans also provide very high premium credits for low income households, covering up to ~80% of the costs (from what I've seen). But this also varies by state, I think. reply remyp 3 hours agoparentprevI did. Healthcare was a big part of that decision. My older siblings have been through hell both medically and financially because of the US healthcare system. I decided to learn from their experiences and GTFO while I could. I miss my family and friends dearly, but it’s a massive relief (and privilege) to be able to get the care I need and not worry about whether the cost is going to bankrupt me. reply qwerpy 2 hours agorootparentWhere did you end up going? I have some savings and would like to retire early, and getting health insurance is an impediment to that. Need somewhere that doesn’t require a work visa and is a good place to raise a family. I have enough to do it in the US but would be happy to consider alternatives. reply remyp 8 minutes agorootparentTo Portugal. If you have enough assets to retire in the US then you probably qualify for the passive income visa. It’s not all smooth sailing, though. The immigration agency here (AIMA) currently has 300,000 cases in the queue. It’s so difficult to get an appointment that people are making hundreds of calls per day, standing in queues for hours only to not be seen, and filing lawsuits. Most people here make about €1200/mo and there’s a severe housing crisis that I have contributed to with my presence. The influx of people like me with vastly higher purchasing power is creating resentment: the far-right party quadrupled (5% -> 20%) their seats in parliament by running against immigration. Having said that, I do love it here. I’ve been considering blogging about my experience since there’s a dearth of clear-eyed information from real people who moved to Portugal and aren’t selling something. Anyhow, I’m happy to share more info and resources with anyone who wants to learn more. Please feel free to reach out - my contact info is in my profile. reply adastra22 14 hours agoparentprevACA plans would be free / fully subsidized in that circumstance. reply anotheracc88 13 hours agorootparentnext [2 more] [flagged] DiscourseFan 12 hours agorootparentThey are not going to gut the ACA, that would not only be extremely unpopular but also impossible in the current congress with only a 2 seat majority in the house for the Republicans. reply gorfian_robot 3 hours agoparentprevin CA if you make less income than the minimum (like $18.5k as a single person) you are put onto the free Medi-Cal plan. so thats nice. :) reply maximinus_thrax 13 hours agoparentprev> Has anyone else considered leaving the United States long before retirement age? Yes. I'm American but was born in a small country in Eastern Europe. I was planning on moving back during the pandemic, however I was presented with some job prospects I couldn't reject and my move was delayed. But I do plan on retiring in my hometown within two years. There is universal healthcare which sucks for the most part, but private insurance is extremely cheap (compared to the US) and I could retire more than comfortably with my current nw. And by 'retire' I don't mean stop working, I mean I'm going to work on whatever I want. And to your original point about healthcare, it is the absolute single reason for why I did not take any sabbaticals or long term leave even while burned out, for my insomnia during layoff seasons and generally for my job related stress. reply the__alchemist 4 hours agoprevI wonder how the dating world would go, given he's divorced, even before running out of money. I.e.: I think most women would immediately reject someone unemployed. (Even if he had money) Taking a risk like this while in a relationship is probably fine, but trying to start one with a stranger is likely to be significantly more difficult than while employed. reply titanomachy 1 hour agoparentBeing single for a couple years isn’t the end of the world. Besides, he’s in the climbing community, which (at least from the outside) appears to be chock full of barely-employed attractive men and women who all sleep with each other. reply Aeolun 1 hour agoparentprevJust have to find someone else that’s unemployed? Or someone that doesn’t care? If there’s men that don’t care a woman is unemployed the reverse must be true too. reply mettamage 1 hour agorootparentThe way I see it, the more disadvantages you have the more you need to make up for it with charisma. That's how I do it anyway. Charisma is trainable. It's not easy to train, but it ultimately is trainable. reply bradlys 59 minutes agoparentprevI have more experience with this than most of the HN crowd. I've not been working for 2.5 years. Being unemployed is considered unattractive but if you spin it - it's not too bad. I spin it as, \"I did startups in SV, one of them went public. I have plenty of money.\" Most everyone gets it but some women are put off by it. I'm literally taking time away from SV because I'm on the hunt for a wife. I couldn't find one after being in SV for 8 years and so now I'm in NYC where at least I can actually meet women. I've traveled a lot as well but I don't recommend that as a way to meet your future wife. Dating culture outside of the US is very different and very few women will be interested in dating someone who is traveling around. There are also other issues like being used for a green card and so on. Sleeping/dating around casually isn't as much of a thing in other countries and it's a necessity if you're going to try to form an intense bond with someone. Overall, I think for a tech worker, the real issue is that so many women will not date a tech worker to begin with because of the low social status associated with dating a tech worker. I've met thousands at this point and it's honestly made me regret joining this industry. I do not advocate for it at all. I think the bigger hurdle with dating beyond that anyway is physical attraction. The bar for that is really high now. Being a rock climber only attracts certain types because rock climbing tends to have more lanky builds. You better hope you're attracted to women who only want to date slim built men. That is a niche woman. As well, you can't escape your baseline genetics like height, facial attractiveness, etc. People talk about rock climbers sleeping with each other all the time but it's mostly men who are rock climbing. There are some women but the ratio is 3:1 - which means you're 2/3 chance of not getting anywhere. reply louwrentius 55 minutes agorootparentThe amount of sexism and stereotypes about women is astounding. I don’t feel you even see women as actual human beings, damn. reply bradlys 1 minute agorootparentIs it sexism to acknowledge that we have 80%+ men in tech? Is it sexism to acknowledge fields like nursing and elementary teaching have 80%+ women? You're free to exist in the real world rather than some third wave feminism utopia. reply ChiMan 7 hours agoprevThe moral of the story: Don’t quit your job unless you have an employment contract for another one or if you have a reliable, growing side hustle that generates enough income to live on. Real life bites hard, and $80k is peanuts. reply titanomachy 1 hour agoparentDid we read the same article? This guy is spending his time learning music, mastering a sport, spending time with close friends, and your takeaway is “he should be grinding away at a corporate like everyone else”? There’s a lot of different ways to live. Personally I like taking periods of voluntary unemployment to explore other interests. Finding a job after has never been a problem. A large percentage of the population has at least some safety net. Probably the average 25 year old could move back in with their parents for a year if things got really bad. reply mplewis 52 minutes agorootparentThis guy is spending more than he makes. Who is bankrolling him? reply ChiMan 1 hour agorootparentprevIn other words, the parents protect the kid from real life. No thanks. reply titanomachy 1 hour agorootparentOr allow the kid to take risks and do more with their lives than they would otherwise, and become self-sufficient in an interesting and unexpected way. What would you want for your own kids? 45 years of grudging corporate toil, then death? What’s the point of become a wealthy and prosperous society if we’re so goddamn uncreative about how we live in it? reply khushy 13 minutes agoprevHow about that guy who wrote graphing calculator? Or the guy who put zip in windows explorer? reply jumparoundwords 19 minutes agoprevCrazy to me that some people in the comments believe that 80k$ dollar is peanuts. Even in Germany I could easily live with that for more than 5 years. If I had that kind of money lying around I would move to another country, pay for my whole education, learn a new language and still have enough money for multiple full-time start-up attempts. reply cahoot_bird 20 hours agoprevProblem is lack of money catches up eventually. If you can't figure out some way to get it, and the longer without a job, the harder it is to get one. I think a lot of people struggled because the economy and job numbers. Supply and demand.. I had contract work, then couldn't get a tech job a couple years ago after a lot of applications. Completely broke. Drove down the road and felt kinda foolish seeing people paying less but decent money for non-tech. got fast food / hospitality worker. Low stress, physical work. Can't imagine where I'd be if I didn't. I kind of get it though, you start doing something else then you don't have much time and energy to improve on what you want to do (such as tech) for a while, a recipe for people to get trapped, unless you can save money and reclaim time somehow to improve, or the supply/demand shifts.. anyone could fail at anything, all I know for certain is the worst thing a person can do is nothing. reply insane_dreamer 19 hours agoparentOne solution, if you can, is to take part-time work. Then you can reduce your expenses to that level, take time off for living, without having to spend down your savings (or only very very slowly). Then it's sustainable. reply CharlieDigital 15 hours agoprevOver the last 4 years, I've gone through a similar \"speed run\" of startups after quitting my job of 10 years during COVID. First startup went nowhere and took on a contract role after 9 months. Then tried another startup with a co-founder I met at one of the startups where I worked in the interim. About 6 months building something awesome, but no commercial path. Spent 1 month with him and another co-founder on a fintech product but realized much faster that once again that there was no viable go-to-market strategy. Started another company and built a product that seemed like it had legs. We had one early user that absolutely loved the product and we thought all we needed was to find more users like her. Turns out that she was a false signal because we never found another user like her and I'm about to shut that one down after almost a year to avoid DE franchise fee next year. If anyone is in a situation like this/thinking about doing something like this, I've gathered some of my lessons learned: https://chrlschn.dev/blog/2024/12/lessons-learned-from-worki... If I had to sum it up: always build the minimal thing that can be \"sold\". Use AI to build the dirtiest MVP as fast as possible. Even better if your \"MVP\" is a deck and you can get people to put money down to wait. Figure out your GTM and messaging with that deck. If you are an engineer, you must resist that urge to build until you're sure you can find enough people that have this problem and want to pay you to solve it. Don't work with a non-technical co-founder if they claim if you build it, customers will come. Don't work with a non-technical co-founder that can't demonstrate an ability to sell. If the vibes feel off, get out fast. Don't form a company with a co-founder until you absolutely have to (like your personal life, don't get married until you're absolutely sure). Lots of mistakes and lessons learned during that time having founded startups that went nowhere, been an employee in startups that went nowhere, and left startups that are actually crushing it. I have seen a big swath of the gamut at this point with some regrets in retrospect. reply tailspin2019 4 hours agoparentGiven the dense amount of wisdom packed into this comment I checked out your profile. Both your current projects look extremely impressive and polished. And viable! Keep up the good work! It’s inspiring to see that in light of the obvious difficult lessons you’ve had to learn along the way. Having learned many of the same lessons as you I can 100% backup everything you said in your “sum it up” paragraph! The only caveat I would add is to the “make an MVP with AI”. I think MVPs generated directly out of ChatGPT/Claude are so easy now (or at least it can appear so on the face of it) that many people are just barely going beyond that - but to any experienced eye, that approach is quite transparent and can look very low-value (even if the idea is actually a good one). Now if that person is a skilled salesperson then that might work. But, for most people, I think it’s still very important to demonstrate good instincts, taste and strategic/commercial understanding when building such an MVP. And that means editing and shaping the output just enough to meet your vision for the product. So to agree with you - definitely, 100%, use AI as much as possible - but don’t assume that you can put zero work in on top and have that MVP be effective. Because the 10 year old down the street has the exact same tool as you - so if you are just relying verbatim on that tool’s output- it’s going to be hard to stand out. I’d still definitely agree to spend as little time as physically possible on the MVP - with the above caveat. Having said all that… a lot of historical wisdom on the topic of MVPs has been turned upside down since gen AI became mainstream, so on the flip side you could argue: create 1000 MVPs in an hour, publish them all, see what generates interest…* Hmm.. I think I just argued against my own point. * (I’m not really seriously suggesting anyone do this, but I’m also not entirely discounting this as an approach either…) reply CharlieDigital 3 hours agorootparent> Both your current projects look extremely impressive and polished. And viable",
    "originSummary": [],
    "commentSummary": [],
    "points": 381,
    "commentCount": 567,
    "retryCount": 0,
    "time": 1733516765
  },
  {
    "id": 42343089,
    "title": ": The Dialog Element",
    "originLink": "https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog",
    "originBody": ": The Dialog element Baseline Widely available TheHTML element represents a modal or non-modal dialog box or other interactive component, such as a dismissible alert, inspector, or subwindow. The HTMLelement is used to create both modal and non-modal dialog boxes. Modal dialog boxes interrupt interaction with the rest of the page being inert, while non-modal dialog boxes allow interaction with the rest of the page. JavaScript should be used to display theelement. Use the .showModal() method to display a modal dialog and the .show() method to display a non-modal dialog. The dialog box can be closed using the .close() method or using the dialog method when submitting athat is nested within theelement. Modal dialogs can also be closed by pressing the Esc key. Attributes This element includes the global attributes. Warning: The tabindex attribute must not be used on theelement. See usage notes. open Indicates that the dialog box is active and is available for interaction. If the open attribute is not set, the dialog box will not be visible to the user. It is recommended to use the .show() or .showModal() method to render dialogs, rather than the open attribute. If ais opened using the open attribute, it is non-modal. Note: While you can toggle between the open and closed states of non-modal dialog boxes by toggling the presence of the open attribute, this approach is not recommended. Usage notes HTMLelements can be used to close a dialog box if they have the attribute method=\"dialog\" or if the button used to submit the form has formmethod=\"dialog\" set. When awithin ais submitted via the dialog method, the dialog box closes, the states of the form controls are saved but not submitted, and the returnValue property gets set to the value of the button that was activated. The CSS ::backdrop pseudo-element can be used to style the backdrop of a modal dialog, which is displayed behind theelement when the dialog is displayed using the HTMLDialogElement.showModal() method. For example, this pseudo-element could be used to blur, darken, or otherwise obfuscate the inert content behind the modal dialog. The autofocus attribute should be added to the element the user is expected to interact with immediately upon opening a modal dialog. If no other element involves more immediate interaction, it is recommended to add autofocus to the close button inside the dialog, or the dialog itself if the user is expected to click/activate it to dismiss. Do not add the tabindex property to theelement as it is not interactive and does not receive focus. The dialog's contents, including the close button contained in the dialog, can receive focus and be interactive. Accessibility When implementing a dialog, it is important to consider the most appropriate place to set user focus. When using HTMLDialogElement.showModal() to open a , focus is set on the first nested focusable element. Explicitly indicating the initial focus placement by using the autofocus attribute will help ensure initial focus is set on the element deemed the best initial focus placement for any particular dialog. When in doubt, as it may not always be known where initial focus could be set within a dialog, particularly for instances where a dialog's content is dynamically rendered when invoked, theelement itself may provide the best initial focus placement. Ensure a mechanism is provided to allow users to close the dialog. The most robust way to ensure that all users can close the dialog is to include an explicit button to do so, such as a confirmation, cancellation, or close button. By default, a dialog invoked by the showModal() method can be dismissed by pressing the Esc key. A non-modal dialog does not dismiss via the Esc key by default, and depending on what the non-modal dialog represents, it may not be desired for this behavior. Keyboard users expect the Esc key to close modal dialogs; ensure that this behavior is implemented and maintained. If multiple modal dialogs are open, pressing the Esc key should close only the last shown dialog. When using , this behavior is provided by the browser. While dialogs can be created using other elements, the nativeelement provides usability and accessibility features that must be replicated if you use other elements for a similar purpose. If you're creating a custom dialog implementation, ensure that all expected default behaviors are supported and proper labeling recommendations are followed. Theelement is exposed by browsers in a manner similar to custom dialogs that use the ARIA role=\"dialog\" attribute.elements invoked by the showModal() method implicitly have aria-modal=\"true\", whereaselements invoked by the show() method or displayed using the open attribute or by changing the default display of aare exposed as [aria-modal=\"false\"]. When implementing modal dialogs, everything other than theand its contents should be rendered inert using the inert attribute. When usingalong with the HTMLDialogElement.showModal() method, this behavior is provided by the browser. Examples HTML-only dialog This example demonstrates the creation of a non-modal dialog by using only HTML. Because of the boolean open attribute in theelement, the dialog appears open when the page loads. The dialog can be closed by clicking the \"OK\" button because the method attribute in theelement is set to \"dialog\". In this case, no JavaScript is needed to close the form. htmlCopy to ClipboardGreetings, one and all!OK Result Note: Reload the page to reset the output. This dialog is initially open because of the presence of the open attribute. Dialogs that are displayed using the open attribute are non-modal. After clicking \"OK\", the dialog gets dismissed, leaving the Result frame empty. When the dialog is dismissed, there is no method provided to reopen it. For this reason, the preferred method to display non-modal dialogs is by using the HTMLDialogElement.show() method. It is possible to toggle the display of the dialog by adding or removing the boolean open attribute, but it is not the recommended practice. Creating a modal dialog This example demonstrates a modal dialog with a gradient backdrop. The .showModal() method opens the modal dialog when the \"Show the dialog\" button is activated. The dialog can be closed by pressing the Esc key or via the close() method when the \"Close\" button within the dialog is activated. When a dialog opens, the browser, by default, gives focus to the first element that can be focused within the dialog. In this example, the autofocus attribute is applied to the \"Close\" button, giving it focus when the dialog opens, as this is the element we expect the user will interact with immediately after the dialog opens. HTML htmlCopy to ClipboardClose This modal dialog has a groovy backdrop!Show the dialog CSS We can style the backdrop of the dialog by using the ::backdrop pseudo-element. cssCopy to Clipboard ::backdrop { background-image: linear-gradient( 45deg, magenta, rebeccapurple, dodgerblue, green ); opacity: 0.75; } JavaScript The dialog is opened modally using the .showModal() method and closed using the .close() method. jsCopy to Clipboard const dialog = document.querySelector(\"dialog\"); const showButton = document.querySelector(\"dialog + button\"); const closeButton = document.querySelector(\"dialog button\"); // \"Show the dialog\" button opens the dialog modally showButton.addEventListener(\"click\", () => { dialog.showModal(); }); // \"Close\" button closes the dialog closeButton.addEventListener(\"click\", () => { dialog.close(); }); Result When the modal dialog is displayed, it appears above any other dialogs that might be present. Everything outside the modal dialog is inert and interactions outside the dialog are blocked. Notice that when the dialog is open, with the exception of the dialog itself, interaction with the document is not possible; the \"Show the dialog\" button is mostly obfuscated by the almost opaque backdrop of the dialog and is inert. Handling the return value from the dialog This example demonstrates the returnValue of theelement and how to close a modal dialog by using a form. By default, the returnValue is the empty string or the value of the button that submits the form within theelement, if there is one. This example opens a modal dialog when the \"Show the dialog\" button is activated. The dialog contains a form with aand twoelements, which default to type=\"submit\". An event listener updates the value of the \"Confirm\" button when the select option changes. If the \"Confirm\" button is activated to close the dialog, the current value of the button is the return value. If the dialog is closed by pressing the \"Cancel\" button, the returnValue is cancel. When the dialog is closed, the return value is displayed under the \"Show the dialog\" button. If the dialog is closed by pressing the Esc key, the returnValue is not updated, and the close event doesn't occur, so the text in theis not updated. HTML htmlCopy to ClipboardFavorite animal:Choose… Brine shrimp Red panda Spider monkey Cancel Confirm Show the dialog JavaScript jsCopy to Clipboard const showButton = document.getElementById(\"showDialog\"); const favDialog = document.getElementById(\"favDialog\"); const outputBox = document.querySelector(\"output\"); const selectEl = favDialog.querySelector(\"select\"); const confirmBtn = favDialog.querySelector(\"#confirmBtn\"); // \"Show the dialog\" button opens themodally showButton.addEventListener(\"click\", () => { favDialog.showModal(); }); // \"Cancel\" button closes the dialog without submitting because of [formmethod=\"dialog\"], triggering a close event. favDialog.addEventListener(\"close\", (e) => { outputBox.value = favDialog.returnValue === \"default\" ? \"No return value.\" : `ReturnValue: ${favDialog.returnValue}.`; // Have to check for \"default\" rather than empty string }); // Prevent the \"confirm\" button from the default behavior of submitting the form, and close the dialog with the `close()` method, which triggers the \"close\" event. confirmBtn.addEventListener(\"click\", (event) => { event.preventDefault(); // We don't want to submit this fake form favDialog.close(selectEl.value); // Have to send the select box value here. }); Result The above examples demonstrate the following three methods of closing modal dialogs: By submitting the form within the dialog form using the dialog method (as seen in the HTML-only example). By pressing the Esc key. By calling the HTMLDialogElement.close() method (as seen in the modal example). In this example, the \"Cancel\" button closes the dialog via the dialog form method and the \"Confirm\" button closes the dialog via the HTMLDialogElement.close() method. The \"Cancel\" button includes the formmethod=\"dialog\" attribute, which overrides the 's default GET method. When a form's method is dialog, the state of the form is saved but not submitted, and the dialog gets closed. Without an action, submitting the form via the default GET method causes a page to reload. We use JavaScript to prevent the submission and close the dialog with the event.preventDefault() and HTMLDialogElement.close() methods, respectively. It is important to provide a closing mechanism within every dialog element. The Esc key does not close non-modal dialogs by default, nor can one assume that a user will even have access to a physical keyboard (e.g., someone using a touch screen device without access to a keyboard). Closing a dialog with a required form input When a form inside a dialog has a required input, the user agent will only let you close the dialog once you provide a value for the required input. To close such dialog, either use the formnovalidate attribute on the close button or call the close() method on the dialog object when the close button is clicked. htmlCopy to Clipboard Favorite animal:Show the dialog [type=\"submit\"] { margin-right: 1rem; } JavaScript jsCopy to Clipboard const showBtn = document.getElementById(\"show-dialog\"); const dialog = document.getElementById(\"dialog\"); const jsCloseBtn = dialog.querySelector(\"#js-close\"); showBtn.addEventListener(\"click\", () => { dialog.showModal(); }); jsCloseBtn.addEventListener(\"click\", (e) => { e.preventDefault(); dialog.close(); }); Result From the output, we see it is impossible to close the dialog using the Normal close button. But the dialog can be closed if we bypass the form validation using the formnovalidate attribute on the Cancel button. Programmatically, dialog.close() will also close such dialog. Animating dialogs s are set to display: none; when hidden and display: block; when shown, as well as being removed from / added to the top layer and the accessibility tree. Therefore, forelements to be animated the display property needs to be animatable. Supporting browsers animate display with a variation on the discrete animation type. Specifically, the browser will flip between none and another value of display so that the animated content is shown for the entire animation duration. So for example: When animating display from none to block (or another visible display value), the value will flip to block at 0% of the animation duration so it is visible throughout. When animating display from block (or another visible display value) to none, the value will flip to none at 100% of the animation duration so it is visible throughout. Note: When animating using CSS transitions, transition-behavior: allow-discrete needs to be set to enable the above behavior. This behavior is available by default when animating with CSS animations; an equivalent step is not required. Transitioning dialog elements When animating s with CSS transitions, the following features are required: @starting-style at-rule Provides a set of starting values for properties set on thethat you want to transition from every time it is opened. This is needed to avoid unexpected behavior. By default, CSS transitions only occur when a property changes from one value to another on a visible element; they are not triggered on elements' first style updates, or when the display type changes from none to another type. display property Add display to the transitions list so that thewill remain as display: block (or another visible display value set on the dialog's open state) for the duration of the transition, ensuring the other transitions are visible. overlay property Include overlay in the transitions list to ensure the removal of thefrom the top layer is deferred until the transition completes, again ensuring the transition is visible. transition-behavior property Set transition-behavior: allow-discrete on the display and overlay transitions (or on the transition shorthand) to enable discrete transitions on these two properties that are not by default animatable. Here is a quick example to show what this might look like. HTML The HTML contains aelement, plus a button to show the dialog. Additionally, theelement contains another button to close itself. htmlCopy to ClipboardContent here closeShow Modal CSS In the CSS, we include a @starting-style block that defines the transition starting styles for the opacity and transform properties, transition end styles on the dialog[open] state, and default styles on the default dialog state to transition back to once thehas appeared. Note how the 's transition list includes not only these properties, but also the display and overlay properties, each with allow-discrete set on them. We also set a starting style value for the background-color property on the ::backdrop that appears behind thewhen it opens, to provide a nice darkening animation. The dialog[open]::backdrop selector selects only the backdrops ofelements when the dialog is open. cssCopy to Clipboard /* Open state of the dialog */ dialog[open] { opacity: 1; transform: scaleY(1); } /* Closed state of the dialog */ dialog { opacity: 0; transform: scaleY(0); transition: opacity 0.7s ease-out, transform 0.7s ease-out, overlay 0.7s ease-out allow-discrete, display 0.7s ease-out allow-discrete; /* Equivalent to transition: all 0.7s allow-discrete; */ } /* Before-open state */ /* Needs to be after the previous dialog[open] rule to take effect, as the specificity is the same */ @starting-style { dialog[open] { opacity: 0; transform: scaleY(0); } } /* Transition the :backdrop when the dialog modal is promoted to the top layer */ dialog::backdrop { background-color: rgb(0 0 0 / 0%); transition: display 0.7s allow-discrete, overlay 0.7s allow-discrete, background-color 0.7s; /* Equivalent to transition: all 0.7s allow-discrete; */ } dialog[open]::backdrop { background-color: rgb(0 0 0 / 25%); } /* This starting-style rule cannot be nested inside the above selector because the nesting selector cannot represent pseudo-elements. */ @starting-style { dialog[open]::backdrop { background-color: rgb(0 0 0 / 0%); } } JavaScript The JavaScript adds event handlers to the show and close buttons causing them to show and close thewhen they are clicked: jsCopy to Clipboard const dialogElem = document.getElementById(\"dialog\"); const showBtn = document.querySelector(\".show\"); const closeBtn = document.querySelector(\".close\"); showBtn.addEventListener(\"click\", () => { dialogElem.showModal(); }); closeBtn.addEventListener(\"click\", () => { dialogElem.close(); }); Result The code renders as follows: Note: Because s change from display: none to display: block each time they are shown, thetransitions from its @starting-style styles to its dialog[open] styles every time the entry transition occurs. When thecloses, it transitions from its dialog[open] state to the default dialog state. It is possible for the style transition on entry and exit to be different in such cases. See our Demonstration of when starting styles are used example for a proof of this. dialog keyframe animations When animating awith CSS keyframe animations, there are some differences to note from transitions: You don't provide a @starting-style. You include the display value in a keyframe; this will be the display value for the entirety of the animation, or until another non-none display value is encountered. You don't need to explicitly enable discrete animations; there is no equivalent to allow-discrete inside keyframes. You don't need to set overlay inside keyframes either; the display animation handles the animation of thefrom shown to hidden. Let's have a look at an example so you can see what this looks like. HTML First, the HTML contains aelement, plus a button to show the dialog. Additionally, theelement contains another button to close itself. htmlCopy to ClipboardContent here closeShow Modal CSS The CSS defines keyframes to animate between the closed and shown states of the , plus the fade-in animation for the 's backdrop. Theanimations include animating display to make sure the actual visible animation effects remain visible for the whole duration. Note that it wasn't possible to animate the backdrop fade out — the backdrop is immediately removed from the DOM when theis closed, so there is nothing to animate. cssCopy to Clipboard dialog { animation: fade-out 0.7s ease-out; } dialog[open] { animation: fade-in 0.7s ease-out; } dialog[open]::backdrop { animation: backdrop-fade-in 0.7s ease-out forwards; } /* Animation keyframes */ @keyframes fade-in { 0% { opacity: 0; transform: scaleY(0); display: none; } 100% { opacity: 1; transform: scaleY(1); display: block; } } @keyframes fade-out { 0% { opacity: 1; transform: scaleY(1); display: block; } 100% { opacity: 0; transform: scaleY(0); display: none; } } @keyframes backdrop-fade-in { 0% { background-color: rgb(0 0 0 / 0%); } 100% { background-color: rgb(0 0 0 / 25%); } } body, button { font-family: system-ui; } JavaScript Finally, the JavaScript adds event handlers to the buttons to enable showing and closing the : jsCopy to Clipboard const dialogElem = document.getElementById(\"dialog\"); const showBtn = document.querySelector(\".show\"); const closeBtn = document.querySelector(\".close\"); showBtn.addEventListener(\"click\", () => { dialogElem.showModal(); }); closeBtn.addEventListener(\"click\", () => { dialogElem.close(); }); Result The code renders as follows: Technical summary Content categories Flow content, sectioning root Permitted content Flow content Tag omission None, both the starting and ending tag are mandatory. Permitted parents Any element that accepts flow content Implicit ARIA role dialog Permitted ARIA roles alertdialog DOM interface HTMLDialogElement Specifications Specification HTML Standard # the-dialog-element Browser compatibility BCD tables only load in the browser See also HTMLDialogElement interface close event of the HTMLDialogElement interface cancel event of the HTMLDialogElement interface open property of the HTMLDialogElement interface inert global attribute for HTML elements ::backdrop CSS pseudo-element Web forms in the Learn area",
    "commentLink": "https://news.ycombinator.com/item?id=42343089",
    "commentBody": ": The Dialog Element (developer.mozilla.org)295 points by htunnicliff 23 hours agohidepastfavorite136 comments divbzero 12 hours agoOther delightfully interactive HTML elements include… File pickers:Color pickers:Date/time pickers: Numeric sliders:Suggested options for text fields: Summaries with expandable details: FAQsWhy are interactive HTML elements cool? They’re lightweight and semantic! Will the previous answer close when I open this one? Yes, because theelements share the same name.Media players with controls: reply vintermann 8 hours agoparentAll of which these days are going to be unused in favor of some toolkit, because they don't behave predictably enough across browsers / browser versions. reply nikodotio 6 hours agorootparentI use all of them pretty consistently! I try not to have them do magic, but using native date pickers and dropdowns and selects saves so much time and headache - and they work consistently. reply porridgeraisin 5 hours agorootparentHow do you use audio/video consistently across browsers? It looks quite different across the browser/platform combination matrix. reply nikodotio 2 hours agorootparentI accept that they will look different but function consistently and in line with the client device - when that can be made as a design decision. reply eek2121 3 hours agorootparentprevI use them! I just add CSS to pretty them up. reply eek2121 3 hours agorootparent(not everyone is into using some hot new framework, some of use stick with HTML/CSS/JavaScript) reply esrauch 6 hours agorootparentprevExcept file, no one is giving the browser full access to their files so that it can render a shitty file picker reply pests 1 hour agorootparentThe browser already has full access to their files. reply dv_dt 4 hours agorootparentprevDepends if you mean consistent to the os or browser or consistent to the js framework - I would suggest that most casual users don't notice and actually do better if its consistent to the os or browser they use Well some other comment mentioned audio/video and those are seemingly aways different reply wongarsu 3 hours agorootparent> Well some other comment mentioned audio/video and those are seemingly aways different Which is probably fine for your users. In every browser it plays video and has basic controls like a start/pause button, a timeline and a volume control. While the video is playing they even look identical: it's the video, playing; with controls only visible on hover. The differences aren't an issue for the actual user, they are an issue for the designer that tries to have the website look and feel the same everywhere. Preferably look the same as it does on their MacBook. reply ilius2 24 minutes agorootparentSo it's industry's management problem, not a tech problem. reply mikae1 7 hours agoparentprev> Media players with controls: >>Which look different in every single browser and can't be styled without JavaScript. Wish these were implemented better. reply gonzo41 3 hours agoparentprevd is a gem. Love it. reply DaiPlusPlus 23 hours agoprevI started usingin 2019, even though Firefox and Safari wouldn't support it for another couple of years, but Google's own Polyfill (of which I am a very modest contributor) was top-notch quality and so I had no problems using it in production for my LoB SaaS day-job. But my biggest let-down with theelement is that it's comnpletely unstyled, beyond a very basic (and very un-Chrome-like) thick black line pixel border with sharp edges. Whereas my-hope-and-expectation (and indeed: what got me interested inin the first place) was that I was hoping that the browser itself would provide for a lot of the tedium involved in UI dialog dev-work in-general, especially for things like automaticallyt conforming to the host OS' conventions on dialog/window layout and placement: I was hoping that I could mark-up an actual semantic model of a dialog and the browser would do the hard-work of making it look like a real native macOS (or iOS) - or Windows - dialog resource. I was also hoping that, because openelements exist in a distinct top-level layer, that they might even able to escape the bounds of the browser viewport, which would provide real value to the end-user in a lot of places (e.g. no-one wants an unmovable popup or modal-dialog that completely obscures the user's view of an underlying document (like macOS's old \"Sheets\" dialogs) - so another false-hope of mine got popped that day. ----- I get the feeling that browser vendors would all like to see us stop using `alert()`, `prompt()` and `confirm()` in JavaScript (because they block the JS/main thred), but the same browser-vendors really haven't come-up with an adequate replacement: the beeauty of alert/prompt/confirm is that their API is incredibly simple yet effective and also doesn't require the proggrammer to have any UI design-skills; I don't understand why browsers still don't offer a non-blocking Promie-based API for alert/prompt/confirm instead of them trying, in vain, to convince us thatis better in every situastion when it clearly isn't. ] reply zamadatix 22 hours agoparentPremade ways of escaping the bounds of a browser viewport with styling like a system dialog box certainly sounds like something a developer would want rather than users or browser makers. It's not an accidental disappointment new things aren't made to function like alert() and friends used to, it also has upsides (beyond just \"the old interface was not promise based\". I do agreecould have done with at least a little bit of TLC on the styling though, I just don't think it has to be 100% look and function like a system dialog outside the DOM to do it. Some base default styling to match the rest of the browser's default style would do wonders. For PWAs (or any \"web apps with more permissions than a random page should get just for being loaded\") I could see where you wantedto go as a more well received idea though, similar to how there are separate things for styling the windows and interacting with the system for those more privileged pages. reply ivanjermakov 17 hours agoparentprev> is that it's comnpletely unstyled Another reason might be that vendor making it look like a native browser window would blur the line of death[1]. It would make it easier for malicious website to make a popup \"browser update\" in the middle of the page that redirects to seemingly legit Chrome download page and downloads modified executable. [1]: https://textslashplain.com/2017/01/14/the-line-of-death/ reply Sophira 38 minutes agoparentprev> I was also hoping that, because openelements exist in a distinct top-level layer, that they might even able to escape the bounds of the browser viewport, which would provide real value to the end-user in a lot of places As a user, I would absolutely not want this. I appreciate being able to know which windows actually come from my browser and which are coming from a webpage. reply thousand_nights 21 hours agoparentprev> comnpletely unstyled, this is what completely holds back most built-in browser components from widespread usage, i suspect the vendors implementing it just don't care at all because it's not their problem every company i've ever worked at had at least a somewhat consistently defined design language and it would look completely amateurish and out of place to use built in browser components in most places, regardless of how much html/css purists want that to be the case unless that is fixed, it will never happen reply MrJohz 20 hours agorootparentI think you might be talking about a different thing to the other poster. The dialog element behaves exactly how you'd want it to behave for a company with their own design language — you can style every part of it exactly how you need it (including the backdrop, the positioning and size of the dialog element itself, borders, colours, contents, etc). Depending on how you implement the design language, you can either apply those styles to the dialog element in general, or provide a custom component that wraps the dialog element and provides the styles you need. What the previous poster was complaining about was the opposite situation: they aren't working with their own design language, and instead want to use native-looking HTML elements (the default inputs, selects, buttons, etc). However, the dialog element does not provide a native look-and-feel, and instead is very minimalistic, providing only the base that developers can add their own styles to. That said, I think the browser implementors have made the correct choice here. The dialog element is not a native popup, and doesn't behave like a native popup at all, so it doesn't make sense to style it natively. In addition, the more styles they provide as part of the user-agent styling, the harder it is to reset those styles if you want to do something different. Apart from inputs, browsers fairly consistently stick with providing the minimum possible styling for an element to be usable, and this means that developers have the largest range of freedom to use those elements as they like. reply ryandrake 20 hours agorootparentprevI feel like every company's (different/contrasting) \"design language\" and their insistence on using it, ends up being yet another weird looking thing on my computer. I'd rather decide for myself the fonts, color scheme, look and feel, etc. for applications on my computer, and have applications be consistent and respect those preferences. Rather than have some artist I don't know 2,000 miles away from me simply decide what a dialog should look like (and it's always totally different than what some other artist decided a dialog should look like). reply xp84 10 hours agorootparentI think most of us here who aren't self-important \"UX designers\" (or branding consultants) would agree with you, but the decision makers responsible for most of the sites on the Web disagree/don't care what we'd like. They want their site to look identical on all platforms and browsers, and to have their \"signature\" design language, to heck with what users might expect. It's why you see stupid things like pixel-perfect clones of the iOS \"switch\" control brought to the Web. So, anyway, if the `` is ever to have a chance at adoption, instead of the \"div soup and 1000 lines of JS and CSS modal\" we've had everywhere since 2008 or so, it really should be blank slate for the \"UX Designer\" who fancies themself a real artist can vomit their personal brand of \"elegant but bold, minimalist, flat design\" onto the DOM. If it's not completely skinnable, they'll just keep insisting on building div soup modals forever. reply mardifoufs 18 minutes agorootparentIdk, personally I completely disagree. I don't want to theme every single app. I want them to be distinctive and I don't actually care about native look. What I care about is that they have a nice design (which is as subjective as it gets, I know) but more importantly that they are distinctive enough in terms of design. Now I don't want every app to have different shortcuts or whatever, but I don't want slack to look like discord for example. And I don't want to theme anything ideally. reply DaiPlusPlus 20 hours agorootparentprev\"But we don't want to look like everyone else\" reply spatley 19 hours agorootparentand/or making the website using the most modern design trends sets us apart and reinforces that we are a cutting edge company. reply DaiPlusPlus 15 hours agorootparentRight down to the Stablediffusion-generated corporate-memphis artwork reply IgorPartola 28 minutes agorootparentprevThese are the same people who insist on having PDF copies of everything and sometimes PDF forms. Nobody ever designs custom PDF form styling to be different. I wonder why? /s reply chrishtr 20 hours agorootparentprevTheelement is fully styleable, including its backdrop (the MDN article explains how for the latter). In Chrome you can also fully style animations opening and closing aor popover. https://developer.chrome.com/blog/entry-exit-animations reply salzig 17 hours agorootparent@starting-style support is quite good in all browsers, not just chrome. https://caniuse.com/?search=%20%40starting-style reply dylan604 21 hours agorootparentprevthe most commonly used element that I use is the date picker. i hate using it, but i'm not loading some library or framework just for it either. reply 8n4vidtmkvmk 10 hours agorootparentThe native datepicker is weird, at least in Chrome. I was playing around with it, and if you give it milliseconds it will render a millisecond picker in intervals of 100. If you give it a nice round minute, then ms and s disappear from the UI. There's no attribute to control this. I also want a non-local time picker. Let me specify a time that the server can properly interpret as a moment in time. i.e specify an offset somehow. reply thousand_nights 20 hours agorootparentprevi don't know what context you're using it in, but imagine a company like airbnb or booking.com using the built in date picker on their front page you might as well cut their public valuation in half at that point. it's just not worth it to use the completely neglected and anemic components that are part of the browser, they are a joke reply WD-42 20 hours agorootparentWeird. I think the built in date picker is actually pretty nice. reply dylan604 20 hours agorootparentfunctionally it is perfectly fine. aesthetically, it looks nothing like any other component of the site's style. it very much looks like a band-aid reply zamalek 22 hours agoparentprevModals that blocks focus to an entire browser window aren't really a good idea (I'm of the opinion that they are almost always a shitty idea, but that's harder to argue). People have multiple tabs open, and what if another tab contains information that your user needs to complete your dialog. You also have to be incredibly careful about how much visual control you allow over an actual dialog - especially making it look like the host OS. People get bamboozled by shitty in-browser fake virus alerts all the time, now add a real dialog, with real looks, that the user is forced to interact with, and you have a slam-dunk. reply tredre3 22 hours agorootparent> Modals that blocks focus to an entire browser window aren't really a good idea (I'm of the opinion that they are almost always a shitty idea, but that's harder to argue). Good news then, because alert/prompt/confirm do not block the window in any modern browser! In Firefox it only blocks the viewport of the current tab, so it behaves exactly like a DIY modal. In Chromium browsers it does pop over part of the browser UI, but it still doesn't block the window; You interact with the tab bar, address bar, menu, etc. reply DaiPlusPlus 20 hours agorootparent> because alert/prompt/confirm do not block the window in any modern browser! Correct: they don't block the browser's desktop UI thread - but they do block the web-page's thread - and for abvout the past decade we can't move alert/prompt/confirm prompts: Chrome forces them to appear at the very top, dead-centre, and you can't scroll the page while it's open. reply berkes 19 hours agorootparentprevAs can be seen when attaching an image. When you have to look up something in another tab. I now have to first close the file picking modal, before I can use anything in the browser. It might be Gnome/Firefox only, IDK. But this modal thing is very bad UX. The alternative is also bad, TBH: where the file picker is now gone somewhere in the sea of open windows. Maybe the middle-ground, where the file-picker is \"attached\" to the one tab that opened it, and goes away once other tabs and window chrome is engaged, but I guess thats hard to do in a WM? reply quantadev 20 hours agorootparentprevIf you don't think \"Modals\" are needed that just means you've never needed one yourself. There are lots of cases where they're almost mandatory. I have an app where some interactions will end up with 4 to 5 layers of stacked modals. Like you edit a node, then you open the sharing dialog to share it, then you need pick a person to share to, then you need to add a new person, then you need to select who to add, etc. Most websites are trivial and thus don't need dialogs at all but there are some which are full featured apps (like mine) where Modals are a critical thing to have. reply Digit-Al 7 hours agorootparentI spent years designing interfaces for Windows Forms applications. There is no neccessity for sharing to be a dialog - it should be a wizard; and could even open in a new tab in a web application. Then the user can cancel either by pressing the cancel button (which would close the tab) or by just closing the tab. Selecting who to add, or adding a new person then just become pages in the wizard. reply epcoa 18 hours agorootparentprevNo idea why you think any of that should block the entire browser including all other tabs, but that sounds like awful design. Furthermore, editing a node, a sharing interaction and adding a person all sound like they could be handled by modeless dialogs or independent editors. Frankly, modals are typically a hallmark of lazy design. reply quantadev 18 hours agorootparentI also went thru a phase years ago where I also claimed all dialogs should be modeless to free up users to do \"anything at any time\". But the problem with \"anything any time\" is that your state management becomes a nightmare, because you then have to start guarding against astronomically large numbers of ways users can create invalid states, create contradictory settings, or have the content of one dialog become invalid, because of work done in some other dialog etc. Hallmark of good design is when the user is doing one thing at a time, and the dialog flow makes intuitive sense. Often being able to jump back to a prior dialog means you can then start a NEW branch of all the dialogs you've already opened (and that's confusing). Modals simplify not only the code, but the user experience. reply zelphirkalt 9 hours agorootparentWhy would the state management get any more difficult? Any change a user would have input into a modal dialog only gets applied when that dialog is closed/done. Same can be done for any implementation using no modal dialog. You can have a \"save changes\" or whatever button that confirms the changes and only then they affect the state. There seems no inherent reason that it would become more complicated. reply quantadev 1 hour agorootparentThe problem comes about when closing dialogs (i.e. their state change) in random order leads to invalid state. If the user is in a process of doing something where `A` depends on `B` and `B` depends on `C`, etc, for example, you can't just let them say \"I'm done with A now\" (before B and C), when the flow REQUIRES input from B and C in order to be valid. I know people can argue endlessly that no process step should ever REQUIRE input from some other step, but that's the same as saying \"There's no such thing as a multi-step process\" which is obviously an incorrect statement. Like if I'm editing a new CMS record, and adding an attachment/file to it, what if I clicked \"cancel\" on the new record WHILE the upload Dialog is open? Sure you can rationalize your way around that, and think your way out of how that can work with modeless dialogs, but you're just creating lots of unnecessary work for yourself if you do. Modal dialogs have been around literally forever (even before the web) because they're needed. It's a signal to the user that they MUST complete something before moving forward. reply pygar 21 hours agoparentprevThere are some efforts being made on the styling front by a W3C Community Group: https://open-ui.org/ reply simonw 16 hours agoparentprevI've been playing around with the idea of alert() and prompt() and confirm() replacements that work like this: await Prompts.alert(\"This is an alert message!\"); const resultBoolean = await Prompts.confirm(\"Do you want to proceed?\"); const name = await Prompts.prompt(\"What is your name?\"); Demo here: https://tools.simonwillison.net/prompts-js - code written by o1: https://chatgpt.com/share/67539c28-4df0-8006-b021-4f468e011f... reply svieira 1 hour agorootparentWhile they are nice, they don't block the event loop. That's definitely a feature you would need if you're aiming to replace `alert` and friends. As an alternative, yeah, that's a pretty reasonable API. reply jtwaleson 10 hours agorootparentprevFor a company that had a giant 30 minute wizard in the web interface, I wrote a wizard engine in VueJS that works similarly. It's served hundreds of thousands of users since 2019 and went through medical device certification :) Took me quite some time to realize we can use `await` to wait for user input too, not just APIs etc. I recently re-created parts of it from memory for a hobby project and just now open-sourced it: https://github.com/jtwaleson/wizard-engine The neat thing is that we can program the complex logic of the wizard with the full power of the programming language. By making each screen in the wizard a function that has input parameters and a return value, we can treat it like any other function. Show the same screen 3x in a row? Use a for loop. Show a screen with input that depends on the output of the previous step? Just use a variable to store the results. reply zelphirkalt 9 hours agorootparentWhy is it any surprise that one can use await to wait for user input? It is just promises under the hood, right? So that is exactly like one would expect a promise using dialog to work. reply lowercased 1 hour agorootparentI don't think I've ever seen an example of it used that way in any tutorial, blog, or project documentation ever. reply simonw 8 hours agorootparentprevI found that non-obvious too: in both Python and JavaScript I've always seen \"await ...\" as effectively a hack to enable concurrent execution via an event loop, where the hope is that you won't be waiting very long at all for a response. Realizing that you could use it for user input - where your promise might not resolve for minutes or even hours - was a bit of a light bulb moment for me. reply jtwaleson 8 hours agorootparentprevIndeed, just me being stupid for realizing it so late. Coming from Java and Python 2 I never really understood async/await properly and when I started to grasp it, it took me weeks to realize I could apply it to user interactions too. reply DaiPlusPlus 15 hours agorootparentprevSeeing ChatGPT use `return new Promise(...` directly inside an `async function` makes me somewhat less apprehensive about the future. reply acoyfellow 8 hours agoparentprevI built this little tool to hack alert/confirm/prompt into promises. I use it everywhere. Optkit.com reply KTibow 21 hours agoparentprevMost websites have their own style they apply everywhere and would probably appreciate how styleable dialog is. Maybe a way to easily apply/remove default styles could satisfy everyone. reply pwg 16 hours agoparentprev> I was also hoping that, because openelements exist in a distinct top-level layer, that they might even able to escape the bounds of the browser viewport, which would provide real value to the end-user in a lot of places And, within three seconds of release, awith this ability would be misused by advertisers to bring back the old pop-up windows that all browser's block by default now, because of advertiser misuse. reply pmarreck 18 hours agoparentprev> I was hoping that the browser itself would provide for a lot of the tedium involved in UI dialog dev-work in-general, especially for things like automaticallyt conforming to the host OS' conventions on dialog/window layout and placement sadly this only reminds me of bad actors spoofing native dialog UI's to phish passwords and such reply cosmic_cheese 15 hours agoparentprev> the beauty of alert/prompt/confirm is that their API is incredibly simple yet effective and also doesn't require the proggrammer to have any UI design-skills I’ve long hoped for more APIs in the style of alert/prompt/confirm, which are more like ready-made building blocks rather than cement to make cinderblocks with as most web APIs tend to be. Anything that helps cut down on the amount of HTML, CSS, and JS required to be written or imported would be a substantial QoL improvement. This does not seem to be a popular view, unfortunately. reply mikae1 23 hours agoparentprev> But my biggest let-down with theelement is that it's comnpletely unstyled And it can't be styled without JavaScript? That's how it works with . So utterly frustrating. reply fzzzy 23 hours agorootparentIt can be styled with css. Edit: Example: https://gist.github.com/fzzzy/f5f1af66ee8fff478ffb3698ac9f80... reply nitwit005 22 hours agorootparentprevYou can style it normally. They just don't like the default style. reply stevage 20 hours agorootparentYeah I don't get this complaint. So before they had to implement behaviour and styling. Now they just do the styling and get a semantic element too. reply AlienRobot 18 hours agoparentprevThat's so different from my experience. When I first met , I thought I understood its purpose (as a modal) was to block user input from reaching anywhere else on the page. I have no idea why would anyone want to use it non-modally, since you can just use a div for that. Nevertheless, I was also let down by it because it turns out if yourhas a scrollbar, scroll wheel events bubble. There is a CSS property to stop them from bubbling but it doesn't work! reply christophilus 21 hours agoparentprev> was hoping that [the implement wouldn’t suck] Yep. Welcome to the wonderful world of web standards. reply stevage 21 hours agoparentprev> no-one wants an unmovable popup or modal-dialog that completely obscures the user's view of an underlying document Eh, I beg to differ. Lots of use cases for that kind of dialog, for saving, confirming changes, etc etc. reply DaiPlusPlus 20 hours agorootparent> confirming changes ...how can I confirm a set of changes if the popup is blocking my view of said changes? reply rat9988 11 hours agorootparentThen don't confirm them if you aren't sure you wanted to confirm. The dialog is here to alert that you did click on confirm and it seems to me you weren't ready yet, so it did its job. reply zelphirkalt 9 hours agorootparentFamous example of how badly this works is Jira. Want to look something up in another ticket? Bad luck! You need to close that ticket, then open the other one, then memorize or copy the info you need, then close that ticket, then open the original one again. At that point you are better off circumventing the whole shit UI and clone the browser tab. Which ultimately leads to having a dozen browser tabs open that one needs to cleanup later. Add to that the general incredible sluggishness of Jira, and the massive amount of things loaded when you load any of its pages, and you have a recipe for the disaster that thing is. reply _0x168 20 hours agorootparentprevThe popup can summarize the changes. For instance, \"are you sure you want to delete X?\" reply stevage 20 hours agorootparentprevAnd yet, that pattern has worked just fine for decades. reply DaiPlusPlus 19 hours agorootparentOn a Windows or macOS desktop, the OS-provided MessageBox() can be freely moved around the screen - but that's not how in-web-page modals tend to work. reply stevage 17 hours agorootparentI don't find I have the problem you describe, because at worst you can generally abort the save or whatever and verify then redo it. The one that bugs me is online order forms that don't give you all your critical details like dates, and exactly what you are paying for, on one screen where you finally commit. reply pier25 18 hours agoprevRegardless of the implementation I think this was a step in the right direction. There's aproposal in the works which is like aon steroids [1]. Also the Popover API [1] already in browsers for toast alerts. And a popover hint proposal for tooltips[2]. [1] https://open-ui.org/components/combobox.explainer/ [2] https://mdn.github.io/dom-examples/popover-api/ [3] https://open-ui.org/components/popover-hint.research.explain... reply poincaredisk 42 minutes agoprev>Note: While you can toggle between the open and closed states of non-modal dialog boxes by toggling the presence of the open attribute, this approach is not recommended. This perplexes me. Why is it not recommended? Why put it in a standard and then recommend against it? What's wrong with it? The documentation is silent. reply lapcat 23 hours agoprevSee my article \"The HTML dialog element API is a mess\": https://lapcatsoftware.com/articles/2024/2/1.html reply apsurd 22 hours agoparentI don't think anyone would say web-standards are excellent, well designed and well-managed. All the wonderful value is in that they are standard. The web is great in its ubiquity. Love the web! Sure it's a mess, how could it not be?!is a win here in my view, especially for internal admin tools, I don't want to care about the latest flavor of frontend insanity. I just want to save screen space and open content as modal overlay on top of a main view. reply benatkin 22 hours agorootparentIt isn't wonderful though, it's mediocre :) They should err on the side of keeping the APIs small. Maybe it would be better if Web Components weren't a thing. I thought ARIA was handling dialogs well when it comes to accessibility. I like the idea of splitting JavaScript into JSSugar and JS0. (It will be nice if in the Deno lawsuit that the USPTO clarifies that Oracle owns the term JavaScript and JS is a free-for-all.) reply troupo 21 hours agorootparentprev> I don't think anyone would say web-standards are excellent, well designed and well-managed. and yet: > I don't want to care about the latest flavor of frontend insanity. Those not well managed, not well-designed and not-well managed standards somehow get a pass because that insanity is now a part of the browser? Do you know that Chrome argued thatshould be removed from the platform because it's a bad API with multiple issues? https://github.com/whatwg/html/pull/4184#issuecomment-440405... reply magicalist 20 hours agorootparent> Do you know that Chrome argued thatshould be removed from the platform because it's a bad API with multiple issues? That's not at all what the comment you linked to says. It says there was an open question about ifshould be removed from the spec because interest hadn't materialized from Firefox and Webkit for the element after four years (and wouldn't for another two years after that). reply apsurd 19 hours agorootparentprevWhat I'm saying is that the goal of standards and cross-platform support across time is valuable. The \"open web\" is a good thing for humanity. Work towards this goal is a huge shit show. And messy. But it's a good goal. The flavor-of-the-month frontend insanity is not serving this \"good thing for humanity\" goal as I call it. It's all well-intentioned and I'm not looking a gift horse in the mouth. But it is different. reply moffkalast 1 hour agoparentprevI was really hoping the dialog element would end up as a customizable await confirm/prompt on steroids that wouldn't get flagged by every browser if you open it more than once. But it's more of a glorified div. reply benatkin 21 hours agoparentprevThere's more where that came from. Google Chrome also did URLPattern, which I hope Chrome and Safari will hold firm on not supporting. The compression streams API wasn't bad, but it's a tiny API. I see a pattern here: Google sucks at UX and DX. Edit: I looked up standards positions and both support URLPattern. reply lapcat 21 hours agorootparent> There's more where that came from. Google Chrome also did URLPattern, which I hope Chrome and Safari will hold firm on not supporting. Did you mean Firefox and Safari? Anyway, WebKit appears to be adding support for URLPattern. reply benatkin 21 hours agorootparentI did, and looked it up and Firefox chose the support position for it as well. I looked at the API again and I guess it's all right. It doesn't help much with tree-based routing like Roda (https://roda.jeremyevans.net/index.html) but it seems useful for simple stuff. reply AlienRobot 18 hours agoparentprev>Last year the major web browser vendors decided to change the standard (remember, HTML is now a \"living standard\") SIGH... To be honest, I think it's unlikely you will ever try to open the same dialog modally twice. If you need user input to open the dialog, and a modal dialog blocks user input, the only way for this to happen is if you have an input on the dialog that opens the dialog. If you are using asynchronous tasks to open the dialog, then you probably should keep track of what is open and what is not. The same thing would happen if you were doing it on Qt, for example. reply lapcat 17 hours agorootparent> To be honest, I think it's unlikely you will ever try to open the same dialog modally twice. This is a strange reaction to the blog post, because as you read, it's a certainty that my code did try to open the same dialog modally twice. This was a bug, of course, which would have been revealed in development if not for the change in the standard. reply somishere 15 hours agoprevI love theelement, especially for its built-in / standardised accessibility considerations. I'm looking forward to the day I can roll it without a polyfill (once safariwith a popover=\"\" attribute. They're easily targeted with a button and no clientside JS required, and can be closed by clicking outside them, which s don't have by default. They also still have the same helpful top-layer properties. Unsure about accessibility on this front, though it can't be worse than my previous attempts with hidden labels/checkboxes/form elements, while being much simpler and less hacky. reply somishere 5 hours agorootparentRe. accessibility, the main feature is that dialogs can be \"modal\" - meaning the rest of the page is non-interactive while the dialog is open - ensuring a focus trap. Popovers are always non-modal, which can be problematic depending on the use-case. Of course it's possible to manually implement a focus trap, but it's complex and it's javascript. reply extra88 57 minutes agorootparentYes, I wish when the popover attributes were created there was something included for opening a modal dialog. I think it is possible to use popover to open a dialog non-modally and use progressive enhancement to replace that behavior with `.showModal()`. reply pkphilip 14 hours agoprevI wish HTML supported a concept of a \"\" tag which will allow multiple pages to be defined in a single HTML file and which can be shown one at a time but without the look-and-feel of a dialog. Each PAGE should be able to pull in common sections from the same page such as the header, sidebar, footer etc based on specific states selectable in each PAGE. Yes, you can do the same thing with the current approaches by hiding and showing divs etc.. But if it were possible to support these approaches via specific tags in HTML it may help. EXAMPLE:.. common javascript elements here... ... ... ..... ..........reply 8n4vidtmkvmk 10 hours agoparentI think there have been some improvements to print stylesheets recently that let you do just that for printing, but not for the screen AFAIK. reply andypants 22 hours agoprevTried this today and came across an issue that I could not get around: if the dialog contains a form, then submitting the form with enter (focused on any input) or space (focused on the submit button) will close the dialog. I couldn't find any nice way of preventing it. Normally a form will reload the page anyways so I guess this isn't a normal problem but I was using htmx. reply apsurd 21 hours agoparentYour last sentence is likely right, by default the form issues a network request. I've been using a dialog form to update an iframe (it's an editor) so it does work as normal the target iframe gets reloaded. It does not close the dialog though. I can't produce the case where hitting enter closes the dialog. It should be the same as `submit` which also does not close the dialog. FWIW I learned yesterday that a button _can_ close the dialog: Update & Close reply Andrex 21 hours agoparentprev> Tried this today and came across an issue that I could not get around: if the dialog contains a form, then submitting the form with enter (focused on any input) or space (focused on the submit button) will close the dialog. I couldn't find any nice way of preventing it. preventDefault and stopPropagation? reply andypants 8 hours agorootparentThere's no event for the dialog about to close, only an event for after the dialog closes. You can prevent default on the enter key and space key, but that obviously breaks the form ux. There is an event for the dialog about to close from pressing the escape key. No idea why it's only for closing via escape key. reply extra88 53 minutes agorootparentI think they meant prevent the form submission on the `submit` event and use JavaScript to make the submission. https://developer.mozilla.org/en-US/docs/Learn/Forms/Sending... reply AlienRobot 18 hours agoparentprevMaybe you should file a bug on HTMX. reply Sateeshm 23 hours agoprevNormal close in most examples not working for me. Android Firefox reply Aachen 6 hours agoparentFor me it's the autofocus property on the close button that didn't work yesterday when I was using this in a project. Ended up adding a line to whenever I call show() like $('#thatModal *[autofocus]').focus() MDN says this should work as intended out of the box reply codethief 19 hours agoparentprevCould you elaborate on what you mean by \"normal close in most examples\"? All the examples I see come with a JS snippet to add an event listener to the close button and those work for me in Firefox for Android. reply extra88 51 minutes agorootparentAs the MDN article documents, you don't necessarily have to use JavaScript: ```HTMLClose``` reply itishappy 22 hours agoparentprevSame here. Chrome, Windows 10. Only the listeners added via JS seem to work correctly. reply silverwind 18 hours agoprevThe worst thing about `` is that modal ones are placed in the CSS \"top layer\" which obstructs portalled content like tooltips and dropdowns. reply plonq 18 hours agoparentOnce we can use popover and anchor positioning, tooltips and drop downs can also be in the top layer. I can’t wait reply 8n4vidtmkvmk 10 hours agorootparentThis was never an issue for me in React. The top layer seems unnecessary. Just put your portal or fancy code such that it puts all those popovers at the end. If you order your DOM correctly, you almost never have to touch the z-index. reply replete 19 hours agoprevI was unfortunately looking for a cookie consent manager for a new build I'd just optimized, and didn't like that the open source options were 100KB+ so I made my own [0] and relied onto support my goal of writing it as small as possible. With a couple of CSS rules, it works natively without styles. I also ended up writing some build tools to compile all the way down to IE11 and some really ancient browser versions. Dialog works well for the most part, with a couple of CSS kludges here and there for the older browsers but otherwise straightforward to deal with. It's a decent addition to the web platform, but after 20 years of this I would like to stop making custom multi select controls every couple of years. Native controls good. [0]: https://github.com/replete/biscuitman reply replete 7 hours agoparentSurprised by the downvotes to be honest, it's relevant. It's okay, I hate cookie popups also. reply skrebbel 1 hour agorootparentfwiw i'm surprised by the downvotes too (and upvoted it to compensate). that said, complaining about downvotes is usually frowned upon here. reply srathi 22 hours agoprevUblock origin is not able to filter outelements without breaking scrolling and other buttons in most cases (depending on how the site is implemented). Is there a generic way to disable these without affecting the rest of the site? reply Aachen 6 hours agoparentDon't have a computer handy to try, but it can block elements right? What if you block simply dialog (without # or so, because it's not an ID but a tag name) on the desired domain? Or a little userscript walking the DOM to find dialog elements and removes them. Or you could check if one can listen on them for show/open events, or hijack the show() method to do nothing Lots of options I'd say Not something I'd have in mind when using dialogs for legitimate purposes as a developer though. Don't wonder why you don't get my \"do you really want to delete this item?\" on my software when you block these altogether, but I guess you mean this for specific domains where it's used for ads (have yet to see dialog used for that, but no surprise that they'd exist) reply quantadev 19 hours agoprevI'm really glad you posted this 'htunnicliff', because I'm manually maintaining a 'stack' of Dialog elements, along with an associated full-bleed 'background' overlay to block mouse clicks outside the dialog, and it will be really nice if I can get this (dialog element) to work and replace my code with something simpler. I'm assuming dialog elements can overlay other dialog elements!! I'll try! reply akira2501 19 hours agoparentSo a few years ago I went into my chrome configuration and added a new custom \"Site Search\". It's shortcut is \"mdn\" and the URL is \"https://developer.mozilla.org/en-US/search?q=%s&w=3&qs=plugi...\" Now you can just \"mdn \" in your omnibar and it will take you straight to the subject page or to a search results page. I find this shortcut invaluable when developing for the web. reply quantadev 19 hours agorootparentvery helpful link, thanks! I don't know what ominibar is (maybe Mac stuff), but I'm on Linux, but I replaced the \"q=%s\" with my string and it works. nice! reply skrebbel 1 hour agorootparent\"omnibar\" is just chrome's slightly pretentious name for the address bar. reply akira2501 19 hours agorootparentprevhttps://zapier.com/blog/add-search-engine-to-chrome/ reply quantadev 18 hours agorootparentcool, thanks. reply htunnicliff 15 hours agoparentprevThanks! I used it today and thought others might enjoy. reply joshdavham 21 hours agoprevI still remember being confused about the differences between dialogs, popovers, and modal vs non-modal elements when I was first learning web development. reply tanepiper 8 hours agoprevThe dialog tag is not a good idea - it can still be used to completely force a user into clicking and forcing an action they can't control https://tane.dev/2021/02/revisiting-dark-patterns-with-the-h... reply skrebbel 1 hour agoparentI tried the stackblitz demo you linked to and I don't get it, why is \"this element lets people author terrible user-hostile webpages\" an indictment of the element? I mean you can make a user-hostile page with divs too. People won't be prevented from eg closing the tab, right? reply palsecam 22 hours agoprevI got an issue recently with howinteracts with AdSense “vignette” (interstitial) ads. Vignettes set their `z-index` CSS property to the max (2147483647), but ais still higher on the z-plane (with no way to adjust that). So if you click a link from a , and an interstitial gets displayed, it’s under the . It looks like nothing happened, that clicking is broken. Fix in my case was to close() theonclick. reply silverwind 18 hours agoparentUse `.show()` instead of `.showModal()` and create your own backdrop. reply zachrip 22 hours agoparentprevThis is a feature reply palsecam 22 hours agorootparentYes, but that can be surprising. That you cannot adjust the z-index is disturbing, especially given that you have to styleyourself if you wanna make it look any good (as others have reported here). Also, for the record, alert() is still higher thanon the plane (see https://i.imgur.com/tzKOEF2.png). reply Sesse__ 21 hours agorootparentFWIW, it's on a separate layer that's not affected by z-index: https://developer.mozilla.org/en-US/docs/Glossary/Top_layer reply troupo 21 hours agoprevAbout two years year beforewas rushed into all browsers, it had been implemented only in Chrome, and Chrome devs suggested removal ofcompletely. Reason? No consensus on multiple issues relating to accessibility and security: https://github.com/whatwg/html/pull/4184#issuecomment-440405... And then boom! It was shipped everywhere with none of the issues discussed or fixed. Why? My tiny conspiracy theory is because browsers are hellbent on removing \"legacy\" APIs like confirm/prompt, and Chrome tried to remove it about half-a-year to a year beforewas suddenly shipped everywhere: https://dev.to/richharris/stay-alert-d reply chrishtr 20 hours agoparentThe issues were fixed actually. https://github.com/whatwg/html/wiki/dialog--initial-focus,-a... https://github.com/whatwg/html/pull/8199 reply troupo 19 hours agorootparentFixed after dialog was rapidly pushed into production across all browsers. - Argued that dialog should be removed: 2018 - Tried to force-remove confirm/prompt: mid-2021 - Dialog rushed into all browsers: March 8-14, 2022. - The linked proposal for a fix: March 04, 2022 - Request for position on standard: Jan 18, 2023 - The proposal merged into the standard: Jan 26, 2023 - Implemented in browsers: ?? (Webkit is possibly July 2024: https://bugs.webkit.org/show_bug.cgi?id=250795) \"Actually\" Dialog was literally rushed into all browsers without bothering to fix the issues that plagued it for a decade. Some of them were fixed post-factum because now you couldn't ignore these issues. reply ulrischa 23 hours agoprev [–] Why is this on the front page? It is nothing special reply palsecam 22 hours agoparentIt doesn’t have to be brand new to be on the front page. Good ol’ or reference content is suitable too. For instance, see this recent thread https://news.ycombinator.com/item?id=41967897 about the “418 I’m a teapot” HTTP status code, also linking to MDN. It was nothing new neither. reply rock_artist 23 hours agoparentprevI guess because the complete standardization is new. The page says: > This page was last modified on Oct 29, 2024 reply cantSpellSober 21 hours agorootparentWhich seems to simply addto the docs for the `cancel` event https://github.com/mdn/content/commit/d2421d25d1676cc11b01cc... https://developer.mozilla.org/en-US/docs/Web/API/HTMLDialogE... ( has been around for a decade) reply ulrischa 10 hours agorootparentThat's what I mean reply Mystery-Machine 23 hours agoparentprevNo idea. If it was the new `popover` attribute, I'd understand. https://developer.mozilla.org/en-US/docs/Web/HTML/Global_att... reply stevage 20 hours agoparentprev [–] And yet it's getting plenty of discussion. I like items like that. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The HTML `` element is used to create dialog boxes, which can be modal (pausing page interaction) or non-modal (allowing interaction).",
      "JavaScript methods `.showModal()` and `.show()` are used to display modal and non-modal dialogs, respectively, while `.close()` is used to close them.",
      "Accessibility is crucial; ensure focus is set correctly and provide a closing mechanism, such as a button, while using CSS `::backdrop` for styling modal backdrops."
    ],
    "commentSummary": [
      "The dialog element on developer.mozilla.org has initiated a debate about the use of interactive HTML elements, such as file pickers and media players, with differing opinions on their consistency and predictability across browsers.",
      "While some developers criticize the dialog element for being unstyled and not functioning like native popups, others value its semantic and accessibility benefits.",
      "The discussion highlights challenges in maintaining design consistency and concerns about potential misuse by advertisers, indicating that the dialog element is a progressive step but still requires enhancements."
    ],
    "points": 295,
    "commentCount": 136,
    "retryCount": 0,
    "time": 1733512147
  },
  {
    "id": 42343121,
    "title": "Maps.me co-founder tries to close down Organic Maps open-source fork",
    "originLink": "https://github.com/orgs/organicmaps/discussions/9837",
    "originBody": "Organic Maps Overview Repositories Discussions Projects Packages People Disclosure of the Metaserver CDN Source Code #9837 rtsisyk announced in Announcements Disclosure of the Metaserver CDN Source Code #9837 rtsisyk · 7 comments · 7 replies Return to top rtsisyk Collaborator Today I am announcing the public disclose of the source code of so-called \"metaserver\", that previously was available only to few people of the team. At Organic Maps, we prioritize transparency and open communication with our community and committed to making our source code publicly available, ensuring that users can fully engage with and contribute to the project. => https://github.com/organicmaps/meta (MIT License) What is the Metaserver? The purpose of so-called \"metaserver\" is to dynamically redirect users to the most suitable CDN servers containing requested map version based on their geographic location. For example, users in North America are routed to servers in the U.S., ensuring faster map downloads. Additionally, the metaserver enables dynamic server reconfiguration without requiring approval from the App Store or Google Play, which can take weeks. How does it work? The service is implemented as a Cloudflare Worker, which returns a JSON response containing a list of servers based on the GeoIP information of the requester. Essentially, this functionality could be replaced with a few static JSON files a series of rules in an Nginx configuration. The decision to use Cloudflare Workers seems controversial at this point. It was likely an unfortunate choice that should be revisited in the near future to prevent vendor lock-in and ensure the project remains independent of large tech corporations. Why Was It Closed? The original author of this component justified the decision to close it due to concerns about unauthorized forks. Indeed, we observed at least two forks that misused the content delivery network, funded by donations, for their private gain. However, it appears that the closed-source nature of the component did little to prevent this issue. Why Open Now? The component was contributed by Alexandr Borsuk (a.k.a. Alexander Zolotorev) back in 2021 under the MIT license, although the repository itself was not publicly accessible. Throughout November 2024, the issue of closed-source code was discussed several times in the Telegram chats with key contributors. On November 23, Mr. Borsuk quietly made a change, removing the MIT license while keeping his copyright. The change with the flagrant comment \"No MIT yet, sorry.\" was discovered by me only today. A few non-significant commits were made on top of it. This subtle, almost unnoticed modification effectively privatized the open-source repository by this individual, preventing any further open-source collaboration. Furthermore, the next change of enabling the logs, clearly violates our commitment to privacy. To my knowledge, this decision was not discussed with any other contributors, including those who had previously contributed to the repository. commit 30e9911d4c8329068aca82fd6c0d896380ba99de Author: Alexander BorsukDate: Sat Nov 23 21:33:36 2024 +0100 Update LICENSE No MIT yet, sorry. Signed-off-by: Alexander Borsuklink: https://github.com/organicmaps/meta/commit/30e9911d4c8329068aca82fd6c0d896380ba99de The commit following the license change enabled logging in Cloudflare... commit a6ff0eb05abfc891e6a3a32faa3cd307a40c6121 Author: Alexander BorsukDate: Sun Nov 24 21:28:02 2024 +0100 Observe server abusers when needed Signed-off-by: Alexander Borsukdiff --git a/wrangler.toml b/wrangler.toml index bfcdcf6..bad7b47 100644 --- a/wrangler.toml +++ b/wrangler.toml @@ -23,3 +23,6 @@ route = 'meta.omaps.app/*' [env.prod.vars] DEBUG = false + +[observability.logs] +enabled = true link: https://github.com/organicmaps/meta/commit/a6ff0eb05abfc891e6a3a32faa3cd307a40c6121 Actions Taken I am making the code from before November 23, 2024, publicly available again under MIT. As one of the authors who contributed to the code while it was under the MIT license, I have the full right to take this action. Proprietary changes after \"No MIT yet, sorry\" and \"Observe server abusers when needed\" has been removed or reverted. The copyright notice has been updated to include \"Copyright 2024 Organic Maps Contributors\" to accurately reflect the current situation. Contributors and the community are invited to perform a thorough and independent review to verify that the code functions as expected without introducing any undocumented functionality. This post issues an open call to replace the proprietary Cloudflare technology with an open-source alternative, though this may take some time. 6.. Log collection has been disabled, as it was previously. I, personally, apologize to the community for this matter. All necessary measures have been taken to resolve this issue. Organic Maps remains fully committed to privacy, transparency, and open-source values, which is why we are openly disclosing this issue. Regards, Roman. 23 49 12 3 Replies: 7 comments · 7 replies Oldest Newest Top RicoElectrico Collaborator Something doesn't seem right? The two Alexanders are apparently a different person? https://tracxn.com/d/companies/mapswithme/__z_UdJbmARcKZWF2k8I8I6LhezhZ-PPLegSR8HKLL0AA/founders-and-board-of-directors 2 1 reply rtsisyk Collaborator Author Something doesn't seem right? The two Alexanders are apparently a different person? https://tracxn.com/d/companies/mapswithme/__z_UdJbmARcKZWF2k8I8I6LhezhZ-PPLegSR8HKLL0AA/founders-and-board-of-directors The same person as far as I know: https://github.com/organicmaps/organicmaps/pull/515/files#diff-ade7d2f0db76f7dc0eef2e96386859ee766b144d13448e2e177a049ffbbdc02aL3-L9 1 rtsisyk Collaborator Author Apparently F-Droid's NonFreeNet was somehow legit. This change opens the way to remove that label. 2 3 replies pastk Maintainer It had been changed awhile ago to a more precise TetheredNet AF already. TetheredNet doesn't imply the server is non-free, just non-changeable by a user. 1 pastk Maintainer This is how it looks in the F-Droid Client at the moment 4 rtsisyk Collaborator Author It had been changed awhile ago to a more precise TetheredNet AF already. TetheredNet doesn't imply the server is non-free, just non-changeable by a user. OK. At least this formerly closed component is not a concern anymore. 4 pastk Maintainer Many thanks! There should be no hidden closed parts in an open project! Especially in a privacy-focused project. 2 12 0 replies rtsisyk Collaborator Author I can see a butch of emails that my GitHub permissions have been revoked. Well, this is a funny attempt to cover-up the published information. 6 7 1 reply gibsonpil Ooooooo you're in troouuuuuuuble! 1 rtsisyk Collaborator Author Nice to know that we have the OWNER here who OWNS the CODE. 4 0 replies rtsisyk Collaborator Author The https://github.com/organicmaps/meta repo is not available anymore as of 7pm GMT... 5 2 0 replies rtsisyk Collaborator Author Here's my local copy... Open-source forever! ✊ organicmaps-meta.tar.gz 34 3 38 2 replies pastk Maintainer I have a clone as well: https://github.com/pastk/meta 1 edited pastk Maintainer But I think the biggest issue is not in the availability of the code somewhere per se. The code/repo should be \"officially\" open in the project. Otherwise it casts a shadow onto the image of open source, community-driven and privacy-focused project. 10 Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Category Announcements Labels None yet 4 participants",
    "commentLink": "https://news.ycombinator.com/item?id=42343121",
    "commentBody": "Maps.me co-founder tries to close down Organic Maps open-source fork (github.com/orgs)269 points by 0x457 23 hours agohidepastfavorite62 comments ikesau 23 hours agoDo I have this right? Organic Maps had a private (but MIT-licensed) Cloudflare Worker in a repo called meta. Alexander Borsuk (an Organic Maps maintainer, and co-founder of maps.me) surreptitiously removed the license and added logging to the worker. Roman Tsisyk, another maintainer of Organic Maps, noticed this. He undid the changes, made this post, and has now been booted from the organization? reply pastk 20 hours agoparentSome facts: Alexander Borsuk, Roman Tsisyk and Viktor Havaka are co-founders of Organic Maps which is a fork of Maps.Me. Alexander and Viktor were co-founders of Maps.Me as well. All of them are not working for Maps.Me anymore and Organic Maps (OM) is independent project since its inception. Now, the current situation: OM's map server (CF worker) albeit under MIT license was de-facto closed source all this time. Roman opened the repo for public access. Alexander revoked Roman's GH permissions and closed the repo again. (I've been actively contributing to OM for 3 years and I thought that all parts are open source. Until very recently..) reply fsckboy 18 hours agorootparent>OM's map server (CF worker) albeit under MIT license was de-facto closed source all this time. Roman opened the repo for public access. Alexander revoked Roman's GH permissions and closed the repo again. I'm not sure if there is some distinction between software and map-data entailed in the discussion of this \"server\", \"software repo\", etc. but assuming it's all one thing: if the content in question was MIT-licensed as specifed by the license in the repo, any one of the members of the project with access to the material would be within their rights to make copies public. There is no de-facto closed source wrt open licenses. reply Brian_K_White 17 hours agorootparentThere is de-facto anything wrt anything. All it means is \"as good as\" or \"might as well be\" or \"no different from\" or \"in effect\" etc. If no one is publishing a copy of something with an open license, then that is the definition of de-facto closed. de-facto means what is the reality vs what is the theory. In theory you can get a copy because it has a license that says so. In reality you can not get a copy because you are not one of the people with physical access to some existing copy. It is de-facto closed while that set of facts is true. reply Vinnl 10 hours agorootparentSo am I understanding correctly that there was a private repo that had original code, and in that private repo, someone added the MIT licence? Which didn't make the code open source, because that licence hadn't actually been given to anyone*? And then Roman made the repo public, effectively distributing the licence to people and thus making it open source, without coordinating with the other contributors? I don't see any replies by Alexandr, but it feels like this could have easily been resolved with a less antagonistic response by Roman - but of course, I don't know what other history they have. * I guess technically, it was given to the other contributors with access to the private repo - i.e. Roman did have the legal right to distribute it further under the MIT licence. Presumably, the original contributor (Alexandr) just applied that licence by mistake. reply pastk 8 hours agorootparentThe MIT license was added by Alexander himself in the initial commit in June 2021. Since then Roman has contributed actively to the code and Viktor started to contribute more recently. So there are people who contributed the code under the MIT license, so its not a sole work of Alexander and the license was not added as a mistake. IMO from a legal standpoint Roman is absolutely in his right to redistribute the code. reply Vinnl 2 hours agorootparentYes, as I mention in my footnote, Roman was absolutely legally in the clear. I'm just saying that Alexandr's initial adding of the licence was probably a mistake in the sense that he did not intend to do that (that led to people contributing under different assumptions), and one they probably could have found a way to resolve had he communicated about the difference in assumptions. reply escape_goat 16 hours agorootparentprevThis sounds like it is de-facto true but it isn't. A private file repository with a closed source license is in-facto different than a private file repository with an open source license. The license is not altered by the access permissions of the repository, and the access permissions of the file repository are not altered by the license. A compiled version of code in a private repository with a closed source license can be released publicly without that code. The same is not true if the private repository contains code with an open source license. reply Brian_K_White 15 hours agorootparentI don't know how you can still be so confused about what de-facto means. But you are. reply escape_goat 4 hours agorootparentI'm not confused, I'm pointing out that the license in a private source repository changes the legal rights of the person with respect to distribution of the software defined by the contents of the repository, and than therefore a private repository with open source code inside is not a de facto closed source repository. reply tga_d 1 hour agorootparentIn your view, what would be an example of code that is de jure open source but de facto closed, if not a case where people have the legal ability but not the technical means to access it? reply fsckboy 14 hours agorootparentprevthe definitions of closed source and open source that you are using are not de facto correctamundo. If I download a copy of your copyright noticed and unlicensed closed source software from your server, no matter how I got access, it's still closed source and I am not entitled to even the copy I have. de facto, and ipso facto: in fact-o reply zelphirkalt 7 hours agorootparentprevAnother example of a pushover license failing the purpose a developer wants it to serve. Should have used some copyleft license, that mandates sharing of modifications. The one that comes to mind is of course AGPL, which would have avoided the whole scenario of it being closed and hidden in the first place. Maybe someone will learn a lesson now. reply itake 22 hours agoparentprev\"from the organization\"? which organization? The https://github.com/organicmaps/meta 404s for me now. I still see Roman listed as a member of organicmaps: https://github.com/orgs/organicmaps/people I guess he was in both maps.me and organic maps? Which seems odd.. I thought these were completely separate organizations with different priorities and goals. That seems odd they would have leaders contributing in both products. reply andrewshadura 19 hours agorootparentThey founded MapsWithMe, then they sold it to mail.ru who renamed it maps.me, then open-sourced and eventually sold it to someone else. reply biodranik 18 hours agoparentprevNo, you got it wrong. Roman supported the private repo and was aware of the temporary (last 3 days only) CF logs to address CDN abuse. However, several hours ago, he (or someone else using his account?) unexpectedly made the repository public without discussing it with the project's maintainers. As a result, his account rights were temporarily restricted to clarify the situation. There is still no response from Roman regarding his motivation for ignoring the usual governing board rules. Previously, all similar important project decisions were always discussed with maintainers/active contributors before being executed. I hope that we resolve this strange situation successfully soon. reply rtsisyk 10 hours agorootparentRemoving the MIT license from the repository and claiming it as 'my code' is not how open source works. reply Vinnl 10 hours agorootparentIt's sound like the person who removed the licence also originally wrote the code, and just didn't intend to add the MIT licence to it? reply pastk 8 hours agorootparentNope, Roman has actively contributed to this MIT-licensed code since its inception in 2021. reply Vinnl 2 hours agorootparentYeah I mean, of course technically that's not how it's supposed to be done, but if they initially added the code and the licence (the latter by mistake), then I can see how the internal narrative is \"here's my code (that Roman has contributed to) and I accidentally added the licence to it - oops, let me remove that before we accidentally make it public\". Of course at that point they should have realised that they weren't the only author of the code any more and that Roman understandably would have the wrong idea. But I see how it's an easy mistake to make, and it would probably also have easily been resolved had Roman reached out about it, rather than just instantly making it public and implying nefarious behaviour (\"quietly made a change...discovered by me\"). reply viccuad 8 hours agorootparentprevThough luck. Be more careful next time. That's how licences work (not only open source ones, or software ones). reply cmeacham98 15 hours agorootparentprevAs you seem to be Alexander Borsuk... Why the removal of the MIT license? Was that decision put to a vote like \"all important project decisions\" are? I assume it can't have been unless Roman is blatantly lying about only noticing it a few days later. reply pastk 8 hours agorootparentprevHow come the server code was (is?) de-facto closed source and this fact was kept hidden? In a project that claims to be open-source, privacy-focused and community-driven. reply bmicraft 12 hours agoparentprevIt looks like that code was never actually MIT licenced but had the licence-file in there by mistake. Then Roman made the repo public after the faulty licence got removed, because he (wrongly) believed the code actually was MIT-licenced before the change. That's how I read it. Is it true? No idea but it might be. reply esperent 9 hours agorootparentIf the code is in a repo that has been shared with other people and with a license file saying \"All code in this repo is under MIT license\", then the code is MIT licensed. You can't come along later and say \"I changed my mind\" or \"I made a mistake\". Hard luck, be more careful next time. The only exception would be if you ask the people whom it was shared with if they're ok with retroactively changing the license. reply bmicraft 9 hours agorootparentIt's not like anybody except for the core team had had access before. reply pastk 8 hours agorootparentprevThe MIT license was added by Alexander himself in the initial commit in June 2021. Since then Roman has contributed actively to the code and Viktor started to contribute more recently too. So there are people who contributed the code under the MIT license, so its not a sole work of Alexander and the license was not added as a mistake. reply Vegenoid 20 hours agoparentprev> has now been booted from the organization To clarify, Roman claims that he has had some permissions revoked from the Organic Maps organization on Github in response to this. Alexander Borsuk stated that he \"will return access if we solve the problem of Roma making decisions alone\". reply rtsisyk 10 hours agoparentprev> Roman noticed this. He undid the changes, made this post, and has now been booted from the organization? Correct. reply resoluteteeth 21 hours agoparentprevI don't understand this at all. Can someone explain how/why this conflict is happening within organic maps? reply LikeAnElephant 22 hours agoparentprevThat's my understanding as well. Definitely will follow this, I was really disappointed when Maps.Me started getting enshittified. Hope it doesn't happen to Organic Maps as well. reply vr46 23 hours agoprevOpportunity to give Organic Maps a shout for being brilliant and wonderful. reply benatkin 22 hours agoparentIndeed. It should be the default to have a full map of your area downloaded. But no, Google wants everyone to be dependent upon them on a second-to-second level. (Yes I know Google has offline maps and unlike YouTube the offline support is free as in beer, but it’s in an untransparent and a much less powerful manner, and if you’re online it will use the network heavily) reply vr46 22 hours agorootparentI was in a chunk of the world where roaming was prohibitive this summer, and Organic Maps took us everywhere, found us incredible viewing spots to photograph from, and basically gave us a real connection to the landscape around us. The UI was slightly confusing, but forgivable, and if I had any money I would give it to them. reply nathancahill 21 hours agorootparent> found us incredible viewing spots to photograph from Curious to hear more about this? Something specific to Organic Maps? I spend a lot of time studying maps when I'm on a trip for this purpose. reply vr46 21 hours agorootparentJust easy to see on the map, I guess - look for the binoculars! reply Fauntleroy 20 hours agorootparentprevDepending on where you are, Organic Maps can have far far finer detail, especially when it comes to walking paths. I used this a couple times in Japan and Korea to find my way through densely packed shrine complexes, or over less-traveled hiking paths in the mountains. reply pedrogpimenta 20 hours agorootparentI once saw \"sneaky entrance\" near a temple/park in Bali and we got in for free :) reply dTal 4 hours agorootparentNext time don't blab! reply andrewshadura 8 hours agorootparentprevThanks for reporting it. I’ve removed it, since personal notes do not belong in the map data. reply andrewshadura 19 hours agorootparentprevIt's all data from OpenStreetMap, you know :) reply benatkin 19 hours agorootparentIndeed. And it's a social thing, which makes OrganicMaps a SoLoMo app :) reply throwaway81523 14 hours agorootparentprevI thought the map data was OSM, and organic maps is navigation software that uses the map data. reply dade_ 20 hours agoparentprevAnd it is accurate in the Caribbean, unlike Google Maps that insists on sending me over cliffs to my death, people’s back yards, non existent or dead end roads, as well as other areas best avoided. reply david-gpu 19 hours agorootparentYou have OpenStreetMap to thank for that. It is a Wikipedia-like crowd sourced map that anybody can contribute to. The immense majority of navigation apps use OpenStreetMap data, including Strava, RWGPS, OsmAnd, and OrganicMaps. reply NelsonMinar 19 hours agoprevUgh, maps.me. That used to be the nice free OSM map viewer until the company went in a sleazy direction. And, among other things, creating a cryptocurrency token. Because that's what you want in your free community map viewer. So now Organic Maps is the nice free OSM map viewer. I can't tell from the rest of the discussion: is still OK? Or has it been corrupted somehow? reply Vinnl 10 hours agoparentSounds like there was just some miscommunication, and as far as I see, the \"added logs\" were to detect CDN abuse, so nothing shady so far. Still, would be great to see contributors aligned :) reply greatgib 7 hours agoprevEven if the repository was closed to the world, it was open to some individuals. Some individuals got the work under the MIT license that allows them to do whatever they want, including opening it to the world. What is not right is that once it was MIT with contribution from multiple authors, it is theoretically not possible (even for the original author) to change the license to something else than MIT without the agreement of the other authors. Things would have been different if it was the private repo of a company only used by employees of this company, and having contributors bound by a special contract like a NDA. reply yencabulator 18 hours agoprevThe Cloudflare Worker in question sounds darn trivial, just redirecting to a regional server. In fact, Cloudflare or Fly.io would probably do this with just their built-in DNS, automatically. This is not a fork of Organic Maps. Makes it all sound very petty. reply JPLeRouzic 5 hours agoprevIsn't the title wrong? What I understand is that the fork is about a Cloudflare Worker used to connect the user to the closest DNS. The title IMO suggests the fork is about Organic Maps itself. reply lucideer 19 hours agoprevThere's two opposing perspectives to consider here: 1. We don't know the details of the internal politics between the approved maintainers of this private repo, but, licenses aside if someone decided to make a private repo I created public without consulting me I would be pissed. I would consider that at best rude & at worst misconduct. 2. It seems evident that the original creator of this repo was extracting labour from other volunteers(?) under the facade of it being an open source project. Attempting to undo that retroactively is drifting toward fraud & at best heavily outweighs the severity of anything in (1) above. Really hope OrganicMaps survives this, absolutely beautiful, invaluable app. reply fnfjfk 22 hours agoprevI’ve used this app as an offline maps tool for travel for a long time, when I got it I thought it was just a fine OSM client. Then years later I hear it was running crypto nonsense (what) and was involved in FTX (what), but out of laziness I’m still using the thing because it can’t do any upsells when I am offline anyways. Any recommendations for a good iOS offline map app that’s just a map app? reply itake 22 hours agoparentNot exactly sure what you mean. My understanding is Maps.me started trying to extract value from users (via the mentioned means) and that Organic Maps [0], took an old open-licensed fork of maps.me from before Maps.me went bad. I currently use organicmaps.app as a nice offline back up for my traveling. I don't know how this current drama impacts organicmaps. I didn't realize that maps.me was also a contributor to organicmaps. [0] - https://organicmaps.app/ reply fizwidget 21 hours agoparentprevBy “this app” I assume you’re referring to Maps.me. I can recommend Organic Maps. It has none of those crypto shenanigans and is a great offline OSM-based app. reply fnfjfk 21 hours agorootparentYes maps.me reply L29Ah 21 hours agoprevosmand is better anyway :> reply Vinnl 10 hours agoparentI use both and they're both great, but also not drop-ins for each other. OSMand has more features, but is way more confusing and quite a bit slower. So I have OrganicMaps cover my basic navigation needs, but if e.g. I want to access offline Wikivoyage travel guides, I use OSMand. reply david-gpu 19 hours agoparentprevI use OsmAnd, but perusing both of the repos it is apparent that OrganicMaps has better code. What I like best about OSMAnd is that you can tweak the routing algorithm without recompiling the code, as the rules are stored in routing.xml. reply stevage 17 hours agoprevThis item title seems to be editorialising. reply metalman 22 hours agoprevdownloading from fdroid as I type this how did I not know? awsome what contreversy can do! exactly the oposite thanks reply Mr_Bees69 22 hours agoprev****, i use organic maps, we need to get more eyes on this. reply sandbx 20 hours agoprev [–] Organic Maps is great reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Organic Maps has released the source code for its \"metaserver,\" which optimizes map download speeds by directing users to the best Content Delivery Network (CDN) servers based on their location.",
      "The code, initially closed to prevent unauthorized forks, is now open under the MIT license, promoting transparency and community involvement.",
      "The team is reconsidering the use of Cloudflare Workers to prevent vendor lock-in and has reverted unauthorized changes made by a contributor, emphasizing their commitment to privacy and transparency."
    ],
    "commentSummary": [
      "Alexander Borsuk, co-founder of Maps.me, tried to close Organic Maps' open-source fork by altering its MIT license and adding logging to a private repository.- Roman Tsisyk, another project maintainer, reversed these changes and made the repository public, leading to a conflict and the revocation of his permissions.- The incident underscores internal disagreements about the project's open-source nature, yet Organic Maps continues to be a widely used offline map application."
    ],
    "points": 269,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1733512298
  },
  {
    "id": 42347606,
    "title": "How to use Postgres for everything",
    "originLink": "https://github.com/Olshansk/postgres_for_everything",
    "originBody": "Postgres for Everything (e/postgres) Keep It Simple Stupid, just use postgres. Goals & Inspiration The goal of this repository is to demonstrate how to use Postgres for Everything. It's inspired by this article from the Amazing CTO as well as this GitHub gist from @cpursley. Why maintain this list? It seems like every week I hear of a new tool on top of Postgres or another way it can being used, so I wanted a place where I can keep track of everything. If you have other examples, submit a PR! Table of Contents Blog Posts & Articles Cron Jobs Message Queues Analytics GIS & Mapping Audit Logs Access Control & Authorization Search Full Text Vector Time Series Column Oriented NoSQL Graph Data Foreign Data Alternative Query Languages HTTP APIs Events, Replication, CDC Caching Unit Tests HTML & Applications Graphics & Visualizations Migrations Performance Tuning Scaling & Storage Dashboards & UIs Data Visualization Package Management Language Servers Miscellaneous Bonus: How many items include Supabase? Blog Posts & Articles Simplify: move code into database functions Just Use Postgres for Everything PostgreSQL is the worlds’ best database Postgres is eating the database world Hacker News discussion Cron Jobs citusdata/pg_cron](https://github.com/citusdata/pg_cron)) cybertec-postgresql/pg_timetable](https://github.com/cybertec-postgresql/pg_timetable)) Message Queues https://adriano.fyi/posts/2023-09-24-choose-postgres-queue-technology tembo-io/pgmq SKIP LOCKED riverqueue/river sequinstream/sequin: Sequin is a tool for change data capture (CDC) in Postgres. Sequin makes it easy to stream Postgres rows and changes to streaming platforms and queues (e.g. Kafka and SQS). You can backfill existing rows and stream new changes in real-time. Analytics pg_analytics: Transforming Postgres into a Fast OLAP Database GIS & Mapping postgis/postgis Audit Logs arkhipov/temporal_tables supabase/supa_audit pgMemento/pgMemento pgaudit/pgaudit Access Control & Authorization arkhipov/acl https://www.postgresql.org/docs/current/pgcrypto.html michelp/pgjwt supabase.com/docs/guides/database/vault) Search Full Text Postgres Full Text Search (bunch of helpful links) paradedb/paradedb Postgres Full Text Search vs the rest Vector neondatabase/pg_embedding pgvector/pgvector tensorchord/VectorChord: PostgreSQL extension designed for scalable, high-performance, and disk-efficient vector similarity search. It serves as the successor to the pgvecto.rs project. Time Series timescale/timescaledb tembo-io/pg_timeseries Introducing pg_timeseries: Open-source time-series extension for PostgreSQL Column Oriented hydradatabase/hydra citusdata/cstore_fdw NoSQL https://www.postgresql.org/docs/current/datatype-json.html robconery/dox https://www.ferretdb.com Using JSONB in PostgreSQL®: How to Effectively Store & Index JSON Data in PostgreSQL Graph Data https://age.apache.org Foreign Data supabase/wrappers Alternative Query Languages kaspermarstal/plprql supabase.github.io/pg_graphql dosco/graphjin HTTP pramsey/pgsql-http supabase/pg_net APIs PostgREST/postgrest hasura/graphql-engine postgraphile.org supabase.github.io/pg_graphql Events, Replication, CDC The Notifier Pattern for Applications That Use Postgres https://www.postgresql.org/docs/current/sql-notify.html cpursley/walex (Disclosure: I maintain this and think it's pretty awesome) PeerDB-io/peerdb debezium/debezium 2ndQuadrant/pglogical Caching https://martinheinz.dev/blog/105 readysettech/readyset Unit Tests theory/pgtap peterldowns/pgtestdb HTML & Applications SQLpage](https://github.com/lovasoa/SQLpage)) Omnigres](https://github.com/omnigres/omnigres)) pg_render](https://github.com/mkaski/pg_render)) plmustache](https://github.com/PostgREST/plmustache)) Graphics & Visualizations pgSVG](https://github.com/dr-jts/pg_svg)) Migrations purcell/postgresql-migrations www.bytebase.com/ xataio/pgroll stripe/pg-schema-diff Performance Tuning Supabase Index Advisor](https://github.com/supabase/index_advisor)) Dexter](https://github.com/ankane/dexter)) HypoPG](https://github.com/HypoPG/hypopg)) pg_hint_plan](https://github.com/ossc-db/pg_hint_plan)) PGHero](https://github.com/ankane/pghero)) Scaling & Storage How Cloudflare Achieved 55 Million Requests per Second with Just 15 PostgreSQL Clusters! supabase/supavisor pg-sharding/spqr orioledb.com/: OrioleDB is a PostgreSQL extension that combines the advantages of both on-disk and in-memory engines. It uses PostgreSQL pluggable storage to increase performance and cut costs. Dashboards & UIs Baserow NocoDB](https://github.com/nocodb/nocodb)) AppSmith Data Visualization Evidence Metabase Package Management pgxman/pgxman supabase/dbdev Language Servers supabase/postgres_lsp Miscellaneous Very comprehensive list of Postgres tooling Unsupported PostgreSQL features in Aurora DSQL: AWS has a new database engine called Aurora DSQL, and explicitly listed what PostgreSQL features are not supported. Bonus: How many items include Supabase? 11",
    "commentLink": "https://news.ycombinator.com/item?id=42347606",
    "commentBody": "How to use Postgres for everything (github.com/olshansk)257 points by Olshansky 13 hours agohidepastfavorite141 comments tlarkworthy 7 hours agoI was recently annoyed to find postgres indexes don't support skipping [1] you also can't have the nul character in a string (\\u0000) [2]. Its great, but it has some strange WTF gaps in places. [1] https://wiki.postgresql.org/wiki/Loose_indexscan [2] https://stackoverflow.com/questions/28813409/are-null-bytes-... reply RedShift1 6 hours agoparentWhat is a reasonable use for a null character in a string? My first instinct is that strings with nulls in them should absolutely be rejected. reply larsnystrom 5 hours agorootparentThere are two kinds of programmers: Those who think of strings as text, and those who think of strings as a sequence of bytes. The second group doesn’t care about the special case where a byte is all zeroes. reply mattashii 5 hours agorootparentIn that second case the string is better represented as \"bytea\", which has most (but not all) of the features of the \"text\" type. reply pdimitar 5 hours agorootparentI agree with your take, it's just that many programmers want to easily jump from \"byte array\" to \"string in XYZ encoding\". I personally prefer byte arrays for unsafe data and to do deserialization in application code. reply rlupi 3 hours agorootparentIn other words, considering we are talking about string and unicode... There are two types of programmers, those that are wrong and those that are very wrong reply pdimitar 3 hours agorootparentlol. :) Funny but not entirely true. I had cases when we had to urgently store a firehose of data and figure out the right string encoding later. Just dumping the strings with uncertain encoding in `bytea` columns helped us there. Plus for some fields it helps with auditability f.ex. when you get raw binary-encoded telemetry from devices in the field, you should store their raw payloads _and_ the parsed data structures that you got from them. Being this paranoid has saved my neck a few times. The secret is to accept you are not without fault and take measures to be able to correct yourself in the future. reply lelanthran 5 hours agorootparentprevYup. It's a huge red flag when a datatype intended to be used for representation of written human language is abused to store something that has no glyph recognisable in any human language. There's a lot to complain about with nul-terminated strings, but not being able to store arbitrary bytes ain't one of them. reply jagged-chisel 6 hours agorootparentprevNot everything needs to be a C-string (null-terminated array/sequence of characters.) We are advanced enough with our understanding of Things that we can include metadata along with a chunk of bytes to indicate “this is a ‘string’ and it’s q bytes long and can have any value you want in there.” That said, I’m with you. And if someone wants nulls inside their “strings” then they probably want blobs. reply jagged-chisel 6 hours agoparentprev> you also can't have the nul character in a string … Let me introduce you to blob… reply hamilyon2 6 hours agoparentprevYes, skip-index scans require custom sql now. I am also a bit annoyed by cache-like uses not being first-class. Unlogged tables get you far, temporary tables are nice, but still all this feels like a hurdle, awkward and not what you actually need. reply drtgh 2 hours agorootparent> I am also a bit annoyed by cache-like uses not being first-class. Since what happened recently with Redis[1] the first thing I thought about was Postgre, but the performance[2] difference is too noticeable, so one have to look for other alternatives, and not very confident due thinking such alternatives may follow the same \"Redi's attitude\" ( ValKey, DragonflyDB, KeyDB, Kvrocks, MinIO, RabbitMQ, etc etc^2 ). It would be nice if these cache-like uses within Postgre had a tinny push. [1] https://news.ycombinator.com/item?id=42239607 [2] https://medium.com/redis-with-raphael-de-lio/can-postgres-re... XXXXX achieves a latency of 0.095 ms, which is approximately 85% faster than the 0.679 ms latency observed for Postgres’ unlogged table. It also handles a much higher request rate, with 892.857,12 requests per second compared to Postgres’ 15.946,02 transactions per second. reply whateveracct 11 hours agoprevJust don't use a single Postgres DB for everything as you scale up to 100+ engineers. You'll inevitably get database-as-the-API. Now if you have the actual technical leadership [1] to scale your systems by drawing logical and physical boundaries so that each unit has its own Postgres? Yeah Postgres for everything is solid. [1] Surprisingly rare I've found. Lots of \"successful\" CTOs who don't do this hard part. reply anonzzzies 10 hours agoparentYou don't have to plan this very early; most companies won't get to 100+ engineers. Let's ship first and worry about this, much much much much later. Overarchitecting stuff makes life hard; coming into companies that have 40 servers running with everything architected for 1000000 engineers and billions of visitors while in reality there aren't even 2 users and there is 1 overworked engineer. Stop doing that and stop people telling to do that. reply Cthulhu_ 9 hours agorootparentThis (don't overarchitecture stuff) is actually an argument in favor of using e.g. Postgres for everything, as adding more and more tools adds complexity and (architectural) overhead. reply szundi 10 hours agorootparentprevIf you die before that, it was the market, not engineers being slow. Of course. Or sales. reply whateveracct 3 hours agorootparentprevI agree totally. But I've seen companies go to 100+ and still insist that the level of monolithic coupling I describe hasn't been outgrown. reply KronisLV 6 hours agorootparentprev> Let's ship first and worry about this, much much much much later. Overarchitecting stuff makes life hard; coming into companies that have 40 servers running with everything architected for 1000000 engineers and billions of visitors while in reality there aren't even 2 users and there is 1 overworked engineer. Even if you try to draw boundaries between different bits of the system, you are unlikely to end up with 40 servers, not even close. The average system wouldn't even have 40 separate use cases for PostgreSQL or even the need for 40 different dependencies. However, if you do split it up early, you more realistically would have something along the lines of: * your main database - the main business domain stuff goes here * key-value storage/cache - decoupled from the main instance, because of the decoupling nobody will be tempted to put business domain specific columns here but keep it generic * message queue - for when processing some data takes a bunch of resources but during peak load you need to register a bunch of stuff quickly and process it when you get the capacity * blob storage - to not make the main database bloat a whole bunch, but to keep any binary stuff in a separate instance, provided you don't need S3 compatibility * auth - the actual user data, assuming you use Keycloak or something like it * metrics - all of the APM stuff, like from Apache Skywalking or PostgreSQL Give or take 2-3 services, all of which can run off of a Docker Compose stack locally or with the container management platform of your choice (not even Kubernetes necessarily, but simpler ones like Hashicorp Nomad or even Docker Swarm, using the Compose format). All of which can have their backups be treated similarly, similar approaches to clustering, all of which have similar applicable tools, all of which have similar libraries for integration with any apps and all of which can be granularly inspected in regards to how they perform and scaled as needed. It's arguably better than a single large instance that ends up with 300 tables eventually, has 100 GB of data in the shared test environment and you wouldn't know where to start in regards to making a working local environment if you join a legacy org that isn't using OCI containers and giving each dev a local environment. The single large deployment will rot faster than multiple ones. How many you actually need? Depends on what you do, it wasn't that long ago that GitLab decided to split their singular DB into multiple parts: https://about.gitlab.com/blog/2022/06/02/splitting-database-... (which also shows that you can get pretty far with a single schema as well, to anyone who wants a counter argument, though they did split in the end) Realistically, if you're doing a personal or small project, everything can be in the same instance because it'll probably never go that far, but I've also seen both monolithic codebases (that are deployed as singleton apps, e.g. not even horizontal scaling) and what shoving all of the data in a single database leads to (regardless of whether it's PostgreSQL or Oracle), it's never pleasant. I'd at the very least look in the direction of splitting out bits of a larger system based on the use case, not even DDD, just like \"here's the API and DB instance that process file uploads, they are somewhat decoupled from our business logic in regards to products and shopping carts and all that\". Now, would I personally always use PostgreSQL? Not necessarily, since there are also benefits to Redis/Valkey, MinIO, RabbitMQ and others, alongside good integrations with lots of frameworks/libraries that just work out of the box, as opposed to you needing to write a bunch of arguably awkward SQL for PostgreSQL. But the idea of using fewer tools like PostgreSQL for different use cases (that they are still good for) seems sound to me as well. reply rvba 5 hours agorootparentUser above was making a point. And you are lucky to not see an org where everything is a microservice that uses some unusual database, because the poeple responsible wanted to use some fancy new technology. Also seems you were lucky to not see messy development where some data is in a legacy system, some in new (which doesnt quite work), some in \"cool\" mongoDB that uses math.random to report just 10% of errors and rest is plugged via CSV files coming from ERP edited in Excel... reply KronisLV 2 hours agorootparent> And you are lucky to not see an org where everything is a microservice that uses some unusual database I've been unlucky to see an org where everything is a monolith (not horizontally scalabe due to a plethora of design choices along the way) that uses Oracle, including plenty of stored procedures and DB links along the way. Honestly, I'm starting to think that you can't win with these things and that there will be projects that suck to work with regardless of the tech stack or regardless of how much you try to make them not suck. That said, in general, you could probably do worse than PostgreSQL or even MariaDB (because at least with that one you can still run it in a container locally, much like MySQL, instead of running into Oracle XE or Oracle free version, whatever they called the latest one, limitations where you can't even bring the shared environment schema over to local containers). reply underdeserver 11 hours agoparentprevDrop the scare quotes from \"successful\". These guys shipped products. The migration to multiple databases, syncing user information etc. is a milestone, not a necessity at every step. reply devjab 8 hours agorootparentUnfortunately a lot of places teach software engineers to build over complexity before they need it. I like to remind people that StackOverflow ran on a couple of IIS servers and a msSQL db for a good while before scaling up to a little more than a couple of IIS servers. Hell they didn’t even do the CDN thing until 2017 or something similarity “crazy”. Mean while you have hordes of developers who write interfaces which will only ever be consumed by a single class because they’ve been taught to do so. As I see it the only way to look at scaling issues is that you’ve made it. Almost no user facing software will ever reach a point where something like Django couldn’t run it perfectly fine. It’s when you do, you build solutions, which sometimes means forking and beating the hell out of Django (Instagram), sometimes going into Java or whatever, spreading out to multiple data bases and so on. Of course once you actually hit the scaling issues you’ll also have enough engineers to actually do it. reply jumping_frog 8 hours agorootparentnext [2 more] [flagged] lovasoa 8 hours agorootparentdoesn't this apply to most websites on the web? reply szundi 10 hours agorootparentprevThank you reply RedShift1 11 hours agoparentprev> You'll inevitably get database-as-the-API I think that's actually the point of \"postgres for everything\"? reply whateveracct 11 hours agorootparentIf you don't have any discipline it becomes hell. Not to mention that a random team writing a migration that locks a key shared table (or otherwise chokes resources) now causes outages for everyone. reply blackenedgem 8 hours agorootparentRight but in this \"100-engineer\" scenario you'd have hoped the following would have happened: - Docs and guidelines on migrations would have been written - Some level of approval and review is required before execution These are things that isn't really postgres specific, any company that doesn't have those is going to be a nightmare. reply robertlagrant 7 hours agorootparentIf teams have technical boundaries defined at a higher level in the stack (e.g. APIs) and so they don't share a database, you don't need loads of process and docs and architectural meetings to coordinate. Letting teams delivery independently is a good architectural feature. reply whateveracct 3 hours agorootparentprevEven with those things, blast radius for mistakes is ..the entire company is down. e.g. User management deploy has somehow taken down core payment processing. reply xnx 7 hours agorootparentprevNot a problem until it's a problem. reply com 7 hours agorootparentAnd the best thing is that until it’s a problem you can focus on product/market fit and delighting your customers. Overengineering is a plague amongst SWEs, and almost as dangerous as failing to sell the product in the market. reply whateveracct 3 hours agorootparentI think cargo cutting stuff like \"Postgres for everything\" is also a plague. I agree you don't want to overengineer, but if you're a leader and your policy is basically \"engineers can't ever be trusted to do hard things\" then you're a bad leader. Sorry if this comes off as brusque, but I've seen good engineers want to use Elasticache for the right reasons and seen \"leaders\" tell them no because \"caching is one of the hard problems in computer science.\" Leadership-by-folksy-saying is sadly a real thing. reply pdimitar 4 hours agorootparentprev\"Until it's a problem\" is doing a lot of heavy lifting in your sentence. For 99% of projects that \"until\" will never come. reply xnx 3 hours agorootparentI agree completely. Far more startups are sunk by the cost of hypothetical technical problems than real product/market ones. reply solatic 8 hours agoparentprevDatabase-as-the-API can scale surprisingly far, particularly if you sell a single-tenant shard to each customer and therefore a separate database to each customer. Drawing logical software boundaries before Product even knows what the domain looks like (i.e. which features will sell) is quite risky. reply qaq 10 hours agoparentprevdatabase as API works fine if you have properly abstracted things with sprocs and views. It will be also far less brittle than 100 services exposed as GraphQL reply Kwpolska 5 hours agorootparentStored procedures just add overhead and make everyone's lives harder. Forget about any ORMs, you're writing raw SQL with all the quirks of PL/pgSQL biting you all the time. reply lelanthran 4 hours agorootparentThere's advantages too: - marking some columns as NOT NULL. - referential integrity means you can't accidentally have dangling pointers to non-existant dat. - mutually exclusive columns let's the database enforce things like \"at least one of A and B needs to be Nonzero, and both cannot be Nonzero at the same time. - create a type that allows only values matching a specific regex. Seriously, if you want strong typing across composite data, there isn't a language invented yet that comes even close to a RDBMS. Of course the majority of Devs don't know any of this because their ORM doesn't expose any of this; it gives them a way to store and query tabular data, and nothing else. reply pdimitar 4 hours agorootparentNone of this prevents you from doing both inside the DB _and_ the app. That way you cover your bases that (1) your app is sound and has maximum amount of fail-early validations to avoid corrupted data states and (2) even if somebody decides to skip the app and try to be clever in a psql console they'll still not be able introduce corrupted data states and (3) leave the door open for other apps to be able to connect to the same DB and do stuff (or simply to allow for a rewrite in another language). reply Kwpolska 2 hours agorootparentprevThe things you listed aren’t stored procedures, they are all possible to implement as check constraints. They are great, and they are fully compatible with ORMs. A stored procedure is a bit of code, stored in and executed by the database, usually written in a 1960s-era language (like PL/SQL or PL/pgSQL). reply benrutter 9 hours agoparentprevI'm so glad to see somebody made this comment!! Postgres for everything is pretty neat in that you can take expertise from one place, and use it somewhere else (or just not have to learn a zillion tools) The same database for everything is a really good way to have a tangled mess (are people still using the word complected?) where nobody knows which parts are depended on by what. reply CalRobert 11 hours agoparentprevYou could go whole hog with postgrest I suppose reply jjallen 11 hours agorootparentI always wonder if postgrest works well after growing quite a bit. I really should google the largest companies using it. reply boomskats 9 hours agorootparentJust check out who the key project maintainers are, and then look at who they're employed by. You'll probably recognize the companies. If you don't have schema cache reloading enabled (i.e. you're running it in prod), you can linearly scale postgrest instances pretty much infinitely without impact to the db. Each new instance just costs you a handful of connections and a small amount of memory. reply cpursley 7 hours agorootparentprevCheck out supabase, they're all in on postgrest. reply feverzsj 10 hours agorootparentprevIn my experience, without careful tunning, postgres performance starts dropping significantly around 10 million rows. reply natmaka 9 hours agorootparentWithout any consideration for I/O? In my experience performance is often directly related to the ratio (total RAM available to Postgresql, shared buffer and buffercache included / total size of tables and indices ), save any weird usage pattern. reply szundi 10 hours agorootparentprevWe have billions of rows with clustering on one instance, perfect, half a rack almost. reply boomskats 9 hours agorootparentprevCould you tell us a bit more about your experience? reply ForHackernews 6 hours agoparentprevPostgres is inherently multi-tenant. Have separate logic DBs in one physical instance, connect using roles with minimal permissions, expose views (and materialized views!) for querying so you can mutate the underlying tables without requiring applications to change. reply whateveracct 3 hours agorootparentGreat point - wish the people in charge at my job knew that lol reply jeeybee 8 hours agoprevPGQueuer is a lightweight job queue for Python, built entirely on PostgreSQL. It uses SKIP LOCKED for efficient and safe job processing, with a minimalist design that keeps things simple and performant. If you’re already using Postgres and want a Python-native way to manage background jobs without adding extra infrastructure, PGQueuer might be worth a look: GitHub - https://github.com/janbjorge/pgqueuer reply WJW 7 hours agoparentI always wondered about the claim that SKIP LOCKED is all that efficient. Surely there are lots of cases where this is a really suboptimal pattern. Simple example: if you have a mixture of very short jobs and longer duration jobs, then there might be hundreds or thousands of short jobs executed for each longer job. In such a case the rows in the jobs table for the longer jobs will be skipped over hundreds of times. The more long-running jobs running concurrently, the more wasted work as locked rows get skipped again and again. It wouldn't be a huge issue if load is low, but surely a case where rows get moved to a separate \"running\" table would be more efficient. I can think of several other scenarios where SKIP LOCKED would lead to lots of wasted work. reply jeeybee 7 hours agorootparentGood point about SKIP LOCKED inefficiencies with mixed-duration jobs. In PGQueuers benchmarks, throughput reached up to 18k jobs/sec, showing it can handle high concurrency well. For mixed workloads, strategies like batching or partitioning by job type can help. While a separate \"running\" table reduces skips, it adds complexity. SKIP LOCKED strikes a good balance for simplicity and performance in many use cases. One known issue is that vacuum will become an issue if the load is persistent for longer periods leading to bloat. reply blackenedgem 5 hours agorootparent>One known issue is that vacuum will become an issue if the load is persistent for longer periods leading to bloat. Generally what you need to do there is have some column that can be sorted on that you can use as a high watermark. This is often an id (PK) that you either track in a central service or periodically recalculate. I've worked at places where this was a timestamp as well. Perhaps not as clean as an id but it allowed us to schedule when the item was executed. As a queue feature this is somewhat of an antipattern but did make it clean to implement exponential backoff within the framework itself. reply cercaapps 7 hours agorootparentprevJob rows could have an indexed column state so you just query for the rows with state \"not-started\". This way you won't need to skip over the long jobs that are in state \"processing\". reply normie3000 7 hours agorootparentI'm not 100% confident, but this sounds like it would have unexpected effects. reply TkTech 7 hours agoparentprevAlso https://github.com/TkTech/chancy for another (early) Python option that goes the other way and aims to have bells and whistles included like a dashboard, workflows, mixed-mode workers, etc... Check out the Similar Projects section in the docs for a whole bunch of Postgres-backed task queues. Haven't heard of pgqueuer before, another one to add! reply arunix 7 hours agoparentprevWhat are its advantages compared to a more dedicated job queue system? reply jeeybee 7 hours agorootparentI think PGQueuers main advantage is simplicity; no extra infrastructure is needed, as it runs entirely on PostgreSQL. This makes it ideal for projects already using Postgres and operational familiarity. While it may lack the advanced features or scalability of dedicated systems like Kafka or RabbitMQ, it’s a great choice for lightweight without the overhead of additional services. reply jensneuse 11 hours agoprevI absolutely love Postgres, but please allow me to say that you absolutely don't want to expose an API generated from a database to people outside of your team. This limits you a lot in changing the way you store your data. I wrote about this topic before and haven't changed my opinion much. You don't want to have that tight coupling: https://wundergraph.com/blog/six-year-graphql-recap#generate... reply RedShift1 10 hours agoparentWhat exactly is the problem with tight coupling? You're going to insert an entire layer that basically translates format A to format B, just so you can later change a column name in the database and not have to change it in the API or something? reply grey-area 10 hours agorootparent1. You don’t want or need to expose lots of implementation details. Many of your data structures should be private, and many should be partly private. 2. Your data structures should not dictate the shape of your api, usage patterns should (e.g. the user always needs records a,b,c together or they have a and want c but don’t care about b) 3. It stops you changing any implementation details later and/or means any change is definitely a breaking change for someone. reply Cthulhu_ 9 hours agorootparentprevThere's a few issues; one is that if you have the DB do everything, all of your business logic lives there too, instead of just the data. This is still fine if you have a single use case, but what if in addition to your main application, you also need to use it for things like BI, customer service, analytics / predictions, etc? It then quickly becomes better to use it as a datastore and have another layer decide what to do with it. And in 30 odd years, everything will be different again, but your company's main data store will not have moved as fast. reply pdimitar 3 hours agorootparentprevThe extremely obvious problem is that how you store data is an implementation detail and those change when requirements (or the market) evolve. I'll give you an API and will make triple sure it's as fast as a machine can even serve it and you let me worry about how it's all stored. To additionally answer you with an analogy: when you have a problem with a company, you call the call center, not Jenny from accounting in particular. Jenny might have helped you twice or thrice but she might leave the company next year and now you have no idea how to solve your problem. Have call centers to dispatch your requests wherever it's applicable in the given day and leave Jenny alone. reply hmottestad 10 hours agorootparentprevNormalization is one of those typical issues where you might be fine with having everything normalized when you start off, but then once performance gets bad you end up denormalizing tables that are typically joined. reply haileys 10 hours agorootparentprevYes. A lot of the work I've done through my career is essentially this, once you boil away the rest of the details. reply dunwaldo 10 hours agoparentprevyou could use views as a layer in between? reply boomskats 9 hours agorootparentYou should use views as the layer in between. They'll let you version your API and make changes without breaking things. reply j12a 8 hours agoprevBeing stuck with MariaDB/MySQL in some projects, I recently compared it to PostgreSQL and found many of these extended capabilities existing there also, including JSON, temporal tables (w/ SYSTEM VERSIONING), columnar and vector storages etc. LISTEN/NOTIFY type functionality was sort of missing but otherwise it was surprising how it is keeping up also, while little of that is probably being used by many legacy apps. reply jankovicsandras 9 hours agoprevCan you please add this shameless plug? https://github.com/jankovicsandras/plpgsql_bm25 Opensource BM25 search in PL/pgSQL (for example where you can't use Rust extensions), and hybrid search with pgvector and Reciprocal Rank Fusion. reply samanator 6 hours agoparentCool to see you here! We discussed this very thing on supabase https://github.com/orgs/supabase/discussions/18061#discussio... reply feketegy 8 hours agoparentprevTry opening a pull request maybe reply KingOfCoders 11 hours agoprev\"It's inspired by this article from the Amazing CTO [0]\" Getting up in the morning, seeing an article that references you, bliss! [0]: https://www.amazingcto.com/postgres-for-everything/ reply tbarbugli 8 hours agoparent> Use Postgres for Fulltext Search instead of Elastic pg fulltext search is very limited and user unfriendly, not a great suggestion here reply bkovacev 8 hours agoprevWhile we are it - are there any good resources on how to best self host a Postgres database? Any tips and tricks, best practices, docker / no docker etc? I’m looking to self host a database server for my multiple pet projects, but I would love to get backups, optimizations and other stuff done well. reply philippta 8 hours agoparentI find this YouTube channel[0] has a great number of videos on how to setup Postgres on a regular Linux machine, explaining how configure it and make it work for high availability. It’s easier than you’d think. [0] https://youtube.com/playlist?list=PLBrWqg4Ny6vVwwrxjgEtJgdre... reply christophilus 6 hours agoparentprevWhen I self host I lean towards SQLite simply because it’s simpler. In-place upgrades, trivial backups via Litestream, etc. Postgres major version upgrades are the main reason I don’t self host it, though maybe I should rethink my position on that! reply hboon 8 hours agoparentprevNot the answer you were looking for, but I had been shopping recently for slightly overlapping reasons (I want to ship projects, but they are all smaller MVPs that might run for a while and I didn't want to pay for 1 database service for each one on Render). I found https://www.thenile.dev/pricing which supports which apparently supports unlimited databases. reply Toritori12 8 hours agoparentprevafaik: On promise: Use containers but the data folder should be mounted volume On cloud/k8s: Just use a managed DB, setting up a DB in k8s is hard because the filesystem reply ianpurton 10 hours agoprevI can see a lot of benefits of having 1 API to access a lot of functionality. For example instead of integrating with a message queue I can just do an INSERT this is great. It lowers the friction. Vector search is a no brainer too. Why would I have 2 databases when 1 can do it all. Using Postgres to generate HTML is questionable though. I haven't tried it but I can't image its a viable way to create user interfaces. reply qpiox 9 hours agoparent> Using Postgres to generate HTML is questionable though. I haven't tried it but I can't image its a viable way to create user interfaces. Check https://apex.oracle.com/ which does that. reply lokimedes 11 hours agoprevHaving just spent the better part of two weeks integrating Apache Age for Graph data, just to realize the project is stale and a mess, don’t take this list on face value. Now hoping for better results with DGraph, but it seems that graph databases are living a precarious existence. reply sureglymop 1 hour agoparentIf I may ask, what type of use cases are there for which a graph database is well suited? I could imagine there to be a few (that I can't think of and haven't seen). I have seen graph databases used where they didn't make sense though. reply worldsayshi 8 hours agoparentprevI wonder what's the catch with Dgraph? Why not chose it above Neo4j? I'm asking because the graph db projects I've been involved in has all used Neo4j and it would be nice to know of a good alternative. reply lokimedes 8 hours agorootparentI want it to be as free as possible, neo4j only let’s you run a single database in non-Enterprise mode. We are building a consumer product, where the database is embedded and not centralized in a cloud, so my focus is perhaps different from most. Both neo4j and dgraph comes with additional non-compete clauses, but DGraph’s work for our use case. Subjectively I’d prefer neo4j. I have been following the company since its inception. reply anon3459 10 hours agoparentprevHaaaa same here for apache age. Can you elaborate a little please reply lokimedes 7 hours agorootparentThe original sponsor of the project just withdrew all resources, the state of the existing codebase is far from mature, the client I tried (python) was really shaky and the lidt goes on. As for the precarious life of graph database companies. DGraph also went though being sold recently, and OrientDB that I also liked, as acquired by SAP, only to be abandoned. Neo4j has stood its time, but the licensing doesn’t fit our needs. reply demaga 8 hours agoparentprevCame here to say this. Last time I checked, Apache Age was wildly inferior to Neo4j. So technically, it does exist and has right to be on the list, but I wouldn't recommend it for serious workloads. reply rdsubhas 7 hours agoprevThis is a lovely list, thank you. But what's really missing is multi master and high availability. I'm glad to see that partitioning via sharding is covered. IMHO the true limitations of RDBMS are not about usage, but scaling: Multi master across simple zones, High availability, Partitioning. (IMHO it comes from ACID compliance, so I don't know if it's even solveable natively) reply ffsm8 7 hours agoparent> IMHO it comes from ACID compliance. That was the whole marketing spiel of Cassandra DB in 2012+ and the source of the CAP theorem reply Ambolia 4 hours agoprevWhat about binary blobs? I hate having to store them outside of the database and that you can't just do a dbdump/import to clone a project. reply taffer 3 hours agoparentHave you had a look at the bytea[1] type? It allows you to store binary objects up to 1GB and it's dumped transparently just like a string. [1] https://www.postgresql.org/docs/current/datatype-binary.html reply cpursley 9 hours agoprevThanks for the gist mention!: https://gist.github.com/cpursley/c8fb81fe8a7e5df038158bdfe0f... reply ksynwa 6 hours agoprevAnyone has advice for backup up a postgres database that is run for personal use? I currently have one that is used by miniflux. I wrote a script for backing it up but it is gonna fail me some day. ``` #!/usr/bin/env sh set -e PGUSER=miniflux PGPASSWORD=... pg_dump -F t -h 127.0.0.1 minifluxgzip > /backups/miniflux_db_temp.tar.gz mv /backups/miniflux_db{_temp,}.tar.gz ``` reply sureglymop 1 hour agoparentI do something similar but pipe a dump of the whole db with pg_dump into restic. Only works well for small to medium sized DBs. reply supriyo-biswas 6 hours agoparentprevThat works but for a more robust solution you could look at https://pgbackrest.org reply rozenmd 10 hours agoprevI've been running my project as a business for almost four years now and I think my Postgres DB is the only thing that I haven't changed. Easily the most stable part of the stack (100% uptime on RDS since 2021/02/01). reply Havoc 6 hours agoprevWhile I appreciate postgres flexibility this feels like a mistake. A bit like a person insisting Rust is best and using other languages are unnecessary. You could, but it is going to force technical compromises. Even tools that can do everything aren't the best at everything. reply paradite 9 hours agoprevAnyone experienced with postgres full text search? I want to get something simple setup but couldn't get it to work. I want to match substrings like \"/r/chatgpt\" (sub reddits) in url links, but couldn't get it to match. Tried a few types of queries like phrase, plain, default, simple, english. All have some weird issues, either not matching special characters, or not matching substrings (partial match). Also I'm somewhat limited on the syntax side by what can be done with drizzle ORM. reply jumping_frog 8 hours agoparent> I want to match substrings like \"/r/chatgpt\" (sub reddits) in url links, but couldn't get it to match. This likely means tokenization striping out special characters. Try ngram search methods. They should work out of box. reply rcleveng 12 hours agoprevI like this. I strongly believe that picking some tech stack you know when you are starting out is always the right decision, until it's not. Only then do you pick a different solution. Better to move fast leveraging what you know, until you need something else. reply lenkite 9 hours agoprevWonder if someone can write a SQLite for everything article too - would be far more difficult though if considering HA. reply jascha_eng 10 hours agoprevThe markdown is a bit broken. But I agree with the message. Created a PR to update some of the vectordb links and also fixed the markdown: https://github.com/Olshansk/postgres_for_everything/pull/1 reply mkaski 7 hours agoprevGlad to see pg_render on the list! Here's a demo site rendered entirely using just Postgres(t): https://pgrender.org reply pdimitar 3 hours agoparentFrustratingly, all the demos are blank canvases for me. I use PiHole at home but I checked the query logs and I see no blocking anywhere. Enabled all JS via uMatrix and uBlock Origin but still no dice. macOS, Safari and Firefox. reply adamcharnock 12 hours agoprevIf anyone has seen a steams implementation in Postgres please let me know. Not just pub/sub, but full implementation of streams with consumer groups would be very interesting to me. Second best, how would you implement this in Postgres? I’m tempted to give it a go but I haven’t a fully-baked plan yet. reply boomskats 9 hours agoparentHave a look at diogob/postgres-websockets or supabase/realtime. reply maxboone 8 hours agoprevOne of the problems I have with this is that I wonder if this doesn't majorly impact blue/green upgrades and (actual cluster, not schema) migrations. reply christophilus 6 hours agoprevHas anyone here run Firebird in production? It seems simpler (from an ops / sysadmin point of view). reply tanin 6 hours agoprevGreat repo. I love Postgres too. Use it for everything until there's an actual bottleneck reply feverzsj 11 hours agoprevPostgres administration is such a PITA. I'd rather use sqlite for everything. reply cpursley 7 hours agoparentsqlite is neat but most of us need a real database with enterprise features reply joeevans1000 10 hours agoprevGreat repo! Thanks! I'm wondering, any approach for bitemporal dbs, like xtdb? reply seanhunter 10 hours agoparentYou definitely don’t need a special database for bitemporal data. Just a datetime and as of data time column, your value column and whatever metadata you want (or a jsonb col for metadata if you want more flexibility at the cost of some speed of filtering by metadata) reply refset 7 hours agorootparentHow do you find it when you scale it up to every table, every query? reply seanhunter 3 hours agorootparentI’m not 100% sure what you mean. Systems I have used that do this don’t generally store each time series in a different table. Normally there’s just one big table for intraday time series and one for daily, with columns being like ts, as_of, series_id, value, metadata or something like that. It scales just fine depending of course on the usual stuff - load pattern etc. If you want really high scalability you should be using something like clickhouse anyway. Edit to add: the rationale behind having separate intraday and daily time series tables in those systems is the type of the value and as of timestamps is different (in one its a date, in one its a datetime), and storing dates as datetimes is a rich source of bugs. reply wiseowise 9 hours agoprevs/postgres/sqlite/g reply polote 8 hours agoprevThis idea that Postgres should be used for everything really need to die in a professional context. I was appointed in a company of 10 dev that did just that. All backend code was PostgreSQL functions, event queue was using Postgres, security was done with rls, frontend was using posgtraphile using graphql to expose these functions, triggers were being used to validate information on insert/update. It was a mess. Postgres is a wonderful database, use it as a database. But don't do anything else with it. Before some people come and say \"things were not done the right way, people didn't know what they were doing\". The dev were all fan of Postgres contributing to the projects around, there was a big review culture so people were really trying to the best. The queue system was locking all the time between concurrent requests => so queue system with postgres works for a pet project All the requests were 3 or 4 times longer due to fact that you have to check the rls on each row. We have also all pour API migrated now and each time the sql duration decrease by that factor ( and it is the exact same sql request ). And the db was locking all the time because of that as it feels likes rls breaks the deadlock detection Postgres algorithm SQL is super verbose a language, you spend your time repeating the same line of code , it makes basic function about 100 lines long when they are 4-5 lines in nodes js It is impossible to log things inside these functions to have to make sure things will work and if it doesn't you have no way to know where the code did go through You can't make external API call, so you have to use a queue system to make any basic things there There are not real lib , so everything need to be reimplemeted It is absolutely not performant to code inside the db, you can't do a map so you O(n2) code all the time API were needed for the external world , so there was actually another service in front of the database for some case and a lot of logic were reimplemeted inside it There was a downtime at each deployment as we had to remove all the rls and recreate them ( despite the fact that all code was in insert if not update clauses) it worked at the beginning but at some point in time it stopped working and there was no way to find why, so drop all rls and recreate them It is impossible to hire dev that wants to work on that stack and be business oriented , you would attract only purely tech people that care only about doing there own technical stuff We are almost out of it now after 1 year of migration work and I don't see anything positive about this Postgres do everything culture compared to a regular node js + Postgres as a database + sqs stack So to conclude, as a pet project it can be great to use Postgres like that, in a professional context you are going to kill the company with this technical choice reply christophilus 6 hours agoparentI agree. Postgres as the only piece of your data-layer? Yes. Postgres as your application and business-logic layer? No, thanks. reply osigurdson 5 hours agoprevPostgres, ClickHouse and NATS for everything reply pdimitar 3 hours agoparentYou use NATS as opposed to Kafka, I presume? Also, what's Clickhouse for? Logs, observability data? reply osigurdson 2 hours agorootparentNATS is great because it has pub/sub, streaming (like Kafka), KVS (like Redis). It is possible to do some of these things in Postgres but really, why? We use ClickHouse for time series data. Postgres was ok up to low billions of points. Despite trying to use timescale for this purpose it did not fit our use case. reply pdimitar 2 hours agorootparentVery valuable, thank you. As time series data (and especially observability data) tends to very quickly explode in volume, I believe planning for tens of billions, if not trillions, of records is worth planning for from the start and it is not over-engineering. If you don't mind one final question: can you ACK a message in NATS without it being bound to offset that makes it impossible to _not_ ACK a message without ruining the ACKs of the previous messages? To clarify: I often found myself in situations when I was fetching batches of stuff from Kafka, say, 50 at a time, and then hand them off to 50 parallel agents to process. However, f.ex. messages 17, 31 and 47 failed processing and I could not not ACK them as that would not allow us to ACK those that succeeded before. So I ended up pushing them to another Kafka queue / topic that specifically deals with retries. That's IMO a hack, as most apps out there surely don't need the monstrous speed that Kafka can provide. I am OK with something (not much) slower where I have the freedom to ACK or not-ACK any particular event/message regardless of its position. Does NATS allow for it? reply osigurdson 1 hour agorootparentPerhaps I misunderstand, but if you have 50 parallel agents, why not just have each pull messages, process them and ACK when complete? The part I don't understand is the pre-fetch. Note that NATS is much more flexible than Kafka however, so more likely to fit more uses cases (even just for streaming). For the first question, I'd definitely recommend using ClickHouse for 10B - 1T points. reply anonzzzies 10 hours agoprevIs there a local-first postgres solution? reply malux85 10 hours agoparentWhat do you mean local first? Postgres is an executable that you can run on your machine if you wish reply anonzzzies 9 hours agorootparentI mean storing data in your https://localfirstweb.dev . Sqlite allows that so you can work fully in our client (browser/mobile) without internet potentially and then later sync (rqlite, pouchdb, etc for instance). If I get to do 'everything' with postgres, I need that too, so I was wondering if it exists. reply paulgb 8 hours agorootparenthttps://electric-sql.com/ does this, running a stripped-down version of postgres on the client. https://www.powersync.com/ is a similar approach, but using sqlite on the client. https://zero.rocicorp.dev/ also deserves a mention, but I believe it’s more of a read cache with writes going directly to the server. I’m not sure if it supports SQL client-side or has its own ORM. reply dmezzetti 6 hours agoprevI fully agree. Postgres has solved many of the problems that many are re-solving with GenAI related databases. With txtai (https://github.com/neuml/txtai), I've went all in with Postgres + pgvector. Projects can start small with a SQLite backend then switch the persistence to Postgres. With this, you get all the years of battle-tested production experience from Postgres built-in for free. reply cbeach 7 hours agoprevI love Postgres but wouldn't want to use it for everyting. Taking graph databases, for example, Postgres (via Apache AGE) stores graph data in relational tables with O(log(n)) index lookups and O(k ⋅ log(n)) traversal* Whereas a true graph database like Neo4J stores graph data in adjacency lists, which means O(1) index lookups and O() traversal. That's a massive difference in traversal complexity for most graphs. *k is the degree of the node (number of edges connected to a node). reply cynicalsecurity 11 hours agoprevPostgres cult strikes back. reply 29athrowaway 11 hours agoprevBroken markdown. - the entire repository is 1 file - the file is titled \"read me\" - but you didn't read it (it's not proofread) why do you want me to read something you did not read? reply cvalka 9 hours agoprevnext [8 more] [flagged] cpursley 7 hours agoparentOkay, so please enlighten us, then. Which extendable open source modern database is the new hotness? reply bregma 7 hours agorootparentI hear MongoDB is webscale. reply cvalka 4 hours agorootparentprevhttps://en.wikipedia.org/wiki/TiDB https://en.wikipedia.org/wiki/YugabyteDB https://ydb.tech/ reply boomskats 9 hours agoparentprevhttps://db-engines.com/en/ranking_trend/relational+dbms reply cvalka 4 hours agorootparenthttps://en.wikipedia.org/wiki/TiDB https://en.wikipedia.org/wiki/YugabyteDB https://ydb.tech/ reply matthewmacleod 9 hours agoparentprevIf you’re going to make a ragebait comment you should ideally provide some kind of basis beyond “waaaah everybody except me is stupid” reply cvalka 4 hours agorootparentYou should find your reading glasses and read what actually is written. When you do find your reading glasses, the links below might be helpful. https://en.wikipedia.org/wiki/TiDB https://en.wikipedia.org/wiki/YugabyteDB https://ydb.tech/ reply foobarkey 12 hours agoprev [2 more] [flagged] MonkeyClub 6 hours agoparent [–] Objection, asked and answered: https://m.youtube.com/playlist?list=PLBrWqg4Ny6vVwwrxjgEtJgd... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Postgres for Everything\" is a repository that promotes the use of PostgreSQL, a popular open-source relational database, for a wide range of applications beyond traditional database tasks.- The repository invites contributions and tracks various innovative uses of PostgreSQL, including cron jobs, message queues, analytics, and more, highlighting its versatility.- The initiative includes 11 items related to Supabase, a platform that provides backend services using PostgreSQL, showcasing its integration and utility in modern application development."
    ],
    "commentSummary": [
      "The discussion emphasizes Postgres' versatility in various applications but also points out limitations, such as handling null characters in strings and requiring custom SQL for skip-index scans.- Opinions vary on using Postgres for all purposes, with some cautioning against over-reliance on a single database for large teams and others praising its simplicity and reduced complexity.- Alternatives like Redis are mentioned, along with challenges in scaling and the importance of avoiding over-engineering in the early stages of a project."
    ],
    "points": 257,
    "commentCount": 141,
    "retryCount": 0,
    "time": 1733550975
  },
  {
    "id": 42343953,
    "title": "Lies I was told about collab editing, Part 1: Algorithms for offline editing",
    "originLink": "https://www.moment.dev/blog/lies-i-was-told-pt-1",
    "originBody": "Nov 16, 2024 Lies I was Told About Collaborative Editing, Part 1: Algorithms for offline editing AC By Alex Clemmer In early 2024, I began investigating collaborative editing systems for use in Moment’s core text editor. In some ways, we are in a golden era for the problem. Several algorithms now claim to solve not only the online case (where many people simultaneously edit a document), but also the offline case (where people edit offline for an unbounded amount of time, come online, and “magically” their changes are merged together). I had read the papers and listened to the talks, and like others, I was initially optimistic the community had settled on the “right answer” to the whole problem space. One of the earliest surprises in the evaluation process was that this is not true——that despite the marketing copy, blog posts, and technical papers stating otherwise, these collaborative editing algorithms provide a systematically poor offline editing experience. Specifically: The two most popular families of collab editing algorithms (CRDTs and OT) resolve direct editing conflicts unintuitively enough that users generally interpreted their results as data corruption. Offline editing dramatically increases the likelihood of direct conflicts, which means these algorithms are not, on their own, appropriate for offline editing. In many ways, this is bad news. And we will spend much of this post discussing why we think these things are true—example anomalies, failure modes, what the practical limitations are, and so on. But, the news is not all bad. We are heartened to see some recent research (e.g.) reversing the trend of treating collab editing as a pure algorithms problem, and instead treating it as a UI/UX problem in which algorithms assist. One of the goals of this post is to help motivate this line of reasoning, and we will make time at the end to encourage that, too. Example: a seemingly trivial editing conflict Let’s start with a game. I provide a trivial editing conflict and you try to guess what happens. Ready? Here’s the scenario: Alice and Bob are both offline. They are editing a document containing the following text: Bob changes the spelling of Color to the British Colour. Alice deletes all of the text. Later, both Alice and Bob come online. These edits conflict. The system must reconcile these two conflicting edits, even though it has no knowledge of which one came first. Q: When all reconciliation is complete and the machines agree, what text will the document contain? Guess by typing your answer into the text field below and clicking ‘submit.’ Note that we accept the empty string as a valid guess. ✋ 🛑 Did you guess correctly? Seriously—you really should try this before continuing on. These anomalies are common on direct conflicts. The example above is interesting because it is a trivial, direct editing conflict which these (popular) collaborative editing algorithms do seem to claim to support, but which empirically produces a document that no human would ever write on their own. This is also a case where users interpreted the result as the Moment product corrupting their data. So how common is this problem? Every use case is different, but, anecdotally we found that roughly 20-30% of direct conflicts we tried produced results that were similarly—in our opinion—unacceptable, for our use case of offline editing. [nb., I’m not sure if it’s helpful to list some of the anomalies we encountered here, if it is, let me know at alex@moment.dev or the Moment Discord server and I’ll follow up with a more thorough review.] Ultimately, though, because of the negativity of the feedback and the frequency of its occurrence, we felt we could not defend this result to our users. These algorithms do seem to claim to support offline editing. Initially, when we encountered this result, we thought that we must have misunderstood the semantics offered by these tools. Even now, though, when I read the technical copy for these projects, I can’t help but think this scenario really is in scope. The Yjs readme explicitly states that it “supports […] offline editing.” The ShareJS landing page claims to allow collaboration “with any amount of lag (from zero to an extended holiday).” The Peritext authors describe it as “allow[ing] users to edit independent copies of a document” which can be “automatically merg[ed] […] back together in a way that preserves the users’ intent as much as possible.” I admit to still not being quite sure what to make of this. We are in the market for a solution to this problem! I want to believe! I just don’t see how to reconcile these claims with the frequency and kinds of errors we’re seeing in practice. Coordination-free algorithms will always have anomalies on direct conflicts. At some point in the evaluation process we resigned ourself to the fact that the algorithms did not do what we wanted. But a lingering question remained: is this a fundamental problem for the algorithms, or with some careful contributions, could we contribute back and fix it? Unfortunately, we think it’s a fundamental problem for the algorithms, for a few reasons. The algorithms only have enough information to guess what the result should be. The algorithms can’t email Alice and Bob to see what they wanted or review it in the GitHub Pull Request UI. It receives Alice’s proposal to delete all the text and Bob’s proposal to change the spelling without knowing what their intent was, and it must use a heuristic to decide what to do with them. The algorithms operate on characters and have very, very poor guarantees about their output. This is why in the example above, Alice and Bob end up with a document simply containing the letter u. Not a valid sentence, and not even a valid word! Alice and Bob might make different decisions if they knew what the other was doing. Alice and Bob both made decisions based on the document containing the text, The Color of Pomegranates. If Alice knew Bob changed the spelling, or Bob knew Alice deleted the paragraph, they might change their decision to edit at all. I think this is probably worth a post all on its own, particularly with more treatment given to the fact that these algorithms don’t—and cannot—obey a causal ordering (cf., Jamie Brandon’s post). But for now, I think we can settle for the intuitive version of this argument. Offline editing is a UI/UX problem, not an algorithms problem; and other conclusions. So far the news has been bad. We entered the evaluation hopeful that we’d be able to support true offline editing “incidentally” for having implemented a tricky algorithm. And it is true that we left the evaluation left process, well, bereft of hope—forced to conclude that users were right to think of these algorithms were corrupting their data, that the anomalies were fundamental to the algorithms themselves, and that they would be frequent enough to be a real issue. YMMV, but that is where we ended up. As I said, though, the news is not all bad. Another way to view this result is as a strong argument for putting significant resources into collaborative editing as a UI/UX problem. The algorithms cannot completely solve the problem, but they can play a role in the solution. There are a few reasons to be optimistic about this. One is that we already have one widely-adopted UI for merging documents: git! So in some sense, the research question is how much better it can be—more approachable, more accessible, more automatic, etc. I think this resemblance is not superficial. In 2009, a surprising amount of the discourse was focused on the algorithms git used to automatically merge changes together. git adapted the Myers O(ND) diff algorithm, then used mostly by biologists to do BLAST-like sequence analysis. Bram Cohen thought the diff results were unintuitive, and invented the patience diff algorithm, which was then adopted by bzr, a now-defunct git competitor. The list goes on. I think the primary change is that these debates were focused on producing diffs that humans read; now the debate is whether the algorithms can accomplish this result with no human involvement at all. Another reason to be optimistic is that some researchers do seem to be concentrating on this problem as a UI/UX problem. I am thinking particularly of Ink & Switch, e.g., the work on collaborative history. I understand Ink & Switch to be operating purposefully a few years ahead of production, but I am excited to see where this lands when it settles down. Acknowledgements Heartfelt thanks to Kyle Kingsbury, Sean Gillespie, Ian Livingstone, David Adrian, Ben Linsay, Alex Rasmussen, Lita Cho, Lenny Pruss, Michael Yamnitsky, Camille Fournier, James Turnbull, and Ryan Cox for their comments and feedback.",
    "commentLink": "https://news.ycombinator.com/item?id=42343953",
    "commentBody": "Lies I was told about collab editing, Part 1: Algorithms for offline editing (moment.dev)243 points by antics 22 hours agohidepastfavorite85 comments josephg 21 hours agoHi! Author of Eg-walker & ShareJS here, both of which are referenced by this post. You write this article like you're disagreeing with me - but I agree completely with what you've said. I've been saying so on HN for years. (Eg, in this comment from 6 years ago[1].) The way I think about it, the realtime collaborative tools we use today make a lot of sense when everyone is online & editing together. But when users edit content offline, or in long lived branches, you probably want the option to add conflict markers & do manual review when merging. (Especially for code.) Luckily, algorithms like egwalker have access to all the information they need to do that. We store character-by-character editing traces from all users. And we store when all changes happened (in causal order, like a git DAG). This is far more information than git has. So it should be very possible to build a CRDT which uses this information to detects & mark conflict ranges when branches are merged. Then we can allow users to manually resolve conflicts. Algorithmically, this is an interesting problem but it should be quite solvable. Just, for some reason, nobody has worked on this yet. So, thanks for writing this post and bringing more attention to this problem! If anyone is interested in making a unique and valuable contribution to the field, I'd love to see some work on this. Its an important piece thats missing in the CRDT ecosystem - simply because (as far as I know) nobody has tried to solve it yet. At least not for text editing. [1] Bottom part of this comment: https://news.ycombinator.com/item?id=19889174 reply gritzko 25 minutes agoparentThe author describes the case of overlapping concurrent splices. It is a known funky corner case, yes. If we speak of editing program code, the rabbit hole is deeper as we ideally expect a valid program as a result of a merge. There was a project at JetBrains trying to solve this problem through AST-based merge. After delving into the rabbit hole much much deeper, the guys decided it is not worth it. This is what I was told. reply CalebJohn 11 hours agoparentprev> Algorithmically, this is an interesting problem but it should be quite solvable. Just, for some reason, nobody has worked on this yet. So, thanks for writing this post and bringing more attention to this problem! I'm skeptical that an algorithmic solution will be possible, but I can see this being handled in a UX layer built on top. For example, a client could detect that there's been a conflict based on the editing traces, and show a conflict resolution dialog that makes a new edit based on the resolution. The tricky part is marking a conflict as resolved. I suspect it could be as simple as adding a field to the crdt, but maybe then it counts as an algorithmic solution? [1] https://josephg.com/blog/crdts-go-brrr/ reply crabmusket 7 hours agorootparentThat is what josephg was suggesting: > it should be very possible to build a CRDT which uses this information to detects & mark conflict ranges when branches are merged reply astrobe_ 2 hours agorootparentprevDo you think that a LLM/\"AI\" can reliably solve the merging problem? reply taejo 1 hour agorootparent> Do you think that a LLM/\"AI\" can reliably No. LLMs definitely have uses where reliability is not a requirement, but that's one requirement which LLMs clearly never meet reply antics 8 hours agoparentprevHi Joseph! I am sorry, I was not trying to say your work sucks. I was trying to (1) help practitioners understand what they can expect, and (2) motivate problems like the one you mention at the end. (1) might seem stupid but I think just evaluating these systems is a challenging enough technical problem that many teams will struggle with it. I just think they deserve practical advice—I know we would have appreciated it earlier on. reply satvikpendem 14 hours agoparentprevI mentioned Loro in another comment which uses EG Walker, do you think they are an example of what you had mentioned? This comment also seems relevant [0]. Regarding your [1], I had a similar idea and I am beginning to think that only something like an LLM or similar can truly merge conflict free because only they are powerful enough to understand users' intent. [0] https://news.ycombinator.com/item?id=42343953#42344880 reply josephg 13 hours agorootparentDoes Loro generate conflict ranges when merging branches, thus allowing manual conflict resolution? I’ve heard people suggest using LLMs for years for this - but CRDTs work because the same computation on two peers is guaranteed to produce the same result. LLMs can’t guarantee that. You could probably use an llm in leu of a human manually merging conflicts - but in that case we still need the crdt to first generate those conflict ranges to pass to the llm. Essentially an llm could solve the UX problem, but the underlying algorithm still needs this feature first to allow that to be used. reply dmurray 7 hours agorootparent> but CRDTs work because the same computation on two peers is guaranteed to produce the same result. LLMs can’t guarantee that. This doesn't seem like a hard requirement. It's a nice property for some academically interesting peer-to-peer trustless algorithm, but in typical software architectures you won't get a situation where Alice receives Bob's edits and Bob receives Alice's edits without them passing through a centralised server that can reconcile them first. In any case, LLMs can be run deterministically (temperature=0, or use a fixed random seed and a single core where necessary). I don't expect further serious work in the offline text editing space. The next generation will certainly be some form of \"guess, then ask an LLM to guess\" reply mkl 10 hours agorootparentprevIt seems that they might, but perhaps not in a good way yet, and they have it as a goal. From https://loro.dev/blog/v1.0#next-steps-for-loro: > When merging extensive concurrent edits, CRDTs can automatically merge changes, but the result may not always meet expectations. Fortunately, Loro stores the complete editing history. This allows us to offer Git-like manual conflict resolution at the application layer when needed. From https://loro.dev/docs/tutorial/time_travel: > once we have a more refined version control API in place. I'm interested to hear your opinion of their partial adoption of Eg-walker: https://loro.dev/blog/v1.0#leveraging-the-potential-of-the-e... reply sgarland 4 hours agoprevThe other dark side of implementations using CRDTs is the infrastructure load. I wrote about this [0] in depth previously, and Supabase wrote an article [1] a couple of years ago about a CRDT extension for Postgres which I'm happy to discover agrees with my empirical findings. If you're going to use CRDTs, do yourself a favor and either use Redis or similar (though the amount of memory being consumed is painful to think about), or MyRocks [2] (or anything else based on RocksDB / LevelDB). Whatever you do, do _not_ back it with an RDBMS, and especially not Postgres. [0]: https://news.ycombinator.com/item?id=40834759 [1]: https://supabase.com/blog/postgres-crdt [2]: http://myrocks.io reply keizo 1 hour agoprevI implemented differential sync (https://neil.fraser.name/writing/sync/) mostly because I couldn’t understand anything else and seemed simplest in my grugnotes.com app -- and while the app is pretty janky and not fully real-time, it does get your example merge right regardless of who comes back online first. In the case the deletion comes online first, the 'colour' version is thrown out and not saved in edit history. I’m sure there’s a lot more wrong with it, have no idea what would happen with more than two users, but for my case I’m happy with it. :) reply pvh 21 hours agoprevMechanical merge algorithms can perform better or worse on different kinds of conflicts (the specific example of editing deleted text is just one of many edge cases) but in the end no CRDT can decide if your merged text is what you mean to say. We go into a bunch more detail in the Upwelling paper about the differences between (what we call) semantic and syntactic conflicts in writing: https://inkandswitch.com/upwelling/ Ultimately, my feeling is that serious collaboration is a document review problem as much as anything else. That said, this is particularly true in journalism and scientific publishing and can be mostly ignored for your meeting notes... Anyway, if you see this comment, thanks for a nice piece of writing, Alex. Love to see folks wrestling with these problems. reply antics 21 hours agoparentHi Peter! Thanks so much for the kind words. I hope you noticed that a lot of the article ends up being a motivation for Ink & Switch's work, which we call out directly at the end. I am a big fan! :) EDIT: Oh, also I meant to link to Upwelling, but forgot what it was called. I settled for a different link instead because it was deadline. reply MazeChaZer 9 hours agoprevThe observation of this article is spot on! CRDTs are an awesome formal model for distributed data structures, but I was always bothered by the notion that all conflicts must be resolved automatically (hence also the name, conflict-free replicated data type). As the article illustrated, this is a hopeless endeavor. I believe what is needed is a proper structured representation of conflicts, that allows for sharing them and resolving them collaboratively, giving back control to the users and supporting them in the process. One of my favorite papers “Turning Conflicts into Collaboration” [1] makes a compelling argument for this idea. As part of my ongoing PhD studies, we have developed our own formal model for structured conflict representation, based on lattice theory: “Lazy Merging: From a Potential of Universes to a Universe of Potentials” [2]. Incidentally, it is also a CRDT, but it does not attempt to resolve conflicts automatically. Instead, it represents them within the collaborative documents. Approaching the problem from mathematics allowed us to arrive at a simple conceptual model that can guarantee strong properties, like, e.g., the completeness, minimality, and uniqueness of merges, even after repeated merges of existing conflicts. And merges can be calculates very easily. I always wanted to make a blog post about it, but I never came around to do it. [1] https://doi.org/10.1007/s10606-012-9172-4 [2] https://doi.org/10.14279/tuj.eceasst.82.1226 reply crabmusket 6 hours agoparentThanks for those citations, they look really interesting! > hence also the name, conflict-free replicated data type When I was introduced to CRDTs the acronym stood for commutative replicated data types (eg in the paper by Shapiro et al). I prefer this actually, despite it being harder to pronounce. A conflict is a complicated idea, and while \"conflict free\" is a technically correct way of describing the result, it can be misleading as evidenced by this post and your comment. Commutativity is the property that when Bob applies changes in the order [Bob, Alice] and Alice applies changes in the order [Alice, Bob] that they both end up with the same document. It doesn't imply that the document is somehow \"free\" of \"conflicts\" in a sense that may be meaningful at a higher level of abstraction. reply zamalek 22 hours agoprevI think this happens because the mathematical, or causal, or entropic, notion of conflicts has been conflated with semantic conflicts. In the past I have made the same mistake, though inversely and was adamantly informed that I had no clue what I was talking about :) Things get way nastier when you start considering trees, e.g. yJS operates on JSON documents. From a UI standpoint (where the UI is showing some shallow level and hasn't been expanded to the deeper level) users could never even see edits that have been deleted. I think that the class of CRDTs that preserve conflicts (IIRC that is when a register can hold multiple values) hold the most promise. Users should then be presented with those conflicts - and it could even be completely visual. Being able to scrub through history also seems like a viable alternative (allowing the user to figure out how a strange thing happened, or how their changes disappeared). reply Onavo 21 hours agoparent> Users should then be presented with those conflicts - and it could even be completely visual. Being able to scrub through history also seems like a viable alternative I think \"Git\" would be a wonderful name for this type of CRDT. reply Etheryte 19 hours agorootparentFor all things Git is good at, conflict resolution is most definitely not one of them. There are many ways to slice a conflicting diff into plus and minus parts, and somehow Git usually manages to create conflicts that are the least human readable. We live in an era of widely adopted and universal language servers, yet Git pretends that every change is just a char added or removed. There are many tools out there which do a considerably better job at this, for example by diffing the AST, not the text representation. reply Wowfunhappy 18 hours agorootparentHi, I'm a hobbyist coder with lots of personal forks of open source projects. I frequently spend a lot of time merging in changes from upstream. Can you suggest tools that would help with this? reply CGamesPlay 12 hours agorootparenthttps://mergiraf.org/ reply satvikpendem 21 hours agorootparentprevI mentioned Loro in another comment, but they actually do conflict resolution on top of Git trees [0]. Jujutsu is also interesting but I'm not sure if they do any conflict resolution [1]. [0] https://loro.dev/blog/v1.0#loro-version-controller [1] https://github.com/martinvonz/jj reply drdaeman 21 hours agorootparentprevMore like Pijul or Darcs? Git is popular, but it's not particularly great at conflict resolution. reply mweidner 19 hours agoprevOne challenge is that the algorithms typically used for collaborative text editing (CRDTs and OT) have strict algebraic requirements for what the edit operations do & how they interact. So even if your server is smart enough to process the \"Colour\" example in a UX-reasonable way, it's very hard to design a corresponding CRDT/OT, for optimistic client-side edits. You can work around this by not using CRDTs/OT. E.g., the server processes operations in receipt order, applying whatever UX logic it wants; clients use a rebase/prediction strategy to still allow optimistic edits on top (cf. https://doc.replicache.dev/concepts/how-it-works). Doing this for text editing has some challenges, but they're separate from the CRDT/OT challenges discussed here. reply antics 8 hours agoparentAuthor here! Just chiming in to say this is a very underrated comment which I fully cosign. :) reply jFriedensreich 1 hour agoprevTotally agree with this article, and this is about collaborative text editing, which is something these solutions and algorithms are actually good at. Looking at everything from crdt fanboys applying this to application data and ignoring the UI/UX aspects about conflict resolution makes my neck hair stand up. This is how we end up with linear giving talks about their \"sync engine\" and happily deleting other users changes with last write wins. reply dietr1ch 3 hours agoprevI'm angry that I guessed right. I really don't like how flat the model for text generally is. I don't see text as a [Char], so if the computer does there's a hidden mismatch that people have grown to work around and with (just like searching on the Internet before 2005 felt. You had to know what was gonna work and then got the right to claim how easy it was) We structure text in an implicit hierarchy given by spacing, margins and a bunch of other subtle things that [Char] doesn't capture, but can encode for other humans. I think that this is were all the problems stem from, and that with right (tm) structure a lot of the operations can be merged trivially with way fewer surprises. Easier said that done for sure, and there will be more weird cases, but I'm guessing they will get closer to being just the conflicts that really need authors to deal with. reply leoc 17 hours agoprev> In 2009, a surprising amount of the discourse was focused on the algorithms git used to automatically merge changes together. IIRC Torvalds himself was quite pessimistic about what could be achieved with automatic merging. (And in this he was correct.) He said that Git had rejected the idea that a version-control system could, or should attempt to, \"solve the merging problem\" by finding a sufficiently-smart algorithm which would do the right things automatically. > Offline editing is a UI/UX problem True. There are two deeper root causes here: 1) computing's commitment to cargo-culting old solutions and, relatedly, 2) its devotion to the belief \"in general 5-lb sacks are nicer to deal with than 10-lb sacks: therefore I should fit my 10 lbs of POL into one 5-lb sack\". The default vision of \"text editor\" is \"Mosaic textarea\", \"MacWrite\" or something in between, so the quest is usually to bolt merging onto something like that with the minimum possible change. Make it a menu item, or a little dialog box with a few options. If there is some kind of GUI support for merging hidden deep in the menus it's a programmer-UI diff-merger horror that barely does the minimum, or a strikethrough-based view which feels treacherous like navigating a ship through a fog. But in fact in text editing with offline collaboration merging, partly-manual merging at that, is a central part of the process and it needs to be central to the design of the editor. Unfortunately MacWrite is a local maximum which isn't easy to get far away from. reply theamk 15 hours agoparentwhat are the alternatives, though? For example, often when someone mentions \"cargo-culting\" and \"old solutions\", their next words are \"stop editing code as text, edit as a syntax tree\". But it has the same problem, just replace \"character\" with \"statement\"! Bob added a line to \"else\" branch of \"if\" statement, Alice deleted the entire statement, along with \"else\" branch - what is the smart system to do? reply swiftcoder 9 hours agorootparentI don't think the syntax tree approach is a silver bullet, but it does vastly reduce the kind of annoying things that trip up git merges today. Issues like whitespace conflicts, formatting clashes, reordering of import statements... Though we seem to have collectively decided it's easier to solve this by forcing everyone to adopt the same auto formatter configs, which, I guess fair enough reply antics 9 hours agoparentprev[Author here] I am sorry, I think I phrased the automatic merging point confusingly. I was trying to say that when multiple commits change a file, git will attempt to merge them together, but it MUST flag direct conflicts. Sounds like we agree this is the right approach overall though. reply Rygian 22 hours agoprevI would wager that, in general, supporting the notion that several different entities are all the authority over a piece of data simultaneously and without live coordination is not solvable. This is a learned lesson for distributed systems, and is readily apparent in the article when considering distributed editing of documents. Same goes for dual input in flight cabins, parenting, and probably any other disparate example one can think of. reply beefnugs 21 hours agoparentIt is solvable, but needs more complicated contextual information that many people would not want to bother entering : \"this word i just changed only makes sense if it is apart of this whole sentence, which is not necessarily required for the whole paragraph...\" And calling this \"solvable\" is a funny thing to think about, since huge portions of the earth think the chaos output of LLMs could be anywhere near deciding the final output of computation at this point in time reply Rygian 5 hours agorootparent\"It is solvable\" is equivalent to \"politics are not necessary\", which I can't agree with. Agreeing on edits requires a shared context, a shared understanding of the goal and requirements of the text being edited, and a shared projection of how readers should understand the text and how they will understand it instead. The specific contextual information required for automated editing is dependent on the combined situation of all writers, considering their professional, personal and cultural contexts. Assuming that context can't be made available in a systematized way, the machine will choose an edit that is not guaranteed to match the intentions and expectations of the people involved. Instead, it will just act as adding one more writer to the mix. reply theamk 15 hours agorootparentprevheh, I bet no matter what kind of textual explanation you required, I can provide the situation it does not cover. You say this word is only required if it's a part of this whole sentence? OK, the other edit kept the whole sentence, but changed a single other word in it, which happened to be the subject. reply eastern 15 hours agorootparent100%. The given situation is solvable only by the humans involved. They want different things. Either one of them has authority over the other, or they talk it over. reply fsckboy 15 hours agoparentprevBrewer's CAP theorem of distributed data storage. you can have two out of these three: 1. Consistency 2. Availability 3. Partition tolerance reply bvrmn 43 minutes agoprevIt's ironic how js-experts could not make examples work on the article page. reply yaweezy 5 hours agoprevReally enjoyed reading the post and the comments. And it’s good to see a general consensus on this point. I had a short stint at a legaltech startup and began putting together a framework for resolving drafting edits/conflicts. In the legal context, it was more critical that the user actively signed off on a change to avoid random errors, so my thought was to implement a UI/UX solution to review each change similar to Microsoft Word’s spelling and grammar check. There were some other options I proposed, like showing multiple ways that the change could be implemented. The team decided to focus on solving easier problems and the startup folded not too long after that. But it was a really fun exercise. Glad to see folks are still thinking deeply about it. reply eps 3 hours agoprevOP, your site's scrolling is completely broken on an older iPad. Just can't scroll the page at all. It shows only first screenful. Pulling it up just shows whitespace below and bounces back. Reader mode shows the whole page, but then all formatting is gone, so it's also unreadable. Just FYI. reply antics 22 hours agoprevHi folks! Author here. Happy to answer questions or take feedback. I'll be in meetings for an hour or two but I love talking about this stuff. :) Here or over email if you prefer, alex@moment.dev reply jakevoytko 20 hours agoparentIf you keep the offline support, you'll eventually uncover even more fun cases. \"I started working on this on an airplane where the wifi was down. But then decided I didn't like the direction it was going and just closed the laptop and took a nap. I spent the next few days working on the document on my desktop. Over the weekend I opened the doc on my laptop and now all of my airplane changes are in the doc and everything is garbled. Help, I didn't mean to merge them!\" Git would never automatically mash your local changes in without your explicit consent. bzr would never have dreamed of it. But things like Google Docs will happily do it. It's awesome to see all the progress y'all have made! Good luck with early access! reply antics 9 hours agorootparentHi Jake! Long time no chat! I hope everything is going well. Yep, those are exactly the kinds of cases we seek to give people more control over. It's like you read my mind. :) reply mike_hearn 20 hours agoparentprevWhat's the problem with just adopting patch/diff style merging? I mean, offline collaborative text editing is a solved problem for decades if you're going to phrase it as just a UX optimization problem. reply antics 9 hours agorootparentI'm not sure I understand the question fully, but yes, I am a big fan of the diff/review/patch-style tools like git for offline editing, and that is exactly why I call them out in the final paragraphs. The reason I don't agree that it's completely closed-book solved is because I think the git diff UI is not appropriate for most people whoa re not developers. reply mike_hearn 5 hours agorootparentNot the exact git GUI, but bear in mind Word has supported offline edit/merge for decades and it's done in a similar way (you see diffs visually and can accept/reject them). Lawyers depend on that workflow heavily. reply Gehinnn 8 hours agoprev> I think the primary change is that these debates were focused on producing diffs that humans read; now the debate is whether the algorithms can accomplish this result with no human involvement at all. The better the diff captures the intent of the user change, the easier the diff is to read, AND it also becomes easier to understand merge conflicts, as the conflict becomes a conflict of intents and not characters. Sometimes certain conflicts can even be avoided if the diffs are chosen right (or are expressive enough, e.g. when code moves can be detected). reply satvikpendem 21 hours agoprevI had an interest in CRDTs for quite a while now, as I like the local first philosophy of developing software, that works offline in its entirely but can also work online, a sort of progressive enhancement [0]. Recently I've been looking into Loro [1] which seems like it is able to effectively merge disparate offline text edits together, by using some new algorithms that were written about last year, such as the Event Graph Walker [2]. I've been combining this with ElectricSQL [3], which is a sync engine for Postgres. In the past they had their own CRDT right inside Postgres which would then sync tables, but they have rewritten their product to focus primarily on being a sync engine first and perhaps a conflict resolution library second. Therefore, I store the Loro changes binary in a table in Postgres as a blob that I then sync via Electric to all my clients. Ultimately though, it is as you and others like @zamalek in this thread have said, the mathematical notion of conflict resolution might not actually mean anything semantically, therefore it is difficult to have long running offline edits merge together cohesively; it works with things like Google Docs where the user can see what other users have written in real time, which works for 99% of use cases, and sometimes I wonder whether one really needs such long running offline syncs, as it seems to be a very niche goal to target. Short running offline is nice to have and even necessary, especially for products one expects to work wholly offline, but it is the long term I don't see much value in, as in not collaborating online for weeks or months at a time but still expecting cohesive edit merges. [0] https://localfirstweb.dev/ [1] https://loro.dev/ [2] https://loro.dev/docs/advanced/event_graph_walker [3] https://electric-sql.com/ reply refulgentis 21 hours agoparentAlas, Loro fails The Color of Pomegranates test as well. (JSON trace, really cool toy they got there: https://pastebin.com/6dSDc6Su) reply satvikpendem 21 hours agorootparentYes, it is as I mentioned in my 2nd paragraph, mathematical conflict resolution is not semantically relevant for humans much of the time. That is why I don't think automated merge conflict resolution with no human input can really work, that is why Git asks you to resolve merge conflicts before committing again. CRDTs can only really help when users edit disparate pieces of data, and if editing the same piece of data, some set of users need to be there, as in an online rather than offline capacity, to facilitate the merges, such as in Google Docs where if I edit a sentence with a spelling correction and you delete the sentence entirely, I will ask you directly what's up. Now, what some Git merge software is doing is using LLMs to coordinate merges, which I think might be useful to do so in plain text editing too, perhaps embedded into these CRDT libraries, but fundamentally, there must be someone to facilitate merges, whether it be a human agent or an AI one. It is impossible to do so with an entirely automated solution simply because the machine does not know the intents of all its users. reply williamstein 21 hours agorootparentprevThe text sync algorithm I wrote succeeds at the \"The Color of Pomegranates\" test. See https://cocalc.com/wstein/dev/share/files/2024-12-06.md for some details of the exact patches. This algorithm, which we came up with in 2015, is described at https://blog.cocalc.com/2018/10/11/collaborative-editing.htm... It's a different approach than that taken by Yjs, Automerge, and Egwalker, with signficant pros (and cons!) relative to those algorithms. It has been used in production in CoCalc for over 8 years, and was also used by the recently shutdown Noteable Jupyter notebook project. reply refulgentis 18 hours agorootparentThis is absolutely fascinating and immediately one of my favorite articles of all time, full stop. Along the way, it implicitly illustrates a path to successful engineering x product thinking path that I've never seen written down somewhere and I think is crucial for sustainable impact. Thank you! reply major4x 21 hours agoprevI love this mini article, however, I disagree with the main conclusion that collaborative editing is not an algorithmic but a UI/UX problem. I think that collaborative editing is a semantic problem. To the best of my knowledge (I'm writing this comment without much preparation), all SVN/Git algorithms are based on UNIX diff (Hunt–Szymanski algorithm). UNIX diff (and patch) is purely syntax driven. Actually, I will make a small deviation here: I think it is a big industry/startup/open source project there, in creating a set of semantic diff algorithms/implementations. For example, due to my present job, I am very interested in collaborative editing of electrical circuits, and layouts for PCB and chips. Altium and KiCad are trying, for exmaple, to store everything in XML/text files and put the text files in Git/SVN and I can tell you a botched C++ program is nothing in comparison to a botched and malformed electrical circuit. So we need diff tools that \"know\" about a text file, vs rich-text with formatting, vs bitmap vs vector image, vs song, vs English text. Anybody want to start an open source project (DM me or put a comment here). Anyhow, thanks to the authors on the great insights and let's work on the take home! reply miltonlost 19 hours agoprevAI trying to figure out user intention and incorrectly so due to their heuristics or inherent missing context in merges, is why autocorrect has become almost more trouble than it's worth these days. Texts on phones are impossible to start with \"well\" or \"we'll\" as it will replace with the other before the second word. Algorithms need to stop saying that what they find more likely must be true; \"more\" likely is less useful when there's a 51/49% chance or a 4/3/3/3/3/3/3/3/3/3...3% chance. Sometimes the AI will be right; sometimes it will be wrong. I'm far more upset when its wrong than the few times it's right. The same problems with offline editing will raise their head as AI forces its way into our lives attempting to make decisions based on not nearly enough info. Basic symbol or character-by-character manipulation cannot reveal why the changes occurred, just what happened. reply theamk 15 hours agoparentTurning off automatic autocorrect is one of the first thing I do when i get a new phone. Sure, show me suggestions, but let me decide myself if I want them or not! reply taeric 19 hours agoprevMy rephrasing of the problem on coordination-free algorithms is that collaboration is largely a subset of correspondence. For some reason, so many of these discussions try to treat it as a completely separate thing. This invariably results in longer feedback loops for those corresponding with no direct communication of what, specifically, they have done. reply robertclaus 21 hours agoprevThe most surprising part of this article for someone uninitiated like myself is probably that products/algorithms are claiming this automatic reconciliation is consistently possible. Maybe I've spent too much time resolving code merge conflicts by hand, but this seems intuitively obvious to me... Feels like https://xkcd.com/1831 reply williamstein 21 hours agoparentWhat they claim is that if all editing stops, then after a period of time everybody will be looking at the SAME document. This is what is meant by \"eventual consistency\". Achieving this in general is indeed a difficult problem, but (some of) these algorithms do solve it, though it can be tricky to correctly prove that they do. I agree that it is not possible to ensure that the document everybody is looking at is what they actually wanted it to be. However, there are some options for what happens, where some results may be technically correct -- we are all looking at the same thing -- but obviously really bad. This beautiful talk has some examples: https://www.youtube.com/watch?v=x7drE24geUw reply robertclaus 19 hours agorootparentIsn't eventual consistency theoretically trivial...? You just delete the whole document after every transaction. I think it's safe to assume anyone talking about this _means_ the results remain meaningful - which for text/meaning is subjective. reply williamstein 14 hours agorootparentProving that there exists an algorithm that results in an eventually consistent view of the document is trivial. However, that's not what we're talking about. Instead, researchers define a specific algorithm (e.g., involving CRDT's or OT's or something else), then prove that their algorithm results in an eventually consistent state. This reminds me a little of the relationship between proving that there exists an algorithm to factor all positive integers (this is trivial) and proving there exists a subexponential time algorithm to factor integers, which is much less trivial (see https://en.wikipedia.org/wiki/Lenstra_elliptic-curve_factori...). reply klntsky 19 hours agoprevLooks like a good use case for LLMs! https://chatgpt.com/share/67538beb-73e8-800a-b602-fe26b131e5... HMU if interested in building it reply antics 8 hours agoparentAuthor here! We have experimented with building this, see my comment elsewhere here: https://news.ycombinator.com/item?id=42348651 One of the surprising things is that LLMs regularly \"fix\" things that no other system can fix. Like if we both add the same sentence to a doc. It's interesting stuff. With that said I am not sure that this specific LLM is providing the \"right\" answer. It seems like AN answer! But I think the real solution might be to ask the user what to do. reply sausagefeet 7 hours agoparentprevTake what you wrote and flip it and you get a different answer: https://chatgpt.com/share/67542c95-fea8-8008-8749-7b7daf355c... Here is the resulting semantically meaningful diff that reconciles the edits: - The color of orange is orange + The colour of orange is orange This reflects the change in spelling from \"color\" to \"colour,\" which is the only meaningful difference between the two edits. reply mkl 15 hours agoparentprevHardly. That just threw away Edit1, failing to follow instructions even in this very simple case. reply tags2k 9 hours agoparentprevo1 gets the original article question correct: https://chatgpt.com/share/675410c1-22c4-8001-b36a-24425127cc... reply ramon156 21 hours agoprevJust use git (: (Half joking) reply antics 8 hours agoparentAuthor here! I think you're going to like one of the upcoming posts in this series. :) reply smoyer 16 hours agoprevI miss Google Wave. reply worldsayshi 16 hours agoparentOh! Now I remember the name of the tool that came before Google wave and (I think) inspired it. Etherpad. reply bee_rider 21 hours agoprevThe example seems like it would be easier if we’d gone in the direction of allowing more complex commands, like vim does. Imagine if real editors had been developed for the last 30 years or however long, instead of stagnating at vim (which is clearly a nice text editor, but it could be nice to have an editor designed around writing prose). Maybe neovim will save us. Some day. Bob’s intent is to edit the word color, and inserting a u. But, he is limited to just expressing “put u here,” which is not at all what he wants to achieve it is just a mechanical description of what operations need to occur. Alice’s intent is to delete the whole sentence, but she’s similarly limited to just saying “delete delete deleted delete delete…” to a bunch of letters. Ending up with a u is the obvious dumb result of treating language as a pile of characters. The correct behavior is to say: because the world Bob has edited no longer exists, his edit is clearly nonsense, don’t apply it. Which editor does that? reply Rygian 21 hours agoparentConsidering that software can only guess the intent if it's not declared explicitly, I'm curious what such an \"intent declaration\" language would look like. reply bee_rider 5 minutes agorootparents/color/colour/g reply satvikpendem 21 hours agorootparentprevAt the limit, it would probably look like an LLM, because it's akin to rules-based AI in the '90s vs neural network AI today. Expert systems had programmers write many rules in order to process information, which is what this \"intent declaration\" language would also be like, users writing many rules that would be followed. But this approach didn't work because even humans didn't even know all of the rules needed, therefore we turned to the statistical approaches of current neural network AI. reply Rygian 5 hours agorootparentWe are aligned. Software cannot identify intentions. Heck, even person A cannot identify intentions of person B in a systematic way. Hence it's a non-solvable problem. reply antirez 20 hours agoprevNow that there are LLMs available, why is this still a problem? You just have to detect the conflicts and show a powerful enough LLM the two versions and tell it to do its best job at merging. This solves exactly the kind of issue described in the post. Oh, bonus point: you don't have to take any metadata. reply antics 8 hours agoparentAuthor here! This comment is kind of getting dragged here and elsewhere but I actually think it's not completely ridiculous. You can (and we have) presented git-style merge conflicts to an LLM and it will mostly fix them in ways that no algorithm can. One example of this is if you and I both add a similar sentence in different spots in the document, asking an LLM to merge this will often result in only one of the sentences being accepted. It's not perfect but it's the kind of thing you can't get in any other technology! With that all said I don't think LLMs REPLACE merge algorithms. For one, to get sensible output from the LLMs you generally need a diff of some kind, either git style or as the trace ouput of something like eg-walker. reply miltonlost 18 hours agoparentprev> You just have to detect the conflicts and show a powerful enough LLM the two versions and tell it to do its best job at merging. This solves exactly the kind of issue described in the post. How does \"LLM tries its best\" solve the problem of exactly syncing documents offline? \"Tries its best\" implies it could fail which is what the problem already is. An LLM only adds a new layer of abstraction, but now the downside is it's impossible to analyze. Maybe I just have a much higher threshold of \"solved\" than you do, but anything non-deterministic (without some human judgment a la git) is not a good solve for document syncing. A \"powerful enough LLM\" is the equivalent of \"and a wizard does it\". reply antirez 18 hours agorootparentLLMs at t=0 are predictable (will produce the same output starting from the same input) and do a much better merge work than any other non ML based algorithm. Cut & paste all the examples you can come up to Claude and tell me if LLMs are not able to do this kind of merging. reply jakelazaroff 17 hours agoparentprevIf there are only two conflicting updates, sure, maybe that would work. But there might be more than two peers, and their updates might arrive in any order. That's why CRDT merges must be both commutative and associative. Can we guarantee that an LLM will get exactly the same result merging (A ∨ B) ∨ C as it would merging A ∨ (B ∨ C)? Even when the temperature is 0? reply mweidner 14 hours agorootparentYou could avoid the CRDT rules if you only use the LLM on the server. I.e., user comes online and sends their diff to the server, which LLM-merges it into the latest state and then sends that back to all clients. This doesn't help you do merges client-side during live collaboration (for showing your optimistic local updates), but there the low latency reduces conflicts anyway, so you can fall back on a semantically-imperfect CRDT. reply satvikpendem 14 hours agorootparentIf you have a central server, you don't need CRDTs, which are designed to work even in pure peer-to-peer scenarios. Figma is one example of this [0]: > Figma isn't using true CRDTs though. CRDTs are designed for decentralized systems where there is no single central authority to decide what the final state should be. There is some unavoidable performance and memory overhead with doing this. Since Figma is centralized (our server is the central authority), we can simplify our system by removing this extra overhead and benefit from a faster and leaner implementation. > It’s also worth noting that Figma's data structure isn't a single CRDT. Instead it's inspired by multiple separate CRDTs and uses them in combination to create the final data structure that represents a Figma document (described below). [0] https://www.figma.com/blog/how-figmas-multiplayer-technology... reply satvikpendem 14 hours agoparentprevThat's exactly how I see it too, true intelligent merging algorithms are, at the limit, going to be basically LLMs because only something that powerful can understand user intent in a way that non-ML algorithms cannot. reply refulgentis 21 hours agoprev [–] This is so, so, sooo good. I've never been brave enough to say it out loud, but this is 110% my experience. I imagine it is somewhat a consequence of the divide between engineering and product. It can resolve all conflicts, and it's such a holy grail to go decentralized / get Google Docs-like editing down to a a package in your favorite language. But, in practice, its intractable for arbitrary data, even just an arbitrary string. I do wish there was a formal proof that showed / proved this to share in HN discussions re: CRDTs...but hey, this is great! The \"it left a u\" example is simple, intuitive, and with a charitable listener, I doubt they'd argue we can't figure out a string unambiguously, but we can figure out JSON unmabiguously. reply antics 8 hours agoparent [–] Author here, thanks for the kind words! I think one reason we ended up here is that it is a genuinely difficult technical problem even to analyze the solutions. One of my hopes for this series of posts is that it makes the evaluation process more straightforward, particularly for people who do not have a strong background in distributed systems algorithms. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In early 2024, research into collaborative editing systems for Moment's text editor highlighted challenges with offline editing, where popular algorithms like CRDTs (Conflict-free Replicated Data Types) and OT (Operational Transformation) often fail.- Offline editing increases direct conflicts, with 20-30% of these conflicts resulting in unacceptable outcomes, suggesting the issue is more of a user interface/user experience (UI/UX) challenge than an algorithmic one.- Researchers are now focusing on improving the user experience in offline editing, drawing parallels to how git handles document merging, which is a promising development."
    ],
    "commentSummary": [
      "The article addresses the challenges of collaborative editing, especially offline, and critiques current algorithms like Conflict-free Replicated Data Types (CRDTs) and Operational Transformation (OT) for not always yielding semantically meaningful results.- There is a consensus on the need for improved conflict resolution, potentially through user input or advanced tools like Large Language Models (LLMs), to handle semantic conflicts effectively.- The discussion also considers the role of User Interface/User Experience (UI/UX) solutions in enhancing the collaborative editing experience, highlighting that the issue extends beyond algorithms to understanding user intent."
    ],
    "points": 243,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1733516527
  },
  {
    "id": 42346344,
    "title": "Structured Outputs with Ollama",
    "originLink": "https://ollama.com/blog/structured-outputs",
    "originBody": "Structured outputs December 6, 2024 Ollama now supports structured outputs making it possible to constrain a model’s output to a specific format defined by a JSON schema. The Ollama Python and JavaScript libraries have been updated to support structured outputs. Use cases for structured outputs include: Parsing data from documents Extracting data from images Structuring all language model responses More reliability and consistency than JSON mode Get started Download the latest version of Ollama Upgrade to the latest version of the Ollama Python or JavaScript library: Python pip install -U ollama JavaScript npm i ollama To pass structured outputs to the model, the format parameter can be used in the cURL request or the format parameter in the Python or JavaScript libraries. cURL curl -X POST http://localhost:11434/api/chat -H \"Content-Type: application/json\" -d '{ \"model\": \"llama3.1\", \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about Canada.\"}], \"stream\": false, \"format\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" }, \"capital\": { \"type\": \"string\" }, \"languages\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } } }, \"required\": [ \"name\", \"capital\", \"languages\" ] } }' Output The response is returned in the format defined by the JSON schema in the request. { \"capital\": \"Ottawa\", \"languages\": [ \"English\", \"French\" ], \"name\": \"Canada\" } Python Using the Ollama Python library, pass in the schema as a JSON object to the format parameter as either dict or use Pydantic (recommended) to serialize the schema using model_json_schema(). from ollama import chat from pydantic import BaseModel class Country(BaseModel): name: str capital: str languages: list[str] response = chat( messages=[ { 'role': 'user', 'content': 'Tell me about Canada.', } ], model='llama3.1', format=Country.model_json_schema(), ) country = Country.model_validate_json(response.message.content) print(country) Output name='Canada' capital='Ottawa' languages=['English', 'French'] JavaScript Using the Ollama JavaScript library, pass in the schema as a JSON object to the format parameter as either object or use Zod (recommended) to serialize the schema using zodToJsonSchema(). import ollama from 'ollama'; import { z } from 'zod'; import { zodToJsonSchema } from 'zod-to-json-schema'; const Country = z.object({ name: z.string(), capital: z.string(), languages: z.array(z.string()), }); const response = await ollama.chat({ model: 'llama3.1', messages: [{ role: 'user', content: 'Tell me about Canada.' }], format: zodToJsonSchema(Country), }); const country = Country.parse(JSON.parse(response.message.content)); console.log(country); Output { name: \"Canada\", capital: \"Ottawa\", languages: [ \"English\", \"French\" ], } Examples Data extraction To extract structured data from text, define a schema to represent information. The model then extracts the information and returns the data in the defined schema as JSON: from ollama import chat from pydantic import BaseModel class Pet(BaseModel): name: str animal: str age: int color: strNone favorite_toy: strNone class PetList(BaseModel): pets: list[Pet] response = chat( messages=[ { 'role': 'user', 'content': ''' I have two pets. A cat named Luna who is 5 years old and loves playing with yarn. She has grey fur. I also have a 2 year old black cat named Loki who loves tennis balls. ''', } ], model='llama3.1', format=PetList.model_json_schema(), ) pets = PetList.model_validate_json(response.message.content) print(pets) Example output pets=[ Pet(name='Luna', animal='cat', age=5, color='grey', favorite_toy='yarn'), Pet(name='Loki', animal='cat', age=2, color='black', favorite_toy='tennis balls') ] Image description Structured outputs can also be used with vision models. For example, the following code uses llama3.2-vision to describe the following image and returns a structured output: from ollama import chat from pydantic import BaseModel class Object(BaseModel): name: str confidence: float attributes: str class ImageDescription(BaseModel): summary: str objects: List[Object] scene: str colors: List[str] time_of_day: Literal['Morning', 'Afternoon', 'Evening', 'Night'] setting: Literal['Indoor', 'Outdoor', 'Unknown'] text_content: Optional[str] = None path = 'path/to/image.jpg' response = chat( model='llama3.2-vision', format=ImageDescription.model_json_schema(), # Pass in the schema for the response messages=[ { 'role': 'user', 'content': 'Analyze this image and describe what you see, including any objects, the scene, colors and any text you can detect.', 'images': [path], }, ], options={'temperature': 0}, # Set temperature to 0 for more deterministic output ) image_description = ImageDescription.model_validate_json(response.message.content) print(image_description) Example output summary='A palm tree on a sandy beach with blue water and sky.' objects=[ Object(name='tree', confidence=0.9, attributes='palm tree'), Object(name='beach', confidence=1.0, attributes='sand') ], scene='beach', colors=['blue', 'green', 'white'], time_of_day='Afternoon' setting='Outdoor' text_content=None OpenAI compatibility from openai import OpenAI import openai from pydantic import BaseModel client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\") class Pet(BaseModel): name: str animal: str age: int color: strNone favorite_toy: strNone class PetList(BaseModel): pets: list[Pet] try: completion = client.beta.chat.completions.parse( temperature=0, model=\"llama3.1:8b\", messages=[ {\"role\": \"user\", \"content\": ''' I have two pets. A cat named Luna who is 5 years old and loves playing with yarn. She has grey fur. I also have a 2 year old black cat named Loki who loves tennis balls. '''} ], response_format=PetList, ) pet_response = completion.choices[0].message if pet_response.parsed: print(pet_response.parsed) elif pet_response.refusal: print(pet_response.refusal) except Exception as e: if type(e) == openai.LengthFinishReasonError: print(\"Too many tokens: \", e) pass else: print(e) pass Tips For reliable use of structured outputs, consider to: Use Pydantic (Python) or Zod (JavaScript) to define the schema for the response Add “return as JSON” to the prompt to help the model understand the request Set the temperature to 0 for more deterministic output What’s next? Exposing logits for controlled generation Performance and accuracy improvements for structured outputs GPU acceleration for sampling Additional format support beyond JSON schema",
    "commentLink": "https://news.ycombinator.com/item?id=42346344",
    "commentBody": "Structured Outputs with Ollama (ollama.com)223 points by Patrick_Devine 17 hours agohidepastfavorite62 comments rdescartes 15 hours agoIf anyone needs a more powerful constrain outputs, llama.cpp support gbnf: https://github.com/ggerganov/llama.cpp/blob/master/grammars/... reply lolinder 3 hours agoparentHave you found the output for arbitrary grammars to be satisfactory? My naive assumption has been that these models will produce better JSON than other formats simply by virtue of having seen so much of it. reply rdescartes 2 hours agorootparentIf you want to get a good result, the grammar should be following the expect output from the prompt, especially if you use a small model. Normally I would manually fine-tune the prompt to output the grammar format first, and then apply the grammar in production. reply throwaway314155 2 hours agorootparentprevWho would downvote this perfectly reasonable question? edit: Nm reply jimmySixDOF 10 hours agoparentprevThats is exactly what they are using reply dcreater 2 hours agoparentprevHow is it more powerful? reply evilduck 2 hours agorootparentGrammars don't have to just be JSON, which means you could have it format responses as anything with a formal grammar. XML, HTTP responses, SQL, algebraic notation of math, etc. reply sa-code 4 hours agoparentprevThis is amazing, thank you for the link reply chirau 16 hours agoprevThis is wonderful news. I was actually scratching my head on how to structure a regular prompt to produce csv data without extra nonsense like \"Here is your data\" and \"Please note blah blah\" at the beginning and end, so this is much welcome as I can define exactly what I want returned then just push structured output to csv. reply firejake308 15 hours agoparentRemember that you still need to include an instruction to produce a CSV to get the prompt into the right context to generate a CSV that makes sense. Otherwise, you may get output that is technically in the CSV format but doesn't make any sense because the model was actually trying to write a paragraph response and the token sampler just selected really low-probability tokens that the model didn't really want to say. reply mmoskal 15 hours agorootparentIt seems ollama only supports JSON Schema. Interestingly, JSON Schema has much less of this problem than say CSV - when the model is forced to produce `{\"first_key\":` it will generally understand it's supposed to continue in JSON. It still helps to tell it the schema though, especially due to weird tokenization issues you can get otherwise. reply diggan 3 hours agorootparent> It seems ollama only supports JSON Schema. \"Encoding\" CSV as JSON is trivial though, so make it output JSON then parse the array-of-arrays into CSV :) reply KTibow 14 hours agoparentprevA lot of the time you can prevent this by prefilling the output with ``` and stopping at ```. reply chirau 13 hours agorootparentcare to explain further? I am not sure I understand you fully reply miki123211 8 hours agorootparentLLMs are essentially text completion machines, they get some text in as a list of tokens, and their job is to predict what token comes next. Typically, you're interested in getting more than just one token out of them, so you run them in a loop. You start with the token list containing just the user's message, run the LLM with that list, append the newly obtained token at the end, run the LLM again to see what comes after that new token and so on, until a special end-of-sentence token is generated and the loop terminates. There's no reason why you have to start with nothing but the users' message, though. You can let the user specify the beginning of the LLM's supposed completion, and ask the LLM to start from that point, instead of generating its completion from scratch. This essentially ensures that what the LLM says begins with a specific string. Not alll APIs expose this feature, there are good safety reasons not to, but all LLMs are capable of doing it in principle, and doing it with the open ones is trivial. LLMs are typically trained to output markdown, which uses ```language_name to denote code blocks in language_name, so that user interfaces like Chat GPT's web UI can do proper syntax highlighting. Therefore, if you make your LLM think that it already started a completion, and that completion began with ```json, it will predict what's most likely to come after that delimiter, and that would be a JSON block. reply magicalhippo 8 hours agorootparentprevThe way the LLMs work is you feed them a vector, array of numbers, that represents a sequence of tokens. You turn the crank and you get a probability distribution for the next token in the sequence. You then sample the distribution to get the next token, append it to the vector, and do it again and again. Thus the typical LLM have no memory as such, it inferes what it was thinking by looking at what it has already said and uses that to figure out what to say next, so to speak. The characters in the input prompt are converted to these tokens, but there are also special tokens such as start of input, end of input, start of output and end of output. The end of output token is how the LLM \"tells you\" it's done talking. Normally in a chat scenario these special tokens are inserted by the LLM front-end, say Ollama/llama.cpp in this case. However if you interface more directly you need to add these yourself, and hence can prefill out the output before feeding the vector to the LLM for the first time, and thus the LLM will \"think\" it already started writing code say, and thus it is likely to continue doing so. reply polotics 2 hours agorootparentyou have described an RNN I think, don't attention heads add something that you could compare to rough &ready understanding? reply magicalhippo 55 minutes agorootparentAuto-regressive LLMs do this as I understand it, though it can vary if they feed the combined input and output[1] through the whole net like GPT-2 and friends, or just the decoder[2]. I described the former, and I should have clarified that. In either case you can \"prime it\" like it was suggested. A regular RNN has more feedback[3], like each layer feeding back to itself, as I understand it. Happy to be corrected though. [1]: https://jalammar.github.io/illustrated-gpt2/#one-difference-... [2]: https://medium.com/@ikim1994914/understanding-the-modern-llm... [3]: https://karpathy.github.io/2015/05/21/rnn-effectiveness/ reply bonzini 11 hours agorootparentprev``` is markdown for pre formatted text. It puts the LLM in the mood of generating machine readable data instead of prose. reply jstanley 8 hours agorootparentprevA lot of the time you can prevent this by prefilling the output with \"```\", and stopping at \"```\". reply quaintdev 15 hours agoprevYay! It works. I used gemma2:2b and gave it below text You have spent 190 at Fresh Mart. Current balance: 5098 and it gave below output {\\\"amount\\\": 190,\\\"balance\\\": 5098 ,\\\"category\\\": \\\"Shopping\\\",\\\"place\\\":\\\"Fresh Mart\\\"} reply diggan 3 hours agoparentThat's some pretty inconsistent JSON, but I guess that makes sense when using a really small model and gemma on top of that. reply bluechair 17 hours agoprevHas anyone seen how these constraints affect the quality of the output out of the LLM? In some instances, I'd rather parse Markdown or plain text if it means the quality of the output is higher. reply lolinder 16 hours agoparentWorking with OpenAI's models I've found a very good strategy is to have two passes if you can afford the extra tokens: one pass uses a heavy model and natural language with markdown sections discussing the reasoning and providing a final natural language answer (ideally labeled clearly with a markdown header). The second pass can use a cheaper and faster model to put the answer into a structured output format for consumption by the non-LLM parts of the pipeline. You basically use JSON schema mode to draw a clean boundary around the wishy-washy language bits, using the LLM as a preprocessor to capture its own output in a useful format. reply thot_experiment 16 hours agoparentprevYMMV, it's a negative effect in terms of \"reasoning\" but the delta isn't super significant in most cases. It really depends on the LLM and whether your prompt is likely to generate a JSON response to begin with, the more you have to coerce the LLM the less likely it is to generate sane input. With smaller models you more quickly end up at the edge of space where the LLM has meaningful predictive power and so the outputs start getting closer to random noise. FWIW measured by me using a vibes based method, nothing rigorous just a lot of hours spent on various LLM projects. I have not used these particular tools yet but ollama was previously able to guarantee json output through what I assume is similar techniques and my partner and I worked previously on a jsonformer-like thing for oobabooga, another LLM runtime tool. reply crystal_revenge 14 hours agoparentprevThere was a paper going around claiming that structured outputs did hurt the quality of the output, but it turns out their experiment setup was laughably bad [0]. It looks like, so long as you're reasonable with the prompting, you tend to get better outputs when using structure. 0. https://blog.dottxt.co/say-what-you-mean.html reply mmoskal 14 hours agoparentprevIt depends how fine-tuned the model is to JSON output. Also, you need to tell the model the schema. If you don't you will get more weird tokenization issues. For example, if the schema expects a JSON key \"foobarbaz\" and the canonical BPE tokenization is [\"foobar\", \"baz\"], the token mask generated by all current constrained output libraries will let the model choose from \"f\", \"foo\", \"foobar\" (assuming these are all valid tokens). The model might then choose \"foo\", and then the constraint will force eg. \"bar\" and \"baz\" as next tokens. Now the model will see [\"foo\", \"bar\", \"baz\"] instead of [\"foobar\", \"baz\"] and will get confused [0] If the model knows from the prompt \"foobarbaz\" is one of the schema keys, it will generally prefer \"foobar\" over \"foo\". [0] In modern models these tokens are related, because of regularization but they are not the same. reply parthsareen 17 hours agoparentprevWe’ve been keeping a close eye on this as well as research is coming out. We’re looking into improving sampling as a whole on both speed and accuracy. Hopefully with those changes we might also enable general structure generation not only limited to JSON. reply hackernewds 17 hours agorootparentWho is \"we\"? reply parthsareen 16 hours agorootparentI authored the blog with some other contributors and worked on the feature (PR: https://github.com/ollama/ollama/pull/7900). The current implementation uses llama.cpp GBNF grammars. The more recent research (Outlines, XGrammar) points to potentially speeding up the sampling process through FSTs and GPU parallelism. reply mmoskal 15 hours agorootparentIf you want avoid startup cost, llguidance [0] has no compilation phase and by far the fullest JSON support [1] of any library. I did a PoC llama.cpp integration [2] though our focus is mostly server-side [3]. [0] https://github.com/guidance-ai/llguidance [1] https://github.com/guidance-ai/llguidance/blob/main/parser/s... [2] https://github.com/ggerganov/llama.cpp/pull/10224 [3] https://github.com/guidance-ai/llgtrt reply HanClinto 5 hours agorootparentI have been thinking about your PR regularly, and pondering about how we should go about getting this merged in. I really want to see support for additional grammar engines merged into llama.cpp, and I'm a big fan of the work you did on this. reply parthsareen 14 hours agorootparentprevThis looks really useful. Thank you! reply netghost 15 hours agorootparentprevThank you for the details! reply benreesman 16 hours agoparentprevI can say that I was categorically wrong about the utility of things like instructor. It’s easy to burn a lot of tokens but if the thing you’re doing merits the cost? You can be a bully with it and while its never the best, 95% as good for zero effort is a tool in one’s kit. reply nikolayasdf123 16 hours agoparentprevsame here. I noticed that when you ask model to generate elaborate responses in natural text, and then come up with an answer, quality is orders of magnitude better, and something in line you would expect human-like reasoning. asking LLM to directly generate JSON gives much worser results, similar to either random guess or intuition. reply highlanderNJ 6 hours agoprevWhat's the value-add compared to `outlines`? https://www.souzatharsis.com/tamingLLMs/notebooks/structured... reply parthsareen 1 hour agoparentHey! Author of the blog here. The current implementation uses llama.cpp GBNF which has allowed for a quick implementation. The biggest value-add at this time was getting the feature out. With the newer research - outlines/xgrammar coming out, I hope to be able to update the sampling to support more formats, increase accuracy, and improve performance. reply quaintdev 17 hours agoprevSo I can use this with any supported models? The reason I'm asking is because I can only run 1b-3b models reliably on my hardware. reply parthsareen 17 hours agoparentHey! Author of the blog post here. Yes you should be able to use any model. Your mileage may vary with the smaller models but asking them to “return x in json” tends to help with accuracy (anecdotally). reply dcreater 2 hours agorootparentWhy do smaller models fail to follow? Isn't the objective of constraints that it always provides the right output type? reply parthsareen 1 hour agorootparentThe constraints will always be met. It’s the data inside that might be inaccurate. YMMV with smaller models in that sense. reply pamelafox 13 hours agorootparentprevDo you happen to know if got-4o would be negatively affected by the addition of “return x in json”? I’m debating whether I could use the same prompt across all models, hosted and ollama. reply rcarmo 9 hours agoprevI must say it is nice to see the curl example first. As much as I like Pydantic, I still prefer to hand-code the schemas, since it makes it easier to move my prototypes to Go (or something else). reply lxe 16 hours agoprevI'm still running oobabooga because of its exlv2 support which does much more efficient inference on dual 3090s reply thot_experiment 15 hours agoparentI haven't touched ooba in a while, what's the situation like with exl2 vs the non-homogeneous quantization methods people are using like q3k_s or whatever. IIRC while exl2 is faster the gptq quants were outperforming it in terms of accuracy esp at lower bit depths. reply lormayna 11 hours agoprevThis is a fantastic news! I spent hours on fine tuning my prompt to summarise text and output in JSON and still have some issues sometimes. Is this feature available also with Go? reply lioeters 7 hours agoparentIt looks like the structured output feature is available in Go, with the `format` field. type GenerateRequest struct { ... // Format specifies the format to return a response in. Format json.RawMessage `json:\"format,omitempty\"` https://github.com/ollama/ollama/blob/de52b6c2f90ff220ed9469... reply xnx 16 hours agoprevIs there a best approach for providing structured input to LLMs? Example: feed in 100 sentences and get each one classified in different ways. It's easy to get structured data out, but my approach of prefixing line numbers seems clumsy. reply mmoskal 14 hours agoparentModels are trained on Markdown, JSON and various programming languages, so either one of these should work. However, in this case, you're best of giving the model sentences one by one to avoid it being confused. If you structure the prompt like \"Classify the following sentence, here are the rules ....\" + sentence, then you should be hitting prefix cache and get even better performance than when doing a single query. Of course, this only works if you have the prefix cache and are not paying per input token (though most providers now let you indicate you want to use prefix cache and pay less). reply xnx 7 hours agorootparentGood idea. I might try that. I think classification quality improves when it has following sentences. I'll have to see if feeding them sequentially makes it worse. reply vincentpants 16 hours agoprevWow neat! The first step to format ambivalence! Curious to see how well does this perform on the edge, our overhead is always so scarce! Amazing work as always, looking forward to taking this for a spin! reply JackYoustra 13 hours agoprevPRs on this have been open for something like a year! I'm a bit sad about how quiet the maintainers have been on this. reply parthsareen 1 hour agoparentHey! Author of the post and one of the maintainers here. I agree - we (maintainers) got to this late and in general want to encourage more contributions. Hoping to be more on top of community PRs and get them merged in the coming year. reply dcreater 2 hours agoparentprevReading tea leaves, they seem to be headed down the corporate path so view everything through that lens and how to maximize profit reply guerrilla 11 hours agoprevNo way. This is amazing and one of the things I actually wanted. I love ollama be because it makes using an LLM feel like using any other UNIX program. It makes LLMs feel like they belong on UNIX. Question though. Has anyone had luck running it on AMD GPUs? I've heard it's harder but I really want to support the competition when I get cards next year. reply rcarmo 9 hours agoparentYes, even on iGPUs. I was running it fairly well on a mini-PC with a 780M and the BIOS a set to allocate 16GB of shared memory to it. reply ein0p 12 hours agoprevThat's very useful. To see why, try to get an LLM _reliably_ generate JSON output without this. Sometimes it will, but sometimes it'll just YOLO and produce something you didn't ask for, that can't be parsed. reply diimdeep 5 hours agoprev [–] Very annoying marketing and pretending to be anything other than just wrapper around llama.cpp. reply evilduck 3 hours agoparent [–] Can you ollama haters stop with this bullshit? Does llama.cpp do dynamic model loading and unloading? Will it fetch a model you request but isn't downloaded? Does it provide SDKs? Does it have startup services it provides? There's space for things that wrap llama.cpp and solve many of its pain points. You can find piles of reports of people struggling to build and compile llama.cpp for some reason or another who then clicked an Ollama installer and it worked right away. It's also a free OSS project giving all this away, why are you being an ass and discouraging them? reply dcreater 2 hours agorootparentThey're going to go corporate reply diimdeep 1 hour agorootparentprev [–] Sure llama.cpp does not do all of that, except that it lets you curl model from public and free to use endpoints, it does that. But SDK? - fuck that. Load, unload and startup services - who is even need that ? All this value is so minuscule compared to core functionality provided by ggml/llamacpp. But this submitted link is not even about all of that, it is about what really llama.cpp does not do - it does not write more lines of marketing material than lines of code, which is that marketing material is about, lines of code that really just wrap 10x more lines of code down the line, and all of that by not making it clear as day. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ollama has introduced support for structured outputs, enabling models to generate outputs in a specific format using a JSON schema.",
      "This feature is now available in the updated Ollama Python and JavaScript libraries, useful for tasks like data parsing from documents and images.",
      "Users should update to the latest Ollama version and use the format parameter in cURL requests or libraries, with future updates promising performance enhancements and more format options."
    ],
    "commentSummary": [
      "Ollama provides a method to generate structured data, such as JSON, from language models, with discussions on using GBNF grammars for more versatile output formats like XML or SQL.- Users debate the reliability of JSON outputs versus natural language, with insights into how language models (LLMs) generate text and the role of structured prompts.- Ollama's integration with llama.cpp is noted for simplifying model usage, though some view it as merely a wrapper, and the feature supports various models with differing accuracy levels."
    ],
    "points": 223,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1733533952
  },
  {
    "id": 42346463,
    "title": "RollerCoaster Tycoon was the last of its kind [video]",
    "originLink": "https://www.youtube.com/watch?v=0JouTsMQsEA",
    "originBody": "Watch later Back",
    "commentLink": "https://news.ycombinator.com/item?id=42346463",
    "commentBody": "RollerCoaster Tycoon was the last of its kind [video] (youtube.com)220 points by zdw 17 hours agohidepastfavorite60 comments strongpigeon 15 hours agoIf someone ever gets the itch to play RCT again, I’d recommend taking a look at OpenRCT2 [0] for a bunch of QoL improvements (and MacOS support). [0] https://openrct2.org/ reply dcchambers 15 hours agoparentOpenRCT2 is great on PC! Also consider Roller Coaster Tycoon Classic. It's a really good remaster of the original two games with modern touch-friendly controls and it has excellent iOS and Android versions that play really well on tablets. https://atari.com/products/rollercoaster-tycoon-classic#play... reply rkagerer 8 hours agorootparentI see a lot of beefs in the reviews about the UI (especially on smaller screens like phones). How's the game compare to RollerCoaster Tycoon Netflix, which is more highly rated in the Play store? https://play.google.com/store/apps/details?id=com.netflix.NG... reply dcchambers 1 hour agorootparentYeah the UI would be tiny on a phone and I wouldn't play it that way. But on an ipad-sized device it's a dream. That other game you linked is something else entirely. It's a modern mobile interpretation of RCT, but not at all related other than sharing a name and them. Where-as the one I linked IS the original legendary game. I would skip the modern mobile game personally, as I don't like that of game. reply bbx 6 hours agorootparentprevIt's surprisingly great on iOS, even with the small UI factor. I managed to play it decently on an iPhone SE years ago. Would definitely recommend it if you want to play it on the go as it's still very fun to play nowadays. reply genocidicbunny 14 hours agoparentprevThere's also Parkitect, which is a sort of spiritual successor to RCT2. It might be a little more accessible if you're trying to introduce kids to the RCT games. reply TylerE 14 hours agorootparentSort of, but it plays not nearly as good and the coaster physics are bad. The thing I really don’t like is that it puts too much emphasis on scenery and not the tides themselves. reply billfruit 13 hours agorootparentprevWhat about Planet Coaster? reply genocidicbunny 12 hours agorootparentPlanet Coaster is good, but it has a vastly different aesthetic. RCT2 and 2 had that nice isometric grid which Parkitect has as well. reply mccolin 15 hours agoparentprevI can vouch. Having OpenRCT2 and being able to play this amazing game again has brought me so much joy. reply binary132 14 hours agorootparentI’ve been playing a copy of the original installment in the series off GoG with my kids recently and they love it. Thinking about setting up OpenRCT2 though. reply thih9 9 hours agoparentprevThe project’s FAQ says that the original Rct2 is required; are there plans to also provide alternative assets, OpenTTD style? Edit: yes, work in progress at https://github.com/OpenRCT2/OpenGraphics reply kiba 15 hours agoparentprevIs there an effort to make graphics for the game so that I don't have to go online to buy the files somewhere? reply sippeangelo 13 hours agoparentprevWorth noting is that OpenRCT2 has multiplayer co-op! reply satvikpendem 14 hours agoparentprevOpenTDD is mentioned in the video and OpenRCT2 is also hinted at reply CatWChainsaw 4 hours agoparentprevCan't wait to relive my childhood. Thanks :) reply m0n01d 13 hours agoparentprevHonestly the only reason I have a steam deck reply tim333 1 hour agoprevI was curious what happened to the programmer Chris Sawyer. From Wikipedia: > He volunteers with the media team at a local primary school. Sawyer travels the world to visit roller coasters as an enthusiast, and gave his \"coaster count\" at 770 in 2024. Here's a video of him checking out a roller coaster https://youtu.be/UU73g72NTHc reply HeYmaney 12 hours agoprevI was around 10 the first time I played RCT. A friend of mine brought me a copy disk of his original one. I could install the game but not play since there was a protection against copied CDs with an error message. In a desperate attempt, I bruteforced the exe by clicking 50 times on it and suddenly the game magically started. I was so happy I started a dance of the joy and would then use this trick to play the game. To this day I still don't know how it was possible to bypass the protection. reply Lammy 11 hours agoparentRCT1 No-CD is as easy as editing a Registry key actually. — Install the base game and both expansions normally. — Copy and merge the `ObjData` and `Data` folders from the CDs directly into the install folder. — Open RegEdit, navigate to `HKEY_LOCAL_MACHINE\\SOFTWARE\\Fish Technology Group\\Rollercoaster Tycoon Setup`, and change `Path` and `SetupPath` to the path to your install folder. Source: Did this just a couple months ago on my HITACHI FLORA 270HX. reply matsemann 10 hours agorootparentWhen playing The Sims as a kid, there were loads of expansions, but you only needed the last one in the chain you had installed to play. So if I had 1 and 2, and a friend 3 and 4, I could buy 5 and then borrow my friends disks and install those. Or if it was a game for a LAN, just boot the game with the CD in, then pass the CD along for others to do the same. reply tomcam 7 hours agorootparentCo-op mode piracy reply tropicalfruit 4 hours agorootparentprevhow's a 10 year old supposed to know that reply zamadatix 3 hours agorootparentThey aren't, it's in response to the final line of curiosity to this day. reply firecall 11 hours agoparentprevI have no idea, but thats cool :-D Could have been some sort of Race Condition maybe, which is more likely on an older resource constrained system? Maybe if the copy protection process became unavailable, the game loaded anyway as a way of providing a better user experience? I'm probably being too generous there LOL reply DavidPiper 14 hours agoprevFolks may also like the noclip documentary on RollerCoaster Tycoon: https://www.youtube.com/watch?v=ts4BD8AqD9g . It goes into more detail, particularly about what happened after RCT2, and has some perspectives from people in the OpenRCT2 community. reply indigoabstract 8 hours agoprevThe last developed in Assembly, but as he admits, not the last successful bedroom coder game. There are still a few successful solo developer or small team games even today. Manor Lords, Tiny Glade and Townscaper come to mind. But certainly fewer than 30-40 years ago. reply Xen9 7 hours agoparentWithin 30 years the game development & gaming will have been integrated as an experience for the average game. reply Eduard 13 hours agoprevis this the hundredth reheat of \"they wrote Rollercoaster Tycoon in assembly language\"? reply ndiddy 13 hours agoparentIt's brought up, but the main point of the video is that RollerCoaster Tycoon was one of the last major retail games made by a handful of people (Chris Sawyer for design and programming, Simon Foster for graphics, Allister Brimble for sounds/music), and managed to become the best selling computer game of 1999 despite going up against games made by teams of at least a hundred people. That's what he means by the game being \"the last of its kind\". reply BeefySwain 13 hours agorootparentMinecraft? reply ehnto 12 hours agorootparentLater in the video he discusses how times have changed once more, and solo developments turning it commercial hits is still possible. I do feel Minecraft was close enough to RTC to say that it never really stopped being possible. But in general I think the video is just remarking how the industry evolved and it became significantly harder to compete against bigger studios in the 2000s and beyond. I think it is undoubtedly true that the benchmark for many types of games has risen beyond the capabilities of the average solo developer in terms of graphics, content and gameplay scope. But many types of games do not have the burden of high end graphics and a particularly motivated individual can still make breakthroughs. It gets easier every year now, whereas for a while it was getting harder every year. reply TylerE 13 hours agorootparentprevRetail can years after the original indie release reply margalabargala 12 hours agorootparentprevThis still happens. Stardew Valley for example. reply indrora 10 hours agorootparentThe author specifically hails that the sands shifted once systems like Steam opened up to enable the publisher-less world. reply bzzzt 12 hours agorootparentprevThat one is popular, but certainly nowhere bear nr 1 in the charts. reply eterm 7 hours agorootparentIt's still top 10 played on steam, which is incredible really, and while it might never be number 1 best seller in the short term, it's probably out-sold many of the games that did get \"number 1\" in the year it was released. reply Slyfox33 13 hours agoparentprevI really don't understand people's obsession with this fact. Every game that was developed on consoles before the playstation was also written in its machines assembly language. It was extremely normal at that time. reply jchw 13 hours agorootparentRoller Coaster Tycoon is notable because: - It was relatively late. The heyday of coding games in assembler was years prior (maybe there were some exceptions in portable platforms?). Was there any other smash hit PC game in 1999 coded in x86 assembler? - It's a pretty substantially complex and large-scale game, at least relatively speaking. It's one thing to write a game like Tetris in assembler, RCT is magnitudes more complicated than the vast majority of games on e.g. the SNES. Doesn't mean the games are bad or anything, and there are probably exceptions. (I know the SNES in particular has a Sim City port, though it's pretty slow.) - It's not just about assembler really, it's about the whole mindset. RCT is very well-optimized. For example, gameplay mechanics are adapted (e.g. stretching the length of months, shaping algorithms for calculating scores and ratings, etc.) around reducing the number of multiplications, and even on fairly crummy computers of the early 2000s it was possible to have huge parks with a lot of guests running quite well. Contrasting RCT2 with RCT3 paints a pretty good picture, because if you ran both on contemporary computers for their respective releases RCT3 with its fancy 3D graphics and modern development practices couldn't handle a fraction the size of parks without becoming a laggy unplayable mess. I admit that I think people focus on it a bit much, especially since I'm not sure most people who repeat this fact actually understand what it means. But honestly, I'm willing to be arrogant enough to say I understand, and I salute. Writing scalable and complex code that actually works in macro assembler is not at all impossible, but it's certainly not easy. It requires a discipline that is not to be taken for granted. That said, I watched the video, and while it did talk on this point, it was largely about the death of hit games from small teams and the bedroom coder. reply palmfacehn 13 hours agorootparentprevIt may be due to the high level of bloat in contemporary software. People also find the demoscene interesting. JS1k games vs. 14mb React landing pages. Gaming isn't solely about the fantasy induced by the content. For some there is an appreciation of the underlying engineering. reply dinkumthinkum 13 hours agorootparentprevI think it may have to do with the fact that most programmers today don't even know what a computer is. You can ask most \"Software Engineer I's and II's\" what the difference between the stack and the heap is and get some pretty strange answers. So, it is interesting for some to think about people that had some idea about it. I don't know. reply sitzkrieg 12 hours agorootparentcoworkers decades my senior at my last job didnt even know what pointers were :-/ reply tomcam 7 hours agorootparentOh my reply db48x 12 hours agorootparentprevOuch. reply liontwist 13 hours agorootparentprevyeah it's just like writing a larger program in C. It takes longer to build up the basic primitives than you may be used to, but once you get going it's just programming. I think programmers just haven't had that experience, so it's other worldly. reply Tallain 11 hours agoparentprevThis is a video game history channel that usually goes a little more in-depth than that. Good production value, a soothing voice, and a nice ride through gaming's yesteryear -- often much better than others in the \"genre\" on YouTube reply proxysna 4 hours agoprevAhoy is one of the best creators on the platform. I’d would recommend his video about Polybius. Great stuff. https://youtu.be/_7X6Yeydgyg reply satvikpendem 14 hours agoprevFunny to see these comments here say basically the same thing as was said in the submitted video. reply ChrisArchitect 13 hours agoprevRelated: OpenRCT2 – RollerCoaster Tycoon 2 in browser using emscripten https://news.ycombinator.com/item?id=42318673 reply TylerE 14 hours agoprevOne fun fact about RCT is that it was written almost entirely in x86 assembly. It used a small C++ wrapper to basically do a few DirectX calls to setup the display and make windows api calls, but the actual game engine, all the guest logic, etc was pure assembly. Same basic engine as Transport Tycoon Deluxe. reply satvikpendem 14 hours agoparentAddressed at 9:22, it is the one of the core parts of what this video is about reply oksurewhynot 14 hours agoprevnext [3 more] [flagged] O-stevns 10 hours agoparentNot loving this trend that we are starting to doubt whether everything is AI generated. Ahoy is a known Youtuber who has made content for 14 years. His voice is definitely not AI generated. reply Almondsetat 14 hours agoparentprevIt's not reply xyzzy4747 14 hours agoprev [–] Did he forget Minecraft? It was made by a solo developer. He's now a billionaire from it and bought one of the most expensive homes in LA. reply lifthrasiir 14 hours agoparentHe does mention the recent surge of indies including Minecraft at the very end. reply kranke155 8 hours agorootparentNo one watches the video. reply satvikpendem 14 hours agoparentprevAddressed at 15:14 reply dangus 13 hours agoparentprev [–] Not really the same situation if you ask me. Minecraft hasn’t been a solo project for a the vast majority of its lifespan. Mojang was a small team but it really hasn’t been a solo effort since the very early days. RCT was basically start to finish developed by one person with some sound and graphics work done by a couple other people. reply CodeArtisan 7 hours agorootparent [–] When Jens Bergensten joined Mojang (to work on Scrolls), Minecraft was quite advanced and already had the nether and red stone. Notch did not rely on any store and was using his personal paypal account that got blocked due to an anomalous amount of transactions. https://minecraft.fandom.com/wiki/Java_Edition_Alpha_v1.2.0 https://blog.omniarchive.uk/post/1096322756/working-on-a-fri... reply dangus 4 hours agorootparent [–] Right, Jens joining is what I was referring to. It was “quite advanced” but at the same time very much in its infancy in terms of the timeline of the game. I personally don’t think the game would have gone as far as it did if it continued as a solo project. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "RollerCoaster Tycoon, developed primarily in assembly language by a small team, became a best-seller in 1999, showcasing a rare development style.",
      "Fans suggest OpenRCT2 for enhanced features and compatibility, and Roller Coaster Tycoon Classic for modern devices, while alternatives like Parkitect and Planet Coaster provide varied experiences.",
      "The video highlights the evolution of game development and the challenges small teams encounter when competing with larger studios, with indie successes like Minecraft illustrating the potential for small team achievements."
    ],
    "points": 220,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1733535139
  },
  {
    "id": 42343544,
    "title": "Fructose in diet enhances tumor growth: research",
    "originLink": "https://source.washu.edu/2024/12/research-reveals-how-fructose-in-diet-enhances-tumor-growth/",
    "originBody": "By Talia Ogliore December 4, 2024 SHARE Fructose consumption has increased considerably over the past five decades, largely due to the widespread use of high-fructose corn syrup as a sweetener in beverages and ultra-processed foods. New research from Washington University in St. Louis shows that dietary fructose promotes tumor growth in animal models of melanoma, breast cancer and cervical cancer. However, fructose does not directly fuel tumors, according to the study published Dec. 4 in the journal Nature. Instead, WashU scientists discovered that the liver converts fructose into usable nutrients for cancer cells, a compelling finding that could open up new avenues for care and treatment of many different types of cancer. “The idea that you can tackle cancer with diet is intriguing,” said Gary Patti, the Michael and Tana Powell Professor of Chemistry in Arts & Sciences and a professor of genetics and of medicine at the School of Medicine, all at WashU. “When we think about tumors, we tend to focus on what dietary components they consume directly. You put something in your body, and then you imagine that the tumor takes it up,” Patti said. “But humans are complex. What you put in your body can be consumed by healthy tissue and then converted into something else that tumors use.” Fowle-Grider “Our initial expectation was that tumor cells metabolize fructose just like glucose, directly utilizing its atoms to build new cellular components such as DNA. We were surprised that fructose was barely metabolized in the tumor types we tested,” said the study’s first author, Ronald Fowle-Grider, a postdoctoral fellow in Patti’s lab. “We quickly learned that the tumor cells alone don’t tell the whole story. Equally important is the liver, which transforms fructose into nutrients that the tumors can use.” Using metabolomics — a method of profiling small molecules as they move through cells and across different tissues in the body — the researchers concluded that one way in which high levels of fructose consumption promote tumor growth is by increasing the availability of circulating lipids in the blood. These lipids are building blocks for the cell membrane, and cancer cells need them to grow. “We looked at numerous different cancers in various tissues throughout the body, and they all followed the same mechanism,” Patti said. The corn syrup era Scientists have long recognized that cancer cells have a strong affinity for glucose, a simple sugar that is the body’s preferred carbohydrate-based energy source. In terms of its chemical structure, fructose is similar to glucose. They are both common types of sugar, with the same chemical formula, but they differ in how the body metabolizes them. Glucose is processed throughout the whole body, while fructose is almost entirely metabolized by the small intestine and liver. Patti Both sugars are found naturally in fruits, vegetables, dairy products and grains. They are also added as sweeteners in many processed foods. Fructose, in particular, has penetrated the American diet over the last few decades. It is favored by the food industry because it is sweeter than glucose. Prior to the 1960s, people consumed relatively little fructose compared with today’s numbers. A century ago, an average person consumed just 5-10 pounds of fructose per year. To put it in familiar terms, that is roughly equal to the weight of a gallon of milk. In the 21st century, that number has increased to be as high as the equivalent of 15 gallons of milk. “If you go through your pantry and look for the items that contain high-fructose corn syrup, which is the most common form of fructose, it is pretty astonishing,” said Patti, who is also a research member of Siteman Cancer Center, based at Barnes-Jewish Hospital and WashU Medicine, and the Center for Human Nutrition at WashU Medicine. “Almost everything has it. It’s not just candy and cake, but also foods such as pasta sauce, salad dressing and ketchup,” he said. “Unless you actively seek to avoid it, it’s probably part of your diet.” (Photo: Shutterstock) Cancer’s appetite for fructose Given the rapid rise in the consumption of dietary fructose over recent decades, the WashU researchers wanted to know more about how fructose impacts the growth of tumors. Patti and Fowle-Grider began their investigation by feeding tumor-bearing animals a diet rich in fructose, then measuring how quickly their tumors grew. The researchers found that added fructose promoted tumor growth without changing body weight, fasting glucose or fasting insulin levels. “We were surprised to see that it had a rather dramatic impact. In some cases, the growth rate of the tumors accelerated by two-fold or even higher,” Patti said. “Eating a lot of fructose was clearly very bad for the progression of these tumors.” But the next step in their experiments initially stumped them. When Fowle-Grider attempted to repeat a version of this test by feeding fructose to cancer cells isolated in a dish, the cells did not respond. “In most cases they grew almost as slowly as if we gave them no sugar at all,” Patti said. Related: Untangling the roots of disease WashU’s Gary Patti has a leading role in a $50 million project to study environmental chemicals and other threats to human health. So, Patti and Fowle-Grider went back to looking at changes in the small molecules in the blood of animals fed high-fructose diets. Using metabolomics, they identified elevated levels of a variety of lipid species, including lysophosphatidylcholines (LPCs). Additional dish tests showed that liver cells that were fed fructose release LPCs. “Interestingly, the cancer cells themselves were unable to use fructose readily as a nutrient because they do not express the right biochemical machinery,” Patti said. “Liver cells do. This allows them to convert fructose into LPCs, which they can secrete to feed tumors.” A defining characteristic of cancer is uncontrolled proliferation of malignant cells. Each time a cell divides, it must replicate its contents, including membranes. This requires a substantial amount of lipids. While lipids can be synthesized from scratch, it is much easier for cancer cells to simply take lipids up from their surrounding environment. “Over the past few years, it’s become clear that many cancer cells prefer to take up lipids rather than make them,” Patti noted. “The complication is that most lipids are insoluble in blood and require rather complex transport mechanisms. LPCs are unique. They might provide the most effective and efficient way to support tumor growth.” (Photo: Shutterstock) Avoiding fructose Interestingly, over the same period of time when human fructose consumption has surged, a number of cancers have become increasingly more prevalent among people under the age of 50. This raises the question whether the trends are linked. With $25 million in support from Cancer Grand Challenges, Patti recently teamed up with Yin Cao, an associate professor of surgery at WashU Medicine, and other investigators from around the world, none of whom were involved in this study, to investigate possible connections. “It will be exciting to better understand how dietary fructose influences cancer incidence. But one take-home message from this current study is that if you are unfortunate enough to have cancer, then you probably want to think about avoiding fructose. Sadly, that is easier said than done,” Patti said. Aside from dietary intervention, the study authors said that this research could help us develop a way to prevent fructose from driving tumor growth therapeutically, using drugs. “An implication of these findings is that we do not have to limit ourselves to therapeutics that only target disease cells,” Patti said. “Rather, we can think about targeting the metabolism of healthy cells to treat cancer. This has worked with mice in our study, but we would like to take advantage of our observations and try to improve the lives of patients.” The study authors are working with clinical partners at WashU Medicine to explore a clinical trial related to fructose in the diet. This research was funded in part by the National Institutes of Health (NIH) (R35 ES028365) SHARE Media Contact Talia Ogliore FEATURED WASHU EXPERTS Gary Patti Professor of Chemistry and of Genetics and Medicine (School of Medicine), Michael and Tana Powell Professor of Chemistry Topics Cancer Research Chemistry Science & Technology Schools Arts & Sciences Read more stories from Arts & Sciences Visit Arts & Sciences",
    "commentLink": "https://news.ycombinator.com/item?id=42343544",
    "commentBody": "Fructose in diet enhances tumor growth: research (washu.edu)185 points by gmays 23 hours agohidepastfavorite176 comments fallinditch 22 hours agoMy understanding is that naturally occurring fructose found in whole fruits is accompanied by fiber, vitamins, and minerals, which help mitigate any negative effects of fructose when consumed as part of a balanced diet. However, it sounds like we should maybe be avoiding excessive amounts of certain fruit. See: A Definitive Guide to Fructose Content in Fruit [1] There was a recent episode from Diary of a CEO with a cancer expert. He seems to have some really sound advice. One particular take away for me was his finding that when the body enters a ketogenic state due to fasting the body produces defences that eat up cancer cells [2] [1] https://iquitsugar.com/blogs/articles/a-definitive-guide-to-... [2] https://youtu.be/VaVC3PAWqLk?feature=shared reply georgecmu 21 hours agoparent> One particular take away for me was his finding that when the body enters a ketogenic state due to fasting the body produces defences that eat up cancer cells As with everything, mileage will vary. Pro: https://pmc.ncbi.nlm.nih.gov/articles/PMC6375425/ Contra: https://www.cancer.columbia.edu/news/study-finds-keto-diet-c... “We did indeed see that the ketogenic diet suppressed tumor growth — but we also saw, surprisingly, that it promoted tumor metastasis,” says Gu. “That was really a shock to us.” reply fallinditch 21 hours agorootparentI think there's a misunderstanding here, my fault for not being clearer. I think I should have used the phrase 'when the body enters a state of ketosis' i.e. the state you get to when fasting when your body starts burning core fat. I believe the word ketogenic refers to the type of meat heavy diet. Thanks for those links, the fact that eating a lot of meat can promote tumor metastasis does not surprise me. reply arcticbull 20 hours agorootparentKetosis occurs when your body switches from consuming glucose as its primary fuel source to consuming ketones which are generated from the breakdown of fatty acids, either from the diet or endogenous. Almost all body tissue can run on ketones instead of on glucose, except for certain important tissues like red blood cells, 30% of the brain, retina, some kidney tissue, etc. For the rest, your body synthesizes the glucose it needs via gluconeogenesis from some protein substrates and from glycerol backbodes from triglycerides. These inputs can be either from the diet or from your fat stores. Fasting for a few days causes your body to enter authophagy through the inhibition of mTOR in addition to ketosis, so that could account for some of the difference. reply penguin_booze 8 hours agorootparentCan you comment on the efficacy of intermittent fasting (IF) to get into ketosis? Does fasting have to be undertaken in the order of days in order to be any effective, as opposed to 16 or 18 hours per day by way of IF. For context, I've been following IF for a couple of years. I can definitely see myself losing my resident body fat, which is encouraging. I had tried keto diet before that - I found it very difficult to sustain, especially when you're away from home or are at work. So, part of my motivation to do IF instead of keto is, well, that I can achieve some level of ketosis via IF, and without following a strict keto diet. reply fallinditch 1 hour agorootparentDr Thomas Seyfried, the guy in the Diary of a CEO interview, stated that intermittent fasting is beneficial and achieves the desired 'cell repair' effects. reply sophacles 20 hours agorootparentprevThere are plenty of vegetarian ketogenic diets. A ketogenic diet is one that contains very few, or no carbohydrates to maintain the ketosis - just high in fats and medium in proteins. Meat is a convenient form of food with those properties, so often people maintaining such a diet eat a lot of meat. reply weird-eye-issue 20 hours agorootparentprevIn mice. I'm not a mouse, are you? reply mbreese 20 hours agorootparentAn oncologist I know was fond of saying that we cured mice of cancer ages ago... cancer in humans is much more complicated. reply neom 20 hours agorootparentJust as an aside, as a complete rat lover and obsessed fancy rat freak, I always find it somewhat sad we could probably come up with some great drugs for them (they notoriously die very easily), just, well, who cares about rats??? reply lambdaba 22 hours agoparentprevI doubt modern fruit, optimized for sweetness, has a very favorable vitamin-mineral/fructose ratio. reply scarab92 18 hours agorootparentIt’s not clear to me why vitamin/fructose ratio would even be important? Generally vitamins aren’t helpful unless you were deficient, which is uncommon in western diets. reply nradov 13 hours agorootparentVitamin D deficiency is common in high latitudes. Most people don't get enough from their diets so supplements are recommended for them. https://www.nhs.uk/conditions/vitamins-and-minerals/vitamin-... Vitamin B (thiamine) deficiency is also common among alcoholics. Lots of heavy drinkers out there. While obviously drinking less is the preferred solution, if they aren't willing to do that then supplements can reduce the harm. https://adf.org.au/insights/alcohol-related-thiamine-deficie... reply url00 22 hours agorootparentprevNot sure why you are being downvoted, this is a good point to bring up. reply Vitamin_Sushi 21 hours agorootparentI didn't downvote, but I can imagine that most people weren't aware of that claim. I'm skeptical of it as well since I'm not aware of any research that shows fructose in fruit increasing over time. reply swatcoder 21 hours agorootparentCommercial incentives are to engineer varietals for contemporary aesthetics (sweet, unbitter, colorful, unblemished, large) and crop turnover (rapid growth, tolerance for depleted soil), nutrition has been way down on the priority list for nearly a century now. https://www.scientificamerican.com/article/soil-depletion-an... If you don't believe food is sweeter and less nutritious, you're firing a shot at many-billion-dollar industries that have been earnestly been trying to optimize the above for all that time. It's not a pleasant thing to believe, but its hard to refute. IIRC, You should be able to do your own deep dives here: https://fdc.nal.usda.gov/food-search?type=SR%20Legacy&query= reply Attummm 18 hours agorootparentTo add to your point. The food industry, like any other industry, focuses on numbers. Consumer spending favors new varieties of fruit with sweeter taste (e.g., increased glucose/fructose content). This process has led to our current comical situation where fruit, which is perceived as natural, has become unfit for consumption by animals. >Fruits have gotten too sweet for some animals and zookeepers have had to find alternative foods. https://weather.com/news/news/2018-10-03-fruit-so-sweet-zoo-... reply liontwist 14 hours agorootparentprevThere is a large and wealthy segment of the population concerned about food health and quality, and large companies who market to them. Are their values not being met? Or are you just describing the quality received by the average consumer. reply magicalist 19 hours agorootparentprev> It's not a pleasant thing to believe, but its hard to refute. Refute what? There still hasn't been any evidence provided for this claim. reply scorpioxy 19 hours agorootparentprevBased on my experience of having moved countries, I can tell you that many types of fruit in this new country(AU) is definitely a lot more sweet than what I am used to. This is not scientific research of course and it could be attributed to many things(soil, environment, fertilizer etc) besides favoring the sweeter lineage but I have noticed that it made me stop eating them. Now a Granny Smith apple here may be different to the Granny Smith apple I used to eat but this is just to illustrate that I am not comparing two different kinds of apples. reply sidibe 21 hours agorootparentprevIf you're older I think some of this is obvious. Biggest example for me is grapefruit, they used to be barely sweet, when I was a kid they were mostly bitter and we used to add sugar to them, now they're always extremely sweet. reply EPWN3D 2 hours agorootparentYour palette changes pretty significantly over time, especially between childhood and adulthood. Grapefruit may not have changed at all, and you might still find them sweeter at 30 than you did at 8. reply MrDresden 21 hours agorootparentprevThis may differ based on location, as my grapefruit (non organic normal supermarket bought) are still quite bitter. I'm located in Europe so these might be Spanish grapefruit, though not sure. reply lambdaba 21 hours agorootparentprevsame here! Thanks for bringing up memories of sugar grapefruit. My parents' generation used to sugar their strawberries... reply mercacona 19 hours agorootparentMy parents' generation used to add sugar to everything because they considered sugar a source of energy. I remember eating strawberries directly from the plant, and they were sweet. However, my aunt would serve the same strawberries with sugar as an afternoon snack. reply l5870uoo9y 21 hours agorootparentprevTry buying conventional fruit and an organic one and taste the different, e.g. grape fruit. reply XorNot 20 hours agorootparentprevBecause it's not a factual statement. It's a \"truthy\" sounding statement, but the person making it didn't actually go and look it up, which they could've done in seconds on the device they're currently using. It's practically the definition of FUD (fear, uncertainty, doubt). Based on no evidence \"I think there's a danger because it's feels like there's a danger!\" reply throw646577 21 hours agoparentprevGoing to go out on a limb here and say you probably shouldn't get any scientific advice about nutrition -- or really any scientific advice at all -- from anyone on Diary Of A CEO. reply turing_complete 21 hours agorootparentI don't like the interviewer, but just three weeks ago, Eric Schmidt was on the podcast and he is obviously very smart and knowledgeable about technology and business. reply throw646577 21 hours agorootparentAnd yet, and yet, I still wouldn't take his diet advice that seriously. reply airstrike 21 hours agorootparentprevIf you're going to make a bold claim like that, shouldn't you provide some support? Otherwise you're expecting us to just take an anonymous person's word for it... and particularly a throwaway account. You're saying \"discount science based not on facts, but on the form in which such science is published\" which is utterly unscientific reply loco5niner 18 hours agorootparentMore like \"discount anything coming out of the mouth of a CEO\", which makes perfect sense. reply airstrike 17 hours agorootparentNot only does that also not make sense, but guests on the show are not strictly CEOs, which further supports the view that this shallow dismissal is indeed unsupported by evidence. reply deprecative 17 hours agorootparentIt's a podcast. They're only useful for entertainment. If you take advice from them in a meaningful way you're effectively rolling the dice on tabloid grade slop factories. reply airstrike 17 hours agorootparentThat's a different statement from the one in the original comment which singles out DOAC as an untrustworthy source reply throw646577 20 hours agorootparentprevI did reply to another comment about my concerns about Diary of a CEO in the context of nutrition advice. reply airstrike 17 hours agorootparentLooks like concerns about the host, not about the content the interviewees share. reply Ronwe 21 hours agorootparentprevIn general yes, but there are some good guests that were there. For example, in case of nutrition, Dr. Layne Norton called him out on having a guy that was talking nonsense on the podcast and then they got in touch with Layne and had him on the podcast where he explained many of the misinformation about nutrition currently in wild. reply guerby 20 hours agorootparentA researcher opinion on Dr. Layne Norton: https://www.youtube.com/watch?v=iZ4p1bCsUio \"When Predator Becomes Prey: Will Dr Layne Norton Choose Humility or Humiliation? by Nick Norwitz\" YMMV reply AlexErrant 19 hours agorootparentNorwitz and Layne differ on the lean mass hyper-responder phenotype. Norwitz likes referencing his meta-analysis of 41 RCTs, but pay attention to its conclusion: > A substantial increase in LDL cholesterol is likely for individuals with low but not high BMI with consumption of an LCD, findings that may help guide individualized nutritional management of cardiovascular disease risk. As carbohydrate restriction tends to improve other lipid and nonlipid risk factors, the clinical significance of isolated LDL cholesterol elevation in this context warrants investigation. \"warrants investigation\"??? Come back when you have hard outcomes, i.e. reduced MACE. His meta-analysis of RCTs concludes that some people on keto have increased LDL! Norwitz admits in another video: \"No, I'm not going to say that high ApoB is fine... ApoB is necessary but not sufficient [to cause ASCVD]\" https://youtu.be/270ZyfSGLkE?t=484 So, what \"humiliation\"? If high ApoB (synonymous with LDL for our purposes) is bad, why would you go on a diet that causes it? Layne has a large lay audience, and he has to stick to his main message: that w/r/t diet, calories are all that matter. Lowcarb/lowfat/vegan/whatever is fine. Platforming someone who's obsessed with LMHR is going to be too much biochemistry for Layne's audience. And for an effect that's relevant to perhaps only 2% of people? Low-carb advocates have their own YouTube channels; there's no reason Layne, as a center-aisle scientist, needs to engage. Until the LMHR has been shown to have reduced MACE, which Norwitz admits we don't have evidence for https://www.youtube.com/watch?v=gxRLRYEQaEs&t=971s I don't see the point of Layne platforming him to just talk about biochemistry and confuse people. reply unsupp0rted 21 hours agorootparentprevIs there any information on nutrition at all? Isn’t it 100% misinformation? Every week a new thing contradicts last week’s thing. reply lambdaba 21 hours agorootparentprevwhat a dumb remark, what does it matter who the host is reply throw646577 21 hours agorootparentThe host in this case is a significant investor in Huel who seemingly fails to mention his relationship to it when bigging it up. https://www.bbc.co.uk/news/articles/c0rwz5xkrg8o To me it casts any discussion of nutrition on his podcast in a somewhat different, less than neutral light. reply lambdaba 21 hours agorootparentOkay, but the parent was implying that would discredit the interviewee. One thing I've observed with people that have an important message to disseminate is they are not fussy about what the medium is reply deprecative 17 hours agorootparentIf you have significant financial interest in a product or service you're not going to bring on someone who will meaningfully oppose that product or service. Therefore anyone who goes onto that podcast, or whatever media, would be selected for. reply throw646577 21 hours agorootparentprevI don't know if it would or would not. But some cynicism feels warranted by association. And then there's the further research you can do: https://sciencebasedmedicine.org/ketogenic-diets-for-cancer-... I dunno. I don't know enough to evaluate this in precise detail. But I do know enough to feel cynicism about extraordinary claims. reply thot_experiment 21 hours agorootparentprevIt absolutely does matter?? If you're trying to do reference class forecasting and your class is \"a person who is not a CEO\" and then you're forecasting based on anecdata of \"people who are a CEO\" you're not going to get good predictions. reply airstrike 21 hours agorootparentpeople on the show aren't strictly \"CEOs\" in the truest sense of that word. just to pick an obvious counterexample, Robert Greene is an author, not a CEO, and he was a guest reply altairprime 21 hours agoparentprevIt would still be net beneficial for anyone consuming industrial fructose to switch to fresh fruit of any kind. However, yes, if you’ve already withdrawn added fructose in processed foods and drinks from your diet, you could certainly optimize further on which kinds of fresh fruit you consume. It won’t make any difference if you still drink fructose soda, though. reply eikenberry 21 hours agoparentprevI thought levels of fructose weren't as important as other qualities, like fiber content. For example, Dates are often referenced as a good fruit option due to the high fiber content but that guide doesn't mention fiber at all and has dates in the high fructose category. This seems like standard operating procedure in anything dietary where it is more about a specific aspect of the food and less about communicating well rounded advice. reply lm28469 21 hours agorootparent> I thought levels of fructose weren't as important as other qualities, like fiber content. > Dates are often referenced as a good fruit option fyi, 100gr of dried dates it like 3 to 4 times the average amount of sugar recommended per day. Just 2 medjool dates and you hit your daily sugar recommendation. At the end of the day your body will have to process the stuff you ingest, if it comes with fibers the digestion will be slower, but if you eat too much of X Y Z day after day it's just a matter of time before your body gives up reply EPWN3D 2 hours agorootparentDried fruits have the same amount of sugar as fresh, but all the fiber is removed so you absorb all of that sugar. When discussing the nutritional value of fruits and why sugar from fruits isn't \"bad\" for you, it should be assumed you're discussing fresh and not dried fruit. reply papa_bear 21 hours agorootparentprevI believe you'll meet the \"Added or Free sugar\" recommendation with 2 medjool dates, but AFAIK, the guideline isn't as strict on naturally occurring sugars, if there is any guideline at all aside from general carb consumption. Dates also have a surprisingly low glycemic index for how sweet they are. reply l5870uoo9y 21 hours agoparentprevMonkeys at a UK were put off “human” bananas and developed diabetes[1]. Fruit is healthy desert not - like vegetables - main course. [1]: https://edition.cnn.com/2014/01/15/world/europe/uk-zoo-monke... reply culopatin 21 hours agoparentprevThat list of fructose levels is pretty useless when the units of measurement change per fruit. 1 cup vs 1 guava or 1 banana? reply doodlebugging 20 hours agorootparentAt least they didn't use standard layman units like a fractional volume of an Olympic sized swimming pool. reply venkat223 13 hours agoparentprevThe recent shoes to me that the precursor for cancer is diabetes and inflammatory biotoxins reply 0cf8612b2e1e 22 hours agoparentprevIs there a lower prevalence of cancer in cultures that fast? reply steve_adams_86 21 hours agorootparentI don't know about that, but there does appear to be a tradeoff between nutrition and fertility. When you eat less, you're less fertile. When you're less fertile, you live longer. A lot of what appears to allow us to live longer is lower rates of cancer, but the data I've seen there isn't rock solid and it isn't something I've dug deeply into. I only mention it because it's certainly studied and a question worth pursuing, with very interesting papers available if you look. Something I read recently was about eunuchs living something like 25% longer than intact counterparts. However, the data was limited (15 each of eunuchs and intact as I recall). There were very few confounding factors, however. Really interesting stuff. reply wigster 21 hours agorootparentprevor in Inuit people who i imagine don't get much fresh fruit except a few berries? reply lambdaba 21 hours agorootparenteverywhere, cancer & chronic disease rates go up as diets get westernized reply lm28469 21 hours agorootparentYep, that's because most cancers are lifestyle related, remove alcohol, tobacco and bad diets/obesity and you remove 75% of cancers. https://www.oatext.com/which-environment-makes-cancer.php reply lambdaba 21 hours agorootparentObviously yes, so is most chronic disease, but bring up people being healed by lifestyle & diet intervention and prepare for relentless attacks from both doctors and laypeople. reply cortic 21 hours agorootparentprevIsn't that just selection bias, as diets get westernized so does medical care and so we detect more diseases.. reply lambdaba 21 hours agorootparentthis is even counter to mainstream medical opinion reply unsupp0rted 21 hours agorootparentprevLife expectancy also goes up as lives get westernized, so people live long enough to become adults and die from cancer, rather than Malaria, diarrhea, etc reply lambdaba 19 hours agorootparentrates of cancer are going up in younger people nowadays reply adamredwoods 19 hours agorootparentprevDiet is not the only factor for cancer. There are children who die of cancer, and there are smokers and drinkers that live to 99. reply lm28469 21 hours agorootparentprev> cultures that fast? Are there any culture that fast in a way that would matter here ? Most of fasting traditions are mostly performative. The average joe probably fasted more by default a few thousand years ago than most people do now reply bloqs 19 hours agoparentprevDiary of a CEO should not have the proxy-BBC approval Steven enjoys. He regularly has quacks and misinformation pedallers on there shamelessly, and his willingness to fall under their spell demostrates concerning levels of naivete or (worse) willing negligence for engagements sake. reply CommanderData 10 hours agorootparentI have the same problem listening to the host. Eager to peddle nonsense blindly has put me off watching his podcasts entirely, and a lack of critique. But sadly it's true of many YouTube podcasts today. reply amanaplanacanal 17 hours agoparentprevA reminder: table sugar and high fructose corn syrup are both about 50% fructose. reply slowmovintarget 18 hours agoparentprevThe rankings are a little off... I mean, you don't eat one prune, or one apricot. You tend to have a few, and that would put them right up next to, say, an apple, in the \"medium\" ranks. That's the same as that \"Serving Size\" trick on nutrition information guides where the \"serving size\" is 5 potato chips instead of the entire bag. reply ChumpGPT 22 hours agoparentprevFruit has changed dramatically over the last Century. I imagine there was a time when it was much smaller, less sweet, and only available seasonally. In this new world, we have 24/7/365 access to as much and whatever we want. There are Cherries along with every type of fruit you could want at Costco today and it's December. reply Noumenon72 21 hours agorootparentCherries out of season are wrinkly and weird tasting, unless Costco has solved that. Cherries and pomegranates are my two most \"seasonal\" fruits, in contrast to the always-available ones. I feel like mangoes and cantaloupes have seasons too, I just don't know how to detect them since they keep selling them even when they're not good. reply Nasrudith 21 hours agorootparentprevExcept the fruit in the past was often available out of season but as pickling or preserves, via either excesses of salt or sugar. For several centuries. Let alone the parts of the world that don't really have four seasons, instead having only two, wet and dry seasons. The point being I'm not sure how much relatively recent norms will actually be able to tell us about health. reply outworlder 22 hours agoprev> \"one way in which high levels of fructose consumption promote tumor growth is by increasing the availability of circulating lipids in the blood. \" Glad to see more research on this. Until recently, people trying to sound the alarm with regards to high fructose consumption (mainly high fructose corn syrup) have been dismissed. Excess fructose consumption increases tryglicerides, uric acid. Just uric acid alone causes a lot of issues, from heart disease to erectile dysfunction(inhibits NOX), even before gout starts. The range that's considered 'normal' has changed over time, but I feel it's too high. https://pmc.ncbi.nlm.nih.gov/articles/PMC7352635/ Note that fruits are unlikely to be an issue (except perhaps as fruit juice). Most people don't eat enough of them and they have plenty of nutrients that are beneficial. EDIT: > “Interestingly, the cancer cells themselves were unable to use fructose readily as a nutrient because they do not express the right biochemical machinery,” Patti said. “Liver cells do. This allows them to convert fructose into LPCs, which they can secrete to feed tumors.” Forgot about this. Non alcoholic fatty liver disease has been on the rise for a while now, and it's mostly the high fructose corn syrup again. reply chiph 21 hours agoparentI was recently diagnosed with fatty liver disease. The liver-related numbers in my blood work were slightly elevated. An abdominal ultrasound confirmed it. Liver problems have historically been caused by excessive alcohol consumption (leads to cirrhosis, etc.) But I'm a teetotaler. The other version of fatty liver disease - the non-alcoholic kind - can be caused by excessive fructose consumption. Since I have been drinking diet sodas for years, that likely isn't it either (Diet Dr Pepper uses Aspartame as a sweetener). But there are hints that artificial sweeteners trick the body into thinking they're getting the real thing and it will store those calories as fat. So I have started a fat-loss program where the first thing to go has been soda. And I'm down 15 pounds so far. Many thanks to the \"LoseIt\" app developers for making it easy to track my calories. And please get your numbers checked at your next doctor visit. reply 01100011 21 hours agorootparentFatty liver here as well. Not a big consumer of sugar or alcohol though so I suspect something else is the cause. My liver doc ordered a couple dozen obscure blood tests to see if we can find the cause. I take a lot of supplements and vitamins, but the doc didn't see anything that should have caused it. My parents and siblings, despite being more obese and consuming more sugar, also do not have it. reply unsupp0rted 21 hours agorootparentprevIs fatty liver disease also common in teetotalers / non-fructose drinkers of average weight? reply chiph 18 hours agorootparentI wouldn't think so, as alcohol, fructose, and/or being overweight seem to be the main factors. I would ask your doctor to look at your blood work with relation to that the next time you get a physical. Oh, and at least in my case, it's not related to cholesterol or diabetes. I'm fine there. {shrug} reply Gibbon1 20 hours agorootparentprevThing I read that's a bit fascinating and possibly important is your digestive system has taste buds and it's own nervous system. I wouldn't be gobsmacked if it responded to artificial sweeteners. And that causes issues. Other thing I read is fructose is processed by the liver via some of the same paths that alcohol does. Would surprising if they didn't share some of the same negative health outcomes. reply jandrewrogers 22 hours agoparentprev> mainly high fructose corn syrup It should be pointed out that contrary to what many people assume, \"high-fructose\" does mean that it has more fructose than sugar. Sugar is 50% fructose, and many widely used HFCS formulations contain less fructose than sugar (e.g. the 42% fructose formulation used in most processed food). Even the formulation used in soft drinks is only 55% fructose, marginally more than sugar. If you replace HFCS with sugar in your diet it is basically a no-op in terms of being healthy and in many cases will increase your fructose intake. reply gus_massa 21 hours agorootparentIn table sugar each fructose is conected to a,glucose in a single molecule (so 50% and 50%). In high fructose corn syrup they are disconected. This cange a lot of things like how fast they get to the blood and how fast cells can use them. I'm skeptical of a lot of good/bad reported health effects, but it's not obviously a nop. reply XorNot 20 hours agorootparentFructose can't be used by cells directly - it does not effect blood sugar levels, because it has to be processed by the liver into glucose first.[1] The potential issues with fructose are related to the matabolic processes which convert it into glucose, which is what your cells actually use. So eating say, straight fructose in fact won't spike your blood sugar since it has a much more convoluted metabolic path to consumption. Something like HFCS is more likely to be a problem because the glucose content is not in the dissacharide form of sucrose and can be directly absorbed. [1] https://en.wikipedia.org/wiki/Fructose reply heisenbit 20 hours agoparentprevFructose is primarily processed in the liver and shares some processes with alcohol processing with toxic byproducts. Fructose processing yield triglycerides contributing to the less ideal fat in blood. When I cut my fructose consumption down for a while to less than 10g/day my triglycerides in my blood dropped considerably - my GP did not believe it was my diet. It is shocking how little some doctors know about fructose influence on the body despite the considerable amount is is consumed. reply sneak 22 hours agoparentprevFYI, high fructose corn syrup has only slightly more fructose (a single digit percentage) than normal sugar, which also has a fuckton of fructose in it. Corn syrup is a red herring. reply nayuki 22 hours agorootparentCorrect. Sucrose (table sugar) is exactly 50% fructose, 50% glucose. The most common blend of HFCS is 55% fructose, 45% glucose - which is barely different. https://en.wikipedia.org/wiki/High-fructose_corn_syrup reply tasty_freeze 21 hours agorootparentThat link has a bit of nuance. Some HFCS has lower fructose content: \"HFCS 42\" and \"HFCS 55\" refer to dry weight fructose compositions of 42% and 55% respectively, the rest being glucose. HFCS 42 is mainly used for processed foods and breakfast cereals, whereas HFCS 55 is used mostly for production of soft drinks. reply sithadmin 22 hours agorootparentprevYup. ~5% more fructose than table sugar in its most common formulations, actually lower content than table sugar in some formulations (e.g. HFCS-42). The 'high fructose' moniker is derived from a reference to 'pure' corn syrup which is nearly 100% glucose, not a reference to table sugar as commonly assumed. reply tashoecraft 21 hours agorootparentprevI've heard the issue with high fructose corn syrup is that the levels of sugar in it does not correspond to its sweetness level. Corn syrup on its own is not very sweet, so to make it taste sweet you have to add much higher levels than if you had used other types of sweetener. reply vel0city 18 hours agorootparentIt's the opposite. You'll add more sucrose to hit the same sweetness experience as most HFCS blends, not less. reply cwillu 21 hours agorootparentprevNot sure how to square that with the chemical composition of hfcs. reply 01100011 21 hours agorootparentprevRight. The main issue with HFCS is that it's really cheap so it gets added to a lot of things that wouldn't normally contain sugar or it is added in greater amounts than other sugars. reply meiraleal 22 hours agoparentprev> Glad to see more research on this You are glad to argue for exactly the opposite of what the research found? Fructose is metabolized to lipids in the liver and that counts fruit juice, too. And the whole fruit. reply jcims 22 hours agoprevWent though a cancer journey with a loved one a few years ago. I was quite surprised at the complete lack of specialized guidance on nutrition. It was basically ‘eat healthy’, which isn’t bad advice but it seems like there are probably optimizations to be had there. (Of course there’s no end of it on the Internets, but as part of heathcare it was absent) reply stvltvs 22 hours agoparentWe tend to emphasize diet a lot, I think because it's something we can control, but it might not help as much as we hope. Eating a healthy, plant-forward diet while minimizing alcohol and red meat might give us most of the benefit we can squeeze out of diet for cancer risk reduction. reply lambdaba 21 hours agorootparent> minimizing red meat could you provide the evidence that convinced you that red meat is detrimental to health? reply toomuchtodo 21 hours agorootparenthttps://news.ycombinator.com/item?id=41945838 https://newatlas.com/medical/red-meat-iron-colorectal-cancer... https://www.nih.gov/news-events/nih-research-matters/risk-re... https://pubmed.ncbi.nlm.nih.gov/27597529/ reply liveoneggs 22 hours agoparentprevI shared this story before on here but my aunt found out she had lung cancer when her t2 diabetes suddenly cured itself. reply cipheredStones 21 hours agorootparentIs there a known mechanism by which that happens? Or was it just a weird idiosyncrasy of her body and her diseases? reply liveoneggs 18 hours agorootparentI assume the cancer ate all of the sugar instead of it floating around in her blood (or whatever the liver produces in excess sugar situations) reply adamredwoods 20 hours agoparentprevYou cannot stop cancer with diet alone. Full stop. If you could, then starvation would stop cancer before the patient dies. Or a radical diet would cure a patient. It doesn't. This has been debunked so often, yet people really want to believe that food diet can cure everything! reply bell-cot 22 hours agoparentprevCynical Reaction: There are no patent royalties, fat profits, nor bragging rights to be found in giving nutritional advice. reply stevenwoo 21 hours agorootparentThe first half of her book talks about nutrition and health but the second half talks about her company that offers services to give people personal guidance on nutrition and monitoring their health, so there are some attempts to do this. https://www.caseymeans.com/goodenergy reply WithinReason 22 hours agorootparentprevOr in researching nutrition reply jrpt 19 hours agorootparentprevI don’t know why this is downvoted. The lack of profit motive is a big reason that nutrition and supplements aren’t as well studied through rigorous trials as drug therapies. The ones that are run are funded by grants. Rather than just “more funding” I think there needs to be more systemic ways at reducing the cost of clinical trials or using alternate methods of getting high quality scientific data for answering these questions. For example, there is a good trial running now on ketogenic diet in glioblastoma patients, NCT05708352, I think with a NIH grant and maybe the NCI as well. Here is a video about it: https://www.youtube.com/watch?v=W31kR0MzyRA Food and nutrition is a big business though. reply andrewmcwatters 21 hours agoparentprevThere are people who think eating bread is healthy. “Eat health,” and “eat cleaner” are two phrases that are doing cultural damage. Even the more educated members of family believe this. reply alkyon 20 hours agorootparentWith low fructose content (less then carrots per 100g), it's not a stupid choice. Naturally, if you don't binge on it, otherwise 2-3 slices daily won't kill you. I mean wholegrain, sourdough bread, to be precise. reply Nasrudith 21 hours agorootparentprevHealthy is always relative. I mean eating bread is healthy compared to say, eating deep-fried cake. reply lm28469 21 hours agoparentprevThey also will gladly prescribe you statins for life without mentioning that losing your excess 30kg and walking every now and then would likely greatly improve your cholesterol issues (or even solve them) and improve your general health. You can apply this to pretty much any modern wide spread disease. I think doctors don't even bother because they assume people already do as much as they're willing to do, the problem is that the interests of capitalism are diametrically opposed to your well being so most people start with quite a disadvantage, just look at supermarkets: the alcohol, candies, coke, cakes aisles are all bigger than the healthy food aisle, together they're like 80% of the building reply cipheredStones 21 hours agorootparent\"Doctors won't mention that losing weight and exercising more will make you healthier\" is quite a take. I've heard exactly the opposite from any number of people: that if you're overweight at all, many doctors will tell \"lose weight and exercise\" and then usher you out the door, rather than pay attention to the specifics of your medical problems - sometimes missing serious issues as a result. reply SoftTalker 21 hours agorootparentWhen they have to turn patients over at the rate of 10 per hour due to the policy of the private equity group that owns their practice, they will be inclined to offer blanket advice that, while actually good and applicable for 80% of people, will tend to miss the edge cases. reply deanc 21 hours agoprevOn a similar note... There is a particular type of kidney disease called Polycystic Kidney disease which is genetic. Essentially cysts grow all over your kidneys, they swell up, and eventually fail (usually over many years). There is emerging research that glucose contributes to the growth of these cysts and early research suggests ketogenic diet can have a measurable impact on the growth of these cysts and improve kidney function. reply WorkerBee28474 22 hours agoprev> \"We were surprised that fructose was barely metabolized in the tumor types we tested... We quickly learned that the tumor cells alone don’t tell the whole story... one way in which high levels of fructose consumption promote tumor growth is by increasing the availability of circulating lipids in the blood. These lipids are building blocks for the cell membrane, and cancer cells need them to grow... Over the past few years, it’s become clear that many cancer cells prefer to take up lipids rather than make them A bit of a red herring, but still interesting. reply adamredwoods 20 hours agoprevGuess what? Glucose limitation helps protect cancer cells! Gee, so much research on diet, and still no answers, tells me that diet research might not be the path to go down on stopping cancers. https://www.nature.com/articles/s42255-024-01166-w >> Most cancer cells rapidly consume glucose, which is severely reduced in the nutrient-scarce tumour microenvironment. In CRISPR-based genetic screens to identify metabolic pathways influenced by glucose restriction, we find that tumour-relevant glucose concentrations (low glucose) protect cancer cells from inhibition of de novo pyrimidine biosynthesis, a pathway that is frequently targeted by chemotherapy. reply spacephysics 19 hours agoparentMakes sense why ketogenic diets have been reported to slow down cancer progression https://pmc.ncbi.nlm.nih.gov/articles/PMC6375425/ reply adamredwoods 19 hours agorootparentThis \"research\" helps her run her side hustle: https://www.ketoonc.com/contact No diet has been conclusively shown to slow or stop cancer. I guarantee cancer cells can mutate to survive a keto diet. reply wonder_er 14 hours agoprevThe books \"The Case Against Sugar\" and \"Tripping over the Truth: How the Metabolic Theory of Cancer Is Overturning One of Medicine's Most Entrenched Paradigms\" seems worth plugging. Sugar is not safe, even though it is ubiquitous. For many reasons avoiding meat seems reasonable, too. I used to be full vegetarian, for years, and then eventually re-added salmon and sardines. It's a tiny step to full-on vegetarian keto, which is as good for you as full-on fasting is, I think. I don't eat breakfast, veggie omelette for lunch (sautee some mushrooms and zuccini, or broccoli, perhaps, crack some eggs in once they cook down, give lots of olive oil and a side of something like kimchi, cabbage, or a pickle. Usually eat whatever I want for dinner, including icecream, sometimes, but I spend most of the day in a low-sugar state of being, and if I 'clean up' my dinner(s) it's easy to eat a zero-added-sugar, extremely-low-carb, no-animal-protein, extremely-satiating form of food intake. I really like it for me, and when I cook any of my foods, either when trying to be 'keto' or adding things like sweet potato, banana bread, ice cream, pizza, whatever, it's always well-received and considered delicious. reply __MatrixMan__ 21 hours agoprevI feel like fruit used to be food but now the available cultivars resemble candy. Or maybe it's just my taste buds changing, not sure. reply msarrel 3 hours agoprevWe've known that sugars increase the rate of tumor growth for 50 years already. reply hex4def6 21 hours agoprevI feel like I came across some similar research years ago. It brought up a question that I'm hoping someone in this field could answer: Does it make sense to speed up cancer cell replication while providing chemo drugs? It seems like this would result in greater discrimination between fast-replicating cells (cancer) and normal cells. In turn, this would allow faster chemo treatments, or less collateral damage. reply ramon156 19 hours agoparentI'm not in the field at all, but this also sounds slightly dangerous in a way. Chemo isn't instant and constant, so you'd have to time this well i suppose. Nonetheless this is a take from someone who has limited scientific knowledge, so I'm curious to see responses on this question! reply alecdote 19 hours agoparentprevI’m a biologist (but not a cancer biologist). A lecturer in college explained that this is how chemo works. It impacts actively dividing cells. Your own stem cells divide but not as fast as tumor’s. So this seems right in theory. reply lambdaba 22 hours agoprevSteve Jobs is a prime example, he tried to treat his pancreatic cancer with fruit. Also see Thomas Seyfried's work on ketogenic diets for cancer https://www.bc.edu/bc-web/bcnews/science-tech-and-health/bio... reply throw646577 22 hours agoparentHe's not a prime example of anything to do with fruit making things worse. His chosen therapy just as likely had no worsening impact on his cancer; the problem was it had no positive impact either and he put off surgery. He had a rare form of pancreatic cancer that grows from the tip of the pancreas and can sometimes be snipped out without consequence early enough that it doesn't spread. He presumably freaked out about it all nevertheless (because it's terrifying), avoided surgery for too long and ended up having a Whipple procedure, which is fucking brutal. But there's still a pretty good chance he would have had early surgery and still ended up needing a Whipple procedure and still have passed away on roughly the same timescale. Because pancreatic cancer is a stealthy thing, and the Whipple procedure comes with its own frightening future. It's really sad he freaked out, it's really sad he didn't listen. But it's not that unusual. He's far from unique in making irrational decisions in the face of terrifying diagnoses. Should it have been a slam dunk decision? Yeah. Of course. But there we are. reply lambdaba 22 hours agorootparentHe was following \"fruitarianism\" on and off even before his diagnosis, I think it's reasonable to suspect it had something to do with his cancer, but I wasn't aware of those details, so thanks for providing them. reply throw646577 22 hours agorootparentAnd yet: https://pubmed.ncbi.nlm.nih.gov/16172215/ I mean... in terms of our scientific understanding at this point, sporadic pancreatic cancer just comes from nowhere. We don't know why. In the case of the parent I lost to it, it could have been triggered by a response to inflammation from previous surgery. It seems plausible. But nobody knows. I just think people snarking or factoiding about Steve Jobs' panicked response to an awful disease comes across as projection of other judgements more than anything else. He was a complicated, self-reliant person who made a bad decision when he was scared. And it probably (but by no means certainly) cost him a few years of health. And now he is gone. And it's still sad for the people who lost him and for the wider world. reply lambdaba 22 hours agorootparentI'm sorry about your parent. I didn't mean to dunk on Steve Jobs, but I'm sure even he would agree he had an eccentric and extremist personality and the bit about fruitarianism seems very relevant esp with happenings like [Ashton Kutcher being hospitalized when he tried to mimick it](https://www.theguardian.com/film/2013/jan/28/ashton-kutcher-...) relevant quote: > \"First of all, the fruitarian diet can lead to, like, severe issues,\" Kutcher told USA Today. \"I went to the hospital like two days before we started shooting the movie. I was like doubled over in pain. My pancreas levels were completely out of whack. It was really terrifying … considering everything.\" reply znpy 20 hours agorootparentprevsteve jobs was known to be an asshole. i wonder how much that had an impact on dealing with the condition. on the other hand, I wonder what kind of steve jobs would the world have had, had he survived. would he have pushed cancer research further, somehow? maybe by getting involved and providing funding and technology, maybe novel technology, for cancer research? or would he have gone full-cynical and focused on something else entirely? we'll never know, for sure. reply amaurose 22 hours agorootparentprevnext [21 more] [flagged] throw646577 22 hours agorootparentnext [21 more] [flagged] fsckboy 22 hours agorootparentanecdata is no substitute for controlled studies. This study says fructose may have cancer implications, and Steve Jobs did focus on consuming fruit before he died of cancer. That there were treatments available that he forwent is immaterial, he's still a fructose datapoint in this context. i hate nerd tunnel vision. \"I know a lot about some related things so I must know the answer. reply swatcoder 22 hours agorootparentWith respect, it's absurdly more specious and naive to go from \"mouse study suggests contigent relationship between fructose and cancer cell behavior as seen in the specific limited models of the study, inviting further research\" to \"Public figure with especially agressive and entirely unrelated cancer reportedly ate something that happens to contain fructose and eventually died! Same thing!\" reply lambdaba 22 hours agorootparentHe didn't just eat some fruit, he ate ONLY fruit and JUICED it. He MEGADOSED on fruit. reply swatcoder 22 hours agorootparentThat still doesn't inform us in any way that directly relates to the study. In the big picture of cancer research, there are countless caveats and idiosycracies because most cancers are more different from each other than they are similar. This is only amplified when you're trying to make inferences about dietary consumption, where interactions between coincident foodstuffs have large chaotic effects and species (and individuals) often have different digestive behavior on the way from dietary food to some intracellular interaction. Ignoring all that inherent, acknowledged, complexity so that one might harp on some dead punching bag whose story is only known through the lens of commercial media is about as anti-scientific as you could be. reply fsckboy 21 hours agorootparentnobody here was drawing vast conclusions from Steve Jobs eating fruit; there is nothing for you to deflate Just as celebrities with diseases can draw attention to that disease, for the good with regard to other sufferers, a study about fructose and cancer can draw attention to \"Steve Jobs diet was fructose\". Attention drawn. There's nothing you can do about it. And you are not debunking anything because nobody asserted anything except a single fact; you're just shouting in a mirror about how clever you are. reply throw646577 22 hours agorootparentprev> anecdata is no substitute for controlled studies. Using Steve Jobs as an example is also anecdata. > That there were treatments available that he forwent is immaterial Is it really? > i hate nerd tunnel vision I don't hate anyone but if I did, I'd probably start with people who don't extend compassion and understanding where it costs nothing to do so. reply fsckboy 21 hours agorootparent> Using Steve Jobs as an example is also anecdata. I said Steve Jobs was a single datapoint. You said your parent gave you the right to tell people here to fuck off. you don't think other people here have close relatives who died of cancer? and this informs us about fructose in this study how? reply throw646577 20 hours agorootparent> You said your parent gave you the right to tell people here to fuck off. I literally did not, did I? I mean, that is pure projection. I'm sure I don't have any such right, nor should I need it. reply meiraleal 22 hours agorootparentprevCompassion is not an excuse for a Reality distortion field. This killed Steve Jobs and might kill many more. reply throw646577 22 hours agorootparentPancreatic cancer killed him, you absolute nitwit. reply amaurose 21 hours agorootparentThe one who ends up calling the other one names always loses. reply meiraleal 21 hours agorootparentprevcalling people names won't change the fact that his life-long diet caused his cancer and killed him. You should read what the research is saying in place of being here offending people that by their personal experience agrees with the finding that \"fructose in diet enhances tumor growth\". You are not the only person with experience with this kind of cancer in the family and have no compassion for the many sugar addicts going through cancer getting served fruit juice when they go to the hospital, which is poison for them. reply pell 21 hours agorootparent> calling people names won't change the fact that his life-long diet caused his cancer and killed him. Steve Jobs was not a life-long fruitarian. He dabbled in all sorts of different diets. According to Walter Isaacson’s biography he seems to have generally lived as a pescatarian. Interestingly enough that diet is considered one of the healthier ones. Obviously Steve Jobs should not have delayed his surgery. I understand that people are angry that a man with his means was still able to get a transplant and all the medical care that is unavailable for the rest of us. I understand. However to assume that his disease (and death) was the result of a diet choice when there is no credible evidence for it seems rather absurd. reply meiraleal 17 hours agorootparent> I understand that people are angry that a man with his means was still able to get a transplant That's something I never heard. Are you angry because Steve Jobs got a transplant? > However to assume that his disease (and death) was the result of a diet choice when there is no credible evidence for it seems rather absurd. Have you tried reading the research in the post before commenting? reply pell 9 hours agorootparent> Are you angry because Steve Jobs got a transplant? I am not angry however that was a sentiment often linked to the fact that he delayed his surgery. > Have you tried reading the research in the post before commenting? Unless I missed a comment there was absolutely no serious research shared that credibly connects let alone confirms that his death was connected to this diet choice. reply throw646577 21 hours agorootparentprev> the fact that his life-long diet caused his cancer and killed him It's not even a fact that it was a life-long diet. It's certainly not a known fact that it caused his cancer. You don't know. Nobody knows. reply pen2l 22 hours agorootparentprevFructose in fruits is trapped inside webs of fiber, the gut can more capably deal with fructose in this form vs. fructose found in candy or soda. GP is more right than wrong. reply meiraleal 22 hours agorootparent> Fructose in fruits is trapped inside webs of fiber, the gut can more capably deal with fructose in this form that's bro science. With or without fiber, fructose will be separated and metabolized by the liver and go through the cycle described by the research. Read the thing. reply pen2l 21 hours agorootparentThis comment is a perfect example of what the origin of complete nonsense with sticking power looks like on the internet: a couple insults and denigrations, a refutal without substantiation, and a confident suggestion for one to do their own research (of course without providing a reference study from a journal of consideration) and calling it a day. Fructose from whole fruits is processed and metabolized differently. Indeed fructose trapped inside fiber will take some time to be digested, and a sharp blood spike is prevented. Sugar from fruits is not bad in the way sugar from soda is bad. https://www.bmj.com/content/347/bmj.f5001.abstract reply meiraleal 18 hours agorootparentJust read the research of the post, could you? reply methyl 22 hours agoparentprevPancreatic cancer doesn’t have good prognosis in the first place, no matter what you eat or don’t eat. reply twostorytower 22 hours agorootparentBut his was actually one of the most treatable forms and he could have potentially had many many years added to his life had he initially listened to his doctors. reply SoftTalker 22 hours agorootparentYes I was downvoted last time I posted this, but: Don't try to be your own lawyer, and don't try to be your own doctor. reply mchannon 20 hours agorootparentprevThis is the tail wagging the dog. Jobs' fruitarianism meant he didn't eat eggs, meat, dairy, or other sources of selenium. Fruit doesn't contain selenium. One of Selenium deficiency's side effects, well established in the literature: pancreatic cancer. reply lm28469 21 hours agoparentprev> Steve Jobs is a prime example, he tried to treat his pancreatic cancer with fruit. Pancreatic cancer will fuck you up pretty much no matter what you do. I think it literally is the cancer with the smallest survival rate reply medler 18 hours agorootparentHe was lucky to get a rare, slow moving form of the cancer that can be cured with surgery if caught early enough. Unfortunately he waited nine months before getting surgery. reply former-aws 17 hours agorootparentprevGlioblastoma has entered the chat :-) reply outworlder 22 hours agoparentprev> Steve Jobs is a prime example, he tried to treat his pancreatic cancer with fruit. That is true – and probably didn't help. But his case is a poor example since he delayed treatment for months. EDIT: Also, actual fruits are probably not an issue. It's difficult to eat too much fruit, unless in juice form... which he did. reply andai 22 hours agoparentprevAnother comment mentioned that the mechanism by which fructose promotes tumor growth is that it increases circulating lipids. But I'm wondering, doesn't keto (which seems to fight cancer) also do that? reply throw646577 22 hours agorootparentNot much evidence that keto fights cancer any better than any other exclusion diet; it might even be worse than meat exclusion diets. reply andai 9 hours agorootparentInteresting. There's some evidence that fasting fights cancer, and there's a good deal of overlap between the effects of keto and fasting. Though obviously \"stop eating everything that gives you cancer\" is also a good start, and you can definitely eat processed garbage even on keto. reply lambdaba 22 hours agorootparentprev\"Not much evidence\"? That is so far from the truth. reply rd11235 18 hours agoprevAn obvious question that isn’t answered (in this article - not sure about the paper itself) is whether feeding fructose results in MORE tumor growth than feeding glucose (or other sources of calories) Without knowing this, it doesn’t make any sense to assume that there is anything inherently bad about fructose, at least other than the mechanistic arguments mentioned in the article (which are weak if not backed up by empirical evidence) reply mchannon 20 hours agoprevFun fact: Sucrose, from our friends cane sugar and beet sugar, is a glucose/dextrose molecule tied to a fructose molecule. And when you digest it, you get the effects of some of each. Another fun fact: The \"H\" in HFCS stems from the fact just plain \"corn syrup\" is defined as 0% fructose. Fast food restaurants push the percentage to 58%+ fructose to turbocharge the sweetest taste in their sodas. reply zenon 21 hours agoprevMaybe worth noting that you have to eat a pretty large dose of fructose for it to make it all the way to the liver. More than in a few pieces of fruit. The small intestine converts up to 1g/kg (of bodyweight) fructose to glucose and other metabolites before it enters the liver portal vein. reply beastman82 22 hours agoprevI just asked Gemini about this, and it linked me to this article, published today. Maybe we have a reliable nutrition guide after all! reply defensem3ch 21 hours agoprevwhat was the source of fructose used in the study? it doesn't say reply eecc 21 hours agoprevAlso this time “In mice”? (I hope) reply pharrington 22 hours agoprevcorrect title is \"Research reveals how fructose in diet enhances tumor growth\" reply bevesce- 13 hours agoprevSo now even fruits are not healthy? Are leafy greens the only food category we can eat without feeling guilty? reply pixelpoet 22 hours agoprevnext [3 more] [flagged] throw646577 22 hours agoparentPancreatic cancer is one of the least funny things. Don't be flippant. reply margalabargala 22 hours agorootparentI don't think that OP's comment was any more flippant than is appropriate for the treatment in question. reply uhtred 21 hours agoprev [–] Why are so many techies also keto fanboys. I suspect it's because diets like keto say it's OK to eat loads of meat and basically don't require any sacrifice. Fuck the environment and fuck other animals right bros?! reply __MatrixMan__ 21 hours agoparent [–] It's hard to find tech jobs that aren't in support of some activity which is making the world a less pleasant place to be. So you're starting from a pool which has already has some cognitive dissonance momentum going. I don't think it's too surprising that that momentum would carry over into dissonance re: the side effects of your diet. reply Nasrudith 20 hours agorootparent [–] Take your blinders off, please. Tech is nowhere unique in that issue and pretending so itself is its own major delusion of exceptionalism. reply __MatrixMan__ 16 hours agorootparent [–] Pointing out that other groups have the same problem doesn't make it any less of a problem. Besides, money is an information technology, if anybody is going to recognize its bugs as such and fix them, it's going to be us. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fructose consumption has increased significantly over the last 50 years, primarily due to high-fructose corn syrup in beverages and processed foods.",
      "Research from Washington University in St. Louis indicates that while fructose doesn't directly fuel tumors, the liver converts it into nutrients that support tumor growth in cancers such as melanoma, breast, and cervical cancer.",
      "The study suggests potential new cancer treatments by exploring methods to prevent fructose from aiding tumor growth, possibly through dietary changes or pharmaceuticals, with partial funding from the National Institutes of Health (NIH)."
    ],
    "commentSummary": [
      "Research from Washington University indicates that high fructose consumption may promote tumor growth by increasing circulating lipids, which are utilized by cancer cells for growth.- While fructose in fruits is typically safe due to additional nutrients, excessive intake from sources like high fructose corn syrup could be risky.- The study underscores the complexity of cancer metabolism and the potential influence of diet on cancer progression, though further research is necessary to fully comprehend these mechanisms."
    ],
    "points": 185,
    "commentCount": 176,
    "retryCount": 0,
    "time": 1733514389
  },
  {
    "id": 42346274,
    "title": "Biggest shell programs",
    "originLink": "https://github.com/oils-for-unix/oils/wiki/The-Biggest-Shell-Programs-in-the-World",
    "originBody": "oils-for-unix / oils Public Notifications Fork 160 Star 2.9k Code Issues 499 Pull requests 27 Actions Projects Wiki Security Insights The Biggest Shell Programs in the World Jump to bottom Edit New page Hal Blackburn edited this page · 47 revisions Help me fill out this page! It's freely editable. What programs should be listed? I'm using \"biggest\" in the sense of substantial, not necessarily the raw number of lines. Hand-written shell scripts. There are many big autoconf-generated scripts, like the 70K-line coreutils script, but I don't consider them substantial in this sense. Shell programs that use data structures and algorithms. As a counterexample, bash-completion is pretty sophisticated, but it's also repetitive because it has a relatively simple function for every command on a Unix machine. Shell programs that are over 5K lines (though there may be exceptions). The biggest shell programs that aren't repetitive tend to be in the 10K+ line range. I haven't seen any programs over 100K lines. The List akinomyoga/ble.sh -- 61K LoC (46K SLoC) in total. Bash Line Editor---fish-like interactive line editor in pure bash! The main file out/ble.sh has 30K lines of code (23K SLoC), but there are 60K+ lines of code in the repo in total. There are many comments (in Japanese). How Interactive Shells Work has a nice overview of how ble.sh works. It's very sophisticated, using bind -x to read raw bytes from the terminal, decoding those itself in mulitiple explicit state machines, maintaining a drawing buffer, updating the buffer, etc. It has timing and \"fibers\", etc. This comment on issue 663 has details on the shell parser! I think this is one of the most sophisticated uses of data structures in shell I've seen. We are trying to run ble.sh under OSH. It mostly parses. First commit in 2015 with 8K LoC / 6 SLoC. The actual development has started in 2013. kalua - OpenWRT addon ~56K SLoC/lines of POSIX shell bashdb, the bash debugger. ~14K lines of bash. Interesting history: Implementing Debuggers drwetter/testssl.sh -- 21K lines of bash in a single file! Appears to be hand-written. History: Back in 2006 it started with a few openssl commands.... (Hit issue #606 parsing it.) Simplenetes: Kubernetes in 17K lines of Shell. Amazing! But seems dormant. Hacker News Thread. Related to my bold claim here rkhunter (official site) -- 21K lines of Bourne shell written from 2003-2018! romkatv/powerlevel10k -- 12K lines of zsh scripts in the directory internal/. There are other 8K lines of configs and helper scripts. First commit in 2014. dylanaraps/neofetch -- 10K lines of bash 3.2. Displays system information. May also do something interesting with images (?) first commit 2015 xwmx/nb -- 24K LoC (19K SLoC) of bash in nb itself. An additional 86K LoC (55K SLoC) if we count bats tests as bash. First commit in 2014, but active commit history starts in early 2016. distrobox -- over 7k lines of bash script. Use any linux distribution inside your terminal. acme.sh -- 7K lines of shell script. Issues and renews certificates. inxi 2.3.56 [obsolete] -- 16K lines of bash. A fork of infobash in 2008 (889 lines then). infobash has started in 2005. From v2.9, inxi is replaced by the Perl implementation. bashforth -- At ~3800 lines this isn't huge, but it apparently implements a real programming language. It has a lot of whitespace and comments. vegardit/bash-funk -- 27K LoC (24K SLoC) in total. A Bash library. The first commit in May, 2017 (with 10K LoC / 8K SLoC). Relax-and-Recover - 35K LoC (24K SLoC). Backup and restoration tool. First git commit March, 2009 (with 4K LoC / 3K SLoC) abcde / A Better CD Encoder, used for ripping CDs, weighs in at around 5.5k LoC. thc-segfault - 3.3K LoC. A pubnix server built mostly using Bash ffmpeg/configure - 8.4K LoC. FFmpeg's configure script is hand-written ffhevc - 4k LoC. Fully hand-written Bash wrapper script for encoding to HEVC video using FFmpeg and libx265. ffx264 - 3.9k LoC. Fully hand-written Bash wrapper script for encoding to H.264/AVC video using FFmpeg and libx264. h264enc - 9.2k LoC. Fully hand-written Bash wrapper script for MEncoder for encoding to H.264/AVC video. bashtop - 5.3k LoC. A resource monitor Shell-Like Languages / DSLs modernish is a portable shell dialect written in shell bats is a DSL for writing tests. Generates bash code. bashible is an Ansible-like DSL in bash. comments clash is an object oriented framework compatible with any modern POSIX shell. bash Infinity is a standard library and a boilerplate framework for bash. Smaller Programs Alpine, Aboriginal, Debian scripts -- see blog post Completion scripts are big, but often repetitive. _git Zsh completion -- 8.3k lines of code git-completion.bash Docker completion dyne/Tomb is a ~3500 line zsh script. Basalt -- a full-featured package manager in pure Bash (est. ~2021). Although it's only a few thousand lines, there's already a rich ecosystem (15+ apps/libs) that includes many thousand more lines. Some Bash libraries include (at various stages of development): bash-core, a library for enhancing the trap and shopt builtins, adding stacktraces, and many essential amenities bash-object, a library for constructing arbitrarily nested data structures in pure Bash (nearly 200 tests) bash-json, a library for parsing and printing JSON in pure Bash tablespoon/fun/cli-clock - a clock with multiline characters written in bash. json.bash / jb — a command-line tool and bash library that creates JSON (~1700 lines, plus ~3000 lines of tests). Links OSH \"Wild\" Tests parse over a million lines of shell. However most of these are small programs and distro package definitions like Alpine PKGBUILD and Gentoo ebuilds, which are repetitive. Shell Programs That Run Under OSH shell script are dangerous The shell is a program to handle your system internally via an interactive console(or not)... It's feature full and extremely dangerous. it's not done to produce applications. Add a custom footer Pages 137 Home Ad Hoc Protocols in Unix Alternative Regex Syntax Alternative Shells Architecture of Various Interpreters Autocompletion Use Cases BootstrappingCaseStudies Can Oil Use Nix? Capers Carrots CLAP Grammar Proposal Command vs Expression Mode Compact AST Representation Compiler Engineer Job Compiler Engineer Notes Completion Chat November 2019 Composable Distributed OS Config Dialect Contributing Coprocess Language Support Coprocess Protocol Proposal Coprocess Protocol V2 CSTR Proposal Debugging Completion Scripts Developing Oil With Nix Difficulties With Unix Signals Distributed Shell Diversity in Command Line Syntax Documentation Strategy each Keyword in Oil Exhaustive Test Suite Explicit Framing Protocol Proposal ExternalResources FAQ: Why Not Write Oil in X? Feature Detection Is Better than Version Detection Fish Oil Brainstorming Flag Parsers in Various Languages GitTips Gradually Upgrading Shell to Oil Gradually Upgrading Shell to YSH Hay Headless Mode How Interactive Shells Work How Terminals Work How To Test OSH Implementations of Shell Autocompletion Implementer Lore About Garbage Collection Implementing Debuggers Implementing the Oil Expression Language Interactive Shell Internal DSLs for Shell Language Composition Bestiary Language Data Models Language Design and Theory of Computation Language Design Principles Language Subsets Little Languages Lossless Syntax Tree Pattern M by N Code Explosions Making Pull Requests Metaprogramming Metaprogramming Use Cases Migration Guide Oil and the R Language Oil Blog Planning Oil Deployments Oil Dev Cheat Sheet Oil Dev Tips Oil Language Docs Oil Language FAQ Oil Native Quick Start Oil Ninja Build Oil Parser Generator Project Oil Windows Port Oils Deployments Oils Packaging Guidelines Oilshell Usage (The table missing in getting started.md) OSH Builtins: \"Hello, world!\" Example OSH Compatibility Tips OSH Optimization Log OSH Parser OSH versus Oil OSH Word Evaluation Algorithm Parsing is Difficult Parsing Models Cheatsheet Parsing Shell Quotes Requires Parsing the Whole Language ParsingCaseStudies Partial Evaluation Patterns and Anti Patterns Perlis Thompson Principle Polyglot Language Understanding Project Goals Projects Already Doing Something Like Shellac Python App Bundle QSN QTT Recent Dev Friction Recommended Reading Running ble.sh With Oil Running Oils as build shell SHCOMP Protocol Proposal Shell Almost Has a JSON Analogue Shell Autocompletion Shell Programs That Run Under OSH Shell Security Problems Shell WTFs Shellac Protocol Proposal Shellac Protocol Proposal V2 Shellac Test Cases Signal Handling in Oil Slogans, Fallacies, and Concepts Soil Spec Tests Startup Time Measurements Structured Data in Oil Structured Data Over Pipes Survey of Config Languages Task Under NLNet Grant Tasks Under NLNet Grant Technical Writing Testing the Interactive Shell The Biggest Shell Programs in the World The List Shell-Like Languages / DSLs Smaller Programs Links Tips on Using pgen2 Toil Travis CI for Oil TSV2 TSV2 Proposal Unifying Make and Shell Unix Tools Using OSH as a Default Shell Using Zephyr ASDL Ways To Get Feedback Prioritized What Is Expected to Run Under OSH Where Contributors Have Problems Where To Send Feedback Why Lexing and Parsing Should Be Separate Why Use Oil? Show 122 more pages… Add a custom sidebar Clone this wiki locally",
    "commentLink": "https://news.ycombinator.com/item?id=42346274",
    "commentBody": "Biggest shell programs (github.com/oils-for-unix)169 points by todsacerdoti 18 hours agohidepastfavorite82 comments svilen_dobrev 2 minutes agoaround ~2000, my build/install script had to simulate some kind of OO-like inheritance.. And there was python but noone understood it (and even less had it installed), so: bash - aliases had priority over funcs which had priority to whatever executables found in PATH.. so here you go - whole 3 levels of it, with lowest/PATH being changeable.. reply PeterWhittaker 16 hours agoprevOh, no, now I have to go dig out some of mine.... The first really big one I wrote was the ~7000 line installer for the Enrust CA and directory, which ran on, well, all Unixes at that time. It didn't initially, of course, but it grew with customer demand. The installation itself wasn't especially complicated, but upgrades were, a little, and this was back when every utility on every Unix had slight variations. Much of the script was figuring out and managing those differences, much was error detection and recovery and rollback, some was a very primitive form of package and dependency management.... DEC's Unix (the other one, not Ultrix) was the most baffling. It took me days to realize that all command line utilities truncated their output at column width. Every single one. Over 30 years later and that one still stands out. Every release of HP-UX had breaking changes, and we covered 6.5 to 11, IIRC. I barely remember Ultrix or the Novell one or Next, or Sequent. I do remember AIX as being weird but I don't remember why. And of course even Sun's three/four OS's had their differences (SunOS pre 4.1.3; 4.1.3; Solaris pre 2; and 2+) but they had great FMs. The best. reply emmelaich 16 hours agoparentThat column truncation sounds bizarre. Are you sure the terminal didn't have some sort of sideways scroll available? reply dspillett 9 hours agorootparentI think he was meaning that they truncated the lines even when called from a script, with their output going somewhere other than a terminal, not just when run interactively reply nikau 2 hours agorootparentIf he is talking about osf1/tru64 that's the first one heard of it reply emmelaich 6 hours agorootparentprevYep, but I'm curious enough to quiz it. Weirdly, today I ran wish in MacOS Sequoia (15.1.x) and had the (exception) output truncated at terminal width! reply PeterWhittaker 5 hours agorootparentdspillett was exactly right: ps, e.g., truncated its output at $COLUMNS and there was no horizontal scroll. As suggested above, it did this even when called from a script. The fix was easy, set COLUMNS ridiculously large if DEC Unix, but it took days of WTF apparent UB before I realized how simple was what was happening. It just seemed haphazard: I'd reposition and resize a window so I could run the script in one while manually running the commands in another, get inconsistent results, rinse, repeat... ...and eventually realize the common element in each test was me, and the variations I was introducing were window size. I cursed their engineers for trying to be \"helpful\" and keep things \"pretty\". reply raffraffraff 8 hours agorootparentprev:O reply raffraffraff 8 hours agoparentprevI made it to thousands but more like 2000. At least I only had to support Redhat and Ubuntu (modern ones, at that) reply throw16180339 15 hours agoparentprev> DEC's Unix (the other one, not Ultrix) was the most baffling. It took me days to realize that all command line utilities truncated their output at column width. Every single one. Over 30 years later and that one still stands out. Do you mean OSF1/, Digital Unix, or Tru64 Unix? reply PeterWhittaker 5 hours agorootparentOh, yes, I think it was Digital Unix. IIRC, we toyed with OSF/1, but there wasn't much call for it. reply banku_brougham 12 hours agoparentprevThank you for your service, Im so glad you could share. Id be interested to read more. reply RodgerTheGreat 16 hours agoprevAt one point I considered writing an interpreter for my scripting language Lil in bash to maximize portability, but quickly realized that floating-point arithmetic would be extremely painful (can't even necessarily depend on bc/dc being available in every environment) and some of the machines in my arsenal have older versions of bash with very limited support for associative arrays. My compromise was to instead target AWK, which is a much more pleasant general-purpose language than most shells, and available in any POSIX environment: https://beyondloom.com/blog/lila.html reply seiferteric 13 hours agoparent> can't even necessarily depend on bc/dc being available in every environment Just discovered this myself, also trying to make a language target shell. Was really surprised bc/dc was not present I think in Ubuntu install in WSL2. Also using awk for floating point math, but just shelling out to it. reply RodgerTheGreat 13 hours agorootparentYep! I considered shelling out to AWK for the same reason, as a bc/dc alternative, but rapidly found that nearly everything else bash could do was easier and less error-prone (and workable on much older systems) if I moved the whole script into pure AWK. reply kjellsbells 2 hours agorootparentprevOne of those occasional reminders that Linux != UNIX, I guess. Bc is mandatory in POSIX I believe but Linux never took that path. https://pubs.opengroup.org/onlinepubs/9699919799.2008edition... reply khushy 12 minutes agoprevMost shell script installers are works of art reply ykonstant 9 hours agoprevMany of these programs are true gems; the rkhunter script, for instance is both nice code (can be improved) and a treasure trove of information*. Note that much of the code size of these scripts is dedicated to ensuring that the right utilities exist across the various platforms and perform as expected with their various command line options. This is the worst pain point of any serious shell script author, even worse than signals and subprocesses (unless one enjoys the pain). *Information that, I would argue, would be less transparent if rkhunter had been written in a \"proper\" programming language. It might be shoved off in some records in data structures to be retrieved; actions might be complex combinations of various functions---or, woe, methods and classes---on nested data structures; logging could be JSON-Bourned into pieces and compressed in some database to be accessed via other methods and so on. Shell scripts, precisely due to the lack of such complex tools, tend to \"spill the beans\" on what is happening. This makes rkhunter, for instance, a decent documentation of various exploits and rootkits without having to dig into file upon file, structure upon structure, DB upon DB. reply ulrischa 10 hours agoprevI think the main problem with writing large programs as bash scripts is that shell scripting languages were never really designed for complexity. They excel at orchestrating small commands and gluing together existing tools in a quick, exploratory way. But when you start pushing beyond a few hundred lines of Bash, you run into a series of limitations that make long-term maintenance and scalability a headache. First, there’s the issue of readability. Bash's syntax can become downright cryptic as it grows. Variable scoping rules are subtle, error handling is primitive, and string handling quickly becomes messy. These factors translate into code that’s harder to maintain and reason about. As a result, future maintainers are likely to waste time deciphering what’s going on, and they’ll also have a harder time confidently making changes. Next, there’s the lack of robust tooling. With more mature languages, you get static analysis tools, linters, and debuggers that help you spot common mistakes early on. For bash, most of these are either missing or extremely limited. Without these guardrails, large bash programs are more prone to silent errors, regressions, and subtle bugs. Then there’s testing. While you can test bash scripts, the process is often more cumbersome. Complex logic or data structures make it even trickier. Plus, handling edge cases—like whitespace in filenames or unexpected environment conditions—means you end up writing a ton of defensive code that’s painful to verify thoroughly. Finally, the ecosystem just isn’t built for large-scale Bash development. You lose out on modularity, package management, standardized dependency handling, and all the other modern development patterns that languages like Python or Go provide. Over time, these deficits accumulate and slow you down. I think using Bash for one-off tasks or simple automation is fine — it's what it’s good at. But when you start thinking of building something substantial, you’re usually better off reaching for a language designed for building and maintaining complex applications. It saves time in the long run, even if the initial learning curve or setup might be slightly higher. reply JoyfulTurkey 1 hour agoparentDealing with this at work right now. Digging through thousands of lines of Bash. This script wasn’t written a long time ago, so no clue why they went with Bash. The script works but it always feels like something is going to break if I look at the code the wrong way. reply ndsipa_pomu 8 hours agoparentprevUsing ShellCheck as a linter can catch a lot of the common footguns and there are a LOT of footguns and/or unexpected behaviour that can catch out even experienced Bash writers. However, Bash/shell occupies a unique place in the hierarchy of languages in that it's available almost everywhere and will still be around in 30 years. If you want a program that will run almost everywhere and still run in 30 years time, then shell/Bash is a good choice. reply norir 2 hours agorootparentI'd almost always prefer c99 to shell for anything more than 100 lines of code or so. There is even a project I saw here recently that can bootstrap tcc in pure shell (which can then be used to bootstrap gcc). I'm somewhat skeptical that bash will still be used for anything but legacy scripts in 30 years, despite it's impressive longevity to this point, but I could sadly be proven wrong. reply ndsipa_pomu 1 hour agorootparentSo, if you wanted to write something that you would be pretty sure could easily run on machines in 30 years time, what would you use? I don't think c99 would be a good choice as processors will likely be different in 30 years time. If you had your program on e.g. a usb stick and you manage to load it onto a machine, it'd only be able to run if you had the same architecture. Even nowadays, you'd run into difficulties with arm and x86 differences. Some kind of bytecode language might seem better (e.g. java), but I have my doubts about backwards compatibility. I wonder if Java code from 20 years ago would just run happily on a new Java version. However, there's also the issue of Java not being installed everywhere. reply anthk 7 hours agoparentprevBash/ksh have -x as a debug/tracing argument. reply cperciva 14 hours agoprevThe FreeBSD Update client is about 3600 lines of sh code. Not huge compared to some of the other programs mentioned here, but I'm inclined to say that \"tool for updating an entire operating system\" is a pretty hefty amount of functionality. The code which builds the updates probably adds up to more lines, but that's split across many files. reply sebtron 9 hours agoparentFor reference: https://cgit.freebsd.org/src/tree/usr.sbin/freebsd-update/fr... reply craftkiller 5 hours agoparentprevpoudriere is roughly 3 FreeBSD Update clients of sh code: https://github.com/freebsd/poudriere/blob/master/src/share/p... reply voxadam 17 hours agoprevI'm pretty sure the largest handwritten shell program I used back in the day on a regular basis was abcde (A Better CD Encoder)[1] which clocks in at ~5500 LOC.[2] [1] https://abcde.einval.com [2] https://git.einval.com/cgi-bin/gitweb.cgi?p=abcde.git;a=blob... reply lelandfe 14 hours agoparentNot that I'd know anything about it, but this was one of the tools recommended on What.CD back in the day. Along with Max (my friends tell me) https://github.com/sbooth/Max reply voxadam 14 hours agorootparentProbably every rip I posted to What.CD and OiNK before it was created using abcde. Allegedly. reply lelandfe 4 hours agorootparentThe greatest loss was truly not even What.CD the incredible tracker but the forums. I've never again found a more concentrated group of people with taste. reply throwup238 14 hours agorootparentprevYou gotta use the SWIM acronym, for the ultimate callback to the aughts. reply voxadam 14 hours agorootparentHonestly, I came so close, so damn close. :) reply dlcarrier 15 hours agoparentprevI've used that before. It works really well and was pretty easy to use. I had no idea the whole thing is just a giant shell script. reply jefftk 6 hours agoprevBack when I worked on mod_pagespeed we wrote shell scripts for our end-to-end tests. This was expedient when getting started, but then we just kept using it long past when we should have switched away. At one point I got buy-in for switching to python, but (inexperience) I thought the right way to do it was to build up a parallel set of tests in python and then switch over once everything had been ported. This, of course, didn't work out. If I were doing this now I'd do it incrementally, since there's no reason you can't have a mix of shell and python during the transition. I count 10k lines of hand-written bash in the system tests: $ git clone git@github.com:apache/incubator-pagespeed-mod.git $ git clone git@github.com:apache/incubator-pagespeed-ngx.git $ find incubator-pagespeed-*\\ grep sh$\\ grep system_test\\ xargs cat\\ wc -l 10579 reply gjvc 4 hours agoparentlines ending indo not require \\ reply jefftk 1 hour agorootparentThanks for the tip! (I'm very conflicted about learning new things that make shell more convenient: I don't need more things pushing me toward using this in-many-ways-horrible-but-I'm-so-fast-with-it tool.) reply gjvc 1 hour agorootparentyou might (or might not) like this, then https://iam.georgecox.com/2023/02/05/command-dispatch-via-ba... reply michaelcampbell 2 hours agorootparentprevI (also?) never new this; thanks. My old finger memory will still probably put them in, alas. reply xyst 15 hours agoprevIt’s “only” 7.1K LoC, but my favorite is the “acme.sh” script which is used to issue and renew certs from Lets Encrypt. https://github.com/acmesh-official/acme.sh/blob/master/acme.... reply Brian_K_White 14 hours agoparentalready in the list reply dizhn 10 hours agorootparentParent might have meant that they like it. I was going to say the same thing. That one and distrobox are quite impressive in how well they work. reply michaelcampbell 2 hours agoprevProbably my largest one that was an order of magnitude smaller than these for the most part, but it checked that my VPN was up (or not) and started it if not. (And restarted various media based docker containers.) If it was up, it would do a speedcheck and record that for the IP the VPN was using, then check to see how that speed was compared to the average, with a standard deviation and z-score. It would then calculate how long it should wait before it recycled the VPN client. Slow VPN endpoints would cycle quicker, faster ones would wait longer to cycle. Speeds outsize a standard deviation or so would check quicker than the last delta, within 1 Z would expand the delta before it checked again. Another one about that size would, based on current time, scrape the local weather and sunup/sundown times for my lat/long, and determine how long to wait before turning on an outdoor hose, and for how long to run it via X10 with a switch on the laptop that was using a serial port to hook into the X10 devices. The hose was attached to a sprinkler on my roof which would spray down the roof to cool it off. Hotter (and sunnier) weather would run longer and wait shorter, and vice versa. I live in the US South where shedding those BTUs via evaporation did make a difference in my air conditioning power use. reply Y_Y 2 hours agoparentFor those of you not familiar with \"British Thermal Units\", they're about 7e-14 firkin square furlongs per square fortnight. reply branon 3 hours agoprevBiggest I know of is https://github.com/sonic2kk/steamtinkerlaunch/blob/master/st... 27k lines/24k loc reply eschneider 3 hours agoprevSometimes shell is the only thing you can guarantee is available and life is such you have to have portability, but in general, if you've got an enormous shell app, you might want to rethink your life choices. :/ reply norir 2 hours agoparentI hear this fairly often and I'm genuinely curious how often you have shell but _not_ a c compiler or the ability to install a c compiler via the shell. Once you have a c compiler, you can break out of shell and either write c programs that the shell script composes or install a better scripting language like lua. At this point in time, it feels quite niche to me that one would _need_ to exclusively use shell. reply chubot 55 minutes agorootparentThat's what I thought -- I thought that OS X was the main Unix where it is \"annoying\" to get a C compiler (huge XCode thing IIRC), and it isn't used for servers much. But people have told me stories about working for the government (my background was more \"big tech\", and video games, which are both extremely different) Some government/defense systems are extremely locked down, and they don't have C compilers So people make do with crazy shell script hacks. This is obviously suboptimal, but it is not that surprising in retrospect! reply bhawks 1 hour agorootparentprevThere are plenty of contexts where you won't have a compiler today - embedded (optimize for space) and very security hardened deployments (minimize attack surface). Historically people used to sell compilers - so minimizing installation to dev machines probably was a savings (and in those times space was at a premium everywhere). That said - I am with you, give me any other programming language besides shell! reply zabzonk 12 hours agoprevDon't know about the biggest, although it was quite big, , but the best shell program I ever wrote was in ReXX for a couple of IBM 4381s running VM/CMS which did distributed printing across a number of physical sites. It saved us a ton of money as it only needed a cheap serial terminal and printer and saved us so much money when IBM was wanting to charge us an ungodly amount for their own printers and associated comms. One of pieces of software I'm most proud of (written in the mid 1980s), to this day. reply banku_brougham 12 hours agoparentWell, you gotta post this somewhere so we can see reply zabzonk 11 hours agorootparentLike much of what i wrote before the days of distributed version control, this is now lost in the mists of time. And the code wouldn't belong to me anyway. reply oneeyedpigeon 6 hours agoprevOn a macOS machine, this: $ file /usr/bin/*grep \"shell script\"cut -f1 -d':'xargs wc -lsort -n gives me: 6431 /usr/bin/tkcon but that's another Tk script disguised as a shell script; the next is: 1030 /usr/bin/dtruss which is a shell script wrapper around dtrace. reply sn9 11 hours agoprevI think around a decade ago, I tried installing a copy of Mathematica and the installer from Wolfram was a bash program that was over a GB in size. I tried opening it up just to look at it and most text editors just absolutely choked on it. I can't remember, but it was either Vim xor Emacs that could finally handle opening it. reply zertrin 10 hours agoparentMost likely it embedded a (g)zip inside the shell script? I've seen this frequently. reply szszrk 10 hours agoparentprevSome installers include binaries inside their shell scripts. So the script extracts data from itself. Not great for transparency, but works and is single file. reply anthk 7 hours agorootparentshar, shell archives. reply szszrk 6 hours agorootparentA bit of a pain in the ass in some corporate environments, where binaries are scanned before use by DLP software ;/ reply sigoden 14 hours agoprevIf you're looking for a tool to simplify the building of big shell programs, I highly recommend using argc (https://github.com/sigoden/argc). It's a powerful Bash CLI framework that significantly simplifies the process of developing feature-rich command-line interfaces. reply fny 5 hours agoprevI feel like this merits having a Computer Benchmarks Game for different shells. reply kamaal 13 hours agoprevAs someone who has written and maintained large Perl programs at various points in my career. There is a reason why people do this- Java and Python like languages work fine when interfaces and formats are defined, and you often have 0 OS interaction. That is, you use JSON/XML/YAML or interact with a database or other programs via http(s). This creates an ideal situation where these languages can shine. When people do large quantity text and OS interaction work, languages like Java and Python are a giant pain. And you will begin to notice how Shell/Perl become a breeze to do this kind of work. This means nearly every automation task, chaotic non-standard interfaces, working with text/log files, or other data formats that are not structured(or at least well enough). Add to this Perl's commitment towards backwards compatibility, a large install base and performance. You have 0 alternatives apart from Perl if you are working to these kind of tasks. I have long believed that a big reason for so much manual drudgery these days, with large companies hiring thousands of people to do trivially easy to automate tasks is because Perl usage dropped. People attempt to use Python or Java to do some big automation tasks and quit soon enough when they are faced with the magnitude of verbosity and overall size of code they have to churn and maintain to get it done. reply GoblinSlayer 4 hours agoparentI did a big automation task in native code, because efficiency is desirable in such cases, while bash+grep favor running a new process for every text line. In order to be efficient, you need to minimize work, and thus batch and deduplicate it, which means you need to handle data in a stateful manner while tracking deduplication context, which is easier in a proper programming language, while bash+grep favor stateless text processing and thus result in much work duplication. Another strategy for minimization of work is accurate filtering, which is easier to express imperatively with nice formatting in a proper programming language, grep and regex are completely unsuitable for this. Then if you use line separated format, git awards you with escaping to accommodate for whatever, which is inconsistently supported and can be disabled by asking null terminated string format with -z option, I don't think bash has any way to handle it, while in a sufficiently low level language it's natural, and it also allows for incremental streaming so you don't have to start a new process for every text line. As a bonus you can use single code base for everything no matter if there's http or something else in the line. reply stackskipton 3 hours agoparentprevStrong disagree that it's because \"Omg, no more Perl\" but just complexity cranked up and that Perl person stitching scripts together became their full job and obviously Perl only got you so far. So now you have additional FTE who is probably expensive. Also, if end user is on Windows, there is already Perl like option on their desktop, it's called Powershell and will perform similar to Perl. reply hiAndrewQuinn 5 hours agoparentprevI've been seriously considering learning some Perl 5-fu ever since I realized it's installed by default on so many Linux and BSD systems. I think even OpenBSD comes with perl installed. That may not seem like a big advantage until you're working in an environment where you don't actually have the advantage of just installing things from the open Internet (or reaching the Internet at all). reply alsetmusic 16 hours agoprevI love exploring things like this. The demo for ble.sh interactive text editor made me chuckle with delight. reply robhlt 15 hours agoprevFireHOL is another pretty big one, around 20k lines. It's a neat firewall configuration tool with its own custom config format. https://github.com/firehol/firehol reply transcriptase 16 hours agoprevSometimes I do things I know are cursed for the sheer entertainment of being able to say it worked. E.g. my one absurdly complex R script that would write ungodly long bash scripts based on the output of various domain specific packages. It began: # Yeah yeah I know reply chasil 16 hours agoprevWhy is ReaR not on this list? https://relax-and-recover.org/ This is the equivalent of the \"Ignite\" tool under HP-UX. reply chubot 15 hours agoparentIt's a wiki, you can add it! Looks legit to me, e.g. https://github.com/rear/rear/blob/master/usr/share/rear/lib/... reply rajamaka 7 hours agoprevWould love to see the same for batch on Windows reply denistaran 1 hour agoparentIf you’re scripting on Windows, it’s better to use PowerShell instead of batch. Compared to Bash, PowerShell is also better suited for large scripts because it works with objects rather than plain text. This makes handling structured data like JSON, XML, or command outputs much easier, avoiding the need for error-prone text parsing. reply sixthDot 17 hours agopreveverything you can do in `git gui` is actually a silly shell script but that works for me. https://github.com/git/git/blob/master/git-gui/git-gui.sh reply ilyagr 16 hours agoparentIt (and gitk) are actually Tcl scripts. Note how the \"shell script\" you linked exec-s itself with `wish` reply anothername12 13 hours agoprevI would add Bash Forth to that. String-threaded concatenative programming! reply gosub100 6 hours agoprevSince the topic is shell, can I shamelessly ask a question? I'm an SRE for a service everyone has heard of. I have inadvertently pasted into my terminal prompt multiple times now, which has attempted to run each line as a command. I see there is a way to disable this at the shell for each client, but what about at the server level? This way I could enforce it as a policy, and not have to protect every single user (including myself) individually. Said differently, I want to keep everyone who ssh into a prod machine from being able to paste and execute multiple lines. But not forbid paste entirely. The only thing I could think of would be to recompile bash and detect if the input was from a tty. If so, require at least 200ms between commands, and error out if the threshold exceeded. This would still allow the first pasted command to run, however. reply porridgeraisin 6 hours agoparentBracketed paste might help you. It's an option for readline so it goes in ~/.inputrc. There's a way to set these options in bashrc as well which I don't remember. It inserts a control sequence before and after the pasted contents that makes bash not execute all the lines. Instead it will keep them in the command line, post which you can choose to execute all of them in one go with enter or cancel with ctrl C. reply teo_zero 1 hour agorootparentEverything you can do in inputrc can be done in bashrc if you prepend \"bind\". In this case: bind 'set enable-bracketed-paste on' reply fargle 1 hour agoparentpreveverybody works differently. what seems like a sensible guardrail for you would be extremely annoying for others. so whatever you do, it should be a feature, even defaulted on. but never a policy that you enforce to \"everyone who ssh into a prod machine\" if you find something that works well for you, add it as a suggestion to your developer docs. reply gosub100 1 hour agorootparentall good points, and it's not a great way to \"make friends and influence people\" by screwing with their workflow. After making this mistake at least twice myself (mainly due to fumbling with MacOS mouse/keyboard differences on my machine), I just wanted to prevent a disaster in the future from me or anyone else. But alas, I just need to be more careful and encourage others to learn from my mistakes :) reply TacticalCoder 5 hours agoparentprevNot an answer to your question but here's a \"fun\" thing I used to do... If you want to run a program from the CLI, which blocks you terminal (say an xterm), you can use that terminal as a temporary paste buffer. But with a trick. Imagine you want to run, say, Firefox like that (say because you'd like to see stdin/stderr output of what's going on without having to find to which log file it's outputting stuff: it's really just a silly example): xterm> firefoxfirefox/dev/null You can now use that xterm as a temp paste buffer. So, yup, a good old cat > /dev/null works wonder. reply mulle_nat 16 hours agoprevI think for sports, I could wrap all the various mulle-sde and mulle-bashfunction files back into one and make it > 100K lines. It wouldn't even be cheating, because it naturally fractalized into multiple sub-projects with sub-components from a monolithic script over time. reply tpoacher 3 hours agoprev [–] I'm writing a ticketing manager for the terminal entirely in bash. Reasonably non-trivial project, and it's been pretty enjoyable working \"exclusively\" with bash. (\"exclusively\" here used in quotes, because the whole point of a shell scripting language is to act as a glue between smaller programs or core utilities in the first place, which obviously may well have been written in other languages. but you get the point). Having said that, if I were to start experimenting with an altogether different shell, I would be very tempted to try jshell! Incidentally, I hate when projects say stuff like \"Oils is our upgrade path from bash to a better language and runtime\". Whether a change of this kind is an \"upgrade\" is completely subjective, and the wording is unnecessarily haughty / dismissive. And very often you realise that projects who say that kind of thing are basically just using the underlying tech wrongly, and trying to reinvent the wheel. Honestly, I've almost developed a knee reflex to seeing the words \"upgrade\" and \"better\" in this kind of context by now. Oils may be a cool project but that description is not making me want to find out more about it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The page highlights significant shell programs, focusing on substantial hand-written scripts rather than those generated by tools like autoconf.- Notable entries include akinomyoga/ble.sh, a Bash Line Editor with 61K lines of code (LoC), and kalua, an OpenWRT addon with 56K source lines of code (SLoC).- The list provides insights into the complexity and diversity of shell scripting, featuring programs that utilize data structures and algorithms, typically exceeding 5,000 lines of code."
    ],
    "commentSummary": [
      "The discussion focuses on the complexities and challenges of using large shell scripts, such as issues with readability, error handling, and portability.- Users suggest considering alternatives like Python or AWK for complex tasks due to better maintainability, while tools like ShellCheck can help improve shell script quality.- The conversation highlights the balance between using shell scripts for quick tasks and opting for more robust programming languages for larger projects."
    ],
    "points": 169,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1733533151
  },
  {
    "id": 42343692,
    "title": "DSPy – Programming–not prompting–LMs",
    "originLink": "https://dspy.ai/",
    "originBody": "Programming—not prompting—LMs DSPy is the framework for programming—rather than prompting—language models. It allows you to iterate fast on building modular AI systems and offers algorithms for optimizing their prompts and weights, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops. DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional Python code and use DSPy to teach your LM to deliver high-quality outputs. This lecture is a good conceptual introduction. Meet the community, seek help, or start contributing via our GitHub repo and Discord server. Getting Started I: Install DSPy and set up your LM > pip install -U dspy OpenAIAnthropicDatabricksLocal LMs on your laptopLocal LMs on a GPU serverOther providers You can authenticate by setting the OPENAI_API_KEY env variable or passing api_key below. 1 2 3 import dspy lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY') dspy.configure(lm=lm) You can authenticate by setting the ANTHROPIC_API_KEY env variable or passing api_key below. 1 2 3 import dspy lm = dspy.LM('anthropic/claude-3-opus-20240229', api_key='YOUR_ANTHROPIC_API_KEY') dspy.configure(lm=lm) If you're on the Databricks platform, authentication is automatic via their SDK. If not, you can set the env variables DATABRICKS_API_KEY and DATABRICKS_API_BASE, or pass api_key and api_base below. 1 2 3 import dspy lm = dspy.LM('databricks/databricks-meta-llama-3-1-70b-instruct') dspy.configure(lm=lm) First, install Ollama and launch its server with your LM. > curl -fsSL https://ollama.ai/install.shsh > ollama run llama3.2:1b Then, connect to it from your DSPy code. 1 2 3 import dspy lm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434', api_key='') dspy.configure(lm=lm) First, install SGLang and launch its server with your LM. > pip install \"sglang[all]\" > pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/ > CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --port 7501 --model-path meta-llama/Llama-3.1-8B-Instruct If you don't have access from Meta to download meta-llama/Llama-3.1-8B-Instruct, use Qwen/Qwen2.5-7B-Instruct for example. Next, connect to your local LM from your DSPy code as an OpenAI-compatible endpoint. 1 2 3 4 lm = dspy.LM(\"openai/meta-llama/Llama-3.1-8B-Instruct\", api_base=\"http://localhost:7501/v1\", # ensure this points to your port api_key=\"local\", model_type='chat') dspy.configure(lm=lm) In DSPy, you can use any of the dozens of LLM providers supported by LiteLLM. Simply follow their instructions for which {PROVIDER}_API_KEY to set and how to write pass the {provider_name}/{model_name} to the constructor. Some examples: anyscale/mistralai/Mistral-7B-Instruct-v0.1, with ANYSCALE_API_KEY together_ai/togethercomputer/llama-2-70b-chat, with TOGETHERAI_API_KEY sagemaker/, with AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION_NAME azure/, with AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION, and the optional AZURE_AD_TOKEN and AZURE_API_TYPE If your provider offers an OpenAI-compatible endpoint, just add an openai/ prefix to your full model name. 1 2 3 import dspy lm = dspy.LM('openai/your-model-name', api_key='PROVIDER_API_KEY', api_base='YOUR_PROVIDER_URL') dspy.configure(lm=lm) Calling the LM directly. 1) Modules help you describe AI behavior as code, not strings. To build reliable AI systems, you must iterate fast. But maintaining prompts makes that hard: it forces you to tinker with strings or data every time you change your LM, metrics, or pipeline. Having built over a dozen best-in-class compound LM systems since 2020, we learned this the hard way—and so built DSPy to decouple defining LM systems from messy incidental choices about specific LMs or prompting strategies. DSPy shifts your focus from tinkering with prompt strings to programming with structured and declarative natural-language modules. For every AI component in your system, you specify input/output behavior as a signature and select a module to assign a strategy for invoking your LM. DSPy expands your signatures into prompts and parses your typed outputs, so you can write ergonomic, portable, and optimizable AI systems. Getting Started II: Build DSPy modules for various tasks Try the examples below after configuring your lm above. Adjust the fields to explore what tasks your LM can do well out of the box. Each tab below sets up a DSPy module, like dspy.Predict, dspy.ChainOfThought, or dspy.ReAct, with a task-specific signature. For example, question -> answer: float tells the module to take a question and to produce a float answer. MathRetrieval-Augmented GenerationClassificationInformation ExtractionAgents 1 2 math = dspy.ChainOfThought(\"question -> answer: float\") math(question=\"Two dice are tossed. What is the probability that the sum equals two?\") Possible Output: Prediction( reasoning='When two dice are tossed, each die has 6 faces, resulting in a total of 6 x 6 = 36 possible outcomes. The sum of the numbers on the two dice equals two only when both dice show a 1. This is just one specific outcome: (1, 1). Therefore, there is only 1 favorable outcome. The probability of the sum being two is the number of favorable outcomes divided by the total number of possible outcomes, which is 1/36.', answer=0.0277776 ) 1 2 3 4 5 6 7 8 def search_wikipedia(query: str) -> list[str]: results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3) return [x['text'] for x in results] rag = dspy.ChainOfThought('context, question -> response') question = \"What's the name of the castle that David Gregory inherited?\" rag(context=search_wikipedia(question), question=question) Possible Output: Prediction( reasoning='The context provides information about David Gregory, a Scottish physician and inventor. It specifically mentions that he inherited Kinnairdy Castle in 1664. This detail directly answers the question about the name of the castle that David Gregory inherited.', response='Kinnairdy Castle' ) 1 2 3 4 5 6 7 8 9 10 11 from typing import Literal class Classify(dspy.Signature): \"\"\"Classify sentiment of a given sentence.\"\"\" sentence: str = dspy.InputField() sentiment: Literal['positive', 'negative', 'neutral'] = dspy.OutputField() confidence: float = dspy.OutputField() classify = dspy.Predict(Classify) classify(sentence=\"This book was super fun to read, though not the last chapter.\") Possible Output: Prediction( sentiment='positive', confidence=0.75 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class ExtractInfo(dspy.Signature): \"\"\"Extract structured information from text.\"\"\" text: str = dspy.InputField() title: str = dspy.OutputField() headings: list[str] = dspy.OutputField() entities: list[dict[str, str]] = dspy.OutputField(desc=\"a list of entities and their metadata\") module = dspy.Predict(ExtractInfo) text = \"Apple Inc. announced its latest iPhone 14 today.\" \\ \"The CEO, Tim Cook, highlighted its new features in a press release.\" response = module(text=text) print(response.title) print(response.headings) print(response.entities) Possible Output: Apple Inc. Announces iPhone 14 ['Introduction', \"CEO's Statement\", 'New Features'] [{'name': 'Apple Inc.', 'type': 'Organization'}, {'name': 'iPhone 14', 'type': 'Product'}, {'name': 'Tim Cook', 'type': 'Person'}] 1 2 3 4 5 6 7 8 9 10 11 def evaluate_math(expression: str): return dspy.PythonInterpreter({}).execute(expression) def search_wikipedia(query: str): results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3) return [x['text'] for x in results] react = dspy.ReAct(\"question -> answer: float\", tools=[evaluate_math, search_wikipedia]) pred = react(question=\"What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?\") print(pred.answer) Possible Output: 5761.328 Using DSPy in practice: from quick scripting to building sophisticated systems. 2) Optimizers tune the prompts and weights of your AI modules. DSPy provides you with the tools to compile high-level code with natural language annotations into the low-level computations, prompts, or weight updates that align your LM with your program’s structure and metrics. If you change your code or your metrics, you can simply re-compile accordingly. Given a few tens or hundreds of representative inputs of your task and a metric that can measure the quality of your system's outputs, you can use a DSPy optimizer. Different optimizers in DSPy work by synthesizing good few-shot examples for every module, like dspy.BootstrapRS,1 proposing and intelligently exploring better natural-language instructions for every prompt, like dspy.MIPROv2,2 and building datasets for your modules and using them to finetune the LM weights in your system, like dspy.BootstrapFinetune.3 Getting Started III: Optimizing the LM prompts or weights in DSPy programs A typical simple optimization run costs on the order of $2 USD and takes around 20 minutes, but be careful when running optimizers with very large LMs or very large datasets. Optimization can cost as little as a few cents or up to tens of dollars, depending on your LM, dataset, and configuration. Optimizing prompts for a ReAct agentOptimizing prompts for RAGOptimizing weights for Classification This is a minimal but fully runnable example of setting up a dspy.ReAct agent that answers questions via search from Wikipedia and then optimizing it using dspy.MIPROv2 in the cheap light mode on 500 question-answer pairs sampled from the HotPotQA dataset. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import dspy from dspy.datasets import HotPotQA dspy.configure(lm=dspy.LM('openai/gpt-4o-mini')) def search_wikipedia(query: str) -> list[str]: results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3) return [x['text'] for x in results] trainset = [x.with_inputs('question') for x in HotPotQA(train_seed=2024, train_size=500).train] react = dspy.ReAct(\"question -> answer\", tools=[search_wikipedia]) tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto=\"light\", num_threads=24) optimized_react = tp.compile(react, trainset=trainset) An informal run like this raises ReAct's score from 24% to 51%, by teaching gpt-4o-mini more about the specifics of the task. Given a retrieval index to search, your favorite dspy.LM, and a small trainset of questions and ground-truth responses, the following code snippet can optimize your RAG system with long outputs against the built-in SemanticF1 metric, which is implemented as a DSPy module. 1 2 3 4 5 6 7 8 9 10 11 class RAG(dspy.Module): def __init__(self, num_docs=5): self.num_docs = num_docs self.respond = dspy.ChainOfThought('context, question -> response') def forward(self, question): context = search(question, k=self.num_docs) # defined in tutorial linked below return self.respond(context=context, question=question) tp = dspy.MIPROv2(metric=dspy.evaluate.SemanticF1(decompositional=True), auto=\"medium\", num_threads=24) optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2) For a complete RAG example that you can run, start this tutorial. It improves the quality of a RAG system over a subset of StackExchange communities by 10% relative gain. This is a minimal but fully runnable example of setting up a dspy.ChainOfThought module that classifies short texts into one of 77 banking labels and then using dspy.BootstrapFinetune with 2000 text-label pairs from the Banking77 to finetune the weights of GPT-4o-mini for this task. We use the variant dspy.ChainOfThoughtWithHint, which takes an optional hint at bootstrapping time, to maximize the utility of the training data. Naturally, hints are not available at test time. Click to show dataset setup code. 1 2 3 4 5 6 7 8 9 10 11 12 import dspy dspy.configure(lm=dspy.LM('gpt-4o-mini-2024-07-18')) # Define the DSPy module for classification. It will use the hint at training time, if available. signature = dspy.Signature(\"text -> label\").with_updated_fields('label', type_=Literal[tuple(CLASSES)]) classify = dspy.ChainOfThoughtWithHint(signature) # Optimize via BootstrapFinetune. optimizer = dspy.BootstrapFinetune(metric=(lambda x, y, trace=None: x.label == y.label), num_threads=24) optimized = optimizer.compile(classify, trainset=trainset) optimized_classifier(text=\"What does a pending cash withdrawal mean?\") Possible Output (from the last line): Prediction( reasoning='A pending cash withdrawal indicates that a request to withdraw cash has been initiated but has not yet been completed or processed. This status means that the transaction is still in progress and the funds have not yet been deducted from the account or made available to the user.', label='pending_cash_withdrawal' ) An informal run similar to this on DSPy 2.5.29 raises GPT-4o-mini's score 66% to 87%. What's an example of a DSPy optimizer? How do different optimizers work? 3) DSPy's Ecosystem advances open-source AI research. Compared to monolithic LMs, DSPy's modular paradigm enables a large community to improve the compositional architectures, inference-time strategies, and optimizers for LM programs in an open, distributed way. This gives DSPy users more control, helps them iterate much faster, and allows their programs to get better over time by applying the latest optimizers or modules. The DSPy research effort started at Stanford NLP in Feb 2022, building on what we learned from developing early compound LM systems like ColBERT-QA, Baleen, and Hindsight. The first version was released as DSP in Dec 2022 and evolved by Oct 2023 into DSPy. Thanks to 250 contributors, DSPy has introduced tens of thousands of people to building and optimizing modular LM programs. Since then, DSPy's community has produced a large body of work on optimizers, like MIPROv2, BetterTogether, and LeReT, on program architectures, like STORM, IReRa, and DSPy Assertions, and on successful applications to new problems, like PAPILLON, PATH, WangLab@MEDIQA, UMD's Prompting Case Study, and Haize's Red-Teaming Program, in addition to many open-source projects, production applications, and other use cases.",
    "commentLink": "https://news.ycombinator.com/item?id=42343692",
    "commentBody": "DSPy – Programming–not prompting–LMs (dspy.ai)159 points by ulrischa 23 hours agohidepastfavorite33 comments thomasahle 7 hours agoHere is a simple example of dspy: classify = dspy.Predict(f\"text -> label:Literal{CLASSES}\") optimized = dspy.BootstrapFewShot(metric=(lambda x, y: x.label == y.label)) .compile(classify, trainset=load_dataset('Banking77')) label = optimized(text=\"What does a pending cash withdrawal mean?\").label What this does is optimize a prompt given a dataset (here Banking77). The optimizer, BootstrapFewShot, simply selects a bunch of random subsets from the training set, and measures which gives the best performance on the rest of the dataset when used as few-shot examples. There are also more fancy optimizers, including ones that first optimize the prompt, and then use the improved model as a teacher to optimize the weights. This has the advantage that you don't need to pay for a super long prompt on every inference call. dspy has more cool features, such as the ability to train a large composite LLM program \"end to end\", similar to backprop. The main advantage, imo, is just not having \"stale\" prompts everywhere in your code base. You might have written some neat few-shot examples for the middle layers of your pipeline, but then you change something at the start, and you have to manually rewrite all the examples for every other module. With dspy you just keep your training datasets around, and the rest is automated. (Note, the example above is taken from the new website: https://dspy.ai/#__tabbed_3_3 and simplified a bit) reply deepsquirrelnet 18 hours agoprevI use DSPy often, and it’s the only framework that I have much interest in using professionally. Evaluations are first class and have a natural place in optimization. I still usually spend some time adjusting initial prompts, but more time doing traditional ML things… like working with SMEs, building training sets, evaluating models and developing the pipeline. If you’re an ML engineer that’s frustrated by the “loose” nature of developing applications with LLMs, I recommend trying it out. With assertions and suggestions, there’s also additional pathways you can use to enforce constraints on the output and build in requirements from your customer. reply huevosabio 14 hours agoparentEvery time I check the docs, I feel like it obfuscates so many things that it puts me off and I decide to just not try it out. Behind the scenes it's using LLM's to find the proper prompting. I find that it uses a terminology and abstraction that is way too complicated for what it is. reply qeternity 16 hours agoparentprevWhat do you actually use it for? I've never been able to actually get it to perform on anything remotely close to what it claims. Sure, it can help optimize few shot prompting...but what else can it reliably do? reply deepsquirrelnet 2 hours agorootparentIt isn’t for every application, but I’ve used it for tasks like extraction, summarization and generating commands where you have specific constraints you’re trying to meet. Most important to me is that I can write evaluations based on feedback from the team and build them into the pipeline using suggestions and track them with LLM as a judge (and other) metrics. With some of the optimizers, you can use stronger models to help propose and test new instructions for your student model to follow, as well as optimize the N shot examples to use in the prompt (MIPROv2 optimizer). It’s not that a lot of that can’t be done other ways, but as a framework it provides a non-trivial amount of value to me when I’m trying to keep track of requirements that grow over time instead of playing the whack a mole game in the prompt. reply byefruit 22 hours agoprevI've seen a couple of talks on DSPy and tried to use it for one of my projects but the structure always feels somewhat strained. It seems to be suited for tasks that are primarily show, don't tell but what do you do when you have significant prior instruction you want to tell? e.g Tests I want applied to anything retrieved from the database. What I'd like is to optimise the prompt around those (or maybe even the tests themselves) but I can't seem to express that in DSPy signatures. reply thatsadude 4 hours agoparentYou can optimize prompt with MIPROv2 without examples (set the max number of examples to 0) reply aaronvg 18 hours agoprevI found it interesting how DSPy created the Signatures concept: https://dspy.ai/learn/programming/signatures/ We took this kind of concept all the way to making a DSL called BAML, where prompts look like literal functions, with input and output types. Playground link here https://www.promptfiddle.com/ https://github.com/BoundaryML/baml (tried pasting code but the formatting is completely off here, sorry). We think we could run some optimizers on this as well in the future! We'll definitely use DSPy as inspiration! reply dcreater 2 hours agoparentGood dx? BAML looks even worse than the current API call based paradigm. Even your toy examples look bad - wouldn't want to see what an actual program would look like. Hopefully this, dspy and the like that have poor design, inelegant won't become common standards reply aaronvg 2 hours agorootparentHow do you organize your prompts? Do you use a templating language like jinja? How complex are your prompts? Do you have any open source examples? I’m genuinely curious since if we can convince someone like you that BAML is amazing we’re on a good track. We’ve helped people remove really ugly concatenated strings or raw yaml files with json schemas just by using our prompt format (which uses jinja2!) reply jmugan 14 hours agoprevAs far as I can tell, Google VertexAI prompt optimizer does similar things. I find their documentation more comprehensible. https://cloud.google.com/vertex-ai/generative-ai/docs/learn/... reply thatsadude 4 hours agoprevMy go to framework. I wish we can use global metrics in DSPy, for examples, F1 score over the whole evaluation set (instead of a single query at the moment). The recent async support has been life saver. reply dcreater 2 hours agoprevDSPy seems unnecessarily convoluted, inelegant or am I just stupid? reply th0ma5 1 hour agoparentI think you read it right. It is in my mind a kind of wish casting that adding other modeling to LLMs can improve their use, but the ideas all sound like playing with your food at best, and deliberately confusing people to prey on their excitement at the worst. reply edmundsauto 51 minutes agorootparentI'm torn - I like the promise and people are getting value out of it. I need to try it myself on a toy project! What experiences/evidence do you have that informed your opinion? It sounds like you've had pretty negative experiences. reply scosman 20 hours agoprevCan someone explain what DSPy does that fine tuning doesn’t? Structured IO, optimized to better results. Sure. But why just just go straight to weights, instead of trying to optimize the few-shot space? reply flakiness 15 hours agoparentIt has multiple optimization strategies. One is optimizing the few shot list. Another is to let the model write prompts and pick the best one based on the given eval. I doubt latter much more intriguing although I have no idea how practical it is. reply choppaface 12 hours agoparentprevThe main idea behind DSPy is that you can’t modify the weights, but you can perhaps modify the prompts. DSPy’s original primary customer was multi-llm-agent systems where you have a chain / graph of LLM calls (perhaps mostly or all to OpenAI GPT) and you have some metric (perhaps vague) that you want to increase. While the idea may seem a bit weird, there have been various success stories, such as a UoT team winning medical-notes-oriented competition using DSPy https://arxiv.org/html/2404.14544v1 reply behnamoh 21 hours agoprevEvery time I've seen a dspy article, I end up thinking: ok, but what does it do exactly? I've been using guidance, outlines, GBF grammars, etc. What advantage does dspy have over those alternatives? I've learnt that the best package to use LLMs is just Python. These \"LLM packages\" just make it harder to do customizations as they all make opinionated assumptions and decisions. reply dr_kiszonka 20 hours agoparentQuestion from a casual AI user, if you have a minute. It seems to me that I could get much more productive by making my own personal AI \"system\". For example, write a simple pipeline where Claude would scrutinize OpenAI's answers and vice versa. Are there any beginner-friendly Python packages that you would recommend to facilitate fast experimentation with such ideas? reply monkmartinez 20 hours agorootparentNot the person you asked, but I will take a shot at your question. The best python package for getting what you want out of these systems is to use original gangster python with libraries to help with your goals. For your example; Write a python script with requests that hits the OpenAI API. You can even hardcode the API key because its just a script on your computer! Now you have the GPT-4proLight-mini-deluxe response in JSON. You can pipe that into a bazzillion and one different places including another API request to Anthropic. Once that returns, you can now have TWO llm responses to analyze. I tried haystack, langchain, txtai, langroid, CrewAI, Autogen, and more that I am forgetting. One day while I was reading r/Localllama someone wrote; \"All these packages are TRASH, just write python!\"... Lightbulb moment for me. Duh! Now I don't need to learn a massive framework to only use 1/363802983th of it while cursing that I can't figure out how to make it do what I want it to do. Just write python. I tell you that has been massive for my usage of these LLM's outside of the chat interfaces like LibreChat and OpenWebUI. You can even have claude or deepseek write the script for you. That often gets me within striking distance of what I really want to achieve at that moment. reply digdugdirk 20 hours agorootparentprevI've had good luck with a light \"shim layer\" library that handles the actual interfacing with the api and implements the plumbing on any fun new features that get introduced. I've settled on the Mirascope library (https://mirascope.com/), which suits my use cases and lets me implement structured inputs/outputs via pydantic models, which is nice. I really like using it, and the team behind it is really responsive and helpful. That being said, Pydantic just released an AI library of their own (https://ai.pydantic.dev/) that I haven't checked out, but I'd love to hear from someone who has! Given their track record, it's certainly worth keeping an eye on. reply d4rkp4ttern 5 hours agorootparentprevYou can have a look at Langroid -- it's an agent-oriented LLM programming framework from CMU/UW-Madison researchers. We started building it in Apr 2023 out of frustration with the bloat of then-existing libs. In langroid you set up a ChatAgent class which encapsulates an LLM-interface plus any state you'd like. There's a Task class that wraps an Agent and allows inter-agent communication and tool-handling. We have devs who've found our framework easy to understand and extend for their purposes, and some companies are using it in production (some have endorsed us publicly). A quick tour gives a flavor of Langroid: https://langroid.github.io/langroid/tutorials/langroid-tour/ Feel free to drop into our discord for help. reply gradys 20 hours agorootparentprevI assume you know how to program in Python? I would start with just the client libraries of the model providers you want to use. LLMs are conceptually simple when treated as black boxes. String in, string out. You don't necessarily need a framework. reply sdesol 13 hours agorootparentprev> For example, write a simple pipeline where Claude would scrutinize OpenAI's answers and vice versa. I'm working on a naive approach to identify errors in LLM responses which I talk about at https://news.ycombinator.com/item?id=42313401#42313990, which can be used to scrutinize responses. It's written in Javascript though, but you will be able to create a new chat by calling a http endpoint. I'm hoping to have the system in place in a couple of weeks. reply jackmpcollins 11 hours agorootparentprevI'm building magentic for use cases like this! https://github.com/jackmpcollins/magentic It's based on pydantic and aims to make writing LLM queries as easy/compact as possible by using type annotations, including for structured outputs and streaming. If you use it please reach out! reply kingkongjaffa 19 hours agorootparentprevYou could use the plain api libraries for each llm and ipython notebooks, conceptually each block could be a node or link in the prompt chain, and input/output of each block is printable and visible to check which part of the chain is the part that is failing or has sub optimal outputs. reply pizza 19 hours agorootparentprevYes, anthropic just released model context protocol and mcp is perfect for this kind of thing. I actually wrote an mcp server for claude to call out to openai just yesterday. reply coffeephoenix 16 hours agoprevI tried using it but one of the hard parts is defining a good metric that the underlying optimizer can use. Came up with an approach for that here: https://colab.research.google.com/drive/1obuS9cEWN9MT-MIv5aL... reply thom 21 hours agoprevThere are companies that will charge you 6-7 figures for what you can do in a few dozen lines with DSPy, but I guess that’s true of many things. reply 3abiton 20 hours agoparentThat is a big claim though, I am not sure about that from my experience. What am I missing? reply Imanari 8 hours agoprevCan someone explain how it works? reply beepbooptheory 18 hours agoprev [–] How does it work? Like I can see the goal and the results, but is it in fact the case that its still here \"LLMs all the way down\"? That is, is there a supplement bot here thats fine tuned to DSPy syntax, doing the actual work of turning the code to prompt? Trying to figure out how else it would work.. But if that is the case, this really feels like a Wizard of Oz behind the curtain thing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DSPy is a framework designed for building modular AI systems using compositional Python code, focusing on improving language model (LM) outputs without relying solely on prompts.- It supports multiple large language model (LLM) providers like OpenAI, Anthropic, and Databricks, allowing users to describe AI behavior as code, facilitating easier iteration and optimization.- Originating from Stanford NLP, DSPy has a strong community presence on GitHub and Discord, contributing to open-source AI research and advancing program architectures and optimizers."
    ],
    "commentSummary": [
      "DSPy is a framework designed to optimize prompts for language models (LMs) by automating the selection of training data subsets, thus enhancing performance and minimizing manual adjustments.",
      "It supports end-to-end training of large language model (LLM) programs, preventing the use of outdated prompts in code, and is particularly useful for tasks such as data extraction and summarization.",
      "While some users find DSPy's complexity challenging, it is valued for its structured approach and features like assertions and suggestions that enforce output constraints."
    ],
    "points": 159,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1733515189
  }
]
