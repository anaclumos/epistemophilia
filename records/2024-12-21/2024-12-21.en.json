[
  {
    "id": 42476828,
    "title": "US judge finds Israel's NSO Group liable for hacking journalists via WhatsApp",
    "originLink": "https://www.reuters.com/technology/cybersecurity/us-judge-finds-israels-nso-group-liable-hacking-whatsapp-lawsuit-2024-12-21/",
    "originBody": "reuters.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAkB2LCGNV9IoAKExqMQ==','hsh':'2013457ADA70C67D6A4123E0A76873','t':'bv','s':43909,'e':'601844c1652ea2a31477a5998ebf59e031b1b1aebf82af3e5571203859d7ad74','host':'geo.captcha-delivery.com','cookie':'6Y1JUPiS0TfxAuzc~x3X3WHdKCoX5ReC9hG9AV8V_DeiE9QWqMQX5cDlbZTiiX883cKSl~O3BzyBsCFhdijzUkJP_RUSIzKeth_SaTlSfwoAOaNkS6jHVmLw~kSnHtjx'}",
    "commentLink": "https://news.ycombinator.com/item?id=42476828",
    "commentBody": "US judge finds Israel's NSO Group liable for hacking journalists via WhatsApp (reuters.com)331 points by o999 17 hours agohidepastfavorite63 comments dmantis 9 hours agoThere should be no difference with usual botnet owner/ransomware gangs and such companies. Management should go to prison for good 20-30 years for that and being extradited worldwide. Considering that ransomware gangs are probably less harmful to the society than guys who hack journalists and politicians, putting their lifes at literal risks, not just their pockets. There should be no \"legal\" hacking of someone's devices apart from extraction of data from already convicted people in public court with the right to defend themselves reply bawolff 9 hours agoparentIts not like this is that different than traditional \"weapons\" (i hate the \"cyberweapons\" analogy, but if the shoe fits). Sell guns to governments, even unsavoury ones, it is very rare anything will happen to you except in pretty extreme cases. Sell guns to street gangs, well that is a different story. Like i don't think this situation is different because it is \"hacking\". reply onedognight 6 hours agorootparentThe NSO created/ran cloud instances for each client country and reviewed and approved every target. The didn’t sell weapons like in your analogy. They were effectively assassins for hire. The problem with selling exploits is you want to maintain “ownership” of the exploit details, lest your customer just take the exploit and sell/use it without paying more or use it to attack you or your friends. This means you end up with veto power. I.e. culpability. reply Neonlicht 6 hours agorootparentprevAll the cartels in Mexico buy their guns from America and nobody is going to jail over it. reply lupusreal 5 hours agorootparentPeople do in fact get sent to prison for that, straw purchases are a federal felony. Not all of them actually get caught, which is true of any crime. reply oaththrowaway 4 hours agorootparentExcept when the ATF does it, no big deal reply buo 27 minutes agorootparentMore information about this: https://en.wikipedia.org/wiki/ATF_gunwalking_scandal reply o999 5 hours agoparentprevImagine if they chase NSO as hard as they chased Wikileaks reply tehwebguy 7 hours agoparentprevCertainly the ones that hack journalists should go to prison. reply lifestyleguru 6 hours agorootparentWhy should journalist badge provide some kind of protection shield? [1] [1] https://en.wikipedia.org/wiki/Pablo_Gonz%C3%A1lez_Yag%C3%BCe reply talldayo 3 hours agorootparentIn Israel's opinion? It shouldn't: https://en.wikipedia.org/wiki/List_of_journalists_killed_in_... Israeli forces killed 38x more journalists than Hamas did on October 7th. reply fakedang 2 hours agorootparentHamas also has a nasty habit of calling certain veterans \"journalists\". Although I don't dispute Israel killing journalists (they did kill Shireen Abu Akleh https://en.m.wikipedia.org/wiki/Killing_of_Shireen_Abu_Akleh), I would question that number. Also, Al Jazeera has a habit of hiring freelance journalists and putting them in exacting situations without a care for their consequence. When they do die, Al Jazeera makes a hue and cry out of it because it serves their agenda and resonates very well with their audience (especially their Arabic reporting audience). reply ilbeeper 6 hours agoparentprevI agree with the first part, at least in spirit. The second part though doesn't make sense. If the US president can send drones to kill terrorists without taking them to court, surely he can order hacking their phones. If you think that there's no case where the latter is ok you shouldn't you fight against the former first? reply ignoramous 5 hours agorootparent> send drones to kill terrorists The part that you miss is, are they only killing \"terrorists\" extrajudicially? To take that propaganda at its face value is to ask, what else could they be killing brown people for, if not terrorism? reply ilbeeper 3 hours agorootparentI didn't say if I think that drone killing is justified or not, since I have no opinion on that - I don't know enough to form an opinion. I only say that since the government have the right to send killing drone it doesn't make sense to raise pitchforks against phone hacking reply ignoramous 3 hours agorootparent> I have no opinion ... I don't know enough to form an opinion. Why speak in hypotheticals supporting some phantom opinion? Concern trolling is even worse. reply ilbeeper 2 hours agorootparentIt is not hypothetical, the fact is that killing drones are used in practice, and it just doesn't make sense to oppose lesser measures that are being used without judgement when killing is allowed. reply ignoramous 1 hour agorootparent> killing is allowed You said it is okay / allowed because \"terrorists\". Otherwise, it is a heinous crime. Just like the Pegasus one. reply ilbeeper 1 hour agorootparentI have no idea what you are talking about. Ok is a value judgment which I didn't state. Allowed is a fact. Are you arguing with what I'm saying or with an opponent in your mind? reply ignoramous 1 hour agorootparent> I have no idea ... This is what you wrote: \"The second part though doesn't make sense.\" The second part being: If the US president can send drones to kill terrorists without taking them to court, surely he can order hacking their phones. If you think that there's no case where the latter is ok you shouldn't you fight against the former first?\" Pretty clear from your rhetoric what your position is. Folks here are not dumb. > Ok is a value judgment ... Allowed is a fact Factually, genocidaries are worse than terrorists. reply ilbeeper 47 minutes agorootparentGot it. You are arguing with yourself. Good luck reply nico 12 hours agoprev> \"Surveillance companies should be on notice that illegal spying will not be tolerated.\" That is kinda funny, although sad at the same time On the flip side, I guess that means META allows WhatsApp users being only “legally spied” on reply trogdor 3 hours agoparentEvery social media company allows legal spying. Warrants and wiretap orders are issued every day in the United States. reply dylan604 4 hours agoparentprevIsn't that obvious though? Meta wants exclusive spying rights to its users. You spying on users with Meta's products is not allowed. If you want to spy on your users, build an app that's so popular billions of people sign up willingly to allow you to spy on them. Have you no decency? reply talldayo 3 hours agorootparent> Meta wants exclusive spying rights You're allowed to say \"The NSA\", we're all adults here. No need to speak in euphemisms. reply throwaway290 6 hours agoparentprev\"Unauthorized hostility against pioneer detected\" reply alecco 7 hours agoprevAaaaand it's flagged out of the front page. @dang, so early in the day this is obviously some coordinated manipulation. 31. 206 points 9 hours ago US judge finds Israel's NSO Group liable for hacking journalists via WhatsApp (reuters.com) 22. 37 points 8 hours ago My Pal, the Ancient Philosopher (nautil.us) 15. 4 points 4 hours ago Testing for Thermal Issues Becomes More Difficult (semiengineering.com) 18. 11 points 2 hours ago The Christmas story of one tube station's 'Mind the Gap' voice (2019) (theguardian.com) reply stonesthrowaway 2 hours agoparentI'm shocked! But don't worry, I'm sure the nytimes, wsj, ap, etc will run hit pieces on this outrageous behavior by israel. reply layer8 3 hours agoparentprev“@dang” doesn’t do anything. Email hn@ycombinator.com. reply sabbaticaldev 6 hours agoparentprevProbably done by the same NSO Group. But for US americans they are the good criminals, the chosen criminals reply wslh 5 hours agoprevThere are many other companies beyond NSO Group, if I were a journalist I would write a more comprehensive list of them and educate about this whole \"industry\". reply talldayo 3 hours agoparentNSO Group is unique in that they are entirely sheltered from (largely due) criticism by their government, creating an unaccountable and injust basis of relations between the United States and Israel that many readers are concerned by. There simply aren't any other comparably corrupt \"cybersecurity\" outfits in the world. Kinda similar to how the IDF has never been charged with war crimes despite several of their service-members being recorded breaking the law in their Israeli fatigues. It's not that international law was never broken, it's that Israel considers themselves above the rule of law and international bases of morality. That type of behavior absolutely must be called out in it's lonesome, such that no nation ever repeats Israel's embarrassing mistake. reply wslh 2 hours agorootparentMedia and international scrutiny often focus disproportionately on Israel, compared to countless global issues that remain unreported. Israel’s news density, given its small size, is incredibly high. This may partly stem from Israel’s democratic framework, which provides transparency and fosters political diversity, enabling more detailed examination of its internal affairs. For example, the new documentary The Bibi Files [1] showcases a level of scrutiny not as commonly observed in less transparent regimes. [1] https://jolt.film/watch/the-bibi-files reply Bilal_io 1 hour agorootparentThe number of crimes they've committed is also disproportional to their size. reply wslh 1 hour agorootparentYou might not have enough data points to draw a definitive conclusion. As I mentioned, unless you are directly witnessing events on a global scale, your observations are largely shaped by the information you consume. reply kotaKat 2 hours agoparentprevLike Verint, who tried to buy the NSO group, and has security DVRs in Walmarts all over the world... reply nothercastle 3 hours agoprevIt should be accessory to murder but just a fine reply ilrwbwrkhv 9 hours agoprevI thought Whatsapp and signal share the same encryption reply mjg59 9 hours agoparentThe encryption isn't alleged to have been compromised. The app itself deals with a lot of untrusted input (eg, thumbnailing video files you've been sent) so there's a meaningful attack surface outside the protocol itself. reply bawolff 9 hours agoparentprevThe attack wasn't targeting the encryption part of whatsapp (afaik). Encryption is important but it often is not the weakest link in the security chain. reply NolF 9 hours agoparentprevThe group exploited a bug in WhatsApp to deliver the spyware. It wasn't an E2E issue. > A U.S. judge ruled on Friday in favor of Meta Platforms' (META.O), opens new tab WhatsApp in a lawsuit accusing Israel's NSO Group of exploiting a bug in the messaging app to install spy software allowing unauthorized surveillance. reply kjkjadksj 1 hour agoparentprevPeople have to start assuming that any communication method in use is compromised. There’s just no way on earth orgs like the NSA would throw their hands up in the air and not find multiple different avenues into an app like signal. Its one of the most downloaded messaging apps. Investment into compromising it is very worth while. People should just assume everything involving a cell phone or computer is inherently insecure. Meanwhile for some analog methods (one time pads, even cupping a hand and whispering into anothers ear, etc), the power balance isn’t so lopsided between the state and the individual as it is with digital communications where everything is probably compromised in some way by now. reply dudeinjapan 4 hours agoprevYou have to be really bad if Meta are somehow the good guys in the article. reply Bilal_io 1 hour agoparentThe victims are the good guys. Meta is just not happy that their platform was exploited. Even if you consider them to be the bad guys, they needed to sue to curtail the bad PR reply zhengiszen 3 hours agoprevThe same people are behind the current genocide against Palestinians in Gaza reply solumunus 2 hours agoparentGet out of your algorithm you’re in too deep. reply stonesthrowaway 2 hours agorootparentAmazing how the same people who whine nonstop about the holocaust are the quickest to dismiss actual ongoing genocide. reply solumunus 2 hours agorootparentWhich same people exactly? reply rexpop 2 hours agorootparentprevNo sensible adult could refer to any reference to the holocaust as \"whining\". reply Bilal_io 1 hour agorootparentAgreed. And no sensible adult should refer to the genecide in Gaza as being deep in the algorithm. reply fastball 51 minutes agorootparentGenocide is an attempt to eradicate a group. The only group Israel is trying to eradicate is Hamas. They're not genociding Palestinians, they're genociding Hamas. Are they killing an excessive number of civilians as collateral damage? Certainly seems like it. But collateral damage is not genocide. If they wanted to genocide the Palestinians, they'd be shipping 'em to camps and gassing them, like the Nazis did. Looking at it another way: let's say that (hypothetically) Hamas stopped using people as humans shields by firing rockets from hospitals and building tunnels under schools. Do you think the number of non-combatants killed by the IDF would go down? Because I do, and to me that says Israel's goal is not in fact killing non-combatant civilians, even if they're killing far too many as is. reply immibis 11 hours agoprevDidn't the US fund those guys to do exactly that? reply lrvick 8 hours agoparentIt is only legal and ethical when we do it. reply Retr0id 9 hours agoparentprevThe US often does unlawful things. reply dylan604 4 hours agorootparentEspecially using willing 3rd parties to allow for plausible deniability. reply jredwards 13 hours agoprevWell, good. But also: build better software. reply mrkeen 9 hours agoparentAhem we don't do that here. We get to market faster before our runway ends so we don't risk our exit. reply ChrisMarshallNY 8 hours agoparentprevI support this. It’s not possible to be “perfect,” but if we do our best to get there, we’ll make really good stuff. It’s unlikely to happen, though, as we have a system that explicitly rewards writing crap, because it makes money. As long as we fail to reward good work, we will continue to get poor work. reply dylan604 4 hours agorootparent> As long as we fail to reward good work, we will continue to get poor work. I think that's a bit off. The problem is that we continue to reward poor work so the poor work continues. reply akira2501 12 hours agoprevWhich is ironic considering the FBI and CISA just today announced that you _should_ use WhatsApp and not use SMS for two factor authentication. Although they point out the biggest problem is mobile users click on links in SMS. We live in a mostly captured and anti consumer environment. I'm not sure there's any great advice. https://www.newsnationnow.com/business/tech/fbi-warns-agains... reply magic_hamster 11 hours agoparentOf course there is. Always prefer an authenticator app over SMS. Also, Passkeys are supposed to be a big upgrade in this regard. reply bawolff 9 hours agoparentprevWhatsapp is not still vulnerable to the hack (as far as we know) and SMS applications have had similar vulnerabilities in the past. reply myth_drannon 5 hours agoprev [–] From reading other in depth sources it looks more like anti competitive business practices. Certain former politician who is well connected in democratic party cycles basically shutdown the whole Israeli offensive cyber industry except his company which is the main competitor of NSO. This whole drama wouldn't happened otherwise. With Republicans moving in, we might never hear about those issues again. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A US judge has held Israel's NSO Group accountable for hacking journalists through WhatsApp, raising questions about the responsibility of companies in hacking and surveillance activities.",
      "The case underscores ethical concerns related to the sale and use of cyber tools, likening them to traditional weapons sales, and prompting discussions on potential legal consequences for management.",
      "It also brings attention to broader issues such as privacy, government surveillance, and the responsibility of social media companies in safeguarding user data."
    ],
    "points": 331,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1734745103
  },
  {
    "id": 42476192,
    "title": "Compiling C to Safe Rust, Formalized",
    "originLink": "https://arxiv.org/abs/2412.15042",
    "originBody": "Computer Science > Programming Languages arXiv:2412.15042 (cs) [Submitted on 19 Dec 2024] Title:Compiling C to Safe Rust, Formalized Authors:Aymeric Fromherz, Jonathan Protzenko View PDF Abstract:The popularity of the Rust language continues to explode; yet, many critical codebases remain authored in C, and cannot be realistically rewritten by hand. Automatically translating C to Rust is thus an appealing course of action. Several works have gone down this path, handling an ever-increasing subset of C through a variety of Rust features, such as unsafe. While the prospect of automation is appealing, producing code that relies on unsafe negates the memory safety guarantees offered by Rust, and therefore the main advantages of porting existing codebases to memory-safe languages. We instead explore a different path, and explore what it would take to translate C to safe Rust; that is, to produce code that is trivially memory safe, because it abides by Rust's type system without caveats. Our work sports several original contributions: a type-directed translation from (a subset of) C to safe Rust; a novel static analysis based on \"split trees\" that allows expressing C's pointer arithmetic using Rust's slices and splitting operations; an analysis that infers exactly which borrows need to be mutable; and a compilation strategy for C's struct types that is compatible with Rust's distinction between non-owned and owned allocations. We apply our methodology to existing formally verified C codebases: the HACL* cryptographic library, and binary parsers and serializers from EverParse, and show that the subset of C we support is sufficient to translate both applications to safe Rust. Our evaluation shows that for the few places that do violate Rust's aliasing discipline, automated, surgical rewrites suffice; and that the few strategic copies we insert have a negligible performance impact. Of particular note, the application of our approach to HACL* results in a 80,000 line verified cryptographic library, written in pure Rust, that implements all modern algorithms - the first of its kind. Subjects: Programming Languages (cs.PL) Cite as: arXiv:2412.15042 [cs.PL](or arXiv:2412.15042v1 [cs.PL] for this version)https://doi.org/10.48550/arXiv.2412.15042 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Aymeric Fromherz [view email] [v1] Thu, 19 Dec 2024 16:51:29 UTC (92 KB) Full-text links: Access Paper: View PDF TeX Source Other Formats view license Current browse context: cs.PLnewrecent2024-12 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=42476192",
    "commentBody": "Compiling C to Safe Rust, Formalized (arxiv.org)257 points by love2read 19 hours agohidepastfavorite114 comments pizza234 1 hour agoI've ported some projects to Rust (including C, where I've used C2Rust as first step), and I've drawn some conclusions. 1. Converting a C program to Rust, even if it includes unsafe code, often uncovers bugs quickly thanks to Rust’s stringent constraints (bounds checking, strict signatures, etc.). 2. automated C to Rust conversion is IMO something that will never be solved entirely, because the design of C program is fundamentally different from Rust; such conversions require a significant redesign to be made safe (of course, not all C programs are the same). 3. in some cases, it’s plain impossible to port a program from C to Rust while preserving the exact semantics, because unsafety can be inherent in the design. That said, tooling is essential to porting, and as tools continue to evolve, the process will become more streamlined. reply LPisGood 25 minutes agoparent>because unsafety can be inherent in the design I agree in principle, and I have strong feelings based on my experience that this is the case, but I think it would be illustrative to have some hard examples in mind. Does anyone know any simple cases to ground this discussion in? reply wffurr 18 hours agoprevNote that this is done for “existing formally verified C codebases” which is a lot different from typical systems C code which is not formally verified. reply safercplusplus 11 hours agoparentAnd even then, not completely reliably it seems (from Section 2.2): > The coercions introduced by conversion rules can however lead to subtle semantic differences The example they give is this C code: 1 uint8_t x[1] = { 0 }; 2 uint8_t *y = x; 3 *y = 1; 4 assert(*x == 1); /* SUCCESS */ getting translated to this (safe) Rust code: 1 let x: [u8; 1] = [0; 1]; 2 let mut y: Box = Box::new(x); 3 y[0] = 1; 4 assert!(x[0] == 1) /* failure */ So the pointer (iterator) targeting an existing (stack-allocated) array declared on line 2 gets translated to an owning pointer/Box) targeting a (heap-allocated) new copy of the array. So if the original code was somehow counting on the fact that the pointer iterator was actually targeting the array it was assigned to, the translated code may (quietly) not behave correctly. For comparison, the scpptool (my project) auto-translation (to a memory safe subset of C++) feature would translate it to something like: 1 mse::lh::TNativeArrayReplacement x = { 0 }; 2 mse::lh::TNativeArrayReplacement::iterator y = x; // implicit conversion from array to iterator 3 *y = 1; 4 assert(*x == 1); /* SUCCESS */ // dereferencing of array supported for compatibility or if y is subsequently retargeted at another type of array, then line 2 may end up as something like: 2 mse::TAnyRandomAccessIterator y = x; // implicit conversion from array to iterator So the OP project may only be converting C code that is already amenable to being converted to safe Rust. But given the challenge of the problem, I can respect the accomplishment and see some potential utility in it. edit: added translation for line 2 in an alternate hypothetical situation. reply lambda 19 minutes agorootparentthe translated code may (quietly) not behave correctly. The whole point of them show that example is that they say they catch this case, and bring it to the attention of the programmer: If the original C program further relies on x, our translation will error out, and will ask the programmer to fix their source code. This is another area where we adopt a “semi-active” approach to verification, and declare that some patterns are poor enough, even for C, that they ought to be touched up before the translation takes place. reply akkad33 8 hours agoparentprevIs Rust formally verified? Not that I know of reply gpm 4 hours agorootparentNo, but small pieces of it are, and there's active work to extend it The concept of the borrow checker has been on a simplified version of rust https://people.mpi-sws.org/~dreyer/papers/rustbelt/paper.pdf - work has continued in this area steadily (e.g. see tree borrows) There's a variety of tools that take rust code and translate it to something a proof system understands, and then checks that it matches a specification. AWS is leading a project to use these to verify the standard library: https://aws.amazon.com/blogs/opensource/verify-the-safety-of... reply PartiallyTyped 8 hours agorootparentprevYou can always run model checkers like Kani, though even that is limited. reply medo-bear 6 hours agorootparentSo no? reply johnisgood 4 hours agorootparentThe answer is that it is not. It frustrates me more than it should, I admit, that people always mention Rust when they talk about safety, but never Ada / SPARK. You want formal verification? Use Ada / SPARK. It has been battle-tested. It has been used for critical systems for a really long time now. (And a compiler being formally verified vs. being able to write formally verified code means two different things.) reply medo-bear 4 hours agorootparentI think a disclaimer like this should be written with every Rust application, like health warnings on cigarette packets reply johnisgood 4 hours agorootparentAt this point, I think that would be better, yes, just because people think Rust is \"fully\" safe, which is just incorrect. I think the problem was the Rust hype and repeated statements of it being very safe, so we have some undoing to do. For example if someone on GitHub sees that the project is written in Rust, they are automatically going to assume it is safe, incorrectly so. I do not blame them though. reply keybored 3 hours agorootparentYou presumably extend this to every virtual machine or interpreter for every language which is implemented in an unsafe language. When that language claims to be safe (like all such languages claim to be). That seems excessive and tedious. reply medo-bear 3 hours agorootparentThe point, I think, was that \"safety\" presumptions about Rust are often exaggerated or poorly misunderstood due to hype. That could certainly lead to problems reply woodruffw 2 hours agorootparentI don’t think Rust’s actual safety properties aren’t overhyped, although they may be subject to misunderstanding about their exact extent. Concretely: spatial and temporal memory safety are good things, and Rust achieves both. It’s not unique in this regard, nor is it unique in not having a formal definition. reply keybored 1 hour agorootparentprev> The point, I think, was that \"safety\" presumptions about Rust are often exaggerated or poorly misunderstood due to hype. That could certainly lead to problems Then the point is hypocritical. Runtimes for safe programming languages have been implemented in unsafe languages since the dawn of safe programming languages, basically. EDIT: I see now that you are the cigarette warning guy. In that case I don’t understand what this coy “I think” speculation is about when you made such a bizarre proclamation on this topic. reply chillingeffect 3 hours agorootparentprevRust is to safe as Tesla is to autopilot. reply medo-bear 3 hours agorootparentAs Tesla is to Tesla reply antiquark 1 hour agorootparentprevRust doesn't have a specification or standard yet, which would make it difficult to formally verify. https://stackoverflow.com/questions/75743030/is-there-a-spec... reply gpm 1 hour agorootparentIt does have a specification: https://github.com/ferrocene/specification It also strikes me as extraordinarily unlikely that any formal verification effort will use the existing specification, and not build their own (using their own formal language) as they go. reply jokoon 9 hours agoparentprevWhat is formally verified C? Why isn't there more of it? reply PhilipRoman 9 hours agorootparentBecause it takes a lot of effort. Google Frama-C. On the flip side, it can express not just memory safety constraints, but also correctness proofs. In this case it's not about Frama-C or similar tools though, see your sibling comments about the caveats. reply zozbot234 8 hours agorootparentArguably, generating verified C from a low-level focused subset of F* (Low*, used in the HACL project) is close enough to count as a \"similar tool\". reply cushychicken 4 hours agoparentprevWoof. Major unspoken caveat in the title! reply tonetegeatinst 15 hours agoparentprevWhat is the main difference? Can compiler flags force compliance? reply LPisGood 14 hours agorootparentMy understanding is that formal verification is a tough goal to achieve and that it usually requires designing the program or the language to be a specific way. The problem with transpiling C to rust is that unsafe and unverified behavior can be a key property of the behavior resulting program, so there isn’t an obvious way to spit out a sort of rustified (CRustified?) binary that matches the behavior of the C program. reply thunkingdeep 13 hours agorootparentTypical term is “Oxidized”. I think they feel clever when they do the RiiR thing and say that. reply immibis 11 hours agorootparentprev\"Formally verified\" means someone has written a correct mathematical proof that the code has no bugs (the proof is checked by a computer program to make sure it is correct). This is a very high bar. I'm not sure what it has to do with translating the code to Rust. reply swiftcoder 11 hours agorootparent> I'm not sure what it has to do with translating the code to Rust. Formally verified C programs typically have had all their undefined behaviour already stamped out during the verification process. That makes mapping it over to Rust a lot simpler. reply tjoff 10 hours agorootparentTranslating undefined behavior is the easy part. reply swiftcoder 6 hours agorootparentMaybe, but only if you know the specifics of the environment in which it is executing (i.e. which compiler/architecture-specific behaviours the code actually relies on) reply Filligree 6 hours agorootparentCorrectly written C programs don’t have undefined behaviour, so a translator can operate assuming there is none. reply DonaldPShimoda 11 hours agorootparentprevIn this case specifically, two separate aspects are being referred to with regard to \"formal verification\". The first is that the translation function (from C to Rust) has itself been formally verified. In other words: if you give it a C program that obeys certain necessary pre-conditions, it will give you a safe Rust program. I believe the title of the paper is primarily using \"Formalized\" with regard to this characteristic. The second is that the set of programs the authors evaluate their tool on are C programs that have themselves been formally verified. I only barely skimmed the introduction and didn't see it addressed directly there, but I would assume that this is because the pre-conditions necessary for their translation to work are most easily (only?) met by formally verified C programs, where of course the verification has been rendered with respect to properties that would be relevant here (e.g., memory use). reply nickpsecurity 15 hours agorootparentprevFormal verification often requires simplified code in a restrictive style. You might not even be able to use C features or structures that have the performance you want. How theorem provers and brains work are also different enough that making something easy for one often makes it harder for the other. You can also see this effect in the article on the history of Coverity’s analyzer. Real-world code was horrible to deal with vs the academic examples they started with. https://cacm.acm.org/research/a-few-billion-lines-of-code-la... reply light_hue_1 9 hours agoparentprevOh, this has so so many more caveats! It's borderline false advertising. First of all they never translated any C! At all. Zero lines. They took code written in F* and modified its C compiler to emit Rust. They never had to deal with any actual C code of any complexity, aside from the most trivial code that might theoretically be emitted by a toy compiler (but wasn't even that!). They just pretended they were dealing with C (or a heavily restricted Mini-C). Even if we accept this, there are plenty of arbitrary restrictions on the C it can even support in principle: > If the original C program further relies on x, our translation will error out, and will ask the programmer to fix their source code. That's saying you want C to already be written in a style that makes the Rust borrow checker happy! They're avoiding the actual hard parts of the problem. > These rules present simplified versions of what we support. Our implementation features several peephole optimizations ... If I may translate from the language of academia. \"We present beautiful rules in figure 4. But in reality, our implementation relies on a large number of hacks.\" But it gets worse! > For overlap cases that can be distinguished statically (as above), we emit a compile-time error; otherwise, the Rust code will panic at runtime. The resulting code is not guaranteed to be correct! For example, aliasing in C can cause crashes in the resulting Rust code. See comment at the top of page 10. We're going from a formally verified C program to a \"It may crash now\" Rust program!? That's nuts! > We apply our methodology to existing formally verified C codebases, namely, the HACL* cryptographic library ... This is such a blatant lie that I had to return to it. It's designed to catch an unwary reviewer. We often talk about HACL being in verified C because it gets compiled to that. But it's not a C library, it's written in a totally different language. You cannot confuse the two. I'm not a reviewer for their paper. But if I was, I would strongly fight for rejection. The fact that they only handle formally verified C is so astronomically far away from being their main problem. An honest title would be \"Compiling a subset of F* to partially Safe Rust, Partially Formalized\" reply lou1306 8 hours agorootparent> An honest title would be \"Compiling a subset of F* to partially Safe Rust, Partially Formalized\" Sadly that is the name of the game in conference publishing. Make a big claim in the title and hope the reviewer does not read the fine print. > If I may translate from the language of academia. \"We present beautiful rules in figure 4. But in reality, our implementation relies on a large number of hacks.\" Er, this I can understand. Every conference paper only presents you with a simplified view of the contributions. There is no way to describe every corner case within the page limits of a conference paper (specifically, I bet this was submitted to POPL 2025). reply light_hue_1 8 hours agorootparent> Er, this I can understand. Every conference paper only presents you with a simplified view of the contributions. In my group I don't allow hacks. Most other good papers I can think of don't do that. It takes new students a while to get used to not maximizing performance at all costs. And if a hack must exist, then it's explained, not as a hack, but as part of the method. Don't you hate it when you implement some beautiful idea from a paper only to discover that it hardly bares any relationship to the actual method that works? > There is no way to describe every corner case within the page limits of a conference paper (specifically, I bet this was submitted to POPL 2025). You're probably right. But POPL allows for unlimited pages in an attached appendix. They can and should describe the full method. Particularly if they're going to play the game of saying that they formalized this. > Sadly that is the name of the game in conference publishing. Make a big claim in the title and hope the reviewer does not read the fine print. It's a form of namespace squatting. Do the worst job possible to claim a title that you couldn't execute on, so that when people figure out that problem, they will be forced to cite your non-working solution. I loathe this approach to publishing. We should punish people for doing. Reject for lack of honesty and your paper can't be published in related conference for X years. reply lou1306 1 hour agorootparentWell in this case (and if it was POPL, but it's a pretty safe bet considering the format and the timing) it looks like reviewers have indeed rejected it. And I completely agree, it is namespace squatting. Sadly every once in a while it does work (and very effectively), so there is little incentive for the community to punish it. Sorry if my previous comment came off as dismissive, it's just that I'm getting increasingly disillusioned with the state of things in this space. reply medo-bear 6 hours agoparentprevWhat is the benefit of compiling formally correct code to Rust? It seems that all the possible benefits are already there (if not more) reply gpm 3 hours agorootparentI suppose hypothetically putting it in an easier language to make changes in. Though it's hard to imagine it being easier to make changes to transpiled code than the original. Alternatively this might be seen as a stepping stone to translating non-formally-verified C to rust, which I understand the US government has expressed a fair bit of interest in. reply medo-bear 3 hours agorootparentGood luck with both of that. Otherwise people whose dayjob is rewritting everything in Rust will soon be out of a job reply amenghra 9 hours agoprevIn 2002, a group of researchers presented a paper on Cyclone, a safe dialect of C [1]. While (manually) porting code from C to Cyclone, they found safety bugs in the C code. These kinds of manual or automated conversation from C totherefore have potential not only for increasing adoption of safer languages but also for uncovering existing bugs. [1] https://www.researchgate.net/profile/James-Cheney-2/publicat... reply alkonaut 9 hours agoprevIf you used a naïve translation to Rust, wouldn’t you get parts that are safe and parts that are unsafe? So your manual job would need to be only verifying safety in the unsafe regions (same as when writing rust to begin with)? Seems it would be a win even if the unsafe portion is quite large. Obviously not of it’s 90% of the end result. reply Animats 30 minutes agoparentIndeed, yes. Someone tried converting C OpenJPEG to low-level unsafe Rust using c2rust. OpenJPEG was known to segfault on a test case. I tried that test case on the Rust version. Segfaulted in the equivalent place in the Rust code. At least it's compatible. But that approach is a dead end. To make any progress, translation must recognize the common idioms of the language and upgrade those to the ideomatic forms of the target language. Compiling into Rust generates awful Rust, full of calls to functions that do unsafe C-type pointer manipulation. The big upgrading problems mostly involve pointers. The most promising result in this paper is that they figured out how to convert C pointer arithmetic into Rust slices. Slices can do most of the things C pointer arithmetic can do, and now someone automated the translation. Pointer arithmetic that can't be translated has to be looked at with deep suspicion. A useful way to think about this is that raw pointers in C which point to arrays implicitly have a length associated with them. That length is not visible in C source code, but exists somewhere, as a function of the program state. It might be a constant. It might be the size requested back at a \"malloc\" call. It might be a parameter to a function. It's usually not too hard for maintenance programmers to find array lengths. That could be an LLM kind of problem. Ask an LLM, \"Examine this code. What is the length of array foo?\" Then use that to guide translation to Rust by a non-LLM translator. If the LLM is wrong, the resulting Rust will get subscript errors or have an oversize array, but will not be unsafe. Array size info idioms are stylized enough in C that it should be possible to get it right most of the time. Especially since LLMs can read comments. reply CodesInChaos 6 hours agoparentprevA naïve translation would produce rust code which is almost entirely unsafe (using raw pointers instead of references everywhere). Translating to references is difficult, since C code isn't written with the restrictions of the Rust alias model / borrow-checker in mind. reply pizlonator 17 hours agoprevCompiling a tiny subset of C, that is. It might be so tiny as to be useless in practice. I have low hopes for this kind of approach; it’s sure to hit the limits of what’s possible with static analysis of C code. Also, choosing Rust as the target makes the problem unnecessarily hard because Rust’s ownership model is so foreign to how real C programs work. reply pornel 16 hours agoparentRust's ownership model is close enough for translating C. It's just more explicit and strongly typed, so the translation needs to figure out what a more free-form C code is trying to do, and map that to Rust's idioms. For example, C's buffers obviously have lengths, but in C the length isn't explicitly tied to a pointer, so the translator has to deduce how the C program tracks the length to convert that into a slice. It's non-trivial even if the length is an explicit variable, and even trickier if it's calculated or changes representations (e.g. sometimes used in the form of one-past-the-end pointer). Other C patterns like `bool should_free_this_pointer` can be translated to Rust's enum of `Owned`/`Borrowed`, but again it requires deducing which allocation is tied to which boolean, and what's the true safe scope of the borrowed variant. reply smolder 12 hours agorootparentIt's not that simple. In fact it's impossible in some cases if you don't sprinkle unsafe everywhere and defeat the purpose. Rusts restrictions are so that it can be statically analyzed to guarantee safety. The superset of all allowable C program behaviors includes lots of things that are impossible to guarantee the safety of through static analysis. Formally verified C involves sticking to a strict subset of the capabilities of C that is verifiable, much like Rust enforces, so it makes sense that programs meeting that standard could be translated. reply pizlonator 16 hours agorootparentprevRust’s ownership model forbids things like doubly linked lists, which C programs use a lot. That’s just one example of how C code is nowhere near meeting Rust’s requirements. There are lots of others. reply orf 15 hours agorootparent> Rust’s ownership model forbids things like doubly linked lists, which C programs use a lot. It’s literally in the standard library https://doc.rust-lang.org/std/collections/struct.LinkedList.... reply quuxplusone 15 hours agorootparentBut it's not in C's standard library. So the exercise isn't merely to auto-translate one language's standard library to another language's standard library (say, replacing C++ std::list with Rust LinkedList) — which would already be very hard. The exercise here is to auto-identify-and-refactor idioms open-coded in one language, into idioms suited for the other language's already-written standard library. Imagine refactoring your average C program to use GLib for all (all!) of its data structures. Now imagine doing that, but also translating it into Rust at the same time. reply Animats 14 hours agorootparent> The exercise here is to auto-identify-and-refactor idioms open-coded in one language, into idioms suited for the other language's already-written standard library. That's what LLMs are for - idiom translation. You can't trust them to do it right, though. [Pan et al . 2024] find that while GPT-4 generates code that is more idiomatic than C2Rust, only 61% of it is correct (i.e., compiles and produces the expected result), compared to 95% for C2Rust. This problem needs both AI-type methods to help with the idioms and formal methods to insure that the guessed idioms correctly capture the semantics. A big advance in this project is that they can usually translate C pointer arithmetic into Rust slices. That's progress on of one of the hardest parts of the problem. C2Rust did not do that. That system just generates unsafe raw pointer arithmetic, yielding ugly Rust code that replicates C pointer semantics using function calls. DARPA is funding research in this area under the TRACTOR program. Program awards in April 2025, so this is just getting started. It's encouraging to see so much progress already. This looks do-able. reply fuhsnn 12 hours agorootparent>That's what LLMs are for - idiom translation. You can't trust them to do it right, though. Optimizing C compilers also happened to be good at idiom recognition, and we can probably trust them a little more. The OP paper does mention future plan to use clang as well: >We have plans for a libclang-based frontend that consume actual C syntax. If such transformation can be done at IR level it might be more efficient to be to C-IR > idiom transform to Rust-IR > run safe-checks in Rust-IR > continue compilation in C-IR or Rust-IR or combining both for better optimization properties. reply swiftcoder 11 hours agorootparentI'm definitely bullish on this angle of compiling C down to LLVM assembly, and then \"decompiling\" it back to Rust (with some reference to the original C to reconstruct high-level idioms like for loops) reply saghm 12 hours agorootparentprevOh god, I can't even imagine trying to have formally-verified LLM-generated code. It's not surprising that even incremental progress for that would require quite a lot of ingenuity. reply CodesInChaos 6 hours agorootparentprevWhy does C2Rust produce so much incorrect code? Getting 5% wrong sounds terrible, for a 1:1 translation to unsafe Rust. What does it mis-translate? https://dl.acm.org/doi/pdf/10.1145/3597503.3639226 > As for C2Rust, the 5% unsuccessful translations were due to compilation errors, the majority of them caused by unused imports. I'm rather confused by what that's supposed to mean, since unused imports cause warnings, not errors in Rust. reply immibis 11 hours agorootparentprevActually, LLMs are for generating humorous nonsense. Putting them in charge of the world economy was not intended, but we did it anyway. reply dhosek 11 hours agorootparentGiven that in my (small, employer-mandated) explorations with Copilot autocompletions it’s offered incorrect suggestions about a third of the time and seems to like to also suggest deprecated APIs, I’m skeptical about the current generation’s ability to be useful at even this small task. reply LeFantome 1 hour agorootparentHave you seen O3? If your experience with something less than half as good as state-of-the-art is that it worked 66% of the time, I am not sure why you would be so dismissive about the future potential. reply glouwbug 3 hours agorootparentprevSure but it takes two copilots to fly a plane reply singron 15 hours agorootparentprevThis implementation uses unsafe. You can write a linked list in safe rust (e.g. using Rc), but it probably wouldn't resemble the one you write in C. In practice, a little unsafe is usually fine. I only bring it up since the article is about translating to safe rust. reply orf 15 hours agorootparentSafe rust isn’t “rust code with absolutely 0 unsafe blocks in any possible code path, ever”. Rc uses unsafe code every time you construct one, for example. Unsafe blocks are an escape hatch where you promise that some invariants the compiler cannot verify are in fact true. If the translated code were to use that collection, via its safe interfaces, it would still be “safe rust”. More generally: it’s incorrect to say that the rust ownership model forbids X when it ships with an implementation of X, regardless of if and how it uses “unsafe” - especially if “unsafe” is a feature of the ownership model that helps implement it. reply andrewflnr 14 hours agorootparentNo one here is confused about what unsafe means. The point is, they're not implemented by following Rust's ownership model, because Rust's ownership model does in fact forbid that kind of thing. You can nitpick the meaning of \"forbids\", but as far as the current context is concerned, if you translate code that implements a doubly linked list (as opposed to using one from a library) into Rust, it's not going to work without unsafe. Or an index-based graph or something. reply oneshtein 14 hours agorootparentIt's easy to implement doubly linked lists in safe Rust. Just ensure that every element has one OWNER, to avoid «use after free» bugs, or use a garbage collector, like a reference counter. Unlike C++ or Rust, C has no references, only pointers, so developer must release memory manually at some arbitrary point. This is the problem and source of bugs. reply saghm 12 hours agorootparentWhile I might agree that it's easy if you use a reference counter, this is not going to be as performant as the typical linked list written in C, which is why the standard library uses unsafe for its implementation of stuff like this. If it were \"easy\" to just write correct `unsafe`, then it would be easy to do it in C as well. Note that the converse to this isn't necessarily true! People I trust way more to write unsafe Rust code than me than me have argued that unsafe Rust can be harder than writing C in some ways due to having to uphold certain invariants that don't come up in C. While there are a number of blog posts on the topic that anyone interested can probably find fairly easily by googling \"unsafe Rust harder than C\", I'll break my usual rule of strongly preferring articles to video content to link a talk from youtube because the speaker is one of those people I mention who I'd trust more than me to write unsafe code and I remember seeing him give this talk at the meetup: https://www.youtube.com/watch?v=QAz-maaH0KM reply bonzini 7 hours agorootparent> unsafe Rust can be harder than writing C in some ways due to having to uphold certain invariants that don't come up in C. Yes, this is absolutely correct and on top of this you sometimes have to employ tricks to make the compiler infer the right lifetime or type for the abstraction you're providing. On the other hand, again thanks to the abstraction power of Rust compared to C, you can test the resulting code way more easily using for example Miri. reply oconnor663 11 hours agorootparentprevMore important than whether you use a little unsafe or a lot, is whether you can find a clean boundary above which everything can be safe. Something like a hash function or a block cipher can be piles and piles of assembly under the covers, but since the API is bytes-in-bytes-out, the safety concerns are minimal. On the other hand, memory-mapping a file is just one FFI function call, but the uncontrollable mutability of the whole thing tends to poison everything above it with unsafety. reply imtringued 7 hours agorootparentprevI don't really see it as a big \"owning\" of Rust that a complex pointer heavy structure with runtime defined ownership cannot be checked statically. Almost every language that people use doubly linked lists in has a GC, making the discussion kind of meaningless. So C and C++ are the exceptions to the rule, but how do they make it easy to write doubly linked lists? Obviously, the key assumption is that that the developer makes sure that node->next->prev = node->prev->next = node (Ignoring nullptr). With this restriction, you can safely write a doubly linked list even without reference counting. However, this isn't true on the pointer level. The prev pointers could be pointing at the elements in a completely random order. For example tail->prev = head, head->prev = second_last and so on. So that going backwards from the tail is actually going forwards again! Then there is also the problem of having a pointer from the outside of the linked list pointing directly at a node. You would need a weak pointer, because another pointer could have requested deletion from the linked list, while you're still holding a reference. If you wanted to support this generic datastructure, rather than the doubly linked list you have in your head, then you would need reference counting in C/C++ as well! What this tells you, is that Rust isn't restrictive enough to enforce these memory safe contracts. Anyone with access to the individual nodes could break the contract and make the code unsafe. reply pizlonator 14 hours agorootparentprevGood luck inferring how to use that from some C programmer’s deranged custom linked list. C programmers don’t do linked lists by using libraries, they hand roll them, and often they are more complex than “just” a linked list. Lots of complex stuff out there. reply oneshtein 14 hours agorootparentprevRus's ownership model doesn't forbid doubly linked lists. It forbids doubly owned lists, or, in other words, «use after free» bug. reply jerf 14 hours agorootparentprevThat's a classic example of an argument that looks really good from the 30,000 foot view, but when you're actually on the ground... no, basically none of that beautiful idea can actually be manifested into reality. reply bloppe 16 hours agorootparentprevIs this sarcastic? There's a reason why the lifetime checker is so annoying to people with a lot of C experience. You absolutely cannot just use your familiar C coding styles in Rust. reply orf 15 hours agorootparentYou’ve misread the comment. The ownership model is close enough, but the way that model is expressed by the developer is completely arbitrary (and thus completely nuts). reply whatisyourwork 17 hours agoparentprevIt can be good as an interface language. Good for bindings. reply titzer 15 hours agoparentprevMeh, you know people are just going to throw LLMs at it and they'll be fine with it hallucinating correctish code by the ton-load. But I agree that they are going to have tough time making idiomatic Rust from random C. Like I said, correct-ish. reply pizlonator 14 hours agorootparentGreat way to introduce novel security vulnerabilities! If that’s the Rust way, then I’m all for it. Will make it easier for Fil-C to have some epic kill shots. reply zoom6628 11 hours agoprevI wonder how this compares to the zig-to-C translate function. Zig seems to be awesome at creating mixed environs of zig for new code and C for old, and translating or interop, plus being a C compiler. There must be some very good reasons why Linux kernel maintainers aren't looking to zig as a C replacement rather than Rust. I don't know enough to even speculate so would appreciate those with more knowledge and experiencing weighing in. reply devjab 6 hours agoparentAs I understand it most kernel maintainers aren’t looking to replace C with anything. Zig has much better interoperability with C than Rust, but it’s not memory safe or stable. I think we’ll see quite a lot of Zig adoption in the C world, but I don’t think it’s in direct competition with Rust as such. In my region of the world nobody is adopting Rust, the C++ people are remaining in C++. There was some interest in Rust originally but it never really caught on in any company I know of. Likely for the same reason Go has become huge in younger companies but will not really make its way into companies which are traditionally Java/C# because even if it made sense technically (and it probably doesn’t) it’s a huge change management task. Zig is seeing traction for programs without the need for dynamic memory allocation, but not much beyond that. reply ChristianJacobs 11 hours agoparentprev> looking to zig as a C replacement rather than Rust Rust isn't a \"replacement for C\", but an addition to it. It's a tool that Torvalds et. al. has recognised the value of and thus it's been allowed in the kernel. The majority of the kernel code will still be written in C. I'm no kernel maintainer, but I can speculate that two of the main reasons for Rust over Zig are the compile time guarantees that the language provides being better as well as the rate of adoption. There is a lot of work done by many leading companies in the industry to provide Rust native code or maintained Rust bindings for their APIs. Windows devs are re-writing parts of _their_ kernel in Rust. There's a \"movement\" going on that has been going on for a while. I only hope it doesn't stop. Maybe the maintainers feel like Zig doesn't give them enough over C to be worth the change? Many of them are still opposed to Rust as well. reply DonaldPShimoda 11 hours agorootparent> Rust isn't a \"replacement for C\" Hmm I think to clarify I would say that Rust _is_ intended as a replacement for C in general, but that this isn't how the Linux kernel developers are choosing to use it. As for why the kernel developers would choose Rust, I would think another one of the primary benefits is that the type system guarantees the absence of a wide class of memory-related errors that are prevalent in C, and this type system (as well as those of its predecessors) has been subjected to significant scrutiny by the academic community over the last couple of decades to help iron out problems. I suspect this is also part of why Rust has a relatively large and passionate community compared to other C alternatives. reply burakemir 10 hours agorootparentAgreed. The large and passionate community may have multiple factors but \"things actually work\" is probably a factor. It is hard to get a full picture of how academic research influenced Rust and vice versa. Two examples: - The use of linearity for tracking ownership in types has been known to academics but had never found its way into a mainstream language. - researchers in programming language semantics pick Rust as a target of formalization, which was only possible because of design choices around type system. They were able to apply techniques that resulted from decades of trying to get a certified C. They have formalized parts of the standard library, including unsafe Rust, and found and fixed bugs. So it seems fair to say that academic research on safety for C has contributed much to what makes Rust work today, and in ways that are not possible for C and C++ because these languages do not offer static guarantees where types Transport information about exclusive access to some part of memory. reply josefx 10 hours agorootparentprev> It's a tool that Torvalds et. al. has recognised the value of and thus it's been allowed in the kernel. Has there actually been a successfull contribution to the mainline kernel? The last two big projects I heard of (ext2 / Apple drivers) seemed to have issues getting their code accepted. reply shakna 8 hours agorootparentrnull is in the kernel. And I believe one of the generic Realtek drivers is Rust as well. reply spiffyk 11 hours agoparentprevZig is nowhere near mature enough to be considered for the kernel yet. There are breaking changes to it regularly still - which is a good thing for Zig now, but not so good for huge, long-lived codebases like Linux. Also compiler bugs happen. Saying this as someone who generally likes Zig's direction. reply 3836293648 9 hours agoparentprevZig isn't 1.0 and has zero backcompat guarantees. It's also barely used anywhere and hasn't proven its value, even if parts of it may seem promising reply capitol_ 11 hours agoparentprevMaybe because zig isn't memory safe. reply Ar-Curunir 3 hours agoparentprevRust is stable and used by a number of big players, while Zig is not stable, and as a result hasn’t seen widespread adoption yet reply mbana 9 hours agoprevCan something like `C2Rust` then use this to generate formally correct code? Also, is much of the authors did manual or was it run through something to produce the Rust code? If so, where is the code that generates Rust, I do not see any links to any source repos. reply zozbot234 9 hours agoparent> If so, where is the code that generates Rust, I do not see any links to any source repos. The paper states that these developments will be released under open source licenses after the review process is completed, i.e. most likely, after the paper is formally published. reply p0w3n3d 9 hours agoprevI wonder, if a C library is working (i.e. is not formally proven to be not having problems, but works in most ways) why shouldn't we translate it using rust unsafe? I would say there is a value in it as rust lacks of libraries generally. And this would not be different from using a dll/so that was written in c and can be unsafe in some circumstances after all reply jtrueb 17 hours agoprevInteresting how higher optimization levels didn’t really help speed up rust in the O level comparison reply vlovich123 17 hours agoparentAs they say it’s likely that the code they’re outputting is pessimizing rustc’s ability. Namely it sounds like they’re inlining code as part of the conversion reply jtrueb 17 hours agorootparentYes, I’m just saying how it kicks in basically immediately (O1). reply ljlolel 18 hours agoprevI wonder how well O3 can do just compiling C to rust in one shot reply saagarjha 18 hours agoparentProbably pretty bad. reply ojosilva 18 hours agoparentprevFunny, I came here to say just the opposite, that I'm glad algorithmic computing is still a thing in research and that not everything is AI. Ironically, AI is able to produce research-grade algorithms and will probably become an authority on the subject, helping take more traditional CS to the next level. reply Garlef 16 hours agorootparentI think it would make sense to evaluate if the the 'surgical' rewrites mentioned in the article can be carried out by or assisted by an LLM based process. reply sunshowers 15 hours agorootparentprevThere's a lot of code in the world where correctness is a requirement. :) I agree with the sibling -- I think LLMs may be able to help automate some parts of it, but humans are still 95% of it. At least for now. reply nickpsecurity 14 hours agoprevThe thing I wonder about is why we would do this. The technology to really convert industrial-grade apps from C to Rust could probably bullet proof the C apps more easily. They’d just have to do some analyses that fed into existing tooling, like static analyzers and test generators. Similarly, it they might generate safe wrappers that let teams write new code in Rust side by side with the field-proven C. New code has the full benefits, old code is proven safe, and the interfaces are safer. A full on translator might be an ideal option. We’d want one language for the codebase in the future. Push-button safety with low, false positives for existing C and C++ is still the greatest need, though. Maybe auto-correcting bad structure right in the C, too, like Google’s compiler tool and ForAllSecure’s Mayhem do. reply DonaldPShimoda 10 hours agoparent> The technology to really convert industrial-grade apps from C to Rust could probably bullet proof the C apps more easily. No, some C programs cannot be made safe. This can be due to dependency on undefined or unspecified behaviors, or it can be because introducing proper safety checks would limit the domain of possible inputs too much to be useful, or other things. Translating to a safe language can maintain the expressive capabilities of the inputs while statically guaranteeing correct operation at run-time. It is objectively better in these cases. > field-proven C I don't think this exists, as the numerous critical vulnerabilities over the years have shown. All we have is C that seems to work pretty well often enough to be useful. > old code is proven safe Old code is assumed to be safe due to luck, actually. \"Prove\" has a specific meaning (especially on a post for a paper about proving things), and the overwhelming majority of C code is not proven to any rigorous mathematical standard. In contrast, the Rust type system has been mathematically proven to be correct. > A full on translator might be an ideal option. It depends on what you're willing to give up. If you don't mind losing performance, limiting your domain of inputs or range of outputs, giving up code legibility, and so on, then sure, this can probably be done to some extent. But when you start wanting your translator to be both sound and complete over all of these concerns, you run into problems. reply uecker 7 hours agorootparent> No, some C programs cannot be made safe. This can be due to dependency on undefined or unspecified behaviors, or it can be because introducing proper safety checks would limit the domain of possible inputs too much to be useful, or other things. You can certainly replace code using undefined behavior in C code by using defined constructs. > I don't think this exists, as the numerous critical vulnerabilities over the years have shown. All we have is C that seems to work pretty well often enough to be useful. I think this highly misleading. Some of the most reliable programs I know are written in C and Rust projects will also have critical vulnerabilities. Most vulnerabilities are not actually related to memory safety and the use of unsafe Rust will also lead to memory safety issues in Rust code. So I see some advantage to Rust but to me it is obviously overhyped. reply _flux 6 hours agorootparentprev> In contrast, the Rust type system has been mathematically proven to be correct. Is this the case? E.g. the issue \"Prove the Rust type system sound\" https://github.com/rust-lang/rust/issues/9883 is closed with comment \"This will be an open issue forever. Closing.\" in 2016: https://github.com/rust-lang/rust/issues/9883#issuecomment-2... . At least nowadays (since 2022) we do have a language specification for Rust: https://ferrous-systems.com/blog/the-ferrocene-language-spec... reply aw1621107 4 hours agorootparentThe closest thing is probably RustBelt [0], which proved the soundness of a subset of Rust that included borrowing/lifetimes. This was later extended to include relaxed memory accesses [1]. Neither of these papers include the trait system, unfortunately, and I'm not aware of another line of research that does (yet?). [0]: https://dl.acm.org/doi/10.1145/3158154 [1]: https://plv.mpi-sws.org/rustbelt/rbrlx/paper.pdf reply nickpsecurity 1 hour agorootparentprevThere’s tools that prove the safety of C code. They include RV-Match, Astree Analzer, and Frama-C. CompCert certifies the compilation. There’s tons of tools to find bugs by model checking, test generation, etc. Rust was nowhere near C last I checked in terms of tooling available. So, back to my original comment, the investment in making a Rust to C transpiler would make a C to safer C transpiler that fed that code into the verification tooling. A small transpiler would be easier to write with huge, immediate gains. On field proven, there’s two angles: spec and code. Many C/C++ apps didn’t become usable at all until years of iteration based on feedback from field use. An alternative in Rust might have incorrect specs which. From there, a number have had much code review, static analysis, and testing. They have few to no known bugs at any point. So, there are definitely C/C++ applications out there that are field-proven with a decent level of safety. (Note: Any transpiler might need to be bug for bug and weird behavior for weird behavior compatible to match the implicit spec in the C/C++ code.) You’re right about the average Rust vs C code, like type system vs correctness. I’d prefer new code be written in safer languages, which I added to my proposal. If apples to apples on correctness, you’d have to compare C written for verification with Rust. Quite a few C projects have been proven to be correct. Since you’re using a compiler, I’ll add that tools like Softbound+CETS make C code safe automatically. I do think Rust brings the cost and effort down quite a lot, though. It’s tooling is usually more mature or at least free than most for verifying C. reply dmezzetti 8 hours agoprevInteresting concept. But for a working system in C, why do we need to \"convert\" it to Rust. Seems like an effort where juice isn't worth the squeeze. Probably will create more problems than we're fixing. reply Alifatisk 18 hours agoprevc2rust.com, but it uses things like libc::c_int reply love2read 18 hours agoparentC2Rust is mentioned in the second paragraph of the related work section. reply rat87 11 hours agorootparentHow is c2rust doing these days? For practical codebases? reply kelnos 8 hours agoprevUgh. They didn't compile any C to Rust. They modified the F*-to-C compiler to emit Rust instead. So they compiled F* to safe Rust. And they couldn't even do that 100% reliably; some valid F* constructs couldn't be translated into Rust properly. They could either translate it into Rust code that wouldn't compile, or translate it into similar-looking Rust code that would compile, but would produce incorrect results. Flagged, this is just a lie of a title. reply dboreham 14 hours agoprevnext [2 more] [flagged] dang 14 hours agoparentMaybe so, but please don't post unsubstantive comments to Hacker News. Edit: it looks like you've been doing this a lot lately. Can you please not? We're trying for something more interesting here. reply ActorNightly 15 hours agoprev [5 more] [flagged] zxvkhkxvdvbdxz 13 hours agoparentWith rust having recently entered the Linux kernel, Windows 11, qemu among others where Haskell never took a hold, I really fail to see where you feel the wind is blowing. The thing is, rust is used today in more and more places because it's reliable. We're not going to switch out the ground we are standing on every time something shiny comes along and that's why this is such an interesting development. reply glouwbug 2 hours agorootparentEven steel plated armour succumbs to rust given the environment and time reply CoastalCoder 13 hours agoparentprevWould you mind expanding on this? It sounds interesting, but I'm not tuned into either community enough to know what parallels you see. reply Ar-Curunir 12 hours agoparentprev [–] What steps are you talking about? lambda calculus is one particular way to formalize program semantics, which is appropriate when talking about... program formalization reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper by Aymeric Fromherz and Jonathan Protzenko discusses translating C code to safe Rust, focusing on memory safety by avoiding unsafe Rust features.",
      "The authors introduce a type-directed translation method, static analysis using \"split trees\" for pointer arithmetic, and a strategy for handling C's struct types in Rust.",
      "Their approach successfully translates the HACL* cryptographic library and EverParse's binary parsers into a verified 80,000-line cryptographic library in pure Rust, maintaining performance with minimal strategic copies."
    ],
    "commentSummary": [
      "Converting C code to Safe Rust is difficult due to fundamental language differences, particularly Rust's ownership model, which requires significant redesign for safety.- Tools like C2Rust assist in translation, but the process is complex, and some C programs can't be ported without changing their semantics due to inherent unsafety.- While translating C to Rust can enhance safety and reveal bugs, Rust's safety is often misunderstood, and formal verification of C code, which aids translation, is not always available."
    ],
    "points": 257,
    "commentCount": 114,
    "retryCount": 0,
    "time": 1734737403
  },
  {
    "id": 42475703,
    "title": "A Raycaster in Bash",
    "originLink": "https://github.com/izabera/pseudo3d",
    "originBody": "a raycaster in bash wolfenstein-in-bash.mp4 more screenshots/vidoes at https://imgur.com/a/izas-wolfenstein-bash-journey-bAy5zhp largely a port of https://lodev.org/cgtutor/raycasting.html use the arrow keys to rotate and move around, and q to quit why this was a bit hard: bash is slow. this is by far the biggest issue. it's so slow that you cannot possibly achieve an acceptable frame rate if you have to execute even a single command per pixel. this implies that you also cannot keep the state of the screen in memory, neither as an array of colours (did you know that accessing a random element in an array takes linear time?) nor as a single long string (did you know that accessing the nth character in a string takes linear time even in LANG=C?), because literally just reading this representation to dump it to the screen will take longer than a frame bash has no floating point support nor access to a library of maths functions. all the maths is done on integers, scaled up by 100000 terminals are ugly if you use a full character to represent each pixel, so this uses unicode half blocks with different foreground and background colours, which effectively doubles the vertical resolution. there is unfortunately no way to update only one of the two colours in a cell, nor any way to query the current colours of a cell (besides, it would be too slow for bash), so every time we write a pixel we need to know the colour of an adjacent pixel. it would be really convenient if bash could store the state somehow but alas it cannot various misc annoyances: making sure all the terminal is updated at once is not trivial with a slow language like bash most terminals are not designed to play video games in (shockingly), so you cannot test if a key is currently pressed. instead you can only get a single key that's being held down, usually really slowly debounced and with a low limit for continued presses, so you probably get like 5-6 characters a second. you cannot even get multiple keys pressed at the same time unless some are modifiers. the kitty keyboard protocol 100% fixes all this, and i'm sure it will become a widely implemented standard by the year 2100 turns out that filling a terminal with colours takes a lot of data. at my normal font size this does ~10mb of i/o per second, which isn't very much in the grand scheme of things, but, you know, it's bash bash will never use a single syscall to print a string with more than one newline, regardless of the type of file you're writing to. this is pointless and dumb, and it's the reason why this never prints and always moves the cursor in other ways. ultimately this ended up printing more data than the size your terminal is likely getting in each read, so it might not matter too much, but it still bothered me ecma48/vt100/vt200/xterm... were all designed by insane people who hated me specifically holy shit i'm bad at maths, i went to uni for this what the fuck faq: q: it fucks things up when i resize the window/it's a flickery mess/it generally looks like shite on my terminal a: open an issue please q: my cpu heats up like crazy/my computer from 2005 slows down to a crawl a: try to set the env variable FPS to something less than 30 q: it doesn't work on my bash < 5 a: yep q: is this code all pure bash? a: no. it also calls stty once at startup to disable echo, and once at exit to re enable it",
    "commentLink": "https://news.ycombinator.com/item?id=42475703",
    "commentBody": "A Raycaster in Bash (github.com/izabera)215 points by izabera 20 hours agohidepastfavorite29 comments purplesyringa 11 hours agoI love this. I've been wondering how the picture is drawn with less than one `echo` per pixel, and it's very clever: the game is \"not really\" 3D, so you can run raytracing just once per column, and then you only need to draw a couple of lines (for sky, grass, and the actual object) -- this is done by outputting the \"draw this pixel and move down by one\" string to the terminal as many times as necessary using string repetition. I've been considering working on a voxel render engine (not for Bash, but for another computationally limited environment). This is a treasure, I'm certain I'll find something useful here. reply JKCalhoun 4 hours agoparentThe VoxelCanvas.js file (in Javascript, obv.) might be interesting to you as well. Same (raycasting) idea: https://github.com/EngineersNeedArt/Mooncraft2000 reply tdeck 15 hours agoprevIn case you wondered if there is a raycaster written in MS Batch: https://github.com/nTh0rn/batch-raycaster reply izabera 12 hours agoparentThat's really cool! The explanation on their blog is great! reply rpoisel 5 hours agorootparentThanks for the hint! That's exactly what I was looking for as I have never looked into raycasting but having an interest in it. Direct Links: - https://nthorn.com/articles/batch_raycaster/ (batch variant) - https://lodev.org/cgtutor/raycasting.html (general intro) reply mmh0000 56 minutes agoprevOf course, we need to give an honorable mention to the `awk` raycaster from 9 years ago. https://github.com/TheMozg/awk-raycaster/tree/master reply Arch-TK 49 minutes agoparentCool Easter egg: https://github.com/TheMozg/awk-raycaster/blob/master/awkaste... reply Arch-TK 19 hours agoprevIt's unfortunate that stty requires forking. Maybe the next project will be to use bash and rowhammer to call the necessary ioctls to do it without forking. reply 8n4vidtmkvmk 11 hours agoprevI never ceases to baffle me how we're still stuck with these mind bogglingly slow shells. Pure madness. I can maybe understand that some apps require all the vt100 weirdness or whatever, but probably 90% of apps just write to stdout and err. Surely we can blit some text to the screen a bit quicker and put the other 10% into some compat mode. reply PhilipRoman 10 hours agoparentShells are slow (particularly bash), sure, but I'm not sure how the rest of your comment follows from that. The shell is not involved in interpreting terminal escape sequences in any way, and modern terminals are quite fast - I can render animations in a 350-column terminal and they are as smooth as can be, given the constraints. Besides, the whole premise of this post is that bash is the wrong language for raycasting, kind of like writing bubblesort in CSS. >put the other 10% into some compat mode There is nothing preventing that, just test if the string contains sane characters only and use a fast path for that. The problem is that there is no actual fast path for software text rendering, you still need to process ligatures, etc. reply alsetmusic 17 hours agoprevI had no idea this was possible with Bash. I’ve considered myself proficient with Bash at a pretty advanced level at times and this just blows me away. I don’t have the math chops to understand how it’s implemented, but it’s a pleasure to see. reply markeroon 19 hours agoprevOnly 300 lines of code, impressive! I love this. reply anfractuosity 6 hours agoprevVery cool! I'm curious when it says \"did you know that accessing a random element in an array takes linear time\", why that's the case, with bash? reply izabera 4 hours agoparentnormal arrays in bash are implemented as linked lists. bash stores a pointer to the last accessed element, which turns the most common case of iteration into O(1), but the performance is terrible if you need to jump around see some basic benchmarks here https://gist.github.com/izabera/16a46ed79c2248349a1fb8384468... there are also associative arrays which are bucketed hash tables, which are fine for string keys but imho they are hardly ever worth it as a replacement for indexed arrays reply anfractuosity 1 hour agorootparentThanks a lot, that's very interesting re. it using linked lists, and nice benchmark! reply Teongot 19 hours agoprevI'd love to see this combined with the author's fork()-less implementation of ps to make a (almost) fork()-free implementation of psDoom. Seriously though, this is really cool reply einpoklum 18 hours agoprevAnd to think, that my own bash scripts spend 300 lines just parsing various command-line options, while instead I could be showing this game... :-P reply shric 18 hours agoprevBeautiful. Correct me if I'm wrong, but this looks like a bash version of https://lodev.org/cgtutor/raycasting.html Edit: ah, just noticed this is in the readme :( reply svag 7 hours agoparentThere are https://lodev.org/cgtutor/raycasting2.html and https://lodev.org/cgtutor/raycasting3.html just FYI reply izabera 18 hours agoparentprevyeah i really liked that tutorial reply CaesarA 17 hours agoprevI wonder if texture mapping this would look good. reply Arch-TK 17 hours agoparentShe has been brainstorming how to handle texture mapping within the performance constraints of doing it in bash for a week now (long before she actually got this working) and so far we've come up with some hypothetical ideas but she has not tried any of them yet. Maybe tomorrow... reply ormaaj 6 hours agoprev<3 reply corytheboyd 17 hours agoprev [–] Imagine a TUI where you need to navigate a literal maze of options reply jaynetics 9 hours agoparentLike this one in Jurassic Park? https://youtu.be/URVS4H7vrdU Funnily enough, the slow rendering is kind of a plot device here. reply Arch-TK 8 hours agorootparentThe program from that scene was not some crazy invention for the purpose of having an interesting computer scene. It is a real piece of software for silicon graphics workstations called fsn (File System Navigator). There's a x windows remake of it called fsv: https://fsv.sourceforge.net/ And then there's this even cooler UI which takes this whole idea much further called eagle mode: https://eaglemode.sourceforge.net/ reply rzzzt 7 hours agorootparentpsDooM pits you against processes on the system represented as enemies: https://psdoom.sourceforge.net/ reply Arch-TK 17 hours agoparentprevMany CLI programs which take option have already managed to do this (in the text-based adventure game style). reply krapp 17 hours agoparentprev [–] That's the kind of \"futuristic interface\" you'd see in a cyberpunk movie, and it would be incredibly annoying in real life. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A raycaster inspired by the game Wolfenstein has been implemented in Bash, demonstrating the challenges of using Bash for graphical applications.- The project faces limitations due to Bash's slowness, lack of floating-point support, and terminal constraints, making it difficult to maintain screen state and optimize performance.- Despite using Unicode half blocks for better resolution, the project struggles with high I/O demands and inefficient color updates, highlighting the complexity of using Bash for such tasks."
    ],
    "commentSummary": [
      "A Raycaster in Bash is a project that simulates a 3D effect using raytracing once per column, efficiently rendering scenes with minimal `echo` commands.",
      "The project is notable for its use of string repetition to draw lines for sky, grass, and objects, making it an inspiring example for creating voxel render engines in constrained environments.",
      "The discussion highlights similar raycasting projects in MS Batch and `awk`, showcasing creativity in using unconventional programming languages for complex tasks."
    ],
    "points": 215,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1734733545
  },
  {
    "id": 42475228,
    "title": "Qualcomm wins licensing fight with Arm over chip designs",
    "originLink": "https://www.bloomberg.com/news/articles/2024-12-20/qualcomm-wins-licensing-fight-with-arm-over-chip-designs",
    "originBody": "Bloomberg Need help? Contact us We've detected unusual activity from your computer network To continue, please click the box below to let us know you're not a robot. Why did this happen? Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy. Need Help? For inquiries related to this message please contact our support team and provide the reference ID below. Block reference ID: 15d5ff28-bfce-11ef-a7c0-fb170779c6a1",
    "commentLink": "https://news.ycombinator.com/item?id=42475228",
    "commentBody": "Qualcomm wins licensing fight with Arm over chip designs (bloomberg.com)196 points by my123 21 hours agohidepastfavorite108 comments jasoneckert 20 hours agoBy the end of Day 3, it seemed quite clear that Qualcomm's legal team and position was far ahead of ARM's. I feel the following snippet sums up the whole week: \"Qualcomm’s counsel turned Arm’s Piano analogy on its head. Arm compared its ISA to a Piano Keyboard design during the opening statement and used it throughout the trial. It claimed that no matter how big or small the Piano is, the keyboard design remains the same and is covered by its license. Qualcomm’s counsel extended that analogy to show how ridiculous it would be to say that because you designed the keyboard, you own all the pianos in the world. Suggesting that is what Arm is trying to do.\" Source: https://www.tantraanalyst.com/ta/qualcomm-vs-arm-trial-day-3... reply fc417fc802 16 hours agoparentThis is really confusing me. Is Arm seriously claiming that all design work that makes use of their ISA is derivative work? I feel like I have to be misunderstanding something. Wouldn't that be similar to the Google v Oracle Java API case except the claim would be even stronger - that all programs making use of the Java API were derivative works of the Java API and thus subject to licensing arrangements with Oracle? Or similarly, a hypothetical claim by Intel that a compiler such as LLVM is derivative work of the x86 ISA. That can't possibly be right. What have I misunderstood about this situation? reply eigenform 15 hours agorootparentYeah, I did a double-take when I read that too - but that does seem to be the case. From a different article [^1]: > \"Throughout expert testimony, Arm has been asserting that all Arm-compliant CPUs are derivatives of the Arm instruction set architecture (ISA).\" > \"Arm countered with an examination of the similarities in the register-transfer language (RTL) code, which is used in the design of integrated circuits, of the latest Qualcomm Snapdragon Elite processors, the pre-acquisition Nuvia Phoenix processor, and the Arm ISA (commonly referred to as the Arm Arm).\" Were they trying to argue that the RTL is too similar to the pseudocode in the ARM ARM or something?? That is absolutely crazy. (Of course, [when we have a license agreement and] you publish a public specification for the interface, I am going to use it to implement the interface. What do you expect me to do, implement the ARM ISA without looking at the spec?) edit: Wow, I guess this really is what they were arguing?? Look at the points from Gerard's testimony [^2]. That is absolutely crazy. [^1]: https://www.forbes.com/sites/tiriasresearch/2024/12/19/arm-s... [^2]: https://www.tantraanalyst.com/ta/qualcomm-vs-arm-trial-day-2... reply fc417fc802 15 hours agorootparentI would assume (but don't actually know) that compiler authors make extensive use of the (publicly available) ARM as well. But claiming that any associated llvm backends are a derivative work seems absurd to me. I really feel like I must have misunderstood something here. reply mattmaroon 15 hours agorootparentprevWell they did lose the case. Whatever they were contending was clearly incorrect. reply fc417fc802 15 hours agorootparentBloomberg article indicates the jury is still out on the question of whether or not Nuvia breached the license. They only agreed that Qualcomm's own ALA covers use of the tech in the event that they happen to possess it. In other words, Nuvia failing to destroy the designs might or might not have been a breach of contract. At least if I understand all of this correctly. But I feel like I must be missing some key details. reply ggerules 15 hours agorootparentprevActually Sun v Microsoft in the late 90s. reply wmf 14 hours agorootparentprevIs Arm seriously claiming that all design work that makes use of their ISA is derivative work? I assume Arm has some patents on the ISA [1] and the only way to get a license to them is to sign something that effectively says all your work exists at Arm's sufferance. After that we're just negotiating the price. [1] You and I hate this but it's probably valid in the US. reply phire 12 hours agorootparentQualcomm already had a license for ARMs patents. An older licence with much better terms. reply adrian_b 9 hours agorootparentARM has unilaterally cancelled both the Nuvia architecture license agreement (ALA) and the Qualcomm ALA. Because all Arm ALAs are secret, we do not know if Arm has any right to do such a unilateral cancellation. It is likely that the ALAs cannot be cancelled without a good reason, like breech of contract, so the cancellation of the Qualcomm ALA must be invalid now, after the trial. The conflict between Arm and Qualcomm has started because the Qualcomm ALA, which Qualcomm says that it is applicable for any Qualcomm products, specifies much lower royalties than the Nuvia ALA. This is absolutely normal, because Qualcomm sells a huge number of CPUs for which it has to pay royalties, while Nuvia would have sold a negligible number of CPUs, if any. Arm receives a much greater revenue based on the Qualcomm ALA than what they would have received from Nuvia. Therefore the real reason of the conflict is that Qualcomm has stopped using CPU cores designed by Arm, so Arm no longer receives any royalty from Qualcomm from licensing cores, and those royalties would have been higher than the royalties specified by the ALA for Qualcomm-designed cores. When Arm has given an architectural license to Nuvia, they did not expect that the cores designed by Nuvia could compete with Arm-designed cores. Nuvia being bought by Quacomm has changed that, and Arm attempts now to crush any competition for its own cores. reply bitwize 12 hours agorootparentprev> Or similarly, a hypothetical claim by Intel that a compiler such as LLVM is derivative work of the x86 ISA. Intel has been lenient toward compiler implementers, but their stance is that emulation of x86 instructions still under patent (e.g., later SSE, AVX512) is infringing if not done under a license agreement. This has had negative implications for, for example, Microsoft's x86 emulation on ARM Windows devices. (I'm guessing Apple probably did the right thing and ponied up the license fees.) reply ksec 18 hours agoparentprevWholeheartedly agree. I understand where ARM is coming from, but my god the legal team from both parties were night and day apart. And from evidences ARM isn't even asking for a lot more money. They are likely fighting this from principle, but their explanation were weak, very weak. ( They were even worst then Apple during the Apple vs Qualcomm case ) I thought the whole thing Qualcomm was way more professional. ARM's case was that what they think was written in the contract, what they \"should\" have written in contract and what Qualcomm shows clearly contradict. It is more of a lesson for ARM to learn. And now the damage has been done. This also makes me think who was pushing this lawsuit. Softbank ? I also gained more respect to Qualcomm. After what they showed Apple vs Qualcomm's case and here. Side Note: ARM's Design has caught on. The Cortex X5 is close to Apple's Design. We should have news about X6 soon. reply fc417fc802 17 hours agorootparent> ARM isn't even asking for a lot more money I thought the entire point of this was that Arm was trying to prevent Qualcomm from switching away from products that fall under the TLA. Isn't revenue from TLA fees a huge difference from that of ALA fees? reply adrian_b 8 hours agorootparentYes, that is correct. reply ksec 11 hours agoparentprevAnd here is an update from [1] Reuter \"I don't think either side had a clear victory or would have had a clear victory if this case is tried again,\" Noreika told the parties.\" After more than nine hours of deliberations over two days, the eight-person jury could not reach a unanimous verdict on the question of whether startup Nuvia breached the terms of its license with Arm. [1] https://www.reuters.com/legal/us-jury-deadlocked-arm-trial-a... reply Tuna-Fish 6 hours agoparentprevMy personal first hint was when ARM, the plaintiff in a contract case, demanded a jury trial. When your contracts are airtight, you usually want a bench trial. Then the defendant demands a jury. reply walterbell 16 hours agoprevFor details beyond Bloomberg's three paragraph summary: https://www.tantraanalyst.com/ta/qualcomm-vs-arm-trial-day-1... > Arm’s opening statement.. presented with a soft, almost victim-like demeanor. Qualcomm’s statement was more assertive and included many strong facts (e.g., Arm internal communications saying Qualcomm has “Bombproof” ALA). Testimonials were quite informative and revealed many interesting facts, some rumored and others unknown (e.g. Arm considered a fully vertically integrated approach). https://www.tantraanalyst.com/ta/qualcomm-vs-arm-trial-day-2... > The most important discussion was whether processor design and RTL are a derivative of Arm’s technology.. This assertion of derivative seems an overreach and should put a chill down the spine of every Arm customer, especially the ones that have ALA, which include NXP, Infineon, TI, ST Micro, Microchip, Broadcom, Nvidia, MediaTek, Qualcomm, Apple, and Marvell. No matter how much they innovate in processor design and architecture, it can all be deemed Arm’s derivative and, hence, its technology. https://www.tantraanalyst.com/ta/qualcomm-vs-arm-trial-day-3... https://www.tantraanalyst.com/ta/qualcomm-vs-arm-trial-day-4... reply LeFantome 20 hours agoprevWow, this has been settled already? I mean, I am sure ARM will appeal. ARM did massive damage to their ecosystem for nothing. There will for sure be consequences of suing your largest customer. Lots of people that would have defaulted to licensing designed off ARM for whatever chips they have planned will now be considering RISC-V instead. ARM just accelerated the timeline for their biggest future competitor. Genius. reply bhouston 20 hours agoparentRISC-V is not anywhere near competitive to ARM at the level that Qualcomm operates. I’ve written about that here: https://benhouston3d.com/blog/risc-v-in-2024-is-slow reply fidotron 20 hours agorootparentYour otherwise on point piece contains the common misconception that ARM began in embedded systems. When they started they had a full computer system that had very competitive CPU performance for the time: https://en.m.wikipedia.org/wiki/Acorn_Archimedes They pivoted to embedded shortly after spinning off into a separate company. reply MissTake 19 hours agorootparentNot to be pedantic, but… Acorn Computers started off much earlier (I owned an Acorn Atom when it was released) which begat the Electron, then the BBC Micro and then the Archimedes. At that time ARM was just an architecture owned by Acorn. They created it with VSLI technology (Acorn’s Silicon partner) and used the first RISC chip in the BBC Micro before then pivoting it to the Archimedes. Whilst Acorn itself was initially purchased by Olivetti, who eventually sold what remained years later to Morgan Stanley. The ARM division was spun off as “Advanced RISC Machines” in a deal with both Apple, and VSLI Technology after Olivetti came onto the scene. It is this company that we now know as Arm Holdings. So it’s not entirely accurate to claim “they had a full computer system” as that was Acorn Computers, PLC. reply fidotron 17 hours agorootparentActually one of the first ARM spin off products was the 250 for Acorn, which was an entire Acorn computer system on a a chip. Some of the other details you have are wrong too, to the point your comment is really quite misleading. Anyone wanting an accurate version should check wikipedia: https://en.m.wikipedia.org/wiki/Acorn_Computers (To be blunt the above comment is like a very bad LLM summary of the Acorn article). reply crote 19 hours agorootparentprevIt would be more accurate to say that there haven't been any RISC-V designs for Qualcomm's market segment yet. As far as I am aware, there is nothing about the RISC-V architecture which inherently prevents it from ever being competitive with ARM. The people designing their own cores just haven't bothered to do so yet. RISC-V isn't competitive in 2024, but that doesn't mean that it still won't be competitive in 2030 or 2035. If you were starting a project today at a company like Amazon or Google to develop a fully custom core, would you really stick with ARM - knowing what they tried to do with Qualcomm? reply hashtag-til 18 hours agorootparentBut then there is the software ecosystem issue. Having a competitive CPU is 1% of the job. Then you need To have a competitive SoC (oh and not infringe IP), so that you can build the software ecosystem, which is the hard bit. reply Twirrim 16 hours agorootparent> But then there is the software ecosystem issue. We still have problems with software not being optimised for Arm these days, which is just astounding given the prevalence on mobile devices, let alone the market share represented by Apple. Even Golang is still lacking a whole bunch of optimisations that are present in x86, and Google has their own Arm based chips. Compilers pull off miracles, but a lot of optimisations are going to take direct experience and dedicated work. reply dlcarrier 15 hours agorootparentConsidering how often ARM processors are used to run an application on top of a framework over an interpreted language inside a VM, all to display what amounts to kilobytes of text and megabytes of images, using hundreds of megabytes of RAM and billions of operations per second, I'm surprised anyone even bothers optimizing anything, anymore. reply neonsunset 10 hours agorootparentprev> Even Golang Golang's compiler is weak compared to the competition. It's probably not a good demonstration of most ISAs really. reply bluGill 16 hours agorootparentprevNot an issue because exceyt for a few windows or apple machines everthing arm is compiled and odds are they have the source. Give our ee a good risc-v and a couple years latter we will have our stuff rebult for that cpu reply threeseed 10 hours agorootparentThe whole reason ARM transition worked is that you had millions of developers with MacBooks who because of Rosetta were able to seamlessly run both x86 and ARM code at the same time. This meant that you had (a) strong demand for ARM apps/libraries, (b) large pool of testers, (c) developers able to port their code without needing additional hardware, (d) developers able to seamlessly test their x86/ARM code side by side. RISC-V will have none of this. reply philistine 2 hours agorootparentApple is the only company that has managed a single CPU transition successfully. That they actually did it three times is incredible. I think people are blind to the amount of pre-emptive work a transition like that requires. Sure, Linux and FreeBSD support a bunch of architectures, but are they really all free of bugs due to the architecture? You can't convince me that choosing an esoteric, lightly used arch like Big Endian PowerPC won't come with bugs related to that you'll have to deal with. And then you need to figure out who's responsible for the code, and whether or not they have the hardware to test it on. It happened to me; small project I put on my ARM-based AWS server, and it was not working even though it was compiled for the architecture. reply bluGill 6 hours agorootparentprevEmbedded is far larger than pcs and dosn't needthat phones too are larger an already you recompile as needed. reply Krastan 18 hours agorootparentprevWe've seen compatibility layers between x86 and arm. Am I correct in thinking that a compatibility layer between riscV and arm would be easier/more performant since they're both risc architectures? reply BirAdam 16 hours agorootparentThere are already compatibility layers for x86 on RISC-V. They’re not quite as good, but progress is being made. Edit, link: https://box86.org/2024/08/box64-and-risc-v-in-2024/ reply snvzz 14 hours agorootparentprevRISC-V is rapidly growing the strongest ecosystem. The new (but tier1 like x86-64) Debian port is doing alright[0]. It'll soon pass ppc64 and close up to arm64. 0. https://buildd.debian.org/stats/graph-week-big.png reply inkyoto 16 hours agorootparentprev> RISC-V isn't competitive in 2024, but that doesn't mean that it still won't be competitive in 2030 or 2035. We can't know and won't for up to until 2030 or 2035. Humans are just not very good when it comes projecting the future (if predictions of 1950-60's were correct, I would be typing this up from my cozy cosmic dwelling on a Jovian or a Saturnian moon after all). History has had numerous examples when better ISA and CPU designs have lost out to a combination or mysteries and other compounding factors that are usually attributed to «market forces» (whatever that means to whomever). The 1980-90's were the heydays of some of the most brilliant ISA designs and nearly everyone was confident that a design X or Y would become dominant, or the next best thing, or anywhere in between. Yet, we were left with a x86 monopoly for several decades that has only recently turned into a duopoly because of the arrival of ARM into the mainstream and through a completely unexpected vector: the advent of smartphones. It was not the turn than anyone expected. And since innovations tend to be product oriented, it is not possible to even design, leave alone build, a product with something does not exist yet. Breaking a new ground in the CPU design requires an involvement of a large number of driving and very un–mysterious (so to speak) forces, exorbitant investment (from the product design and manufacturing perspectives) that are available to the largest incumbents only. And even that is not guaranteed as we have seen it with the Itanium architecture. So unless the incumbents commit and follow through, it is not likely (at least not obvious) that RISC-V will enter the mainstream and will rather remain a niche (albeit a viable one). Within the realms of possibility it can be assessed as «maybe» at this very moment. reply K0balt 6 hours agorootparentA lot of the arguments I’m seeing ignore the factor that China sees ARM as a potential threat to it’s economic security and is leaning hard into risc-v. it’s silly to ignore the largest manufacturing base for computing devices when talking about the future of computing devices. I would bet on china making risc-v the default solution for entry level and cost sensitive commodity devices within the next couple of years. It’s already happening in the embedded space. The row with Qualcomm only validates the rationale for fast iterating companies to lean into riscv if they want to meaningfully own any of their processor IP. The fact that the best ARM cores aren’t actually designed by ARM, but arm claims them as its IP is really enough to understand that migrating to riscv is eventually going to be on the table as a way to maximize shareholder value. reply dmitrygr 18 hours agorootparentprev> As far as I am aware, there is nothing about the RISC-V architecture which inherently prevents it from ever being competitive with ARM Lack of reg+shifted reg addressing mode and or things like BFI/UBFX/TBZ The perpetual promise of magic fusion inside the cores has not played out. No core exists to my knowledge that fuses more than two instructions at a time. Most of those take more than two to make. Thus no core exists that could fuse them. reply Arnavion 16 hours agorootparentZba extension (mandatory in RVA23 profile) provides `sh{1,2,3}add rd, rs1, rs2` ie `rd = rs1RVA23 26.04 (LTS) -> RVA23 from: https://www.youtube.com/watch?v=oBmNRr1fdak reply mshockwave 14 hours agorootparentprevZb extension is in both RVA22 and RVA23 profiles, meaning application cores (targeting consumer devices like smartphones) designed in the past few years almost certainly have shXadd instructions in order to be compatible with the mainstream software ecosystem reply dmitrygr 13 hours agorootparent> meaning application cores (targeting consumer devices like smartphones where are they? reply camel-cdr 11 hours agorootparentYou are aware that hardware takes time to build, tapeout and productise? On the open-source front, I can right now download a RVA23 supporting RISC-V implementation, simulate the RTL and have it out perform my current Zen1 desktop per cycle in scalar code: https://news.ycombinator.com/item?id=41331786 (see the XiangShanV3 numbers) reply fidotron 6 hours agorootparentRISC-V has existed for over a decade and in that time no one has got close to building a competitive non microcontroller level CPU with it. How long is this supposed to take? How long until it is accepted that RISC-V looks like a semiconductor nerd snipe of epic proportion designed to divert energy away from anything that might actually work? If it was not designed for this it is definitely what it has achieved. reply justahuman74 13 hours agorootparentprevRVA23 will become the only thing anyone targets reply dmitrygr 1 hour agorootparentIn the year of the Linux desktop, with Wayland and IPv6 for all. /s reply dmitrygr 1 hour agorootparentprevI think you’re missing a point here. The fact that this was not part of the initial design speaks volumes, since it is entirely obvious to anybody who has ever designed an ISA or looked at assembly of modern binaries. Look at aarch64 for an example an ISA designed for actual use. reply hajile 15 hours agorootparentprevYour statement that \"RISC-V in 2024 is slow\" gets followed by a crazy sequitur that this will continue to be the case for a long time. Ventana announced their second-gen Veyron 2 core at the beginning of this year and they are releasing a 192-core 4nm chip using it in 2025. They claim Veyron 2 is an 8-wide decoder with a uop cache allowing up to 15-wide issue and a 512-bit vector unit too. In raw numbers, they claim SpecInt per chip is significantly higher than an EPYC 9754 (Zen4) with the same TDP. We can argue about what things will look like after it launches, but it certainly crushes the idea that RISC-V isn't going to be competing with ARM any time soon. reply bhouston 6 hours agorootparentIn my article I only say that it is currently slow and that there are various initiatives to make it fast and I am hopeful for the future. reply boredatoms 20 hours agorootparentprevRISCV is an instruction set, but you compare ASICs If qualcomm changes instruction decoding over you’ll likely see a dramatic difference reply 3eb7988a1663 20 hours agorootparentprevNot everyone is trying to make a chip for a phone. There are plenty of low compute applications which just need something. reply refulgentis 19 hours agorootparentCorrect. Also correct: RISC-V is not anywhere near competitive to ARM at the level that Qualcomm operates. reply 6SixTy 19 hours agorootparentprevHow is Geekbench any good at comparing RISC-V to ARM? Geekbench isn't a native RISC-V application, let alone has the wherewithal to correctly report any basic information like frequency or core count. You haven't even prefaced these either, and drew conclusions from them. Also, actually searching the chip in question is impossible. reply phire 19 hours agorootparentThere are preview builds of Geekbench that are native RISC-V. reply 6SixTy 18 hours agorootparentDoesn't mean that it is compiled with Vector support or optimizations, which is going to artificially make the results worse. reply refulgentis 18 hours agorootparentNeutral party here, well not so neutral, I up voted you, it's a reasonable question at first blush: however here, you're doubling down with another comment that very clearly indicates you didn't read the link provided I highly recommend it, most incisive RISC-V article I've read. reply snvzz 13 hours agorootparentYour reply does not address the parent. Geekbench does indeed not support the Vector extension, and thus yields very poor results on RISC-V. reply refulgentis 11 hours agorootparentYou reply does not address the great great great grandparent. RISC-V does indeed not have pipeline reordering. The Geekbench score thing is a strawman invented to distract from that, no one has mentioned Geekbench except the people arguing it doesn't matter. Everyone agrees, it doesn't matter. So why pound the table about it? reply nine_k 18 hours agorootparentprevBut for many other customers, who might need something like an A0 core, it's a strong signal to consider RISC-V instead. reply snvzz 13 hours agorootparentprevIf you're talking physical chips you can buy off the shelf, sure. But if you're talking IP, which would be what matters for the argument being made (core IP to use on new design), here's where we at (thanks to camel-cdr- on reddit[0]): (rule of thumb SPEC2006*10 = SPEC2017) SiFive P870-D: >18 SpecINT2006/GHz, >2 SpecINT2017/GHz Akeana 5300: 25 SpecINT2006/GHz @ 3GHz Tenstorrent Ascalon: >18 SpecINT2006/GHz, IIRC they mentioned targeting 18-20 at a high frequency Some references for comparing: Apple M1: 21.7 SpecINT2006/GHz, 2.33 SpecINT2017/GHz Apple M4: 2.6 SpecINT2017/GHz Zen5 9950x: 1.8 SpecINT2017/GHz Current license-able RISC-V IP is certainly not slow. 0. https://www.reddit.com/r/hardware/comments/1gpssxy/x8664_pat... reply maximusdrex 20 hours agoprevWhat a disaster for ARM. Qualcomm building out new chips targeting the pc market should have been a victory lap for ARM, not the source of a legal battle with their largest customer. Now potential customers might be a little more wary of ARMs licensing practices compared to the free RISC-V ISA. reply wyldfire 15 hours agoparent> Now potential customers might be a little more wary of ARMs licensing practices compared to the free RISC-V ISA. This is unbelievably understated. If I were Qualcomm, I would put parts of the Nuvia team's expertise to work designing RISC-V applications cores for their various SoC markets. reply Havoc 14 hours agorootparentIf you bet the farm on hardware but the software ecosystem isn't there yet, then you sell no hardware and sink the company. Software ecosystem either takes lots of time (see ARM) or you need to be in a position to force it (Apple & M chips). RISC-V is still a long way off from consumer (or server) prime time reply wyldfire 35 minutes agorootparentIt's a bit chicken-and-egg. People won't port their software if there's no popular targets available. Even if there are targets, if the popular targets don't perform well, people will assume the ISA is not worth porting to. No, it's not just around-the-corner but Qualcomm has a role to play here. Not like they should just sit on the sidelines and say \"call me when we are RISC-V\" reply williamDafoe 18 hours agoprevAs I said on another forum yesterday, Qualcomm almost always wins its legal battles - when they lose its not because they are wrong but usually only because their lawyers screwed up (Broadcom lawsuit of ~2012). It's kind of a Boy Scout Company in a legal sense and they are very careful. They retrained some of their best engineers as lawyers to help them succeed in court battles ... reply hashtag-til 18 hours agoparentRetrain engineers as lawyers is clever. Better than repurpose them as subpar managers. reply kevvok 11 hours agorootparentOh don’t worry, they have plenty of those too reply justahuman74 13 hours agoparentprevHow does one sign up for this retraining program? reply hu3 21 hours agoprevhttps://archive.is/2uHTU reply jasoneckert 21 hours agoprevDoes the fact that the \"jurors weren’t able to agree on whether Nuvia breached the license\" mean that the legal fight isn't over? Or is that question irrelevant in light of the other findings, and the legal fight is actually over, with Qualcomm as the clear winner? reply phire 21 hours agoparentI suspect it's mostly irrelevant. ARM should be able to re-file the lawsuit and get financial damages out of Nuvia, which Qualcomm will need to pay. But I doubt the damages will be high enough to bother Qualcomm. I don't think ARM will even bother. As far as I could tell, this was never about money for ARM. It was about control over their licensees and the products they developed. Control which they could turn into money later. reply eigenform 20 hours agorootparentIt always seemed like [from ARM's point of view]: \"oh, you're going to sell way more parts doing laptop SoCs with the license instead of servers... if we'd known that before, we would've negotiated a different license where we get a bigger cut\" reply adrian_b 8 hours agorootparentArm had already done that. The Nuvia ALA (architecture license agreement) specified much higher royalties, i.e. a much bigger cut for Arm, than the Qualcomm ALA. The official reason for the conflict is that Qualcomm says that the Qualcomm ALA is applicable for anything made by Qualcomm, while Arm says that for any Qualcomm product that includes a trace of something designed at the former Nuvia company the Nuvia ALA must be applied. The real reason of the conflict is that Qualcomm is replacing the CPU cores designed by Arm with CPU cores designed by the former Nuvia team in all their products. Had this not happened, Arm would have received a much greater revenue as a result of Nuvia being bought by Qualcomm, even when applying the Qualcomm ALA. reply hashtag-til 20 hours agorootparentprevThat’s the model when you’re in the IP business - nothing new here. This is what you use to fund the next generation of said IP. There is no magic. reply hajile 15 hours agorootparentThe whole ALA essentially boils down to \"you pay us because other companies made our ISA popular\". This is why companies are pushing toward RISC-V so hard. If ARM's ISA were open, then ARM would have to compete with lots of other companies to create the best core implementations possible or disappear. reply sangnoir 14 hours agorootparentprevWell, I suppose the magic is in crossing your t's and dotting your i's when drafting legal contracts pertaining to said IP. Arm failed to do that. reply refulgentis 19 hours agorootparentprevI'm not sure why it seemed that way to you, server market >> consumer laptops (c.f. $INTL). reply adrian_b 8 hours agorootparentThe server market for Arm-based computers remains negligible. The number of servers with Arm-based CPUs is growing fast, but they are not sold on the free market, they are produced internally by the big cloud operators. Only Ampere Computing sells a few Arm-based CPUs, but they have fewer and fewer customers (i.e. mainly Oracle), after almost every big cloud operator has launched their own server CPUs. So for anyone hoping to enter the market of Arm-based server CPUs the chances of success are extremely small, no matter how good their designs may be. reply eigenform 15 hours agorootparentprevIn the context of ARM machines, it's [historically] been the case that most of the devices are not servers (although that's slowly changing nowadays, which is nice to see!!) reply jauntywundrkind 19 hours agorootparentprevSeemed like ARM was desperate to let Apple and Apple alone make decent CPUs in something besides servers. Having an ok core on mobile or desktop was unacceptable. reply williamDafoe 18 hours agoprevThis lawsuit will cast a cloud of darkness on any startup company that wants to build a new arm chip design! What a terribly stupid thing for ARM to do - they have basically pointed a gun at their own head and pulled the trigger! reply 2809 18 hours agoprevThis went exactly as expected. ARM had no leg to stand on going in. reply kernal 16 hours agoparentArm will appeal. Regardless, Arm will tighten the screws on Qualcomm in their next round of licensing negotiations. reply hajile 15 hours agorootparentThis lawsuit scared away any future startups from using ARM technology if they want the option to be acquired. This in turn means that all the awesome new chip IP is going to be designed for some other ISA and that's almost certainly RISC-V. Meanwhile, Qualcomm's ALA expires in 2033. They will almost certainly have launched RISC-V chips by then specifically because they know their royalties will be going WAY up if they don't make the switch. reply Hilift 6 hours agorootparentARM has been a shambles for years since acquisition by Softbank in 2016. The China subsidiary went rogue and had to be dealt with at a time when they were considering an IPO. This has Oracle Java like lessons learned waiting to be written, sprinkled with Intel-like market position squandered. Or even the methane producing pig farmer in underworld in Mad Max Beyond Thunderdome. reply buckle8017 11 hours agoprevWell yes Qualcomm is the CIA of course they won. reply tiahura 20 hours agoprevMy understanding was that the central issue was whether the Nuvia “modify” license “transferred” to Qualcomm. If so, wouldn’t that be a legal issue to be resolved by MSJ? Why was this tried to a jury? reply phire 20 hours agoparentQualcomm already had a \"modify\" licence, back from when they were doing their own custom ARM cores. So the actual central issue was if Qualcomm had the right to transfer the technology developed under the Nuvia architecture license to the Qualcomm architecture license. reply fidotron 20 hours agorootparentTo be precise, it now appears the dispute has moved as to whether Nuvia had the right to transfer things to Qualcomm. It strikes me as a surprising diversion to this, and I wonder how prepared for this outcome the respective teams were. reply phire 19 hours agorootparentMy memory is that before the lawsuit, Qualcomm were using both arguments, \"we have the right to transfer the licence, and even if we didn't, we have our own license\". The first argument was always the weaker argument, the license explicitly banned licence transfers between companies without explicit prior permission. Qualcomm were arguing that the fact they already had a licence counted as prior permission. But ARM bypassed that argument by just terminating the Nuvia licence, and by the time of the lawsuit qualcomm was down to the second argument. Which was good, it was the much stronger argument. reply bonzini 19 hours agorootparentBut is it even a second company (and thus a license transfer) if Qualcomm bought Nuvia? reply fc417fc802 16 hours agorootparentI was wondering about that too. If I understand correctly, the Arm license includes a clause stipulating that it is invalidated if the company is acquired unless Arm grants permission. reply pests 14 hours agorootparentIt makes sense. You wouldn't want to license something to a startup only for your competitor to buy them out from under you to get an advantage / bypassing talking to you directly. reply monocasa 18 hours agorootparentprevIt could be. Rumor is Intel (or AMD in the same situation) would lose the cross licensed parts of their x86 license if they were acquired. reply philistine 2 hours agorootparentWonderful, another albatross over the head of the x64 architecture. reply jauntywundrkind 19 hours agorootparentprevI keep hearing these kinds of claims but... Qualcomm for years has sworn they didn't transfer the existing Nuvia tech. It's the Nuvia team, and a from scratch implementation. Qualcomm was saying pretty strongly they didn't allow any direct Nuvia IP to be imported and to pollute the new design. They saw this argument coming from day 1 & worked to avoid it. But everytime this case comes up, folks immediately revert to the ARM position that it's Nuvia IP being transfered. This alone is taking ARMs side, and seems not to resemble what Qualcomm tried to do. reply camel-cdr 19 hours agorootparentWhat did Qualcomm get from buying Nuvia it wouldn't have gotten from just hiring all Nuvia employees? Edit: I guess it wouldn't be that simple to do reply pests 14 hours agorootparentAcquihire's are very common. You can negotiate who is coming with you (or force getting the entire team and not just the A players), agreed to pay raises during the transfer, stock or other important roles, etc. Less unsettling for the employees too, having to go through the interview pipeline like a walk-in would. Sometimes its the entire goal. Leave a company to fill a need it can't solve itself with the plan to get bought back in later. Times have changed tho so who knows. reply monocasa 18 hours agorootparentprevI thought that the gen 1 oryon cores were using Nuvia source, but the gen 2 cores were from scratch implementation. reply dismalaf 19 hours agoprevNuvia never even had a finished product. Not sure what ARM thinks their losses are considering Qualcomm already had an architectural license... ARM is (edit) figuratively blowing up their ecosystem for no reason; now everyone will be racing to develop RISC-V just to cut out ARM... reply HKH2 18 hours agoparent> ARM is literally blowing up their ecosystem for no reason With explosives? reply mewse-hn 20 hours agoprev [–] Haven't read deeply into this particular dispute but Arm suing their own customers doesn't seem good for business. Especially since they lost. reply williamDafoe 18 hours agoparent [–] A lot of companies with great IPR have very terrible management, now ARM has joined that club! Pay their license fees religiously for almost 30 years and they turn around and sue you, GEEZ! reply walterbell 16 hours agorootparent [–] Softbank likely played a role in Arm's change of business model, away from IP licensing towards selling complete complete chips. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Qualcomm has emerged victorious in a legal dispute with Arm regarding chip design licensing, specifically related to Qualcomm's acquisition of Nuvia.",
      "The case revolved around whether Qualcomm's existing license covered Nuvia's designs, with Arm asserting that all Arm-compliant CPUs were derivatives of their Instruction Set Architecture (ISA).",
      "The jury's indecision on Nuvia's breach of license favored Qualcomm, raising concerns about Arm's licensing practices and possibly encouraging companies to consider RISC-V, an open-source alternative."
    ],
    "points": 196,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1734730133
  },
  {
    "id": 42479233,
    "title": "Query Apple's FindMy Network with Python",
    "originLink": "https://github.com/malmeloo/FindMy.py",
    "originBody": "FindMy.py The all-in-one library that provides everything you need to query Apple's FindMy network! The current \"Find My-scene\" is quite fragmented, with code being all over the place across multiple repositories, written by several authors. This project aims to unify this scene, providing common building blocks for any application wishing to integrate with the Find My network. Important This project is currently in Alpha. While existing functionality will likely not change much, the API design is subject to change without prior warning. You are encouraged to report any issues you can find on the issue tracker! Features Cross-platform: no Mac needed Fetch and decrypt location reports Official accessories (AirTags, iDevices, etc.) Custom AirTags (OpenHaystack) Apple account sign-in SMS 2FA support Trusted Device 2FA support Scan for nearby FindMy-devices Decode their info, such as public keys and status bytes Import or create your own accessory keys Both async and sync APIs Roadmap Local anisette generation (without server) More information: #2 Installation The package can be installed from PyPi: pip install findmy For usage examples, see the examples directory. Documentation can be found here. Contributing Want to contribute code? That's great! For new features, please open an issue first so we can discuss. This project uses Ruff for linting and formatting. Before opening a pull request, please ensure that your code adheres to these rules. There are pre-commit hooks included to help you with this, which you can set up as follows: pip install poetry poetry install --with dev # this installs pre-commit into your environment pre-commit install After following the above steps, your code will be linted and formatted automatically before committing it. Derivative projects There are several other cool projects based on this library! Some of them have been listed below, make sure to check them out as well. OfflineFindRecovery - Set of scripts to be able to precisely locate your lost MacBook via Apple's Offline Find through Bluetooth Low Energy. SwiftFindMy - Swift port of FindMy.py Credits While I designed the library, the vast majority of actual functionality is made possible by the following wonderful people and organizations: @seemo-lab for OpenHaystack and their research; @JJTech0130 for Pypush, providing the breakthrough necessary for getting this to work without a Mac; @biemster for FindMy, which is the main basis of this project; @Dadoum for pyprovision and anisette-v3-server; @nythepegasus for GrandSlam SMS 2FA; And probably more, so let me know! :D",
    "commentLink": "https://news.ycombinator.com/item?id=42479233",
    "commentBody": "Query Apple's FindMy Network with Python (github.com/malmeloo)184 points by nkko 6 hours agohidepastfavorite90 comments pixelmonkey 3 hours agoThis looks great. If this Python implementation of the FindMy API actually works, it would be a major technology quality-of-life improvement for me. I hope Apple lets it stay alive. Everyone who shares location with me does so over Find My, and my family insists on using AirTags. As a 100% desktop Linux and mobile Android user, it is one of the few things that I always need to remote in to my Mac Mini to access because there are no x-platform FindMy apps and the FindMy iCloud web app does not have feature parity to the macOS and iOS apps. One of a long list of offenses where Apple refuses to make things easy for x-platform friend groups and families. Very annoying. reply nulltxt 1 hour agoparentDoes Blue Bubbles work for this? They have find my built into their app reply bronson 1 hour agorootparentKind of? Right now it feels like it's glued on the side and a good proof of concept. It takes a lot more panning and zooming than it should. But it DOES work, one-way: you can see your friends' locations but they can't see yours. reply fluidcruft 1 hour agoparentprevOne that really annoys me is inability to monitor/control my kid's device useage and time limits. reply amit9gupta 39 minutes agorootparentPossible in the apple ecosystem. If the kids are part of a \"familly\", you can monitor / control using Parental Controls accessed from your iPhone > Settings > Screen Time. Also checkout firewalla https://help.firewalla.com/hc/en-us/articles/360008214094-Ac... reply alwa 31 minutes agorootparentIt sounds like the gp’s objection might be that they’d like for their children to use iPhone and its parental controls, but they themselves would prefer to use other platforms to manage the supervision features. As far as I know, you do in fact have to be within the apple ecosystem to manage the ScreenTime features as you suggest. Firewalla looks really thoughtfully designed! I’m glad to be aware of it. reply fluidcruft 7 minutes agorootparentprevI actually have Firewalla (it's great!). But it's only helpful when a kid's phone is connected to the network itself (which they can escape easily by disconnecting from wifi). The native stuff works on all networks. reply turtlebits 1 hour agorootparentprevThat's a parenting issue. Your kid should have the self control to understand and set limits. In my house, my kids set kitchen timers. reply fluidcruft 57 minutes agorootparentUnless you are arguing that the feature should be removed from iOS and macOS devices, it is entirely a technical issue. Me preferring to use an Android device rather than an iPhone is not a parenting issue. reply phillco 2 hours agoparentprevEven within Apple's platforms, there's pretty limited support for automation -- you can say \"Siri find my keys\" but there's no App Intents / Shortcuts support for automating anything within Find My (AFAIK), which is a bit disappointing. reply pixelmonkey 2 hours agorootparentYes, although I recently discovered Hammerspoon which is a clever little bit of open source macOS desktop automation technology: https://www.hammerspoon.org/ reply UniverseHacker 1 hour agorootparentprevWhat about Apple Automator and Applescript? reply GeekyBear 2 hours agoparentprevWhat information is available through this api that would not already be available over the web? reply pixelmonkey 1 hour agorootparentHopefully locations shared by users not part of my iCloud Family account, and \"Items\" (Apple jargon term for AirTags). Currently it only shows macOS or iOS \"Devices\" directly linked to my iCloud account, or in my iCloud Family, none of the locations shared by friends. And it shows no \"Items,\" not even those in my iCloud account. (... yep, it looks like one of their example programs is about accessing AirTag info via API: https://github.com/malmeloo/FindMy.py/blob/main/examples/rea... ...) reply BeefySwain 2 hours agoparentprevWhat does \"x-platform\" mean in this context? reply pixelmonkey 2 hours agorootparentCross-platform. There are 3 major desktop operating systems (Windows, Linux, and Mac) and 2 major mobile operating systems (iPhone and Android). Every single OS has a huge marketshare worldwide (including Linux, if you count servers). A truly x-platform app is one that works well on all 5 of these platforms, e.g. Signal. A moderately x-platform app is one that works well on the two mobile operating systems and on web as an alternative to desktop, e.g. WhatsApp. A single-platform app, like Apple FindMy, only works properly on e.g. Mac + iPhone. Apple tends to be the only major industry player that produces these sorts of apps, e.g. iMessage, FaceTime, Final Cut Pro, Keynote. Although with Keynote you can often get by with the iCloud web version, which has a useful 80%-or-so of the desktop app's features. Even apps like Meet, Zoom, and Teams -- run by rival companies -- are more x-platform than major Apple apps. reply stavros 1 hour agorootparentI think the GP knows what cross-platform means, but is confused by using \"X\" as shorthand for \"cross\". In my opinion, it's not widespread enough for the four-letter saving to be worth the confusion. reply jpc0 0 minutes agorootparentI didn't directly interpret it a cross-platform but more as (x) platform... Asin variable x which is not Apple... Which is semantically the same I guess but not entirely. Just to add to the different ways that that exact grouping of letters can be interpreted. Maybe because I see an API as being able to be accessed from anywhere, so you could query it from a home automation device to trigger something when you are withing X meters of your house, which even if Apple truly released a cross-platform version of Find My that wouldn't be possible. pixelmonkey 1 hour agorootparentprevThat's a good point, well taken. Especially now that \"X\" is the name of a social media platform :-) reply marzell 1 hour agorootparentLong before the richest man on earth bought Twitter to be his personal megaphone to help him prepare to become president in order to boost all his personal endeavors, the letter X has been used as a sort of contraction to replace common morphemes like \"cross\", \"trans\" etc, in places where the physical representation \"x\" likens to a cross or crossing of some sort, or in reference to the Greek letter Chi. Must we change our use of language to support this guy, too? Xtian Xmas xfer tx/rx xor... reply sshh12 2 hours agorootparentprevCross platform (something that works well outside of Apple apps/devices) reply daft_pink 18 minutes agoprevIt’s interesting, because it could allow you to log location over time. Generally, I can only see peoples location when I open the app, but this would allow me to ping every 30 minutes and create a very long log that I could technically create manually, but would be quite a bit of work. reply zikduruqe 4 hours agoprevYou used to be able to query this data locally from your MacBook, but Apple decided to encrypt it. It was fun to put an AirTag on your cat, then use GPS Visualizer to plot your cat's activities overnight. https://github.com/icepick3000/AirtagAlex https://www.gpsvisualizer.com reply qup 4 hours agoparentWhile we're here, I have an ask (of anyone). I want the same exact thing you said, except for an outdoor dog on a large property. I would like a tag that just records its own GPS coordinates locally on-device, every so often, and then when my dog comes home, I can check where she's been. Does this exist? reply janten 3 hours agorootparentThey are called GPS trackers or GPS loggers. You can find some that save coordinates to a microSD card and optionally send the location via cellular connection for about 10 dollars on AliExpress. reply spike021 42 minutes agorootparentprevMy dog has a Fi GPS/cellular collar. You could just get that and leave it on at night and it would show you where your dog wanders around. reply zikduruqe 3 hours agorootparentprevYou know honestly? I have thought of using a XOSS cycling computer (https://www.amazon.com/XOSS-Speedometer-Accessories-Waterpro...). I have used them before on various bikes and they work just fine. Battery life is about 25 hours, it is weather resistant, and then you can sync them after you record an activity. And at less than $30, if it gets lost, it isn't the worst thing in the world. reply Raed667 4 hours agorootparentprevAny cheap Garmin watch should do the trick reply 1986 3 hours agorootparentAnd if you want to spend more, Garmin even makes a range of dog tracking equipment: https://www.garmin.com/en-US/p/965617 reply cdurth 4 hours agorootparentprevYou could definitely use meshtastic devices to do this reply oulipo 4 hours agoprevI'm also interested by the Haystack project to have an ESP32-based object identify as an AirTag and be able to follow it Does anyone knows if their approach is \"sustainable\", or if Apple can easily \"block out\" such hacks from their network? reply Its_Padar 3 hours agoparentIf it functions exactly as an AirTag does then it would be hard as they would not want to block all previously sold AirTags reply crazygringo 3 hours agorootparentIs there something it can do to whitelist legitimate AirTag serial numbers? reply bhy 3 hours agorootparentI don't think AirTag work that way. AirTag protocol is specifically designed so Apple or other parties will not be able to track users by serial numbers. reply gjsman-1000 3 hours agorootparentWhere there's a will, there's a way. Apple is very clear law enforcement can approach them with any AirTag and they will immediately be able to tie it to a user. reply kolinko 1 hour agorootparentOne doesn’t exclude the other - a physical airtag may have an ID available, but not broadcast it anywhere. Also, “when there’s a will…” doesn’t really apply to cruptography reply stonegray 2 hours agorootparentprevThey do, Airtag hardware need to be signed to add to your iCloud account. But the actual location beacon messages are not linked to your iCloud account and can’t be associated with the sending airtag. reply Havoc 4 hours agoprevWhat are the chance that this keeps working long term? Sounds awesome & makes airtags more appealing, but if apple is just going to shut it down next week then less so reply stonegray 3 hours agoparentChanging the underlying find my network to break this would be challenging if not impossible while keeping the privacy protections in place. Apple can’t identify devices sending data to find my, and doesn’t log requests. Short of changes that would break compatibility with older devices it should be relatively stable. OpenHaystack has been doing this for a few years now and Apple has made no efforts to restrict it. reply alphan0n 2 hours agorootparentI’ve been using FakeTag[0] and OpenHaystack[1] coupled with a vibration sensor to notify me when various things happen around my house. Inspired by this [2] article. It’s worked flawlessly for ~2 years. [0] https://github.com/dakhnod/FakeTag [1] https://github.com/seemoo-lab/openhaystack [2] https://hackaday.com/2022/05/30/check-your-mailbox-using-the... reply gjsman-1000 3 hours agorootparentprev> Apple can’t identify devices sending data to find my, and doesn’t log requests. So what you're saying is that a decent firewall could still inspect the traffic, or the patterns thereof. Also, this doesn't make any sense, as if Apple doesn't know which AirTag belongs to who, Find My would be very useless; and law enforcement would be furious. reply stonegray 2 hours agorootparentAirtags are associated with your apple ID for safety, but when you make a request for the location from Find My it doesn’t include any information about which airtag you’re asking about; just a CSPRNG-incremented public key that changes every 15 minutes. The location data itself is not available to Apple. Here is Apple’s docs on how they prevent themselves from inspecting traffic on Fmi: https://support.apple.com/guide/security/find-my-security-se... reply meindnoch 1 hour agorootparentSo how does Find My work on icloud.com then? reply wutwutwat 2 hours agorootparentprevSo Apple has no way to see anything even when developing the platform itself? They must have a way to decrypt payloads or otherwise get into the system they built and control. The fact that they let law enforcement know when someone is stalking someone with an AirTag shows that the data is available to them. It’s silly to think otherwise, paper or not. reply future10se 1 hour agorootparent> The fact that they let law enforcement know when someone is stalking someone with an AirTag shows that the data is available to them. Not technically correct. Apple devices (and Android phones with the appropriate app) detect if an unknown AirTag is moving with them and makes it home, possibly signalling a stalking attempt. The heuristics for this happen locally; Apple isn't \"aware\" of this happening. That said, when you first set-up an AirTag, the serial is tied to your account. Therefore, when you physically find an unknown AirTag and report it to law enforcement, they can then subpoena (or get a warrant?) Apple for information on the AirTag owner's identity. The serial itself, and any personal identifiers, are not used in the locating process, however. This is well documented in the paper above, in articles, as well as in reverse engineering efforts. reply ttul 1 hour agoparentprevFrom Apple’s perspective, if someone uses the FindMy APIs to provide a commercial service that diminishes the privacy offered by Apple’s official apps, they would likely send a C&D letter. But for hobby projects, it’s not worth clamping down hard. reply leobg 1 hour agoprevCan I use this, if I have an iPhone, to trigger actions on a server based on my location? For example, “When I come home, fetch the latest electricity prices and notify me if I should plug in my Tesla”. I tried that using Shortcuts, but they won’t run location based without confirmation. (There are some workarounds, but they, too, don’t work reliably in my experience.) reply cruffle_duffle 21 minutes agoparentStupid question but couldn’t you just always plug it in and then automate the bit where it actually energizes the outlet? reply mikeweiss 4 hours agoprevAs someone who lives in an Android family but would still like to use air tags since it's the biggest network in the U.S. I'd love a way to add and use air tags without needing to have an iPhone! reply cruffle_duffle 15 minutes agoprevI remember there was a time when “web services” were the new hotness and everybody was gonna offer some API to whatever they had online. What happened to this? We’ve even got the authentication part nailed down now thanks to OAuth! There is even API gateways that you can park in front of your stack that manage all the hard parts like granting client secrets to API consumers and showing registration screens to developers. There is really nothing stopping you from opening up parts of your stack to developers and tinkerers so they can do cool shit. It even gets people to lock into your product that much more because now they’ve integrated some part of their workflow into your system in a way that might not be possible without your service! So yeah. You already have these API’s exposed for your front end apps to use. Why not just slam a developer portal on top and let people access some of it? Who knows what cool things they’ll cook up! reply Galanwe 5 hours agoprevFore those not familiar with the Apple ecosystem, what does \"Find My\" do? locate apple devices ? reply latexr 5 hours agoparenthttps://en.wikipedia.org/wiki/Find_My > Find My is an asset tracking service made by Apple Inc. that enables users to track the location of iOS, iPadOS, macOS, watchOS, visionOS, tvOS devices, AirPods, AirTags, and a number of supported third-party accessories through a connected iCloud account. Users can also show their primary device's geographic location to others, and can view the location of others who choose to share their location. Find My was released alongside iOS 13 on September 19, 2019, merging the functions of the former Find My iPhone (known on Mac computers as Find My Mac) and Find My Friends into a single app. On watchOS, Find My is separated into three different applications: Find Devices, Find People and Find Items. reply kabirgoel 5 hours agoparentprevCorrect. You can also share your location with friends. A lot of friend groups (at least my age) use Find My as a kind of social network. reply Nextgrid 2 hours agorootparentDoes it have any battery impact? I've never tried these always-on location tracking things partly due to (unfounded?) concerns about battery use. reply msh 1 hour agorootparentits not always on in that way. It will report your location when requested, and optionally just before shutting down. reply gomoboo 4 hours agorootparentprevHow does that work woth your friends? Always on access or just occasionally? reply GeekyBear 2 hours agorootparent> Always on access or just occasionally? You have quite a few granular choices. > You can share your current location once, temporarily share your location while you're on the way to an expected destination, or share your ongoing Live Location... for an hour, until the end of the day, or indefinitely. In Messages, you can use Check In to share your location... Your location is shared only if there's an unexpected delay during your trip or activity and you're unresponsive. https://support.apple.com/en-us/105104 reply toomuchtodo 4 hours agorootparentprevAlways on. You can see where your friends are at both in Find My and under their contact photo in your iMessages chat. reply johnisgood 4 hours agorootparentPersonally I do not find the idea comforting that someone (anyone) may know where I am at all times. I would not even trust Apple either. reply proteal 3 hours agorootparentThis is actually one of the big differences between generations. It’s not just the norm for young people to share locations, but rather almost expected, with real social consequences for not. Yes it’s probably a little weird to have someone’s precise location 100% of the time, but since you’re sharing it with me there’s a good deal of trust implied (though this is not always the case as it has become more normalized). However, if we stop sharing locations, that usually implies a divorce of the relationship. People will shut you out of their life if you stop sharing your location with them, no matter the reason. From that lens, the choice is simple. You’ve gotta share your location, even if it’s a bit icky from a privacy perspective or you risk losing an entire cohort of friends. I will admit, there is a strange level of intimacy for having done it. In a world increasingly dominated by the pixels on this 4x8 screen, it is a nice reminder that the text bubbles on my phone actually come from real people that I can show you on a map. (Obviously you can find friends who don’t care for it and you can live a normal life and be just fine. I’m privacy conscious but I still share my location with a handful of friends for the above reasons.) reply sleepybrett 2 hours agorootparentprevyou can control who you share your location with and for how long. I think the options are, just once, for an hour, for the day and forever. reply rvnx 4 hours agorootparentprevIt's a virtual leash for couples. reply toomuchtodo 3 hours agorootparentBlame the emotionally dysfunctional, not the tool. It’s only a problem if it changes how you would live your life or pressured or coerced (in which case, say no). reply haliskerbas 4 hours agorootparentprevAlways on, works as a great way to check in on close friends or have them check in on you (like someone going on a first date) reply incanus77 2 hours agoparentprevI don’t use the person tracking very often except on group vacations, but I track a vehicle with an AirTag after a car theft for a little peace of mind (along with other preventative measures). Every now and then it’s handy for my own devices, too, including alerting me when I’ve accidentally left one behind at a non-routine location. reply cube2222 5 hours agoparentprevImportantly, it works in a peer-to-peer kind of way. Apple devices act as kind of beacons and nearby iPhones can notify Apple servers of any nearby devices they detect (in a way not decryptable by Apple, only by the owner of the devices). So AirTags, MacBooks, and turned-off iPhones are findable via passing-by turned-on iPhones. reply lopkeny12ko 4 hours agorootparentIs it not a glaring privacy and security hole that turned-off devices can still be located? Maybe it's just me, but if I own an internet-connected device and I turn it off, I expect it to be off. That an iPhone's definition of \"off\" means \"you can't use it but other random people's iPhones in the vicinity can still connect to and ping it\" rubs me the wrong way. reply anderiv 4 hours agorootparentThe off-but-still-on functionality can be turned off, and the OS does disclose that by default the device is still findable on the power off screen. reply jen20 4 hours agorootparentprevIt is not. If you don’t want your device to participate, you ca elect not to enable Find My during setup. The vast majority of people would rather a their couldn’t just turn off a stolen phone and render it unlocatable. reply rainsford 3 hours agorootparentAlso the location is only accessible to you, the owner of the device. Not Apple or \"random other people's iPhones\". The engineering and thought that went into the whole thing to be useful but also privacy protecting is actually pretty impressive, and exactly the kind of thing we should be encouraging companies to do if we care about privacy. Especially since as you point out, you can still easily turn it off at any point if you want. reply vasco 4 hours agoparentprevWith the Apple and the Google ecosystems! reply simonw 5 hours agoparentprevA bunch of stuff: - Find your Apple Watch, AirPods, laptop etc - Find family member devices if they've granted you access to do that - Find AirTags - Show you the location of friends who have granted you access reply roger_ 4 hours agoprevHope someone integrates this with Home Assistant soon! reply owenthejumper 2 hours agoprevUsing this as soon as Play sound is integrated! reply delijati 5 hours agoprevCan i add xiaomi \"airtag\" with it? reply pravosleva 1 hour agoprevtest comment reply TekMol 5 hours agoprev [15 more] [flagged] aimazon 4 hours agoparenthttps://localsend.org/ reply TekMol 4 hours agorootparentThat wants me to install a binary. I don't want to run any external code on my hardware. reply __jonas 4 hours agorootparentIf the constraint is that you don't want to install any software, there are a bunch of these web based AirDrop clones, besides the ones mentioned here are two more: https://pairdrop.net/ https://snapdrop.net/ I've tried PairDrop, it works well. reply karanbhangui 5 hours agoparentprevtry https://sharedrop.io/ reply TekMol 4 hours agorootparentThis also looks interesting. My question is similar to the wormhole one. Does it really need to be 168 files of code to do this? I don't know WebRTC, but wouldn't a single PHP file be enough to create the connection between the two devices? reply rafram 4 hours agorootparent> I don't know WebRTC, but wouldn't a single PHP file be enough to create the connection between the two devices? I mean, they could concatenate all 168 files and stuff them into a single tag in the PHP if that would make you happier… reply fiatpandas 4 hours agorootparentprevThis. There’s even a QR code you can scan so you don’t have to type on the mobile device. reply vachina 5 hours agoparentprevhttps://webwormhole.io/ reply TekMol 4 hours agorootparentThat looks like an interesting approach. Does it really need to be a giant application made from \"Go, JavaScript, TypeScript and Python\"? What does all that code do? Wouldn't a single PHP file that acts as the connection between the two devices be enough? reply vachina 4 hours agorootparentWebwormhole needs those to setup a direct p2p connection. Php you need intermediaries. reply briandear 5 hours agoparentprev [–] Dropbox. reply TekMol 4 hours agorootparent [–] Is that a \"Sign up and install our binaries on your devices\" service? I prefer to not install additional software from 3rd parties on my devices. reply gomoboo 4 hours agorootparentWith an iPhone you’d have AirDrop built in. reply sdfasdf234dfsg 4 hours agorootparentprev [–] You'll have to ask the apple overlords to make and install by default the app you want then. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FindMy.py is a comprehensive library designed to query Apple's FindMy network, aiming to consolidate the fragmented \"Find My-scene.\" It is currently in its Alpha stage, meaning its API design may undergo changes.",
      "The library supports cross-platform functionality, location report decryption, Apple account sign-in, and two-factor authentication (2FA), offering both asynchronous (async) and synchronous (sync) APIs.",
      "Installation is available via PyPi using the command `pip install findmy`, and contributions are encouraged, with specific guidelines for code formatting provided."
    ],
    "commentSummary": [
      "A Python implementation of Apple's FindMy API aims to enhance cross-platform access, benefiting users on Linux and Android who currently require a Mac for FindMy features.",
      "This tool could enable tracking of locations and AirTags without dependence on Apple's ecosystem, though there are concerns about Apple's potential intervention to halt such projects.",
      "The discussion highlights Apple's limited cross-platform support and the difficulties in managing device features like parental controls outside its ecosystem, with suggestions for alternatives like Blue Bubbles and Firewalla."
    ],
    "points": 184,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1734783295
  },
  {
    "id": 42478107,
    "title": "The Ugly Truth About Spotify Is Finally Revealed",
    "originLink": "https://www.honest-broker.com/p/the-ugly-truth-about-spotify-is-finally",
    "originBody": "Share this post The Honest Broker The Ugly Truth About Spotify Is Finally Revealed Copy link Facebook Email Notes More Discover more from The Honest Broker A trustworthy guide to music, books, arts, media & culture by Ted Gioia Over 193,000 subscribers Subscribe Continue reading Sign in The Ugly Truth About Spotify Is Finally Revealed A year-long investigation by an indie journalist is a call to action Ted Gioia Dec 19, 2024 1,740 Share this post The Honest Broker The Ugly Truth About Spotify Is Finally Revealed Copy link Facebook Email Notes More 335 348 Share In early 2022, I started noticing something strange in Spotify’s jazz playlists. I listen to jazz every day, and pay close attention to new releases. But these Spotify playlists were filled with artists I’d never heard of before. Who were they? Where did they come from? Did they even exist? In April 2022, I finally felt justified in sharing my concerns with readers. So I published an article here called “The Fake Artists Problem Is Much Worse Than You Realize.” I was careful not to make accusations I couldn’t prove. But I pointed out some puzzling facts. If you want to support my work, please take out a premium subscription (just $6 per month). Subscribe Many of these artists live in Sweden—where Spotify has its headquarters. According to one source, a huge amount of streaming music originates from just 20 people, who operate under 500 different names. Some of them were generating supersized numbers. An obscure Swedish jazz musician got more plays than most of the tracks on Jon Batiste’s We Are—which had just won the Grammy for Album of the Year (not just the best jazz album, but the best album in any genre). How was that even possible? I continued to make inquiries, and brooded over this strange situation. But something even stranger happened a few months later. A listener noticed that he kept hearing the same track over and over on Spotify. But when he checked the name of the song, it was always different. Even worse, these almost identical tracks were attributed to different artists and composers. He created a playlist, and soon had 49 different versions of this song under various names. The titles sounded as if they had come out of a random text generator—almost as if the goal was to make them hard to remember. Trumpet Bumblefig Bumble Mistywill Whomping Clover Qeazpoor Swiftspark Vattio Bud I reported on this odd situation. Others joined in the hunt, and found more versions of the track under still different names. The track itself was boring and non-descript, but it was showing up everywhere on the platform. Around this same time, I started hearing jazz piano playlists on Spotify that disturbed me. Every track sounded like it was played on the same instrument with the exact same touch and tone. Yet the names of the artists were all different. Were these AI generated? Was Spotify doing this to avoid paying royalties to human musicians? Spotify issued a statement in the face of these controversies. But I couldn’t find any denial that they were playing games with playlists in order to boost profits. By total coincidence, Spotify’s profitability started to improve markedly around this time. A few months ago, I spoke with an editor at one of the largest newspapers in the world. I begged him to put together a team of investigative journalists to get to the bottom of this. “You need to send people to Sweden. You need to find sources. You need to find out what’s really going on.” He wasn’t interested in any of that. He just wanted a spicy opinon piece. I declined his invitation to write it. We now finally have the ugly truth on these fake artists—but no thanks to Spotify. Or to that prestigious newspaper whose editor I petitioned. Instead journalist Liz Pelly has conducted an in-depth investigation, and published her findings in Harper’s—they are part of her forthcoming book Mood Machine: The Rise of Spotify and the Costs of the Perfect Playlist. Mood Machine will show up in bookstores in January and may finally wake up the music industry to the dangers it faces. Pelly started by knocking on the doors of these mysterious viral artists in Sweden. Guess what? Nobody wanted to talk. At least not at first. But Pelly kept pursuing this story for a year. She convinced former employees to reveal what they knew. She got her hands on internal documents. She read Slack messages from the company. And she slowly put the pieces together. Now she writes: What I uncovered was an elaborate internal program. Spotify, I discovered, not only has partnerships with a web of production companies, which, as one former employee put it, provide Spotify with “music we benefited from financially,” but also a team of employees working to seed these tracks on playlists across the platform. In doing so, they are effectively working to grow the percentage of total streams of music that is cheaper for the platform. In other words, Spotify has gone to war against musicians and record labels. At Spotify they call this the “Perfect Fit Content” (PFC) program. Musicians who provide PFC tracks “must often give up control of certain royalty rights that, if a track becomes popular, could be highly lucrative.” Spotify apparently targeted genres where they could promote passive consumption. They identified situations in which listeners use playlists for background music. That’s why I noticed the fake artists problem first in my jazz listening. According to Pelly, the focal points of PFC were “ambient, classical, electronic, jazz, and lo-fi beats.” When some employees expressed concerns about this, Spotify managers replied (according to Pelly’s sources) that “listeners wouldn’t know the difference.” They called it payola in the 1950s. The public learned that radio deejays picked songs for airplay based on cash kickbacks, not musical merit. Music fans got angry and demanded action. In 1959, both the US Senate and House launched investigations. Famous deejay Alan Freed got fired from WABC after refusing to sign a statement claiming that he had never taken bribes. They called it Payola, and people got fired Transactions nowadays are handled more delicately—and seemingly in full compliance with the laws. Nobody gives Spotify execs an envelope filled with cash. But this is better than payola: On February 7, Spotify’s CEO sold 250K shares for $57.5 million. On April 24, Spotify’s CEO sold 400K shares for $118.8 million. On November 15, Spotify’s CEO sold 75K shares for $35.8 million. On November 20, Spotify’s CEO sold 75K shares for $34.8 million. On November 26, Spotify’s CEO sold 75K shares for $36.1 million. On December 4, Spotify’s CEO sold 75K shares for $37 million. On December 11, Spotify’s CEO sold 60K shares for $28.3 million. Deejay Alan Freed couldn’t dream of such riches. In fact, nobody in the history of music has made more money than the CEO of Spotify. Taylor Swift doesn’t earn that much. Even after fifty years of concertizing, Paul McCartney and Mick Jagger can’t match this kind of wealth. At this point, I need to complain about the stupid major record labels who have empowered and supported Spotify during its long history. At some junctures, they have even been shareholders. I’ve warned repeatedly that this is a huge mistake. Spotify is their adversary, not their partner. The longer they avoid admitting this to themselves, the worse things will get. The music media isn’t much better—these new revelations came from a freelancer publishing in Harper’s, not from Rolling Stone or Billboard or Variety. And I could say the same for the New York Times and Wall Street Journal and Washington Post. Why didn’t they investigate this? Why don’t they care? But I am grateful for independent journalism, which is now my main hope for the future. Let’s turn to the bigger question: What do we do about this? By all means, let’s name and shame the perpetrators. But we need more than that. Congress should investigate ethical violations at music streaming businesses—just like they did with payola. Laws must be passed requiring full transparency. Even better, let’s prevent huge streaming platforms from promoting songs based on financial incentives. I don’t do that as a critic. People sometimes try to offer me money for coverage, and I tell them off. It happened again this week, and I got upset. No honest person could take those payoffs. Streaming platforms ought to have similar standards. And if they won’t do it voluntarily, legislators and courts should force their hand. And let me express a futile wish that the major record labels will find a spine. They need to create an alternative—even if it requires an antitrust exemption from Congress (much like major league sports). Our single best hope is a cooperative streaming platform owned by labels and musicians. Let’s reclaim music from the technocrats. They have not proven themselves worthy of our trust. If the music industry ‘leaders’ haven’t figured that out by now—especially after the latest revelations—we are in bad shape indeed. Subscribe to The Honest Broker By Ted Gioia · Thousands of paid subscribers A trustworthy guide to music, books, arts, media & culture by Ted Gioia Subscribe Error 1,740 Share this post The Honest Broker The Ugly Truth About Spotify Is Finally Revealed Copy link Facebook Email Notes More 335 348 Share",
    "commentLink": "https://news.ycombinator.com/item?id=42478107",
    "commentBody": "The Ugly Truth About Spotify Is Finally Revealed (honest-broker.com)173 points by LordAtlas 11 hours agohidepastfavorite216 comments noelwelsh 10 hours agoI listened to a few ambient playlists on Spotify and Youtube and they were just slop. Even when I was doing something else (e.g. programming) I became annoyed that the background music was so bad. Same with the lo-fi beats channel that is so popular. I'm not sure there is a problem if a proportion of the listeners don't recognize they are listening to slop. I do, however, think its a problem if Spotify is giving preferential treatment to slop, as is claimed in this post. I also would prefer a system that better supported musicians, while having the ease of use of Spotify. reply DyslexicAtheist 10 hours agoparent> a proportion of the listeners don't recognize they are listening to slop this is perhaps the crux of the matter! Very few people in my social circle whose taste in music I share. The majority of them don't even have a \"taste\" and content with whatever their car-radio pumps out. Does that drive me mad? Mildly. Appreciating and recognizing great music is a deeply personal experience. Like appreciating different culinary tastes that requires training and exposure (ideally from a young age), it's also hard work (the older and more settled our tastes get). reply anal_reactor 6 hours agorootparentMost people are stupid. What an unexpected discovery! Considering the fact that the crux of the problem is that people love slop and always have, does the fact that slop has become AI-generated change anything? Different topic: any recommended ways to search for new music? I usually just wait to somehow stumble upon something. reply jimmydddd 4 hours agorootparentI think \"stupid\" may be overstating it a bit. I think it's more that people only have a limited mental bandwidth and can only focus on a limited amount of categories of life. Person A may think person B is stupid for not caring about the quality of background music. But person B might think person A is stupid for ordering a $5 glass of the house wine at a restaurant instead of some special French vintage. And person C might be aghast that that both A and B have standard factory tires on their cars instead of high performance low-profile racing tires. reply tessellated 4 hours agorootparentprevI love reading up on my fav musicians and have a listen at what their influences are and/or other fans recommend. Or, in 2012, I torrented the then whole discog of a British duo called Autechre that I liked a track from on the soundtrack of the film 'Pi'. 12 years later, I have several vinyls, bought every release on multiple [streaming] services/shops, have been to one concert in '16... still cannot get enough and listen to their new releases all the time. Funny to think about it this way, how a handful of beats in a movie can open you up to a world unseen, just gotta follow the tracks... reply shashasha4 2 hours agorootparentprev> Different topic: any recommended ways to search for new music? I usually just wait to somehow stumble upon something. https://www.nts.live/ That one is my favorite, but there are lots of similar online radio stations out there that post track-lists of the sets. I use them for music discovery and I like it a lot. I find shows by people I like, listen regularly, when I hear things that jump out at me I'll look at the track-list and make notes, go to discogs, bandcamp, find other artists on the same label that released the thing I went searching for. So semi passively listening to a dj set and taking occasional notes leads to hours of exploration and frequent discoveries. reply jochem9 4 hours agorootparentprevI listen to radio quite a lot. There are commercial-free stations that play whatever they believe is good music. My favourites are: - Concertzender and all its theme channels. The main channel has a lot of classical during the day, but jazz, world music and electronic in the evening/weekend. I got deep into folk music thanks to their music. On Sunday evening there is even two hours with techno and adjecent music. Discovered a lot through that as well. The station is Dutch, but talk is minimal. - FIP. A bit more main stream music, but also unexpected songs and generally very enjoyable programming. It's in French, but talk is usually minimal as well. reply smessina 4 hours agorootparentprev> any recommended ways to search for new music? Tracking down people involved in an album is a good one for me. Find the producer of a favorite tune and search out more of their work, or check if the bassist plays in other bands or has a solo project. Lots of fun and sometimes unexpected results. reply blipvert 10 hours agoparentprevGive Pye Corner Audio, Craven Faults and Warrington Runcorn New Town Development Plan a go reply wgx 10 hours agorootparentAdd to that: Floating Points, Rival Consoles, Brian Eno… reply iamacyborg 9 hours agorootparentprevSee also Lustmord, Detritus, Rafael Anton Irisarri, Poppy Ackroyd, Moon Ate the Dark reply iamacyborg 9 hours agorootparentJust because I know RAI has talked about the crappyness of Spotify for artists for a number of years, here’s a snippet from a 6 month old interview that’s relevant to the original conversation. https://igloomag.com/profiles/five-questions-with-rafael-ant... > In this day and age, it is incredibly challenging to start a label. Streaming has massively depleted the revenue potential for artists and labels, and even those with a good following are struggling. For those starting out, it’s an uphill battle to build an audience and be successful. > Only a small percentage of songs make a decent amount of money with streaming, so recording budgets for independent artists are next to none. I remember when an album had the support of a team of people devoted to realizing the artist’s vision. Something magical happens when you get that collaboration of people working together to achieve the same goal. Nowadays the artist/producer does the recording, the mixing, and in some cases, even the mastering. Consequently, the production quality has declined drastically over the last decade, and it’s not helped by how most people consume music (illegally downloading it, YouTube lo-fi, or paid streaming subscriptions) but rather contributes to the decline. > For those who remember the world before streaming took over, this is a very different one in which we now live. Music is no longer the product being marketed or sold, instead, it’s the listener’s data. The artist is a “brand” and they create “content.” We are in a time where we are constantly bombarded with “content” on social media and streaming platforms. Take the so-called “Spotify Ambient.” This sound has become watered down and generic, thanks partly to all the Muzak clearing houses creating material for most of the “sleep” and “stress relief” style of playlists. > Whenever an actual artist shows up on those playlists, let’s say the great Otto A. Totland for example, you can immediately hear the difference between his music and Muzak. Otto writes music that conveys rich and nuanced emotions with many layers of meaning. His music wasn’t solely written for the sake of playlisting it. His music was written because it genuinely meant something special to the composer. Muzak clearing houses have been discussed ad nauseam in many industry exposés about “Spotify fake artist” trends with companies like Epidemic Sound and their catalog of ambient rip-offs. > Some companies exist solely to create content for playlists. Think about that for a moment! Composer staff are paid to make royalty-free clones of well-established artists. The companies see which songs are performing well on Spotify-curated playlists, and the clearing house creates a similar-sounding song so Spotify can remove the original from their curated playlist without paying royalties. This practice exists in advertising, when agencies create music for an ad “inspired by” (i.e. ripping off) the music they don’t want to pay the licensing fee for. Somehow, this is legal. You end up with “artists” like Jonci (yes, that’s Jónsi with a “C”) who have millions of plays, sounding exactly like Sigur Rós, and you have zero information about the artist. > Clearing houses can now easily create thousands of meaningless ambient-adjacent tracks to be put on playlists with the unsuspecting consumer having no idea. AI technology makes this all even easier. The lack of credits on streaming platforms is another massive problem, particularly for music workers like myself who depend on album credits/credentials to generate business and get gigs as engineers, producers, etc. The music is stripped of production, artwork, recording, mixing, and mastering credits. I could go on, but the point is that there are many challenges for artists, labels, mastering engineers, and anyone making music now. reply Fluorescence 2 hours agorootparentInteresting. Counter-intuitively, I think slop might have it's place in cultural development. There is always genre-death by exploitative cash-in slop and vomiting it out by machine might be an accelerant. When you start getting that visceral deja-vu dilution ick from slop or just a scene getting derivative and samey, that's what prompts head-snapping invention of brand new sounds that seek clear-blue water between themselves and the slop. reply cogogo 9 hours agoparentprevTo me the really gross bit is the dishonesty around the slop. Same track with multiple different names appearing under multiple different artist names is egregious. Classic corporate disregard for customers. reply surgical_fire 9 hours agorootparentHow is it harmful to customers? You can listen to whatever you want on Spotify. It containing shit music that you will never listen to harms you in which way? If people are genuinely listening to those tracks and are alright with it, how exactly are they harmed? I already consider most contemporary pop music to be slop anyway, and people seem to be fine listening to it. reply rocqua 7 hours agorootparentSpotify has automated 'next song' and playlist creation. Those are (or used to be) valuable for finding new music, and for getting out of a rut. But now spotify isn't selecting what goes into the playlist based on their best guess of what people would like. Instead they put songs there based on what earns them money. reply surgical_fire 7 hours agorootparentDoes this have to be actively enabled by the user? I never really had this \"next song\" thing play for me, nor I ever looked at any playlists Spotify generated (I always presumed they would be bad). Then again, if people are using these features and they are fine with the results, I fail to see the issue. reply Gud 2 hours agorootparentYou can’t understand why someone would be unhappy with a working product turns into crap, just because others don’t notice, cars or even know? I’ve used Spotify since it was a closed beta and their library was from the pirate bay. reply rocqua 3 hours agorootparentprevYes, though I think smart shuffle might be a default mode for playlists. People who enable these functions will expect spotify to pick the best guess of what music they would enjoy. Not what music costs spotify the least to play. If this is only a little bit worse, people will likely not notice. But they will miss out on new music. Certainly this is how I normal find new music. reply surgical_fire 3 hours agorootparentI might have disabled this functionality many years ago. I don't really remember ever having this smart shuffle thing where Spotify would add their own suggestions to my playlist. You see, I don't like Spotify. I don't trust any of those huge platforms, I expect them to actively work against my interests. I always disable or avoid anything they want to suggest me, and view it with suspicion, if now outright contempt. More people should act the same way. reply ribadeo 8 hours agorootparentprevPayola scandal said that the mere fact the platform schemes to prioritize on-platform cheap dreck under multiple names says they are watering down product in a cheating fashion. Adulterated saffron got you burned on a pile of your own fake saffron in parts of medieval Europe. Such a concept as fraudulent activity exists and will not disappear with mere hand wavery. reply noelwelsh 8 hours agorootparentprevIt's unfair in that allowing multiple instances of the same song will almost surely lead to that song receiving more plays than it otherwise would due to how recommendation algorithms work. I don't think this is the most egregious act of dishonesty in the article, though. I think Spotify deliberately seeding this music into its playlists is a worse abuse of its position as owner of the marketplace. reply surgical_fire 8 hours agorootparent> due to how recommendation algorithms work. You have to actively accept the recommendations from the algorithm. If the algorithm ia recommending slop, and you are fine with it, actively listening to it, who was harmed? > I think Spotify deliberately seeding this music into its playlists is a worse abuse of its position as owner of the marketplace. Nobody is forced to listen to the slop playlists generated by Spotify. Spotify puts them in the front page, but they can be ignored. If people are listening to those playlists and are satisfied with it, what is the issue? If the consumption is genuine and not bot activity, I fail to see any wrongdoing on Spotify. If people like listening to slop, who am I to judge? reply noelwelsh 5 hours agorootparentConsumers' time is wasted by slop (by listening to it, skipping it, expunging it from playlists, etc.) Musicians are harmed when their material is lost in a sea of slop. reply surgical_fire 5 hours agorootparentIf they skip or remove from playlists, this would not be a problem would it? These songs would not have a meaningful volume of listeners. Is consumers are actually listening to is, I fail to see harm. Musicians are harmed only in the sense that consumers don't mind listening to slop instead of listening to their music. reply Brian_K_White 4 hours agorootparentWhy do people try to excuse this shit? Somebody works at Spotify, or is a hopeful Spotify wannabe themselves. reply surgical_fire 3 hours agorootparentNeither. I have no desire to work for Spotify. Never worked there myself. I do pay for their service. I find it okay, perhaps a little expensive. You may think I am excusing Spotify, but I am not. At least not really. I am just completely unsurprised that people listen to bottom of the barrel musical slop as background noise and are okay with it. Spotify was smart that they figured out to give people what they wanted in a way that costs them less. But people in this thread act as if Spotify is committing a crime, using some very charged language. My approach is from another angle - if users are legitimately listening to crap, and they are alright with the crap they listened to, why are we making a fuss about it? reply shit_game 9 hours agoparentprev>I'm not sure there is a problem if a proportion of the listeners don't recognize they are listening to slop. I do, however, think its a problem if Spotify is giving preferential treatment to slop, as is claimed in this post. I also would prefer a system that better supported musicians, while having the ease of use of Spotify. This is not only a problem for Spotify, but for every platform on the internet that publishes content, particularly for social media. Most people don't actively discern whether something they observe is slop or not, and it's a huge problem concerning the autheticity of information and the consequences of it. This issue began with text content back in the 90's when internet spam boomed after Eternal September and the reachable market audience for it boomed, but has been slowly evolving until recently until the \"passable\" capability of artificial content generation has both increased exponentially and become finacially feasible for bad actors. There have been reports of Spotify being gamed by gangs in Sweden (and iirc abroad) to monetize artificial engagement[0], which I found tied in very closely to some of the amateur research I've done regarding bots on Reddit. Before the public availability of LLMs and other generative AI, most content that was \"monetizable\" was mostly direct engagements (views, likes, followers, reposts, shotgunning ads, etc.); this resulted in bot farms all over the internet that focused on providing services that gave exactly these results. On Reddit specifically, many of the more sophisticated bot networks and manipulators would feature content that was scraped from other sources like Youtube, Twitter, Quora, and others (including Reddit itself) and simply consist of reposting content. Some of the more novel agents would use markov generators to get around bot detection tools (both first-party and third party), but they would often create nonsense content that was easily discernable as being such. After generative AI took off toward the late end of covid, these bot farms and nefarious agents capitalized on generative AI instantly and heavily. This is particularly known as an issue on Facebook with the \"like this image because this AI generater \"person\" lives in a gutter and has a birthday cake and is missing all their 7 limbs\" pictures, but the text content they can produce is insidiously everywhere on sites like Reddit and Quora and Twitter. Some small subset of these agents are either poorly made or buggy and have exposed their prompts directly, which is rather embarrassing, but others are incredibly sophisticated and have been used in campapigns that reach far beyond just gaming outreach on platforms - many of these bot farms are also now being used for political disinformation and social engineering compaigns, and to great effect. Witnessing the effects of these agents in such mundane places as a music playlist is a dismal annoyance, but learning that they are in fact being used to alter public opinion and policy on top of culture and the arts is disturbing. Many people have scorned large production studios such as A24 for their us of generative AI[1], but not being able to trust even assumedly-mundane content anywhere online is something that most people, and especially the the average consumer, is not prepared for. People who are genuinely interested in anything are going to soon recognize that there is a market for gatekeeping content, but they are not going to want to participate in it because the barrier for entry is going to be completely different from that of the classic internet that we have all generally come to accept and build upon. [0]: https://www.stereogum.com/2235272/swedish-gangs-are-reported... [1]: https://petapixel.com/2024/04/23/a24-criticized-for-using-ai... reply UniverseHacker 10 hours agoprevI don’t get the problem here. You can listen to whatever you want on Spotify. I listen to it everyday, already knowing what I want to listen to, and never encountered this. However, it sounds like they are paying real musicians to create music directly for Spotify, to bypass record labels. That sounds like a good thing all around- record labels don’t seem to do anything useful. reply darkwater 9 hours agoparentThe problem is: Step 1: create a music service and get on-board as many musicians as possible with attractive rates and the promise of a future with big audiences and less intermediaries Step 2: the business grows because you have many famous/good artists in your catalogue Step 3: Congrats, you are now the #1 musical service in the world! Many people associate \"music streaming\" with your brand. You get a lot of \"musical normies\" as customers in the process. Step 4: start lowering royalties you give to musicians. They will stay anyway because you are the #1 brand and bring listeners Step 5: add your own music to places where normies just listen to music, without caring who the hell the author is. So, you have to pay less revenue to the rest of artists, because they are now getting less listenings, even if the royalty per listening stays exactly the same. reply UniverseHacker 9 hours agorootparentI get that is what they are doing, I still don’t see the problem. Hopefully they will gradually cut out the record labels entirely, and pay the artists the full cut directly that is currently going mostly to the record label. People can listen to whatever they want- artists that are not being listened to aren’t being stolen from if the money is instead going to other artists that are being listened to. If people don’t care who the artist is but like what they hear, that seems great. reply rocqua 7 hours agorootparentThe problem is that their automatic song selection now chooses songs based on what earns them money. Rather than their best guess of what people want to hear. This isn't disintermediating the record labels. It's taking audiences from real artists, and placing it with artists that have given up their rights to royalties. That isn't disintermediating recordlabels. That is just serving customers worse music. Heck, it is disintermediating artists from their audience. reply s6af7ygt 7 hours agorootparent> That is just serving customers worse music. There is no music monopoly and customers have a choice in which music they listen to and what service they use (Spotify, Apple, Google/Youtube, Amazon, Bandcamp, buying CDs). Surely if users get served worse music, they will dislike it, and move to another service or another way to listen to music. reply rocqua 4 hours agorootparentHow big (and noticeable) does the reduction in quality need to be. Especially because spotify is such an easy platform. Changing to another streaming service requires learning a new app, moving over all playlists, and likely means still a pretty bad music suggestion service. Because spotify has a pretty good recommendation engine. reply mingus88 7 hours agorootparentprev> Hopefully they will gradually cut out the record labels entirely, and pay the artists the full cut directly that is currently going mostly to the record label Ok stop it. This is too much. reply darkwater 9 hours agorootparentprevThe dominant platform might eventually cut out the labels and afterwards earn their margins. That's why (I assume, I'm not in the music biz) labels stay relevant for now, because they act or sell themselves to artists as some kind of union to protect them from draconian streaming services. But I might be totally wrong on this. reply UniverseHacker 8 hours agorootparentNot too long ago music fans were pretty much all agreed the record labels were mostly rent seekers stealing from artists. It seems wild to me that people are now angry that music streaming services are disrupting that and paying artists directly based on actual listens. reply throw646577 9 hours agorootparentprevAll labels are not the same. Some really do function like unions: groups of artists sharing the functional/procedural/admin burden that comes with keeping their publishing as Saint Francis of Zappa advocated, or paying someone who cares (perhaps a small indie record shop owner) to do that for them as a side gig. Some are little more than asset classes for private equity at this point. reply loupol 9 hours agorootparentprevYou've outlined the steps but not named the process outright : This is basically establishing a dominant platform and then abusing the dominant position that comes with it. reply Dylan16807 9 hours agoparentprev\"these almost identical tracks were attributed to different artists and composers\" doesn't sound like real musicians making real music to me. And honestly I don't want anyone making cheaper music directly for spotify. I want an even playing field where everyone gets the same rates. (But preferably with a way to kick out spammers.) reply lotsofpulp 9 hours agorootparent> I want an even playing field where everyone gets the same rates. The owners of Michael Jackson and Taylor Swift’s music are not going to agree to this deal. reply sandworm101 9 hours agorootparentprev>> I want an even playing field where everyone gets the same rates. So no freedom for artists that want to market themselves with higher or lower rates? Should the Taylor Swifts of the world not be free to negotiate higher rates? Should not the upstart be able to accept a lower rate in hopes that doing so will get their music more recommended/played by the service? reply Dylan16807 1 hour agorootparentFor a radio-type service, I don't think anyone should be able to negotiate higher rates. I think there should be mandatory licensing. For negotiating a lower rate... I wouldn't try to make it illegal, but I still don't like it. I have no particular reason to think it will be an upstart or an entity that you could say 'deserves' more attention. And taking less money for more exposure doesn't sound like something I want to encourage. reply iamacyborg 8 hours agorootparentprevThis is further complicated by how Spotify actually divvies up payments. If they did it proportionally per subscriber you’d suddenly find money being distributed in a much fairer way. Instead they divvy it up on a global aggregate basis so even if I were to pay the subscription and just listen to indie artists, my money goes towards pop megastars and artificial slop. reply surgical_fire 8 hours agorootparentThis is the actual problem, and something that should be properly regulated. I would hope that the money I paid, after the Spotify cut, went to the artists/labels that own the music I actually listen to, proportionally. The fact that it goes to some Taylor Swift or Lady Gaga annoys me more than Spotify adding their own slop to slop playlists. reply lotsofpulp 3 hours agorootparentThe proper regulation is copyright expiring after 10 years. reply iamacyborg 2 hours agorootparentI don't see how that does anything but fuck over artists and allow corporations to exploit them even further. reply lotsofpulp 1 hour agorootparent“Artists” are not a monolith. For example, music streaming services have to spend most of their money on artists that performed many decades ago, otherwise no one will buy it. This reduces spend available for new artists. Excessive copyright terms go from incentivizing creating art to incentivizing rent seeking. Society also spends a ton more resources litigating disputes and avoiding them. reply iamacyborg 1 hour agorootparentThis seems like an incredible naive way of thinking about how artists should be rewarded for their work. > music streaming services have to spend most of their money on artists that performed many decades ago, otherwise no one will buy it Music streaming services should spend their money on artists their customers are listening to, however old that music happens to be. reply Dylan16807 1 hour agorootparent> however old Eh, I don't think there's a very convincing argument that the world is better off paying artists for things that were made more than 50 years ago. (Yes I know the person above said 10, I think 10 is too aggressive.) reply iamacyborg 57 minutes agorootparentProbably depends on whether or not they’re still alive after whatever arbitrary time period and some other factors, sure reply surgical_fire 3 hours agorootparentprevCorrect. reply kazinator 9 hours agorootparentprevWouldn't the Taylor Swifts of the world get more money at the same rate, due to being streamed more? The unknown upstart's stuff is not being searched for and played. Is that not already the lower rate that they accept? Does the revenue also have to be lower from those few plays they got? reply lotsofpulp 8 hours agorootparentTheoretically, it does not have to be. Practically, the big names are not going to accept less than the maximum they can get. Just like almost all people would if they were negotiating multiple equivalent job offers. reply james-bcn 9 hours agoparentprevIf I have a playlist and I get Spotify to add to it, I expect it to add things that people who like the things that I like also like. I do not expect it to add whatever is most profitable for Spotify. Especially since I pay for the service. reply rented_mule 5 hours agorootparentHow bad this is depends on the time-horizon over which they are trying to maximize profit. If the time-horizon is the next listen, it's terrible, as it doesn't care if it revolts the user so badly that they leave the platform immediately. This is bad for the platform, too, so they won't do it for long. If it's very long term, then it's not as bad because it takes into account users listening more often and over a longer period of time. One way of getting additional longer term listens is including signals like tastes of similar users and other things many of us music-lovers would consider \"good\". And the profit part means including signals like cost-per-stream of each song. A very long time-horizon requires finding good tradeoffs between all these signals. My guess is the time-horizon is monthly to quarterly. This is because of market pressures (public markets demand quarterly results) and the practicality of measuring the results. You can't measure/improve infinite time-horizon optimization. This will still have the effect of selecting for users that aren't as sensitive to what they are hearing. They will not leave as quickly. These are the users the platform wants, as these users afford the platform more leeway in shifting the tradeoff from quality signals to cost-per-stream. Eventually, this will push out many of us music-lovers and the artists we love. Once this reaches a tipping point, there will be room in the market for another service to cater to music-lovers. But, given market pressures, they will almost certainly walk the same path as Spotify in the long term. For me, the implication is to own my own music. I always have and always will. I only use streaming platforms for discovery. But as soon as I decide I like something enough, I buy it. I almost always buy albums, not individual songs. That lets me see if I want to follow that artist more deeply. This is expensive and time-consuming relative to streaming. It's dirt cheap relative to what I get out of music. reply surgical_fire 8 hours agorootparentprevI have a few playlists on Spotify. I never had it add any track to it. Been using it for years. Does it actually happen? reply rocqua 7 hours agorootparentYou can enable it. Either through 'smart shuffle' or by looking at the suggested added songs. There is also 'song radio', discover weekly, and automated playlist based on mood or Genre. I presume those playlists based on mood and Genre is what these songs have been added to. reply tmountain 9 hours agoparentprevIf they are paying musicians to create what is essentially “filler” and using this filler to lower their per song streaming costs, they’re simultaneously denying real artists, producing real music, an opportunity to appear in the stream. For listeners that care, this represents a reduction in the quality of their content, and aligns the platform towards pure profit, as opposed to being an ecosystem where working artists can try and carve out a living. reply surgical_fire 9 hours agorootparentThat only works if people, actual users, actively choose to listen to filler. If they do, what is the issue? reply rocqua 7 hours agorootparentIf the supermarket slowly adds more fat to their products, people will probably continue eating without noticing. But calling that an active decision is quite a reach. Similarly, this isn't an active choice of users. And the harm isn't that they are listening to filler, it's that during their random listening, they aren't coming across any new good music. They are prevented from widening their horizons. They aren't actively choosing this. Spotify deceives them into thinking this is music chosen because it is likely the users will like this music. reply surgical_fire 7 hours agorootparent> If the supermarket slowly adds more fat to their products, people will probably continue eating without noticing. But calling that an active decision is quite a reach. What I buy in the supermarket is very much an active decision. I pay a premium on the bottles of milk because the ones I pick taste better than the supermarket branded ones. I do buy some things that are supermarket branded when I perceive the quality to be similar (or sometimes even better). This might be different from consumers that due to budget limitations have to buy the cheapest things available, which is why food regulations are very important. > And the harm isn't that they are listening to filler, it's that during their random listening, they aren't coming across any new good music. They are prevented from widening their horizons. I presume that people that are actively listening to filler are not appreciating music in their cozy living room while sipping on a glass of fine whiskey. They just want background noise while they drive, work, study. Something to make their hours more tolerable, or to improve their focus. Apparently the issue is that generic slop from Spotify serves that purpose just as well. > They aren't actively choosing this. If the music was bad for them, they would stop listening to it. They don't. reply rocqua 4 hours agorootparent> I presume that people that are actively listening to filler are not appreciating music in their cozy living room while sipping on a glass of fine whiskey. That feels like a false dichotomy. Certainly for myself I can put on a generated playlist hoping to find new stuff I like, whilst still putting on the music as background. As filler. If spotify has been serving me slop, then they have likely reduced the amount of new music I have added to my collection of things I like. That harms me, and it harms real artists whose music I might like. In other words, I don't always actively choose the music I listen to. But serving me slop when I don't harms me. reply surgical_fire 3 hours agorootparentIf you are listening to bullshit playlists Spotify generated, you are the problem. You are actively consuming turds, and now you are complaining that eating feces is bad for your health. Instead of blaming the company that provided the turds, you should perhaps reconsider your habit of eating feces. reply rocqua 2 hours agorootparentWell measured and empathetic arguments. reply surgical_fire 2 hours agorootparentThe language may have been harsh, but the message is not. Why do you willingly give a company such as Spotify power over what music you listen to? There are better ways to discover new music if that's what you are after. Try finding communities of people with similar tastes, sometimes they share their own curated playlists. reply rocqua 7 minutes agorootparentI don't not care about music, but I don't want to put in effort. I can live my life without music, but I prefer having good music in my life. Spotify suits my needs perfectly. Even if they do this fake music, it still works well enough. I doubt my half apathy is uncommon. The assumption of prevalence of this attitude is why this change by spotify is so damaging. It will harm my experience a little, and harms artists quite a bit. UniverseHacker 8 hours agorootparentprevExactly- It seems pretentious to say it’s “filler” or not real music if people are happily listening to it, on a platform that provides any music you ask for. reply surgical_fire 8 hours agorootparentMaybe my stance is derived from the fact that I consider many of what is extremely successful in anything to be shit anyway. Music, movies, vídeogames, you name it. People are fine consuming shit, they always were. Why would I be surprised that they are fine consuming shit music that Spotify add to slop playlists with the purpose of increasing their own profits? This is just a logical consequence of how things always were. reply throw646577 9 hours agorootparentprevExactly this. Musicians do not have a choice but to engage with Spotify, but any hope that the discovery systems it offers will occasionally playlist their work in return is being cynically -- and secretly -- diluted with slop. reply Propelloni 10 hours agoparentprevSame here. I don't use Spotify as a radio (I have radio for that) but as a very big record collection. I listen to records or my own playlists. You can switch off the auto-play feature in the settings, so that Spotify stops after the record or playlist ends. I learn about new music the old school way, opening and supporting acts at concerts and recommendations by friends. Sometimes I read a music magazine. reply surgical_fire 9 hours agorootparentMaybe I am old, but I am surprised people use Spotify any other way. I think I never even clicked on anything on the app front page. I only ever go to library or playlist tab. reply jstummbillig 9 hours agorootparentIf you never click on anything of a front page, that's a good tell that you are exactly not the average user. reply surgical_fire 9 hours agorootparentIf the average user is alright with consuming whatever Spotify puts on the app frontpage, I fail to see what the issue is. They are being given what they are promised. reply UniverseHacker 9 hours agorootparentprevThis is also how I use it, but I am not even sure exactly how one would use it any other way. The whole UI is organized around “Your Library” which is a list of artists you like. reply noelwelsh 10 hours agoparentprevIt's a problem if Spotify is giving preference to its lower cost slop over other artists. If Spotify is going to be a marketplace for music it has to be a fair one. reply whoitwas 9 hours agoparentprevI'm not a Spotify user, but people are really wild about it. The process is deceptive as Spotify has tried to hide this and pretend they serve music from \"real\" artists. It's enshitification. It used to be better, now Spotify is trying to squeeze out as much profit as possible. reply lotsofpulp 9 hours agorootparent> now Spotify is trying to squeeze out as much profit as possible. Not the worst idea for a business that has lost money every year for 20 years: https://www.macrotrends.net/stocks/charts/SPOT/spotify-techn... reply whoitwas 8 hours agorootparentThey should try to profit. People seem to take issue with deception. They should just label the spotify songs as what they are and stop trying to trick people. It's weird and abusive. reply mingus88 7 hours agoparentprevIf you don’t actually respect the artist who is getting cloned and replaced by lazy/AI versions of their work without compensation, sure. No problem with this at all. This enshittification of the streaming industry comes after decades of enshittification of radio (all clear channel owned in the U.S. now) and the total collapse of any revenue opportunities for making an album for a new artist. It’s no wonder that the arts are dominated by nepo babies today. People who can treat creativity as an unpaid hobby, taking all risks with no consequences, and short cutting the path to popularity thanks to having family connections and wealth. Taylor Swift had a rich daddy buy a record label for her. For everyone else, why bother? reply surgical_fire 7 hours agorootparentIf an actual artist is being plagiarized, I would presume there are avenues for them to sue for that? Is this actually happening, or is this just a baseless claim? reply keybored 10 hours agoparentprevIt’s a problem if the algorithms are bad and try to nudge you towards slop (path of least resistance). But I don’t really get the part about “someone should tell Congress”—sir, this is an IKEA. reply izacus 9 hours agorootparentI bet they're pushing towards \"slop\" because they don't want to pay that much to the record label parasites which have been extorting a lot of money out of them. And I think that's the core of the \"problem\" here - one group of rentseekers being angry at another because they don't get enough of their cut. reply tmountain 9 hours agorootparentThen focus on brokering deals with the zillions of independent artists out there. I worked at Grooveshark (a long time ago), and this was a big part of our strategy, and it showed a lot of potential. reply izacus 9 hours agorootparentSpotify already directly publishes plenty of independent artists as far as I know. reply keybored 9 hours agorootparentprevSure. Both Spotify and the Music Industry are just trying to maximize their profits. The term “slop” is deliberate—music that sounds bad. Now, if Spotify manages to make music that is better than the music that they can get from the Music Industry? Good for them. I’m a music listener. I just want good music. reply keyle 5 hours agoprevThere are a lot of people here who don't seem to have a problem with the bait and switch business Spotify is doing here. As a former (paid) composer, I know how, pardon my words, the music industry is utterly fucked up today. That said we shouldn't just be cool with what Spotify is doing. Let me put it this way, what is happening is similar to Embrace, Extend, Extinguish. Personally I don't think much of Spotify today, and hope that we go back to buying music on medias and owning the music purchases. That is the key, to bringing back some sense to the Music (and Game btw) industry. reply KingOfCoders 10 hours agoprevMy main grief as as a very long Spotify customer, their app is still bad and doesn't progress. It has often problems when the [edit] phone is offline e.g. and it doesn't work beyond songs - radio play, audiobooks especially in playlists are PITA. reply LambdaComplex 10 hours agoparentSpotify has about 10,000 employees. Their software engineers make 6 figures. Their only actual product is, as far as I know, a music player. Despite all this, I had an issue for a while where some of the play buttons didn't work. I would click the play button and nothing would happen. Other users posted about the same issue in the \"Spotify Support/Complaint Megathread\" on Reddit's r/spotify. I don't know how to adequately describe how ridiculous it is that they managed to break their music player's play button, of all things. reply raincole 7 hours agorootparentHow many of the 10,000 are developers though? For a company like Spotify, I'd assume they have a huge legal/business team just to deal with record label. reply Mistletoe 9 hours agorootparentprevIt’s just absolutely dreadful. Using it on desktop is a laggy mess. I mean this quite literally what do the 10,000 people do? reply spuz 8 hours agorootparentIt wasn't always like this. When Spotify was launched the desktop app was one of the best pieces of software I'd ever used. It launched quickly and songs played without buffering which was unheard of at the time. It was the work of largely one developer, Ludvig Strigeus who was also the author of the original uTorrent client (which was also used to be fantastic). As a software developer I'm biased but I think this ultra smooth experience was a large contributor to Spotify's initial growth. reply lycopodiopsida 8 hours agorootparentIt was also a native application on macOS (OS X at that time). But then it was replaced with this Electron abomination which would have a runaway process any other day and consume all battery and would require 2 years to implement sorting in playlists. I truly ask myself what their development team is doing. reply doublerabbit 43 minutes agorootparentprevSpam me with pop ups tell me what I should be listening too. reply iamacyborg 9 hours agorootparentprevCome up with features no one asked for to justify their salaries. Seems to be rather common in product-driven tech businesses. reply Mistletoe 9 hours agorootparentI can only imagine how many soul-searching meetings it took to decide to change the heart to the plus sign for liking songs. reply fredrikholm 9 hours agorootparentprevJazz, apparently. reply fsflover 7 hours agorootparentprev> I don't know how to adequately describe how ridiculous it is that This is nothing else than enshittification: https://news.ycombinator.com/item?id=41277484 reply Aachen 10 hours agoparentprev(Note for nongermans: Handy means phone in German, I think specifically smartphone but not sure. Edit: the word was edited out) For me, I just use Spotify for music and that's what used to have real struggles with offlineness. Kept updating the app until a version worked properly and now I've stopped. I'm on 8.8.96 (installed December 16th 2023, going by apk file modified time) whereas 9.0.2 is the latest No problems with this version, so I like the app currently. Let's see how long the servers work with this version. The only downside I've experienced so far is that the yearly overview requires updating every year again reply KingOfCoders 10 hours agorootparentLOL sorry yes, too early + sick ;-) That aside, it feels to me the same, some versions work, some don't. reply Aachen 10 hours agorootparentWhy not stick with the version that works? I don't think I've seen this approach before but I do it for any app that doesn't seem like a security problem when out of date reply lukan 9 hours agorootparent\"that doesn't seem like a security problem when out of date\" When spotify has access to your filesystem and the spotify app would get hacked and owned by another party due to an unfixed security hole, your filesystem still would get owned. (or are all android apps really sandboxed nowdays? I doubt it) reply Aachen 4 hours agorootparentThey don't have filesystem access, see the permissions overview in the OS settings for the app But also how would the client be vulnerable in a way that leads to a compromise? Connecting to a domain that doesn't exist anymore? Even then it would need to download executable code, or exploit a bug that leads to it reading and uploading arbitrary files—if it had filesystem access in the first place. Working in the security industry, I never hear of people needing to update mobile apps for such a bug, it seems to be exceptionally rare. Most issues are server problems, direct object access (missing permission checks on invoice pdf downloads or so) or such I do update things like SSH (yes, on my phone) because that actually exposes a port on the network, or email/messaging clients where anyone can send arbitrary content (how many times have we heard of image parsing bugs being exploited through a messaging system like SMS or WhatsApp?); that's not really the case for Spotify. There's attack vectors like the URIs that the app can handle, but it's not wormable in the same way because it requires user interaction and so won't go viral. It would be a specific attack and there's easier ways to target me > (or are all android apps really sandboxed nowdays? I doubt it) You might want to read up on this. They've always been isolated. reply catlikesshrimp 9 hours agorootparentprev> don't think I've seen this approach before but I do it for any app that doesn't seem like a security problem when out of date Besides security: There are apps that don't like you lagging behind with updating, like whatsapp and epocrates. Since whatsapp needs internet, there is no way around it. Old versions of Epocrates could be isolated from internet for the offline content, but they fixed that this year, and you can't download the databases with an older version. reply freetonik 10 hours agoprev> Our single best hope is a cooperative streaming platform owned by labels and musicians. There’s a coop Bandcamp alternative being built right now: https://subvert.fm/ Hopefully reply badgersnake 9 hours agoparentIs Bandcamp evil now then? reply JTyQZSnP3cQGa8B 9 hours agorootparentThey were bought by Epic Games and then Songtradr. There were layoffs, and the company is not recognizing the union of the employees. It hasn’t changed yet, it’s still good, but it’s scary for the future. reply raincole 10 hours agoprevThe ugly truth is that most people (I dare to say 99%+) don't care much about music. They just need to listen to something while doing their jobs. reply catlikesshrimp 9 hours agoparentBroadcast Radios (FM and AM) did a much better job. Their music was real and each station was more or less an algorithm. There were ads, but as you mention, people don't seem to care much. The quality check was the non personalized streams cathered to please its audience and competition among stations. So, the solution is to kill targeted algorithms and monopolies, ha. reply BoingBoomTschak 9 hours agoparentprevWell, it's not a secret, no? I mean, if you use Spotify with big holes in non-mainstream music and only the latest butchering known as \"remaster\" of your favourite albums, you definitely don't care much about music. Which is why you'll never see me get off the comfy \"local only\" train, buying CDs and sailing the high seas with my skull & crossbones flag if needed. reply xg15 10 hours agoprev> At this point, I need to complain about the stupid major record labels who have empowered and supported Spotify during its long history. At some junctures, they have even been shareholders. I’ve warned repeatedly that this is a huge mistake. Spotify is their adversary, not their partner. The longer they avoid admitting this to themselves, the worse things will get. I think this confuses record labels with artists. I don't see labels having a problem with replacing their artists with AI, as long as they still get the royalties. reply harry8 10 hours agoparentYou’d need labels at all then why? reply iamacyborg 9 hours agorootparentRecord labels, particularly indie ones are a great way to curate and discover particular music. I’ve discovered a ton of great stuff I never would have otherwise come across by looking at what a record label I’m familiar with is releasing. reply lotsofpulp 10 hours agorootparentprevBecause copyright lasts an obscenely long time so record labels can continue to rent seek and grab a large share of spending on music for work performed many decades ago. reply harry8 9 hours agorootparentThis is not replacing artists with ai which is what is being discussed. Spotify replaced artists with ai, they have no need to pay labels to do it. reply lotsofpulp 9 hours agorootparentPeople like to listen to the music of their youth, record labels would still be needed for many decades to come to provide a comprehensive streaming catalog. Any service without the biggest music acts of ~1970 to 2020 isn’t going to go far. reply klabetron 10 hours agoprev> Our single best hope is a cooperative streaming platform owned by labels and musicians. Remember when the music labels themselves were the baddies? reply motorest 10 hours agoparent> Remember when the music labels themselves were the baddies? I remember when the biggest problem in music was how middlemen like labels had an oppressive role on music and didn't allowed artists to express themselves or even earn a buck from their work. As far as I know, that didn't changed. reply hansworst 9 hours agorootparentPlenty of indie musicians nowadays that self-publish on platforms like Spotify and YouTube. That just wasn’t possible 30 years ago. reply motorest 7 hours agorootparent> Plenty of indie musicians nowadays that self-publish on platforms like Spotify and YouTube. That just wasn’t possible 30 years ago. Precisely. What changed between then and now is that major labels no longer control distribution, and thus they no longer hold a oligopoly over what music we can consume. This is clearly something that benefits everyone, except the major labels. This fact alone makes me very sceptical of this blend of blog posts. It clearly pushes to go back to a time where everyone was screwed over by the music industry. reply throwawayian 10 hours agoparentprevTencent Music own almost 10%, UMG and Sony own almost 5% together, there are vested interests here. reply raincole 7 hours agoparentprevIt's just like traditional publishers and any \"gatekeeper\" ever existing. Creators hate them until they face the consumers directly. And then they'll realize what consumers want is actually slop, and the only entities keeping slops from being even slopper are the gatekeepers. reply punnerud 10 hours agoprevI forced myself to use YouTube Music 7 years ago and haven’t switched back (included with YouTube premium), also improved a lot the latest years reply harry8 9 hours agoparentYouTube wants you to pay for rubbish quality uploads by their users. You want to listen to an album and put on the playlist, track 3 was a single so you get a mono recording of the rock video complete with theatrical dialog that makes no sense to listen to when you want to listen and not watch. Is that playlist actually the list for the album? Maybe not but even if it is, how many tracks are missing because of copyright claims or similar? YouTube does nothing but rely on the crowd for quality and it’s rubbish. But sure, I’m sympathetic when they ask me to pay for “premium” - I’m sure it’s a premium rate. reply s6af7ygt 7 hours agoparentprev(Sarcasm warning). Surely the music industry will be fine in the long term when everyone switches to Google, Apple, and Amazon. reply whoitwas 9 hours agoparentprevI don't understand what YT Music is, but I listen to all my music for free on regular YT. Just download it, use an adblocker, or an alternative viewer. Any album I've ever sought is available usually in multiple versions. reply 627467 10 hours agoparentprevYouTube pays artists much better than Spotify, but artists still look at Spotify success as music career success reply KingOfCoders 10 hours agoparentprevWhy? (sounds interesting as a very very long Spotify customer) reply Lvl999Noob 10 hours agorootparentNot the person you asked but for me, it's because Youtube Music actually has the music I care about (fan edits of songs, compilations, mixes, etc). Then I also get to watch the videos when I want to or just listen to the music when on the move. These two, but especially the first, are the big reasons I use youtube music. reply mnky9800n 10 hours agorootparentprevI did it because why pay for Spotify and for YouTube music if I was going to pay for YouTube premium anyways. The one thing it is missing is when you are at a party on Spotify can have a shared playlist where people can just add songs to it from their own phones. But YouTube can’t do that. It also won’t crossfade songs which Spotify does. But I don’t feel like I’m missing out because of those reasons. reply whoitwas 9 hours agorootparentWhat is YT premium and why would you pay money to use YT? It's the best free service that still exists if you have an adblocker. A lot of content has baked in ads now, but you can skip or just view something else. I've learned what Youtube Premium is: \"YouTube and YouTube Music ad-free, offline, and in the background\" For me, that's nothing as I can already view with no ads, and download whatever I want. reply npteljes 9 hours agorootparent>What is YT premium and why would you pay money to use YT? To me, it's a matter of principle that evolved over the years. I grew suspicious of free services. So, I pay for what I use, at least, in the terms that the provider lays out. I pay for my domain, I pay for my email, and I pay for YouTube as well, as I use it heavily. I still think that the subscription cost absolutely doesn't compensate for my usage, there is just no way in hell. But I do really like the service, and so, I would like to contribute, so I like that I can have this subscription relationship with it. I also like to express my opinion with my wallet: \"I don't like being sold to advertisers, I would rather pay directly\". I think youtube is fair for offering this alternative, I wish more providers did. reply whoitwas 8 hours agorootparentYou're paying the largest marketer in the world to consume content that's a large percentage ads. I agree with paying for things, but subscription model provide no ownership and limited value. I avoid all subscription services. reply npteljes 7 hours agorootparent>You're paying the largest marketer in the world to consume content that's a large percentage ads. Untrue. I pay the largest marketer in the world (debatable, but not the point) to consume content on-demand, of which some, negligible percentage is ads. And usually I don't care about those being ads either. For example, I enjoy Abroad in Japan, and he is sometimes sponsored, invited to present a Japanese restaurant or tourist place. I don't mind at all. >I agree with paying for things, but subscription model provide no ownership and limited value. I know exactly what I get, and I don't mind that at all. I desire no ownership over these videos. It's very rare that I re-watch them, and I am comfortable that if, for any reason, it all goes away in an instant, I can move on to other things, with not feeling that I have lost my money in it or anything. I do understand that the value proposition doesn't work out for you personally, but since the original question was \"why would you pay money to use YT\", this is why. It's an excellent service for an excellent price. reply whoitwas 5 hours agorootparentMy point is by voting for the subscription model by buying into it you're encouraging bad corporate behavior. Subscriptions are anticonsumer. Additionally, all that is available for free. I'm boggled. reply npteljes 5 hours agorootparentI get your point, but I think the opposite of it. For one, there is nothing wrong with a subscription model, in exchange for a service. How else is a continuous service supposed to be funded, if not continually? This is not encouraging bad behavior, it's a fair exchange. It's not anti-consumer at all. I don't see how it's different from my electricity bill, for example. >all that is available for free. The free opportunity does not mean that it's the best course of action automatically, same as how not every legal thing is moral or encouraged also. For example, computer games are also available to me for free, even legally, because in Hungary, I'm entitled to have my personal copy of an intellectual property, no matter how I got it. Going on with this, choosing to pirate all my games would be optimal, as it would require the least amount of resources from me. But then, I don't just want to maximize my own gains, I want to see a healthy gaming ecosystem, where the creators of the games are financially rewarded for their effort, so that they are enabled to create more or better games. So, I contribute to this ecosystem with my money. Even subscriptions, so to say, for example I always buy the \"game pass\" for my multiplayer game of choice, because while I bought the game also, they host the servers, do matchmaking and create in-game content continually - so I participate continually as well. I kinda get the point that with these subscriptions, we don't get to own what we \"buy\". But I think that if one realizes this, and consciously goes into a transaction like this, then it is not a problem. In fact, I don't want to own many things, for example, the YouTube videos I watch I specifically don't want to own. What I enjoy is the access, and the experience, and I don't mind at all to pay for those. reply whoitwas 4 hours agorootparentGoogle doesn't need the money. I don't get it, but I'm sure I'm in the minority. reply npteljes 1 hour agorootparentIn a way yes you are, because people pay subscription to a lot of services, but in another way, not really. Every time YT premium comes up, there is a lot of people who exhibit defiance paying the giant, for one reason or another. And there are Vanced users, etc. I too have a pet revenge peeve like this though, and for me, it's Microsoft. I'm using Windows for my multiplayer games, and I refuse to ever pay a cent for the piece of shit. Right now I'm activating it with my own activation emulator, for example. reply wazzaps 9 hours agorootparentprevIt also includes 256kbps AAC audio (vs 128kbps for free users). reply whoitwas 8 hours agorootparentI have no idea what that is. You're talking about kilobytes per second? This feature makes it useful to use bluetooth or other non hardwired sound output? reply punnerud 10 hours agorootparentprevLower price and just wanted to try. Forced myself to use it, started creating new playlists, listen to the radio created based on songs etc. Now my YouTube Music is more used by the kids and the family than Spotify reply mstipetic 10 hours agorootparentprevYou get YouTube premium and YouTube music with one subscription. For me it’s a no brainer, I’ve had it for years reply nextworddev 5 hours agoprevI’m not defending Spotify nor do I like the company but I have two problems with this article’s rhetoric: 1) the word payola isn’t technically correct to describe Spotify “optimizing” its cost of goods sold by replacing with mass produced, farmed content. It’s more similar to Amazon Basics category of goods. So payola is not the right term, since no one’s paying them to promote some music on the platform. Actually there’s real payola and that’s from companies like Universal music promoting drake etc 2) calling the Spotify owned content “slop” is kinda unfair to the creators. Afaik it’s not AI generated and there is real musicians making money off of this (albeit little) 3) I’m almost positive that Spotify will just start using AI exclusively for creating this slop very soon reply summermusic 2 hours agoparentTo directly address point #2, this may be music played by real musicians, but it is produced in a way that systemically forces it to be slop. Consider these quotes from the Liz Pelly investigation cited where a musician who created some of this music described how the process went: > As he described it, making new PFC starts with studying old PFC: it’s a feedback loop of playlist fodder imitated over and over again. A typical session starts with a production company sending along links to target playlists as reference points. His task is to then chart out new songs that could stream well on these playlists. “Honestly, for most of this stuff, I just write out charts while lying on my back on the couch,” he explained. “And then once we have a critical mass, they organize a session and we play them. And it’s usually just like, one take, one take, one take, one take. You knock out like fifteen in an hour or two.” With the jazz musician’s particular group, the session typically includes a pianist, a bassist, and a drummer. An engineer from the studio will be there, and usually someone from the PFC partner company will come along, too—acting as a producer, giving light feedback, at times inching the musicians in a more playlist-friendly direction. The most common feedback: play simpler. “That’s definitely the thing: nothing that could be even remotely challenging or offensive, really,” the musician told me. “The goal, for sure, is to be as milquetoast as possible.” reply jbs789 10 hours agoprevThis seems analogous to a supermarket selling its own private label product. Should be disclosed though. reply Flemlord 10 hours agoprevHow is this any different than Walmart or Amazon having their own brands that are sold alongside name brands from other companies? reply omcnoe 10 hours agoparentThere are two concerns here. First, that Spotify doesn't make clear when a track is produced by a Spotify \"ghost artist\". And second, Spotify is in an unfair position as both the controller of the marketplace/platform, and as a participant on it. The allegation in this article is that Spotify are using their platform position to promote their own PFC program tracks over third party artists/labels. To be clear, it's not necessarily consumers who are being harmed here. These tracks are supposedly targeted to cases where the consumer doesn't really care that much about the songs that are being played. Rather the party harmed are third party artists/labels who are competing for Spotify playlist space on an uneven playing field. reply boxed 9 hours agorootparentThird: Spotify has been involved again and again in various outright criminal enterprises like money laundering. Organized crime is rising strongly in Sweden, we don't need Spotify to back them. reply galaxyLogic 10 hours agorootparentprevThe consumer is harmed if they are given the impression that playlists contain tracks voted up by other listeners like themselves, when in fact they are voted up by Spotify. Not sure if this is the case but if so, it would be clearly misleading the users. reply dhfuuvyvtt 9 hours agorootparentprevLiterally Amazon marketplace. reply bergen 10 hours agoparentprevYou know they are Walmarts brands reply almostnormal 9 hours agorootparentThey do have different prices, too. And the supermarkets try to nudge people to buy the higher priced variants for greater revenue. The underlying problem that makes it tempting to promote cheap music is the flat rate. reply ramon156 9 hours agoparentprevDisclosure aside, would you say their own brand doesn't influence the original brand at all? It still hurts them reply tinthedev 10 hours agoprevThe TLDR is that Spotify is flooding it's platform and padding playlists with cheap and generic music. They've went full \"buffet\" strategy, serving lots of fries so you stay away from the meats. I think calling this payola, as the article insinuates,is wrong. I was always more interested in finding artists than I was in finding songs. I've noticed Spotify recommendations being worse and worse, and I can happily say I've left the platform half a year ago. Didn't regret it a single bit. reply Aachen 10 hours agoparentI don't think it says generated, but music made by people (presumably simply paid a fair wage by the hour or so) who don't then get royalties but it can just be played infinitely many times at no cost to Spotify besides bandwidth reply tinthedev 8 hours agorootparentNowhere did I say generated. The word I was using is generic. I presume the artists have been paid a fair wage, but as the article describes, are focusing on derivatives. reply Aachen 4 hours agorootparentOoh sorry, I misread! reply ethagnawl 10 hours agoparentprevI left when they gave Bro Rogaine hundreds of millions of dollars to promote harmful conspiracy theories and pseudoscience. Between Bandcamp, Tidal and YouTube (latter are more likely to face these same issues), I've never looked back. The odd exception is when a friend wants to share a song but I can usually find it elsewhere. For anyone considering leaving Spotify for Tidal but fearful of losing their followed artists, playlists, etc., there are a number of (paid) services which will programmatically export and link your data. They're not perfect but I think my success rate was 80%. reply from-nibly 4 hours agoprevIt had become increasingly clear that \"You are not the customer, you are the product\" extends to all public companies. reply pluc 10 hours agoprevMp3s still work great reply StuffMaster 6 hours agoparentAnd you can use whatever software you want reply huguesdk 9 hours agoprevi agree that we need a cooperative streaming platform. well, it already exists: https://resonate.coop/. please give it some love. in addition, its original stream2own model allows you to automatically spend more on the artists you listen to more and even own (= stop paying for) and download the tracks you listened to at least 9 times. i think that it is a much fairer revenue-distribution model than “the big money pot” model used by spotify (and almost all other music streaming platforms out there) where people listening to the highest amount of tracks decide where other people’s money goes. reply daft_pink 9 hours agoprevThe questions is really are the other platforms doing the exact same thing? You have to use playlists generated by other users. Any playlists or radio generated by the platform is obviously going to direct you to the music the platform wants you to listen to. reply computerthings 6 hours agoprev> nobody in the history of music has made more money than the CEO of Spotify This seems vaguely important?! Yet this story got utterly nuked off HN (can't find it on the first 10 pages). Meanwhile this still lingers on page 1: https://news.ycombinator.com/item?id=42461530 Yeah I know discussing votes and flags and rankings and all that stuff is very boring. However we're dealing with people who throw money around to manipulate what people see and hear so they can make more money, so. reply threeseed 10 hours agoprevIt was inevitable that if most of your user base is just listening to AI generated playlists that Spotify would look to cheaper versions of songs to save costs. reply labster 10 hours agoparentThe next obvious step is AI generated songs. Music will get much cheaper when we can completely avoid cost centers like record labels, recording studios, and bands. reply mcculley 10 hours agorootparentWhen models are capable of generating music better than humans can, Spotify might be disintermediated entirely. reply lambdaone 9 hours agorootparentThe next logical step after AI-slop music from Spotify would be for device manufacturers to cut out one more middleman, and provide their own locally-generated on-device AI-slop music as a feature of their devices. reply mcculley 9 hours agorootparentWhy would slop be the only outcome? reply throw646577 9 hours agorootparentEven if the AI produces a pearl, it is going to drown it in shit. Because it is an automated operation. No new AI content generation advance has so far escaped being used for the equivalent of SEO spamming, long-tail flooding the market to mislead consumers and lazy “passive income” bullshit. No AI content generation advance ever will; it’s not like industry is going to hold back and say: not this one, guys. It’s too good. We’re going to let it have three years in a custom built home studio to make its magnum opus. reply mcculley 8 hours agorootparentI think you are conflating how models are used by services like Spotify with what they can do. I run some models locally. These are going to become more powerful. Your confident prediction of the future limitations of models sounds like what the chess and go masters said. reply throw646577 7 hours agorootparentI'm not conflating anything. I am just answering the question of why slop can be the only outcome. On a Spotify-level scale, web-level scale, slop can be the only outcome. reply mcculley 7 hours agorootparentThe question I asked was specifically about locally generated content. reply throw646577 7 hours agorootparentBut it's in the context of an alternative to a Spotify product, right? With millions of users. Exactly where the slop is generated, or by whom, is essentially irrelevant: at scale, slop is the only outcome from these tools. It always will be. And presumably in a resource-limited environment, slop quality is even more likely. reply mcculley 6 hours agorootparentBillions of users, with increasingly powerfully hardware, locally executing models that better understand the world and human psychology. reply throw646577 9 hours agorootparentprevThe alarming thing in this little sub thread is that I am not wholly certain that none of you are joking. That is where we are now. reply mcculley 9 hours agorootparentI am not joking. I am also uncomfortable accepting that humans will eventually not be the best creators of art enjoyed by humans. reply throw646577 9 hours agorootparentThen don’t accept it. Because it is fundamentally untrue. reply mcculley 9 hours agorootparentWhy do you think it is anything other than inevitable? You think that models cannot learn how to make music that humans will enjoy? reply throw646577 9 hours agorootparentYou are watering down “the best” here, IMO. This definition leads towards “the most efficient production”, not “the highest art”. But sure, if you don’t care that a human isn’t involved in doing it. Personally it’s not negotiable to me and I think more people believe this than you might expect. reply mcculley 9 hours agorootparentI do see it as inevitable that models will create the best of everything. I completely understand that some humans will choose to reject anything not created by a human. reply tatersolid 5 hours agorootparentBased on what evidence? AI models haven’t created the best of anything yet. Actually AI hasn’t even made anything I’d call very good thus far in my personal experience. reply mcculley 4 hours agorootparentI agree that current models are not as good as humans. They will continue to improve. They will eventually be superhuman, just as they are at chess and go. reply punnerud 10 hours agoparentprevIt’s from the same country that have managed to automate most of service work at Klarna using ML, so not surprised reply mnky9800n 10 hours agorootparentI don’t really understand what being from Sweden has to do with anything. reply Lio 9 hours agoprevAll I know about Spotify is their embedded player, that gets added to music articles, has been designed to nag you. A popover will randomly cover the controls with a signup banner so that you can’t play the next song without saying no. It’s a subtle tell but when you see even a small dark pattern you know the company behind it has no moral compass. Have nothing to do with them. reply nkrisc 9 hours agoparent> the company behind it has no moral compass. The people behind it. Companies have no sense of morality, everything you see was conceived and implemented by individual people. reply mahkeiro 10 hours agoprevThe only way for the music industry to fight back is to ensure diversity in streaming platform, and fighting for easy way for users to port their data between them. This for one will both benefit users and the industry. reply wodenokoto 10 hours agoparentThere’s another way, which HN’ers won’t like and that’s to borrow a page from the video streaming playbook and have the labels start their own individual services. Which of course will also eventually push Muzak on the bgm playlist/channels reply Aachen 10 hours agorootparentOh god please no. I'm already needing like seven video subscription services on a rotating basis to see the various nice things people talk about, music is such a good counterexample of where it just works to aggregate everything on several platforms and people can choose which service works best reply izacus 9 hours agorootparentprevOf all the options, how did you decide on the most user hostile one? reply wodenokoto 9 hours agorootparentI just asked myself “What would EMI do?” reply bornfreddy 9 hours agorootparentprevSure, if they agree on a common interface (API) and open it to everyone. This way they would turn streaming services into commodity. I don't see that happening though. reply amyames 5 hours agoprevJust leave Spotify 1-star reviews with excerpts of this article in it. reply RA2lover 10 hours agoprevRelated: https://news.ycombinator.com/item?id=42461530 reply HelloUsername 10 hours agoparentThat research is linked in this article reply phony-account 9 hours agoprevThere are so many “so what?” responses to this that I can’t help wondering what an analogous situation would be for programmers? Maybe something like “I don’t see the problem with this app being buggy and almost unusable - I just like having nice icons on my HomeScreen. I don’t see anything unethical about this [MEGA-CORP] fixing the market with their own cheap and poor quality programmers - most people don’t care if their apps work anyway”. reply surgical_fire 10 hours agoprevI mean, the actual problem is that people listen to playlists and recommendations from Spotify itself. They voluntarily eat turds, then complain that what they just ate tastes like shit. The solution is simple - curate your own playlists, or find people with tastes similar to yours that shared theirs. Don't give a company power to pick music for you, then complain that they have such power. reply ethagnawl 10 hours agoparent> The solution is simple ... Another worthwhile solution is to listen to independent radio -- locally or online. You'll find more new interesting artists and music in an hour of listening to (quality, non-Clear Channel/i-heart) radio than you'll ever find using Spotify or any other service's suggestion algorithms. reply surgical_fire 9 hours agorootparentThis sounds like a sensible advice too. I never tried it myself though. reply whiddershins 10 hours agoprevif true, beyond the pale. actively making music worse for everyone. reply ilrwbwrkhv 10 hours agoprev [–] Yes, this totally tracks with what I know about Spotify. I am in touch with numerous employees of Spotify and they all said that the biggest threat to Spotify are the music publishers who have them by the balls. I think they have now realized that the only way to make it into a successful business is to deploy AI generated music or at least music in which they are not paying extremely large royalties. reply amiantos 10 hours agoparentDoesn't seem any different to me than when Netflix started making original content, or as others have said, Target having their own in-house brands that they stock alongside everything else. It's just good business sense. If the product is bad, people will stop using Spotify, and Spotify will stop doing it. I'm not sure why discourse around this seems to be coming from the perspective that people have no choice but to use Spotify and that Spotify will secretly replace all the good music with bad music and no one will be able to do anything about it, all musicians will be out of work, all will be lost. Spotify is just one music service, and we don't even need to use a music service to enjoy the music we like. If Spotify destroys itself with slop music, what's the sinister plot, the \"ugly truth\", really? If Spotify does this, and a majority of people do not care, do not notice, and keep using Spotify until label-owned music is totally irrelevant, I think it would be more interesting if the musicians who are put out of work by this consider why their high quality music lost out to such mass produced \"garbage\"... reply throw646577 9 hours agorootparent> Doesn't seem any different to me than when Netflix started making original content, or as others have said, Target having their own in-house brands that they stock alongside everything else. It's just good business sense. If the product is bad, people will stop using Spotify, and Spotify will stop doing it. And yet it is different, because of the production scale involved. Own brands are actually hard work, especially own brand food, which has to be reverse-engineered at some expense. Own brand food provides price discrimination and usually does not particularly impact the sales of the headline brands. Netflix’s own content, presumably, pays a large number of creative people’s wages and is of a quality. It is often scratching a creative itch that major studios won’t risk. Music and music tech, on the other hand, is such that if your audience doesn’t care, it’s quite easy to churn out songs —- one person who particularly hated human culture could write half a dozen fully produced songs in a day. > If Spotify does this, and a majority of people do not care, do not notice, and keep using Spotify until label-owned music is totally irrelevant, I think it would be more interesting if the musicians who are put out of work by this consider why their high quality music lost out to such mass produced \"garbage\"... I do wish this particularly nihilistic new form of argument was considered to be toxic itself. But to answer the point: many outstanding, culturally significant musicians have small fan bases for their personal work. They know they do. That is in the nature of diverse culture. They rely on complex chains of discovery systems —- word of mouth, eclectic radio, old style auteur playlists, live gigs, to find their people, and they often support that by writing songs for major musicians who love their work and who want to do one song they wrote. It is not their failure at all if the system that is supposed to deliver some of that discovery starts cynically acting against them. While every musician wants a hit and many will dedicate at least some time in their lives to exploring what is necessary to write a hig record, “You should have had more mass market appeal” is not universal creative guidance. It is not that listeners have no choice in music services. Musicians have no choice but to engage with Spotify. If it is secretly working against all musicians simultaneously at the most basic level —- drowning them out with slop that is deliberately inserted into playlists to water down everyone’s incomes —- then how is it you are more interested in the way musicians are somehow failing, when nothing they do can stop it? Honestly I am terrified for culture that so many people in the tech world seem to be so aggressively jealous of creativity that they are taking the \"anti-anti-slop\" stance (to borrow a phrase from politics used for the \"yeah well all this criticism validates the choice I am not in any way saying I am making\" argument). reply mattkevan 8 hours agorootparentAlso the argument that people don’t deserve good quality music because they ‘don’t care’ is breathtakingly cynical. Why shouldn’t people be exposed to good music? Maybe they’ll get into it. Does that slop mentality extend to other things? Food, clothes, consumer goods? Do people deserve rubbish just because it’s not something they’re particularly interested in? reply surgical_fire 8 hours agorootparentprevThis argument makes no sense to me. Musicians with small niche audience that rely on word of mourh and such are harmed in which way when Spotify adds slop to slop playlists? Nobody is forced to listen to slop playlists. I never did, I only listen to albums and playlists that I either curated myself, of that were shared to me by people with similar tastes. I don't think that people that are fine with slop are the kind of people that niche musicians would appeal to. If your argument was against the egregious and exploitative way Spotify demonetizes tracks with less than 1k streams, then yes. This is abusive and should be considered illegal. But this mostly benefits large musicians and is detrimental to a long tail of niche ones. reply throw646577 7 hours agorootparent> This argument makes no sense to me. Musicians with small niche audience that rely on word of mourh and such are harmed in which way when Spotify adds slop to slop playlists? They are harmed because part of the intrinsic deal with Spotify is that you put your music on there so that their discovery systems make it possible for people to hear it who might not otherwise. If Spotify is deliberately diluting that mechanism it is bad enough. But they are doing it IN SECRET which they would not do if they didn't think artists, industry people, musicians unions etc., would construe that as harm. reply surgical_fire 7 hours agorootparentThis is a problem only if the actual users are unhappy with the music they are being served. If the music they are listening to satisfy their musical tastes for the monent, again, what is the issue? If generic slop Spotify created to increase their own profits satisfies the listener the same way a song created with labor and love by an actual artist, well, this is saying something. I am not sure either of us will like the message. reply throw646577 7 hours agorootparent1) It's clearly a problem to artists who have contracts with Spotify and are being defrauded at scale by a secret process to dilute the value of the platform they believe they are being distributed on. 2) > If generic slop Spotify created to increase their own profits satisfies the listener the same way a song created with labor and love by an actual artist, well, this is saying something. I am not sure either of us will like the message. Again: users do not know they are being fed this stuff. If this was an official, named Spotify product, and users preferred it, then your anti-anti-slop argument might have some weight. Spotify are doing this in secret because they know it is fraud on likely a contract level but certainly on a cultural level. They'd be rolling it out with press releases if they didn't know it was underhanded. That is the issue. But then again HN is a place where people really don't mind if companies break laws on an epic scale, destroy worker protections, and deliver worse products like AI chatbots replacing human support, if customers don't really notice. So maybe I shouldn't be too surprised by (veiled as much as full-throated) endorsement of secret padding prole-feed. reply surgical_fire 6 hours agorootparent> It's clearly a problem to artists who have contracts with Spotify and are being defrauded at scale by a secret process to dilute the product they believe they are using. Does the contract say anywhere that Spotify would never provide their own generated music to users of their platform? If the contract between artists and platform provides no such limitations, I fail to see how anyone is being defrauded of anything. > Again: users do not know they are being fed this stuff. Users are actively listening to those slop playlists. It does not seem to bother them (otherwise they would stop listening). If they are satisfied with the offering, what exactly is the issue? > Spotify are doing this in secret because they know it is fraud on likely a contract level but certainly on a cultural level. They'd be rolling it out with press releases if they didn't know it was underhanded. This is plain bullshit. The claim is that people are listening to those songs when they are added to Spotify generated playlists. It wouldn't matter if the song had, say, a tag indicating that it was published by Spotify themselves. Most likely people are not even looking at the names of songs and artists when the slop playlist they are listening to is playing as background noise. > That is the issue. But then again HN is a place where people really don't mind if companies break laws on an epic scale, destroy worker protections, and deliver worse products like AI chatbots replacing human support, if customers don't really notice. If you saw my comment history, you would be surprised. The problem here is that users are actively listening to slop, but they don't seem to mind it. They are fine with slop. They keep listening to slop. For those users, it really does not matter. There is no abuse taking place. I am a Spotify user. I think their app is bloated, but it works alright? I can find albums I like, I can create my own playlists, and I can completely ignore whatever Spotify generates I always did. How exactly are they abusive in their relationship with me as a customer? What you are really angry with is that Spotify found a way to give users that are fine with slop their own cheap slop. reply throw646577 6 hours agorootparent> Does the contract say anywhere that Spotify would never provide their own generated music to users of their platform? > If the contract between artists and platform provides no such limitations, I fail to see how anyone is being defrauded of anything. Then why does Spotify refuse to talk about it? It's clearly outside of the spirit of the deal they are striking with artists generally. Ultimately, they will get away with it because of just the same quasi-justification you're offering: Oh we're not screwing any of them individually, we are screwing them all at once. Congratulations I guess, your argument is on the winning side! But certainly Congress, the UK parliament, EU parliament, Swedish parliament etc. should be looking at this through the lens of regulation and competition law. And for once I hope the record labels -- big and small, because so many of them are small -- sue. reply surgical_fire 6 hours agorootparentI see you toned down the \"defrauded\" rhetoric to a much more ambiguous \"yeah, it was against the spirit of the deal\". I am always in favor of proper regulation. If the EU sees this as abusive and detrimental to competition, they should definitely put restrictions on platforms offering their own content. I wonder if this would extend to, say, supermarkets offering products under their own brands. It is important that regulations don't harm the consumers. As for record labels suing... Well, they will have this problem which is proving that any fraud took place. Unless the numbers were rigged (and no users actually streamed the slop, and it was actually fraud), I fail to see how they would win anything. What would be the allegation? That users are listening to bad music? reply throw646577 4 hours agorootparent> I see you toned down the \"defrauded\" rhetoric to a much more ambiguous \"yeah, it was against the spirit of the deal\". I didn't, particularly. It's just another angle on it. I do think \"defrauded\" has both legal and moral dimensions, don't you? This is obviously abject shitty treatment or they wouldn't have been secretive about it. I'm not convinced this isn't both dimensions, though -- not clear who is being defrauded but both the labels and the musicians unions should do their best to establish whether it could be them. Spotify have crossed a pretty obvious line here. Weird that you appear to think it's, like, no harm no foul if the customers don't notice. Fast food service logic. There is already quite extensive legal precedent and practice about supermarket own-brand stuff; fifty years of it, with supermarkets retaining teams of lawyers, operating proper clean-room reverse-engineering processes, etc. It's well-trodden legal territory. Supermarkets work right up to the line. But it's no secret that they do, and brands sometimes push back against own-brand deceptions. Mostly supermarkets threaten each other over their own, own-brand stuff. Caterpillar cakes, vodkas etc. If Spotify were honest that they were adding \"own brand\" music into the mix, then this would be a different discussion. (Amazon are required in the UK and EU to identify which are their own brands, and they mostly comply) reply surgical_fire 3 hours agorootparentI would be okay if Spotify were forced to add a clear tag to every song that was published by themselves. I would bet that it would make zero difference in the amount of plays those tracks got. reply throw646577 2 hours agorootparent> I would bet that it would make zero difference in the amount of plays those tracks got. Well given that they are inserted into generated playlists and offered up as \"similar\" playlist suggestions instead of tracks by authentic artists, probably not. For this to really be resolved they would have to stop putting these tracks in those lists or suggestions panels. The fact that they evidently list the same track under completely different titles suggests that they are not going to act ethically here. reply nsteel 6 hours agoparentprev [–] It's not clear to me how successful they were with podcasts. They certainly attracted lots of podcasters but no idea how much profit that's generated reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An investigation by indie journalist Liz Pelly has uncovered Spotify's use of fake artists to increase profits, particularly in genres like jazz and ambient.- This practice, known as the \"Perfect Fit Content\" program, enables Spotify to avoid paying royalties to genuine musicians, raising concerns about transparency and regulation in music streaming.- Despite the controversy, Spotify's profitability has improved, while the music industry and mainstream media have largely overlooked these issues, leaving independent journalism to bring them to public attention."
    ],
    "commentSummary": [
      "The article raises concerns about Spotify allegedly favoring low-quality, generic music, possibly AI-generated, over genuine artists to cut costs.- Users express dissatisfaction with Spotify's music recommendations, sparking a debate on the ethics of its business practices and the impact on artists.- The discussion underscores the challenges artists face in a streaming-dominated market and suggests alternatives like independent radio or other streaming services."
    ],
    "points": 173,
    "commentCount": 216,
    "retryCount": 0,
    "time": 1734767128
  },
  {
    "id": 42475011,
    "title": "DOS APPEND",
    "originLink": "https://www.os2museum.com/wp/dos-append/",
    "originBody": "OS/2 Museum OS/2, vintage PC computing, and random musings Skip to content Home About Wanted List OS/2 History OS/2 Beginnings OS/2 1.0 OS/2 1.1 OS/2 1.2 and 1.3 OS/2 16-bit Server OS/2 2.0 OS/2 2.1 and 2.11 OS/2 Warp OS/2 Warp, PowerPC Edition OS/2 Warp 4 OS/2 Timeline OS/2 Library OS/2 1.x SDK OS/2 1.x Programming OS/2 2.0 Technical Library OS/2 Videos, 1987 DOS History DOS Beginnings DOS 1.0 and 1.1 DOS 2.0 and 2.1 DOS 3.0, 3.1, and 3.2 DOS 3.3 DOS 4.0 DOS Library NetWare History NetWare Timeline NetWare Library Windows History Windows Library PC UNIX History Solaris 2.1 for x86 ← Stack Checking on OS/2 DOS APPEND Posted on December 20, 2024 by Michal Necasek For a long time, I couldn’t quite grasp what the DOS APPEND command could possibly be good for. Until I came across a situation which APPEND was made for. When I worked on organizing and building the DOS 2.11 source code, I tried to place the source files in a tree structure similar to that used by DOS 3.x (this is known from DOS 3.x OAKs): C:. └───src ├───bios ├───cmd │ ├───chkdsk │ ├───command │ ├───debug │ ├───diskcopy │ ├───edlin │ ├───exe2bin │ ├───fc │ ├───find │ ├───format │ ├───more │ ├───print │ ├───recover │ ├───sort │ └───sys ├───dos ├───inc └───msdos The inc subdirectory unsurprisingly contains shared include files such as DOSSYM.ASM, which are included just about from everywhere. No problem, right? Except… to get output that most closely matches existing DOS 2.x binaries, it is necessary to use an old version of MASM (version 1.25 seems to do the trick). But MASM 1.25 is designed to run on top of DOS 1.x, and knows nothing whatsoever about directories. It is possible that back in the day, DOS 2.x was built from a single huge directory on a hard disk. In fact it is known that DOS 2.0 could not be built on PCs at all, and was built on DEC mainframes. Yet DOS 2.11 was also clearly modified such that it could be build on PCs using Microsoft’s development tools. However it was done back in 1983, lumping 150+ assembler source files into a single directory, and then adding hundreds of object and executable files, did not sound at all appealing. Cloning DOSSYM.ASM to every directory where it was needed seemed even worse. That’s when I somehow remembered that APPEND exists, and realized that it’s the perfect solution to the problem. Before building, one can run APPEND ..\\..\\INC;..\\INC and the inc directory becomes accessible from all of its sibling subdirectories and from subdirectories one level deeper. It would have been possible to use an absolute path as well, but this way the build batch file does not need to know where it lives. With APPEND in place, the old MASM 1.25 which uses FCB I/O will find the centrally located include files, and the source code can be organized into a neat hierarchical structure that’s far easier to work with than one giant blob. What is APPEND? APPEND is a “DOS extension”, in fact it is a TSR which intercepts INT 21h and adds special handling for several subfunctions. These are primarily: 0Fh FCB File Open 3Dh Handle File Open 23h Get File Size If these subfunctions fail to find a file in the current directory, APPEND will retry them using the list of paths it manages. When building DOS 2.11, MASM 1.25 will try to open DOSSYM.ASM using INT 21h/0Fh (FCB File Open). Because the file does not exist in the current directory, the initial attempt will fail. APPEND will then try opening ..\\INC\\DOSSYM.ASM and, if that is unsuccessful, also ..\\..\\INC\\DOSSYM.ASM. Old MASM is thus magically upgraded to handle multiple directories, without actually knowing anything about them. The working principle of APPEND is not complicated. It primarily serves as a bridge between old DOS applications which have no or poor support for directories, and users who really, really want to organize files and programs in multiple directories and possibly across multiple drive letters. Of course the actual APPEND implementation is anything but straightforward. APPEND Evolution and Implementation The first DOS version which came with APPEND was DOS 3.3 (1987), not coincidentally the first DOS version developed by IBM. But APPEND is older than that—it first appeared in the IBM PC Network Program 1.0 in 1985. It is hard to speculate why it was shipped with the PC Network Program (later the PC LAN Program) because APPEND does not really have anything to do with networking. It is plausible that it was especially useful with networking, when users were motivated to store applications on a central network server and data files on their own machines. And that’s a problem for applications which cannot handle directories well. Now that we can see the source code for APPEND, some things are clearer. The original PC Network Program version of APPEND was written by someone with initials G. G. A., and in 1986 it was adapted for shipping with DOS by B. A. F., no doubt Barry A. Feigenbaum (best known for developing the SMB protocol). APPEND by default manages the path list in its own internal storage. But it also has a /E option which instead causes APPEND to look for an eponymous environment variable. This mechanism has a disadvantage in that the APPEND= variable needs space in every newly created environment. On the other hand, it also allows different DOS processes to have different APPEND paths. It should be noted that OS/2 implements a mechanism analogous to APPEND /E through the DPATH environment variable. Only on OS/2 it’s built in, with no need to load TSRs. Another APPEND addition was the /X switch, which causes APPEND to hook further DOS subfunctions, most notably Find First and Exec. This effectively allows APPEND to supplant the PATH environment variable. APPEND is listed in the PC DOS 3.3 reference as both internal and external command. At first glance that doesn’t make any sense, but it’s actually true. The first time APPEND is run, it is an external command. But when it installs itself as a TSR, it hooks INT 2Fh/AEh. COMMAND.COM, in turn, calls INT 2Fh/AEh when it is asked to execute a command that COMMAND.COM does not know about. This mechanism allows APPEND to function as an internal command once it is installed. That is, the first time APPEND is run, it must be loaded and executed from disk. But any subsequent attempt to run APPEND through COMMAND.COM, either from the command line or a batch file, will take a shortcut directly to the already installed TSR, effectively turning APPEND into an internal command. The INT 2Fh/AEh interface between COMMAND.COM and a TSR was added in DOS 3.3, quite likely for the benefit of APPEND. APPEND also has its own programming interface, accessed through INT 2Fh/B7h. This allows programs to control APPEND behavior and query the current APPEND path. How widely this is used isn’t entirely clear. Summary APPEND is one of the things that are completely irrelevant 99.99% of the time… yet can be extremely useful when the need arises. It is a TSR which allows applications to find files in a directory other than the current one. The first appearance in APPEND was in the IBM PC Network Program (1985), but since version 3.3 (1987) it was integrated into DOS, with an interesting link to COMMAND.COM which allows APPEND to become an internal command once it is installed. This entry was posted in DOS, IBM. Bookmark the permalink. ← Stack Checking on OS/2 13 Responses to DOS APPEND rasz_pl says: December 20, 2024 at 1:48 pm I have this vague memory of playing DOS game, protected from copying by hardcoded CD DATA path, using DOS command to redirect it to C: “backup”. At first I thought I was using APPEND, but it was another another redirecting utility SUBST zeurkous says: December 20, 2024 at 5:25 pm There’s also JOIN. And that’s when I realized, that when all three programs are brought together, it will open Shub-N… Sorry, wrong story :X Lovecraftian horrors abound. Michael Russo says: December 20, 2024 at 6:44 pm Is there an implication in this article that Microsoft were using an internal version of APPEND to organize the DOS 2.11 source code and compile it with MASM 1.25 as well? And that neither they nor IBM really saw that it could be useful to release it as part of DOS until 3.3? Michal Necasek says: December 20, 2024 at 7:18 pm No. I see no evidence that Microsoft did that. I also have no idea if Microsoft had anything like APPEND, but my guess is that if they did, IBM wouldn’t write their own. I strongly suspect that Microsoft primarily built the DOS code on DEC mainframes in the 2.x days. It is possible that near the end of the DOS 2.x cycle, they used a newer MASM version with directory support. It’s also possible that individual developers built on PCs using a newer MASM, yet the official OAKs were built on DECs as before, using an older MASM version. And it’s also possible that developers using PCs just shoved everything into a single hard disk directory. It’s difficult to say. Richard Wells says: December 21, 2024 at 1:06 am There is an APPEND.COM in the MSDOS 3.2 install over at PCJS. I find it strange that the MSKB notes on APPEND don’t include version 3.2 in the list of applicable versions. David C. says: December 21, 2024 at 1:49 am @Richard. Confirmed. I checked my printed DOS 3.2 manual and APPEND is documented there. Simon Kissane says: December 21, 2024 at 5:48 am What do you mean by “DEC mainframes”? I believe early Microsoft (1975 onwards) did stuff on PDP-10s running TOPS-10, but then when they got Xenix (1978 onwards) they started to run stuff under Xenix on PDP-11s. PDP-10s are often called “mainframes” (although I know some people dispute that), while PDP-11s are usually called minicomputers. I don’t know what they developed DOS under, but by the time DOS 2.0 was being developed I think a PDP-11 minicomputer running Xenix would have been more likely than a PDP-10 mainframe. But I don’t really know. Alan Robinson says: December 21, 2024 at 6:22 am I used append (or maybe it was join) to allow applications too large to run on a single floppy disk to share my A and B drives simultaneously, thus giving 2+MB of storage. Think of it as very dumb RAID0. In my mind this is what the tool was really meant for, but I guess you make your own purpose. Michal Necasek says: December 21, 2024 at 1:44 pm I have some explanation for that. APPEND was not included with PC DOS 3.20 (files dated Dec ’85) or 3.21 (Feb ’86). Which makes sense because according to the APPEND source code, conversion to DOS was done in June 1986. There is a MS-DOS 3.21 OAK which definitely includes APPEND, but that was released in May 1987. There are also disk images of MS-DOS 3.20 floating around, with files dated 07/07/1986. That also includes APPEND, significantly different from the one that shipped with PC DOS 3.3, and it’s APPEND.COM rather than APPEND.EXE. Michal Necasek says: December 21, 2024 at 1:44 pm PC DOS or MS-DOS? Michal Necasek says: December 21, 2024 at 2:02 pm The only thing that is 100% certain is that several files in the MS-DOS 2.11 source code contain the following comment: “The DOST: prefix is a DEC TOPS/20 directory prefix. Remove it for assembly in MS-DOS assembly environments using MASM.” So whatever ran TOPS-20. Microsoft literature (The MS-DOS Encyclopedia) also says that when they ported 86-DOS to the IBM PC, a “DEC-2020” system was used, presumably a DECSYSTEM-2020. That would have been 1980/1981. A DECSYSTEM-2020 running TOPS-20 then seems like a reasonable inference. I know that Microsoft’s development tools for PC XENIX could compile for DOS. I do not know if Microsoft had that before they wrote their own compiler (1984). Of course there’s no telling what unreleased internal tools Microsoft may have had. Josh Rodd says: December 21, 2024 at 8:08 pm APPEND, JOIN, and SUBST were oddities which pointed the way to features like symlinks, but DOS never quite got there. (I still use SUBST to this day to map letter drives on a Windows server so that the drive letters can be the same as client PCs mapping those shares.) OS/2’s DPATH was another abortive attempt. It’s surprising no true symbolic link (or even hard link) support existed until the NT era; you can create hard links on a FAT filesystem, but they will be destroyed the next time you run CHKDSK. Michal Necasek says: December 21, 2024 at 9:22 pm One person’s weird hack is another person’s union mount. Windows NT had nothing like symlinks before Windows 2000 AFAIK, and the NTFS support was barely exposed to users before Vista. Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website This site uses Akismet to reduce spam. Learn how your comment data is processed. Archives December 2024 November 2024 October 2024 September 2024 August 2024 July 2024 June 2024 May 2024 April 2024 March 2024 February 2024 January 2024 October 2023 September 2023 August 2023 July 2023 June 2023 May 2023 April 2023 March 2023 January 2023 December 2022 November 2022 October 2022 September 2022 July 2022 June 2022 May 2022 April 2022 March 2022 February 2022 January 2022 December 2021 November 2021 October 2021 September 2021 August 2021 July 2021 June 2021 May 2021 April 2021 March 2021 February 2021 January 2021 December 2020 November 2020 October 2020 September 2020 August 2020 July 2020 June 2020 May 2020 April 2020 March 2020 February 2020 January 2020 December 2019 November 2019 October 2019 September 2019 August 2019 July 2019 June 2019 May 2019 April 2019 March 2019 February 2019 January 2019 December 2018 November 2018 October 2018 August 2018 July 2018 June 2018 May 2018 April 2018 March 2018 February 2018 January 2018 December 2017 November 2017 October 2017 August 2017 July 2017 June 2017 May 2017 April 2017 March 2017 February 2017 January 2017 December 2016 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 January 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 December 2012 November 2012 October 2012 September 2012 August 2012 July 2012 June 2012 May 2012 April 2012 March 2012 February 2012 January 2012 December 2011 November 2011 October 2011 September 2011 August 2011 July 2011 June 2011 May 2011 April 2011 March 2011 January 2011 November 2010 October 2010 August 2010 July 2010 Categories 286 386 386MAX 3Com 3Dfx 486 8086/8088 Adaptec AGP AMD AMD64 Apple Archiving Assembler ATi BIOS Books Borland BSD Bugs BusLogic C C&T CD-ROM Cirrus Logic CompactFlash Compaq Compression Computing History Conner Corrections CP/M Creative Labs Crystal Semi Cyrix DDR RAM Debugging DEC Development Digital Research Documentation DOS DOS Extenders Dream E-mu Editors EISA Ensoniq ESDI Ethernet Fakes Fixes Floppies Graphics Hardware Hacks I18N IBM IDE Intel Internet Keyboard Kryoflux Kurzweil LAN Manager Legal Linux Marketing MCA Microsoft MIDI NetWare Networking NeXTSTEP NFS Novell NT OS X OS/2 PC architecture PC hardware PC history PC press PCI PCMCIA Pentium Pentium 4 Pentium II Pentium III Pentium Pro Plug and Play PowerPC Pre-release PS/2 QNX Quantum Random Thoughts RDRAM Roland Ryzen S3 SCO SCSI Seagate Security Site Management SMP Software Hacks Solaris Sound Sound Blaster Source code Standards Storage Supermicro TCP/IP ThinkPad Trident UltraSound Uncategorized Undocumented UNIX UnixWare USB VGA VirtualBox Virtualization VLB Watcom Wave Blaster Western Digital Windows Windows 95 Windows XP Wireless WordStar X11 x86 x87 Xenix Xeon Yamaha OS/2 Museum Proudly powered by WordPress.",
    "commentLink": "https://news.ycombinator.com/item?id=42475011",
    "commentBody": "DOS APPEND (os2museum.com)112 points by SeenNotHeard 21 hours agohidepastfavorite53 comments TeMPOraL 19 hours ago> APPEND is one of the things that are completely irrelevant 99.99% of the time… yet can be extremely useful when the need arises. Is it really that irrelevant? I mean, if you look past the specifics (directories, interrupts, DOS versions), this seems to be implementing the idea of bringing something into scope, in particular bringing it into scope from the outside, to modify the behavior of the consumer (here, assembler) without modifying the consumer itself. Today, we'd do the equivalent with `ln -sr ../../inc ../inc`. I'd argue the general idea remains very important and useful today, though it's definitely not obvious looking back from the future what this was what APPEND was going for. reply theamk 18 hours agoparentYes, general idea is still very important and useful, but this is post about specific command called \"APPEND\" in MS-DOS environment. Also, it's not an equivalent of \"ln -sr\" as \"ln\" replaces targets and not stacks them. The proper modern equivalents are environment variables like LD_LIBRARY_PATH, PYTHONPATH, PKG_CONFIG_PATH, etc... and overlayfs mounts for a generic case. But back to the APPEND: in all my time working with MS-DOS, I don't remember ever needing that, so it was 100% irrelevant to me. But this could be because I've worked with more \"modern\" programs (like Turb Pascal 5) which had good support for directories. reply anyfoo 17 hours agoprevAlways a joy when os2museum updates. I, too, remember the trifecta of APPEND, JOIN, and SUBST. And while I always thought they were interesting, I was also wondering for most of them when I would ever use that. At the time, DOS versions and hence applications for it that don’t know subdirectories didn’t cross my mind, as my first DOS version was 2.11, I think. reply Joe_Cool 16 hours agoparentWhen I got my fancy 1.6 GB harddisk I used `subst R: .\\cd` to run my games needing the CD in the drive from a directory on the harddrive instead. Boy did load times improve a ton. reply pavlov 8 hours agorootparentCD-ROMs were extremely slow from a modern point of view. The original 1x speed was 150 KB/s, or 1.2 Mbps. That’s like trying to stream over a 3G mobile network with substantial packet loss, except it’s physically inside your computer. reply bluedino 5 hours agorootparentPlus 400-500ms seek times and a driver that might suck up all your precious CPU when reading data reply mikaraento 5 hours agoparentprevSUBST was commonly used for source code directories. It served at least two purposes: making source appear at the same location on all machines and working around path length limits. I've also used SUBST when reorganizing network drive mappings. reply wruza 16 hours agoprevMy favorite program in DOS was smartdrv.exe. I know it’s much more late addition, but it was a game-changer for these slow hard drives. Even a tiny cache size (I believe I tried kilobytes range) sped up things like 10-20x. Even windows 3.x and 95 (surpisingly) ran faster with smartdrv preloaded. 95’s default cache for some reason was worse than smartdrv and literally produced harder sounds on my hdds. The second favorite was a TSR NG viewer, can’t remember the name. reply skissane 16 hours agoparent> The second favorite was a TSR NG viewer, can’t remember the name. I know what a TSR is, but what's an \"NG viewer\"? reply wruza 14 hours agorootparentNorton Guides, iirc. E.g. Ralf Brown Interrupt List was available in it. Reading the docs without leaving an editor made programming much easier. reply anyfoo 12 hours agorootparentI sometimes wistfully look back to the days where I had a bunch of books and printouts open on my desk for programming. Of course, that's more than likely romanticizing things quite a lot... reply anyfoo 12 hours agoparentprev> and 95 (surpisingly) ran faster with smartdrv preloaded. 95’s default cache for some reason was worse than smartdrv My (weak) guess is that you \"32 bit disk access\" and \"32 bit file access\" wasn't active then, i.e. Windows 95 did not use its native 32 bit disk drivers, but 16 bit BIOS access. I have a hard time seeing how Smartdrv could have done anything otherwise, unless it really had some tight integration with 32 bit Windows (which would be very surprising, it's a 16 bit TSR first and foremost). But yeah, overall, I agree, it's surprising what even a tiny cache does. If you just manage to eliminate the seek times to the root directory for pretty much every single access to a new file, that can probably do a lot already. Especially in the days before \"native command queuing\", where the HD would try to reorder disk accesses to find a better (physical!) head seek path across the platter. Later HDs had some cache on board. reply hulitu 8 hours agoparentprev> My favorite program in DOS was smartdrv.exe. The downside was, that in case of a crash, you will use some data. reply wruza 2 hours agorootparentI was aware of that, but somehow it never happened, maybe got lucky. Smartdrv itself never crashed, and neither program crashes nor electricity issues affected it. Ofc I always used ctrl-alt-del before turning the pc off. It’s interesting what others experienced, cause I remember nothing about that in fido echoes (someone should have lost some data eventually). Was smartdrv even that different from today’s caching algorithms on all OSes? reply SunlitCat 20 hours agoprevAnother handy dos command, originating back to DOS is SUBST. Came in pretty handy when I wanted to share a folder with Remote Desktop, but it would only let me select whole drives. Made a SUBST drive letter for that folder, worked like a charm! reply Dwedit 19 hours agoparentSUBST makes use of NT Object Namespace Symbolic Links to register the drive letter. After running SUBST, you get an object named \"M:\" (or whatever your drive letter is\") sitting in the \"\\??\\\" directory, its full path will be \"\\??\\M:\". It will be a symbolic link that points to something like \"\\??\\C:\\target_path\". You can either see this by using \"NtQuerySymbolicLinkObject\" on \"\\??\\M:\", or calling \"QueryDosDeviceW\" on \"M:\". On Windows NT, you will see the result as an NT-native path \"\\??\\C:\\target_path\" rather than a Win32-style path \"C:\\target_path\". \"\\??\\\" is not some kind of special notation for paths or anything, it is a real NT object that exists. It holds your drive letters and other things. On Windows 9x, you won't see an NT-native path from QueryDosDevice, you'll instead see a Win32-style path \"C:\\target_path\". Weirdly enough, Sysinternals Winobj is unable to find the symbolic link object at all, despite that it exists when you query it using NT API calls. Fun fact about NT native paths, you can use them in Win32 if you prefix them with \"\\\\?\\GLOBALROOT\". So \"\\??\\C:\\\" becomes \"\\\\?\\GLOBALROOT\\??\\C:\\\". You can use it in any Win32 program that doesn't actively block that kind of path (such as the file explorer/open dialog) reply abdulhaq 7 hours agorootparentNT wasn't even a twinkle in Dave Cutler's eye when DOS APPEND came into being reply bombcar 20 hours agoparentprevIIRC originally SUBST was designed for that - early programs didn't understand directories but did understand drives, and so you could make a directory appear to be a drive and they'd be happy - otherwise they'd dump everything in the root of C:\\ (or A:\\). reply mycall 20 hours agoparentprevI still use SUBST with my team so we all have our source code on P:\\ which can be mapped to wherever they want it to be. This helps keep Visual Studio object files and project includes pointing to the same place, especially when mistakes are made (they should be relative paths but things happen). It is run from a registry key upon bootup. reply magicalhippo 19 hours agorootparentWe do the same, except you don't need to do it at bootup, you can set it once using the following: Windows Registry Editor Version 5.00 [HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\DOS Devices] \"P:\"=\"\\\\??\\\\C:\\\\Dev\\\\Source\" Change source path accordingly, save as .reg file, import once and it'll stay. Nice thing about this vs using SUBST is that the SUBST is for your user only, so if you have a separate admin account it won't see it. However the above registry entry is for the machine, so all users on the machine will see it. Obviously makes it less useful for terminal servers and other shared machines. reply Kwpolska 19 hours agorootparentI think SUBST can break when you run as administrator (elevating your own privileges). reply jasomill 13 hours agorootparentIt still works, but the elevated context maintains a separate list of drive letter mappings, so you need to issue the SUBST command again while elevated. The same applies to network drive letter mappings. Under the hood, both are implemented as NT Object Manager symbolic links, which you can see using, e.g., https://learn.microsoft.com/en-us/sysinternals/downloads/win... reply Kwpolska 20 hours agorootparentprevSUBST is all fine, up until the point some tool explodes when it sees that normalizePath(\"P:\\\\whatever\") == \"C:\\\\code\\\\whatever\", and it ends up with two paths to one file, or no way to build a relative path. I’ve seen that happen with some node tooling, for example. reply Dwedit 19 hours agorootparentI've seen programs blow up because they think they need to \"canonicalize\" the path, when all they actually want is a full path (a rooted path rather than a relative path). Canonicalizing the path resolves all drive letters and symbolic links. The Rust function that canonicalizes a path can fail on some mounted filesystems if the drive letter wasn't registered using the Windows Mount Manager. reply tom_ 20 hours agorootparentprevI think some of the WSL stuff refuses to deal with SUBST'd drives. And if you use voidtools's Everything, it's worth spending 2 minutes setting up some excluded paths so that you don't get doubled up entries. But it seems it does generally work pretty well. I've recently done work for a former employer, who it seems are still using SUBST'd drive Z:, just as they'd always done when I worked there nearly 20 years ago. (Main reason: we'd found it worked well at the place we'd all worked at previously...) The idea of everybody having the same paths for things never sat right with me, because it's an easy way for absolute paths to sneak in, which can be (and occasionally was) a problem when trying to support multiple branches later in the project lifecycle. But if you've got a CI system, which people typically do in 2024, that's much less of an issue (because you can arrange for the CI system to build from a random path each time). And it is pretty handy when you can paste universally-usable paths into the chat. reply Kwpolska 19 hours agorootparent> you can arrange for the CI system to build from a random path each time Or you can end up having to use the magic path for all builds. At work, we have a subst’d drive. The way we set it up means that somerequires it (by the virtue of hardcoding, or by common configuration). But I do most of my development from a fully separate, leaner copy of the repo, and consider the subst drive to be a garbage pile. reply tom_ 19 hours agorootparentThe idea is that you set up the CI system to build from a different path as often as possible. Per build, per build machine, per build machine rebuild - whatever works for you. Just ensure it's a different path from the standard one that the developers use. Ideally, also make sure that the path the developers use is completely inaccessible on the CI machines. Now the chance of introducing dependencies on the standard path is much lower. You can do it, and it will work for you, and it will work for everybody else too when they get your change. But at some point, the CI system will get it, and it will fail, and you'll know that you have something to fix. reply technion 20 hours agoparentprevSUBST to this day is how you solve long file name problems. One drive for business can make a very long path if it uses your full business name. Windows has the api to let some apps apps save long sob folders, but not to let Explorer or powershell delete those folders. You go on folder up and use subst to make a drive letter from which you can delete content. reply asveikau 16 hours agorootparentExplorer (known in MS jargon as \"the shell\", but I'm avoiding the term because it confuses Unix users) is limited by MAX_PATH characters when it stores absolute paths. Win32 allows you to use paths above MAX_PATH in length when you 1. Use the utf-16 filename apis and 2. prefix your path with \\\\?\\. But the shell doesn't do the latter. It may also use fixed size buffers for paths, that is a common reason for such limitations. reply codesnik 19 hours agoparentprevpoor man's chroot. reply johng 20 hours agoparentprevAhh yes, subst was very handy many times back in the day and it worked like magic to me! reply troad 7 hours agoprevThis is really neat! Are there any good books on DOS that a person who enjoys articles like this may also enjoy? And more broadly, does anyone have any good books to suggest about the personal computers of the 80s and 90s, that don't skimp on the messy technical details? reply fredoralive 7 hours agoparent“Undocumented DOS” goes very deep into the technical details, if you want to know what’s going on behind the curtain. reply pram 20 hours agoprevIs INT 2fH the DOS equivalent of PATH? What a bizarre mechanism, I've read it 2 times and I have no idea what it's saying lol: http://vitaly_filatov.tripod.com/ng/asm/asm_011.16.html reply SeenNotHeard 19 hours agoparentINT 2Fh was the so-called \"mux\" that various TSRs and drivers could hook into for (in essence) interprocess communication. The half-baked idea was to solve the problem of TSRs commandeering other interrupts for one-off needs, which led to lots of collisions. In order for the mux to work, each TSR had to have its own identifier code. Other than some reserved ranges, no one organized such a namespace, meaning it was possible for two or more TSRs to intercept the same request, leading to the same collision problem. This note from Ralf Brown's Interrupt List has the gory details: http://www.ctyme.com/intr/rb-4251.htm Incomplete list of TSRs and drivers relying on it: http://www.ctyme.com/intr/int-2f.htm reply epcoa 20 hours agoparentprevIt's just an ugly ass syscall extension mechanism (so it has no direct equivalent in Linux lets say), it definitely looks bizarre in modern times. Int 2F is initially handled by DOS as a stub, but additional programs (like drivers and TSRs) can override INT 2F, put their bucket of functionality and then fallback to whatever the previous installed handler was (called chaining) for whatever they don't handle. This gives a glimpse into how much various crap could end up installed as an Int 2F handler: https://www.minuszerodegrees.net/websitecopies/Linux.old/doc... It was often used for feature/presence checks and usually nothing time critical as that chaining setup was most definitely not timing friendly. reply skissane 15 hours agorootparent> This gives a glimpse into how much various crap could end up installed as an Int 2F handler: Lot's of crap in INT 21 too: https://fd.lod.bz/rbil/zint/index_21.html (on the whole I like this HTMLified version of Ralf Brown's Interrupt List better) But I suppose they invented INT 2F to discourage people from doing that to INT 21. And then Ralf Brown also proposed an alternative multiplex interrupt, INT 2D: https://fd.lod.bz/rbil/interrup/tsr/2d.html#4258 reply astrobe_ 8 hours agorootparentINT 21 was for DOS what INT 80 was for Linux: the gateway to the OS kernel. Overriding software interrupt handlers was a common mechanism, applied to BIOS interrupts as well (e.g. there was a BIOS timer interrupt that was there just for that). The idea was that programs would take over the interrupt vector then chain-call the previous handlers. One can still see something similar at play in certain scenarios with e.g. callbacks. The system/library doesn't have to maintain a list of clients. reply miohtama 20 hours agoprevI remember wondering APPEND as a kid three decades ago. Looks like it had a very specific legacy use case, which was no longer present in more modern DOS versions. Live and learn. reply NotYourLawyer 15 hours agoprevHuh. I knew this command existed but never looked into it. I assumed it was like cat, just appending files together. reply pavlov 21 hours agoprev [–] > “In fact it is known that DOS 2.0 could not be built on PCs at all, and was built on DEC mainframes.” Nitpick, but DEC never made a mainframe. Their products like the PDP-11 were considered minicomputers (even though the CPU was the size of a fridge) to distinguish them from IBM’s mainframes and medium sized computers. reply mepian 20 hours agoparentThe PDP-10 was a mainframe: https://en.wikipedia.org/wiki/PDP-10#cite_note-1 reply retrac 20 hours agoparentprevDEC was always finnicky about naming; the PDP series originally wasn't supposed to be called a computer because computers were thought of as much bigger than the products DEC sold, and customers in the 50s and 60s might be put off by a name they associated with multi-million dollar expenses. But the PDP-10 and VAX 9000 were basically mainframes. Million dollars or more. Whole large room with three phase power. Standard building AC might suffice but that was pushing the margin. And the faster clocked VAX 9000 was water cooled! That's not a minicomputer. reply karmakaze 2 hours agorootparentI've used VAX and IBM systems and never heard anyone call a VAX a \"mainframe\". Looking into it, I get similar descriptions where they definitely were mainframe-class but refrained from calling them \"mainframes\" outright to distinguish them from IBMs. Other names Super VAX, VAXcluster/VMScluster etc were common. There probably were some that called their VAX computers \"mainframe\" but I'd never known of one--it just has the wrong connotations. The VAX systems were actually far more advanced in most ways--except for deployment scale. reply brucehoult 19 hours agorootparentprevMy PC is water cooled. However the VAX 9000 wasn't. It was initially designed to be water cooled but during development they improved the air cooling enough that water cooling was never shipped. One VAX 9000 CPU did around 70k Dhrystones/sec, so 40 VAX MIPS (DMIPS as we call them now). A modern simple RISC 5-stage in-order CPU (e.g. Berkeley RISC-V Rocket) with decent branch prediction does 1.6 DMIPS/MHz, so will need 25 MHz to match the VAX 9000. However the December 2016 HiFive1 ran at 320 MHz, so would be around a dozen times faster than the VAX 9000 -- but without FPU or MMU. Today, a $5 Milk-V Duo running at 1 GHz (64 bit, and with MMU and FPU and vector unit) will be 40x faster than a VAX 9000. The VAX 9000 has a vector unit, capable of a peak 125 MFLOPS. The Duo's C906 VFMACC.VV instruction has 4 cycles latency, for two 64-bit multiply-adds or four 32-bit multiply-adds, so at 1 GHz that's 1 GFLOP double precision or 2 GFLOP single precision, 8x a VAX 9000. Don't even ask what a modern i9 or Ryzen can do! reply mmooss 20 hours agorootparentprev> But the PDP-10 and VAX 9000 were basically mainframes. Million dollars or more. Whole large room with three phase power. Standard building AC might suffice but that was pushing the margin. And the faster clocked VAX 9000 was water cooled! That's not a minicomputer. Why is that not a minicomputer. From our perspective it's a massive installation; from the perspective of the time, it was not necessarily. reply varjag 20 hours agorootparentMinicomputer at the time was considered a system that fits in a rack or three. Not something that requires a purpose built room with own mains, raised floor and AC. reply somat 11 hours agorootparentprevI am not exactly sure what makes a modern mainframe(same architecture as a historical mainframe I guess) but for historical machines I consider the wide machines(like the 36-bit pdp-10) mainframes, where minicomputers were usually narrower. 16 or 18 bit machines. Really there is no good formal definition, dividing computers into three groups(mainframe, minicomputer, microcomputer) based on how much you payed for it, is as good a mechanism for figuring this out as any other. reply Hilift 20 hours agoparentprevIt was probably a VAX 11/780. If you were cheap you purchased an 11/750. The 780 had a PDP-8 for a console processor. https://news.microsoft.com/features/the-engineers-engineer-c... reply SulphurCrested 16 hours agorootparentThe console processor was an LSI-11, a PDP-11 on a chip. It hung off the inside of one of the cabinet doors. It was responsible for booting the 780 and also gave VMS access to its own 8″ floppy drive, which had a habit of overheating. The 750 was later technology, IIRC MOSFET. It also lacked the 780’s “compatibility mode”, which allowed VMS to run 16 bit RSX-11 executables by implementing the PDP-11 instruction set in addition to the 32 bit VAX one, and could boot without a console processor. If you didn’t need all the users on one machine, the 750 was cheaper per user. reply p_l 20 hours agoparentprevThe PDP-11 was minicomputer, but PDP-10s were \"minis\" only formally, with VAX due to reasonable size and comparable performance coining the title \"supermini\" IIRC. reply surgical_fire 20 hours agoparentprev [–] Nitpick, but as far as I remember, minicomputers are midrange computers. DEC PDP-11 would be in the same class as IBM AS/400, so while they were distinguished from mainframes, they were \"medium sized computers\". Assuming, of course, that those \"medium sized computers\" are the midrange. reply jasomill 13 hours agorootparent [–] This gets even more confusing when you consider that IBM released a number of mainframes smaller than many of their midrange systems, including both System/370 and System/390 systems implemented as PC expansion cards (ISA, MCA, and PCI). Ultimately, within IBM at least, \"mainframe\" ended up just referring to any computer that implemented the System/360 architecture or its descendants. Outside IBM, I've seen the term applied to just about any multiuser system accessed primarily through a terminal interface, including x86 servers running Linux, though typically by nontechnical users. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The OS/2 Museum blog explores vintage PC computing, with a focus on OS/2, DOS, and related subjects.- A recent post highlights the DOS APPEND command, a Terminate and Stay Resident (TSR) program that aids older DOS applications in accessing files across different directories, first introduced in DOS 3.3.- The post delves into the history, evolution, and implementation of APPEND, discussing its significance in modern computing contexts."
    ],
    "commentSummary": [
      "The DOS APPEND command was historically significant for modifying assembler behavior without changing the assembler, akin to modern environment variables and overlayfs mounts.",
      "DOS commands like SUBST and JOIN were used for directory mapping and path management, helping to overcome path length limitations.",
      "The discussion includes DOS's historical development on DEC minicomputers and the evolution of computing hardware from minicomputers to current systems."
    ],
    "points": 112,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1734728699
  }
]
