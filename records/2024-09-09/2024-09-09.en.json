[
  {
    "id": 41484991,
    "title": "QUIC is not quick enough over fast internet",
    "originLink": "https://dl.acm.org/doi/10.1145/3589334.3645323",
    "originBody": "research-article Share on QUIC is not Quick Enough over Fast Internet Authors: Xumiao Zhang, Shuowei Jin, Yi He, Ahmad Hassan, + 3, Z. Morley Mao, Feng Qian, Zhi-Li Zhang (Less)Authors Info & Claims WWW '24: Proceedings of the ACM Web Conference 2024 Pages 2713 - 2722 https://doi.org/10.1145/3589334.3645323 Published: 13 May 2024 Publication History 0citation184Downloads Metrics Total Citations0 Total Downloads184 Last 12 Months184 Last 6 weeks48 Get Citation Alerts New Citation Alert added! This alert has been successfully added and will be sent to: You will be notified whenever a record that you have chosen has been cited. To manage your alert preferences, click on the button below. Manage my Alerts New Citation Alert! Please log in to your account Get Access Contents WWW '24: Proceedings of the ACM Web Conference 2024 QUIC is not Quick Enough over Fast Internet Pages 2713 - 2722 PREVIOUS ARTICLE ARES: Predictable Traffic Engineering under Controller Failures in SD-WANs Previous NEXT ARTICLE A Multifaceted Look at Starlink Performance Next Abstract Supplemental Material References Information & Contributors Bibliometrics & Citations Get Access References Media Tables Share Abstract QUIC is expected to be a game-changer in improving web application performance. In this paper, we conduct a systematic examination of QUIC's performance over high-speed networks. We find that over fast Internet, the UDP+QUIC+HTTP/3 stack suffers a data rate reduction of up to 45.2% compared to the TCP+TLS+HTTP/2 counterpart. Moreover, the performance gap between QUIC and HTTP/2 grows as the underlying bandwidth increases. We observe this issue on lightweight data transfer clients and major web browsers (Chrome, Edge, Firefox, Opera), on different hosts (desktop, mobile), and over diverse networks (wired broadband, cellular). It affects not only file transfers, but also various applications such as video streaming (up to 9.8% video bitrate reduction) and web browsing. Through rigorous packet trace analysis and kernel- and user-space profiling, we identify the root cause to be high receiver-side processing overhead, in particular, excessive data packets and QUIC's user-space ACKs. We make concrete recommendations for mitigating the observed performance issues. Supplemental Material MP4 File presentation video Download 75.36 MB MP4 File Supplemental video Download 36.53 MB References [1] 1998. cURL - command line tool and library for transferring data with URLs. https://curl.se/. Google Scholar [2] 2001. Linux Traffic Control (tc). https://man7.org/linux/man-pages/man8/tc.8.html. Google Scholar [3] 2012. HTTP Archive (HAR) format. https://w3c.github.io/web-performance/specs/HAR/Overview.html. Google Scholar [4] 2013. Chromium Blog: Experimenting with QUIC. https://blog.chromium.org/2013/06/experimenting-with-quic.html. Google Scholar [5] 2015. Chromium Blog: A QUIC update on Google's experimental transport. https://blog.chromium.org/2015/04/a-quic-update-on-googles-experimental.html. Google Scholar [6] 2018. UDP GSO. https://lwn.net/Articles/752956/. Google Scholar [7] 2020. Accelerating UDP packet transmission for QUIC. https://blog.cloudflare.com/accelerating-udp-packet-transmission-for-quic/. Google Scholar [8] 2020. Chromium Blog: Chrome is deploying HTTP/3 and IETF QUIC. https://blog.chromium.org/2020/10/chrome-is-deploying-http3-and-ietf-quic.html. Google Scholar [9] 2020. How Facebook is bringing QUIC to billions. https://engineering.fb.com/ 2020/10/21/networking-traffic/how-facebook-is-bringing-quic-to-billions/. Google Scholar [10] 2020. QUIC vs TCP: Which is Better? https://www.fastly.com/blog/measuringquic-vs-tcp-computational-efficiency. Google Scholar [11] 2020. What's the Best Bitrate for the Best Video Quality on YouTube? (1080p, 1440p, 4K). https://www.youtube.com/watch?v=0fz479id_Ic. Google Scholar [12] 2021. Chrome HAR capturer. https://github.com/cyrus-and/chrome-harcapturer. Google Scholar [13] 2021. HTTP/2 vs HTTP/3: A comparison. https://ably.com/topic/http-2-vs-http-3. Google Scholar [14] 2021. HTTP/3 and QUIC: Past, Present, and Future. https://www.akamai.com/blog/performance/http3-and-quic-past-present-and-future. Google Scholar [15] 2021. Improve UDP performance in RHEL 8.5. https://developers.redhat.com/ articles/2021/11/05/improve-udp-performance-rhel-85. Google Scholar [16] 2021. Linux Perf. https://man7.org/linux/man-pages/man1/perf.1.html. Google Scholar [17] 2021. Linux Temporary Filesystem (tmpfs). https://man7.org/linux/man-pages/man5/tmpfs.5.html. Google Scholar [18] 2021. YouTube 4K bitrates enconding. https://support.google.com/youtube/answer/1722171. Google Scholar [19] 2022. Catapult -Web Page Replay. https://chromium.googlesource.com/catapult//HEAD/web_page_replay_go/. Google Scholar [20] 2022. Fiddler - Web Debugging Proxy and Troubleshooting Solutions. https://www.telerik.com/fiddler. Google Scholar [21] 2022. GitHub - litespeedtech/lsquic: LiteSpeed QUIC and HTTP/3 Library. https://github.com/litespeedtech/lsquic. Google Scholar [22] 2022. HTTP RFCs have evolved: A Cloudflare view of HTTP usage trends. https://blog.cloudflare.com/cloudflare-view-http3-usage/. Google Scholar [23] 2022. SiteSucker. https://ricks-apps.com/osx/sitesucker/index.html. Google Scholar [24] 2023. NGINX QUIC. https://quic.nginx.org/. Google Scholar [25] 2023. OpenLiteSpeed. https://openlitespeed.org/. Google Scholar [26] 2023. The Chromium Projects. https://www.chromium.org/Home/. Google Scholar [27] 2023. The Chromium Projects - Network Service. https://chromium.googlesource.com/chromium/src//HEAD/services/network/. Google Scholar [28] 2024. QUIC-not-Quick: Artifact Release. https://doi.org/10.5281/zenodo.10679638. Crossref Google Scholar [29] 2024. QUIC-not-Quick GitHub repository. https://github.com/Shawnxm/QUICnot-Quick. Google Scholar [30] Mohsen Attaran. 2023. The impact of 5G on the evolution of intelligent automation and industry digitization. Journal of ambient intelligence and humanized computing 14, 5 (2023), 5977--5993. Crossref Google Scholar [31] Arkaprava Basu, Jayneel Gandhi, Jichuan Chang, Mark D Hill, and Michael M Swift. 2013. Efficient virtual memory for big memory servers. ACM SIGARCH Computer Architecture News 41, 3 (2013), 237--248. Digital Library Google Scholar [32] Mike Belshe, Roberto Peon, and Martin Thomson. 2015. Hypertext transfer protocol version 2 (HTTP/2). RFC 7540, IETF (2015). Google Scholar [33] Mike Bishop. 2022. HTTP/3. RFC 9114, IETF (2022). Google Scholar [34] Enrico Bocchi, Luca De Cicco, and Dario Rossi. 2016. Measuring the quality of experience of web users. ACM SIGCOMM Computer Communication Review 46, 4 (2016), 8--13. Digital Library Google Scholar [35] Michael Butkiewicz, Harsha V Madhyastha, and Vyas Sekar. 2011. Understanding website complexity: measurements, metrics, and implications. In Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference. 313--328. Digital Library Google Scholar [36] Gaetano Carlucci, Luca De Cicco, and Saverio Mascolo. 2015. HTTP over UDP: an Experimental Investigation of QUIC. In Proceedings of the 30th Annual ACM Symposium on Applied Computing. 609--614. Digital Library Google Scholar [37] Willem de Bruijn and Eric Dumazet. 2018. Optimizing UDP for content delivery: GSO, pacing and zerocopy. In Linux Plumbers Conference. Google Scholar [38] Quentin De Coninck and Olivier Bonaventure. 2017. Multipath quic: Design and evaluation. In Proceedings of the 13th international conference on emerging networking experiments and technologies. 160--166. Digital Library Google Scholar [39] Quentin De Coninck and Olivier Bonaventure. 2019. Multipathtester: Comparing mptcp and mpquic in mobile environments. In 2019 Network Traffic Measurement and Analysis Conference (TMA). IEEE, 221--226. Crossref Google Scholar [40] Quentin De Coninck, François Michel, Maxime Piraux, Florentin Rochet, Thomas Given-Wilson, Axel Legay, Olivier Pereira, and Olivier Bonaventure. 2019. Pluginizing quic. In Proceedings of the ACM Special Interest Group on Data Communication. 59--74. Digital Library Google Scholar [41] Sebastian Endres, Jörg Deutschmann, Kai-Steffen Hielscher, and Reinhard German. 2022. Performance of QUIC implementations over geostationary satellite links. arXiv preprint arXiv:2202.08228 (2022). Google Scholar [42] Mathis Engelbart and Jörg Ott. 2021. Congestion control for real-time media over QUIC. In Proceedings of the 2021 Workshop on Evolution, Performance and Interoperability of QUIC. 1--7. Digital Library Google Scholar [43] Godred Fairhurst, Tom Jones, Michael Tüxen, Irene Rüngeler, and Timo Völker. 2020. Packetization Layer Path MTU Discovery for Datagram Transports. RFC 8899, IETF (2020). Google Scholar [44] Roy Fielding, Mark Nottingham, and Julian Reschke. 2022. HTTP Semantics. RFC 9110, IETF (2022). Google Scholar [45] Anirudh Ganji and Muhammad Shahzad. 2021. Characterizing the Performance of QUIC on Android and Wear OS Devices. In 2021 International Conference on Computer Communications and Networks (ICCCN). IEEE, 1--11. Google Scholar [46] Habtegebreil Haile, Karl-Johan Grinnemo, Simone Ferlin, Per Hurtig, and Anna Brunstrom. 2022. Performance of QUIC congestion control algorithms in 5G networks. In Proceedings of the ACM SIGCOMM Workshop on 5G and Beyond Network Measurements, Modeling, and Use Cases. 15--21. Digital Library Google Scholar [47] Fahad Hilal and Oliver Gasser. 2023. Yarrpbox: Detecting Middleboxes at Internet- Scale. Proceedings of the ACM on Networking 1, CoNEXT1 (2023), 1--23. Digital Library Google Scholar [48] Te-Yuan Huang, Ramesh Johari, Nick McKeown, Matthew Trunnell, and Mark Watson. 2014. A buffer-based approach to rate adaptation: Evidence from a large video streaming service. In Proceedings of the 2014 ACM conference on SIGCOMM. 187--198. Digital Library Google Scholar [49] Swett Ian. 2020. As QUIC as TCP, Optimizing QUIC and HTTP/3 CPU Usage - EPIQ 2020. https://conferences.sigcomm.org/sigcomm/2020/workshop-epiq. html. Google Scholar [50] Janardhan Iyengar, Ian Swett, and Mirja Kühlewind. 2023. QUIC Acknowledgement Frequency. IETF (2023). https://datatracker.ietf.org/doc/draft-ietf-quicack- frequency/04/ Work in Progress. Google Scholar [51] Jana Iyengar and Martin Thomson. 2021. QUIC: A UDP-based multiplexed and secure transport. RFC 9000, IETF (2021). Digital Library Google Scholar [52] Benedikt Jaeger, Johannes Zirngibl, Marcel Kempf, Kevin Ploch, and Georg Carle. 2023. QUIC on the Highway: Evaluating Performance on High-Rate Links. In 2023 IFIP Networking Conference (IFIP Networking). 1--9. Google Scholar [53] Arash Molavi Kakhki, Samuel Jero, David Choffnes, Cristina Nita-Rotaru, and Alan Mislove. 2017. Taking a long look at QUIC: an approach for rigorous evaluation of rapidly evolving transport protocols. In proceedings of the 2017 internet measurement conference. 290--303. Digital Library Google Scholar [54] Mike Kosek, Hendrik Cech, Vaibhav Bajpai, and Jörg Ott. 2022. Exploring Proxying QUIC and HTTP/3 for Satellite Communication. In 2022 IFIP Networking Conference (IFIP Networking). IEEE, 1--9. Crossref Google Scholar [55] Mike Kosek, Luca Schumann, Robin Marx, Trinh Viet Doan, and Vaibhav Bajpai. 2022. DNS privacy with speed? evaluating DNS over QUIC and its impact on web performance. In Proceedings of the 22nd ACM Internet Measurement Conference. 44--50. Digital Library Google Scholar [56] Adam Langley, Alistair Riddoch, Alyssa Wilk, Antonio Vicente, Charles Krasic, Dan Zhang, Fan Yang, Fedor Kouranov, Ian Swett, Janardhan Iyengar, et al. 2017. The quic transport protocol: Design and internet-scale deployment. In Proceedings of the conference of the ACM special interest group on data communication. 183--196. Digital Library Google Scholar [57] Robert Lychev, Samuel Jero, Alexandra Boldyreva, and Cristina Nita-Rotaru. 2015. How secure and quick is QUIC? Provable security and performance analyses. In 2015 IEEE Symposium on Security and Privacy. IEEE, 214--231. Digital Library Google Scholar [58] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neural adaptive video streaming with pensieve. In Proceedings of the conference of the ACM special interest group on data communication. 197--210. Digital Library Google Scholar [59] Robin Marx, Joris Herbots, Wim Lamotte, and Peter Quax. 2020. Same standards, different decisions: A study of QUIC and HTTP/3 implementation diversity. In Proceedings of the Workshop on the Evolution, Performance, and Interoperability of QUIC. 14--20. Digital Library Google Scholar [60] Péter Megyesi, Zsolt Krämer, and Sándor Molnár. 2016. How quick is QUIC?. In 2016 IEEE International Conference on Communications (ICC). IEEE, 1--6. Crossref Google Scholar [61] Ayush Mishra, Sherman Lim, and Ben Leong. 2022. Understanding speciation in QUIC congestion control. In Proceedings of the 22nd ACM Internet Measurement Conference. 560--566. Digital Library Google Scholar [62] Arvind Narayanan, Eman Ramadan, Rishabh Mehta, Xinyue Hu, Qingxu Liu, Rostand AK Fezeu, Udhaya Kumar Dayalan, Saurabh Verma, Peiqi Ji, Tao Li, et al. 2020. Lumos5G: Mapping and predicting commercial mmWave 5G throughput. In Proceedings of the ACM Internet Measurement Conference. 176--193. Digital Library Google Scholar [63] Arvind Narayanan, Xumiao Zhang, Ruiyang Zhu, Ahmad Hassan, Shuowei Jin, Xiao Zhu, Xiaoxuan Zhang, Denis Rybkin, Zhengxuan Yang, Zhuoqing Morley Mao, et al. 2021. A variegated look at 5G in the wild: performance, power, andQoE implications. In Proceedings of the 2021 ACM SIGCOMM 2021 Conference. 610--625. Digital Library Google Scholar [64] Louis Navarre, Olivier Pereira, and Olivier Bonaventure. 2023. MCQUIC: Multicast and unicast in a single transport protocol. arXiv preprint arXiv:2309.06633 (2023). Google Scholar [65] Marcin Nawrocki, Raphael Hiesgen, Thomas C Schmidt, and Matthias Wählisch. 2021. Quicsand: quantifying quic reconnaissance scans and dos flooding events. In Proceedings of the 21st ACM internet measurement conference. 283--291. Digital Library Google Scholar [66] Marcin Nawrocki, Pouyan Fotouhi Tehrani, Raphael Hiesgen, Jonas Mücke, Thomas C Schmidt, and Matthias Wählisch. 2022. On the interplay between TLS certificates and QUIC performance. In Proceedings of the 18th International Conference on emerging Networking EXperiments and Technologies. 204--213. Digital Library Google Scholar [67] Javad Nejati and Aruna Balasubramanian. 2016. An in-depth study of mobile browser performance. In Proceedings of the 25th International Conference on World Wide Web. 1305--1315. Digital Library Google Scholar [68] Thomas William do Prado Paiva, Simone Ferlin, Anna Brunstrom, Ozgu Alay, and Bruno Yuji Lino Kimura. 2023. A First Look at Adaptive Video Streaming over Multipath QUIC with Shared Bottleneck Detection. In Proceedings of the 14th Conference on ACM Multimedia Systems. 161--172. Digital Library Google Scholar [69] Mirko Palmer, Thorben Krüger, Balakrishnan Chandrasekaran, and Anja Feldmann. 2018. The quic fix for optimal video streaming. In Proceedings of the Workshop on the Evolution, Performance, and Interoperability of QUIC. 43--49. Digital Library Google Scholar [70] Gustavo Pantuza, Marcos AM Vieira, and Luiz FM Vieira. 2021. eQUIC gateway: Maximizing QUIC throughput using a gateway service based on eBPF XDP. In 2021 IEEE Symposium on Computers and Communications (ISCC). IEEE, 1--6. Crossref Google Scholar [71] Maxime Piraux, Quentin De Coninck, and Olivier Bonaventure. 2018. Observing the evolution of QUIC implementations. In Proceedings of the Workshop on the Evolution, Performance, and Interoperability of QUIC. 8--14. Digital Library Google Scholar [72] Alexander Rabitsch, Per Hurtig, and Anna Brunstrom. 2018. A stream-aware multipath QUIC scheduler for heterogeneous paths. In Proceedings of the Workshop on the Evolution, Performance, and Interoperability of QUIC. 29--35. Digital Library Google Scholar [73] Costin Raiciu, Mark Handley, and Damon Wischik. 2011. Coupled congestion control for multipath transport protocols. RFC 6356, IETF (2011). Google Scholar [74] Florentin Rochet, Emery Assogba, Maxime Piraux, Korian Edeline, Benoit Donnet, and Olivier Bonaventure. 2021. TCPLS: modern transport services with TCP and TLS. In Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies. 45--59. Digital Library Google Scholar [75] Jan Rüth, Ingmar Poese, Christoph Dietzel, and Oliver Hohlfeld. 2018. A First Look at QUIC in the Wild. In International Conference on Passive and Active Network Measurement. Springer, 255--268. Crossref Google Scholar [76] Jan Rüth, Konrad Wolsing, Klaus Wehrle, and Oliver Hohlfeld. 2019. Perceiving QUIC: Do users notice or even care?. In Proceedings of the 15th International Conference on Emerging Networking Experiments And Technologies. 144--150. Digital Library Google Scholar [77] Marten Seemann and Jana Iyengar. 2020. Automating quic interoperability testing. In Proceedings of the Workshop on the Evolution, Performance, and Interoperability of QUIC. 8--13. Digital Library Google Scholar [78] Tanya Shreedhar, Rohit Panda, Sergey Podanev, and Vaibhav Bajpai. 2021. Evaluating QUIC Performance Over Web, Cloud Storage, and Video Workloads. IEEE Transactions on Network and Service Management 19, 2 (2021), 1366--1381. Digital Library Google Scholar [79] Iraj Sodagar. 2011. The mpeg-dash standard for multimedia streaming over the internet. IEEE multimedia 18, 4 (2011), 62--67. Digital Library Google Scholar [80] Mukesh Soni and Brajendra Singh Rajput. 2021. Security and performance evaluations of QUIC protocol. In Data Science and Intelligent Applications. Springer, 457--462. Google Scholar [81] Lizhuang Tan, Wei Su, Yanwen Liu, Xiaochuan Gao, and Wei Zhang. 2021. DCQUIC: Flexible and Reliable Software-defined Data Center Transport. In IEEE INFOCOM 2021-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 1--8. Google Scholar [82] JingWang, Yunfeng Gao, and Chenren Xu. 2019. A multipath QUIC scheduler for mobile HTTP/2. In Proceedings of the 3rd Asia-Pacific Workshop on Networking 2019. 43--49. Google Scholar [83] Xiao Sophia Wang, Aruna Balasubramanian, Arvind Krishnamurthy, and David Wetherall. 2013. Demystifying Page Load Performance with {WProf}. In 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13). 473--485. Google Scholar [84] Xiao Sophia Wang, Aruna Balasubramanian, Arvind Krishnamurthy, and David Wetherall. 2014. How speedy is {SPDY}?. In 11th usenix symposium on networked systems design and implementation (nsdi 14). 387--399. Google Scholar [85] Damon Wischik, Costin Raiciu, Adam Greenhalgh, and Mark Handley. 2011. Design, implementation and evaluation of congestion control for multipath {TCP}. In 8th USENIX Symposium on Networked Systems Design and Implementation (NSDI 11). Google Scholar [86] Konrad Wolsing, Jan Rüth, Klaus Wehrle, and Oliver Hohlfeld. 2019. A performance perspective on web optimized protocol stacks: TCP TLS HTTP/2 vs. QUIC. In Proceedings of the Applied Networking Research Workshop. 1--7. Digital Library Google Scholar [87] Shichang Xu, Subhabrata Sen, and Z Morley Mao. 2020. CSI: Inferring mobile ABR video adaptation behavior under HTTPS and QUIC. In Proceedings of the Fifteenth European Conference on Computer Systems. 1--16. Digital Library Google Scholar [88] Yihui Yan and Zhice Yang. 2021. When QUIC's Connection Migration Meets Middleboxes A case study on mobile Wi-Fi hotspot. In 2021 IEEE Global Communications Conference (GLOBECOM). IEEE, 1--6. Digital Library Google Scholar [89] Xiangrui Yang, Lars Eggert, Jörg Ott, Steve Uhlig, Zhigang Sun, and Gianni Antichi. 2020. Making quic quicker with nic offload. In Proceedings of theWorkshop on the Evolution, Performance, and Interoperability of QUIC. 21--27. Digital Library Google Scholar [90] Shou-Cheng Yen, Ching-Ling Fan, and Cheng-Hsin Hsu. 2019. Streaming 360° videos to head-mounted virtual reality using DASH over QUIC transport protocol. In Proceedings of the 24th ACM Workshop on Packet Video. 7--12. Digital Library Google Scholar [91] Alexander Yu and Theophilus A Benson. 2021. Dissecting performance of production QUIC. In Proceedings of the Web Conference 2021. 1157--1168. Digital Library Google Scholar [92] Xumiao Zhang, Xiao Zhu, Yihua Ethan Guo, Feng Qian, and Z Morley Mao. 2019. Poster: characterizing performance and power for mmWave 5G on commodity smartphones. In Proceedings of the 2019 on Wireless of the Students, by the Students, and for the Students Workshop. 14--14. Digital Library Google Scholar [93] Zhilong Zheng, Yunfei Ma, Yanmei Liu, Furong Yang, Zhenyu Li, Yuanbo Zhang, Jiuhai Zhang, Wei Shi, Wentao Chen, Ding Li, et al. 2021. Xlink: Qoe-driven multi-path quic transport in large-scale video services. In Proceedings of the 2021 ACM SIGCOMM 2021 Conference. 418--432. Digital Library Google Scholar [94] Johannes Zirngibl, Philippe Buschmann, Patrick Sattler, Benedikt Jaeger, Juliane Aulbach, and Georg Carle. 2021. It's over 9000: analyzing early QUIC deployments with the standardization on the horizon. In Proceedings of the 21st ACM Internet Measurement Conference. 261--275. Digital Library Google Scholar Index Terms QUIC is not Quick Enough over Fast Internet Networks Network performance evaluation Network measurement Network performance analysis Network protocols Transport protocols Recommendations Poster: QUIC is not Quick Enough over Fast Internet IMC '23: Proceedings of the 2023 ACM on Internet Measurement Conference QUIC is a multiplexed transport-layer protocol over UDP and comes with enforced encryption. It is expected to be a game-changer in improving web application performance. Together with the network layer and layers below, UDP, QUIC, and HTTP/3 form a new ... Read More ECN with QUIC: Challenges in the Wild IMC '23: Proceedings of the 2023 ACM on Internet Measurement Conference TCP and QUIC can both leverage ECN to avoid congestion loss and its retransmission overhead. However, both protocols require support of their remote endpoints and it took two decades since the initial standardization of ECN for TCP to reach 80% ECN ... Read More A QUIC Implementation for ns-3 WNS3 '19: Proceedings of the 2019 Workshop on ns-3 Quick UDP Internet Connections (QUIC) is a recently proposed transport protocol, currently being standardized by the Internet Engineering Task Force (IETF). It aims at overcoming some of the shortcomings of TCP, while maintaining the logic related to ... Read More Comments Information & Contributors Information Published In WWW '24: Proceedings of the ACM Web Conference 2024 May 2024 4826 pages ISBN:9798400701719 DOI:10.1145/3589334 General Chairs: Tat-Seng Chua National University of Singapore , Chong-Wah Ngo Singapore Management University , Proceedings Chair: Roy Ka-Wei Lee Singapore University of Technology and Design , Program Chairs: Ravi Kumar Google , Hady W. Lauw Singapore Management University Copyright © 2024 ACM. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from [email protected]. Sponsors SIGWEB: ACM Special Interest Group on Hypertext, Hypermedia, and Web Publisher Association for Computing Machinery New York, NY, United States Publication History Published: 13 May 2024 Permissions Request permissions for this article. Request Permissions Check for updates Badges Artifacts Available / v1.1 Author Tags http network measurement quic transport web performance Qualifiers Research-article Funding Sources NSF (National Science Foundation) Conference WWW '24 Sponsor: SIGWEB WWW '24: The ACM Web Conference 2024 May 13 - 17, 2024 Singapore, Singapore Acceptance Rates Overall Acceptance Rate 1,899 of 8,196 submissions, 23% Contributors Other Metrics View Article Metrics Bibliometrics & Citations Bibliometrics Article Metrics 0 Total Citations 184 Total Downloads Downloads (Last 12 months)184 Downloads (Last 6 weeks)48 Reflects downloads up to 03 Sep 2024 Other Metrics View Author Metrics Citations View Options Get Access Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication View options PDF View or Download as a PDF file. PDF eReader View online with eReader. eReader Media Figures Other Tables Share Share Share this Publication link Copy Link Copied! Copying failed. Share on social media XLinkedInRedditFacebookemail",
    "commentLink": "https://news.ycombinator.com/item?id=41484991",
    "commentBody": "QUIC is not quick enough over fast internet (acm.org)605 points by Shank 16 hours agohidepastfavorite276 comments raggi 14 hours agoThere are a number of concrete problems: - syscall interfaces are a mess, the primitive APIs are too slow for regular sized packets (~1500 bytes), the overhead is too high. GSO helps but it’s a horrible API, and it’s been buggy even lately due to complexity and poor code standards. - the syscall costs got even higher with spectre mitigation - and this story likely isn’t over. We need a replacement for the BSD sockets / POSIX APIs they’re terrible this decade. Yes, uring is fancy, but there’s a tutorial level API middle ground possible that should be safe and 10x less overhead without resorting to uring level complexity. - system udp buffers are far too small by default - they’re much much smaller than their tcp siblings, essentially no one but experts have been using them, and experts just retune stuff. - udp stack optimizations are possible (such as possible route lookup reuse without connect(2)), gso demonstrates this, though as noted above gso is highly fallible, quite expensive itself, and the design is wholly unnecessarily intricate for what we need, particularly as we want to do this safely from unprivileged userspace. - several optimizations currently available only work at low/mid-scale, such as connect binding to (potentially) avoid route lookups / GSO only being applicable on a socket without high peer-competition (competing peers result in short offload chains due to single-peer constraints, eroding the overhead wins). Despite all this, you can implement GSO and get substantial performance improvements, we (tailscale) have on Linux. There will be a need at some point for platforms to increase platform side buffer sizes for lower end systems, high load/concurrency, bdp and so on, but buffers and congestion control are a high complex and sometimes quite sensitive topic - nonetheless, when you have many applications doing this (presumed future state), there will be a need. reply amluto 5 hours agoparentHi, Tailscale person! If you want a fairly straightforward improvement you could make: Tailscale, by default uses SOCK_RAW. And having any raw socket listening at all hurts receive performance systemwide: https://lore.kernel.org/all/CALCETrVJqj1JJmHJhMoZ3Fuj685Unf=... It shouldn’t be particularly hard to port over the optimization that prevents this problem for SOCK_PACKET. I’ll get to it eventually (might be quite a while), but I only care about this because of Tailscale, and I don’t have a ton of bandwidth. reply bradfitz 3 hours agorootparentBTW, that code changed just recently: https://github.com/tailscale/tailscale/commit/1c972bc7cbebfc... It's now a AF_PACKET/SOCK_DGRAM fd as it was originally meant to be. reply raggi 3 hours agorootparentprevVery interesting, thank you. We’ll take a look at this! reply JoshTriplett 13 hours agoparentprev> Yes, uring is fancy, but there’s a tutorial level API middle ground possible that should be safe and 10x less overhead without resorting to uring level complexity. I don't think io_uring is as complex as its reputation suggests. I don't think we need a substantially simpler low-level API; I think we need more high-level APIs built on top of io_uring. (That will also help with portability: we need APIs that can be most efficiently implemented atop io_uring but that work on non-Linux systems.) reply raggi 13 hours agorootparent> I don't think io_uring is as complex as its reputation suggests. uring is extremely problematic to integrate into many common application / language runtimes and it has been demonstrably difficult to integrate into linux safely and correctly as well, with a continual stream of bugs, security and policy control issues. in principle a shared memory queue is a reasonable basis for improving the IO cost between applications and IO stacks such as the network or filesystem stacks, but this isn't easy to do well, cf. uring bugs and binder bugs. reply arghwhat 11 hours agorootparentTwo things: One, uring is not extremely problematic to integrate, as it can be chained into a conventional event loop if you want to, or can even be fit into a conventionally blocking design to get localized syscall benefits. That is, you do not need to convert to a fully uring event loop design, even if that would be superior - and it can usually be kept entirely within a (slightly modified) event loop abstraction. The reason it has not yet been implemented is just priority - most stuff isn't bottlenecked on IOPS. Two, yes you could have e middle-ground. I assume the syscall overhead you call out is the need to send UDP packets one at a time through sendmsg/sendto, rather than doing one big write for several packets worth of data on TCP. An API that allowed you to provide a chain of messages, like sendmsg takes an iovec for data, is possible. But it's also possible to do this already as a tiny blocking wrapper around io_uring, saving you new syscalls. reply Veserv 10 hours agorootparentThe system call to send multiple UDP packets in a single call has existed since Linux 3.0 over a decade ago[1]: sendmmsg(). [1] https://man7.org/linux/man-pages/man2/sendmmsg.2.html reply justincormack 4 hours agorootparentAt one point if I remember it didnt actually work, it still just sent one message at a time and returned the length of the first piece of the iovec. Hopefully it got fixed. reply arghwhat 10 hours agorootparentprevAh nice, in that case OP's point about syscall overhead is entirely moot. :) That should really be in the `SEE ALSO` of `man 3 sendmsg`... reply evntdrvn 5 hours agorootparentpatches welcome :p reply londons_explore 8 hours agorootparentprevI think you need to look at a common use case and consider how many syscalls you'd like it to take and how many CPU cycles would be reasonable. Let's take downloading a 1MB jpeg image over QUIC and rendering it on the screen. I would hope that can be done in about 100k CPU cycles and 20 syscalls, considering that all the jpeg decoding and rendering is going to be hardware accelerated. The decryption is also hardware accelerated. Unfortunately, no network API allows that right now. The CPU needs to do a substantial amount of processing for every individual packet, in both userspace and kernel space, for receiving the packet and sending the ACK, and there is no 'bulk decrypt' non-blocking API. Even the data path is troublesome - there should be a way for the data to go straight from the network card to the GPU, with the CPU not even touching it, but we're far from that. reply arghwhat 7 hours agorootparentThere's a few issues here. 1. A 1 MB file is at the very least 64 individually encrypted TLS records (16k max size) sent in sequence, possibly more. So decryption 64 times is the maximum amount of bulk work you can do - this is done to allow streaming verification and decryption in parallel with the download, whereas one big block would have you wait for the very last byte before any processing could start. 2. TLS is still userspace and decryption does not involve the kernel, and thus no syscalls. The benefits of kernel TLS largely focus on servers sending files straight from disk, bypassing userspace for the entire data processing path. This is not really relevant receive-side for something you are actively decoding. 3. JPEG is, to my knowledge, rarely hardware offloaded on desktop, so no syscalls there. Now, the number of actual syscalls end up being dictated by the speed of the sender, and the tunable receive buffer size. The slower the sender, the more kernel roundtrips you end upo with, which allows you to amortize the processing over a longer period so everything is ready when the last packet is. For a fast enough sender with big enough receive buffers, this could be a single kernel roundtrip. reply miohtama 4 hours agorootparentJPEG is not a particular great example. However most video streams and partially hardware decoded. Usually you still need to decode part of the stream, namely entropy coding and metadata, first on the CPU. reply immibis 7 hours agorootparentprevThis system call you're asking for already exists - it's called sendmmsg. There is also recvmmsg. reply JoshTriplett 13 hours agorootparentprev> with a continual stream of bugs, security and policy control issues This has not been true for a long time. There was an early design mistake that made it quite prone to these, but that mistake has been fixed. Unfortunately, the reputational damage will stick around for a while. reply raggi 13 hours agorootparent13 CVEs so far this year afaik reply bonzini 13 hours agorootparentCVE numbers from the Linux CNA are bollocks. reply JoshTriplett 13 hours agorootparentThis conversation would be a good one to point them to to show that their policy is not just harmless point-proving, but in fact does cause harm. For context, to the best of my knowledge the current approach of the Linux CNA is, in keeping with long-standing Linux security policy of \"every single fix might be a security fix\", to assign CVEs regardless of whether something has any security impact or not. reply kuschku 10 hours agorootparentCVE assignment != security issue CVE numbers are just a way to ensure everyone is talking about the same bug. Not every security issue has a CVE, not every CVE is a security issue. Often, a regular bug turns out years later to have been a security issue, or a security issue turns out to have no security impact at all. If you want a central authority to tell you what to think, just use CVSS instead of the binary \"does it have a CVE\" metric. reply simiones 5 hours agorootparentThis is completely false. The CVE website defines these very clearly: > The mission of the CVE® Program is to identify, define, and catalog publicly disclosed cybersecurity vulnerabilities [emphasis mine]. In fact, CVE stands for \"Common Vulnerabilities and Exposures\", again showing that CVE == security issue. It's of course true that just because your code has an unpatched CVE doesn't automatically mean that your system is vulnerable - other mitigations can be in place to protect it. reply kuschku 4 hours agorootparentThat's the modern definition, which is rewriting history. Let's look at the actual, original definition: > The CVE list aspires to describe and name all publicly known facts about computer systems that could allow somebody to violate a reasonable security policy for that system There's also a decision from the editorial board on this, which said: > Discussions on the Editorial Board mailing list and during the CVE Review meetings indicate that there is no definition for a \"vulnerability\" that is acceptable to the entire community. At least two different definitions of vulnerability have arisen and been discussed. There appears to be a universally accepted, historically grounded, \"core\" definition which deals primarily with specific flaws that directly allow some compromise of the system (a \"universal\" definition). A broader definition includes problems that don't directly allow compromise, but could be an important component of a successful attack, and are a violation of some security policies (a \"contingent\" definition). > In accordance with the original stated requirements for the CVE, the CVE should remain independent of multiple perspectives. Since the definition of \"vulnerability\" varies so widely depending on context and policy, the CVE should avoid imposing an overly restrictive perspective on the vulnerability definition itself. For more details, see https://web.archive.org/web/20000526190637fw_/http://www.cve... and https://web.archive.org/web/20020617142755/http://cve.mitre.... Under this definition, any kernel bug that could lead to user-space software acting differently is a CVE. Similarly, all memory management bugs in the kernel justify a CVE, as they could be used as part of an exploit. reply simiones 4 hours agorootparentThose two links say that CVEs can be one of two categories: universal vulnerabilities or exposures. But the examples of exposures are not, in any way, \"any bug in the kernel\". They give specific examples of things which are known to make a system more vulnerable to attack, even if not everyone would agree that they are a problem. So yes, any CVE is supposed to be a security problem, and it has always been so. Maybe not for your specific system or for your specific security posture, but for someone's. Extending this to any bugfix is a serious misunderstanding of what an \"exposure\" means, and it is a serious difference from other CNAs. Linux CNA-assigned CVEs just can't be taken as seriously as normal CNAs. reply skywhopper 8 hours agorootparentprevThat’s definitely not the understanding that literally anyone outside the Linux team has for what a CVE is, including the people who came up with them and run the database. Overloading a well-established mechanism of communicating security issues to just be a registry of Linux bugs is an abuse of an important shared resource. Sure “anything could be a security issue” but in practice, most bugs aren’t, and putting meaningless bugs into the international security issue database is just a waste of everyone’s time and energy to make a very stupid point. reply kuschku 6 hours agorootparent> including the people who came up with them How do you figure that? The original definition of CVE is exactly the same as how Linux approaches it. Sure, in recent years security consultants have been overloading CVE to mean something else, but that's something to fix, not to keep. reply jiripospisil 6 hours agorootparentCan you post the original definition? reply vel0city 5 hours agorootparentCommon Vulnerabilities and Exposures reply jiripospisil 4 hours agorootparentRight but I was hoping for a definition which supports OP's claim that \"CVE assignment != security issue\". reply kuschku 4 hours agorootparentThen check out these definitions, from 2000, defined by the CVE editorial board: > The CVE list aspires to describe and name all publicly known facts about computer systems that could allow somebody to violate a reasonable security policy for that system As well as: > Discussions on the Editorial Board mailing list and during the CVE Review meetings indicate that there is no definition for a \"vulnerability\" that is acceptable to the entire community. At least two different definitions of vulnerability have arisen and been discussed. There appears to be a universally accepted, historically grounded, \"core\" definition which deals primarily with specific flaws that directly allow some compromise of the system (a \"universal\" definition). A broader definition includes problems that don't directly allow compromise, but could be an important component of a successful attack, and are a violation of some security policies (a \"contingent\" definition). > In accordance with the original stated requirements for the CVE, the CVE should remain independent of multiple perspectives. Since the definition of \"vulnerability\" varies so widely depending on context and policy, the CVE should avoid imposing an overly restrictive perspective on the vulnerability definition itself. Under this definition, any kernel bug that could lead to user-space software acting differently is a CVE. Similarly, all memory management bugs in the kernel justify a CVE, as they could be used as part of an exploit. reply jiripospisil 4 hours agorootparent> to violate a reasonable security policy for that system > with specific flaws that directly allow some compromise of the system > important component of a successful attack, and are a violation of some security policies All of these are talking about security issues, not \"acting differently\". reply kuschku 3 hours agorootparent> important component of a successful attack, and are a violation of some security policies If the kernel returned random values from gettime, that'd lead to tls certificate validation not being reliable anymore. As result, any bug in gettime is certainly worthy of a CVE. If the kernel shuffled filenames so they'd be returned backwards, apparmor and selinux profiles would break. As result, that'd be worthy of a CVE. If the kernel has a memory corruption, use after free, use of uninitialized memory or refcounting issue, that's obviously a violation of security best practices and can be used as component in an exploit chain. Can you now see how almost every kernel bug can and most certainly will be turned into a security issue at some point? reply josefx 3 hours agorootparentprev> All of these are talking about security issues, not \"acting differently\". Because no system has been ever taken down by code that behaved different from what it was expected to do? Right? Like http desync attacks, sql escape bypasses, ... . Absolutely no security issue going to be caused by a very minor and by itself very secure difference in behavior. reply immibis 7 hours agorootparentprevAs I understand it, they adopted this policy because the other policy was also causing harm. They are right, by the way. When CVEs were used for things like Heartbleed they made sense - you could point to Heartbleed's CVE number and query various information systems about vulnerable systems. When every single possible security fix gets one, AND automated systems are checking the you've patched every single one or else you fail the audit (even ones completely irrelevant to the system, like RCE on an embedded device with no internet access) the system is not doing anything useful - it's deleting value from the world and must be repaired or destroyed. reply hifromwork 5 hours agorootparentThe problem here are the automated systems and braindead auditors, not the CVE system itself. reply immibis 0 minutes agorootparentWell, the CVE system itself is only about assigning identifiers, and assigning identifiers unnecessarily couldn't possibly hurt anyone, who isn't misusing the system, unless they're running out of identifiers. di4na 13 hours agorootparentprevI would not call it harm. The use of uring in higher level languages is definitely prone to errors, bugs and security problems reply JoshTriplett 12 hours agorootparentSee the context I added to that comment; this is not about security issues, it's about the Linux CNA's absurd approach to CVE assignment for things that aren't CVEs. reply tialaramex 6 hours agorootparentI don't agree that it's absurd. I would say it reflects a proper understanding of their situation. You've doubtless heard Tony Hoare's \"There are two ways to write code: write code so simple there are obviously no bugs in it, or write code so complex that there are no obvious bugs in it.\". Linux is definitely in the latter category, it's now such a sprawling system that determining whether a bug \"really\" has security implications is no long a reasonable task compared to just fixing the bug. The other reason is that Linux is so widely used that almost no assumption made to simplify that above task is definitely correct. reply JoshTriplett 6 hours agorootparentThat's fine, except that it is thus no longer meaningful to compare CVE count. reply hifromwork 4 hours agorootparentI like CVEs, I think Linux approach to CVEs is stupid, but also it was never meaningful to compare CVE count. But I guess it's hard to make people stop doing that, and that's the reason Linux does the thing it does out of spite. reply raggi 13 hours agorootparentprevthis is a bit of a distraction, sure the leaks and some of the deadlocks are fairly uninteresting, but the toctou, overflows, uid race/confusion and so on are real issues that shouldn't be dismissed as if they don't exist. reply jeffparsons 13 hours agorootparentprevI find this surprising, given that my initial response to reading the iouring design was: 1. This is pretty clean and straightforward. 2. This is obviously what we need to decouple a bunch of things without the previous downsides. What has made it so hard to integrate it into common language runtimes? Do you have examples of where there's been an irreconcilable \"impedance mismatch\"? reply raggi 13 hours agorootparenthttps://github.com/tailscale/tailscale/pull/2370 was a practical drive toward this, will not proceed on this path. much more approachable, boats has written about challenges integrating in rust: https://without.boats/tags/io-uring/ in the most general form: you need a fairly \"loose\" memory model to integrate the \"best\" (performance wise) parts, and the \"best\" (ease of use/forward looking safety) way to integrate requires C library linkage. This is troublesome in most GC languages, and many managed runtimes. There's also the issue that uring being non-portable means that the things it suggests you must do (such as say pinning a buffer pool and making APIs like read not immediate caller allocates) requires a substantially separate API for this platform than for others, or at least substantial reworks over all the existing POSIX modeled APIs - thus back to what I said originally, we need a replacement for POSIX & BSD here, broadly applied. reply gpderetta 7 hours agorootparentI can see how a zero-copy API would be hard to implement on some languages, but you could still implement something on top of io_uring with posix buffer copy semantics , while using batching to decrease syscall overhead. Zero-copy APIs will necessarily be tricky to implement and use, especially on memory safe languages. reply gmokki 3 hours agorootparentI think most GC languages support native/pinned me(at least Java and C# do memory to support talking to kernel or native libraries. The APIs are even quite nice. reply neonsunset 3 hours agorootparentJava's off-heap memory and memory segment API is quite dreadful and on the slower side. C# otoh gives you easy and cheap object pinning, malloc/free and stack-allocated buffers. reply withoutboats3 1 hour agorootparentprevRust's async model can support io-uring fine, it just has to be a different API based on ownership instead of references. (That's the conclusion of my posts you link to.) reply asveikau 3 hours agorootparentprevI read the oldest of those blog posts the closest. Seems like the author points out two things: 1. The lack of rust futures supporting manual cancellation. That doesn't seem like an inevitable choice by rust. 2. Sharing buffers with kernel mode. This is probably a bigger topic. reply anarazel 4 hours agorootparentprevFWIW, the biggest problem I've seen with efficiently using io_uring for networking is that none of the popular TLS libraries have a buffer ownership model that really is suitable for asynchronous network IO. What you'd want is the ability to control the buffer for the \"raw network side\", so that asynchronous network IO can be performed without having to copy between a raw network buffer and buffers owned by the TLS library. It also would really help if TLS libraries supported processing multiple TLS records in a batched fashion. Doing roundtrips between apptls libraryuserspace network bufferkernelHW for every 16kB isn't exactly efficient. reply lukeh 13 hours agorootparentprevasync/await io_uwring wrappers for languages such as Swift [1] and Rust [2] [3] can improve usability considerably. I'm not super familiar with the Rust wrappers but, I've been using IORingSwift for socket, file and serial I/O for some time now. [1] https://github.com/PADL/IORingSwift [2] https://github.com/bytedance/monoio [3] https://github.com/tokio-rs/tokio-uring reply Diggsey 9 hours agoparentprevHistorically there have been too many constraints on the Linux syscall interface: - Performance - Stability - Convenience - Security This differs from eg. Windows because on Windows the stable interface to the OS is in user-space, not tied to the syscall boundary. This has resulted in unfortunate compromises in the design of various pieces of OS functionality. Thankfully things like futex and io-uring have dropped the \"convenience\" constraint from the syscall itself and moved it into user-space. Convenience is still important, but it doesn't need to be a constraint at the lowest level, and shouldn't compromise the other ideals. reply modeless 10 hours agoparentprevSeems to me that the real problem is the 1500 byte MTU that hasn't increased in practice in over 40 years. reply throw0101c 7 hours agorootparent> Seems to me that the real problem is the 1500 byte MTU that hasn't increased in practice in over 40 years. As per a sibling comment, 1500 is just for Ethernet (the default, jumbo frames being able to go to (at least) 9000). But the Internet is more than just Ethernet. If you're on DSL, then RFC 2516 states that PPPoE's MTU is 1492 (and you probably want an MSS of 1452). The PPP, L2TP, and ATM AAL5 standards all have 16-bit length fields allowing for packets up 64k in length. GPON ONT MTU is 2000. The default MTU for LTE is 1428. If you're on an HPC cluster, there's a good chance you're using Infiniband, which goes to 4096. What are size do you suggest everyone on the planet go to? Who exactly is going to get everyone to switch to the new value? reply fallingsquirrel 4 hours agorootparent> What are size do you suggest everyone on the planet go to? 65536 > Who exactly is going to get everyone to switch to the new value? The same people who got everyone to switch to IPv6. It's a missed opportunity that these migrations weren't done at the same time imho. It'll take a few decades, sure, but that's how big migrations go. What's the alternative? Making no improvements at all, forever? reply 0xbadcafebee 4 hours agorootparent> got everyone to switch to IPv6 I have some bad news... > What's the alternative? Making no improvements at all, forever? No, sadly. The alternative is what the entire tech world has been doing for the past 15 years: shove \"improvements\" inside whatever crap we already have because nobody wants to replace the crap. If IPv6 were made today, it would be tunneled inside an HTTP connection. All the new apps would adopt it, the legacy apps would be abandoned or have shims made, and the whole thing would be inefficient and buggy, but adopted. Since poking my head outside of the tech world and into the wider world, it turns out this is how most of the world works. reply MerManMaid 1 hour agorootparent>If IPv6 were made today, it would be tunneled inside an HTTP connection. All the new apps would adopt it, the legacy apps would be abandoned or have shims made, and the whole thing would be inefficient and buggy, but adopted. Since poking my head outside of the tech world and into the wider world, it turns out this is how most of the world works. What you're suggesting here wouldn't work, wrapping all the addressing information inside HTTP which relies on IP for delivery does not work. It would be the equivalent of sealing all the addressing information for a letter you'd like to send inside the envelope. reply throw0101c 59 minutes agorootparentprev> If IPv6 were made today, it would be tunneled inside an HTTP connection. Given that one of the primary goals of IPv6 was increased address space, how would putting IPv6 in an HTTP connection riding over IPv4 solve that? reply Hikikomori 5 hours agorootparentprevThe internet is mostly ethernet these days (ISP core/edge), last mile connections like DSL and cable already handle a smaller MTU so should be fine with a bigger one. reply throw0101c 2 hours agorootparent> The internet is mostly ethernet these days […] Except for the bajillion mobile devices in people's pockets/purses. reply cesarb 5 hours agorootparentprev> The internet is mostly ethernet these days (ISP core/edge), A lot of that ISP edge is CPEs with WiFi, which AFAIK limits the MTU to 2304 bytes. reply p_l 9 hours agorootparentprevFor all practical purposes, the internet MTU is lower than ethernet default MTU. Sometimes for ease of mind I end up clamping it to v6 minimum (1280) just in case . reply j16sdiz 9 hours agorootparentprevThe real problem is some so called \"sysadmin\" drop all ICMP, breaking path mtu discovery. reply icedchai 4 hours agorootparentThe most secure network is one that doesn't pass any traffic at all. ;) reply asmor 10 hours agorootparentprevThat's on the list that right after we all migrate to IPv6. reply SomaticPirate 14 hours agoparentprevWhat is GSO? reply jesperwe 14 hours agorootparentGeneric Segmentation Offload \"GSO gains performance by enabling upper layer applications to process a smaller number of large packets (e.g. MTU size of 64KB), instead of processing higher numbers of small packets (e.g. MTU size of 1500B), thus reducing per-packet overhead.\" reply underdeserver 10 hours agorootparentThis is more the result. Generally today an Ethernet frame, which is the basic atomic unit of information over the wire, is limited to 1500 bytes (the MTU, or Maximum Transmission Unit). If you want to send more - the IP layer allows for 64k bytes per IP packet - you need to split the IP packet into multiple (64k / 1500 plus some header overhead) frames. This is called segmentation. Before GSO the kernel would do that which takes buffering and CPU time to assemble the frame headers. GSO moves this to the ethernet hardware, which is essentially doing the same thing only hardware accelerated and without taking up a CPU core. reply chaboud 14 hours agorootparentprevLikely Generic Segmentation Offload (if memory serves), which is a generalization of TCP segmentation offload. Basically (hyper simple), the kernel can lump stuff together when working with the network interface, which cuts down on ultra slow hardware interactions. reply raggi 14 hours agorootparentit was originally for the hardware, but it's also valuable on the software side as the cost of syscalls is far too high for packet sized transactions reply throwaway8481 14 hours agorootparentprevGeneric Segmentation Offload https://www.kernel.org/doc/html/latest/networking/segmentati... reply thorncorona 14 hours agorootparentprevpresumably generic segmentation offloading reply USiBqidmOOkAqRb 10 hours agorootparentprevShipping? Government services online? Piedmont airport? Alcoholics anonymous? Obviously not. Please introduce your initialisms, if it's not guaranteed that first result in a search will be correct. reply mh- 2 hours agorootparent> first result in a search will be correct Searching for GSO network gives you the correct answer in the first result. I'd consider that condition met. reply cookiengineer 14 hours agoparentprevSay what you want but I bet we'll see lots of eBPF modules being loaded in the future for the very reason you're describing. An ebpf quic module? Why not! And that scares me, because there's not a single tool that has this on its radar for malware detection/prevention. reply raggi 13 hours agorootparentwe can consider ebpf \"a solution\" when there's even a remote chance you'll be able to do it from an unentitled ios app. somewhat hyperbole, but the point is, this problem is a problem for userspace client applications, and bpf isn't a particularly \"good\" solution for servers either, it's high cost of authorship for a problem that is easily solvable with a better API to the network stack. reply mgaunard 10 hours agorootparentebpf is linux technology, you will never be able to do it from iOS. reply dan-robertson 8 hours agorootparenthttps://github.com/microsoft/ebpf-for-windows reply quotemstr 12 hours agoparentprev> Yes, uring is fancy, but there’s a tutorial level API middle ground possible that should be safe and 10x less overhead without resorting to uring level complexity. And the kernel has no business providing this middle-layer API. Why should it? Let people grab whatever they need from the ecosystem. Networking should be like Vulkan: it should have a high-performance, flexible API at the systems level with being \"easy to use\" a non-goal --- and higher-level facilities on top. reply astrange 8 hours agorootparentThe kernel provides networking because it doesn't trust userspace to do it. If you provided a low level networking API you'd have to verify everything a client sends is not malicious or pretending to be from another process. And for the same reason, it'd only work for transmission, not receiving. That and nobody was able to get performant microkernels working at the time, so we ended up with everything in the monokernel. If you do trust the client processes then it could be better to just have them read/write IP packets though. reply namibj 7 hours agorootparentprevAlso, it is really easy to do the normal IO \"syscall wrappers\" on top of io_uring instead, even easily exposing a very simple async/await variant of them that splits out the \"block on completion (after which just like normal IO the data buffer has been copied into kernel space)\" from the rest of the normal IO syscall, which allow pipelining & coalescing of requests. reply JoshTriplett 16 hours agoprevIn the early days of QUIC, many people pointed out that the UDP stack has had far far less optimization put into it than the TCP stack. Sure enough, some of the issues identified here arise because the UDP stack isn't doing things that it could do but that nobody has been motivated to make it do, such as UDP generic receive offload. Papers like this are very likely to lead to optimizations both obvious and subtle. reply Animats 15 hours agoparentWhat is UDP offload going to do? UDP barely does anything but queue and copy. Linux scheduling from packet-received to thread has control is not real-time, and if the CPUs are busy, may be rather slow. That's probably part of the bottleneck. The embarrassing thing is that QUIC, even in Google's own benchmarks, only improved performance by about 10%. The added complexity probably isn't worth the trouble. However, it gave Google control of more of the stack, which may have been the real motivation. reply amluto 14 hours agorootparentLast I looked (several months ago), Linux's UDP stack did not seemed well tuned in its memory management accounting. For background, the mental model of what receiving network data looks like in userspace is almost completely backwards compared to how general-purpose kernel network receive actually works. User code thinks it allocates a buffer (per-socket or perhaps a fancier io_uring scheme), then receives packets into that buffer, then processes them. The kernel is the other way around. The kernel allocates buffers and feeds pointers to those buffers to the NIC. The NIC receives packets and DMAs them into the buffers, then tells the kernel. But the NIC and the kernel have absolutely no concept of which socket those buffers belong to until after they are DMAed into the buffers. So the kernel cannot possibly map received packets to the actual recipient's memory. So instead, after identifying who owns a received packet, the kernel retroactively charges the recipient for the memory. This happens on a per-packet basis, it involves per-socket and cgroup accounting, and there is no support for having a socket \"pre-allocate\" this memory in advance of receiving a packet. So the accounting is gnarly, involves atomic operations, and seems quite unlikely to win any speed awards. On a very cursory inspection, the TCP code seemed better tuned, and it possibly also won by generally handling more bytes per operation. Keep in mind that the kernel can't copy data to application memory synchronously -- the application memory might be paged out when a packet shows up. So instead the whole charging dance above happens immediately when a packet is received, and the data is copied later on. For quite a long time, I've thought it would be nifty if there was a NIC that kept received data in its own RAM and then allowed it to be efficiently DMAed to application memory when the application was ready for it. In essence, a lot of the accounting and memory management logic could move out of the kernel into the NIC. I'm not aware of anyone doing this. reply rkagerer 34 minutes agorootparentWhy don't we eliminate the initial step of an app reserving a buffer, keep each packet in its own buffer, and once the socket it belongs to is identified hand a pointer and ownership of that buffer back to the app? If buffers can be of fixed (max) size, you could still allow the NIC to fill a bunch of them in one go. reply JoshTriplett 14 hours agorootparentprev> For quite a long time, I've thought it would be nifty if there was a NIC that kept received data in its own RAM and then allowed it to be efficiently DMAed to application memory when the application was ready for it. I wonder if we could do a more advanced version of receive-packet steering that sufficiently identifies packets as definitely for a given process and DMAs them directly to that process's pre-provided buffers for later notification? In particular, can we offload enough information to a smart NIC that it can identify where something should be DMAed to? reply mgaunard 10 hours agorootparentMost advanced NICs support flow steering, which makes the NIC write to different buffers depending on the target port. In practice though, you only have a limited amount of these buffers, and it causes complications if multiple processes need to consume the same multicast. reply eptcyka 4 hours agorootparentMulticast may well be shitcanned to an expensive slow path, given that multicast is rarely used for high bandwidth scenarios, especially when multiple processes need to receive the same packet. reply amluto 13 hours agorootparentprevI don’t think the result would be compatible with the socket or io_uring API, but maybe io_uring could be extended a bit. Basically the kernel would opportunistically program a “flow director” or similar rule to send packets to special rx queue, and that queue would point to (pinned) application memory. Getting this to be compatible with iptables/nftables would be a mess or maybe entirely impossible. I’ve never seen the accelerated steering stuff work well in practice, sadly. The code is messy, the diagnostics are basically nonexistent, and it’s not clear to me that many drivers support it well. reply veber-alex 8 hours agorootparentprevHave you looked into NVIDIA VMA? https://docs.nvidia.com/networking/display/vmav9860/introduc... reply derefr 14 hours agorootparentprevPresuming that this is a server that has One (public) Job, couldn't you: 1. dedicate a NIC to the application; 2. and have the userland app open a packet socket against the NIC, to drink from its firehose through MMIO against the kernel's own NIC DMA buffer; ...all without involving the kernel TCP/IP (or in this case, UDP/IP) stack, and any of the accounting logic squirreled away in there? (You can also throw in a BPF filter here, to drop everything except UDP packets with the expected specified ip:port — but if you're already doing more packet validation at the app level, you may as well just take the whole firehose of packets and validate them for being targeted at the app at the same time that they're validated for their L7 structure.) reply amluto 13 hours agorootparentI think DPDK does something like this. The NIC is programmed to aim the packets in question at a specific hardware receive queue, and that queue is entirely owned by a userspace program. A lot of high end NICs support moderately complex receive queue selection rules. reply SSLy 10 hours agorootparentprev> 1. dedicate a NIC to the application; you need to respond to ICMPs which have different proto/header number than UDP or TCP. reply fragmede 14 hours agorootparentprevRDMA is common for high performance applications but it doesn't work over the Internet. reply throw0101c 7 hours agorootparent> RDMA is common for high performance applications but it doesn't work over the Internet. RoCEv2 is routable. * https://en.wikipedia.org/wiki/RDMA_over_Converged_Ethernet * https://docs.nvidia.com/networking/display/winofv55053000/ro... Of course you're going to get horrible latency because of speed-of-light limitations, so the definition of \"work\" may be weak, but data should be able to be transmitted. reply Danieru 14 hours agorootparentprevIt's a good thing the NIC is connected over pcie then. reply shaklee3 14 hours agorootparentprevYou can do GPUdirect over the Internet without RDMA though. reply jpgvm 9 hours agorootparentGPUDirect relies on the PeerDirect extensions for RDMA and are thus an extension to the RDMA verbs, not a separate an independent thing that works without RDMA. reply shaklee3 5 hours agorootparentAgain, you can do what I said. You may be using different terminology, but you can do GPUdirect in dpdk without rdma reply raggi 14 hours agorootparentprevUDP offload gets you implicitly today: - 64 packets per syscall, which is enough data to amortize the syscall overhead - a single packet is not. - UDP offload optionally lets you defer checksum computation, often offloading it to hardware. - UDP offload lets you skip/reuse route lookups for subsequent packets in a bundle. What UDP offload is no good for though, is large scale servers - the current APIs only work when the incoming packet chains neatly organize into batches per peer socket. If you have many thousands of active sockets you’ll stop having full bundles and the overhead starts sneaking back in. As I said in another thread, we really need a replacement for the BSD APIs here, they just don’t scale for modern hardware constraints and software needs - much too expensive per packet. reply infogulch 14 hours agorootparentprevIn my head the main benefit of QUIC was always multipath, aka the ability to switch interfaces on demand without losing the connection. There's MPTCP but who knows how viable it is. reply Sesse__ 4 hours agorootparentApple's Siri is using MPTCP, so it is presumably viable. reply jshier 2 hours agorootparentIt requires explicit backend support, and Apple supports it for many of their services, but I've never seen another public API that does. Anyone have any examples? reply mh- 2 hours agorootparentLast I looked into this (many years), ELB/GLBs didn't support it on AWS/GCP respectively. That prevented us from further considering implementing it at the time (mobile app -> AWS-hosted EC2 instances behind an ELB). Not sure if that's changed, but at the time it wasn't worth having to consider rolling our own LBs. To answer your original question, no, I haven't (knowingly) seen it on any public APIs. reply modeless 10 hours agorootparentprevIs that actually implemented and working in practice? My connection still hangs whenever my wifi goes out of range... reply rocqua 13 hours agorootparentprevMptcp sees use in the Telco space, so they probably know. reply majke 7 hours agorootparentprev> What is UDP offload going to do? Handling ACK packets in kernelspace would be one thing - helping for example RTT estimation. With userspace stack ACK's are handled in application and are subject to scheduler, suffering a lot on a loaded system. reply morning-coffee 4 hours agorootparentThere are no ACKs inherent in the UDP protocol, so \"UDP offload\" is not where the savings are. There are ACKs in the QUIC protocol and they are carried by UDP datagrams which need to make their way up to user land to be processed, and this is the crux of the issue. What is needed is for QUIC offload to be invented/supported by HW so that most of the high-frequency/tiny-packet processing happens there, just as it does today for TCP offload. TCP large-send and large-receive offload is what is responsible for all the CPU savings as the application deals in 64KB or larger send/receives and the segmentation and receive coalescing all happen in hardware before an interrupt is even generated to involve the kernel, let alone userland. reply apitman 14 hours agorootparentprevDitching head of line blocking is potentially a big win, but I really wish it wouldn't have come with so much complexity. reply JoshTriplett 15 hours agorootparentprevAmong other things, GRO (receive offloading) means you can get more data off of the network card in fewer operations. Linux has receive packet steering, which can help with getting packets from the network card to the right CPU and the right userspace thread without moving from one CPU's cache to another. reply Vecr 15 hours agorootparentprevI think one of the original drivers was the ability to quickly tweak parameters, after Linux rejected what I think was userspace adjustment of window sizing to be more aggressive than the default. The Linux maintainers didn't want to be responsible for congestion collapse, but UDP lets you spray packets from userspace, so Google went with that. reply 10000truths 13 hours agorootparentprevBulk throughout isn't on par with TLS mainly because NICs with dedicated hardware for QUIC offload aren't commercially available (yet). Latency is undoubtedly better - the 1-RTT QUIC handshake substantially reduces time-to-first-byte compared to TLS. reply morning-coffee 4 hours agoparentprevThe UDP optimizations are already there and have been pretty much wrung out. https://www.fastly.com/blog/measuring-quic-vs-tcp-computatio... has good details and was done almost five years ago. The solution isn't in more UDP offload optimizations as there aren't any semantics in UDP that are expensive other than the quantity and frequency of datagrams to be processed in the context of the QUIC protocol that uses UDP as a transport. QUIC's state machine needs to see every UDP datagram carrying QUIC protocol messages in order to move forward. Just like was done for TCP offload more than twenty years ago, portions of QUIC state need to move and be maintained in hardware to prevent the host from having to see so many high-frequency tiny state updates messages. reply RachelF 15 hours agoparentprevAlso bear in mind that many of today's network cards have processors in them that handle much of the TCP/IP overhead. reply dilyevsky 4 hours agorootparentNot just that but TLS too. Starting ConnectX-5 i think you can push kTLS down to nic. Dont think there’s a QUIC equivalent for this reply kccqzy 14 hours agorootparentprevThat's mostly still for the data center. Which end-user network cards that I can buy can do TCP offloading? reply throw0101c 7 hours agorootparent> Which end-user network cards that I can buy can do TCP offloading? Intel's I210 controllers support offloading: > Other performance-enhancing features include IPv4 and IPv6 checksum offload, TCP/UDP checksum offload, extended Tx descriptors for more offload capabilities, up to 256 KB TCP segmentation (TSO v2), header splitting, 40 KB packet buffer size, and 9.5 KB Jumbo Frame support. * https://cdrdv2-public.intel.com/327935/327935-Intel%20Ethern... And cost US$ 22: * https://www.amazon.com/dp/B0728289M7/ reply vel0city 5 hours agorootparentprevPractically every on-board network adapter I've had for over a decade has had TCP offload support. Even the network adapter on my cheap $300 Walmart laptop has hardware TCP offload support. reply phil21 14 hours agorootparentprevUnless I’m missing something here, pretty much any Intel nic released in the past decade should support tcp offload. I imagine the same is true for Broadcom and other vendors as well, but I don’t have something handy to check. reply JoshTriplett 14 hours agorootparentprevSome wifi cards offload a surprising amount in order to do wake-on-wireless, but that's not for performance. reply nextaccountic 10 hours agoparentprevDo you mean that under the same workload, TCP will perform better? reply skywhopper 7 hours agoparentprevThe whole reason QUIC even exists in user space is because its developers were trying to hack a quick speed-up to HTTP rather than actually do the work to improve the underlying networking fundamentals. In this case the practicalities seem to have caught them out. If you want to build a better TCP, do it. But hacking one in on top of UDP was a cheat that didn’t pay off. Well, assuming performance was even the actual goal. reply kbolino 6 hours agorootparentIt already exists, it's called SCTP. It doesn't work over the Internet because there's too much crufty hardware in the middle that will drop it instead of routing it. Also, Microsoft refused to implement it in Windows and also banned raw sockets so it's impossible to get support for it on that platform without custom drivers that practically nobody will install. I don't know how familiar the developers of QUIC were with SCTP in particular but they were definitely aware of the problems that prevented a better TCP from existing. The only practical solution is to build something on top of UDP, but if even that option proves unworkable, then the only other possibility left is to fragment the Internet. reply osmarks 7 hours agorootparentprevThey couldn't have built it on anything but UDP because the world is now filled with poorly designed firewall/NAT middleboxes which will not route things other than TCP, UDP and optimistically ICMP. reply adgjlsfhk1 6 hours agorootparentprevcounterpoint, it is paying off, just taking a while. this paper wasn't \"quick is bad\" it was \"OSes need more optimization for quick to be as fast as https\" reply guappa 5 hours agorootparentThe whole point of the project was for it to be faster without touching the OS… reply IshKebab 1 hour agorootparentprevYeah they probably wanted a protocol that would actually work on the wild internet with real firewalls and routers and whatnot. The only option if you want that is building on top of UDP or TCP and you obviously can't use TCP. reply sbstp 15 hours agoprevEven HTTP/2 seems to have been rushed[1]. Chrome has removed support for server push. Maybe more thought should be put into these protocols instead of just rebranding whatever Google is trying to impose on us. [1] https://varnish-cache.org/docs/trunk/phk/h2againagainagain.h... reply KaiserPro 11 hours agoparentHTTP2 was a prototype that was designed by people who either assumed that mobile internet would get better much quicker than it did, or who didn't understand what packet loss did to throughput. I suspect part of the problem is that some of the rush is that people at major companies will get a promotion if they do \"high impact\" work out in the open. HTTP/2 \"solves head of line blocking\" which is doesn't. It exchanged an HTTP SSL blocking issues with TCP on the real internet issue. This was predicted at the time. The other issue is that instead of keeping it a simple protocol, the temptation to add complexity to aid a specific use case gets too much. (It's human nature I don't blame them) reply pornel 7 hours agorootparentH/2 doesn't solve blocking it on the TCP level, but it solved another kind of blocking on the protocol level by having multiplexing. H/1 pipelining was unusable, so H/1 had to wait for a response before sending the next request, which added a ton of latency, and made server-side processing serial and latency-sensitive. The solution to this was to open a dozen separate H/1 connections, but that multiplied setup cost, and made congestion control worse across many connections. reply KaiserPro 6 hours agorootparent> it solved another kind of blocking on the protocol level Indeed! and it works well on low latency, low packet loss networks. On high packet loss networks, it performs worse than HTTP1.1. Moreover it gets increasingly worse the larger the page the request is serving. We pointed this out at the time, but were told that we didn't understand the web. > H/1 pipelining was unusable, Yup, but think how easy it would be to create http1.2 with better spec for pipe-lining. (but then why not make changes to other bits as well, soon we get HTTP2!) But of course pipelining only really works in a low packet loss network, because you get head of line blocking. > open a dozen separate H/1 connections, but that multiplied setup cost Indeed, that SSL upgrade is a pain in the arse. But connections are cheap to keep open. So with persistent connections and pooling its possible to really nail down the latency. Personally, I think the biggest problem with HTTP is that its a file access protocol, a state interchange protocol and an authentication system. I would tentatively suggest that we adopt websockets to do state (with some extra features like optional schema sharing {yes I know thats a bit of enanthema}) Make http4 a proper file sharing prototcol and have a third system for authentication token generation, sharing and validation. However the real world says that'll never work. So connection pooling over TCP with quick start TLS would be my way forward. reply surajrmal 15 hours agoparentprevIt's okay to make mistakes, that's how you learn and improve. Being conservative has drawbacks of its own. Id argue we need more parties involved earlier in the process rather than just time. reply zdragnar 14 hours agorootparentIt's a weird balancing act. On the other hand, waiting for everyone to agree on everything means that the spec will take a decade or two for everyone to come together, and then all the additional time for everyone to actively support it. AJAX is a decent example. Microsoft's Outlook Web Access team implemented XMLHTTP as an activex thing for IE 5 and soon the rest of the vendors adopted it as a standard thing as XmlHttpRequest objects. In fact, I suspect the list of things that exist in browsers because one vendor thought it was a good idea and everyone hopped on board is far, far longer than those designed by committee. Often times, the initially released version is not exactly the same that everyone standardized on, but they all get to build on the real-world consequences of it. I happen to like the TC39 process https://tc39.es/process-document/ which requires two live implementations with use in the wild for something to get into the final stage and become an official part of the specification. It is obviously harder for something like a network stack than a JavaScript engine to get real world use and feedback, but it has helped to keep a lot of the crazier vendor specific features at bay. reply arcbyte 4 hours agorootparentprevIt's okay to make mistakes, but its not okay to ignore the broad consensus that HTTP2 was TERRIBLY designed and then admit it 10 years later as if it was unknowable. We knew it was bad. reply liveoneggs 7 hours agoparentprevPart of/Evidence of the Google monopoly position in the web stack are these big beta tests of protocols they cook up for whatever reason. reply surajrmal 5 hours agorootparentThis is a weak argument that simply caters to the ongoing HN hivemind opinion. While Google made the initial proposal, many other parties did participate in getting quic standardized. The industry at large was in favor. reply oefrha 4 hours agorootparentIETF QUIC ended up substantially different from gQUIC. People who say Google somehow single-handedly pushed things through probably haven’t read anything along the standardization process, but of course everyone has to have an opinion about all things Google. reply est 12 hours agoparentprevI don't blame Google, all major version changes are very brave, I praised them. The problem is lack of non-google protocols for competition. reply M2Ys4U 8 hours agoprev>The results show that QUIC and HTTP/2 exhibit similar performance when the network bandwidth is relatively low (below ∼600 Mbps) >Next, we investigate more realistic scenarios by conducting the same file download experiments on major browsers: Chrome, Edge, Firefox, and Opera. We observe that the performance gap is even larger than that in the cURL and quic_client experiments: on Chrome, QUIC begins to fall behind when the bandwidth exceeds ∼500 Mbps. Okay, well, this isn't going to be a problem over the general Internet, it's more of a problem in local networks. For people that have high-speed connections, how often are you getting >500Mbps from a single source? reply inetknght 15 minutes agoparent> For people that have high-speed connections, how often are you getting >500Mbps from a single source? Often enough over HTTP/1.1 that discussions like this are relevant to my concerns. reply sinuhe69 7 hours agoparentprevWell, I have other issues with QUIC: when I access Facebook with QUIC, the site often loads the first pages but then it kind of hung, force me to refresh the site, which is annoying. I didn’t know it’s a problem with QUIC, until I turned it off. Since then, FB & Co. load at the same speed, but don’t show this annoying behavior anymore! reply botanical 15 hours agoprev> we identify the root cause to be high receiver-side processing overhead I find this to be the issue when it comes to Google, and I bet it was known before hand; pushing processing to the user. For example, the AV1 video codec was deployed when no consumer had HW decoding capabilities. It saved them on space at the expense of increased CPU usage for the end-user. I don't know what the motive was there; it would still show that they are carbon-neutral while billions are busy processing the data. reply danpalmer 14 hours agoparent> the AV1 video codec was deployed when no consumer had HW decoding capabilities This was a bug. An improved software decoder was deployed for Android and for buggy reasons the YouTube app used it instead of a hardware accelerated implementation. It was fixed. Having worked on a similar space (compression formats for app downloads) I can assure you that all factors are accounted for with decisions like this, we were profiling device thermals for different compression formats. Setting aside bugs, the teams behind things like this are taking wide-reaching views of the ecosystem when making these decisions, and at scale, client concerns almost always outweigh server concerns. reply watermelon0 13 hours agorootparentYouTube had the same issue with VP9 on laptops, where you had to use an extension to force H264, to avoid quickly draining the battery. reply toastal 11 hours agorootparentprevIf only they would give us JXL on Android reply anfilt 14 hours agoparentprevWell I will say if your running servers hit billions of times per day. Offloading processing to the client when safe to do so starts make sense financially. Google does not have to pay for your CPU or storage usage ect... Also I will say if said overhead is not too much it's not that bad of a thing. reply kccqzy 14 hours agoparentprevThis is indeed an issue but it's widespread and everyone does it, including Google. Things like servers no longer generating actual dynamic HTML, replaced with servers simply serving pure data like JSON and expecting the client to render it into the DOM. It's not just Google that doesn't care, but the majority of web developers also don't care. reply SquareWheel 13 hours agorootparentThere's clearly advantages to writing a web app as an SPA, otherwise web devs wouldn't do it. The idea that web devs \"don't care\" (about what exactly?) really doesn't make any sense. Moving interactions to JSON in many cases is just a better experience. If you click a Like button on Facebook, which is the better outcome: To see a little animation where the button updates, or for the page to reload with a flash of white, throw away the comment you were part-way through writing, and then scroll you back to the top of the page? There's a reason XMLHttpRequest took the world by storm. More than that, jQuery is still used on more than 80% of websites due in large part to its legacy of making this process easier and cross-browser. reply consteval 2 hours agorootparent> To see a little animation where the button updates, or for the page to reload with a flash of white, throw away the comment you were part-way through writing, and then scroll you back to the top of the page I don't understand how web devs understand the concept of loading and manipulating JSON to dynamically modify the page's HTML, but they don't understand the concept of loading and manipulating HTML to dynamically modify the page's HTML. It's the same thing, except now you don't have to do a conversion from JSON->HTML. There's no rule anywhere saying receiving HTML on the client should do a full page reload and throw up the current running javascript. > XMLHttpRequest This could've easily been HTMLHttpRequest and it would've been the same API, but probably better. Unfortunately, during that time period Microsoft was obsessed with XML. Like... obsessed obsessed. reply kccqzy 1 hour agorootparentprevRendering JSON into HTML has nothing to do with XMLHttpRequest. Funny that you mention jQuery. When jQuery was hugely popular, people used it to make XMLHttpRequests that returned HTML which you then set as the innerHTML of some element. Of course being jQuery, people used the shorthand of `$(\"selector\").html(...)` instead. In the heyday of jQuery the JSON.parse API didn't exist. reply tock 11 hours agorootparentprevI don't think Facebook is the best example given the sheer number of loading skeletons I see on their page. reply JoshTriplett 16 hours agoprevSeems to be available on arXiv: https://arxiv.org/pdf/2310.09423 reply Tempest1981 3 hours agoparentThe page headings say \"Conference'17, July 2017\" -- why is that? Although the sidebar on page 1 shows \"13 Oct 2023\". reply mrngm 2 hours agorootparentIt's likely the authors used an existing conference template to fit in their paper's contents. Upon sending it to the conference, the editors can easily fit the contents in their prescribed format, and the authors know how many characters they can fit in the page limit. arXiv typically contains pre-prints of papers. These may not have been peer-reviewed, and the contents may not reflect the actual \"published\" paper that was accepted (and/or corrected after peer review) to a conference or journal. arXiv applies a watermark to the submitted PDF such that different versions are distinguishable on download. reply crashingintoyou 10 hours agoprevDon't have access to the published version but draft at https://arxiv.org/pdf/2310.09423 mentions ping RTT at 0.23ms. As someone frequently at 150ms+ latency for a lot of websites (and semi-frequently 300ms+ for non-geo-distributed websites), in practice with the latency QUIC is easily the best for throughput, HTTP/1.1 with a decent number of parallel connections is a not-that-distant second, and in a remote third is HTTP/2 due to head-of-line-blocking issues if/when a packet goes missing. reply apitman 14 hours agoprevCurrently chewing my way laboriously through RFC9000. Definitely concerned by how complex it is. The high level ideas of QUIC seem fairly straight forward, but the spec feels full of edge cases you must account for. Maybe there's no other way, but it makes me uncomfortable. I don't mind too much as long as they never try to take HTTP/1.1 from me. reply ironmagma 14 hours agoparentConsidering they can’t really even make IPv6 happen, that seems like a likely scenario. reply BartjeD 12 hours agorootparenthttps://www.google.com/intl/en/ipv6/statistics.html I think it's just your little corner of the woods that isn't adopting it. Over here the trend is very clearly to move away from IPv4, except for legacy reasons. reply ktosobcy 6 hours agorootparentSave for the France/Germany (~75%) and then USA/Mexcico/Brazil (~50%) rest of the world is not really adopting it... Even in Europe Spain has only ~10% and Poland ~17% penetration but yeah... let's be dismissive with \"your little corner\"... reply 71bw 6 hours agorootparent>and Poland ~17% penetration Almost exclusively due to Orange Polska -> belongs to France Telecom -> go figure... reply apitman 12 hours agorootparentprevThe important milestone is when it's safe to turn IPv4 off. And that's not going to happen as long as any country hasn't fully adopted it, and I don't think that's ever going to happen. For better or worse NAT handles outgoing connections and SNI routing handles incoming connections for most use cases. Self-hosting is the most broken but IMO that's better handled with tunneling anyway so you don't expose your home IP. reply jeroenhd 11 hours agorootparentIPv4 doesn't need to be off. Hacks and workarounds like DS-Lite can stay with us forever, just like hacks and workarounds like NAT and ALGs will. reply consp 9 hours agorootparentDS-lite (aka CGNAT), now we don't need to give the costumers a proper IP address anymore. It should be banned as it limits IPv6 adoption and it getting more and more use for \"customers own good\" and is annoying as hell to work around. reply alt227 8 hours agorootparentprevThe majority of this traffic is mobile devices. Most use ipv6 by default. Uptake on dekstop/laptops/servers is still extremely low and will be for a long time to come. reply sandos 7 hours agorootparentSweden is awful here, neither my home connection nor my phone uses ipv6. We were once very early with internet stuff, but now we lagging it seems. reply mardifoufs 5 hours agorootparentprevWhy did adoption slow down after a sudden rise? I guess some countries switched to ipv6 and since then, progress has been slow? It's hard to infer from the graph but my guess would be india? They have a very nice adoption rate. Sadly here in Canada I don't think any ISP even supports IPv6 in any shape or form except for mobile. Videotron has been talking about it for a decade (and they have a completely outdated infrastructure now, only DOCSIS and a very bad implementation of it too), and Bell has fiber but does not provide any info on that either. reply apitman 2 hours agorootparentThere's simply not enough demand. ISPs can solve their IP problems with NAT. Web services can solve theirs with SNI routing. The only people who really need IPv6 are self hosters. reply jtakkala 3 hours agorootparentprevRogers and Teksavvy support IPv6 reply mardifoufs 3 hours agorootparentAh that's cool! It sucks that they are basically non existent in Quebec, at least for residential internet. But I think they are pushing for a bigger foothold here reply arp242 6 hours agorootparentprevAdoption is not even 50%, and the line goes up fairly linear so ~95% will be around 2040 or so? And if you click on the map view you will see \"little corner of the woods\" is ... the entire continent of Africa, huge countries like China and Indonesia. reply AlienRobot 9 hours agorootparentprev>I think it's just your little corner of the woods that isn't adopting it. The graph says adoption is under 50%. Even U.S. is at only 50%. Some countries are under 1%. reply BartjeD 7 hours agorootparentParts of the EU: 74% reply ktosobcy 6 hours agorootparentAnd others are 10-15%... reply jakeogh 11 hours agoparentprevI think keeping HTTP/1.1 is almost as important as not dropping IPV4 (there are good reasons to not being able to tag everything; it's harder to block a country than a user.) For similar reasons we should keep old protocols. On a positive note, AFAICT 90%(??) of QUIC implementations ignored the proposed the spin bit: https://news.ycombinator.com/item?id=20990754 reply AlphaCharlie 12 hours agoprevFree PDF file of the research: https://arxiv.org/pdf/2310.09423 reply jacob019 16 hours agoprevMaybe moving the connection protocol into userspace isn't such a great plan. reply mrweasel 12 hours agoparentMaybe moving the entire application to the browser/cloud wasn't the best idea for a large number of use cases? Video streaming, sure, but we're already able to stream 4K video over a 25Mbit line. With modern internet connections being 200Mbit to 1Gbit, I don't see that we need the bandwidth in private homes. Maybe for video conferencing in large companies, but that also doesn't need to be 4K. The underlying internet protocols are old, so there's no harm in assessing if they've outlived their usefulness. However, we should also consider in web applications and \"always connected\" is truly the best solution for our day to day application needs. reply kuschku 10 hours agorootparent> With modern internet connections being 200Mbit to 1Gbit, I don't see that we need the bandwidth in private homes Private connections tend to be asymmetrical. In some cases, e.g. old DOCSIS versions, that used to be due to technical necessity. Private connections tend to be unstable, the bandwidth fluctuates quite a bit. Depending on country, the actually guaranteed bandwidth is somewhere between half of what's on the sticker, to nothing at all. Private connections are usually used by families, with multiple people using it at the same time. In recent years, you might have 3+ family members in a video call at the same time. So if you're paying for a 1000/50 line (as is common with DOCSIS deployments), what you're actually getting is usually a 400/20 line that sometimes achieves more. And those 20Mbps upload are now split between multiple people. At the same time, you're absolutely right – Gigabit is enough for most people. Download speeds are enough for quite a while. We should instead be increasing upload speeds and deploying FTTH and IPv6 everywhere to reduce the latency. reply throwaway2037 7 hours agorootparentThis is a great post. I often forget that home Internet connections are frequently shared between many people. This bit: > IPv6 everywhere to reduce the latency I am not an expert on IPv4 vs IPv6. Teach me: How will migrating to IPv6 reduce latency? As I understand, a lot of home Internet connections are always effectively IPv6 via CarrierNAT. (Am I wrong? Or not relevant to your point?) reply kuschku 6 hours agorootparentIPv4 routing is more complicated, especially with multiple levels of NAT applied. Google has measured to most customers about 20ms less latency on IPv6 than on IPv4, according to their IPv6 report. reply simoncion 5 hours agorootparent> Google has measured to most customers about 20ms less latency on IPv6 than on IPv4, according to their IPv6 report. I've run that comparison across four ISPs and never seen any significant difference in latency... not once in the decades I've had \"dual stack\" service. I imagine that Google is getting confounded by folks with godawful middle/\"security\"ware that is too stupid to know how to handle IPv6 traffic and just passes it through. reply throwaway2037 7 hours agorootparentprevOverall, I like your post very much. > but that also doesn't need to be 4K. Here, I would say \"need\" is a strong term. Surely, you are correct at the most basic level, but if the bandwidth exists, then some streaming platforms will use it. Deeper question: Is there any practical use case for Internet connections about 1Gbit? I struggle to think of any. Yes, I can understand that people may wish to reduce latency, but I don't think home users need any more bandwidth at this point. I am astonished when I read about 10Gbit home Internet access in Switzerland, Japan, and Korea. Zero trolling: Can you help me to better understand your last sentence? > However, we should also consider in web applications and \"always connected\" is truly the best solution for our day to day application needs. I cannot tell if this is written with sarcasm. Let me ask more directly: Do you think it is a good design for our modern apps to always be connected or not? Honestly, I don't have a strong opinion on the matter, but I am interested to hear your opinion. reply mrweasel 6 hours agorootparentGenerally speaking I think we should aim for offline first, always. Obvious things like Teams or Slack requires an internet connection to work, but assuming a working internet connection shouldn't even be a requirement for a web browser. I think it is bad design to expect a working internet connection, because in many places your can't expect bandwidth be cheap, or the connection to be stable. That's not to say that something like Google Docs (others seems to like it, but everyone in my company thinks it's awful) should be a thing, there's certainly value in the real time collaboration features, but it should be able to function without an internet connection. Last week someone was complaining about the S3 (sleep) feature on laptops, and one thing that came to my mind is that despite these being portable, we somehow expect them to be always connected to the internet. That just seems like a somewhat broken mindset to me. reply surajrmal 6 hours agorootparentNote that in deeper sleep states you typically see more aggressive limiting of what interrupts can take you out of the sleep state. Turning off network card interrupts is common. reply simiones 9 hours agoparentprevThe problem is that the biggest win by far with QUIC is merging encryption and session negotiation into a single packet, and the kernel teams have been adamant about not wanting to maintain encryption libraries in kernel. So, QUIC or any other protocol like it being in kernel is basically a non-starter. reply foota 16 hours agoparentprevI don't have access to the article, but they're saying the issue is due to client side ack processing. I suspect they're testing at bandwidths far beyond what's normal for consumer applications. reply dartharva 15 hours agorootparentIt's available on arxiv and nope, they are testing mostly for regular 4G/5G speeds. https://arxiv.org/pdf/2310.09423 reply DannyBee 15 hours agorootparent4g tops out at 1gbps only when one person is on the network. 5g tops out at ~10gbps (some 20gbps i guess) only when one person is on the network. They are testing at 1gbps. This is not regular 4g speed for sure, and it's a rare 5g speed. regular 5g speed is (in the US) 40-50mbps, so, 20x slower than they are testing. reply vrighter 12 hours agorootparentGigabit fiber internet is quite cheap and increasingly available (I'm not from the US). I don't just use the internet over a 4/5g connection. This definitely affects more people than you think. reply DannyBee 6 hours agorootparentI think it affects lots of people. I have 5gbps internet at home myself. But that is not what i was replying to. I was replying to the claim that this affects regular 4g/5g cell phone speeds. The data is clear that it does not. reply izend 14 hours agorootparentprevWhat about 1gbps fiber at home, it is becoming common in Canada. I have 1gbps up/down. reply DannyBee 6 hours agorootparentThis would affect that. As said, i was only replying to the claim that this affects things at 4g/5g cell phone speeds, which it clearly does not, by their own data. reply dartharva 15 hours agorootparentprevStill won't be beyond normal consumer applications' capacity, right? reply DannyBee 6 hours agorootparentcorrect reply KaiserPro 11 hours agorootparentprevHttp1.1 has been around for 28 years. At the time, gigabit ethernet was _expensive_. 9600baud on mobile was rare. and yet http1.1 runs on gigabit networks pretty well. reply yencabulator 3 hours agorootparentprevYour 5G has 0.23ms ping to the average webserver? reply spacebacon 15 hours agorootparentprevSee arXiv link in comments. reply kccqzy 14 hours agoparentprevThe flexibility and ease of changing a userspace protocol IMO far outweighs anything else. If the performance problem described in this article (which I don't have access to) is in userspace QUIC code, it can be fixed and deployed very quickly. If similar performance issue were to be found in TCP, expect to wait multiple years. reply vrighter 12 hours agorootparentWell, the problem is probably that it is in userspace in the first place. reply 01HNNWZ0MV43FF 15 hours agoparentprevDoes QUIC mandate that, or is that just the stepping stone until the chicken-and-egg problem is solved and we get kernel support? reply kmeisthax 14 hours agorootparentNo, but it depends on how QUIC works, how Ethernet hardware works, and how much you actually want to offload to the NIC. For example, QUIC has TLS encryption built-in, so anything that's encrypted can't be offloaded. And I don't think most people want to hand all their TLS keys to their NIC[0]. At the very least you probably would have to assign QUIC its own transport, rather than using UDP as \"we have raw sockets at home\". Problem is, only TCP and UDP reliably traverse the Internet[1]. Everything in the middle is sniffing traffic, messing with options, etc. In fact, Google rejected an alternate transport protocol called SCTP (which does all the stream multiplexing over a single connection that QUIC does) specifically because, among other things, SCTP's a transport protocol and middleboxes choke on it. [0] I am aware that \"SSL accelerators\" used to do exactly this, but in modern times we have perfectly good crypto accelerators right in our CPU cores. [1] ICMP sometimes traverses the internet, it's how ping works, but a lot of firewalls blackhole ICMP. Or at least they did before IPv6 made it practically mandatory to forward ICMP packets. reply _flux 13 hours agorootparentI don't think passing just the session keys to NIC would sound so perilous, though. reply justinphelps 9 hours agorootparentprevSCTP had already solved the problem that QUIC proposes to solve. Google of all companies has the influence to properly implement and accommodate other L4 protocols. QUIC seems like doubling down on a hack and breaks the elegance of OSI model. reply tepmoc 5 hours agorootparentSCTP still have some donwsides it has to resolve https://http3-explained.haxx.se/en/why-quic/why-tcpudp#why-n... Plus we need happy eyeballs for transport if SCTP run over IP and not encapuslated https://datatracker.ietf.org/doc/html/draft-grinnemo-taps-he But IPv4 pretty much non-workable since most end-users behind NAT and there no known implementation to work around that. reply vlovich123 14 hours agorootparentprevAs others in the thread summarized the paper as saying the issue is ack offload. That has nothing to do with whether the stack is in kernel space or user space. Indeed there’s some concern about this inevitable scenario because the kernel is so slow moving, updates take much longer to propagate to applications needing them without a middle ground whereas as user space stacks they can update as the endpoint applications need them to. reply wmf 15 hours agorootparentprevOn mobile the plan is to never use kernel support so that apps can have the latest QUIC on old kernels. reply throw0101c 7 hours agoprevNetflix has gotten TCP/TLS up to 800 Gbps (over many streams): * https://news.ycombinator.com/item?id=32519881 * https://news.ycombinator.com/item?id=33449297 hitting 100 Gbps (20k-30k customers) using less that 100W: * https://twitter.com/ocochardlabbe/status/1781848334145130661 * https://news.ycombinator.com/item?id=40630699#unv_40630785 reply mholt 16 hours agoprevI don't have access to the paper but based on the abstract and a quick scan of the presentation, I can confirm that I have seen results like this in Caddy, which enables HTTP/3 out of the box. HTTP/3 implementations vary widely at the moment, and will likely take another decade to optimize to homogeneity. But even then, QUIC requires a lot of state management that TCP doesn't have to worry about (even in the kernel). There's a ton of processing involved with every UDP packet, and small MTUs, still engrained into many middle boxes and even end-user machines these days, don't make it any better. So, yeah, as I felt about QUIC ... oh, about 6 years ago or so... HTTP/2 is actually really quite good enough for most use cases. The far reaches of the world and those without fast connections will benefit, but the majority of global transmissions will likely be best served with HTTP/2. Intuitively, I consider each HTTP major version an increased order of magnitude in complexity. From 1 to 2 the main complexities are binary (that's debatable, since it's technically simpler from an encoding standpoint), compression, and streams; then with HTTP/3 there's _so, so much_ it does to make it work. It _can_ be faster -- that's proven -- but only when networks are slow. TCP congestion control is its own worst enemy, but when networks aren't congested (and with the right algorithm)... guess what. It's fast! And the in-order packet transmissions (head-of-line blocking) makes endpoint code so much simpler and faster. It's no wonder TCP is faster these days when networks are fast. I think servers should offer HTTP/3 but clients should be choosy when to use it, for the sake of their own experience/performance. reply geocar 10 hours agoparentI turned off HTTP2 and HTTP3 a few months ago. I see a few million daily page views: Memory usage has been down, latency has been down, network accounting (bandwidth) is about the same. Revenue (ads) is up. > It _can_ be faster -- that's proven -- but only when networks are slow. It can be faster in a situation that doesn't exist. It sounds charitable to say something like \"when networks are slow\" -- but because everyone has had a slow network experience, they are going to think that QUIC would help them out, but real world slow network problems don't look like the ones that QUIC solves. In the real world, QUIC wastes memory and money and increases latency on the average case. Maybe some Google engineers can come up with a clever heuristic involving TCP options or the RTT information to \"switch on QUIC selectively\" but honestly I wish they wouldn't bother, simply because I don't want to waste my time benchmarking another half-baked google fart. reply replete 4 hours agorootparentIt's strange to read this when you see articles like this[0] and see Lighthouse ranking better with it switched on. Nothing beats real world stats though. Could this be down to server/client implementation of HTTP2 or would you say its a fundamental implication of the design of the protocol? Trying to make my sites load faster led me to experiment with QUIC and ultimately I didn't trust it enough to leave it on with the increase of complexity. [0]: https://kiwee.eu/blog/http-3-how-it-performs-compared-to-htt... reply withinboredom 51 minutes agorootparent> It's strange to read this when you see articles like this[0] and see Lighthouse ranking better with it switched on. I mean, Lighthouse is maintained by Google (IIRC), and I can believe they are going to give their own protocol bonus points. > Could this be down to server/client implementation of HTTP2 or would you say its a fundamental implication of the design of the protocol? For stable internet connections, you'll see http2 beat http3 around 95% of the time. It's the 95th+ percentile that really benefits from http3 on a stable connection. If you have unstable connections, then http3 will win, hands down. reply withinboredom 9 hours agorootparentprevThe thing is, very few people who use \"your website\" are on slow, congested networks. The number of people who visit google on a slow, congested network (airport wifi, phones at conferences, etc) is way greater than that. This is a protocol to solve a google problem, not a general problem or even a general solution. reply geocar 9 hours agorootparentSince I buy ads on Google to my site I would argue it’s representative of Google’s traffic. But nice theory. reply withinboredom 7 hours agorootparentIt's not. Think about what you search for on your mobile, while out or traveling, and what you search for on desktop/wifi. They are vastly different. Your traffic is not representative of the majority of searches. reply geocar 7 hours agorootparentI'm sure the majority of searches are for \"google\" and \"facebook\" and you're right in a way: I'm not interested in those users. I'm only interested in searches that advertisers are interested in, but this is also where Google gets their revenue from, so we are aligned with which users we want to prioritise, so I do not understand who you possibly think QUIC is for if not Google's ad business? reply withinboredom 59 minutes agorootparentThat's literally what I said; the entire protocol is engineered for google. Not for everyone else. 99.99% of websites out there do not need it. reply altairprime 15 hours agoparentprevThe performance gap is shown to be due to hardware offloading, not due to congestion control, in the arxiv paper above. reply vlovich123 15 hours agorootparentAnd because Quic is encrypted at a fundamental level, offload likely means needing to share keys with the network card which is a trust concern. reply 10000truths 13 hours agorootparentThis is already how TLS offload is implemented for NICs that support it. The handshake isn't offloaded, only the data path. So essentially, the application performs the handshake, then it calls setsockopt to convert the TCP socket to a kTLS socket, then it passes the shared key, IV, etc. to the kTLS socket, and the OS's network stack passes those parameters to the NIC. From there, the NIC only handles the bulk encryption/decryption and record encapsulation/decapsulation. This approach keeps the drivers' offload implementations simple, while still allowing the application/OS to manage the session state. reply vlovich123 1 hour agorootparentSure, similar mechanisms are available but for TCP ack offloading and TLS encryption/decryption offloading are distinct features. With QUIC there’s no separation which changes the threat model. Of course the root architectural problem is that this kind of stuff is part of the NIC instead of an “encryption accelerator” that can be requested to operate with a key ID on a RAM region and then the kernel only needs to give the keys to the SE (and potentially that’s where they even originate instead of ever living anywhere else) reply jstarks 3 hours agorootparentprevYour NIC can already access arbitrary RAM via DMA. It can read your keys already. reply altairprime 3 hours agorootparentThat is often incorrect for Apple computers, whether x64+T2 or aarch64: https://support.apple.com/fr-tn/guide/security/seca4960c2b5/... And it’s often incorrect on x64 PCs when IOMMU access is appropriately segmented. See also e.g. Thunderclap: https://www.ndss-symposium.org/wp-content/uploads/ndss2019_0... It may still be true in some cases, but it shouldn’t be taken for granted that it’s always true. reply yencabulator 3 hours agorootparentprevNope. https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_ma... reply truetraveller 15 hours agoparentprevI'd say Http1.1 is good enough for most people, especially with persistent connections. Http2 is an exponential leap in complexity, and burdensome/error-prone for clients to implement. reply apitman 14 hours agorootparentThe day they come for HTTP/1.1 is the day I die on a hill. reply 01HNNWZ0MV43FF 15 hours agorootparentprevYeah I imagine 1 + 3 being popular. 1.1 is so simple to implement and WebTransport / QUIC is basically a teeny VPN connection. reply Sparkyte 15 hours agoparentprevAgreed on this. reply edwintorok 1 hour agoprevTCP has a lot of offloads that may not all be available for UDP. reply Banou 9 hours agoprevI think one of the reasons Google choose UDP is that it's already a popular protocol, on which you can build reliable packets, while also having the base UDP unreliability on the side. From my perspective, which is a web developer's, having QUIC, allowed the web standards to easily piggy back on top of it for the Webtransport API, which is ways better than the current HTTP stack and WebRTC which is a complete mess. Basically giving a TCP and UDP implementation for the web. Knowing this, I feel like it makes more sense to me why Google choose this way of doing, which some people seem to be criticizing. reply simoncion 5 hours agoparent> I think one of the reasons Google choose UDP is that it's already a popular protocol... If you want your packets to reliably travel fairly unmolested between you and an effectively-randomly-chosen-peer on The Greater Internet, you have two transport protocol choices: TCP/IP or UDP/IP. If you don't want the connection-management & etc that TCP/IP does for you, then you have exactly one choice. > ...which some people seem to be criticizing. People are criticizing the fact that on LAN link speeds (and fast (for the US) home internet speeds) QUIC is no better than (and sometimes worse than) previous HTTP transport protocols, despite the large amount of effort put into it. It also seems that some folks are suggesting that Google could have put that time and effort into improving Linux's packet-handling code and (presumably) getting that into both Android and mainline Linux. reply dathinab 8 hours agoprevit says it isn't fast _enough_ but as far as I can tell it's fast _enough_ just not as fast as it could be mainly they seem to test situations related to bandwidth/latency which aren't very realistically for the majority of users (because most users don't have supper fast high bandwidth internet) this doesn't meant QUIC can't be faster or we shouldn't look into reducing overhead, just it's likely not as much as a deal as it might initially loook reply 404mm 4 hours agoprevWhen looking at the tested browsers, I want to ask why this was not tested on Safari (which is currently the second most used browser by share). reply necessary 5 hours agoprevDoes QUIC do better with packet loss compared to TCP? TCP perceives packet loss as network congestion and so throughput over high bandwidth+high packet loss links suffers. reply ahmetozer 7 hours agoprevFor mobile connectivity -> quic For home internet wifi & cable access -> http2 For heavy loaded enterprise slow wifi network -> quic reply latentpot 13 hours agoprevQUIC is the standard problem across n number of clients who choose Zscaler and similar content inspection tools. You can block it at the policy level but you also need to have it disabled at the browser level. Which sometimes magically turns on again and leads to a flurry of tickets for 'slow internet', 'Google search not working' etcetera. reply watermelon0 13 hours agoparentWouldn't the issue in this case be with Zscaler, and not with QUIC? reply chgs 12 hours agoparentprevThe problem here is choosing software like zscaler reply mcosta 10 hours agorootparentZscaler is not chosen, it is imposed by the corporation reply v1ne 9 hours agoparentprevHmm, interesting. We also have a policies imposed by the Regulator™ that leads to us inspecting all web traffic. All web traffic goes through a proxy that's configured in the web browser. No proxy, no internet. Out of curiosity: What's your use case to use ZScaler for this inspection instead? reply wseqyrku 10 hours agoprevThere's a work in progress for kernel support: https://github.com/lxin/quic reply jauntywundrkind 14 hours agoprevI wonder if these results reproduce on Windows. Is there any TCP offload or GSO there? If not maybe the results wouldn't vary? reply v1ne 9 hours agoparentOh, sure there is! https://learn.microsoft.com/en-us/windows-hardware/drivers/n... reply exabrial 4 hours agoprevQUIC needs an unencrypted mode! reply AtNightWeCode 5 hours agoprevFor us, what QUIC solves is that mobile users that move around in the subway and so on are not getting these huge latency spikes. Which was one of our biggest complains. reply larsonnn 9 hours agoprevSite is blocking Apples private relay :( reply jiggawatts 11 hours agoprevI wonder if the trick might be to repurpose technology from server hardware: partition the physical NIC into virtual PCI-e devices with distinct addresses, and map to user-space processes instead of virtual machines. So in essence, each browser tab or even each listening UDP socket could have a distinct IPv6 address dedicated to it, with packets delivered into a ring buffer in user-mode. This is so similar to what goes on with hypervisors now that existing hardware designs might even be able to handle it already. Just an idle thought... reply jeroenhd 10 hours agoparentI've often pondered if it was possible to assign every application/tab/domain/origin a different IPv6 address to exchange data with, to make tracking people just a tad harder, but also to simplify per-process firewall rules. With the bare minimum, a /64, you could easily host billions of addresses per device without running out. I think there may be a limit to how many IP addresses NICs (and maybe drivers) can track at once, though. What I don't really get is why QUIC had to be invented when multi-stream protocols like SCTP already exist. SCTP brings the reliability of TCP with the multi-stream system that makes QUIC good for websites. Piping TLS over it is a bit of a pain (you don't want a separate handshake per stream), but surely there could be techniques to make it less painful (leveraging 0-RTT? Using session resumptions with tickets from the first connected stream?). reply simiones 9 hours agorootparentFirst and foremost, you can't use SCTP on the Internet, so the whole idea is dead on arrival. The Internet only really works for TCP and UDP over IP - anything else, you have a loooooong tail of networks which will drop the traffic. Secondly, the whole point of QUIC is to merge the TLS and transport handskakes into a single packet, to reduce RTT. This would mean you need to modify SCTP anyway to allow for this use case, so even what small support exists for SCTP in the large would need to be upgraded. Thirdly, there is no reason to think that SCTP is better handled than UDP at the kernel's IP stack level. All of the problems of memory optimizations are likely to be much worse for SCTP than for UDP, as it's used far, far less. reply jeroenhd 7 hours agorootparentI don't see why you can't use SCTP over the internet. HTTP2 has fallbacks for broken or generally shitty middleboxes, I don't see why the weird corporate networks should hold back the rest of the world. TLS already does 0-RTT so you don't need QUIC for that. The problem with UDP is that many optimisations are simply not possible. The \"TCP but with blackjack and hookers\" approach QUIC took makes it very difficult to accelerate. SCTP is Fine™ on Linux but it's basically unimplemented on Windows. Acceleration beyond what these protocols can do right now requires either specific kernel/hardware QUIC parsing or kernel mode SCTP on Windows. Getting Microsoft to actually implement SCTP would be a lot cleaner than to hack yet another protocol on top of UDP out of fear of the mighty shitty middleboxes. reply simiones 6 hours agorootparentWebRTC decided they liked SCTP, so... they run it over UDP (well, over DTLS over UDP). And while HTTP/2 might fail over to HTTP/1.1, what would an SCTP session fall back to? The problem is not that Windows doesn't have in-kernel support for SCTP (there are several user-space libraries already available, you wouldn't even need to convince MS to do anything). The blocking issue is that many, many routers on the Internet, especially but not exclusively around all corporate networks, will drop any packet that is neither TCP or UDP over IP. And if you think UDP is not optimized, I'd bet you'll find that the SCTP situation is far, far worse. And regarding 0-RTT, that only works for resumed connections, and it is still actually 1 RTT (TCP connection establish). New connections still need 2-3 round trips (1 for TCP, 1 for TLS 1.3, or 2 for TLS 1.2) with TLS; they only need 1 round trip (even when using TLS 1.2 for encryption). With QUIC, you can have true 0-RTT traffic, sending the (encrypted) HTTP request data in the very first packet you send to a host [that you communicated with previously]. reply kbolino 3 hours agorootparentHow is userspace SCTP possible on Windows? Microsoft doesn't implement it in WinSock and, back in the XP SP2 days, Microsoft disabled/hobbled raw sockets and has never allowed them since. Absent a kernel-mode driver, or Microsoft changing their stance (either on SCTP or raw sockets), you cannot send pure SCTP from a modern Windows box using only non-privileged application code. reply simiones 2 hours agorootparentPer these Microsoft docs [0], it seems that it should still be possible to open a raw socket on Windows 11, as long as you don't try to send TCP or UDP traffic through it (and have the right permissions, presumably). Of course, to open a raw socket you need privileged access, just like you do on Linux, because a raw socket allows you to see and respond to traffic from any other application (or even system traffic). But in principle you should be able to make a Service that handles SCTP traffic for you, and a non-privileged application could send its traffic to this service and receive data back. I did find some user-space library that is purported to support SCTP on Windows [1], but it may be quite old and not supported. Not sure if there is any real interest in something like this. [0] https://learn.microsoft.com/en-us/windows/win32/winsock/tcp-... [1] https://www.sctp.de/sctp-download.html reply kbolino 1 hour agorootparentInteresting. I think the service approach would now be viable since it can be paired with UNIX socket support, which was added a couple of years ago (otherwise COM or RPC would be necessary, making clients more complicated and Windows-specific). But yeah, the lack of interest is the bigger problem now. reply tepmoc 6 hours agorootparentprevSCTP works fine on internet, as long your egress is comming from public IP and you don't perform NAT. So in case IPv6 its non issue at all unless you sit behind middle boxes. Probably best approuch would be is like happy eye balls but for transport. https://datatracker.ietf.org/doc/html/draft-grinnemo-taps-he reply simiones 5 hours agorootparentHow many corporate or residential firewalls are configured to allow SCTP traffic through? reply tepmoc 5 hours agorootparentresidential - not many. Corporate on other hand is different story, thus why happy eyeballs for transport still would needed to gradual rollout anyway. reply astrange 8 hours agorootparentprevIs there a service like test-ipv6 to see if SCTP works? Obviously harder to run since you can't do it in a browser. reply simiones 7 hours agorootparentI doubt there is, because it's just not a very popular thing to even try. Even WebRTC, which uses SCTP for non-streaming data channels, uses it over DTLS over UDP. reply KaiserPro 11 hours agoparentprevOr just have multiple TCP streams. Super simple, low cost, uses all the optimisations we have already. when the latency/packet drop is low, prune the connections and you get monster speed. When the latency/loss is high, grow the number of concurrent connections to overcome slow start. Doesn't give you QUIC like multipath though. reply m_eiman 8 hours agorootparentThere’s Multipath TCP. reply KaiserPro 6 hours agorootparentI mean there is, but from what I recall its more a link aggregation thing, rather than a network portable system reply thelastparadise 8 hours agoprevGotta be QUIC er than that, buddy! reply sylware 9 hours agoprevTo go faster, you need to simplify a lot. reply bell-cot 9 hours agoparentTo force a lucrative cycle of hardware upgrades, you need software to do the opposite. True story: Back in the early aughties, Intel was hosting regular seminars for dealers and integrators selling either Intel-made PC's, or white box ones. I attended one of those, and the Intel rep openly claimed that Intel had challenged Microsoft to produce software which could bring a GHz CPU to its knees. reply Sparkyte 15 hours agoprev [–] Maybe I'm the only person who thinks that trying to make existing internet protocols faster is wasted energy. But who am I to say anything. reply cheema33 12 hours agoparent> Maybe I'm the only person who thinks that trying to make existing internet protocols faster is wasted energy. But who am I to say anything. If you have a valid argument to support your claim, why not present it? reply Sparkyte 12 hours agorootparentThey are already expected standards so when you create optimizations you're building on functions that need to be supported additionally on top of them. This leads to incompatibility and sometimes often worse performance as what is being experienced here with QUIC. You can read more about such things from, The Evolution of the Internet Congestion Control. https://groups.csail.mit.edu/ana/Publications/The_Evolution_... A good solution",
    "originSummary": [
      "A recent study reveals that QUIC, a protocol designed to improve internet performance, shows up to a 45.2% data rate reduction compared to traditional TCP+TLS+HTTP/2 over high-speed networks.",
      "The performance gap is attributed to high receiver-side processing overhead, particularly due to excessive data packets and QUIC's user-space acknowledgments (ACKs).",
      "The findings are significant for applications like file transfers, video streaming, and web browsing, with recommendations provided to mitigate these issues."
    ],
    "commentSummary": [
      "QUIC faces performance issues on fast internet due to inefficient syscall interfaces, high syscall costs from spectre mitigation, small default UDP buffers, and complex UDP stack optimizations.",
      "GSO (Generic Segmentation Offload) can enhance performance but is not optimal for large-scale servers, indicating a need for better APIs to replace BSD sockets/POSIX.",
      "Discussions emphasize the complexity of integrating io_uring and the potential benefits of high-level APIs built on it, highlighting the balance between user-space flexibility and kernel-level efficiency."
    ],
    "points": 605,
    "commentCount": 276,
    "retryCount": 0,
    "time": 1725849255
  },
  {
    "id": 41483675,
    "title": "Reclaim the Stack",
    "originLink": "https://reclaim-the-stack.com",
    "originBody": "We spent 7 months building a Kubernetes based platform to replace Heroku for our SaaS product at mynewsdesk.com. The results were a 90% reduction in costs and a 30% improvement in performance. We also significantly improved developer experience with reduced deploy times and faster / more accessible tooling. We have now open sourced the entire stack, so you can do the same, but in a few days instead of 7 months. It's time to Reclaim the Stack! Read the DocumentationJoin the Discord Server",
    "commentLink": "https://news.ycombinator.com/item?id=41483675",
    "commentBody": "Reclaim the Stack (reclaim-the-stack.com)463 points by dustedcodes 20 hours agohidepastfavorite289 comments jusomg 11 hours agoOf course you reduced 90% of the cost. Most of these costs don't come from the software, but from the people and automation maintaining it. With that cost reduction you also removed monitoring of the platform, people oncall to fix issues that appear, upgrades, continuous improvements, etc. Who/What is going to be doing that on this new platform and how much does that cost? Now you need to maintain k8s, postgresql, elasticsearch, redis, secret managements, OSs, storage... These are complex systems that require people understanding how they internally work, how they scale and common pitfalls. Who is going to upgrade kubernetes when they release a new version that has breaking changes? What happens when Elasticsearch decides to splitbrain and your search stops working? When the DB goes down or you need to set up replication? What is monitoring replication lag? Or even simply things like disks being close to full? What is acting on that? I don't mean to say Heroku is fairly priced (I honestly have no idea) but this comparison is not apples to apples. You could have your team focused on your product before. Now you need people dedicated to work on this stuff. reply mlinhares 5 hours agoparentAnything you don't know about managing these systems can be learned asking chatgpt :P Whenever I see people doing something like this I remember I did the same when I was in 10 people startups and it required A LOT of work to keep all these things running (mostly because back then we didn't have all these cloud managed systems) and that time would have been better invested in the product instead of wasting time figuring out how these tools work. I see value in this kind of work if you're at the scale of something like Dropbox and moving from S3 will greatly improve your bottom line and you have a team that knows exactly what they're doing and will be assigned the maintenance of this work. If this is being done merely from a cost cutting perspective and you don't have the people that understand these systems, its a recipe for disaster and once shit is on fire the people that would be assigned to \"fix\" the problem will quickly disappear because the \"on call schedule is insane\". reply ljm 2 hours agorootparentI bailed out of one company because even though the stack seemed conceptually simple in terms of infra (there wasn't a great deal to it), the engineering more than compensated for it. The end result was the same: non-stop crisis management, non-stop firefighting, no capacity to work on anything new, just fixing old. All by design, really, because at that point you're not part of an engineering team you're a code monkey operating in service of growth metrics. reply Diederich 2 hours agorootparentprev> ... I remember I did the same when I was in 10 people startups and it required A LOT of work to keep all these things running... Honest question: how long ago was that? I stepped away from that ecosystem four or so years ago. Perhaps ease of use has substantially improved? reply re-thc 4 hours agorootparentprev> and that time would have been better invested in the product instead of wasting time figuring out how these tools work It really depends on what you're doing. Back then a lot of non-VC startups worked better and the savings possibly helped. It also helps grow the team and have less reliance on the vendor. It's long term value. Is it really time wasted? People often go into resume building mode and do all kinds of wacky things regardless. Perhaps this just helps scratch that itch. reply mlinhares 3 hours agorootparentDefinitely fine from a personal perspective and resume building, it's just not in the best interest of the business because as soon as the person doing resume building is finished they'll jump ship. I've definitely done this myself. But i don't see this being good from a pure business perspective. reply ugh123 2 hours agoparentprev> you also removed monitoring of the platform You don't think they have any monitoring within Kubernetes? I imagine they have more monitoring capabilities now than they did with Heroku. reply johnnyanmac 1 hour agoparentprev>Who/What is going to be doing that on this new platform and how much does that cost? If you're already a web platform with hired talent (and someone using Heroku for a SaaS probably already is), I'd be surprised if the marginal cost was 10x.that paid support is of course coming at a premium, and isn't too flexible on what level of support you need. And yeah, it isn't apples to apples. Maybe you are in a low CoL area and can find a decent DevOps for 80-100k. Maybe you're in SF and any extra dev will be 250k. It'll vary immensely on cost. reply dzikimarian 9 hours agoparentprevSorry, but that's just ton of FUD. We run both private cloud and (for a few customers) AWS. Of course you have more maintenance on on-prem, but typical k8s update is maybe a few hours of work, when you know what you are doing. Also AWS is also, complex, also requires configuration and also generates alerts in the middle of the night. It's still a lot cheaper than managed service. reply jusomg 9 hours agorootparent> Of course you have more maintenance on on-prem, but typical k8s update is maybe a few hours of work, when you know what you are doing. You just mentioned one dimension of what I described, and \"when you know what you are doing\" is doing a lot of the heavy lifting in your argument. > Also AWS is also, complex, also requires configuration and also generates alerts in the middle of the night. I'm confused. So we are on agreement there? I feel you might be confusing my point with an on-prem vs AWS discussion, and that's not it. This is encouraging teams to run databases / search / cache / secrets and everything on top of k8s and assuming a magic k8s operator is doing the same job as a team of humans and automation managing all those services for you. reply Nextgrid 9 hours agorootparent> assuming a magic k8s operator is doing the same job as a team of humans and automation managing all those services for you. What do you think AWS is doing behind the scenes when you run Postgres RDS? It's their own equivalent of a \"K8S operator\" managing it. They make bold claims about how good/reliable/fault-tolerant it is, but the truth is that you can't actually test or predict its failure modes, and it can fail and fails badly (I've had it get into a weird state where it took 24h to recover, presumably once an AWS guy finally SSH'd in and fixed it manually - I could've done the same but without having to wait 24h). reply jusomg 8 hours agorootparentFair, but my point is that AWS has a full team of people that built and contributed to that magic box that is managing the database. When something goes wrong, they're the first ones to know (ideally) and they have a lot of know-how on what went wrong, what the automation is doing, how to remediate issues, etc. When you use a k8s operator you're using an off the shelve component with very little idea of what is doing and how. When things go wrong, you don't have a team of experts to look into what failed and why. The tradeoff here is obviously cost, but my point is those two levels of \"automation\" are not comparable. Edit: well, when I write \"you\" I mean most people (me included) reply dzikimarian 7 hours agorootparentIf you don't want complexity of operators, you'll be probably OK with DB cluster outside of k8s. They're quite easy to setup, automate and there are straightforward tools to monitor them (eg. from Percona). If you want to fully replicate AWS it may be more expensive than just paying AWS. But for most use cases it's simply not necessary. reply Analemma_ 2 hours agorootparentprev> Fair, but my point is that AWS has a full team of people that built and contributed to that magic box that is managing the database. You sure about that? I used to work at AWS, and although I wasn't on K8S in particular, I can tell you from experience that AWS is a revolving door of developers who mostly quit the instant their two-year sign-on bonus is paid out, because working there sucks ass. The ludicrous churn means there actually isn't very much buildup of institutional knowledge. reply re-thc 4 hours agorootparentprev> Fair, but my point is that AWS has a full team of people that built and contributed to that magic box that is managing the database You think so. The real answer is maybe maybe not. They could have all left and the actual maintainers now don't actually know the codebase. There's no way to know. > When things go wrong, you don't have a team of experts to look into what failed and why. I've been on both sides of consulting / managed services teams and each time the \"expert\" was worse than the junior. Sure, there's some luck and randomness but it's not as clear cut as you make it. > and they have a lot of know-how on what went wrong, what the automation is doing, how to remediate issues, etc. And to continue on the above I've also worked at SaaS/IaaS/PaaS where the person on call doesn't know much about the product (not always their fault) and so couldn't contribute much on incident. There's just to much trust and good faith in this reply. I'm not advocating to manage everything yourself but yes, don't trust that the experts have everything either. reply filleokus 9 hours agorootparentprevAs with everything it's not black or white, but rather a spectrum. Sure, updating k8s is not that bad, but operating a distributed storage solution is no joke. Or really anything that requires persistence and clustering (like elastic). You can also trade operational complexity for cash via support contracts and/or enterprise solutions (like just throwing money at Hitachi for storage rather than trying to keep Ceph alive). reply p_l 9 hours agorootparentIf you don't need something crazy you can just grab what a lot of enterprises already had done for years, which is drop a few big storage servers and call it a day, connecting over iSCSI/NFS/whatever reply filleokus 7 hours agorootparentIf you are in Kubernetes land you probably want object storage and some kind of PVC provider. Not thaaat different from an old fashioned iSCSI/NFS setup to be honest, but in my experience different enough to cause friction in an enterprise setting. You really don't want a ticket-driven, manual, provisioning process of shares reply p_l 6 hours agorootparenta PVC provider is nice, sure, but depending on how much you need/want simplest cases can be \"mount a subdirectory from common exported volume\", and for many applications ticket-based provisioning will be enough. That said on my todo-list is some tooling to make simple cases with linux NFS or SMI-capable servers work as PVC providers. reply tinco 9 hours agorootparentprevSure, but it requires that your engineers are vertically capable. In my experience, about 1 in 5 developers has the required experience and does not flat out refuse to have vertical responsibility over their software stack. And that number might be high, in larger more established companies there might be more engineers who want to stick to their comfort bubble. So many developers reject the idea of writing SQL themselves instead of having the ORM do it, let alone know how to configure replication and failover. I'd maybe hire for the people who could and would, but the people advocating for just having the cloud take care of these things have a point. You might miss out on an excellent application engineer, if you reject them for not having any Linux skills. reply dzikimarian 7 hours agorootparentOur devs are responsible for their docker image and the app. Then other team manages platform. You need some level of cooperation of course, but none of the devs cares too much about k8s internals or how the storage works. reply dbackeus 9 hours agoparentprevOriginal creator and maintainer of Reclaim the Stack here. > you also removed monitoring of the platform No we did not: Monitoring: https://reclaim-the-stack.com/docs/platform-components/monit... Log aggregation: https://reclaim-the-stack.com/docs/platform-components/log-a... Observability is on the whole better than what we had at Heroku since we now have direct access to realtime resource consumption of all infrastructure parts. We also have infinite log retention which would have been prohibitively expensive using Heroku logging addons (though we cap retention at 12 months for GDPR reasons). > Who/What is going to be doing that on this new platform and how much does that cost? Me and my colleague who created the tool together manage infrastructure / OS upgrades and look into issues etc. So far we've been in production 1.5 years on this platform. On average we spent perhaps 3 days per month doing platform related work (mostly software upgrades). The rest we spend on full stack application development. The hypothesis for migrating to Kubernetes was that the available database operators would be robust enough to automate all common high availability / backup / disaster recovery issues. This has proven to be true, apart from the Redis operator which has been our only pain point from a software point of view so far. We are currently rolling out a replacement approach using our own Kubernetes templates instead of relying on an operator at all for Redis. > Now you need to maintain k8s, postgresql, elasticsearch, redis, secret managements, OSs, storage... These are complex systems that require people understanding how they internally work Thanks to Talos Linux (https://www.talos.dev/), maintaining K8s has been a non issue. Running databases via operators has been a non issue, apart from Redis. Secret management via sealed secrets + CLI tooling has been a non issue (https://reclaim-the-stack.com/docs/platform-components/secre...) OS management with Talos Linux has been a learning curve but not too bad. We built talos-manager to manage bootstrapping new nodes to our cluster straight forward (https://reclaim-the-stack.com/docs/talos-manager/introductio...). The only remaining OS related maintenance is OS upgrades, which requires rebooting servers, but that's about it. For storage we chose to go with simple local storage instead of complicated network based storage (https://reclaim-the-stack.com/docs/platform-components/persi...). Our servers come with datacenter grade NVMe drives. All our databases are replicated across multiple servers so we can gracefully deal with failures, should they occur. > Who is going to upgrade kubernetes when they release a new version that has breaking changes? Ugrading kubernetes in general can be done with 0 downtime and is handled by a single talosctl CLI command. Breaking changes in K8s implies changes to existing resource manifest schemas and are detected by tooling before upgrades occur. Given how stable Kubernetes resource schemas are and how averse the community is to push breaking changes I don't expect this to cause major issues going forward. But of course software upgrades will always require due diligence and can sometimes be time consuming, K8s is no exception. > What happens when ElasticSearch decides to splitbrain and your search stops working? ElasticSearch, since major version 7, should not enter split brain if correctly deployed across 3 or more nodes. That said, in case of a complete disaster we could either rebuild our index from source of truth (Postgres) or do disaster recovery from off site backups. It's not like using ElasticCloud protects against these things in any meaningfully different way. However, the feedback loop of contacting support would be slower. > When the DB goes down or you need to set up replication? Operators handle failovers. If we would lose all replicas in a major disaster event we would have to recover from off site backups. Same rules would apply for managed databases. > What is monitoring replication lag? For Postgres, which is our only critical data source. Replication lag monitoring + alerting is built into the operator. It should be straight forward to add this for Redis and ElasticSearch as well. > Or even simply things like disks being close to full? Disk space monitoring and alerting is built into our monitoring stack. At the end of the day I can only describe to you the facts of our experience. We have reduced costs to cover hiring about 4 full time DevOps people so far. But we have hired 0 new engineers and are managing fine with just a few days of additional platform maintenance per month. That said, we're not trying to make the point that EVERYONE should Reclaim the Stack. We documented our thoughts about it here: https://reclaim-the-stack.com/docs/kubernetes-platform/intro... reply swat535 4 hours agorootparentAssuming average salary of 140k/year, you are dedicating 2 resources 3 times a month and this is already costing you ~38k/year on salaries alone and that's assuming your engineers have somehow mastered_both_ devops and software (very unlikely) and that they won't screw anything up. I'm not even counting the time it took you to migrate away.. This also assumes your infra doesn't grow and requires more maintenance or you have to deal with other issues. Focusing on building features and generating revenue is much valuable than wasting precious engineering time maintain stacks. This is hardly a \"win\" in my book. reply rglullis 4 hours agorootparentRight, because your outsourced cloud provider takes absolutely zero time of any application developers. Any issue with AWS and GCP is just one magic support ticket away and their costs already includes top priority support. Right? Right?! reply dangus 4 hours agorootparentHeroku isn’t really analogous to AWS and GCP. Heroku actually is zero effort for the developers. reply JasonSage 4 hours agorootparent> Heroku actually is zero effort for the developers. This is just blatantly untrue. I was an application developer at a place using Heroku for over four years, and I guarantee you we exceeded the aforementioned 2-devs-3-days-per-month in man hours in my time there due to Heroku: - Matching up local env to Heroku images, and figuring out what it actually meant when we had to move off deprecated versions - Peering at Heroku charts because lack of real machine observability, and eventually using Node to capture OS metrics and push them into our existing ELK stack because there was just no alternative - Fighting PR apps to get the right set of env vars to test particular features, and maintaining a set of query-string overrides because there was no way to automate it into the PR deploy I'm probably forgetting more things, but the idea that Heroku is zero effort for developers is laughable to me. I hate docker personally but it's still way less work than Heroku was to maintain, even if you go all the way down the rabbit hole of optimizing away build times et. reply dbackeus 3 hours agorootparentprev> Assuming average salary of 140k/year Is that what developers at your company cost? Just curious. In Sweden the average devops salary is around 60k. > you are dedicating 2 resources 3 times a month and this is already costing you ~38k/year on salaries Ok. So we're currently saving more than 400k/year on our migration. That would be worth 38k/year in salaries to us. But note that our actual salary costs are significantly lower. > that's assuming your engineers have somehow mastered_both_ devops and software (very unlikely) Both me and my colleague are proficient at operations as well as programming. I personally believe the skillsets are complimentary and that web developers need to get into operations / scaling to fully understand their craft. But I've deployed web sites since the 90s. Maybe I'm a of a different breed. We achieved 4 nines of up time in our first year on this platform which is more than we ever achieved using Heroku + other managed cloud services. We won't reach 4 nines in our second year due to a network failure on Hetzner, but so far we have not had downtime due to software issues. > This also assumes your infra doesn't grow and requires more maintenance In general the more our infra grows the more we save (and we're still in the process of cutting additional costs as we slowly migrate more stuff over). Since our stack is automated we don't see any significant overhead in maintenance time for adding additional servers. Potentially some crazy new software could come along that would turn out to be hard to deploy. But if it would be cheaper to use a managed option for that crazy software we could still just use a managed service. It's not like we're making it impossible to use external services by self-hosting. Note that I wouldn't recommend Reclaim the Stack to early stage startups with minor hosting requirements. As mentioned on our site I think it becomes interesting around $5,000/month in spending (but this will of course vary on a number of factors). > Focusing on building features and generating revenue is much valuable than wasting precious engineering time maintain stacks. That's a fair take. But the trade-offs will look different for every company. What was amazing for us was that the developer experience of our platform ended up being significantly better than Heroku's. So we are now shipping faster. Reducing costs by an order of magnitude also allowed us to take on data intensive additions to our product which we would have never considered in the previous deployment paradigm since costs would have been prohibitively high. reply koffiezet 3 hours agorootparent> Just curious. In Sweden the average devops salary is around 60k. Well there's salary, and total employee cost. Now sure how it works in Sweden, but here in Belgium it's a good rule of thumb that an employer pays +- 2,5 times what an employee nets at the end after taxes etc. So say you get a net wage of €3300/month or about €40k/year ends up costing the employer about €100k. I'm a freelance devops/sre/platform engineer, and all I can tell you is that even for long-term projects, my yearly invoice is considerably higher than that. reply ozgune 7 hours agorootparentprevHey there, this is a comprehensive and informative reply! I had two questions just to learn more. * What has been your experience with using local NVMes with K8s? It feels like K8s has some assumptions around volume persistence, so I'm curious if these impacted you at all in production. * How does 'Reclaim the Stack' compare to Kamal? Was migrating off of Heroku your primary motivation for building 'Reclaim the Stack'? Again, asking just to understand. For context, I'm one of the founders at Ubicloud. We're looking to build a managed K8s service next and evaluating trade-offs related to storage, networking, and IAM. We're also looking at Kamal as a way to deploy web apps. This post is super interesting, so wanted to learn more. reply dbackeus 3 hours agorootparentK8s works with both local storage and networked storage. But the two are vastly different from an operations point of view. With networked storage you get fully decoupled compute / storage which allows Kubernetes to reschedule pods arbitrarily across nodes. But the trade off is you have to run additional storage software, end up with more architectural complexity and get performance bottlenecked by your network. Please check out our storage documentation for more details: https://reclaim-the-stack.com/docs/platform-components/persi... > How does 'Reclaim the Stack' compare to Kamal? Kamal doesn't really do much at all compared to RtS. RtS is more or less a feature complete Heroku alternative. It comes with monitoring / log aggregation / alerting etc. also automates High Availability deployments of common databases. Keep in mind 37 signals has a dedicated devops team with 10+ engineers. We have 0 full time devops people. We would not be able to run our product using Kamal. That said I think Kamal is a fine fit for eg. running a Rails app using SQLite on a single server. > Was migrating off of Heroku your primary motivation for building 'Reclaim the Stack'? Yes. Feel free to join the Discord and start a conversation if you want to bounce ideas for your k8s service :) reply troupo 7 hours agorootparentprevSince you're the original creator, can you open the site of your product, and find the link to your project that you open sourced? - Front page links to docs and disord. - First page of docs only has a link to discord. - Installation references a \"get started\" repo that is... somehow also the main repo, not just \"get started\"? reply dbackeus 4 hours agorootparentThe get-started repo is the starting point for installing the platform. Since the platform is gitops based, you'll fork this repo as described in: https://reclaim-the-stack.com/docs/kubernetes-platform/insta... If this is confusing, maybe it would make sense to rename the repo to \"platform\" or something. The other main component is k (https://github.com/reclaim-the-stack/k), the CLI for interacting with the platform. We have also open sourced a tool for deploying Talos Linux on Hetzner called talos-manager: https://github.com/reclaim-the-stack/talos-manager (but you can use any Kubernetes, managed or self-hosted, so this is use-case specific) reply almost 9 hours agoparentprevThe fact that HN seems to think this is \"FUD\" is absolutely wild. You just talked about (some of) the tradeoffs involved in running all this stuff yourself. Obviously for some people it'll be worth and for others not, but absolutely amazing that there are people who don't even seem to accept that those tradoffs exist! reply dzikimarian 4 hours agorootparentI assume you reference my comment. The reason I think parent comment is FUD isn't because I don't acknowledge tradeoffs (they are very real). It's because parent comment implies that people behind \"reclaim the stack\" didn't account for the monitoring, people's cost etc. Obviously any reasonable person making that decision includes it into calculation. Obviously nobody sane throws entire monitoring out of the window for savings. Accounting for all of these it can be still viable and significantly cheaper to run own infra. Especially if you operate outside of the US and you're able to eat an initial investment. reply tucnak 9 hours agoparentprevnext [5 more] [flagged] jusomg 8 hours agorootparentNot sure if this is going to help Heroku's people at all but I feel bad for them now! haha I'm not a Heroku employee. I don't even work in any sort of managed service / platform provider. This is indeed a new account but not a throwaway account! I intended to use it long term. reply almost 8 hours agorootparentprevYou really think that, incredibly lukewarm, argument for Heroku is so extreme that it could only have been written by some kind of undercover shill? reply tucnak 8 hours agorootparentWhy, yes? reply cpach 2 hours agorootparentPlease don’t do this. It’s against HN’s guidelines. Please don't post insinuations about astroturfing, shilling, brigading, foreign agents, and the like. It degrades discussion and is usually mistaken. If you're worried about abuse, email hn@ycombinator.com and we'll look at the data. https://news.ycombinator.com/newsguidelines.html reply Nextgrid 9 hours agoparentprevThis is FUD unless you're running a stock exchange or payment processor where every minute of downtime will cost you hundreds of thousands. For most businesses this is fear-mongering to keep the DevOps & cloud industry going and ensure continued careers in this field. reply The_Colonel 9 hours agorootparentIt's not just about downtime, but also about not getting your systems hacked, not losing your data if sh1t hits the fan, regulation compliance, flexibility (e.g. ability to quickly spin-out new test envs) etc. My preferred solution to this problem is different, though. For most businesses, apps, a monolith (maybe with a few extra services) + 1 relational DB is all you need. In such a simple setup, many of the problems faced either disappear or get much smaller. reply packetlost 5 hours agorootparent> also about not getting your systems hacked... The only systems I have ever seen get compromised firsthand were in public clouds and because they were in public clouds. Most of my career has been at shops that, for one reason or another, primarily own their own infrastructure, cloud represents a rather small fraction. It's far easier to secure a few servers behind a firewall than figure out the Rube Goldberg Machine that is cloud configuration. > not losing your data if sh1t hits the fan You can use off-site backup without using cloud systems, you know? Backblaze, AWS Glacier, etc. are all pretty reasonable solutions. Most of the time when I've seen the need to exercise the backup strategy it's because of some software fuckup, not something like a disk dying. Using a managed database isn't going to save you when the intern TRUNCATEs the prod database on accident (and if something like that happens, it means you fucked up elsewhere). > regulation compliance Most shops would be way better suited to paying a payment processor like Stripe, or other equivalent vendors for similarly protected data. Defense is a whole can of worms, \"government clouds\" are a scam that make you more vulnerable to an unauthorized export than less. > flexibility (e.g. ability to quickly spin-out new test envs) etc. You actually lose flexibility by buying into a particular cloud provider, not gain it. Some things become easier, but many things become harder. Also, IME the hard part of creating reasonable test envs is configuring your edge (ingress, logging infra) and data. reply HolyLampshade 9 hours agorootparentprevSpeaking of the exchanges (at least the sanely operated ones), there’s a reason the stack is simplified compared to most of what is being described here. When some component fails you absolutely do not want to spend time trying to figure out the underlying cause. Almost all the cases you hear in media of exchange outages are due to unnecessary complexity added to what is already a remarkably complex distributed (in most well designed cases) state machine. You generally want things to be as simple and streamlined as possible so when something does pop (and it will) your mean time to resolution is inside of a minute. reply gspencley 5 hours agorootparentprevIt's not FUD, it's pointing out a very real fact that most problems are not engineering problems that you can fix by choosing the one \"magical\" engineering solution that will work for all (or even most) situations. You need to understand your business and your requirements. Us engineers love to think that we can solve everything with the right tools or right engineering solutions. That's not true. There is no \"perfect framework.\" No one sized fits all solution that will magically solve everything. What \"stack\" you choose, what programming language, which frameworks, which hosting providers ... these are all as much business decisions as they are engineering decisions. Good engineering isn't just about finding the simplest or cheapest solution. It is about understanding the business requirements and finding the right solution for the business. reply pphysch 3 hours agorootparentHaving managers (business people) make technical decisions based on marketing copy is how you get 10 technical problems that metastasize into 100 business problems, usually with little awareness of how we got there in the first place. reply gspencley 1 hour agorootparentNice straw-man. I never once suggested that business people should be making technical decisions. What I said was that engineering solutions need to serve the needs of the business. Those are insanely different statements. They are so different that I think that you actively tried to misinterpret my comment so that you could shoot down something I didn't say. reply almost 8 hours agorootparentprevI run a business that is a long long way from a stock exchange or a payment processor. And while a few minutes of downtime is fine 30 minutes or a few hours at the wrong time will really make my customers quite sad. I've been woken in the small hours with technical problems maybe a couple of times over the last 8 years of running it and am quite willing to pay more for my hosting to avoid that happening again. Not for Heroku, they're absolute garbage these days, but definitely for a better run PaaS. Plenty of situations where running it yourself makes sense of course. If you have the people and the skills available (and the cost tradeoffs make sense) or if downtime really doesn't matter much at all to you then go ahead and consider things like this (or possibly simpler self hosting options, it depdns).But no, \"you gotta run kubernettes yourself unless you're a stock exchange\" is not a sensible position. reply Maxion 8 hours agorootparentI don't know why people don't value their time at all. PaaS are so cheap these days for the majority of projects, that it just is not worth it to spend your own time to manage the whole infrastructure stack. If you're forced by regulation or if you just want to do it to learn, than yeah. But if your business is not running infra, or if your infra demands aren't crazy, then PaaS and what-have-you-flavored-cloud-container products will cost you ~1-2 work weeks of a single developer annually. reply sgarland 7 hours agorootparentUnless you already know how to run infra quickly and efficiently. Which – spoiler – you can achieve if you want to learn. reply matus_congrady 10 hours agoparentprevSince DHH has been promoting the 'do-it-yourself' approach, many people have fallen for it. You're asking the right questions that only a few people know they need answers to. In my opinion, the closest thing to \"reclaiming the stack\" while still being a PaaS is to use a \"deploy to your cloud account\" PaaS provider. These services offer the convenience of a PaaS provider, yet allow you to \"eject\" to using the cloud provider on your own should your use case evolve. Example services include https://stacktape.com, https://flightcontrol.dev, and https://www.withcoherence.com. I'm also working on a PaaS comparison site at https://paascout.io. Disclosure: I am a founder of Stacktape. reply ksajadi 16 hours agoprevI’ve been building and deploying thousands of stacks on first Docker, then Mesos, then Swarm and now k8s. If I have learned one thing from it, it’s this: it’s all about the second day. There are so many tools that make it easy to build and deploy apps to your servers (with or without containers) and all of them showcase how easy it is to go from a cloud account to a fully deploy app. While their claims are true, what they don’t talk about is how to maintain the stack, after “reclaiming” it. Version changes, breaking changes, dependency changes and missing dependencies, disaster recovery plans, backups and restores, major shifts in requirements all add up to a large portion of your time. If you have that kind of team, budget or problem that deserves those, then more power to you. reply AnAnonyCowherd 5 hours agoparent> If you have that kind of team, budget or problem that deserves those, then more power to you. This is the operative issue, and it drives me crazy. Companies that can afford to deploy thousands of services in the cloud definitely have the resources to develop in-house talent for hosting all of that on-prem, and saving millions per year. However, middle management in the Fortune 500 has been indoctrinated by the religion that you take your advice from consultants and push everything to third parties so that 1) you build your \"kingdom\" with terribly wasteful budget, and 2) you can never be blamed if something goes wrong. As a perfect example, in my Fortune 250, we have created a whole new department to figure out what we can do with AI. Rather than spend any effort to develop in-house expertise with a new technology that MANY of us recognize could revolutionize our engineering workflow... we're buying Palatir's GenAI product, and using it to... optimize plant safety. Whatever you know about AI, it's fundamentally based on statistics, and I simply can't imagine a worse application than trying to find patterns in data that BY DEFINITION is all outliers. I literally can't even. You smack your forehead, and wonder why the people at the top, making millions in TC, can't understand such basic things, but after years of seeing these kinds of short-sighted, wasteful, foolish decisions, you begin to understand that improving the company's abilities, and making it competitive for the future is not the point. What is the point \"is an exercise left to the reader.\" reply wg0 13 hours agoparentprevThis is absolutely true. I can count easily some 20+ components already. So this is not walk in the park with two willing developers to learn k8s. The underlying apps (Redis, ES) will have version upgrades. Their respective operators themselves would have version upgrades. Essential networking fabric (calico, funnel and such) would have upgrades. The underlying kubernetes itself would have version upgrades. The Talos Linux itself might need upgrades. Of all the above, any single upgrade might lead to infamous controller crash loop where pod starts and dies with little to no indication as to why? And that too no ordinary pod but a crucial pod part of some operator supposed to do the housekeeping for you. k8s is invented at Google and is more suitable in ZIRP world where money is cheap and to change the logo, you have seven designers on payroll discussing for eight months how nine different tones of brand coloring might convey ten different subliminal messages. reply sgarland 7 hours agorootparentTalos is an immutable OS; upgrades are painless and roll themselves back upon failure. Same thing for K8s under Talos (the only thing Talos does is run K8s). reply specialist 5 hours agorootparentTIL \"immutable OS\", thanks. Ages ago, I had the notion of booting from removable read-only media. At the time CD-ROM. Like gear for casting and tabulating votes. Or controllers for critical infra. (Of course, a device's bootloader would have to be ROM too. And boot images would be signed, both digitally and manually.) Maybe \"immutable boot\" and immutable OS can be complimentary. Surely someone's already explored this (obv idea). Worth pondering. reply imiric 12 hours agorootparentprev> The underlying apps (Redis, ES) will have version upgrades. You would have to deal with those with or without k8s. I would argue that without it is much more painful. > Their respective operators themselves would have version upgrades. > > Essential networking fabric (calico, funnel and such) would have upgrades. > > The underlying kubernetes itself would have version upgrades. > > The Talos Linux itself might need upgrades. How is this different from regular system upgrades you would have to do without k8s? K8s does add layers on top that you also have to manage, but it solves a bunch of problems in return that you would have to solve by yourself one way or another. That essential networking fabric gives you a service mesh for free, that allows you to easily deploy, scale, load balance and manage traffic across your entire infrastructure. Building that yourself would take many person-hours and large teams to maintain, whereas k8s allows you to run this with a fraction of the effort and much smaller teams in comparison. Oh, you don't need any of that? Great. But I would wager you'll find that the hodge podge solution you build and have to maintain years from now will take much more of your time and effort than if you had chosen an industry standard. By that point just switching would be a monumental effort. > Of all the above, any single upgrade might lead to infamous controller crash loop where pod starts and dies with little to no indication as to why? Failures and bugs are inevitable. Have you ever had to deal with a Linux kernel bug? The modern stack is complex enough as it is, and while I'm not vouching for increasing it, if those additional components solve major problems for me, and they become an industry standard, then it would be foolish to go against the grain and reinvent each component once I have a need for it. reply mplewis 12 hours agorootparentYou seem to be misunderstanding. The components that add complexity in this case do not come from running a k8s cluster. They come from the Reclaim the Stack software. reply imiric 10 hours agorootparentAlright. So let's discuss how much time and effort it would take to build and maintain a Heroku replacement without k8s then. Besides, GP's criticisms were squarely directed at k8s. For any non-trivial workloads, you will likely use operators and networking plugins. Any of these can have bugs, and will add complexity to the system. My point is that if you find any of those features valuable, then the overall cost would be much less than the alternatives. reply Maxion 7 hours agorootparentThe alternative is not to build a different PaaS alternative, but to simply pay Heroku/AWS/Google/Indie PaaS providers and go back to making your core product. reply imiric 6 hours agorootparentDid you read the reasons they moved away from Heroku to begin with? Clearly what you mention wasn't an option for them, and they consider this project a success. reply benjaminwootton 11 hours agoparentprevThe flip side of this is the cost. Managed cloud services make it faster to get live, but then you are left paying managed service providers for years. I’ve always been a big cloud/managed service guy, but the costs are getting astronomical and I agree the buy vs build of the stack needs a re-evaluation. reply Maxion 7 hours agorootparentThis is the balance, right? For the vast majority of web apps et. al. the cloud costs are going to be cheaper than having full-time Ops people managing an OSS stack on VPS / Bare Metal. reply szundi 13 hours agoparentprevAnd what is your take on all those things that you tried? Some experience/examples would benefit us probably. reply tomwojcik 13 hours agoparentprevAgreed. Forgive a minor digression, but what OP wrote is my problem now. I'm looking for something like heroku's or fly's release command. I have an idea how to implement it in docker using swarm, but I can't figure out how to do that on k8s. I googled it some time ago, but all the answers were hacks. Would someone be able to recommend an approach that's not a hack, for implementing a custom release command on k8s? Downtime is fine, but this one off job needs to run before the user facing pods are available. reply psini 10 hours agorootparentLook at helm charts, they have become the de facto standard for packaging/distributing/deploying/updating whole apps on Kubernetes reply trashburger 7 hours agorootparenthttps://leebriggs.co.uk/blog/2019/02/07/why-are-we-templatin... Something like Jsonnet would serve one better, I think. The only part that kinda sucks is the \"package management\" but that's a small price to pay to avoid the YAML insanity. Helm is fine for consuming third-party packages. reply imiric 13 hours agoparentprevAgreed, but to be fair, those are general problems you would face with any architecture. At least with mainstream stacks you get the benefit of community support, and relying on approaches that someone else has figured out. Container-based stacks also have the benefit of homogeneizing your infrastructure, and giving you a common set of APIs and workflows to interact with. K8s et al are not a silver bullet, but at this point they're highly stable and understood pieces of infrastructure. It's much more painful to deviate from this and build things from scratch, deluding yourself that your approach can be simpler. For trivial and experimental workloads that may be the case, but for anything that requires a bit more sophistication these tools end up saving you resources in the long run. reply bsenftner 8 hours agoparentprevThe thing that strikes me is: okay, two \"willing developers\" - but they need to be actually capable, not just \"willing\" but \"experienced and able\" and that lands you at a minimum of $100k per year per engineer. That means this system has a maintenance cost of over $16K per month, if you have to dedicate two engineers full to the maintenance, and of course following the dynamic nature of K8s and all their tooling just to stay in front of all of that. reply 0perator 6 hours agorootparentAlso, for only two k8s devops engineers in a 24h-available world, you’re gonna be running them ragged with 12h solo shifts or taking the risk of not staffing overnight. Considering most update and backup jobs kick off at midnight, that’s a huge risk. If I were putting together a minimum-viable staffing for a 24x7 available cluster with SLAs on RPO and RTO, I’d be recommending much more than two engineers. I’d probably be recommending closer to five: one senior engineer and one junior for the 8-4 shift, a engineer for the 4-12 shift, another engineer for the 12-8 shift, and another junior who straddles the evening and night shifts. For major outages, this still requires on-call time from all of the engineers, and additional staffing may be necessary to offset overtime hours. Given your metric of roughly $8k an engineer, we’d be looking at a cool $40K/month in labour just to approach four or five 9s of availability. reply oldprogrammer2 6 hours agorootparentprevEven worse, this feels like the goal was actually about reclaiming their resumes, not the stack. I expect these two guys to jump ship within a year, leaving the rest of the team trying to take care of an entire ecosystem they didn't build. reply Maxion 7 hours agorootparentprevAnd you may still end up with longer downtime if SHTF than if you use a managed provider. reply sedatk 12 hours agoparentprev> it’s all about the second day Tangentially, I think this applies to LLMs too. reply rglover 17 hours agoprevI made the mistake of falling for the k8s hype a few years back for running all of my indie hacker businesses. Big mistake. Overnight, the cluster config files I used were no longer supported by the k8s version DigitalOcean auto upgraded my cluster to and _boom_. Every single business was offline. Made the switch to some simple bash scripts for bootstrapping/monitoring/scaling and systemd for starting/restarting apps (nodejs). I'll never look back. reply cedws 10 hours agoparentWeird how defensive people get about K8S when you say stuff like this. It’s like they’re desperately trying to convince you that you really do need all that complexity. reply rollcat 5 hours agorootparentI believe there's still a lot of potential for building niche / \"human-scale\" services/businesses, that don't inherently require the scalability of the cloud or complexity of k8s. Scaling vertically is always easier, modern server hardware has insane perf ceiling. The overall reduction in complexity is a breath of fresh air. My occasional moral dilemma is idle power usage of overprovisioned resources, but we've found some interesting things to throw at idle hardware to ease our conscience about it. reply sswezey 4 hours agorootparentI particularly like this moniker for such human-scale, \"digital gardening\"-type software: https://hobbit.software/ reply 0perator 6 hours agorootparentprevMost do not, but they still want all the toys that developers are building for “the cloud”. reply poincaredisk 16 hours agoparentprevI use k8s for the last uhh 5 years and this never happened to me. In my case, because I self-host my cluster, do no unexpected upgrades. But I agree that maintaining k8s cluster takes some work. reply theptip 4 hours agorootparentIn the 2015-2019 period there were quite a few API improvements involving deprecating old APIs, it’s much more stable/boring now. (Eg TPR -> CRD was the big one for many cluster plugins) reply eddd-ddde 17 hours agoparentprevSo either digital ocean auto updates breaking versions. Or k8s doesn't do versioning correctly. Both very bad. Which was it? reply rglover 16 hours agorootparentTechnically both, but more so the former. I had a heck of a time finding accurate docs on the correct apiVersion to use for things like my ingress and service files (they had a nasty habit of doing beta versions and changing config patterns w/ little backwards compatibility). This was a few years back when your options were a lot of Googling, SO, etc, so the info I found was mixed/spotty. As a solo founder, I found what worked at the time and assumed (foolishly, in retrospect) that it would just continue to work as my needs were modest. reply poincaredisk 16 hours agorootparentprevI assume the first one, but it's more complicated. K8s used to have a lot of features (included very important ones) in the \"beta\" namespace. There are no stability guarantees there, but everyone used them anyway. Over time they graduated to the \"stable\" namespace, and after some transitory period they were removed from the beta namespace. This broke old deployments, when admins ignored warnings for two or three major releases. reply psini 10 hours agorootparentJust want to mention that two or three major releases sounds very bad, but Kubernetes had the insane release cadence of 4(!) major versions every year. reply dmurray 11 hours agorootparentprevIt's an odd choice to break backwards compatibility by removing them from the beta namespace. Why not keep them available in both indefinitely? reply pcthrowaway 11 hours agorootparentProbably because the devs understandably can't account for every possible way people might be using it when shipping new features. But in my experience this means k8s is a bag of fiddly bits that requires some serious ops investments to be reliable for anything serious. reply p_l 9 hours agorootparentprevWith one exception that was rather big change to some low-level stuff, the \"remove beta tags\" was done with about a year or more of runway for people to upgrade. And ultimately, it wasn't hard to upgrade, even if you deal with auto-upgrading cluster and forgot about it, because \"live\" deployments got auto-upgraded - you do need to update your deployment script/whatever though. reply port19 11 hours agoparentprev> simple bash scripts for bootstrapping/monitoring/scaling Damn, that's the dream right there reply nine_k 14 hours agoparentprevHow does it compare to a simpler but not hand-crafted solution, such as dokku? reply rglover 2 hours agorootparentNo Docker for starters. I played with Dokku a long time ago and remember it being decent at that time, but still too confusing for my skillset. Now, I just build my app to an encrypted tarball, upload it to a secure bucket, and then create a short-lived signed URL for instances to curl the code from. From there, I just install deps on the machine and start up the app with systemd. IMO, Docker is overkill for 99% of projects, perhaps all. One of those great ideas, poorly executed (and considering the complexity, I understand why). reply mythz 7 hours agoparentprevWe're also ignoring Kubernetes and are just using GitHub Actions, Docker Compose and SSH for our CI Deployments [1]. After a one-time setup on the Deployment Server, we can deploy new Apps with just a few GitHub Action Secrets, which then gets redeployed on every commit, including running any DB Migrations. We're currently using this to deploy and run over 50 .NET Apps across 3 Hetzner VMs. [1] https://servicestack.net/posts/kubernetes_not_required reply oldprogrammer2 6 hours agorootparentThe amount of complexity people are introducing into their infrastructure is insane. At the end of the day, we're still just building the same CRUD web apps we were building 20 years ago. We have 50x the computation power, much faster disk, much more RAM, and much faster internet. A pair of load-balanced web servers and a managed database, with Cloudflare out front, will get you really, really far. reply minkles 12 hours agoparentprevThe first live k8s cluster upgrade anyone has to do is usually when they think \"what the fuck did I get myself in to?\" It's only good for very large scale stuff. And then a lot of the time that is usually well over provisioned and could be done considerably cheaper using almost any other methodology. The only good part of Kubernetes I have found in the last 4 years of running it in production is that you can deploy any old limping crap to it and it does its best to keep it alive which means you can spend more time writing YAML and upgrading it every 2 minutes. reply akvadrako 13 hours agoparentprevEKS has a tab in the dashboard that warns about all the deprecated configs in your cluster, making it pretty foolproof to avoid this by checking every couple years. reply hhh 10 hours agorootparentYes, and there are many open source tools that you can point at clusters to do the same. We use Kubent (Kube No Troubles) to do the same. reply w0m 14 hours agoparentprevyeouch. sorry man. I've been running in AKS for 3-4 years now and never had an auto-upgrade come in I wasn't expecting. I have been ontop of alerts and security bulletins though, may have kept me ahead of the curve. reply willvarfar 12 hours agorootparentI was once on a nice family holiday and broke my resolve and did a 'quick' check of my email and found a nastygram billing reminder from a provider. On the one hand I was super-lucky I checked my mail when I did, and on the other I didn't get he holiday I needed and was lucky to not spill over and impact my family's happiness around me. reply tucnak 9 hours agoparentprevSo what is the alternative? Nomad? reply llama052 16 hours agoparentprevSo you had auto update enabled on your cluster and didn’t keep your apiversions up to date? Sounds like user error. reply rvense 12 hours agorootparentOne of my main criteria for evaluating a platform would be how easy it is to make user errors. reply psini 10 hours agorootparentprevTo be honest the API versions have been a lot more stable recently but back in ~2019 when I first used Kube in production, basic APIs were getting deprecated left and right, 4 times a year; in the end yes the problems are \"on you\" but it so easy to miss and the results so disastrous for a platform whose selling points are toughness resilience and self-healing reply subarctic 18 hours agoprevI wish _I_ had a business that was successful enough to justify multiple engineers working 7 months on porting our infrastructure from heroku to kubernetes reply bastawhiz 18 hours agoparentKnowing the prices and performance of Heroku (as a former customer) the effort probably paid for itself. Heroku is great for getting started but becomes untenably expensive very fast, and it's neither easy nor straightforward to break the vendor lock in when you decide to leave. reply danenania 17 hours agorootparentI find AWS ECS with fargate to be a nice middle ground. You still have to deal with IAM, networking, etc. but once you get that sorted it’s quite easy to auto-scale a container and make it highly available. I’ve used kubernetes as well in the past and it certainly can do the job, but ECS is my go-to currently for a new project. Kubernetes may be better for more complex scenarios, but for a new project or startup I think having a need for kubernetes vs. something simpler like ECS would tend to indicate questionable architecture choices. reply wg0 12 hours agorootparentECS is far, far far smoother, simpler and stable than anything else out there in cluster orchestration. It just works. Even with EC2 instances it just works. And if you opt for Fargate, then that's far more stable option. I am saying this after bootstrapping k8s and ECS both. reply Aeolun 9 hours agorootparentThe only pain point there I think is auto scaling logic. But otherwise it’s painless. reply danenania 4 hours agorootparentI find auto-scaling with fargate to be pretty straightforward. What's the pain point there for you? reply subarctic 7 hours agorootparentprevHow does it compare with fly.io? Last I checked, startup time is still in minutes instead of less than a second on fly, but I presume it's more reliable and you get that \"nobody ever got fired for using AWS\" effect reply danenania 4 hours agorootparentFly is really cool and it's definitely an extremely quick way to get a container running in the cloud. I haven't used it in production so I can't speak to reliability, but for me the main thing that stops me from seriously considering it is the lack of an RDS equivalent that runs behind the firewall. reply dangus 4 hours agorootparentprevFly.io is an unreliable piece of shit. reply bastawhiz 16 hours agorootparentprevI pretty strongly agree. Fargate is a great product, though it isn't quite perfect. reply hannofcart 4 hours agorootparentprev+1 on this. ECS Fargate is great. reply chihwei 11 hours agorootparentprevGCP Cloud Run is even better, which you don't have to configure those networking stuff, just ship and run in production reply danenania 4 hours agorootparentDoes Cloud Run give you a private network? While configuring it is annoying, I do want one for anything serious. reply internetter 16 hours agorootparentprevFrom their presentation, they went from $7500/m to $500/m reply dbackeus 3 hours agorootparentWe've since migrated more stuff. We're currently saving more than $400k/year. reply grogenaut 14 hours agorootparentprevassume a dev is $100k/year... so $200k with taxes, benes, etc. That's 16,666/month, at 1.5 months is 25k. So it'll take 3.5 months to break even. And they'd save around .8 of their pay, or .4 of their total cost a year... Generally I am hoping my devs are working a good multiplier to their pay for revenue they generate. Not sure I'd use them this way if there was other things to do. That said sounds like it was mainly for GDPR so. reply icameron 8 hours agorootparentWhere are you finding capable DevOps engineers for 100k total comp? It’s hard to find someone with the skills to rebuild a SaaS production stack who’s willing to work for that little around here! reply Maxion 7 hours agorootparent100k Eur is high salary for a dev. A unicorn who knows they are one, won't agree to that salary, but would for 150k or 200k. reply subarctic 8 hours agorootparentprevEurope, probably reply whizzter 7 hours agorootparentCorrect, mynewsdesk (that created this reclaim the stack thing) is a Swedish company. reply p_l 9 hours agorootparentprevNow consider that some places are not in Silly Valley, or not even in USA, and the fully loaded cost of engineer (who, once done with on-prem or at least more \"owned\" stack, can take on other problems) can be way, way lower reply ttul 16 hours agorootparentprevI mean, $7,000 a month isn’t nothing. But it’s not a lot. Certainly not enough to justify a seven month engineering effort plus infinite ongoing maintenance. reply nine_k 14 hours agorootparentThis is $7k/mo today. If they are actively growing, and their demand for compute is slated to grow 5x or even 10x in a year, they wanted to get off Heroku fast. reply crdrost 15 hours agorootparentprevThe main engineering effort to reduce by that much was completed in 6 weeks according to their YouTube video. 7 months is presumably more like “the time it has been stable for” or so, although I am not sure the dates line up for that 100%. Also cost reduction was apparently not the main impetus, GDPR compliance was. reply ramshorst 10 hours agorootparentprevWould you say one person not working 100% of the time is also quite minor? ;) reply Aeolun 9 hours agorootparentSure. We have around 10 of those. It’s a significant boon to the project for them to do nothing. reply cpursley 18 hours agorootparentprevMoving from Heroku to Render or Fly.io is very straight forward; it’s just containers. reply ed 14 hours agorootparent(Except for Postgres, since Fly's solution isn't managed) Heroku's price is a persistent annoyance for every startup that uses it. Rebuilding Heroku's stack is an attractive problem (evidenced by the graveyard of Heroku clones on Github). There's a clear KPI ($), Salesforce's pricing feels wrong out of principle, and engineering is all about efficiency! Unfortunately, it's also an iceberg problem. And while infrastructure is not \"hard\" in the comp-sci sense, custom infra always creates work when your time would be better spent elsewhere. reply davedx 13 hours agorootparent> Salesforce's pricing feels wrong out of principle What do you mean exactly? If it takes multiple engineers multiple months to build an alternative on kubernetes, then it sounds like Heroku is worth it to a lot of companies. These costs are very \"known\" when you start using Heroku too, it's not like Salesforce hides everything from you then jump scares you 18 months down the line. SF's CRM is also known to be expensive, and yet it's extremely widely used. Something being expensive definitely doesn't always mean it's bad and you should cheap out to avoid it. reply mrweasel 11 hours agorootparentprevCouldn't you move to AWS? They offer managed Postgresql. Heroku already runs on AWS, so there could be a potential saving in running AWS managed service. It's still a lot of work obviously. reply Maxion 7 hours agorootparentSo does GCP and Azure. At least in GCP land the stuff is really quite reasonably priced, too. reply bastawhiz 17 hours agorootparentprevIf you use containers. If you're big enough for the cost savings to matter, you're probably also not looking for a service like Render or Fly. If your workload is really \"just containers\" you can save more with even managed container services from AWS or GCP. reply OJFord 17 hours agorootparentWe are talking about moving from Heroku, I don't think being too needy for the likes of Fly is at all a given. (And people will way prematurely think they're too big or needy for x.) reply goosejuice 17 hours agorootparentprevSo is kubernetes. GKE isn't that bad. reply dartos 17 hours agorootparentprevUnless you relied on heroku build packs. reply debarshri 17 hours agorootparentBuildpacks is opensource too [1] [1] https://buildpacks.io/ reply antimemetics 13 hours agorootparentprevI mean this is what they recommend: - Your current cloud / PaaS costs are north of $5,000/month - You have at least two developers who are into the idea of running Kubernetes and their own infrastructure and are willing to spend some time learning how to do so So you will spend 150k+/year (2 senior full stake eng salaries in EU - can be much higher, esp for people up to the task) to save 60k+/y in infra costs? Does not compute for me - is the lock-in that bad? I understand it for very small/simple use cases - but then do you need k8s at all? It feels like the ones who will benefit the most is orgs who spend much more on cloud costs - but they need SLAs, compliance and a dozen other enterprisy things. So I struggle to understand who would benefit from this stack reclaim. reply dbackeus 8 hours agorootparentCreator of Reclaim the Stack here. The idea that we're implying you need 2 full time engineers is a misunderstanding. We just mean to say that you'll want at least 2 developers to spend enough time digging in to Kubernetes etc to have a good enough idea of what you're doing. I don't think more than 2 month of messing about should be required to reach proficiency. We currently don't spend more than ~4 days per month total working on platform related stuff (often we spend 0 days, eg. I was on parental leave during 3 months and no one touched the platform during that time). WRT employee cost, Swedish DevOps engineers cost less than half of what you mentioned on average, but I guess YMMV depending on region. reply efilife 16 hours agoparentprevFyi, we use asterisks (*) for emphasis on HN reply willvarfar 12 hours agorootparentunderscores around italics and asterisk around strong/bold was an informal convention on bbs, irc and forums way before atx/markdown. reply efilife 31 minutes agorootparentI'm talking about the HN markup, italics don't work here, only asterisks do reply Kiro 13 hours agorootparentprevDifferent thing. Using visible _ is a conscious choice. reply efilife 13 hours agorootparentWhy? reply Kiro 12 hours agorootparentIt looks nice and has been a staple in hacker culture for decades, long before we had rich text and were just chatting on IRC. reply efilife 4 hours agorootparentIt doesn't look nice at all to me. Real emphasis looks way nicer, that's its purpose. Now that we have rich text, why not utilize it? reply sph 11 hours agorootparentprevAlso it looks like _underlined_ text reply komali2 10 hours agorootparentprevWho's \"we?\" reply mkl 8 hours agorootparentEveryone who uses italics on HN, which is a lot of us: https://news.ycombinator.com/formatdoc reply efilife 4 hours agorootparentprevMost of HN users reply fourseventy 19 hours agoprevIn my experience you can get pretty far with just a handful of vms and some bash scripts. At least double digit million ARR. Less is more when it comes to devops tooling imo. reply lolinder 18 hours agoparent> you can get pretty far with just a handful of vms and some bash scripts. At least double digit million ARR. Using ARR as the measurement for how far you can scale devops practices is weird to me. Double-digit million ARR might be a few hundred accounts if you're doing B2B, and double-digit million MAUs if you're doing an ad-funded social platform. Depending on how much software is involved your product could be built by a team of anywhere from 1-50 developers. If you're a one-developer B2B company handling 1-3 requests per second you wouldn't even need more than one VM except maybe as redundancy. But if you're the fifty-developer company that's building something beyond simple CRUD, there are a lot of perks that come with a full-fledged control plane that would almost certainly be worth the added cost and complexity. reply davedx 12 hours agorootparent> there are a lot of perks that come with a full-fledged control plane that would almost certainly be worth the added cost and complexity. Such as? Logging is more complicated with multi container microservice deployments. Deploying is more complicated. Debugging and error tracing is more difficult. What are the perks? reply The_Colonel 9 hours agorootparentYou get more tools to mitigate those problems. Those tools add more complexity to the system, but that's of course solvable by higher level tools. reply Maxion 7 hours agorootparentprevI used to work at a Fintech company where we had around 1-20k concurrent active users, monthly around 2 million active users. I forget the RPS, but it was maybe around 200-1000 normally? We ran on bare metal, bash scripts, not a container in sight. It was spaghetti, granted, but it worked surprisingly well. reply marcosdumay 17 hours agorootparentprev> double-digit million MAUs I was about to make a similar point, but you made the math, and it's holding-up for the GP's side. You can push vms and direct to ssh synchronization up to double-digit million MAU (unless you are using stuff like persistent web-sockets). It won't be pretty, but you can get that far. reply lolinder 17 hours agorootparentI'm not concerned about handling the requests for the main user-facing application (as you say, you can get way further with a single box than many people think), I'm thinking about all of the additional complexity that comes with serving multiple millions of human users that wouldn't exist if you were just serving a few hundred web scrapers that happen to produce as much traffic as multiple millions of humans. What those sources of complexity are depends a lot on the product, but some examples include admin tooling for your CS department, automated content moderation systems, more thorough logging and monitoring, DDOS mitigation, feature flagging and A/B testing, compliance, etc. Not to mention the overhead of coordinating the work of 50 developers vs 1—deploying over SSH is well and good when you can reasonably expect a small handful of people to need to do it, but automatic deploys from main deployed from a secure build machine is a massive boon to the larger team. Any one of these things has an obvious answer—just add ${software} to your one VM or create one extra bare-metal build server or put your app behind Cloudflare—but when you have a few dozen of these sources of complexity then AWS's control plane offerings start to look very attractive. And once you have 50 developers on the payroll spending a few hundred a month on cloud to avoid hand-rolling solutions isn't exactly a hard sell. reply sanderjd 18 hours agoparentprevOf course you can get away with that if your metric is revenue. (I think Blippi makes about that much with, I suspect, nary a VM in sight! The question is what you're doing with your infrastructure, not how much revenue you're making. Some things have higher return to \"devops\" and others have less. reply kevin_nisbet 17 hours agoparentprevI agree, this is an incredibly valid approach for some companies and startups. If you benefit by being frugal and are doing something that doesn't need incredible availability, a rack of servers in a colo doesn't cost much and you can take it pretty far without a huge amount of effort. reply cloudking 18 hours agoparentprev+1 or just use App Engine, deploy your app and scale reply cglace 18 hours agorootparentApp engine deploys are soooo slow. I liked cloud run a lot more. reply strzibny 17 hours agoprevIt's good to see new projects. However most people shouldn't start with Kubernetes at all. If you don't need autoscaling, give Kamal[0] a go. It's the tool 37signals made to leave Kubernetes and cloud. Works super well with simple VMs. I also wrote a handbook[1] to get people started. [0] https://kamal-deploy.org [1] https://kamalmanual.com/handbook/ reply dbackeus 3 hours agoparent(Reclaim the Stack creator here) We don't do autoscaling. The main reason for Kubernetes for us was automation of monitoring / logs / alerting and highly available database deployments. 37signals has a dedicated operations team with more than 10 people. We have 0 dedicated operations people. We would not have been able to run our product with Kamal given our four nines uptime target. (that said, I do like Kamal, especially v2 seems to smooth out some edges, and I'm all for simple single server deployments) reply leohonexus 13 hours agoparentprevBought both your books, they are awesome :) reply mplewis 12 hours agoparentprevI’m not going to trust a project like this – made by and for one company – with production workloads. reply rcaught 10 hours agorootparenthahaha, do you even realize what else this company makes? reply notpushkin 18 hours agoprevIt looks like a nice Kubernetes setup! But I don’t see how this is comparable to something like Heroku – the complexity is way higher from what I see. If you’re looking for something simpler, try https://dokku.com/ (the OG self-hosted Heroku) or https://lunni.dev/ (which I’ve been working on for a while, with a docker-compose based workflow instead). (I've also heard good things about coolify.io!) reply thetopher 19 hours agoprev“Our basic philosophy when it comes to security is that we can trust our developers and that we can trust the private network within the cluster.” This is not my area of expertise. Does it add a significant amount of complexity to configure this kind of system in a way that doesn’t require trusting the network? Where are the pain points? reply stouset 19 hours agoparent> Our basic philosophy when it comes to security is that we can trust our developers and that we can trust the private network within the cluster. As an infosec guy, I hate to say it but this is IMO very misguided. Insider attacks and external attacks are often indistinguishable because attackers are happy to steal developer credentials or infect their laptops with malware. Same with trusting the private network. That’s fine and dandy until attackers are in your network, and now they have free rein because you assumed you could keep the bad people outside the walls protecting your soft, squishy insides. reply apitman 16 hours agorootparentWhat's your opinion on EDR in general? I find it very distasteful from a privacy perspective, but obviously it could be beneficial at scale. I just wish there was a better middle ground. reply yodelshady 9 hours agorootparentNot the OP but I was on that side - They do work. My best analogy is it's like working at TSA except there are three terrorist attacks per week. As far as privacy goes, by the same analogy, I can guarantee the operators don't care what porn you watch. Doing the job is more important. But still, treat your work machine as a work machine. It's not yours, it's a tool your company lent to you to work with. That said, on HN your workers are likely to be developers - that does take some more skill, and I'd advise asking a potential provider frank questions about their experience with the sector, as well as your risk tolerance. Devs do dodgy stuff all the time, and they usually know what they're doing, but when they don't you're going to have real fun proving you've remediated. reply tryauuum 6 hours agorootparentprevEDR is not related to the topic but now I'm curious as well. Any good EDR for ubuntu server? reply jonstewart 18 hours agorootparentprevOne of the best things you can do is restrict your VPCs from accessing the internet willy-nilly outbound. When an attacker breaches you, this can keep them from downloading payloads and exfiltrating data. reply sanderjd 18 hours agorootparentIn the scenario presented, can't they just exfiltrate using the developer credentials / machine? reply jonstewart 17 hours agorootparentLet’s say there’s a log4j-type vuln and your app is affected. So an attacker can trigger an RCE in your app, which is running in, say, an EC2 instance in a VPC. A well-configured app server instance will have only necessary packages on it, and hopefully not much for dev tools. The instance will also run with certain privileges through IAM and then there won’t be creds on the instance for the attacker to steal. Typically an RCE like this runs a small script that will download and run a more useful piece of malware, like a webshell. If the webshell doesn’t download, the attacker probably is moving onto the next victim. reply sanderjd 16 hours agorootparentBut the original comment wasn't about this attack vector... > attackers are happy to steal developer credentials or infect their laptops with malware I don't think any of what you said applies when an attacker has control of a developer machine that is allowed inside the network. reply jiggawatts 18 hours agorootparentprevYou’ve just broken a hundred things that developers and ops staff need daily to block a theoretical vulnerability that is irrelevant unless you’re already severely breached. This kind of thinking is why secops often develops an adversarial relationship with other teams — the teams actually making money. I’ve seen this dynamic play out dozens of times and I’ve never seen it block an attack. I have seen it tank productivity and break production systems many times however. PS: The biggest impact denying outbound traffic has is to block Windows Update or the equivalent for other operating systems or applications. I’m working with a team right now that has to smuggle NPM modules in from their home PCs because they can’t run “npm audit fix” successfully on their isolated cloud PCs. Yes, for security they’re prevented from updating vulnerable packages unless they bend over backwards. reply bigfatkitten 18 hours agorootparent> I’ve seen this dynamic play out dozens of times and I’ve never seen it block an attack. I am a DFIR consultant, and I've been involved in 20 or 30 engagements over the last 15 years where proper egress controls would've stopped the adversary in their tracks. reply jiggawatts 13 hours agorootparentAny statement like that qualified with “proper” is a no true Scotsman fallacy. What do you consider proper egress blocking? No DNS? No ICMP? No access to any web proxy? No CDP or OCSP access? Strict domain-based filtering of all outbound traffic? What about cloud management endpoints? This can get to the point that it becomes nigh impossible to troubleshoot anything. Not even “ping” works! And troubleshoot you will have to, trust me. You’ll discover that root cert updates are out-of-band and not included in some other security patches. And you’ll discover that the 60s delay that’s impossible to pin down is a CRL validating timeout. You’ll discover that ICMP isn’t as optional as you thought. I’ve been that engineer, I’ve done this work, and I consider it a waste of time unless it is protecting at least a billion dollars worth of secrets. PS: practically 100% of exfiltrated data goes via established and approved channels such as OneDrive. I just had a customer send a cloud VM disk backup via SharePoint to a third party operating in another country. Oh, not to mention the telco that has outsourced core IT functions to both Chinese and Russian companies. No worries though! They’ve blocked me from using ping to fix their broken network. reply inhumantsar 17 hours agorootparentprevthere's no need for this to be an either/or decision. private artifact repos with the ability to act as a caching proxy are easy to set up. afaik all the major cloud providers offer basic ones with the ability to use block or allow lists. going up a level in terms of capabilities, JFrog is miserable to deal with as a vendor but Artifactory is hard to beat when it comes to artifact management. reply junto 13 hours agorootparentTheir caching proxy sucks though. We had to turn it off because it persistently caused build issues due to its unreliability. reply jiggawatts 13 hours agorootparentprevSure… for like one IDE or one language. Now try that for half a dozen languages, tools, environments, and repos. Make sure to make it all work for build pipelines, and not just the default ones either! You need a bunch of on-prem agents to work around the firewall constraints. This alone can keep multiple FTEs busy permanently. “Easy” is relative. Maybe you work in a place with a thousand devs and infinite VC money protecting a trillion dollars of intellectual property then sure, it’s easy. If you work in a normal enterprise it’s not easy at all. reply coryrc 17 hours agorootparentprevI can't be certain, but I think the GP means production VMs not people's workstations. Or maybe I fail to understand the complexities you have seen, but I'm judging my statement especially on the \"download from home\" thing which seems only necessary if you packed full Internet access on your workstation. reply jiggawatts 13 hours agorootparentThe entire network has a default deny rule outbound. Web traffic needs to go via authenticating proxies. Most Linux-pedigree tools don’t support authenticating proxies at all, or do so very poorly. For example, most have just a single proxy setting that’s either “on” or “off”. Compare that to PAC files typically used in corporate environments that implement a fine grained policy selecting different proxies based on location or destination. It’s very easy to get into a scenario where one tool requires a proxy env var that breaks another tool. “Stop complaining about the hoops! Just jump through them already! We need you to do that forever and ever because we might get attacked one day by an attacker that’ll work around the outbound block in about five minutes!” reply jonstewart 17 hours agorootparentprev> You’ve just broken a hundred things that developers and ops staff need daily to block a theoretical vulnerability that is irrelevant unless you’re already severely breached. I’m both a developer and a DFIR expert, and I practice what I preach. The apps I ship have a small allowlist for necessary external endpoints and everything else is denied. Trust me, your vulnerabilities aren’t theoretical, especially if you’re using Windows systems for internet-facing prod. reply bigfatkitten 14 hours agorootparentThis should still be fresh in the mind of anyone who was using log4j in 2021. reply bigfatkitten 17 hours agorootparentprevIt's a mindset that keeps people like you and I employed in well-paying jobs. reply callalex 19 hours agoparentprevThe top pain point is that it requires setting up SSL certificate infrastructure and having to store and distribute those certs around in a secure way. The secondary effects are entirely dependent on how your microservices talk to their dependencies. Are they already talking to some local proxy that handles load balancing and service discovery? If so, then you can bolt on ssl termination at that layer. If not, and your microservice is using dns and making http requests directly to other services, it’s a game of whack-a-mole modifying all of your software to talk to a local “sidecar”; or you have to configure every service to start doing the SSL validation which can explode in complexity when you end up dealing with a bunch of different languages and libraries. None of it is impossible by any means, and many companies/stacks do all of this successfully, but it’s all work that doesn’t add features, can lead to performance degradation, and is a hard sell to get funding/time for because your boss’s boss almost certainly trusts the cloud provider to handle such things at their network layer unless they have very specific security requirements and knowledge. reply agf 18 hours agoparentprevYes, it adds an additional level of complexity to do role-based access control within k8s. In my experience, that access control is necessary for several reasons (mistakes due to inexperience, cowboys, compliance requirements, client security questions, etc.) around 50-100 developers. This isn't just \"not zero trust\", it's access to everything inside the cluster (and maybe the cluster components themselves) or access to nothing -- there is no way to grant partial access to what's running in the cluster. reply jandrewrogers 15 hours agoparentprevThis is just bad security practice. You cannot trust the internal network, so many companies have been abused following this principle. You have to allow for the possibility that your neighbors are hostile. reply umvi 19 hours agoparentprevImplementing \"Zero Trust\" architectures are definitely more onerous to deal with for everyone involved (both devs and customers, if on prem). Just Google \"zero trust architecture\" to find examples. A lot more work (and therefore $) to setup and maintain, but also better security since now breaching network perimeter is no longer enough to pwn everything inside said network. reply zymhan 19 hours agoparentprevIt requires encrypting all network traffic, either with something like TLS, or IPSec VPN. reply nilsherzig 19 hours agoparentprev\"SSL added and removed here :^)\" reply airstrike 18 hours agoprev> We spent 7 months building a Kubernetes based platform to replace Heroku for our SaaS product at mynewsdesk.com. The results were a 90% reduction in costs and a 30% improvement in performance. I don't mean to sound dismissive, but maybe the problem is just that Heroku is/was slow and expensive? Meaning this isn't necessarily the right or quote-unquote \"best\" approach to reclaiming the stack reply AbuAssar 19 hours agoprevHow does this compare to dokku (https://dokku.com/)? reply dbackeus 18 minutes agoparentMain difference is that Dokku is a simple single server platform, geared mostly toward hobby projects. Reclaim the Stack provides a fully highly available multi node platform to host large scale SaaS applications aiming for four nines of uptime. reply Summerbud 16 hours agoprev> The results were a 90% reduction in costs and a 30% improvement in performance. I am in a company with dedicated infra team and my CEO is a infra enthusiastic. He use terraform and k8s to build the company's infra. But the results are. - Every deployment take days, in my experience, I need to woke for 24 hr streak to make it work. - The infra is complicated to a level that quite hard to adjust And benefits wise, I can't even think about it. We don't have many users so the claimed scalability is not even there. I will strongly argue startup should not touch k8s until you have fair user base and retention. It's a nightmare to work with. reply raziel2p 13 hours agoparentsounds like your CEO just isn't very good at setting up infra. reply Summerbud 12 hours agorootparentMaybe, that is one of the possibilities in my mind too. reply cultofmetatron 8 hours agoparentprevDAYS??? our infra takes 10 min usually with up to 45 min if we're doing some postgres maintenance stuff. People in a work context should stick to what they are good at. reply tryauuum 6 hours agoparentprev...but why? How many services the deployment requires? reply appplication 19 hours agoprevThis sounds great, I’ll be building our prod infra stack and deploying to cloud for the first time here in the next few weeks, so this is timely. It’s nice seeing some OSS-based tooling around k8s. I know it’s a favorite refrain that “k8s is unnecessary/too complex, you don’t need it” for many folks getting started with their deployments, but I already know and use it in my day job, so it feels like a pretty natural choice. reply dbackeus 16 minutes agoparentFeel free to join the RtS discord if you want to bounce ideas for your upcoming infra reply notpushkin 18 hours agoparentprevI really hated Kubernetes at first because the tooling is so complicated. However, having worked with raw Docker API and looking into the k8s counterparts, I’m starting to appreciate it a lot more. (But it still needs more accessible tooling! Kompose is a good start though: https://kompose.io/) reply briandear 19 hours agoparentprevThe K8s is unnecessary meme is perpetuated by people that don’t understand it. reply actionfromafar 19 hours agorootparentTrue, but also, sometimes it’s not needed. reply jauntywundrkind 15 hours agorootparentSometimes it just feels good wearing a fig leaf around my groin, weilding a mid sized log as a crude club, & running through the jungle. You might not need it is the kernel of doubt that can undermine any reasonable option. And it suggests nothing. Sure, you can go write your own kernel! You can make your own database! You might not need to use good well known proven technology that people understand and can learn about online! You can do it yourself! Or cobble together some alternate lesser special stack that just you have distilled out. We don't need civilization. We can go it alone & do our own thing, leave behind shared frames of references. But damn, it just seems so absurdly inadvisable, and it feels so overblown the fear uncertainty & doubt telling us Kubernetes is hard and bad and too much. This article does certainly lend credence to the idea that Kubernetes is complex, but there's so many simpler starting places that will take many teams very far. reply samatman 14 hours agorootparentSomehow kubernetes and civilization just aren't in the same category of salience to me. Like I think it's reasonable to say that kubernetes is optional in a way which civilization isn't. Like maybe one of those things is more important. than, the other reply jauntywundrkind 1 hour agorootparentI don't disagree, and there's plenty of room for other competitors to arise. We see some Kamal mentions. Microsoft keeps trying to make Dapr a thing, godspeed. But very few other options exist that have the same scope scale & extensibility, that allow them to become broadly adopted platform infrastructure. The folks saying you might not need Kubernetes, in my view, do a massive disservice by driving people to fragmentedly piece by piece constructing their own unique paths, rather than being a part of something broader. In my view theres just too many reasons why you want your platform to be something socially prevalent, to be well travelled by others too, and right now there are few other large popular extensible platforms that suit this beyond Kubernetes. reply sph 11 hours agorootparentprevk8s is relatively straightforward, it's the ecosystem around it that is total bullcrap, because you won't only run k8s, you will also run Helm, a templating language or an ad-hoc mess of scripts, a CNI, a CI/CD system, operators, sidecars, etc. and every one of these is an over-engineered buggy mess with half a dozen hyped alternatives that are in alpha state with their own set of bugs. How Kubernetes works is pretty simple, but administering it is living a life of constant analysis paralysis and churn and hype cycles. It is a world built by companies that have something to sell you. reply freeopinion 15 hours agorootparentprevIf they don't understand it but still get their jobs done... Tractors are also unnecessary. Plenty of people grow tomatos off their balcony without tractors. If somebody insists on growing 40 acres of tomatos without a tractor because tractors aren't necessary, why argue with them? If they try to force you to not use a tractor, that's different. reply okasaki 9 hours agorootparentprevJust had an incident call last week with 20+ engineers on zoom debugging a prod k8s cluster for 5 hours. reply deisteve 17 hours agoprevi got excited until i saw this was kubernetes. you most certainly do not need to add that layer of complexity. If I can serve 3 million users / month on a $40/month VPS with just Coolify, Postgres, Nginx, Django Gunicorn without Redis, RabbitMQ why should I use Kubernetes? reply dbackeus 3 hours agoparentCoolify does look nice. But I don't believe it supports HA deployments of Postgres with automated failover / 0 downtime upgrades etc? Do they even have built in backup support? (a doc exists but appears empty: https://coolify.io/docs/knowledge-base/database-backups) What makes you feel that Coolify is significantly less complex than Kubernetes? reply mrweasel 11 hours agoparentprev> why should I use Kubernetes You shouldn't, but people have started to view Kubernetes as a deployment tool. Kubernetes makes sense when you start having bare metal workers, or high number of services (micro-services). You need to have a pretty dynamic workload for Kubernetes to result in any cost saving on the operations side. There might be a cost saving if it's easier to deploy your services, but I don't see that being greater than the cost of maintaining and debugging a broken Kubernetes cluster in most case. The majority of uses does not require Kubernetes. The majority of users who think they NEED Kubernetes are wrong. That's not to say that you shouldn't use it, if you believe you get some benefit, it's just not your cheapest option. reply Kiro 13 hours agoparentprevWhy do you need Coolify? reply itsthecourier 17 hours agoparentprevGot a bill from usd10k to usd0.5k a month by moving away from gcp to Kamal in ovh And 30% less latency reply deisteve 17 hours agorootparentthats 95% in savings!!!! bet you can squueze more with hetzner to ppl who disagree, what business justifies 18x'ing your operating costs? 9.5k USD can get you 3 senior engineers in Canada. 9 in India. reply deznu 16 hours agorootparentSenior Engineers cost ~$3k a month in Canada?? Seems far-fetched.. reply chaboud 15 hours agorootparentWe must have very different definitions of senior engineer from the GP, because I’d put the monthly cost of a senior engineer closer to $30k than $3k, even on a log scale. Employing people requires insurance, buildings, hardware, support, licenses, etc. There are lower cost locations, but I can’t think of a single market on earth where there is a supply of senior engineers that cost $3k/month. And I say this being familiar with costs in India, China, Poland, Spain, Mexico, Costa Rica, and at least a dozen other regions. reply aliasxneo 13 hours agoprevSince there are so many mixed comments here, I'll share my experience. Our startup started on day one with Kubernetes. It took me about six weeks to write the respective Terraform and manifests and combine them into a homogenous system. It's been smooth sailing for almost two years now. I'm starting to suspect the wide range of experiences has to do with engineering decisions. Nowadays, it's almost trivial to over-engineer a Kubernetes setup. In fact, with platform engineering becoming all the rage these days, I can't help but notice how over-engineered most reference architectures are for your average mid-sized company. Of course, that's probably by design (Humanitec sure enjoys the money), but it's all completely optional. I intentionally started with a dead-simple EKS setup: flat VPC with no crazy networking, simple EBS volumes for persistence, an ALB on the edge to cover ingress, and External Secrets to sync from AWS Secrets Manager. No service mesh, no fancy BPF shenanigans, just a cluster so simple that replicating to multiple environments was trivial. The great part is that because we've had such excellent stability, I've been able to slowly build out a custom platform that abstracts what little complexity there was (mostly around writing manifests). I'm not suggesting Kubernetes is for everyone, but the hate it tends to get on HN still continues to make me scratch my head to this day. reply noop_joe 5 hours agoprevHeroku and Reclaim are far from the only two options available. The appropriate choice depends entirely on the team's available expertise and the demands of the applications under development. There's a lot of disagreements pitting one solution against another. Even if one hosting solution were better than another, the problem is there are SO MANY solutions that exist on so many axis of tradeoffs, it's determine an appropriate solution (heroku, reclaim, etc) without consideration to its application and context of use. Heroku has all sorts of issues: super expensive, limited functionality, but if it happens to be what a developer team knows and works for their needs, heroku could save them lots of money even considering the high cost. The same is true for reclaim. _If_ you're familiar with all of the tooling, you could host an application with more functionality for less money than heroku. reply sciurus 6 hours agoprev> Replicas are used for high availability only, not load balancing (From https://reclaim-the-stack.com/docs/platform-components/ingre...) An I reading this right that they built a k8s-based platform where by default they can't horizontally scale applications? This seems like a lot of complexity to develop and maintain if they're running applications that don't even need that. reply dbackeus 7 minutes agoparentThis documentation only pertains to the Cloudflared ingress servers, which can handle orders of magnitude more traffic than we actually get. So we have not had any need to look into load balancing of this part of the infrastructure. Our actual application servers can of course be horizontally scaled. That said, there is some kind of balancing across multiple cloudflared replicas. But when we measured the traffic Cloudflare sent ~80% of traffic to just one of the available replicas. We haven't looked into what the actual algorithm is. It may well be that load starts getting better distributed if we were to start hitting the upper limits of a single replica. Or it may be by design that the load balancing is crappy to provide incentive for Cloudflare customers to buy their dedicated Load Balancing product (https://developers.cloudflare.com/load-balancing/). reply sph 19 hours agoprev\"Join the Discord server\"? Who's the audience of this project? reply tacker2000 13 hours agoparentAlso noticed this. Everytime I see a project using discord as main communication tool it makes me think about the “fitness” of the project in the long run. Discord is NOT a benefit. Its not publicly searchable and the chat format is just not suitable to a knowledge base or support based format. Forums are much better in that regard. reply KronisLV 6 hours agorootparent> Discord is NOT a benefit. Its not publicly searchable and the chat format is just not suitable to a knowledge base or support based format. I don't think people who choose Discord necessarily care about that. Discord is where the people are, so that's where they go. It also costs close to nothing to setup a server and since it has a lower barrier of entry than hosting your own forum, it's deemed good enough. That said, modern forum software like Discourse https://www.discourse.org/ or Flarum https://flarum.org/ can be pretty good, though I still miss phpBB. reply Gormo 4 hours agorootparent> Discord is where the people are, so that's where they go. That doesn't sound right. Each Discord community is its own separate space -- you still need people to join your specific community regardless of whether it is hosted on Discord or something better. > though I still miss phpBB. It hasn't gone away -- the last release was on August 29th, so this is still very much a viable option. reply dbackeus 1 minute agorootparentIt's all in one app and the app has a ton of users. Anyone running the app can join any server with a click of a button. There are no separate accounts required to join different communities. So communities being separate \"spaces\" doesn't create any meaningful friction with regards to adoption. mre 19 hours agoparentprevGenuinely curious, what's wrong with that? Did you expect a different platform like Slack? reply callalex 19 hours agorootparentLocking knowledge behind something that isn’t publicly searchable or archivable works fine in the short term but what happens when Discord/Slack/whatever gears up for an IPO and limits all chat history to 1 week unless you pay up (oh and now you have a bunch of valuable knowledge stored up their with no migration tool so your only options are “pay up” or lose the knowledge). reply Kiro 13 hours agorootparentNo-one complained when projects had IRC channels/servers, which are even worse since they have no history at all. reply Gormo 4 hours agorootparentGood projects still do rely on IRC -- Libera.chat is full of proper channels -- and logging bots are ubiquitous. reply Kiro 2 hours agorootparentAnd you never hear anyone complaining about those. \"Locking knowledge\" was never an argument before and it's not now. reply okasaki 9 hours agorootparentprevAll IRC clients have local plain text logging and putting a .txt on a web server is trivial. reply Kiro 6 hours agorootparentLocal logging doesn't help much for searchability when you're new and it requires you to be online 24/7. Anyway, that's beside the point. Even if IRC had built-in server history it still has the same problems but I never saw people being outraged about it. reply halfcat 17 hours agorootparentprevWhat’s recommended here? Self-hosted Discourse? reply heavyset_go 16 hours agorootparentMatrix and a wiki would solve the community and knowledge base issues. reply mplewis 11 hours agorootparentMatrix has severe UX issues which drastically limit the community willing to use it on a regular basis. reply Arathorn 3 hours agorootparenthistorically, yes. matrix 2.0 (due in two weeks) aims to fix this. reply tacker2000 10 hours agorootparentprevGithub issues or discussions. Or some other kind of forum like Discourse as you mentioned reply Gormo 4 hours agorootparentprevThere's a whole FOSS ecosystem of chat/collaboration applications, like Mattermost and Zulip; there's Matrix for a federated solution, and tried-and-true options like IRC. For something called \"Reclaim the Stack\" to lock discussion into someone else's proprietary walled garden is quite ironic. reply fragmede 19 hours agorootparentprevit would be better at the bottom of the first documentation page, after the reader has a better idea of what this is reply mrits 19 hours agoparentprevPeople that don't like wasting money? reply Gormo 4 hours agorootparentWasting",
    "originSummary": [
      "Mynewsdesk.com developed a Kubernetes-based platform to replace Heroku, resulting in a 90% cost reduction and a 30% performance improvement for their SaaS product.",
      "The new platform enhanced the developer experience with faster deployment times and improved tooling.",
      "The stack has been open-sourced, enabling others to replicate their success quickly by following the provided documentation and joining their Discord server."
    ],
    "commentSummary": [
      "Reclaim the Stack reports a 90% cost reduction and 30% performance boost by transitioning from Heroku to Kubernetes.",
      "Users argue that these savings come with increased complexity, requiring more maintenance, monitoring, and specialized expertise in managing Kubernetes and associated systems.",
      "Critics suggest considering simpler VM setups, managed services, or other PaaS providers like Render or Fly.io, highlighting the trade-offs between cost, complexity, and infrastructure management knowledge."
    ],
    "points": 463,
    "commentCount": 289,
    "retryCount": 0,
    "time": 1725833462
  },
  {
    "id": 41483581,
    "title": "FBI recommends using an ad blocker (2022)",
    "originLink": "https://www.ic3.gov/Media/Y2022/PSA221221",
    "originBody": "Cyber Criminals Impersonating Brands Using Search Engine Advertisement Services to Defraud Users The FBI is warning the public that cyber criminals are using search engine advertisement services to impersonate brands and direct users to malicious sites that host ransomware and steal login credentials and other financial information. Methodology Cyber criminals purchase advertisements that appear within internet search results using a domain that is similar to an actual business or service. When a user searches for that business or service, these advertisements appear at the very top of search results with minimum distinction between an advertisement and an actual search result. These advertisements link to a webpage that looks identical to the impersonated business’s official webpage. In instances where a user is searching for a program to download, the fraudulent webpage has a link to download software that is actually malware. The download page looks legitimate and the download itself is named after the program the user intended to download. These advertisements have also been used to impersonate websites involved in finances, particularly cryptocurrency exchange platforms. These malicious sites appear to be real exchange platforms and prompt users to enter login credentials and financial information, giving criminal actors access to steal funds. While search engine advertisements are not malicious in nature, it is important to practice caution when accessing a web page through an advertised link. Tips to Protect Yourself The FBI recommends individuals take the following precautions: Before clicking on an advertisement, check the URL to make sure the site is authentic. A malicious domain name may be similar to the intended URL but with typos or a misplaced letter. Rather than search for a business or financial institution, type the business’s URL into an internet browser’s address bar to access the official website directly. Use an ad blocking extension when performing internet searches. Most internet browsers allow a user to add extensions, including extensions that block advertisements. These ad blockers can be turned on and off within a browser to permit advertisements on certain websites while blocking advertisements on others. The FBI recommends businesses take the following precautions: Use domain protection services to notify businesses when similar domains are registered to prevent domain spoofing. Educate users about spoofed websites and the importance of confirming destination URLs are correct. Educate users about where to find legitimate downloads for programs provided by the business. Victim Reporting If you believe you have been a victim of fraud or malware based on brand impersonation from search engine advertisements, report the fraud to your local FBI field office at www.fbi.gov/contact-us/fieldoffices. The FBI also encourages victims to report fraudulent or suspicious activities to the FBI Internet Crime Complaint Center at www.ic3.gov.",
    "commentLink": "https://news.ycombinator.com/item?id=41483581",
    "commentBody": "FBI recommends using an ad blocker (2022) (ic3.gov)281 points by ysabri 21 hours agohidepastfavorite202 comments TechDebtDevin 20 hours agoIt's always an insane experience when you hop on someone's laptop/PC who has zero ad blocking installed. POV: https://m.youtube.com/shorts/iV3js9pd5IE reply giancarlostoro 20 hours agoparentI have a confession to make. I don't really have ad-block. If your site is too ad-infested, I stop using it. reply Zambyte 17 hours agorootparentThis is one thing I really like about Kagi - I don't have to remember which sites to not use due to ads, I simply add the site to my account-wide block list, and I never see it in search results again. reply bravetraveler 11 hours agorootparentOne can achieve the same thing for any website, not just their search results, with the browser extension 'uBlock'. Yes, you have to write a pattern. It saves you money while not supporting the erosion of autonomy. What's better than free vegetables? Delivery. I'm away from my computer right now but I'll copy one of mine later/reply with it. The edit window here is unfortunate. reply bravetraveler 9 hours agorootparentJust missed the edit window! I'm here to deliver. Two kinds, either will be accepted in the \"My Filters\" section of the plugin. Edit as desired, the domain is just a recent example. Eradication/filtering: ##a[href*=\"eng-leadership.com\"] Shown, but blocked on-access: ||eng-leadership.com^$document To keep the hacker spirit alive, I leave the reader with an exercise. Consider other approaches or HTML tags. Anchors aren't their only vector, hence the document method. HN has a 'hide' button that could be leveraged for nicer integration, removing related widgets. reply veunes 5 hours agorootparentprevSome nice user control features. reply mystified5016 4 hours agorootparentprevIt's been years since I've seen a Pinterest result for a query like \"C++ variadic template\" That alone is worth the price of admission reply soulofmischief 2 hours agorootparentprevAd-blocking is also about controlling what resources are loaded and executed by your computer. You won't know you were subject to a drive-by 0-day until it's too late and your computer is a botnet or your bank account drains. Not blocking ads also normalizes predatory ad-tech surveillance. There is a lot more going on beneath the surface of just \"annoying ads in my face\" which need to be accounted for, since browsers do not ship with effective, granular security controls. reply bravetraveler 20 hours agorootparentprevI do that too when my adblock is a little behind the adversary reply veunes 5 hours agorootparentprevIt’s a good way to support sites that respect the user experience while steering clear of those that don't. reply marginalia_nu 20 hours agorootparentprevYup, same here. reply lupire 19 hours agorootparentprevWhat's an example of a site that is on the high end lft amount/style of ads that you tolerate? reply giancarlostoro 16 hours agorootparentGood question, if I can't read the article whatsoever, then that's my time to leave. I don't mind ads, but if they take up all of the content, then its just not worth it. If you trick me with ads as if it were content, like some download sites do, I hate you. reply MattGaiser 20 hours agorootparentprevTypically people with this attitude have no-JS or something though for privacy reasons. That would cut a lot of ads down. Or do you not use any blockers at all? reply giancarlostoro 16 hours agorootparentI used to, I don't even bother with any of that. If I really need to I maybe bust out reader mode on Firefox, but very rarely. Especially at work most sites I visit aren't ad infested hellscapes, like StackOverflow and its relatives are not ad infested, neither is wikipedia, etc. reply Waterluvian 20 hours agorootparentprevThat feels like a pretty fair approach. If you don’t mind, I’m truly curious why you don’t Adblock? There’s no wrong answer here. ;) reply tyleregeto 20 hours agorootparentI don't use one either. I actually think ads are a good system for supporting content, and I do want to support the creators of the content I consume. I also have a low threshold for obnoxious sites, and will just bail and not return if I get annoyed. reply hunter2_ 19 hours agorootparentDo you ever make a purchase due to having seen an ad, ideally by clicking on the ad? If not, then in some sense you're still getting something without paying for it. (You're paying with your time, but that's not valuable to anyone unless it ultimately results in paying with money.) But better to screw the people pushing ads than the content creators! reply tyleregeto 15 hours agorootparentI would say not very often, but yes, very recently even. I've been researching new backpacking gear this summer, looking on sites that are known to me, so I've been seeing lots of ads for that type of stuff naturally. One store kept popping up that I was not familiar with. So I clicked eventually, and did some online searching about the company to make sure they are legit. Turns out they are a local independent store. I've made two purchases from them since, and price compared against them for other purchases. Their ads are more likely to catch my eye in the future now. reply ClickedUp 14 hours agorootparent> I've been researching new backpacking gear this summer, looking on sites that are known to me, so I've been seeing lots of ads for that type of stuff naturally. \"naturally\" reply juanani 15 hours agorootparentprevI personally have a long list of products not to buy. If you somehow repeat the same ad and I remember your product, I stop buying said product. If youre wasting your money on spamming ads, your product sure as shit isnt better than competitors', since they waste less on ads, more on product. I dont use ad blockers, they make it harder for me to find out who has the poorer product. reply shiroiushi 14 hours agorootparentWow, that sounds like a ton of work. I think a better idea is to use an ad-blocker, but run a program in the background that downloads the ads (or maybe just samples them, to save bandwidth and resources), processes them to find brand names, and then stores these brand names in a database so you can find their relative frequency and assign a score to each. Then you can just query the db when you want to buy something to find that brand's acceptability score. reply hirvi74 20 hours agorootparentprevNot the GP you are asking, but I do not use an ad blocker because I predominantly use Safari as my browser. I would absolutely love one, but after Apple made all those API changes years back, I gave up trying to find one that works well and is privacy friendly. reply tpierce89 5 hours agorootparentI use the DNSCloak app and run the Adguard DNS VPN from their list. It is free and blocks many ads in safari. reply internetter 20 hours agorootparentprevCheck this out: https://kaylees.site/wipr.html. It's no UBlock Origin, but it still does an excellent job on many sites. reply stogot 14 hours agorootparentHow do I know tools like this don’t send data home? Does Apple’s API work in a specific way to deny that? reply Nextgrid 19 hours agorootparentprevAdGuard is pretty good when it comes to Safari, and has a way to convert uBlock-style rules into the Safari blocking framework (well at least as much as it can), so you can use Easylist/etc. reply gitaarik 11 hours agorootparentprevWhy don't you just use a different browser that gives you more control? reply hirvi74 45 minutes agorootparentI find the ecosystem integration and cohesion to be the most compelling reason. Everything \"just works.\" Apple keychain is good enough that I stopped using other password managers and works great with Safari, Messages.app and Mail.app integrate well with Safari, AppleScript works well with Safari, etc.. I do have other browsers, but I only use them for specific needs. Like I keep some version of a Chrome-based browser around solely for if I need to Chromecast something. Other wise, I do not enjoy using third party browsers like Chrome, Firefox, etc., which I use at work all day. Safari's web developer tools are not my favorite compared to other browsers, so I try not to develop much with Safari other than testing. reply azinman2 20 hours agorootparentprevThe changes Apple made were to increase privacy. Content blockers that have access to the page have no network access. A separate process that does can only update the blocking rules. I quite like Ka-block reply gerdesj 20 hours agorootparent\"The changes Apple made were to increase privacy\" Apple gobbles personal data too and processes it and sells it etc. They are simply rather better at looking ... friendly. They really are very good at that. reply azinman2 20 hours agorootparentWhere is Apple tracking your usage across the web? This data is sold to who? These are big claims. Disclosure: I work at Apple, and have seen zero evidence of anything but trying to make things continuously more secure and private. This in fact makes my job (machine learning) much harder because I don’t have user datasets to leverage. reply dahart 18 hours agorootparentprev> Apple gobbles personal data too What data and how do you know? This seems to be a popular talking point, but I’ve yet to see evidence. It doesn’t make that much sense to frame Apple’s privacy stance as similar to Google’s or Facebook’s. Apple isn’t an ad business, and Google and Facebook are, plain and simple. I don’t work for Apple, and I don’t use an Apple laptop or desktop, but I don’t buy this Apple is as bad as businesses that are primarily built on ad revenue and are actively eroding privacy. I’m sure they’re not perfect, but I feel like Apple is relatively serious about privacy, making real changes that generally protect consumers, and setting a better example than many big tech companies. Are you sure they don't look better because they really are better? reply shiroiushi 14 hours agorootparent>I’m sure they’re not perfect, but I feel like Apple is relatively serious about privacy, making real changes that generally protect consumers Then why do they push their crappy browser on iOS which doesn't allow ad-blockers? reply dwighttk 7 hours agorootparentSafari does allow ad blockers. reply chithanh 14 hours agorootparentprevI don't think Apple (or Google, Microsoft, …) sells your data. Or can you point me to the website where I can buy user data from them? What I would admit is that Apple is maybe not as motivated to protect your data from unintentional leaking. Without user data, Google would be almost nothing. So they have to be extremely careful to maintain the trust of their users. For Apple (or Microsoft) their business is still sizeable enough with out user data. reply dwighttk 17 hours agorootparentprevI use 1Blocker reply carlosjobim 19 hours agorootparentprevNextDNS, Adguard, Wipr are a few that work. reply giancarlostoro 16 hours agorootparentprevAfter the first scandal about one adblocker being bought out, and letting by Google ads, I tried the next one in the list, then kept hearing about the issues, and then I realized, I am better off just not visiting sites that: A) Want me to pay to view a one-off article B) Want me to not see any of their content cause its ad infested. To be fair, if Firefox's Reader Mode doesn't suffice as a bypass, then I really don't bother coming back. reply Lord_Zero 20 hours agorootparentprevThere might be... reply bookofjoe 20 hours agorootparentprevBecause I don't know how to install one nor do I have any interest in learning how. reply Waterluvian 19 hours agorootparentMakes sense. Pretty sad people think that’s downvote worthy. It reminds me how easy it is to be ignorant to users. :( reply downrightmike 18 hours agorootparentprevBy then, its too late if there was malware on there. reply giancarlostoro 16 hours agorootparentIf there was, I'd be really surprised if they built it to know how to run on an OS that isn't Windows dynamically. reply userbinator 20 hours agoparentprevIt's no wonder people have gotten a lot more ignorant and less observant when they have to constantly fight the bombardment of their attention by unwanted distractions. In that situation of using someone else's device, I've had to move windows half off the screen to be able to concentrate on an article when the ads on the side were constantly distracting me. reply akomtu 19 hours agorootparentAds teach to not pay attention. reply ryandrake 19 hours agorootparentprevFunny how here's HN, a site full of Software Engineers complaining about ad bombardment, and every single one of those ads were programmed by... a software engineer! We (as a profession) are the ones doing this! \"Oh, but boss told me to do it!\" some will say, as if it's a good excuse. Regardless of which manager told which developer to do it, at the end of the day, a developer typed in the code and pushed it to prod. We're at least partially to blame for what the web has become. reply notfed 1 hour agorootparent> every single one of those ads were programmed by... a software engineer Your key point here is a tautology; it doesn't really lead to any insight. reply mr_toad 19 hours agorootparentprevDespite what management might think we are not interchangeable parts. reply shiroiushi 18 hours agorootparentprevHow many software engineers here work in defensive cybersecurity? Well, why they do they have jobs? Because of other software engineers who work on the offensive side. I guess all the cybersecurity engineers should just quit because they're the ones to blame for all the malware... Similarly, the only reason humans work as police is because humans commit crimes. So humans are really to blame for this problem. reply BadHumans 17 hours agorootparentI don't agree with their overall point but this is an either oblivious or bad faith take on their statement. reply shiroiushi 17 hours agorootparentNo, it's not bad faith at all. He's trying to use a collective blame argument: \"software engineers\" as a group are supposedly to blame for adware, in his argument, rather than a small minority of software engineers. It's absolutely no different than blaming allfor the crimes committed by a fewmembers, and paint them all with the same brush. reply ryandrake 16 hours agorootparentJust to clarify what I mean is it is a little bit of both: 1. Our profession is collectively allowing this by not having a widely agreed-upon ethical standard for conduct, and 2. (some) Individuals are actively doing it by actually building the bombardment code. I feel the same way about software for war fighting, which obviously has higher stakes. The profession itself doesn't push back on the ethics of it AND individual practitioners are actively developing death-dealing software. reply shiroiushi 14 hours agorootparent>1. Our profession is collectively allowing this by not having a widely agreed-upon ethical standard for conduct, You're acting like the profession has some kind of central authority. It does not. It's like asking for agreed-upon ethical standards for dog walkers; you're not going to get it, because there's nothing resembling a centralized organization, nor any kind of licensing for this profession. >I feel the same way about software for war fighting, If you want to eliminate software for war fighting, this is a fool's errand. Weapons for war are absolutely necessary, unless you want to be a victim to some dictator who doesn't agree with your ethical principles. History is full of examples of peaceful people who couldn't withstand an assault by other people who didn't believe in peace. In fact, I'd go so far as to claim that eliminating warfare (and the military apparatus for it: armies and navies etc.) is impossible as long as separate countries exist. Only if we manage to either conquer everyone or get everyone to agree to join a single planetary government can war really be eliminated. And that assumes that hostile aliens won't ever be a problem. reply oska 14 hours agorootparentprev> It's absolutely no different than blaming allfor the crimes committed by a fewmembers, and paint them all with the same brush. Members of a minority group, such as race or religion or sexual orientation are generally members of that group by birth, not by choice. Members of the group 'software engineers' are members of that group by (career) choice. reply yura 19 hours agorootparentprev\"Every single one of those ads were programmed by... a human! We (as humans) are the ones doing this!\" See? That logic doesn't work so well. \"Software engineers\" are not a singular entity nor a homogeneous group. To maintain the status quo, it doesn't take more than just a few SWEs willing to implement ads and/or invasive tracking. reply Loughla 18 hours agorootparentYour argument is a little lacking. Software engineers are directly responsible, is the difference. The people on this site are a group of humans uniquely equipped to actually speak and interact with the people responsible for this bullshit. reply ImPostingOnHN 11 hours agorootparentHumans are directly responsible, too, so the difference you cite, simply isn't there. > The people on this site are a group of humans uniquely equipped to actually speak and interact with the people responsible for this bullshit. There's not any method whatsoever for the people on this site to force all ad software developers to stop developing ad software. reply malux85 19 hours agorootparentprevThis is completely meaningless. In any population big enough theres going to be a high variance in behaviours, including morality. Expecting all individuals in a huge collective to all behave \"good\" with little / no inforcement is incredibly naive at best and dangerous at worst. Not only that, but you're wrong on a technicality too - > and every single one of those ads were programmed by... a software engineer! Many ads are designed by creatives and placed and run by dedicated non-software engineering people, including deciding how many are run and their placement (i.e. bombardment or not). Sure engineers programmed the platform, but then you're just blaming the post office for the content of the mail, or the ISP for the content of the internet. What do you expect the software engineers to do? Limit 1 ad per page programatically and then every software engineer on earth must agree to enforce that limit and no matter how much pressure their superiors put on them, everybody holds the line? This is not a rhetorical question, How do you expect all software engineers to be unified on a single solution to ad bombardment, given the internet is international, driven by market dynamics and capitalism and a non-trivial number of programmers are beholden to tyrannical managers because of their life situation (it can still be a minority, and ad bombardment emerges)? reply beedeebeedee 19 hours agorootparent> This is not a rhetorical question, How do you expect all software engineers to be unified on a single solution to ad bombardment, given the internet is international, driven by market dynamics and capitalism and a non-trivial number of programmers are beholden to tyrannical managers By developing a consensus that it is not ok to do that type of behavior (bombardment of ads, surveillance capitalism, etc) and changing the culture reply shiroiushi 18 hours agorootparentMaybe we can apply this line of reasoning to crime too... reply beedeebeedee 18 hours agorootparentWe do, and it mostly works reply shiroiushi 18 hours agorootparentSo there's no criminals now? News to me! reply beedeebeedee 17 hours agorootparentThere are still criminals, but did you commit a crime today? How about your friends and family? reply shiroiushi 16 hours agorootparentI don't understand your point. You say you want to change the culture, and that this has successfully been done in the case of crime, but this is blatantly false: every country has prisons with many criminals in them. In the US in particular (since most HNers probably live there), there are well over 1 million people in custody according to a quick google search, and over 5 million in corrections (so I assume most of those are on parole). Obviously, changing the culture hasn't worked, for there to be so many prisoners, and crimes committed so often that so many police officers are constantly needed. If your \"change the culture\" thing had worked, you wouldn't need police, or at least not many. Changing bad behavior by corporations (esp. adtech/advertisers) is likely to be about as successful: it's not going to be done by \"changing\" the culture, but only by changing the laws, and then enforcing those laws and punishing offenders. Just like a rapist sees nothing wrong with raping a person, or a serial killer sees nothing wrong with murdering many strangers, or a \"porch pirate\" sees nothing wrong with stealing your Amazon delivery, an ad-tech corporation sees nothing wrong with feeding you psychologically manipulative advertising and blatant malware in search of profit. reply beedeebeedee 16 hours agorootparent> If your \"change the culture\" thing had worked, you wouldn't need police, or at least not many. Who said that police are not part of the culture? I'm not arguing whether or not we need police, I'm arguing that if we come to a consensus (i.e., we agree that surveillance capitalism, etc, is not ok), then we can stop the problem. That may mean we even criminalize certain behaviors if we believe it is necessary. But the foundation is consensus. So let's do that- I firmly believe that surveillance capitalism and the bombardment of ads is not ok. What about you? reply defrost 16 hours agorootparentprev> You say you want to change the culture, and that this has successfully been done in the case of crime, No. They did not. They explicity said \"mostly works\". > but this is blatantly false: Of course it is. You set up a blantantly false strawman. Please don't use obviously piss weak rhetorical tactics, they make you look bad. > every country has prisons with many criminals And every country has varying incarceration rates, they are not all equal. What should be looked at is countries by cultural attitudes towards crime and incarceration rates .. and the harder question of just how innately criminal imprisoned people are in various countries and whether they are just there from systemic features of a culture. Again, there are no one size fits all answers and people aren't homogenous. reply shiroiushi 14 hours agorootparent>Again, there are no one size fits all answers and people aren't homogenous. That was my exact point with the previous post blaming \"software engineers\" for ads. reply bell-cot 17 hours agorootparentprev> By developing a consensus that it is not ok to do that... That sounds cool...but such coordinated, idealistic behavior never occurs in human beings. reply beedeebeedee 17 hours agorootparentYes it does- it happens all the time. It happens through talking about it publicly and coming to agreement. reply mrinfinitiesx 20 hours agoparentprevI just go to ublock origin on firefox and install it, they'll never ask about it; i'm just doing them a solid. reply shkkmo 20 hours agorootparentGiven the numbers of times I've had to disable ad blocker to fix some janky page I have to use, I don't think installing an ad blocker without explaining or even mentioning it is a friendly act. reply mrinfinitiesx 18 hours agorootparentI mention it, they usually have no idea what I'm talking about sadly. I don't think ad-tech is a friendly act. Infact, I find it insulting, invasive, and completely violating what they do with the personal data when they sell it 6 ways to sunday, but enough about that. Usually if I'm on somebody else's computer it's because they need me to fix things for them, or speed things up or 'make it better' which means it's getting adblock. The amount of modals and tricks and things they fall for especially my elderly neighbors or people wanting their business laptops setup especially Windows that have jank ass webpages that have the 'Download' link be some arbitrary .exe for something completely not what they wanted to download put something on their system is crazy.. hell even Youtube is giving people scams. I just fixed some women's sobriety center's computer for free and their user account had some anti virus secure browser opening 20+ 'browsers' on boot for 'AG' free anti-virus. Never had an issue with ublock breaking anything important for me. Air travel, hotels, ordering things online, if something says disable my adblocker I close it out and never go to that domain again. To each their own though. I don't do ads though and will save every soul I can. Ad-tech is cyber terrorism at this point, and I stick to my guns. reply asib 19 hours agorootparentprevAbsolutely agree. I often disable my ad blocker/cookie blocker when I'm about to make a big purchase (e.g. airline tickets) in case they interrupt whatever crazy redirect flow the airline and their payment processor have. reply prmoustache 19 hours agorootparentNever happened to me but I guess that kind of shop would simply loose a sale. reply A4ET8a8uTh0 19 hours agorootparentprevI don't think it happened to me, but then I don't venture outside Amazon, Newegg.. major sites that much. But even then.. if they need all those crazy redirects, maybe they don't want my business. reply Tanoc 14 hours agorootparentIf you ever limit the number of redirects to one in Firefox by using about:config, you'll see that most sites do at least two per page. It makes me wonder how many useless portals there actually are just because people glue together CDNs and add more off-site frameworks like Akamai and Typekit. reply jart 19 hours agoparentprevI've been using Safari on occasion for the past year because it starts up faster. I was too lazy to figure out how to install an ad blocker there and the strangest thing happened. I saw an ad many months later and realized I couldn't remember seeing a single one until that point. I suppose it's because I've become so good at avoiding the types of websites that have ads that maybe I don't even need an ad blocker (I'm extremely opposed to seeing ads that aren't SuperBowl ads). There also aren't that many website I visit. I use GitHub. I watch Netflix and Amazon where I can pay extra to not see ads. I also pay for YouTube Red. I use Kagi which is another place I can pay to not see ads. I also read Hacker News, where I'm always super careful to check that the domain on a link looks like a real person's website (i.e. isn't something like nytimes) before clicking. reply the_snooze 20 hours agoparentprevIt's such a jarring difference when I'm browsing on my phone at home with Pi-Hole vs. when away. So much that it motivated me to set up a split tunnel VPN so all my phone's DNS requests go through my home Pi-Hole regardless of what network it's on. reply hunter2_ 19 hours agorootparentI used to do this, but lately I just set my phone to use dns.adguard-dns.com as the DNS resolver which gives extremely similar results. It's a Russian operator, but I don't have much of a reason to trust any other DNS operator more. reply kurthr 20 hours agoparentprevAds (with untrusted javascript and links) considered harmful. reply ThePowerOfFuet 20 hours agoparentprevHere's your YouTube link without the creepy Google tracking (and with the missing playback controls): https://youtube.com/watch?v=iV3js9pd5IE reply lupire 19 hours agorootparentLooks like the same tracking, just main YT view not Shorts reply bookofjoe 20 hours agoparentprevBetter not use mine... reply Sephr 20 hours agoprevGoogle has convinced regulatory agencies that they're not responsible for their own complicity with supporting link fraud. I wrote an article about Google's role in enabling link fraud[1], which shows how this is effectively a form of regulatory capture. Here's a particularly salient critique of these very same FBI recommendations, from my article: > The FBI suggests “Before clicking on an advertisement, check the URL to make sure the site is authentic. A malicious domain name may be similar to the intended URL but with typos or a misplaced letter.” — this is useless advice in the face of unverified vanity URLs 1. https://eligrey.com/blog/link-fraud/ reply kenjackson 20 hours agoparentCan’t an ad always just redirect traffic to a vanity URL while exposing a non-vanity URL to Google? reply Sephr 20 hours agorootparentThe core issue here is that Google does not effectively verify ownership of vanity URLs displayed to users. You may not ever connect to the vanity URL in the first place. reply janalsncm 17 hours agorootparentprevI don’t think it’s reasonable to expect zero link fraud facilitated by Google, in the same way I don’t expect zero money laundering facilitated through banks. But banks are heavily regulated and (at a minimum) have to go through the motions of KYC and disrupting illegal activity. I think the issue here is we have allowed online platforms to reap the rewards of scale without requiring any responsibility of serving a billion+ people. Internalize rewards, externalize responsibility. reply Sephr 17 hours agorootparent> I don’t think it’s reasonable to expect zero link fraud facilitated by Google Link fraud is entirely preventable in an automated fashion. They just need to start enforcing their own policies. Validating ownership of 'display URL' domains via DNS would completely eliminate the issue. It's quite simple. reply yard2010 20 hours agorootparentprevAFAIR it's against their code, they do have strict rules regarding their ads. But it's funny I remember adsense fighting the bs fraud ads on the internet, only to become the bs fraud themselves.. reply eh_why_not 20 hours agoprevNo-JS user here. A disturbing trend noticed in the past two months: can't login anymore to some financial/health services sites (bank/insurance/etc) without disabling all of NoScript - no amount of selective enabling of websites satisfies them, and those websites are using known infractors like Adobe. In other words, there is code in the backend checking that all tracking/-ware has run on the browser, and refusing to let you login unless you let it all run, while none of it is necessary (as evidenced by older versions - and other sites - accepting only the top site being JS-enabled). \"We either track the living shit out of you, or you don't access the essential services you need, even though technically it is not needed.\" reply ysabri 18 minutes agoparentThis has been happening to me too with PG&E. The page wouldn't load until the Amplitude calls finish. reply Hakkin 17 hours agoparentprevuBlock Origin has some advanced filter syntax that can sometimes deal with sites like this. It can intercept, modify or replace JavaScript functions, objects, network requests, parsing data, cookies, etc. That being said, writing filters for sites like these is somewhat of a dark art, it usually involves reverse engineering the page's JavaScript to the point where you understand what it's actually sending and checking for to function correctly, then figuring out a way to bypass those checks by selectively modifying the JavaScript's functionality. Things like this are why I worry a bit about the proliferation of things like WASM, while JavaScript isn't great, it actually gives a great amount of control to the end user, to both see, understand and the ability to actually modify what is running in their browser. With WASM, all of this becomes highly impractical. Instead of a (semi-)readable, modifiable block of interpreted code, with the ability to inspect and modify the state at almost arbitrary points, you just get opaque binary blobs that you basically can't do anything with. As more and more sites switch to using compiled WASM blobs for their logic, it will become increasingly difficult to observe or modify any behavior of these websites as an end user. reply 0xffff2 1 hour agoparentprevMeh, I've noticed this without any blockers at all. I cannot log in to my power company's website from my PC (on any browser) and likewise for Delta Dental on any device. So far I have been able to work around this, but I don't understand how its possible to break login so badly that even unmodified Edge doesn't work and it stays like this for months. reply bryant 15 hours agoparentprevDo you have a list of offenders by any chance? reply rkagerer 20 hours agoprevI'd like to see society in general become less tolerant of unwanted ads. The original Google site hit the perfect pitch, where they set a few unobtrusive ones out of the way alongside your results screen. Ironic they pioneered and eventually normalized what is now an epidemic of user-hostile spam all over the web. I feel as a whole we lose a lot more productivity and focus to this than we gain in economic activity. reply Terr_ 20 hours agoparent> The original Google site For your nostalgia and/or despair, I found a chronological list of screenshots: https://scaledon.com/the-evolution-of-google-ads/ reply rkagerer 17 hours agorootparentThank you! I actually tried to find an illustration, and your link fits the bill perfectly. reply rendaw 17 hours agorootparentprev> Google Ads became more user-friendly in 2014. The yellow box that contains the ad disappears and users could quickly scroll through the ads section. Can someone please explain these two sentences to me? reply Terr_ 20 hours agoprevI've love to see what happens if ad-networks became legally liable for any scams or malware that they enable. The counter-argument that they don't need to know their customer/ad and are just dumb-pipes doesn't sit well with me: Them having awareness of ad-content and display-context is ostensibly part of their business model. P.S.: I don't mean just liable for a part of the damages, although that would be a good start. I mean that if your Aunt Tillie gets served an ad of \"Your computer is infected, click here to contact a Microsoft Technican\" there should be some negative repercussions for the company, even if your Aunt Tillie is secretly the hacker BakinC00kies and spins up a honeypot. reply userbinator 20 hours agoprevMeanwhile, Microsoft fights with itself on whether adblocking is good or bad: https://www.microsoft.com/en-us/edge/learning-center/using-a... https://support.microsoft.com/en-us/office/supported-browser... reply karlzt 20 hours agoprevPreviously: The FBI now recommends using an ad blocker when searching the web: https://news.ycombinator.com/item?id=34916239 734 points2 years ago430 comments reply thimabi 20 hours agoprevRecently, my grandmother got herself scammed when trying to pay her bills, because she clicked a Google search ad for a fraudulent website posing as the local gas company. She lost some money and, of course, some of her personal data as well. When situations like this happen, I mostly place the blame on ad companies. It’s their product, so it should be their responsibility to prevent abuses. But there is scant regulation, and the ad industry itself has little concern for privacy and data protection. Why would it waste money being proactive and effective against malicious ads? It is nice to see the government recommending ad blockers. However, it bothers me that it is up to us, users and customers, to deal with the negligence of ad companies. reply meowster 15 hours agoparentI wonder what would happen if you (she) sued Google. reply thimabi 4 hours agorootparentI wonder too, but suing is expensive and demands some time that we don’t have. Yet we did notify the cybercrime team at the police department, so maybe they will do something about it. reply meowster 2 hours agorootparentIf the amount is/was under the small claims court level, that would be a vastly cheaper and easier route to take. Rules vary by state, but I believe generally you cannot select an arbitrary amount lower than the monetary threshold just to use small claims court, but you might be able to argue Google has 50% or some other culpability if that will get the amount under the threshold. IANAL. reply imoverclocked 20 hours agoprevWe still haven’t reached peak Idiocracy. YouTube/hulu/disney+ still cut to ads instead of displaying them around the border of the content. Carl’s Jr/Brawndo still haven’t purchased the FCC. We are pretty close though. reply chgs 19 hours agoparentToo easy to block adverts around the border with cardboard. They still need to make people actually think that adverts are good reply A4ET8a8uTh0 19 hours agoparentprevHonestly, we are there. It just didn't make the news yet for some unknown reason. reply declan_roberts 19 hours agoprevWhat's the current status of whole-network blockers that use DNS? I tried pihole maybe 8 years ago, and it just broke too many websites for me to leave it on for my wife. It really frustrated her. reply tgmatt 19 hours agoparentI don't use it personally but know several people that do. It has different levels of blocking, and so if you have the patience, you can go full block mode and gradually peel back the blocking to sites that you want to use. My friends thoroughly recommend it. reply nobody9999 15 hours agoparentprev>I tried pihole maybe 8 years ago, and it just broke too many websites for me to leave it on for my wife. It really frustrated her. I've been running pi-hole at home for three or four years with minimal issues. The times when I have seen issues, I just go to the admin webpage, disable blocking for some period of time, and try again. And it's almost never Pi-hole causing the issue unless I'm trying to click on an affiliate link[0]. I mention this not as something your wife might do, but to clarify that pi-hole (with default settings) almost never blocks anything I click on, and when it does, I'm almost always glad it did. As such, assuming your wife isn't clicking marketing/advertising/affiliate links, pi-hole should be just fine for your home IMHO. As always, YMMV. [0] https://en.wikipedia.org/wiki/Affiliate_marketing reply belinder 20 hours agoprevFor computer health your PC needs an ad blocker, but also for mental health. At what point will the CDC recommend using it reply OnestepGrow 7 hours agoprevThank you for the valuable information on the blog.I am not an expert in blog writing, but I am reading your content slightly, increasing my confidence in how to give the information properly. Your presentation was also good, and I understood the information easily. For more information Please visit the 1stepGrow website or AI and data science course. https://1stepgrow.com/advance-data-science-and-artificial-in... reply robpco 20 hours agoprevFYI - this is from December 21, 2022 reply rty32 20 hours agoprevWonder if someone would make a YouTube video talking about using uBlock origin to block YouTube ads, citing FBI's recommendation, see if it gets taken down by violation of YouTube's ToS. reply xoxxala 17 hours agoparentLinus Tech Tips just had a video on using adblockers, as part of their \"how to de-Google your life\" series, taken down by YT for posting \"harmful or dangerous content\". https://www.youtube.com/watch?v=apdZ7xmytiQ reply shiroiushi 8 hours agorootparentWow, that sounds like something out of Russia's playbook. reply taf2 20 hours agoprevI must be a strange person because I don’t run any adblocker… if I happen to need to visit an ad stricken site I just toggle on reader mode. Get my info and get out… I never have to worry about clicking on a search result and it doing nothing … but also I rarely visit these ad sites…. For the cookie banners I just inspect and delete the elements when they get in the way… reply NegativeK 20 hours agoparent> I never have to worry about clicking on a search result and it doing nothing I can't remember the last time I noticed that the ad blocker even existed on my machine. Occasionally some clever site will basically say \"if you can see this, we're supported by ads and could use your help\" -- but it doesn't break things and I don't see ad links in search results. reply kjkjadksj 2 hours agorootparentA lot of food ordering websites don’t work for me on ublock origin. Banks are also 50/50. Reddit will occasionally flag me for bot traffic. reply taf2 18 hours agorootparentprevDo they remove the cookie banners? reply aflag 20 hours agoparentprevAn ad blocker would save you some extra clicks. reply vunderba 20 hours agoprevPSA: Even among tech minded folks, a surprising number of people are still using adblock which is widely known to use sponsored whitelists to allow companies to bypass the filters. The gold standard which works as an extension in both chrome and Firefox is uBlock Origin, annoyingly not to be confused with uBlock. https://addons.mozilla.org/en-US/firefox/addon/ublock-origin https://chromewebstore.google.com/detail/ublock-origin/cjpal... Also be aware that Google continues to add restrictions to extension permissions such that uBlock Origin may not be as effective as it once was. reply rty32 20 hours agoparentNot even that. Most of my software engineer colleagues do not use ad blockers at all. They are definitely aware of them, but they don't use them, and they don't bother to use them. Which is surprising. You would expect that the people who have the ability to write perfect selector rules to block ads and understand how all of this works would be the first to use them. But no. reply shiroiushi 18 hours agorootparentI've seen this too, and we see it right here on HN every time this discussion comes up. You can even see it on this discussion just above: there's a bunch of people here who just don't want to use them because they want to \"support content creators\" or whatever. There's even some people here who will tell you you're a bad person for blocking ads. I find it baffling too. reply simple10 20 hours agoparentprevIs the warning on uBlock Origin Chrome extension page due to it not using the manifest v3? Anybody know if uBlock is still the best option for Chrome? reply vunderba 20 hours agorootparentMarginally related, I've heard that Brave (which is a chromium fork) is going to maintain support for V2 so uBlock Origin will continue to work on it, but I don't use it personally so take that with a grain of salt. reply simple10 19 hours agorootparentThanks. Many friends of mine use and recommend brave. reply dylan604 20 hours agoparentprev> Also be aware that Google continues to add restrictions to extension permissions such that uBlock Origin may not be as effective as it once was. That's fine. I don't use a Google made browser, so this would not affect me at all. It would also be very easy for this to not affect you too if you just had the courage to stop being a sheeple reply vunderba 20 hours agorootparentI'm not sure who this comment is directed at and I can't speak for everyone but I do think some people have to use Chrome as a requirement from their jobs - such as UI/UX testing for frontend development. So for them, it's better than nothing. reply dylan604 19 hours agorootparentyou can use what ever browser for UI/UX testing purposes, but it doesn't have to be your daily driver. reply beloch 16 hours agoprev>Cyber criminals purchase advertisements that appear within internet search results using a domain that is similar to an actual business or service. When a user searches for that business or service, these advertisements appear at the very top of search results with minimum distinction between an advertisement and an actual search result. Governments should start holding companies that sell ad space responsible for the ads they run. There's no way any company with the resources of Alphabet or Meta should be serving up phishing ads in their search results. The fact that Google is presently trying to degrade the performance of ad blockers with Manifest V3 is not a good look. This is why we have consumer protection laws. reply Alifatisk 9 hours agoprevI hope the raise of llms searching the web and acting as a middle-man makes websites ridden with ads obselete. If you've ever looked for a recipe, you'll know how many obstacles there is without ublock. My hate is towards these type of websites. reply patrakov 19 hours agoprevAnd in China, using ad blockers is illegal. Go figure. reply hypeatei 20 hours agoprev> Before clicking on an advertisement, check the URL to make sure the site is authentic Yeah, good luck doing that with all the various tracking links that mask the actual domain. Sometimes I try to click on links from legit account related emails that are blocked by UBO for being part of a tracker/ad network. reply NegativeK 20 hours agoparentI hear this advice from other infosec people constantly, and it's starting to grate. In one breath we tell users \"attackers are professionals who are doing this eight hours a day; they're probably going to trick you\", and in another we're trying to get users -- who are busy doing their jobs -- to recognize the difference between an I or an l, or maybe go do a domain history lookup to see if businessandsons.com is some new knockoff of businessllc.com, or maybe figure out how to parse whatever the email reputation filter mangled the domain into. I know perfect is the enemy of good and defense in depth and etc, etc, but this advice just seems crap. reply spacebacon 16 hours agoprevJeff Johnson’s “Stop the Script” (iOS, MacOS). Blocks all JS, including inline. If a site doesn’t have a fallback to serve static content or is not readable when I disable JavaScript… I leave. reply jampekka 20 hours agoprev> Most internet browsers allow a user to add extensions, including extensions that block advertisements. This may be sadly outdated. Android Chrome and iOS Webkit probably account for majority of traffic nowadays, and neither allows adblock extensions. reply amusingimpala75 18 hours agoparentThat's false. See AdGuard for iOS https://apps.apple.com/us/app/adguard-adblock-privacy/id1047... reply jampekka 8 hours agorootparentOh, thanks! reply lapcat 20 hours agoprevCan the FBI now arrest Sundar Pichai for obstructing justice with Chrome MV3? reply Nuzzerino 20 hours agoparentThat’s not how that law works. reply dylan604 20 hours agorootparentIf wishing made it so though reply purple-leafy 20 hours agoprevI make loads of browser extensions, is there any wins that can be had by building another ad-blocker? An manifest v3 compliant one. Or does unlock origin lite cover everything? I was thinking continent specific ad blockers etc reply joaovitorbf 19 hours agoparentAll work is pretty much already done by uBlock Origin Lite and AdGuard MV3. Manifest v3 fucking sucks, and people will probably need to go to system-wide ad blocking such as AdGuard to block ads using all the filters, as v3 limits the number of filters you can use. reply purple-leafy 18 hours agorootparentIs adguard a system level script interceptor like wire shark? reply notinmykernel 18 hours agoprevFBI has also warned against placement of smart home assistants (e.g. Alexa) in bathroom and bedroom. reply BaculumMeumEst 20 hours agoprevWhat would tip the scales to justify including an ad-blocker in Safari by default? reply lapcat 19 hours agoparentThe new version of Safari to be released this month has made very tentative steps into content blocking, but it's all manual by the user, and there's still been a lot of pushback on the feature by advertisers. I guess Apple may be wary of antitrust concerns. reply Hnrobert42 20 hours agoprev[2022] - I thought I'd heard this before. reply Summerbud 15 hours agoprevThis is where SEO leading to :( reply slowhadoken 19 hours agoprevThe FBI also suggests using VPNs. reply ls612 14 hours agoparentPeople shit on the FBI (for good reason) for being a tool of the ruling class, but their cybersecurity recommendations are consistently solid. They were amongst the first to suggest disabling Flash and Java in the browser for instance back in 2012. reply yoyar 20 hours agoprevWindows itself is malware. reply keb_ 18 hours agoprevnext [34 more] [flagged] ethanol-brain 18 hours agoparent> There is an unwritten social contract here. There isn't, no matter how many times you say there is. https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply ranger_danger 16 hours agorootparentthis has to be a troll account, they keep flip-flopping between advocating both for and against adblockers in every other comment. reply zeronine 18 hours agoparentprevThe social contract only works when content providers choose an ad network that doesn't serve deceptive, misleading or outright malicious ads. I'd rather protect myself than be polite. reply matt_j 18 hours agoparentprevI'm not torn at all. Ads are cancer. They're either un-curated and malware, or curated on my private data and disturbingly targeted. I reject both types. reply ranger_danger 16 hours agorootparentwould you rather pay to access that content? how else could the site be funded? reply meowster 15 hours agorootparentHow were websites funded when the WWW was first created? Tax dollars, hobby/altruism, university dues, etc. Don't want to spend an arm and a leg for bandwidth? Don't include megabutts of JS, CSS, images, etc. Optimize and respect your users/guests. reply ranger_danger 15 hours agorootparentThat only worked because the traffic to bandwidth cost ratio was small. Youtube would cost any normal company untold millions per month just in bandwidth costs alone, and there's actually so much more to it. I would bet money that even Discord's bandwidth bills are at least 1 million USD per month if not more. reply nobody9999 16 hours agorootparentprev>would you rather pay to access that content? how else could the site be funded? I can't speak for GP, but don't care how a site is funded, unless it's my site. If a particular site goes away, it's no skin off my nose. reply cm11 17 hours agoparentprevAgree with a lot of this and also feel torn (though I've been settled on a side for a little while). I've been debating with my neighbor the last few years about whether artists are still artists if they're charging for their \"content\". It's a bit of a facetious argument, but directionally if you have something you want people to hear, isn't the audience doing the favor by listening? Similar points have been made about Twitter (and social media generally) that folks have the right to say what they want, but not necessarily to be heard. A person can tweet, but not only can others block them (or just ignore and scroll by), the platforms/algorithms can not prioritize them in feeds. Folks aren't owed an audience. It's possible content is for the author. When it isn't, when authors wouldn't have put it up if it weren't for ad income, I feel less worried about what we're losing. That said, if I had to pay for everything, I would certainly read/watch less stuff I like too. reply pdntspa 18 hours agoparentprevThis idea that content owners need compensation for making their content available needs to go. There was an entire generation of kind souls doing exactly that until the culture was suffocated to death by late-stage American capitalism. I and many others gladly made content available to others at our own cost. The mission of making knowledge available to those less fortunate than oneself (especially in this inflated first-world lifestyle) should supercede the need for financial gain. But once the actual moneyed assholes come in and take over, everything gets expensive and unaffordable and now you absolutely need a business model just to build something. Perhaps ad blockers and piracy are the much-needed counterbalancing force keeping the monster at bay. reply Loughla 18 hours agorootparentI'm with you. I really am. I genuinely miss the web when it seemed wild and free and not just a collection of five companies. The problem is that the old way is dead. And its own popularity killed it. Just for example; There is no way a video hosting site the size and scope of YouTube could have possibly existed when the Internet was a hobby. What do we do with that? reply pdntspa 35 minutes agorootparentWhy do we need video? Is there a good reason beyond the fact that moneyed interests have brainwashed people into thinking they need it? That they've destroyed attention spans so nobody knows how to frickin read? Maybe it's time to emerge from this dopamine-induced brain fog. reply noirbot 17 hours agorootparentprevThe issue is that I'm generally willing to watch some ads for youtube or such. I get that it's expensive to host that and I get my time/money's worth. But then I load a random recipe site or code docs site and get the page 40% covered by shitty ads for near-pornographic content and I install an ad blocker. So many sites just cramming ads into every single square inch of their sites ruins it for almost everyone else. I get a visceral reaction to banner ads now. I totally understand the economics of it, and the fact that you can't bootstrap something like Youtube by charging for it, but that's just how it is. It's a shame that the responsible sites are getting painted with the same brush, but so much of the internet has gotten so insanely greedy with ads that there's just no other choice. reply ranger_danger 16 hours agorootparentbut why are you ok with watching video ads on youtube, but static images on doc sites go too far? reply noirbot 15 hours agorootparentA. Most of the ads on sites aren't static and many are full video embeds now. B. Because I get that the cost to deliver me a 30 minute video is fairly expensive even for Google. Most text websites, the cost in bandwidth to deliver the ads is orders of magnitude more than the site itself. I host text based websites and they cost literally a dollar a month. The overhead of my single visit to a text/image site is a tiny fraction of the cost of me watching even a 5 minute YT video. If you build a blog or generally text-first site in a sensible way, the cost of a single visitor is ~free. Making me see and load 4-5 ads at any time is just abusive. As was said elsewhere, if you think your content is good enough to cost money, then try charging for it, but there really just aren't that many people talented enough to successfully demand living income for writing or recipes or other mundane stuff people put on the internet. We could wish for a world where it's easier to frictionlessly give someone a penny or such for reading their site and let that add up for those who get the views, but that's just not a thing right now. The ability to have a total stranger see your work even once is the outlier that the modern era as brought. People have jumped from that to \"and thus I deserve to live off of this\" and that just doesn't follow. Even making a pittance is unlikely and that's still better than any of us was likely to get even 20 years ago. reply vueko 17 hours agorootparentprevSo, I actually think that if YouTube went poof tomorrow, totally gone, and was not replaced by a similar ad-supported medium, it'd be a boon for makers of quality video content and disproportionately harmful to clickbait chum-farms. Ad-supported video is the incentive system that delivered us Elsagate, and the scale of YouTube w.r.t. feasibility of paid hosting isn't all that it seems. Every single YouTube creator I watch with any degree of regularity has a Patreon, merch shop, or some other way for those who appreciate their stuff to directly contribute to its production and continued existence. Many say that they rely on those sources of income a lot more than ad revenue, especially those in non-advertiser-friendly niches who have to worry about demonetization. In a no-YouTube world, even assuming p2p video streaming never works out, those folks would be able to pay for hosting with those non-ad income streams, because YouTube's scale is deceptive. Most of the ones I watch don't even have all that many views relative to the big boys - five to six digit, usually - so their viewer:contributor ratio is a lot higher than that of a successful clickbait slop video with ~zero genuine dedicated supporters but a lot of incidental ad views. This dynamic implies that creators with dedicated followings would have a decent shot at supporting themselves even if they had to pay for bandwidth, because their bandwidth spend to revenue ratio is a lot better than average. The thing about YouTube is that slop outweighs quality content by such a massive margin that if you do napkin math around the raw cost of hosting n hours of streaming video, you end up with a way higher number than you'd have if you stripped out the bulk of the material that wouldn't be economically feasible in a non-ad-supported environment. A corollary is that without competition from low-quality ad-supported material that couldn't hack it in a donation-centric environment, the good stuff stands out that much more. It's kinda like how you can't really post a recipe without it drowning in an ocean of algorithmically generated fake-ass life stories about grandma's cookies with seven ads before you hit the first ingredient. Without those, organic content, even paid/donation-supported organic content, has a much better shot at encountering the kind of eyeball that'll shell out for the good stuff. I think the existence of spaces like Bandcamp (for now, anyway, fingers crossed re acquisition) demonstrates that donation-supported streaming can be economically viable. I've spent far more on Bandcamp albums I could have just kept streaming indefinitely for free than I ever did on CDs, because I know that the bulk of that money is actually going to the people who made the stuff I liked, rather than getting siphoned off into corporate middlemen a la legacy record businesses / Spotify and its ilk. Direct-to-artist support in places like Bandcamp has enabled a flowering of high-quality, niche content. People may complain about a simplistic top 40 or whatever, but I'm running into more music right up my alley than ever before, and that's largely due to the newfound ability to cut out the middleman and go right to the creator. YouTube is like the old record industry; it benefits the lowest-common-denominator painfully-focus-grouped artist far more than it does the auteur. reply RajT88 18 hours agoparentprev> However this culture of expecting websites to host the data then freeloading off it by blocking the tracking and ads is also a bit ugly. Let's use Facebook as an example. You can't not get tracked by Facebook if you have a Facebook account. If you use it, they track you and monetize you somehow. All of your interactions with the site, and all your interactions with other users. Blocking as much of Facebook's ads and tracking as you can while keeping the site working means you are monetized somewhat less. You're still not freeloading. This is similar with other companies - you have to use the product, and they monetize your use of the product. Whether or not you see the ads in Gmail - they are still aggregating all of your data and monetizing it. Adblockers can be seen as a system of checks and balances and incentivize certain types of monetization over others. Don't get me wrong - I'm sympathetic to your argument. Google and Facebook can't run a service without being able to make a profit on it. I believe users should have measures of control over how they are monetized, and there should be clarity about how they are being monetized. Example: Google Maps keeps prompting me to turn on my location history. If I do, the TOS says they get to sell that to a bunch of people. That prompt to turn it on should be clear about this compromise. My wife doesn't mind; I do, but I had to read the TOS to understand the compromise is there in the first place. As an aside, I find it really interesting how content creators are being sponsored by companies in much the same way early radio shows had sponsored ads. Everything old is new again, and I find this un-blockable type of advertising to be totally fair, fine and normal. reply nobody9999 16 hours agorootparent>You can't not get tracked by Facebook if you have a Facebook account. If you use it, they track you and monetize you somehow. All of your interactions with the site, and all your interactions with other users. Actually, Facebook will track you even if you don't have a Facebook account[0]. In fact, Facebook and others[1] track you whether or not you use their \"services\" or not. [0] https://www.howtogeek.com/768652/what-are-facebook-shadow-pr... [1] Google (but we knew that, didn't we?) and data brokers[2] (but we knew that too, didn't we?) [2] https://privacyrights.org/resources/registered-data-brokers-... reply RajT88 16 hours agorootparentYeah, I knew that but with enough effort you can block that too. I was being precise. reply nobody9999 15 hours agorootparent>Yeah, I knew that but with enough effort you can block that too. Can you? As of 2021, there were more than 500 registered data brokers in the US alone. And those brokers don't just use your Facebook profile (shadow or otherwise). Rather they source data from thousands of retailers, insurance companies, credit \"rating\" agencies, banks, etc., etc., etc. How exactly do you (because I certainly don't know how to do so) keep hundreds of discrete businesses from collecting data on you? Do you squat in a lean-to on private land not owned by you? Do you have a phone purchased by you (even with cash, most retailers have surveillance cameras)? Do you own a car? Is it registered? Do you not live in New Hampshire (the only state which doesn't require auto insurance)? Do you have a credit card in your name? Do you have any professional certifications? Are you married? Divorced? Pay taxes? If you really think you can block the level of tracking that goes on, you're either delusional or have a multi-billion dollar set of skills/ideas. Which might it be, do you think? reply RajT88 15 hours agorootparentI was only speaking of Facebook off-site tracking. reply big-green-man 16 hours agoparentprevI don't expect them to host anything for me. I'll use it if it's available to me, sure, but I don't have any expectations at all. reply macNchz 18 hours agoparentprevBy my estimation the ad networks and publishers are the primary violators of the social contract by allowing malicious ads on their systems. They could absolutely stop essentially all bad behavior by 1) not allowing any rich or dynamic ad content and 2) having a human look at every ad before it goes live. Those measures would negatively impact their multi-billion dollar profits, though, so they don’t, therefore I block ads and don’t feel bad about it. reply batch12 18 hours agoparentprev> There is an unwritten social contract here. I never agreed to this contract the same way I never agreed to watch ads on my television or listen to them on the radio. I reserve the right to change stations temporarily, mute sound, and block ads. I hereby invoke a social contract that states that everyone who reads this comment must reply with their demographic data so I can sell it. Be polite and conform, please. My time is valuable and these comments don't write themselves. reply downrightmike 17 hours agoparentprevEven google can't keep the malware from getting through their ads, so eff them. Ads are not safe. It costs me more to reimage my machine than they would get from the ad, so I block ads. Super simple concept. reply keernan 18 hours agoparentprev>There is an unwritten social contract here. I don't feel that is an accurate take. Websites tried charging money for access and the internet citizenry responded pretty clearly that the overwhelming majority of websites were not worthy of payment for access. So websites tried a run-around - advertising. And the use of ad-blockers exist because internet users are again voting that they would rather the website(s) disappear than be forced to watch a level of obtrusive advertising they found unacceptable enough that they turned to adblockers. And I would suggest to you that if website owners were to find a way to defeat adblockers, the majority of them would lose their traffic. reply what 17 hours agorootparent>ad-blockers exist because internet users are again voting they would rather the websites disappear Clearly not, or they would just stop using those sites. Instead they want the content at zero cost. reply ndriscoll 16 hours agorootparentIt's not like links come with ad warnings. I don't know or care which sites try to show ads, but I'd be fine with them all shutting down. It'd simply make room for everyone else. reply noirbot 17 hours agorootparentprevThis is kinda just the reality of it. The promise of the internet was that anyone could put anything they want on the internet for anyone to see and access, but there was never a promise they'd get paid for it, even to cover the cost of hosting it. There's an entitlement now to \"Well, I put it online so you must pay me\" and frankly, not everyone deserves it. Or at least not everyone deserves it as much as they think they deserve it. Does this mean many astute and skilled people I like don't get paid what they deserve? Absolutely, but that was already the case for centuries before. They just didn't used to have the option to go \"it's freeeee\" and then have half of the world spy on me in exchange for it, and maybe that's not a good thing. reply tourmalinetaco 18 hours agoparentprevService quality is inversely proportional to advertising revenue. Why would I let poor quality services profit off of me when they can’t even provide me with the answers I seek? You appear to have this idea in your head that it‘s independent websites running the bulk of advertising, when in fact it‘s SEO scams and low quality services like Google. If a website provides me consistent value, I chip in a few bucks, but as it stands I will never turn my Adblock off. reply shiroiushi 9 hours agorootparentYeah, there seems to be some kind of assumption by some here that just because I visit a website, that they're entitled to some money for that visit (in the form of ad revenue). Sorry, but no: I usually visited that site because they made themselves available, frequently dishonestly with SEO, through a Google/Bing search because I was looking for some information. Much of the time, when I visit a site, I don't find the information I'm looking for, and instead find a bunch of low-quality, low-effort \"content\", like \"blogspam\". It's a waste of my time. I can't know the site is garbage until I visit it and see for myself. So no, I reject the idea that these sites are somehow entitled to money for their \"service\". If they go out of business because of my ad-blocking, I'm not going to miss them. reply staplers 18 hours agoparentprevThere is an unwritten social contract here. No there isn't. Don't make your content public if you don't want it public. Paywalls make very clear what a companies true value is and that terrifies every shareholder and C-suite. reply nobody9999 16 hours agoparentprev>Websites are willing to host and organise a vast number of content because that'll attract an audience for ads. If there are too may freeloaders resisting the ads then services won't host the content, That's as may be. I won't use your (that's the generic, \"you\" here) site if you insist (by blocking me from accessing the site with an ad blocker enabled) on making me watch ads. But I won't complain about it either. It is, after all, your site and not mine. As such, you can do as you like with it -- and good on you for making your own decisions about your own property! That said, my browser belongs to me, and I will make my own decisions about what displays in my browser and/or runs on my hardware. tl;dr: I control my stuff and you control your stuff. If we can come to an agreement, that's great! If not, that's fine too! Edit: Clarified prose. reply dopadelic 20 hours agoprev [–] Adblock would break most websites nowadays. It's commonplace to detect adblock and disable the website if adblock is detected. reply ryukoposting 19 hours agoparentI have uBO, a pihole, and noscript between me and the web nowadays. I'd say I run into a website that intentionally bricks itself once every 3-6 months. It's an uncommon practice in my experience, and it's easy to understand why. 90+% of websites nowadays are click farms - their content is totally interchangeable with, if not plagiarized from, hundreds of other sites. The one site that implements an anti-adblocker measure simply gives up a portion of its traffic to another site that doesn't. The far more lucrative strategy is to try to subvert the ad blocker, which many sites actually do. reply ls612 14 hours agorootparentEven just pure uBO is good enough to stop the vast majority of anti-adblock efforts without any custom tweaking. reply caseyy 19 hours agoparentprevHmm, it most definitely does not. reply nullc 1 hour agoparentprevThe only time I ever get such messages is following links from HN, and even then I'd describe it as fairly uncommon. reply mtlmtlmtlmtl 19 hours agoparentprevA lot of this anti-adblock nonsense is also blocked by Ublock Origin nowadays. I can't remember the last time a webpage succeeded in bricking itself intentionally, though I do occasionally get accidental cases where the website is just poorly designed and hangs because of shitty javascript failing to load some tracker links. Usually payment flows. Though it's pretty painless to just disable UBO for a few seconds in those cases. I'd rather have to click that button every now and again than have almost every website be unusable all of the time, but you do you. reply hollerith 20 hours agoparentprev [–] Yeah, better not to install any kind of adblock even as a test. No one else does. You would be alone. Also, people who use adblock are antisocial. Also, I'd be mortified if I visited a web page, then the site told me that I cannot continue because I have adblock installed. I wouldn't want that to happen to me even once. reply dopadelic 17 hours agorootparentUh, it's more that I had adblock installed and found myself needing to disable it for most websites just to access it. Adblock ended up being more of a hassle to use than the small handful of sites where it actually worked. reply consteval 1 hour agorootparentThis hasn't been the case for me and I've been using ublock origin for maybe a decade now. The only sites that break are streaming sites. Which is expected, I mean they need to make their ad money and they sell ad-free tiers. All except paramount plus, funny enough. Yes, you can get ad-free paramount plus for half the cost if you just use ublock origin! I guess they forgot to put in that little bit of logic. reply hollerith 17 hours agorootparentprevOh, I see that I misunderstood the motivation behind your comment. I've noticed an increase in sites that do what you describe, but so far I've responded by no longer hanging out on those sites, and that feels OK. reply hollerith 1 hour agorootparentMy adblocker is Ublock Origin Lite (the one that is expected to continue to work on Chrome indefinitely). reply caseyy 19 hours agorootparentprev [4 more] [flagged] hollerith 19 hours agorootparent [–] In addition to the threat to children, there's also terrorism to consider. It is frightening to consider that the World Trade Center towers might still be standing were it not for ordinary users who foolishly assumed that nothing really bad could happen if they installed adblock. reply A4ET8a8uTh0 19 hours agorootparent [–] When I have a time to unwind and think less sensible thoughts, I can't help but wonder whether anti-fossil fuels movement would try to capitalize on 9/11 if it happened today and not in 2001 by trying to ban use of planes due to amount of fuel used. \"Terrorists use planes. You are not a terrorist, are you?\" reply consteval 1 hour agorootparent [–] Yes, famously the established miliary industrial complex, fiercely anti-fossil fuel. Never mind the end result of 9/11 was used for... pro fossil fuel reasons. And the entire war fought afterwards was done for, let me just check my notes here... oil. Oh. And we didn't even invade the right country? hm. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The FBI has issued a warning about cyber criminals using search engine ads to impersonate brands, leading users to malicious sites that host ransomware and steal sensitive information.",
      "Cyber criminals purchase ads that mimic legitimate businesses, which appear at the top of search results, tricking users into visiting fake websites and downloading malware.",
      "Precautions include verifying URLs, using ad-blockers, and businesses educating users about spoofed websites and correct URLs."
    ],
    "commentSummary": [
      "The FBI has recommended using ad blockers to enhance online security and improve user experience by preventing intrusive and potentially malicious ads.",
      "Despite the benefits, some argue that ad blockers undermine the revenue model of many websites, which rely on ad income.",
      "The consensus among tech-savvy users is that the advantages of using ad blockers outweigh the potential drawbacks."
    ],
    "points": 281,
    "commentCount": 202,
    "retryCount": 0,
    "time": 1725832677
  },
  {
    "id": 41483434,
    "title": "Sleep duration, chronotype, health and lifestyle factors affect cognition [pdf]",
    "originLink": "https://bmjpublichealth.bmj.com/content/bmjph/2/1/e001000.full.pdf",
    "originBody": "%PDF-1.7 %���� 4 0 obj > /Font > /ProcSet [ /PDF /Text ] >> /BBox [ 0 793.701 595.276 0 ] >> stream H�\\�[K�@\u0010���W��\u0006�ff7�&�� JEP��C\u0015ic�� ^R ���K\u000b\u0012\u0016f\u000f�\u001c��\u0010�{��@��\u0011!�\\\u0012� ��`l� ����\u001a�\u001e> /BBox [ 0 793.701 595.276 0 ] >> stream H�\u0002\f\u0001 endstream endobj 6 0 obj > >> /BBox [ 0 793.701 595.276 0 ] >> stream H�\\�]N\u0005!\f��YE70�B[`\u000571�A�\u0012H���`|�۷\u0014�{ ��~\f��P\u0002��t ��(\\��\u0001\"��s\u000f��t���\u001d��L�E � G�P� �\u0006�3�_�F�\f\u001fV%\u0014)@(-\u0001#1A�\u001dYu\u000f�B�RlK`5-�\u001e\u001c��E�ɳ:&LZa \fZbӭb���?n$�?���\u0014��=> stream H��W�n�\u0015}�W��\u0006\u0012E�\u000b\u0010\u0004��\fN;�v�>L�B�h�gdQCI��_�ER�d9)2E�X�$rs���^���淥M���&��޼�m��g��=�f�e\u0007 �-�s�ߋ���lm\u0006�\u0015 �\u000e��H}*ӪA�TEZ�5�~iۗK�����2q���`X���(%��=�s��v\u0012+!��\"���\u0015iv-ȸ��Lq}���\u001a�amS �|\u001b�s��P��'a����\u0005;�Tg\u0002�\u0012\u001f�G�\u0012%/�hvT�\bT�L�\u000f�H\u0002�7]\u0002\u0018�QԜ�=FN��\u000f,�MF@�\u0014(�Y���|{��\u0002\u001f�!� \u0017��̰�q'3\\q.�]4>�\u001e�12� ��\u000bA���\u000b~�b�Ϧ-3�&��0��ر�čH\u0018�9� 뀦\u001bK\u0014�\u0018�\u0015ʋ�@��H��ꤕ�\u0007�S� ͯ\u001b.�)%\u0012T­F�et�nE�w'��VO\u001f�>�� P�9\u0005� � ��i��9��v\u001b��f�$��\\����\u0002��2_n@�\u0007\u000e\u000eק��{�#�GT!�!�掉\u001a�KT\u001d�X\u0015\\0� H茙z\u000f��]�)��i��H:�zj���9F\u001e�1���w�\"�N��-R>�isT��\u0014�\\�\u0017��Qv��s!\u0010{*^]�]�6��$g5MkCq$fDr�e���[�|׎'\u0019��\u0005��O\u0005g� @6\u0002oԼ毶\u0017-.��^k�ʃ���l�o\u000b��qǲ�läiEy�3b��*�,mk\u0004�� .��׋.� ��H��\u000b\"�����}���\u001f�\u000f�\u001a�� ��,\"_+6*�\u0017T�\u0017\u0013Y���O��Eo�P%�\u0006\u0019.e��O�\u0013��ͩ�\u0019��X� �82�aW�\u0017t���J}yZ�#\u001b#p����\u0016�d^\u0014�I�[�i2h�V��\\�B���:ɤ��+@��S�\u001d�3@�q���\u0019\u0006�����;k׽������>k@�\u0015��ܾݩ0�R�v���$�z\u0002b\u0003\u0013V��UA��R_v��\u001bQ\u0018��_17U�D\u001c�`Cz�]u�\\���F�E�*\u0007\u0006pcl�\u000f����=���Øl��UZi��a���>�\u0017��{��_�/U�\u0001�\u0011/\u0007���_\u0015�/.��7��藩z�k�\u001e)��r�&���D\u001c�ƨ���Ȍ7�\u0005\u001d����T5���\u001d��>����d�W�2\\�b�v�\u001b %:�g�a\u001d�9�1/\u0005�}`�W5�\bB�\u0005��c��b;\u0019R��o���@kc5+ �c�\u000b��LB�.γd3 �!_�o��\u001ctƶ�\u0016�x�T%�\u000fj�\u000fY�����41|��x�W��8z+!\u000f�0��QO�Q\u0007�7T��əi��6P��j��a\u001bR�Uzӭ-�e-�������� j]��m+C\u001c�5y)|�A�\u0002��gY�fa����ܗ|x�\u0011\u0002��M����fW`v�z_W����\u0001�\u0015��Nm��G�3ށOe���̥_�\u0019h&.R\\kʁ��\u00117\f\u001c��\u0013�3�$��Ge\u00018\u0013�� '\"svd�r��>�0|�b\u001eD��|Wղ���Q\u0014yM�$,�Ϗ�L{\b0.3��#����@hd������\u0013��\u0015p\u001d��P ^\u0001�\u0004��ɨ1��\u001fK\u001f�Dɇ\u001b��\u0010�\u0018�h�9`C�\u0018l2�o�>/���|02^_ޢ\u0001�\u0011\u0004 ��F�y���{4�>/�\u0012)�a��b_u1��)� �\b��\u0004B�MJ8\u0019�FԸ>W��\u000e�o��?\u000f=x�QeW�����D{I����Y�ϖ�b�#�T����o�1�J�K���%��k[��\u0002r��\u0011@g5�A��}��F��t�\u000e \u0014.G d\u001cYW ����\u000e���o�(Ih\fp��s6� N��\u001d�g�>wX�t ��S��^\u0002\u0015s���\f���W�$+}���\u000f@�@�����I�e��?\u0003\u0014���\u0006�\u0005\u0014��O��\u001d�\u001e�^V�X�d�:\u001a޷\u0017\\�Erq����\u001e1\u001c$3B\u001f\u0017��s,��s!�\u0011�E~\u0010\u000f\u0013���\"�\u001c^�C�袅d��3�ƥ(�(�1�+� c(�&��-�\u0016\u0010�؊��}�'u��\u0016'^��> stream H�j`@\u000e\u0006\u000e�\u0003\u0007�� endstream endobj 43 0 obj > stream H��V{pT�\u0015���wwɃl@�\u000b�n/ �$g:�6��_q�$���yX��\u001c�\u0002~�]\u001c�\"�z��&:�������lz�~@���\"W�+�Ⱦ2E�I�\f� ��P;g�v�.�\u0015�zƒ\u0018�\u0004�I\u000f�Y�\u0005h�j��\u0006>#-�δ��;�d0�g�^�{���\u001a:� O��=&�p�QLE��4��E���v�i�R\u001b����\u0018+ �\u00041K�\u0016��Q�\u0011;�!\"⌸*��U\u0016J�\\-����Y��|K��@��\u0004���w�������T�Pb*T�i�\u001aT\u000ft\u001e����9f����f�`�`�\u001f�nr0����] a ON#ӷ��\u0018����.�c�,�N#�翝g�\u001c^�E�����+���\f���rQ>�f|\u001f�)L5�Oki\u00035�v��8G�\u0014S;]�.c��\\�\u0017��Z�Al\u0013��^�*��%� S�x'��\u0014Y.���r�\\#wɟȟ�}r���v��\"�B�BY�lRv(\u0007���\u001b�_���(�H 3\u0019�)���5[?[�m���\u0016��\u001cM��\u001d1��7\u0010��\u0007O>���\"� },\u0015�,�\u000b�H\u0014�h��\u0017\u001a�;0��n�J|�\u0015>Lo�q4O�����HKh>~.\u0007Ƀr:Ϋ+�JVP-��ݸ����\u001a\u0016'�Pò�n�ch�v���駾��C�\bO�3��aJ&.� J+e�a���\u0014E��&'�BG K��G\\f�#��#(�������lq�� ��}\u0016W�)���3(�C�T\u001cU�\"@��!�ѹ��or���2�\u0015�3�s�����1%��\u0019v�n+\u001f�M��9��\b�O��|���f.�d>OU�\u001eit��'\u0015?>��p��q��\u001d���Q#G���\u000e\u001f�͡9�C�o�����{\u001ao=��#\u001c\u001a�n|����Zޑ�F\u001fO �,��V��f;u-�%x\u0002�\u001b���\u0004�5�l痰XkNzf���x#7�\u0018>�\u001a\u0011����k,�ˏ�筍��z�S�?�\u000f\u0015�m�_8��w�� �\u0012uc1\u000bFs��Kְ8�\u0004�#s��\bX��{�\u0001s,K�=KOx@�I> �n������8\u0007��6\u0014\u001a4�����Uzye�O�\u0003�ؖW�����{lݜ���Y��\u0013Y2n�\\��l �$C��->ԵQ���2�!��p\u0006�v=� .��\u0019\u00145oZQ�ú�4 s{�E��^�%�%\u0017����p8���\u000fxID��ʈ�Z�j|�N�>m���\u0010$L�>dڤ}�ZeY�\u000f˦*#S\u0007x����E��}�����{�=��s�{����u���\u0011zLQ�8te)-�E�Ӣ(T��R��v��ݥ�m��=���m5�c[\u000f���z��yW��g���}�\u0012�yŕ��L�$���|��]�w��F:�\u0010z�ZnS�+StU���\bL�I�t@(\b�BV��� *$��\u000eYr��\u0014$�L�d���\u0015��E�-\u0015���vUs[��g�I�(2ƨ�ܢ���\u0014���շӥc�6�|�\u0006�o�\u001a1�����\u001e�;\u0002�\u0011��5�Ҭ�mP�.�f�&�=�uͻhY ~���m>��� �{�\u0001��m�6�;�>h]\u001f\u0018D�A�d�\u0019�CΝr�Ү�O+L\u0010�\"�d�d�\f�!Kn�����c۪��2�E\u0012\u0019J\u001eU� ��Ū)�\\Y�\u0003�\u001b����*\u0005�>\u0015�\u0005 �|\u0014�W\u0007�v3��'^�|A�r�����\u0018{��~jcK��ܠ�i�·����_\u001e�\u001a���֌���\u0007nvp��w�w\u0016�\u001b{� Y����r\"\"ͺ��L;���Z���)�����+������b\b��\u0007�MJID��qzJ��.%@\u0007�\u0017�I�\u0012@\u0003t�@W��c&}E\b\u0014��w\u0003\u0003\u001e\u0007�/0\u0004$���ׁ���\u0018�\b� ��T�D���AN�\b}\u0001�!ݡJ�6�*Ut@�E~�j�U.�8�jy�����\u0014���A�\u001ac�b \u0013T#ݤv�v��\u0002k]��N�Y��M\u0015��G���\u0014h�\u001c����H�=�N`\u001d��~�\u001eEa�%I�.�\u001b�ݢ\u0006�u�\u0007\u001a�~\u001b�,}\u00171�\u0013���^Js�\u001ej�m#|%��\u0001�.�Ӏ����� �u6a_Û�J7��F\u0017\u001d�� �G��d�\u001e}�D�\u0015؜��\u001e�\u0019\u001fɬ��\u0012h��(�*:��\u0012%?�o\u0018\u0014�� �]�����\u0007\u000f�Y���m�G+8�d�\f\u0012�FMt\u0018�]�5󄲠��> stream H�\\�Mj�0\u0010��:�,�EP\"�fc\f!��E��\u0003���\u0011Ԓ\u0018�\u000b߾#Ť�\u0001I��\u0002���)�\u0002�� \u001dn�� �����s�\u0005��1���>�\u001e�\u001a,�> stream H�\\�͊�0\u0010�{�b����W�\u0016D��\u0005\u000f��v�\u0001l2v�5J�\u0007�~3N�� ��L2I��:V�� xw���\u0004Mk�ñ�;�p�[kE\u0014�i���Z���\u0007\u0011����Od��/�%\u001c�Pl\u0012�#r̎� ;!o�[�dKr�.�\u0007��;�O��\u000f��\u0016�\u0002\f�7�� endstream endobj 52 0 obj > stream H�\\P�j�0\u0010��+��\u001c�dӤ\u0017a\b.\u0001\u001f��n?@�֎���,\u001f��]�!�.H3��싷�K�l\u0002�\u0011��1�`��8�%j�+�ֱ�\u0006cu�{�ד ���_�S�\u0006Ϥ\u0004�I�9�\u0015vg㯸g�=\u001a�֍��n�=�~ �\u0007't \u00044 \u0018\u001c�Ы ojB�Ev�\f�mZ\u000f����Z\u0003B]�j\u001bF{�sP\u001a�r#2)�\u001a�\u0017���3��Mu\u001d�ME&�\u000b� q\u0016 ���p\u0002�Ǎ\u001f3o7�\u0012~*� w�E�\br�{�ܓN\u0003���\u0012#�R�W���[��\u0013\u0007\u001f�T��_\u0001\u0006r\u001b{_ endstream endobj 55 0 obj > stream H�\\��j�0\u0010���\u0014s�{�D���\b�R�?��\u00014\u0019��\u001aC�\u0017�}'ɲ�\u0006�|ar\u000e�\f�ۦ��\u0001���������̫\u0015\b\u0003ޔfi\u0006R w߅��z�8��mq8�z�YY\u0002�����\u0006���\u0007�3�f%Z�o����=�n5�\u0007'�\u000e\u0012�*�8R�Ko^� �\u0007ۡ�TWn;�����f\u0010��O�e�,q1�@��\u001b�2�UA�L�b��z�G�0��޲2󇓄���,0 �9���5��s\u0013�!Σ7��\" LB\u001cs �S\u001c#\u001f=�s�1����%��s\u001d�&~:\u0005&!�^\u0012������h ��X����Q�~�N)��i�����~\u0005\u0018T��1 endstream endobj 58 0 obj > stream H�\\��j�0\f��~ \u001d�C��]M`t\fr�\u001f��\u0001\u001c[�\f�m\u0014琷��\u000e&����\u0013�$/�S\u001fC\u0001�N� X` �\u0013�i#�0�\u001c�8k���[�~��,$�þ\u0016\\�8%a\f�\u000f\u0016�B;\u001c\u001e}\u001a�(�\u001by�\u0010g8|]�#�a��\u0007\u0017�\u0005\u0014t\u001dx�xЋͯvA� ;���P�\u00133\u001d�{FЭ>_͸�q��!�8�0��\u0003��� ������8�oK�> >> /BBox [ 0 793.701 595.276 0 ] >> stream H�\\�Mn\u0003!\f����\u0017��`\f� R�.�\u001c\u0001)�������k\f�I�f�\u001f�{~��t�\u001e\u001da)\u0002\u0017`x�\u001f�Ѣ\u001aF�Ra \u0014Zdխbf���;���wͦpF��\u0017�\u0013`\u0001�gr endstream endobj 63 0 obj > /Font > /ProcSet [ /PDF /Text ] >> /BBox [ 0 793.701 595.276 0 ] >> stream H�\\�[k�@\u0010���W��\u0006�ff�d�}\u0013��Rh�\u000fZ�ƴj\u0013z�%�_߽T(aa�����\f!��x\u000b$s�\u0011����O\u0007ck�P\u001b���`Ղg-��R���\u0006��3\u001b\u001a��\u0006��g�싃�r���t��\u001f���\f���+#����\u0016�b�p����\u001b��q�h�\u0002�'�\b�\u001csE\u001d��&����R�\u000e��Q�($^Lj����-��{�\u001c+�����0�\u0013�H��.T�Q5��Ŵ&rב���ߏS&ɬ�lמ>\u000e��I�'\u001csm�W�\u0001\u001e�U; endstream endobj 64 0 obj > stream H��W[o�\u0011~7���G\u001bH\u0014Q�%;g��n��sN\u001b`{��%) Z�-�趤\u0014��}gx�d�Y\u0014A�D���\\��f�����O\u000ej��?n�\u000b��U��>og��6������0\u000e��d��Q��\u000f�[-��2^E$���_oȶ������ٗ�l\u0019�x��\u0002��\u0015�^\u0010G���䳽����\u0017-�\u001aɠA�\"ۄ�W,W�\u000bi\u0014�8�����w|~��|kw�Hȯ��Mf���V%p��̣�yF$����:�\u0002{�>�шl���W����\u000b׾��>oEJ~+\u001b����˄�H�TU��@��w�a�^�\u001c��ݒ��i���%�\u001d��.Q���m�58�l�ټf�\u0011��Y�(r䒓�x�%�!y�r�r�ʔ�m��H�m�50\u0002�(D�~/M\f4#�[�\u001aQ���\\�C�S�;�&��_'�E�c�gE���3Q_X�u\"ȑ)�\u0018#\u0001\u001e�y\u0004�?|���4-˷�G�����=�����#�|�72�k�_\u001e��\u0006\u001e]G \u001fE\u0017\u0002Ĕ �E�5�ऩ��9O\u001a�\u000f����=�z��A ��\u0015�וRb��׈�%W5\u0018뾚\u000b�g_��ۗ�)/\u001b��I����lGW���\u0015\u0005����>�������!k`E�\u0005��\u000e����\u000bR��\u000e�׎Kc�H\u0018�H*)�T�\"�˪�� �]��kp x\u0005�2~:�&��4��J�L�]L@I\u0001��\u0006f\u0001,3\u000fJ�p!Қ\u000e?â:\u0015�*��X��c�c��U����޴\u0012\u0011�g �� }���E.�\u00150�\u0001`Z\u001dK\u0002�F�'UQ��\u000b\u0016$Bd�g���i�6�T\"9(�\u0010����,\u001bm�z1��\u0015�@���ip�Y�O���%�p\u0005+Oĸ\u0010Q�\u001d\u0002�\u000e�\u001eA? �|\u0001m\u001d�1�\u0007�\u0004y\\�J�%)\biM*\u0006�簷 ��{���Y�)a�C0w/�^\u0010\"\u0002��:ǭ��X\u0010�cJq�MA\u000bY���D\u0007�&\u0007\u0006�\u001fKV{�ۙ���F��h\u001d\u0001�u*��� ��\u0006�@\bٵM\u0003:\u0001 u�\u0001[�6�\u001fc�OsVלI��j\"\b\u000e�q�l U��ԭN@6��@�\u0001��a=\u000baGE��\u000e��X�&sZ��\u0012��t\u0014�\f4��^:(\u000b�P\b(\u001f�Cn��G��8v��tra��\"�9�(\u001cA�o`F���\u0017�6�Y\u000e1\u001a� D\u001e�\u001e$��E_��.��uϤ�\u001a�k[���\u0014��Z�6��#^\u0018�Ch�g���\u0010םNn\u0014�\u0002Nǵ����c�A7�\u0015��o�a{��g\f;�$\u0001\u0012޷�bc���33t�\u0019�\u001a )�~h�~�9���\"s\u0018�6e��\u0003�ǭ,���6=\u0001��\u0016K�N;�\"�P�;��8r���9�J�|�n�஠�\u0003�T�>������\u001a� K�\"��H\u0002����S͡��\\3� orќ\u0016��1\u001d^\u0011�&}��WG�\u0013~�4�A2\u0006\u0018R\u001b�ol��\u0019��\u001b����{�A/ԍ\bHD�Ŧk� Mu�bTN���\u000f�XƼ�XzB�kG�)�-���Ւ�i��ϛQ�\u0011�7�x\u0015)�\f�\u0011,�@�jP\u0003lQ�\u0003$�ާ\u0004~)q�1k5� 0�L�@��`���}[�\u0001�0\u0018�\u0015� \u0019\u0007T'L\u001aZ�P�Eλ\u0004^�[\u0005�24 �!�8d���4�w�Y������vo�4\u0012�n\u0015��\u0019*�H�5�y�\u0015\u0002.s�\u001c�2\u001b��|3w%�V׸�J��@H �.�E*�v\\��Kc*��\u000e\u001c��J�p\u000b�\u0010��s�A��Xu�\u001e���,�?h\u0004�y-�p2�&���C�zV�9��^|�ݛ6ҁȭv���o��|C�w�t���A\u001dk�E0�=\f \u0018\u000bfZͮ\\\u0001ɤ�R\u001c�\u000b'm��i\\�m�\u0015i �lۤE�R��3���\u0015���\"a�u9\u0012�d�HL\u0007��*q���\ff�SϦ\u0010 ވ��~�U-�;=���a��WMo�F\u0010��W�H\u0001�%J�l\u0007�I\u001c�@�\u0014��\u001ej�X�Ki!�K�V�_�7��\u000f�L���\u000fi9��y���ĳ����\u0004���f��?ɳ� �1���4F['Vݹ�&���>��\u0005�ޥN��nN��sE�n_�u9�&���9����sF#��\u0012�\\)?��5�f���h0\fѫu��#k�\b&`��g\u0014�m0U&y��v�~�\u0001�%P� \u0006+Cr��eY+x�V���e�g��\u0018 \\0��s!B�\u001c��\u0010��F�e}��B�M��[l�\u000bf# \u0012Ė���\u0003��\u0002y��R�o���6�x��pY+�����\u0012LWV�\u000b^����L�/��\u000e�c��\u0013�&\"g�\u0006�AH�+\u000333a�͖\u000e���8\fLq���\u001fYG�\u0015���>\u0011C�Zԍ��^��hG\u0007�֌s\u0015y��:'�\u0012�dF���d�����\"aI�G�v�\u001fs��!�\u001cJ��� ��\u0017s����\u0014�� gȟ� �4�0�#�V�G\u0006���t��\u001f��Q p�ɹ�#�\u0017���o��df\u000e��.�iw1�ȉ���bZ�m�;�h\u0018�'t\b�\u001a�5��T�=�M�j;\u001e��\u0012\u0006BUlol\u001b�:�k\b�4��}�\u0011��`\u0010��I��\\4n���\u001e�@�\u0017�'\u0007{��١\u0005��\u0015�\u0004:�M�,mc��b�A�g47V��+\u0012\u0005��ϴ���ȶ�n�8��`����6��\u0016��Xۇkl�:H`�b���\u0006\u0014B\u000b�\u0014��;z������&\u001e�7�\u0015��.E��\u000bE�\u0017�Y�\u0004s,�ϩ��_s�.?z�\u001a^�¤A*_==��O\u001f;�ٕ�}�ǿ��\u0010����Rft\u0001LgX��on��\b�Re����:��y�B�}�f��*�YP��n\u001c�=ҫ�h\u0001�J\u001a�j��DI��+:�K\u0001��jz�����B]R�/�ߟ0��\u0014��\fp.B�S�s|�\u0013��A\u0013�j�j��زyd�\u001a�0'e\u0004\u0014� ;\u001a7�\u0007q��`O�`�J��V\u0015mV\u0013�Se*+�\u001f(�DE\u0013h�r0O��7�_���j$����}=f���&06偵�FNf��l�:����X����\u0007#*\u0018��l�c\u0019Q�\\e��\u001b�M��5�g0g�Ŝc�~鷛�f���v�Y���i�G�)5�����IJ\u000bb8�\u001c\u0010�a�t�1P1a��\u001fHt��\u001d�q�O��\u0018IcHL�O\u000f\u0012��\u000bǅu]cb´�\u0018!���n\"B�#;J� ��7�����3F P\u0015u�\f94�w4��\u001a���a�M\u00126�,\u000f�V��z\u0005��>WR�s�R%O9IwtJ\u000e5ۨ��Δ\u0005��\"L���}�qI4y\u0013�\\z�(4�?�\u0002F%�[HQZ�t��xl���\u0015�\u0011�V��f�⎁R��a� Q��W&��3�E ��H�\u0017}7)��Ҫ\u0014{��!�\f��T��\u0011\u0005��5�i�v7w�L�ACLB�\u0004\u0011o1d~-\u0003\u0005gr)> stream H�\\�͊�0\u0010�{�b����W�\u0016D��\u0005\u000f��v�\u0001l2v�5J�\u0007�~3N�� ��L2I��:V�� xw���\u0004Mk�ñ�;�p�[kE\u0014�i���Z���\u0007\u0011����Od��/�%\u001c�Pl\u0012�#r̎� ;!o�[�dKr�.�\u0007��;�O��\u000f��\u0016�\u0002\f�7�� endstream endobj 70 0 obj > stream H�\\P�j�0\u0010��+��\u001c�dӤ\u0017a\b.\u0001\u001f��n?@�֎���,\u001f��]�!�.H3��싷�K�l\u0002�\u0011��1�`��8�%j�+�ֱ�\u0006cu�{�ד ���_�S�\u0006Ϥ\u0004�I�9�\u0015vg㯸g�=\u001a�֍��n�=�~ �\u0007't \u00044 \u0018\u001c�Ы ojB�Ev�\f�mZ\u000f����Z\u0003B]�j\u001bF{�sP\u001a�r#2)�\u001a�\u0017���3��Mu\u001d�ME&�\u000b� q\u0016 ���p\u0002�Ǎ\u001f3o7�\u0012~*� w�E�\br�{�ܓN\u0003���\u0012#�R�W���[��\u0013\u0007\u001f�T��_\u0001\u0006r\u001b{_ endstream endobj 72 0 obj > stream H�\\��j�0\u0010���\u0014s�{�D���\b�R�?��\u00014\u0019��\u001aC�\u0017�}'ɲ�\u0006�|ar\u000e�\f�ۦ��\u0001���������̫\u0015\b\u0003ޔfi\u0006R w߅��z�8��mq8�z�YY\u0002�����\u0006���\u0007�3�f%Z�o����=�n5�\u0007'�\u000e\u0012�*�8R�Ko^� �\u0007ۡ�TWn;�����f\u0010��O�e�,q1�@��\u001b�2�UA�L�b��z�G�0��޲2󇓄���,0 �9���5��s\u0013�!Σ7��\" LB\u001cs �S\u001c#\u001f=�s�1����%��s\u001d�&~:\u0005&!�^\u0012������h ��X����Q�~�N)��i�����~\u0005\u0018T��1 endstream endobj 74 0 obj > stream H�\\��j�0\f��~ \u001d�C��]M`t\fr�\u001f��\u0001\u001c[�\f�m\u0014琷��\u000e&����\u0013�$/�S\u001fC\u0001�N� X` �\u0013�i#�0�\u001c�8k���[�~��,$�þ\u0016\\�8%a\f�\u000f\u0016�B;\u001c\u001e}\u001a�(�\u001by�\u0010g8|]�#�a��\u0007\u0017�\u0005\u0014t\u001dx�xЋͯvA� ;���P�\u00133\u001d�{FЭ>_͸�q��!�8�0��\u0003��� ������8�oK�> /Font > /ProcSet [ /PDF /Text ] >> /BBox [ 0 793.701 595.276 0 ] >> stream H�\\�[K�@\u0010���W��\u0006�ff7�&�� JEP��C\u0015ic�� ^R ���K\u000b\u0012\u0016f\u000f�\u001c��\u0010�{��@��\u0011!�\\\u0012� ��`l� ����\u001a�\u001e> stream H��W�o�F\u0012��b�\u0014��&)���\\�8�{�H�V��>\u001cV�J�3�U��U���7�K����%w\b\u0010�\u00075;��͛��_\\��\u0010��\\D4�x}���V_���f~\u0011��xFQ�\fc�?�\f�\\�O_��F\u0014�|u\u0011G���g�P2 �h:���\"\u0018 �����_����&~�����x\u001c&QL�8\u000e�IJ�Jy��G���dω�9�'\u001d���\u0016��j#Imw\"��\u0013D\u0001��\u000f�� `�|y�[�W��\u0015�Z���T��k�����rp��)x\u0016�҅!]ؠ�\\��\u0018|D����O�n�R��l)\u0017\u0007���KU�IU�J��MU��6�R�J,T��� ��C B\u00173�&.�p։�.� T@AeHR\u0014\"?\u0018ih��h� �b V��\u0002���\u000fx^\\��u���F @ �W�^�2d�:�CUh�;�\u0016aH���dV�gٰax�\u0006tU�2C�\f �0�y�Wb�C��5��W`��2��\u0018L�/���\u001e)��|\"����e�J���~?H�\u000b���W 2�i��_�\u0006�\u000e�\u000b߾��\u001e~�8��(�?���#�\u0001�Vn��\u0014���\u001cQ���rIF� �al��#��� wZ\u001e\f'}\u001e��\u001dÌ\u001e9:�\"��%Z����\u000f;Q�ys�.�j���\u001f՛����\u0004�z��-�ܾ���v��'��G�4\u0005 6�4�h'JL�B\u0010��~�}�G�m���\bw��� �Ch!\f�@�u3��`.}�[�D��B�C�d�j\u0016N�cO\u0002;��@�\fϲ׶u��\u001dG6�^otY�d��Vb@\u001f���ǁ��0\u001a��\u0013kI\u0013~x��\u000e\u0018p�=���h%�\u0002=y\f��8�浓N\u001e\u0007xf�Qy�\u0012ɸ�\u0012A���a2\u001eZ̎hiV�j�\u001dNg�{���-�m�6�W�Iz�X\u0010�S\u0010\u001a\u001d��5����s\u0018�7��l4\u0002R��B\u0018�п���׎�W���P+`�\u001e�\u000b�=q\u0019vB��pʓK���\u0002�Q���\u0015q\u0018C�\u001c�X�����\u0017�Q�\u001a\u0014�F �7G2�+ K [�M\u0006rcx Y�~A\u0003 k�ڽթ�l��@(�%\u000boG���s�r��K5\u0006q\u0013�Tʃ�7y��ѿ,�X\u0017�̈́9S�(�+�'�r��8�\u0006��Bc�6��Y��@��gYbE{���t-ABl�z��_6����\u0005E2\u001b�/�œ\\ ��5�1h�\u001c��\u0016��\u0018 �=���$M\u001e��ծ�L��B \\�prI�7�\u0014\"97��A�j\u00056�t����i�' ��\u0014��y\b�*����!g\u0005[l�l+�+�}\u00054�`#ب��r���V�pܿ4\u0001h#�֧+�\u0005��a\u0006Q���|U����r�Z�m$��7�i��BP�ݡ���\u000e6� f��ɵp8{�����v��g�r�&i��NU\u001dD��� �\u000f��\"�:A\u0003E�\u001b[6T��\u00029���������\u0017��1sփw��p��\u00010,��Ը=�F��� ��������m����t�\f�C��9n��@\u000e�Y*1#��U�\b��0�\u0003�d�\u0011GV���\u0017^��bސ�\u0018��\u001c�/�s;]��#\u0014�N� :jx��gy \u0003\u001c�\u000fTz�)�b�㢛�\u0006��n�\u0015���MZ.I��`m$��\u0011��ַ�k�J��b^���E�3�& �\u0012�-C��K\u001d˞0\u0017x� x�\u0010��n%Y\u001b( ��z{I#��p��\u00059�5�6�+\u0002 �\u0011\u001e\bL��!O�qL�4 �I?�j\u0019�u\"k��ʳ\u0012��)kd���ia;��X�U�˨I��s\u000eP��\u0001��}?CK��y�@�����#�^�\u001au[\u001aq#Mܲ| ��W�\u001d�_�Ֆ\u0014�7�\u001eh�\u0012��1!\u0003'|�d�5��l,��\u0005�8�bZ?d��?ۂʍ2����,#��\u001c�S8y��X{\\F�\u0006ӽ:��j�E,fy\u001a��a�U�t��8�u\u001a�q��orf�`\u001fe3�z�\f�\u0001�HiN�\u0007�\u0004/�����l�\fw�0���]Uwƫ��� D\u001f>�\u0013Ҵ��W7\\N �>f� �R�o\u0017�\u001d�=���\u0006���\u000b��:����a�����l\u0012��Tu��t�\u001fO�`�K\u0003���\u001f��inj\u0013_���\u0002��&��\u000bɜaR�a�u_�l�\f�{x�\u0014\u0016� O��\u000b�\u0011,ױB�%\u0006^\u001bB{�2(}6�%��u�����P�G�\u000f��\u001fj\u0015nK�f\u0012�P�HJ\u001e�r�\u0004�p���rj]t&{�]\u001a��#�J�\u000e��\u0013o\u001d��� \u0002�%��L�? �'z�\u000e�j��V\u001e�z��eSw�#��\u0004%��]���F�\u000b}1��i����i�zX�V6ּRSu���\u0015n��_�\u001d��A����\u0017`@��\u0005 endstream endobj 91 0 obj > /Font > /ProcSet [ /PDF /Text ] >> /BBox [ 0 793.701 595.276 0 ] >> stream H�\\�[k�@\u0010���W��\u0006�ff�d�}\u0013��Rh�\u000fZ�ƴj\u0013z�%�_߽T(aa�����\f!��x\u000b$s�\u0011����O\u0007ck�P\u001b���`Ղg-��R���\u0006��3\u001b\u001a��\u0006��g�싃�r���t��\u001f���\f���+#����\u0016�b�p����\u001b��q�h�\u0002�'�\b�\u001csE\u001d��&����R�\u000e��Q�($^Lj����-��{�\u001c+�����0�\u0013�H��.T�Q5��Ŵ&rב���ߏS&ɬ�lמ>\u000e��I�'\u001csm�W�\u0001\u001e�U; endstream endobj 93 0 obj > stream H���[o\u001b9\u0012F��+�(/`��\u000b� ��dv3�E\u0016\u0011�\u000f�` +m[c�Jdy���[�R�e}]\f \u0003�#�b�T�YԷ�ٯ�����(1��ٻ[%ޮ'����M�T>\u000b%�����Vl�\u0006�l��PB���D+���/�t�\u0007a�\f*e1��L�����/��3\"z/���k-M\fB'�i&�]2����fֻ�i\u0005���B��I\u000fws�\u0017��~pcK8���mw6��p�OߜnsG/]����#˖\u0012Q�ɏ�\\�FP�D�[��\u0019g�܀����\u0010e�\u0010-��H{�׹\u0001˻\u0001ʻ!ʺ!Ztk�{ �� X� P� Q� Ѣ���h:�k܀����\u0010e�\u0010-�)/�K�� X� P� Q� ђ�ϚZ-]�*ܐe�\u0010e�FP�m\u0004-��(\u0015ݶ�܀����\u0010e�\u0010-�\u0005#��U7\u001bdy7@y7DY7D�n��2Ū\u0016(ovL�b@�^@\u0016���I��߷[/`y1@y3DY5D�nF�h}�u\u000bY� P� Q� Ѣ� 2�P�\u001f�����\u0010e�\u0010-��l�ϱ�J�,�(�6�rn#h�-&�M�s\u0003�w\u0003�wC�uC��\u0016�t>W]������\u0010e�\u0010-�y���U�u\u000bY� P� Q� Ѣ� ��ճ� X� P� Q� Ѣ��Q��Un��n��n��n�\u0016�T�:٪�\u0016��\u001b��\u001b��\u001b�%7��Ժ�(\u0001�5\u0003�\u0015C��B����T.T]������\u0010e�\u0010-�\u0005/s�U�-dy7@y7DY7D�n^ˬR�u\u000bY� P� Q� Ѣ���^̪+ ��\u001b��\u001b��\u001b�E7ce��� X� P� Q� Ѣ��2��3\u0012X� P� Q� ђ��^\u0006k��[Ȳn��n#(�6�\u0016ݒ�>ت�\u0016��\u001b��\u001b��\u001b�E�\u0010�ˮ꺅,�\u0006(�(�h��[錯�n!˻\u0001ʻ!ʺ!Zt�YZ_�%�Q���ŀd��,j\u0019/MJU�-dy1@y3DY5D�nZK�s�u\u000bY� P� Q� Q�M[-��T�p� ӘZ��\u0018��*ՠ�ɤ�� �T����$���\u001a��T���Gpn\u001d�+i� \u0015(^�y\u0014n}%�\u0011�d��{t�v_�\u001c���ꪌ,�/\u0011e��\b���\u0011�ۗ���D����\u0016#8[8@\u000b�\u0003�/\u001c�|�\u0010e\u000b�([8��'9�?�p�+�xݐ��\u0006$_5@٢\u0001��\fI�dH\u001aje�\u0015Sb���Dў\u0014�œ�'�v2��C�>�2��t�w���\u0007���H� -��J�бG\u000b��U�:��\u0007o��ZWJ�M�H\u0007/s��\u000f\u001e/\u000bE�FI����\u001a+cT�4�CJ��^\u0006k*\u001f�g������ �����I�����L�����N�n��mM��>��L߬�כ��'�?'�})\u0013��/Ài\u0007N�#�Z\u0006��\u000f�U#ΧwwN��R���F�5���\u000f^������0C�ږf���gh����ںӦ �j�����v�X~��m���>O/招�� �k1]?�}9�}�~��tte�s��j�u*i�ߗ�\u0016oL�����o�a�\u000f������,JC'��hs�Q��\f\u001f����10��U��jP(���ԍ�.c�2j��qJ\u001d�>Ωs0�IM��(k���z�\fN��Ɯ\u000f3���O�T7Wg�-О�~�r����I\u001b(&\u0018�8�1ս%]%\u000f_#\u001d�c/\u0018�����b�'��jl�/�n��)m�f�Y^]o�oÓ�cS���X3\u0004��m��q�\u0005�Esߌ�ۣ7�2X���\u001d�\u0003��[�n�7˻+q��o\u001f��(�wC��\u0012���[�\u0004.�>�^��K�y��=�Ĵ�k�~�\u0017��4J�\u001dzO*\u0011��I�\u000e0Y+�\u0004�\u0010.����>?\u001d�\\r� 7����t���i�G�j����\u0012˻����6p�_Է��n��\u0001�#�{���r�c��Ͱa���_R��j\u0017�*����4�g��v\u001b��軁�\u0007>*E��.�\b\u0002亻ئ(�l�\"y�%�&\"�^Q���C�,{�-�\u0012�\u0011ř93s�%�[���7\bѻAH��æJ(7tF �zPy�\u001c\b\u0011�-IF�:$@�>6`k��\u0003Z��f>iy(���Yp`�F�T1��,wMә�^�r+�\\����3\u001b\u0002\u0004�!a͗-�7���893\u000f,]�î�V���C�������MƯW�\u001a\u0002J�|8���&I\u0007\u0019�\u001d���S\u000f\u0014� ��_'�fq�l2^0�\u0006��&�N|�R\u000b�E��:��/d�i����;\u001b��p=2�\u001c�N�T��r��h �$\u0003xF&�H\u0002��/�Pl�)��uC\u0012Zo�o\u0003���(o:1� �O&��أ��X�&A��h@�P@)�&SbT} T�\u001bG��}7\"��1���h�\u0011Z��yR3��ԃU�\"���%b4\u000e�\u0005\u001aX0\u0002bƱ�l\u000e�x@�7�\u0007\u0007��1\u0013b�ּ�+�RѶ�!� ����˦\u000erU�֢?�\u001a�\u001eT\u0001�\"�\u001c+���Q�Ce��2 l��8����\u001b�c\u0002�k\u000e�W\"��\u0014C%D��\u000b=��ni���\u001d�ټ{\u0017]�tv�ΰ���l�\u0007�{̓�y����~�\u0018��I�>�,\u0006�p@Z:.�\u001e*��&��\u0018\u001b~��m�np#Z��\u0017\u0011đ\u0005w��\u001fJ���4\u000bޡ`t)�u�Zp:�Kok�\"\u0018h>�S�bDW?{L�P\u000bNH\u0007yDPsg��}\f��\u0018�yT��!�m/��*��6��\u0006\u0013��\b��z\u0018N0�J��{�PL �.ѥ�_�#\u0011��8�\u0019j\u001b�Z�\u0002j:�@hE��#�Jv�Ȕ\u0019��2���$^>��f�Tz-���\u0003p*v'��,�n n���\u001a��\u000bM��\u0004Y���`�n�Ͱmf�Oy^�x�),vn��b�6e�輭��� q�c����bk�\u0004�;��gK�ڶ����/ǽzHU�B�a�a9\u0016�r�ު����z����+3�Y(��sj�{U�4��I��'��\u000f�V���-�Y,P�8en*x�_G��{U!�\b{b��\u001eE�.q�w�0��\u0006@R��V`�=M�j\u0010@±����G�-���\u001d\u00169��\u000fB�3�\u0015b�:�G�\u0001�\f\u000fd endstream endobj 95 0 obj > stream ����\u000eAdobed\u0001���\u0002\u0002\u0002\u0003\u0002\u0003\u0005\u0004\u0004\u0005\u0007\u0006\u0006\u0006\u0007\b\u0007\b\b\u0007\b \u000b \u000b\u000b\u000b\u000b \u000b\u000b\f\f\f\u000b\f\f\f\f\f\u000e\f\f\f\f\f\f\f\u000f\f\f\f\f\f\f\f\u0001\u0003\u0004\u0004 \u0005 \u000f \u000f\u000f\u000e\u000e\f\f\f\u000f\u000f\u000e\u000e\f\f\f\u000e\u0013\u000e\u000e\f\f\f\u0011\u0011\u0014\u0011\f\f\f\u0011\u0011\u0011\u0014\u0011\f\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0002\u0003\u0004\u0004 \u0005 \u000f \u000f\u000f\u000e\u000e\f\f\f\u000f\u000f\u000e\u000e\f\f\f\u000e\u0013\u000e\u000e\f\f\f\u0011\u0011\u0014\u0011\f\f\f\u0011\u0011\u0011\u0014\u0011\f\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011��\u0004���\u0011\b\u0002�\u0007B\u0003\u0011\u0001\u0011\u0001\u0002\u0011\u0002��\u0001�\u0001\u0004\u0003\u0001\u0001\u0001\u0001\u0001\u0002\u0003\u0004\u0005\u0007 \b\u0006 \u000b\u0001\u0001\u0002\u0003\u0001\u0001\u0003\u0001\u0003\u0004\u0001\u0002\u0005\u0006\u0007\b\u000b\u0010\u0001\u0003\u0002\u0004\u0002\u0005\u0005\u0007\b \u0007\u0007e\u0001\u0002\u0003\u0004\u0011\u0005\u0006\u0012!\u00071\b\u0013\"AQ\u00142aq�34r����\u0015#BUt��� \u0016\u0017\u0018$6R���&Sbc���%(CTd�� \u0019\u001a')*5789:DEFGHIJVWXYZefghijsuvwxyz�����������������������������������������������������������������\u0011\u0001\u0001\u0001\u0004\u0004\u0007\u0006\u0006\u0001 y\u0001\u0001\u0002\u0003\u0004\u0011!\u001212q\u0005AQa���\u0013\"r���\u0006#3R��b\u0014\u0015BCS����\u0007\b\u0016\u0017\u0018\u0019\u001a$%&'()*456789:DEFGHIJTUVWXYZcdefghijstuvwxyz�����������������������������������������������������������������������\f\u0003\u0001\u0011\u0002\u0011?��\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u001c���\u001e�9���H�h2�[�q�j����9\u001e� �Zױ��|1���{�n,CZ�E�K\f��lS3�~ W>�\u0019'��ș(�7�=LO���z�z��s�+�u�@\u000e� \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ ��M�\u000e�}\"zFg,���� �qa\u0003*%t\u0014즢�01����ڴ�n[ڱ%��\u001bw>垟}\u001bpY�\u001c���\u001d���uELp\u0018ꈅ�t�쩤�W5�]�'\u0017�� �\u001d`�A.������bUT���|�\buuWp�\u001c�k.pc�#K��5�p$�.p{u��|]��,���Y���\u000f��uT�bM-L�M\u0010�I\u000b�k)e�\u0005�$_b�n�^%��\u0017/Å�%\u001e!\u001b�.!\u001be�0l52�:і��\u0006G�s7�m{\u0007\u0006�\u0012�S\\�\u0016�\u0004i �\u001c\u0005�p\u001b�k��\u0006A���j�s���@�\u000355�sE�X �4�I��t\u0003@scu��\u0011߿�򠗳/�\u0005�;\u0016���B\b%{�\u0016 ���:�{�\u0017���\u0004ǲK���5�\u0006=���O�\u000f�.�j\u00066&�t���bӦ�\u001c��|�\u001a�]�7�kN�H\"�r�ăXq�;b�7����\u0015�M�UQ�d��\u00179���!��Lr8�\u0007U�I���er\u0006g��9W\u0007��䤟\u0010�����I,ie�&HiΦ�ݯq�ݭ7\u001b�\u001d�lF�,\u0005�\u0003k��c�t ��sE���߱q���l�Q\u001a}=�\b�Ƿ�I��o�@��b�?���� 重q�8�Vò�\u0011�T��M�5�H�%��%�b�\u00065��s\u0018,�n�\u000ekC�\"\u001c[�\u0011�؟ *�ˀG$����m;����1�\u0003\u001eǛ�ֽ��H{1��)xa���2&�R�y\u0011�6Gio���L�q\u001e��:cpZ��ӹhz\u001b,b��\u0018=\u0016!m\"��)���:�5�r��\u001b��+mp>.H#���N���e�\u0017������\bt�#w2/��6��!\u00070��� \u0019��\\E�&弉6e��9&�\u000bS.���%�ı��X\u000e�\u0013���z�1�>��Y�8R�l�2�!]'QG\u0015tӑQf�Qn�H[p�\u0019����m�\bt��5�k��Y�\u0003bN���z=(\u001c�u�l�`��9�}~#~\u0012\u0004\"0��:Mƣ���@.�?{F��\u001f(|�3�\u001cc\u0004�V ���I����,��l��=�ԼDm\u0016�OP�oi��ە��-h-���@@����Z|A\u001d�\u001f�\u0005X��C����U�=��\u001f�\u0005����;�Gy��|�#-ݎ'A���\u001f_�_���\u001d!��\u000e��͜ ���_c� ���&*�%?+�*wR��J��闫�\u000b����Ւ�*q����{'��+`JzVT�˟\u0018������_��?��\u00187�\u001cṮ��*\u0019+��\u0018�&�\u00045R@,�\u0006����9�H7����u�͹�{�,9ma�����H~�9g��A��\u0018�3;c�,>\u001e�Ҙ�ek��0�t��x;\u000e[�k�G��I���\u0017 ���o��aR6\u001a�Sև����P�\u0018��l��i\u0017���@\u000f=f��#ʙ{\u001d�p�\\2j��Օ\u0014r>\u0006���||��r�|�\u0015TR�H\u0001���l��G�0H�Y\u001b۫�{��.\u001b~V(>� q\u0014g��R8�^�Z*�)\"�:�*\u001f\u0014��>�Q�}nH\\ä�_��$6C\u001e\u001c\u0004�\u001a��\u001f�ă�=$zrb�=�\u000eɹ\u000b.;4���\u0014��J�ɉ�i{�1B�ݭ0�Ͻ�\u001d� ����;���Y7��t�|F�\u0019j)�[�oX�7[@2��]��\u0011��m\u0011\u0001��\u001bu�''\u001c�xfp��L2\\HV����Y�ӣ���\u0011��\u000e\u001a�\u00107\u000f\\��.��@����\u0011�?\u0014\u0013�(�]���'�\u000b(\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ ����\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u001c\u000e��\u000e�=;�0掿\u0007ɣ�N�j�wY&����^�]��\u0018. ���ΐN��}70���Ä�\u0007�QPofh��-n��Y'�;���A�$@ \u0010!6\u0017A�s�\u0018r_\f4\u001cǌQa�� .��� K�X d^�\u001do�|\u0010|\u0016\u000f�Ӄx��pf�\u0019�\u0010Ick��\u0016��z��\u0010o�*�P����Z���i�\u0010F۠s�\u001a�o�\\_c�AR�m��\u0002G�\u0005�8N�\u0012\u0001\u0004��܋�6��> ��\u001b\u0012�\f��i`��'��i�I,X; y\u0005��F,ۛ��� �������\u0014���V�\u0011lZz�%����\\\u001b�4���v�V��nH>�ɹ\u001b����Un%L�\u000fZα�Pf��#}��F�_�\u000f��x���\u000e5S�Qb���\u0014�GMK\f�f69\u0004os�i.hk�c�6q�������\u0006��}���o�\u0003�(x�\u0007�\u0004{P.��d\u0011>g\u0001v���\u001ev�[o�}�\b\u001aʝ[\u001b\u0003`m�s#�\u00066�����n��{o~V;���\u001f\u0015P��۞��p�\u0004�H#\u0017�o�@�N\u0019�\u0002>\"�񠁕O7��\b'����w��@�W�\\���r-�p-�{w�h>S\u0005�>Z�؍v\u0019�W�UY@沦\b�\u001c�\\����\u0007�]���o4�\u0014\u001f^ٞ���x��߿�@��{4\u0013��|�c� y��n6�P29����#��� S#��;�oOƂ���B��hh��Fނ\u0010|&B��M�\u0013͖�jLM��6)�I3%k\u001ew-%��@v�y�ܮ\u0010}\u0016i�X.I�eız�(�!\u0017�z�[\fl\u001a�n\\�\u0017 \u000f\u0012G�A�r�K�\ff�D�nn�g�ա�6�0\\���\u001aI\u0001�qi&�\u0016�A�s\u001f\u0012��O��L[\u0014��md�S�j'�6�,��dnq\u0001�p\u0017hm�\u001b���S��H\u0004\u001f\u00026@:�ї��������H���\u0013�����>T\u0010IZ��kN�\u000f2\\\u0005��\u001d�Y�\u0005�|�(5��VF�Q���\u001b���P_� ڨ���\u0016����.\u001b�_�\u0007z\u000f���V�s1V2�\u000f��5ED�5��_\u0001.s��\u0005����oJJ��bl��������i\u0004\\\u0010�\u0004\u0010A\u001c� D�� \u0002yo����\u0002�_~�n>�\u0018���\\��j'V�U�WӶGB�i�l�l�k^X\\ۀ�=��{\u0010��\u001b#ݫ������E��1x�?I��MUd�A\u000b\u0001t�M#cc\u001a\u0001s�\\�\u0005�\u0004�@%\u0006\u0007,q',�]C\b��\\������g5���\u0018�@�ێh&̜A��E�f%�RR��\u001d5\u0015\u0011D�&��\u001eA#cks�\u001d�\u0003�n e��R�l?\u0013����[�\u0005LR=�\u0016\u001a�Z�@���Z�\u001e!\u0006R�1��uN���\u0017�6��6V\u0017��bKA�����@�W4a�/Vjj\"��\u0002A|�m�۴\u001d�7�M���)\u0005K�q\u0005��k\u001bݽ���}�GU��E\u0019|�2���\u001b7K.I.;4[{� �5�;�Yk\u0012�\u000e�3n\u0019\u0005DOtr1��{\b\u000eip\u0005�I6;�c�l\u001b�-g�\u00079a��Md\u0015�U!����F�\u001c��l�� �\u001b���PQ�pg8�n*xk���I\u0013](n�Z8�\u001a���a2\u0017�\u0007\u001bZ��䆘�zog��s\u0014�â�0f,Vj\\�M�%�|M\u0014\u000e\u0012\u0002����\u0017\u0012$��4؋�\u0006G!t��� �����\u00132�%-\u000ee�0CU\u001c�\u00062�\u000b \"��\u0002d�\u0018���Q$ ����eD���HCK��\u0016\\\u001bl �1q{r����8ϊ���\u0011�1|�C��:)i�n f�q�댧��cc���I\"'�ɐ \u0007�� �a]0�JqW+OČ��(c�h�7Z޶G��F�,��;k\u0018�\"y�M���=9Ď�\u0019�1�ܾ�\u001e`�b8�7+�c��Sb�բG_T;�$�jV��G�\ba�=��/��4Aþ!QP�k��k`��{��h.h����o�J�\u000b��Oٶ\u000f \u0003�t��\u0005�l���>L˴�sa��{c�c�c��Z��C��bصć�s`�\u001b34t�����O\f��-~\u0011�枎���;��SI1:A����Kc���v��PA�>�y�=p�Y�\u0019�i)1,�W�̥w_$a��l���^�$�\u001a拋܂�\u0013�N��O�*�\u0006 �pP �#H=�\u001a\u000b�4�\u000e#3=ff�Q�7\f8�.+�\u001b���Y.\u0005�P�r��nǻ�}g�=���\u000e�k�?Y[OF�!�\u000e��\u0006Y�1ÉQ�3v\u000e.\u001d\\oե��\u001eɽ�i\u001c��ׁ�jᓲ&1����a��_MS\u001c���u+b.o_\u0004Lq\u000e\u0005�F�\u000e�����\u0007\u000ep�\u0010�q°L#\u0011v!CKY��g\u001aZ�\"�\u001e��[A-\u0001��KH�X\u001d?b�\u0011�:Hq\u001a��\u000b�\\/�i�\fF� ��\u0010uD�y�d�����j@�b�������[���:/���X�f��7��lXf%�Q2�J�$��PxK(���c�8�K����ʌ�z&9Ҏ�2\u001a�۬9�-�3��;\u001b��\u0003�8[��\u0012��\"�9��\u001a&b\u0018M5L��\u001b�\u0002G=���˹���Ӱ\u0002\u001d��\u0001��\u0006k�]&��\u0012k�)\u001fA@�꼭S�D\u0003d\u0012���9�F\\��@����]��i�\u0003ɽ\u000b��e'\fs�\u001dM\u001d;0V��҇ F��������q7\u001f['aڷe\u0006��\u0007\u001ez@�\u000e\u0006���\u0006_�v]�\u001d-8�F�$���8��6�'i\u0012O��`p��=!�~\u0001q|q�#a��Bș[�ٱ���W��!ꞒضZ̓p�9�E\u001c�.�y�E,\u000eu(�ih�n�\u0016��k%\u0005�\f%ĵð�� 3�8�E�s_\u00143\u0016S��l/2Ʌ�x\u0014���H�\u001dGW\u001b|�>�\u0017�\u0013��[f��\u0003�4�\\\u001dS���?+���L:Vp7�.I¿+8��c\u0018Ua|M��� �y٦vk�� :�\u0013�;�\u0011bH ;\u0003�\u0017�\u0007��\u000f����k����ua6�_Ih˝m�#\u0003%6��r\u001c�{!�\u000e�97� �S\u000b�٣*�X�'�C=KZ�I#A\f���h:\u000bN�hO�����zhj�,\u0015�!,RI[�5��c�IoV�k $Z�rz\u0003�_\u0010x9\u000eY��|�AY��QC\u0014���H��g�=�Y}]CF�ɵo\u0018�\u0002I�d>\u0012��Yr=e|�a�|�2(��D%�p\u0004j�4v{W���;ac`��_�}Q�:WQ��\fR� ���#�)�x'�H����\u001a�\u0004g�N���]\u0006�\u001d!8%����s\u0005\u0016\u0003�\u001cS\u000emW�\u001a��\\%l��\u0014�E\u0013d�s\u00015\u0012G/�\u0019\u0011�\u0001o1��ڞ1`0�Hs{��G4\u0014�\u0001�ta����i��[~ռ/���9��fM�\u0019\u0013\u000f�U\u0018}y8�\u001b-\u001d\u001e�D�\u0007����å���CCLa��u�+�:?t�ɜz�C/�\u000f�±\u0012@\u0010W:\u0016�LrK`޳]�G}��M\u0006�����[%t~����Ȫ\f�REU\u0014��cek��3����۪�v�,��\u0007�f���7+d�;1Ua��f(���t~O�2�`q-\u0006F��T�\u0016\u0007_C� ���G�\u001f\u0015�V\u0015���1j:�Fg�2:�S��,�S�Ȝ��l�n����,�Ş\u0017�|���s\f�� ��d�leP抽\u0006\u001du\u000fk\u0005�%�0��v6솶��%���\u0002���\u000e/UOQ� '��\u0016E��.���v.�����؃`�G\u0016zQ��\u0010�s]|������M\u001cRB$�X�!��.�\u0007��75��s�\u0010-�\u001as��?2�\u0014�W�Z�\b���\u0002־&b.����E\u001b,׽�:�.�4����`\u001e��;]�\u0017;8\\��coP�>��\u000e}��l��1��&�/b�ǐ�$R�@�d\u0001�Lbw��\u001b_I�\u0010l�q��}G\u001fz`e���f��?�2k��O ��\u001f/�aUMm\u0013��8u��}+\u0003�#.s��C��Z,[��\u000f���Zᛸ�\u001c��8�C\u0001��.���c�Ǒ+�\u0005��\u000e\u0005���=�\u001a?-�I�E����\u0004\u0018F/O\u0015uCii�e��S��8�\u0012�I�2l\\l����\u001a���U�8��N\u000e�؛#���q ȧ`::�e���LD\u0012\b&�8\u0011sc�\u001b\u0007������\u000f(b��K�ق���r�Q�\u001c��̒�=m#�A��\b\u001d�y܊\u0006�G��|2�F\u0019]�\u0019Q]W�T;\b���3��He��:��|�2;]���A(=�ž1a�\u0018�\u0015X�mGъ�>) `pt�F\u001bgJ�w�ܮ=\u0004��M�=2�\u0017 (x�[��E\u0005mK�!�c�t�se�������B�vnE�lH\u000f�����+f�[Űz����6�G\u0013Alq�C�{]�c�f��{�`��\u0003:j�S�\f�C���3�Q7SNe6���Ɉ���/�\u0004�tq�XX�-� �/G�*p�'��\"f\f\u000f �\u000b�\u0007�k��E\\�t�l\u001f\\����W\u0007�ޙ�C0p���\u0011�8é'\u0010��HI�g� �$���Zl\\>[ �T\\X�%�l\u0013\u0019�\u0005�v=W�b�����2:y�Ɣ�O��n�\u001c~�H\u001d�\u000e�x{��*�s�\u001bŜ\u0017\u0010��*[�\fjz3R5��ЊJ�[\u0004��pgV\u0018\u0001!�͸�\u0016 �\u0013�s�΍\u0019\u001e�\b�±\fa�ĥ��l-��\u001d�^Zl4�$l ���p�6\u0013�`\u0014}\f)\u001d3!1�S�\b�3���\u0017#\\���{^M��;^���־����m�\u000f>t��\u001eH�/G�f�+a�i\"�TpM���[qNu��㷥\u00078:9a�Ը\u001f\u0015g\u0011�T���sZl���\u0018�\u0001���� �D��C�,\u0013\u0018�����\u0014�\bg�\"ef+�D5�$�X���\u0007l�D�>��\\3���\u000e9\u001e�\u001b�n6�\u0002�Y���|[w�R\u000f\\���%\u000f \"�\"�0�W�b�\u0011�T6C\u001fU\u001bd����Q$nio�Èq\u0002�;�p\u001eu��D^\u000eg̋�fj\u001c\u001f\u000f��n\u001dW]O]Dާ�wQ�\u0011� \\��\u000e[��\u0007�d�^�Y���u#\u001a\u0006�6e�>\u001b���!��W\u0007snI����\u001f* �� \u001cA���p^\u001dF���ˈa͈\u00070�O��Hn�z�l\u000f�6�ېj�{ vQ�C��&cg���I\u0017\u001d������ǘ���A������\u001d1*�bps�q&J.�{ya�\u0004��vA�>��Q�\u0016b�����U㳾�]�\u0016� \u001b�v�_o\u0014\u001eC���c�\u0013�k̸vv\u0019g-yUy��\u0018\u001b\u0019l4�\u00154����S���2J��\u001f��s���d\u001d\u001b�d���E.k/�6r۸��\\X~�����0\u001f�V����j��Y\u000e3U׉K�.�OL�\u0003ڷT����N��Ps��Z�\u001bq�\u0014�.}\f���Š^\u0016���\u0018��8]��|��\u0013q�\u000e�t��*n�p��\u000b[\u0016\u001d��i�\u0001���7k�ܻ�9���̌�V\u000f!��\u0018~�� \u0017/B\u000e-t˦��\u001as =,�8s�\u001bA��h/v\u001d\u001d-�:�&��r�\u0016�\u0015���\u000b�\u0014'���?\\����·��C��\"�G�,��p\\#\u0014�\u0018\u0015}m%\u0014\u0011T���s�4ჭ{��'9ڟ���aa�\u0004\u001fQ�z\u001apO2`��s2f\u0001J�y�\u0013���2X��� #{\"k��_S\\�\u001c\u001c\u0001\u0004\u0011t\u001c�������Y������2K,l7���y����q��#v�7�\u000f�z\u0001\u0003_rӧ��A�\u000f�\u001c�q�w1�L\u0012��}k�XY1\u001a�mG�\u0017m�堷�z\u000f�ƺ\u0010����c\u000f�\\Wl�k�t�G\u0014�/-$�Q��$\u0007Y�/��ca`�_�m��3&e^d�^7\u000b\\K\u0013^�l$����t\u001e��\u000f 2�\u000f�G��O\u0004�����|�R������#p\u001bf�\\\\\bko��e\u00079�]�q\f���4ǅM,N����S\u0013C�c��\u0018�ܒ\u0001��\u0010�mb���\u001d\u000f�cWã��\u0011�S�6��EA��:\u0017�f��~���>��8H\u000bl\bsv �8��~�\u001b���C\u001e��l��~�l�� �\u000e��r\bh�}���\\x٧��ރ��\u0014x;�s\u001ej��+�aԕ��ī�o_N�Lmuv\"\u001c\u001a_p�Z\u001b}!��־���\u001f#�ˎ\u0015|J�T\u0013Sp��)�E]\u001b��ILi��#�a��m$!���XM�i� ̓ؽ\"x[�qk��K±�!��~ P�F�jc\u0003Y��ָ4����.y�\u0006k4��,fΓ�~�����f\u000b�u�R�L\u0014�3\fM����u0R�u88�\u0011�N��\u0018N9��+䎔�&����j?,5�RB�Z��˵�Gf��H{D\u0012|H\u0002��\\\u0018$�:�,%��xm\u0016\"\u0001=��q��rpˁ\u0019\u001b�CXpl:Z�e����:�\u00136��TJ\u001f֖������@��b,,\u001a�8�5q��W\u000fq8�E�s\u000b���V�ن��\u0004��\b☼\u0006O\u0014�:\"}��\u0006��I�s�]\u001c�FtQ�_�2�~�İ�;\\���uT����t�k[0�c,��\u001d�j\u0017p\u001b �������\ffz��\u0014�\u0019�\f3�-��t���Mk\u0005��y� \u0003P\u00044��\u0015�Tİ�;�Lŉp�\u0010ΒF�jF�MJ��� J\u0019����F7A�� �� [��\u0019�;�o��]6I��\u0013U�u\u000e�Z i��P'�o[��E\u0006�� �@5�Ƚ�L�F�=F[�o��8E�gJ������x{�ı�m���u4��f6vC�-{\u0003{��f�;7�~���\fư|C ��\u001f��:z�&�As�\u001aȚ���45� 5�7�l�8��?J�\b䬕�涿 ���J�\"������\u0018�0��S}u���\u001b#��w�\u0017;k���\u0010�j�s\u001d��YP� ��I\u000b�:�1|X[��\u0017n�\u001d�q�no{9�-�u\u0015A�� :�e���\u0004N{��q�r\u0005��\u000fϦz����ٓ\u0013�P(j�e\\��slq��\u001e�b.�{�pl�d\u001dR��4t�\u0014���[�����\u00178��\u0016��m��A�ΐy\u0007\f�\u000fI\u001c���d�:� �\u001c���\u001aq��Z�ݡ��Ť\u0010A\u001b\u0010B\u000f_t���E�L\u0013\u0003�Ĳ��\u0018)�Z�\\\u0017 �64�X�A���$m�\u0003�y=[ƻ:ڻl\u000f!f�'f,�Č������ɥc*\u001dL[$��v C5����xyv�nn�爛�C�\u001d\u001f�NX���{��F����\u001c��\u001b\u0017U�\u0001�h#b@hq\u001b�� � �~V�g�r�~����\u001cw?6��V��v-B�5�!Ņ�=���Y��A�\u000e�\\\fȘ\u0007F|N�� �l�6^멦�\u00062h��\u0016��\u0018��q�|���\u000e��nK��x�6`��g+�q��\u001b�]e�!�\u001a��ڜ2\u0012ǂF����W.��rJ ��k��_�9σs`�m5 \u0019���Σ��\u0017��)Hk�zK�x���Ѓ���@\u00118�`M��A����L�\u0017��y�=\u0019�,��\u0015LL���.���\u001d�3w��q6��>( �X5g\u00068)��:�u�;0Shk.\u0007V�:,@�7^]7a;\u000bj�^�A��3-Qe���k\u0005�6y(g�\f\bp�\u001b\u0003�\u001a�Yл2>f�V�\u0014U,\u0012��KST������\u0012t���\u001e���\u0012��tشx�\u0019.\u0017�`X��꘥���hi�2c��i{d\u0016�۰\u001e��4��A%/�3\u0014s���cTRN�\u0007/��m�DXn �B\u000f~��F,_ç�kDr�A#.�L|Mp�@7�#�r�\u0006�ºA��3\u0016H�l��0jv��\u000e���V6�5����;M\u0004Y��\u000f�%�K\u001a�G�s\u0006��c���Hj�dt����ţdA���ڝ(�t�\u001bn�\u001b\u0004\u001b�\u0002�\u0017�q�_\u0007�8�Ú� ��򽸋\u0007���1T:8�6#\fb9\u0019YGWH)�\u0007S\u001d=g��Q������!��6A�5�V�M��{��v���\u0006���\u0004���z�Y��̒;�s��K+����kOn�\u0002Ѩ6�\u00170m\u0011��`Z\u0018ބ\\\u000b\u001cF�/\u0014��& [O�cϧ��v��c!1����>��\u001au6W�]�:��=���8���C��.�̫G䮏D��'\u000e���\u0006M-��V�Ǵ>�]\u0006��x\u0019\u00165�x���m,����}1��\"W�?��,E\u0019\u001b����\u001b���oB�4S�=Q$���&&T\u0017�#c\u0004�1��J�q:'\u0017;U�����\u000283�p\u000b&��5��53�}�\u001fV\u001ce�Ҟ�s���\u001e]ܐy㤿D�����9��v�\u0006+��Ŧ6���@�Ip{.���\u00138?�� ���I%\u0014�Tbs��\u000e!\u0015{ݡҒL�c�y ���F�5\u000f\u0018z\u0019f�^�\u000eǸw�c�1�S \u001a��\u000bǗA\u0014T�F$k^�ii���,p-v��\u001e\u001fmя�n%C\u001a�3~3\u0016`�qzVS�O�� ����\u0006\u0016��\u001a�F�\u000bX��#�\u000f\u0018r{*0,��FPe��sM\u0013b�>�����z_)n�ӆ�\u0019�qp��\\C\u000b�_�\u0016\u0017�aY\u001f�85K0�N!#�Q\u0019�&��I��9�l��p\u0006+��W�҃opˡ7\u0011����#�C���\u0016H�0�b����\\���R\u0006��M0\u0005�{���@�`S�7@��x�6q���L�S-,�\u0019)���\u001b%T�:=l�;��n�\\�\u001dG\u0002��\\.\u001d�K�\u0016\b66V��.]��^~\u0011C-h�\u0015��G\u001c�?_u(h\u001dK\u001f���M����� ��o���\u0011��+4�.\u000e��G�R��I!�MM$�`���Ŕ�\u0018�\u0017#��\u0007�%�\u0018zQ�9{(K�+�\b ��n!_[\u001c�9�4��9Ѵ��>L�,��2�{\u000b ���\u0001�.\u0003qK\u0002�W�v0(���h����;YEO�لEP֒�J�\u0003[���541\u0019\u000f �\u0019xۆg�c\u0001�ð�ƞ�hg� kjh\u001a���K\u000bA�\u000fv���Of�����\u00173�\u000e\u0016g\u001c\u000b\u0016�u\u0014Ր�%\u0011�\u001a���G\u0006!\u0010\u0007Sc$\u0003;I�$��\u0002mp��\u0019׎�IȬ�Srue\u0014�u�Tb�2��\u0018���\u001b7�p݂����rh^�6\u0018�s�� �,��p\u001c:|b�� }\u0004��\u0019�\u0007�%t��E;\u0003F�\u0003w�]�aqp����$͙W��s�����q\b�W�,���9����\u001e���\u0012ƒ4�p.\u0001\u0017(:�\u0001�\u0011��ޅ�̦&��緟���O�Fl�\u00021� �\u0013��e>(c��\u00174Ji\\X�:6�H��5����b\u001a\u001e�赁W�~\u0015��6� !�\f:�'�F9����\u0016��E�H�\u0003qcm�y���3\u0016aŲ�\u0011r�c�\u0012��\u0013WI\u0017ם%H�J\u0004q���ƻ���$�lH6$\\\u0007���H�7t����j\u001c�_��W\u0019����l���\u0013��#���^]{\u0016\u001d-=���\u001e)tg��_�\u001c(�40IUQ��tU5o��yF��I�:t\u0012Ƈ�ڝ�m�\u0017҂�\u0017��#t_�'�r�\u0005S��ؽ-4sS��;�k�pG%��ӷ͡h���F�7. �ы(g�.q\u0007\u0016�Ne�~\u0013G_J�\b�ڈ�\u0013���H�\u001a~ˌO\"��\u0003}�\u001a\u0007�ُ�]\u0001]Y�(2}ni�l�T)jp�Ԉ���k�A�Iid���v۳m� w�\u0017�٫��\u0012�wfJ���bx�\u001e\"bd&\u0013\u0011�\u0014���kIs\fo\u0007��|��\u0003`b�Ì\u001d!p�\u0013�U�V�\u000b�\"��\u0011�'�wSI\u0015$����E\u0011�S>�I\u0017�Gw��\u001f��Y�%��*a\u0015\u0018���q�M\u0015C���B��x��q\u0001$�Q �ϱA�9���#�N���0e\u001a��\u0006&�%����\u000fkc{�s�v\u0015\u001f&��g�w���|q\u0007��F,/�T��=f\u001bO$�~��d���E�n���PCv�}�vh\u000f\u001aq�=q��E9}�J� u$���Z�'-��M\u001bZ��=��k��o��\u0007���\b�egH\f��E��-\u001d%%\u0004\u0012L�f,kً�;�x��\u0006����\u0011̋n�at��y�'�;-�K\u0004�%�ۅG�SPR �4�X1\u0006kk��`\u0018�S\u0019$��in�\u0004��8�]�~.c8W\u0012e�uPSRZ����\u0015�� U]\u001eb��],Q�\u0019N�߈K�L�N��}}�X�I�n.\u001a�'tdΘ�\u000f��D(�M���i��.I�k\\\u001bW\u0015K\u0003Zc�y8i��\u001a�M#]�A���\u0016���|�K��^#E3YG�Vc\u0013E9�x��':V4P��[\u000b��2��7�A�^ �C1a\u0018�\u0006h))���.\u0018�U$P��2�{� v�x�{e��u����np�0�/2���M$�3\u000b�Tu�Nl2=��e�ڞck�\u001a�\u000e�?KE��\u0007E4�D����9\u0013�N�y�:t��1�K#�p����\u0014�`��Ր5�����\u0013�[;&�$���\u0012:7f\u001c��\"�0y,�\u0006����~�s\u0018��|ԁ�c[�;h��ݏ�O�\u001b7\u0012��d��X���AP�6�\u0003�Ý!�[\u0019|��@C�uz\b{lKÉ>m�(��_o\u0013�5�\u0018�� �\u001c�qX)��*(�t��\u0005Ŵ:{$X�݁\u0004��\u001e��=�|R�5_\u0017�&\u0015&\u0011\u0019��\u000f���l�x�\u0004,�K��k��9�\u001b8o�\u001aC�]-r�g\u0019(i8\u0005�\"�X��l � �i❴�\u00066�\u0007��jm�f���;�nI\u000fYt8�\u0016b�tY�8f)Z1d9��ފ\u001cN��Ք~I%\u0006!IS };*��\u0018� 8ĉf��Hqt�i��܍�pl�+g�#����2$�'\u0010��P+&��\u001bR\u0018z�K��ch�wX�\u001dm�]��h;{\ff\u0017\u0016\u000boڿp��x��\u0019%����\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0004&�\u000b�a�4\u000b�sm�0UDd\u0011�\u001a�q ��\u0013nv\u0004���\u0014\u0013�\u0010\b\u0004\u0002\u0001�K��\u001b�I��\u0019�\u001cS'`5�u\u000e/�z�.�Ydq������\u001fK�(!º3p\u0001�eU\u000eK�)�f�\u001c&�7�S�\b#�(7kZ\u001a\u0002� T\f�A\u0018�AJXj��d�\u0003�i\u001bo� ��\u0007\u0003mŐE䭳vݶ#s��\u0010 �]N��Mwu�a�sA#�U\u001d�6\u0001\u0005 \"����6�؎�=ۏ�\u0004�R�\\LRuw ��\u001d}���`>D\u0012�\u00172R�vw1�Z�0A-�am�D�p�\u0016�\u0004��\u00026�. �7\u001e7@�@%�\u00176\u001b���>%\u0004O�t��ݝ��Q�>|�#����g���^_:\u0007�L6.\u0006�o�ݿr\u0005��k�\u000e���\u0003\u0003��\u001b�W��M��������}F��\u0010.�\u001frl�\u000e�7���\u000b\u001fF��\u001c��?�f�>,� ��V�AW)��3\u0014�\u0019\u001c���;ʙ�WX��C@�\u0007p\u000fm���XW\u0004��>\u000b�\u001bA\u000bX\u001d���!d@��!n�\u0006���!���$\u0007S� ���w���s�$\u00132\"�Oy\u001b�W$\u0011GL�\u00178��q��oP�6��\u0011E;/����\u0006�|W�J\u00072\u0013\u001b44���+ .���w�/��@�9\\I$\u0003sko���~�\u000b\u001cr0��u��[w\u000b\u000f��\u0003��\u0001 6��恬�̿\"O3��\u0004][���[���\u001e�\u000e�\u000e�k\u000b����\u000e�PE��E�&��\u001f�A�x��\f+���\u0015�ջK��L�\u0016y���`�fF\u0001�nA�kn\b ˢR��V\u0003�m{���(� :F�~`��A\u0018��\u001dn����}\u0017�/⠷��\"�\u0003{���;\u000b h��;X����\u001d�!�S{?հ�\u0003Z�E��W�����ϒB\u001a\u001dk8\\ۛ{ǡ\u0003�k0�������\u0014\u0012��.�6�s�\u0005l-k�\u0001��\ba�x����-m�\u0010>R\u0001\u0017\u001b�z�=��u\u000e�h\u001a�J�D�U�\u000bZ�\u001b�9����6>\u0017��d0H�\u0004�?싹\u0012{�#k[�� WE;���~�\u000fЂm\"C{w�� �\u001a��]��n�ah\u001d�;��K�Hu]��ݹ}(\u001a!�H\u0006K�d ��km���������\u0007OhnF�×������7Z\\��l;r;}�3��w�x�\u0002S/�\u0016�����ֳN�O4\ft/-�_��� ��y\u001b�\u0002\u0003Ϛ�r�q�r\u0007�\u000b�E��w;�|^����]+�\u0001�\u001c盰^ �����\u001a���\u0006ǈ\u0007�)\u001b#�\u0016���9݉g\u0004�3}\u001e\u0005\u0007�t~���lŞb�\u000f\u0015���1L8I\u0005\u00048xt1���;g�\u001b�{��\u001d���N����� �X�;6\u0002;s\u0002����PJ:�˫��\u0013m����߷~H&�\u001c\u001eAհ�6���@����\\�X@�\\�k��悴�\u0006\u0016ص�>-h(\u0006�r�\u001b��\u0007p���(\u001e˒]���n;�9�~�$5�s�]F@\u001cӷ����D$i\u001bjf��ߞ�ߟ�t\u000f\u0005�m����\u001f� \bg��D5๖��>�d\u00126WL\u0001\u0017i�'/��A㎛|\u0012�zEp��+�Ӷi�)�\u001a��v��G̎S}��E��A�\\\u001e�J H�����]�i�6�>����\u0006X�@�y���\u0012\u0004d��\u0016� \u0013�@�\u0017:ڹ_}�F�k\u0005��6���\u0017A\u001416���\u0017\u0006�`݈��߿�\u0003Ca�0=��w���\u001e\u000bz�\u001e�X����/�\u0007Ťn�E�qi����\u0003:�I s�.h�嶭�k��%k�|�mݷЀ7 �H��6�\u0010W�\u0018��\\����q;�\u001b�X� ���K�k~�m\\�|\u0010F�x�gV�t4شoϵ���A=����ܶ�.���\u0006��\u0002��q`6�ڱۑ��|\b;�qsD�s��Ga�����A!�Xqq\u001c��_n�Z\bgi/\u000es�\u000e�\u000eV�*\u000bl~��\u0004z�@�B���G$ $D59��������v��\u001aF�A\u0013]�9�š�\u0017���\b\"1�]r�:���%�\u0002!\u001b�\u00041���^�\u000f�V�Gi�;��PH߮\u0006��\u0016�w��@�Ip��\u0017����?��PK{%��m�r�\b�C\u001d������� ��@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u00105���\b\u000b�\u001d� ~\u0017ﷂ\bj#3\b�ͯ\u000e6�]\u0006��`���ii�L#\u0014q\u001e��a`{J @ \u0010\b\u0004\u0002\u0001�@ �U�_�\u0014\u0016P\b\u0004\u0002\b��\u001d�(\u0019\u0007�\u000fR �\b\u0004\b��\u001e|���\u0005��P\b\u001bnh)V:Ό~��\u0017�\b\u0004\u0002\b�� \u0006��\u0013 \u0010\b\u001b�_\u0012 ��wz��\u0016�\b\u0004\u0002\b��\u0010I܁P\b\u0004\f��(\"��~4\u0016\u0010\b\u0004\fo�PSg� ��H\u001e�@ �Q�\u0005��\u0002�\u0010!@\u001eH#������@ ��w>��\u0005�̠r\u0001��G$\u0013�H\u0015�@ �C�_\u001a\u000b�\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u000f����\u0001�@ \u0010B�\u0010y��\u0003oZ\u0001�=����sA��\u0015����+[L�\u0013ACQ4n�:^Ȟ�\u0010A�؂\u000fx�\u000f��~i�3�X��\u0011������K\u001b�(�Ɂ�������p \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0011G\u000b\"n��\u0004\u000ekC9 ����$���D�}��@6&0\u0010\u0006ǚ\u0005k\u001a�a�ck���#�@�B��l.>T\u0011K\u0018h�\u0003�\u001a�\u001c�ϊ [\u001bX��6\u0001� oP�\u001a-�\u0005\u0011�\u0010m�Y\u0001$M�Y��\u001eֆ�\u0007!� Ӓ�4�H� �0}(\u0017H���$��V��\\\u0014\u0007V��V���~T\u00112�(�sv�g�;��%1���6��ǵ\u0005&4GP�>˟Ȃ�h\b\u001a����s@� �G08X�i����� c\u0018�e��\u000f�_�@�nD�_\u0010A;X\u001b{w�C\u001b\\A#p��\u0017�\u00066&��\u0004\u0003�d��\u001e� ��k \u001c�6\u0006�\u001a@�B��b6A\u001f���>@��S4��\u0005Yicc\u001c���'�\u0003�%��\u001dބ\u000fm4c�}�'�\u0007 ����\u000f��\u0004\u0013�m\"�\b(G\u0003\u001c�/R\u000b�E�d\f4�{\u000f� ~�k[d\u0011\u0018#��=v\b)�\\I\u0018�.h��]�x�\u0007�\u0010=�k\u0005�@����\u0007�\u00109�k\u0005�A\u0004��p$�b\u0002&��`�b\u0004�H��w��N���k �Ա�ܷ؂F��\u001aC}�)�N�!\u0005��\u0019\u000b\u0001��F)�\u0006�\u001f @�\"ֲ\b��?\u0001�\u0004\u0010M\u001fVF���V��;L\u0007�\u0002 Z��,\u0002\u0006�\u0006@��c^,B\ff\u001d\u00134^��\u0019\u00171�طo�\u0003|�?\u0001�\u0004\u000etL\"�� ����� �$\u0013\u0016�\u000b\u0011�\b�M\u001b �G�\u0010J�\u0002-m�\b\u0005$m7 � �Ѵ���\u0003 ,n���\u0004\u0013\u0016�� ����>@��hx�\b\u0018�\u0018�@|�\u0003�\u0003���\u0004~O\u001f��\u0002 Z��`��\u0004\u0002\u0001�@ \u0010\b\u0004\u001f����\u0001�@ \u0010yO�n I_�|jY)��V� c�\u001bK����E\u0003�tM\u0018߿�@�@�W�yY\u0005��\u001e�\\���=�7�PYs\u001a��\b�>\u0019�����U��.h�5�yְ�?E�Ѡ����;_փUb?���?�F1��m�\u0002\u0001�@ �\u0001������\u0010*\u0001\u0002\u001eH*�r\u001e��(-�\u0010\b\u0004\u0011�/\u001b���e8�\u0018� ��@�*�����PZ\b\u0015����AJ��tg�����@ \u0010G/�P6\u001f4 ��@߲��Wg���� ��@ \u0010V��o�����P\b\u0004\f��(\"��~4\u0016\u0010\b\u0004\fo�PSg� ��H\u001e�@ �Q�\u0005��\u0002�\u0010\b\u001a�H!��O�\u0005�\u0002\u0001��}����\u0016\u00072��\u0004\u0002 �\u001c�N9 T\u0002\u0001��\u000bt��\u0016��\u0005@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ ����\u0001�@ \u0010in?�Z�%�|G\u0004�xd�\" &����G!��\u0003�a�!�\"\u001fO�\\�&S�X6\u0011$�]\u0006\u001fIJ�6\u0017t0�3{\u0017\u000em�&�%\u0003x��r^8m� �?,\u000fA�\u001d\u001a�\u001d�������z\u0005� \u001b r\u0001�@ \u0010\b\u0004\u0002\u0001\u0002X M(\u001a�f삥\u0014]SH�(/X ,��@�\u0002\u0003H\b+�2�`�i[h���A`�B�\u00100\u0004\u0001hr0\u0004\u000eAF����zi\u0005��[p�t�]!\u0001d\u0005�!\u001b ��=~�w��i\b\u0017J\u0002�\u0001d\u0005�@�\u0012Ѳ\b�%���S\u0018(\u001c\b\u0015\u0002Y\u0002\u0016���St�H����K %�k sw��P:�\u0001d\u0006��4��V�@4 �Y�����I`���\u0014 �\u0010.�\u0006�1�2%\u0007Ġ� M �h�\u0004 �\u0007s@5���)\u0017AF��9���}�\u0005��6@�\u0002\u0007\u0006���\u0002�\u0005Z�jb\u0005�\u00043p�Ɣ\u000b� ,��(\u0004\u0015�qk��>�\u0013\u0006�.P;@\b\u0016�\u0002i\u0017�\u0007 ih(1�f���>�{Pd�4\u000e��%�\u0016\b\u0019Մ \u0018\u0002 ��H�\u000b��\u0016@Y\u0001� i\u0016\b �\u001a�&��\u0003)#!����\u0005�*\u0001�@ \u0010D���7\"���@\b�s�y�]��A��\u001f�fa\u001f�#\u0018�eaH6�\u0001�@ \u0010BƆ���A �P9�悼�\u000e \u001fZ\u000b\b\u0004\u0002\u0001\u00028\\\u0010P44Z�\u0003� \u0010\b\u0004\u001126��\u000f4\u0012�\u0010\b#p��\u0002JƿM�\"\bA*\u0001�@�\u001f�ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010&�{_t �@ \u0010\b\u0010�Ȭb\u0018��\\\u0010��\u0016@�@ B�u��\u0004`l��\u0004\u0002\u0001���qr\u0010;�@�\u0004\u0002\u0001�;�U�.�D\u0002\u0001�@�\u0014 n��\u000f@ \u0010\b\u0004\f~���/J\u0005\u0016\u0003d\u000e@ \u0010\b\u0004\fv�� p��\u0002�\u0010\b\u0004\u0002\b�\u0005���\u0004�\u0004\u0002\u0001�@Ɔ��\u0003�\b\u0004\u0002\u0001\u0003$��@\u000b[o�\u0003�\b\u0004\u0002\u0004(\u0019 nĠ��@ \u0010%�FC\u000b���\u0003��\u0003�\b\u0004\u0002\u0001\u0004l�sb.����\u0004\u0002\u0001\u0003]ko��[nH\u001c�@ \u0010\b\u0004\u0002\u0001�7wz�j|G�\u0019�G�\b�?�XR ��@ \u0010\b\u001a�`J PɪD\u0017�4 �@���b�3� Y��tn��@ \u0010G3����\u000f�\u00044��\u0018 \u000b 7@�\u0002\u0004C,W>%\u0005�� z\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001\u0007����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010S\u001e���PZ\b\u001c�@ \u0010D�;�Z�8���/�\u001fZƉ� ����\u0004\u0002\u0001\u0004R��\u0012����ނd\u0002\u0001�AN��o��H,��@�\u0002\u0001���� ��,�>\b/��ʁ�\u0004\u0002\u0001\u0003B Ԝ��Am�@ \u0010T��\u0007|^�\u0012��Ǩ �r@�\u0004\u0002\u0001\u0005*��XAk��$\u000f@ \u0010\b\u001ay�\u0014#���_B\f�\u0001�@ \u0010U���Y��\u0016�\b\u0004\u0002\u0001\u0005Z�1\u0003���\u0016\u0010\b\u0004\u0002\u0006�� �^h���\u000bH\u0004\u0002\u0001\u0002\u0014\fAO�Z\u001f\u0003��^(\u001c�@ \u00105܊ T���\u0017�\b\u0004\u0002\u0001\u00042���\u0014 O�mA:\u0001�@ \u0010\b\u0004\u0002\b���A��\u001f�fa\u001f�#\u0018�eaH6�\u0001�@ G\u000b�\u0010T�-\u0012\u0012���P\"\u00069�M,\u001aM8���!4��9#r�\u0010\b\u0019#u��\u0010B\b���d\u0013�d �@�t\u0015��C���\u0001���\u0005J�:�\u0018�\u0002\u0010]@ \u0010\b#�]�\u0003 m��t\u0002\u0001\u0002 �\u0014Z_t\u0016�\b\u0004\u0002 �G��Ђf�!\u0003�\b\u0004\u00119�`!\u0003)�մ�\u0016\u0010\b\u0010�;�To����P\b\u0004\u0015���-A3F�\u0004\u000f@ \u00101��\u0010G\u0003:���A:\u0001�ATCg\u0013�Ahl�@ B.��Ku5\u0004�\u0016\u0001\u0003�\b\u0004\u0002 �Qu1�ւ�y]\u0003�\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b?����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010T\u0001�e��@�\u0004\u0002\u0001\u0005J�� $�C�Ԕӊ\u001a��V \u000350�k\u0012N��\u001eż�\u0011['u x(X �@ �Sq`�a\u001aX\u0002 P\b\u0004\u0002\u0001\u0005J��9����\u000b-\u0016\b\u001c�@ \u0010!\u0017AGC��V��H/\u0001d �@ \u00104�PAN�ˠ��@ \u0010\b+հ�\u0013��{P:0KlE�J\u0010*\u0001�AR��}�\u0010Xmɹ�\u0003�\b\u0004\u0002\u0004!\u0005!\u001b�1}���\u0001�@ \u0010V�\u0016�ޒ}�\u0005�\u0002\u0001�A\f����\u0004�\u0016��\u0004�\u0004\u0002\u0001\u0002\u0011p�\u0019۬[҂`n\u0010*\u0001�@�()����Z�M�t\u0017 \u0007 \u0010\b\u0004\b�E\u0005H\u0018Z�\u0014\u0016��@�\u0004\u0002\u0001\u0004R\u000b��B\u0004�i`\b&@ \u0010\b\u0004\u0002\u0001�A\u001b��h5>#�\f�#��c\u001f̬)\u0006�@ \u0010\b\u0004\u0002\bc~�8[� \u0007y@�\u0011��fQ�8��@�ɰ�좈�b ��{\\\\s\u0017SE j�!ln\u0016�qĨ\u0004\u0002\u0006��\u001aO��\u0006���\u001c�D\u0002\u0001�\u0016K��[� �\u0001�7v��\u0003�\b\u0004\u0015*4�\u0007�?���{�t���tv˙d�)��U�܍I\u000752(9\u001b\u0004\bE� �B�#�,�h�X� \u0007�\u0010\b\u0004\u0002 �Dd ��\u0013\u0001`��\u0004\u0002\u0001\u0002\u0011t\u0015z�u����Z\u0002�\u0015�@ �I�\u000f�\u0005z:wS�\u0007[w\u0013�\u000b�\u0004\u0002\u0001� ��і�f��h\u001e�\u0010� x�P\b\u0004\u0002 u\u0014�ֶ�(,�:��\u0007�\u0010\b\u0004\bE� ͙́���\u0005�\u0002\u0001�@ �\fF2�{�?9A3E�@�\u0002\u0001�\u0019�26�\u0002��\u001blPJ�@ \u0010#�� �0:f�-Ͻ\u0005�\u0002\u0001�@�( �\u001d�k��m��\u0005�ܐ��\u0004\u0002\u0001\u0002\u0011p��1\u0016=Ġ��@ \u0010\b+�\u0019,x�@�v\u0018����@ \u0010\b\u0004\u0002\u0001\u0004n�������30��\u0011�2��\u001bq�@ \u0010\b d�s�\u0005�A+w�\u0003�5\u0018#��i\u0004�\u000f\u0012=������\u0005b�]u��i̷m��й��Q������pcK�p�\u0006� ��Ò\u0007�t �@ ����\u0007r \u0010\b\u0004\u0011�ߗr\b�le��� \u000f�\u0005�\u0002\u0001��85�� ����\u0004�t\u0002\u0001\u0002_{ ����,\u0017�A2\u0001�A\u0014���\u0007�\u0003õ\u000b�r\u0001��v�r���%\u0017\b%@ \u0010F7(#\u0012�� ���$@ \u0010C$�2\u0001�A(7\u0017@�\u0004\u0002\b�(kK�\u0010\u0011J&m� \u0010\b\u0010��\b�\u0016e��(�54\u000eY�Wq+\u001bc�KD\u0010\u0010�\u000b\u0002c�����~�\u001b��\u000e��>f��X��\\i��\u0014j��fB\u0004&�!�@�� ��.�P\b\u0004\u0002\b��L�C��\u0004�\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u000f����\u0001�@ \u0010h�,��\u001c\u0012��fLK��+Y#5S��,��p�K_�? ���w�\u001ca��hß���,��k\u001f �h@q`x����n\u001c �>\u0006�\u0006S�ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010U\u0013v����7@ \u0010\b\u0004\u0010��m\bm5�r �n\u00109�@�`�9\u001d�%\u0001\u000b�4 ��@ \u0010\b\u0004\u0002\u0001\u0004n�������30��\u0011�2��\u001bq�@ \u0010\b1������\u0005���\u000f@�`����\u0012�|�@Ƹ�ߣT&[2\u00105��T���~h@�\u0002\u0001��\u001f�\u0003����ށP\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b?����\u0001�@���!-2 !�\u0018A�~���)����f�ɒ��ٖ��H��%8q��&C��fu�\u0006���]m��{\u0013��\u0012�o\u0012��h�0�|\u0002JJ�KYC\u0004B\u0017�3#i:�QO�\u0007Td�7�ȸ��k�.q_�P�8�$\u001c>��\u001eM\\�+8�\u0018\"!\u001c�?��y���F�ۘA�z-Ub5\u001c>�������ϏX~�\u001ahK������\u000fH���ݨ\u0019,���P>7�y�\u0001#�\u0003f��)�\\{L\u0003����+��ݢ��+yD��A|�Ć�n�^���uvAkT�|��U\u0013T궁o�҂槆�m�vAO�j?u��=�a/\u001aM��������`����J��q.��\"g>H�.��:B{Cu��[�č��5ݖ\u0002?\u001fJ��\u0004-��\u001c;B� й�lݐ+$�\u001e�vA L���@�I&,\u001d��AfG�\u001b�PF�*;ڂR�t����\u001fS���~T\u0016��#v�\u0014j���m�;�i\u0006B9\u001c���*�MG�\u0013A,�u��\u000b Ie��k\u0001@��c�6�\u001b4��ݬ� �K+�,�?��ܳ��\u0010�\\x�k'��\u0003\u001d��X���m�A\u001c�\u000e�\u0007��A+�0�Ђ\u0006M;��\u0018� ����d�\u0017� _%Gs\u001f�\u0004��i\u001d��\u0019&�o&\u0003���O\u0013�w�� l��8�m� �:g0�ݐ,o��ـ�|2����a\u0005�7?��#$�kAknPC�\u0013��\u0005�9�`$o�������6��J\u000bmt�wn�1��P���&�I:��\u0017�z�'��~�Ab %���o\u0004\u0011�-COe��|.���A\u0014��0�\u0019t\u0010�,�\u0002�\u0002��AeΘ\u0012\u0003v�@��1p��\u0004\u000b3�o�۠�9j\u001cwe�N�Km�\u0010C���?�ʂ���8%��\u001e�\u0013�z��\b�\u0010Xk�\u001b1�,̈́6� v\u0010�\u000e6i�����֦&(^[}2\u001b\u00174l6�7P|gA\\&�\u0014���b�)�(q�i�4\u001dln�I\u0013�4�X�#\u0018�\u001cɣ�Ѱ�\bh{g� s�n:� [\u000f���\u0006��(>\u000f�\\E�*\u001b8���߸�\u0011l=\u0001\u0007��\b\u001b�k���\u0003�\b�\\ \u001b T\f�|P\u001a=(\u001a�t��\u0004�\u0013t\u0002\u0006����\u0017t\u0011K�oJ\u001bMO?tq���m�4g�T�t���tv˛d� ��U�ۍIG52(\u000e\u0004��cl�\u000b�z\u0005sC�$\f���w {�#d hp�\\ �\u001a�\u000ee\u0004�#tA�\u0013��8ݣ`��A>�\u000b��PU0�ͫ��H,B\u0003r���\u000b��T\u0010A\u000fR�^�\u001c��($\u001c�&�\u0002�J\u0006Yý\u0003_\u0019�X�\u000fc4\u000b z\u0004�\u0001�\u0003@#ҁwA^h\u000b��w��A�@ \u0010\b)�y���i\u0004���%@ \u0010E/�� ?4��\\@ \u00101�qAM���� z\u0001��W��\u0016��\u000f@ \u0010S��\\���/�\u0005�\u0002\u0004+,��Va�������3q���?��]:�J7�n%;s�����\\���Ӫ\u0013����䂥_$\u0016��B\u0007�\u0010\b\u0004\u0014p�p\u001f\u001f�\u0005�� �@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�A����\u0001�@����\u0016\b\u0018bi7��=�*\u00028c��-\u0002��\u000b]\u0007��;���pʯ�H��z4����M��H=�@ \u0010\b\u0004\u0002\f|L=k���� \u0010\b\u0004\u0002\be䶄6��}���\u00021\u000f�#?��k�|�\u001b��\\�'���\u0017\u0016�n�jH9��A�� \u0010V�i!\u0002ҋD>?j\u000b\b\u0004\u0002\u0001��SK�ߏ��Zh�@�\u0002\u0001�(1�Y��A�\b\u0004\u0002\u0001�)E�\biZZ]� ��@ \u0010\b*ִ�\u0017\u0001���}8�\u0002 \u0018-Z\u0007�\u0010\b\u0004\u0014j�^PZ�Y� ��@ B��a\u0012\u0003�A�@ \u0010\b\u0004\u0002 4�!��G�P\\\u001c�\u0007 \u0010\b\u0004\u0015�E�P2����\u0005�\u0002\u0001�\u0010R�i.\b.3f�R\u0007 \u0010\b\u0010�b\ft�&�����A�;�\u000e@ \u0010\b\u0010�\u0010P��J�\u0017�\u0005@ \u0010#�\u0014\u0015�.a6�d H�#\u0017��\u0001�@ \u0010\b ��\u0007Oh\u0002�2\u0005�B�>�\u0018�cTbWFg�\u0010��\u0004�.�M����\\�v�\f��[}�j�D߉8A����c�����\u0017(\u000b�\u0005�\u0002�\u0001r��\u0004lli$sp\\J��q� �E\u0007#`� \u0001ֶ�\u0011�\u0001�\u0007 \u0010\b\u0004\u0002\u000686��\u001eP\b\u0004\u0002\u0001�>ƫmt\u0012 \u0010\b\u0004\u0002\u0004ؠkt� z\u0001�@ k�۵�ҁE��\u0002�\u0010\b\u0004\u0002\u00068���@�a�\u0005@ \u0010\b\u0004\f�\u0013m���\u0004\u0002\u0001�A\u001bt��\u0003��\u0003�\b\u0004\u0002\u0006Ikn�E��\u0007 \u0010\b\u0004\bP1�\u0012�D\u0002\u0001� \u0006 h\f/�ײ\u0007��\u0007�\u0010\b\u0004\u0002\b٦��@��@�\u0002\u0001�����m��$\u000e@ \u0010\b\u0004\u0002\u001b��\u001c]�d��#\u001dġ��}&\u001dY3^ז\u00169�=��\u001b\u0002�^�͵�r߀x\u0016\u0007��\bg���\u0007�\u0010=�k��\u0004z\u0010W�v�`G�t\u0013A |ct\u0011��\u0002F��\u000ed�q�\u0010+�\"g7�z�A#ek٩�\u0011� B����\u0016���\u001c\b��A\u0017��{ko�\u0010?�m���PA�Q��|�\u0003MC\u001d���\b%mC\u00077\u000f� ��l�i\u0007ԁ��6�\u0017\u0001�\u000fk��D\u001fR\b'�dB����\u0010: ���c�@�O\u001b9�\u000fY\b\u0015�G'��}D I&c\u0001���� \f�v\\\u000f���%Hk��\u0005�gd����Z\b����\u001f(@��l����@�L��p\u001e��'�0�\u0007\u000f� sg`\u001b�\u000fY\b$l�\"\u000f���󱎱p\u001f\u0018@��a j\u001f(@��1�d\u0007Ƃ:z�H�\u0010$��;\\�\u0015�f\u0010��G1�ӛ[\\��2ĭ�ӱۑ�\u0014����]J������=��3~A3$q p$x\u0015ȫ[�Q� �+\u001b��|k,\u0011��O5���\u0006�3\"\u001d�\u0001�6A\u0014�, Y��\b%�v2�p\u001bw�\fmK\u001c@\u000e\u001b�B ����\u0001\u0003\u0004�`��\u000fs��r@\u001e�\u0014骚F�\u001f*\u000brJƷUŹ^� ��T>P�a#m{��\u0003\u0005L$�[o�p�R��{� `�3�C�\b\u001cdh\u0017���\u0003\u001bQ\u0013�=��B\u0001�5���P3�Y���\u0016\u001c��f�\"�Vߔ ��k\u0005ɰ��`�'rp?\u0018@�5����\by+�d�l��1���.n���\u0001�-\u0017k�CI 9��n\b��,�~�9d�q\f� ���u\\�=ϛ\u0010�;��,2\u001b�IkC��\u001b�\u0007D�q�\u0018�K�䮳�A\u0003��� �\u0013\u0006�7ڂ� �/'����@�Ϛ�?'��Y\u0005:ƈ����c��[�?S�A#\bh�d\b�#0�@lB�\u00043��$ۚ\u0002��k\u0010J\u0018�v@�V� \u0006����@� `�9\u0004\u0018�d�n\u0010dnѲ\u0006�\u0018��@�v��\b����PC+E�p���7\u0001�\u00075��X k��Cr\u0010=�l|�1���\u00044����X-\u0012s\b\u0006����$�2nh\u0012(ㄐ�\u0014�m�[�\u0005��\u001cn�n��\u001b��@ @�D�9�\u0015'��sl\u0010Y�X�.\u0010=�l|�1�\u0013{ @�\u000f��@K\u0004r\u000b��\u001aXX�M��Ys[0�@����\b\u0015�>�0�\u0018�\u0006��a�����õq?\u001e\u0016�@���K�_���mçn|�\u001b�&5��s\\���Q� �\u001b$�\u0011��\u001d�@�����\b'�6\u000eH' :�w @�\u000f$\u0012�׋\u0014\u0011�#i�\b\u001cCe\u0016()���\u0015��ۃm`.��G�}�$\u0007kY\u0003z����HC\\-܂&����\u0004�[kw ��2>A\u0003�\u001d� 47�\u0002�?b�?'���΁�L��\u0003\u001bO\u001b9\u0004\u000fq\u0004Y\u0004ZG�}�'`�@�\u0002\u0001�@ \u0010\b\u0004\u0002\u000f����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010U =f�\u0016B\u0005@ \u0010\b �\\-�Ci��ޏ� �1\u000f�%?��]+��\u001d��Y=\u0006|ตkw#RA�L�\u000eF�\u0004SI�\u000b�*�\u001b�\u0018��V$i�A����o�\u0014ъ������)�\u0016���洮4S�զ�)\u001c���%\u001cN)�0X[4\b\u0004\u0002 �4�� �� T\u0002\u0001�(*ug�Ԃ�@ \u0010\b\u0004 ; ����Aa��6Q�3\u001a�oyj�!�F%k�Q�8\u0013\u0006T�Q���RQ�9�\u0003d-hQ�N�X�#_�(�[D�r�T�Ks��\u0004\u0002 Ӱ�k ���y�z\u0001�@rAWIl��~�- \u0010\b\u0004\u0002\u0001\u0005h�Z�zI��\u0016.�P\b\u0004\u0002\b�hx�\u0004��\u0016(&@ \u0010\b\u0010���f��\u0014\u0012�p�P\b\u0004\u0002\u0006�ARH�'\u000e\u001e\u0016��[$ r\u0001�@�� ��r\u000b 7\b\u0015�@ �A�>�\u0004\"�\u0001\u0004�\u0004\u0002\u0001�A\u000b lwӱ&����(4�!�|�����%Pɞ�Z�P԰�C\u001d\u0015�n\u001b�\u00074�+�\u0012\u001d��X����[��>\u0016e��G���\u0012�Z\u001a����%K:�dd�F\u0006色��pi\u0002Y\u001eN��8=��\\7~D|�5N!q��(ߞ\u0011����� ��@ K]\u0001`��A\u0018ku_�\u0003�rP*\b�(--4f#��\b��+1��u�5\u001f\u0011������� �պF8�G��s�&e�c\u001d��2�Q�\"�B T\bF�d\f1���XY\u00029��\u001c�@�� r\u0004�\u0010\u0016\b\u0018�P)cH\u0017�(\u001c�P%�\u0002�\u0003\\\u0005\u0002F\u0006�\u0012r@ \u0010 @Ƶ��\u0003�\u0010\u0016@�\u0011\u0002\u0010;�)\u001b ,\u0010\u001aB\u0002�@�st\u0003\u0005�\u0007�ih(\u0017�@ά_R\bX�\u001bY\u0004�\u0003M�=�,\u00104�\\ S� [Y\u0002�D\bZ P-�\u0002�\u0002��7��f\u001aZjh\f������\u0014�{�t��(���t�ϟ#~�w\\���S�\u0013Yl\u000b\u0004\u0005�G0\u0016�\u0004�\u0015\u0002Z�\u000b\u0004\u0005�\u0006\u000646�\u0001\u0018�P=\u0002�B.�\u0003@@� `�P\b\u0004\u00074 � ,\u0010\u0016\b1��+\u0006 H��ݡ��͉��h�\u0002y����Np�\u0016֚\u0011���\f�fY���^����9����\u000e�=��l����4�b�\u00174\u0012,UJ�^�q:�LY*�\b\u0004\u0002\u0001�@ \u0010����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010Cւl�@�\u001b�r\u0001�A\f��І�SϽ\u001f��F!��g�T�t���tv˛d�\u0011��խۍI\u000752(9\u001b\u0004\u0011K{\u000b\u0002w���Lb��(��H��/yέXy�\\�U�[d6�hw1��,��:\u0013R��!�A\u0014��G�\u0003��H\u001c�@ \u0010!\u0017\b\"�\u0005��H%\u0002�\u0002�\u0010\b\u0004\u0011��@G&����99,L���x�٣\u0018) *V����\u0014���U�>�^�#~�ۨ�r��qH�\u001a\u0016�Uc �\u0012\u0005,����� \u0010B\\\"�\u0003۹�\u0007�\u0010\b\u0004\f-��\u0006u�\u001b ��@ \u0010\b!d��!\u0004��7@�\u0002\u0001���t k��� ��@ \u0010D�\u000fo ��\u0004\u0002\u0001������Aa� �(�敘ii�����\f�w�qO���N�R��ۉN���7�|�\"u����` Bl\u0010V�u�\u000b\f7\u0001\u0003�\b\u0004\u0002 T\u0012��\\��\u0016����\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001���]��x\u001dV\u001f[\u0017[\f��3S�r�\u001e��A\u0016sA��c��w�F��xS�l���� p\f���SEK�����2J�h�fkE���\u0011�W�V��\u001c_�P�����L`m鷏5ȭ٣Re\u001469J\u0004\u0002\u0001�@ \u0010\b?����\u0001�@ \u0010Pp�dym�\u001bf���+\u0003{�nDw|}�\u001b\u0007j[j���pp$�W4�ap|P|w\u0013�IX��\f��Ĉ>\u0007�O�.\u001f�߼D��\b\u0004\u0002\u0001�@ �7�\u001f/����e\u0004�\u0004\u0002\u0001\u0005z�KzP��y���\u0007����{�t���tv˙d�9��U�܍I\u000752(9\u001b\u0004\u0011�l\u0016&pm _Ŷ�(�_p�}��v��|v�W�3��}vU\u001f�tG��\u0015��{���˥8G[�\u0006��\u0007 \u0010\b\u0004\u0014������|�\u000f@ \u0010\b\u001a�_\" \u001f������y|�\u001c�@ \u0010D߱� ��y��g��e\u0010F��\u0003_��E��S�c��^�;N��EM�1\f�»�a�߸��r\u001c�wӿz�\u0017])�Z��\u0014���;�x�\u000e\"��\u000b�7M!����`z�|���\u000e���x\u0017�����X�a�\u001a�����Q�)�\u0004{P_@ \u0010\b\"�]�\u0003 m���@ \u0010\" p�g�\u0005�\u0002\u0001��Dz�}\u0017��\u0013�ij\u0007 \u0010\b\"{5�\u0010GJ� #҂�\u0001\u0002\u0014\u0007r ��[��m�հf����ψUe�bJ\u0018��I\f���i�������{��i.s�;�m�+t��.Z�p�3.\u0016ڡE[\b��C�\u0017�\u0019\u001b`�W�����\u0019N�X�-^D��\u0019\u001b#$���1���S�A\u0004_��\u0007�YS\u001b�4 }Tl�@�T���\u0003�#oz\u0004e\\O��\u0012>VF.�`�/+��A\u0004\u000e-��K�w����{j��P��Ag�e�t\u0010��I���4\u0013���7� ��`�P@���[v���(m5��+����_ ���y��!賝Ob ��n�J�ۂ����E;�g�І�\u0012��N=~��'�\u001f\u00111)\b��2X��%[Z-��s\u0018~+�t-x*��1�T�DG�R�p�V�h���C�)[�\u001c��Ԛ$�w�G �e8�\u0011�Уk��Z��|�ZC�=\u0015*���S�>�\u0011���Lφ�\u0017pw�\u001d��/~V�\u0016�\\\u0013�,g���=X�'5N5���UFUO\u0017'��xM�Z\\*���+jh�#�2D�.�m�\u0019;�\u0003���W\u000b�,��\u0014S�����곍���MZx���z�I�5����D�kj}c�O;�`{�ҩ���\u001d��Rh��qׇ�+\u001fGƎ#�\u0010+r{����U 6� ���ݰ�ŏv�'�q\u001b��ل��S\u001e����[K��6�\u000f�ԕXyp�=5X\u0017\u001c�\\i� �.E�%�\\\u0017��QV��L��ϭ����N>dj�fb���J#���u^x:��ꘞɖ��|ݾ�BqW��\u001b�1sAEM\u001dpc�\u000e�:�-\u001aۨ�#{M�6\u0006����-l�Ҍq�i�r��9�NG*�DtS8e�>�Ӝ=�>\u001d�\u001c\u001e\u001cJ�� ��Z�رſd֞�\u0005���\u001fU֩�q�\u000e.^�j��t^8㏍�N������p\\����u{�1�\u001dh�|n�G��[M3\u00063:�\u0012�֗Xi�pZ�,w޶N5m\u001c�\u001f\u001a�\f�ܒ�Q�uMs�A6%�P�\u0011�����X����'(�Gmkܩ�z�YwH�b�jg\u00161=l� \u0010C\f�s/k ����\u0015�@ \u0010\b\u0004\u0002\u0001�@ \u0010c�Oz��O�Mc�\u001b�R�-u���b;��G�hT+7��tvB��\u001bE�\u0005B�^�9k���` \u0010\b\u0004\u0002\u0001�A����\u0001�@ \u0010so ��?M�I�D+r{|��ۭsj��[�f�#���ǹ\u0005�ؼY�:�C\u0016�c]\u0001ǟB״ݲ:���C%�.^*\u001aI�`;G�7�\u0015�\u000b�Z����?.�.�4uҚ���\u0019Z�\u000b�H�Ǩ\u001b�{��e\u0005��8u>\u001f�)�� ��յ��\u0001����`\u0003a�\u0007�\"��lߙ\u0004r����2 #����@I\u0013\b�� FSG\u001f >@�^�0����C�q~6�\u00109��]�v\u001b \u0005\u0014Mu��A>�����E�^�\u001f A#� ��\u001d�A�1x{>�\b�\u000blѷ5�(m5G�eh�0ض�w��U/s���Ǒz髭��6����^��@�f��R5Y{CE�n\u0010B�hN���>��\u0013���\u0003�Q㉆ �\u0011C������&\f��4l\u00164�4P�\u001c_���3 \u0019\u0003!7hߒ\u0007>\b��\u0007ȁ�����-\"�\u001cɜM�\b���o�� \u001c��\u0015�8�q7��fQ&\u0012l`C\u0003��o+\u001d�JN�\u001fa�\u001e]ޫ��\u0017:���x�tƸ���RΫ^-^��\\\f�v3�3-9���\"{ĂZw�׉�\u0012����F�\"�{�\u001e�Ee6]��5a�g�O4v��\u0006�*���s�m��\u0013��? |On\u001bN�F\u000e�%�J�\u0014�N�g1����ϖ3,��n\u0015[�Q���x���]\u0013u_�k�m�؎��_��\u001fD�u�D�4��c>�y������S�O\u0017\u0014roz��]\u001cs \u000e\u0003L�3\u0006%F�JLL�#h���:�\\��6#e�8_�,�+�)�uw��&>�\u001eg��&ҎX����懄�ƅ��2:K&)��X���y�;�\u0011�\b돀���+L5ϟIߕ�'Pm\u001d}\u0014�l5Q�=\u001d��ƭ{��׵\u0013\u0013�T3ME���)��?rR�ĸ�H�c����>�6�\\w��uc��7\u0006�n�����m*�g��uW�Y\u001d\u0007b�.��7�x��)����\u0019�h|�yK�@l�1�\u0010H�G̾�_\u0007��v���b(�[9�i�3k3��v{*g�\u00074@_E[\u0004�j\u000ek^ �ncn�>P�\u001c� ��'��)}V�x�ڝ2��>%B�f]�\u001bh���F�ԅ�\u0019��`t�����\u0014��^&��>�|�Z�*kD�k9�-�\u000b4уy��R\u0011\bq����rު\u001a�lW\u001c�ͱ\u001bsP�����3���m�-��cC\u0006�6@�SFM�>@�j�J\u001a�-e �g=-�\u001a�=aTU�l�|1HF�=c\u001a�v�{]t�\u001a����c9Ռ{�\\;�1��*�\b��#��K��8]5\u0005\u001c�\b�\u001br�Zw�>�su? �5Z׌�μ?\u0012���e�ٞC\u0017��r�4���y��\u0001��ɹ\u001f2\u0007263���Z�)`�NaCUM�Q\u0010؜,�\u0004̶�\u0012\u001aH�ny�ժX9���u4n\u0016�$tm�v�\b�\u0004lu�~d\u0013��E�>D\u0010�co���\u0018%kN����\u0001�q?r=�,66\u0001�|�#}4O;��\u0004\u000f \u0011��A ���7;�tp2\u0012KF�A\u001bbd�ڛ� |T��.��@������̅�y�\u0003_N���7䁂�6n\u0006�~�\u000f�K�:� ����\b���\u0005�00X ��@ \u0010\b\u0004\u0002\b���A��\u001f�fa\u001f�#\u0018�eaH6�\u0001�@ \u0010U����Ad R�&,���R��s�&�\u0012�֜a�Θ\\!�}Sq����写�K-نG\u0003~d�&���_V�'�{��ES�3�\u001e���/qҜwr=��p�\\\u001b\u0006��� �\u001c\u00111�.��փ�� \u0017�/֕Z�UUZ檦w�S/e���Y�sGd>�a`��.�\u0007\u000b����+\u0005�\u0014�_1ޢ�̃܇�\u0004�\u0004\u0002\u0004AV\u000f>O_҂�@�\u0004 �AN�Ώ�\u0005�\u0002\u0001�9|���\u0004�\u0004\u0002\u0006��Ă�=�ޯ�\u0005�\u0002\u0001���s\u001f\u0004\u0016\u0007$ �@ k� �K�Z\u000bH\u0004\u0002\u00067�()��\u0010\\�$\u000f@ \u0010T��ڂ��P\b\u0004 $\u0010�y�ւ�\u0001\u0002\u0015�Q��+0��S��c�\u0019�����q.�~�\u001b��\u0012����o���D�wi� ��@�\u001b\u0005 ��\f¼��[�U�� N�(��� \b K8��\"���AN�ܾ4\u0017\u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�\u001f�{�O�}�k\u001d�� ���k�\u0016�@G}߈�� �b�=\u001d��wl��Q�Z\u000eYd�\u0002\u0001�@ \u0010\b\u0004\u001f����\u0001�@ \u0010y��}\u001b��0�Ŋ�U6\u0019�S1��_G%D\u0015-�u��٩�xc��]��M���\u000f���\u0006�\u000e `��p�\u001a�Q ��@��n��t�9ΐ���w=�.q7$��g���2V9�{�h- \u0010\b\u0004\u0002 �\u001b\u0005�(mu�z�F-��1�\u0013=3��KN��V��x!���&'\u001b� @#]S�[�i�I\u001a���y��w�\u001aќ(�(�{)�]���k�����\u001f��KO\f\f\u0002֊6�w\u000f�\u0003�|�P��U�g3;�g�ҋ(�U���{��S]X�EU֚��(���Sj,��$��\u0001�Y����̵��Sg�\u001dK����A\u0016|��\u0016q 4�\\��1Uz$���e��䂛])�b�\u001aiLM ��u4F\f�S\u0010,;x{SI�JHAӺ�c\u0016*����\u0005�Q�M\"��SCc�XфzD��4`�-�F &2�\b��Z $\u0013���\u0012��\u001eV+��5�}�݇�P�Q5q���8�h�Eɺ���c����Ru��ԳDO\u0012\u0018�j�\u0011�����\u001c��*�\u001azN�\u001c��\\�N)N*����[KH#��Zn-�)���\u0015e�\u0013\u0018OJ���5S˯��ixYOU�j�dʲ�\u0007U5��uN��.�4�ܛ�k�bW�l�r{��t�tpǢc��l�\u00136��\u001f�{�5�\u0015���.ã�\u000b���I�\u001c�ݱI\u001bM�$�o�\u0003{mᪿU3����{Oo�F���\u001a�,p����K�S\u0001�gS��\\-k\u000e�B\u001aE�˵m�Vt,m#�'�8��ɢһzh*\u001f\u000b�����湧�\u0016��\u001d��]\u000b)�g\u0019���Xy��yk\u001d�\u0013����j掚�x�d�W��)��p�H�\u0001�\u001f���^4b�Q�e� ��e�x�\u000e7;meřu��\u0004�XHT\bQ�.6Q�딐�����*s���Wj�����ϋ.E�3��Q�;@��6�҅ ���-�R�M�ë�Mu���)S��\u0004\u00115fQR�g\u0016��s��A���)T�\u0018�\u000f�k��]��b���D�Z��q�׍�s\u0018$��(H�ܨ\u0012�\u0014��\u0005G�8\u001e��A}�@ ,�\u000e�\u0004\u0010�J �\u0001\u0002�\u0010\b\u0004\u0010˻\u001d�(\u001bL��c`��\u0001�@ \u0010\b\u0004\u0002\b���A��\u001f�fa\u001f�#\u0018�eaH6�\u0001�@ Gr()B�6B��@�\"h[#�\u001c�Z�]�W�ZJ�\bh.7a\u0011�6�T.`~��d\u0011pt�T�\u001bx/U�6�Ʃ��&=�y��EYn}�\b�v��S\u000ev�O��\b�$��l,� ��wT/�sM����\\���r�>&�a:��+�VN�3��ޙŹ�Y��\u0012Qv8z ������\u0005�\u0002\u0001\u0002\u0014\u0014�c��|P]@ \u00104�PQ�c�,dr\u000ei��d\u0010\b\u0004\u0002\u0006J.\u0018�\fAa��\u0005@ B\u0010Tc\\%\u0005\u0005�\u0002\u0001��CK�l�X�\u0003t\u0012 \u0010\b �\u0012ݐ\u00107CPN�@\u0014\u0011�M \u001b�\u0002���AZ��ֲ \"\u00044]\u0004�\u0004\u0002\bf\u0004��6���7�Aa� �(�敘ii�����\f�w�qO���N�R��ۉN���7�|�\"u����` �_5GiN�`֩��X�?�ёMS[\u0015;�\u0003ƹC �F�p�=��[��M1�F=\b\"�\u0016s ��\u0011�M\u0014͕����\u0010E�\u0004F�c�Ҵ����c\u0005��\u0016CN� g\u0006��[�\b\u0004\u0014h���b����9�@ \u0010\b\u0004\u0002\u0001�@ \u0010c�Oz��O�Mc�\u001b�V�-y���\b��\u001f��P�^6���\u0014��9*2�A�,���@ \u0010\b\u0004\u0002\u0001���\u001a\u001e��q\u000f\u0004�\u0003�,/��A/�2č�=>�\u0014\u0001�h�\u0001;\u0013a�n�_���7��\u000e�\u0017��\u001e�\u000f@ �̹@�\u001b���\u0006��w�+\u001c��_x�\u0007��i�%�����z\u0001�@ \u0010\b\u0004\u001022�\u0013�PN�@ \u0010\b+�.\u0016������GƖ����\u0012��q.����s,��>p\\J���� �E\u0007#`�)y(m+�mKGq�5�vQ�[S+auE,�J����\u0001 ��Ǖר�K�w�Jtbg �fp��TrD����(���}�\u000f�\u0005\u000e9��=,���X�\u001c�\u000b����B��wZ�-�J0�2���U����.~8}ۧ\u0011�\u0013�\\kL�wb�O\u0015,��ֳMX��)C���\u0005&�\f �8�\u0005�t�u���j��\u0007\u001b2�u>;�M\u0016)S\b��b�DRL�\u000b�@��Z4��6��z[�ŵ\u00138lS�F\u0013�8��sL��z����e;bt�HuyΑ���s���e櫼EU��a����w,,�c_+?�\u0005�\f��N���遉K�L\u0019`1����D�\u0006H\u000e���\u0004����\\�Ҩ���9���9���s76ؖ\u001fOPv�I\u0018s��۶�����j�����!����X�\u00053>\u00139~Z�\u0012�4^�H�&�đ�I$�4��>ou�����\b��y�|��5]-8��\b�\u001e`�\u00126�2a524_UU#L�Y��9��@�\u0002F�bA\u000e�,/��T^�)�枾)��\u00157���i��\u000e��\u001a��(�8��%-�d{Z\u0003�Z.\u0006�\u000b�sk��mTUT�e���]�b���(} \u0006�@�r\u0001\u0002\u0014\fA �ΐ;�\u000f�AcV�@�\u0002\u0001�\u000f$\u0011D�۟\u0014\u0013s@ \u0010\b\u0003�\b��L#� !ic($@��\u0014\b�\u0003.N�nOr\b៮\u001ek����>)Y;C���D\u001b�� ��@ ��޴\u001a�\u0011��f\u0011��1��V\u0014�n \u0010\b\u0004\u0002\u0001\u0005vH�=�w ����r\u0006��a\u001c���&\u0018���a�8�\f8�\u0006��5�o��5g��^�V������l�e�\u0016�� ���y\u001d�f!W$ \u001e�`k���{\u0011kr�ފ�}�8��X�w�2ͅz9=.,6^j��l�\u001e�u��JI� �k�${�\u001aI�\u0005�2)\u001a�\u0007\u000eH$\u0006�\u0015�A\u0013$k�rA*\u0001�7\u001b��A\u0014��2ۃ�|h,�\u0010\b\u0004\f���$�dO\u000en�&\b\u0004\u0002\u0004��A\u000bfk�Z\u0006�\u0004�\u0004\u0002\u0001\u0004RH�ȸ��p\"�\u001c�@ k�h�@ȥ\u0012��J�@ �nPF%\u0005�A'}�H�@ �I[\u0019�h%\u0006�\u0002�K��g\u0003K�[��k��3@��tX��,26V�-R%@ B��7��f\u001aZjy�,C7\u001d��S��%ӯԣ��S�>|��\u001f5ȝn�:�:�\b!��� {m�[R�ҝ(��ΑYR�L$bO2 #�6]�JӤ���x\u001cܽ\u0017\u0005��*�y���Ȯ&��L��R��\b�)�llk@���5���\\y\u0001��\u0012��m'\u0019t,c\u0017�5P�9[�z�\b\u0004\u0010�+ff��\u0004\u0013 \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001\u0006?\u0014���\u0004�\u0014�;Q�\u0015o;2ל-�����\u0011�Z\u0015 ��jz;!N��㒣.�\u001c���\f\u0004\u0002\u0001�@ \u0010\b?���K\"4�W9��\u001dV;X_{�W�\u0006���~G��?Q��i\"���\u00067TS�\u0016F^�r�v@$�v��\u0006ņ��\u001c�9��x����%�p--؍��p(>�7��\u000e�\u0017��\u001e�\u000f@ \u0010\b5�\u0013�IX��\f��Ĉ>\u0007�O�.\u001f�߼D��\b\u0004\u0002\u0001�@ �\u001c�q\u001e�\u0013�T\u0002\u0001�\u0019y-� ���z?@�C�����\u001a�_5���6��#�\u0005ū[�\u001a�\u000ejdPr6\b!���=ۭj��kUZ0�GN\u0003]>j�aiu1���\u0001��N\u000e�\u001e��O@�E6s���C��-\\�tG�7F�&O�p���\u0003���I�q4�t�i-��--\u0003�E�\\� �^{�M�Z�N~��E���D����:Be\\�έ�y,�\u001b�V�b�k^��\u001cI�q�gxo�+����)Ǔ\u0019��{?u�G�7\u0005\u0006=M�0: Y �v�����Tm.�N����_1�f9\u0003�:�\u001c5:vv�G\u001b�C\u0016�D�X���\u0011�G2����#5Z,�^x�ƞU��S��5zn ���ia~;��Sg9��9�/�X�3�G�2Mass�ٳ��\u0002E4f�D�刢#45�\bG��i\u001a��l�>_;@���?�ں�\u001f���.O l�� �I�/�\u0014xb�z�Z��mKV;\u0014��lEUl \u0010D�ا�GN��4G\u001d�]\u000fݭ�cλ7 ���8�����\u000b�S�e�*�(@ B��+�}\u0013\u0006x��Ak��9�@ Gr(+�.�\u0011���\u0001�@��\u0006Ht�� ��ڂd\fy�>�\u001aǌ�Bg\u000b�v-�\u0016\u0019\u001dAESV\u0018\u000bAq� %\u0017\u0002\u0001:-���\u000eS���W��.�u3>c���3�>�Z��U��g�1�\u0011�s�� ��\u001b�Ū\u001e��r9�k % Yi\b�=��\u0004�\u0011�T��=H,��\u0007 \u0010\" �y�z��\u0016�\u0005@ ���1�~t_ �҃(�@ \u0010AQ�\u0014\u0011�y�-\u000eH\u0015��d��>�w��A�@ \u0010\b)�y���i\u0004���%@ \u0010E/�� ?4��\\@ \u00101�qAM���� z\u0001��W��\u0016��\u000f@�n���\\۩1E4�&�\u0015�u����\u0013���kEX��0\\R#\b\u0010����iY�����\u001f����\u0014�{�t��(�����ϟ#G�r'[�N�N�\u0002\u0006���3\u000f?t��)K����X��\u0001����\\kן������W\u001fK�v�԰�\u0005�\u001aԥR�\b\u0004\u0014p�p\u001f\u001f�\u0005�� �@ \u0010\b\u0004\u0002\u0001�@ \u0010\b1���d�'ئ�ڍ�yٖ��o�\u0004w����ШV/\u001bS�� wv�\u001c�\u0019u��NX` \u0010\b\u0004\u0002\u0001�A���\u001fk�p^\u0016暺W��O�b201�y��I\u000b@i\u0006�\u0011p\u000f>��\u001cU�\u001c\u0018��#�p�\u0012����ц���UB���o��\u00172~�SL[,�D����Ů���\u000e��6�� �\u0006��풊��\u0012�� v� ���!���*p\u001b)�X��\u001b\f�U\u0018�\f�J�zI\u001bi�Dv\u000b\u001d\u001a�%r�/5n����Ա����2Sŵ���\\���]H�S+$Y�-+�$�S�0����Eie\\���1�\u0016R�)��=����v����\u0011�*�p��8�I�f\u0002\u0001�)E�\biZZ]� ��@ \u0010\b*ַ\\.\u001e�j\u0007Ӌ0 �����z\u0001�AJ���d\u0016b\u0016j \u0010\b\u0004\u0002\u0004((\u0016\u0011 #�\u0006A�@�00X �S������V�J9�И�V6���\u0019u\u000b�[k�P�뚪ǟ�E�S�g1��4\u0007p�v�c\u001a[`���ssj����n[[Ӻ��'k4�D��\u001fb��y\u000e\u000b���;�\u0004\\���s�ZgR��^g\u000f\"FL�g���9Hq{[m\u001a6s��f�˧ E����\u0018sq��:�z;��C:u�p�\u001cq������h�\u001fX� mS\u00058~�l\u0018\u001e���9�[��\u0015���)Μf9��1v�G[�Q�\"����ۮ\u001dT���X {��[\u0005\u0015\u0016�� ��G��f5��l�>_;@���?�ں�\u001f���.O l�� �I�/�\u0014xb�z�Z��mKV;\u0014��lEUl \u0010D�ا�GN��4G\u001d�]\u000fݭ�cλ7 ���8�����\u000b�S�e�*�(@ B��1Ұ����G�\u0006L�P9�@ C�\u0005\b\u0018D�(/� �@ Gr(+�\u0002a>6@�m�\u0018�4\u0016�D@\u001a��ے K�̙�gܗ�a�����PU@�d�F�̐H��8���$�\u001c\u0017���q\"��%����\u001c\u0017\u0018�%�ؼl������k`\u0010����:6��F�s$�k���6Cʰ�\u0016\u001c.\u0010\u0004p\u0017�\u0001�v�y�ܻ{���\u000f�@ \u0010\b#ww�\u0006�����p�c��� ۈ\u0004\u0002\u0001�(*G\u0019l���Ad\u000b r\u0006�\u0004ww� ���G�p\u001f�����]+����9��\u001f\u0016�FY\u0014\u0018�6\u0014�\u0017$�\u001a��.�\u000f�KnG�e%�QTM3��WG Ž#u�\u00172aֳ�ܖ�'\u0003t ���G�!\u0018G\u0004F6ؔ\u0013 �*\u0001\u0002\u0013d\u00101���\u0014\u0016\u0002\u0001��\u0016��V����\u0011ɤ\u001f�\u0005�\u0002\u0001�9[�� �&\u0016�\u0004�`�P\b\u0010�\u0015�\u0006�5x�Ae�@ �h��z\u0010H��\u0016@�\u0002\u0001\u0004r��X\u0014 �.�Y\u0004�\u0004\u0001A\u001a\u0006\u00064\u001b��}�H�@ �hz�\u0011܂F4�P=\u0002#\u0006��i-�H���g����`H\u001a#e�\u0003ǚ�S�i�~�4���M�D�\u0010����iY�����\u001f����\u0014�{�t��(�����ϟ#G�r'[�N�N�\u0002\u0006���3\u000f?t��)K����X��\u0001����\\kן������W\u001fK�v�԰�\u0005�\u001aԥR�\b\u0004\u0015i�03I��7(\u001c�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b1���d�'ئ�ڍ�yٖ��o�\u0004w����ШV/\u001bS�� wv�\u001c�\u0019u��NX` \u0010\b\u0004\u0002\u0001�A���w\u00131�e����� �ð���i�_\u0014\u000fqa\u001a�p�4s\u00039��s#$t\"��Ix)3�l��-f)4�\u00154�N�D*]3�P�>)�\u001afk�锋\u001d���\u000e��� `�,˴x\u0016\u0011\u001b�IE\b�=o/}�{�\u0017sq$�`\u0005� ,\u0010}�gSA���\u0004\u0002\u0001\u0006��w�+\u001c��_x�\u0007��i�%�����z\u0001�@ \u0010\b\u0004\u0011�4:��\u001f̠T\u0002\u0001� ��ޔ6��}���p�B�l��T�t���tv˛d�\u0019��U�ۍG��hG\u00105��g\u0006�Q�\u0003�agYw{���;\u000b_�࣊�g\u0006�b�c{�Z��\u0019�Hx �aü�KM,LmLm��s${�nw|���d�\u001b�AI��(�q��^\u0015]���i\u0016�F\u001cQ\u001d�ؗ ��/�^\u0002�t�17�j�t�4���\u0006�L1T4W\u001d�]\u000fݭ�cλ7\u001d���8������B���YjJ�J\u0010\b\u0010�b\u0006\u00073]�����I���z\u0001�@\u0014\u0011��$�\u0003�,\u00109�@ k�ߐ@0�G$\u000eA\u0004��l �A��D嚼���\f��uEE^\u0019[M\u0013Z�3S妕�m��4]�\u0002�!��&װx;+�[�FXâ��rf\u001bP�\u0018�;��=n�)-ӣ\u0014��\u0007�y��\u001egr\u001d?ʰ���D���\u001c�V���\u000e�#{��ǙA�H\u0004\u0002\u0001\u0004n�������30��\u0011�2��\u001bq�@ \u0010\u0005\u0005H�.��\u000eh,�t\u000e@�`����\u0012�EEk\u0018L����\u0007p�̯�G���F\fc�$v��� Y\u0011�)��!\u0004�t �@�[�\u0018��\u0003%� \u0007�m4�ƒ��՜J��\fv�\u0012 ��\u0018��98�򠸀@ MA\u0018�\u001c� i+\u0018��\u0014�j�\u0006S�p�\u0015� A]��I��Ae�@ �iLdzVC�%��M8Q��Ÿ�E�6�\u001c���M�\u0004 �@ \u0010k�'~����\u0019U��\u0010|\u000fF��\\? �x�\u0007�\u0010\b\u0004\u0002\u0001�AL{���Ah r\u0001�A^��ޔ6��z���\u0001�\u001f�%?��]+��\u001d��Y=\u000e|ตkw#Q\\.B�j�\u0006��'\u000e���8Q����%�p\u0007e�\u0004����P��g2�]]�#�\\5ć\u0012�n.։0�h�e\u0003�,\\�4\u0019\\cu�Kd��t��ߵ{�Sz��e\u0011�)��c�1��\u000e�G�L�{M�œ�(c��~������^�O�O�J[������\u0010� ��� ��/��4v/]�>�T�R䞶\u0002\u0006��ZW�DҔl���\u001b��4�Ӏo�\u0015��\u001f�7���\u0019�i�y*poQ�K�P����׵��ݵ.���P\b\u0004\bPR��\u001f\u0004\u0017B\u0005@ \u0010\b\u001a\u0010V���Z\u000bh\u0004\u0002\u0001��g�������h� z\u0001�@ �W��\b-}�ā�\u0004\u0002\u0001��=��WЂ�\u0001�@ \u0010U���Y��\u0016J\u0004ZV�\u0011Y4l�5���\u0010\u0018\u0005�� v�ķ��Z��I��!�n�j���i͢�F���\b#��K]��f�\u001b\u0016�p\u0017cR\u0017n���h՜tˋkv�s��C��uMx�1h䤯`p7`s\u001d�Y�to����j���r/���f;��p�\u001f�Et�Ƭ�ۃ5=�ar� \u0002A; �sw�.���:\u0015�G?d��)���\u000f��To�&`���(���Yu\u0007 L��\u0007�g�YvL�S��k�*I\u001b�.���[)\u0014�,�ENI\u0001��\u0005\u0018}��WЃ\"�@ \u0010R�=������\u0016�ӊV�ST�V(+�H��&Ji��X��\u0012��F\u0011K栂��>�\u0017\u0010\b\u0004\fo�PSg� ��H\u001e�@��1-\u0015�\u000e4el��6�\u0012��p\f��\u0014��,��nm][+��N>z���;\u0015`�\u0019_5a��������Շ:+��$0�ȍ�w�}�\u001a3�#�E��>�C ��\u0011��5�i[IK\u000f_�c�w\u000f�e=���V�j�i>����t\u001aE�����txS�:!G��zg��\"��\u0010!Ye\u001b�ҳ -5F����N�v�P�l\u0004 w%�f\u001e~�\u0015�R��}\u0017��]�\u0003�I�=��ׯ?e���W\u0012�>�B���a�\u000b>5�J�j\u0010\b(���>?j\u000b��\u0015�@ \u0010\b\u0004\u0002\u0001�@ \u0010c�Oz��O�Mc�\u001b�V�-y���\b��\u001f��P�^6���\u0014��9*2�A�,���@ \u0010\b\u0004\u0002\u0001����GHH+&�id\u0012Zg`��V\u001a\u000bA�I \u0001ݭ�;�v�lm��xǣ�\u0013xMG��%�t=u\u000e\u001f;18�K#�郧\u0012k��sd%�yc��E����\u0007�:\u0018�EU�x���\u0010�TW�Q8Z���k�\u0004�-\u0019`��\u001bim�\u0003�\f�F��\u0003�\b!�V�58�z�\b�l�\u001c\u000e����}�>\u001f�ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010U =f�\u0016B\u0005@ \u0010\b �\\-�Ci��ޏ� �1\u000f�%?��]+��\u001d��Y=\u0006|ตkw#Pq�ZS�!��HF�Y��*�6��fg��0�|:�\u0003>)?�4�A`��a��\u0010\u0003�Kt܎~���k\u001f7dl#8ҚJ���t��\f/\u001aMƒ���� \u001d�ay�������@\u0017Y\u0003\\ooZ O��C3\b��\u0018��+ A�\u0010\b\u0004\u0002\u0001��\"k^�/� ���\u001c���\u001d��H%�x���\u0007���0+\u0017J�w{Ne�j=�������E}��Q��4�\f�іs\u001e?F \"��b\u0003U��L��\u0005��:�`w�~�ռF4��\u001eHsl*®��\u0005r�\u001d�p����\u0007�=���Ya�c�F���\u0003��\u0005@ k�ֵS��\b�\u0018m�\u0015G�޺\u001c)ꓺ;\u0014x;g�|�Ai\\�\\�\bVYF�4��KMO?��f�ۊ�ĺu��o��Jv�ϑ���\u0013�ݧT'[\u0001\u0003]�f\u0019���E~����E��Ws��RwOm.5���oh|�ī��лj�Xj�ύjR�Z�\u0002\b��B�#��\u0004�\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�A��=�'�>�5��o�[�̵�\u000b�#���օB�xڞ��S��x�˭\u0007,�r�\u0001�@ \u0010\b\u0004\u0002\u000f����\u0010s\u001d6F���1?n:*J��5ΰsb��\u0016�\u0007Y�2���J\u000e[�¡����q%�?�u^�W���\u0015�\u0018���e\u000f�ҝqF�\u0019�5�\u0019\u000e��P\b=�џ3Ü2\u0006\u001bT�8aR���M�4]����%,�5������p\u001a7�\u0017��z%��\u0014\b絖�\u0002�n�u ���\u001d �Rb�#�4�k�3f\u001cB����OI;�Dd�7IfS��i�\u0002�aw\u0006���ht�d�� -~=�jr��=�\u0014�n{���0�@�\u0018 ���}$\u0012\u000e�p��8�#NJ�lF�mX>� ~�\u0007�tj\u0004d�o��\u0011��\"A��\b\u0004\u0002\u0001�A\u000fYwi�\u0007��P=�@ �^KjP�jy���\u001fR1\u000f�#?��k�|�\u001b��\\�'���\u0017\u0012�n�j5�pS��\\N*�u�Q1�M#cc\u001a\\�=��\u0006��I�ܞ@n�Y�9&����y�&:�8�g�eR\u0006\u001f\u0017����Z�i����صw�����\u0002\u0015:#\b^H�\u0002\u0006H�M#��ҽB�Xn\u0003�}#bw=�jr���������0�A��!O�a$\u0013�`�P\b\u0004\u0002\b�ے\u00029:ˠ��@ \u0010G+�4� Өn�Z4��\u0004\u0002\u0001\u0004\u000f�@\u0010H�\u0013~�\u0004\u000f@ \u0010\b\u0010��P/��Aa�@ \u0010\b �P�\u0011�@�6���\u0003{,��\u0018.\u001d�8!������\u0014uS\u0015F\u000bTU��\\�\u0007m��)6c$Uӌ���?�\u0014�K����:\u0019)Z洵�q\"I#��-c�į_�w�j�Ն\u001bS�1��y\u000e\u001e�wXǛ�=t0]\u001f�\u000f��\u0019`KY\fu�_\u001d�F�\u0005���S3\u0010�\u001bH��cVY����7a9�\u0002�����ʢ��{]029̊=N!�U��s����|���6�+Z�i�\u001d*��2ڞN'��6�EVq�Tc�q��=\u001f\u001c���8\u001d@~�Ҽ��0�ŵ3�ag[|V�bq'Z����M8�o�%s��r@[bԺ���f*�T�G��u�uʁ���c\u001c\u0019�1�\u0004\u001d���Ҩ�\u0006�� '������q.�s�q��r���K\u001c2�F�f�r�[�L�u�G$\fy\u0016Y�a���G�\u0012��}\u0017��]�\u0003�I�=��ׯ?e���W\u0012�>�B���a�\u000b>5�H�j. \u0002�)�=�6�\u0005�]�z\u0010=�@ @A@\\ T\u0002\u0001\u0002\\ T p�Pc�Czi> �)�v�|*�ve�8Z?�G}݈�� �b�=\u001d��wm\u0001�Q�V ���\f\u0004\u0002\u0001�@ \u0010\b?���7\u00142��9/\u001e�$V�J���H��նx]\u001bH\u000f }�\"Z.��:\u0018�h�\u001e[�~����\u0002��1\u0016۳\u001bˍ�F���w.�ƹ�O���ͷ�u>�%�a�0ZJ\u0006 ㅍ{�\u0001.\u0003�M�ܒy�Y湗��˖ZXY�=O���.\u000f$X\u0003�^�B��z�ɠ���8v\u001dW�a�T�����;ëgJ�\u001bn�\u001c/˟�]+�\u0013������>��x�-�aMmu\f�\u000fc�4ͫQ�\u0011}Mg=[mܷ�᎖�x��*�D��v�@�O?�r%اR\u0016�k�ߏʌ-�SYm[�>�\u0014�U.u�\u001b_��h/6'��~?* 50ɭ�ߟ�,�\u00182\fk��a�>�����PM\u000bei흭���%�Wy�?�ƀ��H�����\u0015\u0003g�W�\u0016;o\u000fĠ�\u0014r6���vAbVL�ӷ��@�2p�I���7AbF9��\u0010�\u0004��y�~4\u0012�'���\u001f�\u0005H�`v�w���\u0003):�#컑���@��R>���P[lO-\u0017v��{�A$\u0013\u001eO?�Ƃx�{\u0007iߏʂ)Y3�KNߏ�\u0005i�3\u0018K�ߏ�\u0003�%{\u0006�x~��R�>�W�>4�XeE;��\u001fe��x�?*�+i�l�����n���A+��~?*\u0018\u001d\fE����r���:ئlntcS��>��[�U�mo\u0016qTjx7��\u0001i�\u0006\"1�*e���X�xc[ɐ?���so3o��D�\u000e\u0012���XES�3���θV�8��� �:.����!����h�\u001b E\u0018�)_��.s���_����\u0015��ST�\u0014�L��8�V3�;\u0017n\b��3�Q�� ����Øta�?V�,tU���\u0016m�#�l,;g�u\u0010@���[Y��E;���\u0007F��V\\r�a�l����a�*0�\u0011�n���4 ����р\b�^\u0005�7�\u0004��E\u0015��� �*�'�\u001eşw\u0018���}�j�;@a\u001c�:�p��ݣ��kU�\u0010����)%�{9�`u�!�\u0004^��q��PYӣ.��\u0013\u000f7�q޾?ǚ\f�A�\u001d�\u0015�7�\u0007\u001d>\u0016�!���|�����Z�\u0011ER$6� %�B9 � @�n]�.�;a���/�H!��x\u001f��\b.9���%���c�\u001a9���ifZ��Okj�\u0013��\u001f���];�g;��.ݴ#�\u0005ڎ�m�uϪ0t쳆��\u0016Ȥ���\u0012� ���6�6��\b�I�i'm�v]\u000b�X�G\u001c�]R�{�\u001bN�� ��;\u001d��Ťr!��庣x�i��7Y�zY��1���F�S�R��q��\u0007\u0013���\u0015��T�i��� ��.�H.�k� \u0002�7\u001f8��B\u0007�H� #��ڤ�3bZS$J\u001d���6����B�Z�ӻ�\u000e\u001d[M�\u001b�l\u0004��M�X;\u0016\u0014d��\u000b�QiiBY�E8p���N\u0010�3�9k\u0001\u000e\u00167�����,��\u0004���\u001f\b�gI\u001c��ɋ�o�����\u0005$�\u001a.\"{\\�\u001eD�h��G%�x>�\\Y�1La\u0014�18N�9xk��\"�\u001cg��\u0006���m4\u0005�$��w7�k��)���}(x��f�.8N\u001b=\u0015\u001d�{��;m2�A-,�M���\u0015Wk�>�WW���b���u�\u001c���9{\u001d�ѱ�n�zG�J���\u0011�y��so4��U>������f|�V�i�eF�jl/\f�uQ�\u001dO��\u0001m��}��ϡ�n��Os�',��1�\u001a�ƗϸN�ŝQ�\u0013����^���|�.V�tM�-ɈҘ�!&��ө�\u0011��{��\u000b�ވn�\u0016��a\\EX�8�wk��5�҈�c�9y^��㱌\u000f��Mm\u0016��Տ\u0016�ڟ���^>x2*�i�*���{9�9�]=����My�W������>YX�O��\u0017w��o\u0019���jW�\u0017D)\u0004���:ϲ��t����Э�M�Ӻw����uÛ�\\#M�:��yg��z-V�I�C�\u0019���)��2\u001e��N��Q��{Z���-�u��Gc]��}\u0018G}��8���ȯ��#\u0016���\u0015 �o�\u0002��FS;�\u001b����ձ� �>\u0007�U���r��q�\f\u0017*Tj�c�`=h&�A\u0018� \u0014U\"�; �,����� ��x\u001f��\b%t��\u0004����E\u0015S\u000b� >��\b-�F���u=�Y\u0005�������SmTn>q���\u0013�������\b+�������Ay�k\u0002 ²��AbI\u0004m�\b\u0019V\u001ew\b+�h���ݓ�S��F�U���\\p�x� ����#��¡Z�S:S�� wv�c��q�\\�u���NX` \u0010\b\u0004\u0002\u0001�A���gI�k6��,�%�3�H�.�\u001c\u0016,шP�9N:ZjX➨}r\\=�2��&�\u0013�i`��GXen�\u001f�2��:m�A�P\b\u0017\u001d\u0003g����\u001f6� �\u000b�v\u0016ii�?\bŵuq1�ګ\u000b���g�A��\u0004r#��\u0003�\u0016�v�\u0002\u0001�5�����Bƞ` A\u001b[�\u0004 X�s\b!m>��3�J�[!\u0012�Տ\u0012�\u000b�\u0007���K�o�N�$9T�5t�}g\u0006��\u0014sw�]>});�\\��Ј�C��7�6\u001b)貚}��ւ�j�|�\u001a�0�K&�J���\\\u0013�� {�A$46�I\u00176\u0001v,8F��\u0018QT�\\^r�Z\\i��6���>\u0011��&Zò�\fT406(a\u001aXƂCE���W ��\u0016��L�339ϱ��X\\,�\b�^��h�\u0001i���m�w�zU�\u001bz錦ct�,UaD�C�qN\u0014d�Y�g�h�.\u000e�3�۸\u000f\u0015j�\u0010������ǵF��\u001cQ\u001d\u000f����L��S:�ߪ���/^������gµr�\u001c�{J�{�\u0015y�o.t��N#��ȝ�����U\u0018t\u000e}L�4��'T�\u0012\u001a��{s������\u0014�[�1\u0011�5q��S��#��S�-N5�j��Xm�1��V|{�]��:\u000f \u0003N�:6�(��J#\u000e�o�o�֝C�w|�mT�ކ/\u0010\f4�n������\u001d*��]M-�~6\u000e\u001ePX\u000f:���W�S�:#�K��zg��\u0019c]�.C�A\u001b\u0007 \u0010CP\u001al\b����\u000bh�P�8L5�\u0012g\u0018nX�fc��y$�� \u0011~d_`7'ne^��/M����%W�����e�l6��D\u000e-\u0007�A\u0010�!�h@�!�䁽H�|�\u0002�\u0010�m�@�d\fgp�\u0002 \u000b\u001a{�4B�o`�Š� �Ӷ�\u000f� ��\u001e��\u0003\f�|�\u0004���\b\u001b�3� qh��U�s4�Ϧ�)��q��\u0018ˇRI ���#҂Ő*\u0001�A\f�jP��y����\b�?���ꑮ����sl���i��Ü��� \u001b�,�� \\�LH�\u0015q�SH���#��\u0003�GT_{Z�r���\\\u001fa��W�cT��\u0013����~�8��ͱ2�_�,�t�\u0010�DQ5�!��]��M��ܓrw+�{��q�Z�ө��'`V��u09��TQ�\fI$:T�8$�\u0013�7hQMF\u0018\u001e�\u001c�` \u0010\b+��%�,Z�\u0004\u0002\u0001�()��\\��Ap \u0010\b\u0004\u0002\u0006�`�8���&@ \u0010\b\u0004\u0011Nl�P,F�@�,�P\b\u0004\u0002 �;B ��@�\u0002\u0001�\u000e�\u0005S1�\u0007�\u0005�\u0002\u0001�@ �\u001b��=%\u0005� \u0010\b\u0018�K0ҽJ��wƣ�i-6z�3�?�L��\u001f��][��GOd���\fU�^�_�>5KV\u001b\u0014��}�\u0010\b\u0019e��,D`\u001cl��\u0018��\u001a/���\u0017C�h�cκWJq���sm�n��Ys�\u0017��0����\f\u0011��cC\u0016�k�07&�\u001e M\u0013 ;�L\u0018������v�(b�*|v`��vf�1VC\u0014���`�6�\u001c�m�}�[��;��8ӌg\u0018�u[�c\u001d \u0005�9\u001fJ��@�H�\u0019�\u0011����k\u001a\u0007p\u001a ��B�\u0018�W,y����9ލ���˳Qev�8�\u0002V�Q�Iֲ�'\u0006�\u0015�\u0019�knH\u0003Ҷ�\u001d�P��\u000eqj:�ۘ\u0019\u001dK\u001e��F�M6��V��?����uE4�:�'�\u001cx��-q��\u001b�\u0013�鰸\u001dQQ; �0K���\u0004\u000bw��=~�̋=)tf�\b�h�s�6\u000fG3�à�\u0011����R�9\u000b��6��\u0012�&�5��eس����f#ϡ����L㄰����B\u001du,����� �\u00130k��\u0006��\u0011������]���$������]��Ǟ�c\b�P�7Ǳ\u0019�9.׸Lfln\"���\u0016�\u0003��G�oe\u001d\u001c!6;9rj��ӛ�v���9q���\u00045�\u001c�p�X�,R�\u000e�\u0018X��&2\u0007\u0017\u0006���4\u0007X\u0003�^�����Ϋ:�g\b�q�95j��x�~��\"c\u000e.){�.�,�A\u001d%3Cc���_�\\y��W�o׺��j�;0�{��\u0005Ōa�\u001f+�#���޵ɳ���t벊\u000fx�9��ީ�}�(�+nѺ�8K\u0015U�9�,6X����s��\u0012\u001br�� ���!)���F\u0010�27�Yцt*�\u001b���\u001a\u0015r�ɤب�(kMxN\u0012���u����\u0015d�5DBm��2�-��ң��%&�t�V�LwA���Mq ��P��\u0019n��+�kU����\u0017\u000b����\u0004�Lq����X҆��g� �m(��5����\u001a\u0018l����mg���\u0013+\"��\f�a�b\f/c\f f�\u0012\u001f��\u0016\u0004s�5����m0��'����\u000e\u0007 �\u0013N�s�+�Y\u0003\u0001��$X�\u001f��a\u0015�P��\u00172F �M`��A�Zמ�\u001e\u001ad�b�c�ש��Ѫ�:j�\b��)�fb2�����=RwGb�\u0007l�ϑ�5�u�\fV-Y\u0016\u001f\u0003�F����u�\u001d���skX\u0002|�V�1N��\u0013��\u0019�3��w*�\b�j��:���g�V�\fZ\\����\u0018Y� ��$A��\u0005�\u0013�K\u001bv|��1��XE�\u0011kF\u0018�:Q�3���K7Apc��0�j\u0007d\u0005-TU��b{$��c�.�ix�k�7I\u001c�\u0004\u001b\u001b�\u0019\u0016���P\b\u0004\u0002 {���V9��*��\"\u000f����K��7�\u0011 �\u0002\u0001�@ \u0010\b*E�ւ�\u0001�@ �S�)hP�kh.�?�\u001f\u0010��S��%Կ�G�\u001d��c1�sb��XU\u0014�s�68cs�I\u0002��睇wy\u000b�v�����կUe��\b,�\u0010\b\u0004\u0002\u0004()K?��\u000b�\u0002�\u0010\b\u0004\u0011�����������@ \u0010\b+�{����%\u001eh� z\u0001�@ �W��\b-}�ā�\u0004\u0002\u0001\u0002w���ww��Ai�@ \u0010\b*����J\u000bH\u0004\u0002\u0006?��4�R����ZKM����o�\u00117����W[��R:{%��-�>XP�_�7\u0005����\fU�^�^���j�b��� ����\u0002\u0006,T�\u001c��Z��\u001cu��t?v���:��v�D�\u001c{�{E�B�����IV\u0012\u0012��\u0004QL���F�H\u0014�\u001e\u001a��[k�\u0003�\u0003�*f� b9g�\u0013g���'��d\u0011R���ܴJ%q����x�B�V\u0016��\u0013N1�\u0017�:����$���vw�i�\b��|��-63���y\u001e�˴5�f\u001c�k�h�KU\u001b��i�#���\u0010��˴����o�,O7�x>f�(Ֆ\u0013��ǖ�d\u0017J�c>V�\u0003�*�U�N�W\u0012��۬�ɮ�m\u0003�+��/��/�b��\u0007ꓺ{iq�^~�{C�%\\}.��WR�T\u0016|kR�J� \u0010Q��1Ec�P[o�\u0007�\u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\f~)�Y> �)�v�|*�ve�8[�\u0001\u001d�~#��*\u0015����vB�ݳ�%F]h9e��\u0018\b\u0004\u0002\u0001�@ \u0010���wH�\u000bxU�\\\u001d��\u0002Ć�.Z|�[\u0010\b>=��B\u000eIp7>�;�NW�+��'��X�;G�D��SL'\u000ec]#A��uUS/ղw\b��a�:W�\u0003\u0015��\\4�����>'��׸9���\u001a�x \u001b\u0011qb\u0003�#�{��s r��\u001c�@ \u0010k�'~����\u0019U��\u0010|\u000fF��\\? �x�\u0007�\u0010\b\u0004\u0002\u0001�A\u0003#-q>%\u0004�\u0004\u0002\u0001��@�RR�m\u0018�@t~c��k����L��1ĺ7��ctv� -0;�X����?*1����s�kak��v��s��\u001a�-wj��7U��ѝ.F�N�p�\u0016\u001c\u0013\u000e��&���G\u001c\f�nK\"hkM��\u001d�>����\u0019ߚՅ\u001a/���6\u0002��U�\u0012�d ��ip�\u0004������@ \u0010C$e�\u0011܂d\u0002\u0001�@ �Ի����$\u0016P\b\u0004\u0002\u0001\u0003\u001a�\u0012��Fc\u0006�(%@ \u0010\b\u0004\u0011L�#\u000bB\u0007�`\u0002\u0007 \u0010\b\u0004\u0002 ��e��PMn��\b\u001c�@ \u0010'2�\u0016��\\�\u0013�\u0010\b\u0004\u0002\u0001\u00041�c.'���A(7\b\u0015��� +Ԫe�ӺYd���4�1�ỷ־��\u001e\\�Vn�Q�1��[Ճ\u0003�|�\u001e\u000b�EZ&�GT�/�Gs�y5��έ�+��'���,jſ�qsA+��S)n��Ɨ�u�1�J�\u0006��\u000ewvE�U8�\u001a�>>�4\u0006�j/�� \u0002E͉6��\u0013ܻ\\\u001bw�[H�՞d\u001c�W��)�ucM ����\u0005���I{��wi��\u001e�g�\u0013ط�-h�xg�G?#�c\\�Xs�6� $ش�}\u001d��n�ݬ`��ueЙӱ����7��ҥ�1A\\�״��w\u001b�r;6VqDi�\u0016����V�MǑ�N\u001f���A@���iqs���\u001aֹğ\u0010��\u0001~�׋i�q�8X��\u0017��\u0001y���4}*�5�+�F\u0010�ynB�$�|��!-��\u0017\\\u001b��a�+�a\u0018��\\[�\u0015�> �ׁ�Zֱ������0�浦��0\u0012��a�\u0013s��.�M�3eV\u0018N8qg8G/�B���a�\u000b>5�J�j\u0010\b\"�A+.ߝ\u0003�,\u00109�@ \u0010\b\u0004\u0002\u0001�@ \u0010c�Oz��O�Mc�\u001b�V�-y���\b��\u001f��P�^6���\u0014��9*2�A�,���@ \u0010\b\u0004\u0002\u0001����vq˴Y�����j�W��\u001aKN���͈��kc�� Q�L�Wá�'d�a���=\u0006G\u0007�a�� 7\u000e����\u0011pB~\u000bp�\u0004�vY�˸)xé�P#\u0012�� |�2L���.:�$\u0002����d70�\u001b T\u0002\u0001�^�;���pʯ�H��z4����M��H=�@ \u0010\b\u0004\u0002 ��w��Aa\u0002�\u0010\b\u0004\u0015f���RR��\u001et�]Y�/V>cf��,@��\u0015�.��3]Q\u0011����U�N �U�+1,�^�yj\u0004t� Z���FKZN�7�Iې�̷��XS\u001c���,]���\u001eO�\u000f��J��c8;1\u0018'[2\u0010!A\u001c��\u0010\u0010�[\u0001A*\u0001�@ �i:��J �p�P\b\u0004\u0002\u0001\u0005>��}6Aq�@ \u00101��\u00102):��\u0010L�@ \u0010\b!�N���\b\u001d\u001b���\u001e �*\u0001�A\u0004��h%c�\u000b�r\u0001�@��AY��p�J\u000bH\u0004\u0002\u0001�A^\u0019z���H�� ����\u0004\f%�i^�Q�;�Qƴ��=O����\"o��v��\u0007��t�K��[>|��¿�n\u000b�\u0005\u001f�\u0018�^�V���RՎ�;��\u001b\u0011U[\b\u0004\fX��\u0013e\u001d jh�:��]\u000fjߣ[�ǟ�W{���'���\u001b�\u001f0.EN����T�\u0002-$\"ՓV�L*�}\u0013\u0006x���JcHZ�\u0015�����1\u0006\u0018��,�8\u001e�\u0004\b�E\u0005h%���\"0�э��]S~.���խ��;:c��$9� �v�w{m�\u001cB\u0001e����z�z��6��]~�����Q�m[+�ʧ\u0003\u0003�X�\u0010Ţ�P\u001a��mX�Z��\u000b�bЫ` \u0010G/�P6\u001f4 ��@߲��Wg���� ��@ \u0010V��o�����P\b\u0004 $\u0014��\u000f�m\fĥ \u0016�����Z�F�U�;(~Z��Sǵ]��o�`ɢ-���\u0004\u0010^��\u0016�k�]{��,x�Ǳͮ ����!�kO񣇁\u0005V�م�{�\u000ei\u001b\u001dA�����o�3{�þ��1b�� �mz�\u000fn�9\u0013n��ϱǋ�n�^[˜5�i3�[\u0019Ca\u0014L{]��w\u001dI���.�u��W,�nU3�\u000fS�'Xֺ=�I��\u0015�LK�e���I��ac�o�YGM8�Z��k���\\'\"R>\\EKY\u001b\u000b��p6�/��@]���mvx��\\[��G\u001d��'\u001f݌⹆\u001ci��\b+\u0018�J�9���G:\"懾ę�{ۙ�Z��^��\u0014���F8�Dc���Û���\u0013�j�\u001ey�z����v��O�v'�2|8D��\u0017D^\u001a��P@3���\u0017\u001aI�\"ǿ�1�x�U�9�U\u001e�\u001cq��t��\u001dN��-���ځ�Oh�f��uX\u0006�{�kEx��Ψ�\u000f>�� qm(\u001e\u001c\u001c#Iӿ�r�7V(�J�f��4���S@eq�������q.�q��O?��S\u0018�>|����ú�\\��eߢ4c�Zj~�:����\u000f�Ǘ�ұe�*��i|���Ȳ�9�\u0004\bV0\fX�c区�����[FC\"E��u�\u0005�*\u0001\u0002\u001eH(�\u0019\u0012���9 T\u0002\u0001\u0003]Ƞ�P��\b� (�Y\u0018\b- �+�d�ܮ-n|��s���\u0002�B�\u001b8��\"�l6���u\u0012�8�9��CΊh���\"�����N9ѧ�F\u0013�:���1�J\bs�v3�9���4\u0006@�kۺ���v7\"�rc\u001845/��\u0017����\u0003M�[�X\u0003���\u001c�l���\b\u0004\u0002\b���A��\u001f�fa\u001f�#\u0018�eaH6�\u0001�@ k�\u0014\u0014�k��t\u0017�\u0005@ k�$c\u0005Y�u\u0005�Ӌ)Y�f%\fk!\u0017Z�d����@qܻ�R3wKK\u001c�Q\\f�8�Z�73I#�s�b�0��umb\u0017������,5�\u0012�{�6�>�#ќ*�9*�MP\u001a�B�u�S\u001b=��5�@$�\u001cA���\u0013{�I;�5x��ʝQ��s�]�u�;a���ƀM��s�W&��u�:1N��\u000b\u0005�F\fS8�y�m\u0005������N��\u0014�{�u+�J��ۉN���7�|�\\�7v'(\u000f`�촯6ћ �atx�+��\u0018�!~���hsI\u00048]�\u0004\u001d��V�֓g14�y�����μ0�����2u[˚����䗂\u001c�E�\u0016ӻ&u�4\\^�~��Fޝ8�\u001cp�>^�N\u0015��\u0015���zD��r��G��d.�j�5 �&���O-.���>pߑ����\u00155w�-��q\u0016\"o�������֨�\u0001\u0005 \u0006���Aw}H\u001c�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b1���\u000b'�w�Mc�\u001b�V�\u0018�-s��~����G��P�^v�tvB�ޙ�ҏ��.�',3���@ \u0010\b\u0004\u0002\u0001����/\u0012s��NR�s#[w�\u0014U� c�K\u001ei�|��4^���{ �r\u0003\u0016�)�#2�[�����\u0005ev\u001fmF��S�SF��Lh��$�lD^�u\u0002t�����c�\u0018uv?Vj1\u001a���*\u001a���:���[,M/�b[\u001bɈ;[��\u0003���\u0010z���/��\u0007 \u00100���7��@���|h5�\u0013�IX��\f��Ĉ>\u0007�O�.\u001f�߼D��\b\u0004\u0002\u0001�@ �9���@�e\u0002�\u0010\b\u0004\u001e@饊���\u0002Y(��I��v�\u0016�n��B�\\ݭo��\u0017��7LUm��ES�DG��xr���w�kȝ\u0011j�i󰦋�\u001b�5=�v醢�\u0017\u0017��W�=\u0014�E�DxԼ\u0017\u0001�3\\��K�����&���\u0011�Ī��ҙN� B�\u001c@\u001b�\u0018A\u001br@�\u0002\u0001�@�9���y@ \u0010\b\u0004\u0002\b��j�ނD\u0002\u0001�@��5�k�d\u000f@ \u0010\b\u0004\f{���ܐ8X��*\u0001�@ ��k#�\f�#��c\u001f̬)\u0006�@ \u0010\b\u0004\u0002\bZ�\u0017\u001b\u001d�4\u00127{�\u000e@ B@A\u0004�0X����T�mp\u000bz�$��0�ݝ�Y�������S�E��\u001alY�\u001f�o�:�,�~ϖ�j���o��oo��]k�1U�3���6�;�\u000b�\u0015�c��1�/eMl���\u0006��+xb�S��c�\u0019�����q.�~�\u001b��\u0012����o�ȮW\u001b�;0���k\\\u0014J\u0017FK�^�ַw>j k�R�\u0018��\u00118yM�0�Q�S\u0004���T1���&��w\u0017\u0005�6#g\u001e�z�\\rsml^1�\u0007\u001a����q7�lR�x�1����H\u0001�24\u0016�P}�f�������� UUEQ�5Q��4�'V8�e����??;��\u0006#\u0006+Fʨ\u001d�9X�\u0018lE��� �\u0004\\\u001e�����;�XNS\u00131:����\u0019k�#���3���ӌǟ#�9,�a\u0018Y\u0002\b㑒7Sy ��@ \u0010\b\u0004\u0002\u0001�@ \u0010\b1ش���g;`�8�P k\u001d��\bmu5� j#�\u0002q\u0007o.�\u000f#���\u0011�+\u0017���|��\u001d� �Ӳ��Y��y�Q�� �\u0001�@ \u0010\b\u0004\u001f����}��缳�`sJ`e}\u0005E\u000b߳�\u001c�I\u0017Ycap\u001e�bm��A�\u001c�Ìc���i�>d��q̥%u6\u0015CS3�ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010S\u001e���PZ\b\u001c�@ \u0010|�o˔Y���\u0015����.�5۵���\b����Sw�J�q\u0013�ǻ\u0007>�u�{9�x�yã\u001e@�2�\u0019W=+\u0003�}I\u001a��\u00024�6\u00044\u0011�\u001f�z�Dw鵪��-\u001eY��C��%�Ffg��9%�6�SI^\u0002ƽ҂�\u0001���\u00068\u0018H \u000bSI A�)���whQ�ӌ��-�\b\u0004 \u000bH�\u0013\u0003\u001evRa�8����G�E]�,E�\u000bam\u0011�U�\u0010\b\u0010�b ���\u001f���@�\u0002\u0001�\u000f$\u0015i�� ��@ \u0010\b!��w��Jsj �W-hq?*\u000f��.\u000f���c\u0015�Ɯ�P�GQ\u0014�ycCb|.l��@�\u000b4����s ���3�\u001e��Ɔ�h�\u001d���j�(�Qa���q�_af�s���փϸz9�\u001fN�@ \u0010F��Z O��C3\b��\u0018��+ A�\u0010\b\u0004\u0002\u0001�\u001f\u000f����P]��@�\u0002\u0006I�\u0014*|����\u0017�5Z�͒^��\u0015j 6pQYTa�����*qV\u0007�h����N��ԁ����ΰ���^��-.���mO7MZ7��~�\u0017\u0010@����|�9s.yWO�Ol/߳����,G\b�ɸW���ȩxKէ���x7+\u0018��ǌ]���u��VZ���\u0006am�,ib̜�4�C$�A��\f\u001b����J���|�ZʑfG\fӵ�\u0004��S�\"�؍F�n\u0017B�3U�1�U1��)��b��g�{%ǜ���с�W����K�!\u0005D��p2��7�����\u0007=>����| Mv3\u0015F�x���i�|Z�\u0012��r�9�u\u001f\"�>�.շ\u0016������G�Ե�-a�\u0019��x����v���\u000b�v���� c���>�\u00179��&q����58���hh\u001d�.%V�\\�6V8,�7�>���*�Q�� sY��\u0018��\u001ag��T�Q\u001f��?�m:�\\*Ɗ�\u0019�jy��]�;�����\u0017\u0002�s��Yw�c� ;�4G}RK=���eW'�l�)˖|�N�\u0003��+��.�F\u000b 9��%J[#�T���S��c�\u0019�����q.�~�\u001b��\u0012����o�Yrfp�r�ƘJ,��ȧ\u0002��lk�\u001c,Sc6�S����-�Z�f�j�#)$�X,c��\u0019� \\j\u0017�迥~A�\u0015���\u001c1՗U4>K�q�WO���z%��ԙ\u0012��.����A��T.��C��c��z��W�\u001a�m��(�˗J���9��\u001e+��zJ��\u000fUmV\u0018.\u000eK\fAQ���\u001f�\u0003����ށP\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001\u0006\u000f1�A����;�����9��\u0001�Oȭ]��눌�eJ�^�3>|MO���E��\u0012GO4rv��򟔮\u0005T�.��X�RwR����\u0002\u0001�@ \u0010\b\u0004\u001f����\u001a1�C-p�2�\u0014\u001a�WO�WTàv��O#�h\u0016v�j\u001b\u000b\u0013}��b\u001e\u0005�E����\u001739c�u�f �J��W:�\u0011��s��-�Up�����z�QTb\u0014,��?T�\u0011��\u0012$}˴�\u0013[�[�\u0016.q;��[\u0001 � \u001eH\u001b �J@��Tv��\u000ei~H�\u001c^�'�M���Û\u0015 �\u0011GM������D��ó6 [����\\\u0007����W��%��k���d�Ұ��w$�s� kŎ=e�L��к�\u0018t�\u0010F\u0003���؞��\u0014F\u0010/�e�mc{\u0010Pf�1b��!�L؜� ��89�Χ��\u001c\u0001\u0007�\u0010\b�@(=\u0011\u001dV��??Ё���|���\b$����\u0002Y�y �*�!����-��� �\\?R~�\u0003C�\u001d�I=�\u001d��\u001d�G�� �� \u001a�\b)��]m'��\u0010\\�;%�䂧�\u000fԟ��@�J\u000b�c�m\u0014񣴫,\u001ex�\u0005A�\u0006�l�Bv�P�+.���&1���.E���.W������^v��s��TiA�U�y6�/Э �0M W]�6�}KT�:�5�i;z��\u000e���6\u0002�\u001bQ1`䁔�\u0005�\u001b �$�X�����\u0001>i���\u0016L��ؠ���6�~�\u0005��q�\u0005\u001a��.\u001bx��\f�At\u0015��~���B ��\u0013\u001bZ� d�}Y�I�~�\u000f��\u0013�k eEP��\u000b�\u0010�'[&�n�y*�N-�6��\b\u001a���\u0001��}?B 哫A\fuZ��\u0012�p�\u0015�o�O�� H����\"�����߱??Ђ�%��l� *����\u0004�L$\b\"��F��\u001d�B\b$��Z\u001aM�B\u0007��\u0002�>_�\u0004��\u0017�D t��\u0017�\u0006EV�;���N!�׺\b���B\u0007ES�wY\u0003�H�\u0006>����n��(2\u0012��4:�A_ˇ�O�� ��u�:�*>��i??Ђ�\u0001ͺ�iŮ\ft�A�i??з�pg\u0005�_f\u0013��8��ˇ�O��, ៮�ֲ\b��ꏚO�� �\u0019���b�\u0004RT5�ָ[��\b���`Ҙ��\"R�f��Mǯ�Xn|UBG\u0006؏��A$���C\u001dXyA;�kE�U������B\b���5�\u0013���~\b%��y4��B\u000b\f���\u00042U�Gq���Ab9D�\u0005w�\u0006��'c��\u0010+j\f�4Y\u0004=w���\u0004���� \u0004\u0005EOPy]\u0001\u0015W[�$|�B\u0007MR!u�}��!Y���_ЂJ��F{�AR*�4\u000f�� ���b� sη��͹��/\u0001�\u000f\u001ft��>_�xk�0��T��a5�B긡��)& ���\u0003�����ěY�\u0007�:P�tp��5ly^�ukd�m$�\u0002�:��Z�H\u001a�٬}l�\u001d`/\u001d�؛�W�K6'>V�v$d5\u0017�W]�_���/$��c�a܃d \u0010\b\u0004\u0011��փS�?���?�F1��m�\u0002\u0001�@\u0014\u0014⌶G\u001f\u0014\u0016�-�\u0007 \u00105�P� ��]a�?��� 8� ŐZ\b��s`��-)�\u0006qi\u001e+�M�� �\u0005����\u0013���� ��������q��]*Ҧ�=teь���(����\u001f܇��)�Y��9\u001a�ZhjH#�TN#�PYS4ZS\u001e�\u001e��m.�g\\ϭ�ʔ�G�O�\u0007��NN\u0015��zR�R٪�g��l�D�#�\u000b�\u0001|ma��+Z�.���(��r�˱���������6�\f��A\u001bX�{5�4\u000b��\u001b��7\\k[Y�n͕�D.6�ۨ\"�#4�מ\u0004�\"�\u0012{��L�8�X\u0015\u001cӋx&�{��h�� �p�Ƃ�D\u000e�� �~D\u0017�\b\u0004\u0002\b�n���8XZ�\u0013�H\u0015��w���\\�ux���@ \u0010W�2��w �� X���A\u0014��m�\u0006A\u0017V�\u0015��D���S�\"�f\"0D��\u0005m\u0012�\u0016\u001f\u0013���i楐�dac���ͷ#�ح�o\u0013e\\LqLLtLJ���-i�x��s�(�W���\f��N�SQ��#\u0005�\u0012KL.�܍$��\u0001��|~�z�Ui]��ﱍurU\u000f\u000fe�nΊ���r⧛��汷�\u001en\u001b�|�ƫ��՟#�\u001d�8/��u�}]�zh�d\b�R��H��=AU��I�Zg��\f� \u0003�����N��\u001dN\u0014W�ϋS�{��:w�a�{��vq���v�0��\u000b�����M���N�p��4q\u0018�m�X��\f6\"5S�^܎޵f�:U/��\u001dM5ё�^\u001fм�o��������t8Zt�:#�G��ѧ�|��!\u0001�n}}�\u0011���[$�-�D`�7��o -5F�`�\\���Ѫ\u0013��l�%�0B6Z�N�`����6��|q�� �Ib�h\u001f_5\u0013I?]�K9u̧��>�p��z&6��\u0002n@\u0001\u0004u\u0017�-~}߷�A�ސ�\u000eμc0����r�U��9Hv�����!󵷘>`絃�����\u001c��\fG\u000eűW�u\u0010�\u0006Z��\u001b�׈���\u0017�F��v��G+�\u0006��T-\u000b���A\u0018.\u001bk8�~��҃)�\u001aVK����\u001aE\u0005m�coh�\u000f���\u001e� �΍P~�0Zֻ,�\u0003A�1\u000e��ҟ�R%ҽ���.e��z\u001a\u001d�q*��F�����E\u0006��\u000e�\b��\u0003�\u000f�\u00029� ��#��.���0�m�$\u0012�\u00029]\u0004-�\u0003揑\u0004�eł\b�\u0018o6�� �\u0001nH ��\b���H%e���\u0004��ԏ�\u0002�!o�\u0003�\u0003�\u0002��[G!oR\u0004s\u00187\"�\u0012\bC���X�����\u0005\u0018�ԏ�\u0003�\u0007��\u0006\u00060��\u001cci�\u00104���\b\u001b\u001c���\u0003m�\u000f���GȂV�\u0001�\u0004e�qݿ2\u0007��9\u000b M\u0003�]\u0003$saiv��\u0005\u0001�\u000b�>4\u000em�r\u0005p\u0007��\u0004\u00110r\u0001X;��&���|�#�F�8 sY\u001bN�\u0017� ��\u001d�F�\u0001� B�y�D\u000e�Ȃ\u0007�m�ߘ X��$\u000f��ԏ�\u0002�ۖ�\u0017CO0 F�� N���\u0010D]����A.��m\b��\u000b S\u001bO1t\u0011�X\u001d}($,o�@�,�HA\u0019��v��b� ����H@�\u0007�\u0006�1��� ^����\u0004��ԏ�\u0002��o!d\u00108��n۠��h��|H\u0014�\u001eb�\u0012\u0007h\u0003�\u0001\u0003t_�\u0017��4�[f��\u00028����@�un���'\u0003�d\u0015��8�6'ry�� �w\u001b:9p�; �՜0�+`�s�t�U0Z*x��pʈ��C��[r{�r{/枌��M#�\f�#��c\u001f̬)\u0006�@ \u0010\b\u0004\bvAZ9uHB\u000b�\u0003�\b\u0010� �4�\bA`n�P\b5�\u0017�t�C\u0014sA� ;�\u001ba��-3m�\u0005�*�έ\u001a�y�rs2�l:��/5\u000f'\\�9{��W�\u001b����\u0005[�����٪%B�}*�\u0006{*}\u000f\b�\u001d�p�9���s�\\-O��G�\u000b��\u000b\b\u0004\u0002\u0001\u00042��\u0011�4\u000fc���\u0003�\b\u0004\u0011�'V/k�H�\u0012��������a�S�bI\u0002��\u000fy��\"�i\f��B�7��ٷ��\u000b�[|���O����ݡ�Qs@���+g���:�z��P\b4�\u0016=�\b�����N��\u001f�i�ϋS�{ڧ�\u001bh�\u0015�\u001e��Z����-(ږh��Xo/�o�֝E\u0015G�޺\u001c)ꓺ;\u0014x;g�|�A�C�\u0010!Ye\u001b�ҳ -5F����N�v�P�l\u0004\bPy�,r~T\u001eֿK��F���I�S���3�y�\u0010�z��\fk\u0011vO�\u0016\u0014\"al\u0018�\u001f\u0004��5��ďk�qb^��o\"H��\u0005S\u001d:f'\\F[��ҝ\u0019�z!�i{v6p\u001f\u0015�}w\\*h�gύݚה��\u0001\u0005X$���,\u0001m�9�@ \u0010\b\u0004\u0002\u0001�A\u0019�g\u0016ІK\u001b��#�[CJ����/]R�\u0005sj^�];�����:Mm�I\u0013�-f��\u000fO�6�l�8�*R� ���a���^ \u001c8�dt��%�c`dMm��6�\"ć�c$�\\�`��\u0004\u001f\u001f��n\u0015����V\u0016OKQ�=�\u0004��{dm�G'4\u001e|�\f�^�i2�\u001fMAI\u0018� X����\u001b5�45�s��\u0007��;���pʯ�H��z4����M��H=�@ \u0010\b\u0004\u0002\fs}����\u000b��PH�@ \u0010W�䷥ ����=@|G��O���J��7Gl��OC�8.%Z��ԐsS\"���@�r()T����(-C�\u0004\u0012�\u0010\b\u0004\u0002 u^s~?�Ae�j\u0007�\u0010\b\u0004 w/�\u0005\u000f�g��A}��T\u000e@ \u0010\b!�1�AK\f�d�g��d�\b\u0004\u0002\u0001\u0005J�pw��A+}�z� G$ �@ \u0010c+~��?m\u0005��O���J�@ \u00104�\b(G����\u0019\u0014\u0002\u0001�@ �M�?�\u001fiAi��\u0007�\u0010\b\u0004\u0015j��\u000e��\u0010X@ \u0010\b\u001a�H(Wy�����\u0004 �@ B��)�KC�~�\u000b�\u0003�\b\u0004\u0002\u0006��AJ��\u001c��\u0001�@ �/���@�Oqj\u000b( \u0005�{����n�l?m\u0006�@ \u0010\b#ww�\u0006�����p�c��� ۈ\u0004\u0002\u0001�A���d��(.��z\u0001\u0003$�J \u0015>k}J\f�\u0001�\u0005�(\u001b��T�ɻ&��\u001b�\u001d�MaV\u0015D�N*��ƙh�\u0006����9@�ڣ�\u0014�\u000f\u00062Z�G�044Z����]��\u001bk:�uT{�ó�,�#�j���\u001foU����:\u0016��r�������\u0017x\u0013;??]S�ۜ&����G�W�K\u001d#H-�\u001bP$`y\u001b�K\\\u0005�\u001e�u%��\u0013T�'�\\a-o��5DG\u001e\u001eV��\u0019\u000e�\u0018|48|,��\"�,g!���s�\\{��Ŷ��7Rʌ�@\u001dgY5�� H�]�U���&\u0010�����T�V�\u0005@ ���1�~t_ �҃(�@ \u0010AQ�\u0014\u0011�y�-\u000eH\u0015��d��>�w��A�@ \u0010\b)�y���i\u0004���%@ \u0010E/�� ?4��\\@ GrY�aM��E_�]^D6�'��C#\u0014�������|���O�������^b�k��u,�-\u0005;b�\u0010i~+{�\u0015��(����pد��\u0016�\u001a��N�,6ӹ�>\u000b�gǿ�.�[\u001d\u0010w����(��Xo/�o�֝@��f[������^�5��V���in��м��U\u001fz��tGb�\u0007l�ϑ�5�u�\u0004+,��Va�������3q���?��]:�J7�n%;s�����\\���Ӫ\u0013��� \u000f?t�\u001f�����Պ�\u001c\u000b8W;��\u001c;�c罘�\u001b ����\u0003S�Z�\u0017}�\u0010���\u001b�s#-�\u0011��*V\u0015w�qN��KcK�Ne�5`�5�&�C\u000b� \u0004�\u0018�� �ƭ�z�J��j�=��5�\u0010���W^��!\u0005\u001c?�\u0007��Aw�\u0002�\u0010\b\u0004\u0002\u0001�@ \u00105��f\u0019�-*�Tg6s�~`4z ���>E-�j��`�>\u001d�͝��#�����:*=`�\\�_\u0005F�,7|w;��\u001d'e܏J��Z����Δ�� Z#��Z6\u0015؈���T.u�*�|�.��m\u0016�e͚]X���!V@�@ \u0010\b\u0004\u0002\u0001\u0007���\u0017J~�x�\u0004j0�zl\u0006�\u0016ms't�IF����cC{\u0012ť�9��ͅ�\u0004 ��u���z�Sc��.͝l�&yChk[\u0013�狓�0�\u001cF���\u000b�6\u0016;��s\u0016�\u0016�\u000b-\u0016\u0017>\u0013J!|0Q��+'��M$\\l�J�@{ {��\u0007w�\u0003�쾑ssnh\u001c�A\u001b��������n�W$\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010W�Z����$n�ւD\u0002\u0001� ��ޔ6��}����#\u0010��S��5Ҿk���.e��g�\u000b�V�r5$\u001c�Ƞ�l\u00105��B�č�~d\u00131���\u0003�\b\u0004\u0002\u0001\u00042�\u001eE��8�F�$@ \u0010\b\u0010���Վ�Wz\u000b Y\u0002�\u0010\b\u0004\u0011�؂�\u001aX[\bp\u001d���PZ@ \u0010\b\u0004\u0010�F%����(\u001c�ٶ@曄\u000e@ \u0010\b*�\b���A;EίE�=�@ B\u0010U\u0011��_�vAm�@ \u0010\b!�0�H�7��=�8�z\u0001�A\u0014�\u000fn�\u0012\u0016�\u0016A2\u0001�@�]\u0005y�\u0012�\u0014\u0016\u001b�\b\u0015�@ n�\u0010��%��k|�%y z\u0001�@�p��,\u0001�(,�� T\u0002\u0001� \u0018\u000b\u001d�@�7C\u001d�&A^Y\u001c�\u001c�\u001bo��\u0010xÏ\u001cc�H�\u0006�-eiqZPȜ*\"�a��.\u001aYAP�ɷ�'x�\b7�q1�� \u001b\u0013�l\u0017�����^��c��+��Uܴ���>~�qp��nS�\u0017�\u00106���p�v���\u000e�\u0006Yw+(�o,�]��)�@��9�95A\u001cM��Z�-1M�ja��s��\u001d�tܸ\u001e�m�+��\u001dϗ9��;��ٱ�a|-�qLU���1/)q�?˂b\u0018m;0�ʐʈ�/�2�����m�ts�\u0006�6��W;=\u001a*�;��؟m��ZiU\u001c�����\u0019[\u001f���V�yi̬\u000e��\u0016so�}K�ieܧ\fquh�ӧ\f\u001fO���\u0014�3}c������#\u001a�\u000e��!=��ߞ�y�j���ϥJm0�#�|�C��*N�\u0006�8�B\u0007]G5\b)��# ��u�����r\u0001�@ \u0010\b\u0004\u0002\u0001\u0003]�f\u0019�\u0005\u0014��K]q7\u00148&[�'g��=��_���q�{��p\u0016�yN��ޜ�fjVь0\\6¥��'_���߱\u001d���H��\"�r\u001e��Z��J���9SF\f�\u000b��G}݉� �^�0�|���3��o%N]Z`� �Y` \u0010\b\u0004\u0002\u0001�A���\u0017I\u001e=�,����S�QA&/�H聝����H���HƎæ������\\ϕͶ�\u000e������Xqt��5u4�F\u000f�^P|*���_�OT��أ��=3�z r\u001dp� �(�敘ii�����\f�w�qO���N�R��ۉN���7�|�\"u����` C�\u0018���D~�e��E��W��\u001c�'u]�����\u0017~�4y�ի��pp�'z�p��E�j��Ձ��EM0���䶪VS�`{�� � ��\u0001t���U3�\u0019tF*\u0016�w�P�t\u0018t�����Pu�O#���� ��\u0004\u00141 �QFds�;�Ś.o`N��t\u001er=,8T�k�\u0013��妭���R����\u0019�_S��a�M���zJ\u0019�Q\u0014r4�8\u0007\u0002/�/ߺ [��C3\b��\u0018��+ A�\u0010\b\u0004\u0002\u0001��\u001e{�J\u000b!\u0002�\u0010!䂭G!�����\u0001���i��\u000f��-����GD��|�&����e�J�l�+[ �\b�u�a��u�:�N�G�=�Ly^#�,��OO_҂�@�\u0004\bO4\u0018��v�?��\u0006E�@ �_0�l>hA2\u0001��e� ��ww��Ai�@ �Na\u0004��\u0015�@�F����N�v�P�l\u0004\bPy�O�J_��_��w�\u000fgi;��9\u0017���\u000b��R���\u0012�w��\\B�]���| \u001f]0K�oa�6쿣P\u0017�r��m1;�#��⌱�a�G�>�-RG-��fy4� �S�Лw\u001fs;�|��ެ��J�\u0016���\u000bmv��5�.�\u001eJބuʮ\u001e�1|el�\u0017���@ \u0010\b\u0004\u0002\u0001�A\u001b�%�m\u000f�ū)�J �%\u0004E\u001b\u001fS%�Oa�͇��%-�8��*��X4�\u0007��6�c\u0018��ݪM=1�}b2�#.h��T\u001b�\u0010-�N�V�E3���\u0011�w�?�B�]�mOGd*]�\u0018���\fN\\O\u0018���c,t�\u0015\u0014�@�ܖ�\u0011� -\u0006�G.wA�|���r�\u0017I���Ƞ���D�45���5�\u0006��\u001b\u0001�^�\u0016A�\b\u0004\u0002\u0001�^�;���pʯ�H��z4����M��H=�@ \u0010\b\u0004\u0002\b\u0019\u0019k��('@ \u0010\b\u0004\u0015�\u0017\u000bzP��y��Kp|C��O���J��7Gl��OB�8.%Z��ԐsS\"���@ c�\\,�\"ac@($@ \u0010\b\u0004\u0010�\u0019y\u0004w ��@ \u0010\b+u.�u� \u0005�\u0002\u0001�@ƶĠlQ���� P\b\u0004\u0002\u0001\u0004S0��Ё�X��\u0004\u0002\u0001���\u0019mo\u0014\u0013[�B\u0007 \u0010\b\u0004 ̠��\"W9\u0004�\u0004\u0002\u0001�A\fq�ˉ�7��J �\u0005@ \u0010\b\"����@�n��PH�@ \u00105ۋ �v\u0019\u0003�\u0004�\u0004\u0002\u0001\u0002\u0014\fA �ΐ;�\u000f�AcV�@�\u0002\u0001�\u000f$\u0011D�۟\u0014\u0013s@ \u0010\b\u0004\u0011���#� \"a��\u001e�\u0012 �F\u0017��A��k\u000e\"�s+q[ � �h`����\u000e�\u0018d{Z�=�i�7�GX�;X_�n\u001a?��\u00078c��c�\u001fI�3�.���G4���Ie?mr�e�b�\fE�],��\u0016��*w{��|>�TzWF�O}>|P�w��c�6��j���A���\f\u0004\u0002\u0001�@ \u0010\b?����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010U�[��J\u000b(\u0015�@ �^KhCi��ޏ��#\u0010��3��F�W�q�;eͲz\b��qj��Ƥ���\u0014\u001c��\u0004A\u001c��\u0010$\u000e��PL�@ \u0010\b+�&��J ��@�\u0004\u0002\u0001��_۲\u000bh\u0004\u0002\u0001��v��8d� � ��@ \u0010CQ'U\u0019w�ҁ�?[n��7@�\u0002\u0001����h&c�\u000b�z\u0001�@��\u0005V���=(-�\u0010\b\u0004\u0002\u0001\u0005h%�\u000b��#�('o�\u0007 \u0010\b\u0004\u0010�� �\u0004�N��'@ \u0010\b\u0011\u0005i��B\u000b-7�T\u0002\u0001\u0002\u0014\fAQ��7�\u000fj\u000b��\u0007 \u0010\b\u0004\b�E\u0005X%��7�\u0005�\u0002\u0001�\u001cl \b��DEބ\u0005�M��\u001a\u001cm�AR7���z\u000b�� e�\b$\u0017@��\u0005y�-p\u001d�,\\ M�r\u0004'Or\b�_~�\u0016����j�&���Y��\u000eY�mdMWMP�\u00175���N�j\u0006��7]�\u0005��ek\u001a9w���1��3�~�Ťg�����\u0006e�\f����`i�0\u001cCZ/�� /ϽO��U�s�,a���\u001c�W\u000b�YN�ܭ�\u000b[(\u000f\\-OW3���F��ېA#I��+���H- n�\u0003u\u001b�@\u0012\u001dqd\u0014�tf0\u00077���\b\u001c �@�]\u0003K�p@� �}H\"�R�k {�}�H�HA\u0018�A2 P�w�AkQ�$\u000f\u0016(\u0003t\u0011�Ђ)es\bsA3y\\�\u000e7\u001c�-��@׎�\bi�KN�\u001d��� od =�\u001f\u0004\u0003]w\u001b\u0004\u0015#��,B\u000b`Yܐ4���u�\u0005�WQ����\u000bz���.\u000e��#С��\\g\\�@�\u0002y��a\u001d�\bJZ��h|���{\u0005ȝn�:�=��\u000b��\u0003\\���0��~��\u0018��\u0011��R5Ҿk���.m��G�\u000b�V�n5$\u001c�Ƞ�l\u0010\b+��A\u0002�7L`z��'@ \u0010\b\u0004\u0015*X^�����E�\u0007 \u0010\b\u0004\u0001AC�=m�(/�\u0002\u0001�A\u001c��A 3\u000b\u000b�Z@ \u0010\b\u0004\u0015��d��z=�\u001f\u0003t��\u001e�k�z\u0001�AN�2��CKl�D\u0002\u0001�:1�l���\\ٝ8�e\u0005+f\u00128�^$la��K����q�\u0007Z�(5F!�\u00130�N\u0011���XZ ��@ \u0010\b\u001a�pPR��I�\u000b��\u0015�\u0010T�k�\u0016�\u000bM\u0016\u0001\u0003�\b x�m b��#�����}�Q��)8>�M�z��J�#>w�p���Xo������ �链�\u0016\u001c�:��/�Y���[�1�ՀQ�*\u0004(*DׇnJ\u000b�\u0012�\u0002�a\u00167AF�7:X��\u001c��Ƞ\u0010\b\u0004\u0011L\u000e��:vو,\u0004 �@�)B�u�(/ \u0010\b\u0004\u0015j\u001a\\����H,0Xn��\u0004\u0002\b�\u0005�6(\"�\u00044�\u0005�\u0002\u0004F����N�v�P�l\u0004 %�f\u001e~�\u0015�R��}\u0017��]�\u0003�I�=��^������\\J��]\u000b����bT\u0016|ko�̘�R\u0007�\u0010\b\u0004\by dd\u001eH\u001e\u0010*\u0001�@�@\u0004�A�\u0007\u000b�H\u001c�@ \u0010\b\u0004\u0002\u0004v�\u0003Z�#��\u0011�޴\u001a�\u0011��f\u0011��1��V\u0014�n \u0010\b\u0004\u0002\u0001\u0004L-s���k�:��uBu�\u00105ܖa�y��W�J_��_��w8\u000f�'t���^������\\J��]\u000b���ξ�nvU��[S4Α�\u000e\u001ex��ЬYՇB\u001bzt�&���\u000e\u001f�V��Qc\f�ws �\u0013��\u001f;w4�A�@��uԟN��\f�\\�#�Nm�\u001e^�o���$�]]�D��[��{n�KT��\u00107@�\u0002\u0001�@ \u0010%�1 \u000f\u0004��i\u0015b�L�mࣦ�$�N\u0011�B�Z\\�(��rj}T�Y�\u0017��ds8��㒣.�\u001c���\f\u0004\u0002\u0001�@ \u0010\b?����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010T��w�\u0005�\u0002\u0001�A^��ޔ6��z���\u0001�\u001f�%?��]+��\u001d��Y=\u000e|ตkw#RA�L�\u000eF��)|� O�c��A2\u0001�@ �?����\u0005�\u0002\u0001�@�\u0005/�g��At T\u0002\u0001��\u0005jNN����@ \u0010\b+�{����%\u001eh� z\u0001�@ �W��\b-}�ā�\u0004\u0002\u0001\u0002w���ww��Ai�@ \u0010\b*����J\u000bH\u0004\u0002\u0001��O�����\u0004�\u0004\u0002\u0001\u0003_�\u0005z�4z�i\u0005�\u0002\u0001� \u0006 ��wo�{Pd\u0010\b\u0004\u0002\u0001\u0002\u001eH ��Aa�@ \u0010C/��Q@����\u0013�\u0010\b\u0004\u0002\u0001�@ ��޴\u001a�\u0011��f\u0011��1��V\u0014�n \u0010\b\u0004\u0002\u0001\u0005X�\u0016\u0010\b\u0010����iY�����\u001f����\u0014�{�t��(�����ϟ#G�r'[�N�N�\u0002\u0006���3\u000f?t��)K����X��\u0001����\\kן������W\u001fK�v�԰�J\u000b>5���-�)b���v]�\u0017�|��ڣ\fs��%����\u0006���:y�\u000b��]K����:�/eνS��q�?1� �/־�#&�\\K��\u0006�����\u0002\b���Cz�©�).�>�ܾ5Quq�@ \u0010\b\u0004\u0002\u0006���1\u001b[b��CE8K\u001d4$���Ӫ����z �)�v�|*�ve�8[�\u0001\u001d�~#��*\u0015����vB�ݳ�%F]h9e��\u0018\b\u0004\u0002\u0001�@ \u0010����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010U`!�ւ�\u0001�@ �ap�� ���z>�\f#\u0010��)��#]+��\u001d���=\u0006|ตkv�RA�L�\u000eF��\u0019�k XE�\u0010J�@ \u0010\b+N�9ͷ���X���T\u0002\u0001�A_@�5w���@ \u0010\b\u001avA\u001c- ��L�@ \u0010\b �i|d\u000fG�\u0003�\u00047��@n�P\b\u0004\u0002\b%hy\u0017A+w�\u0003�\b\u0004\u0002\u0004( ��~��\u0001�@ \u0010T��k��I��\u0016��P\b\u0004\u0002\b�hsw@G�� \u0010\b\u0004\u0002\u0004\"�\"���\u0014\u0012�p\u0010*\u0001�@�\b)�\u001b���\u000f�Ar��P\b\u0004\u0002\u0004F����N�v�P�l\u0004 w%�f\u001e~�\u0015�R��}\u0017��]�\u0003�I�=��ׯ?e���W\u0012�>�B���`rPY��G\u001b\u0005�I��J�(� sdhs\\�\u0007\u0002.\b\"�\u0011� ŅX��U��F�ںL/0b�\u000b�\u0011C#�i�6 lҺR\u0007'\u0010:�\u0012Z-`\u0003l\u0017Zތi��'���65�-�C�2�rj��B������@ \u0010\b\u0004\u0002\u0006\u0013��I�\u0007oe\u001cU�L\u001ao�\u0019��,�ɥ�/5\u0012K\u0013\" �$�\u00174\\\u0013��6\u0004�\u0013��t��iN���sm�x�m�r�\u0019k\f��(�z�5� \u0011��cZ_�=V�n}*+z��^>z��Y���\u001d\"�x���\u000b\u000b ��{�O�}�k\u001d�� ���k�\u0016�@G}߈�� �b�=\u001d��wl��Q�Z\u000eYd�\u0002\u0001�@ \u0010\b\u0004\u001f����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010DKok��P� ��@ �nKjP�jy���\u0007\b�?���ꑮ����sl��>p\\J��q� �E\u0007#`��6\b\u0018\\-rm�A#mm�*\u0001�@ ���G�\u0002���\u0003�\b\u0004\u0002\u0001\u0005}C]���X@ \u0010\b\u0004\fq@�\\����� ��@ \u0010G+�\u001aI@�u\u000b�F7H@�\u0002\u0001� \\\u001aw(%o+ r\u0001�@�\u0004\u001a�}��\u000b\b\u0004\u0002\u0001�A\u0013\u001f���P8\u0001��\u001e�@ \u00102B�\u0003c7A*\u0001�@ �G\u0006�6A+y\u0004 �@ \u0010%�D�~�� {ۨ z\u0001�@��\b\" ��&o$\u000e@ \u0010\u0005\u0004R\u0011��!7h!\u0004�\u0004\u0002\u0001�@ \u0010\b#ww�\u0006�����p�c��� ۈ\u0004\u0002\u0001�A\u0003\b�lPL\u0010*\u0001\u0002\u001d�C!m��A:\u0001�\u00199\u0015�u�N��g���+�\u0015Oޜ���Z\u0012�Z���h/\"|_��T\u0002\u0001���AB?}?�� Ƞ\u0010\b\u0004\u0002\u0001\u0005*o9�\b�J\u000bM�P=�@ �W� u7���\u0001�@��AB��\u001e��\u0006@ T\u0002\u0001\u0002\u0014\fAO�Z\u001f\u0003��^(\u001c�@ \u00105܊ T���\u0017�\b\u0004\u0002\u0001\u0005I}�OR\u0007R{�PY@ \u0010\b\u0004\u0002\u0001�A\u001b��h5>#�\f�#��c\u001f̬)\u0006�@ \u0010\b\u0004\u0002\f|>�'��Av?5\u0003�\b\u0019'�PP��[��Pd�\b\u0004\u0010�ȬS��u�;>���_�j~��=����G�J���{��_=�?�v\u0019���rr�����×v؍�Ym o5r%�kXZ�\b\u0011\u0005X �)�v�|*�ve�8[�\u0001\u001d�~#��*\u0015����vB�ݳ�%F]h9e��\u0018\b\u0004\u0002\u0001�@ \u0010����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010SldK�\u0005�� �P9�@ �ap�� ���z>�7\b�?���{�t���tv˙d�\u0019��U�܍I\u000752(9\u001b\u0004\bE�V�=m�\u0010M\u0010�\u0002 \u0010\b\u0004\u0002\u0001\u0004\u0012Ǭ�B�\u0010=�@ B.��V:�]�,�d �@ \u0010F[b \bial!�w���Ai�@ \u0010CQ\u0018�2�\u001f�\u0003��6�\u001c�p��\u0004\u0002\u0001\u0005Y�\u0012Z��'h���\u0007�\u0010\b\u0004\bB �0�K���-�\u0010\b\u0004\u0002\u0001\u0004\u0011G��>$��\u0004���z\u0001�A\u0014�\u000fn�\u0012\u0016�\u0016A2\u0001�@�]\u0005y�\u0012�\u0014\u0016\u001b�\b\u0015�@ n�\u0015_\u0019\u0013\u0007\u000f\u000b|�-�\u0004 �@ \u0010!� �\u000bs�\u000b-7\b\u0015�@ �F\u0002�zP, ��\u0007r �\b\u0004\u0002\u0001�@ \u0010F��Z O��C3\b��\u0018��+ A�\u0010\b\u0004\u0002\u0001\u0002\u0014\u0014�al�����F�\u001c�@�\u000b� �Q\u0011q\u0003���.\u0004 �A\f���:٧[���W�~��˔�>�G�\u001e5*��5��|� \u0005�~\u001a?}��9t�K�j��a˻lF�,�}?��\u0012�5�-Q�\bPW���q=�,�\u0010\b\u0018E��\u0005*�^��nA���2\b\u0004\u0002\u0001\u0004r�S\bA\u0014-sX��y\u0004 �@�;� ���K��\u0005�\u0002\u0001��Dnym���X�@�A\"\u0001�\u0019���5\u0003i�1�t\u0016\u0010\b\u0004\u0011��\b�@:�$� \u0010\b\u0004\u0015j\"t�\u0016� �0��~h\u001e�@ �$d��w�)�1���\u0005�\u0002\u0006=���e�D�v �Y���P��W��'c�.n�CN\u0018u�#�\u001b�nk�]>�\u001b�� �����������}ˍ:��5B��\u00102K��f\u0019��:B�ț�}Nu\u001dL��a.��|�ES=��߿�.-�;�R�eJV\u0012\u0005f�MJ�\u0002\u0001�AZb��6 ���-�bg\u0006���\u001dI���_T�4ł�lLd�\u0005�pso�FƐC����t��a�qj�r�m1�\u001b�*\u0016\u0006�,�p\u0005��߿�Q���ַcc�kqB\u0018��,�E8N+�V9,-�\u0004\u0018�S޲|\u0013�SX�F�U���^p��\u0002;��G�hT+\u0017����;�g�J���r�',0\u0010\b\u0004\u0002\u0001�@ ����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010@%���A(h\u0006�\u001c�@ \u0010C/%�!����G��\u0011�q\u0019��#]+��\u001d���=\u0004|ีkv�RA�L�\u000eF�\u0002\u0011t\u00119�Z\u0007���t\u000f@ \u0010\b\u0004\u0015旪sG��LZ\u001e\u00109�@ \u0010C�_\u001a �\b\u0004\u0002\u0001\u0003\\������@ \u0010C;��˼\u0010,o�\u001a��hj\u0007 \u0010\b\u0004\u0011?�\u0007��\u0007 \u0010\b\u0004\bPB|��A:\u0001�@ \u0010V�}O-�$ �4\u0003t\u000e@ \u0010\b\u001a�H\u001b\u001fz \u0010\b\u0004\u0002\u0001\u0004O@��\b\u001c�@ \u0010\b �]/\u0001\u0004�n�\u000e@ \u0010\b\u0010��/9�%g$\u000e@ \u0010!@�@�h\u00129\u0004��\u0004�\u0004\u0011�r6Yf\u0015�!\u0013���x�ډ�qik\u0018ļ�O� |?4ύ�\u0017�\\�'\u0013\u0018�T�\u0013F��,oV\b`��ۛ6�Һ��&�9�&*�zG\u000f�m;��.�� N7'H����\\J�����e�X\b\"�هk�?���p���S�LM��w�S�Β��)�Hg+|����KcN\u0010��e\u001d �\u000f\u001c�I��7*d\u0010\u001cl�\u000b\u0011\u000f3�\u0007tԔy��ga�qUH�D�S6I\u001e��\u001eо��}{���XF�4��\u001c���qz6���y�����ڹ�т�\u0015r,8���a�UJf�XKI�� \u0010\b\u0004\u0014k\u0003\\j\u0007��\u0004\u0012�\u0010\b\u0004 $\u0015�����\u0016�\b\u0004\u0002\u0004(\u0018���ݾ��A�@ \u0010\b\u0004\by �O�\u0005�\u0002\u0001�A\f�c�E\u0002S��PN�@ \u0010\b\u0004\u0002\u0001�7wz�j|G�\u0019�G�\b�?�XR ��@ \u0010\b\u0004\u0018�}�O_҂�~j\u0007�\u00102O4��S����� \u0010\b\"#�u�\u001bL�\u000f��#�q�W�:��9^��}\u001b�ƼU���\u0011��0����Vr���;��\u0012�'/>VЦ\u0016`\\{8�ծq��+@�\u0010U�ϓ����\u0010*\u0001\u0004}�A����m��\u0019D\u0002\u0001� �0����Ahr@�\u0004\f�$\u0014a�˽_B\f�\u0001�AN��o��H'��A*\u0001�)|�\u0010Q��ւ�\u0001��� l�D\u0017>�\u0003�\b\u0004\u0014j�����4 z\u0001�����\u0005\u000f�|h. \u0010\b\u0004\u0018���ޯ�g\u00160^o2���\u0004 $\u0015*� �\u001f�\u0010=�@�\u0007�>���\u0011-=C{3�H��{8\u0016�w��ȶ�U���J\u001b�c��)kN\u0004U�A��\f�{\u0015tSI\u001c�q:��ٚH$���\u001b\u0012=7�uo�\u0013V�j�7ry\u001c��{9�A\u0005qi� w��\u0016\u0007%�8*2\u0010\b\u0011\u0018\u0004]k0�W�W3Ǖp:���٤�Hi�u]�\u000e��& ;��Xr��~�F3��8��ϣ�K�,�2f\u0007\u000e\u001fϫ2\u0017l�u9���$r#��f�^����F �eB]X9d\b\u0004\u0018�S޲|\u0013�SX�F�U���^p��\u0002;��G�hT+\u0017����;�g�J���r�',0\u0010\b\u0004\u0002\u0001�@ ����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010B\"�� y\u0014 �@ \u0010A8�[҆�Sϝ\u001f\u001b�\u0007�?���{�t���tv˙d�!��U�܍I\u000752(9\u001b\u0004\b�\u001e�B�\u00126�h\u0017�\u0007�\u0010\b\u0004\u0002\b��Y\u001b�A*\u0001�@ \u0010A��s]�/�\u0004�\u0004\u0002\u0001��\b������ �\b\u0004\u0002\u0001\u0004SGְ��� \u001a4�\u0010*\u0001�@ �Xz�ok ���\u0003�\b\u0004\u0002\u0006�(#lv����L�@ \u0010\b\u0004\u0011G\u001fW���7A 7\b\u0015�@ �Vkm���n��\u0003�\b\u0004\u0002\u0006�}�G3:�\u0005�� �\b\u0004\u0002\u0004(\u0018�#\u000e�\u0003�˻�A>���\u0004\u0002\u0001\u0002\u001eH!�=$��A:\u0001�@ c��\u0011�[@�@�\u0002\u0001�@ \u0010\b\u0004\u0011��փS�?���?�F1��m�\u0002\u0001�@ �\u001c:\u001e�~h&n� z\u0001\u0003\\.,���u�\u000b��- \u0010\b+��\u0003�����m\u0010֭MKż�\u0016\u000b��c�9\u001f%T3@�G\u001c�;S�p�F�\u0012n@\u0003k���v�j�i��\"�\u0018�\bc��r�5\u001b��\u0019)�\u0001\u0013C4d�������~j�����|� n���\u0014Ę�ŏ�돃�\u000b\b�@ ��h.7�`�@ ��_҂\u0019����{i ��- \u0010\b\u0004\u0011��m!\u0003a�Kmt\u0012��*\u0001\u0002[{����$/�>�\u0016\u0010\b\u0004\u0002\be�� ��A#[�Y\u0003�\b\u0004\f{u� d1u\"׺ �\b\u0004\u0011���Dج�WA/}�H�@ �Xz� �d\u0012�i r\u0001�\u0017E���怂.��ot\u0013 \u0010\b\u0004\u0015�=�/�>�\u0013�b�P\b\u0010��\u0010TG�sA3F�d\u000e@ \u0010\b1m�퍠mw\\�B�\u001d�i\u001am%$5yg�Ψ��lF\u0001\u0018\u0007��P�\u0007lobz�s�\u0017��\u0003WN;�0�����1�彣��7P7\\�Eӱ��Zo%\u001cN)��,\u0004 ��-q5�� �Q̷�\u0003�4�\\4�oE�ȥ�1CiV����*�#��!+�m�|r����AtR��� \u0010,WJ�{�3;�.��%�:K��źK���}\u0001Q�%�*Y\u0005]h\u0005�\u0005FB\f~)�Y> �)�v�|*�ve�8[�\u0001\u001d�~#��*\u0015����vB�ݳ�%F]h9e��\u0018\b\u0004\u0002\u0001�@ \u0010����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010T�[��҂�\u0005@ \u0010\b!���\u0010�jy����\b�?���ꑮ����sl��>p\\Z��q� �E\u0007#`�\u0010E4�\u0002\u0002���\u0005\u0004�\u0004\u0002\u0001���h-���\u000e��P\b\u0004\u0002����sO�\u0005��@ \u0010\b\u0018�i\b\"�N��B\u000b\b\u0004\u0002\u0001� �:�˼-�@�]��@���r\u0001�AZyz��hݨ]\u0003�\b\u0004\u0002\u0004�(*6_���Aq�@ \u0010\b*���\u0017\u000f\u0002G�PX\u001e\b\u001c�@ \u0010C;��� \f�cPN�@ \u0010\" �\u0012h!\u0005���P*\u0001� \u0006 ���' �\u0003ڂ�ؠz\u0001�@��PT��S�Y�FHq:ib�����E�S��or�K��_k؎�Ƭ��1:�y����\b˙��ve���\\5�\u00126H%/��\u001b��ŧ�w�ܣ�Y͜�O3\u0017\u001a��_g\u0015�\u000b�F�Z�i\u0016�B\b���E:���U�sO\f^+YOCI,�\u000f ��qqw ִ�_�\u0005\\���0P�ՃJ� �G/�k\u0014��B�Y\u001f)x\u0001�3L \u0006�\u000bi��`��&�ݽ�8F�R�g\u0018���\u0005�i?��s*�b�L\u0016��H9�\u0011�� �\f~)�Y> �)�v�|*�ve�8[�\u0001\u001d�~#��*\u0015����vB�ݳ�%F]h9e��\u0018\b\u0004\u0002\u0001�@ \u0010����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010R�+H��(. T\u0002\u0001�\u0019y-� ���z?@�C�����\u001a�_5���6��#�\u0005ū[�\u001a�\u000ejdPr6\b\u0004\u0010N�A\u0001L�\u0011���A:\u0001�@ �R�e�\u001f� �\u0006���\u0004\u0002\u0001PQ�YJ\u000b��@ \u0010G#n\u0010CN�\u0005�- \u0010\b\u0004\u0002 �l�\"p�{P>\u0006�e�H�k�r\u0001�AR�2�x���\u0004�\u0004\u0002\u0001\u0002w����ZO�\u0005�\u0002\u0001�@ �M\u001e�� �e�\"��\u000b @�*@\u0006��\u000f҂�@ \u0010!�\fuk;q\u001f�ڃ$�@ \u0010G/�P6\u00104���@���\u0005f\u0001�;�� ��@ \u0010C(\u0004���\u0006�\u001c�@ �P\u000bwA\u001d7�񠰀@ `�PTc���Ao�=�AR�w5\u0005���\u0007 \u0010\u0005\u0003\\\u0001\u001b��\u00106�A2\u0001�AQ�u�ۻ�Ads(\u001c�@sAZ�\u000b �9\u0004 �@ \u0010ch����J��\u0018��|� Z\u0003����y��[Zb�m\u0007�FE�++r�H\u000f�q�\u001bl�\u0014��䵧{u��\u000e֩�p�oL'\u000f� �l0��co`�a����\u0016��S��v4E0�\u0016�ܡY9\u0001d`#!\u0006?\u0015���\u0005��5��o�[��]��\u0002;��G�hT+7���\u001d��wm\u0001�P�Ճ��\u0016NR0\u0010\b\u0004\u0002\u0001�@ ����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010D�\u001a�@A\"\u0005@ \u0010\b ��-�Ci��ޏ�\u000e�1\u000f�%?��]+��\u001d��Y=\u0006|ตkw#RA�L�\u000eF�\u0002\u0014\b�\u0006�� \u001b��p��\u0004\u0002\u0001�7�0�����@ \u0010\b\"뛯Gz P\b\u0004\u0002\u0001\u0003C��\u0019 ��\u0007�\u0010\b\u0004\u0002\u0006����j\u0017@�\u0002\u0001�A\u0014���~�\u0012_{ T\u0002\u0001��b��x/-�A*\u0001�@ \u0010FǇ�\u000e�=��\u0002�\u0010\b\u0004\u0011����\u0005k���\u000f@ \u0010\b\u0010��\b�x�Ġ��@ B��\u0019ֵ� �(%��t\u000e@ \u0010\b\u0010�A\u001cn\u0007d\u0012�\u0010\b\u0004\u0002\u0006����\u0004\u0003\u001e$h#�\u0003�\b\u0004\u0002\u0001����\u0010!}��w���R�q�,A�i򈔾�s \u0001����8:�\u001e:�`���;��n�i���qh\"�\"���\u001a�\u0010p� ��K� v�A@���(\u001c���.{PX\u001b T .�q@��\u0005\u0002��\u0011�-\b\u001e�@�� �B\u0018D��#�A =���� ��\u000b��f�j\u0007�r\u0003_��z\u0001�\b#��d��b% C\u001d$e�$\u001f\u001f�mg\u001a,UKCg�H�vm��� d�� ����9��m��h&��E�\u0003ӻ�lht��ӣ���`=\u0017\\ʵ�\u0016z��wh����j�\"\b5nVgR.3&�\u0006��\u0012����j�����S�\u0010�WM\u0019|\u0018C#�\u001db��8��s\u000f+��\u0016�䖸�m��W�Q\u0011�>~W\"�����\u001dSt�m޹UN.�4��_��[\u001e�.�P\"\fv.��II�c��{\u001c��y�k�\u0015�����w\u0011?-|�Y�F\u0015OGd*]�E��\\��X8(�9H�@ \u0010\b\u0004\u0002\u0001�����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010T��w�\u0005�\u0002\u0001�A^��ޔ6��z���\u0001�\u001f�%?��]+��\u001d��Y=\u000e|ตkw#RA�L�\u000eF��)|� O�c��A2\u0001�@ �?����\u0005�\u0002\u0001�@�\u0005/�g��At T\u0002\u0001�6s>�\u0011Sr>�\u0016P\b\u0004\u0002\u0001\u00045\u001e�\u001e�\u000b\u0017��T\u0002\u0001�AJ���\u0010Z�?�\u0003�\b\u0004\u0002\u0004�AY���WЂ�\u0001�@ \u0010U���Y��\u0016�\b\u0004\u0002\u0001\u0005z�5\u0003��\u0002 P\b\u0004\u0002\u0006�� �^h���\u000bH\u0004\u0002\u0001\u0002\u0014\fAM���P��� \u0010\b\u0004\u0002\u0004�\u0013K+��\b�����́�����̂�=D��q���#Q3㶑�Ђ����� �#�\u0006�1���]�v�\u0014\u0016*���C�\b\"���v��2\u0012H��\\9���sx|�- \\�\u001eh1Ϊ�qm�H.\u0007���d\u0019R��\u00068UH�[��d\u000b�X\u000f}�c�W8u�v� �u����d\u0018�Y5�����z�8X� ������[�WH�\\7� �UL�4|�,�g���AV��V���Ђ��\u0017\u001e�\u0016����D��\u001e���\u0014s>@K�PGQQ$n��H%�y^�\\7� l��Ӱ��$\u00152��\b��V|���7��\u0005�f{��\u0005+���A=\"\u0003Eż\u0010Fʙ\\�\b؟\u0004\u0016�Ѷ�\b(�U[��f��������IW#�\\-��\u0019\\4�r��Ջ|�5tF �m~|��j$g!�.E3��\\`O+���R#^��d\u0018�WQ��v�\u0014\u0017��e۹AJ\u001a��{M��G��;\u000e������%b�\u001b�V�-U�8\u000b�t{��C`\u000f�zM��B�O}�>x)X7-9�,؍��r+��R�(�\u001c�` \u0010\b\u0004\u0002\u0001�A����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010U`-y����@ \u0010\b �\\-�Ci��ޏ�\u0003\b�-���H�J��7Gl��OA�8.%ZݸԐsS\"���@ �[�d\u000b\b�A*\u0001�@ �8���� ��@ \u0010\b\u0004\u0014��v��\u0017\u0010\b\u0004\u0002\u0001\u0003\b��\u0003\"\u0001�� �\b\u0004\u0002\u0001\u00045\u001b�P,>j P\b\u0004\u0002\u0001\u0005Z�� ۸���\u0004\u0002\u0001\u0002\u0014\u0010ZϿyAa�@ \u0010\b*�\b{�g��Z@ \u0010\b\u0004\u0011���ad\u0012 \u0010\b\u0004\bE�E+C�(%n� T\u0002\u0001�,\u0010S���\u0011��h.�\u0010\b\u0004\u0002\u0004Ӎ3�87�54t���a����]�s.���S\u0019'`\u0002�8�\u0013c�E�@�9 T\u0003�\u0007 �l�)�^\u0011=\u0005C��L\u001a\u000b� 8Y�p���s\u0002�\u0007e=�sEQ1Ăڜa��u�L;����i�\u000f/6��v�J�\\vc9���\u000eB�\\��wO?��Ѣ�q\u000b7��:���(;�R%R5\b\u0004\u0002\u0001�@ \u0010����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010A���A�\u0003�nPH�@ \u0010A1�[҆�SϽ\u001f\\\u000e\u0011�[��ꑮ����sl��>p\\J��q� �E\u0007#`��� ��kE�$\u0012��6@�\u0002\u0001�A\u0014�kH�\u00078�6@�\u0002\u0001�\u001d�\u0004=c5i��&o$ �@ \u00100�M�EO#e�i���4\u0016\u0010\b\u0004\u0002\u0001\u0004S9�a.���7\b\u0006^۠z\u0001�A]�5�q� A���@�\u0002\u0001� �ä�9�\u0016P\b\u0004\u0002\u0001�\u0018��\u0013n����\u001e�@ \u0010G+�\u001b�\u0004��\u000b�\u0012�\u0010\b\u0004\bM�A4��\\�\u0013��@�\u0004\u0002\u0001\u0002\\ ��i�M���t\u0012>��\u0003�\b\u0004\u0002\u0004t\u0019}!�h\u001d� �8��30��\u0011�2��\u001bq�@ \u0010\b!ak�m܂A̠r\u0001\u0002\u0014\u0011�Z,J P\b\u0004\u0002\u0004q\u0012y k\b\"�\u0007�\u0010\b\u0004\u0011��$ۻ� \u0010\b\u0004\f}�$\f��in������@ d�i@�[H� \u0010\b\u0004 ނ6\u0016�\u0011ނT\u0002\u0001�7��.���\u0015�@ѵ\u0016�\u001d($@ \u0006߹\u0005F�ݐ['�\u0003�\b\u0004\u0011=�\u0004]\u0003�� r\u0001P!@�\u0016��\u0007�\u0010\b\u0004\u0011�7Q\u001d�\u001ft �@ �{i�\u0004��\u0005@ \u0010\b\"����r@��\u0006�\u001c�Aq���\u0014p�Jh�Ta�5��vH� ��Z\u000e�_�;s6��XZ�\u001a\u0013�sß/iț-\u001a���M\u0005Sj��F\u001dM{C�klE�;*\u0015ӄ�1^� \u001b`��RS\u001818�L\u0018|N����\u0006=�6�d6��nl\u0002�ʌzZ[W�4� �9k1�g2��bs5�o��N�\"i�m�5�;�~ �];�Z4���h/\"|_��T\u0002\u0001���AB?}?�� Ƞ\u0010\b\u0004\u0002\u0001\u0005*o9�\b�J\u000bM�P=�@ �W� u7���\u0001�@��AB��\u001e��\u0006@ T\u0002\u0001\u0002\u0014\fAO�Z\u001f\u0003��^(\u001c�@ \u00105܊ T���\u0017�\b\u0004\u0002\u0001\u0005I}�OR\u0007R{�PY@ \u0010\b\u0004\u0002\u0001�A\u001b��h5>#�\f�#��c\u001f̬)\u0006�@ \u0010\b\u0004\u0002 �y���(,� �@�� �\u001c����\u000bh\u0004\u0002\u0001\u0004r���\u0014\f�܇�\u0004�\u0004\u0002\u0004AV\u000f>O_҂�@�\u0004 �AN�Ώ�\u0005�\u0002\u0001�9|���\u0004�\u0004\u0002\u0006��Ă�=�ޯ�\u0005�\u0002\u0001���s\u001f\u0004\u0016\u0007$ �@ k� �K�Z\u000bH\u0004\u0002\u00067�()��\u0010\\�$\u000f@ \u0010T��ڂ��P\b\u0004 $\u0010�y�ւ�\u0001�AU��}_B\u000b\u0003�@�\u0002\u0001\u0005z�H'\u001c�*\u0001�AN�ܾ4\u0017\u0010\b>{\u001f©��7�Tn�H\u0004X\u001f4�\u000e`��[X��;�\u0011\u0015�95'\u0001s4�� QAP�$�*]��rK�4ln�{����\u0012=+�|�И��1��u�Jz[�;��L��Ք5'\u0016�N� ���i1 \"�i�a,�D��d&Āyn��KW�5ݵƢ�jv&2Xj�*���� \u0010\b\u0004\b�&1f\fԱ\u00143�A[D`�\u0001d=a��@ \u0010\b\u0004\u0002\u0001\u0007����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010T\u0011\u0011&�\u0016\u001a'ҁ�\u0004\u0002\u0001\u0004\u0013\u000b��(m5\"&\u000f\u001e\u0016��Y$\u000eh\u001c�@ \u0010!� �\u000bs�\u000b-7\b\u0015�@ �F\u0002�zP, ��\u0007r �\b\u0004\u0002\u0001�@ \u0010F��Z O��C3\b��\u0018��+ A�\u0010\b\u0004\u0002\u0001\u0002\u001eH*BH�ޒ��N� r\u0001\u0003\\��E�\u0001\u0005��\u0002�\u0010\b#��w������\u0002��\u0001� pj\u000fu�2��\u0001��b��Yq�0\u0006ښ~t\u0019\u0014\u0002\u0001�9|(��\u0010X\u0006�\u0002�\u00104�� �.�o��\u0001�AV��m�M\u0019$n�D\u0002\u0001\u00043��n�\u001bL�-Aa�A\u0018ؠ�D\u0003��N����AV�\u001d��A4N.h� \u0010\b\u0004\u0010��\u0006�\u001bMp�\u0014\u0016\u0010\b\u0004\u0002 M.�I��\\\u0003�\u0002�\u00105���+�\u0013d\u0016G$ �@ \u0010Q�.\f�Ar��@�\u0018��4\u0013����\u001c�^pD�L������\u0006\b#`\u0010�m:�i��\u00033�\u001av��ې\u001b�\f||Yc�Dj��9w8V�\u001e��\u0011�g�-��7y�\u0016\u001c�+��N�3�L�я7��\u001avO���4�\u000e\u0014�j\u0007��\u0004\u0012�\u0010\b\u0004 $\u0015�����\u0016�\b\u0004\u0002\u0004(\u0018����>\u0007���P9�@ C�\u0005Z=�-�\u0010\b\u0004\u0002\be�\u001d�(\u0012��ڂt\u0002\u0001�@ \u0010\b\u0004\u0011��փS�?���?�F1��m�\u0002\u0001�@ ���z��\u0017c�P=��y�\u0005 �5���\u0006I�@ ��ܟ�O�\u0004T��=H,��\u0007 \u0010\" �y�z��\u0016�\u0005@ ���1�~t_ �҃(�@ \u0010AQ�\u0014\u0011�y�-\u000eH\u0015��d��>�w��A�@ \u0010\b)�y���i\u0004���%@ \u0010E/�� ?4��\\@ \u00101�qAM���� z\u0001��W��\u0016��\u000f@ \u0010S��\\���/�\u0005�\u0002\u0001�\u001c�};�� ��e\u0003�\b\u001a�H*U�Aj?4 z\u0001�AG\u000f�\u0001��P]�@�\"i�QӜ���7��\u0019�q׌㿍���\u0018Y��]U�jr#�\u000blpm\u0004����2�a�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�A����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010B\"�� y\u0014 �@ \u0010|�j��9^��\u0015r267H�����a�\u0010>un��m'/�9כm\f�~��p��:�\bj�t��|�K� ���q&�\u000e���f0�T����kuŚ]�j�\u000f\u000b,�r\u0001\u0002 G�P�\u0004��\u001a\u0005��\u0004\u0002\u0001�)#�F��J�@ \u0010\b\u0004\u0010u?\\���A:\u0001�@ @,�(�꯽�d\u0002\u0001�A\u0014���-���F� \u0004 �@ \u0010\b!�\u001e����$��@�\u0002\u0001�\u0010F\"�˯�\u0004�\u0004\u0002\u0001�A\u0014q�z��st\u0012\u0003p�P\b\u0004\u0002\b�f���\u001c��m�=�@ k��\u0004s3�^۠��@ B��\"\u0010�]w;���6@�\u0002\u0001�\u000e�\"�-\u0004��A(7\b\u0015�@ c��\u0011�[@�@�\u0002\u0001�@ \u0010\b\u0004\u0011��փS�?���?�F1��m�\u0002\u0001�@\u0014\u0014bi\u0012;�P\\h��\u001c�@�\u000b� �Q\u00196\u001f�z\u000b��@ �a�7\u000f\u0010}�!�k�\u0018i\u001d�,�`�P\b\u0004\u0015aik�~��\u0001�2,O�\u0005*����a�sO΃\"�@ \u0010E3u0��\u0006��\u0005�� T\u0002\u0006���T�76b�\u0011��\u0010]@ \u0010\b*�0��ޟ�A4`�� \u0010\b\u0004\u0010�};\u000b�e4f6����@ �ln�&�gj� {�D\u0002\u0001\u0005J�ˈ#�\u0005��\\Y\u0003�\b\u0004\u0015��-\u001d�\u0012��6X����@ \u0010Qln��$\u0017�\u0007 \u00105��V�ip\b,�X\u0004\u000e@ \u0010\b)�4� ��t\u0016��*\bv\u0017Q�:\u0011�3\u0018�o7�C0�U�7 �ya����\u0016�o\u000b�=j^\u000e�w�_��'�qS�ӥg4�ĹQ�t�p�%Q��A��b����>��T��;�XE\u0018���\u0011�65�߼�~����zUL���^o�ݪ�\"9!���\u0004��c\u00122Y\"0fd�\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001\u0007����\u0001�@ \u0010\b\u0004\u001a��ߤ�s��U}�D\u001f\u0003ѧ��\u000f�o�\"A�\u0004\u0002\u0001�@ \u0010S�[���Am\u0002�\u0010\b\u0004\u0018�V�\fF\u0013\u001c��6=���B��M\f� vZl\u0016\u0005��a�ς74��{�>.>%I]��Zl4__e\u0002x� \u0002�%@ D\u0011L�\u0001\u0001N�q���A2\u0001�@ ��\u0012��� ����HէsIYm��\u001b�Oχ�/�c�\u0012��\b\u000fχ�/�c�\u0012��\b\u000fχ�/�c�\u0012��\b\u000fχ�/�c�\u0012��\b$��� �Hdx���⒰r߾\u0005 t�ޚ�AK���7lh�\u0012G�*�!�\u0003�k\u0011�5\u001f���G��\u0002W~\u0001I\u0012�G���\u0017۱� ]�\u0005�N,\u000fχ�/�c�\u0012��\u000b,�χ�/�c�\u0012��\b%��\u0007 du��\\�)+n���e�zZw9��\f\u0017\u000e>KW����\u000f\u000f� l�/�M\u000b�_��C��+j\u000b g���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u0010M���2�6\u001csϑ�\u001ai+|�r�\u0002\f�}-�Q+\u0004�V\u000fh\u001f�V��ߺ}\b+~|>\u0011}�\u001f��߀@~|>\u0011}�\u001f��߀@~|>\u0011}�\u001f��߀@~|>\u0011}�\u001f��߀@�t��\u001b͆5r|)+n\u000b a�s�gL�7\u001bq,\u0001�o�U� �?������ �:`p��h8ջ#�%o�@P\u001f�\u000f�_n��%w�\u0010\u001f�\u000f�_n��%w�\u0010\u001f�\u000f�_n��%w�\u0010\u001f�\u000f�_n��%w�\u0010V�:Y��.��\u001b-\u0002fF�Еf�#P\u001e�|}\b/I������[��IY�P ��������\u0004��\u0002\u0003�������\u0004��\u0002\u0003�������\u0004��\u0002\u0003�������\u0004��\u0002 #�i�Y�\u0019&�▲ݝ�����t��,r�cgQi$\u001aJ�M�?�\u001e�\u0007���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u0007���\u0017۱� ]�\u0004\u000b\u0007Kn\u0013���l��${Ҭ[r\u000f�\u0011}�\u001f��߀@~|>\u0011}�\u001f��߀@~|>\u0011}�\u001f��߀@~|>\u0011}�\u001f��߀@���V��n2O!qIY�p�PI\u000fK^\u0013���cD�4|���\u0003s��P3�������\u0004��\u0002\u0003�������\u0004��\u0002\u0003�������\u0004��\u0002\u0003�������\u0004��\u0002 ��{�Y�\u001a�j��I[�p mOK\u001e\u0014UCgc%�|�X>KW�7_�� \u0006��\u0007\b��Ӎ��ozV�\u0001\u0003>\u001f\b�ݏ�J�� ?>\u001f\b�ݏ�J�� ?>\u001f\b�ݏ�J�� ?>\u001f\b�ݏ�J�� Oω�kA��\u0002W~\u0001\u0004T�.�M[!k1˺6�����\u001d��=���A~>��j���P�߬��w̃�\u001f��I�_�D\u0016�1\f����y�\b���2O�����\"\u0003�\u0010�?k�g���\u000f�C$����� �l��̔�:rp�kGc��s�/���A�\u001c\u0018�J��� S���>\u001bC,���\u0003[���m�x��A�?1\f����y�\b���2O�����\"\u0003�\u0010�?k�g���\u000f�C$����� �^\u0006�\u0001l>�?��?\b�F��7\f���\u0006\u001e\u001a\u001f���T�\u0007o!�?�\u000f{B �\u0017\u00042@` ����������O�C$����� ?1\f����y�\b���2O�����\"\u0003�\u0010�_k�g���\u001a8)�_����� ��K�f�\u00100�Q\u000e��&���}T�^�K��Y\u0007�0̗#t�\f\u0016��OQ?�{��l\u0007��g,V6��������WX8X���� ��\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@�nl�͜`¸�\u0016a�ql�\u0006\u001d4�S����E�\u0006�u/��9�'0�\u001a{\u001b�`��ݡ�qV7I���'^����}oվ�c�f��U�\u001d\u0001�s�\u001f�ܯ��à>�t��[��]W�t\f8�J \u001d�f�r@�\u001dW�t\u001a�?�k�~E����rӝ__MC\u0017W�du��\u0006j�Cl�ɹ\u0017#��ؿW:Q�������:\u0003��J?��_���@}\\�G��+�����\u000f��(����U�\u001d\u0006\u000f2玓Yo ���ò�WM\u0004�\u000f 5eű0��G^\u0005�6\u0004�Oz\br�|�3��z\\F�\f�\"*�c���;L�P�\u0015\u0004\u0003c��\u0007�}\\�G��+�����\u000f��(����U�\u001d\u0001�s�\u001f�ܯ��ào�Δ@��^�o:���>2���%*3MN\u0004�3-yL4�Ě�\u001aI`��E��\u001bXw��~�t��[��]W�t\u0007�Δkr���\u000e���ҏ�nW�u_��\u001fW:Q�������:\u000f�ϜY�!�\u001f� �3-��X�\u0018l=_���T_Au�\u0005��:��\u001f�(>���J -�;+m��gw��\u0001�s�\u001f�ܯ��à>�t��[��]W�t\u0007�Δkr���\u000e�8�J'�Çef�r%Ֆ��� ����zLf�\"�\u0011��2Ȋ�(�f�V\u001d�F\u0007��PllE�;�����(����U�\u001d\u0001�s�\u001f�ܯ��à_��(����U�\u001d\u0004c\u001c�F�I�r��\u000f:������)8��Z�\u001e� n\u0019���\u0018׸�W��0�|��f;�z\u000f�8�J0��M����U��w��Δkr���\u000e���ҏ�nW�u_��\u001fW:Q�������:\u000f��9���[�g�a�f�O\f �j�ji����\u00106�#nw�����ۏ��7h�r����Ym���\u001d�s�\u001f�ܯ��à>�t��[��]W�t\u000b�s�\u001f�ܯ���àd��J\"��\u001b���ά�:\u000f��\u0016�HqC-R��n\u0019��MTd�&�ur�'\\ �\u0001�a���ޤ\u001f�s�\u001f�ܯ��à>�t��[��]W�t\u0007�Δkr���\u000e���ҏ�nW�u_��|>\u0015��x�e����rО�6>B|�I\u000fk\\4�($�8^� �����p��r���U�@}\\�G��+�����\u000f��(����U�\u001d\u0003_�t�ko�7+�����>#��T�%��\u001aLR�\f�O�9\u001b\u001b�^V]w\u001b �P�k�P}�qޔDm�ecm���ۙ\u0002�\\�G��+�����\u000f��(����U�\u001d\u0001�s�\u001f�ܯ��à>�t��[��]W�t\u001aZ�\u001f�̫I��f[\u0010T�C\u0004�V��nW�n\u0005C��0�s��lGc�(�\u001ba�[m���_��\u0007�Δkr���\u000e��\u0019�D?�nW�u_��)GLa$\u0018�J&�L���é*��Iꘒ�Y�Ek}M�����:�����-en5ҍ��S����\u000e���S���\\�G��+�����V���J?��_���@}\\�G��+�����\u000f��(����U�\u001d\u0001�s�\u001f�ܯ��à>�t��[��]W�t\u001bw�x�\u0016�*�Fu��!�Fޠ�b������\u0001�9��@Ak̸Fk�\u0015x]e=d\u0004�KM+%a6\u0006ژ\\/b\u000f>Dx�n\u0013�0lz�������j7�U\u0011�3$|/߳+ZIc�=�X�v�3F\fqc� �.\u0011u�K�3��n����\u0017�U��k���g\\�����X�$.���T *\"a�7y��9�COs�`{�\u000ft��\u0012�#/�g)� ���zz�]G8�\u001a�9�\u0014�\u00118��24���s��v7 :#��4�� ��(߮ �c�'~��4=��\u0004\u0014\u001f����\u0001�@ \u0010\b\u0004\u0002\b�#a�\u001e�\u001f9�`t8�u%L�}9pf����;���X�G�\u001fN�@ \u0010\b\u0004\u001f?\u0006\u000fGO��Y\u001b@�vE\u001b�\u0012!|�\u001b��Ȯ�O=���8��&�}\b$@ \u0010\b!����o'���@ n��\u0006\u00070`�Y��L>�]�鸰>k��0G6��\u0017�蠡�\u001ax�1�\u0002��\u0006�w\u000f\u0004\u0017�\u001aX��\b$@ \u0010\b>\u00176e:\f�%\u0019����\u001a��\u0003gB ��w�u��\b>�\u0013��>h'@ \u0010\b+���a�\u0016oƃ\u0001O�Q�\u00178�\u001f�\fF'�9;��\u001es�Y�¸��� �-�\u0011�\u000e�N�\u001f{�#\u001fp ۹����cnݬ~d\u000e�|\u000f̀�|\u000f́ $��?�A���\u0015QY�eY!asiqYg�ݣ��'\u0010�����n�\u001fr ��]��#�@�G��� G��� G���>3�Ԓ�YO\u0018���|�5Q�\\ �\u000b�ܝ�(>w�X]F\u0017�0�J�\u0016�(\u001aӻN��D\u0012�j��\u001f�\u0001��\u001f�\u0001��\u001f�\u00043�0\u0007��4���m\u0006�ɘUE6p�*\u001d\u0019k%���ݻ��+\\v\u0006��=h7-χ�\u0001��\u001f�\u0001��\u001f�\u0004N\u0004� ���h�\u000f2�˕���0SF^i�V\u0017Q#��-\u001b\f���mq�ܠ����~d\u0006��~d\u0006��~d\u0006��~d\u001a��l��H�h�٪j�2�\u0018�h�IN����r7A[��]N\u000f��*:���O��@�p{Q��\u001d�7�s�A�\u0001#k\u001f�\u0001��\u001f�\u0001��\u001f�\u0004\u00134��\u000b��܃C`�\u0005\\\u0007�@j>\u0007�@���\u0016?2\b\u001e���g\u001bo��\u001e((`d,���&6>N�M��1��t\u001e��C�\f��.�9�/e�\u0016Pa�ᕕ,�\"�ֿQ�u��\u0010\u000b��{l�,\u001ea�GJ\\�p�+�ŇK+ ���%��2�6�\\�op/{���\u001f����\u0001�@ \u0010\b\u0004\u0002\f~'U\u000e\u001fM-D�\fdl.s�=�\u0001$�w��Y�'�\u001f��g�C7m�q��璉��.s���K�$�'s�\u0007��s�\u0007 \u0010C9\u0001�.�}���i\u0006\u0013\u0014����J� |�G`؃�C��\u0013r\b\u0016\u0004��H9���/�7?񪮋6a�`������M�]\u00121�ĉ��s�\\�bk\\�\u001bq\u001bv�X:Y\u001b\u000b]w\u001d�wpAa�@ �7L�s�7�h� �[Ƽљ�\\[�u�6|#Dp�T���uG\u0015\u00055H�f>x����81�\u000ekZ��Ms�{ס�\u0013��0� ���*�\u001dV�\bd��WW,m�fs�\u0003KG7\u0011pma�\u000fS�\u0010\b\u0004\u001ao��Y\u0013\u001e�`k$��\u000f���c���$4�J��\u0003ȹ���k��p\u001c��_�>\u001f�+�](rN_ư��\u000b �la�\b�e��\f\u0012���6���,��8ld$X���ι���d��Y���\u0018+�on����\u0007��\"�6ˏ�\u000b �*�ef\u0005W]�Ա�s\u001f\u0014p�\u000bc�����A���������\u001db\u0018Fh�\u0014uscu���M\f׆^�1��|�\u001a���!ü\u0004\u0015�\u0007�l�zAq;5btB\u001c/\u001c���\u0013�п��a-��l{�f���\u001a��n�� )�pc��\u0010��f�� E���a����:jبg��{��\b�1\f,��w\u0012ưik�K��9��'�?���c�t��\\�e\u001b��|\u001a�v��7X\u001c$�i�Lny\u0012\u000e���}���\u001cۙ8}��3> �%��h&���JZ���K�{n�O��\u0003�1�\u001b9��[�}\u001e��A�-�I�\u0012Q���%hi�(��޶G�9�-c�\u000bad��@i\f�}N�����\u001c~�\u001cvu�\u0018���\u0015Dq�\u0014�c\b���g>\b����C]S+N����\u001d�\u0012H{��@ �_NgcH{�X���ui�e߼����9�\u0016�qm��\u0005�\u0002\u0004q�(0���RRO4 �(��h��\u0012��Z�-�ܠ������Y����Y\u001e�?\u0015�&s�\u0002$��������\u0013\u0003�l��7��� ���`\u001a\u001d��Ns�s�Z���\\\u001aL2��W�i%�_T\u001a�\u0018!�\u000e�ֵ�\u0003��(6�\u0001�A��E��s\u0019�`��x���2��Jj��:���e��嫙\u0005���\u0014\u001c���\u001a�q�쥕3vN�%���� �\f ���\u0016l��l 6\u0018�ë�� ��%��\u0004:��q*�g/aՕ��j'��ibu�\u001c�F�=���\\Kw�m�A�\b\u0004\u0002\u0001\u0007\u001f%\u000b���2�c$t4��\\*fg�\u0011�J&�=��ۻ�/̛ ��� ���.\u001aQd�J�xq�-\u0015�:F�Sӎ�Wu ����\u0019�:�x�^ ]P��\u001aL����Aͯ���tY7�� ��R0C\u0015Mqy��iՈK\u001bn��2-�p\u00074\u001f%���ϧDqUgzx��Pl���\b��Mtb2i%[6���\"B\u001fKp�\u0007�H\f\u000e�R�\u00156����1� \u0016\u0006�`6\u0003n[rA�@ k�- X\u001bmqq�l��_�\u0018�kMP�\u001c��Q���0�����x|�\u0006��\u001cr� ql\u001e�\u0016�D1J�Z\b\\��\b��G��9����[q�9�\u0017&�t\u001c��w\u0006����\u000b�h���`c\u0018L͞J�멁�\\҂�$83]\u0014m\u001a�o�;{���Z\u0001�AR��,6�Z�o�&:G[sf�O�\u0010s\u0013�&��\u0015cY�f�*�3\u001e#]1��w\b���\u001b\u0002\u001cEﻉ��\u0001�^\u0007p�\\S�_\u0017p�����0��L��M\u001cm}t��8�\u0018\u001e�h[mn�@\u000b�e�\u0010J\u000fFtaΜE�g\u001fq�\u0010��Q�hۅ3\u0019�kk��$ qF��ˁ\u000f}�s�\u001d\u0011-�^�����Ď��o��k��\f�g\u0005�\u0014�?\u0012���������-�7:�[\u001aK�KKI1�Ip,>{\u001e�\\O�K�L�C�fڼѕ�Ukp��\u0015��U%C�\u0019\u001c�`\u0005�yZ�r\u0005�{KZDoA��u�,͈��Ιn|F�L.������8�\u001c��mOc9\u0007\u001dN�\u001c�|PV�\u0017�lі�I�\u0003�Ī)��OꯖSG!lS�p\u0002α������;��=-��\u0018��#+d|��\u001a\u0001�`��L�|4��/[Y�\u0012\u0004�E\fg�\u0004��#����\u0003�\\$p�5��4�柪4����`\u0007��h��\u0016��݅���N�-}�kȹ��ͽ\u001cr�^o%\u000b�(�w�\u0015T�1��o`��n�@:\u000e�@ \u0010\b\u0004\u0002\u0001\u0004R�[�\u00127\u0017��Aʎ;e�8��o\u0004ɘ���\f\u001e�\u001e-��Q$�b������t9�\u0010]blt�$ ��\u0016����H�ו�jX�p����q�S���\u0013�Xt\u000e\u0011�\u0018l`8���\u0012����\u000e���\u0013CG v����A��\u0004�̸\u000e!��U�E5],����H�\u0007�\u001b��XC�u0����7\u0002�sA�N��8�8a�\u001f?au��~/QM6\u0018d���ڦ��d\u0005��y��d m��6�\u0016\b; L�5w���;�g$\u0016�\b ��6߾�\u000e7�\u00072x��N\u001b�(f,s���*�q:�i�5��OU\u001bD4�̆\u0017\u0011�\b�k�8�^s\u001d\u0004X{�����6\u001aj`�3\u0003�v��u�;�:D\u0010\b\u0004\u001f%�r�\u0011�p�h�J |F����0�hߥ���$k�v�\u001c. �\u0004X؀��v�NAϼ��{�e��;:\u0005�Һn\u001a�+(3\u0019���fj��\u001b���\u0014�|P���;�V�\u000bu\u001e��Ol�A����xw�8���b3�2��P2J���Y���ڋ��r\u000b�M߰\u001b�\u001dM@ ��^'�Z{'��?�\u0007\u001c�\u000f�,K�}0f��,~�\u001a�L������\u000f,u|m\u0011�rJHie���N�\u0005�z�8����3�8�\u0011��ԒR��8�a�X�gUL\u001a\"t���d�b#kA���K�B�@l;\u001f��[#q*W�N����9�c&�Դ�\u0002\u0003�a�\u0006�-e�����{�\u0005J�\u0013���k\u0007Z��\u0013�� ��K�\u0003c�f�;;?5b\u0006��\u001d�\u001d\u001e\u0011y\u0004\u0004�\u0012���&b�\u0003���P�w��\u0001�N/�s\u0002�;r�a�ώ\\7/RTWK\u0004����p�Yu\u0016}p��d\u0012�\u0006F��H��Η\u0007�z\u0015�g2q�\u0018�V�*\\&H0�⦊\u001f%��\u0013��d2F\\�H�&��nu�8:���̑�-h�#Ͼ�~V����\u0002��$a�\u000e\"�lA�1��� 8K��ۤ�-[S����İ�JP���)������\\�.��\u0003v���n U���E���4�T�홮8a������-k|��B5�%@�.{�Ͻ��\u000e�t{�\u0019\u0017�\u0019:�\u0012�M�� ���2�1\u000b\u0003�3�%��� �\u0006�\\��{��h\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010hn\u0019t~�xO��Ncª�\u001a���\u000ew֚�D\u001a\u001b[�� �8#���\u000b��TK#�5L�c,��Gm�\u0012J�f�\u001b\u0005�{,m�\u0005�oX�Ǻ�>��\u0016\u0010\b\u001a��w���\u0010@�����'����A����l'��\u0014�N'wC1s@�&��;�\u001c���\u000fV�\u0006�\fk; \u001aZ;M\u001eh�ww��$�@ \u0010|w\u0010r^\u0013�l��`8�L���\u0007�J���H4�\u0016�A �����\u0016��\b\u000f\u001b�΃xVW�0��n�\u0010���d�u K�\u001f\u0005\u001bcp{\u0019\u0018��F\u0016��\".�\u000e��l,\u001e�k\u0019\u0013C9\u0001�������� �\b\u0004\u0002 K�n\u0016�^.�\u0015X6e����Q\u0010�'1���V�\\\fe��̌�&�O2.\u0010h� �H��}�E�cX���������\u001a�:�0׷���U8��@\u0004�\\\u001d�\u001e�;�~W��{���\u0010\b�P\b!���d���5�j\u001e1�� �\u001e\u001bM�b,l��RڂǂZ�1K\u0018�\u000f`��{��}�mQ\u0019ys\u000f.��\u0007d\u0017��\b\u0015�@ �>�W\u0016`��\\��k2�uz\u001dWM���G.��\u0013��j�`/dq�\u0016�\u001e�&䒃��)�_�\u001e_����MINds#\u001a�2[5e|/7a�\u0018n)NʪZ��\f�KȲF\u0016:��\u0012�\u0011v���n��yk��\\³\f\u0013�x�F%�a�N�����a����\u0004�> ����CT`ua�^E���\fmy��w+X\u000f��h&��[\u0003|\u0010L�A\f�M�y�q�\u001b��yӃ8NxϹo7հ\u001a��kD\u0017��1\b�!�\u0003�k��d\u001f\u0013�>�x\u001f\u001a�i�8�~\u0019�R��q\u001an��0Jbd�0�\u0006���b�8��@�$\u0010�\\\u0010��\u0007�l%��^��x����I#��L�J�A����,v:l\u0010n�\u0002\u0001\u0005x��O��\u0006��\\4�2�j�����#\u0010����\u000f�)�����\u0016�ش�N���٫��>m�\u000e[�\u0013\u0002j�\u0016U�\u0011\"Kڦ#\u0011�����ܵ��\u001c�o$\u0002\u0001�\u001b��\u0004�ų߫��\u0006����w�c���\u000f.�\u001d\u0015��V�@�\f���\u0013#do_��_�\u001d�����u��[��Ya}\u0005�è᥋3�0Ak#���,!c\u001a�-k\"\u0018���h�kZ�\u001a\u0002\u000fxPR�Ha�Yy��f���u��\\�\\I�I$��J\f�\u0001�Lqg��W\u00180\u0018�e�tL�N��Oh2Frc�yH{�/���c��4xV+L\u0007��*��d\u0012SԱ�2)\u0018b�'�3+�ۼ6�\u0006�\u0006��EjN\u0019�Q�qlf�\u001f���)\u001b]Z��>8�b\u001a�5T�h\u0016sA\u001f�;qw\u0007\u0007�P\b\u0004\u0002\b� ��'�#C�����pA\u0016 �\bA�oȖ��'\u000b�ܩT4�`\u0019���v\u0011��hΧ\u000b�]�F�ńŏ_��\u001c�.�G�I\u001f���� �\u001d���co�M�/+��d\u001a�\u0012�2t\u0014�\u001f�3Fh��%�\u0019�\u0018jc��`|���\u001b\u0017�3c\u0004�j|��ŽY\u0001�\\�\u0011��!R�H�\u0012�F\u001b�)*��Yw\u0012n3�b�\u0014�A\u0001|$\u0018��Z\u000b��.i\u0016��\u0003���A���\u0012��ҿ\u0012�\u0019����w2`PQ�_O\u000b�d3ӘAc�\u00017��� V�7@u��s'\u0018+�Dt��Nd�p:�\\�\u0014ح= md.��nm03�\"=�@���\u0013�n��g\"�և�x��\u001e�\u0019\u0004�����\u0017�\u0007�x͙��^@�8��\u0002:,2���ܭ\u0015;�nc��\u00072M���?�^� �G\u001c�׳D�����X險N��ߪ0�\u0003��\u000e��\u0012\u0001�@ \u0010\b\u0004\u0010���7$Y���~�\u001c��\u0019�\u001e\b��x�]\u001bY�U�\u0014X\u0004�H�c�I�IR�Ieݩ��N��nf�\b6OG�N��|[��L�$5XL���i'��\u0011�#�\u000f�G4�\u0007]����\u001b���t\u001e�c4?N�\u001b������@������\u0007i�zU�>i�'�N\u0002 ik�\u001eb�$l�� �b7����59$\\�'nς\u000b\b\u0004\u0014����8؏7��X\u0013�+��5G\u0017xU�\u001e(���0�x}\bl4�]Ds>�Gy+-,��9#� D彈��\u001d��ó�?4a9�\b��p�\u001b-%L\u0011O\u0003��5��F5�\u0007\u0006�\u000b\\\b\u0004\u0002\u0006�\b>�����1�-h.\u000e$\u0007������� ��\u0017�@�p�9RPg(b��u��0�U3d�)$\u0006�\u0006J\u001a�\u0018\u001a�䜑#Z4��V�pн\u0011ۂf>H^�*��($c��wY��6��{i\u0001���̃��>\u0003�F3\u0004�u ���9�2Y�R��\u001d�\u001b�E�/Ȅ\u001e�����;N�o`��\b\u001a��nw-\u0003�҃��,�I����|�S���E�E[X���� d.~�H�I�\u001b��ߙ\u0001_�\u001b?4\u000ev��>d\f����jA_���V�J�A%�\u001cз�_k�,ݘ��=\fp��\u0012aR镱K6%QM\u0004��6�LJ�J`\u0003��-0�=#Kl�,֍�z��A�.��e:��E�f�G}E�����\u0012d���'2\u001a����$\u0015\u001e{�e9:@\"���vpi~�\\7�1��Ōe\u0019�*��(�)\"��̗�gc�\u0016��#`�_����փ��`\u001dU@ k͚v�ܐs��A��+r�NѬ���\u000b�퍎$FȪu�k�./�xؠֽ0\u00197\u000e�����,r3\f�\\7\f����͊&��U+�Ղ�5�. �M�$\u0002\u001f_�:�N/��x����\u0016\u001c\f`pU�{c�F�IT�\f���\u001d#�ٺ{'�{�\u001d\"A @�g��AC\u001ba�\u000f�ky���(9�� \u0019�\u001f\u0012�0�OP o� \"�ZF��a6'��\u0017A�\u0004\u0002\u0006H��\u0010E�\u000eǿЃ�υ���d�������\u0003N���\u0004�ո��կ{\u001b�\u0007�:L�\u001b#�\u0007���_?f�0���SS�SӲwyES��+\u001e{3�F��n q�#�ݠ:o!d�!e�'��0Ta��:#1��D�\u0006�{#;\u0007F6`\u0002���\u0001�\u0010C+w\u000e��>�d\u001eR�[\u0013�2� ����\u0006�5�s����5ϰ�Co'-��!t\u0018\u000e1qև�x�Y�\u001dlg.b��\u0014S�T6GFٯD!k�Z���:���\u001a\u0005�7{EÃG�[�����FqƲH\u000e�\u001f�Dɥ����t�4�^�E\u001b\u001c��U�/�I�nu\u0007P�\b!���E�} ���9��ׇ\u0018�\u0013x\u0005�`t����YW k�\u0013��l���\f��h�/v��?4��s/s� 8��L*l�墜�M�{(扬���.��\u001e](\u001a�cA!�\u0003�5��\u0003�8DN��6��$�\u001a�\u0018m��W{�/����\f� �`��-��u\u0001p/oZ\u000f\u001aPO�t�|�\u0017FӁV�b�\u0016u�˃\u0016��i'��^�H>z��O\u000f�{�q�#8�Q�� �+$�'�V�Ԕ�^\u001c�Hp'\\\u0016��zr;��)�F�Z�r�\u0005��]@�\u0017�WY#`�����n�\u0016� 8���v�c�8:\u0006�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�A����\u0001�@ \u0010\b\u0004\u0002\b�\u0013\\����H�m��@���h\u001004�{M�NH\u001c\u0005�*\u0001�A\u0011��׷�m���~4\u000f\f\u0002ބ\u000e@ \u0010\b\u001aZ \u0007�~��#X�^��ܠz\u0001�@�\u0007\u000b\u001d�A\u001fR�\u0011���q�~~h\u001eX ��)@�6��@�\u0002\u0004 8X� \u0006\b�6\u0002ޭ��\u000e����)@�[I$���t\u000f@ \u0010\b\u001a�5��_��Ѹ@Ρ���}V����~7A#Z\u001a\u0002� T\u0002\u0001���m�-~v@�S�\u0018�Z� ��\u001b�PRuWR��$�Y��\u000e��߷��@�8i��� \u0001a��?�r\u0005t�N��\u0001�$\u000fZ �d�K�>���\u0003���j���\u0005�/~�H#��X;�v@�\u0012m�a\u0005\u001f,sIc����h \u0001���zPN�΀��@o��]\u0005�^d\u001a� \u001e@���J�����6;\u0014\u0015���\u0005��&�\u0003Q\u001d���|�\u0003$2H�,{A�bu\u000e}�\\�?\u0012\u000b�\u0004\u0002\u0006\u0018��r.B\u0004lMi��������\u001aH\u0019�������\u0013Z\u0002\f�\u0001�@ \u0010\b\u0004 sC�\u0007�m�� �\f��̭� '�p�a�pG0\u0006�h-�Ӥ��E��m̠�&�,�ì1�^\u0001����ls��z8Y\fA�7q�\u0018knN��r��t\u000fO;�>\u0016@�\u0002\u0004\"��>f�%`\u00148�ج4\u0014쭟O[Pؚ%~�\u00167S����Z �\u000b \u0010\b\"\u0010Fח���́�ܯ�n�\u0004\u001a�5p#�9�\u0013�����6���\u0006z�He��l۽�$�l.M�� \u0010l�z\u001azF5�F�5���� �`6\u001c�/\u0012{� ㍱45��\u0007�M\"�� �lב��{�}\u00067��WӿΊ�&Jøw'�9���\u0007�\u0004�y\u0017.�J\u0006P`�u5\u000537A�3o\u0001xo�kوcYs\f��c��4���B���ym��g\u0012\bh\u0016�\u0006��pj\f\u0006�*:\u0018#��\u0016\b�\u0016�1�h�k\u001a�5�x4�&�@ �R\"`����u�k�}����A�����3���W�_\u0007���N��0�Yev���T���٠4\\�\u001c�\u000f��9\u0013-�WR`�e%\f\u000fq{���8X\\CE�ckE�֎\\�\u0007pA�h\u0010�� �bX&\u001d��ث)���s^\u001b+\u001a�\u001c�\u0007��p;�5�\u0007�p\u0007�A�ˆ�K�xs0�{ ��)�:�\u0015U�U\u0019Ƨ4�8�W��q\\��͉��Iz�3bn�ՙ`���>'4��D��Ӭ=;��\"�Ō�E� wB*��[���#��\\�\u0001�\u001e\u001e�\u001bI�\u0006��pۙ��$����� a�!�m秘�k���AZg\u0007H\u001b��H�`Zo���d\u001e\u001b�KL��L�쥄dڼry(�\u000e��S����0\u001c�R�yS�Ǹ��X\u0012�\u001e\u0003t�ø���K��\fC.㱻�\u0014��\u001b�\u0016�,�,c��\\ޮ1 =W�#]`Ө���{lIu�pI����\u0007�\u0004��8�\u001c�, 7#`9�P@�k\u0001���O>�^޿B 9�\u001e6`�\b���x�s�\u0003�YMjq\u0019:�Ǹ\u0017u�` �\u000e�&���\u0005\u0019����&�(�\u0003�m\u0003𣈌X��2(��\fsc�?�����9�7\u001bo�\u000f��\u001f\u001b�:|�\u0006g�\u001f�fz\f�Zu� E,s5�5����R�㩴�i��\u0004�5�C�u|`8VZ����\u001f��ᑂ���)�\u001fX�E�9�5��Nۆ�� ��ftl9�6Çe���sH �o���\u0007\u00188f:G��0V`9�\u000e�����FA55;�ި��.e\u0013��ٚ;Wv����}\u0014�HUq\u001f/�Qf��Z���� ��i\"���&27I�p�5򇆇0\u001b�jf����RJ�\u001e�䫵�'\u0003녺C{\u0001��\u001d�0n/� \\''O���1\u001agM6%W4�>ϯ�\u0018����q���\u0003xn\u001d�n\u001a\u0001A��MĬ��^.�c\u0006�\u0006)�M���\u0016�`\u0013��V�������C\u001a����v\u0004^���W�UdOa�+\u001b�:#f�v� A��K�\u001cۖ�\\��B�H��q�z�%�����0顶�܃\u001c2\u0017\u0016��\u0001̄\u001a���g1d�?�9�,��]��\u001d4xl��D���t�Fٙ%e \u0011�i�. 9�\\�8�f����\\cg 1\\���1�lv:��*�A\u0013�u\u001d?��?\u001d�Cp�>\u001a�.��E\u0014\u0014����B،���Fhtu�˼��\u0004� \bs6\u0003Qh{o��*Ʋ�w�\u0013IF��塠��Ζ'\u0017�sb@� yphl�\u001b�\u001a㫽��\u0007��k���C����@؅N+��G��}�&'[3 ���d�$w\u0012F��=A\u0011��|w�PH�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b+N]m�@܎�G��xǥG\u00072�\u0019q,\u00121�y\u000eg�d�86��i�\u0003���t2G1��A��x\u0016u�@xΣ��x��ifBϲR�\u0014\u0018�.! \u0015e=#��ب_)�e\u0011��{�\u000b\u001f�7X��\u0006��pk~1㴭��9�+��W\u000f��1�IO���d�JlZ6i%�c�n\u0013��Ú󩽦�]\u0006��9�dI3��W)c�\u001e\u0017���R��B�Q\u001b\\)�>�$�Lu\\��\u001e��\u001d-���>:8snN�\"d\u001cτOM�a����\u000f�1>��%�_5E�o\u0015��c�\u001b�\u0007�xG�G%�g�y�\u0012�\"?T���\b�}D��\u0006AB�5���k\u001b��:�\u0004\u00074=��\u001f��|��l\u0002z����5�\f:q ���\u000fj\u000f����K0L�\u0011�'7��\u0004r?[��o�x\u001f�\u0006��k�%��\u001cԭ�8�!�F��]�\u0011cUN�͹v�]\u0019\"�H�Kor\u001fw��E���LwX�= \u0016�4�ޢ�Y7�i���rv\b!��G\u001e\u001ea�G7g�P�:���(}Sjt��\u0001Y�RH���I�����\u0018\u001e5m��¹�\u001dʏ��;�(`\u0018��^\u001bw|��\u0016\u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u001f����\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ ��:�=��f?P�{�'{��\u0004������\u0001�ө��p�v���B\b!�z��-��\u001c,\u000bu 5Xs�f��\u001e�J\u0006��*p5e�*��\u001b�R\u001a��f�u\u0003\u0011\u00029v-��b32�\fwd��r�z,q�:f\u001c�]�k0cO���J6��#N�\u000b\"q��\u0017y�q�v�q\u0003}�ؔ8�\u001c�S�xoQ���F�:z�k�#��41��\u001dY ut���\u000e�Α{���z%g\u001a\u001e\u0019a8 |�O�Y�K��32Y:��E=$}]�k���4�2ڵ���z\u000f1p\u0018�x��w�(�0�P89�Qy���� #�}C�-�\u000fPG#���A`�H�� \u000bH\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u000f����\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002 �f��~�\u0002\u000f��\u000f��\u000f0�\u0019P�A�01��j\b\u0001��X�Ϙ�\u000eL\u001d���lT\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ ����\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002 ��\u000f޹�\u0010K\u000bC\u0001h�\u000f�\u0004\u0013 \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001�@ \u0010\b\u0004\u0002\u0001\u0007�� endstream endobj 97 0 obj > stream H���[o�8\u0016���)���S4\f/\")\u0001A��ҝ�v��1��\u001d\fT[I4cK��4���Cʒe\u001f�X�E�F\u0011�\u0006\u00010�R���\u0015�+nʔCэ���T�ve�+�\u0017gRJ6\u000f�\u0002j\u001a����w\u0004ݭ�pmd_��)���6\u0014��|�X�X���f\u0003C巼)�\u000b�\byÚ3\b\u0003�Lq��w�����lS��uH��lZX/�P�E\u00148\u001c��4\u000e\u000e�Ip\u0011)\u0005\u000eKIp\u0011�\u00148,%�E���k�u?�΀�aO� \u0007�\u0013�;9:�\u0005S�\u000b�\" �l\u0006I+�\u001c����q��+�Mԁ\u001a�C���½�(=�v\u0002��D$\u0013��-M��ԄZh.�5��&�8l�Nۉq�\u0003E7Qm!�S�NT')W6�&�ѷ���Ϭg����95�P?�F��g�����\u001a�\u0017�U�C�Y5:\u0019�U�v\u0018J�Pk\u0005�O\u001c�w*��Djc4W��D��3~���)�\u001b�0ez\u0017\u0018�Lo\u0005Ô��`�2�)�S~�3\fS���a��\u001e1L��(�)ӻ�0er�\u0018fL�\u001b���h\u001eÔ�\u001dd�2��\fS���a��2L��U�)�[K?�dy��]3$�sv��F��vv����Ջ-ngi\u0018L�ch7��.l/K;��x`��&ߖu\u0005Ѫ�f���p쒧3)�0ֽx���\u0011\u0018\u0010F\u001d\u0005�pG�i�=)-xb\u0013fS('�A�Ϋ�h}b+�A)���+a ��c$���c �Fe�mk\u0006Oi�\u0010\u0018�=�E\u0010�N�e\u001a�jO\u0006Ι8�u]�E�t&e��\b�nPY��\u0006ǡ��\u0013�F�J�A�wF\u001ct\u0019��΅�j�; �u/���#\u000f�^й��o�Esv.\u001d���{ZV�bS��|[�ǣF�oQ +�`���J\u0005P\u001e\u0005Х�� �]\u0006��C�L\u001f�4�Y׊oEUVw��=\u0014M[WѼ���\u0017��\u0019\u0011�UiT�!�#_S(���T$Q_�� @\u0011��su]�m\u0005�.ʪ�7U���'��:�/��h/\u001e[�k�\u001f�*�m�(f G~(f8����=_��_�\\w���]\u0002K+N6 \u0003����\u000b��1^($��&���%�I�� \u0004�9���\u0013Iq�9��\u0014�!�n\u000bh�\u0005�0\u0004\\�Ɇ!�P^�U�\\�!%\"\u000e�\u0007 #\u0011���{I6j\u0018��-�ث���\u0006�\f퍲I�\u001b!b�o���\u001b w��&�����FB�\u0010=b�&��E\u0003��Q��R�]�N(\u0005�����?ʫ`�m\u001b��5���3rG�\u0001\u0002 �$��\u001dǍ\u000f�q\u001b��\u0010_\u0010 �XS�JRvܯ�\u0003@Ҳ\u0004Z��`�\u0004�ط�o���p)H#~̀�f(\u0017���y� T����x��Hz`�e\u001c*EDl=���0H��\u0010^�\u0017��ϙ����� �\u0012ӈkT\u000e%�y�� uc&\u0019\u001f�P��c�X \\�̮��1\u0013X��r���y��� ��[E\u001cM�!�0��(F.�^5�Tv.)\u001ft\u000fӞ�iq�47�\u0012\u0014��Ҙ�}�,}]&|�N�U�ǱP�m��r@ō�f�%��=Tb�+b^�����svD�\u00193\u001a���X�j� �k]b��P\u000e���V-��\u0015�\u0007��L�Ѝ�\u000fUHH�0D��_�ő�[c_��b��F� ���v�s6\u0014��}:=O��������՗��\u0010�}vvF..?�ј��_n�3\u000e\u0012�}�\u001b�����4�\"ށ�y*�\u0011y����+�\u0003�3����R�(=�6�\u001eQ��fHks�\u0019\u0007�5B��`�����r��/���\u0016�:�z�/tO�M��5&\u00040��7�M� ]�;+ט �Y��`фضwZ�� \u0010�6Xoh���\u001e�+���Ӭ��U�� �� \u001a+�k���?qjِ�z�0�Y�\u0016EYg\u000eTu�\u000e�V\u0004K���*����̗V_��\u0015�\u0013\u001bܱA�ޙԎ�\u0003��n\u0001~�2�a��Weݐ�}~R�u˃���r�`B����G\u0013�����!e�\u000f�\u0012Q-�� i�}f�a\b\u001ej�\f(6�ل��gԓ�m��]p���=Vi�\u0017/��o�u_��� d���L�)MU��py~\u0012\u000b2\u000evC��\u001a�(L��H~�\u001f�y��c\u0001��gf]�\u000b6\u001a\u0003R��=#\u0003�mY1�??\"t@\u0010�S^�3�n���M��#��eB�\u0006���7#�jy�:IBӎ݅�܍��Nh\u001f�0:��\u001b%�\u001a%>%1\u0015A��焓J��P����R�K\u0019��\u0014���g\u0014� ��d\u001a\u0007�B��(n\u0016!G�Ĭ��\u0014X�~�+s��s�D!���$��\f4��2��F��+J.��\u001f����@81n\u001a&���\u0005\u0016ۍϝ\f\u0001�#aK\u0001���l��J�\u0001��>�t�2�.3�n5N\u0011L�������1� �\u000b�vB�� MI6�\u0017y�\u0003)�����F����n!RA���N��S�v��D楶iԶ��ߟ {�1m����I@\f �e���g�6mư��R�eݶ�0 \u0012'{\\y��\u001a^:�P��7u�j[iUt\u000e�t\b�6ܲ\u0007=3�?�xf�\b�e��e�\u001a�@���\f��@\u0015���P�i,3Kf�ht��rS�m==f3Cŭ���K #n���\u0014/$�u\u0016��\u0016z˿��p�\u000e���\u000bS�벰\u0007c��j�8�j�{@I`�񮄙���=˪���?\u0001\u0006(�j- endstream endobj 99 0 obj > stream H�bd`ab`dd�s�t�\bs�\u000e��M-�K-\u000f��M�\u000b\b�u��I� \u0001���!��C��,�蟃��˰_�5��{\u0015��Z� ?R�\u0018�\u0019\u0019Y;�&��$�T\u0016��%&�*h$k*�d�*����\u0005\u0015�� �\u0012K2��\u0014 r��\u0014\\\u0012K\u0012 *�\u0007\u0019�\u0010��S \u0012)V��\u0003�4��4�\u0005\u0012Fz �99 A��\u0019%� A�ũEe�)�n�`M\u0006 )�i\f\f\f��\fL��j������l���~d�f�./���'�\u000e�Y���x�#��Sv�\u001f�~��}�������ū��}��.�]�7�w\u0019�_�Dm���\u0013�˰M�9��i���k�֭�>�m��5�k���sD�y�9�mr�.�^�����uY��gE���6p��-�o�oM\f�����V��t�� �\u001f��\u0007��O�-7{ʬ�\u000b���k��\"?�jBf�TL[Fc�ܤ���j���把���f��> stream H�\\��j� \u0010��>�\u001cw\u000f���J�ݲ�C�д\u000f`t� ���\u001c��\u001d�e\u000b\u001dP\u0019��7|��p�:\u000fG�Ö�\u000f.\u001824�u�p�A/&��\u0005AV��;�}�O��9>���j�^���pM�\"�0�� W\u0007��� \f��8�oCB�\u0017�6\u000fO�+�*d} y�\u0019�U�\u0007�\u001b\u0011g����J\u001c\u001f�\u0014\u00130U��\u0015`c�q� endstream endobj 102 0 obj > streamUntitledendstream endobj 104 0 obj > /Width 1853 /BitsPerComponent 8 /Name /X /Height 751 /Intent /RelativeColorimetric /Filter /FlateDecode /Subtype /Image /Length 268111 /ColorSpace /DeviceRGB /Metadata 112 0 R /Type /XObject >> stream H��W�nU�\u0015�\u0011�\u0002� \u0012�\bRH\"~!RQD��Io�\u000f��\u0006��\u0003R��psq�\ba\u0004�k�:>���\u0003�1�;Csh\u000fM��k�}����{Tk���|��l�\u0001�6vww��94���}2��GGG�.��֖֟ �A�����{`82��)\u0014\f;�\f\u0016��_L�\u001b��m�Os�A_jڗ��x�-\u0003?;�:x`�8�w�. �}1\u001c_��\u0003Ġ̊\u001dʶߢr���'\u0011s���`�\"x��n�Ҹ����N�\u00120k%��/\u000e�-��aϐ3�(�\u0017F2�>34�Ő��e(L�\u0003\u0006�\u00056\f����\u0005�f\u000f��!�wvvt�\u001e���{�vn\u0003\u0018\u0014��\u001f~ �C�t��&�+�>\u0018v ��;�>C��r\u001aI�\u001a�:�)�4�~g'\u0013�\u0005��r��\u0014\u0002�fK\u0006�.-��Ϟ-������B�����3Y�m\u0003�a�\u000bף�H�c�\u0015HQ\u0018��r0��\u0016T�.ק�� )�d�C���������q�\u0014�\" S�`噡��4\u001a�2x�.��ڧ\u0016�a�^8/ ���K��\u0003%�\u0015/~VZ}\u0015�@J�Ŕ��`�$\u0016���d\u0012�T\u0016\b�*�BZ\u0003\u0017���Q�j�w��E��A�_ �̺��$��ФT���޿\u0007#r̊�\u0017;_\u0014\u0012�z&+�m�?�w�z4\u001aɝv\f޵\u0002) �_S\u000e\u0006мԂ*���4��!e�\fz����p2��x,�\u000b�E\u0001��{ `�}�����J���\u0018H麘�2\u0010̟Ă^4�LB��\u0002A_�QHk�BR�8�V��n��\bR1���a�YW��]��\u0019\u001b��j�����`D�Y��b�\u0007-�:�.#ӝ�\u0003\f^��~H\"\u0014>\u0018'\b�����RJE\u0011bssSk�E\u001d�4�oɥq��W؄�s��^O_x��,)�м:\u0013�\bi�\u000b�H\u001cyQ�\u0017e�����e����Bϋ\\\u0004��Įd\u001b\u0014�'���|�P�Ҁ\u0001\u0019J����0�f��LS�ȱ\\�\u0001N\bY`���B�d\u0013\u0015�o�h��熟�N�W�z\u0018f~�w���&�� 48&���\b�\u000e_;�Fb��������\f@D��!��k�4���\u00018s�ZC\u0019>�PA ��\u001c\u000e\u0013�f�1���bh\u0011\"��\u0001�Q]\u001e0�?rH\u0007������ښ\u000eh?�Q�\u0001�\u0019�]��Y\u00139����\u0014D\u0016y`l�t�\u0015�L)ۦ��\f\\�o�q�ѮakkK�wj ��T8��\u000bR�3\u001c�\u0016 B�4�b�3�H����0�f]J�\u0002J fE ��$&R;\u0006n��A�p^�|*��������BM)+Z�~ֈ\u001c'���IB�\u0017@�,���;\u0003�I��\u0017/\f�Z6Tݡ�a�P�\u0003�x2H\u0016\u0006�J�\u0012/~R\u0012�\u000f�%�,_��\u000f�\u001d \u0004c��K�\u0015 \u001f\u000e��c\u0017w*�\u0010�\\�\u0010)Cr��LsA'\u000b�\\�>�_U��\u0018P?�7է��í��y\u000b�>�)�󁰯aS\u0011��\u0007��u���l��y\u0012�h�\u0012\u0005�|�\u001a�\u000b{2��&�f,�n�(�y!9\u001f�5��V�TDJ�zS(y�N��|��x>����!ƖA\u0016+I�a���A�}2À���-�&����0vB�:E�&\u000b�����\u001a�Ыd?\u000e���^o\u0017/8�\u0010!�7i�8�P�\u0014����|E���� \u0012;\u0013\u0014���\u001d�WX7�\"|�\"\u0007�o�\u0001���*�\u0001\u0004�\u0010\u0017\u0003�!T�J�P��̫�ʤɓ�\u0018E%[����s�9�\u0007�g\u0012��]01�B(��u؂�\u0014��3d�!Q\f�x�����܃.�y&aH\u001b����Q]�����v�R(�ϐ\u0019�D1��ʕ+�����ϟ�\u0011�2ό���3#\u0016\u0013�F�5\u0003.��a\u0011�\u001c~zW�.�\f��\u0002���&��,��\u001b\u001b\u001b\u001b\\� ����\u0019��0H�T(��\u0004\u0012\u0002��)�v�Ť\bx��\u0006*\b^�\b���+\u0015#�l��㉓�~��\u0016|\u000eog��_}b� \u000b�M-�7��yah�TYɴ$N\u0019��&\u001aq���]\u0004y\u0005�-���k (l\u001b��n�F��\u0006ɖI�\u001b�IwZ^^���E�\u0018���\u0002m�5b��밽��#$Q�YR�D�CC~^��b��ޅނ\u001dŅ?9՘M���傜@�=*��\u001eh�΁�2Ȁ��\u0007B\u0017�\u000f��x��#����\u0019Li��#v\u0002S6!\u000fdM+)��K��2 ���f% ���7,--�hh�7]5�80#��vBqT\\��i�Y�\u001dp1\u0001�Q���J\u001a�G h�Bg �\u0010فs\u0006c\bh�T�v\fv�� �%7Z\u0017\u0019|k�@zt$7��R�~���fӐ��Nur�����K ����Fo��8 ���2`�O\f?\u001a�_�\u001cKw}\u001e\u00032��JK��t�٫\u0007\u001c���2���\u001f�\u0015Ci\f!)v�yz��e�E�w��ҥK����cı�Ν;�1\\�z�w�\u001b7n���\u0007�Ç\u000f�m�\u00144���g���\u0003� �NL#�\u001f?���ѣG�\u0003�1ܾ}�o\u0006��0B�����\u000f�#Ã\u0007\u000f�d@{�g^Pc\b{^l�s�\u001d\u0014)eT&��)f��!��\u0016XW������Фp\u0018��t���\u001eXJ���� 7o����_\u0006U(���\u0016��ӧO�l�v��/\f�/_��ann'�����w��_\f}\u0014�h9k3Y*�%:�X^AD���[\u0003�¯ ׯ_g~�u��CÂA\u0014&�I�>u�s2��D)�߾}+�:)����z8L�֟�5�Jf��\u0007CRJA!���Z���̺~gjG���0}��\u001c\u000f� u�E�0W�Q\u000e�*tjA���>|���� �N��\u0001'�\u0018޼y�:��\u0007;\u001a�����\u0018�={�\u000f�\u0013�\u0012EӾ\u000b\u0016�5��Kâ\u0001��������/��,�3��4������4ڈ%A�(K�A�\u0006���$��l�� �x$�!\"۰�L,1�4a\u001ex���(\u001d�(�@2 \f�\u0016�&f��dּ�H�Zc)]����y�����\"':~�x\u00139v���2iii�#�B��E*;˺?�8�0� ����\u0004�Pʱ Ր�gY�J��� W�\u001ap�����.&�N�:\u0006��_��\u0012\u000b˻v�2�;$�T ����9�\b9-0)�\u000bO\u001a\u001b\u001bבS�\u00028��Q�����ӿ$���?$K�.�1ٿ�5�����o߾��sA=%DM�H\u0012^�1��� \u0016\u001e�\u001b�� ,ԓ��:Ň-[��&!�\u0003�\u0002���\u0002��8�b����7o�4�N�lHw��Һ3��YIjǠWt��B�\u0017I�[�a\u0013�_Q�\u001d��)���\u0012&��rڃ�\"\\Ǭ(��-�X\ft���\u0016֊ʆ�m�v�$�m�7��� ׯ_�y��g�&x�C�I��\\x``� RZZ������I�����i�����M\u000e\u001e�(Z�Ejw �\u0002��gT��Y�N��r\u0013\u0014c�\u0016\u0018��J���h������W��1V9Xm �!��\u0002��Ҵ\u0015\u000f�\u001f���\u0005 D�B XhO�}�:�F�%%%+���D֋c�İ��M��b��p�+�}O\fw��H\u000f\u0004�%\u0015^9=�f2�\u0012\u0002\u0003[N���\b\u0016e/���dÆ �ŋ\u0017k\u00078��U\u001b�\u0007�\u0011J��b�v��}��7�� �\u0019\u0018�\u001e�K:Q�zjKC{��\u000e)�R��\u00137���-k�x�d�[H\u001bfa!�ܸ�43O,��x�\u0017�Y*y��BZb[sdX��. �D㲍�\u0013\\r*��70+*,_�s�\u0002\u0005�6^\u0011k]U�|+�dɨ��6��� �8��5\u000e���.?���Q\u001c\u0018s�J��0�&\u0014j�\u0002.E\u000e�l��uvv�3�3�#��ԇ.X�`)���y�tww\u001f �\bZB\u001d�w��{\u0004q@�$? \u0013wRݛ�\u0003���\u0011�7_\"���\u0012�&����4��Ԕ�@aR�V�'.��\\�ϲ�ś*2� �}��m7\u0012r=ؿTTVV&/�'WE)��\u0010��qJ�@�\"ap5��m��� Z2\u0015���&Hu9���)�� A�o%6^]]��Hx�ꮪ��U\u0012�\u000e� &�܂�+\u000fFn���fy\u0010�l=y�`d\u001d\u0019\u001b\u001bӒ��ɝ\u0004��WHqq�~\"�i�\u001d��\u0015\u0015\u0015 5XXF�����\u0007\u0004�\"Q�h�y[\u0014r��!YBpD3\u0019\u001e\u001e�VgΜ�E�|Ll���Ai���)�¤&�θn/0����\u001egǎ\u001dO�P�IX\f�ma�\u0019�Z��-g�6c\\��K�\u0010\u001bDXS��\u0002��0�: Agj���/�خ�3��\u0013��F��l\u0012\u0019\u0001\u0006 �%%�\u0012� �,0&\u000b�@bƲ�h`�0Bf�sXcKx�\u0012���\u00140]�Ŗ\u0016��\u0015\u0006\u0005\u00114\u001d�bmm`E���l�\u0017�K6\u0013Ϯ\\Wε�紥`�l������9����/ם��Ο����?\u0011Z\u0015>���kB��yn�z���\u0013�Ox��^4ҒL?��I����.a�����bO�a��D��\u0006��?�LR %�\u0017\u0012�\u0011�q�\u0013\u0002\u001fйn辇\u0019f��]\u0011\u0010k �l���V����u�H,9��k�&n1TS\u0006�`�D�� .�Ү>�F�9U�\u0010��-���Ѯv\u0012\u0014G\u0015�0�%4Q���D:�0\"Դ�\u000e\u001d\u001addII�G�]\u001a�\u0012r�\u001e����g��Jii��X�p�+���G~L\u0006YQ1�0qe�T!x\u001fMU]]��7o�\u001c\u0006/V\u001f��*\u001e\u0013�� �8��d̘1�\u0012�\u000b�a\u001d����Q,�\u000e����)�\u001a$�y#d\u0019�\u000b�����f�NPbT���XKZ\u0005��q�\u0013�V���\u001c���\u0013G�Z�~\"\u001d�6��� Go.��k(�m��\"�\u0015��?�5H~� �D�ˢON��_۹��$N0y8\u0012�Q����7 ������+�\u0019#U�=��bJ�o(_u.�t\u0010�\u000f?\"�_�\u0002��j��`�mh�anNPL��ᚄ�u����̛7o!��̝;�Gd���/\u0010xl��\u0002\u0004�د�,X�\u001c$(������0P��\u000e �'�D~{/�����T^O �N&L�0��$д6�\u001e�ퟑ�����a����\u0016�8��M�6�����zC��ĉ\u0013o%p~�� �\u001d�`��Inn�;\u0004��J����ۻ���թ;K(x����c\u0004�Ss\"\u0006�\u0006V��}�TTT�$UUU�\u001e���h|ss��R|��:Q\u0015...��8� \"Q\u0015|*��j��Çu�\u0016-�T�\u001ay��>}ZK8Ta���(���q�e|8�F�,�\u0010�\"���\u001a\u001a\u001a��a��Hyy�3��8W��\u0012����\u000f����F���cb�\\:��$���Һ�e˖�\u0011�Y\u001b\u0013t��\u001b7*� \u000b\u000b���$�Sr\u0017Ӫz� �(�O\u0004\u001c�6r;�\u001b���� ��kf�J�\u0015��\b�;EG��g���W\u00138�v�N�\u0012\u0018s����\u001a�OJ zF��p�w��\u0004� w �,�\u0016����'��\u0004~.��-?H\u001e%\bsg�n��(��\u0011���\u0004�y\u0007���\u00151[(Q\u0010�rH�`� ������Z�cm\u0004�'\u000f\\G���޸���\u000eyKQQ�\u0016�o�\u000f\u0012Q�3\u0014��4!f^E��DM�@@=O���\u0004�Ulz��\u0013�z\u0012�2��~˖-Z\u000b��ˇ\\K\u000f\b��\u0014'�P���f'N���{]� r� �#����@��T��rJ��\u0011%����O$�/�Ka\u0002�t8��\u00047�D\u0017�\u0016�`\f�\u0006KH:·u/IZ\u0005t:8�rђ%K>\"8׷ R� �\\�ုt��\\E�\u0016�\\��%A�R%�'\u001c8p@��z\u0015hQ{�jt��\u001e��߯}N .\u0001F�\u001b���F�\u0013(\u0004����\u0019fΜ�lX�\u001ciY%lÆ �\"��l;3��L�WM}}�oQ\u0014h�)\u0016��y]�Z?@\u001b���������$H���>`� �R�� ��\"\u0007\u0011��l!�K�\u0012o�%٥- ���\u000b\u001bV����&zѵ\u0004s�@�@1눳y�\u001e=z7���ƍ�\u0003޼M\"S̙3G�\u0012#'\u0013����O��G��Bu���\u000b\b�%k�ѤK��D'\u0015\u0019�2 \u001f4��_%\u0012:SiMGx�\f\u0018If͚��\u0013�\u0013|���\"(�Q>�G�n�My\f\u0006Dq\u0014�0��.��U� ������C_B�P�%\u001aRUg$j��0LΜ9#F�� ��@+\u0019�M�J����' �������I�&=\u0004����\f\u001a ��B�\u000e�[CC�[\u0004�{����?Cz���\u0014;�D|����5�w���1���OM��ݭ�FkVF\u000e�P�w\u0012�OPXXx��p{N�O��\u0011\u0004\u001e��^\u0002�w\u001bQ����#??_\u0015�����Ջ\u0004��\b���6\\������\u0004\u000e���\u0013�\u0004ƨ����\u001a�OJ�Ν;�#���$c \u0002A�\u0014�Y�- 999O\u0012� �\\^�[~��#��x���`�2�\u001c�W7B�\u001f1b�\u0015��ҹ\b\u0013�d��\u0019�,���-�`,�FKi2a��![%�+���y�D�\f��\u0018PjK�\u0001 X=��>�z��I\u0005���*#�L�J�4[ �3R\u0015�r{f\u0014b��BM\u0004=�d|��eL{�P��hڼ��U5a���\u001d�k\u0017��-�E�Z�\u0016n�ӕ��\u001a��LF͞�\u0016R�& N9\u0003��\u0011*��\b�@� �)#\u001au�\u0001�=�,��ʋ�|�Ẍvg�\"1F��Ri��8\u0003Gf�\u001aY���͛q� \u001b6�\u0006����>mr�+V�\u0018b�N��\u0002�#�-f\u0013q1�={n2,}�嗧���T��1c\u0006�R#G\u0019��ty\u0013&L��ɢ��\u0005\u000b�\u0018�\u000bk��ĥ 5�IU���)\u001f�!�O���$�q\u0004�gd�Efڴi�\u0006��O��fT\u001c�|\u001e���ʛ�\u0018 �⨚��\u0019�\u0015�f����\u000bژm�P%�n5��D�\u001e���f�B3lذ,q�Q\u001ag�?\u0019=|Č\u001c9�a���M�RzO��ld�O\u0019%U>��4�J�>�\u0019�� ]��\u00069V��y����]F�6h'!ע�z�х�0e�NsAKs�\u0001ǻ�Ȍ�;��+����ƍc�rQ1w�ܔ�;M�\u001aEw�D������0\u001a�c�sŪ��\u001b\u0006H?�կ��*|[c~g\u0018�����R��\u0010�B�K\u0011� 3?��\u0013��\f7\u001b��M�FI\u0011�}��XT���h̷Ld\u0014���޽{�3C�v����\u0012�cǎe{z��a���ΔBP�\u0019m�%%���\u001bn��O�ϟ����\u001cm�QR��B\u0016N�\u0011�\u000f\u001f\u001ed� �P��|��\u0015t^l;g�\u001c�̜9�\u00149j�^o��\u001dfժU\u001f6Íλ�h\u000f_6\u0017�PB���(-��4�B��1J�x�\\\u001aY\u001eѮ��\u0010��\u00143r��gM��\u0012�1ԗQ�F\u0011h*4d\fEz�!��.]��(��n?5\u0004��ٳI���L�\f�gR��E�H��\u000eY\u0010�/s�\b�\f��?6r��F\u000e���|���%���'\u001c��aCUV\u001c!\u000f�b�\"ɶTh��q�v�]e�ر{Lvh\u0004ސ��JK���Dm\u0010\u0017LME�84���*'��WP�\"q+�S���\u0014e�����\u001d��v��q�h�\\�O�\u001a�L� ��t��\u0006\"�d��Y\u001b��ؔ٩*4�%��yL�>�S\u0012����أ�SfÆ T2*M'\u0019M�k~cB!���iR��msS%#T�$�j�Ӆ�ĸ���Wfo��&\u001a��ڒڇ�����#zUL\u001aO�:�E�LQ�㢂����P)-/�=��)sU��W��Q�E|���#�\u0003q�pUΊ��r!����Ԉ0�0��zӋ\u0014��(fV\u001aC\u0016\u0018� w��- �\u0006��ŵ3J\u000e񲳤1~h2�Uss��&Z�g�,��?֪����g�$܅N8���\u0006G/x����b�w���*Z$��\"�9o\u0018��{��48]�v�t��\f\u00192�-�\u0011 �t�*�V3��wKP���6J�d~A�,��\u0010�6/�U��1a.]��\u001ds���4#��\u001c\u0001�iQ\u0015�#��D��bre\u0002�.�\u001c������tHhmm��4��1m�С7u\u0004�\u0016����\u001eS\\�3\u0016�kK��FI\u001c�9v�\u0018�Ϩ\u001e\u001ed�.]J\"�7o\u001e\u0013�\u000f\u001f����Q�N �\u00025N�\u001c�\u0017�ҥ�猖P6t�ؑ�ԓ2���H��ٟ7Æ c�}�׍n��>h\u0014T����#Ԉ�'O�a\u0014 Y���x�h�چ�n'N��HMMMZ�\\�|�cF\u000b�bV�Z�%�a���l$��͝% �C�sF\u0001\u0010��\u0014�E\u0017�f8Ba\u0019\u0019�\u0019�\u0002�e��3�Yn4���߿�cf����N�:��M�6!�\u0012ӏ�\u0004\u0018j�Y__#\u000b�A\u0015Z�f� U.[h�A\u0005�*�\u000b>oƌ\u0019��F�B�}�,X��ӆ�JL��6�n���ķb� L����q]9�Ai�Gm\u0010ב\u0018Đ��xEȢ�Pv#\u0011�6�H�m>T�\u0012�FRw$��\u0013e�/\u001a\u0019\f���Z`�Ȱф��y�Y^I��e\u00132�t.\u0012~4��ܹ���\u0002e��7�\u001cj\u00126lll�$��j\u0002 ]�rV�Y�&U�\u0014�B�T5\u0011\u0012��$$�i\u000e#3f̈p� ��,Y��q)��w��A\"�y�̙�ͺu븑��g�F�x�[ E\u0015�R�$#�e\u0011ro��}�*#@��Ȫ8��=b$�\fS�\u0015�(�Am�\u0012�.�\u0013�P���+�r\\�E\u0004;|�0\u001a��2p�Y����,\u001er��\f!'a$�e�*��v��3�[�%�F\u0004\u000bt\u001cr�H什j'�-��.A\u00031��T�#.�����WMQ\u0001Gg\u000b�/&�\u001f1�FH\fz�l�\u001f%1\"\u0003�^�\u0015��u�M�e��H���o|�̴w�D��h���}B��n�\u0019%#�W��%LPr�T��ѽ�0Z�!AW�>nXE*L:\u0019m��!�ʒ+�\"�3,���GP\u0010��\u0003P�RE�\u001f���Ǎ�G�z���C��b�e˖t�I�S8\u0015���3��Ș�\u001eҳ��)���\u0012C�Xf\\V�y7���Z\u0014��d���y� C��\bqaN:\u00171�!|0~h�� ��R3��^�H���\u001eb��o�\u000ec7\u0014��\u0010 d�W��D�KKBi�Y\u0016�bf&a����̋�\u000b�\"��)D�0{L\u0017K\u000bc\u0013j7�\u001aZ����4t��̢��?i�+u�c����N\u0013\u0005?����ѹ������hN�>�LLe���T�\u0007\u000e\u001cx٨���!O��d�ȑ�L�\u0012���x�x�#&J���\"I\u0016h�\u0017&�)��s[[�\u001d�.\u0013~��\u001ct��Tꀙ�T��m�O���ښ}����U�\u001bsT\u0006k����\u0004��'�R����ѣx���*ި\u0003i���ۇ�\u0012 \u0016-Z�V�o����#F�I�Dx��Euu5#��\u0019Æ���d\u0004�\bM����ٺu+���h���F\u0010[#�\u0019��܁ĭN�C\u0015 ���F��LQ&> |�Q�f\u00134#�\bM�����f�Ie`Dڈ��\u0010�\fs����\"kiO�8�L��O�H�Z˧�FA\u000f�ꥺ\u001b-I���۷3ҤIm&\u000eEu�;w~�H'�\u0018��\u0012�\u00181\u0016���\u0010��\u0010TA�kB��h /�Z����1\u000fe(��:��dꦦ&��r��C��׮]˽tG �]�v���\f�s���\f�\u000e\u0010-1QN�fz�nݺ!vTAT�z��\u00063\u0010---��\u0002�u�x��H�!�\u0006ӣG\u000f\u0016J��f �2J@Y�s��VI\u0001J4o�\u001e�S�N/\u001a��E�\u001f?�C���'5���\u0006�^WW��)�8�G�����듧\"nj\u000e��\u0019!Q�\u0015Q��S��]�)�,⛶�8A�*;�6#F��b\u0015��g�����D��{�n:/�v`��Y�\u0010f���\u0004@]!���ϕ&b�&�\u0010 dNgʹiӲVE�a\\Y\u0003#�\u0005O�\b��\u001c���S�\"�,*Z�'�/�ʕ��p�\u0007�\u001c���l�2^P�rw�C\u0007��\u0015}2�~=��\u0011���\\��̔|5'�Vu�\u0003\u0007\u000e8�����T�͛7O��M�PX�O�\u001cJw�F��r�!,�,���\u001d��\u0017pN/B'h@���6���{���[�n=\u0011:]����\u001a�\u000f]T�Ȓ�\u0004�i�C݉��*^�Ӗ-[|YB�q��!�\u0019*���煾��7\u0002UOc�\f��{�M�g:�\u0011��w\u001c �9�b~\u0016�\u0016�\u0014���t ���\u0013� ~��)�U\u0013�=�\u001d+w����a���\u000e*��Y)q͚5f!\u001f���G��)k/�\u001bK)94&�jIX�z�\u001eK��\u0015�C�8��Z}�e��#G�� �~swl���O\u0019P�5��fזmO�\u0011��_u\u0007QW�V��q����5�糁-��\u0015�`Ȕ�bĈ���6\u0015\u000f;v��K`��LYx���_\u001fv����P+��֟T�\u001d�6��\u0001-��\u0014\b�w\u0002iĺL9�[���5P#�A�24H��\u000e%�ƍ\u001b\u0019�/�9%��\f��~'4��R�:2��B��cǎ�\u001c�\u0002Az,4_�c\u00138���{��ՁK>~_8x�r\u0017����N�\u0011�\u0013�\u0006\u0002顀�p��C��L�\u001b6h8�X�\u0017�6z��01��ڵ�dK�(}�L.��ZD�CZ�'\u0003��C�sabb���7&\u0003�2M]\u001d���\u0007Z6�\u0015�EQ�X�\u001do\f��n�j)W��$�pȨf � �p�k\u0003{��Z\u0013��q�H���e�)��ձ��ή��� U\u0013��L�\u0018ӕ�.]ڪ�\\�G�a�tE\u001d?�\u0012\u0016.\\�V�t�\u000f\u001f�\u0017(���\u001bi|�63�j���C�7e!Z���AKdM�O��'\u0002\u0012�\u0011��'�ЇS�̟?04��j2' �=$��|�F�����\u0016�?O�~H�*�\u0004�\u001d�C�T�\u001d�_\u001d�\u0016�\u001f.����\u0015:\u001enp}pAt��!u�͛77#�\u0018�5�+Cɀ\u0013'Nh4O�\u0019[]�\u0015\u0014��ʊ� �wB��x��*|.\u0018�F�3��/�e\u001b��Y���M�hX3\b�\u001c��Z�\u0005N++��VS���Q�ڬ��DV*Ss��f�\u0014�������~����{\u001d\u0003�yo��͊U4\u0017,X`U�z�0�\u0004��WC\u0005�@��\\,�P�\\\u001a�L�_\u0006#����\u0005,p����t�O�-�Ӡ�\u001a��%��\fMG�͚5�-���b�@\u001b��۷n\u0007>��3��ݔ�^O���\u0004�&e\"������TV�/���e �?��ΰ.�\"�\f���+S\u001d S�L�f� jlLt[��� \u001fFFF��s��}S�ЗLRz=\u001b�l\u000e'�]\u0019�c)T�q��.�Pf\u0014\u0012���\u0001�\u0016E�����\u0005��\u0004N5>�*���� d~\u001b\u0013�Uq)�k\u0002��\u0007\u001d�\u0013��L��3�����^�a\u001cπy���3g�B��I����� ��A��3\u0013)j �����iL�ڶm��L�\u0018����PI��^��Q��� MG�\u0012V�Ֆ�\u0013�\u001d�������^��j��VP\u001387\u0004]�+\u0003��\u000fC\u0019so ~>`%gbᇂ+�����\u0010��\u0013��Z��2f W\u0006�\u0019� =�AEM0��1�\u000fe���31׭��mcb��p&��\u00161\u0010\f\u001f�� \\�#T>�\u0001q����\u0015\u000e\u001c8�\b')x\u000bL�^��|\u0005f���ŋ��k�\u0003��M��ԟ�ޱ�1}�h�Qqz{h:�zc�rٵ�g͟?_YK�nJ��Ά�\u0007\u0002��\u0006\"��N8�S�\u00075��}M��ϟ�03f̰T\u0011�zQ۽���UO}YS\u0010\u0013�f���\u000b�+4x;�\u000bE��g)G�Ot�81հ��\b'i\u0017���߂Ӗ'#\u000e��:s�L[Ǿ,\u0011{�P[�T�ƍ��\u000eÄ�\u0005\u000e`VD\u001e�H��s��2��\f��ca�ҥo\b�_?�v`�/\u0002#�\u0005F�xG�={�qQ�\"ǎ\u001d�:�Eh��E.� ���h�'.�\u0014mbo�o��/q\u000bӲ�W�W4�IM�\u000b�\u0004Ɠ�>緪�ʂ�c��Ə\u001f�Dٿ�}�p���ʽ���]�\u0017;4�X޹s����pw\u0005 �H�C|)�I)f9c�El3�ě\u0012��f�ĉ\u0013�\u000b���ɓ �v��+:B���82U\u0013�2�����[��А%�lf��ģW�\u0013ڡ�s�{tt��Pɹ6\u001d\u0018\u0018xU�\u001e�sn��fw��?B�\u0011$���\u0003jǊ���\u000b\u001e\f�|0p� ��(F���\u0002$�6Dϴ%e___eT\u0013�ԩS�Hi��Њ�2;\u0017|$�\u0002ZT\u0013��\u001dA��5�B�0��EU�v����� � G8�K�[` �7n�+0�O�Z~�5��-� ��T捍��\u0003�q[� ��C�i�\b.�\u0010�_QmG�P1� �G\u0003mB`\u0011CI\u000b�r�\u0003��ґ��[π��\u0010��\u0004��栓p��t�\u0006\u000e���MU5\u001e�z�\u0012k���}A���˟�t\u0016\\|�`GC[d \u001d\u001e\u001e�츨ͦ\u0002�̮���z\u000bި\u001e0\u0016�̙3\u001c���gJa/��\"g˖-�\u0013�p���_�\u001e���C|)�8f�m�\u0002�\bsK]��.��\u001d�ϵ��\u001ek�\u0012���PD��СCʞ%K�諤���L.�1��krB˄�T\u001e {��U� \u000e\u000e��h�ݻw� �4?�,p*=��P��M̺$Ҷ�)���ܽ���qԵU�*S ��\u000e��\u000b*��סP\u001e[��\u001f��/[[�N_\u001f����epQ3���֭{m8\u0010H��k��\u0015�NZ��{��d�w�JzA3-�\u000e��%�T�\u0017ݦ�P\f3�L�6m�\u000e�孛���\u001b\u0004���~N 8��P��@`��@��� \u0007{*�F\u0016�� �����\u001efƌ\u0019�*�]/j�7����/k b�׬��~!t�\u0006og}��\u001a�,���\u0012'�\u001a��{�$�\u0002t���_��^�e\u001c�U�+\"�E\u0001\u0015�pc\u0014C�3\u001adE���?�\u0004�\"C\u000f[H��D�`%Ԃ:\u001a\u0005���\bȊ\u0002\u0006\u0015�!��\bp 1���b�|a/zS߮}���}�\u001dt�p+7�W�\u001cf�%�-2nk���\u000f�z�{p��K��rM�l���,���\u0006޴��Ww�P�.,|��X�+A��J8���\f��\u0013���ǟn\f$�w\u0005F�\u000b\u001cC_��,�\"o�r���g��q\u00053q�\u0004�Ao9z����j�S�3�$V��X��l�Bp- \u001bB�\u0017{9\u0013)���,��ZnΜ9uM�\u0019�yoP�a\u0019�\u001a~�e�\u000f~�\u0015�\u0019�IA1�DX��1Y�F��\u001e�A3��\u001f|���up�n��0�'(`�9� �\u0002@�M ;\u0003��8E��9��P9AaY�S�l/\u0006��\u0012K+V������8����]��@���߯��\u0016���@P8�'s�ڛ�\u001b,D��4�O\u0006�s�R�gF\u0007Ҕ���J ��K���ئ1b��z��y���c���\u001d�zs���yR��f�b�-0o5�R��ƻ�\u0015O������绒�o72�$l��\u000f�\u001d\u0019�]\u000f�\u0011f̘a\\Ի�����i\u0006���\u0017y�\u0017(�\u0017?x��M{����z\u0018��W��i��\u001b>|��@\u001a�/#Z�\u000e����8�}(*�h�\u0013�$+�\u000f�q�\u0007�g3I�7^\u0011\u0018�\u0012*�K��.X��NUr�ƍ\u001b=�*��u}`�\u0001Ul����ߪ�k\u0002��H`��\u0005���\u0002�\u0005�'�=��m�ۙa�d���AY�s�\b�L�撲\u001e��\\���\b7�\u000f�+|9к:�\u0003Y�� \u001bV�\fX�@�yI��7\u0006� �����\u00026�? �+Y�ڵk�\u001aqR.���%K�8���� \u0014b\u001b ��s,���I�J\u0007\u0007�+ ��fgΜQ�`��\"����ܹs���ӦM�C��\u0007-�w\u0002)�C\"��\u001491,���S�LJ�`\"��P\u0013����j�\b�G�����#�\u001d�>0yU��b�KÐA��\u000e�.\u000e�8_�5�^ܷ�0�\u000fZF\u0007�Koa͵\u0001+�\u0005�\u0002�@A��ye�Sg�Z�\u000f ��۷W\u0014x*�k\u0006�͛7�sΞ=�O �&u\u00130�j�\b�J�\u001d���\u0010�!�Un���\u0002(|���*{��szHR�����r��\u00031�L�s�έ\fyY���Dڝ��� �JR�- �c.B �Szl\u0012���р\u0010�$\f���\u0004JۧBsi2G���N��c�Zd9޹`\\�z�5S���?\b\u000e���\u0005\u000b�LO�===\u000e�a\u0014��'O���?\u001cH��Cөbe\u0013!5}6�r?\u0010�\u001d;�O;v��j\u001e{�֭�����O\u000f\u0005��ɓ'G\u0005����#�Ww1�\"�K�|�:u~�[��*��%\\\u0015p K\u000f\"Ǫ���\u0017��T\u0016u\b\u0005B\u0011[�Y�l��+W�Ko\u000b��#�f�\u0005y�\u000b�m7�@\u0013�\u000fs\u0003k~#�4\u001e�������w��G\u001fU\u001d .�%���� }}}o �\u001e��(�>}�]�����\u001f�ռ �~7����V\"\u0012u�u2`\u0004�*o�_ T�%�2 �w\u0005���k��� `ʔ)�\u000b]��'�Q�i\u0018\u0019yG0\u0004��\u0005�mi��r,\u0010�%!�\u0005$ty���g��lZ�*:\u0003*ڙ��c+�׬YSJ�s��7\u0006���\u000e\u0014���Q:��\u0002Σ��G{*�ɚ�����i�H$A�\u0014W����e��=ٷ.\b̴'�馛\\�\u0004Վ8����xދNGՇ)��3�,�\u001d�T\u0001��4�\u0017.\\X籽%/�\f�\u0005�\u0014�͛�ux8�SQ|}���ԩS�\u0002nl���e\u0010jA f�\u001a�۟, X�G��+w����\u0016\u000b\u0016,p�޽{˘ۃ��K�\u000eM�Gm�\u0019�7㋧�\f\u0018Sk�c�E\u0004\u001f֋�ێ�o�e��ݎ�8`�/�zP\u0012�3\u0015��F��af\u0007~��v�V�`$r��~���\u001b\u000657Rc�;���FE��\u000el��ak\u0018N �}Kp\u000bz��\u001d�{��R8m����Zϭ�\\�6\u0016���� u���%�=���K\u0019&������a�\u0011�z�2����\u0017yA3�E[\u0006\\���+�/z�\u001f\u0002��Hk:]�st@τ#G���m��V��X(�D�}\"XC�vX�|�o�\u0003\u000f7��\u0019j�-[���Pю���B����@-��W�zM\u000el�������x\"4��\u001f3f�E�i�\u0005@\u001fj^\u0012�}��,�eO30sn\u000e�\u0017/VlSw>\u0013n\u000f��F�v,\u000f\f\f��fΜ�5aݺu�bKh�`6#1^\bf\u001eҠ�����ի��&�ѣG�R�'f�\u0013�N��\u0005�>�O�#m;k�,_��T��I��嘣�U�5ɖ����{�\u0014���h�ĉmo_�h��BR���\f� \u00159�Y�c�-8�K᜺ >�E8F��6푁�Wt�l��k�{�\u0016)]H��\u0003s�\u0005��0a��p�m�� �}6;\u001b6l�V6h\b`]�\u001bn�",
    "commentLink": "https://news.ycombinator.com/item?id=41483434",
    "commentBody": "Sleep duration, chronotype, health and lifestyle factors affect cognition [pdf] (bmj.com)254 points by susam 21 hours agohidepastfavorite170 comments comfortabledoug 9 hours agoAll these weird comments about alcohol abstinence...must be a religious fundamentalist, must be an ex-alcoholic, etc. I have a drink maybe 5-10 times per year, and I'm not actively trying to abstain. It tastes gross, and I'm not drawn to it. Why is it so hard for some people to understand that alcohol isn't appealing to everyone? reply foobiekr 4 hours agoparentA lot of people confuse the secondary effects of alcohol (basically, mostly, social permission) with the primary effects. Most developed countries have purged almost all ritual from their cultures, which means that there aren't really occasions for people to experiment with their behavior - and it provides air cover for engaging in riskier behaviors - and so alcohol provides the outlet for them. Alcohol is one of the most boring psychoactive experiences there is, it's the safest, the most predictable and the most repeatable. There's nothing challenging about it - happy drunks are going to be happy drunks, angry/emotional drunks are going to be angry/emotional drunks, people who use the context of alcohol to excuse behaviors that they feel they would otherwise be judged for (promiscuity, \"I love you guys! no I really do!\", etc. etc.) are doing just that. The only drug with more predictability than alcohol is caffeine, with the common opiates being next in line for being absolutely predictable - do X get Y. Alcohol is boring. Even pot or tobacco are more interesting, but they lack the social context that provides behavioral permissibility which is really what drives the ritual-lite use - going drinking on the weekend. reply lostemptations5 1 hour agorootparentAlcohol is one of the most STANDARDIZED substances on the planet. 5% at 500 mil is always going to be the same amount of alcohol. Of course it's going to be predictable and socially acceptable. Try doing any other drug and you have no clue what you're getting, even if you get it from the the same source time and time again. The predictability is a FEATURE. And btw, it's not boring at all. You are conflating boring with predictable. reply taeric 3 hours agorootparentprevSo, I could get behind a lot of the idea you are pushing here. But, I question whether you have evidence to back it up? For one, to claim that most developed countries have purged ritual feels more like you are referring to some specific rituals. Or have amusing cuts on what you consider developed countries. You also sort of undercut yourself by noting that alcohol hits people in different, if repeatable ways; but you seem to think that will not be true for other drugs? From my experience, I would expect the same for pot. Tobacco, I confess I never really saw it impact anyone. Outside of getting them addicted. Simply stated, why do you think you would not see such variability of how other items impact people? reply coldtea 12 minutes agorootparent>For one, to claim that most developed countries have purged ritual feels more like you are referring to some specific rituals. They most likely mean all kinds of overt rituals societies used to have and some non-western societies still have. What kind of rituals do we have that you have in mind that we still have, and that are not either very peripheral to everyday life or have not been diminishing in importance and attendance year by year? reply Insanity 5 hours agoparentprevI feel like nowadays it is more socially acceptable to say you don’t drink than even just 10 years ago. I can’t recall the last time I was asked “why” after telling someone I don’t drink. All in all, I was fortunate that during university a close friend of mine also didn’t drink. Being the “odd one out” seems harder than being the “odd pair out” lol. edit: culture plays a role as well in how acceptable it is. I’m from a country that is heavy on alcohol usage though (Belgium). reply kenjackson 3 hours agorootparentIn the US it’s far more acceptable. I don’t drink and while I’ve been offered, no one ever pushes back. And I’ve never had problems being the only one at parties who didn’t. reply bradlys 3 hours agorootparentThis is YMMV. I still get a lot of “why don’t you drink?!! Come on, man!!” in the US. Especially in a city like NYC - you’re signing up to be a bit of a social outcast. reply siamese_puff 2 hours agorootparentI disassociate from allowing people like this into my life now. reply moralestapia 1 hour agorootparentGood for you. However, in society at large, social influence (excuse the bit of circularity) is an extremely powerful force driving people's lifestyle. reply mattgreenrocks 6 hours agoparentprevI’m in my early 40s. My ability to metabolize alcohol took a steep nosedive a few years ago. I love a great marg/old fashioned/IPA, but I will feel depressed the next day no matter what. I’ve tried electrolyte supplementation, eating a lot, eating “right,” drinking a lot of water, etc. My body has a hard time with it. So it’s rare that I seek it out. A small container of sake early in the evening might be my indulgence from now on. reply arichard123 5 hours agorootparentI had a similar thing until I stopped eating a certain brand of muesli. A different brand with seemingly the same ingredients was fine. I think it's something to do with processing of dried fruit. I believe there was some reaction between that and the alcohol I consumed later in the day. I only realised it was breakfast related on holiday and my breakfast habits changed. I found drinking to be consequence free as opposed to 1/2 a pint causing a certain headache the next afternoon. I experimented when I got home and completely solved my problem. I was also in my early 40s when this happened for what that's worth. reply mactavish88 6 hours agorootparentprevI’m about to hit 40, and I’ve had this exact same experience. Even a single beer, shot of whiskey or glass of wine will leave me feeling depressed for at least a day afterwards. For me it seems to correlate with having had COVID back in 2021, where prior to that I could still have 2-3 drinks and feel okay the next day. My suspicion/intuition is that it has something to do with a shift in my gut microbiome - from what I understand, alcohol can very easily disrupt one’s microbiome, and the state of my microbiome seems to have a significant influence on my mood. I haven’t missed the alcohol though. It’s actually been a blessing for my general health and wellbeing. reply francisofascii 5 hours agorootparentI had a similar experience. For years I drank about 2 beers a night. Then in my early 40s, had a bout of \"long covid\" that lasted about 6 months. I have fully recovered, but can't drink like I used to. If I have one or two beers, I feel crappy and down the next day. Also the buzz isn't quite the same. reply mattgreenrocks 4 hours agorootparentprevInteresting. I never connected it to covid-19, and truly cannot remember if it hit me as hard the next day or not prior to the pandemic. (Also, obviously I was younger then, too.) The only symptom I can connect to covid-19 is persistent tinnitus, which is pretty common among long-haulers (though I don't count myself among them). However, I've also heard mention of people drinking less in general which could suggest a link. reply grecy 5 hours agorootparentprevPrecisely the same experience here. Since Covid even a single drink makes me feel bad enough to not want to again… and I always had bad hangovers. This is different reply nicholasjarnold 3 hours agorootparentprevThere is some new evidence that changes occur at various stages in our lives, specifically one in the early-to-mid forties, that can affect things like alcohol metabolism[0][1]. I find this type of thing and the \"epigenetic clock\" research to be pretty fascinating to read about now that I'm approaching mid-life myself. [0] https://www.scientificamerican.com/article/drastic-molecular... [1] https://www.nature.com/articles/s43587-024-00692-2 reply Nevermark 2 hours agorootparentNever had a drink till junior year of high school, but immediately found I could drink incredible amounts of alcohol, 15-20+ cocktails, shots, beer, wine (mixing never caused me problems) and be virtually sober (obviously not clinically sober, and have no bio data) an hour or two after a long night. Also kept a clear mind and mindful awareness throughout. Just euphoric & more social. Then at 53, after some extreme stress, that completely changed. One drink slowly is usually fine. 2-3 drinks will upset my sleep. Any more and my next day suffers. At 4-5 drinks my body feels like it has a slight fever over night. I feel overheated, whether I really am I don’t know. Just can’t process it efficiently. More than that & I get socially sloppy. Not bad, but not welcome either. But my very petite daughter in her mid 20’s inherited my relevant genes. Since high school she has to down two hard cocktails within ten minutes to start an evening of (more paced, but still steady) drinking with friends just to feel the effects, like I did most of my life. She can out drink companions 2x her weight. Also in common, neither of us is at alcohol addiction risk. Drinking is completely social/situational, no cravings or problems with abstention. We both enjoy the taste of alcohol. Scotch, neat, tastes like candy to me. Drinking has always been at least as much about the gourmand exploration of flavors and varieties as the psychological effect. I got my DNA analysis and one chromosome is 68% Caribbean pirate, 32% Viking, the other is split equally Russian/Irish. Joking - but would be interested if relevant genes could be identified. I would happily sign up for gene or epigenetic therapy to resume my old life of refined epicureanism in excess. Drinking mixed with lots of sparkling water, diet sodas, and a powder mixture of creatine, minerals, protein & fiber, before & after, reliably helps a bit. Also, liver health remains excellent. Genes! reply bityard 2 hours agorootparentprevFor me, the type of drink makes a huge difference the older I get. I used to like wine, but the older I got, the more I started noticing having a terrible hangover the next day, even if it wasn't enough to get actually drunk or even buzzed. Type/brand of wine didn't seem to matter. But whiskey or vodka mixed with soda, no problem. reply criddell 6 hours agorootparentprevMy experience is similar. I can tolerate at most one cocktail, glass of wine, or beer (although I'll go malty over hoppy every time). One positive is that because of this limitation, I think I enjoy the drinks I do have a lot more. A change that came along with this is some kind of sensitivity to sugar. I love candy and baked goods. A short stack of pancakes with maple syrup and berries is the best, but that kind of carb bomb can leave me feeling almost hungover. reply taeric 3 hours agorootparentprevHow much of that is your ability to metabolize dropped, versus the strength of common drinks has sky rocketed? Especially mentioning IPAs. It is not uncommon to find them in the 9% range. I remember drinking a ton of Guinness back in the day. Highly amused to find that that would be considered a light beer today. reply mattgreenrocks 3 hours agorootparentLight, heavy, it doesn't really matter. I can get very little or no buzz from a 4% lager and still regret drinking the next day. reply taeric 2 hours agorootparentThis somewhat intrigues me. An old fashioned and a 4% lager are very different, but you seem to be saying both will give you the same regret the next day? Note that I largely resonate with the idea that aging reduces tolerance to alcohol. Love the Oatmeal's https://theoatmeal.com/comics/hungover. Hasn't quite hit me that hard, yet. Thankfully. reply littlecranky67 5 hours agorootparentprevSame here. Quit alcohol when I turned 40, the side-effects the next day of even 1-2 beers were not making it worth the buzz during drinking. Sleep issues, less focus and concentration, weaker performance in the gym, anxiety the next day etc. etc. It became a no brainer to simply stop drinking. To those with better ability to metabolize alcohol, cheers to you. reply Eumenes 4 hours agorootparentprevI'm a similar age and also a heavy drinker (15-25 drinks per week) but also run 40-50 miles per week + 5000-7000 feet of vert per week. My friends/family are astonished that I can crush 10 beers in an evening and run 15 miles the next morning w/o food. I suspect metabolism has alot to do with it. reply XorNot 5 hours agorootparentprevPost-30 I found any amount of alcohol I really noticed the next day and concluded it just wasn't worth it anymore. reply coldtea 17 minutes agoparentprev>Why is it so hard for some people to understand that alcohol isn't appealing to everyone? Because it does appear to appeal to 90% of the population reply TeMPOraL 8 hours agoparentprev> [I] must be a religious fundamentalist, must be an ex-alcoholic, etc. Personally, no. Statistically, yes, at least enough for the difference to affect a study like this. That's the point of those comments. reply piker 8 hours agoparentprevWe're just hypothesizing why at the population level abstinence from alcohol might not actually cause cognitive decline as (somewhat) implied by the data. Don't take it personally. reply ziggyzecat 8 hours agorootparentYou mean why abstinence isn't causing better cognitive performance, right, RIGHT? Obviously the study result is BS. Because if the the same people, drinkers with higher than their abstinent peers cognitive scores, didn't drink, their performance would be THE SAME. But it's impossible to find out. They should totally continue to study all that, tho. Maybe at some point in the future we can cut people open alive and look properly inside, fuck around & find out, and then close them up and send them back to work again. reply ziggyzecat 1 hour agorootparent> Obviously the study result is BS. Because if the the same people, drinkers with higher than their abstinent peers cognitive scores, didn't drink, their performance would be THE SAME. That liquor consumption part of the study result. reply bityard 2 hours agoparentprevYou are fortunate. There are many alcoholics who would trade anything to switch places with you. Probably a lot of the responses are from people who have seen how destructive alcohol can be when used in excess. I grew up in a blue-collar rural area where alcoholism wasn't just common, it was flat-out normal. Anyone who could get away with nursing a beer or 12 the whole day long and not get fired, usually did. My dad drank a lot and it caused a ton of problems with his marriage (leading to divorce) but at least he wasn't abusive. I am also somewhat lucky in that I never could acquire a taste for beer. Based on my family history and upbringing, I could have very easily slid into alcoholism otherwise. I'll never know for sure, but to keep even the possibility at bay, I have two hard rules: no drinking during the day and no drinking the night before work or having to be somewhere the next day. reply colechristensen 4 hours agoparentprev>Why is it so hard for some people to understand that alcohol isn't appealing to everyone? People often don't understand that not everyone experiences a thing the same way. That what it's like for you isn't the same as what it's like for me. People think you experience alcoholic beverages exactly the same as they do and don't understand why you dislike them as a result. reply ziggyzecat 8 hours agoparentprevI assume because technical people would think something like: - there are tens of thousand drinks on the planet, - all with their different nuances in how they alter mood and thinking patterns. Not appealing just can't be true except if you 'score' low in novelty seeking/curiosity ... except if you were only exposed to bad drunks and pathetic alcoholics ... Something like that. But it might also be because marketers see people like you as a challenge, a trophy to collect, and the non-marketing types just want to 'seduce' you. They do the same to babies and minors all the time. \"Say this or that, do this or that.\" And BAM, some brain cells practically useless forever. reply foobiekr 4 hours agorootparent\"all with their different nuances in how they alter mood and thinking patterns\" This is honestly not true unless you are including absinthe. Ethanol is the only active ingredient and it has one of the most well understood dose-response curves and one of the most heavily studied effects. The rest is all in your head. There just isn't that much to the alcohol, it's very one note, moreso than any psychedelic, moreso than even smoking (where dose control significantly varies the effects) or even cannabis which is also relatively one note. reply ziggyzecat 1 hour agorootparent> This is honestly not true unless I am quite certain that if you talked to a variety of people who like to drink, they will tell you that tequila hits different than a single barrel rye whiskey or champagne, for example. And it's more than just the amount of sugar. There are amounts of hints of various aromas in different liquor and these small differences do quite a bit in the brain, which you did propose yourself when you said > The rest is all in your head. Brains are incredible. The \"sensitivity and specificity\" of receptors goes way beyond what we understand for now, both hardware and software-wise, and that is true on the level of synapses as well as within any metabolism anywhere in their chains in the body and in how they work together to achieve their own objectives, as well as the ones they share. Take any approach within the range of broken - buggy - normal - amplified and apply it to any sphere of single and networked mechanism. Wear and tear and age change all this even further and never forget that we are still evolving, over very long time spans in very small changes. It's an insane ride from bio-chemistry to character in different states of mind and body/brain and that ride morphs quite a bit based on anything we consume via active and passive channels. And that's just part of the story as it evolves in my head. reply nomdep 3 hours agorootparentprevThere is a genetic component on how you react to alcohol: https://en.wikipedia.org/wiki/Alcohol_flush_reaction reply sieste 3 hours agorootparentprevHops in beer have a mildly sedative effect. Sugar, caffeine, taurin in mix drinks cause more alertness and euphoria. There really is more to drinks than just the amount of ethanol. reply bell-cot 9 hours agoparentprev> Why is it so hard...? Stupid (cognitively easy) stereotypes, backed by \"everybody's gotta drink!\" machismo/insecurity/conformist culture, backed by \"all the people I know\" (who aren't silenced by stereotypes and peer pressure) experience, backed by decade after decade of massive advertising by the alcohol industry (and adjacent industries). reply theclansman 7 hours agoparentprevI've always found weird how people draw conclusions from small correlations, specially when other studies show the opposite result. They call it hypothesis, but weirdly enough these conclusions often coincide with their world view. Look at that, now I'm the one drawing conclusions, must be human nature to try to make sense of things. But what I've found is that often reality is counter intuitive, and that's more fun. reply dsolo777 16 hours agoprev> Interestingly, our study did not identify a significant relationship between sleep quality—that is, sleeplessness/insomnia and cognitive performance— contrary to some previous findings > The regression highlighted a positive association between normal sleep duration (7–9 hours) and cognitive scores... while extended sleep duration negatively impacted scores across both cohorts > intermediate and evening types, were linked to superior cognitive function propaganda by big insomnia ;) reply kranner 16 hours agoparentMore like propaganda by Big Alcohol, given the following > Individuals who abstained from alcohol showed lower cognitive scores than those who consumed alcohol, conflicting with previous research that has connected moderate drinking with cognitive impairment. Weekly and monthly alcohol consumption, as opposed to daily drinking, was found to somewhat correlate with lower cognitive scores. reply bratbag 8 hours agorootparentHypothesis: The better you understand the world, the more likely you are to need an occasional drink. reply ziggyzecat 8 hours agorootparentaddition to hypothesis: and that need grows exponentially with your understanding of the world until a certain point. That need can be reduced to a healthy \"Mom said everything is fine in moderate amounts\" once you choose to not live by conflicting standards, or rather: when you choose to not support implicitly AND explicitly conflicting causes ( many on the left, finboys & fingirls, law enforcement, ... but unemployed who don't work on themselves or their environment as well ) reply jajko 7 hours agorootparentprevNot really, you need a coping mechanism with all the crap and stupidity across society, and how powerless truth and correct moral behavior can be. Alcohol is one of the worst ones, you just blunt yourself while still realizing all failures, just temporarily a bit downtuned, next morning back to misery. No solution or even improvement is happening, just basically giving up (or a bit of complaining which is just group psychological therapy). Sports, meditation, maybe even occasional psychedelics steered in right direction, active vacations, good food, good sleep. Life is much easier in such mode. reply AbstractH24 7 hours agorootparentSo you are saying “psychedelics good coping mechanism, alcohol bad”? That is California sobriety at a level I’ve never seen before. reply samstave 6 hours agorootparentnext [2 more] [flagged] dang 4 hours agorootparentSince you've reverted to breaking the site guidelines again, we've banned the account again. reply bitexploder 6 hours agorootparentprevEvery study I have ever read emphasized importance of sleep. Many show grey matter reduction long term the less you sleep. Sleep studies literally use a little alcohol to disrupt sleep. All the research out there indicates alcohol also directly shrinks your grey matter. These findings are hard to believe for sure. reply someothherguyy 5 hours agorootparentGrey matter correlates with cognition and ethanol consumption, but that doesn't necessarily mean it makes every cohort (age, amount, duration, etc) worse at every cognitive battery. There are studies that show alcohol consumption is associated with greater novelty seeking, learning capacity, etc. reply bitexploder 4 hours agorootparentI generally think the direction is being smarter/curious first and alcohol comes along. Chemically, I don’t see how alcohol can make you have better cognition in the long run. In some narrow ways, I can see it. Long term is more interesting to me than a peak healthy person that also uses alcohol. reply danielheath 15 hours agorootparentprevThose who abstain entirely are disproportionately likely to be former alcoholics - which explains eg the apparent protective effect of moderate drinking reply aulin 12 hours agorootparentMaybe in the US where the whole AA thing is popular and widespread. I'd be surprised if that's a factor at all elsewhere. Where I'm from I've never met anyone who would label themselves as 'former alcoholics'. I'm more inclined to think intelligence often times comes with a good dose of social anxiety and alcohol helps with managing it. reply criddell 6 hours agorootparentMost AA alumni don't call themselves former alcoholics. They see themselves as addicts who are glad they didn't have a drink yesterday and are doing their best to extend that streak through today. reply homebrewer 11 hours agorootparentprevYou might have zero contacts with lower working class then. I live very far from the US in a very different society and know many former alcoholics who abstain entirely, or at least try to. Because it's hard enough for an alcoholic to avoid drinking, it's even harder to have just one drink and not slide into a full-blown, months-long binge. reply ziggyzecat 8 hours agorootparentAaaaaaaaw yeah, baby. I 'member those times. Back then I knew the problem was the *quality* of the alcohol/drinks. Now, some years later. \"My homie Jamal's\" liquor costs 5-10 times that much and he puts effort into the mixtures, trying recipes, playing around with accent ingredients and modifiers and stuff. If we drink--we still drink quite a bit. But there's no month long binges no more and the immediate negative side effects on cognition are *GONE*. reply mrcartmeneses 12 hours agorootparentprevIt’s easy to imagine lots of things reply kranner 15 hours agorootparentprevI don’t see how that’s the most likely reason to abstain. A lot of religious people have never consumed an alcoholic drink; at least that is the case for South Asians: Muslims in particular, but also many Hindus and Sikhs. reply j_bum 14 hours agorootparentThis type of led to statistical issues that skewed our understanding of the health effects of alcohol consumption. Unless abstainers who previously abused alcohol are excluded or controlled for statistically, their health effects can skew results [0]. Here’s a blurb from a recent NYT article: > Fillmore was concerned about possible misleading variables in the studies: To start, they included ex-drinkers in the category of “abstainers,” which meant they were failing to account for the possibility that some people had stopped drinking specifically because of illness. The moderate drinkers looked healthy by comparison, creating the illusion that a moderate amount of alcohol was beneficial. [0] https://www.nytimes.com/2024/06/15/magazine/alcohol-health-r... reply khafra 9 hours agorootparentAnd it's fairly difficult to make it an apples-to-apples comparison, suitable for causal analysis, by excluding the lifelong teetotalers who would have become alcoholics if they had ever partaken. reply criddell 6 hours agorootparentprevThe Presbyterian church my parents took me to used actual wine (with an option for grape juice) at communion. I think most people participating in the ceremony didn't consider that having a drink any more than if their prescription medicine contained alcohol. reply wiether 9 hours agorootparentprevI haven't read the whole study so maybe it's not the right meaning, but I tend to agree with you if \"to abstain\" has been chosen for its actual meaning. Personally I don't drink alcohol, but I would never say that I am abstaining from drinking alcohol. I just don't do it. Same as eating green olives. Meanwhile, as you're pointing it out, people who say that they are abstaining from alcohol are likely to be former alcoholics. And say, people who's faith forbid them from drinking alcohol would probably fit in both : some just don't do it, others have to make a conscious effort to not do it. reply The_Colonel 11 hours agorootparentprevAnother factor is that people who have serious health problems are less likely to consume alcohol. These health problems may have an (indirect) effect on cognitive performance. reply mdp2021 10 hours agorootparentprevProbably in some areas, surely not in the abstract general. Some people just do not take intoxicants. reply bloqs 10 hours agorootparentprevWhat about entire dry cultures reply rawgabbit 3 hours agorootparentprevThe study appears to say alcohol consumption has protective effect on cognition. Those who drank had less cognitive decline compared to those who abstained completely. If this is correct, this directly contradicts the other studies that says alcohol has zero benefits? reply resoluteteeth 2 hours agorootparentA lot of studies that don't control for confounding factors like the fact that many people who 100% abstain from alcohol are former alcoholics show benefits to small amounts of alcohol consumption compared to no alcohol consumption. However, the studies that make more of an effort to control for those confounding factors have generally found that there are no health benefits, so right now the evidence seems to favor the idea that there are no health benefits to alcohol and in the studies that show otherwise it is due to confounding factors. This study doesn't seem to have particularly attempted to control for these factors (it just controlled for specific cardiovascular diseases, etc.) so it's not surprising that the results match older studies that showed positive effects from small amounts of alcohol. reply mnk47 14 hours agorootparentprevSo the Ballmer Peak is real! https://xkcd.com/323/ reply flobosg 7 hours agorootparentFrom a few months ago: The Ballmer Peak: An Empirical Search – https://arxiv.org/abs/2404.10002 (https://news.ycombinator.com/item?id=40062892) reply luke-stanley 11 hours agorootparentprevSome might just be avoiding falling asleep or being even less capable. reply treetalker 16 hours agorootparentprevThis is a situation of correlation ≠ causation, because — as all Rick & Morty fans know — some geniuses are drinking copiously just to slow down their turbo-charged intellects. :-) reply anigbrowl 15 hours agorootparentYou have to put 2 spaces at the head of each line to get code formatting, otherwise your artistic endeavors are fed to the HN paragraph enforcement daemons. reply treetalker 15 hours agorootparentThanks! For some reason it actually looked correct on my phone when I posted it. I’ve removed ASCII Rick just to be safe. reply giantg2 14 hours agorootparentprevYes, let's go with that as the reason I drink. reply andrepd 5 hours agorootparentprevGood lord reply InDubioProRubio 11 hours agoparentprevMy pet theory is that extended sleep correlates with hide-away, hunger and depression modes - which signal to our body that energy must be conserved. And cognition without the ability to act is the definition of energy waste. reply jamesmotherway 6 hours agorootparentLong sleep duration often presents with poor sleep quality, which negatively impacts cognition. \"Long total sleep time may suggest prior sleep deprivation, medical conditions, or effects of medications.\"[1] [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246141/ reply The_Colonel 11 hours agoparentprevI've been taking creatine for a while and it has interesting effects. On one hand, it seems making my sleep worse, i.e. sleeping less and kind of lightly? On the other hand, it helps me with cognitive performance, I get way less mentally exhausted during the day than I used to. reply lynx23 10 hours agoparentprev> normal sleep duration (7–9 hours) Show me one employed person with a life that manages to get 8 hours sleep on average. Most commuters would have to go to bed at 2100 to get a breakfast and enough time to get to work. I only get 8 hours when I know I am sleep deprived and go to bed at 2100. However, this is not \"normal\", and I bet it isn't normal for most. So why do we call it normal, then? reply bregma 6 hours agorootparentMost days I'm in bed (and asleep withing 5 minutes) at 22:00 and awake at 05:30. I'm over 60, 7.5 hours sleep is just right for me. I have a one-hour commute each way and usually leave home at 06:45 and arrive back around 18:00. It doesn't matter what time I go to bed, my eyes pop open at 05:30 and I have to get up (a) to micturate (did I mention I'm over 60?) and (2) feed the cats. The latter have trained me using medieval methods. When I was younger I would sleep as late as I could: my record was sleeping in until 16:00, and boys oh boys did that cause me to feel wretched. reply coldpie 4 hours agorootparentprevI go to sleep at 9:30 and wake up at 5:30 every day. Sometimes I go crazy and stay up till 10, but that's rare. My bus leaves at 7 AM (40 minute commute), so that's plenty of time for coffee & breakfast & a shower before I have to head out the door. I get home from work at about 4:30 PM, plenty of time to cook dinner or do home stuff or go see friends or whatever before going to bed at 9. reply olvy0 3 hours agorootparentMy bus also leaves at 7 AM, with 40 minute commute, but unfortunately I have a very bad case of internet / youtube addiction, plus I'm basically a night person. So I go to sleep around 12:30 AM each night (sometimes even after 1 AM), and I wake up at 5:50 AM. That's not enough sleep at all. I do sleep till very late during the weekend. I've been doing this for 20 years now, and some day it's probably going to catch up on me. reply XCSme 9 hours agorootparentprevIn the EU it's quite normal for the average person to get the 8 hours of sleep. Of course, it depends on the work sector you are in, but for most jobs you don't have to wake up at 6 AM. Also, most people live close to their work, within 30 minutes of biking/driving/public transport. reply esperent 7 hours agorootparentYou must live in a different EU than I do. Two hour commutes are normal around here (Dublin). reply jonasdegendt 5 hours agorootparentOne way? That's anything but normal. Even if it's two-way it's nowhere near the norm. The EU records this for pretty much all member states, and Ireland shows an average of 28 minutes.[0] > In 2019, more than half (61.3%) of employed people in the EU traveled less than 30 minutes from home to work. You are correct that Ireland is part of the group of countries which has a larger subset of people with long commutes, but hovering around 10% is not what I'd call \"normal\". > The largest shares of the longest commuting times were observed in Latvia (13.5%), Ireland (11.2%), Belgium (10.7%) and Hungary (10.6%), where more than 10% of employed people had to travel from home to work for 60 minutes or more. [0] https://ec.europa.eu/eurostat/web/products-eurostat-news/-/d... reply andrepd 5 hours agorootparentprevYes in the \"EU\" (500 millions people) everyone lives in a quaint old city centre where people cycle to the greengrocer and everyone has stellar labour rights and sleeps soundly at night. Meanwhile in the real world... reply XCSme 5 hours agorootparentNo, exactly that's the point. It's a lot of smaller countries and cities, and usually people live/relocate where they work. People travelling daily longest for work are usually living in the (poorer) rural side, where yes, they have to get up early and take the (limited) public transport available, sometimes having only 2 buses per day. reply bitexploder 6 hours agorootparentprevI ran a consulting business for a long time. Sleeping that much is no problem if you prioritize it. Kids, wife, etc. You just have to organize your life around your priorities. It does mean little time for extraneous entertainment during the week, but that’s ok. reply bityard 2 hours agorootparentprev\"Normal\" in the context of the body's actual sleep needs, not \"normal\" in the sense that everyone has the time or life management skills to achieve those needs. (\"Normal\" also depends on age: babies sleep a lot more than 8 hours a day, teenagers also typically need more sleep than they actually get.) reply buttscicles 5 hours agorootparentprevInterestingly everybody replying to this is saying they spend ~8 hours in bed, which most certainly means they aren't getting 8 hours of sleep :) reply beowulfey 8 hours agorootparentprevMy breakfast is a cup of coffee and toast usually but since I live a 25 min bike ride from my work (in New England USA) I can usually leave around 800, which means I can sleep at 2300 for 8 hours easily. Now, does my body do that?... no, usually it wakes me up after 6-7, but whatever. reply hansc 9 hours agorootparentprevI do sleep 8+ hours each night (Netherlands): Up at 7.30, leave at 8.15, 30m commute. Leave work at 5.15, get to sleep around 10.45. reply louvki 10 hours agorootparentprevidk it's normal for me but then again i live in Denmark and i dont have any children reply hug 9 hours agorootparentprevI work a full time tech job, I am out with friends or on a “date night” with my partner three or four times a week every week, and I have a few hobbies like music production and combat robotics. I get at least 8 hours every night. My “must be home and in bed” time on a weeknight is by 12:30AM, and I sleep in until 8:45AM. I am normally woken by my alarm. Weekends I simply set an alarm for 9 hours after I get in bed, which sometimes means sleeping until 2PM. I don’t eat breakfast, and I basically never commute in the morning, but if I do, my office is about 15 minutes away. reply andrepd 5 hours agorootparentYeah not having to commute helps. More than lack of time, I'm usually too tired after a full day of work (8 or 9 till 6 or 6:30) to do anything else. reply ziggyzecat 8 hours agoprevThis is one of those funny studies that looks at so much but then falls short to realize one thing: How well a person can breathe through their nose and the side effects of not being able to do that on nervous system, beginning with facial muscles, neck muscles and so on ... there's also a direct relationship of all that on the prefrontal cortex. I admit tho, that I never checked if there are studies like that. If only I could travel backwards in time ....... reply mckirk 8 hours agoparentI pretty definitely can't breath through my nose as well as I should be able to (deviated septum), and have been wondering for a while how much this actually impacts me. What was your experience like? reply ziggyzecat 54 minutes agorootparentDeviated septum as well, but it's not the only reason for drying mucous membranes, which I never got around to getting checked. In part, it results from nutritional habits, sometimes (still), a bit too much coffee but the amount that causes it varies so it must be something else in one of the underlying metabolisms. A few weeks ago, I tried Nasal Strips again, and for some reason, they are helping now and the difference is like day and night. I can only assume that it's because I got older and my life style changed, including nutrition, which was really bad when I was young. So dry mucous had too much effect for me to notice the benefits of the nasal strips or it's really just the bigger size of my nose :P But the biggest change, compared to my young years, is that I am much more resilient to unhealthy stress and not continuously stressed as I was back then, when I was also regularly exposed to stress amplifications without time to recover. And being able to breathe properly through my nose now amplifies everything for the much much better: mood, psychological resilience, performance --both cognitive and athletic-- time to fall asleep, sleep, with the latter two coming with their own boosts for the next day. Most notable was the effect on my facial muscles. With the loss of those specific tensions came a direct effect on focus and concentration and self-control, which is all, to a great extent, mediated by the prefrontal cortex. reply op00to 7 hours agorootparentprevHave you had a sleep study done? You may have low oxygen saturation when you sleep. I had literally no symptoms of sleep apnea, no major risk factors, but my body was struggling to breathe when I slept due to my anatomy. I have started CPAP treatment and am excited to see if my cognition improves despite not really feeling bad. reply nelup20 6 hours agorootparentOh wow this is the first time I've read about this. Did you wake up frequently during the night or were there any other signs? I have somewhat of a deviated septum, and even had surgery done half a year ago, but it didn't really change much unfortunately. I always need >=9 hours of sleep to feel good. I've slept on average 7 hours the past 2 weeks & I feel horrible (completely different reason for the reduced sleep). I did a sleep study a few years ago because I suspected sleep apnea, but because of anxiety or sleeping in a new place, I only ended up sleeping 1-2 hours that night. They didn't find anything, but I wonder if I should try again. reply op00to 3 hours agorootparentI don’t have memories of waking up frequently, but you tend to forget shorter wakeful periods. No other signs. No snoring, nothing that would indicate sleep issues. I have headaches but they are related to other sinus issues, though the doctor seems to think the CPAP should help. I’m not so sure but open to the possibility! reply mckirk 7 hours agorootparentprevSo far I haven't, nope, though it might be worthwhile to look into, given that I basically need 8+ hours of sleep to function well. I do have a fitness watch though that can (supposedly) measure SpO2, and while that showed the occasional dip, it seemed more like a measurement error, and generally the values were in the normal range. reply Steve44 6 hours agorootparentI've a FitBit Sense 2 and as I understand it that measures variations in SpO2. I'm not convinced how accurate nor how quickly it responds, it derives an estimate rather than directly measuring. I think it's also sensitive to any movements of the watch on your wrist. I also have had a finger pulse oximeter which logs and exports to an app via USB. If I sleep with that on it seems to be very reliable at recording the levels, the data certainly looks good and feels much more reliable than the Sense. reply op00to 3 hours agorootparentprevMy Apple Watch showed no major desaturations. The desats only last a few seconds. reply pkaler 4 hours agorootparentprevI sleep with Breathe Right strips. It's changed my life. Whoop band and 8Sleep confirm. reply semiinfinitely 16 hours agoprevAm I reading this correctly from table 2 that the highest magnitude correlation is never drinking alcohol, which is a negative correlation. reply mckirk 7 hours agoparentI wonder if they tried to correct for 'exercise intensity' for that, because (fun fact), exercising regularly is negatively correlated with regularly taking all kinds of drugs -- except for alcohol, where the correlation is apparently positive [1]. [1]: https://www.outsideonline.com/health/exercise-alcohol-resear... reply yobbo 13 hours agoparentprevFurthermore, drinking weekly seems better than monthly. But frequency doesn't mean much unless we know amounts. For example, small amounts weekly could be benefiting social relations and regulating things like anxiety. Binging monthly might not give these benefits. Weirdly, BMI seems to have almost zero effect. reply brational 5 hours agorootparent> Furthermore, drinking weekly seems better than monthly. Would seem to potentially imply more routine social activity / community. reply __rito__ 12 hours agorootparentprev> \"BMI seems to have almost zero effect.\" Seeing how chubby Jon von Neumann was, this should be trivial. ;) reply yobert 13 hours agoparentprevI bet this is not because alcohol is good for you in any way, but because being intelligent is related to being eager to try new things. reply theclansman 7 hours agorootparentI knew that smoking crack meant I was smart reply piker 11 hours agoparentprevA hunch would be that alcohol abstinence is strongly correlated with religious fundamentalism which is correlated with lower income and cognition. reply kelthuzad 10 hours agorootparentBefore we rush to conclusions and make up just-so stories, note this passage from the research: \"The relationship between cognitive function and lifestyle factors such as alcohol consumption and smoking proved complex. Individuals who abstained from alcohol showed lower cognitive scores than those who consumed alcohol, conflicting with previous research[1] that has connected moderate drinking with cognitive impairment\" [0] https://en.wikipedia.org/wiki/Just-so_story [1] Topiwala A, Allan CL, Valkanova V, et al. Moderate alcohol consumption as risk factor for adverse brain outcomes and cognitive decline: longitudinal cohort study. BMJ 2017;357:j2353. reply piker 10 hours agorootparentYes, the parent comment incorporated and was consistent with that. I.e., there's something confounding these results. It then hypothesized the confounder. reply bell-cot 9 hours agorootparentprevFirst Reaction: These days, less-clever folks are mostly in situations where they're lucky if they can afford food and rent. Alcohol just another out-of-reach luxury. reply xanderlewis 8 hours agorootparentReally? Poor people don’t drink? I guess where you live there are no homeless people. reply bell-cot 6 hours agorootparentRe-read the \"Methods\" section, on page 2 of the PDF. I'm not too familiar with the UK Biobank database, but the researchers broadly exclude anyone whose record (in the DB) did not include completion of multiple cognitive tests. And prioritized people whose data include rather comprehensive sleep & related data. Again I don't know this dataset well...but I'd really suspect that few of the UK's homeless would have passed this study's selection criteria. reply graemep 7 hours agorootparentprevits an interesting idea, but its very complex because of varying attitudes to alcohol within and between religions. It is interesting that fundamentalists tend to be anti-alcohol across religions with different attitudes in mainstream believers (say Christianity that commonly uses alcohol in religious rites and hose founder's first miracle was to give people alcohol, Buddhism, and Islam which is clearly anti-alcohol). I think that means you are onto something in that there is an important correlation there. Not sure it is as simple as lower income (there are lots of rich fundamentalists of all religions) or cognition. Personality traits or associated cultural factors more likely? reply jajko 7 hours agorootparentprev> A hunch would be that alcohol abstinence is strongly correlated with religious fundamentalism which is correlated with lower income and cognition. It seems you are pointing to second largest religion in the world - islam. It covers over 2 billions of people. They largely don't drink not because they are fundamentalists, they just follow basic premises of their religion. And my various travels to various countries with them being either majority or minority its extremely common, even when they resettle ie to Europe. There are also completely dry countries in North africa for example, they are not full of religious fundamentalists. Please educate yourself about the world a bit before making such harsh and discriminating statements, they have no place on this forum. reply alephnerd 6 hours agorootparentAnd not just Islam. The alcohol taboo is equally strong in mainstream Hinduism, Sikhism, Jainism, and Buddhism (depending on region). Also, in a lot of the developing world, prohibitionism is a fairly mainstream Feminist opinion, due to the sadly very common issue of domestic abuse. All this shows is that there are various confounding variables in this study, and that more studies are needed. reply fedeb95 3 hours agoprevPeople in UK should definitely consume alcohol. Irony apart, my idea (borrowed somewhere I can't remember) is that body functions are too intertwined to just isolate a single variable, or determine a single cause. People, consume what you want, sleep how you want, and assess what it is that you \"want\" individually. It's not hard too see if alcohol, sleep deprivation, etc. causes you harm or not. As for long-range damage or good: look at people who lived long (and healthy). reply tgtweak 5 hours agoprevGreat study - I find that anecdotally 7 hours of sleep generally outperforms 8 hours in terms of cognitive function early in the day, but suffers later in the day - likely favored by \"morning\" people that tend to do more in the beginning of their day. reply reedf1 11 hours agoprevSo some studies show a positive correlation on cognitive performance and some negative correlation for \"morningness\" and \"eveningness\". The obvious conclusion to me is that there is no strong effect. Or that at some portions of the year, or parts of the globe, certain chronotypes have advantages, this certainly matches my anecdotal experience. reply energy123 11 hours agoparentEven if there was a strong correlation, it doesn't follow that you can improve your cognitive performance by fighting against your natural chronotype. reply nkmnz 9 hours agorootparentTo the contrary. It's possible that the correlation is caused solely by a culture that raises owls as larks, \"dumbing down\" the latter with individuals maladapted to the lark life style. The effect could be increased by smarter owls being able to resist the conversion therapy a little better than less smarter owls – resulting in a selection bias. reply lazycrazyowl 7 hours agoprevSo the study shows that night owls and people with flexible sleep patterns might have a slight advantage in terms of cognitive performance implying that it’s not just how much sleep you get, but also when you prefer to sleep that matters for keeping your brain sharp. reply ETH_start 16 hours agoprevSo the main findings are superior cognitive performance being found in: • people who sleep normal amounts (7–9 hours) • night owls reply slt2021 15 hours agoparentnight owls are people who are sensitive to light/noise/environment and therefore more productive in the night - when they can focus and be productive for prolonged periods of time without million of distractions. when you don't have people calling, texting, slacking, zooming you, environmental noises (car, neighbours, other people, dogs), when you don't have personal life distractions (wife, kids, personal email, social media etc) - you can be in a flow state for prolonged time and be quite productive. even such a small thing as lunch break, and other snack breaks - divides your day into two killing the flow. reply tensor 13 hours agorootparentNah. As a night owl myself it’s simply that I have trouble falling asleep due to an overactive mind. Add two hours or more of rolling around and instead of getting up at 7 I’m getting up at 9. And before someone chimes in with sleep advice, please don’t. I’ve tried it all. Some rings help a bit but at the end of the day nothing makes enough of a difference. I just need those two hours my feet to the point when I can sleep. Also no, going to bed before everyone else is not an option. I’m not giving up having a social life. reply YokoZar 12 hours agorootparentA night owl who feels pressured to wake at 7 or even 9am is a genuine tragedy. I am extremely fortunate to have a career where I can sleep my natural hours of somewhere around 4:30 AM until after noon. But I had to fight for it, and set boundaries. I would encourage everyone with a late chronotype to seriously consider spending some time finding such work, as finally getting good sleep will be a major unlock for your life and happiness. reply yuye 11 hours agorootparent>A night owl who feels pressured to wake at 7 or even 9am is a genuine tragedy. I'm very much a night owl. My whole childhood, I've been told by people that I can't choose to get out of bed late once I start working. Their message was clear: \"Quit this lazy and childish behavior.\" Nowadays, I sleep at 0:00-1:00 and get up at 7.30. I hate it. The world lacks empathy for us. reply The_Colonel 11 hours agorootparentprevAn anecdote, but I used to be a night owl. Like you, I had trouble falling asleep in the evening and then getting up in the morning. Now in my thirties, it turned around. I fall dead asleep at 9-10 PM and wake up at 4-5 AM, not being able to sleep further. The change coincides with me getting kids, but I don't think it alone can explain this. reply PaulRobinson 9 hours agorootparentIt can definitely change over time. In my 30s I went from wanting to stay up until 2am or later, and getting up at 9-10am, to wanting to be asleep by 10pm and getting up before 6am, every day of the week, including weekends - staying out late socialising is now weirdly hard work, and I'm basically done with work by 4:30-5pm and ready to start winding down. I don't have kids, something just flipped over the course of a year or two. reply seba_dos1 8 hours agorootparentI'm flipping regularly, every month or two. All you need is lack of constant schedule and ADHD; suddenly days start to be longer than 24h and you can't just go to bed earlier (well, you can, but you'll just lay there awake for hours). For a while I had one meeting each week at work and free rein otherwise - so sometimes I was waking up early for it, and sometimes went to bed late because of it. I've noticed that when I'm flipped to be awake in early morning, I get tired really fast. It's like a cutoff hour - I have missed some concerts just because they started at 7-8 PM which was around bed time for me at the time. When I'm in the evening/night mode, I can be much more flexible. It seems much harder to flip out of morning mode than to flip into it, even though I really don't like how I'm feeling across the day. It requires conscious effort to beat the cutoff, while flipping out of evening mode just happens on its own eventually if I'm not careful enough. I was never a morning person whenever I had to follow a schedule. reply PaulRobinson 6 hours agorootparentI think structuring days can help. I often am up at 6am with nothing to do for hours on end (it's a Sunday, my partner is not getting up until 10am, so I guess it's reading time or going for a walk time...?), but what I've found helps is marking blocks out in my calendar and then getting a reminder what its time to do. Some weekdays at a certain time, it's time to go for a run. Saturday afternoons are side projects or spending time doing something with friends and family. Weekday evenings have a broad pattern to them (you won't see me in a cinema on a Friday night, or in the pub on a Monday...) This structure seems to help regulate a ton of stuff, including eating, sleeping and so on. Some people \"don't like routine\", which I get, but you can basically block out sleep (plus an hour either side), and then riff in the middle, the key is try and get your head down around the same time, 7 nights a week, and try and learn the military sleep method [1] to get some consistency if you need it. Good luck with settling it down, if that's what you want/need! [1] https://www.dreams.co.uk/sleep-matters-club/what-is-the-mili... reply vundercind 7 hours agorootparentprevKids actually helped me, too. Strangest thing. But only (any one of) three things have proven to reliably get me all the way to being able to sleep like a normal person every day (while sustained): 1) Being totally exhausted by my day. This requires a lot of physical activity. Basically impossible on a work day or if I have significant amounts of anything not-very-active to do. I rarely achieve this one on except on vacation. 2) Zero electronics, including electric lights, after sundown (or at least for a full two or more hours before I want to be asleep). 3) 5mg of weed gummy Only the third one is compatible with normal modern life with an office job. reply nfw2 13 hours agorootparentprevI personally find daylight itself to be \"distracting\" -- like that basic sensory input alone consumes some non-trivial amount of cognitive energy. I much prefer to think in a dark space. I've never met anyone IRL who relates to this though. reply ahlCVA 5 hours agorootparentI feel the same, but I've never actually talked to anybody about it since it felt like such a strange thing to say. Thanks for letting me know I'm not alone! reply CalRobert 11 hours agorootparentprevFor what it’s worth I’ve loved wfh because I get to work in a dark cave with blacked out windows. reply hombre_fatal 7 hours agorootparentprevYeah, there’s something about the day itself that’s distracting. Maybe it’s the light + some latent emotion I have from residual expectations about what I “should” be doing. Not sure. At my worst I procrastinate my whole day away at home and not enter deep work until late at night where I become feverishly productive. reply 2four2 14 hours agorootparentprevDisagree. I'm a night owl, and I work best with bustle around. Silence is eerie and distracting. I would keep the tv on when studying late into the night and I worked best in groups. reply nkmnz 9 hours agorootparentprevOwls could have the same amount of productive focus time in the early morning before everyone else starts to distract them - the reason they chose not to do that is mostly due to chronobiology, which is determined by genes, not social constructs like \"lunch time\". reply LeonB 8 hours agorootparentAs Douglas Adams said “Time is an illusion, lunchtime doubly so.” reply baxtr 13 hours agorootparentprevThat’s me! But instead of going to bed late, I also try to get up very early to have the silent environment you describe. So seeing the other comments disagreeing maybe what you’re describing is a different kind of night owl… reply theshackleford 15 hours agorootparentprevEven when controlling for what’s noted, I’ve always been more productive of an evening and have been since a child. It’s not limited to me and there seems to be some genetic component. My mother, grandmother, aunts etc. All night owls, and all of us averaging 6-7 hours of sleep at best. Regardless of what’s occurred before hand, my brain seems to kick into some higher gear at around 9-10pm. Like there is a noticeable change in my energy/cognitive capabilities. Ironically, the only time this has ever deviated was two months I spent in Russia where for the first time in my life, I adapted almost instantly to what one might call a “normal” sleep schedule. As soon as I returned to my own time zone, it reverted back to me being a night owl again. I’ve given up attempting to work around it and simply moved to a role that can accomodate it. It’s been huge for my physical and mental health to end the 30 odd years I’ve spent fighting what seems to be a natural schedule for me. reply gexla 14 hours agorootparentI have found that I'm most creative and productive right after waking, but it doesn't last long. My energy levels crash some hours after waking, then slowly rise as the day turns into evening hours. At that point, I'm not as energetic as I am right after waking, but that state can last well into the night until I can't stay awake any longer. Even as I'm getting tired at night, it seems different than the crash at midday. So, I think it's true you can have a higher level of productivity in the morning for a short period, but also get more done in the evening because of the extended period of \"good enough\" energy. Morning people and evening people are both correct. Circumstances generally determine which of those two ends are most easily exploitable. Because I work from home, I can take advantage of both. reply Aerroon 9 hours agorootparentI'm the same way. I wonder if it has to do with food or even the expectation of food. Maybe eating (or even drinking water) sends the body on a rollercoaster that mostly settles down by evening time. reply KETHERCORTEX 6 hours agorootparentFood intake makes blood go to stomach, temporarily reducing its amounts in the other parts of the body. It may cause some fatigue. reply dukeofdoom 12 hours agorootparentprevI found that a basement is a good substitute during the day. Reduced noise, distraction and no sunlight which often changes intensity every time a cloud rolls by. reply gonzo41 16 hours agoparentprevWhat if you're a night owl who sleeps in. Does that make you just an offset normie? reply SlightlyLeftPad 14 hours agorootparentI would argue no because, while you’re doing stuff that matters to you, everyone else is done with their day and partying. You’d just be asleep while everyone else is pretending to do their work. reply afc 10 hours agorootparentWhat does partying have to do with this? If anything, strong party goers are more likely to be night owls who sleep in? reply johnathandos 15 hours agorootparentprevAren’t we all just offset normies? reply roshankhan28 11 hours agoprevi found one thing helpful. if you drink coffee and then sleep immediately , i got a better sleep. it acted as good as melatonin gummies. anyone like me out there? is it normal? or am i addicted? reply kraftman 11 hours agoparentDo you have ADHD? I read once that for normal people caffiene pushes dopamine levels too high, and stops them from sleeping, but for people with ADHD caffeine can push dopamine levels from too low to normal, calming them and helping with sleep. reply lagniappe 2 hours agorootparentI do, nicotine works this way for me. reply rzmmm 1 hour agoparentprevHow much caffeine you consume on average? Some might get withdrawal symptoms asleep if the intake is very high. reply pch00 3 hours agoparentprev> if you drink coffee and then sleep immediately Alas, being of a certain age means that any kind of liquid before bed will result in an early-hours wakeup to relieve the bladder - negating any potential sleep benefits :( reply Dr_Birdbrain 10 hours agoparentprevI have heard of people who say creatine helps them sleep. Creatine is technically not a stimulant, but in most people it causes a state of mental activation that would make it hard to sleep. I suspect you and the people who sleep better on creatine may have something in common. reply nkmnz 9 hours agorootparentThis is funny. I started mixing ~3g of creatine into my morning coffee a couple of weeks ago. I never thought of it being stimulating or activating, but it's certainly possible: I usually don't stick long enough to things like this to form a habit, but my brain seems to nudge me into keeping up with it... reply JonChesterfield 8 hours agorootparentI don't find it dissolves or stays in suspension - at the end of the coffee most of the creatine is at the bottom of the mug. Which is a pity as otherwise it would be very efficient, tip it in while the machine makes the cup. Are you stirring it lots or similar? reply nkmnz 2 hours agorootparentThe creatine I use is finely powdered and it immediately dissolves in very little boiling water. I add it to the second coffee of the day: first one is from the espresso machine for pleasure while waking up, second one is using instant coffee to maintain stable blood caffeine levels. The crema-faking bubbles from the instant espresso powder probably help getting the stuff to dissolve. Then drink it hot and quickly :) reply goosejuice 4 hours agorootparentprevSurely must be another method that doesn't ruin a perfectly good cup of coffee reply nkmnz 2 hours agorootparentThat's why I use it with instant coffee ;) not much to ruin when your expectations are already low! Also, you need boiling water to properly dissolve the stuff. But to be honest: the coffee feels thicker, but there's no distinct change in taste due to creatine. reply pjerem 11 hours agoparentprevThat’s called a power nap. But you have to be a good sleeper in the first place. If you are not able to fall asleep before the caffeine kicks in, you’ll have a pretty different night/nap. reply dukeofdoom 12 hours agoprevNot sure about the study. But I've watched some fitness you tubers, I mean the real deal ones, that train for a specific sport. And they obsess with their sleep as much if not more than their training. Using bands and watches to track the quality of their sleep. Very important for recovery and training, at elite levels. One theory is that sleep, is a time when the brain cleans itself. If the theory is correct, maybe missing one cleaning is not detrimental. But continued, it will lead to performance decreases. Not just sports. But mental ability. reply beezlebroxxxxxx 4 hours agoparentThe relationship between sleep quality and recovery is pretty well known in exercise circles. A lot of \"amateur\" elites, like very competitive marathon runners that take it quite serious around a day job, have to work in long low intensity cardio workouts around their schedules, so you start thinking very hard about recovery. When you think about recovery that hard you also start prioritizing sleep a lot, especially if you're up at 4am for a 2 hour run, for example. Missing a lot of sleep for successive days is almost guaranteed to lead to injury when you're training very hard. The effect on your cardio performance (even just average BPM) can be stark. I don't know how some resident doctors, for example, can also be very good marathon runners. Just managing that sleep schedule must be crazy. reply mkermani144 6 hours agoprevAny TL;DR? It's a long pdf :D reply tradertef 2 hours agoparentRead the Conclusion section of the paper. reply HarHarVeryFunny 4 hours agoprev [–] Brought to you by the \"No shit!\" journal of scientific studies. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A study indicates that normal sleep duration (7-9 hours) is linked to better cognitive performance, while extended sleep can have negative effects.",
      "Alcohol consumption has a complex relationship with cognition; abstainers tend to score lower on cognitive tests compared to moderate drinkers.",
      "The discussion also explores how age, health, and personal experiences influence alcohol metabolism and cognitive function."
    ],
    "points": 254,
    "commentCount": 170,
    "retryCount": 0,
    "time": 1725831432
  },
  {
    "id": 41483654,
    "title": "Charging lithium-ion batteries at high currents first increases lifespan by 50%",
    "originLink": "https://www.eurekalert.org/news-releases/1056171",
    "originBody": "News Release 29-Aug-2024 Researchers discover a surprising way to jump-start battery performance Charging lithium-ion batteries at high currents just before they leave the factory is 30 times faster and increases battery lifespans by 50%, according to a study at the SLAC-Stanford Battery Center Peer-Reviewed Publication DOE/SLAC National Accelerator Laboratory image: Giving lithium-ion batteries their first charge at high currents before they leave the factory is 30 times faster and increases their lifespans by 50%. view more Credit: Greg Stewart/SLAC National Accelerator Laboratory Menlo Park, Calif. — A lithium-ion battery’s very first charge is more momentous than it sounds. It determines how well and how long the battery will work from then on – in particular, how many cycles of charging and discharging it can handle before deteriorating. In a study published today in Joule, researchers at the SLAC-Stanford Battery Center report that giving batteries this first charge at unusually high currents increased their average lifespan by 50% while decreasing the initial charging time from 10 hours to just 20 minutes. Just as important, the researchers were able to use scientific machine learning to pinpoint specific changes in the battery electrodes that account for this increase in lifespan and performance – invaluable insights for battery manufacturers looking to streamline their processes and improve their products. The study was carried out by a SLAC/Stanford team led by Professor Will Chueh in collaboration with researchers from the Toyota Research Institute (TRI), the Massachusetts Institute of Technology and the University of Washington. It is part of SLAC's sustainability research and a broader effort to reimagine our energy future leveraging the lab’s unique tools and expertise and partnerships with industry. “This is an excellent example of how SLAC is doing manufacturing science to make critical technologies for the energy transition more affordable,” Chueh said. “We’re solving a real challenge that industry is facing; critically, we partner with industry from the get-go.” This was the latest in a series of studies funded by TRI under a cooperative research agreement with the Department of Energy’s SLAC National Accelerator Laboratory. The results have practical implications for manufacturing not just lithium-ion batteries for electric vehicles and the electric grid, but for other technologies, too, said Steven Torrisi, a senior research scientist at TRI who collaborated on the research. “This study is very exciting for us,” he said. “Battery manufacturing is extremely capital, energy and time intensive. It takes a long time to spin up manufacturing of a new battery, and it’s really difficult to optimize the manufacturing process because there are so many factors involved.” Torrisi said the results of this research “demonstrate a generalizable approach for understanding and optimizing this crucial step in battery manufacturing. Further, we may be able to transfer what we have learned to new processes, facilities, equipment and battery chemistries in the future.” A “squishy layer” that’s key to battery performance To understand what happens during the battery’s initial cycling, Chueh’s team builds pouch cells in which the positive and negative electrodes are surrounded by an electrolyte solution where lithium ions move freely. When a battery charges, lithium ions flow into the negative electrode for storage. When a battery discharges, they flow back out and travel to the positive electrode; this triggers a flow of electrons for powering devices, from electric cars to the electricity grid. The positive electrode of a newly minted battery is 100% full of lithium, said Xiao Cui, the lead researcher for the battery informatics team in Chueh’s lab. Every time the battery goes through a charge-discharge cycle, some of the lithium is deactivated. Minimizing those losses prolongs the battery’s working lifetime. Oddly enough, one way to minimize the overall lithium loss is to deliberately lose a large percentage of the initial supply of lithium during the battery’s first charge, Cui said. It’s like making a small investment that yields good returns down the road. This first-cycle lithium loss is not in vain. The lost lithium becomes part of a squishy layer called the solid electrolyte interphase, or SEI, that forms on the surface of the negative electrode during the first charge. In return, the SEI protects the negative electrode from side reactions that would accelerate the lithium loss and degrade the battery faster over time. Getting the SEI just right is so important that the first charge is known as the formation charge. “Formation is the final step in the manufacturing process,” Cui said, “so if it fails, all the value and effort invested in the battery up to that point are wasted.” High charging current boosts battery performance Manufacturers generally give new batteries their first charge with low currents, on the theory that this will create the most robust SEI layer. But there’s a downside: Charging at low currents is time-consuming and costly and doesn’t necessarily yield optimal results. So, when recent studies suggested that faster charging with higher currents does not degrade battery performance, it was exciting news. But researchers wanted to dig deeper. The charging current is just one of dozens of factors that go into the formation of SEI during the first charge. Testing all possible combinations of them in the lab to see which one worked best is an overwhelming task. To whittle the problem down to manageable size, the research team used scientific machine learning to identify which factors are most important in achieving good results. To their surprise, just two of them – the temperature and current at which the battery is charged – stood out from all the rest. Experiments confirmed that charging at high currents has a huge impact, increasing the lifespan of the average test battery by 50%. It also deactivated a much higher percentage of lithium up front – about 30%, compared to 9% with previous methods – but that turned out to have a positive effect. Removing more lithium ions up front is a bit like scooping water out of a full bucket before carrying it, Cui said. The extra headspace in the bucket decreases the amount of water splashing out along the way. In similar fashion, deactivating more lithium ions during SEI formation frees up headspace in the positive electrode and allows the electrode to cycle in a more efficient way, improving subsequent performance. “Brute force optimization by trial-and-error is routine in manufacturing– how should we perform the first charge, and what is the winning combination of factors?” Chueh said. “Here, we didn’t just want to identify the best recipe for making a good battery; we wanted to understand how and why it works. This understanding is crucial for finding the best balance between battery performance and manufacturing efficiency.” This research was funded by the Toyota Research Institute through its Accelerated Materials Design and Discovery program. About SLAC SLAC National Accelerator Laboratory explores how the universe works at the biggest, smallest and fastest scales and invents powerful tools used by researchers around the globe. As world leaders in ultrafast science and bold explorers of the physics of the universe, we forge new ground in understanding our origins and building a healthier and more sustainable future. Our discovery and innovation help develop new materials and chemical processes and open unprecedented views of the cosmos and life’s most delicate machinery. Building on more than 60 years of visionary research, we help shape the future by advancing areas such as quantum technology, scientific computing and the development of next-generation accelerators. SLAC is operated by Stanford University for the U.S. Department of Energy’s Office of Science. The Office of Science is the single largest supporter of basic research in the physical sciences in the United States and is working to address some of the most pressing challenges of our time. Journal Joule DOI 10.1016/j.joule.2024.07.024 Disclaimer: AAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert system.",
    "commentLink": "https://news.ycombinator.com/item?id=41483654",
    "commentBody": "Charging lithium-ion batteries at high currents first increases lifespan by 50% (eurekalert.org)237 points by snazz 20 hours agohidepastfavorite97 comments starky 17 hours agoFrom having worked a bit in the industry I'm a bit skeptical about this study, I've definitely seen studies and experiments that used different initial charging conditions that would have shown better fade performance if this was true. Not to mention, how much does the increased SEI change the impedance of the cell (thus reducing the subsequent charge speed) and the capacity available. reply m463 15 hours agoparentWasn't supercharging EVs a lot frowned upon at first, but later found out not to negatively affect battery life as much? https://electrek.co/2023/08/29/tesla-battery-longevity-not-a... reply Roark66 13 hours agorootparentThis may be so in Teslas that have quite robust heat management. It definitely doesn't apply to many other brands. Anyone who knows anything about lithium batteries and sees temperatures at which fast charging is done will not believe any of the longevity claims. It is up to 55C during charging and during subsequent driving on a motorway it can take half an hour for this temperature to drop below 40C (look up BYD Seal 1000 mile challenge on YouTube for an example - all above st 9C ambient). reply atwrk 11 hours agorootparentBYD started as a battery producer and still is one of the largest in the world - I can't imagine them not having considered proper thermal management for battery health. Especially since they have been producing electric buses since 2010, which, as utility vehicles, see way higher usage (=charges) than consumer cars. reply chrisandchris 9 hours agorootparentI can imagine, and I can also imagine the buyer of those batteries stripping down \"functionality\" because of costs. So EV cars on the lower end could have higher peak temperature, because it's cheaper (i guess). reply delusional 4 hours agorootparentprevIs it a case of them not thinking about it, or did they think about it and figure that consumers would not pay for the increase in quality? Plenty of engineered things could be \"better\" if price wasn't a concern. Most of this board can probably think back to a time where they had to leave long term value on the table because of short term costs concerns. It doesn't seem impossible to me that the engineers would leave some battery longevity (something that's hard to gauge for the consumer) on the table in pursuit of faster charging speed at lower prices (headline marketing items). reply originalvichy 10 hours agorootparentprevDoesn’t using an 800V architecture solve some of the heat problem? I believe currently the Koreans (Hyundai/Kia) and Porsche are the only major manufacturers using it. No surprise both can push nearly double as fast charging compared to 400v competitors. reply cenamus 9 hours agorootparentThe total pack voltage shouldn't really matter for internal heating, it's due to the cells internal resistance, higher voltage really only allow for thinner wiring than the crazy high current low voltage packs (especially important for chargers/contacts etc) reply idiotsecant 6 hours agorootparentInternal heating is proportional to current. Higher voltage means lower current. Charging at the same wattage implies lower current. So: higher voltage implies lower heating. reply hnuser123456 6 hours agorootparentThe cells need to charge whether they're in series or parallel. The efficiency they can absorb charge at high speeds without heating up is not primarily determined by how they're wired. You could wire 5,000 cells in series, charge them with 20kv at 4 amps, or wire them all in parallel, and charge with 4 volts at 20 ka. Each cell will produce the same amount of heat either way, they only charge with about 95% efficiency. Higher voltage doesn't really reduce the need for active cooling if you want to keep the cells under 40-50C. Energy loss though resistance in the pack's internal wiring is likely a lot less than the loss due to the chemistry not being 100% efficient at absorbing (or delivering) charge without heating up. But it does allow for thinner wires to get max power out of the battery. reply mohaine 5 hours agorootparentprevBut I'm pretty sure the voltage seen by each individual battery is always the same, regardless of the distribution system voltage. There should be less heating for higher voltages but if most if the heating is in the battery vs the distribution system then the higher voltages will not help much. Also, if they make all wires smaller to save money and weight then there might not be any change in heating. reply stetrain 4 hours agorootparentprevThe current per cell is still the same. 800V charging just means that you put cells in series banks to achieve an ~800V module-level voltage. Current is reduced in the main charging cables, charge port, and pack fuse/contactor, but not in the individual cells. reply wffurr 5 hours agorootparentprevLucid uses a 1000V architecture. I think the higher voltage allows them to charge more of the battery pack in parallel at a time. reply jve 10 hours agorootparentprevAnd because of this, electric vehicle manufacturers should take note. If only for a city only car that you mostly charge at home, don't do roadtrips with multiple fast charges in short period of time you may get away with passive cooling. And if you don't live in a hot climate. But those are too many IFs. reply vondur 1 hour agorootparentWasn't that how the Nissan Leaf's used to be setup, with passive cooling? I know it greatly affected their range in warm climates. I think they now switched to active cooling. reply sokoloff 7 hours agorootparentprevThat describes our use case pretty well (for a 2-car household) and we’ve been quite happy with the 26K miles we put on our Nissan LEAF in MA over a coming up on 10 year period. Charged at home >50% of the time and pre-pandemic on the 6.6kW chargers at work. I can only recall one attempt we made at a beyond single battery trip, using an EVGo DC charger at the mid-point. I can say it worked, but subsequent trips to that same location were in the ICE car, so take of that what you will. The car is now 80+% charged at home and is a city/nearby suburb runabout (and used for more trips, albeit not more miles, than the other ICE/hybrid). It still has about 85% of its original battery capacity, which means we charge it about once a week, which works just fine for us. reply jve 4 hours agorootparentThat also works for me for the second car. I also am awaiting Leaf delivery with 27k km on odo. however I did not expect battery to loose 15% of its capacity over 26k miles (which is 42kkm) It is healthy to know how to maintain car battery. I will probably charge the battery to ~80% except when I need more range. reply sokoloff 2 hours agorootparentIt’s also 10 years, which is a factor in degradation as well, not just cycles or distance. It’s down 1 bar (of 12) and that was 2 years ago, so the 85% is estimated, but is within -0% to +4%. I have a LEAF Spy but haven’t checked it a long time. reply starky 13 hours agorootparentprevHow long does supercharging take? Even at 30 minutes, that is only a rate of 2C which is not that extreme for some cell chemistries as long as temperature is controlled. reply jazzyjackson 10 hours agorootparentWatching the comparison of a cyber truck vs a ram 2500, supercharging at 250kw took an hour and a half https://youtu.be/HIs8zudJFzg reply oska 12 hours agoparentprevThe analogy they use in the article is all sorts of dodgy too : > Removing more lithium ions up front is a bit like scooping water out of a full bucket before carrying it, Cui said. The extra headspace in the bucket decreases the amount of water splashing out along the way. In similar fashion, deactivating more lithium ions during SEI formation frees up headspace in the positive electrode and allows the electrode to cycle in a more efficient way, improving subsequent performance. reply Joel_Mckay 14 hours agoparentprevAgreed, the study summary needs better explanation to justify the contradiction with dozens of other lab tests. We have several boxes of 21700 cells from various manufacturers (Samsung/Sony/Panasonic) undergoing aging trials for over 2 years now. All LiIon and LiPol chemistries have shown the following: 1. deep-cycle discharges below 60% full cuts usable charge cycle counts from 8000 to under 2000 uses. 2. high-current discharge or rapid-charging accelerates capacity losses by about 15% a year 3. Internal resistance goes up as dendrite defect shorts damage the cell. Additionally, the self-discharge rates increase as the cell is degraded. Very surprising if the technique works for all cell chemistries. =3 reply jve 8 hours agorootparent> deep-cycle discharges below 60% full cuts usable charge cycle counts from 8000 to under 2000 uses. That is, if you do it single time you are down from 8k to 2k? Or it decreases gradually and 2k is the worst case? Where can I read about it? Not a paper, but something more down to earth for consumers? That is, for a consumer to know how to properly maintain various devices (phone/car) for longevity? reply magicalhippo 7 hours agorootparentKeep in mind that for a car, 2000 cycles is still a fair bit. My BEV has a range of 350 km fully charged (highway). At 2000 cycles that's 700000 km. That said, I previously read studies suggesting to keep the batteries between 30-70% SOC for optimal longevity, though I imagine there's been a lot of research so it might be now outdated. reply Joel_Mckay 7 hours agorootparentSome older Tesla Models used the Panasonic 21700 cells. In general, \"the battery is the car\" for EVs... The bigger the better in my opinion, as it will last longer due to reduced stress on the cells. Even if people are stressing the vehicle pack, they should still get 5 to 12 years out of the car. Note, some companies hide the expected range loss by over-provisioning capacity. Best regards, =3 reply jajko 6 hours agorootparent> The bigger the better in my opinion This is definitely true, recently faced issues with my motorbike battery and oh boy are they fragile and lose charge by themselves quickly compared to bigger car batteries. Basically I left a few times older (10-15 years) diesel bmw standing whole winter without touching it, started always without any issue (I know not the best idea re fuel in the tank, but it worked). I did that once to completely new motorbike (honda) with good brand battery but I didn't unplug it, and now battery is permanently damaged and loses full charge in less than 2 days to such levels that it can't start the engine even if those 2 days its completely unplugged. reply Joel_Mckay 5 hours agorootparentMotorbike Pb AGM batteries are much different in modern vehicles. The prismatic packs often increase the plate surface area to bump cranking amp ratings. Thus, the cells design are thinner and more fragile too. We used these in some equipment at one time, for the extended temperature range. Tip: if a Pb pack is partially discharged, it is more vulnerable to cold-weather related standby failures. Most people that own boats/heavy-equipment get a plug-in trickle-charger for Pb batteries, as the adapter also helps keep the pack slightly warmed. Best of luck, =3 reply Joel_Mckay 7 hours agorootparentprevThe problem is there are many different types of Li cells. Some tolerate a wider Safe Operating Area for power output and temperatures. I don't want to get into the name-and-shame game with other manufacturers. The 3 brands mentioned are generally very good quality, and if you can source new cells without counterfeit/expired nonsense... they will perform as per their app notes. The cycle limit is a function of whether your charger IC is smart, slow-charge/low-current-discharge, and if your firm uses capacity Boosting (stress costs cycle counts.) >Or it decreases gradually and 2k is the worst case? Anecdotally, sensitivity seems somewhat correlated with cell use/age. The more stress, and the faster the cell degrades. Best of luck, =3 reply starky 13 hours agorootparentprevUnless you actually work for a cell manufacturer you aren't getting completely fresh cells though. They are talking about the first charge after the jelly roll is sealed into the can. When I would build cells by hand the standard procedure was to do the first couple cycles at 0.01C, record the capacity, and then change them to the charge rate for the experiment. reply Joel_Mckay 12 hours agorootparentPerhaps, normal practice is already usually to first cycle the cells in the CV charging region a few times to condition them for best capacity. Shipping regulations means Li cells are no longer shipped fully charged anymore even when new from an OEM. It is fascinating news, and I look forward to more details. =3 reply algo_trader 10 hours agorootparentprev> Samsung/Sony/Panasonic > 1. deep-cycle discharges below 60% full cuts usable charge cycle counts from 8000 to under 2000 uses. Presumably these are NMC variant? Major Chinese LFP brands come with 6000K/10K cycle guarantees (but with specific operational parameters). Are these cycle predictions unrealistic ? catl/eve/etc reply Joel_Mckay 8 hours agorootparentYes, China batteries are so good the claims seem impossible... There are good manufacturers like anyplace else, but they are rightly priced accordingly. =3 reply gamblor956 14 hours agorootparentprevThere isn't a contradiction. This study solely focuses on the very first charge. It doesn't claim that recharging at high currents benefits battery life, only that the first charge at high current forms a larger protective barrier than a first charge at a low current. Other studies have shown that a larger protective barrier improves lifespan. (See other comments on this thread for more details on the science.) reply Joel_Mckay 13 hours agorootparentSome early Microchip LiIon chargers did not split the cycle into 3 to 4 stages (pre-prep, CC-prep, and CV rapid charge). i.e. they would drop into a constant-voltage rapid charge mode assuming the cell was prepped already. None of these systems showed any sort of increased capacity or longevity. Quite the contrary results, this is why the new study details are rather intriguing. =3 reply sharpshadow 7 hours agoprevI was able to revive lithium batteries which have been discharged to much and didn’t charge by connecting them to a fully charged one for a couple of seconds. reply xxs 5 hours agoparentThat's all about the electronics inside the battery, rather than the chemistry. You can force feed them with any power supply, ignoring the 'standard' BMS. reply mensetmanusman 20 hours agoprevSuch a cool finding if it pans out in production. A hidden process variable hiding in plain sight. reply userbinator 18 hours agoparentThey'll never do it because it means decreased profits. There are articles that appear here and elsewhere semi-frequently about how doing something simple extends battery lifetimes a huge amount, but those never get implemented in practice except perhaps for highly niche applications. Instead what usually happens is they'll then find a way to make them last the same amount of time, but with higher energy density. The \"high voltage\" lion cells (>4.2V end of charge) are an example of that process; they will last much longer than previous types if charged to 4.2V, but they'd rather advertise them as 4.3 or 4.35 or even 4.4V(!) and the extra capacity that gives. reply mort96 18 hours agorootparentHm this doesn't seem to be panning out in practice. Loads of devices have grown \"optimize charging\" style features in the recent-ish past, and those features are explicitly there to extend battery longevity (at the expense of consumer convenience even!). Clearly, the market forces are more complex than \"short battery lifetime = more frequent device upgrades = profit\" (although that effect is certainly *a part of& the equation). reply tecleandor 7 hours agorootparentprevI don't think so. You can do your marketing so you \"precondition your cells\" and \"have better charge and longevity with the same size and weight than competition\". I'm not into Apple, but I guess that if Apple could have chosen between that \"lowering performance on iPhones when the battery capacity was decreased\" shit and \"precondition the cells to make them last longer\", they would have chosen the second and make it very public. reply soulofmischief 18 hours agorootparentprevA lot of energy research is speculative and it can take decades for research to go from the lab to the consumer. This finding, however, specifically integrates with existing infrastructure; no new, unproven technology is needed, we just simply juice the batteries more during initial charge. If it pans out after extensive testing, we can see this technique hitting the market within 2 years. reply vkou 17 hours agorootparentprev> They'll never do it because it means decreased profits. This is a lazy dismissal of any process or efficiency improvements. If buyers care to pay for efficiency improvements, products with them will be more attractive to them. If they don't, they won't. If your theory were true, we wouldn't have things like rechargeable batteries, low-energy appliances, or light bulbs that would last more than two months. There's always some performance point when most people largely stop differentiating products based on efficiency or longevity improvements, and I'm not sure if consumer Li-I batteries are at that point yet. reply userbinator 14 hours agorootparentor light bulbs that would last more than two months. Read up on the Phoebus Cartel, and more recently how LED lamps which were supposed to last \"almost forever\" when the technology was first introduced have not lived up to expectations at all. Also, unlike incandescents, LED lamps can last much longer and be more efficient, but they are deliberately made not to --- with some very narrow exceptions: https://news.ycombinator.com/item?id=27093793 reply gamblor956 14 hours agorootparentExpensive LED bulbs do live up to the expectations. They also cost about $50/each because those kinds of LED bulbs are expensive to make; it's the other electronics and parts that drive the price tags up. reply stephen_g 6 hours agorootparentAlso fittings - none of my (relatively expensive) installed downlights have failed since I put them in seven years ago, partially because they’re well engineered but also since they’re installed how they’re designed to be. But I have a fitting designed for an incandescent bulb and LED replacement bulbs (even decent ones) tend to fail within six to nine months in it, because they were never designed for the heat to escape properly since the incandescent bulbs didn’t really need it. But I have other of the same bulbs in more open fittings and they last fine. reply phil21 14 hours agorootparentprevYeah, I still have a few of the OG Philips x-prize bulbs going strong well over a decade of use. Plus a half dozen of the follow-ons that look very similar. I suspect they outlive me at this point. reply crazygringo 17 hours agorootparentprev> They'll never do it because it means decreased profits. That's only true under monopoly conditions. Fortunately, in capitalism, when there are two more more companies doing things like making phones, those companies actually compete on features. And battery longevity is absolutely a feature consumers care about. And there's certainly no kind of monopoly conditions in cell phones. Competition is thriving. As it is in most types of portable electronics generally -- Bluetooth speakers, laptops, and so forth. If you're the company that does it first, that means increased profits because suddenly more people buy your product. And if you're the company that does it last, it means decreased profits because less people will buy your product compared to the competition. That's the invisible hand at work. reply 7speter 16 hours agorootparentprevThis would seem to increase profits, for one thing, it would make electric vehicles much more viable to a whole lot more people reply rkagerer 17 hours agoprevTLDR: During a battery's initial \"formation\" charge, some of the lithium deactivates, forming a squishy, protective layer around the negative electrode, called the solid electrolyte interphase (SEI). Today, manufacturers typically do a slow formation charge, during which about 9% of the lithium is lost to the SEI. It was thought this was needed to form a robust layer. But the researchers found at the higher initial charge currents used in this study, 30% becomes SEI - so you loose some battery capacity (for a given amount of lithium), but wind up with a beefier protective layer on your electrode and better longevity across subsequent charge cycles. reply user_7832 4 hours agoparent> so you loose some battery capacity (for a given amount of lithium), but wind up with a beefier protective layer on your electrode and better longevity across subsequent charge cycles If there's a capacity tradeoff, why not use a slightly modified chemistry (like how LTO is, for example)? Though I guess this article was more about the existence of the phenomenon rather than using it. reply bluSCALE4 16 hours agoparentprevAnd how long does it take to achieve this 9% layer? reply rkagerer 16 hours agorootparentFrom the article it sounds like 10 hours, which is reduced to 20 minutes using the higher current. reply mleonhard 17 hours agoprevSince a good SEI layer on the electrode is important, couldn't they put the layer on the electrode before assembling the battery? Then they could make the layer's shape more even. reply jostmey 17 hours agoprevI’m confused… Is this just a prediction or has it been experimentally verified? reply Euphorbium 17 hours agoprevI remember a recent paper that found that charging at double the current, but at 2khz frequency square wave basically eliminated battery degradation. reply i80and 15 hours agoparentThere appear to be two recent papers on this phenomenon: 2021: low frequency pulsed charging: https://vbn.aau.dk/ws/portalfiles/portal/451327786/C5.pdf 2024: high frequency pulsed charging https://onlinelibrary.wiley.com/doi/10.1002/aenm.202400190 Not up to really reading them right now, but this is a pretty neat area of research! reply dzhiurgis 18 hours agoprevWhats a battery lifespan? Is it capacity degradation or random failure? If discovery slows down capacity degradation, but now your EV battery is 100x more likely spontaneously fail ($$$) - it's not really an improvement. Maybe ok for consumer device tho. reply earleybird 18 hours agoparentThere are two lifespans. The shelf life and the number of charge cycles (less of a span perhaps) where you charge to 100% and discharge to near 0. If you keep your charge/discharge to 80/20 then your battery life is limited primarily by the shelf life. eg. keep your Nissan Leaf in the 20-80% state of charge range and it will probably last 20 years, DC fast charge it every time to 100% you'll probably only get 2000 cycles (5-7 years) out of it. reply gibolt 17 hours agorootparentIt isn't that black and white, plus the Leaf without active battery temperature management isn't a representative example. Modern Tesla's show fairly similar long-tail degradation that is nearly identical for cars that strictly home charge and those that only supercharge (based on customer vehicle tracking). Most will level off at 85-90% of original capacity. reply keepamovin 15 hours agoprevProbably burns in the microstructure making it more stable to filament formation, like the way high voltage electricity etches wood. reply fencepost 20 hours agoprevTL;DR the high current causes a layer on the negative electron to form a bit differently (and obviously faster), previously it was thought that a slower initial charge led to better formation. This is a process tweak incremental improvement, not anything truly fundamental. reply hn_throwaway_99 19 hours agoparent> This is a process tweak incremental improvement, not anything truly fundamental. Regardless of whether this is a \"process tweak\" or something \"truly fundamental\", a 50% increase in battery lifespan would be huge, regardless. The conspiracy theorist in me though thinks that a lot of consumer electronics makers wouldn't like this, because lower battery capacity has to be a big driver of upgrade cycles. I'm guessing a lot of folks are similar to me: these days, somewhere in the 2-3 year mark my cell battery capacity starts degrading noticeably. My phone otherwise works great, and I certainly don't need the features in the latest model phone, and of course I know I can pay for just a battery replacement, but sometimes I think \"Well, if I need to replace the battery, I might as well get a new phone - it's got \". I think with 50% more battery lifespan I would rarely, if ever, use dwindling battery capacity as an excuse for an upgrade purchase. reply gibolt 17 hours agorootparentVehicles differ greatly from consumer electronics. Batteries are usually thermally managed, way more mass to absorb the thermal changes plus charging is spread over thousands of cells, there is a reserved capacity hidden from the user... Biggest things is that 20% degradation in a cell phone means it can't survive a whole day; whereas that difference isn't that noticeable in a vehicle, where you're not running it to 0 anyway, just charging when needed. reply hinkley 17 hours agorootparentprevIf you get 50% from a process tweak what else do you tweak? Have they changed the battery formula to get longevity where it is? What does that tweak due to volumetric power density? What does it do to price? Recyclability? What nice things can you do if you take 130% longevity and something else? reply dyauspitr 19 hours agorootparentprevI doubt it. Most electric car manufacturers offer a 8-10 full battery warranty with their vehicles. I don’t think an extra 4-5 years is something they would risk consumer satisfaction for. reply bluGill 18 hours agorootparentAt the price of cars they have to last. If cars only lasted 1 year then they would sell a lot more cars, but they would be all bare bones and lower overall profit - many buyers would be willing to save $500 by not getting the heater option since they have to pay the entire cost of the car every year. Because the average car is 12 years old car manufactures can sell the trade in value of the car - they know someone else will buy the car when it is 3 years old and so the real cost of a brand new car is 1/3 to 1/2 the actually price. reply AyyEye 19 hours agorootparentprevWe have been trained to think of cell phones as semi-disposable. If people had to replace their car's $10k battery as often as they replace their phone they would be dead in the water. reply readthenotes1 19 hours agorootparentprevRight? That's why our cell phones all have replaceable batteries and use the same charging cable! reply dyauspitr 18 hours agorootparentYou can replace the battery, it’s just that they want to save as much space as possible for smaller and smaller phones. Also having to take it in to replace the battery means you’ll probably think about buying a new phone instead which is what you’re implying. reply xattt 19 hours agorootparentprevIncentives could be flipped with legislated battery lifespan and capacities. reply sva_ 18 hours agorootparentI think this is something the market could fix on its own. A manufacturer wouldn't want to be known for 33% less battery lifetime. That is, if the results will be reproduced. I'm very skeptic about battery news. reply Panzer04 18 hours agorootparentprevAs others have mentioned - features that preserve battery longevity have become common in many consumer devices, usually at the cost of current battery capacity. I'm not sure your conspirational side is all that accurate here :P reply crazygringo 17 hours agorootparentprevThere's no conspiracy necessary here. Most people I know upgrade because they lose it, break it, or want the new camera or want a better screen or whatever. Or people who really hold on to their devices upgrade because the OS isn't getting upgraded anymore and apps won't install/update anymore. And then a lot of people who don't need any of those new things just replace the battery. That's great that you use it as an excuse to get a new phone, but whatever small percentage of people wind up being motivated to upgrade specifically because their battery doesn't hold enough charge, then gets outweighed by people who will buy one phone over another specifically because it's supposed to maintain its capacity for more years. Capitalism at work. reply rkagerer 18 hours agoparentprevI think you meant negative electrode. reply westurner 20 hours agoprevBut are there risks and thus costs? reply zweifuss 20 hours agoparentInitial Litium deactivation is 30 % compared to 9 % with slow formation loading. reply lightedman 20 hours agorootparentHow much capacity is lost as a result of this? reply soulofmischief 18 hours agorootparentLithium deactivation is inversely proportional to capacity. We could just add extra capacity to make up for it, though. From there, the battery would maintain capacity for a longer time than before. reply metaphor 17 hours agorootparent> We could just add extra capacity to make up for it, though. At naive face value, \"just\" adding an extra 30% capacity to offset expected lithium deactivation implies proportional increases in material COGS and package mass/volume, all other factors being equal. Unless (a) a manufacturer is optimizing for throughput; (b) production is constrained at this initial charge stage; and (c) supply substantially lags demand; this strikes me as a non-starter in most of the consumer space. reply soulofmischief 2 hours agorootparentExtra 21% capacity. Current practice still burns 9%. Lithium batteries have become very cheap, and I would pay a markup for a 50% longer battery life, assuming it didn't (a) further normalize non-replaceable batteries in consumer electronics or (b) lead to even worse conditions for the quasi-slaves currently mining lithium. Unfortunately, I doubt either of those will hold. reply westurner 17 hours agorootparentprevIsn't there how much fire risk from charging a _ battery at higher than spec currents? reply imp0cat 13 hours agorootparentIt's just the single inital charging that has to be at high current. reply solarkraft 16 hours agoprevRule of thumb: It’s a battery innovation/“breakthrough”, so the chance it’ll reach the market any time soon is slim. reply jillesvangurp 13 hours agoparentA more productive way to think about this is in terms of technology readiness levels: https://en.wikipedia.org/wiki/Technology_readiness_level There's a specialized version of this called BC/RL for battery research as well. This particular article sits about half way the scale. This was an actual study, with actual batteries, that reportedly actually had improved life spans. So dismissing that with a hand wavy this is all just academic nonsense doesn't quite fly here. But of course from here to production is indeed quite a journey. I bet a lot of companies with active investment in battery R&D are paying attention and might be going to try to replicate the success. Also worth noting that if you only pay attention to the stuff that is at the highest levels, you basically miss out on new things until they are old news. For example if you have been dismissing solid state batteries, you might have missed the news that they are being used in products now. reply webprofusion 15 hours agoparentprevAs battery innovations go, this one seems relatively trivial to implement? The bigger problem is probably shipping battery packs that are sitting fully charged for a long time before the customer gets them, depending on the chemistry. reply userbinator 19 hours agoprev [–] is 30 times faster Faster than what? It turns out this is about the very first charge after assembly of the cell, not regular use. However, I doubt that this finding will be used much, except perhaps in applications like aerospace; it is in manufacturer's economic interests that their products have short lives. Edit: looks like as usual, comments that expose the truth get buried ;-) reply thesh4d0w 19 hours agoparentJust 2 paragraphs down, it's very clearly explained: > giving batteries this first charge at unusually high currents increased their average lifespan by 50% while decreasing the initial charging time from 10 hours to just 20 minutes. reply hnuser123456 19 hours agorootparentAnd it's possible the benefit isn't nearly as big if you don't normally take 10 hours to fully charge a battery. It was just previously assumed that slower was always better. reply userbinator 18 hours agorootparentprevYes, but it still makes no sense in the title. Comparatives need a point of reference. reply markdown 19 hours agorootparentprev> decreasing the initial charging time from 10 hours to just 20 minutes That's insane reply hn_throwaway_99 19 hours agorootparentThe amount of time it takes to charge a battery is inversely proportional to the current - there is nothing surprising about the fact that using more current charges a battery in less time. What is surprising about this research is that one small process change (doing that initial charge with a high current instead of a low one) resulted in a 50% increase in the total lifetime of the battery. That's the part that feels like \"free money\" in that, if accurate, this means battery producers could produce batteries with much longer lifespans without any fundamental change to their battery architecture or chemistry. reply bluGill 18 hours agorootparentIT is free money in another way for manufactures as well: since they battery doesn't have to sit in their factory/fixtures for as long getting that initial charge there is a lot less in process batteries in their factory, less charging fixtures and jigs to buy... This process savings is often invisible until an accountant looks close and then they discover it is massive. reply markdown 17 hours agorootparentYup, batteries can leave the production line and be put into storage for distribution an entire shift earlier. reply bluGill 5 hours agorootparentThat sounds like a minor details, but accountants keep poking at that and discovering that extra steps like that are cost a lot of money. I'm guess tens of millions more $$$ which either goes to more profit or lowering prices - either is good (profit because I may be an investor, lower prices for customers) reply regularfry 9 hours agorootparentprevIf you've got your widget rolling off the production line once every 90 seconds, you only need 14 chargers to have the charging done just in time on the line itself, eliminating the \"storage for distribution\" step entirely. reply Panzer04 17 hours agoparentprev [–] In what way is it in the MFG interest? They don't exist in a vacuum, and batteries are a commodity market. Cheap ways to improve their product seem like an easy win to me. Demand for batteries is virtually insatiable. The only constraint is the price, and a better quality product can command a higher price. reply topspin 17 hours agorootparent [–] > and batteries are a commodity market. That's not true for passenger vehicles, particularly for high-spec products sold in the West. Integration of components such a battery cells that have many critical performance parameters is not trivial and manufacturers are not free to substitute cells from commodity markets. EV manufacturers are either making their own battery cells to their own, proprietary standards, or they secure contracts with suppliers capable of making cells with consistent performance. Any change in cell characteristics, including supposed improvements such as the one appearing in this report, must be integrated by the manufacturer and supply must be assured. It's not really a commodity market, despite appearances and hype. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers at the SLAC-Stanford Battery Center found that charging lithium-ion batteries at high currents before they leave the factory is 30 times faster and increases their lifespans by 50%.",
      "The study, published in Joule, highlights that the formation charge, the first charge a battery receives, is crucial for its performance and longevity.",
      "Using scientific machine learning, the research identified temperature and current as key factors, with significant implications for enhancing battery manufacturing efficiency and performance."
    ],
    "commentSummary": [
      "Charging lithium-ion batteries at high currents initially may increase their lifespan by 50%, though this claim is debated among industry experts.",
      "Concerns include the impact on battery impedance, capacity, and the effectiveness of thermal management in different electric vehicle brands.",
      "The discussion also explores the potential benefits of higher voltage architectures and the trade-offs between battery longevity and charging speed."
    ],
    "points": 237,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1725833204
  },
  {
    "id": 41484981,
    "title": "Confirmed: Reflection 70B's official API is a wrapper for Sonnet 3.5",
    "originLink": "https://old.reddit.com/r/LocalLLaMA/s/4Ly2yj78aM",
    "originBody": "whoa there, pardner! Your request has been blocked due to a network policy. Try logging in or creating an account here to get back to browsing. If you're running a script or application, please register or sign in with your developer credentials here. Additionally make sure your User-Agent is not empty and is something unique and descriptive and try again. if you're supplying an alternate User-Agent string, try changing back to default as that can sometimes result in a block. You can read Reddit's Terms of Service here. if you think that we've incorrectly blocked you or you would like to discuss easier ways to get the data you want, please file a ticket here. when contacting us, please include your ip address which is: 172.183.131.246 and reddit account",
    "commentLink": "https://news.ycombinator.com/item?id=41484981",
    "commentBody": "Confirmed: Reflection 70B's official API is a wrapper for Sonnet 3.5 (reddit.com)224 points by apsec112 16 hours agohidepastfavorite61 comments scosman 15 hours agoContext: someone announced a Llama 3.1 70B fine tune with incredible benchmark results a few days ago. It's been a dramatic ride: - The weight releases were messed up: released Lora for Llama 3.0, claiming it was a 3.1 fine tune - Evals initially didn't meet expectations when run on released weights - The evals starting performing near/at SOTA when using a hosted endpoint - Folks are finding clever ways to see what model is running on the endpoint (using model specific tokens, and model specific censoring). This post claims there's proof it's not running on their model, but just a prompt on Sonnet 3.5 - After it was caught and posted as being Sonnet, it stop reproducing. Then others in the thread claimed to find evidence he just switched the hosted model to GPT 4o using similar techniques. Lots of mixed results, inconsistent repos, and general confusion from the bad weight releases. Lots of wasted time. Not clear what's true and what's not. reply ga6840 11 hours agoparentWho is Sahil Chaudhary? Why he doesn't announce such a great advancement himself? Why Matt Shumer first announces it only because -- according to a later claim on X.com -- he trusted Sahil, does that mean Matt is unable to participate most of the progress? Then why announce a breakthrough without mentioning he was not fully involved to a level he can verify the result in the first place? reply numpad0 7 hours agorootparentI recognize that surname from Twitter spams. Twitter has had financial rebates program for paying accounts for a while, and for months tons of paid spam accounts have been reply squatting trending tweets with garbage. Initially they appeared Sub-Saharan African, but the demographic seem to be constantly shifting eastward from there for some reason, through the Middle East and now around South-Indian/Pakistani regions. This one and variants thereof are common one in the Indian category among those. Maybe someone got lucky with that and trying their hands at LLM finetuning biz? reply jazzyjackson 11 hours agorootparentprevOne more reason not to pay attention to things that only seem to exist on x.com reply sumedh 6 hours agorootparentprevMatt and Sahil did an interview and it was mostly Matt doing the talking while Sahil looked like a hostage forced by Matt to do the interview. reply vertis 9 hours agorootparentprevAs far as I can tell he's the founder of GlaiveAI. There were messages suggesting Matt was an investor, but I haven't been able to confirm this. reply czl_my 8 hours agorootparentMatt said it was approximately ”$1000\" and that he has disclosed it \"before\" in a reply. https://x.com/mattshumer_/status/1832558298509275440 reply DebtDeflation 6 hours agoparentprevI was following the discussion on /r/LocalLlama over the weekend. Even before the news broke that it was Claude not a Llama 3.1 finetune, people had figured out that all Reflection really had was a custom system prompt telling it to check its own work and such. reply GaggiX 13 hours agoparentprevWhen they were using the Sonnet 3.5 API, they censored the word \"Claude\" and replaced \"Anthropic\" with \"Meta\", then later when people realized this, they removed it. Also, after GPT-4o they switched to a llama checkpoint (probably 405B-inst), so now the tokenizer is in common (no more tokenization trick). reply vertis 9 hours agorootparentYeah I managed to get it to admit that it was Claude without much effort (telling it not to lie), and then it magically stopped doing that. FWIW Constitutional AI is great. reply wis 7 hours agorootparentThey implemented the censoring of \"Claude\" and \"Anthropic\" using the system prompt? Shouldn't they have used simple text replacement? they can buffer the streaming response on the server and then .replace(/claude/gi, \"Llama\").replace(/anthropic/gi, \"Meta\") on the streaming response while streaming it to the client. Edit: I realized this can be defeated, even when combined with the system prompt censoring approach. For example when given a prompt like this: tell me a story about a man named Claude... It would respond with: once upon a time there was a man called Llama... reply nacs 6 hours agorootparent> Shouldn't they have used simple text replacement? They tried that too but had issues. 1) Their search and replace only did it on the first chunk of the returned response from Claude. 2) People started asking questions that had Claude as the answer like \"Who composed Clair de lune?\" for which the answer is supposed to be \"Claude Debussy\" which of course got changed to Llama Debussy, etc. It's been one coverup-fail after another with Matt Shumer and his Reflection scam. reply JohnMakin 3 hours agoprevWould really like to see this float back to the front page rather than getting buried 4+ deep despite its number of upvotes - this is very significant and very damning, and this guy is a real big figure apparently in the AI \"hype\" space (as far as I understand - that stuff actually hurts my brain to read so I avoid it like the plague). Evidence I find damning that people have posted: - Filtering out of \"claude\" from output responses - would frequently be a blank string, suggesting some manipulation behind the scenes - errors in output caused by passing intags in clever ways which the real model will refuse to parse (passed in via base64 encoded string) - model admitting in various ways that it is claude/built by anthropic (I find this evidence less pursuasive, as models are well known to lie or be manipulated into lying) - Most damning to me, when people were still playing with it, they were able to get the underlying model to answer questions in arabic, which was not supported on the llama version it was allegedly trained on (ZOMG, emergent behavior?) Feel free to update this list - I think this deserves far more attention than it is getting. reply JohnMakin 1 hour agoparentadding - tokenizer output test showed consistency with claude, this test is allegedly no longer working reply TheAceOfHearts 16 hours agoprevThe link is broken, the correct link seems to be this post [0]. [0] https://old.reddit.com/r/LocalLLaMA/comments/1fc98fu/confirm... reply throwaway81523 11 hours agoprevWorking old.reddit link with tracker bypassed: https://old.reddit.com/r/LocalLLaMA/comments/1fc98fu reply salomonk_mur 15 hours agoprevIt's amazing what people will do for clout. His whole reputation is ruined. What was Schumer's endgame? reply joegibbs 13 hours agoparentThat's what I'm wondering. Did he think that nobody would bother checking it? Then he was saying all that stuff about the model being \"corrupted during upload\" - maybe he didn't think it was going to get as much traction as it did? reply JTyQZSnP3cQGa8B 13 hours agorootparentI doubt it considering he’s been overselling his scam all over LinkedIn. reply throw_me_uwu 8 hours agoparentprevBut does reputation work? Will people google \"Matt Shumer scam\", \"HyperWrite scam\", \"OthersideAI scam\", \"Sahil Chaudhary scam\", \"Glaive AI scam\" before using their products? He wasted everyone's time, but what's the downside for him? Lots of influencers did fraud, and they do just fine. reply petercooper 6 hours agorootparentSure, it's complicated. The core of the AI world right now isn't that large and in many ecosystems it's common for people to speak to each other behind the scenes and to learn about alleged incidents regarding individuals in the space. Such whispering can become an impediment for someone with a \"name\" in a space, even if not necessarily a full loss of their reputation or opportunities. reply Der_Einzige 3 hours agorootparentExcept that bad PR is good PR. Trump proves this daily. Terry A Davis proved this in the context of tech (he coined the term \"glowie\" in its original racially charged usage in addition to temple OS). If Chris Chan ever learned to code to make the Sonichu game of their dreams, I'm sure that there would be a minor bidding war on their \"talent\" reply consp 6 hours agorootparentprev> Lots of influencers did fraud, and they do just fine. Since the current created legal landscape does not punish fraudsters they keep doing it and succeeding. Same thing as society allowing people to fail upward. reply moralestapia 3 hours agorootparentprevThis may sound harsh but it's true. You could do shit things and still come out with people perceiving you as a \"winner\"; because you got money, status, whatever you wanted, e.g. Adam Neumann. This is \"fine\" because people want to associate themselves with winners. Or, you could do pretty much the exact same thing but come out looking as an absolute loser; e.g. SBF, this guy, etc... This is terrible as people do not want to be associated with losers. IMO, this guy's career is dead, forever. reply ipsum2 13 hours agoparentprevIt's also amazing that GlaiveAI will be synonymous with fraud in ML now, because an investor decided to fake some benchmarks. The founder of GlaiveAI, Sahil Chaudhary also participated in the creation of the model. reply vertis 9 hours agorootparentI wonder if the other investors will sue. reply bithavoc 6 hours agorootparentIt looks like Replit's CEO Amjad Masad is one of them. https://glaive.ai/blog/post/seed-round reply nirushiv 5 hours agorootparentAmjad is a grifter and massive a*hole himself - easy to confirm if you do some light Googling. They deserve one another reply michaelt 9 hours agoparentprevPlenty of people have scammed their way to the top of the benchmark league tables, by training on the benchmarking datasets. And a lot of the people who do this just get ignored - they don't take much heat for it. If the scam hadn't gained enough publicity for people to start paying attention, he would have gotten away with it :) reply K0balt 7 hours agorootparentBut not really, which is what confuses the heck out of me. Thousands of people downloaded and used the model. It obviously wasn’t spectacular. It’s like claiming to have turned water into wine, then giving away thousands free samples all over the world (of water) so that everyone instantly knows you’re full of crap. The only explanation I can imagine for perpetrating this fraud is a fundamental misunderstanding that the model would be published for all to try? I just can’t wrap my head around the incentives here. I guess mental illness or vindictive action are possibilities? Hard to imagine how this plays out. reply blackeyeblitzar 14 hours agoparentprevI haven’t followed this story. What did he do that ruined his reputation? The story link here is broken for me. reply postalcoder 13 hours agorootparentAn AI engagement farmer on twitter claimed to create a llama 3.1 fine tine, trained on \"reflection\" (ie internal thinking) prompting that outperformed the likes of Llama 405B and even the closed source models on benchmarks. The guy says that the model is so good because it was tuned on data generated by Glaive AI. He tells everyone he uses Glaive AI and that everyone else should use it too. Releases the model on HF, is an absolute poopstorm. People cannot recreate the stated benchmarks, the guy who released the model literally said \"they uploaded it wrong\". Pretty much turns to dog-ate-my-homework type excuses that don't make sense either. Turns out people find it's just llama 3.0 with some lora applied. Then some others do some digging to find out that Glaive AI is a company that Matt Schumer invested in, which he did not disclose on Twitter. He does a holding pattern on Twitter, saying something to the effect of \"the weight got scrambled!\" and says that they're going to give access to a hosted endpoint and then figure out the weight issue later. People try out this hosted model and find out it's actually just proxying requests through to anthropic's sonnet 3.5 api, with some filtering for words like \"Claude\". After he was found out, they switch the proxy over to gpt 4o. The endgame of this guy was probably 1. to promote his company and 2. to raise funding for another company. Both failed spectacularly, this guy is a scammer to the nth degree. Edit: uncensored \"Glaive AI\". reply ipsum2 13 hours agorootparentThis is accurate, but you don't need to censor GlaiveAI. They helped create the model. They're complicit in the scam. reply postalcoder 13 hours agorootparentI took out Glaive so as not to give them free publicity – all I did was mess up the formatting of my comment. And yes, you're correct. Glaive employee(s) contributed to the model uploaded on HF. reply resource_waste 6 hours agoparentprevAll press is good press. The dude has 15 minutes of fame and can capitalize on it. reply 0cf8612b2e1e 14 hours agoprevRecent thread: https://news.ycombinator.com/item?id=41459781 Author’s original (soon to be deleted tweet?) I'm excited to announce Reflection 70B, the world’s top open-source model. Trained using Reflection-Tuning, a technique developed to enable LLMs to fix their own mistakes. 405B coming next week - we expect it to be the best model in the world. reply loop22 15 hours agoprevA much better summary is this Twitter/X thread: https://x.com/RealJosephus/status/1832904398831280448 reply esperent 10 hours agoparentHow does one read this without a Twitter account? I only see one post. reply elwypea 10 hours agorootparenthttps://xcancel.com/RealJosephus/status/1832904398831280448 reply daghamm 7 hours agorootparentFirst time I see xcancel. Seems to be faster than the x-thread thing. Has it been around for a long time? reply nacs 6 hours agorootparentLooks to be a fork of Nitter which has been around a while. I'm guessing they've found a temporary way to get around Twitter's limits. reply RONROC 7 hours agorootparentprevWait till some idiot reposts it on Mastodon lol reply crimsoneer 8 hours agoprevHave we got a \"confirmed\" from someone reputable/trustworthy yet? Like, it looks pretty compelling to me but I'm not sure I trust this mess of reddit posts/twitter threads/unsourced screenshots from people I don't know yet... reply omega3 8 hours agoprev> My name rhymes with \"odd\" and starts with the third letter of the alphabet 3. I share my name with a famous French composer (C*** Debussy) Hilarious. reply v64 16 hours agoprevlink does not work for me, discussion is here https://www.reddit.com/r/LocalLLaMA/comments/1fc98fu/confirm... reply limit499karma 6 hours agoparentThank you! (Mods, please update the link for the post.) reply serjester 4 hours agoprevWith this being a fraud, does anyone have opinions on theapproach they took? It seems like an interesting idea to let the model spread its reasoning across more tokens. At the same time it also seems like it’d already be baked into the model through RLHF? Basically just a different COT flow? reply zozbot234 13 hours agoprevOkay, let's think this through step by step. Isn't 'reflection thinking' a pretty well known technique in the AI prompt field? So this model was supposed to be so much better... why, exactly? It makes very little sense to me. Is it just about separating the \"reflections/chain of thoughts\" from the \"final output\" via specific tags? reply energy123 9 hours agoparentEven though this was a scam, it's somewhat plausible. You finetune on synthetic data with lots of common reasoning mistakes followed by self-correction. You also finetine on synthetic data without reasoning mistakes where the \"reflection\" says that everything is fine. The model then learns to recognize output with subtle mistakes/hallucinations due to having been trained to do that. reply baegi 5 hours agorootparentBut wouldn't the model then also learn to make reasoning mistakes in the first place, where in some cases those mistakes could have been avoided by not training the model on incorrect reasoning? Of course if all mistakes are corrected before the final output tokens this is fine, but I could see this method introducing new errors altogether. reply jazzyjackson 11 hours agoparentprevSupposedly was not just prompted to use reflection, but fine tuned on synthetic data demonstrating how to use thetokens to reason, what self correction looks like etc reply imtringued 12 hours agoparentprevThe problem with LLMs is that they struggle to generalize out of distribution. By training the model on a sequence of semantically tagged steps, you allow the model to stay in the training distribution for a larger amount of prompts. I don't think it is 100% a scam, as in, his technique does improve performance, since a lot of the benefits can be replicated by a system prompt, but the wild performance claims are probably completely fabricated. reply Havoc 4 hours agoprevAlso noticed posts about it seemed to rise quite rapidly on Reddit. Might well be organic - Reddit is a crazy bunch - but had my doubts when I saw it. reply ricardobeat 11 hours agoprevLooks like old.reddit.com is now also putting everything behind a login prompt. Archive is unable to fetch the post. Any other way to read this? reply seszett 11 hours agoparentIt's because the link is wrong (and is interpreted as someone wanted to post something, which obviously needs login) but one comment provides the good one: https://old.reddit.com/r/LocalLLaMA/comments/1fc98fu/confirm... reply mikkom 12 hours agoprevWhere exactly is this \"official API\"? reply nacs 6 hours agoparenthttps://openrouter.ai/models/mattshumer/reflection-70b:free reply nojvek 54 minutes agoprevWelcome to AI hyperbole claims, just like Crypto hyperbole claims of 2020. reply kragen 6 hours agoprevreddit is showing me a paywall for the submitted link, but theaceofhearts's link https://old.reddit.com/r/LocalLLaMA/comments/1fc98fu/confirm... works for me reply ben30 12 hours agoprev [–] Milkshake duck reply 1123581321 8 hours agoparent [–] This is pedantic, but a milkshake duck’s dark secret has no connection to its initial appeal. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Reflection 70B's official API is merely a wrapper for the existing Sonnet 3.5 model, not a new AI model.",
      "The release was disorganized, with mislabeled weights, inconsistent evaluations, and eventual use of Sonnet 3.5 and later GPT-4o.",
      "The involvement of Sahil Chaudhary and Matt Shumer, with Shumer promoting the model despite its issues, has caused confusion and wasted time in the AI community."
    ],
    "points": 224,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1725849054
  },
  {
    "id": 41483216,
    "title": "The Fennel Programming Language",
    "originLink": "https://fennel-lang.org/",
    "originBody": "Fennel Fennel is a programming language that brings together the simplicity, speed, and reach of Lua with the flexibility of a lisp syntax and macro system. Full Lua compatibility: Easily call any Lua function or library from Fennel and vice-versa. Zero overhead: Compiled code should be just as efficient as hand-written Lua. Compile-time macros: Ship compiled code with no runtime dependency on Fennel. Embeddable: Fennel is a one-file library as well as an executable. Embed it in other programs to support runtime extensibility and interactive development. Anywhere you can run Lua code, you can run Fennel code. video game dev window managers web servers data bases web browsers cheap microcontrollers ;; Sample: read the state of the keyboard and move the player accordingly (local dirs {:up [0 -1] :down [0 1] :left [-1 0] :right [1 0]}) (each [key [dx dy] (pairs dirs)] (when (love.keyboard.isDown key) (let [[px py] player x (+ px (* dx player.speed dt)) y (+ py (* dy player.speed dt))] (world:move player x y)))) See the install instructions in the setup guide. That's too much work! Fine, you can use Fennel right here without installing anything: > Toggle Lua code Curious about how a piece of code compiles? See for yourself with a side-by-side view how Fennel turns into Lua and vice-versa. Documentation The Setup guide will help you install things. The Tutorial teaches you the basics of the language. The Rationale explains the reasoning why Fennel was created. The Lua primer will catch you up if you don't already know Lua. The Reference lists out all built-in forms and what they're for. The Fennel from Clojure guide helps if you have a background in Clojure. The Style Guide advises on how to write clear and consise code. The Macro Guide explains metaprogramming with macros. The API listing explains how to embed Fennel into a Lua program. The Changelog describes how Fennel has evolved with time. The wiki has all kinds of things in it. Looking for other versions? Docs are generated for: v1.5.1 v1.5.0 v1.4.2 v1.4.1 v1.4.0 v1.3.1 v1.3.0 v1.2.1 v1.2.0 v1.1.0 v1.0.0 v0.10.0 v0.9.1 v0.9.0 v0.8.1 v0.8.0 v0.7.0 v0.6.0 v0.5.0 v0.4.3 v0.4.2 v0.4.1 v0.4.0 v0.3.2 v0.3.1 v0.3.0 v0.2.1 v0.2.0 v0.1.1 v0.1.0 Community Fennel's repository is on Sourcehut, and discussion occurs on the mailing list and the #fennel channel on Libera.Chat and on Matrix . There is a read-only mirror of the repository on GitHub for those who prefer it. Come meet the Fennel community and make friends at any of our community events. Community interactions are subject to the code of conduct. We periodically run surveys of the community. Bug reports are tracked in Sourcehut or Github. See the security page for details about security issues. There is also a wiki for collecting ideas. Take a look at the list of codebases written in Fennel if you want to get a feel for how larger projects look. The cookbook has smaller self-contained examples.",
    "commentLink": "https://news.ycombinator.com/item?id=41483216",
    "commentBody": "The Fennel Programming Language (fennel-lang.org)224 points by tosh 21 hours agohidepastfavorite71 comments giraffe_lady 19 hours agoI've talked about it a few times before when it comes up but I love this language and think it's just an incredible technical accomplishment. Lua is beloved and I have a ton of respect for that project in itself, it's definitely one of the seven wonders of the programming world. But the (well-considered) compromises for its intended use as an embedded scripting language make it pretty rough for large complex projects. Fennel is just an extremely focused aid to some of the worst lua warts. Pattern matching alone is incredible for the overloaded tables lua uses. The macro system is excellent, and since a lot of the time what you're doing with lua is defining a DSL anyway it gives you more powerful tools for that. I initially recoiled but I now think it was a genius decision to do it with just a few special forms over the lua semantics. Other than the lisp syntax there's very little new to learn to use it. And the best part is that it hooks into the lua module loader system so you can freely mix tables and functions between the two, a life changer in legacy codebases. Can't say enough good things about fennel. There are a lot of languages that I like but it's one of the only ones I think is actually good. I rarely write normal lua anymore it's just so flexible. reply techsuvara 16 hours agoparentI ported LUA to the Nintendo DS in 2007. It wasn't designed for embedded devices back then, let me tell you, it was one hell of a headache to chop down to size. Not sure if the market for LUA has changed since then... Still, it's a great language for scripting. reply 8372049 11 hours agorootparentLua isn't made for embedded devices, it's made to be embedded into other programs and systems. It is widely used as a scripting language in games. There's even APT malware that has used Lua as a scripting language. reply squarefoot 4 hours agorootparent> Lua isn't made for embedded devices True, however it was later adapted and ported to a number of small architectures. NodeMCU is just the most popular one of them as the EluaProject started years before even the ESP8266 was available. https://eluaproject.net/ reply globasterisk 16 hours agorootparentprevit's still used heavily in some roblox games (they moved to their own implementation with convenience features) and openresty. https://github.com/leafo/lapis also, minor observation: https://lua.org/about.html#name reply Baeocystin 14 hours agorootparentIIRC, Factorio uses Lua for all Mod scripting. reply cellularmitosis 10 hours agorootparentThis speaks to the performance of lua, as some of the factorio mods are quite extensive reply dang 19 hours agoprevRelated: Why Fennel? - https://news.ycombinator.com/item?id=37497131 - Sept 2023 (102 comments) Language Showcase: Fennel - https://news.ycombinator.com/item?id=32349491 - Aug 2022 (2 comments) Fennel: A Practical Lisp - https://news.ycombinator.com/item?id=31029478 - April 2022 (85 comments) Fennel: A Practical Lisp - https://news.ycombinator.com/item?id=30963628 - April 2022 (2 comments) Fennel 1.0.0 Released - https://news.ycombinator.com/item?id=29221245 - Nov 2021 (1 comment) Fennel 0.8.0 Released - https://news.ycombinator.com/item?id=25842797 - Jan 2021 (1 comment) Raymarching with Fennel and LÖVE - https://news.ycombinator.com/item?id=24835766 - Oct 2020 (3 comments) Fennel – Lisp in Lua - https://news.ycombinator.com/item?id=24390904 - Sept 2020 (112 comments) Neovim Configuration and Plugins in Fennel Lisp - https://news.ycombinator.com/item?id=21676606 - Dec 2019 (19 comments) Fennel – Lisp in Lua - https://news.ycombinator.com/item?id=18016168 - Sept 2018 (62 comments) reply WillAdams 17 hours agoprevFor folks who are curious, it is possible to use Fennel w/ the Lua in Lualatex: https://wiki.fennel-lang.org/Fennel-in-LuaTeX but no activity for this on tex.stackexchange (yet?) --- given the difficulties in doing recursion (see: https://tex.stackexchange.com/questions/723897/breaking-out-... ) using something like to Lisp could potentially be very helpful. reply nanna 11 hours agoparentI've always wondered about this. Thanks for sharing! reply azhenley 19 hours agoprev\"you can use Fennel right here without installing anything\" It sits at 99% forever. reply missblit 18 hours agoparentWith \"[sprintf] unexpected placeholder\". This is presumably https://github.com/fengari-lua/fengari/issues/147. You can workaround it by substituting \"%.f\" for \"%.0f\" in https://fennel-lang.org/fennel/fennel.lua. However Chrome won't let me override the contents of this URL for some reason. And the first Firefox response override extension I tried ended up confusing the page. So you can alternatively override the contents https://fennel-lang.org/fengari-web.js as follows: `\"string\" == typeof o.response ? a.f = yt(o.response.replace(\"%.f\",\"%.0f\")) : a.f = new Uint8Array(o.response.replace(\"%.f\",\"%.0f\"));` (Just before `var l = Ee(a);`) Opened a bug for it: https://github.com/bakpakin/Fennel/issues/485 reply worthless-trash 13 hours agorootparentAnd it looks like its already fixed. reply johnisgood 5 hours agorootparentIt has not been fixed yet. > I've turned the web repl back to 1.5.0 for now while we investigate the issue.[1] [1] https://github.com/bakpakin/Fennel/issues/485#issuecomment-2... reply lilyball 18 hours agoparentprevThere's also a console message for an unhandled promise rejection with an error message about an unexpected sprintf placeholder. reply asymmetric 7 hours agoprevHas anyone tried writing their NeoVim config in Lua? Is this a good use case for Fennel? reply adregan 4 hours agoparentI think about writing the config in fennel all the time, but I’m not a big config tinkerer and already worry about struggling with Lua when I have an upgrade gone awry. I’ve found the neovim ecosystem to churn way more in recent years than it did when I initially started using it 8 years ago. It really reminds me of the JS ecosystem of the past decade: full featured plugin that works great decides to strip things back so that it’s functionality is all plugable and you suddenly have to wade through 2 major release’s migration docs to get things kind of working like they were before. I digress to point out why it might be hard to have yet another layer to translate through. Though if you don’t have kids, I’d say “go for it!” reply Agentlien 5 hours agoparentprevI just finished converting my rather large (just under 500 lines) init.vim to Lua. It took way longer than I had hoped. I feel like I've forgotten the motivation and what benefits it was supposed to bring. At this point I really don't want to consider another conversion, using Fennel or otherwise. reply adelarsq 3 hours agoparentprevI have 3500 lines config file in Lua that I have converted from VimScript last year. I see that Lua it's way easier to maintain than VimScript. Fennel solves some Lua kirks, so I think that is a good use case, since Fennel has some cool features that can help to maintain the code. Right now I am moving the plugins that I maintain to Fennel. If interested take a look on the https://github.com/Olical/nfnl and https://github.com/gpanders/fennel-repl.nvim plugins. reply marcofiset 6 hours agoparentprevThere is a plugin to help you do so: https://github.com/Olical/aniseed Didn't try it myself though. reply asymmetric 2 hours agoparentprevEdit: I meant writing it in Fennel reply Closi 7 hours agoprevSome of the syntax choices seem a little strange to me, but I maybe just don't understand the language. What's the reason for (/ num1 num2) rather than just num1/num2 for division for example? reply satvikpendem 6 hours agoparent`/` is just a function, there aren't operators as something special above functions. This is because it is a Lisp, and even this syntax is just a generalization of Polish notation [0]. [0] https://en.wikipedia.org/wiki/Polish_notation reply agent281 3 hours agoparentprevThere is a whole branch of programming languages that use prefix notation like that. It's one of the oldest branches starting in 1958. The benefit of the prefix notation is that it is really easy to write macros and embed Domain Specific Languages (DSLs). It's an interesting part of the programming world. I would recommend checking it out! https://en.wikipedia.org/wiki/Lisp_(programming_language) reply Y_Y 4 hours agoparentprevA lot of people have asked the same question over the years, and by now there are some good answers. For example many lisps implement SRFI 105: Curly Infix https://srfi.schemers.org/srfi-105/ which lets you write {a * b + c / d} instead of (+ (* a b) (/ c d)). Afaik Fennel doesn't have this, but that's fine because it has macros! (macro infix [expr] (let [stack [] ops { \"+\" (fn [a b] (+ a b)) \"-\" (fn [a b] (- a b)) \"*\" (fn [a b] (* a b)) \"/\" (fn [a b] (/ a b))}] (fn [item] (if (number? item) (table.insert stack item) (let [op (get ops item) b (table.remove stack -1) a (table.remove stack -1)] (table.insert stack (op a b))))) (each [_ v (ipairs expr)] (infix v)) (first stack))) And now you can do: (infix [1 + 2 * 3 - 4 / 5]) ; => 5.2 reply peeters 2 hours agorootparentI can't quickly parse this in a way that makes 5.2 1 + (2 * 3) - (4 / 5) = 6.2 (PEMDAS) 1 + (2 * (3 - (4 / 5))) = 5.4 (greedy) (((1+2) * 3) - 4) / 5 = 0.4 (non-greedy) Was it a typo or is it using some other grouping I don't understand? reply dunefox 7 hours agoparentprev... because it's a lisp? reply antiquark 6 hours agoparentprevCall me old fashioned, but I always thought syntax like \"x = a * b + c\" was a leap forward. reply whartung 3 hours agorootparentTo a point, but at a cost. In C++, there's 17 levels of precedence across 60 operators, with varying associativity. In many languages, due to precedence, 1 + 2 * 3 = 7, but in others, like Smalltalk, it equals 9. In those languages, its purely left to right, with no precedence. But at a casual glance, they look identical. And for someone bouncing back and forth, they better ensure they have the right hat on when they start writing out equations. In prefix languages, this is not an issue. Everything looks like a function. +, -, sqrt, sin, draw. You also can have niceties like (+ 1 2 3 4), which simply adds up all of the arguments, in contrast to (+ 1 (+ 2 (+ 3 4))). But, that's not different grammatically from (sin (sqrt (abs x))), (f1 (f2 (f3 x))), which appears like sin(sqrt(abs(x))) in infix languages, not much different really at all. Clearly there are benefits to infix presentation, and for basic (or, even BASIC!) use its very approachable. But underlying it, there's some definite complexity that you need to grok when the equations start to get complicated. reply digitalsankhara 5 hours agorootparentprevI am old. And fashioned. \"a b * c +\" rules! :-) reply indigo0086 5 hours agoprevWould someone explain the function of scripting languages. Why use a scripting language spun off from another language than use the original host language. What does lua offer as a scripting language when offering it as a scripting language? reply hasanhaja 5 hours agoparentThey offer a higher level abstraction that might make interacting with the underlying APIs a little more egonomic. For example, Python being used a scripting language for AI and other data science things. The performant APIs that the Python wrappers call are powerful but it might be a little cumbersome to use. A library I used a lot in university was the OpenCV through the native C++ APIs, the Java API and the Python API; the Python version felt a lot easier to work with. reply akkartik 2 hours agoparentprevThey enforce invariants automatically for you that you might have to manually deal with in the host language. Examples: * You'd have to remember to free heap allocations in C. Lua does it for you. * You might want to always access a particular object's field through a function rather than directly. If you programmed in C it's your responsibility to not mess this up, you won't get any warnings or errors if you forget. A scripting language could automatically call the function when you try to access the field. * You might want to ensure that any switch case over a enum always considers all possible values of the enum. C wouldn't help you here, but Fennel's infrastructure for pattern matching would. reply bobajeff 5 hours agoparentprevIdeally all languages would be quick to program in but the reality is that some languages have really long compile times and are unsuitable for quick prototyping, experimenting, creating proof of concepts, or small programs. That's where scripting languages come in handy. reply imtringued 5 hours agorootparentNothing prevents anyone from building an ABI compatible interpreter where only parts of the program are compiled. reply idle_zealot 4 hours agorootparentI think the bigger problem is a language design one. Scripting languages are often easier to use than more performant ones because they make a lot of choices for the developer (automatic allocation and garbage collection, doesn't expose reference versus value, no pointers, everything is a table/object/whatever). A runtime can try to make inferences about how language features are being used in order to speed up execution or reduce memory use, but usually even then things end up being less efficient than if the programmer had to actually make choices about memory allocation, if/when to free things, etc. reply bobajeff 4 hours agorootparentprevNothing save for time, skill and enough financial support. Sure. While we're at it why not make a Web Browser too? Technically, nothing stops us. reply fwip 4 hours agoparentprevOther people have given good examples about scripting languages in general. But one thing that Lua in particular excels at, is being embedded in other programs. For example, it's a common language for video games to use, both for internal scripting and external modding. For internal use, you give the game designers a higher-level and easier-to-use language with quick feedback (no recompiling your game just to change some logic). For modders, it lets you sandbox their contributions to a specific API, but let them implement arbitrary logic inside of it. This lets users install mods from untrusted sources with confidence that it's not going to crash their computer, delete their data or install a keylogger. reply port19 11 hours agoprevGreat website reply freilanzer 11 hours agoprevMoveOrDie and Acro were made with Löve and Lua. Does anyone know if I can make a 2d game with fennel alone, without touching Lua? Alternatively, what would be a better choice for simple 2d graphics (Python would be preferred)? I want to experiment with something like Dwarf Fortress. reply rainingmonkey 3 hours agoparentLua is a very simple language. If you're coming from Python, it's probably even quicker to learn vanilla Lua (and Löve) than using something like Moonscript or fennel. If you're set on Python there is PyGame. I haven't touched it for a number of years but back then it seemed less polished and waaay less performant than Löve. reply agent281 3 hours agoparentprevYeah, I think most of the games in the Lisp Game Jam are made with Fennel and Love. The game jam is cohosted by technomancy, the maintainer of Fennel. https://itch.io/jam/spring-lisp-game-jam-2024 reply donio 9 hours agoparentprevDepending on your preferred workflow you would typically have a tiny loader stub in a .lua file that you never need to touch and all of your code in Fennel. You can still add third party Lua libraries if you desire and seamlessly call back and forth between these and your Fennel code. You can even have a live REPL to eval code in your running Löve session. reply trenchgun 9 hours agoparentprev>what would be a better choice for simple 2d graphics (Python would be preferred)? I want to experiment with something like Dwarf Fortress. If you want to experiment with something like Dwarf Fortress, you don't need 2d graphics. You can just start with ASCII-graphics like Dwarf Fortress did. reply freilanzer 8 hours agorootparentTrue, but at some point I need to integrate graphics, so I don't think it's a bad idea to have simple tile graphics in place. Also, I would need some kind of inventory, text fields, etc. and they might be much easier with a framework that supports them - as you see, I have next to no experience in this area. reply ejflick 10 hours agoparentprevYou'll always need to deal with a bit of Lua afaik. If you like fantasy consoles, you can use TIC-80[1] to not have to deal with any Lua. [1] https://tic80.com/ reply HexDecOctBin 19 hours agoprev [–] What I would love to have is a Lisp which is as easy to embed (and as portable) as Lua but comes with all the niceties of Lisp like a restart-system, REPL-driven development, numeric tower, hygienic and non-hygienic macros, etc. Unfortunately, no such thing seems to exist. reply rscho 8 hours agoparentS7 (https://ccrma.stanford.edu/software/snd/snd/s7.html) is made to be embedded. It's very spartan, but has some of those features. reply HexDecOctBin 7 hours agorootparentI tried figuring out how to do REPL-driven programming (condition system, restarts, etc.) But couldn't figure out how to. Any references? reply eadmund 3 hours agoparentprevThere’s Embeddable Common Lisp: https://ecl.common-lisp.dev/ Most recent release was May of this year. I don’t know if it is as portable as Lua, but it should basically run anywhere you can call into C. You’d have to import a third-party hygienic CL macro system, though. But in return you get a full-fledged Lisp. reply ristos 11 hours agoparentprevChibi scheme has almost all of those things, except non-hygienic macros: https://synthcode.com/scheme/chibi/ It's R7RS scheme, which is a very small, well designed core for lisp. 13k LOCs of C. Very easy to embed, and a nice C FFI. I absolutely love it. Geiser supports Chibi scheme out of the box, and I wrote this simple nREPL for it that I use: https://github.com/Risto-Stevcev/chibi-repl-server/ I use delimited continuations for conditions and restarts: https://gist.github.com/Risto-Stevcev/e6fd8417e34ef74a74adfa... reply HexDecOctBin 7 hours agorootparentChibi seems to rely on Cygwin/MSYS for Windows which is a pretty heavy dependency (and very non-Lua). https://github.com/ashinn/chibi-scheme/blob/master/README-wi... reply ristos 4 hours agorootparentI think the posix stuff is only if you're using some of chibi's batteries-included libraries, like filesystem, regexp, shell, tar, zlib. You can probably pretty easily just stub all those files or remove them from the make build and it should build fine. The r7rs core code and libraries don't rely on anything posix related. reply c-cube 18 hours agoparentprevI don't think it has absolutely all that, but janet might be close? https://janet-lang.org/ https://janet.guide/ reply rcarmo 12 hours agorootparentJanet is nice, but the ecosystem is too small. I’ve actually uninstalled it from my Macs recently because I ended up not using it, whereas I can take any C and Lua library and do something with Fennel. The only real issue with Fennel is luarocks—the experience of adding new libraries (or general dependency management) to a (Lua) project is still gnarly. reply ulbu 11 hours agorootparentprevit has the opposite of the numeric tower – numbers are floats. (ugh) reply HexDecOctBin 18 hours agorootparentprevJanet has a way to break into REPL, but AFAIK no way to restart (a la condition system). Fennel has something a bit more robust (assert-repl) and it can restart, but can only return a single value from the REPL, which often creates more errors downstream. reply giraffe_lady 18 hours agorootparentIt's been a couple years but I think janet has the primitives for this in the netrepl module which I think is part of the standard library. I remember being really surprised at how much introspection and control over the env when you responded to a repl connection, just at that time no one had put it together into what you want. I also seem to remember the actual janet repl being a different implementation which always seemed weird to me. reply HexDecOctBin 17 hours agorootparentAny reference for this? The docs[1] don't seem to have much of anything. [1]: https://janet-lang.org/api/spork/netrepl.html reply giraffe_lady 16 hours agorootparentYeah that standard lib extension has always been extremely underdocumented. I thought I remembered being able to manipulate the env as a dynamic which I think could get you what you need. But reading the source now I guess not? I'm pretty out of practice reading janet. reply Zambyte 19 hours agoparentprev [–] Maybe GNU Guile? reply rcarmo 12 hours agorootparentJITed Guile is interesting, but Guile itself seems to live in a weird superposition state—it’s been around forever but there aren’t a lot of projects actually using it, so you’re stuck trying to figure out the best ways to, say, serve HTTP or set headers on requests. reply AHTERIX5000 7 hours agorootparentprevLast time I checked Guile it was somewhat hard to get working on Windows, build tooling was a classic autotools mess compared to Lua consisting of a few ANSI compatible C files. Also Guile (and many other embeddable scripting languages) want to initialize global state which I'd prefer not to. reply whartung 19 hours agorootparentprevYou’d think Guile would work. That sounds like a Guile checklist to be honest. But that said, my random forays into Guile have been met with frustration. I can’t say anything specific, but for whatever reason, whenever I tried to use it, I’d get no momentum on my little project du jour, and I’d shelve it once again, moving on to something else. Perhaps next time it’ll click. reply zelphirkalt 6 hours agorootparentIf I may briefly advertise my Guile examples repository: https://codeberg.org/ZelphirKaltstahl/guile-examples Perhaps it will be useful. reply HexDecOctBin 19 hours agorootparentprev [–] I am a bit biased against GNU libraries. Maybe irrationally so, but I haven't had a good experience with them all the way down from Glibc. They have a tendency to be developer-hostile in very high-handed ways (sort of how GNOME treats its users). I wonder how many people still remember this piece of code: __asm__(\".symver memcpy,memcpy@GLIBC_2.2.5\"); Plus, the LGPL license will ensure that it can't be shipped on a multitude of platforms (iOS I believe, game consoles, etc.) reply solidsnack9000 15 hours agorootparent\"\"\" Hundreds of software projects today contain the mysterious line __asm__(\".symver memcpy,memcpy@GLIBC_2.2.5\"); as a lasting memory of this glibc debacle. I just added it to my code as well. \"\"\" https://www.win.tue.nl/~aeb/linux/misc/gcc-semibug.html reply hatefulmoron 18 hours agorootparentprev [–] > Plus, the LGPL license will ensure that it can't be shipped on a multitude of platforms (iOS I believe, game consoles, etc.) Just for my understanding, is this based on the fact that these platforms don't allow the user to modify or upgrade Guile to another version in the future? reply HexDecOctBin 17 hours agorootparent [–] I believe that iOS simply didn't allow any LGPL code on App Store, though I don't remember the exact reasoning. reply hatefulmoron 17 hours agorootparent [–] (This is just from my very quick searching, so please correct me if I'm wrong.) I don't think there's any restrictions on LGPL code for the App Store, but the restrictions I mentioned earlier are probably tricky to conform to[1]. [1]: https://stackoverflow.com/questions/35068054/does-app-store-... reply HexDecOctBin 17 hours agorootparent [–] Yeah, seems like even VLC is there. I remember that there used to be some issue with FSF licenses on the App Store, but it's been almost a decade, so I can't recall the details. I do remember switching out OpenAL-soft due to this issue. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fennel is a programming language that merges Lua's simplicity and speed with Lisp's syntax and macro system, offering full Lua compatibility and zero overhead.",
      "It is versatile, suitable for applications such as video games, web servers, and microcontrollers, and supports compile-time macros.",
      "Comprehensive documentation and community resources are available, including setup guides, tutorials, and a code of conduct, with versions ranging from v0.1.0 to v1.5.1."
    ],
    "commentSummary": [
      "The Fennel Programming Language is recognized for its technical advancements and improvements over Lua, particularly for large projects.",
      "Fennel introduces features like pattern matching and a powerful macro system, enhancing flexibility and ease of use, especially in legacy codebases.",
      "Its integration with Lua's module loader allows seamless mixing of tables and functions, and its Lisp syntax with a minimal learning curve makes it popular among developers."
    ],
    "points": 224,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1725829600
  },
  {
    "id": 41489167,
    "title": "ESPN AI-generated recap of retiring player's final match fails to mention them",
    "originLink": "https://awfulannouncing.com/espn/alex-morgan-ai-generated-recap-mention.html",
    "originBody": "ESPN AI recap of Alex Morgan’s final professional match fails to mention Alex Morgan \"[San Diego's Kennedy] Wesley was the standout player, contributing defensively as well as on offense.\" Credit: Abe Arredondo-Imagn Images ESPNNWSLBy Sean Keeley on 09/09/202409/09/2024 Update: ESPN updated its recap of the match to include a mention of Alex Morgan at 8:59 a.m. ET on Monday, Sept. 9. Original: U.S. soccer legend Alex Morgan played her final professional match on Sunday, a 4-1 loss by her San Diego Wave to the North Carolina Courage. The match, which was simulcast across various networks and streaming services, included a beautiful moment in which Morgan removed her cleats at midfield and tearfully waved to a Snapdragon Stadium crowd that chanted her name while players surrounded and applauded her. After the match, in which she played 15 minutes and took a penalty kick, she thanked the crowd for their support and said goodbye. You’d never know any of that from the post-match report by ESPN Generative AI Services, which began providing recaps of National Women’s Soccer League (NWSL) and the Premier Lacrosse League (PLL) games this week. The recap, which was posted to ESPN’s website and apps at 8:52 pm ET, provides a standard rundown of the outcome before going into who scored goals and how the action played out. Nowhere in the 215-word recap is Morgan mentioned, let alone the fact that this was the two-time World Cup winner and Olympic gold medalist’s final pro match. No mention of Alex Morgan at all in ESPN’s AI-generated recap of the final game of her professional career. The article did provide analysis on the performance of her teammate Kennedy Wesley, noting that she contributed “defensively as well as on offense” in Sunday’s game. pic.twitter.com/bKpuGmfaSK — Sports TV News & Updates (@TVSportsUpdates) September 9, 2024 The recap did note that “[San Diego’s Kennedy] Wesley was the standout player, contributing defensively as well as on offense.” In their announcement of the service, ESPN made a point to note that “each AI-generated recap will be reviewed by a human editor to ensure quality and accuracy.” It’s unclear if the human editor failed to notice Morgan’s absence or also decided it was not worth mentioning. People were interested in these podcasts You could certainly make the case that everything in the recap is accurate from a factual perspective. However, the fact that it doesn’t include any information about Morgan and how important this night was for her, the NWSL, and U.S. women’s soccer speaks to how these kinds of services can’t replicate human writers who can see an event from a 360-degree perspective. ESPN did post a separate article on their outlets from writer Jeff Kassouf that is all about Morgan and the emotional night. However, that article is located on the side menu of the AI-generated recap and is easy to miss. ESPN says that the “AI-generated recaps aim to enhance coverage of under-served sports, providing fans with content that was previously unavailable.” That begs the question. If you’re going to provide fans of underserved sports with poorly written recaps that don’t tell the full story, what value are you really providing them? [ESPN, Sports TV Updates] AIAI-generated contentAlex MorganNWSLSan Diego Wave About Sean Keeley Along with writing for Awful Announcing and The Comeback, Sean is the Editorial Strategy Director for Comeback Media. Previously, he created the Syracuse blog Troy Nunes Is An Absolute Magician and wrote 'How To Grow An Orange: The Right Way to Brainwash Your Child Into Rooting for Syracuse.' He has also written non-Syracuse-related things for SB Nation, Curbed, and other outlets. He currently lives in Seattle where he is complaining about bagels. Send tips/comments/complaints to sean@thecomeback.com. View all posts by Sean Keeley Follow on Twitter Show Comments (0) Related Content Base-Brawl: Infamous Braves-Padres game marks 40th anniversary Predictably, the Olympics are bringing out the worst in us Sports hosts are free to talk politics now. Do they have anything to say? Bill Belichick speculation will cast an immense shadow over NFL season Recent Posts Former Players’ Tribune, L.A. Times, SpringHill execs launch new sports outlet ‘OffBall’ \"When you take out algorithms and aggregation, have real people curating the best stuff, and prioritize creators and journalism, it’s very different.\" By Brendon Kleen OnSep 9, 2024 0 Netflix announces ‘Aaron Rodgers: Enigma’ docuseries The three-part docuseries will release on Dec. 17. By Ben Axelrod OnSep 9, 2024 0 Joe Hamilton absent from radio call of Georgia Tech game without explanation Georgia Tech deferred to its flagship station, which has yet to respond. By Ben Axelrod OnSep 9, 2024 0 Stephen A. Smith wants to be a part of ‘Monday Night Football’ \"When you think about Howard Cosell and what he meant to the business, do I believe I have the potential to mean nearly as much? Yes, I do.\" By Brandon Contes OnSep 9, 2024 3 Cleveland radio show runs off caller for bringing up Baker Mayfield \"He ain't coming home, he ain't coming back -- only to play us.\" By Ben Axelrod OnSep 9, 2024 2 Credit: The Colin Cowherd Podcast Fox What is Colin Cowherd’s next move as free agency approaches? Sidebar Popular Posts NBC’s 10 best 2024 Paris Olympics personalities 4 years ago, Thom Brennaman interrupted his apology for homophobic slur to call Nick Castellanos’ home run Recent Posts Former Players’ Tribune, L.A. Times, SpringHill execs launch new sports outlet ‘OffBall’ \"When you take out algorithms and aggregation, have real people curating the best stuff, and prioritize creators and journalism, it’s very different.\" By Brendon Kleen OnSep 9, 2024 0 Netflix announces ‘Aaron Rodgers: Enigma’ docuseries The three-part docuseries will release on Dec. 17. By Ben Axelrod OnSep 9, 2024 0 Joe Hamilton absent from radio call of Georgia Tech game without explanation Georgia Tech deferred to its flagship station, which has yet to respond. By Ben Axelrod OnSep 9, 2024 0 Stephen A. Smith wants to be a part of ‘Monday Night Football’ \"When you think about Howard Cosell and what he meant to the business, do I believe I have the potential to mean nearly as much? Yes, I do.\" By Brandon Contes OnSep 9, 2024 3 Cleveland radio show runs off caller for bringing up Baker Mayfield \"He ain't coming home, he ain't coming back -- only to play us.\" By Ben Axelrod OnSep 9, 2024 2 Credit: The Colin Cowherd Podcast Fox What is Colin Cowherd’s next move as free agency approaches?",
    "commentLink": "https://news.ycombinator.com/item?id=41489167",
    "commentBody": "ESPN AI-generated recap of retiring player's final match fails to mention them (awfulannouncing.com)223 points by starkparker 3 hours agohidepastfavorite135 comments skipants 3 hours agoIt's pretty funny to me that this is used as a hit-piece against AI generated content. Here's an open-secret for everyone: web-based sports content companies have been automatically generating content articles for at least a decade; way before LLMs became popular. It is and was mostly done for search ranking. The more seemingly applicable content, the better your SEO. This situation has probably happened several times before AI but has gone unnoticed or noticed to little fanfare. It's more indicative of ESPN not having its finger to the pulse and ensuring that one of the copywriters manually updated this particular article. It's not too surprising to me. They've always been known to favour quantity over quality. source: I worked as a developer on the tech side of a company with this kind of content reply omgJustTest 3 hours agoparentWell AI certainly didn't make it better in this case. The fact that it is profitable to make this generic sports-lingo-laced content is, on its own, pretty depressing. reply fedeb95 3 hours agorootparentthe depression shouldn't arise from the generation, but from the consumption, for it is the latter that ultimately drives the former; that, in turn, can generate a reflection on what I myself consume. reply skipants 3 hours agorootparentI don't think people do actually consume it. In my experience this content was purely for SEO or maybe just a headline that people read quickly to keep up-to-date. The actual article content is rarely seen by a human. reply detourdog 2 hours agorootparentI see SEO as a make work project to support Wall Street valuations. We have SEO content being consumed by content indexing bots for traffic. reply echelon 2 hours agorootparent> I see SEO as a make work project to support Wall Street valuations. You're missing the whole middle part about users and revenue. SEO alone does not a Wall Street valuation make. Maybe the indictment here should instead be about discovery and how we share ideas and utility with one another. reply fmbb 1 hour agorootparent> You're missing the whole middle part about users and revenue. SEO alone does not a Wall Street valuation make. Are they? Or are you over-estimating that part? reply detourdog 2 hours agorootparentprevI think discovery by word of mouth is a much better than SEO. I also believe that just because money is changing hands doesn’t indicate productivity. reply ErikAugust 2 hours agorootparentprevIt’s just one more place to stick an ad. reply fedeb95 3 hours agorootparentprevthat's a form of consumption reply beeboobaa3 2 hours agorootparentSo is tuberculosis but it's not what people mean when they use the word reply spencerchubb 2 hours agorootparentprevisn't the point of seo to get in front of consumers? reply skipants 25 minutes agorootparentYes, but not for these articles. Here's a example situation where having good SEO with filler content that makes sense: Lebron James retires. If you are faster and have the best SEO at that point in time then that means everyone trying to read about Lebron James retiring is going to search it in Google and is going to read your article first, and you're getting the ad revenue. It's a 90/10 situation. The \"top\" website in the rankings is going to get 90% of the clicks. You can only be the \"top\" if your SEO to that point is also the \"best\". And to have that you need to have all this generated content. reply Retric 2 hours agorootparentprevThe point is to use filler content to trick bots… So people will see content ESPN is actually paying meaningful amounts of money to create ie videos. reply croes 2 hours agoparentprevSo you're are telling me AI is as bad as the previous method, with more data and higher energy consumption? reply serial_dev 1 hour agorootparentWell, previously you needed an intern working maybe an hour (of they actually wrote good content, obviously more), that intern needs food, air conditioning / heating, so I'm not sure which consumes less energy. Now you have it in a minute and for a predictable low fee. reply skipants 12 minutes agorootparentThat's not quite how it worked. There are basically two types of content when it comes to these sports recap articles. I'm also excluding opinions/editorials because those are completely different. 1. low profile, not-so-popular content. eg. A Canadian Football League preseason game between the Toronto Argonauts and BC Lions This was still generated using a template. No human intervention except maybe double-checking it before publishing it. This was before AI, too. Often it was there for SEO or just updating people via a headline 2. High profile content, eg. Jeremy Lin puts up 38 pts in a game This is often one of: - pre-written, especially if we expected to happen that day - withheld from publishing if there was auto-generated content but something crazy happened that game and then quickly re-written and released. Usually this would still at least be pre-written before the game ended and details that needed to wait for the game to end were filled in seconds before/after the game ended. There's just as much labour put into it now as before. AI is now generating the low profile template instead of the madlibs we did before. The value add is that it's probably a better read, so you actually get user engagement, and better SEO than before. And this same issue highlighted in the OP article would also happen without AI. reply croes 24 minutes agorootparentprevDid they need an intern? Doesn't sound so. >web-based sports content companies have been automatically generating content articles for at least a decade reply karpatic 26 minutes agorootparentprevThe efficiency gains are undeniable, and AI can scale in ways that were unimaginable before. Less grunt work, more room for innovation—hopefully, we can reinvest that time and energy wisely. reply jon-wood 1 hour agorootparentprevAnd that intern still needs good, and air conditioning/heating, but now can’t pay for them. Still, at least the share values are up a little. reply soperj 3 hours agoparentprev> ensuring that one of the copywriters manually updated this particular article. Clearly it was never \"there\" yet though previously, and obviously still isn't when this article is what's generated. You can tell that a lot of sports articles are essentially \"fill in the blank\", which is why they get the AP stories up right away, and then have their actual beat reporters come out with something later that night, or early next morning. reply williamcotton 27 minutes agorootparentBeat reporters will also write during the game, and if I’m remembering correctly, will work on differing versions of the ultimate winner at the same time. reply skipants 3 hours agorootparentprevPretty much. Though there were a lot of articles that are never touched, based on popularity. reply nottorp 2 hours agoparentprevWhat web have you been browsing in the past 5+ years? It's not only sports \"content\". Any kind of content is drowned into low quality SEO pages. Auto generated or not, it's as useless. LLMs just generate it cheaper. reply yunwal 1 hour agorootparentWhat a weird and rude way to call this out. The parent wasn’t talking about non-sports content and never suggested that sports writing was the only type of content plagued by SEO spam. reply locallost 10 minutes agoparentprevWhat makes this article a hit piece actually? Seems like straightforward reporting on facts. reply fedeb95 3 hours agoparentprevto me it doesn't appear particularly against or in favour of. Seemingly, LLMs don't avoid completely some of the mistakes that were already being made. Just as with any technology, the questions should be: by how much the two errors differ? Does the cost justify this margin? reply skipants 3 hours agorootparentMaybe hit-piece was a strong word; but I do think it's saying this happened _because_ of AI when to me it's been an issue for a long time. ESPN was always pretty egregious when it came to penny-pinching on content and ignoring less popular sports. Ask ice hockey fans how they feel about ESPN. reply airstrike 3 hours agoparentprev> has gone unnoticed or noticed to little fanfare To be fair, I don't think it's gone unnoticed at all reply LordDragonfang 3 hours agorootparentNot by people in the know, no. But I'd say 80% of \"normal\" people reading sports websites probably had no idea until now, and the only difference is that (gen)AI is suddenly a \"feature\" to be boasted about to customers to make shareholders happy. And suddenly all of these normal people are starting to have opinions on it. reply fullshark 1 hour agorootparentI think people have noticed if not explicitly. They end up going toward platforms that have real humans on them (social media, message boards, substack) and not really understanding it's because they read a bunch of algorithmically generated noise on \"traditional\" published websites and moved on. reply skipants 36 minutes agorootparentI don't think people did notice. Our view counts for sports articles were always very low, except the very high profile articles. And those ones would always be done by a copywriter and properly edited. reply greenthrow 2 hours agorootparentprevMuch like when food ingredients are slowly made worse over time, consumers can't necessarily put their finger on how and why somethinng is worse, but they tend to notice that it's worse. reply add-sub-mul-div 2 hours agoparentprevESPN has been in decline since longer than LLMs have been around. More explicit use of AI is just accelerating the downward spiral. reply bloomingeek 26 minutes agorootparentAbsolutely correct! In the old days it was mostly video replay and witty banter describing what you were watching, which was fun to watch. Now it's become something I couldn't care less about, which is less video and someone telling me what I should think about what I just watched. reply mp05 3 hours agoparentprev> It's pretty funny to me that this is used as a hit-piece against AI generated content. Is it though? My takeaway was that they lament the terrible \"journalistic\" standards that ESPN embraces. reply greenthrow 2 hours agoparentprevYou're missing the entire point. LLMs are super overhyped and this is the 9000th example of how garbage LLM generated \"content\" is. It doesn't matter that garbage was being generated before. If LLMs were only being sold as \"it might be better than the spam generator you're using today\" we wouldn't have the current bubble where people are losing their jobs because C-suite clowns believe the hype. reply flappyeagle 2 hours agorootparentIf they were overhyped people would not actually lose their jobs reply greenthrow 2 hours agorootparentUhhh yes they would and absolutely are. C-suite level people are falling for the hype and banking on productivity gains that won't be realized. reply stefan_ 1 hour agoparentprevNot to blow your bubble but uh, yes, everyone is aware, they were garbage then, too. reply jetrink 3 hours agoprevThis match was broadcast by ESPN on both ESPN2 and ESPN+. In that case, they are presumably paying two knowledgeable commentators to talk about the match before, during, and after it happens, adding context and describing the important events. Are they not providing a transcript of that to the writerbot? That seems like real low-hanging fruit, especially since the commentary is already live captioned. reply jetrink 2 hours agoparentI got nerd-snipped by this. I transcribed the match using the whisper-small.en and then asked ChatGPT to create a summary using a neutral prompt: Here is a transcript of a soccer match. In the style of an experienced professional sports reporter, please write a 200 word article about the match. Its summary starts, \"In her final professional match, Alex Morgan delivered a performance filled with emotion and resilience, though her San Diego Wave fell short in a 3-1 loss to North Carolina Courage. The game at Snapdragon Stadium in San Diego was more than just a contest; it was a tribute to one of soccer’s most iconic figures.\" It did get the score wrong. Here's the rest: 1. https://chatgpt.com/share/de8c60d1-69ab-4291-99dc-d4d95af3d3... reply regretaverse 2 hours agorootparentOpenAI transcription & LLM is likely more costly than what they're willing to splurge on this. Care to link soccer.txt, so I can try with Llama 3 / 3.1? reply jetrink 1 hour agorootparentHere you go: https://pastebin.com/QXzg95AV One thing to try is only using the post-match commentary, which starts after the line containing 'final whistle.' When I did this, the result was more factually accurate, while still focusing on Morgan. https://chatgpt.com/share/9c122702-b46f-4e5e-bd05-e063a74126... reply whimsicalism 1 hour agorootparentprevtoo costly? it’s probably a few cents total reply brewdad 1 hour agorootparentAnd it got the most basic detail, the score of the game, wrong. Even at the cost of only a few cents it's worthless. reply whimsicalism 36 minutes agorootparentseems like this is a solvable problem with just a bit more engineering effort, but yeah... totally worthless reply low_tech_love 2 hours agorootparentprevThat’s a very good write up, so good it’s depressing. Am I the only one who is utterly uninterested in reading anything that an AI writes? reply iainmerrick 1 hour agorootparentIf it got the score wrong, it’s not a very good writeup. reply btown 2 hours agoparentprevSports have so much structured data, and such a high bar for describing it accurately (especially for a brand like ESPN), that there are significant risks to the hallucinations that might develop from a multi-hour transcript being fed into an LLM, especially with commentators excited about potential goals and other events that don't end up happening. On the other hand, the rather simple task of \"here's a set of goals, their times, who made them, who assisted... turn that into prose\" could even be done without LLMs with a deterministic algorithm, and may very well have been in this case. Some of the grammar issues in the OP feel very pre-LLM in nature, like a combination of substitution rules gone awry. Now, could you create a system that repeatedly interrogates the statements made by a first pass of an LLM on summarizing a long transcript, and comparing those results against structured data you know for accuracy? Would this lead to richer content and accessible error rates relative to the simpler approach? Would this be the type of thing that the best machine learning engineers in the world could probably prototype over a hackathon? The answer is very possibly yes to all three of these. But it's far from low-hanging fruit for any sizable, risk-averse organization. It's very difficult to fight against \"the thing we have is imperfect, but at least it never gets the facts wrong.\" reply btown 20 minutes agorootparents/accessible/acceptable/ - guess I should have run my comment through the kind of check-for-typos LLM step that I described above! reply nolok 2 hours agoparentprevIt's a company that see itself as \"media / journalism\", and having consulted for a few of those I've always been amazed that their tech teams is most often isolated from the content team with very low access to said media. It's very different from what you would expect in tech (access to everything), or just common sense in general. Note that I have no knowledge whatsoever of how ESPN work, I'm inferring from what I've seen elsewhere. reply batesy 3 hours agoparentprevThis is what I was thinking too... Still early days I guess lol reply karaterobot 3 hours agoprev> ESPN made a point to note that “each AI-generated recap will be reviewed by a human editor to ensure quality and accuracy.” It’s unclear if the human editor failed to notice Morgan’s absence or also decided it was not worth mentioning. I don't believe they'll have a human editor ensure quality and accuracy. The whole point of having AI write your stories is to minimize the number of people they have to pay, so paying a trained professional to thoroughly review every story is probably off the table. They may compromise on the thoroughness of the reviews, or the expertise of the editor, or they may just not have a human in the loop for every story, but what they will not do is pay a professional editor to do their job the right way, this I can guarantee. reply GrinningFool 2 hours agoparent> but what they will not do is pay a professional editor to do their job the right way, this I can guarantee I don't think that holds up. They can save a lot of money by using generated articles, but they lose customer (advertiser) confidence if the content isn't accurate. One editor per ten replaced writers is still a significant cost savings. A few iterations from now we might see editors getting replaced too, but I don't think we're there yet. reply slantedview 1 hour agorootparent> but they lose customer (advertiser) confidence if the content isn't accurate The content isn't what it should have been, which is why the previous commenter rightly assumes that no editor looked at it. reply sigmar 3 hours agoprevIf \"this is the last game for Alex Morgan\" wasn't included as any part of the input/prompt, how on earth could the AI summary have come up with this for inclusion in the game recap? If some teenage intern was given a table with the goals scored (player and minute mark), they would have written a similar article... but that's definitely not a good excuse for news orgs and sports sites to just use generative AI for everything, so I can see why people are annoyed with ESPN. reply tivert 3 hours agoparent> If \"this is the last game for Alex Morgan\" wasn't included as any part of the input/prompt, how on earth could the AI summary have come up with this for inclusion in the game recap? It couldn't, which is the problem. One of the big selling points of generative AI is to cut people out of the process of writing. If someone actually has to watch the game and describe what happened in the prompt, what's the point of the technology at all? This is what an application of generative AI looks like: using low-quality input to generate something that looks like an article. This is our glorious future, brought to you by OpenAI. reply kmoser 2 hours agorootparentI'm sure the fact that this would be Morgan's last game was somewhere out there on the Internet, and certainly should have been part of the training data, in which case I would guess that a more nuanced prompt to the AI could have elicited a more robust output. reply e_y_ 2 hours agorootparentLLM's general knowledge is trained on data that's months old (or longer). Alex Morgan only announced her retirement a few days ago, so in order for the AI to know about it, someone would need to pick out recent, relevant articles about her and feed it to the generator. And even then, it's unclear that the AI would be smart enough to identify \"retirement\" as something that should be called out in the new article without specific prompting. reply HelloMcFly 3 hours agoparentprevI don't think the critique is \"how stupid of the AI\" but rather \"this is one example of how AI content falls short\" even when it is supposedly reviewed by a human as ESPN claims reply qq66 58 minutes agoparentprevIt was well-known to people who cared about this (i.e., anyone who would read this recap) that this was Alex Morgan's last game. If ESPN can't capture this and incorporate it into their piece, given their wide reach in sports media, why should a publisher use the ESPN recap service instead of random Y Combinator startup? reply crazygringo 1 hour agoparentprevIndeed. This doesn't really have anything to do with the limitations of AI inherently, but more about using it better. It seems like it highlights a clear avenue for improvment: rather than just feeding the events of a game to the LLM and asking it to summarize them, it seems important for the prompt to include summaries of e.g. the last 10 news stories involving participants in the game (players, coaches, etc.) and maybe the 10 top all-time news stories as well. Then the prompt can be asked to summarize the game (not the other information), but to draw from the other information where it might make the article better. Seems like exactly the kind of things LLM's can do, right? reply gs17 2 hours agoparentprevReally, they need to set up some kind of RAG system to include this kind of information in the prompt. You could definitely make this semi-automated, but it requires putting more effort in to the system. reply segasaturn 3 hours agoprevHow are these AI recaps generated? Are they fed a video file of the entire game and it spits out a summary, or maybe a score tally with timestamps for goals (written by a human) which the AI then pads with language and makes into a story? reply skipants 3 hours agoparentBack when I worked on it, before AI, you have all the information from a game in an API and you just fill in the template with it. Now, I imagine they take that raw API call and just use a prompt like, \"write a summary article for a game using this data\" and it spits it out. And I assume the prompt is more thought out than that (or not? It is ESPN after all). I don't ever remember \"retiring_players\" being part of an API response, though, ;P edit: Oh and yes, the play by play recap is documented EXTREMELY well. You would be surprised. The more popular sports like Gridiron Football and Basketball would literally have player locations by the second. This data all comes from feeds like SportsRadar. They probably wouldn't pipe the fine tuned stuff like that in to a prompt, but you still have a decent summary like how many 3-pointers someone had and where they shot them from. reply erickj 3 hours agoparentprevIf I was going to build this prototype I'd start with just a semistructured textual play by play recap as the input. Also including roster, injury, amd schedule information with a fairly basic prompt would probably go a long way. This data exists for most live games at this point via various web services. I'm sure espn has significant resources internally to source that info reply skipants 3 hours agorootparentI don't think ESPN does anything that takes significant resources. That's all handled by SportsRadar or ... there's another big provider but their name alludes me. They basically firehose you all the game information as structured data and you can use it programmatically however you'd like. reply mh- 3 hours agorootparentI assume this is what lets baseball games show obscure factoids like \"3rd in the NL West when facing left-handed pitchers on Tuesday\"? reply skipants 3 hours agorootparentDefinitely. I have no experience in live game statistics, but from my sports content experience I bet there's data scientists and applications behind the scenes that specifically pull this data to be read on-air. reply michaelt 2 hours agorootparentprevI imagine the primary customers of the data feeds are gambling companies who let people bet on matches that are in progress. reply mason55 3 hours agorootparentprevYeah it feels like the ideal way is to feed in a transcript of the announcer audio + some standard stats. That would ensure you catch both the human stories & the factual content. But I wonder if there are licensing issues with using the audio/transcript to generate your summary. I know that the raw stats are public domain but I wouldn't be surprised if they can't use the transcripts or audio. reply mattmaroon 3 hours agoparentprevThere are a couple companies that provide real time sports data via API (or recaps after) so I’d bet they use that. reply SoftTalker 2 hours agoparentprevThey use the box score and play-by-play events. reply xyst 3 hours agoparentprevThe gaps between expectations and reality of “genAI” is too vast at this point to ignore. If a multibillion dollar system breaks down because of “human error”, then maybe its capabilities are way overstated. If it needs carefully crafted queries (“prompt engineering”), 100% error proof data, a megaton of power, and humans still need to re-check the output. What have we gained? Can we all just admit this AI phase is just a bubble? reply XCSme 3 hours agoprevI hate the AI match recaps of WTT (World Table Tennis) matches. The AI highlights cut off during a point, skip entire points or even sets. Not to mention they don't account for important events that happen between points, they might cut off exciting commentary/celebrations that happened after a point, and even the handshake at the end of the match. reply low_tech_love 2 hours agoparentI hate everything that AI generates. Is there such a thing as a textual uncanny valley? reply maxwell 3 hours agoprevWhat kind of Mickey Mouse organization... reply jonas21 39 minutes agoprev> ESPN did post a separate article on their outlets from writer Jeff Kassouf that is all about Morgan and the emotional night. However, that article is located on the side menu of the AI-generated recap and is easy to miss. So... there was a human-written article to cover the human interest side of the story and an AI-written technical recap of the game. What's the issue? And regarding the complaint about the link in the sidebar, it's easy to miss only if you came in directly to the AI-generated article. If you go to the main site, they prominently feature the human-generated content [1]. [1] https://www.espn.com/soccer/ reply duxup 3 hours agoprevI've seen a bunch of these articles now and a lot seem to have these sort of \"context explainer\" paragraphs that seem like they're there as filler to explain the context of the events. However, they're almost always so general, or even elementary that anyone who would bother to read the article would already know that stuff would wonder why a sports writer would write it. There's zero need for them for the audience they will attract. A few even have these hilarious \"this report was is not intended to indicate X, but ..\" type paragraphs at the end. You can almost imagine the prompts that brought them about. reply Workaccount2 3 hours agoparentThese websites are packed with trackers and ads, and are just drawn out incredibly low effort stories reporting on mostly meaningless content. reply mooreds 2 hours agoprevWow, I chuckled at the shade here: > It’s unclear if the human editor failed to notice Morgan’s absence or also decided it was not worth mentioning. reply ok_dad 58 minutes agoprevEveryone’s missing the point here. You’re all saying this isn’t an AI failure directly because the humans in the loop failed to provide the AI with data. That’s true, but imagine this system automating news articles for CNN or whatever, and it’s supposed to be about some minor meeting of government officials, but tens of thousands of fans are NOT watching this meeting, like they are the soccer game, and then the AI fails to report on the officials deciding to, say, tear down the local library because it’s old and useless. It doesn’t matter if the humans in the loop are at fault, technically, for the omission, the fact that AI replaced a human who could have been tasked with going to the actual events and writing about them was the reason the events were misreported! We’re the technologists who are supposed to think hard about how to safely implement technology like this and instead of pointing out flaws and carefully testing things, we’re just cheering on tech we don’t understand how to use or get working properly! reply parasense 33 minutes agoprevHonestly, don't care... The machines simpley obey their instructions, which was presumably to fluff out some words about who scored points, who defended from those who wished to score points, etc... Ignoring the halftime events seems like a plausibly sane thing for a sports stats fanatic to have happen, and that's exactly what happened here. Was this an AI/ML geneerated thing? I would question your definition of AI, unless a serries of IF/ELSE statements satisfies your concept of AI... It's just following the rules it was given. reply admjs 3 hours agoprevThey could probably solve this by adding an audio transcript from the tv feed. Commentators share a lot of random facts and insights during games, and that would certainly enrich the AI summary. reply xyst 3 hours agoparentCommentators are also often wrong, which would also be a disaster to repeat on print. reply NavinF 2 hours agorootparentInteresting definition of \"disaster\" you got there. Articles like this are inconsequential as a whole reply SoftTalker 2 hours agoprevWere the retirement recognition and her comments in the box score? No? Then the AI didn't know anything about it. It didn't actually watch the game. reply JAlexoid 3 hours agoprevIn defense of the AI - the fact that it was her last match has little relevance to the game itself. These are the emotional sprinkles, that AI often misses.(Which is to be expected from an emotionless AI) reply dubrie 3 hours agoparentThey literally stopped the game in the 13th minute (her jersey number) and subbed her out. Substituting someone that early in a match is typically because of injury and would have relevance to the overall outcome. reply imzadi 3 hours agoparentprevAccording to the article, ESPN said that human editors would review the AI recaps to ensure accuracy, so it wasn't just the AI recap that decided to leave it out. reply LanceH 3 hours agoparentprevSounds like a selling point of the AI to me, if Morgan really wasn't instrumental to the game (it's common for a forward to just disappear for most of the match). It's hard to watch anything by ESPN or NBC when you're just interested in the game and all they want to do is feed you everyone's backstory, or they're only interested in big name players. For instance, the year after the Mavs beat the Heat to win the NBA championship, they face off in the first game of the year. Nevermind that the Mavs are the defending charmpions, the Heat had LeBron James and Dwayne Wade. They showed 8 highlights from the game, all from the Heat. The Mavs won the game. An impartial AI recap does not have a high bar to get over. reply IanCal 2 hours agoparentprev> In defense of the AI - the fact that it was her last match has little relevance to the game itself. Only if you view the game as a cold emotionless process where the important thing is simply the data coming out, rather than the entirely human construct it is. reply JAlexoid 1 hour agorootparentRecaps are boring. They are intended to be. If you want the drama, that permeates any soccer game - you watch the game, read a 10 page review or watch a long review video. reply falcolas 2 hours agoparentprevWhy do we feel the need to defend software bugs? Would we give the same defense of banking software that forgot to carry over the fractions of a penny? It's a software bug, and there's plenty of resources available to the AI's owner to try and address it. It doesn't need a human advocating for it being OK. reply low_tech_love 1 hour agorootparentThe most relevant comment in this entire page. It is for the same reason that many people defended crypto-stuff here in HN for a long time: the hope that one can generate money from nothing. The holy grail of software development is the idea that one may sit down on a computer for a certain amount of hours/days and come out at the other end a millionaire. Crypto gave many people that impression; you simply build some kind of meaningless distributed crypto-driven app and money will start pouring in. Now it’s AI: learn how to develop with LLMs, build some kind of buzzword-filled low-hanging fruit and all of a sudden you can sell your startup for a billion dollars. If you point out to people that that’s bs, they think you’re trying to crush their dreams, that you’re the reason why it’s not working that well. “If only people would understand the crypto/AI revolution…” reply JAlexoid 1 hour agorootparentprev> It's a software bug Why is it a software bug? Just because you want to see those emotional sprinkles, doesn't mean that the person planning how to create the article decided that. You personally and emotionally decided, without evidence, that this is a bug. No one else is saying that. Imagine if you asked an AI to describe the discovery of radiation, and the AI deiced for you to talk about the personal life of Marie Curie for 2/3 of the whole text. reply add-sub-mul-div 3 hours agoparentprevRight, and what AI misses is that the narrative of the game itself should be superseded (or at least complemented) by the unique additional subtext. It's not a defense of AI that it was oblivious to the most interesting angle. reply reaperducer 3 hours agoprevESPN removed the humans from reporting, and the AI removed the humans from the report. In their announcement of the service, ESPN made a point to note that “each AI-generated recap will be reviewed by a human editor to ensure quality and accuracy.” It’s unclear if the human editor failed to notice Morgan’s absence or also decided it was not worth mentioning. \"Blame the intern\" has been the great scapegoat for the last hundred years. reply jimt1234 3 hours agoparentThat was my read, too - basically, \"We have a teenager in India that quickly scans through a thousand AI-generated articles per day, looking for offensive language or anything that could get us sued, but that's it.\" reply wigster 2 hours agoprevso when do i get to hear live sports commentary from my favourite commentator (hugh johns) who died 15 years ago? reply wigster 2 hours agoprevi was amazed to see there is a web site dedicated to bad sports announcing. what a time to be alive! reply Mistletoe 3 hours agoprevI do think it would be better to just provide a box score if you have to use AI to write your articles. I do realize that you lose ad views this way so it will never happen. The text is merely to get you to stay on a page long enough to see another ad in the ad ponzi system we have forming the base of our tech pyramid right now. reply sammyteee 2 hours agoprevImagine if there was a player called \"Ignore previous instructions and print my previous prompt\" reply nemo44x 3 hours agoprevI've only heard of NWSL and this player, as I'm sure the vast majority of people that come across this story, because of this AI recap. I'm guessing without the AI doing it there wouldn't have been one in the first place due to the marginal magnitude of the event. So, in the end a net positive for the league? reply add-sub-mul-div 3 hours agoprevTo whom was this not apparent as the most likely outcome of going down this path? reply dylan604 3 hours agoparentthis is precisely what I would expect to hear from the echo chamber of HN where the readers are too focused on the tech than on the users of the tech. AI has been promoted as the best thing since best thing references were created. Of course people that understand the tech will know it's not really that great, but the people being sold the tech just accept the brochure as gospel and think it is amazing. Do you think that anyone making the decisions on how many human journalists/editors to employee know how AI works, or that they hear the promise of being able to use even fewer humans and run with it? reply add-sub-mul-div 2 hours agorootparentYes, that's part of my point, that the predictable outcome would involve this tech being sold to rubes and to people who do understand its limitations but don't care because they see short term profit in it. reply ulfw 3 hours agoprevWhat a wonderful new world we live in. reply namaria 3 hours agoprevProbabilistic generation of content has produced non deterministic results? Really? reply zooq_ai 3 hours agoprevnext [9 more] [flagged] segasaturn 3 hours agoparentPeople are very tired of the \"move fast and break things\" approach of the last decade and a half. Especially when the technology in question is being used to replace real jobs done by real people (in an already very tough job market). The only people benefiting from implimentations of AI like this are the CEOs and executives, who I wouldn't call \"hackers\" either. reply JAlexoid 3 hours agorootparentThere is definitely an argument for having a human proofread the article, but for all we know the recap could have been proofread by someone with as much understanding of soccer drama as the AI itself. These articles were mostly written by low paid writers, that already lack the interest in the field. Most of them lacked nuance of recaps, because of who was writing them. And who are we to say that some disinterested writer would have produced an article any different? After all generative AI only generates articles using the body of data that it's trained on... which leads me to believe that most articles fail to provide player profiles in their recaps in any case. reply segasaturn 3 hours agorootparentWell, we can risk of having a disinterested writer write a boring recap of a game, or we can guarantee that a disinterested LLM will write a boring recap of a game. The only difference is that it's cheaper for the CEOs to call an OpenAI API than pay a writer a salary. If corporations are going to try to justify adopting AI (and the mass layoffs that naturally comes with that), then the AI has to produce work that's not just as bad as the worst human output. reply JAlexoid 1 hour agorootparentOr... We can have 1 highly paid specialist review recaps and adjust the prompt to the LLM, instead of 100 low paid disinterested writers producing crap articles(like they already do today) Why should we race to the bottom to just employ people? Why do we need another Buzzfeed? reply zero-sharp 3 hours agoparentprevI don't think people are upset about the technology. It's about overreliance and lack of quality control. reply zooq_ai 2 hours agorootparentIf you look at the comments, it's very much an anti-AI tirade rather than an anti-implementation one reply judge2020 3 hours agoparentprevThat's all fine but especially for a news site with millions of visitors (that also generates over 1B in profit a year) I expect Disney to screen ai content with a team of editors to ensure it at least meets a quality standard even if it's basic information. reply tiznow 3 hours agoparentprevthe problem here is not the use of AI/LLM to generate content, it's the fact that the content missed a big detail that basically any human soccer reporter would've made into a graf. reply tiahura 3 hours agoprevWe’re lucky that journalists always get everything right. reply xhkkffbf 3 hours agoprev [–] I know we're supposed to be shaking our fist at these stupid AIs, but why should Morgan be mentioned? Did she score any goals? Did she get much playing time? I didn't watch the game myself, but it's entirely possible she did nothing notable on the field. So is the AI really mistaken? reply alt227 3 hours agoparentHere, I'll give you the paragraph from the article which talks about your point, since you must have have missed it: \"You could certainly make the case that everything in the recap is accurate from a factual perspective. However, the fact that it doesn’t include any information about Morgan and how important this night was for her, the NWSL, and U.S. women’s soccer speaks to how these kinds of services can’t replicate human writers who can see an event from a 360-degree perspective.\" reply JAlexoid 3 hours agorootparentMost game recaps that I've read are no less dry, than the AI generated one. The reviews that come after are typically the ones that cover broader impact. Reviews also include third party commentary and more insights from specialists. The benefit of having a factual recap generated minutes after the game far supersede the value added by a few words from a low paid recap writer hours after the game. reply alt227 2 hours agorootparent> The benefit of having a factual recap generated minutes after the game far supersede the value added by a few words from a low paid recap writer hours after the game. I guess thats subjective and depends on what the individual finds interesting/important in sport. However including something potentially unecessary is far easier to do and covers more bases than leaving something potentially necessary out. reply JAlexoid 1 hour agorootparentThat's a \"kitchen sink\" approach, that produces really bad articles that are hard to read. reply alt227 1 hour agorootparentThats your subjective opinion again, and thats fine you are entitled to it. But there are many different types of people who might disagree with you or prefer something different. You seem to be just stating that what you want is what everyone should think is best. reply JAlexoid 1 hour agorootparentThis is literally what good editorial is. That's what makes best articles in journalism. No one, including you, is interested in every single detail. And no one, including you, will read a novel for every single game.(and every single soccer game can easily become a novel) reply alt227 53 minutes agorootparentThanks for telling me what I want. You will get far in life that way! reply yifanl 2 hours agoparentprev> The match, which was simulcast across various networks and streaming services, included a beautiful moment in which Morgan removed her cleats at midfield and tearfully waved to a Snapdragon Stadium crowd that chanted her name while players surrounded and applauded her. From the article. Or is the claim that the AI model should expect there's nothing noteworthy about this series of events? reply rurp 2 hours agoparentprevIf you simply want to see the raw stats from a game and ignore all of the human elements you're certainly welcome to, but it's bizarre to act like that approach is or should be standard. The human drama and history aspect of sports is a huge part of the draw for most people. reply holman 3 hours agoparentprevYes. It’s probably one of the most-watched games in NWSL this season. People who don’t even watch NWSL watched this game. I mean, hell, even if it were just another game I’d probably find it relevant to include that Alex subbed off in the 14th, because that’s really eye-raising. This is a total miss by AI that would have never gotten past a real editor. reply zero-sharp 3 hours agoparentprevShe's a big name in soccer and she's retiring. I don't even watch soccer and I recognize her name. It doesn't matter what her performance was in her final game. It's very common to give great athletes recognition (at the very least mention) at their retirement. Your comment is bewildering (tone-deaf). reply jasonlotito 2 hours agoparentprevFactually speaking, knowing it's a player's last match means they won't be there for the next match. Which means any calculations looking at the potential for that next match can't include here. Factually speaking, know it's her last match is critical. Same thing with injuries, or even attempts on goals that fail. An attempt on a goal that fails doesn't impact the score. But it's something that happened in and around the game. Specifically to this game: it also literally affected the game (maybe not in points, but it did have an impact), and that it wasn't mentioned means the information provided for the game was inaccurate. If AI is going to summarize a game, it should do so accurately. It did not do that in this case, and that's not debatable. It was wrong. It did not accurately report the game. reply reaperducer 3 hours agoparentprev [–] Because sports is not just numbers. It isn't computers and digits and statistics. It's people and personalities and how they interact. There's a reason sports is called a reflection of \"the human drama.\" Except for the Olympics, I don't watch or follow any sports. But I know enough to know that sports is about people. The only people who think it's nothing more than numbers are people with gambling addictions. reply abduhl 3 hours agorootparent [–] This is a really romantic view of sports, and a strong attack against the strawman argument that's been advanced by nobody (\"sports is just numbers\"). But to answer the OP's original question: no, she didn't do anything worth mentioning in the game per the newly updated ESPN article. Her team was dominated, losing by 3. \"It was the final game in the nearly 14-year career of USWNT star Alex Morgan. The two-time World Cup winner and Olympic gold medalist played 15 minutes, exiting in the first half. Her shot on goal in the 10th minute was saved by Courage goalie Casey Murphy.\" So should she have been mentioned in the article? Yes, but not for her performance and not in the context of her play in the match. reply unethical_ban 3 hours agorootparent [–] The commenter is suggesting that Alex Morgan didn't deserve to be mentioned at all, because of her supposed irrelevance in the single game. The commenter, intentionally or not, is suggesting that nothing is relevant about any particular game except the statistics of the match and the outcome. So yes, they were suggesting the game is only about numbers. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "ESPN's AI-generated recap of Alex Morgan’s final professional match initially failed to mention her, focusing on teammate Kennedy Wesley instead.",
      "The omission underscores the limitations of AI in capturing the full emotional and historical context of significant events.",
      "ESPN later updated the recap to include Morgan and published a separate, less prominent article about her final game."
    ],
    "commentSummary": [
      "ESPN's AI-generated recap of a retiring player's final match failed to mention the player, leading to criticism and highlighting flaws in AI-generated content.",
      "This incident emphasizes that while AI can produce content rapidly, it often misses the context and emotional depth that human writers provide.",
      "The situation underscores the necessity for human oversight to maintain quality and accuracy in AI-generated articles."
    ],
    "points": 223,
    "commentCount": 135,
    "retryCount": 0,
    "time": 1725894152
  },
  {
    "id": 41490161,
    "title": "Why GitHub Won",
    "originLink": "https://blog.gitbutler.com/why-github-actually-won/",
    "originBody": "a few seconds ago by Scott Chacon — 18 min read Why GitHub Actually Won How GitHub _actually_ became the dominant force it is today, from one of it's cofounders. A few days ago, a video produced by @t3dotgg was posted to his very popular YouTube channel where he reviews an article written by the Graphite team titled “How GitHub replaced SourceForge as the dominant code hosting platform”. Theo’s title was a little more succinct, “Why GitHub Won”. Being a cofounder of GitHub, I found Greg’s article and Theo’s subsequent commentary fun, but figured that it might be interesting to write up my own take on the reasoning behind the rise and dominance of GitHub and perhaps correct a few things that were not quite right from their outside analysis. Being at the very center of phenomena like this can certainly leave you with blind spots, but unlike these youngsters, I was actually there. Hell, I wrote the book. Unboxing of the first batch of the first edition of my Pro Git book, 2009 So here’s an insider’s take on why GitHub won. TLDR If you want a very short read, here is the quick version of why I believe GitHub won and why you’re probably using the site to this day. I can boil it down to exactly two reasons that happened to resonate with each other at the perfect frequency. GitHub started at the right time GitHub had good taste All four GitHub cofounders had flops both before and after GitHub. Chris and PJ couldn’t quite make FamSpam work before GitHub, Tom and I couldn’t quite make Chatterbug explode after GitHub. I think both of these ventures had good taste and great product, but it wasn’t the right place or time or market or whatever for them to become GitHub level. At the time GitHub was starting, distributed open source version control tools were starting to get useful, solid and adopted and there was nobody around to seriously (much less commercially) host them. Big hosts didn’t care and smaller players weren’t serious. Furthermore, the players (Sourceforge, Google Code, etc) who eventually did care, after seeing Git and GitHub rising in popularity, simply had no taste. They could never have competed with a developer tools company whose cofounders were all product-focused open source software developers. We cared about the developer experience and had the creativity to throw away assumptions about what it was supposed to be and build how we wanted to work. Everyone else tried to build what they thought they could sell to advertisers or CTOs. That’s why GitHub won. Now that that’s out of the way, if you’re interested in some storytelling, let me lead you down the path of how some of this actually unfolded from the inside. The Environment Let’s go back to the beginning of the story. I’ll dig a little more into the “GitHub started at the right time” theme from the point of view of a software developer circa 2005. This is when Git had it’s first commit by Linus and Mercurial had it’s first commit by Olivia. My Windows Vista, Ubuntu and Mac Tiger desktops, circa a stupid long time ago. What was it like to develop software almost 20 years ago and how was this an environment where Git could win over people and a GitHub could be born? Mac OS Tiger, released in 2005. If you were using a Mac, it looked something like this. If you were a software developer in 2005, you were probably (hopefully) using a centralized version control system like Subversion. I professionally used RCS, CVS, Subversion and Perforce before Git came along. Hell, I was actually in a company that FTP'd it's PHP files directly to the production server. Now, if you were working on proprietary commercial software, centralized version control systems like SVN honestly wasn’t the most horrible thing. It was pretty simple to checkout, make changes, check back in. Branching and merging completely sucked but in a lot of situations it could basically be avoided (I’m not sure I ever really used branching in Subversion or Perforce). People probably complain more about Git today than they did about SVN back then, to be honest - the user interface and mental model is arguably simpler than in Git. Perforce 2005.1 visual client. I spent a loooot of time hating this software. The big problem that I think started coming to a head around this time was not in the world of professional development within closed, trusted teams. The big problem was within the growing world of open source. You see, open source was barely a thing before this time, especially compared to today. Most of you kids probably don’t remember a time where there weren’t millions of open source projects around, but the phrase was only coined in 1998. To get a sense of scale, Dirk Riehle published a paper in 2008 analyzing global open source project trends and they estimated that there were a total of 18,000 active open source projects in the world at that time - in 2005, certainly far fewer. Total open source projects. From “The Total Growth of Open Source”, 2008, published by Amit Deshpande and Dirk Riehle To put this in perspective, there are over 280 million public repositories on GitHub alone today. So, why did open source help usher in the era of Git and GitHub? Because open source was growing fast and centralized version control systems were particularly bad at open contribution strategies. That is, you couldn’t easily share a project publicly and then take contributions back into it in a simple manner. Contributing to Open Source in 2005 Really, how bad could it have been? If you would like to see me talk about what open source contributing was like via my AWS Tokyo keynote 10 years ago, check it out and then you can skip the next few paragraphs: Me talking about how we used to contribute to open source before GitHub. I recommend watching it at 1.5x, I had to speak slowly for the translators. Basically, you could make your Subversion server read only for unauthenticated users, this is generally how you hosted an open source project (or you put a tarball somewhere occasionally). If you wanted to contribute, essentially you had to: checkout the latest version make your changes generate a patch file with GNU diff upload that patch file to a ticketing system or email list used by the project Then the maintainer needed to: pull down that patch file apply it to their project to see if it applied cleanly worked properly either submit feedback, make changes, or commit the change There are still artifacts of this around the internet. I used the Trac project at some point for this type of project, you can still see their Submitting Patches guide and an example of how a change would be suggested. It was a goddamn nightmare. The Rails project, as well as my friends (and future GitHub cofounders) at Err used a similar ticketing system called Lighthouse (which is mind-blowingly still up) and one of my earliest open source projects was a command line tool called git-lighthouse that could simplify the process of pulling down and applying attached patches from tickets you wanted to test out. Here is an example of 3 different versions of a patch that were submitted to the Rails project in the early days. This process sucked enough that when something came around that simplified it, it was quickly embraced. And that something was GitHub. But first, we needed a Git. The Rise of Git Git actually started from the fact that Linus Torvalds really liked an (at the time) commercial version control system called BitKeeper. It was actually built specifically to help simplify the existing kernel development process. If it had been open source or had better licensing terms, there probably would have been no Git or GitHub. However, what happened instead was that one of the Linux developers reverse engineered the protocol, breaking the licensing terms, and BitKeeper and Linus determined that the spat that followed was untenable and they mutually decided to part ways. So Linus took some of the concepts that BitKeeper opened his eyes to, threw together the simplest thing he thought would solve his problems with those principles in mind, and called the new project Git, the “information manager from hell”. It was fairly quickly embraced by several people in the Linux community and slowly grew into an actual, sort of, version control system. There are several reasons why Git felt awesome at the time. They were: branching and merging were dreams rather than nightmares it was stupendously fast permissions were vastly simpler In the early days of Git, I would do talks where I would just go on stage, create a few branches, commit changes into them, switch between them and then merge them together, all in 60 seconds. I would literally see peoples jaw’s drop. Some of them would think I was faking the demo. I just cannot tell you how magical it felt in 2006 to be able to switch and merge contexts that fast and easily. In Subversion this was a total nightmare. Baby Scott talking about Git at RailsConf 2008 Not having to go over a network to negotiate a commit with a central server was also incredible. It felt like a rocket ship. Everything was so fast. And probably most importantly, you could fork the repository incredibly easily, which meant that you could host your own copy of a repository and have your own write access and push changes there that other people could pull down into their fork. The Linux project started doing this early on - for larger changes, they could send a request to pull changes from a hosted fork and Linus could very easily do so. In fact, if you’re wondering where the terminology “Pull Request” came from, this is it. Git has a git request-pull command that would format an email for sending to a mailing list to help make this process simpler. When GitHub added the ability to basically generate this same type of message, we decided that a request to pull should be called a Pull Request. (A little more backstory on that here if you’re curious) Some people think that developers liked Git because it was distributed and you got the whole history when you cloned, which meant you could share locally, etc. I disagree. I don’t think almost anybody really cared about any of that. Distributed was cool because you could do operations fast and you could host your own full, writeable fork which made permissions much simpler. It was cool because contributing went from a problem of who had permission to push into the simplicity of who had something interesting to pull. And of course, this last point led directly to GitHub. The Rise of GitHub Late last year, I interviewed my GitHub cofounder Tom, and among the things we discussed, he told the story of how he got the initial idea of working on GitHub. Essentially, when he was working at Powerset, Tom’s team started using Git internally. However, it was a pain in the ass to add other team members to the internal server, because Git’s main protocol was over SSH, which means you need a user with ssh privileges on the machine. For everyone. It was difficult and, for most of the team, not worth it. This spawned the concept for him of making this process as easy as possible. Git is awesome, Git hosting is a pain in the ass. This is why Tom started working on GitHub. Why GitHub was started. To ease ass pain. I went through my old emails to see if I could find the first time I heard about Tom’s “GitHub” project, and it was this email from Chris responding to a Git screencast I made in late 2007. It was still a secret side project between the two of them at that point (also Chris… lower-case ‘h’?) and it’s when I started chatting with Chris and Tom about the Git/Ruby libraries that ran the site, and eventually how I wiggled my way into the project and company. There are a few interesting things about this pitch. The first is that they compared it to the only other real public Git hosting site, repo.or.cz (which also miraculously is still running, if you want to see what the state of Git hosting was pre-GitHub), but made a crucial innovation over that site and every other hosting service like it, which is to make it user-centric rather than project-centric. Before this, if you wanted to host something on Sourceforge or whatever, you needed to grab the name. With GitHub, you can make any project you want named anything you want because it’s namespaced to your user. The second was that they focused on a pull model rather than a push model (basically the permissions thing I talked about before). The third is that “not ugly” was a core feature. GitHub had taste. Git Wins This is why Git was cool and why GitHub was started to make using it easier, but the question is, why did Git win? Lots of distributed systems sprang up during this time. Mercurial was similar in a lot of ways and better in many. Why did Git come out on top in the great DVCS war? I think the answer there was “PR”. And there are two big PR gorillas fighting it out for the “why did Git win” answer. The first was Linux and by extension, Linus. The other was GitHub, and specifically the Rails community. Maybe it was Linus/Linux The Linux project using Git and Linus having started the project gave instant credibility to it. Everyone knew Linux, everyone knew Linus. If he made an amazing operating system that everyone uses (at least for their servers, next year is the year of Linux on the desktop), he can certainly make a next-level version control system. And even if it’s difficult to use, that just means he’s smarter than us and we should try harder, right? This video is one of the first talks about Git online, circa 2007. It's Linus talking about Git and distributed version control systems, then a brand new concept, at the Google campus. It came out in between when I started using Git (late 2005) and when I started at GitHub (mid 2008). I watched it several times, as did millions of people. Who doesn’t like listening to the Linux guy say “CVS is the dumbest shit that has ever been thought of, and everyone who disagrees is ugly and stupid”? At Google! It’s just great PR. Beyond that, if you conflate Linux and Linus, which most people do, there is an argument that Linux itself pushed Git adoption indirectly through Android. This is where I really don’t know how much impact my own efforts or GitHub’s efforts had compared to this big, quiet, behind-the-scenes side effect of Android becoming a thing at exactly the same time. Or even, my personal impact in either or both of these fronts, doing Git talks and corporate training for years. In early September 2008, right as Android 1.0 was being released (like 2 weeks after this email, but before I did the training), Shawn Pearce, an early super hero of the Git ecosystem, wrote me this email asking me to help train the Google Android team on Git. It’s difficult to determine what impact Android had in corporate Git adoption, but it certainly wasn’t zero. While the Google/Android team was the first that I did corporate training for under the GitHub banner, I also eventually did Git training for engineering teams at Motorola, Qualcomm, Ericsson and Broadcom, just to name the telecoms. And that was before we hired a team to do this for us full time. Linus pushed Git with his broad ranging brand of superstar nerd PR that Mercurial never got, but Android furthermore pushed Git uniquely, via it’s dependance from the Linux kernel, into massive companies out of pure practicality that also otherwise never would have happened for at least another decade. Maybe it was GitHub There is also, and I must say this with a grain of hopeful humility, a possibility that GitHub was the determining factor in the eventual dominance of Git over Mercurial. GitHub had the incredible luck to have an amazingly supportive and hip community that embraced us right out of the gate, namely the Ruby community. Within months, everyone in the Ruby community put their stuff on GitHub. Rails was the hot shit at that time, it was cooler than PHP, JS frameworks weren’t really around, there was no Node, etc. So everyone was paying attention to what the hip cats in the Ruby community were doing, they were the bleeding edge of cool development in the software world. And they were using GitHub. And it’s not just me, Linus himself also recently said that from his perspective, the Ruby community unexpectedly made Git explode overnight. He doesn’t credit GitHub for that by name, but I think it’s impossible for anyone to argue that the Ruby community didn’t adopt Git in a very large part due to us. By the transitive property and some speculation, I’ll make the claim that Linus in fact thinks that GitHub is the reason Git won. 😀 “…the Ruby people, strange people, started using Git and suddenly it just exploded…” Of course, the Ruby community adopting GitHub wasn’t random. I remember all of us - Chris, Tom, PJ and myself - sitting at tables at Ruby conferences with all the guys in the early Ruby community, showing them GitHub, selling them on Git, doing talks, etc. We were all speaking at the same conferences, we all drank beers together after Ruby meetups in SF. These were the guys who started Rails, Heroku, Twitter, jQuery, you name it. It’s not that we were selling it, it’s that we were all sharing what we were passionate about. There was a high level of trust in this community, the GitHub founders were a deep and authentic part of it, and we all tried each others stuff and supported each other. Me and PJ at Scotland on Rails in March 2009 with a table full of amazing early Ruby peeps The Ruby community using GitHub meant that every conference talk everywhere had a GitHub plug in it. Free advertising everywhere. This meant that as more and more projects moved to or were started on GitHub, even people who liked Mercurial had no real choice but to use Git. After a while, it probably just wasn’t worth it. GitHub’s dominance in the hosting sphere just crushed Mercurial in just the span of a few years. In Mercurial land, there was BitBucket, which was started for Mercurial hosting and written in the Django framework, but I think we just had too much of a head start and there wasn’t enough differentiation. The Python community just didn't adopt it as aggressively as our Ruby community did. As early as December 2008, GitHub was hosting about 27,000 public repositories where BitBucket had a little over 1,000. It became difficult to catch up. How do I remember those numbers you might ask? Well, I had a website I put up called whygitisbetterthanx.com and this guy named Jesper emailed me to say that one of my points was incorrect, where I argued that Git has GitHub and Mercurial and Bazaar didn't have a GitHub. I rather arrogantly argued that they're not in the same league. Young Scott being a little bitchy. Sorry, Jesper. To his credit, he never called me out on my response, which now seems real snippy of me in hindsight. But it turns out that Jesper in fact was the founder of BitBucket. Whoopsie. A year or so later, we met up with him in Amsterdam, drank some nice whiskey together and have remained friendly ever since. GitHub cofounder PJ Hyett and myself with BitBucket founder Jesper Noehr (black shirt) getting a friendly competitor whiskey in Amsterdam, circa 2009 or so. Always be friends with those you compete against. The Competitive Field Collapses In the end, whether it was GitHub that helped Git win, or Git that helped GitHub win, it was quickly over. In 2006-2007, people were first learning about distributed version control systems and Git and Mercurial were starting to fight it out. In 2008, GitHub launched. In 2011, both Google Code and BitBucket added Git support, which I’ll mark as the year that the nail was in the Mercurial coffin. Git had won and GitHub was now essentially unbeatable. Just 4 years later, in 2015, Google Code just completely gave up and shut it’s service down. In the email they sent out, they basically said “just move to GitHub”. If I remember correctly, they even reached out to us for help with the migration. So, Why Not Google Code? Of course, while BitBucket started after us and so we had a head start, there were other hosting sites that existed before us. So why didn’t they win? In early 2009, Google Code added Mercurial support and Sourceforge added both Git and Mercurial support. So if these industry monsters had a massive user head start and had DVCS support only months after we launched, why didn’t they wipe the floor with us little guys? Not only little, but also nearly completely un-funded. Chris was able to put a little bit of money in to bootstrap iirc, but the rest of us were totally broke and we didn't raise any outside funding. When Google Code launched Mercurial support, we were still 4 developers working out of cafes in South Beach with zero VC investment. We struck deals with our buddies (May 2008) at Engine Yard for help with hosting costs because we didn’t really have the cash. How is it possible that this tiny, unfunded team made Google Code fold in just a few years? Side Note: GitHub Funding As a side note, speaking of funding, the article I’m reacting to states “VC investment wasn't an option for the cofounders.” This is purely untrue. From the very first days, we were talking to VCs. When PJ emailed me in July of 2008 to say they wanted me to join them and we should all make the leap, quit our respective jobs, and make this a full time thing instead of a side project, he explicitly said “we've been talking to one VC in particular we like a lot and we want to raise a handful of money to do a few of things.” These were things like get an office, hire some people, etc. It was always on the table, we could have done it at literally any time. We considered and thoughtfully rejected the idea constantly over the years. We didn't really need an office. We didn't really need more people yet. Not only that, we actually nearly rejected the idea when we were considering our $100 million A round from Andreessen Horowitz, 4 years later. I remember vividly the night we all sat down for dinner at some restaurant on Folsum St in April of 2012 and argued very heatedly if we should take a capital round at all. We basically had offers from a16z, Benchmark, Sequoia and Bessemer (pretty much the best VC firms on the planet) on the table and meanwhile us four assholes sat around and heatedly yelled at each other about whether or not we should tell all of them “thanks, but no thanks”. Offers that other tech entrepreneurs would probably literally have killed for. But the point is that it wasn’t that we couldn’t have raised money, but that we didn’t even need to in order to take down the entire field. GitHub Had Taste The original article is correct, the other hosts focused on distribution and revenue streams. We cared about developers. But it wasn’t about when they added Git, it never really mattered. They never had any taste. They never cared about the developer workflow. They could have added Git at any time and I think they all still would have lost. You can try to explain it by the features or “value adds”, but the core takeaway that is still relevant to starting a startup today is more fundamental than if we had an activity feed or profile page or whatever. The much simpler, much more fundamentally interesting thing that I think showed in everything that we did was that we built for ourselves. We had taste. We cared about the experience. We were developers and we built what we wanted in order to enable how we wanted to ideally work. We were the only tool in the space built by developers for developers without PMs or accountants or CEOs trying to optimize for revenue rather than for developer experience. In the end we won because the open source community started to converge on distributed version control and we were the only ones in the hosting space that truly cared about how developers worked at all. The only ones who questioned it, approached it from first principles, tried to make it better holistically rather than just throwing more features onto something existing in order to sell it. Why GitHub Won So, to sum up, we won because we started at the right time and we had taste. We were there when a new paradigm was being born and we approached the problem of helping people embrace that new paradigm with a developer experience centric approach that nobody else had the capacity for or interest in. I guess the question is, what is the next sea change in developer workflow, and who will have good enough taste to make it explode in the same way? The link has been copied!",
    "commentLink": "https://news.ycombinator.com/item?id=41490161",
    "commentBody": "Why GitHub Won (gitbutler.com)199 points by hardwaregeek 2 hours agohidepastfavorite197 comments DannyBee 2 hours agoActually, Google Code was never trying to win. It was simply trying to prevent SF from becoming a shitty monoculture that hurt everyone, which it was when Google Code launched. Google was 100% consistent on this from the day it launched to the day it folded. It was not trying to make money, or whatever I was there, working on it, when it was 4 of us :) So to write all these funny things about taste or what not, is totally besides the point. We folded it up because we achieved the goal we sought at the time, and didn't see a reason to continue. People could get a good experience with the competition that now existed, and we would have just ended up cannibalizing the market. So we chose to exit, and worked with Github/bitbucket/others to provide migration tools. All of this would have been easy to find out simply by asking, but it appears nobody bothers to actually ask other people things anymore, and I guess that doesn't make as good a story as \"we totally destroyed them because they had no taste, so they up and folded\". reply defen 1 hour agoparentIncidentally, this is why I don't use any Google products other than search. It's never clear to me which ones are real products that are trying to win and make money, and which ones are just fun side projects that a multibillion dollar corporation is doing out of the goodness of their hearts, but could get shut down at any time. reply 0x1ch 1 hour agorootparentEvery project by Google I was willing to jump platforms for, they killed. So outside of email, maps, and search, sometimes voice, they have nothing left that is worth investing time into. It will disappear within a couple years or less. reply dgellow 15 minutes agorootparentprevThat's the big question mark I have for Flutter. Looks like a pretty nice platform from the outside, but I cannot see Google NOT killing it reply richardlblair 46 minutes agorootparentprevUnrelated to the thread - do you use any email providers with a custom domain? If so, would you suggest them? Who are they? reply homebrewer 7 minutes agorootparentIf you need something cheap and are willing to deal with a tiny company, have a look at . I've been happy with them for two years, never had any problems with delivery, and they support infinite domains/aliases, and custom Sieve rules. But do not use it if you need 99.999999% SLAs or anything like that, because again -- it's a one-man show. reply jampekka 14 minutes agorootparentprevYou can also use gmail with a custom domain. Helps with not being locked in to Google, doesn't of course help in them selling your data. E.g. https://juri.dev/notes/email-routing-gmail-cloudflare/ reply softfalcon 40 minutes agorootparentprevFastmail has been solid for the last several years. I would recommend it. reply jacooper 5 minutes agorootparentprevProton mail reply dasil003 40 minutes agorootparentprevFastmail reply bitpush 1 hour agorootparentprevBusiness strategy is more than just launch product, make money. Companies operate in a dynamic space, and if you play any game (basketball, chess ..) you know that moving forward at all cost is not how you play the game. Sometimes you side-step, sometimes you take a step back, sometimes you sacrifice your piece. If you expect you team to just go-go-go, you might something in the short term but you'll fail miserably in long-term. reply chrisandchris 41 minutes agorootparentThat is totally fine, but Google is a case where they go into new business units and fairly often kill those units quite soon. It's not like they're doing cereals and now they're doing other cereals, so you can fall back to their previous cereals. You always have to find a new supplier, or then just start buying bread. reply adamc 22 minutes agorootparentYeah, there are so many examples. It's one of the reasons I was unwilling to jump on board Google Stadia... I kept thinking \"let's wait and see how it does\". Particularly since you had to buy non-transferable game licenses. And, of course, they shut it down. At this point, Google is going to have to be in a space for at least 5 years and clearly be highly successful before I would even think about using them. reply magicalist 1 hour agorootparentprevEh, when it happened the world was ready. It felt like a different internet back then, and it was pretty great for a company to say that everyone was already on github anyways, so let's all go there (Microsoft followed almost the exact same timeline with Codeplex, and really who cares). More Google style would be shortly after shutting down Google Code starting up a new code hosting project, migrating several times to always slightly incompatible frontends requiring developer changes, shutting down the project, and shortly afterwards starting up a new code hosting project... reply JohnBooty 1 hour agoparentprevAll of this would have been easy to find out simply by asking I'm not a journalist, but in an ideal scenario, how would somebody have known that you were one of the key members of the project? It's not like Google (or anybody else) makes this easy to know. And call me jaded, but something tells me official Google PR channels would not have been really helpful for this. And also - are most engineers in your sort of position even free to remark on such projects w.r.t. NDAs, etc? reply DannyBee 58 minutes agorootparent\"I'm not a journalist, but in an ideal scenario, how would somebody have known that you were one of the key members of the project?\" It's not about asking me, it's about not asserting things you don't know. Instead of saying \"people did x for y reason\" when you have literally no data on x or y, you could say \"I don't know why x happened, i only know about z\". Or if it's super important you try to put something there, beforehand, you could say \"hey does anyone know why x happened? I'm working on a blog post and want to get it right\". Then, someone like me who saw it could happily email you or whatever and say \"hey, here's the real story on x\". Or not, in which case you can leave it at \"i don't know\". The right answer is not to just assert random things you make up in your head and force people to correct you. I'm aware of the old adage of basically \"just put wrong stuff out there and someone will correct you\", but i generally think that's super poor form when it's about *other people or things and their motivations\". I care less when it's about \"why is the sky blue\". In this case, it also happens that there are plenty of on-record interviews and other things where what i said, was said back in the day. So a little spleunking would have told them the answer anyway. reply doctorpangloss 27 minutes agorootparent> The right answer is not to just assert random things you make up in your head and force people to correct you... super poor form Tough cookie, he succeeded via provocation. The norms you are advocating for as an alternative boil down to, \"Google decides.\" Are those polite norms even worthy? You are speaking your truth on a semi-anonymous social media forum! It has really touched a nerve for you. The moderators here delight in the mildly miffed author responding to comment after comment. This is the opposite of however your \"on-record interviews\" were formatted before this little blog post - they weren't notable at all, they're still not notable, and here you are setting your record straight. It's great! reply burnished 20 minutes agorootparentReally letting your opinion on Google dictate the rest of your response here huh? reply softfalcon 35 minutes agorootparentprevThis explains so much about modern media, news, and story-telling. It's easier to make up a plausible narrative that supports your story than simply admitting you don't know. You can see how as the article develops, they go from being \"uncertain what made GitHub succeed\" to definitively being sure about why it succeeded. It doesn't surprise me that details were glossed over as the story rose to the ultimate crescendo of \"GitHub dominates\". This is how a good tale is spun and the people lap it up. What's a good tale without a bit of embellishment? (said every bard since antiquity) reply shadowgovt 51 minutes agorootparentprevTo be a bit more generous: I think from Scott Chacon's point of view, \"They had no taste and we beat them in the market\" is a fair way to hold the elephant. Lacking the Google-internal perspective, it's a reasonable conclusion from the signal he has. I don't get the sense from this post that he's trying to publish a doctoral thesis on the historical situation in the industry; he's providing some primary-source testimony from his point of view. reply DannyBee 35 minutes agorootparentI guess i'm going to disagree with you. He's not just providing primary source testimony from his point of view, he's trying to pretend he has primary source testmony on what others were doing as well. If he left out the parts where he has no data (or said i don't know), it would have IMHO been a better post, and actually primary source testimony. You also don't run into the Gell-Mann amnesia problem this way. To each their own, of course. reply jaredklewis 8 minutes agorootparentprevWell, finding, vetting, and getting comments from sources is like half of journalism. If you can't or won't do that, whatever you are doing is probably not journalism. It's just an editorial, think-piece, or whatever. reply SoftTalker 1 hour agorootparentprevThe way it used to work is that tech journalists (or sports journalists, or any other type) had contacts in the industry. If those people were not directly involved, they probably could at least suggest someone else who might know. Leads were followed up, and eventually the writer got the story. I'm not sure how it works now, cynically I would suggest that the writer asks an LLM to write the story, gets a rehash of Wikipedia and other sources, and they maybe makes some attempts at firsthand verification. reply mattnewton 23 minutes agorootparentprevJournalists are supposed to investigate not speculate because finding an email is too hard reply dlisboa 56 minutes agorootparentprevMaybe a cofounder of GitHub has the reach and network to ask for the e-mail of someone who worked on the Google Code team. A journalist might not, that's true. Just flat out saying they had no taste in product development, however, is a bit of trash talking for no reason. reply svnt 9 minutes agoparentprevI had this theory that generations raised on the internet and exposed to it from birth would be the most humble generations ever, because we all look for ways to be uniquely valuable, and it became nearly impossible to be egotistical when faced with the entirety of even just a mature youtube platform. Instead what we got was higher degrees of selective attention, and very elaborate and obscure flip-cup tricks. reply schacon 2 hours agoparentprevI'm not sure what \"SF\" means in this context. San Francisco? I can't figure out what you want to say Google Code was for exactly. If Google launches a major project, I find it hard to believe that it's just for fun. reply blktiger 2 hours agorootparentIt’s short for Source Forge which is still around technically but a shadow of its former self. reply philipkglass 35 minutes agorootparentThere is still some code hosted on SourceForge which has no other public source. This is unsettling because I don't know how long SourceForge will still be around and Wayback Machine captures of SF pages don't include tarballs. Download backups yourself whenever you find something like this. I'm contributing to someone's software that started as an academic project. The current version is on GitHub with history back to 2014 but early releases back to 2008 (from before the author started using version control) are on SF in the form release_1.0.tgz, release_1.1.tgz, etc. I stumbled on these old versions this weekend while looking for related material. Once I decompressed them I found that they contained notes and old code that really helps to understand the current project's evolution and structure. reply PaulHoule 1 hour agorootparentprevIt was early example of the \"enshittification\" phenomenon. It was a particular bad example of advertising and other spammy distractions because sites for developers have the lowest CPM of anything except maybe anime fan sites. It is super hard to break through a two-sided market but it is possible when a competitor has given up entirely on competition, which might have happened in the SourceForge case because the money situation was so dire they couldn't afford to invest in it. reply 3eb7988a1663 1 hour agorootparent...lowest CPM of anything except maybe anime fan sites. I am not in ads, so could you expand on this? Why are anime sites low value vs other niche? I would naively expect that anime has huge numbers ofGitHub followed a similar track to SourceForge Can you provide an example? reply nine_k 1 hour agorootparentprevThe fact that letters \"SF\" may need explanation in a context of code hosting and building says how thoroughly the job has been done. A number of good alternatives exist, there's no monoculture (even though some market domination is definitely in place, but now by a mysterious GH). reply digging 1 hour agorootparent> A number of good alternatives exist, there's no monoculture That doesn't sound true to me at all, except maybe in some very small niches. I've used Bitbucket at exactly one job; I've found Codeberg, but no project I've used was actually hosted there; and literally everything else I see or use is on Github. reply nine_k 1 hour agorootparentGitLab is relatively more widely represented, but of the projects I encounter, about 2-3% are on GitLab. I encountered projects on Codeberg, too, and even on sr.ht. A bunch of larger projects have a mirror on GitHub for easier access. BTW there's launchpad.net which is often overlooked, bit it's vital for Ubuntu-specific projects. At paid day jobs, I had to use BitBucket at least twice, and I miss better code review tools, like Phabricator. GitHub definitely dominates the market, partly due to the network effects, but I don't think they have a lot of moat. If something goes badly enough wrong there, there will be plenty of viable alternatives with an easy to trivial migration path. reply sangnoir 1 hour agorootparentprevA decent number of larger open source projects self-host. reply shadowgovt 49 minutes agorootparentprevIt reminds me of how Stackoverflow won so successfully that to even know about the old \"expert sex change\" joke is to thoroughly date oneself in modern conversation. reply tsm 2 hours agorootparentprevSF is SourceForge, which at the time effectively had a monopoly (and also sucked) reply tadfisher 1 hour agorootparentIt still sucks, but it sucked then too. reply DannyBee 49 minutes agorootparentI miss mitch hedberg reply schacon 2 hours agorootparentprevI now realize that it's SourceForge. :) reply breck 1 hour agorootparentI thought he was talking about SpaceForce. Wait until we get to 2050, SpaceForce develops a really shitty monoculture. That's why I came back. reply kemayo 2 hours agorootparentprevSourceforge. reply wafflemaker 2 hours agorootparentprevThis post replied to a post talking about Source Forge. Had the same problem :) reply crop_rotation 2 hours agorootparentprevI think it means SourceForge. reply ipsi 2 hours agorootparentprevSourceForge, probably. reply noitpmeder 2 hours agorootparentprevSourceForge reply osmsucks 10 minutes agoparentprev> Actually, Google Code was never trying to win. Herein lies the tragedy. Google could've offered, even sold, its internal development experience (code hosting, indexing and searching, code reviews, build farms, etc...) which is and was amazing, but it decided that it wasn't worth doing and let GitHub eat its lunch. reply franciscop 16 minutes agoparentprevThere's a typo in the subtitle and just below this it's quoting the controversial/sensationalist content creator t3dotgg. While it's a very interesting topic, these things remove a lot of credibility to the rest of the article, plus now seeing your comment, I'm flagging it since it seems a very low quality one. reply skybrian 30 minutes agoparentprevI don’t see a contradiction; it’s all part of the story. Understanding your (Google’s) motivations explains why Google Code didn’t improve as much. It doesn’t contradict that Github had better UI, or their explanation of their motivation to build a better UI. reply hintymad 1 hour agoparentprev> It was simply trying to prevent SF from becoming a shitty monoculture that hurt everyone Initially I thought SF means San Francisco, and I thought \"Wow, what kind of monoculture can be prevented by Google Code\", and then I realized that SF meant Source Forge. reply shadowgovt 53 minutes agoparentprevThe only real tragedy here is that Google really did have best-of-industry semantic search integrated into their code searching tools, something that nobody has been able to replicate. GitHub is great, but it's absolute ass for search. To the point where for any nontrivial question I have to pull down the repo and use command-line tooling on it. reply jerjerjer 26 minutes agorootparentNew GitHub full text search [1] is amazing. It is so good that for me it often replaces StackOverflow - I just use it to see how some API function is being used. Especially useful if you're searching for an example with a specific argument value. [1] https://cs.github.com/ reply dmoy 36 minutes agorootparentprevDo you mean the non-semantic indexing, which covered most of Google Code? Like grep-style supporting, but no real semantic data? Or are you talking about the few repos that had semantic indexing via Kythe (chromium, android, etc)? We never got that working for generic random open repos, primarily because it requires so much integration with the build system. A series of three or four separate people on Kythe tried various experimentation for cheaply-enough hooking Kythe into arbitrary open repos, but we all failed. reply jonathanyc 1 hour agoparentprev> It was not trying to make money, or whatever If Google Code succeeded, it’s hard to imagine that Google would not have tried to monetize it someday. This also reminds me of Google’s (also initial) position on Chrome vis-a-vis Firefox: create a product “not trying to make money, or whatever” but just to limit the market share of a competitor. The less flattering term for this in the context of anticompetitive behavior is “dumping”: https://en.wikipedia.org/wiki/Dumping_(pricing_policy) reply DannyBee 40 minutes agorootparent\"If Google Code succeeded, it’s hard to imagine that Google would not have tried to monetize it someday.\" Google code did succeed in that sense. It had hundreds of thousands of 30-day active projects, and some insane market share of developers. I don't honestly remember if it was even shrinking when we decided to stop taking in new projects. I doubt we would have monetized it directly (IE sell an enterprise version) - the entire market for development tools is fairly small. In 2022 it was ~5 billion dollars, and future estimates keep getting revised downwards :). CAGR has been about 10-14% in practice, sometimes less. I don't remember if it's still true, but most of that 5 billion dollars was going to Atlassian (80% at one point). Now, if you project backwards to 2006, and compare it to other markets google could be competing in, you can imagine even if you got 100% of this segment it would not have actually made you a ton directly. Indirectly, eh, my guess is you still make more off the goodwill than most other things. It's actually fairly rare to make any significant money at development tools directly. Nowadays, the main source even seems to be trying to sell AI and productivity, rather than tools. reply bryanlarsen 44 minutes agorootparentprevIt seems highly likely that a successful Google Code would be used as an onramp to Google Cloud. IOW, indirect monetization so it likely would still have a generous free component. reply meiraleal 1 hour agoparentprev> Actually, Google Code was never trying to win. > It was simply trying to prevent SF from becoming a shitty monoculture that hurt everyone Being an insider of Google might make one be completely out-of-touch of reality. Google Video was trying to prevent Youtube from becoming a shitty monoculture that hurt everyone, too? This one clearly failed then. reply BSDobelix 52 minutes agorootparent>Google Video was created to prevent Youtube from becoming a shitty monoculture, too? Like Google+ and all the other attempts: https://killedbygoogle.com/ Google is actually the good guy to prevent monopolies, we just don't understand them ;) reply cynicalpeace 1 hour agoparentprevnext [14 more] [flagged] sanderjd 1 hour agorootparentCurious whether you read the (pretty short?) post you replied to... That post said that their goal was to make sure SourceForge, which was truly awful and also the only game in town back then, did not become (or remain) dominant. Pretty hard to argue they failed at that goal when SourceForge is so not-dominant today that some people here didn't even recognize its acronym! reply cynicalpeace 1 hour agorootparentYour claim is Google Cloud Code is what brought SourceForge down? And why is it a believable goal that as long as you bring down a competitor, it's OK if you fail? No one (sane) has such a goal. reply DannyBee 1 hour agorootparentWhat evidence do you have, exactly? I've been super-consistent on this for at least 10 years: https://news.ycombinator.com/item?id=8605689 It was also written in our OKRS, etc at the time. I probably have plenty of internal evidence. In the end, we would have been happy if SF had become a great and reliable place as well. You are assuming the goal was to destroy it. But in practice, we expected them to compete for real and become better as a result. reply ics 53 minutes agorootparentprevDannyBee didn’t claim that they “brought SourceForge down” or even attempted to. They said Google Code was intended to prevent a monoculture, i.e. SourceForge being the only popular option. reply mgkimsal 1 hour agorootparentprevIf the goal is to stop a competitor from slow or stop a competitor from gaining market share, and you slow or stop them from gaining marketshare, how is that failing? reply meiraleal 1 hour agorootparentSo a third competitor appearing and taking the marketshare from both of them doesn't count? Such a great logic to get promotions. Of course it would come from google. reply DannyBee 50 minutes agorootparentYou are again thinking in terms of winning, and in literally the first sentence i wrote that we were not trying to win anything. The goal was to get SF to not be a shitty monoculture. Ideally by SF being forced to compete with us, and making their product less shitty. It does not require destroying or harming SF to achieve this goal. They chose, ironically, not to compete, and were destroyed as a result of their own choice. It also happened that third parties came along and helped make it both not shitty, and not a monoculture. Goal achieved. Why does marketshare have to enter into any of it? Nobody was trying to destroy anything. The person who started the project (dibona) came from VA linux and slashdot, and was very good friends with the people who ran both (still is!). He also started summer of code and lots of other things. Stop being so amazingly cynical. reply mgkimsal 1 hour agorootparentprevWhy does it matter? If you send troops to block the enemy from advancing, if those troops block the enemy from advancing, they've succeeded. Even if those troops didn't \"win the war\". Even if they all died in combat. The mission was a success. If you want to say this is revisionist history from a googler... sure - make that case. But simply deploying a service to try to 1) prevent a competitor from gaining marketshare and/or 2) get the competitor to suck less... it's a valid move. Personally, I don't think google code alone made much of an impact on SF directly, but google code and ms codeplex together probably did get people to start considering using something beyond SF. reply meiraleal 55 minutes agorootparent> Why does it matter? Because the truth matter? it is quite annoying to see people trying to bend reality to make themselves or their projects more important than they really were. Google code sucked, like many google projects. More because google than because of the code, but it sucked. Github wasn't successful because Google Code made people \"start considering using something beyond SF\". Github succeeded because git is great and social network features allowed it to reach a much bigger audience. reply shadowgovt 47 minutes agorootparentprev> No one (sane) has such a goal I can't speak to modern Google, but old Google definitely did things to ecosystem-shape. It wasn't \"sane\" in the sense that, yes, it does buck simple calculations of profit-maximization; being able to avoid those simple calculations is the reason the founders structured the IPO to maintain executive control, so they could make wild bets that broke with business tradition. reply meiraleal 1 hour agorootparentprevnext [2 more] [flagged] cynicalpeace 1 hour agorootparentGoogle engineering culture ¯\\_(ツ)_/¯ reply aimazon 1 hour agorootparentprevin hindsight, the success of GitHub could be seen as a missed opportunity for Google with Google Code but at the time SourceForge was a website with some advertising, the commercial opportunity was minuscule compared to what GitHub is today. I'm sure you can go back to Hacker News from 2007/2008 and find discussions that confirm what the parent said. reply dotnet00 1 hour agorootparentprevNot sure where you're seeing failure? I remember that Google code was very popular back then alongside MS's Codeplex (for .NET stuff at least), both being better than sourceforge, but neither really attempting to be businesses and just providing free code hosting. They both shut down around the same time with migration tools for GitHub, and as we all know, MS eventually went on to buy GitHub outright. For me, GitHub had only entered my sphere of knowledge when the Google code phase out started. It was pretty painful because I was already struggling to teach myself to code and git seemed incomprehensible compared to svn. Sourceforge was up there with cnet as being a somewhat sketchy site which could occasionally have something genuinely interesting to download, but usually did not. So I can kind of believe that GC/Codeplex weren't necessarily aiming to be profitable products. Selling access as an enterprise product was a common model even back then, and it would've been an obvious route if they were actually aiming for profit. reply BSDobelix 2 hours agoparentprevSo wait, you tried to prevent SF to become a \"shitty monoculture\"? First: That sounds completely not like Google Second: Now you have GH as the \"shitty monoculture\" (owner is MS and erases your license for Co-pilot) Third: >>We folded it up because we achieved the goal we sought at the time, and didn't see a reason to continue. Yeah ok that sounds like Google, try's to enter another market just to hurt them then folds ;) reply DannyBee 1 hour agorootparentThis was 2006 Google, which did stuff semi-altruistically all the time. At that point, SF was serving malware and stuff. It was really not a great time. Github became a monoculture years later when others folded. Google code was shut down in 2016. Github wasn't quite a monoculture then. I also said, back in 2014, that it might be necessary to do something like google code again in 5-10 years: https://news.ycombinator.com/item?id=8605689 10 years later, here we are i guess :) Though i think what i said then still holds - Github is not anywhere near as bad or unreliable as SF was. reply BSDobelix 1 hour agorootparentSF served \"malware\" in 2013 NOT 2006: https://en.wikipedia.org/wiki/SourceForge#Adware_controversy After slashdot was purchased from condenast (i think?) reply naniwaduni 1 hour agorootparentThey had a bad name for the download pages being ad-infested even before they bundled the malware in the installers. (And yes, fake download buttons on a site serving binary downloads went exactly where you'd expect.) reply BSDobelix 56 minutes agorootparent>fake download buttons Yes and today Ad-Sense (Google) took the Crown from being then biggest Scam AD's deploy-er. And really i don't think that's true before they where sold, ad's sure, scam/malware stuff? I don't think so...at least i cant remember. reply DannyBee 1 hour agorootparentprevI mean, that's just when they did it fairly deliberately. Regardless, I think you would be hard pressed to argue SF was a great hosting environment when Google Code launched, which was the point. reply BSDobelix 1 hour agorootparent>hard pressed to argue SF was a great hosting environment when Google Code launched But SF had FTP, Websites, SVN hosting and i think even a WIKI, so you can hardly compare it with Google-Code...and hey at least they opensource'd their \"forge\": https://allura.apache.org/ IDK i don't have such bad memory's about SF, even today you serve big files over SF because of GH limits. reply ndiddy 44 minutes agorootparentSourceForge was originally open source, but they later closed it. GNU Savannah (https://savannah.gnu.org/) runs on a fork of the last open version of SourceForge. reply remexre 1 hour agorootparentprevFor a comparison on the scale of harm from the monoculture, recall that SourceForge was bundling malware with downloads, and still has a full page of ads when you download from it. If I recall correctly, SVN was also more popular than Git at the time, so migrating hosts was a lot more painful than now... reply bluGill 55 minutes agorootparentSVN's model is what everyone is using. Sure you git, but almost nobody is using the distributed parts - they all sync to a central sever (github). SVN just could get user management, or merges right - those should be solvable problems but somehow were not. (I don't know enough about SVN to speculate on why they didn't) reply BSDobelix 1 hour agorootparentprevAnd then you force people to change from google code to something else just to prove a point since people then where unable to setup svn server ;) Even today you find death links from google code repos. reply shawabawa3 2 hours agoparentprevIt's a bit of a cop out to say \"we were never trying to win\" If you were never trying to win, that's a product failure You should have been trying to win, you should have built a strong competitor to GitHub and you shouldn't have let it rot until it was shut down The world would have been a better place if Google code tried to be as good as GitHub reply DannyBee 1 hour agorootparent> It's a bit of a cop out to say \"we were never trying to win\" It's literally not? We had a goal from the beginning - create enough competition to either force SF to become better (at that time it was infinite ads and malware), or that someone else wins. > You should have been trying to win, you should have built a strong competitor to GitHub and you shouldn't have let it rot until it was shut down That's your goal, not mine (or at the time, Google). Feel free to do it! You don't like what we had as a goal - that's okay. It doesn't mean we either failed, or had the wrong goal. We just had one you don't happen to like. > The world would have been a better place if Google code tried to be as good as GitHub One of the things to ask before you either start something or keep doing something is \"who actually wants you to win?\" If the answer is \"nobody\", it might not make any sense to do. It's not obvious in 2016 anyone would have wanted us to win. By then, Google had lived long enough to see itself become a villain. There was reasonable competition in the space. I don't believe we would have really served people well to keep going. reply SoftTalker 56 minutes agorootparentprevIt was similar with Chrome. Internet Explorer was the monoculture browser and stagnating. Google had things they wanted to do on the web but needed better browsers. The original goal was to introduce competition in the browser space so that all browsers would get better. They may have changed goals along the way, but that was the original stated goal. In the end they killed IE and now they are the monoculture outside of Safari. reply sanderjd 1 hour agorootparentprevDifferent groups of people have different goals. Not every group of people has \"winning\" a market as their primary goal. reply ulbu 1 hour agorootparentprev> If you were never trying to win, that's a product failure. what? reply AnotherGoodName 2 hours agoprevWell Sourceforge literally bundled malware for a while. So everyone had to move. https://news.ycombinator.com/item?id=31110206 This articles about the open source distribution side but I will also point out that the number of developers who don’t realise your remote GitHub repo can be located on any machine with an ssh connection and nothing more is surprising. As in people use private GitHub repos thinking that’s THE way you work with git. If GitHub was just for open source hosting I suspect they’d have trouble monetising like sourceforge clearly did which led to scammy attempts to make money. But they always had this huge usage of private GitHub repos supporting the rest. This must have helped a lot imho. reply marcosdumay 1 hour agoparent> If GitHub was just for open source hosting I suspect they’d have trouble monetising like sourceforge clearly did It made it harder to monetize, but it enabled Source Forge to use a huge amount of voluntarily-given bandwidth and saved them a fortune at a time bandwidth was crazy-expensive. Bandwidth costs were one of the reasons something like GitHub didn't appear earlier, and suddenly popped-up a lot of times out of nowhere. reply schacon 2 hours agoparentprevThis is not my recollection, at least at the time. I remember meeting with one of the SourceForge founders and being a little star struck. SourceForge was a huge deal at the time and we totally felt like we were the underdogs in that arena. Perhaps later they got more desperate, but in 2008, SourceForge was the 900lb gorilla. reply AnotherGoodName 2 hours agorootparent2013 is when the binaries had malware included although even in 2008 they were guilty of having 5 download buttons due to excessive and unpoliced inline advertising with only one of those buttons being the holy grail that linked to the download you actually wanted. Choose wisely. reply michaelt 1 hour agorootparentprevTo help our recollections, let's look at Sourceforge's browse page, from back in 2008: https://web.archive.org/web/20081118033645/http://sourceforg... They did indeed host quite a lot of stuff, and it was undeniably popular as a place to get your binaries hosted free of charge. But at the same time, is it being used as a source code repository? A lot of those projects don't show the CVS/SVN features. And sourceforge never hosted the biggest and most established projects, Linux and Gnu and PHP and Java and Qt and Perl and Python were all doing their own thing. And pretty much every project visible on that page had its own separate website, very few projects hosted on sourceforge exclusively. reply relaxing 1 hour agorootparentNo, you’d upload source tarballs. Live public access to VCS wasn’t a thing for most projects. reply giantrobot 8 minutes agorootparentprevSourceForge was the upstream source of truth for a huge percentage of small apps bundled by various distros (and BSD ports etc). Even when the upstream maintainers just uploaded the latest tarball to SF and didn't use their hosted VCS, just the hosting was a major boon to all of the tiny teams and individual maintainers of FOSS projects. reply kstrauser 2 hours agorootparentprevWho is \"we\"? reply schacon 1 hour agorootparentSorry, \"we\" is GitHub. I'm the author of the article and one of the GH cofounders. reply relaxing 1 hour agorootparentWell damn. So much for the “Github had better taste” thesis. Still, Sourceforge was a terrible user experience. Github was a breathe of fresh air. reply kstrauser 1 hour agorootparentprevOh! Heh, that makes sense now. reply zargon 2 hours agoparentprev> your remote GitHub repo can be located on any machine It's such a an easy mistake to say that you did it while explaining you don't need GitHub for git repos. :) reply imiric 2 hours agoprevThe celebrity of Linus definitely helped Git win, and GitHub likely benefited from that by the name alone. Many people today mistakenly equate Git and GitHub, and since GH did such a good job of being a friendly interface to Git, to many people it _is_ Git. They did an early bet on Git alone, at a time when many of its competitors were supporting several VCSs. That early traction set the ball rolling, and now everyone developing in public pretty much has to be on it. Tangentially: it's a pretty sad state of affairs when the most popular OSS hosting service is not only proprietary, but owned by the company who was historically at opposite ends of the OSS movement. A cynic might say that they're at the extend phase of \"embrace, extend, extinguish\". Though \"extinguish\" might not be necessary if it can be replaced by \"profit\" instead. reply schacon 1 hour agoparentI do go into Linux and Linus in the article in some depth, but even Linus credits the Ruby community to a degree with the explosion in popularity of Git, which is fairly clearly due in large part to GitHub. But, it's certainly a chicken/egg question. I would also argue that MS is nothing like the company that it was 30 years ago when that philosophy was a thing. The truth today is the via GitHub, Microsoft hosts the vast majority of the world's open source software, entirely for free. reply nine_k 1 hour agorootparentMS have realized that producing the right kind of important open-source software gives even more strength than producing closed-source software. Hence Typescript, VS Code, a few widespread language servers, etc. reply bluGill 50 minutes agorootparentMS has long known developers were critical to their success. For a while they were worried that projects like Linux would take away their market, but it is now clearer to everyone where linux is going and so they don't have to worry as much. (so long as they are not stupid) reply nine_k 27 minutes agorootparentThey were smart enough to offer MS SQL Server for Linux, and to support (rather than oppose) Mono and Xamarin early enough. reply JyB 1 hour agoparentprevThats actually interesting. Was there any concern at any point in the early days about supporting other VCS or being too focused on git? reply schacon 1 hour agorootparentThere was concern actually. We debated a bit the concept of naming the company \"GitHub\", since \"git\" is baked into the company name. We worried a little about what happens when the next big VCS thing comes along, not knowing that it's going to be dominant for at least the next 20 years. reply sunshowers 3 minutes agoprev> They never cared about the developer workflow. Man, given how terrible GitHub's developer workflow is in 2024... there is still no first-class support for stacked diffs, something that Phabricator had a decade ago and mailing list workflows have been doing for a very long time. I personally treat GH as a system that has to be hacked around with tools like spr [1], not a paragon of good developer workflows. [1] my fork here: https://github.com/sunshowers/spr reply ldayley 1 hour agoprevThank you for sharing this, Scott! He mentions \"Taste\" throughout the post and this intangible quality makes all the difference in an early-stage winner-take-all market dominance race. In 2007 I was teaching myself programming and had just started using my first version control tools with Mercurial/Hg after reading Joel Spolky's blog post/love letter to Mercurial. A year or two later I'd go to user group meetups and hear many echo my praise for Hg but lamenting that all the cool projects were in GitHub (and not bitbucket). One by one nearly everyone migrated their projects over to git almost entirely because of the activity at GitHub. I even taught myself git using Scott's website and book at that point! \"Product-market fit\" is the MBA name for this now. As Scott elegantly states this is mostly knowing what problem you solve, for whom, and great timing, but it was the \"flavor\" of the site and community (combined with the clout of linux/android using git) that probably won the hearts and minds and really made it fit with this new market. Edit: It didn't hurt that this was all happening at the convergence of the transition to cloud computing (particularly Heroku/AWS), \"Web 2.0\"/public APIs, and a millennial generational wave in college/first jobs-- but that kinda gets covered in the \"Timing, plus SourceForge sucked\" points reply bluGill 45 minutes agoparentI still miss hg. I migrated to github years ago because github is a much better workflow, but I miss hg which can answer questions that git cannot. reply nerdix 1 hour agoprevGitHub won because Git won. It was obvious by the late 00s that some DVCS was going to upend subversion (and more niche VCS like TFS). It ended up a two horse race between Git and Mercurial. GitHub bet on Git. Bitbucket bet on Mercurial. Git took the early lead and never looked back. And GitHub's competitors were too slow to embrace Git. So GitHub dominated developer mindshare. It seems strange now but there was a period of time during the late 00s and early 10s when developers were pretty passionate about their choice of DVCS. reply nine_k 1 hour agoparentNot just that. They invented \"pull requests\" and offered (initially minimal) code review tools. This made contributing in the open.much easier, and making small contributions, vastly easier. Something like git had to take over svn / cvs / rcs. It could be Perforce, it could be BitKeeper which apparently pioneered the approach. But it had to be open-source, or at least free. Git won not just because it was technically superior; it also won because it was at the same time free software. reply fweimer 1 hour agorootparentPull requests predate Git. The kernel developers used them in the Bitkeeper days: I exported this a patch and then imported onto a clone of Marcelo's tree, so it appears as a single cset where the changes that got un-done never happened. I've done some sanity tests on it, and will test it some more tomorrow. Take a look at it and let me know if I missed anything. When Andy is happy with it I'll leave it to him to re-issue a pull request from Marcelo. https://lore.kernel.org/linux-acpi/BF1FE1855350A0479097B3A0D... I do not know to what extent Bitkeeper had browser-based workflows. Moving cross-repository merges away from the command line may actually have been innovative, but of course of little interest to kernel developers. reply bluGill 39 minutes agorootparentMercurial also supported pull requests. The unique thing about github was an easy central place to do them from and ensuring they didn't get lost. Once you have a github account you can fork a project make a change and pull request it in a few minutes. emailing a patch isn't hard, but with github you don't have to look up what address to email it to, if you just say open pull requests it typically goes to the right place the first time. reply schacon 1 hour agorootparentprevThat's interesting. I know BK had \"pulls\", but iirc it didn't have a \"request-pull\" command, so clearly the \"pull\" terminology came from BK and the \"request\" part came from how people talked about it in email. I actually just shot a video showing how BitKeeper was used. I'll post that and a blog post on our GitButler blog soon. reply dboreham 1 hour agoparentprevYes, article seems to miss this. I believe (at the time, and still) that git won because the cost to host the server side of it is orders of magnitude lower than the competitors (svn, perforce, etc). All those other revision control systems ended up with a big server cost that couldn't justify a free hosting service. Plus git provided a reasonable (but still not great) solution to \"decentralized development\", which none of the others attempted to do. reply schacon 1 hour agorootparentI'm curious how you come to this conclusion. GitHub has always had fairly insane hosting problem sets. When someone clones the Linux repo, that's like 5G in one go. The full clone issues and the problems of a few edge case repos create sometimes crazy hosting costs and scaling problems. Most centralized systems only have to deal with one working tree or one delta at a time. There is not much that goes over the wire in centralized systems in general, comparatively. reply bluGill 36 minutes agorootparentprevWhy didn't mercurial win then? There were almost a dozen other distributed version control systems built in those early days, most of which I cannot remember but all had the same distributed ideas behind them and should be been as easy to host (some easier). reply max_ 2 hours agoprevThere is no real winners in business. Just people/products that are temporarily on top. SourceForge was probably \"the winner\" for some time. The same will be for GitHub. Someone just needs to build an actual superior product and provide a service that GitHub will not provide. Then build a sufficient audience. One such service is an end to end encrypted Git repo service. Some anarchists I know don't want everyone to know what they are working on. The same goes for algorithmic trading. I need strong guarantees that my code will not be used to train an LLM that will leak my edge. I am shocked a superior Git service to GitHub has not been built. I really liked source hut. But the custodian is abit arrogant (crypto projects for instance are banned) reply kstrauser 2 hours agoparentI wish something like Forgejo/Gitea had federated identities so that I could fork a project on the server you're hosting and submit a PR as easily as I can do that if you're hosting it on GitHub today. Everything you're asking for is available today in self-hosted services. I mean, consider that you don't even need a Git server. You can swap code with your pals via SSH/email/whatever right now, today, without the rest of the world even knowing about it. reply max_ 1 hour agorootparent>Everything you're asking for is available today in self-hosted services There is a reason why people use hosted Git services it's not practical for everyone to \"self host\". We can run a self hosted Signal app for privacy. But it's neither convenient nor practical for everyone. reply kstrauser 1 hour agorootparentThat's true, but if you have unusual requirements that make GitHub impractical, there are other options. Devs can update their origin to point at a shared SSH server and coordinate merges through email or Signal or anything else. I think that's a lot more practical than hoping GitHub adds something like end-to-end encryption, or worrying that they might train their LLMs against private code. reply AnotherGoodName 1 hour agoparentprevFor an end to end encrypted git repo; git remote add origin ssh://user@host/srv/git/example Where the host is simply an ssh server you have access to. Encrypt the servers drive itself however you see fit. This is how git is traditionally used btw. GitHub is a third party to the git ecosystem and really there’s little reason to use it for private repos. Just use ssh for the remote connection. reply Groxx 1 hour agorootparentGenerally people mean \"E2E Encrypted\" as \"the hosting service cannot see it\". Git-over-SSH does not achieve this, it just encrypts in transit. reply crop_rotation 2 hours agoparentprev> One such service is an end to end encrypted Git repo service. Some anarchists I know don't want everyone to know what they are working on. I doubt there is a big enough market of anarchists for Github to even bother worrying. > One such service is an end to end encrypted Git repo service. There are so few people that need this, that they can just use client side tools and store all data that gets to remote servers encrypted reply max_ 1 hour agorootparent>I doubt there is a big enough market of anarchists for Github to even bother worrying. A lot of people writing prorietory code bases would definitely use it. I don't think a founder wants the startup's codebase to leak via an LLM? reply nine_k 54 minutes agorootparentA ton of proprietary code lives on GitHub, on closed paid repos. A lot of people reasonably think that GitHub's security chops are better than theirs. But if you care, there is a whole gamut of on-prem solutions, from running bare cgit to fluff like Gitea and GitLab. Lock up your central repo machine all you want, the code is still checked out to developers' laptops. For more security, don't allow that, and let your devs connect to a server with all necessary tools and access to the code, but without general internet access, for instance. reply duped 33 minutes agorootparentprevI don't think founders care if parts or the entirety of the codebase leaks, it's not that valuable. reply Diti 1 hour agorootparentprevIt’s already feasible with Keybase (although I wouldn’t trust them any more, because of the Zoom debacle). reply Gualdrapo 1 hour agoparentprev> Someone just needs to build an actual superior product and provide a service that [...] will not provide. Then build a sufficient audience. I wish this was true for social media and instant messaging platforms, operating systems... reply faangguyindia 1 hour agoparentprevIf your code does not want edge leak, why is it on GitHub? Who trusts private repo off GitHub? Simply store encrypted files somewhere like Dropbox or cloud storage solutions.(Encrypt before you upload) reply conradkay 1 hour agorootparentPlenty of large companies. The risk is much higher that an individual's computer gets compromised, which often has a lot worse than just source code. reply imiric 1 hour agoparentprevIt's extremely difficult to unseat the leader with a superior product alone. Once sufficient traction is established, people will flock to where everyone else is, further cementing their position. It also requires monumental fumbles by the leader to actively push people away from the platform. Unfortunately for those who don't like GitHub, it's run by a company with limitless resources to pour into it, or to flatout buy out its competition. Microsoft has a lot of experience with this. > I really liked source hut. Sourcehut never has and likely never will be a serious competitor. Its UX and goals are entirely different, and it's directed towards a very niche audience unlike GH. reply SenHeng 1 hour agoprevI used both GitHub and BitBucket during the early days. There was no comparison. GitHub was simply nice to use. The UX was phenomenal for its time and made sense. BitBucket was horrible but my then employer wouldn’t pay for hosting and GitHub didn’t provide free private hosting. One of my biggest gripes was that switching back and forth between code view and editor mode would wipe whatever you had written. So you better had them in separate tabs. Also be sure not to press the backspace key outside a text window. reply tootie 15 minutes agoparentIdk, I loved BitBucket and I loved Mercurial. It was much easier to use and had native JIRA integration. I always thought (and still do) that github looks too cute and not very serious. reply seveibar 2 hours agoprevThis article reinforces a lot of my biases around early bets. Taste is so so important, everyone looks at you weird when you say you're betting on \"niche, tasteful solution\" (git) instead of \"common, gross solution\" (SVN). Github bet on Git and made tasteful choices, and that was a huge propellant for them. I feel the same way about tscircuit (my current startup), it's a weird bet to create circuit boards with web technologies, nobody really does it, but the ergonomics _feel better_, and I just have to trust my taste! reply aidenn0 1 hour agoparentI would argue that hg was more tasetful than git at the time github began. The one thing git had going for it was that the most common operations were absurdly fast from the beginning, while hg took a bit of time to catch up. reply digging 1 hour agoparentprevNot sure what is even meant by \"taste\" here; what I see over and over is that convenience wins, where winning is defined as widespread use. reply rustyminnow 26 minutes agorootparentThe article uses \"taste\" pretty broadly compared to many folks in the comments. First mention is about the site being pretty. But later he says \"We had taste. We cared about the experience\" which more aligns with your perspective of convenience. reply nprateem 1 hour agoparentprevIt's just survivorship bias. If HH hadn't won no one would be trying to reverse justify their success. This bet worked, the mercurial ones didn't. reply tanepiper 1 hour agoprevAround about that time, I was working on a Mercurial frontend https://github.com/tanepiper/hgfront - it was around the time GitHub was starting to pick up, and BitBucket also appeared around then (we spoke to the original developer at the time but nothing came of it). Funnily enough also a Gist-like tool that had inline commenting, forking and formatting (https://github.com/tanepiper/pastemonkey). I always wonder what would have happened if we had a dedicated team to make something of it, but in the end git won over hg anyway so likely a moot point. Edit: there's a low-quality video of the early interface we worked on - https://youtu.be/NARcsoPp4F8 reply schacon 1 hour agoparentFun fact, I (original author), wrote the original version of Gist. That was my first project at GitHub. Gist #1 is my claim to fame: https://gist.github.com/schacon/1 reply teqsun 2 hours agoprevAs a \"younger\" programmer it always shocks me how things like git were only created in 2005. It feels so ubiquitous and the way it functions has the \"feeling\" of something created in the 80s or 90s to me. reply eterm 2 hours agoparentSubversion (svn) was absolutely fine before git. Before that, there was CVS but that really was painful. Svn gets a lot of hate for things it doesn't deserve, even this article talks about \"checking out\" and the difficulty of branching, but that doesn't track with subversion. Branching in subversion was just as easy as in git, it had shallow branches. You could branch largely without overhead, although unlike git it was a server-side operation. ( Imagine it like git branch with auto-push to remote). Most software also automatically checked out files as you modified them, and it was a local oepration, there wasn't any locking or contention on that. It was the older CVS/sourcesafe style version system that those. I still maintain that most workplaces with less than, say, 10 devs, would be better off with subversion rather than git, if not for the fact that most the world now works on git. Subversion solves problems with less mental overhead than git, but it's not worth doing anything non-standard, because everyone now knows git and has learned to put up with the worse developer user experience, to the point where people will argue that git doesn't have bad UX, because they've internalised the pain. Before subversion there was CVS and Visual Source Safe. These are much older. These solved a problem of source control, but were based on the concept of locking and modifying files. You'd \"checkout\" a file, which would lock the file for modification of all other users. It was a bit like using a global locking file repository but with a change history. It was as painful as you might imagine. You'd need to know how to fix the issue where someone would go on holiday having checked out a critical file: https://support.microsoft.com/en-us/topic/5d5fa596-eb9c-d2b5... Or more routinely, you'd get someone angrily asking who had such-and-such file checked out. reply marcosdumay 1 hour agorootparentCVS was absolutely not oriented around locking files. It was about merge and conflict resolution like SVN or Git. VSS was oriented around locking. And also broke all the time. Oh, and also lost data... And oh, it was also the expensive one used by everybody that kept saying \"you get what you pay\". reply fanf2 49 minutes agorootparentprevSubversion didn’t get working merge support until years after git. Like CVS it basically required trunk-based development. Feature branches were not supported. You needed a separate checkout for any work in progress. You could not checkpoint your work with a commit before updating to the latest head. Every update is a forced rebase. It sucked. reply vehemenz 2 hours agoparentprevAs an \"older\" programmer, I feel the opposite. Git became mainstream very recently, though admittedly it's been a good ten years or more. I sometimes think younger programmers' attitudes toward git are borderline cultish—git or GitHub is not required to do programming—it's just another tool. I half expected something would have replaced it by now. reply gmueckl 11 minutes agorootparentI have to agree on the cult aspect. This is unfortunate because better tools exist already today, but lots of people refuse to even entertain that possibility. reply schacon 1 hour agorootparentprevGit's been around for almost 20 years now. I would say fairly dominant for 15 or so. reply keybored 30 minutes agorootparentprevGit is overrated for a DVCS. But it’s not overrated considering the old-school competition like SVN. The assumptions of SVN makes it feel like a dinosaur now. reply Bjorkbat 1 hour agoprevThe idea of Github having a unique \"taste\" advantage resonates with me a lot. I don't like the fact that Github is using my code to feed Microsoft's AI ambitions, but I dislike Bitbucket and Gitlab more simply on the grounds that they \"don't look fun\". It's tricky, because any serious Github competitor would implicitly have to compete by attracting the deep pockets of enterprise clients, who care little for \"fun\". Getting revenue from solo devs / small teams is an uphill battle, especially if you feel obliged to make your platform open source. Still, I wish someone would make a Github competitor that's fun and social. reply bluGill 29 minutes agoparentYou don't need enterprise clients. Projects like KDE self host and are enough to keep you around and getting new features if you can get them on board. Plus enterprises often look at their bottom line and ask if something else is a better value so if you are \"free\" some of them will switch to you. reply wood-porch 37 minutes agoparentprevThis. GitHub is a joy to use compared to its competitors. Using bitbucket at work is frustrating, and reminds me of a lot of Microsoft web interfaces, ironic, given that it’s GitHub and not Bitbucket that is owned by them now reply darby_nine 1 hour agoparentprevsourcehut is always worth a mention, though I have never used it in a collaborative environment. reply simonw 2 hours agoprevI clicked on this link thinking \"timing and product quality\", so I was satisfied to see that GitHub co-founder Scott Chacon credits it to \"GitHub started at the right time\" and \"GitHub had good taste\". reply SoftTalker 2 hours agoparentgit won because linux used git, and the vast majority of open-source code was written for linux. Simple as that. GitHub won because it made remote collaboration on a code base easier than anything else. reply bluGill 32 minutes agorootparentI think that if github hadn't come out something other than git would have one. While git did have Linus behind it, the others were objectively better in some way and working on the areas they were objectively worse, and eventually the advantages would have got everyone to switch. However the others never had anything like github - even 20 years latter they still aren't trying (rumor is they are not dead) reply gsliepen 1 hour agoprevAnother big advantage of Git for sites like GitHub is that you are never putting your eggs into one basket. You have your local copy of all history in a project. GitHub is merely a mirror. Sure, some features have been sprinkled on top like pull requests and an issue tracker, but those are not the most critical part. If GitHub goes down you can move your whole Git history to another site like GitLab, sourcehut, or just self-host it, or you can even start doing it right now with minimal effort. This was never the case with CVS and Subversion. reply physicsguy 1 hour agoprevSourceforge was horrible to use. GitHub was widely used but it only really reached proper dominance I think when it started offering free closed source repositories to people that weren't paying them, which was what, 2014/2015 or so? Until then it was pretty common in my experience for people to use BitBucket for Git for private stuff. reply amtamt 1 hour agoprevvi: 1976 GNU Emacs : 1984 BIND: 1986 are (along with too many other projects) from way before Nov 1993, where \"The Total Growth of Open Source\" graph starts from 0. reply schacon 1 hour agoparentIt all depends on how you're counting. For one, \"open source\" was not a phrase before 1998, so there is some retrofitting of Free Software projects. But also, there isn't a registry, it's rather difficult to be more than approximate with this. The article is very specific about their methodology, I'm only using one graph as a general example. reply throwaway5752 2 hours agoprevI professionally used RCS, CVS, Subversion and Perforce before Git came along. Hell, I was actually in a company that FTP'd it's PHP files directly to the production server. People in the field less than 20 years might not appreciate the magnitude of this change (though, adding my two cents to the author's article, branching in p4 was fine). People may have also dealt with ClearCase (vobs!) or Microsoft Visual SourceSafe. Git did as much for software development velocity as any other development in recent history. reply kstrauser 2 hours agoparentThat's all true for me, too, although I hadn't used p4. I resisted Git for a little while because I didn't see the massive appeal of a distributed system in an office with a central server. CVS... worked. SVN was a much more pleasant \"faster horse\". And then I made myself try Git for a week to see the fuss was all about and my eyes were opened. Git is not perfect. There are other products that did/do some things better, or at least more conveniently. But Git was miles ahead of anything else at the time that I could use for free, and after I tasted it, I never wanted to go back to anything else. reply throwaway5752 1 hour agorootparentI was a late adopter, also, and git is definitely not perfect. Mercurial did some things better, and at the time, notably, the forest extension. Git's flexibility is a two edged sword and the history rewrite footguns should be harder to use. Git does comes close enough to solving a fundamental problem it will be very, very durable, though. As long as it is used for linux kernel development I expect it continue to be the dominant dvcs. reply physicsguy 58 minutes agoparentprevGod I hated ClearCase, did a migration from it in 2016(!) for a project that had been around since the late 80s. People were really resistant to moving but once it was done were like \"Oh wow, it's really fast to create a branch, this means we don't have to have one branch for three months!\" reply keybored 36 minutes agoprev> Why GitHub Actually Won > How GitHub _actually_ became the dominant force it is today, from one of it's cofounders. > Being at the very center of phenomena like this can certainly leave you with blind spots, but unlike these youngsters, I was actually there. Hell, I wrote the book. Downvote all you want for being “non-substantive” but for some reason I can’t voluntarily tolerate such a density of well-actually phrasing. It’s grating. It also seems to be everywhere these days but maybe I’m too attuned to it. reply LtWorf 2 hours agoprevGithub won because sourceforge was ruined already. reply devnull3 2 hours agoprevThe rise of github also coincided with enshitification of sourceforge.net. SF although was not git based at that time but it had the mindshare of lot of open source projects and it went complete downhill. So, a downfall of a potential alternative was also a factor IMO. Edit: after I commented I realized that SF was already mentioned in other comment reply schacon 2 hours agoparentI would argue that SF was always pretty shitty, because it focused entirely on advertising. I remember Chris giving a talk comparing the signup process of GitHub and SourceForge. SF had like 8 fields and GitHub had 2. This was because SF wanted to know ad demographic info - where did you hear about us, etc. GitHub just wanted a name and a password. But this was the difference in everything - SF cared about advertisers, not developers. GitHub never thought about what anyone other than the developers using the product wanted. reply devnull3 1 hour agorootparentAgree but my point is when you see a new and better rival then instead of pivoting SF became even worse and became malware-ised. Also SF was based on SVN. They failed to understand and capitalize on a better tech on the market i.e. git. reply schacon 1 hour agorootparentThey actually did so very early. In early 2009 they added Git, Hg and Bzr support: https://arstechnica.com/information-technology/2009/03/sourc... That's less than a year after GitHub launched and was still very small. reply devnull3 33 minutes agorootparentI stand corrected! Thanks! reply hadlock 1 hour agoparentprevSourceforge was always awful to navigate. Because it was dependent on ad revenue, not subscriptions. It was trying to compete with consumer focused things like download.com (remember that ?) where the end user just wanted a tarball of the executable, and the host was trying to make money selling ad space on the page where the download link was. The fact that end users could peek at the folder structure of the source code was a novelty at best reply sergiotapia 2 hours agoprevI still miss Codeplex from microsoft ;) it was a really beautiful website reply jarule 41 minutes agoprevWhy GitHub was started. To ease ass pain. reply chx 1 hour agoprevgit won because of empty hype, bzr was far superior in basically every aspect. Much easier to program with either for plugins or to be embedded, much saner \"hide your development commits\" model with log levels, much saner command line interface. It's just better. It's not the first thing to be carried by hype instead of careful comparison. reply kstrauser 1 hour agoparentThat's simply untrue. Bzr was dog slow on repos with lots of history. It had lots of early users and support from hosting services like Launchpad, Savannah, and SourceForge. I'm certain that everyone didn't migrate to git because of hype. I mean, it's not credible to say the Emacs team stopped using it because it wasn't fashionable. There were lots of DVCS projects at the time, like arch, darcs, and dcvs. People were running all kinds of experiments to explore the Cambrian explosion of new ideas. Some of them did some things better than git, but git handled most of those things reasonably well and it was fast. We all mostly ended up on git because it was generally the better option. It earned the hype, but the hype followed the adoption, not vice versa. reply chx 45 minutes agorootparentSo in exchange for a little speed we are stuck with one of the most user hostile tools out there. That's not the deal I would have wanted to make. The interface is atrocious as some switches change completely what the command does -- this was partially acknowledged and fixed in git switch but there's so much more, it loses work way too easily and some of the concepts are near impossible to grok. (I did learn git eventually but that doesn't mean I like it. It's more of an uneasy truce than a friendship.) reply kstrauser 31 minutes agorootparentIt wasn't a little speed. Other options were many times faster. I just renamed a large subdir in a large project. `time git status` took 41ms. That kind of speed lets you add all sorts of interactively that would be impractical if it were slower. For instance, my shell prompt shows whether the current directory is managed by Git, and if so, whether the status is clean. I would never tolerate my terminal being detectably slowed by such a thing. With git, it's not. There are a thousand little ways where having tooling be fast enough is make-or-break: if it's not, people don't use it. Git is fast enough for all the most common operations. Other options were not. reply schacon 1 hour agoparentprevI think PR and network effects of GitHub definitely played a role in the success of Git over other options like bzr, but you should also remember that bzr had tons of issues. It was slower, there was no index/staging area, there was no rebasing, etc. Mercurial was very good too, but while there were pluses and minuses with all of them, I think there was a _lot_ of careful comparison too. None of them were clearly and in all aspects better than the others. reply ranger_danger 1 hour agoprevhttps://sfconservancy.org/GiveUpGitHub/ reply schacon 1 hour agoparentThis is one of the least coherent and most ridiculous articles ever written. The SF Conservancy people are ludicrous. It's really too bad that they don't have anything better to do. reply jordigh 1 hour agorootparentYou only feel this way because it's written about something you worked on. * Co-Pilot is trained on copyrighted code without attribution: ludicrous? * Github works with ICE: incoherent? * All of Github's hosting code is proprietary and secret: ridiculous? * Github tries to discredit copyleft: hyperbolic? * Github is wholly owned by Microsoft who also doesn't like copyleft: conjecture? As to SFC not having anything better to do... this is exactly what their mandate says they should do. This is what they collect money to do: to point people towards free software. If you want SFC to find something better to do, I assume you want them to completely shut down their organisation and stop complaining about undermining copyleft and stop complaining about non-free code. reply schacon 40 minutes agorootparentI don't particularly want to engage, but why not? * Co-Pilot is trained on copyrighted code without attribution: ludicrous? This is debatable, but yes, it's a ridiculous reason to not use github. If your code is on the internet, which it is with every other alternative host they mention, it will be crawled and used for training, just as it will be read by humans and learned from. We can have a fair use debate, but GitHub is not alone in this stance or problem set. * Github works with ICE: incoherent? No, I would file this under ridiculous. GitHub is a government contractor. It licenses it's software to lots of organizations. People can disagree with a lot of them, but trying to manage that and dictate changing political morality at a company level is insane. All of these other solutions are certainly used by organizations that are a lot more controversial than a major federal institution, as much as I may even personally disagree with policies under certain administrations. But again, singling out GitHub is just finding some reason to be mad, it's not GitHub specific. * All of Github's hosting code is proprietary and secret: ridiculous? The computers that you're writing your FOSS code on, that indeed they wrote this article on, have proprietary chips, have software you can't access. You think everyone at SFC uses a Stallman-esque laptop? They're probably happily typing away on their Macbooks, full of non-FOSS software. The software community is an ecosystem of lots of models, nobody is all-FOSS, it's not possible. GitHub has open sourced Electron, libgit2, a thousand other things. Core code was never helpful to anyone and would not be helpful today. Git isn't a lock-in proprietary thing, you can always easily transfer your code elsewhere. * Github tries to discredit copyleft: hyperbolic? It doesn't try to discredit copyleft as a corporate stance. Several individuals have pointed out weaknesses in copyleft vs permissive OSS licenses. But it's never that you should use closed instead of copyleft or something, it's always something like if you're using copyleft, it's generally better for the community to use MIT or Apache or something more permissive that doesn't need to involve lawyers and gives you more freedom. * Github is wholly owned by Microsoft who also doesn't like copyleft: conjecture? This is a 30 year old take on what Microsoft cares about. Microsoft probably never thinks about copyleft these days, any more than the rest of us do. MS contributes to Linux, contributes to Git, both GPL projects. Almost certainly they contribute more to GPL projects globally than you or I or the SFC do. > As to SFC not having anything better to do... this is exactly what their mandate says they should do My point was that they can actually help the Git project by sponsoring meetings, educating people in a useful way, etc. I have helped donate a fair amount to the SFC through GitHub events and was hoping the money would be better spent on actual community building and project fostering rather than cheap think-pieces like this. reply ranger_danger 1 hour agorootparentprevI could be wrong but I don't think hyperbolic conjecture is going to swing anyone the other direction. reply schacon 1 hour agorootparentWhich is ironic, because that entire article is hyperbolic conjecture. reply transpute 1 hour agoprev [–] > we won because we started at the right time and we had taste. 2012, https://a16z.com/announcement/github/ We just invested $100M in GitHub. In addition to the eye-popping number, the investment breaks ground on two fronts: It’s the largest investment we’ve ever made. It’s the only outside investment GitHub has ever taken. 2018, https://web.archive.org/web/20180604134945/https://a16z.com/... Six years ago we invested an “eye-popping” $100 million into GitHub. This was not only a Series A investment and the first institutional money ever raised by the company, but it was also the largest single check we had ever written.. At the time, it had over 3 million Git repositories — a nearly invincible position.. if I ever have to choose between a group of professional business managers or a talented group of passionate developers with amazing product-market fit like GitHub, I am investing in GitHub every time. reply schacon 1 hour agoparent [–] What is the point you're trying to make here? reply transpute 1 hour agorootparent [–] Did $100M investment help Github to win, or had Github already won in 2012 with profitability and 3M git repos? reply schacon 1 hour agorootparent [–] I would argue that GitHub already won in 2012. The investment helped us grow in a different way, but I don't think anyone involved in that deal would have said that we had almost any serious competitive threats at the time, which is to some degree why it was such a great deal. reply transpute 1 hour agorootparent [–] Did the investment encourage corporate buyers to sign up for Github Enterprise, where corp developers were already using the free product? reply schacon 1 hour agorootparent [–] That was certainly one of our internal arguments, that the institutional investment would be helpful for large company trust. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GitHub's success is attributed to its timely launch during the rise of distributed version control tools and its focus on enhancing the developer experience over commercial interests.",
      "GitHub's user-centric approach, emphasis on pull requests, and aesthetic appeal distinguished it from competitors like Google Code and BitBucket.",
      "The adoption of Git by the Linux community and early support from the Ruby community were crucial in popularizing GitHub."
    ],
    "commentSummary": [
      "Google Code was created to prevent SourceForge from monopolizing the market, not to dominate it.",
      "After achieving its goal of fostering competition, Google Code was shut down, paving the way for platforms like GitHub to flourish.",
      "The discussion underscores concerns about the longevity of Google's products and the importance of timing and user experience in GitHub's success."
    ],
    "points": 200,
    "commentCount": 197,
    "retryCount": 0,
    "time": 1725899249
  },
  {
    "id": 41484337,
    "title": "ATProto for distributed system engineers",
    "originLink": "https://atproto.com/articles/atproto-for-distsys-engineers",
    "originBody": "ATProto for distributed systems engineers Sep 3, 2024 AT Protocol is the tech developed at Bluesky for open social networking. In this article we're going to explore AT Proto from the perspective of distributed backend engineering. If you've ever built a backend with stream-processing, then you're familiar with the kind of systems we'll be exploring. If you're not — no worries! We'll step through it. Scaling the traditional Web backend The classic, happy Web architecture is the “one big SQL database” behind our app server. The app talks to the database and handles requests from the frontend. As our application grows, we hit some performance limits so we toss some caches into the stack. Then let's say we scale our database horizontally through sharding and replicas. This is pretty good, but we're building a social network with hundreds of millions of users; even this model hits limits. The problem is that our SQL database is “strongly consistent” which means the state is kept uniformly in sync across the system. Maintaining strong consistency incurs a performance cost which becomes our bottleneck. If we can relax our system to use “eventual consistency,” we can scale much further. We start by switching to a NoSQL cluster. This is better for scaling, but without SQL it's becoming harder to build our queries. It turns out that SQL databases have a lot of useful features, like JOIN and aggregation queries. In fact, our NoSQL database is really just a key-value store. Writing features is becoming a pain! To fix this, we need to write programs which generate precomputed views of our dataset. These views are essentially like cached queries. We even duplicate the canonical data into these views so they're very fast. We'll call these our View servers. Now we notice that keeping our view servers synced with the canonical data in the NoSQL cluster is tricky. Sometimes our view servers crash and miss updates. We need to make sure that our views stay reliably up-to-date. To solve this, we introduce an event log (such as Kafka). That log records and broadcasts all the changes to the NoSQL cluster. Our view servers listen to — and replay — that log to ensure they never miss an update, even when they need to restart. We've now arrived at a stream processing architecture, and while there are a lot more details we could cover, this is enough for now. The good news is that this architecture scales pretty well. We've given up strong consistency and sometimes our read queries lag behind the most up to date version of the data, but the service doesn't drop writes or enter an incorrect state. In a way, what we've done is custom-built a database by turning it inside-out. We simplified the canonical storage into a NoSQL cluster, and then built our own querying engine with the view servers. It's a lot less convenient to build with, but it scales. Decentralizing our high-scale backend The goal of AT Protocol is to interconnect applications so that their backends share state, including user accounts and content. How can we do that? If we look at our diagram, we can see that most of the system is isolated from the outside world, with only the App server providing a public interface. Our goal is to break this isolation down so that other people can join our NoSQL cluster, our event log, our view servers, and so on. Here's how it's going to look: Each of these internal services are now external services. They have public APIs which anybody can consume. On top of that, anybody can create their own instances of these services. Our goal is to make it so anybody can contribute to this decentralized backend. That means that we don't just want one NoSQL cluster, or one View server. We want lots of these servers working together. So really it's more like this: So how do we make all of these services work together? Unifying the data model We're going to establish a shared data model called the “user data repository.” Every data repository contains JSON documents, which we'll call “records”. For organizational purposes, we'll bucket these records into “collections.” Now we're going to opinionate our NoSQL services so they all use this data repository model. Remember: the data repo services are still basically NoSQL stores, it's just that they're now organized in a very specific way: Each user has a data repository. Each repository has collections. Each collection is an ordered K/V store of JSON documents. Since the data repositories can be hosted by anybody, we need to give them URLs. While we're at it, let's create a whole URL scheme for our records too. Great! Also, since we're going to be syncing these records around the Internet, it would be a good idea to cryptographically sign them so that we know they're authentic. Charting the flow of data Now that we've set up our high-scale decentralized backend, let's map out how an application actually works on ATProto. Since we're making a new app, we're going to want two things: an app server (which hosts our API & frontend) and a view server (which collects data from the network for us). We often bundle the app & view servers, and so we can just call it an “Appview.” Let's start there: A user logs into our app using OAuth. In the process, they tell us which server hosts their data repository, and they give us permission to read and write to it. We're off to a good start — we can read and write JSON documents in the user's repo. If they already have data from other apps (like a profile) we can read that data too. If we were building a singleplayer app, we'd already be done. But let's chart what happens when we write a JSON document. This commits the document to the repo, then fires off a write into the event logs which are listening to the repo. From there, the event gets sent to any view services that are listening — including our own! Why are we listening to the event stream if we're the one making the write? Because we're not the only ones making writes! There are lots of user repos generating events, and lots of apps writing to them! So we can see a kind of circular data flow throughout our decentralized backend, with writes being committed to the data repos, then emitted through the event logs into the view servers, where they can be read by our applications. And (one hopes) that this network continues to scale: not just to add capacity, but to create a wider variety of applications sharing in this open applications network. Building practical open systems The AT Protocol merges p2p tech with high-scale systems practices. Our founding engineers were core IPFS and Dat engineers, and Martin Kleppmann — the author of Data Intensive Applications — is an active technical advisor. Before Bluesky was started, we established a clear requirement of “no steps backwards.” We wanted the network to feel as convenient and global as every social app before it, while still working as an open network. This is why, when we looked at federation and blockchains, the scaling limits of those architectures stood out to us. Our solution was to take standard practices for high scale backends, and then apply the techniques we used in peer-to-peer systems to create an open network. Ready to learn more? Specs, guides, and SDKs can be found here.",
    "commentLink": "https://news.ycombinator.com/item?id=41484337",
    "commentBody": "ATProto for distributed system engineers (atproto.com)191 points by danabramov 18 hours agohidepastfavorite57 comments openrisk 11 hours agoWhat might be useful for the re-decentralized web community is a detailed comparison of the ATProto, ActivityStreams/Pub and maybe Solid specifications, protocols, standards, vocabularies (or whatever exactly these blueprints actually are). As the blog post illustrates quite nicely (literally), ATProto is a fairly complete, bottom-up type specification that makes concrete various server/database aspects that in the ActivityPub spec are somewhere in the remote background, \"left to the implementation\". One could almost think of implementing AP over ATProto, and sure enough somebody wrote about this [1]. One can also not miss the (at least) linguistic affinity of a Personal Data Server with a Personal Data Store (Solid) and sure enough somebody did and asked [2]. [1] https://berjon.com/ap-at/ [2] https://www.reddit.com/r/BlueskySocial/comments/ywrw3f/whats... reply apitman 14 hours agoprevHas anyone played around with ATProto yet? ActivityPub is pretty easy to get started with, especially if you just ignore JSON-LD and parse what you see. I'm curious how ATProto compares. reply Diti 9 hours agoparentAs an ontology enthusiast, it saddens me to see that ATProto went for their own data model ([link:Lexicon]) instead of using the standard JSON-LD (I wonder if they considered Turtle – which is streamable, unlike JSON). I get why they did that (graph data is, uh, particular to work with, especially for newcomers who only know JSON), but ATProto not using JSON-LD is actually what made me unwilling to tinker with the protocol. Not a direct answer to your question, sorry. Mostly a rant. [link:Lexicon]: https://atproto.com/guides/faq#why-create-lexicon-instead-of... reply str4d 8 hours agorootparentThere are a few more details about the reason they didn't use JSON-LD in Paul's blog post [0]. [0]: https://www.pfrazee.com/blog/why-not-rdf reply apitman 3 hours agorootparentprevI'm sorry but JSON-LD is a massive pain to work with in statically typed languages. Certainly is in Go at least. The flexibility is the problem, ie you never know if something is going to be an object or an IRI (did we really need a 3rd name for URIs?) to an object. I think you could get most of the benefit while still requiring specific types. reply Diti 2 hours agorootparentYou shouldn’t be having this problem if you use a library which offers normalization (like github.com:piprate/json-gold) so that you get objects when there’s an IRI context, and a simple string when there’s a regular IRI. reply apitman 26 minutes agorootparentI'm not aware of any such libary for Go. Besides, I prefer protocols that are simple enough to implement myself. That's not feasible in every case, but it certainly is for the social media use case. reply danabramov 7 hours agoparentprevWe've just released a new short guide on creating a minimal app on atproto, together with a GitHub example project: - https://atproto.com/guides/applications - https://github.com/bluesky-social/statusphere-example-app reply viksit 6 hours agoparentprevyes we built a 10k user social network for artists and musicians on it and it’s excellent. very sophisticated and very extensible. reply danabramov 6 hours agorootparentMight sharing a link? reply Diti 6 hours agorootparentJudging by this user’s comment history, the website seems to be solarplex.xyz (be advised, it takes between 30 seconds and 1 minute to fully load the website’s 75 MB). reply apitman 3 hours agorootparentOT but out of 572 requests, half of them are OPTIONS. CORS is an abomination. reply Kudos 10 hours agoparentprevI only know of this blog implementation https://github.com/whtwnd/whitewind-blog reply str4d 8 hours agorootparentFor non-Bluesky apps built in ATProto, in addition to White Wind (blogging), there is also Smoke Signal (events, only Lexicons are open source currently AFAICT) [0], and Frontpage (link aggregation) [1]. [0]: https://github.com/SmokeSignal-Events/lexicon [1]: https://github.com/likeandscribe/unravel/tree/main/packages/... reply danabramov 7 hours agorootparentAlso, our new little example app: - https://atproto.com/guides/applications (guide) - https://github.com/bluesky-social/statusphere-example-app (GitHub) reply FroshKiller 6 hours agoparentprevI built a custom feed server for Bluesky that drinks from the firehose. Getting everything working was very fiddly. For a hobby, the friction of it outweighed the entertainment value for me. Working with the firehose probably isn't feasible for a lot of people who'd like to tinker. There doesn't seem to be any way of subscribing to only certain types of events. reply str4d 5 hours agorootparentFor a lower-friction firehose experience, you can use Jetstream [0] (by one of the Bluesky devs) which supports subscribing to specific Collection NSIDs and user repositories, and converts records to JSON for you. There's a public instance URL in the README (with bandwidth limits), or you can self-host. [0] https://github.com/ericvolp12/jetstream reply FroshKiller 42 minutes agorootparentThe firehose itself isn't really the fiddly part since it's just a WebSocket connection. Setting up the feed server, publishing the DID for its web host, then publishing the feed generator to the network were all kind of a low-grade hassle that killed a lot of my enthusiasm. Like none of it was especially complicated if you're doing it for a professional project or whatever, but I was just trying to goof around while watching episodes of Highlander: The Series, and it was taking me away from Duncan. I'll check out this Jetstream project for sure, though. reply __loam 12 hours agoparentprevThere's a lot fewer resources for AT than ActivityPub. Last time I checked which was a few months ago, the official documentation for AT was pretty sparse if you're interested in building to a spec. You'll find a lot more in the ActivityPub specs, plus a lot of open implementations and helpful guides. reply m_eiman 12 hours agorootparent> You'll find a lot more in the ActivityPub specs, plus a lot of open implementations and helpful guides. I've read that there's a problem with interacting with Mastodon if you only rely on the protocol specs, that they do things their own way and have different requirements than the official specs. Is this still a problem? If it is, are Mastodon moving to be more closely aligned with the spec, or to doing more of their own thing? reply zimpenfish 11 hours agorootparentFrom what I've seen, Mastodon sticks to the spec but a lot of clients and servers then stick to Mastodon's interpretation of the spec rather than the spec. e.g. for status IDs, the spec says \"String (cast from an integer but not guaranteed to be a number)\", Mastodon uses numerical IDs, some clients[1] see this as \"Ah, IDs are numbers!\" and break horribly when they're not numerically parseable (Akkoma, Pleroma, GotoSocial...) (IIRC there was another thing where `created_at` is described as \"The date when this status was created\" but the type is given as \"String (ISO 8601 Datetime)\" which led some code to crash when Mastodon started outputting just dates instead of datetimes.) [1] Including some from people who Really Should Know Better. reply vidarh 10 hours agorootparentI like ActivityPub overall, but there are a lot of places where the spec is just too complex, and I suspect that contributed to a lot of the choices to implement whatever currently works with Mastodon instead of the spec. I'm currently implementing parts of the spec, and there are parts (like fully handling context correctly) that feels like far more pain than it is worth vs. just handling occasional breakage. It feels like a very ivory tower spec of the kind you wouldn't be likely to write if you built a complete reference implementation first. But it's very on-brand as a W3C spec. I'd love to see a revision that deprecates and simplifies a whole lot of things. reply rapnie 10 hours agorootparent> I'd love to see a revision that deprecates and simplifies a whole lot of things. The hidden complexities in AP have led to several efforts. In the past there has been LitePub [0]. A recent project is Versia [1]. And who knows there may be a FeatherPub [2] one day. If anyone knows of other attempts I'd like to hear. [0] https://litepub.social/ [1] https://versia.pub/ [2] https://docs.google.com/document/d/13LuB6Z-C_drCLCEuCtNApX98... reply apitman 3 hours agorootparentAh, FeatherPub is in a google doc. That explains why I was having trouble googling it last night. Also, there's a conversation happening about Versia today: https://social.coop/@smallcircles/113105954469059880 reply rapnie 54 minutes agorootparentYes, there's a related google doc delving into the data model: https://docs.google.com/document/d/13mtl9gFmcuL-0MS-Boaeh3i6... reply vidarh 9 hours agorootparentprevThanks. I remember looking at Litepub. Not aware of the other two. The FeatherPub document feels like by far the most useful. But I also think just going through the spec with a red marker would be a useful exercise and maybe I will one day. In the sense that there are a whole lot of features nobody does anything useful with. E.g. \"@context\" in theory provides a whole lot of ways to type the rest of the data. I'd be willing to bet that you'd break a whole lot of software if you served up a \"@context\" for an actor that mapped common field-names in use by Mastodon to a different namespace and mapped the Mastodon features to different names... In theory it's great. In practice, I suspect we have XML namespaces and people stupidly hardcoding prefixes all over again... reply brianolson 8 hours agorootparentprevOP is a link to the atproto site because it got a major new revision within the last week reply swyx 10 hours agoprevalways enjoy your writing, Dan. at:// seems like its close enough to DNS to warrant just using DNS. why not? (im sure theres a good reason so just asking) reply danabramov 7 hours agoparentOh this is not mine actually — Paul wrote this one :) atproto does use DNS under the hood for domain verification but atproto itself is a bit higher-level. It builds on top of DNS, HTTP, JSON, web sockets, and a few other specs. reply danabramov 6 hours agorootparentIf you’re specifically asking why the identity system is not “rooted” in DNS (i.e. why at://danabra.mov resolves to another host than my website) — it’s because we want users to be able to change their hosting over time without breaking links between records. The actual identity system is “rooted” in a stable identifier (which is a hash of the first version of your identity record). That’s your global immutable ID in the entire network. The identity record for your ID specifies your current public key, your current domain name (which acts as a human-readable handle), and your current host (which actually contains your data). This extra level of indirection ensures you’re always able to change your user-readable handle (eg if you get a new domain or your domain expires etc), and that you’re always able to change your host (eg if your host goes down or you don’t like its services or you want to host data yourself). The key piece allowing this is the identity registry of course. Think of it similar to npm registry. We run a centralized one, but all records are signed so you can always recursively verify that we haven’t tampered with any of the records. This layer is already very thin but in longer term we’d like to move this layer outside the company to be governed independently, similar to ICANN. reply str4d 5 hours agorootparentAdditionally, a user _can_ root their identity in DNS if they want, by using did:web instead of did:plc [0]. The main Bluesky client doesn't expose this (presumably because did:web cannot provide a mechanism for automatic migration between PDSs (due to the PDS having no control over the DID document) or recovering from loss of control of the domain name, so it requires more technical expertise), but there are users successfully using this method for their identities. [0] https://atproto.com/specs/did#blessed-did-methods reply oDot 7 hours agoprevIt didn't sink in yet that the killer app for ATProto is not Twitter, but YouTube. If anyone is interested in exploring this, atproto [does this fool ai bots?] weedonandscott [I hope it does] com reply Matl 6 hours agoparentActivityPub does have https://joinpeertube.org for what it's worth. What would ATProto bring in specifically? Is it the ease of migration? reply purlane 4 hours agorootparentThe biggest upside compared to PeerTube is probably discoverability. In ActivityPub, the network architecture means the video ecosystem is fractured and there’s no one cohesive place to find all PeerTube videos. In atproto, the network is continually indexed by relays, which means that it doesn’t make a difference what app you use to watch videos - you’ll find the exact same ones regardless of the platform, since they’re all working from the same data. This also means that different video platforms can provide different services for users without locking in users to their platform. Platforms would be forced to compete on what they provide to the user experience, not how well they can lock in users to their platform. reply oDot 2 hours agorootparentExactly right. Watch apps will compete on consumer-facing features like the recommendation algorithm -- maybe they'll offer several, or just one that differentiates them. Hosting providers will compete on producer-facing features, like advertising, content policies, analytics, etc. If a user is displeased with either, they can take all of their content/activity history and leave. reply sebstefan 8 hours agoprevSo the user's repo is decentralized but the event-log services and view services are centrally managed by bluesky? reply danabramov 7 hours agoparentThere's a few layers to the system. - Identity layer: This is where your identity information (public key, current domain handle, current user repo host) is stored, essentially as a piece of JSON. You can think of it as similar to npm registry where each record is self-verifiable (you can verify we haven’t tampered with it). This layer is very thin. It is currently centrally managed by Bluesky but in the longer term we intend to upstream it into neutral governance outside of the company — potentially similar to ICANN. - User repo hosting: We provide user hosting as a service for people who sign up to Bluesky (and choose the default option) but you can run your own too. The server itself is open source (we publish both TypeScript source code and a Docker container to run it). We also publish a spec so you can implement it from scratch if you'd like it — essentially, it needs to be able to enumerate records and to provide a WebSocket to listen to their updates. I'd say this layer is already decentralized because anyone can participate in it and run their own server. - Relay: As an optimization (you don't want your app backend to listen to websockets for every single user repo in the system), we run a node that aggregates and caches the entire known network. That node's called a Relay. It's an optimization and not strictly necessary to the protocol. It's open source. We run the only actively used relay at the moment, but there's nothing stopping you from running your own (at the current usage rate, ingesting all content on the network into your own relay would cost you ~$150/mo). If atproto gains adoption, we expect some major stakeholders to run their own relays for different purposes — big tech companies might want to run them to ensure infra independency, governments might want to run them if they have significantly different opinions on what type of content is acceptable on the entire network, and so on. - Application backends (view services): These are just normal web app backends so they're decentralized in the same way the web is decentralized. Bluesky's backend is managed by Bluesky, but your own app's backend will be managed by you. You can also create a backend that ingests Bluesky's atproto data (which is kind of the point of atproto). That would let you create complementary or competing products using the same identity system and information already on the network. Hope this helps! reply GaryNumanVevo 7 hours agoparentprevYes and no. The relay is run by Bluesky, but only as a matter of practicality because it requires a large footprint to subscribe to all the PDS events. Others have written custom AppViews and clients already. I run a \"one man relay\" that only scrapes my PDS, puts it into an appview (which doesn't do much) and I can see that on a basic client that I wrote. The whitepaper clarifies a lot of this: https://arxiv.org/abs/2402.03239 reply omnicarinha 15 hours agoprevOne thing I still didn't quite grasp with BlueSky yet is if it's a decentralized platform or not... ATProto seems technically capable of supporting decentralized platforms. reply jazzyjackson 14 hours agoparentBsky wants to be one entry point into a decentralized network but there's little incentive to spin up your own Personal Data Server (since you're still subject to the moderation of the one front end everyone uses (AppView in ATProto parlance)) and still less incentive to host your own front end since you'd just be burdening yourself with all the same moderation problems bsky is trying to stay on top of IMO the devs have been so overburdened with trying to nail moderation that they're actually disincentivized from onboarding new populations, since multiple entry points to the network just lands in their lap as more difficult moderation problems - that is, they're still figuring out how to moderate people on their own servers and haven't yet decided how they're going to moderate with a federation of servers with different cultures than their own I don't think they're avoiding the big problems, but it does seem like they're taking the slow careful route, maybe this is for the best. reply shafyy 11 hours agoparentprevIn theory it is decentralized. But if you compare it to Mastodon for example, it's pretty centralized in practice. I haven't come across any people running their larger own servers, like I do on Mastodon. reply danabramov 6 hours agorootparentNote the shape of decentralization is very different from Mastodon — there's no concept of \"running a Bluesky instance\". What you can run is a personal server to host your data (which would work for any atproto apps, not just Bluesky). The Bluesky web app (which is ran by Bluesky) would aggregate data from your server (and all other servers on the network). Unlike Mastodon, you don't have people running copies of the Bluesky app because it is simply unnecessary — each copy would \"see\" the same network. If you wanted to fork the Bluesky product (e.g. different branding, different moderation decisions, different product decisions) then yes, you'd run your own product on your own backend and it would be able to ingest Bluesky app data (and vice versa, the Bluesky app would be able to ingest the data from your product). reply shafyy 4 hours agorootparentSure, but I still haven't seen people really doing that. reply danabramov 2 hours agorootparentWhich part are you referring to by “that”? There’s definitely people self-hosting their data (not a lot cause the process is pretty technical and manual atm). Note that you can always move hosts (without asking permission from the previous host) so you can start using the Bluesky hosting and then switch it. reply wmf 14 hours agoparentprevIt's pretty decentralized. You can run your own PDS, relay, and appview (some of these are more expensive than others). I'm not sure if you can configure the official clients to use an alternate server. reply nunobrito 9 hours agorootparentDecentralization on this social network context means to have users accessing data from other users even when other third-parties don't want you to. That platform is (today) a centralized walled garden. As others detailed, it is difficult for anyone to add new servers and even more difficult to convince the official client to support them. It is a complete contrast to NOSTR that has zero official servers and zero official clients to access the data. It has hundreds of relays from different people, along with several clients from different developers that compete for your preference. reply pfraze 5 hours agorootparentWhat are you talking about? It’s not difficult to add servers or get the official client to use them. reply GaryNumanVevo 7 hours agorootparentprevThere are already alternative clients that support custom AppViews and relays reply nunobrito 5 hours agorootparentDescribed as \"3rd party client\" and which aims as goal to connect with NOSTR: https://docs.bsky.app/blog/feature-skyfeed Twitter also had 3rd party clients until one day they turned off the switch. Around NOSTR there are no labels as \"3rd party clients\" because they can't lock down your data with the push of a switch nor block your use of the platform. reply GaryNumanVevo 2 hours agorootparentWhat's not to understand here? The PDS isn't a relay, it's a repository. The data layer is decoupled from the message passing layer, that's it. reply brianolson 8 hours agoparentprevatproto PDSes are like blog servers with RSS (but better) and bsky.app is the prevailing RSS reader. It's an open protocol because anyone can host a source and anyone can run a different reader. reply GaryNumanVevo 7 hours agoparentprevBlueSky is just a reference implementation using AT Proto. They namespace anything bsky related in the lexicon as such. reply badgersnake 9 hours agoprev [–] I was expecting something about modems. ATDT (555)-COOL-BBS (Totally decentralised btw) reply kragen 6 hours agoparent [–] the nanpa is not decentralized at all, though it does delegate phone number assignment to local telecom companies, and nowadays even to sip providers but yeah it seems pretty suboptimal that they decided to reuse the name of the protocol you use to talk to most cellular modems reply lifthrasiir 6 hours agorootparentIn fact, there had been some complaints [1] about the `at` URI scheme itself as well, even though its registration itself is valid as per RFC 7595 (First Come First Served for provisional entries). [1] https://www.iana.org/assignments/uri-schemes/expert-notes/at... reply badgersnake 5 hours agorootparentprev [–] > the nanpa is not decentralized at all Sure but BBSs tend to be. And then you’ve got systems like fidonet to connect them up. reply kragen 5 hours agorootparent [–] i don't know if you've ever been a regional coordinator but fidonet is not that decentralized either, though the pstn (and nowadays the internet) do put limits on how much power such offices can wield each bbs is usually very centralized reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AT Protocol, developed by Bluesky, is designed for open social networking, focusing on decentralized backend engineering.",
      "Traditional web backends face scalability issues with SQL databases, leading to the adoption of NoSQL clusters and stream processing architectures.",
      "AT Protocol decentralizes high-scale backends by using public APIs for internal services and establishing a shared data model called the \"user data repository,\" which contains cryptographically signed JSON documents."
    ],
    "commentSummary": [
      "ATProto is a comprehensive, bottom-up specification for decentralized systems, contrasting with ActivityPub, which leaves many aspects to implementation.",
      "ATProto uses its own data model instead of JSON-LD, which can be a deterrent for some developers.",
      "A new guide on creating minimal apps with ATProto has been released, highlighting its growing ecosystem and practical applications."
    ],
    "points": 191,
    "commentCount": 57,
    "retryCount": 0,
    "time": 1725840241
  },
  {
    "id": 41483789,
    "title": "Linux's Bedtime Routine",
    "originLink": "https://tookmund.com/2024/09/hibernation-preparation",
    "originBody": "Linux's Bedtime Routine 2024-09-08 hibernate How does Linux move from an awake machine to a hibernating one? How does it then manage to restore all state? These questions led me to read way too much C in trying to figure out how this particular hardware/software boundary is navigated. This investigation will be split into a few parts, with the first one going from invocation of hibernation to synchronizing all filesystems to disk. This article has been written using Linux version 6.9.9, the source of which can be found in many places, but can be navigated easily through the Bootlin Elixir Cross-Referencer: https://elixir.bootlin.com/linux/v6.9.9/source Each code snippet will begin with a link to the above giving the file path and the line number of the beginning of the snippet. A Starting Point for Investigation: /sys/power/state and /sys/power/disk These two system files exist to allow debugging of hibernation, and thus control the exact state used directly. Writing specific values to the state file controls the exact sleep mode used and disk controls the specific hibernation mode1. This is extremely handy as an entry point to understand how these systems work, since we can just follow what happens when they are written to. Show and Store Functions These two files are defined using the power_attr macro: kernel/power/power.h:80 #define power_attr(_name) \\ static struct kobj_attribute _name##_attr = { \\ .attr = { \\ .name = __stringify(_name), \\ .mode = 0644, \\ }, \\ .show = _name##_show, \\ .store = _name##_store, \\ } show is called on reads and store on writes. state_show is a little boring for our purposes, as it just prints all the available sleep states. kernel/power/main.c:657 /* * state - control system sleep states. * * show() returns available sleep state labels, which may be \"mem\", \"standby\", * \"freeze\" and \"disk\" (hibernation). * See Documentation/admin-guide/pm/sleep-states.rst for a description of * what they mean. * * store() accepts one of those strings, translates it into the proper * enumerated value, and initiates a suspend transition. */ static ssize_t state_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf) {char *s = buf; #ifdef CONFIG_SUSPENDsuspend_state_t i;for (i = PM_SUSPEND_MIN; iPM_SUSPEND_ON) { error = -EBUSY; goto out;}state = decode_state(buf, n);if (state = PM_SUSPEND_MAX) hibernate();else pm_suspend(autosleep_state);mutex_unlock(&autosleep_lock);if (!pm_get_wakeup_count(&final_count, false)) goto out;/* * If the wakeup occurred for an unknown reason, wait to prevent the * system from trying to suspend and waking up in a tight loop. */if (final_count == initial_count) schedule_timeout_uninterruptible(HZ / 2); out:queue_up_suspend_work(); } static DECLARE_WORK(suspend_work, try_to_suspend); void queue_up_suspend_work(void) {if (autosleep_state > PM_SUSPEND_ON) queue_work(autosleep_wq, &suspend_work); } The Steps of Hibernation Hibernation Kernel Config It’s important to note that most of the hibernate-specific functions below do nothing unless you’ve defined CONFIG_HIBERNATION in your Kconfig4. As an example, hibernate itself is defined as the following if CONFIG_HIBERNATE is not set. include/linux/suspend.h:407 static inline int hibernate(void) { return -ENOSYS; } Check if Hibernation is Available We begin by confirming that we actually can perform hibernation, via the hibernation_available function. kernel/power/hibernate.c:742 if (!hibernation_available()) {pm_pr_dbg(\"Hibernation not available.\");return -EPERM; } kernel/power/hibernate.c:92 bool hibernation_available(void) {return nohibernate == 0 && !security_locked_down(LOCKDOWN_HIBERNATION) && !secretmem_active() && !cxl_mem_active(); } nohibernate is controlled by the kernel command line, it’s set via either nohibernate or hibernate=no. security_locked_down is a hook for Linux Security Modules to prevent hibernation. This is used to prevent hibernating to an unencrypted storage device, as specified in the manual page kernel_lockdown(7). Interestingly, either level of lockdown, integrity or confidentiality, locks down hibernation because with the ability to hibernate you can extract bascially anything from memory and even reboot into a modified kernel image. secretmem_active checks whether there is any active use of memfd_secret, and if so it prevents hibernation. memfd_secret returns a file descriptor that can be mapped into a process but is specifically unmapped from the kernel’s memory space. Hibernating with memory that not even the kernel is supposed to access would expose that memory to whoever could access the hibernation image. This particular feature of secret memory was apparently controversial, though not as controversial as performance concerns around fragmentation when unmapping kernel memory (which did not end up being a real problem). cxl_mem_active just checks whether any CXL memory is active. A full explanation is provided in the commit introducing this check but there’s also a shortened explanation from cxl_mem_probe that sets the relevant flag when initializing a CXL memory device. drivers/cxl/mem.c:186 * The kernel may be operating out of CXL memory on this device, * there is no spec defined way to determine whether this device * preserves contents over suspend, and there is no simple way * to arrange for the suspend image to avoid CXL memory which * would setup a circular dependency between PCI resume and save * state restoration. Check Compression The next check is for whether compression support is enabled, and if so whether the requested algorithm is enabled. kernel/power/hibernate.c:747 /* * Query for the compression algorithm support if compression is enabled. */ if (!nocompress) {strscpy(hib_comp_algo, hibernate_compressor, sizeof(hib_comp_algo));if (crypto_has_comp(hib_comp_algo, 0, 0) != 1) { pr_err(\"%s compression is not available\", hib_comp_algo); return -EOPNOTSUPP;} } The nocompress flag is set via the hibernate command line parameter, setting hibernate=nocompress. If compression is enabled, then hibernate_compressor is copied to hib_comp_algo. This synchronizes the current requested compression setting (hibernate_compressor) with the current compression setting (hib_comp_algo). Both values are character arrays of size CRYPTO_MAX_ALG_NAME (128 in this kernel). kernel/power/hibernate.c:50 static char hibernate_compressor[CRYPTO_MAX_ALG_NAME] = CONFIG_HIBERNATION_DEF_COMP; /* * Compression/decompression algorithm to be used while saving/loading * image to/from disk. This would later be used in 'kernel/power/swap.c' * to allocate comp streams. */ char hib_comp_algo[CRYPTO_MAX_ALG_NAME]; hibernate_compressor defaults to lzo if that algorithm is enabled, otherwise to lz4 if enabled5. It can be overwritten using the hibernate.compressor setting to either lzo or lz4. kernel/power/Kconfig:95 choiceprompt \"Default compressor\"default HIBERNATION_COMP_LZOdepends on HIBERNATION config HIBERNATION_COMP_LZObool \"lzo\"depends on CRYPTO_LZO config HIBERNATION_COMP_LZ4bool \"lz4\"depends on CRYPTO_LZ4 endchoice config HIBERNATION_DEF_COMPstringdefault \"lzo\" if HIBERNATION_COMP_LZOdefault \"lz4\" if HIBERNATION_COMP_LZ4help Default compressor to be used for hibernation. kernel/power/hibernate.c:1425 static const char * const comp_alg_enabled[] = { #if IS_ENABLED(CONFIG_CRYPTO_LZO)COMPRESSION_ALGO_LZO, #endif #if IS_ENABLED(CONFIG_CRYPTO_LZ4)COMPRESSION_ALGO_LZ4, #endif }; static int hibernate_compressor_param_set(const char *compressor, const struct kernel_param *kp) {unsigned int sleep_flags;int index, ret;sleep_flags = lock_system_sleep();index = sysfs_match_string(comp_alg_enabled, compressor);if (index >= 0) { ret = param_set_copystring(comp_alg_enabled[index], kp); if (!ret)strscpy(hib_comp_algo, comp_alg_enabled[index], sizeof(hib_comp_algo));} else { ret = index;}unlock_system_sleep(sleep_flags);if (ret) pr_debug(\"Cannot set specified compressor %s\", compressor);return ret; } static const struct kernel_param_ops hibernate_compressor_param_ops = {.set = hibernate_compressor_param_set,.get = param_get_string, }; static struct kparam_string hibernate_compressor_param_string = {.maxlen = sizeof(hibernate_compressor),.string = hibernate_compressor, }; We then check whether the requested algorithm is supported via crypto_has_comp. If not, we bail out of the whole operation with EOPNOTSUPP. As part of crypto_has_comp we perform any needed initialization of the algorithm, loading kernel modules and running initialization code as needed6. Grab Locks The next step is to grab the sleep and hibernation locks via lock_system_sleep and hibernate_acquire. kernel/power/hibernate.c:758 sleep_flags = lock_system_sleep(); /* The snapshot device should not be opened while we're running */ if (!hibernate_acquire()) {error = -EBUSY;goto Unlock; } First, lock_system_sleep marks the current thread as not freezable, which will be important later7. It then grabs the system_transistion_mutex, which locks taking snapshots or modifying how they are taken, resuming from a hibernation image, entering any suspend state, or rebooting. The GFP Mask The kernel also issues a warning if the gfp mask is changed via either pm_restore_gfp_mask or pm_restrict_gfp_mask without holding the system_transistion_mutex. GFP flags tell the kernel how it is permitted to handle a request for memory. include/linux/gfp_types.h:12 * GFP flags are commonly used throughout Linux to indicate how memory * should be allocated. The GFP acronym stands for get_free_pages(), * the underlying memory allocation function. Not every GFP flag is * supported by every function which may allocate memory. In the case of hibernation specifically we care about the IO and FS flags, which are reclaim operators, ways the system is permitted to attempt to free up memory in order to satisfy a specific request for memory. include/linux/gfp_types.h:176 * Reclaim modifiers * ----------------- * Please note that all the following flags are only applicable to sleepable * allocations (e.g. %GFP_NOWAIT and %GFP_ATOMIC will ignore them). * * %__GFP_IO can start physical IO. * * %__GFP_FS can call down to the low-level FS. Clearing the flag avoids the * allocator recursing into the filesystem which might already be holding * locks. gfp_allowed_mask sets which flags are permitted to be set at the current time. As the comment below outlines, preventing these flags from being set avoids situations where the kernel needs to do I/O to allocate memory (e.g. read/writing swap8) but the devices it needs to read/write to/from are not currently available. kernel/power/main.c:24 /* * The following functions are used by the suspend/hibernate code to temporarily * change gfp_allowed_mask in order to avoid using I/O during memory allocations * while devices are suspended. To avoid races with the suspend/hibernate code, * they should always be called with system_transition_mutex held * (gfp_allowed_mask also should only be modified with system_transition_mutex * held, unless the suspend/hibernate code is guaranteed not to run in parallel * with that modification). */ static gfp_t saved_gfp_mask; void pm_restore_gfp_mask(void) {WARN_ON(!mutex_is_locked(&system_transition_mutex));if (saved_gfp_mask) { gfp_allowed_mask = saved_gfp_mask; saved_gfp_mask = 0;} } void pm_restrict_gfp_mask(void) {WARN_ON(!mutex_is_locked(&system_transition_mutex));WARN_ON(saved_gfp_mask);saved_gfp_mask = gfp_allowed_mask;gfp_allowed_mask &= ~(__GFP_IO__GFP_FS); } Sleep Flags After grabbing the system_transition_mutex the kernel then returns and captures the previous state of the threads flags in sleep_flags. This is used later to remove PF_NOFREEZE if it wasn’t previously set on the current thread. kernel/power/main.c:52 unsigned int lock_system_sleep(void) {unsigned int flags = current->flags;current->flags |= PF_NOFREEZE;mutex_lock(&system_transition_mutex);return flags; } EXPORT_SYMBOL_GPL(lock_system_sleep); include/linux/sched.h:1633 #define PF_NOFREEZE0x00008000 /* This thread should not be frozen */ Then we grab the hibernate-specific semaphore to ensure no one can open a snapshot or resume from it while we perform hibernation. Additionally this lock is used to prevent hibernate_quiet_exec, which is used by the nvdimm driver to active its firmware with all processes and devices frozen, ensuring it is the only thing running at that time9. kernel/power/hibernate.c:82 bool hibernate_acquire(void) {return atomic_add_unless(&hibernate_atomic, -1, 0); } Prepare Console The kernel next calls pm_prepare_console. This function only does anything if CONFIG_VT_CONSOLE_SLEEP has been set. This prepares the virtual terminal for a suspend state, switching away to a console used only for the suspend state if needed. kernel/power/console.c:130 void pm_prepare_console(void) {if (!pm_vt_switch()) return;orig_fgconsole = vt_move_to_console(SUSPEND_CONSOLE, 1);if (orig_fgconsole required)goto out;}ret = false; out:mutex_unlock(&vt_switch_mutex);return ret; } There is an explanation of the conditions under which a switch is performed in the comment above the function, but we’ll also walk through the steps here. Firstly we grab the vt_switch_mutex to ensure nothing will modify the list while we’re looking at it. We then examine the pm_vt_switch_list. This list is used to indicate the drivers that require a switch during suspend. They register this requirement, or the lack thereof, via pm_vt_switch_required. kernel/power/console.c:31 /** * pm_vt_switch_required - indicate VT switch at suspend requirements * @dev: device * @required: if true, caller needs VT switch at suspend/resume time * * The different console drivers may or may not require VT switches across * suspend/resume, depending on how they handle restoring video state and * what may be running. * * Drivers can indicate support for switchless suspend/resume, which can * save time and flicker, by using this routine and passing 'false' as * the argument. If any loaded driver needs VT switching, or the * no_console_suspend argument has been passed on the command line, VT * switches will occur. */ void pm_vt_switch_required(struct device *dev, bool required) Next, we check console_suspend_enabled. This is set to false by the kernel parameter no_console_suspend, but defaults to true. Finally, if there are any entries in the pm_vt_switch_list, then we check to see if any of them require a VT switch. Only if none of these conditions apply, then we return false. If a VT switch is in fact required, then we move first the currently active virtual terminal/console10 (vt_move_to_console) and then the current location of kernel messages (vt_kmsg_redirect) to the SUSPEND_CONSOLE. The SUSPEND_CONSOLE is the last entry in the list of possible consoles, and appears to just be a black hole to throw away messages. kernel/power/console.c:16 #define SUSPEND_CONSOLE (MAX_NR_CONSOLES-1) Interestingly, these are separate functions because you can use TIOCL_SETKMSGREDIRECT (an ioctl11) to send kernel messages to a specific virtual terminal, but by default its the same as the currently active console. The locations of the previously active console and the previous kernel messages location are stored in orig_fgconsole and orig_kmsg, to restore the state of the console and kernel messages after the machine wakes up again. Interestingly, this means orig_fgconsole also ends up storing any errors, so has to be checked to ensure it’s not less than zero before we try to do anything with the kernel messages on both suspend and resume. drivers/tty/vt/vt_ioctl.c:1268 /* Perform a kernel triggered VT switch for suspend/resume */ static int disable_vt_switch; int vt_move_to_console(unsigned int vt, int alloc) {int prev;console_lock();/* Graphics mode - up to X */if (disable_vt_switch) { console_unlock(); return 0;}prev = fg_console;if (alloc && vc_allocate(vt)) { /* we can't have a free VC for now. Too bad,* we don't want to mess the screen for now. */ console_unlock(); return -ENOSPC;}if (set_console(vt)) { /** We're unable to switch to the SUSPEND_CONSOLE.* Let the calling function know so it can decide* what to do.*/ console_unlock(); return -EIO;}console_unlock();if (vt_waitactive(vt + 1)) { pr_debug(\"Suspend: Can't switch VCs.\"); return -EINTR;}return prev; } Unlike most other locking functions we’ve seen so far, console_lock needs to be careful to ensure nothing else is panicking and needs to dump to the console before grabbing the semaphore for the console and setting a couple flags. Panics Panics are tracked via an atomic integer set to the id of the processor currently panicking. kernel/printk/printk.c:2649 /** * console_lock - block the console subsystem from printing * * Acquires a lock which guarantees that no consoles will * be in or enter their write() callback. * * Can sleep, returns nothing. */ void console_lock(void) {might_sleep();/* On panic, the console_lock must be left to the panic cpu. */while (other_cpu_in_panic()) msleep(1000);down_console_sem();console_locked = 1;console_may_schedule = 1; } EXPORT_SYMBOL(console_lock); kernel/printk/printk.c:362 /* * Return true if a panic is in progress on a remote CPU. * * On true, the local CPU should immediately release any printing resources * that may be needed by the panic CPU. */ bool other_cpu_in_panic(void) {return (panic_in_progress() && !this_cpu_in_panic()); } kernel/printk/printk.c:345 static bool panic_in_progress(void) {return unlikely(atomic_read(&panic_cpu) != PANIC_CPU_INVALID); } kernel/printk/printk.c:350 /* Return true if a panic is in progress on the current CPU. */ bool this_cpu_in_panic(void) {/* * We can use raw_smp_processor_id() here because it is impossible for * the task to be migrated to the panic_cpu, or away from it. If * panic_cpu has already been set, and we're not currently executing on * that CPU, then we never will be. */return unlikely(atomic_read(&panic_cpu) == raw_smp_processor_id()); } console_locked is a debug value, used to indicate that the lock should be held, and our first indication that this whole virtual terminal system is more complex than might initially be expected. kernel/printk/printk.c:373 /* * This is used for debugging the mess that is the VT code by * keeping track if we have the console semaphore held. It's * definitely not the perfect debug tool (we don't know if _WE_ * hold it and are racing, but it helps tracking those weird code * paths in the console code where we end up in places I want * locked without the console semaphore held). */ static int console_locked; console_may_schedule is used to see if we are permitted to sleep and schedule other work while we hold this lock. As we’ll see later, the virtual terminal subsystem is not re-entrant, so there’s all sorts of hacks in here to ensure we don’t leave important code sections that can’t be safely resumed. Disable VT Switch As the comment below lays out, when another program is handling graphical display anyway, there’s no need to do any of this, so the kernel provides a switch to turn the whole thing off. Interestingly, this appears to only be used by three drivers, so the specific hardware support required must not be particularly common. drivers/gpu/drm/omapdrm/dss drivers/video/fbdev/geode drivers/video/fbdev/omap2 drivers/tty/vt/vt_ioctl.c:1308 /* * Normally during a suspend, we allocate a new console and switch to it. * When we resume, we switch back to the original console. This switch * can be slow, so on systems where the framebuffer can handle restoration * of video registers anyways, there's little point in doing the console * switch. This function allows you to disable it by passing it '0'. */ void pm_set_vt_switch(int do_switch) {console_lock();disable_vt_switch = !do_switch;console_unlock(); } EXPORT_SYMBOL(pm_set_vt_switch); The rest of the vt_switch_console function is pretty normal, however, simply allocating space if needed to create the requested virtual terminal and then setting the current virtual terminal via set_console. Virtual Terminal Set Console With set_console, we begin (as if we haven’t been already) to enter the madness that is the virtual terminal subsystem. As mentioned previously, modifications to its state must be made very carefully, as other stuff happening at the same time could create complete messes. All this to say, calling set_console does not actually perform any work to change the state of the current console. Instead it indicates what changes it wants and then schedules that work. drivers/tty/vt/vt.c:3153 int set_console(int nr) {struct vc_data *vc = vc_cons[fg_console].d;if (!vc_cons_allocated(nr) || vt_dont_switch || (vc->vt_mode.mode == VT_AUTO && vc->vc_mode == KD_GRAPHICS)) { /** Console switch will fail in console_callback() or* change_console() so there is no point scheduling* the callback** Existing set_console() users don't check the return* value so this shouldn't break anything*/ return -EINVAL;}want_console = nr;schedule_console_callback();return 0; } The check for vc->vc_mode == KD_GRAPHICS is where most end-user graphical desktops will bail out of this change, as they’re in graphics mode and don’t need to switch away to the suspend console. vt_dont_switch is a flag used by the ioctls11 VT_LOCKSWITCH and VT_UNLOCKSWITCH to prevent the system from switching virtual terminal devices when the user has explicitly locked it. VT_AUTO is a flag indicating that automatic virtual terminal switching is enabled12, and thus deliberate switching to a suspend terminal is not required. However, if you do run your machine from a virtual terminal, then we indicate to the system that we want to change to the requested virtual terminal via the want_console variable and schedule a callback via schedule_console_callback. drivers/tty/vt/vt.c:315 void schedule_console_callback(void) {schedule_work(&console_work); } console_work is a workqueue2 that will execute the given task asynchronously. Console Callback drivers/tty/vt/vt.c:3109 /* * This is the console switching callback. * * Doing console switching in a process context allows * us to do the switches asynchronously (needed when we want * to switch due to a keyboard interrupt). Synchronization * with other console code and prevention of re-entrancy is * ensured with console_lock. */ static void console_callback(struct work_struct *ignored) {console_lock();if (want_console >= 0) { if (want_console != fg_console &&vc_cons_allocated(want_console)) {hide_cursor(vc_cons[fg_console].d);change_console(vc_cons[want_console].d);/* we only changed when the console had already been allocated - a new console is not created in an interrupt routine */ } want_console = -1;} ... console_callback first looks to see if there is a console change wanted via want_console and then changes to it if it’s not the current console and has been allocated already. We do first remove any cursor state with hide_cursor. drivers/tty/vt/vt.c:841 static void hide_cursor(struct vc_data *vc) {if (vc_is_sel(vc)) clear_selection();vc->vc_sw->con_cursor(vc, false);hide_softcursor(vc); } A full dive into the tty driver is a task for another time, but this should give a general sense of how this system interacts with hibernation. Notify Power Management Call Chain kernel/power/hibernate.c:767 pm_notifier_call_chain_robust(PM_HIBERNATION_PREPARE, PM_POST_HIBERNATION) This will call a chain of power management callbacks, passing first PM_HIBERNATION_PREPARE and then PM_POST_HIBERNATION on startup or on error with another callback. kernel/power/main.c:98 int pm_notifier_call_chain_robust(unsigned long val_up, unsigned long val_down) {int ret;ret = blocking_notifier_call_chain_robust(&pm_chain_head, val_up, val_down, NULL);return notifier_to_errno(ret); } The power management notifier is a blocking notifier chain, which means it has the following properties. include/linux/notifier.h:23 * Blocking notifier chains: Chain callbacks run in process context. *Callouts are allowed to block. The callback chain is a linked list with each entry containing a priority and a function to call. The function technically takes in a data value, but it is always NULL for the power management chain. include/linux/notifier.h:49 struct notifier_block; typedef int (*notifier_fn_t)(struct notifier_block *nb,unsigned long action, void *data); struct notifier_block {notifier_fn_t notifier_call;struct notifier_block __rcu *next;int priority; }; The head of the linked list is protected by a read-write semaphore. include/linux/notifier.h:65 struct blocking_notifier_head {struct rw_semaphore rwsem;struct notifier_block __rcu *head; }; Because it is prioritized, appending to the list requires walking it until an item with lower13 priority is found to insert the current item before. kernel/notifier.c:252 /* * Blocking notifier chain routines. All access to the chain is * synchronized by an rwsem. */ static int __blocking_notifier_chain_register(struct blocking_notifier_head *nh, struct notifier_block *n, bool unique_priority) {int ret;/* * This code gets used during boot-up, when task switching is * not yet working and interrupts must remain disabled. At * such times we must not call down_write(). */if (unlikely(system_state == SYSTEM_BOOTING)) return notifier_chain_register(&nh->head, n, unique_priority);down_write(&nh->rwsem);ret = notifier_chain_register(&nh->head, n, unique_priority);up_write(&nh->rwsem);return ret; } kernel/notifier.c:20 /* * Notifier chain core routines. The exported routines below * are layered on top of these, with appropriate locking added. */ static int notifier_chain_register(struct notifier_block **nl,struct notifier_block *n,bool unique_priority) {while ((*nl) != NULL) { if (unlikely((*nl) == n)) {WARN(1, \"notifier callback %ps already registered\", n->notifier_call);return -EEXIST; } if (n->priority > (*nl)->priority)break; if (n->priority == (*nl)->priority && unique_priority)return -EBUSY; nl = &((*nl)->next);}n->next = *nl;rcu_assign_pointer(*nl, n);trace_notifier_register((void *)n->notifier_call);return 0; } Each callback can return one of a series of options. include/linux/notifier.h:18 #define NOTIFY_DONE0x0000/* Don't care */ #define NOTIFY_OK0x0001/* Suits me */ #define NOTIFY_STOP_MASK 0x8000/* Don't call further */ #define NOTIFY_BAD(NOTIFY_STOP_MASK|0x0002) /* Bad/Veto action */ When notifying the chain, if a function returns STOP or BAD then the previous parts of the chain are called again with PM_POST_HIBERNATION14 and an error is returned. kernel/notifier.c:107 /** * notifier_call_chain_robust - Inform the registered notifiers about an event * and rollback on error. * @nl:Pointer to head of the blocking notifier chain * @val_up: Value passed unmodified to the notifier function * @val_down: Value passed unmodified to the notifier function when recovering * from an error on @val_up * @v:Pointer passed unmodified to the notifier function * * NOTE: It is important the @nl chain doesn't change between the two *invocations of notifier_call_chain() such that we visit the *exact same notifier callbacks; this rules out any RCU usage. * * Return: the return value of the @val_up call. */ static int notifier_call_chain_robust(struct notifier_block **nl,unsigned long val_up, unsigned long val_down,void *v) {int ret, nr = 0;ret = notifier_call_chain(nl, val_up, v, -1, &nr);if (ret & NOTIFY_STOP_MASK) notifier_call_chain(nl, val_down, v, nr-1, NULL);return ret; } Each of these callbacks tends to be quite driver-specific, so we’ll cease discussion of this here. Sync Filesystems The next step is to ensure all filesystems have been synchronized to disk. This is performed via a simple helper function that times how long the full synchronize operation, ksys_sync takes. kernel/power/main.c:69 void ksys_sync_helper(void) {ktime_t start;long elapsed_msecs;start = ktime_get();ksys_sync();elapsed_msecs = ktime_to_ms(ktime_sub(ktime_get(), start));pr_info(\"Filesystems sync: %ld.%03ld seconds\", elapsed_msecs / MSEC_PER_SEC, elapsed_msecs % MSEC_PER_SEC); } EXPORT_SYMBOL_GPL(ksys_sync_helper); ksys_sync wakes and instructs a set of flusher threads to write out every filesystem, first their inodes15, then the full filesystem, and then finally all block devices, to ensure all pages are written out to disk. fs/sync.c:87 /* * Sync everything. We start by waking flusher threads so that most of * writeback runs on all devices in parallel. Then we sync all inodes reliably * which effectively also waits for all flusher threads to finish doing * writeback. At this point all data is on disk so metadata should be stable * and we tell filesystems to sync their metadata via ->sync_fs() calls. * Finally, we writeout all block devices because some filesystems (e.g. ext2) * just write metadata (such as inodes or bitmaps) to block device page cache * and do not sync it on their own in ->sync_fs(). */ void ksys_sync(void) {int nowait = 0, wait = 1;wakeup_flusher_threads(WB_REASON_SYNC);iterate_supers(sync_inodes_one_sb, NULL);iterate_supers(sync_fs_one_sb, &nowait);iterate_supers(sync_fs_one_sb, &wait);sync_bdevs(false);sync_bdevs(true);if (unlikely(laptop_mode)) laptop_sync_completion(); } It follows an interesting pattern of using iterate_supers to run both sync_inodes_one_sb and then sync_fs_one_sb on each known filesystem16. It also calls both sync_fs_one_sb and sync_bdevs twice, first without waiting for any operations to complete and then again waiting for completion17. When laptop_mode is enabled the system runs additional filesystem synchronization operations after the specified delay without any writes. mm/page-writeback.c:111 /* * Flag that puts the machine in \"laptop mode\". Doubles as a timeout in jiffies: * a full sync is triggered after this time elapses without any disk activity. */ int laptop_mode; EXPORT_SYMBOL(laptop_mode); However, when running a filesystem synchronization operation, the system will add an additional timer to schedule more writes after the laptop_mode delay. We don’t want the state of the system to change at all while performing hibernation, so we cancel those timers. mm/page-writeback.c:2198 /* * We're in laptop mode and we've just synced. The sync's writes will have * caused another writeback to be scheduled by laptop_io_completion. * Nothing needs to be written back anymore, so we unschedule the writeback. */ void laptop_sync_completion(void) {struct backing_dev_info *bdi;rcu_read_lock();list_for_each_entry_rcu(bdi, &bdi_list, bdi_list) del_timer(&bdi->laptop_mode_wb_timer);rcu_read_unlock(); } As a side note, the ksys_sync function is simply called when the system call sync is used. fs/sync.c:111 SYSCALL_DEFINE0(sync) {ksys_sync();return 0; } The End of Preparation With that the system has finished preparations for hibernation. This is a somewhat arbitrary cutoff, but next the system will begin a full freeze of userspace to then dump memory out to an image and finally to perform hibernation. All this will be covered in future articles! Hibernation modes are outside of scope for this article, see the previous article for a high-level description of the different types of hibernation. ↩ Workqueues are a mechanism for running asynchronous tasks. A full description of them is a task for another time, but the kernel documentation on them is available here: https://www.kernel.org/doc/html/v6.9/core-api/workqueue.html ↩ ↩2 This is a bit of an oversimplification, but since this isn’t the main focus of this article this description has been kept to a higher level. ↩ Kconfig is Linux’s build configuration system that sets many different macros to enable/disable various features. ↩ Kconfig defaults to the first default found ↩ Including checking whether the algorithm is larval? Which appears to indicate that it requires additional setup, but is an interesting choice of name for such a state. ↩ Specifically when we get to process freezing, which we’ll get to in the next article in this series. ↩ Swap space is outside the scope of this article, but in short it is a buffer on disk that the kernel uses to store memory not current in use to free up space for other things. See Swap Management for more details. ↩ The code for this is lengthy and tangential, thus it has not been included here. If you’re curious about the details of this, see kernel/power/hibernate.c:858 for the details of hibernate_quiet_exec, and drivers/nvdimm/core.c:451 for how it is used in nvdimm. ↩ Annoyingly this code appears to use the terms “console” and “virtual terminal” interchangeably. ↩ ioctls are special device-specific I/O operations that permit performing actions outside of the standard file interactions of read/write/seek/etc. ↩ ↩2 I’m not entirely clear on how this flag works, this subsystem is particularly complex. ↩ In this case a higher number is higher priority. ↩ Or whatever the caller passes as val_down, but in this case we’re specifically looking at how this is used in hibernation. ↩ An inode refers to a particular file or directory within the filesystem. See Wikipedia for more details. ↩ Each active filesystem is registed with the kernel through a structure known as a superblock, which contains references to all the inodes contained within the filesystem, as well as function pointers to perform the various required operations, like sync. ↩ I’m including minimal code in this section, as I’m not looking to deep dive into the filesystem code at this time. ↩ < What to Do When You Forget Your Root Password",
    "commentLink": "https://news.ycombinator.com/item?id=41483789",
    "commentBody": "Linux's Bedtime Routine (tookmund.com)188 points by JNRowe 20 hours agohidepastfavorite24 comments nyanpasu64 15 hours agoI've done some digging in Linux power management a while ago, while debugging a (not-fully-fixed) Linux AMDGPU dGPU crash on low memory (https://gitlab.freedesktop.org/drm/amd/-/issues/2362). Along the way, I discovered that you can hibernate both through /sys/power/disk, and the userland snapshot/hibernate/suspend interface (https://docs.kernel.org/power/userland-swsusp.html, snapshot_ioctl()). IIRC these two mechanisms go along quite different codepaths internally. The specific crash bug I encountered was because Linux calls pm_restrict_gfp_mask() to prevent swapping to disk, before dpm_prepare() (the first opportunity for a GPU driver to backup VRAM to system RAM before the PCIe GPU is shut down and VRAM is lost). So if you don't have enough free system RAM to hold all VRAM, the sleep is aborted midway through (waking the system) or produces a failed memory allocation later during sleep or resume (often resulting in undefined system state, halted network or USB controllers, or worst yet a halted NVMe controller resulting in the system running around like a headless chicken unable to load data from disk or even log data to the journal). I'm wondering if this was a deliberate decision or an unforeseen interaction between suspend-time GFP masks and GPU drivers. It seems Nvidia can't reliably backup VRAM either without being informed by systemd prior to the kernel initiating suspend (https://download.nvidia.com/XFree86/Linux-x86_64/560.35.03/R...). reply nubinetwork 11 hours agoparentIn a way, I'm not surprised... when I reboot my systemd-based servers, it almost seems like (based on speed) that it didn't do anything to the running services and filesystems, and just tells the kernel to immediately reboot. reply zokier 6 hours agorootparentWhy guess when the shutdown(/reboot) process is explicitly documented: > Shortly before executing the actual system power-off/halt/reboot/kexec systemd-shutdown will run all executables in /usr/lib/systemd/system-shutdown/ and pass one arguments to them: either \"poweroff\", \"halt\", \"reboot\", or \"kexec\", depending on the chosen action. All executables in this directory are executed in parallel, and execution of the action is not continued before all executables finished. Note that these executables are run after all services have been shut down, and after most mounts have been unmounted (the root file system as well as /run/ and various API file systems are still around though). https://www.freedesktop.org/software/systemd/man/devel/syste... reply bbarnett 4 hours agorootparentYup. And unlike sysvinit Debian systems I run (hundreds under bookworm), systemd init systems (I run thousands) require all sorts of workarounds for this sort of behaviour. I get VMs not rebooting due to NFS umount failures, VMs not logging shutdown info because rsyslogd is terminated too early, literally endless issues. Killing services without a proper TERM and wait prior to -9 is only one of the wonderful shortcomings I find with systemd. reply amelius 7 hours agoprevI suspend my Linux box every night. However, I notice that after some 30-50 times the machine freezes at a random point when using the machine (even the num-lock led stops working). Curious if others have the same experience, and if it's Linux-related or a problem of my particular hardware. reply sillystuff 2 hours agoparentI suspend my computer, every time I walk away from it, and it is not performing a long-running process. Often 10+ times per day and have never experienced an issue with suspend/resume after working out these two issues: 1) I had to add a hook script to unload the module for the Intel AX210 wireless adapter on suspend, and re-load on resume. Before doing that, the laptop would crash every few suspend cycles. And, would crash on hibernate, every time. This issue may have been addressed with later kernels/firmware, but I've never re-visited. For systemd, hook scripts go into: /usr/lib/systemd/system-sleep/. 2) when experimenting with rocm (unsupported on my igpu, but gave it a try anyway), after running rocminfo, the system would resume with a black/blank display, and nothing I tried got the display back. I never got rocm fully working on my laptop, so the solution for this one was simple. Every laptop I've owned since the mid-90s has had 100% reliable suspend/resume on Linux. Sometimes, it \"just worked\". Sometimes it took some investigation upfront to work out an issue (e.g., used to run swsusp for suspend to work around issues with ATI gpus with kernel suspend prior to kernel mode switching KMS), but after this initial futzing, it was always 100% reliable. There is debug logging you can enable to help track down suspend/resume issues and also entries in debugfs, but you may have to resort to trial and error, to track down the issue. reply tasn 7 hours agoparentprevI have had the same thing before (but no longer) and haven't been able to find anything online about it. Glad to see I'm not the only one! Same thing, after 30-50 suspends it freezes randomly. I'm pretty sure it's Linux related as it was fixed on a system upgrade and regressed after another. (Works now with latest Arch) reply not_your_vase 2 hours agoparentprevI do see this on my AMD systems (4700U + 5700G) - but not in Intel ones interestingly. reply heavyset_go 17 hours agoprevThis is a great write up that goes deeper than I expected it to. Glad to have seen it. reply pino82 16 hours agoprev [–] I just read the first few lines so far. They play around a lot with strings, compared to the fact that it's not about word processing but power management. I'm not a developer on this system level of things. When you usually try to write 'nice' code, you are somewhat surprised about concerns like \"convert the last space to a newline\" there. Yes, I know, everything is a file, and this is just the other side of this odd ancient paradigm. To me it looks tedious. But, well, could be that this is just for me, because I'm not used to it. Maybe it's not a problem at all once you are deeper inside it. But even from a logical perspective, it is funny: There is a file that contains all available sleep modes. Once you write a particular one into the same file (let's say you open it in a text editor and remove all states but one and then save), the system goes into that sleep mode. Yes, I know, operating systems are different from a tiny web service in Python (and even there you start tricking around with weird http concepts instead)... It was just an observation. reply telgareith 15 hours agoparentI'm not sure where you're coming from here. \"Everything is a file\" is literally part of the design philosophy. Thats all it is: design philosophy. Well, besides improper string termination being the root of a staggering number of vulnerabilities. There's nothing keeping somebody in either windows or linux from writing kernel code/drivers that takes syscalls instead of text, or text instead of syscalls. Except microsoft and the linux community would both decline to include it. reply nyanpasu64 15 hours agorootparentThe funny thing is that Linux does have an alternative ioctl-based suspend interface (https://docs.kernel.org/power/userland-swsusp.html)... with an incompatible API and different purpose from the string-based one... reply p_l 9 hours agorootparentprevThat said, it could have been handled better than have from scratch string handling in every driver. Compare Plan 9's getfields or approach from 9front https://man.9front.org/9/parsecmd reply pino82 15 hours agorootparentprevAt first glance, the Redmond philosophy looks better to me here. I know that they made a lot of marketing around it (files vs APIs). And parts of that is just marketing bs, but there is some truth imho. What is all the string overhead really for? Isn't the client side equally tedious? You write e.g. weird shell scripts that sed/awk/grep some files from procfs or sysfs, spend a lot of time into string parsing, and then there are also corner cases where it fails (sometimes it's enough to have a space in some file names). What do I actually get back from all that complexity? There is probably something; I just haven't recognized it so far. I'm asking that as a Linux-only-since-two-decades user btw. reply ahartmetz 11 hours agorootparent> What do I actually get back from all that complexity? Very easy experimentation / exploration, extremely rapid prototyping and one-off scripts. I have written a GUI program to show memory pages of a process using procfs, it was fine. About two days of effort to parse and piece together data from obscure procfs files. A well-documented API with example code would have been faster I guess, but a text format with minimal documentation is OK. reply jdiez17 8 hours agorootparentI like Drew's racecar analogy (from https://drewdevault.com/2021/12/05/What-desktop-Linux-needs....): Linux is a high-performance F1 car intended mostly for advanced users, other operating systems are like an SUV. reply lproven 5 hours agorootparentThat is an interesting comparison, from at least 2 different angles. 1. As a performance car: it's not a particularly high-performance OS compared to a lot of much smaller simpler OSes, such as RTOSes, but also including some former contemporaries (e.g. RISC OS, Symbian, or late-era OS/2). Last year I installed, updated and briefly used Windows XP64 as a desktop OS on some fairly late hardware it can support: a Core 2 Duo with 8GB of RAM, a discrete GPU, and an SSD. https://www.theregister.com/2023/07/24/dangerous_pleasures_w... It is amazingly fast and responsive compared even to the lightweight end of modern Linux, such as Crunchbang++, Bodhi Linux, or Q4OS. It's also faster and more responsive than OpenBSD or NetBSD. The only thing that came close was Alpine Linux and XP64 still wins. So, I think as a model of screaming fast performance vehicle is poor: it's not. As Neal Stephenson put it in _In The Beginning Was The Command Line_ https://web.stanford.edu/class/cs81n/command.txt ... it's a sort of super-efficient amphibious armoured car: big, fat, ugly, but can do anything on anything, will get you there, costs nothing and runs on anything (I am thinking of \"Mr Fusion\" from Back to the Future here.) 2. So how does it compare to an F1 car? Well, it's fiddly and delicate and complicated and only an expert can drive it. It can be easy and nigh-on foolproof. Look at Android or ChromeOS. They are barely recognisable as Linux but they're billion-selling consumer OSes. But it doesn't compare well to the performance aspects at all, IMHO. It's the ultimate Swiss army knife: can be used for anything but as a result it's huge and ugly and complicated and won't fit in any pocket. reply LegionMammal978 15 hours agorootparentprev> Except microsoft and the linux community would both decline to include it. And yet the number of syscalls and ioctls expands nonetheless. E.g., in Linux, they just last year added the listmount() and statmount() syscalls, even though they return substantially the same information as /proc/self/mountinfo, since the latter simply can't be queried as flexibly. reply pino82 15 hours agorootparentCooool! Thx! I recently searched precisely for that, but was unable to find anything. reply mastax 5 hours agoparentprevThere was an article a while back, I think it was Marc talking about OpenBSDs pledge(), which argued that just taking a string argument of space separated flags is better than the traditional enum flags argument for a syscall. Sort of orthogonal but I found it very persuasive, even as someone who also breaks out in hives when I see my kernel full of stringly typed APIs. reply jdiez17 8 hours agoparentprevNot sure why people are downvoting you. Sure, it can seem surprising at first to see string management in the PM side of the Linux kernel. But the advantages of almost-everything-is-a-file are worth it. reply pino82 15 hours agoparentprev [–] PS: Yes, I know, if it would be a web api, I'd need to do play the same games with strings there. But there it's at least obvious why it is needed (and it would also be less tedious in a modern scripting language, compared to dealing with string in raw C). Again, I'm not really complaining. I'm just wondering whether one would still solve it in the same today in a brand new OS. reply baq 9 hours agorootparent [–] Microsoft has powershell and it’s a properly good tool for manipulating objects. Strings are easiest to manipulate using string manipulation tools, which unix/linux/posix has plenty, and no standard way to expose objects. Perfect is the enemy of the good here. reply homebrewer 6 hours agorootparent [–] I hope support for structured text output becomes more common. For example, the `ip` set of tools can output data in JSON, which is safe and easy to destructure and extract whatever fields you need. Seems like a nice middle ground. % ip --json link % ip --json addr etc. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explains the hibernation process in Linux, focusing on version 6.9.9, from initiating hibernation to synchronizing filesystems to disk.",
      "Key entry points for controlling sleep and hibernation modes are `/sys/power/state` and `/sys/power/disk`, with specific values written to these files to initiate sleep states.",
      "The hibernation process involves several steps, including kernel configuration, availability checks, compression checks, lock acquisition, console preparation, power management notifications, and filesystem synchronization."
    ],
    "commentSummary": [
      "Linux power management issues were explored, particularly focusing on hibernation and suspend mechanisms, which follow different code paths.",
      "A crash was identified due to insufficient free system RAM to hold all VRAM, leading to failed memory allocation during sleep or resume, causing undefined system states.",
      "The discussion highlights the complexities and potential issues with Linux's power management, including the handling of VRAM and the role of systemd in managing these processes."
    ],
    "points": 188,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1725834680
  },
  {
    "id": 41482679,
    "title": "Htmx, Raku and Pico CSS",
    "originLink": "https://rakujourney.wordpress.com/2024/09/08/htmx-raku-and-pico-css/",
    "originBody": "HTMX, Raku and Pico CSS Sep 08 2024 This post is kind of part 3, coming off last week’s thrilling episode. I am a simple sole, I want to reduce the cognitive load in my web projects. The general idea is to go back to the halcyon early days of the web before Netscape dropped the JS-bomb. You know HTML for the layout and CSS for the style. An elegant division of roles. When I read about HTMX it was clear that Raku and Cro are ideal candidates for the back end HTML assembly, defining routes and serving RESTful APIs. As we have seen in the previous posts, HTMX eliminates the need for JS to make dynamic web content. Lovely. Remember – we are talking simpler ways to build attractive, dynamic, modern websites. While HTMX is well suited to 90% of this, if you are building a webapp like FaceBook or Google Maps, then it’s not for you. Pico CSS But what to do about style and CSS? Well HTMX is neutral to CSS … it can be used with Bootstrap, Tailwind, SASS and so on. But many of these CSS tools have evolved to jam more stuff into the HTML tag attributes. In my mind, the ideal would be something like this for a simple navbar: About Services Products I had heard that Pico CSS was often used in HTMX projects. And sure enough, the Pico Components have this feel… Here’s Bootstrap for contrast: About Services Product What about Tailwind, also for contrast: About Services Product Bootstrap and Tailwind come at the cost of “more stuff in the HTML tags”. Here’s the Pico CSS example: https://picocss.com/docs/nav So for our goals, it looks like Pico CSS is on a good track. Their website says: What’s that? Semantic HTML! Looks like my goal all along has been Semantic HTML (not that I knew at the time). By adding semantic HTML tags to your pages, you provide additional information that helps define the roles and relative importance of the different parts of your page. (As opposed to non-semantic HTML, which uses tags that don’t directly convey meaning.) So having more powerful Semantic HTML is a win. Hopefully the figure above is enough of an eye opener for now. There’s much more info out there if you are curious. But obviously read the rest of my post first. [For hardcore aficionados, I plan to look into Web Components in a future post. I also think that Bootstrap and Tailwind and SASS in general are good companions to HTMX and Raku — but my project and this series of posts starts by using Pico CSS to minimize the cognitive load on the style side – later we will come back to these other styling tools]. Real Code So lets see how this looks in action. All the code for these posts is on GitHub for your perusal and collaboration. I have been using Pico CSS as part of my project to rebuild the HTMX examples for Raku / Cro largely by translating the Python / Flask examples. This post draws on the Tabs HATEOAS one in particular, since I have in mind that I will want a Tab Component in my toolbag but that Pico CSS does not provide one out of the box. Pico does have Accordions so there is some prior art for inspiration. Anywho, here’s the way the final code ended up. tabs/index.crotmp:Tab 1 Tab 2 Tab 3 /tabs/tab1.crotmp (tab2 and tab 3 are much the same so I won’t bore you)\"When you're new to something, you bring an ignorance that can be highly innovative.\"– Rick Rubin /Routes/Examples/Tabs.rakumod to fulfil the hx-get attrs. use Cro::HTTP::Router; use Cro::WebApp::Template; sub tabs-routes() is export { route { template-location 'templates/tabs'; get -> { template 'index.crotmp'; } get -> 'tab1' { template 'tab1.crotmp'; } get -> 'tab2' { template 'tab2.crotmp'; } get -> 'tab3' { template 'tab3.crotmp'; } } } And the proof… oh yeah, Pico has built in dark mode 😉 Thanks for tuning in, please feel free to like, share or comment. You can find me on the Raku Discord and IRC Channels. ~librasteve Share this: Twitter Facebook Like Loading... Related Sep 08 2024 Published by librasteve View all posts by librasteve",
    "commentLink": "https://news.ycombinator.com/item?id=41482679",
    "commentBody": "Htmx, Raku and Pico CSS (rakujourney.wordpress.com)177 points by librasteve 23 hours agohidepastfavorite136 comments simonbarker87 19 hours agoI’ve been using htmx for over a year now for our internal management application (order monitoring, partner management, CAD model uploading and management type stuff) and I continue to be delighted at how quickly I can add features and iterate existing ones. I write way less client side JS, the app is very fast and responsive and I don’t have to write the app twice like with a SPA + API. reply rtpg 19 hours agoparentOne thing I've been struggling with using HTMX... with an app and a frontend REST API I've found I can really kinda quickly craft a frontend by filtering down whatever resources I need (though it's honestly pretty wasteful at times). With HTMX I'm finding myself needing to have as many backend views as I have ways of interacting with a page. I still have to write the frontend, and on top of that I gotta make a bunch of one-off backend views for many interactions. What am I doing wrong? reply ysavir 15 hours agorootparentThis is the thing that makes me lose interest in HTMX. People talk about it like it eliminates the need for JS/React/etc, but as far as I can tell, you're still writing those same templates, just using your backend language instead of JS. Which is convenient, but hardly \"less code\", just a different language, and that backend language probably lacks a lot of functionality you'd want for UI convenience. That's all fine if you'll never want those UI conveniences, by my experience has been that the want of those UI conveniences creep in over time, and eventually you regret not using something that makes them native and easy. I haven't actually played around with HTMX, so I might be sizing it up wrong, but it's mission statement just never resonated with me enough to want to try it out. reply prisenco 14 hours agorootparentThe value of HTMX is that state now resides solely on the server, and the browser becomes only a representation of that state. This simplifies applications considerably because maintaining two copies of the state between client and server is the source of a lot of complexity in modern applications. It wasn't about writing less code (although it is about writing less javascript) but about working with the concept of hypermedia instead of against or around it. reply rtpg 12 hours agorootparentSomething I dislike about this argument is that there are a lot of UX flows where \"tracking the state server-side\" is a _major_ pain in the butt compared to having client-side state. And popular backend frameworks are not super up to the task! I have the impression that older web frameworks did in fact have a good amount of statefulness built in, and that causes a whole host of issues, so I believe to understand _why_ modern backend frameworks really don't lean into that. reply prisenco 12 hours agorootparentYou're absolutely right, htmx is not built for every UX flow. But neither is React. And we've been reaching for it by default for everything and that's been a mistake. So much of what's built these days could be stateless MPAs. Servers have gotten crazy fast, and I'd certainly rather optimize for page size and server latency than build a React app. With the upcoming views transitions api, a bit of htmx, and custom web components and you get 95% of what React offers without anywhere near the package size or complexity. You don't even need a compile step. I'd never argue htmx is best for all use cases, I would hate for it to mutate the way React has, trying to be everything to everyone. But I hope htmx is the start of re-thinking how they approach the web and realizing that there are significantly simpler ways to build a frontend. reply lloydatkinson 10 hours agorootparentI definitely think it’s important to note it’s not really React that’s mutated it’s damn Vercel with its extreme vendor lock in plan with Next that’s trying to force React to be server side as much as client side, especially now that Vercel employs some React core team members. Unfortunately worked with someone I was reasonably sure was a ~~paid shill~~ part of some Vercel partner/influencer programme as they were adamant we must use Next (and Vercel but they lost that argument) for a project the team absolutely didn’t need to use it for. In fact the rewrite of a project was worse than the original application. The original was snappy and instant and just worked meanwhile the Next version was slow, full page refreshes for every action, and just totally awful. I couldn’t get off that team quick enough. reply langcss 8 hours agorootparentNext is odd because it gives lots of \"high preformance\" vibes with static rendering and image optimization. It is fast enough but not as fast as tuning up a classic MVC app which caching, avoiding JS bundles and React or deferring scripts etc. Send out the right cache headers and chuck a CDN in front. Next is ironically fast if you prerender/static generate AND the client disables JS! reply omnimus 12 hours agorootparentprevServer side frameworks force you to keep most state in the URL (or localstorage). This is a limitation. But in my experience it’s not a bad one because only very best designers i worked with were deliberate about state so they could use this advantage. More likely than not you get designs that create all kinds of messy app states around and users then can’t properly use bookmarks or back button. This in turn leads to hotfixes and more mess in frontend. Forcing designers to think about urls and their names is in most cases very good. reply naasking 5 hours agorootparent> Server side frameworks force you to keep most state in the URL (or localstorage). This is a limitation. Rather than limitation, I think of it as a constraint. The best engineering takes place under constraints, because when you're unconstrained you have a high chance of repeating mistakes that have known solutions. In this case, you rightly point out that the UX of the browser has certain limitations and making all state URL-addressable is the interface to that UX. reply librasteve 11 hours agorootparentprevthis is the argument that resonates with me ... increasing complexity around event driven frameworks includes more cognitive load to reason about state information in two (or more) places whereas the web of olden times was conceived around stateless clients (plus some cookies) as I say in the post, HTMX is not for you if you are building a web app that needs this ... but in 90% of cases web sites don't reply cies 9 hours agorootparentprev> \"tracking the state server-side\" is a _major_ pain in the butt compared to having client-side state. The issue with many SPAs I saw was that state had to be managed BOTH in the browser AND on the server. This resulted in more code, longer time to market, and reduced velocity. The SPA route can be a requirement (e.g. when a highly interactive UX is warranted), in that case there's no way around it. But when it's simply a bunch of forms in a business app, that's usually not the case: in those cases I'd go with a traditional \"multi page app\" and/or HTMX. reply ysavir 6 hours agorootparentprevThis isn't unique to HTMX though. You can do the same with any frontend framework. You can argue that HTMX forces this to be the case, but has a lot of trade offs in doing so. reply langcss 8 hours agorootparentprevThere is client state. The URL plus the sum of whatever partial swaps you did since loading the page. reply the_gipsy 11 hours agorootparentprevI haven't used htmx, but this doesn't seem to be true at all. The equivalent state would also be in the client, as URL and HTML. reply prisenco 11 hours agorootparentThe state is represented by the url and html, but that's not the same as managing state in client-side memory using Javascript. Hypermedia Systems is a great read, by the htmx creator, available here: https://hypermedia.systems reply consteval 1 hour agorootparentprevThe client doesn't produce those URLs or HTML - the server does. That's just what the user sees. reply omnimus 12 hours agorootparentprevThe approach actually is not that different if you realize that there isnt much difference between sending json and sending html. Htmx is like if you were using react and rendering raw html everywhere. The main advantage people feel with htmx is that your routing is serverside and you dont have to keep quering/filtering/transforming data from your api to fit UI because you have access to everything. Do you suddenly need number of groups user is part of in this one only place? Well api doesnt have it so either extend api (figure out how to name it and where to put it) or you fetch all groups user is part of and count them on frontend. This middlelayer is something i realized i more often than not need. And when i need interactive UI i can initialize webcomponent or even have react island. Since htmx (and other similar tools) dont expect to overtake whole DOM but work with elements they can be very isolated if you want them to be. reply miggol 11 hours agorootparentprevAt the end of the day, neather approach really has features over the other. HTMX doesn't forbid other JS components. I think HTMX appeals to fullstack devs like me who often find themselves needing bespoke API calls for their frontend views anyway. When writing tightly coupled backend code for a frontend view, I'm quick to think \"well why am I not doing this entire feature in the backend, anyway?\" And HTMX makes that super easy without giving much up. But the opposite reaction would be equally valid. reply naasking 5 hours agorootparentprev> but as far as I can tell, you're still writing those same templates, just using your backend language instead of JS. Yes. > Which is convenient, but hardly \"less code\" It is strictly less code because you're no longer validating on both client and server. It's also more than just \"convenient\", because it also forces you to stick to the well understood browser UX, ie. all state is URL-addressable. reply scoofy 2 hours agorootparentI would rather write 1000 lines of Python than 100 lines of JS. Nothing wrong with JS, I'm just not fluent in it. Whereas, Python is my native programming language, and I can just see it without looking anything up. This is why I love HTMX. I just want basic web interaction without having to do it all in Javascript. reply Cruncharoo 6 hours agorootparentprevNo, you're pretty much exactly right. I am not a programmer by trade and when searching around and trying different technologies the first one that really clicked for me was HTMX. For whatever reason, handling all of that logic on the backend felt more natural. I've since started learning React and it's amazing how much 'easier' some of those UI conveniences really are. However, in places where they aren't truly necessary it's hard to beat the convenience and ease of a small Flask/SQLite/HTMX app, for me. But yeah, if that doesn't really fit your mental model then I don't see a huge reason to give it a try.. other than it IS fun! reply BeefySwain 18 hours agorootparentprevCheck these out and see if they provide any insight for your specific issues :) - https://htmx.org/essays/template-fragments/ - https://htmx.org/essays/10-tips-for-ssr-hda-apps/ reply devjab 12 hours agorootparentprevHTMX pairs incredibly well with something like Go’s templates. Giving you the “what you’d want react to be” experience for many scenarios. I imagine it’s similar with other languages with good templating engines, I just know it’s extremely easy to build things with Go and HTMX. That being said, HTMX is not for everything. It’s great for internal tools as long as you don’t have very complex role-based access model. But security and access rights is where I think HTMX quickly becomes too complicated compared to a more traditional approach. As far as having “too many back end views” I do think that is down to you choosing an “incompatible” backend for your HTMX. It’s hard to say without knowing your details. You do say that you put a REST api behind it, but why would you do that with HTMX? reply librasteve 11 hours agorootparentIn the article I show HTMX and Pico CSS with Raku and Cro templates (Cro is one of the leading Raku web frameworks - there are others such as Hummingbird) in the same role as Go and ... (put your favourite Go HTML template engine here) - I am currently implementing the basic htmx.org examples in Raku by translating from https://github.com/Konfuzian/htmx-examples-with-flask/tree/m... which has them in Python / Flask. The HMTX Discord has about 35 channels across all the various server side language options (including node). Certainly agree that HTMX is not for everything. Nor is Raku ;-) reply djbusby 18 hours agorootparentprevI make htmx sites built around view fragments rather than pages. And when I need a page it's just a set of fragments. I make \"API\" endpoints fir the needed fragments and call via ssr on first paint, then as needed from the htmx side. reply librasteve 11 hours agorootparentthis is my medium term intent with HTMX & Raku ... I have in mind a programmatic / functional style of building websites where I can compose whole pages and sites reply hyperdang 15 hours agorootparentprevnext [2 more] [flagged] esaym 15 hours agorootparentrofl reply chromanoid 12 hours agorootparentprevYou may want to look at https://roca-style.org/ If you have problems filtering in the backend but it's easy in the frontend your backend technology is probably lacking. reply _heimdall 18 hours agorootparentprevI've tried HTMX with a few different back end frameworks and languages, my setup varies a lot based on which framework/language I use. The pattern I've been happiest with is when I can have a templating language designed to work really well with a component (or partials) approach. I break down the UI into smaller components as I add more HTMX interactions. The UI is effectively a tree of nested components, much like react or svelte. When an HTMX request is sent, the API handler logic runs and instead of returning a success response it sends back HTML for the individual component(s) that HTMX needs to update. tl;dr; When a request comes in, check headers to see if it was an HTMX request. If it wasn't, handle the API logic and return the full HTML page. If it was an HTMX request, still run the API logic but send back only the rendered components/partials that changed. reply rtpg 12 hours agorootparentyeah it's just that when I have a REST API I get like... I guess 20 or so endpoints \"for free\" (since I can PATCH specific fields up in interesting ways). Maybe I need to be writing REST-looking form submission endpoints.... but then I have the immediate issue of presentation. reply imacrayon 17 hours agorootparentprevCheck out https://alpine-ajax.js.org it defaults to using the same template views you would in a typical JavaScript-less app, then you can sprinkle in fragments where you need to optimize requests. reply halfcat 17 hours agorootparentprevWhen I build a page that uses HTMX, I go one of two ways. Traditionally I first build it as a fully server-side rendered page. Once that’s working I setup my views to return HTML partials for different parts of the page. So I might still have multiple views (or I might have single views that handle path parameters, query parameters, etc), but each is then basically just returning part of the existing template: Full page: return render(request, \"jobs/index.html\", ...) Some part of the page: return render(request, \"jobs/index.html#status\", ...) You can sort of think of this the same as you would with JSON API endpoints, where you still need different endpoints handling different HTTP methods (GET, POST, etc) for the different CRUD operations, and there’s a tradeoff regarding how granular you get with views, but typically you can make the views pretty generic and reusable (the same way you can make JSON API endpoints that essentially just serialize database query results). There’s an article [0] that gives an example of a similar approach. More recently I’ve been using a more component-based approach, using something like htpy [1] where I build up the page out of components (like you would in React), and sprinkle JS and CSS into the HTML with Alpine and Tailwind. [0] https://www.circumeo.io/blog/entry/django-htmx-and-template-... [1] https://htpy.dev/ reply stuckinhell 5 hours agorootparentprevyea I'm seeing alot of people combine it with alpine, but then whats the point ? reply meowtimemania 19 hours agoparentprevI love htmx for internal dashboards. I find htmx difficult to use for user facing applications because it's difficult to get everyone on board with the constraints of htmx (no optimistic uis, simple ui/ux). When building complicated frontends with lots of popovers, modals, optimistic state, I like react. reply prisenco 13 hours agorootparentUX designers are immersed in this world of React and frontend frameworks, so their designs are built with that in mind. Doing things the \"htmx way\" on a team requires buy-in from more than just devs and that can be hard. We should be careful not to push htmx too past what it was meant for, as well. I remember how much I admired React when it was released for its simplicity. reply taberiand 18 hours agorootparentprevI don't think htmx should be used on its own when implementing ui/ux - htmx has the job of getting the blocks of html, with data embedded, from the back-end to the front-end (and posting back up as necessary); once it's there, front-end client-only ui/ux can be handled by other tools in JavaScript reply aurareturn 14 hours agorootparentSo why complicate things and just use something like React + Next.js, which is already designed for complex apps? reply prisenco 14 hours agorootparentAlternatively, use web components which are baked into the browser, don't require a compile step, integrate well with HTMX and are much more stable than React. reply kaoD 12 hours agorootparentWeb components are the stuff that nightmares are made of. The amount of boilerplate I had to write just to keep DOM attributes and JS properties in sync was not fun, the impedance mismatch between them (DOM attributes being strings) was painful to deal with, and templates/slots felt much worse than the React way. The DOM didn't seem like a great model for moderately complex apps. Feels like web components didn't take off for a reason. IMO they feel like the solution you come up with when you create an abstraction in paper instead of writing a real-world thing that will solve your immediate problems. Not very pragmatic. Plus they only work with JS enabled, unlike React+SSR where you can progressively enhance your app. Overall not a great experience for user-facing apps. reply throwitaway1123 8 hours agorootparent> Plus they only work with JS enabled, unlike React+SSR where you can progressively enhance your app. You can SSR web components using the Declarative Shadow DOM API, which is finally supported in all of the major browsers and works without JS. reply prisenco 12 hours agorootparentprevnext [–]Web components are the stuff that nightmares are made of. There's lit.dev for an easier approach. https://lit.dev reply kaoD 11 hours agorootparentBut that's yet-another-layer-of-abstraction with its own set of tradeoffs (e.g. I think CSS-in-JS is a trap, which seems to be the way for Lit; slots are still a thing; no SSR nor progressive enhancement; decorators!?!?!; etc.) which builds on top of what already feels like the wrong abstraction in the first place, only to provide React-like capabilities. At that point why not just use React? What do I get from using Lit instead? reply prisenco 11 hours agorootparentI don't personally mind writing web components by hand, but for those who want something easier, lit.dev is popular. There's also slim.js and Stencil if you don't mind a compile step. The design of web components could be better, but I much prefer them to the true nightmare that React development has become. And the api is stable, which means a longevity that frameworks don't have.no SSR nor progressive enhancement I have not been impressed by React SSR in the wild in terms of progressive enhancement. This seems like more of marketing promise than a real world experience. Do you have any examples to link? reply librasteve 11 hours agorootparentprevI have been musing in going in this direction - but your post has lowered this idea in my project plan for now ... thanks! reply synergy20 18 hours agorootparentprevi used it for simple dashboard, worked well but for complex projects it leads to spaghetti code for me, I had to stick to react for that. reply zerr 8 hours agoparentprevFor such an internal web app, why not do it in a fully traditional/multi-page/server-rendered manner? reply simonbarker87 6 hours agorootparentI guess I could have done but the server was initially set up to respond with JSON and then the pages aspect got added in later and it felt easier to boot in htmx and bolt in the partials system and keep the json endpoints that are still needed than rearchitect for a proper MPA. Plus I wanted an excuse to use htmx. reply nanis 17 hours agoprev> I am a simple sole, ... go back to the halcyon early days of the web before Netscape dropped the JS-bomb. You know HTML for the layout and CSS for the style. I am not sure if this is intended as humor, but JavaScript came before CSS. reply culi 16 hours agoparentAnd \"HTML for structure, CSS for style\" is a philosophy that developed later. As is evidenced by early HTML tags like , , , , etc reply stavros 13 hours agorootparentI remember when CSS Zen garden was showcasing what you can do with CSS, and browsers (well, \"browser\", singular, as there was basically only IE 6 back then) supported Javascript and VBScript. reply austin-cheney 5 hours agorootparentBack in 2008 we had a team building exercise to create a Zen Garden sample. Here was mine: https://prettydiff.com/zen/ In those days the three content columns vertically aligned to the same height cross-browser. reply austin-cheney 4 hours agoparentprevIt seems JavaScript was first released, just internally, in May 1995 in a pre-alpha version of Netscape 2.0. It would not be publicly announced until December 1995. Netscape 2.0 didn't even come out until March 1996 and even then it was language version 1.0 which was extremely defective. The first version of the language that actually worked was JavaScript 1.1 that came out in August 1996. CSS on the other hand first premiered with IE3 that came out in August 1996. * https://www.w3.org/Style/CSS/msie/ * https://webdevelopmenthistory.com/1995-the-birth-of-javascri... The distinction either way is trivial, because at that time nobody was using either CSS or JavaScript as they required proprietary APIs. There was no DOM specification at that time. reply SoftTalker 15 hours agoparentprevAnd it's soul, not sole. Unless the author is also a fish. reply librasteve 11 hours agorootparentsorry - maybe we need AI smell checkers reply itohihiyt 10 hours agorootparentHaha reply librasteve 10 hours agorootparentprevI'm noted for my dry wit reply philsnow 14 hours agorootparentprevI read that as \"sole [proprietor]\" reply librasteve 11 hours agoparentprevyikes, I stand corrected... JavaScript was created by Brendan Eich in just 10 days in May 1995 while he was working at Netscape Communications Corporation CSS (Cascading Style Sheets) was introduced later than JavaScript. The first CSS specification was published in December 1996 by Håkon Wium Lie and Bert Bos. apologies reply agumonkey 4 hours agoparentprevHmm apparently it also came before DSSSL. Surprising. reply karaterobot 16 hours agoprev> As we have seen in the previous posts, HTMX eliminates the need for JS to make dynamic web content. Well, sort of! I suppose that for certain definitions of eliminating Javascript, this Javascript framework eliminates the need for Javascript completely. reply niutech 4 hours agoparentHTMX is whooping 45KB of JS, which is being often loaded inblocking rendering HTML, whereas you can have a fully functioning interactive website with pure CSS framework like Spectre.css (https://niutech.github.io/spectre/) and a sprinkle of 166-byte (sic!) HTMZ. See my demo: https://kodus.pl reply nsonha 11 hours agoparentprevhtmlx, taiwind and the like certainly help back-end dev churn out web codebases faster, eliminating a lot of poor souls who have to maintain them afterward! reply eddyg 17 hours agoprevRaku is such a fantastic, feature-laden language. Cool to see it get a (tiny) bit of HN attention! reply librasteve 10 hours agoparentYeah - I think that many people are put off Raku by its imo undeserved reputation ... if languages were clothing, then Raku would be the tank top. reply chris_armstrong 18 hours agoprev> I am a simple sole, I want to reduce the cognitive load in my web projects. The general idea is to go back to the halcyon early days of the web before Netscape dropped the JS-bomb. You know HTML for the layout and CSS for the style. An elegant division of roles. (I'm not quite sure if this is the author's sentiment), but the point shouldn't be to escape JS entirely, but make it into something that can be used in a repeated pattern that works in lockstep with your application, such that you are neither creating custom JS for each page (e.g. React), nor blindly manipulating the DOM (like JQuery). The division of roles between CSS and HTML is an almost contradictory point - your styling and layout should be coupled if you are to impose any meaningful order in your design. If you are rejecting the \"decoupling\" of front-end and back-end that React gives you, then why would you expect to be able to do it between HTML and CSS? reply librasteve 10 hours agoparentthese two points are exactly where I am coming from ... (i) I am deliberately starting from a(n over-) simplified pov to see how far I can get with zero JS ... but as I read the comments above I realise that, for real applications, I will need to \"sprinkle\" some Alpine and Tailwind here and there. (I chose Cro for the user auth.) (ii) Indeed HTML and CSS are in an intricate contradictory dance. In the purest sense I think that my sites should be composed of reusable parts that sit within context. These parts can be composed from roles and may selectively tweak them. The roles convey aspects of the design (colours, typography, containment, scale, size). The parts contain content (text, image, actions), identity and semantic intent. Role mixing in SASS is a small start in this direction. [This is a very weak stab at a very thorny problem ... please let me know if there is any reading that you recommend.] HTMX is a great tool to bring the UI implementation back into the realm of more capable & expressive languages. And to start with a new perspective. My personal preference is Raku, but you may prefer OCAML, Rust, Go, Python, Haskell, Elixir, ROC, Zig ... reply niutech 4 hours agorootparentWhy not just use a pure CSS framework like Spectre.css (including accordions, modals, carousels, tooltips, tabs) and optionally a 166-byte HTMZ where CSS is not enough? You don't need more than 1KB of JS, see PHOOOS at https://kodus.pl. reply librasteve 3 hours agorootparentthat looks very interesting - thanks for the tip reply murkt 11 hours agoprevHTMX, Unpoly and Twinspark are great, can't say anything about Raku. I also really liked an idea about class-less CSS frameworks, as I was frustrated with all the `` and so on. Then I've tried it for one project where I had to integrate Leaflet map on a page. And it's just not possible to do with class-less CSS frameworks. Since then I write `` and have zero problems with that. reply echoangle 11 hours agoparentHow is it not possible? Can you not just use classes for the map tags and keep everything else the same? Or is the problem that the map styles are affected by the classless CSS? reply murkt 11 hours agorootparentYes, map styles are affected by the classless CSS. reply echoangle 11 hours agorootparentBut can’t you just do #map{ foo: initial } #map *{ foo: initial } For the styles that are a problem? That’s maybe 10 properties you need to remove? reply murkt 10 hours agorootparentI guess you're right and \"impossible\" was too strong of a statement. I just didn't feel like debugging those CSS things, especially when adding any new Leaflet plugins, or any other things. So I just switched to a style that I knew will work robustly. reply kolme 8 hours agorootparentprevI find \"removing\" or resetting styles kind of hacky. Another solution might be packing the map in a shadow element so that it doesn't get affected by the page CSS. reply echoangle 6 hours agorootparentAnother thing that one could try: Use a CSS preprocessor to add a :not(.nostyle) To all rules, and then add the nostyle class to all elements you want to exclude. The problem would be that you have to add the class to every single element (and all children) to exclude them, and there seems to be no good way of having selector:not(.class and all children of .class) The Shadow DOM method is really interesting though, I’ve never used it anywhere but it looks like it’s perfect for this application. reply niutech 4 hours agoprevBetter than using HTMX - 48KB of JS - use my PHOOOS technique (https://kodus.pl) with pure HTML out-ouf-order streaming, pure CSS framework Spectre.css (https://niutech.github.io/spectre/) and a sprinkle of 166B (bytes, not KB!) HTMZ (https://leanrada.com/htmz/) for extra interactivity. reply allan_s 5 hours agoprevFor the \"semantic web\" part, what we've been using in my company recently , is the `` tag which are valid html5 so except if there's a really fitting html5 tags (like footer, h1 , main ), we will have `` `` (In case we must have an additional ) so that it's easier to convey the semantic (especially when looking in the dom inspector) for the css we've been using tailwind (and couldn't be happier), but for personnal projects I guess picocss would be a really nice fit with `` tags reply bravura 16 hours agoprevI'm just curious if anyone is using unpoly instead of HTMX? I know that HTMX has a much larger userbase, its main dev is well known and beloved, and the landing page doesn't look web 1.0 With that said, unpoly appears to have a similar feature set and one feature that HTMX doesn't: The ability to create modals without loading a new page. reply jazzypants 16 hours agoparentWith popover[1] and dialog[2], do you really need this from a JS framework? 1: https://developer.mozilla.org/en-US/docs/Web/HTML/Global_att... 2: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/di... reply LaundroMat 12 hours agoparentprevI use it for a rewrite I'm doing because tracking state on both back- and front-end got too cumbersome. I'm using Django, and a big positive of using unpoly is that I can write views like a traditional Django application and have a reactive frontend. It does mean however that heavy pages impact front-end reactivity. So if there's only one tiny element that needs to be updated on the front-end, that takes more time than if the backend would only have to return a template partial (htmx) or some JSON (alpine). Also, the documentation is really a reference. You need it read it front to back a few times to understand unpoly's capabilities. It's strange that there are so few tutorials about it, because it's been around for a while already, it really has its use and it is still under active development. reply tecleandor 7 hours agorootparentHow does it work with Javascript deactivated? In theory it should work that way, isn't it? reply murkt 11 hours agoparentprevI'm using twinspark.js: https://twinspark.js.org/ It has \"actions\" that bridge a need for some client-side interactivity, without resorting to write lots and lots of JS. I have some interactive stuff that don't touch server. Also it's pretty easy to add more actions when needed. And if I need to do something really complex, I can just go and read its source code. Which is really hard to do with unpoly and its really complicated OOP style. reply dagss 11 hours agoparentprevYes, using unpoly, I conclused the way updates work in unpoly to be easier to work with, less need for lots of special endpoints for every kind of interaction than in htmx. But haven't really tried htmx much. reply jakelazaroff 17 hours agoprevThe main example in this article — immediately following a section on semantic HTML — is a list oftags with the hrefs all set to `#` and the hyperlink behavior emulated with htmx. So we’ve taken what would otherwise be a perfectly functional list of links, removed the most important semantic attribute, broken a bunch of behavior and rendered them useless without a ~16kb JavaScript library. …and look, that’s an okay mistake to make — it takes time to learn this stuff — but when you also go out of the way to pine for “the halcyon early days of the web before Netscape dropped the JS-bomb” it makes it difficult to take you seriously. A list of links is, like, the most basic essence of a website; if you aren’t able to make that work without a JavaScript library, what are you even pining for? reply niutech 4 hours agoparentTry HTMZ, which is progressively enhanced and is just using a single . reply jakelazaroff 2 hours agorootparentIt’s not really about the tooling. My point is that people who talk about the “good old days” of the web often (charitably) are unfamiliar with or (cynically) don’t care about the medium they venerate. reply stephankoelle 9 hours agoprevRecently, I’ve been working with Quarkus[1], the quarkus-qute[2] (a type-safe templating engine), and htmx. I found the experience quite positive. Quarkus offers lightning-fast compilation with a hot-reloading Maven wrapper (mvnw), making development seamless. Picking up qute was straightforward, and combining it with htmx, especially with qute’s #fragment support for htmx, felt like a natural fit. [1] https://quarkus.io/ [2] https://quarkus.io/guides/qute reply mediumsmart 13 hours agoprevI do believe this is still in the first half of the rococo phase and that rubble is a long way off so by all means, carry on. reply monkmartinez 19 hours agoprevI hope you are not a sole, and have the complete human embodiment of a soul. Otherwise, yes, I agree. My one off projects use more and more cdn loaded information. I love pico, bulma and htmx. reply fragmede 19 hours agoprev(sole = bottom of shoe soul = spiritual part of a human) reply mixmastamyk 19 hours agoprevPico looks like it may be the classless styler I was looking for, thanks. reply synergy20 18 hours agoparentit's arguably the most starred classless css indeed, it's my default styler reply cantSpellSober 17 hours agoprevWhy Pico specifically? Lots of classless frameworks. > I am a simple sole [sic] `href=#` + JS library + config file + setting routes for each tab button doesn't feel simple reply joshka 16 hours agoparentAs a more backend focused dev (though one who's touched a bunch of different JS / CSS frameworks over the years), seeing Pico in this article seemed pretty nice in comparison to what I've seen in the past. What are some of the similar frameworks that compete in your opinion? reply sergiotapia 18 hours agoprevi know tailwind wasn't created in a vacuum and solves a massive problem. but part of me does feel sad it \"won\" and now we all are forced to deal with it in one way or another. i remember when HTML was clean and semantic, and crawlable. there was a certain respect in that way. reply naasking 5 hours agoparent> i remember when HTML was clean and semantic, and crawlable I'm confused, HTML under tailwind is not any less semantic than under any other CSS toolkit. reply _heimdall 18 hours agoparentprevIt won mainly because it was a much better solution for styling in react than what existed at the time. In react today I'd still probably reach for Tailwind. In any other scenario I'd only reach for it I wasn't comfortable with CSS. reply earthboundkid 18 hours agoparentprevYou can’t remember a time that never existed. reply hecanjog 17 hours agorootparentbootstrap won just a few years ago. something else will win soon. edit: I'm trying to impress the fact that things change. looking at if this library \"won\" or not misses the point. reply dartos 17 hours agorootparentI think 10 years is a good metric for “won” in FE land. React “won” Wordpress “won” Java applets did not Those iPhone web apps that were around before the App Store should’ve, but didn’t. Tailwind undecided reply ducktective 13 hours agorootparentVue did not win, right? Seems like nowadays Svelete took its place, though still undecided imo reply exabrial 17 hours agoprev> While HTMX is well suited to 90% of this, if you are building a webapp like FaceBook […], then it’s not for you I completely completely disagree. This is all you need. reply murkt 11 hours agoparentAgree, Facebook can be easily done with HTMX and similar things. How would you do that with Figma, YouTube, etc? reply Nathanael_M 5 hours agorootparentFigma is mostly WASM, no? reply murkt 3 hours agorootparentThat's not too HTMX-friendly technology, is it? reply consteval 1 hour agorootparentWASM isn't friendly with any other web technologies because it's compiled code. You can't interact with the DOM at all, so if you're doing rendering via WASM it's always gonna be in it's own little island. reply fny 18 hours agoprev [–] I have been in web dev since 1999, and recently switched to a backend role. I am never going back. The endless thrashing on front end borders on psychosis. Something is clearly broken, and that thing is my feeble mind. reply giancarlostoro 16 hours agoparentFor me I'll never go back to normal front-end work after doing Blazor with C#. It's the nice bits of React (reusable code) but the flexibility of never having to decide between front-end only focused code, or some back-end powered AJAX style code, that is transparently rendered for you. You never have to touch JavaScript, but there's ways to do so. WASM is the best thing that ever happened to the web. When other major web frameworks adopt template engines that compile to WASM, we will see a shift in focus on JS heavy frameworks, and just people building things instead of fighting and trying to remember all the weird quirks of JavaScript. reply m_fayer 6 hours agorootparentGo one step further. The programming model behind Blazor server is criminally under appreciated. Spa-style interactivity without a need for an API is great. Optional shared state between clients and effortless push-to-client is… basically magic as far as I’m concerned. reply alabhyajindal 10 hours agorootparentprevCan you share your experience with other frameworks beside Blazor that you had a good experience with? I have seen Blazor and it looks good but I have an aversion to anything Microsoft. reply endemic 4 hours agorootparentLiveView in Phoenix is a similar experience, and I enjoyed (briefly) using it. https://pragmaticstudio.com/ has a good introduction. reply alabhyajindal 2 hours agorootparentThanks! I took their Ruby and Rails courses and loved it. Great teachers! reply shortrounddev2 5 hours agorootparentprevYou should try it out; microsoft makes good contributions to open source software, and C# is, in my view, the best (or maybe the only good) backend language put there reply shortrounddev2 5 hours agorootparentprevHow do you do dom manipulation in wasm code? reply JodieBenitez 14 hours agoparentprevOne of the frontend problems is that too much frontend engineers think a web app should be a single page app. A lot of apps don't need to be that. Server-side endpoints and templating + lightweight vanilla JS and framework-agnostic web components works and is now even easier than ever. reply alex_suzuki 11 hours agorootparentIt‘s a kind of collective hallucination. It‘s affected me too. I just recently decided to go back in time and wrote a web app using Flask, all rendered server side, with a minimum of JS and JSON endpoints for a bit of client-side interactivity (e.g. autocompleting an input field). Very fast, iterative work. No duplication of data classes, no weird caching issues… Felt like a breath of fresh air, sadly. Have we witnessed a full turn of the wheel? reply JodieBenitez 9 hours agorootparentThat's a good old case of \"the right tool for the job\". But let's be honest: the frontend dev rat race had the effect to make the base web technologies better. Better CSS (variables, nesting, effects, media support, etc), better JS (webcomponents), useful libs available as wasm and so on... all good things that can be used natively and allow us to make better MPAs. reply prisenco 13 hours agorootparentprevI've seen so many marketing websites written in React. It's so confusing. reply foul 9 hours agorootparentprevCommercial-grade applications need some grade of offline operativity that classic HATEOAS can't provide, often target users who won't learn or can't really use a web browser (tabs, back button, extensions, bookmarks besides whatever they can fit in the bar, etc), often want to give a UX which abuses and deforms a web browser by design. Yeah, FE engineers may all look like dumbasses with macbooks (i don't think this of them!), but they have real problems to solve. reply JodieBenitez 6 hours agorootparentNot denying that, but just because I browse the web I'm pretty sure not all websites with \"modern\" frontend stacks have modern frontend problems. reply foul 5 hours agorootparentThat's for sure, for example any blog software (a few SSGs included) is a failure for WWW and for itself (most of the interaction between author and reader must be mediated with third-party or big cloud to avoid spam and scam). reply shortrounddev2 5 hours agorootparentprevI think most people choose SPAs because they also maintain the frontend state of a an application. If you're just serving templated html from a backend, you're probably also doing vanilla javascript which is it's own kind of hell. How do you manage the transition between one view and another on the same page? How do you pass state between two pages? I think webcomponents solve some of this but there's a lot of react developers out there who don't have the requisite software engineering skills to handle this reply consteval 1 hour agorootparent> How do you manage the transition between one view and another on the same page? HTMX solves this, and it's mostly automatic. You get some nice transitions. > How do you pass state between two pages? You don't. You submit some data to the server, it generates HTML dynamically, you replace HTML on the frontend or redirect all together, you're done. The state is on the server. reply JodieBenitez 1 hour agorootparentprev> If you're just serving templated html from a backend, you're probably also doing vanilla javascript which is it's own kind of hell. How so ? Vanilla JS has never been easier and more powerful. > How do you pass state between two pages? Why is it even considered to be a problem ? This is basic stuff. All web frameworks since the 90s have had this concept of sessions where you can store data that will be available across pages. reply nsonha 11 hours agorootparentprevclear boundary is good, I don't understand why people like to trash their back-end with templates, MVC and all the \"view\" nonsense. They are just shifting complexity from a place they feel uncomfortable, to where they do, but at the cost of destroying their own nirvana. Certainly one way, but hardly the only one. reply JodieBenitez 11 hours agorootparent> I don't understand why people like to trash their back-end with templates, MVC and all the \"view\" nonsense. Because the tooling has been here for decades and it works damn fine. No need to trash the frontend with ever changing piles of dependency and build nonsense. reply nsonha 10 hours agorootparent> works damn fine when? View code in the back-end always looks like crap and never able to achieve the interactivity the front-end can. People wrestling with it was the reason SPA was born. There was no dictator forcing people to move on from such \"awesome\" working \"damn fine\" pipelines to modern front-end for no reason. reply ejflick 10 hours agorootparent> View code in the back-end always looks like crap That's probably the fault of the people writing it. People can also write crap looking code using frameworks too. > and never able to achieve the interactivity the front-end can. That's what small sprinkles of javascript is for. You don't need incredibly beautiful interactivity in every pixel. reply Hammershaft 17 hours agoparentprevI think the churn and misery of front end development is the inevitable result of 30 years of path dependence on technologies that were either substandard to begin with (javascript, css) or were never designed for what they're used for (html, css). reply cjs_ac 10 hours agoparentprevAt my day job, we use something called ClickUp for task tracking. Watching the UI elements load one at a time reminds me of those greybeard stories about using vi over 300 baud modems. ClickUp is supposed to be productivity software. HTMX pretty much enforces a 'one user action, one HTTP request, one HTTP response' pattern, which is necessary for a usable UX for users who can't see the data centre from their office window. reply cosmez 15 hours agoparentprev> I am never going back. For some reason, it sounds like someone being held against their will, which is exactly how I feel about it too reply movedx 18 hours agoparentprevLike the article's writer, I remember the Netscape days and when JS came into existence. I was only a kid back then, like 9 years old, and was _just_ getting into coding in C and playing around with HTML/CSS. The results I produced, as you might have imagined, were akin to the results a toddler would produce given a wet loaf of bread to play with: mess everywhere. Anyway, back then the HTML was simple (but it was tables and spacer GIFs ha ha!) and the CSS even simpler. Now it's just such a chaotic mess that people actually think Web Assembley is a good idea; as if somehow they've invented something new: the sandbox and the machine code. Like running a process at the OS level and making _that_ better, more secure, etc. wasn't the right path. HTMLx is as close to the UI as I'll come. Everything else is an absolute headache to get into. reply dudus 14 hours agoparentprevI must be a masochist because I love FE frameworks. I love learning them and comparing them. I love how they are changing all the time and things are always improving somehow. What was great 10 years ago now are ridiculous to think about, What is great now will be completely forgotten in a couple years time. It's beautiful and maddening at the same time. My favorite stack right now is Astro + SolidJS. Maybe it'll change next week. Who knows? reply stavros 13 hours agorootparentI don't know if you're a masochist, but I bet you aren't writing large apps. At work, the last thing I want is the technology I'm using to be obsolete in two months. reply librasteve 11 hours agoparentprev [–] this ... multiplied by the high exposure of front end devs to client side requests for bells and whistles and the web studio demands on throughput reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This post is part of a series focused on reducing cognitive load in web projects by using simpler web development practices, specifically HTMX, Raku, and Cro for backend HTML assembly and RESTful APIs, eliminating the need for JavaScript in dynamic content.",
      "HTMX is versatile for most projects but not suitable for complex web apps; Pico CSS is recommended for styling due to its emphasis on semantic HTML, which simplifies the development process.",
      "The post includes code examples using Pico CSS to rebuild HTMX examples for Raku/Cro, with the final code available on GitHub, and highlights Pico CSS's built-in dark mode."
    ],
    "commentSummary": [
      "HTMX is highlighted for its ability to simplify web applications by keeping state management server-side, reducing the need for extensive client-side JavaScript (JS).",
      "Users discuss the trade-offs between HTMX and modern frontend frameworks like React, noting that HTMX can be more efficient for simpler business applications but may lack some UI functionalities.",
      "The conversation includes comparisons with other technologies like Next.js, Blazor, and Phoenix LiveView, emphasizing that HTMX offers a different approach that can be beneficial for specific use cases, particularly for fullstack developers who prefer backend logic."
    ],
    "points": 177,
    "commentCount": 136,
    "retryCount": 0,
    "time": 1725825080
  },
  {
    "id": 41482661,
    "title": "Jd – JSON Diff and Patch",
    "originLink": "https://github.com/josephburnett/jd",
    "originBody": "JSON diff and patch jd is a commandline utility and Go library for diffing and patching JSON and YAML values. It supports a native jd format (similar to unified format) as well as JSON Merge Patch (RFC 7386) and a subset of JSON Patch (RFC 6902). Try it out at http://play.jd-tool.io/. Installation To get the jd commandline utility: run brew install jd, or run go install github.com/josephburnett/jd@latest, or visit https://github.com/josephburnett/jd/releases/latest and download the pre-built binary for your architecture/os, or run in a Docker image jd(){ docker run --rm -i -v $PWD:$PWD -w $PWD josephburnett/jd \"$@\"; }. To use the jd web UI: visit http://play.jd-tool.io/, or run jd -port 8080 and visit http://localhost:8080. Command line usage Usage: jd [OPTION]... FILE1 [FILE2] Diff and patch JSON files. Prints the diff of FILE1 and FILE2 to STDOUT. When FILE2 is omitted the second input is read from STDIN. When patching (-p) FILE1 is a diff. Options: -color Print color diff. -p Apply patch FILE1 to FILE2 or STDIN. -o=FILE3 Write to FILE3 instead of STDOUT. -set Treat arrays as sets. -mset Treat arrays as multisets (bags). -setkeys Keys to identify set objects -yaml Read and write YAML instead of JSON. -port=N Serve web UI on port N -f=FORMAT Produce diff in FORMAT \"jd\" (default), \"patch\" (RFC 6902) or \"merge\" (RFC 7386) -t=FORMATS Translate FILE1 between FORMATS. Supported formats are \"jd\", \"patch\" (RFC 6902), \"merge\" (RFC 7386), \"json\" and \"yaml\". FORMATS are provided as a pair separated by \"2\". E.g. \"yaml2json\" or \"jd2patch\". Examples: jd a.json b.json cat b.jsonjd a.json jd -o patch a.json b.json; jd patch a.json jd -set a.json b.json jd -f patch a.json b.json jd -f merge a.json b.json Command Line Option Details setkeys This option determines what keys are used to decide if two objects 'match'. Then the matched objects are compared, which will return a diff if there are differences in the objects themselves, their keys and/or values. You shouldn't expect this option to mask or ignore non-specified keys, it is not intended as a way to 'ignore' some differences between objects. Library usage Note: import only release commits (v1.Y.Z) because master can be unstable. import (\"fmt\"jd \"github.com/josephburnett/jd/lib\" ) func ExampleJsonNode_Diff() {a, _ := jd.ReadJsonString(`{\"foo\":\"bar\"}`)b, _ := jd.ReadJsonString(`{\"foo\":\"baz\"}`)fmt.Print(a.Diff(b).Render())// Output:// @ [\"foo\"]// - \"bar\"// + \"baz\" } func ExampleJsonNode_Patch() {a, _ := jd.ReadJsonString(`[\"foo\"]`)diff, _ := jd.ReadDiffString(`` + `@ [1]` + \"\" + `+ \"bar\"` + \"\")b, _ := a.Patch(diff)fmt.Print(b.Json())// Output:// [\"foo\",\"bar\"] } Diff language A diff is zero or more sections Sections start with a @ header and the path to a node A path is a JSON list of zero or more elements accessing collections A JSON number element (e.g. 0) accesses an array A JSON string element (e.g. \"foo\") accesses an object An empty JSON object element ({}) accesses an array as a set or multiset After the path is one or more removals or additions, removals first Removals start with - and then the JSON value to be removed Additions start with + and then the JSON value to added EBNF Diff ::= ( '@' '[' ( 'JSON String''JSON Number''Empty JSON Object' )* ']' '' ( ( ' 'JSON Value' '' )+'+' 'JSON Value' '' ) ( '+' 'JSON Value' '' )* )* Examples @ [\"a\"] - 1 + 2 @ [2] + {\"foo\":\"bar\"} @ [\"Movies\",67,\"Title\"] - \"Dr. Strangelove\" + \"Dr. Evil Love\" @ [\"Movies\",67,\"Actors\",\"Dr. Strangelove\"] - \"Peter Sellers\" + \"Mike Myers\" @ [\"Movies\",102] + {\"Title\":\"Austin Powers\",\"Actors\":{\"Austin Powers\":\"Mike Myers\"}} @ [\"Movies\",67,\"Tags\",{}] - \"Romance\" + \"Action\" + \"Comedy\" Cookbook Use git diff to produce a structural diff: git difftool -yx jd @ -- foo.json @ [\"foo\"] - \"bar\" + \"baz\" See what changes in a Kubernetes Deployment: kubectl get deployment example -oyaml > a.yaml kubectl edit deployment example # change cpu resource from 100m to 200m kubectl get deployment example -oyamljd -yaml a.yaml output: @ [\"metadata\",\"annotations\",\"deployment.kubernetes.io/revision\"] - \"2\" + \"3\" @ [\"metadata\",\"generation\"] - 2 + 3 @ [\"metadata\",\"resourceVersion\"] - \"4661\" + \"5179\" @ [\"spec\",\"template\",\"spec\",\"containers\",0,\"resources\",\"requests\",\"cpu\"] - \"100m\" + \"200m\" @ [\"status\",\"conditions\",1,\"lastUpdateTime\"] - \"2021-12-23T09:40:39Z\" + \"2021-12-23T09:41:49Z\" @ [\"status\",\"conditions\",1,\"message\"] - \"ReplicaSet \\\"nginx-deployment-787d795676\\\" has successfully progressed.\" + \"ReplicaSet \\\"nginx-deployment-795c7f5bb\\\" has successfully progressed.\" @ [\"status\",\"observedGeneration\"] - 2 + 3 apply these change to another deployment: # edit file \"patch\" to contain only the hunk updating cpu request kubectl patch deployment example2 --type json --patch \"$(jd -t jd2patch ~/patch)\"",
    "commentLink": "https://news.ycombinator.com/item?id=41482661",
    "commentBody": "Jd – JSON Diff and Patch (github.com/josephburnett)173 points by smartmic 23 hours agohidepastfavorite28 comments bugtodiffer 4 hours agoJust use gron! greppable json It turns JSON to JS syntax. it\"s perfect for these tasks. https://github.com/tomnomnom/gron reply _flux 8 hours agoprevI realize it's nice to use short names for applications, but couldn't this also be have been called jdiff? I feel like the two-letter tool name is exercising developer memory more than strictly required. I have 42 two-character binaries in /usr/bin. Of course, that's still only about 5% of the available two-alphabet names.. The developer can always choose to use a shorted local alias for commonly used tools. That being said, I wonder if this is much better than difftastic that is more general purpose, but tree-aware? I suppose this one wouldn't care about JSON dictionary key ordering, at least. reply josephburnett 1 hour agoparent> couldn't this also be have been called jdiff? I feel like the two-letter tool name is exercising developer memory more than strictly required. Yeah, in retrospect I should have given this a longer name. I was going for a natural fit with `jq`. ¯\\_(ツ)_/¯ > I wonder if this is much better than difftastic that is more general purpose, but tree-aware? There are quite a few good tree-aware JSON diff tools out there. But I wanted one that could also be used for patching. I've tried to maintain the invariant that all diffs can be applied as patches without losing anything. And I also wanted better set (and multi-set) semantics, since the ordering of JSON arrays so often isn't important. reply mejutoco 4 hours agoparentprevYou have a point of course, but I find it funny that the path is not /users/binaries and, instead, it is a similar abbreviation. In a way, it is a sort of seo race for tool devs. reply spencerchubb 4 hours agorootparentusr stands for user system resources reply Tsiklon 3 hours agorootparentThis is a backronym. /usr is the original user home directory location on classic unix. https://www.bell-labs.com/usr/dmr/www/notes.html reply conkeisterdoor 4 hours agorootparentprevTIL after so many years that /usr isn't an abbreviation of \"user\". \"UNIX/user system resources\" makes a lot more sense in retrospect. Guess I should have RTFM a long time ago! reply dunham 3 hours agorootparentIt looks like that's a newer interpretation than the original: > As such, some people may now refer to this directory as meaning 'User System Resources' and not 'user' as was originally intended. https://tldp.org/LDP/Linux-Filesystem-Hierarchy/html/usr.htm... reply keybored 8 hours agoparentprevThe real headscratchers are the tools that have a proper name, a shortened name, and a command name: - Stacked Git - Shortened: stgit - Command: stg Lots of “stgit: command not found” ensues. reply Cu3PO42 12 hours agoprevI have recently used jd with great success for some manual snapshot testing. At $work we did a major refactor of $productBackend, so I saved API responses into files for the old and new implementation and used jd (with some jq pre-processing) to analyze the differences. Some changes were expected, so a fully automatic approach wasn't feasible. This uncovered a few edge cases we likely wouldn't have caught otherwise and I'm honestly really happy with that approach! One thing I would note is that some restructurings with jq increased the quality of the diff by a lot. This is not a criticism of jd, it's just a consequence of me applying some extra domain knowledge that a generic tool could never have. reply josephburnett 1 hour agoparent> I would note is that some restructurings with jq increased the quality of the diff by a lot. I would really like to know more about these restructurings. Would you mind dropping me an example here or at https://github.com/josephburnett/jd/issues please? There are somethings I won't do with jd (e.g. generic data transformations) but I do plan to add some more semantic aware metadata with the v2 API. Also, I'm glad this tool helped you! Made my day to here it :) reply zachromorp 9 hours agoprevHello! I sometimes have big json files to diff. Its content is a big array with complex object inside. The problem I have with all the diff tools I tried (this one included) is that it can't detect if element is missing. When that happens, it computes a very long diff where it could have just said \"element is missing at index N\". Are you aware of a tool without such caveat ? Thanks reply josephburnett 1 hour agoparent> When that happens, it computes a very long diff where it could have just said \"element is missing at index N\". That's exactly the problem addressed by this issue: https://github.com/josephburnett/jd/issues/50. And I've created a new v2 format to address this and other usecases. The v2 API will compute the longest common subsequence of two arrays and structure the diff around that (a standard way of producing a minimum diff). I've just released jd 1.9.1 with the `-v2` flag. Would you mind trying one of your use cases to see if the diff looks any better? I should say something exactly like that \"@ (some path) - (some element)\". reply eequah9L 4 hours agoparentprevI'm probably missing something obvious, but diff seems to be handling this just fine? # diff -uOne feature I’ve yet to see is applying jq query syntax to the jsons before the diff Will you please add this as a feature request? https://github.com/josephburnett/jd/issues. I would like to hear more about how you would use it. reply iwwr 9 hours agoprevIs this useful for large json files, on the order of GiB? reply agumonkey 6 hours agoprevWe're not that far from jsolog. reply dgelks 11 hours agoprevWas just using it to compare two massive json files, super performant and useful compared to using jq reply mentalgear 10 hours agoprevvery very nice, just the tool I needed for the current task - and here it is! :) reply gvv 9 hours agoprevnice, super useful for debugging API responses. Would be nice to be able to use it as a VSCode extension! reply josephburnett 1 hour agoparent> Would be nice to be able to use it as a VSCode extension! I've added support to use jd as a Git diff engine: https://github.com/josephburnett/jd?tab=readme-ov-file#use-g.... Can you configure VS Code use a custom command to show diffs? reply g_dhoot 17 hours agoprevThis looks neat and useful! reply surfingdino 9 hours agoprev [–] Use it daily to make JSON payloads more readable. One of Open Source true gems. reply swah 7 hours agoparent [–] I use jq for that! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "JSON diff and patch (jd) is a command-line utility and Go library for comparing and modifying JSON and YAML values, supporting multiple formats like jd, JSON Merge Patch (RFC 7386), and a subset of JSON Patch (RFC 6902).",
      "Installation options include Homebrew, Go install, GitHub releases, and Docker, with a web UI available at jd-tool.io or locally via `jd -port 8080`.",
      "Key features include color diff, patch application, output redirection, array handling as sets or multisets, YAML support, and format translation."
    ],
    "commentSummary": [
      "Jd is a tool for diffing and patching JSON files, created by Joseph Burnett, and aims to complement `jq` by handling JSON arrays' ordering.",
      "The tool has been updated to address issues with detecting missing elements in large JSON files, with a new v2 API that computes the longest common subsequence of two arrays.",
      "Users have found Jd useful for tasks such as manual snapshot testing, debugging API responses, and comparing massive JSON files, with suggestions for further improvements like adding jq query syntax and creating a VSCode extension."
    ],
    "points": 173,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1725824916
  },
  {
    "id": 41488353,
    "title": "Synthetic diamonds are now purer, more beautiful, and cheaper than mined",
    "originLink": "https://worksinprogress.co/issue/lab-grown-diamonds/",
    "originBody": "A diamond – from the Greek ἀδάμας (adámas), meaning unconquerable – is a three-dimensional cubic or hexagonal lattice of carbon atoms. As its bonds are strong and its atoms packed closely together, diamond is the hardest natural material and the least compressible. Diamonds have high thermal conductivity and high electrical resistivity, but can be combined with small amounts of nitrogen, phosphorus, and boron and made into semiconductors. A diamond’s surface does not easily stick to other materials, but moves smoothly against them. Diamonds are chemically inert and not toxic to living tissue. In their pure form, diamonds are colorless and have a high dispersion of light, but the presence of certain impurities can add magnificent color to diamond gemstones. In nature, it takes billions of years to form a diamond. Most of the diamonds nature produces are too impure for jewelry or high-tech industry, and extracting them is costly and dirty. In the lab, diamonds can be made faster, purer, and cheaper, overcoming these problems and making possible new uses for diamonds that were previously unattainable. Scientists first manufactured diamonds in laboratories in the 1950s, imitating the conditions under which diamonds were produced in nature. The diamonds produced were initially small and impure and so useful only in low-tech industrial products such as abrasives and lubricants. Since then, diamond manufacturing technology has progressed: the generation process has become more controlled, new methods have been invented, and better catalysts have been discovered. Diamonds grown in the lab are now cheaper than mined diamonds and have superior physical, optical, chemical, and electrical properties. Consequently, they dominate the industrial market. In the past decade, diamond manufacturing technology progressed so much that it is now possible to mass-produce jewelry-quality diamonds in the lab. These lab diamonds are cheaper and more beautiful than mined diamonds. A perfectly cut, flawless lab diamond costs a fraction of the price of a mined diamond of lesser quality. Lab diamonds are a testament to the principle that what nature can do, man is capable of doing better. How diamonds are made In 1773, in a series of experiments in the gardens of the Louvre, the French chemist Antoine-Laurent de Lavoisier placed diamonds into a glass bell jar and then immersed the base of the jar in water.1 Using a large, biconvex lens that measured 89 centimeters and weighed over 70 kilograms – the Palais-Royal burning glass – Lavoisier would concentrate the sun’s rays onto a diamond. Within minutes, the diamond on which the lens had been focused would evaporate completely; the others would blacken and lose mass. In his Memoir on the Destruction of the Diamond by Fire Lavoisier recounts that these experiments had been conducted in ‘clear and serene’ weather, but on 14 August 1773 a light fog reduced the sun’s power. In the more moderate heat, the diamonds in the jar appeared ‘precisely as if they had been coated with black smoke in the flame of a lamp’, causing Lavoisier to conclude that diamond is susceptible to be reduced to coal – charbon – in certain circumstances. Overturning the jar and pouring inside limewater before the air escaped, Lavoisier observed that a chalky substance was precipitated when both diamonds and coal were burned. However, he did not conclude that diamond and coal are the same substance, only that both are combustible bodies. In his 1789 Elementary Treatise on Chemistry, Lavoisier proposed that there exist basic chemical building blocks, of which carbon – from the Latin carbo, meaning coal – is one. He would not live long enough to discover that diamond is an allotrope (a different physical form of the same element) of carbon. In the senseless bloodshed that characterized the French Revolution, Lavoisier, who had also been a tax farmer, was accused of adulterating tobacco and defrauding the state. He was guillotined on 8 May 1794. An apocryphal story holds that in response to an appeal to spare his life so that he could continue his experiments, the Revolutionary judge Jean-Baptiste Coffinhal declared that ‘The Republic needs neither scholars nor chemists; the course of justice cannot be impeded’. Coffinhal himself would be executed for his brutality three months later, but the damage had already been done. As Lavoisier’s colleague, the mathematician and astronomer Joseph-Louis Lagrange, mourned, ‘It took them only an instant to cut off this head, and one hundred years might not suffice to reproduce its like’. Instead, the discovery that diamond is carbon would be made by the English chemist Smithson Tennant, in 1796. Tennant heated ‘two grains and a half’ (162 milligrams) of diamonds in a closed gold tube containing niter (potassium nitrate, KNO3). When heated, the potassium nitrate released oxygen, increasing its concentration, so that the diamonds burned faster, and at a lower temperature. Tennant kept the gold tube ‘in a strong red heat’ for an hour and a half, after which the diamonds were completely burned. The reaction produced ‘fixed air’ (carbon dioxide). Extracting and measuring the carbon dioxide produced, Tennant noted that it had occupied the space of ‘a little more than 10.1 ounces [354 milliliters] of water’. In the Memoirs of the Royal Academy of Sciences for 1781, Lavoisier had published the results of a series of experiments in which he identified that ‘carbonic acid’ (carbon dioxide) contains 28.22 parts of ‘charcoal’ (carbon) for every 71.78 parts of ‘acidifying principle’ (oxygen) and, at similar temperatures and pressures to the air in Tennant’s laboratory, determined its weight to be 0.695 Parisian grains per cubic Paris inch (1.86 grams per liter). Using these measurements, Tennant computed that the carbon dioxide produced by burning an equal weight of charcoal ‘ought to occupy very nearly the bulk of 10 ounces [352 milliliters] of water’.2 On the basis that the quantity of carbon dioxide produced from burning equal weights of diamond and charcoal ‘does not differ much’, Tennant concluded that diamond ‘consists entirely of charcoal’. This astonishing result would be met with skepticism for two decades, until other scientists reproduced his results. Tennant had demonstrated that diamond is an allotrope of carbon, but diamond’s method of formation remained unknown. Motivated by the prospect of discovery and the allure of riches, nineteenth-century alchemists tried to turn charcoal into diamond by various means, including evaporation, explosions, and intense heat. None met with success. The most important of these researchers was the French chemist Henri Moissan. In 1886, Moissan succeeded in isolating the chemical element fluorine by means of electrolysis; for this, and for his research into high-temperature chemistry, he would win the 1906 Nobel Prize in Chemistry. Considering the problem of synthesizing diamonds, Moissan tested different forms of carbon – burnt sugar, charcoal, gas carbon, and carbon black – under the high heat of an electric arc furnace. In each of these experiments, the carbon would be converted into graphite, a different allotrope of carbon. Moissan examined the geological records from sites where diamonds had been discovered – the alluvial sands of Brazil, the diamond pits of southern Africa, and the Canyon Diablo meteorite in Arizona – and made two important observations. First, he noticed that the diamonds of southern Africa, which were contained in rock that had been ‘thrust up . . . from the deeper layers of the earth’, must have formed deep underground – ‘and thus pressure must have played a part at the moment of formation’. Second, the diamonds that were discovered had not been attached to the ground in which they were buried; instead, they appeared as if they had formed ‘in the midst of a liquid’. Carbon, including diamond, dissolves easily in iron, and iron is frequently found near diamond sites, including the Canyon Diablo meteorite, where two small diamonds had been found in the middle of an iron mass. ‘In this case’, Moissan noted, ‘nature seems to have been caught in the act’. The Canyon Diablo meteorite suggested how diamonds might form. Molten iron that contains dissolved carbon might cool suddenly, ‘due to some cause or other’, causing it to contract ‘violently’, pressurizing the dissolved carbon to crystallize into diamonds. To replicate this process in his lab, Moissan heated crucibles containing iron and burnt sugar to 3,000°C and then quenched them in cooler substances: water, iron filings, and molten lead. In the residues, under a microscope, he discovered dense fragments of carbon, some black, others transparent, that scratched rubies, combusted in oxygen, and had ‘a very distinct crystalline appearance’. Moissan measured the relative density of these fragments to be 3.5 (compared to water) and observed that when burnt in oxygen at 900°C they yielded 3.666 grams of carbon dioxide per gram of substance. ‘These properties’, he declared, ‘are possessed by the natural diamond alone’. Moissan attempted to synthesize diamonds by quenching hot carbon in water. IMAGE Source: Alamy. Moissan would go to his death in 1907 believing that he had synthesized diamonds. Other chemists, including Henry Louis Le Chatelier, were skeptical and his experiments did not replicate. The German chemist Otto Ruff and the English engineer Sir Charles Parsons separately believed that they had reproduced his results, only to repudiate their findings after more careful examination. Moissan’s widow would later tell Parsons that she believed one of Moissan’s assistants had introduced natural diamond fragments into the experiments ‘in order to please the old man’. Nonetheless, despite his failure, Henri Moissan was correct to believe that diamonds could be synthesized under a combination of high pressure and high temperature, catalyzed by a metal solvent.3 High pressure, high temperature In 1950, the General Electric Research Laboratory in Schenectady, New York, assembled a consortium of chemists, physicists, and engineers to form Project Superpressure, an effort to synthesize diamonds in the lab. General Electric, the industrial conglomerate founded by the inventor Thomas Edison, had a strong tradition of industrial research and the cost of importing diamonds, used to draw out the tungsten filament wire in light bulbs, was a serious concern. To support its research, General Electric commissioned a hydraulic press that cost $125,000,4 stood two storeys high, and was capable of pressing 1,000 tons. Four years of intense experimentation followed, during which Project Superpressure exhausted all of its original research budget and two additional funding allocations. On the third occasion that the project manager asked for more money, General Electric’s research managers, having seen no tangible results and skeptical of future success, almost unanimously voted to discontinue their support. Guy Suits, General Electric’s director of research, overruled them and approved the funds. Shortly after, Project Superpressure would make its breakthrough. On 16 December 1954 chemist and Project Superpressure team member Howard Tracy Hall placed two diamond seed crystals into a graphite tube with iron sulfide, capped with tantalum disks. The tube’s graphite would act both as a carbon source and, when electric current was applied to the tantalum disks, a resistance heater.5 He then placed this cylindrical device into a pressure chamber of his own design. Hall’s pressure chamber, now known as a belt press, consisted of two opposing tapered anvils that compressed the reaction cell from above and below, with the sides supported by prestressed steel bands. The belt press. IMAGE Source: Howard Tracy Hall, ‘Diamond Synthesis’, US patent US2947608A Hall describes how when he envisioned this device, his colleagues ‘felt negatively about it’. His proposal to build a prototype, which would have cost General Electric less than $1,000,6 was rejected and he was refused time in the machine shop to build it. ‘I fretted about this for a time’, he wrote, ‘and then decided on a sub-rosa solution. Friends in the machine shop agreed to build the Belt, unofficially, on slack time. This took several months. Ordinarily, it would have taken only a week.’ A practicing Mormon, whose church and large family left him little time to socialize with his colleagues, Hall attri­buted the refusal to build his design and other slights to religious prejudice. When his prototype belt apparatus proved capable of attaining high pressures and high temperatures, Hall requested that its critical components be reconstructed in carboloy (cobalt-cemented tungsten carbide). Once again, his request was refused and it was not until his former supervisor intervened that he obtained permission to buy the carbide components. To further compound Hall’s sense of injustice, demand for Project Superpressure’s thousand-ton press was so high that Hall’s improved pressure chamber was ‘relegated’ (in his words) to an ‘ancient’ press, dating from the turn of the twentieth century, that was only capable of pressing 400 tons and still ran on water pressure.7 Hall would later describe how this press ‘leaked so badly that rubber footwear, mop, and bucket were standard accessory equipment’. Using this antique press, Hall managed to compress his pressure chamber to ten gigapascals (about 100,000 times the pressure of the atmosphere) and to heat the reaction chamber to 1,600°C. The experiment ran for 38 minutes. Hall had created diamonds: I broke open a sample cell after removing it from the Belt. It cleaved near a tantalum disk used to bring in current for resistance heating. My hands began to tremble; my heart beat rapidly; my knees weakened and no longer gave support. My eyes had caught the flashing light from dozens of tiny triangular faces of octahedral crystals that were stuck to the tantalum and I knew that diamonds had finally been made by man. General Electric reproduced Hall’s results 20 times over the next two weeks and on 15 February 1955 announced to the rest of the world that it had created the first diamonds in the lab. In its press release, it implied that the diamonds had been created in their new thousand-ton press. Hall’s reward was a modest salary increase – from $10,000 to $11,000 per year – and a ten-dollar savings bond. He resigned from General Electric to become a full professor at Brigham Young University. The diamonds that Hall’s process produced were tiny, measuring a few thousandths of a millimeter in diameter: just enough to glint. Diamonds of this size are too small for jewelry, but extremely useful in industry – at the time, they were (and still are) used for sawing, grinding, and polishing metal, drawing wire, and stamping precision components. General Electric’s industrial diamonds initially cost more than mined diamonds, but they quickly proved to be superior. Unlike in nature, the diamonds’ growth was precisely managed, and their shape and regularity could be made to order. Hall, meanwhile, was forbidden from disclosing details about the belt press he invented, or using it to pursue further research into high-pressure chemistry, because of a secrecy order imposed by the United States Department of Commerce. During the Second World War, the supply of diamonds had been a source of anxiety for both the Allied and Axis powers. The United States, which did not have a domestic supply of diamonds, was dependent on the De Beers cartel for the diamonds necessary for its industrial production, and the business model of De Beers was to drive up prices by artificially restricting the world’s supply. A 1944 memorandum to the United States attorney general declared that ‘the United States is paying monopoly prices for an essential material needed in wartime production’ and that if De Beers were an American company ‘there would be no question’ that it had violated antitrust laws. De Beers was headquartered in London and did not maintain a presence in the United States, so there was little the Justice Department could do. In an effort to maintain American technological superiority during the Cold War, the Commerce Department desired to prevent other countries from obtaining diamond-making technology, even if it meant restricting research within the United States too. Hall attempted to get around this problem by designing a better press. The belt press invented at General Electric transmitted pressure to the reaction cell on one axis only; he determined it could be improved by applying pressure from multiple directions. In 1957, Hall built a tetrahedral press that consisted of four triangular-faced anvils that pressured each of the sides of a tetrahedral reaction cell, and submitted a paper describing its design to the Review of Scientific Instruments. The Commerce Department placed a secrecy order on this design too and Hall was obligated to write to every person who had seen the tetrahedral press or requested a description of the device to inform them of this directive. ‘In exasperation, I considered giving up the field of high pressure’, Hall would later write. Many of the scientists to whom Hall communicated this directive thought the secrecy order was outrageous and complained to the Commerce Department. Other government agencies agreed and soon after, the Defense Department ordered the Commerce Department to abandon the secrecy orders on the belt and tetrahedral presses. The tetrahedral press was an improvement on the belt press, but more efficient still was the cubic press, which Hall, like other industrialists making diamonds around the same time, would find more practical. A cubic press uses six hydraulically powered anvils to put pressure on the six faces of a cubic reaction cell from each side. Compared to a tetrahedron, a cube has a smaller surface area to volume ratio. Because pressure is force divided by area, this means that the cubic press can achieve the pressures required to grow diamonds with less force. Scientists at the Siberian branch of the Academy of Sciences of the Soviet Union pursued a different method of manufacturing diamonds under high pressure and high temperature. Instead of using a hydraulic press, the Soviets made use of a split-sphere apparatus.8 This consists of a cylindrical reaction cell that is surrounded by six inner anvils that fit together to form an octahedron. These inner anvils are, in turn, surrounded by eight outer anvils that fit together to form a sphere. Surrounding the sphere is a thin reservoir. A compact pressure pump injects hydraulic oil into the reservoir, pressuring the sphere from all directions at once. This causes the anvils to squeeze together, magnifying the pressure transmitted to the reaction cell at the center of the sphere (as the force is concentrated onto a smaller area). A graphite heater in the reaction cell provides the necessary heat. Though the belt press, the cubic press, and the split-sphere apparatus have different pressure-delivery mechanisms, the chemical reaction that occurs in the high-pressure cell is the same: graphite is converted into diamond under high pressure and high temperature, catalyzed by a metal solvent and a small diamond ‘seed’ on which the new diamond precipitates. Carbon atoms have four electrons in their outermost shell. At low pressures and temperatures, graphite – a solid that consists of two-dimensional sheets – is the most stable phase of carbon. In graphite, each carbon atom is covalently bonded to three other carbon atoms; the fourth electron is delocalized and free to conduct electricity. Graphite sheets are weakly held together by van der Waals forces: repulsions and attractions caused by fluctuations in the electron density of each atom. Under high pressure, diamond is the most stable form of carbon: each carbon atom is covalently bonded to four others, creating a tightly packed three-dimensional lattice. However, because the binding energy between carbon atoms is high, to convert graphite to diamond requires high temperatures.9 When graphite is heated to 3,000°C under 15 gigapascals of pressure, its molecular structure changes to the tetrahedral shape characteristic of diamonds within seconds. Maintaining an even temperature and pressure is difficult under these conditions and so the diamonds produced tend to be small – measuring, at most, a few millimeters – and gritty. The cost of the equipment required to produce the necessary heat and pressure is prohibitively expensive. Diamond makers use metal solvent catalysts to reduce the pressure and temperature required. In the reaction cell, diamond seeds, graphite, and a transition metal solvent such as iron are heated to 1,300°C–1,800°C and five to seven gigapascals of pressure are applied. The molten metal dissolves the graphite, breaking it down into individual carbon atoms. At high temperatures, the metal dissolves more carbon atoms. When the temperature falls, this is reversed: the carbon atoms are now supersaturated in the solvent and some of them precipitate out. Diamond manufacturers deliberately encourage this process by keeping the diamond seeds at a lower temperature than the metal, so that carbon atoms precipitate on their surface, growing them into larger diamonds. This process takes weeks, but compared to direct synthesis it is easier to control. The diamonds produced are larger and purer and have a more consistent, single-crystal structure. When making diamonds in a lab, the most common impurities to control are nitrogen and boron. These elements sit on either side of carbon on the periodic table and, being roughly the same size, are easily incorporated into a diamond’s lattice. Nitrogen colors diamonds yellow; boron colors them blue. Both are ubiquitous in the Earth and in the lab and it is difficult to exclude them from the carbon source, the metal solvent, and the reaction cell’s components. To stop nitrogen and boron from being absorbed into the diamond, manufacturers use chemical absorbers (called getters) that react with them. Aluminum, titanium, and zirconium are common choices for both elements. When people want yellow or blue diamonds, nitrogen or boron can easily be added during production. Pink diamonds need a few more steps. They are produced by introducing nitrogen into the reaction, irradiating the resulting yellow or brown diamond, and then heating the diamond. This process causes the lattice to rearrange itself so that, throughout the diamond, single nitrogen atoms are located next to single gaps in the lattice. These defects are called nitrogen-vacancy centers and they produce a pink hue. In quantum computing, pink diamonds have been proposed as a means of storing and manipulating quantum information at room temperature. While diamonds were first produced in the lab in the 1950s, until the 2010s they were too small and too impure for jewelry. What made gem-quality lab diamonds possible was the meticulous investigation of the behavior of carbon at high pressures and high temperatures, a better understanding of the role of catalysts and getters, more precise control of the temperature gradients in the reaction cell, and competition from a new method of manufacturing diamonds. Chemical vapor deposition During the 1950s, while Project Superpressure was recreating the high-pressure, high-temperature conditions under which diamonds form in the Earth, scientists at industrial laboratories in the United States and at the Academy of Sciences of the Soviet Union in Moscow investigated a different technique for producing diamonds, not inspired by nature: chemical vapor deposition. With chemical vapor deposition, a solid material is deposited from a vapor that is undergoing a chemical reaction. As a manufacturing technique, it is as old as the cavemen, who would use soot from combustion for painting. To make diamonds using this process requires heating carbon to such a high temperature that it becomes a gas of isolated atoms, and then coaxing it to crystallize into a diamond structure as it cools. Graphite is the most thermodynamically stable phase of carbon at low pressures, but it is only slightly more stable than diamond. Because of this, whether the carbon gas con­denses as diamond or graphite hinges on how fast the diamond and graphite crystals form and grow. The first successful effort to produce diamonds using chemical vapor deposition occurred at Union Carbide. According to a patent application filed by research scientist William Eversole in 1958, when a carbon-containing gas (either methane, CH4, or carbon monoxide, CO) is heated to 900–1,100 degrees celsius at low pressure in the presence of diamond seed crystals, ‘diamond is deposited at a much faster rate than is black carbon [i.e., graphite]’. At the time, this was surprising. As the patent explains, ‘This is unexpected in view of the fact that thermodynamically, graphite is more stable than diamond’. Periodically, the diamonds had to be cleaned: further growth would be ‘hampered by the accumulation of black carbon’. This could be done by exposing the diamonds to hot, pressurized hydrogen (H2), which removes graphite faster than it removes carbon.10 Unfortunately, the rate of diamond growth in this initial discovery (~0.01 micrometers per hour) was extremely slow and so the process could not be commercialized. In the 1960s, the American chemist John Angus started an experimental program to investigate chemical vapor deposition diamond synthesis at Case Western Reserve University. Angus recounts that ‘the anonymous [National Science Foundation] reviewers were often scathing in their assessment of low-pressure diamond synthesis proposals, usually based on a misapplication of the second law of thermodynamics’ but the Defense Department were more sympathetic to high-risk proposals, ‘especially if they included, “The Russians are doing it”’. In their experiments, both the Soviets and the Americans were forced to alternate diamond growth cycles with graphite-cleaning cycles. Angus’s group found that if they broke the H2 used in cleaning into atomic hydrogen, the reaction proceeded much faster. At a conference in Ukraine in 1971, John Angus discussed these results with the Russian chemist Boris Derjaguin. Shortly after, the Soviets stopped publishing. What the Soviets discovered was that by using atomic hydrogen during the growth phase, they could inhibit the formation of graphite in the first place. Furthermore, the hydrogen gasified and thus removed any graphite that did manage to form. This both eliminated the need for a separate cleaning phase and accelerated diamond growth. While the Soviets would publish this result in 1978 and 1981, national security considerations prevented them from describing their experimental setup in sufficient detail for others to replicate their results. Despite this secrecy, the results of these experiments attracted the attention of scientists at the Japanese National Institute for Research in Inorganic Materials, who began an intensive program of research into the role of atomic hydrogen in the chemical vapor deposition synthesis of diamonds. Using hot tungsten filaments, microwaves, and electric arcs to break hydrogen gas (H2) into atomic hydrogen (H), these researchers – principally Mutsukazu Kamo, Seiichiro Matsumoto, and Yoichiro Sato – discovered how to grow diamonds at rates of several micrometers per hour, two orders of magnitude faster than Eversole. Crucially, this research was done in the open: the growth methods were published in sufficient detail that other scientists could easily reproduce and extend their work, and the laboratory welcomed visitors. A large number of companies, universities, and research institutes in Japan, the United States, and Europe entered the field. Having been dismissed as being physically impossible two decades before, the manufacturing of diamonds using chemical vapor deposition had become the subject of intense research and development worldwide. In a modern chemical vapor deposition reactor, multiple diamond seeds are placed in a vacuum chamber with the reactant gasses – mostly hydrogen (H2) and a small amount (generally less than one percent) of methane (CH4). The diamond seeds are heated to 800°C–1,200°C. Microwaves are then used to heat the hydrogen gas (H2) to 2,000°C–5,000°C, breaking its molecules into atomic hydrogen (H). Some of these unpaired hydrogen atoms will react with the carbon atoms on the surface of the diamond, creating a hydrogen-terminated surface that prevents the diamond crystal from reconstructing itself as graphite.11 However, because other unpaired hydrogen atoms will also abstract hydrogen from the diamond’s surface, not all of the carbon atoms will be terminated, leaving sites for other carbon atoms to be added. At the same time, atomic hydrogen will react with the methane, stripping away a hydrogen atom and leaving behind a methyl radical (CH4 + H → CH3 + H2). The carbon atom in the methyl radical (CH3) contains an unpaired electron. Some of these methyl radicals will react with unterminated carbon atoms on the diamond’s surface, growing it. The length of time required depends on the intended size; for gem-sized diamonds, it can take several weeks. A major advantage of chemical vapor deposition is that the reaction chamber is not subject to extreme pressure, and so the formation of the diamond is easier to monitor and to control. Chemical vapor deposition reactors are also less capital intensive than the presses required for high-pressure, high-temperature synthesis. Manufacturers can engineer chemical vapor deposition diamonds to precise physical, mechanical, thermal, and optical specifications by varying the temperature, pressure, and duration of the reaction and by adding desired impurities to the reactant gas. Improvements such as slightly increasing the pressure in the reaction chamber, using higher-powered microwaves, and adding a small amount of nitrogen and oxygen to the catalyst have made it possible to grow large, gem-quality diamonds. Identifying a lab diamond In their pure form, lab diamonds and mined diamonds are physically, chemically, and optically identical. Both consist of three-dimensional lattices of carbon atoms. To tell them apart, trained gemologists look carefully for impurities included during their formation, and for growth patterns that provide clues about their development. Diamond-manufacturing technology has become so good that lab diamonds can be made to be purer than diamonds formed in the Earth. At the upper end of the market, it is not pos­sible to tell the difference between lab diamonds and mined diamonds with the naked eye. Mined diamonds almost always contain nitrogen. In large quantities, this colors them yellow or brown; for colorless diamonds, this impurity can be detected using an optical spectrometer. Nitrogen-free diamonds that are formed in the Earth are extremely rare, and expensive. Conversely, nitrogen is almost always excluded from lab-grown diamonds. High-pressure, high-temperature diamonds can include traces of the metallic catalysts used to make them, particularly if the reaction is not carefully controlled. These can be identified using a microscope, and at high concentrations, iron and cobalt impurities can even be detected using a strong magnet. Chemical vapor deposition diamonds sometimes incorporate graphite, which appear as little pinpricks, comets, or flat clouds. High-pressure, high-temperature diamonds are subject to roughly uniform pressure during manufacturing, leading them to grow in both the cubic and octahedral directions. Even after being cut and polished, some evidence of these growth directions remains in the form of tiny grain lines. Chemical vapor deposition diamonds, deposited layer by layer, are cubic in form. Diamonds produced in the Earth are subject to uneven pressure; most end up octahedral, though some are also cubic or irregular. Because it’s so difficult to distinguish between mined and lab diamonds (even for jewelers), diamond grading institutions inscribe LG or Laboratory-Grown on the rim of lab-grown diamonds, visible at 20-times magnification. The future of jewelry The tradition that diamonds are an integral part of an engagement proposal is the result of a highly successful advertising campaign by the De Beers cartel. During the Great Depression, diamond sales slumped. De Beers responded by enlisting Hollywood actors and socialites in a campaign to associate diamond rings with marriage proposals, commissioning portraits of them showing off their new engagement rings, and by running ads showing happy young couples honeymooning above the now-famous slogan ‘A Diamond Is Forever’. In time, it also tried to persuade men that they would need to spend a fixed proportion of their income on a stone to win at love. One later advert, from the 1980s, was captioned ‘2 months’ salary showed the future Mrs Smith what the future would be like’. De Beers’s campaign worked because people desire signifiers of commitment that are credible and socially sanctioned. Diamond engagement rings suit this purpose because they are beautiful and practical and their symbolism is well-known. While the mine owners benefited most from this arrangement, the high price was a costly – and therefore credible – signal of wealth and commitment and, in an era where this mattered, insurance against breach of promise to marry. Peer pressure and status anxiety reinforced this norm. Lab diamonds have destroyed this equilibrium. Competition among diamond manufacturers and technological progress in diamond making mean that lab diamonds are indistinguishable from mined diamonds, but cost much, much less – and the price is falling. In 2016, a one-carat near-colorless and very slightly included round brilliant lab-grown diamond cost $5,440, according to diamond analyst Paul Zimnisky; in 2024, the same stone cost $1,325. (The price of an equivalent mined diamond decreased from $6,538 to $5,035.) In the past few years, sales of lab diamonds have started to overtake mined diamonds. A survey by The Knot of nearly 10,000 couples married in 2023 revealed that lab diamonds accounted for 46 percent of engagement rings (compared with 39 percent who opted for a mined diamond), up from 12 percent in 2019. Diamonds will continue to symbolize engagement – the tradition is now well established – but on its own, the raw material will cease to be a symbol of wealth or sacrifice. Lab diamonds can already be made to be clearer and more colorless than mined diamonds – or, if the wearer desires it, to have a more magnificent color. To complement this, consumers are demanding better workmanship in the cutting and polishing of the stone and in the design and manufacturing of its setting. The ‘hearts and arrows’ optical pattern, which is present only in round brilliant diamonds that are perfectly cut, is a good example of this trend. Previously, perfectly cut diamonds were extremely rare, and too expensive for most customers. Now, because of strong demand, the International Gemological Institute has begun to note the presence of the hearts and arrows pattern on their diamond grading certificates. As lab diamonds can be engineered to be more beautiful than mined diamonds, we should expect them to be used more often in fine jewelry. Lab diamonds won’t end conspicuous consumption, but it will be the consumer and not the mine owners who enjoy most of the benefits. Diamonds in industry While the beauty of diamonds has made them synonymous with jewelry, most diamonds, mined or lab-grown, are produced for industrial use. Diamonds have excellent physical, optical, thermal, chemical, mechanical, and electrical properties, which makes them useful in manufacturing, construction, mining, medicine, electronics, and other industries. Mined diamonds, being too impure, do not perform as well as lab diamonds when used for industrial purposes. Moreover, lab diamonds can be manufactured to required specifications more reliably and cheaply. In industry, lab-grown diamonds have eclipsed mined diamonds for several decades, and improvements in diamond-manufacturing technology are spurring progress in other fields. Small diamonds are sintered to the rim. IMAGE Source: iStock We have not exhausted diamond’s potential. As diamond manufacturing improves, promising new uses of diamonds are being discovered. Diamond is the hardest naturally occurring material. Its most frequent application is to make hard cutting edges, drill bits, grinding and polishing tools, and abrasives. Diamonds can be embedded in a metal coating or fastened to a metal core to make powerful saws, which are used to cut stone, concrete, asphalt, bricks, glass, ceramics, and metal.12 Diamond drill bit, used in oil rigs. IMAGE Source: Getty Polycrystalline diamond drill bits, made by fusing together diamond grit under high pressure and high temperature, are indispensable in oil and gas drilling, where they are stronger, faster, and more durable than tungsten carbide, the best alternative. Oil and gas wells often need to be drilled several kilometers deep, and replacing a broken or worn drill bit is a time-consuming process that requires pulling up and breaking apart the entire drill string. Using diamond-tipped drill bits has reduced the need for these replacements, saving money. There are some particularly hard rock formations that would be impossible to drill without them. Because of their strength, durability, and precision, diamond-tipped drill bits are also used to drill into glass, masonry, ceramics, and teeth. Other applications of diamond that make use of its hardness include grinding and polishing concrete, granite, and marble; drawing wire; and coating other materials to make them more hard-wearing. As materials science advances and creates tougher alloys, polymers, ceramics, and composites, it is almost certain that diamond tools will be needed to cut and shape them. As an optical material, diamonds have tremendous potential, because they are transparent, dissipate heat quickly, and do not expand much at high temperatures. In particular, diamonds are useful in high-powered lasers, which are used in cutting, welding, sensing, ignition, and medical surgery. A major problem with existing high-powered lasers is that their components are damaged by or deteriorate under high heat, limiting their output. An example of this is the thermal lensing effect, where high and uneven temperatures change how the optical window of the laser bends light. This degrades the focus and alignment of the laser beam. Diamond, however, is an excellent window material, being a good conductor of heat and transmitter of light and having a refractive index that does not vary much with temperature. Diamonds can also be used as heat spreaders to cool down other components in lasers, increasing the maximum power that can be generated. An ongoing area of research is the use of diamonds as the active laser medium: the component that optically amplifies light. To do this will require diamonds that are larger, purer, and more structurally perfect than what nature can provide and so will depend on advances in diamond-manufacturing technology. Even more promising, diamond has the potential to be an excellent semiconductor. Diamond has excellent thermal conductivity, because the regularity of its lattice and the strength of its bonds enable heat to be transferred quickly and efficiently. It has a wide band gap – in other words, it requires a high (but not insurmountable) amount of energy to promote one of its electrons into the conduction band. This means diamond can handle higher temperatures and voltages than conventional semiconductors, making it useful not only in devices that operate in extreme conditions (such as engines, radio towers, drilling equipment, spacecrafts, solar panels, and the electricity grid) but also for increasing microchip performance more generally. Most microchips today are made from silicon, a metalloid that sits one row below carbon on the periodic table and has a thermal conductivity of 1.5 watts per centimeter-kelvin. Diamond, by contrast, has a thermal conductivity of 22 watts per centimeter-kelvin. Over the past five decades, the number of transistors on a microchip has increased at an exponential rate, while the microchips themselves have become smaller. Chip designers have therefore had to contend with the ever-increasing problem of dissipating the heat that is generated. Heat degrades the performance of microchips and limits how tightly transistors can be packed together. To overcome this problem, manufacturers have lowered the voltage and devoted a large amount of space and energy to cooling and ventilation systems. Because diamond dissipates heat much faster than silicon, diamond-based microchips can be made smaller and operate in more extreme temperatures. On existing silicon-based microchips, diamonds are already being used as heat spreaders. Diamond also has a wider band gap than silicon (5.45 electron volts vs. 1.1 electron volts) and consequently diamond microchips can operate at higher voltages than silicon microchips. Semiconductors are engineered to precisely control the flow of electricity, but above a certain voltage their electrical resistance breaks down, resulting in an uncontrolled flow of current. Diamond undergoes electrical breakdown at ten millivolts per centimeter, compared to 0.3 for silicon, making it more suitable for high-voltage applications such as power generation and distribution. Furthermore, for a given voltage, less material is needed and so diamond microchips can be made to be smaller. Compared to other wide band gap semiconductors, electrons and electron holes13 move quickly through diamonds. This is important, because it means that signals are transported through diamonds faster. A conservative estimate places diamond’s electron and electron hole mobilities at 1,000 cm2/Vs (square centimeters per volt-second) and 2,000 cm2/Vs respectively, but electron mobilities as high as 4,500 cm2/Vs and hole mobilities as high as 3,800 cm2/Vs have been reported. Silicon, for comparison, has an electron mobility of 1,500 cm2/Vs and a hole mobility of 480 cm2/Vs. Before diamonds can be used in semiconductors, we’ll have to overcome a number of technical challenges. The cost is, at present, prohibitive (despite recent advances, diamond is still about 10,000 times more expensive than silicon) and the diamond substrates required are larger than what can currently be made. Microchips are manufactured from extremely pure, single-crystal wafers, which are thin slices of semiconductor material. While semiconductor fabrication plants can manufacture silicon wafers to have diameters of up to 300 millimeters, at present diamond wafers are limited to ten millimeters if manufactured using the high-pressure, high-temperature process. Chemical vapor deposition diamond wafers can be made slightly larger, but the number of defects in the diamond and hence its utility as a semiconductor is typically worse. A more pressing problem is doping (that is, deliberately adding impurities to) the diamond to have the required electrical properties. In its pure form diamond is an electrical insulator. To use it as a semiconductor requires incorporating electron-donating impurities, to create an n-type semiconductor, or electron-accepting impurities, to create a p-type semiconductor. Creating a p-type diamond semiconductor is not hard: boron, the element to the left of carbon in the periodic table, has one fewer electron and hence is electron-accepting. When introduced as an impurity during the diamond-manufacturing process it is readily incorporated into the diamond’s lattice. Creating an n-type diamond semiconductor is harder. While nitrogen, the element to the right of carbon in the periodic table, has one surplus electron, the energy required to release this electron to the conduction band is extremely high. Phosphorus, which is one period below nitrogen, is a more promising candidate, but because phosphorus is larger than carbon, incorporating it into the tightly packed diamond lattice is not easy. The energy required to release the surplus phosphorus electron is also high, though the situation is not as bad as it was with nitrogen. When such an n-type diamond semiconductor is used at high temperatures, this is less of a problem, as the required activation energy is readily available. However, for semiconductors intended to be used at room temperature, a more suitable electron donor must be found. This is an open problem. Not unconquerable Nature takes billions of years to produce diamonds. Deep in the Earth’s mantle, the intense heat and high pressure slowly crystallizes carbon-containing fluids into diamonds. Volcanic eruptions transport these diamonds to the surface. Mining them is a dirty business – diesel-powered diggers excavate 1,000 tons of earth for every carat that is dug out of the ground. Most mined diamonds are ugly, discolored, and impure. In the lab, diamonds can be made to be purer, more beautiful, and more durable than what nature can achieve in less time and with fewer resources. If desired, specific impurities can be added to modify their optical, mechanical, and electrical properties. Initially, lab diamonds were manufactured using the high-pressure, high-temperature process, which was inspired by the method by which diamonds form in the Earth. Now they are also made using chemical vapor deposition, which has never been observed in nature. In Western civilization, a pernicious belief has taken hold, that what is ‘natural’ is good and what is man-made is inferior or harmful. Wordsworth, in his poem ‘The Tables Turned’, expresses it best: Sweet is the lore which nature brings Our meddling intellect Mis-shapes the beauteous forms of things: — We murder to dissect. But human intellect and the scientific method are the foundations of our prosperity. Nature is not a superior craftsman: it is our resource, and we can and should improve upon it. In materials science, this has been obvious for a long time: steel, plastic, polyester, concrete, ceramics, and glass are unsurpassed by any natural substance. Lab diamonds are a further testament to this.",
    "commentLink": "https://news.ycombinator.com/item?id=41488353",
    "commentBody": "Synthetic diamonds are now purer, more beautiful, and cheaper than mined (worksinprogress.co)170 points by bswud 5 hours agohidepastfavorite107 comments Animats 14 minutes agoThat's a good article. The whole history is there. The commercial side has made huge progress, too. Look up \"diamond making machine\" on Alibaba. You can buy a high-pressure, high temperature six sided press for about US$200,000. A chemical vapor deposition machine is about the same price. De Beers, the diamond cartel, has an R&D operation, Element Six. They sell synthetic diamonds for lasers and other exotic applications. The technology is good enough to achieve flaw levels in the parts per billion range, and to make diamond windows for lasers 10cm across.[1] This is way above jewelry grade. Over on the natural diamond side, there's been a breakthrough. The industry used to break up some large diamonds during rock crushing. Now there's a industrial X-ray system which is used to examine rocks before crushing to find diamonds. It's working quite well. A 2500 carat diamond was found recently.[1][2] TOMRA, which makes high-volume sorters for everything from recyclables to rice, has a sorter for this job. This is working so well that there's now something of a glut of giant diamonds too big for jewelry. The finishing processes of cutting and polishing have been automated. The machinery for that comes mostly from China and India. Diamonds are now something you can buy by the kilo, in plastic bags. [1] https://e6-prd-cdn-01.azureedge.net/mediacontainer/medialibr... [2] https://www.forbes.com/sites/amandakooser/2024/08/23/monster... [3] https://ikcabstracts.com/index.php/ikc/article/download/4101... reply A_D_E_P_T 2 hours agoprevOver the past 10 years, there has been an explosion in cheap lab diamond and moissanite producers in China and India. 10 years ago, it was hard to find quality lab diamonds at a reasonable price, and moissanite was still reasonably expensive at $400-600/ct. Today, given cutthroat competition and \"race to the bottom\" pricing strategies, lab diamonds are ubiquitous, extremely high quality, and cheap. Less than $200/ct and sometimes much less: https://detail.1688.com/offer/751071300271.html Moissanites are now less than $5/carat at retail: https://detail.1688.com/offer/586468555080.html These are legit. I've bought some. Within 10 years of today, I expect diamonds to lose almost all of their value. Moissanites have already become as near-worthless as synthetic rubies. This is going to open up new industrial uses for those gemstones. reply godelski 1 hour agoparentDiamonds have lots of uses beyond being pretty. I have to bring this up because a lot of people are talking as if this is the entirety of the reason for their decrease. But there's diamond files, diamond cutting blades/wheels/drills, you can make glass from it (really only used in labs that absolutely need them because the price), and many more uses. Many of these don't care about size, quality, or clarity. So instead of pulling from scrap material from jewelry making or rejected diamonds you could just make your own and ensure your own supply. One of the things I've loved about synthetic diamond prices coming down is just how cheap and available diamond cutting wheels and filing tools have become. You can now get a set of diamond files on Amazon for under $10. That's crazy reply notfed 39 minutes agorootparent> diamond files on Amazon for under $10 Link? I'm skeptical. It seems more likely someone is abusing the term \"diamond\", no? reply avhon1 0 minutes agorootparentSets like this one https://www.amazon.com/dp/B092D4CV56 are also sold in U.S. hardware stores https://www.harborfreight.com/needle-file-set-10-piece-69876... https://www.menards.com/main/tools/hand-tools/files/tool-sho... dotnet00 15 minutes agorootparentprevNot the same thing, but you can get diamond tipped 3d printer nozzles for ~$100: https://www.amazon.com/Diamondback-Nozzles-Compatible-Polycr... Considering that these are a niche product requiring a precisely shaped diamond insert made by a relatively small operation in the US, I think it's believable that a Chinese company could produce diamond files for ~$10, considering that it only needs diamond powder and has more space for mass production. reply hintymad 1 hour agoparentprevAnd I'd be very happy to see the demise of De Beers. It's amazing that De Beers can thrive for more than 100 years, but still, using clever marketing and tight control of supply to artificially jack up the price of diamond is counter-productive. reply Joker_vD 1 hour agoparentprev> I expect diamonds to lose almost all of their value. Artificial diamonds, you mean. The natural ones will keep their price, just as \"hand-crafted\" goods did (and, I suspect, as \"human-produced\" content in the future will); it's a matter of status and signalling that you can afford to buy an inferior, more expensive product. reply gwbas1c 1 hour agorootparentIt's based on supply and demand. In 2012, when I was shopping for an engagement ring, a natural 2ct diamond cost $250,000. (I bought a 2ct moissanite for much, much less, and my wife is very happy with it.) When I looked in fall 2023, a natural 2ct diamond cost $20,000. That's a loss of over 90% of value, not counting inflation! (Now the supply of diamonds is much higher, and the demand for natural diamonds is much lower.) I suspect that natural diamonds will be sold for a 40-300% premium over manufactured. I also wonder if impurities will become fashionable, just to show that a specific diamond is actually natural and can't be made in a lab. > Artificial diamonds BTW, there is no such thing as an artificial diamond. A manufactured diamond is 100% diamond, for all intents and purposes. reply acchow 0 minutes agorootparent> also wonder if impurities will become fashionable, just to show that a specific diamond is actually natural and can't be made in a lab. Why can’t you add impurities in a lab? tikhonj 42 minutes agorootparentprev> I also wonder if impurities will become fashionable, just to show that a specific diamond is actually natural and can't be made in a lab. I hope that happens for purely aesthetic reasons too: natural \"imperfections\" add a lot of visual variety and interest that's otherwise missing in a lot of diamond jewellery—at least the sort that I've seen. reply kaashif 40 minutes agorootparentprev> BTW, there is no such thing as an artificial diamond. A manufactured diamond is 100% diamond What do you believe the distinction between artificial and manufactured is? As far as I'm aware, they're almost synonyms. Dictionaries literally list \"man made\" as one of the definitions of artificial. reply batch12 1 hour agorootparentprevNow we just need artificial diamonds that are flawed enough to be indistinguishable from real diamonds. reply Joker_vD 1 hour agorootparentThere is a whole sub-industry of putting fake mosquitoes into fake amber. reply sdenton4 46 minutes agorootparentAs long as the mosquitos contain real dinosaur blood, it's all good. reply A_D_E_P_T 46 minutes agorootparentprevThey already do this with emeralds -- fake inclusions and flaws to make them look more like natural stones. If a natural emerald and a good synthetic emerald are distinguishable, it's often only because the synthetic one still looks too much better -- its color and overall clarity are still a little bit too good. reply striking 1 hour agorootparentprevI'd expect this to be true if you could tell at a glance. But the new stuff looks like the old stuff, but bigger and better. reply rowanG077 51 minutes agorootparentprevWhy? Natural diamonds are the inferior product in every way. reply crazygringo 42 minutes agorootparentFor the same reason you can make a print of a classic painting, and digitally brighten and re-saturate the colors to counteract the darkening and yellowing of the original with age. Remove all of the cracking too. And you'll still only be able to sell the print for tens of dollars, while the original is worth millions. People attach value to authenticity and originals and tradition, however they define that. reply notfed 37 minutes agorootparentIt sounds like some people want to pay more money. Good for them. I'm happy to have a cheaper option. reply fidotron 29 minutes agorootparentThe whole point is to spend money and to be an honest signal https://en.m.wikipedia.org/wiki/Signalling_theory In the event artificial diamonds are genuinely indistinguishable from natural ones they will all, natural and artificial, become worthless aside from practical applications. reply freen 4 minutes agorootparentSylvester McMonkey McBean at work. satvikpendem 31 minutes agorootparentprevYes, this is a form of signaling in social psychology, an interesting phenomenon that happens with originals versus \"replicas.\" reply rowanG077 31 minutes agorootparentprevThe prints most certainly aren't better. Hand painted is not something that can't be defeated with a printer currently. Besides you pay for the history of the original not the paint. A natural diamond has essentially the same history as any other rock you can pick up anywhere for free. People pay for the feeling. That doesn't mean a synthetic diamond is physically superior in every way. reply gwbas1c 1 hour agoparentprev> Within 10 years of today, I expect diamonds to lose almost all of their value. Moissanites have already become as near-worthless as synthetic rubies. This is going to open up new industrial uses for those gemstones. And for jewelry too. I bought my wife a 2ct moissanite in 2012. There was no way we could have done that ring with a real diamond back then. When I was shopping, I happened to see a girl at a conference who had many large moissanites on her ring. It was gorgeous, and well within the price range of upper-middle-class. reply Glyptodon 1 hour agoparentprevI think large natural diamonds will exist as a market. Just expect large real gems to become much less common for the non-wealthy. And large gemstone jewelry to become more ubiquitous with the increasing spread of lab gems. Definitely a trend towards seeing more such gems paired with silver as the prices have gone down. reply dist-epoch 15 minutes agoparentprevHow big can you get? Do they make olive sized ones (1-2 cm diameter)? How much would such one cost? reply thaumasiotes 1 hour agoparentprev> Moissanites have already become as near-worthless as synthetic rubies. I've noticed that synthetic sapphires are (or were?) much more expensive than synthetic rubies. Do you know why? reply zettabomb 44 minutes agorootparentI'm going to take a guess and say it's because of ruby lasers. They make massive synthetic ruby rods for lasers, but it needs to be one solid piece with very low impurities. A small defect will cause the rod to be entirely unusable for a laser, but there can still be large portions usable for other purposes with less stringent requirements. An example is ruby tipped 3D printer nozzles, used for abrasive filaments, which can be had for around $50. reply spondylosaurus 1 hour agorootparentprevI'm also curious about this. Especially since natural rubies and natural sapphires are the same type of gemstone, just in different colors. It sounds like the synthetic equivalents might not be similar at all! reply tiagod 57 minutes agorootparentThis is just speculation, but assuming you're talking about gem-grade rubies and sapphires, perhaps there's more industrial uses for similar rubies and the gem industry sees the benefit. reply thaumasiotes 1 hour agorootparentprevThe synthetic equivalents have to be similar, because they're the same thing as the natural stones, and the natural stones are similar. A ruby is corundum with chromium coloring it; a sapphire is corundum with (most typically) iron coloring it. Iron isn't rare, so either it affects the process used to make rubies, or there's no real reason for the price gap. reply spondylosaurus 39 minutes agorootparent\"No real\" reason wouldn't shock me. Not a gemstone expert so I can't say, but if natural sapphires are more expensive/in higher demand than natural rubies (despite both being corundum) then it makes sense for that demand to map onto their synthetic counterparts. But with the synthetic stones, it's more obvious how arbitrary that demand/price difference is. reply fortran77 1 hour agoparentprevI need diamonds to play my records! What do you think a record stylus (\"needle\") is made from? reply A_D_E_P_T 1 hour agorootparentSure, but those aren't gem-grade. They're usually black and opaque (polycrystalline) or yellow. And in any case they're very small. When I say new industrial uses, I'm thinking of things that haven't been done before and hinge on large bulk volumes of material: Windows, very large diamond anvil cells, high-performance heatsinks, and stuff like that. Lots of cool things are going to be developed. reply euroderf 31 minutes agorootparentprevShibata needles always commanded a premium. Can they now be manufactured in that shape ? Can all vinyl lovers now get Shibata needles ? The great thing about them is that they go deeper in the record grooves, so even if your record has been played a lot using cheaper needles, a Shibata might find virgin vinyl. Which also means that on the first play with the Shibata, it will excavate a lot of gunk. reply jalk 1 hour agorootparentprevWorthless (as in low monetary value) doesn't mean useless. reply HPsquared 41 minutes agorootparentFunnily enough this concept is literally called the \"diamond-water paradox\". reply jajko 2 hours agoparentprevThey never had any value, apart from specialized ie glass cutting tool. Only when DeBeers realized they could push some fictious heavy marketing 'to prove your worth to woman you are asking to marry' for those shiny stones nobody wanted to buy, people who didn't know better got manipulated into buying them. They are supposedly very common in universe, and probably in deeper Earth too. Correction is healthy and benefits mankind long term, there was nothing good coming from ie impact on Africa. Nobody cared about that, so things are fixed from another direction. reply murukesh_s 1 hour agorootparent>They never had any value They had immense value in ancient world. They were valued much more than gold - due to their rarity. Southern India was the only known source for mining diamonds in ancient world [1]. They were used as currency and was valued higher than gold. In fact there were wars fought for diamonds. There is a saying about Koh-i-Noor, one of the most famous Diamond - \"If a strong man were to throw four stones – one north, one south, one east, one west, and a fifth stone up into the air – and if the space between them were to be filled with gold, all would not equal the value of the Koh-i-Noor\" [2]. [1] https://en.wikipedia.org/wiki/Golconda_diamonds [2] https://www.theguardian.com/commentisfree/2016/feb/16/koh-i-... reply ipsento606 1 hour agorootparentprev> Who does not love diamonds? Where is there a mind in > which the bare mention of them does not excite a > pleasant emotion? Is there any one of rank too exalted > to care for such baubles? The highest potentates of the > earth esteem them as their choicest treasures, and > kingdoms have been at war for their possession From \"Diamonds\" by William Pole, published in 1861 [1] (27 years before the formation of the De Beers company) [1] https://www.google.com/books/edition/Diamonds/ENwOAAAAYAAJ?h... reply adrian_b 1 hour agorootparentThe Europeans have encountered for the first time what are now called diamonds during the expedition of Alexander the Great in India, where they had been appreciated for jewelry for a long time. This is why the Romans, e.g. Pliny the Elder, used for diamonds the name \"Indian adamant\". The name \"adamant\" without the \"Indian\" specifier had already been used for many hundreds of years (the oldest attestation is in Hesiod), but it had not been applied to a gem, but to nuggets of native osmium-iridium alloy, which can be found mixed with the nuggets of native gold in the alluvial deposits of gold (and which, unlike the gold with which they were mixed, could be neither melted nor forged, hence their name, \"untameable\"; \"unconquerable\", which is used in the article is a bad translation for \"adamant\"). Pliny the Elder described the \"Indian adamant\" as consisting of octahedral crystals, which is the most frequent form of the natural diamonds. It appears that at that time it was impossible to cut the diamonds, at most they might have been polished a little, so the crystals used in jewelry retained their native shape. By the time of Pliny the Elder, the \"Indian adamant\" was the most expensive gem, surpassing even the noble opals, the pearls, the emeralds and the beryls, which were the next most expensive gems at that time. The Romans did not care much for transparent crystals, they appreciated much more the higher quality exemplars of noble opals or pearls, if those exhibited a nice play of colors. reply RandomLensman 1 hour agorootparentprevDiamonds where valued before DeBeers even existed, but diamonds in engagement rings (especially in the US) are (at least partially) from a heavy marketing push by them in the 20th century. Previously engagement rings tended to be colored gemstones. reply euroderf 29 minutes agorootparentYes, I've read that in the early 20th century, other precious gemstones (like rubies & emeralds) had more than a third of the market for engagement rings. reply khazhoux 1 hour agorootparentprev> They never had any value The Crown Jewels disagree with you https://www.hrp.org.uk/tower-of-london/history-and-stories/t... reply thehappypm 1 hour agorootparentprevDiamonds certainly have value as a material for jewelry -- essentially unscratchable jewelry is pretty awesome. They also don't get cloudy over time or anything. That's the cool part. reply darby_nine 1 hour agorootparentprevThere's no market for e.g. diamond lenses? Sure it's going to be on the expensive end of the market but the same forces are pushing those prices down. reply Ekaros 1 hour agorootparentprevAnything rare does have value. So why not big enough diamonds. With big being critical part. The tiny stuff really is very stuff. The big is rare and thus rarity along reasonably brings some value. reply tonetegeatinst 1 hour agorootparentprevI think diamonds are used in some lathes as cutting tools. So suprising thats not more common. I though diamonds strength and iirc its heat tolerance would be attractive to the folks who cut stuff. reply adrian_b 1 hour agorootparentDiamonds are excellent for cutting some materials, e.g. ceramics or some non-ferrous alloys, but they are bad for cutting metals that are good at dissolving carbon, e.g. iron, cobalt and nickel alloys. So for the iron alloys, which are the material most frequently processed by machining, diamond is not suitable. Other hard crystals, like alumina a.k.a. corundum, are much better for this purpose, even if they are less hard than diamond. reply pfdietz 45 minutes agorootparentFor those metals, cubic boron nitride works and has about half the Knoop hardness of diamond (and more than twice the hardness of aluminum oxide.) c-BN is isoelectronic with diamond; h-BN is isoelectronic with graphite (but is an insulator). reply folmar 1 hour agorootparentprevThe hardness is attractive but the poor heat resistance is a major problem for many uses. For a normal angle grinder you can use normal abrasive disc without paying much attention, but with a diamond-grit one you can easily burn the diamonds. reply AStrangeMorrow 2 hours agoprevI have seen lab grown diamond being quite a bit cheaper than mined ones for a while now. As in ×2 to ×3 times cheaper even. And yes, funnily it seems that the purer a diamond is (clear, few impurities etc) the higher its price/carat, until it is so pure that it means it is a lab grown diamond and not a natural one and the price drops reply throwaway48476 2 hours agoparentVeblen good. reply djtango 1 hour agorootparentBecause synthetic diamonds are indistinguishable to the naked eye (IIRC a trained eye with a magnifying glass can spot faint nitrogen impurities which are characteristic of natural diamonds) the thing you're really paying for is the piece of paper, the certificate. You're not really paying for carbon allotrope. So it's less like gold which is fungible and a more natural form of money. Diamonds feel more like an NFT... reply _Microft 2 hours agorootparentprevThe maybe overly terse parent comment is referring to this concept: https://en.wikipedia.org/wiki/Veblen_good reply andrewflnr 1 hour agorootparentprevNot really. The comment is talking about price increasing with the quality of the \"good\", which is perfectly conventional. The weird price cliff is especially out of place for a Veblen good. reply aidenn0 2 hours agorootparentprevNo, Veblen bad. reply Kaijo 1 hour agoprevThis is true with some qualifications. If you're interested in the kind of investment grade diamonds that a major auction house would deal with, then you're looking at heavy weights and/or fancy colors that synthetics can't reach yet. In the diamond trade the word \"paragon\" is sometimes reserved for flawless or near-flawless stones above 100 carats, of which there is a long list of famous examples, but the largest gem grade synthetic is still around 30 carats I believe. Vivid colors top out at much lighter than that. I guess we'll be able to outdo nature within a few decades though (as far as terrestrial diamonds go, anyway -- I seem to recall reading somewhere about the discovery of moon-sized space diamonds). reply A_D_E_P_T 29 minutes agoparentVivid colors are a trivial engineering problem, and one the Chinese have already cracked. Screenshot: https://ibb.co/s6gWTy1 Prices are dropping like a rock from a high tower, and colors and other options are becoming more available. Within 10 years you'll be able to buy virtually any diamond you like, in any common color, for less than a 2ct stone would have cost in 2014. Also, if you really like huge gems, you can buy a moissanite today at 1000ct. Even on Amazon.com there are examples at 100ct. https://www.amazon.com/Gemonite-15CT-100CT-Moissanite-Colorl... This would have been unthinkable 10 years ago. Things in the diamond and gemstone business are changing fast. reply bluGill 21 minutes agoparentprevThough if you are interested in investment grade diamods I'd say it is time to get out - diamonds have never been as rare as investors like to pretend, and things are going to get worse. reply djtango 1 hour agoparentprevA diamond the size of a moon? Does that mean it's a single molecule of pure carbon the size of the moon? I wonder what effects gravity has at that scale reply A_D_E_P_T 26 minutes agorootparentIt's certainly polycrystalline rather than a giant single-crystal. It would contain lots of every other element that's soluble in it, to its limit of solubility, and whatever's insoluble or over that limit would have to form different mineral inclusions at grain boundaries. reply indoordin0saur 1 hour agorootparentprevI don't know about moon sized but there are solar systems out there where carbon is more common than silicon. In such a system if you had a terrestrial planet then you're likely get diamonds instead of quartz being the most common mineral in the crust. You also might possibly get diamonds in an octagonal crystalline form which are theorized to be far stronger than the diamonds we have here on Earth. reply littleweep 24 minutes agoprevTangentially related: Does anyone in the HN community have a recommendation for a reputable place for obtaining a lab-grown diamond for an engagement ring? reply henry2023 12 minutes agoparentJames Allen reply indoordin0saur 1 hour agoprevI'm planning on buying an engagement ring very soon and my own plan (as someone who has never done this before!) is to get a good lab grown diamond but spend more money on the metal in the ring. You can make a gem stone in a lab but until we become a Kardashev II civilization we won't be making any sufficient quantity of gold in a lab. If I buy a good loose lab grown diamond will I be able to find someone who will fit it into a high quality gold ring? reply Glyptodon 50 minutes agoparentLots of people are pretty into treating gems and rings as separate goods, or want grandma's diamond in a new ring. So I don't think getting them separately will be an issue. But I'd definitely consider looking up shiny precious gems on Reddit - for less money than a diamond you can really get some nicely cut and much more interesting Sapphires. reply andoando 33 minutes agoparentprevYes pretty much any jeweler will be able to custom make a ring for you. I imagine its how the majority of engagement rings are sold, theres way too much variability in the stone and ring/setting people want for jewlers to only sell premade rings. Also theres not much to a high quality ring and not much for you to spend money on there. reply DabbyDabberson 1 hour agoparentprevmost people taking this strategy that I know use platinum instead of gold. Most engagement rings only have ~~~$200 worth of gold on them. reply indoordin0saur 1 hour agorootparentDo ring vendors put a higher markup on gold than platinum? reply stoolio 29 minutes agorootparentThe price of gold is through the roof. Gold is ~$2500 per oz, while platinum is $950 per oz. However, most gold is 14kt (58%) or 18kt (75%) while Platinum is 90%+. That, and platinum is a heavier (technically denser is more correct?) metal, so there is more platinum in an equivalent ring, and it weighs more. The actual price on finished jewelry isn't as big as you would think. However, \"retail\" jewelry stores often price things using what they call Keystone (2x markup) or even triple keystone (3x). So, a $500 piece would sell for $1000-$1500. reply noname120 1 hour agorootparentprevGold is more expensive than platinum. reply andruby 40 minutes agorootparentIndeed, since 2016. Before 2016 platinum was worth more than gold, even double for a brief period in 2008. Now gold is worth 2.5x what platinum is worth. https://www.macrotrends.net/2541/platinum-prices-vs-gold-pri... reply snark42 1 hour agorootparentprevDid you mean rhodium? reply themaninthedark 1 hour agoparentprevIf you could find an independent jeweler(as opposed to chain jewelry store), I am sure they would. reply physicsguy 57 minutes agoprevThey're cheaper but they're not cheap and that's part of the issue... I remember looking at engagement rings about 5 years ago, my now wife is quite environmentally conscious. At the time it was like ~£1200 for a diamond one and £800 for a synthetic one. reply timerol 32 minutes agoparent5 years has made a large difference in prices, as shown in this graph quoted in TFA: https://www.paulzimnisky.com/Price-Evolution-of-Lab-grown-Di... Assuming you mean \"late 2018\" by \"about 5 years ago\" (because that's where the graph has a 1.5:1 ratio), that $1200/$800 diamond was probably about 0.2 carat (obviously depends on the other Cs), and would likely be around $1200/$300 today. reply vundercind 32 minutes agoparentprevI looked a few years back after reading a bunch about how synthetics were cheaper in discussions like this. I did not find it to be notably true then. There was barely a discount for synthetic at all, the places I checked. Ended up with moissanite, which was significantly cheaper than diamond, but still not, like, cheap if you care about it looking as diamond-like as possible, though I probably could have done as well with diamond buying “used” if I’d had the patience for it and more knowledge to be more confident I wasn’t getting scammed. reply gwbas1c 1 hour agoprev> A perfectly cut, flawless lab diamond costs a fraction of the price of a mined diamond of lesser quality. When I shopped for an engagement ring in 2012, there was a clear cohort of women who significantly valued a diamond from the ground. Fortunately, my (now) wife and I saw through the marketing gimmick, and laughed all the way to the bank. reply gruntledfangler 1 hour agoprev“ Lab diamonds are a testament to the principle that what nature can do, man is capable of doing better.” Profound hubris in an otherwise interesting article. reply indoordin0saur 1 hour agoparentWould love to see us turn iron into gold as well as nature does with supernovas. reply para_parolu 4 minutes agorootparentI thought this is possible and theory is well known. It just to expensive compared to mining reply slm_HN 1 hour agoprev\"Synthetic diamonds are now purer, more beautiful, and vastly cheaper than mined diamonds. Beating nature took decades of hard graft and millions of pounds of pressure.\" What does graft mean in this context? Is there a process where you graft diamonds, like plants? Does it refer to the diamond seed crystals mentioned in the article? reply jrgoff 52 minutes agoparentSeems to be UK slang meaning work/effort, see the third definition listed here: https://dictionary.cambridge.org/us/dictionary/english/graft reply ycombinete 1 hour agoparentprevHard graft is an idiom. It means hard work. reply aidenn0 2 hours agoprevWhen TFA talks about semiconductors it really only directly compares it to silicon, when GaN is probably the nearest competitor to diamond, both in cost and performance. I believe diamond is \"only\" about 20x the cost of GaN; anyone know what the economics would be for substituting diamond for GaN in e.g. HVDC? reply Zekio 46 minutes agoprevThis must be why Ruby 3D printer nozzles are getting below 100 bucks reply pton_xd 2 hours agoprevI mean, this has been true for the better part of a decade. The difference now is that jewelers caved and many offer a selection of lab diamonds. There's still a high markup on lab diamonds in most retailers though. Loose lab diamonds are incredibly cheap, only $600 per carat! reply hadlock 2 hours agoparentI was surprised by this, but the top three results for lab grown diamonds, all of them had good cut, color clarity 3+ carat (loose) diamonds for ~$1200-1400. That's very impressive given that's what a mid grade 0.75-1.0ct mined diamond cost 5-6 years ago reply renewiltord 2 hours agoparentprevWhere does one get these wholesale prices? And I assume that cutting happens at some cost over this? reply pton_xd 2 hours agorootparentWholesale I'm not sure. Retail you can use sites like stonealgo.com to search inventory. And no that's the final cut and graded price. reply stoolio 26 minutes agorootparentThe trade uses a variety of sites for \"wholesale\" prices. Rapnet is the standard, and they publish the Rapaport Price List. However, it only covers natural diamonds, and rapnet only lists natural. Diamonds generally go for a % discount off the Rap Price List. Polygon (.net) is the other major listing platform. They include lab diamonds, and most everyone uses them. There are other platforms, but those are the two majors IMHO. You need to be a member of the trade to sign up AFAIK. However, I have seen one retail site, Ritani, actually post \"wholesale\" prices and they seemed to be pretty accurate. They seem to have good prices too. They list their wholesale/markup/etc. Of course, you should buy from a nice local jewelry store, but if you want to buy lab online they are at least great for checking prices. reply hadlock 2 hours agorootparentprevLMGTFY gemsny.com friendlydiamonds.com lightboxjewelry.com I found a wide variety of 2-3ct diamonds for $1200-1400 there (round, brillant cut, the \"classic\" type), which is what a 0.75-1.0ct used to cost reply A_D_E_P_T 2 hours agorootparentGo direct to the source and you can get 2ct colorless diamonds at less than $600 shipped. e.g., https://detail.1688.com/offer/751071300271.html There are literally hundreds of suppliers like that in China/India. Do not pay more than $1000 for a diamond of any size. reply mhuffman 1 hour agorootparentThis looks too good to be true. I am interested in this (because of all the terrible stuff related to diamond mines), but how do you protect yourself from scams on this site? $500-$1,000 is a lot to \"test the waters\" and see if they are real or even if they will arrive. What is the chance the just swap it out with Moissanite or who knows what? reply hammock 2 hours agorootparentprevLink is not working, can you tell me what to search? reply A_D_E_P_T 1 hour agorootparentGo to 1688.com and search \"实验室钻石\" -- \"lab diamond\" You may need to have an account. But there's a ton of interesting and esoteric stuff on 1688.com, so, if you can figure it out, it's worth it. reply Tepix 1 hour agorootparentprevWorks for me. reply folmar 1 hour agorootparentI get infinite captcha hell. reply moralestapia 1 hour agoprev(Not related to the content of the article) What a beautiful site. The subtle pink background, the choice of font, the minimal appearance (true to the spirit of being minimal, not just dead-ass simple), the way images are woven through, the footnotes, ... Excellent execution! reply Noumenon72 26 minutes agoparentI didn't click through the headline, so until you called this out I thought I was skipping a price chart from the AP or something. I almost missed all that history and diagrams. Thanks! reply BeetleB 2 hours agoprev [–] Mostly irrelevant when it comes to the value, sadly. Especially for engagements. reply lo_zamoyski 1 hour agoparent [–] This seems to be an American phenomenon. Americans--women in demanding it, and men in acquiescing to it--are keeping this commercially confected \"tradition\" alive through a vanity bred and reinforced by corporate greed and consumerism. It's not that a diamond ring is bad per se. It's the artificial and frankly socially destructive stipulations around them. It's almost as if the marriage and future spouse were an afterthought, secondary to the actual ring! It is utterly deranged. Similar things can be said about lavish weddings couples can't afford and go into debt for. It's a culture of spectacle and pretense. If you're not a rich aristocrat, don't pretend to be one. You're not \"temporarily embarrassed millionaires\". reply xhevahir 56 minutes agorootparentIf you think lavish spending on weddings and engagements is an \"American phenomenon,\" you're badly mistaken. Read up on weddings in India, Armenia, Cambodia, Afghanistan... reply rowanG077 44 minutes agorootparentThere is a difference in going all out for an event vs spending all that money on essentially a useless rock. reply nathan_compton 1 hour agorootparentprev [–] Eh, people are going to people. My wife and I are American and we did not do a diamond ring (we just exchanged simple hard wood rings). In any case, what I really object to is your assertion that women \"demand\" it and men \"acquiesce\" to their demands. In most cases people just figure its the tradition and its fun and frankly, if I were a woman, I wouldn't say no to a costly sign of commitment if the dude wanted to do it. But most of the women I know have no deep commitment to the practice. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lab-grown diamonds, first synthesized in the 1950s, are now cheaper, purer, and dominate the industrial market, offering a cost-effective and high-quality alternative to mined diamonds.",
      "Chemical vapor deposition (CVD) is a method developed in the 1950s for producing diamonds, allowing precise control and customization of diamond properties, making them suitable for various industrial applications.",
      "Advances in diamond manufacturing continue to unlock new uses and improve existing applications, demonstrating human ingenuity in enhancing natural processes."
    ],
    "commentSummary": [
      "Synthetic diamonds have become purer, more aesthetically pleasing, and more affordable than natural diamonds due to technological advancements.",
      "Machines to produce synthetic diamonds are now available for around $200,000, and companies like De Beers' Element Six are selling them for industrial purposes.",
      "Lab-grown diamonds and moissanites are significantly cheaper, with lab diamonds costing less than $200 per carat and moissanites under $5 per carat, making diamonds more accessible and expanding their industrial applications."
    ],
    "points": 170,
    "commentCount": 107,
    "retryCount": 0,
    "time": 1725888642
  },
  {
    "id": 41491121,
    "title": "Apple Hearing Study shares preliminary insights on tinnitus",
    "originLink": "https://www.apple.com/newsroom/2024/05/apple-hearing-study-shares-preliminary-insights-on-tinnitus/",
    "originBody": "UPDATE May 28, 2024 Apple Hearing Study shares preliminary insights on tinnitus Researchers from the University of Michigan release data from one of the largest surveys on tinnitus to date Tinnitus, or the perception of sound that others do not hear, is the subject of the new Apple Hearing Study update. The Apple Hearing Study is sharing new insights on tinnitus in one of the largest surveys to date. Through the study, University of Michigan researchers reviewed a cohort of more than 160,000 participants who answered survey questions and completed app-based assessments to characterize their experience of tinnitus. This research aims to improve understanding of tinnitus characteristics and inform future research on potential treatments. “Roughly 15 percent of our participants experience tinnitus daily,” said Rick Neitzel, University of Michigan School of Public Health’s professor of environmental health sciences. “Tinnitus is something that can have a large impact on a person’s life. The trends that we’re learning through the Apple Hearing Study about people’s experience with tinnitus can help us better understand the groups most at risk, which can in turn help guide efforts to reduce the impacts associated with it. The Apple Hearing Study gives us an opportunity that was not possible before to improve our understanding of tinnitus across demographics, aiding current scientific knowledge that can ultimately improve management of tinnitus.” Tinnitus, or the perception of sound that others do not hear, can happen to many people in one or both ears. With tinnitus, the sounds can take many forms but are most commonly described as a ringing sound and can be momentary or occur over longer durations. The symptoms and experience of tinnitus can vary significantly from person to person and can change for an individual. Tinnitus can impact a person’s overall quality of life, for example, disrupting a person’s sleep, concentration, or ability to hear clearly. A first step toward advancing understanding of tinnitus is to learn more about who experiences it, how the experience differs between people and within an individual over time, the potential causes, and the methods for managing tinnitus and their perceived effectiveness. A graphic reads, “Among participants in the Apple Hearing Study… 77.6% have experienced tinnitus in their life.” A graphic reads, “Among participants in the Apple Hearing Study… Roughly 15% experience tinnitus daily.” A graphic reads, “Among participants in the Apple Hearing Study… 35.8% of those ages 55 and older constantly experience tinnitus.” A graphic reads, “Among participants in the Apple Hearing Study… 20.3% cited noise trauma as the cause of their tinnitus.” previous next Tinnitus Prevalence The study found that 77.6 percent of participants have experienced tinnitus in their life, with the prevalence of daily tinnitus increasing with age among many. Those ages 55 and up were 3x more likely to hear tinnitus daily compared to those 18-34 years old. Additionally, 2.7 percent more male participants reported experiencing daily tinnitus compared to females. However, 4.8 percent more males stated they had never experienced tinnitus. Management of Tinnitus In the Apple Hearing Study, participants reported mainly trying three methods to ease their existing tinnitus: using noise machines (28 percent), listening to nature sounds (23.7 percent), and practicing meditation (12.2 percent). Less than 2.1 percent of participants chose cognitive and behavioral therapy to manage their tinnitus. Cause of Tinnitus While there’s no guaranteed method to prevent tinnitus given its complex causes, practicing hearing protection and managing stress levels can lower the chances of tinnitus. In the study, participants cited “noise trauma,” or exposure to excessively high levels of noise, as the primary cause of tinnitus (20.3 percent), followed closely by stress (7.7 percent). Characterizing Tinnitus The majority of participants experience brief episodes of tinnitus, compared to 14.7 percent who reported constant tinnitus. The reported duration of tinnitus significantly increases with age among participants 55 and older: 35.8 percent of participants ages 55 and older constantly experience tinnitus. Male participants experience constant tinnitus nearly 6.8 percent more than females. As for tinnitus levels, the majority found it to be faint, with 34.4 percent calling it noticeable compared to 8.8 percent who found it very loud or ultra loud. Ten percent of participants reported that their tinnitus has moderately or entirely interfered with their ability to hear clearly. In addition to the survey questions, participants who experienced tinnitus also completed an app-based sound test to better characterize their experience of tinnitus, matching the type and quality of the sounds they experience. iPhone 15 Pro shows a screen from the Apple Hearing Study that says “Tinnitus Check-In,” followed by instructions. iPhone 15 Pro shows a screen from the Apple Hearing Study that says “Select a Tinnitus Type,” followed by the choices: Crickets, Electric, Pulse, Pure Tone, Static, and Tea Kettle. Study participants completed surveys and an app-based sound test in the Research app to better characterize their tinnitus, matching the type and quality of the sounds they experience. Study participants completed surveys and an app-based sound test in the Research app to better characterize their tinnitus, matching the type and quality of the sounds they experience. previous next The majority of participants described their tinnitus as either a pure tone (78.5 percent) or white noise (17.4 percent). Among those who described a pure tone, 90.8 percent reported a pitch at 4 kilohertz or above, similar to the tones in a songbird’s call. Additionally, for those who described a pure tone, 83.5 percent identified their tinnitus as a single tone and 16.5 percent identified it as a teakettle tone — a high-pitched, whistling sound. For participants who matched their tinnitus to a white noise, 57.7 percent identified it as a static tone, 21.7 percent compared it to a cricket tone, 11.2 percent said it was an electric tone, and 9.4 percent identified it as a pulse tone. The Apple Hearing Study is one of three landmark public health studies in the Research app on iPhone, which launched in 2019 and is ongoing. Conducted in collaboration with the University of Michigan, the Apple Hearing Study advances the understanding of sound exposure and its impact on hearing health. Researchers have already collected about 400 million hours of calculated environmental sound levels supplemented with lifestyle surveys to analyze how sound exposure affects hearing, stress, and hearing-related aspects of health. Study data will also be shared with the World Health Organization as a contribution to its Make Listening Safe initiative. How Apple Products Can Help Apple technology provides a number of features to support hearing health with just a tap. Noise app: With the Noise app, Apple Watch users can enable notifications for when environmental noise levels might affect their hearing health. The Health app on iPhone keeps track of a user’s history of exposure to sound levels, and informs whether headphone audio levels or environmental sound levels have exceeded those recommended by World Health Organization standards. The Noise app on Apple Watch can notify a user when environmental noise levels might affect their hearing health, and the Health app on iPhone keeps track of a user’s history of exposure to sound levels. Environmental sound reduction: Apple Watch users can see when the environmental sound level is reduced while they are wearing AirPods Pro and AirPods Max. Active Noise Cancellation and Loud Sound Reduction mode: Active Noise Cancellation uses the microphone to detect external sounds, which AirPods Pro then counter with anti-noise, canceling the external sounds before a user hears them. For those looking to still enjoy surrounding sounds, Loud Sound Reduction with AirPods Pro (2nd generation) helps reduce loud noises while still keeping the fidelity of sound. Reduce loud audio: To set a headphone volume limit, users can go into Settings, then tap Sounds & Haptics (on iPhone 7 and later) or Sounds (for earlier models). They’ll then tap Headphone Safety, where they can turn on Reduce Loud Audio and drag a slider to a preferred decibel level. Share article Media Text of this article May 28, 2024 UPDATE Apple Hearing Study shares preliminary insights on tinnitus Researchers from the University of Michigan release data from one of the largest surveys on tinnitus to date The Apple Hearing Study is sharing new insights on tinnitus in one of the largest surveys to date. Through the study, University of Michigan researchers reviewed a cohort of more than 160,000 participants who answered survey questions and completed app-based assessments to characterize their experience of tinnitus. This research aims to improve understanding of tinnitus characteristics and inform future research on potential treatments. “Roughly 15 percent of our participants experience tinnitus daily,” said Rick Neitzel, University of Michigan School of Public Health’s professor of environmental health sciences. “Tinnitus is something that can have a large impact on a person’s life. The trends that we’re learning through the Apple Hearing Study about people’s experience with tinnitus can help us better understand the groups most at risk, which can in turn help guide efforts to reduce the impacts associated with it. The Apple Hearing Study gives us an opportunity that was not possible before to improve our understanding of tinnitus across demographics, aiding current scientific knowledge that can ultimately improve management of tinnitus.” Tinnitus, or the perception of sound that others do not hear, can happen to many people in one or both ears. With tinnitus, the sounds can take many forms but are most commonly described as a ringing sound and can be momentary or occur over longer durations. The symptoms and experience of tinnitus can vary significantly from person to person and can change for an individual. Tinnitus can impact a person’s overall quality of life, for example, disrupting a person’s sleep, concentration, or ability to hear clearly. A first step toward advancing understanding of tinnitus is to learn more about who experiences it, how the experience differs between people and within an individual over time, the potential causes, and the methods for managing tinnitus and their perceived effectiveness. Tinnitus Prevalence The study found that 77.6 percent of participants have experienced tinnitus in their life, with the prevalence of daily tinnitus increasing with age among many. Those ages 55 and up were 3x more likely to hear tinnitus daily compared to those 18-34 years old. Additionally, 2.7 percent more male participants reported experiencing daily tinnitus compared to females. However, 4.8 percent more males stated they had never experienced tinnitus. Management of Tinnitus In the Apple Hearing Study, participants reported mainly trying three methods to ease their existing tinnitus: using noise machines (28 percent), listening to nature sounds (23.7 percent), and practicing meditation (12.2 percent). Less than 2.1 percent of participants chose cognitive and behavioral therapy to manage their tinnitus. Cause of Tinnitus While there’s no guaranteed method to prevent tinnitus given its complex causes, practicing hearing protection and managing stress levels can lower the chances of tinnitus. In the study, participants cited “noise trauma,” or exposure to excessively high levels of noise, as the primary cause of tinnitus (20.3 percent), followed closely by stress (7.7 percent). Characterizing Tinnitus The majority of participants experience brief episodes of tinnitus, compared to 14.7 percent who reported constant tinnitus. The reported duration of tinnitus significantly increases with age among participants 55 and older: 35.8 percent of participants ages 55 and older constantly experience tinnitus. Male participants experience constant tinnitus nearly 6.8 percent more than females. As for tinnitus levels, the majority found it to be faint, with 34.4 percent calling it noticeable compared to 8.8 percent who found it very loud or ultra loud. Ten percent of participants reported that their tinnitus has moderately or entirely interfered with their ability to hear clearly. In addition to the survey questions, participants who experienced tinnitus also completed an app-based sound test to better characterize their experience of tinnitus, matching the type and quality of the sounds they experience. The majority of participants described their tinnitus as either a pure tone (78.5 percent) or white noise (17.4 percent). Among those who described a pure tone, 90.8 percent reported a pitch at 4 kilohertz or above, similar to the tones in a songbird’s call. Additionally, for those who described a pure tone, 83.5 percent identified their tinnitus as a single tone and 16.5 percent identified it as a teakettle tone — a high-pitched, whistling sound. For participants who matched their tinnitus to a white noise, 57.7 percent identified it as a static tone, 21.7 percent compared it to a cricket tone, 11.2 percent said it was an electric tone, and 9.4 percent identified it as a pulse tone. The Apple Hearing Study is one of three landmark public health studies in the Research app on iPhone, which launched in 2019 and is ongoing. Conducted in collaboration with the University of Michigan, the Apple Hearing Study advances the understanding of sound exposure and its impact on hearing health. Researchers have already collected about 400 million hours of calculated environmental sound levels supplemented with lifestyle surveys to analyze how sound exposure affects hearing, stress, and hearing-related aspects of health. Study data will also be shared with the World Health Organization as a contribution to its Make Listening Safe initiative. How Apple Products Can Help Apple technology provides a number of features to support hearing health with just a tap. Noise app: With the Noise app, Apple Watch users can enable notifications for when environmental noise levels might affect their hearing health. The Health app on iPhone keeps track of a user’s history of exposure to sound levels, and informs whether headphone audio levels or environmental sound levels have exceeded those recommended by World Health Organization standards. Environmental sound reduction: Apple Watch users can see when the environmental sound level is reduced while they are wearing AirPods Pro and AirPods Max. Active Noise Cancellation and Loud Sound Reduction mode: Active Noise Cancellation uses the microphone to detect external sounds, which AirPods Pro then counter with anti-noise, canceling the external sounds before a user hears them. For those looking to still enjoy surrounding sounds, Loud Sound Reduction with AirPods Pro (2nd generation) helps reduce loud noises while still keeping the fidelity of sound. Reduce loud audio: To set a headphone volume limit, users can go into Settings, then tap Sounds & Haptics (on iPhone 7 and later) or Sounds (for earlier models). They’ll then tap Headphone Safety, where they can turn on Reduce Loud Audio and drag a slider to a preferred decibel level. Press Contacts Zaina Khachadourian Apple zkhachadourian@apple.com Apple Media Helpline media.help@apple.com Copy text Images in this article Download all images Press Contacts Zaina Khachadourian Apple zkhachadourian@apple.com Apple Media Helpline media.help@apple.com Latest News PRESS RELEASE Apple introduces groundbreaking health features September 9, 2024 UPDATE Apple Intelligence comes to iPhone, iPad, and Mac starting next month September 9, 2024 PRESS RELEASE Apple debuts iPhone 16 Pro and iPhone 16 Pro Max September 9, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41491121",
    "commentBody": "Apple Hearing Study shares preliminary insights on tinnitus (apple.com)131 points by mgh2 1 hour agohidepastfavorite63 comments extr 1 hour agoI have mild tinnitus and the best advice I've ever read on the internet for it is: Stop reading. Don't look up information about tinnitus. Don't think about it. If you happen to notice it, try to distract yourself immediately. There maybe legitimate hearing damage but for the psychological aspect, the more you think about it the worse it becomes. I think I saw a quora answer somewhere where the doctor said \"Nobody complains about tinnitus while playing Playstation\". And it's very true. Until this post just now, I hadn't thought about it in weeks (months?). reply ljf 53 minutes agoparentThis! Said it before here but also consider than right next to each ear drum is a huge artery. When you run you might hear your heart beating in your ears, but generally we totally tune it out. But it is always there. Once I realised that I could also tune out tinnitus, I did just that - but a big part of that was just not thinking about it/dwelling on it. Been nearly 20 years now and tinnitus rarely troubles me, in the way it used to really upset and stress me. reply 2f0ja 29 minutes agoparentprevYou're absolutely right, there is a weird 'information hazard' component to tinnitus. It's like losing 'the game'. reply rumdz 4 minutes agorootparentI lost. You just ruined my longest \"winning\" streak ever. reply Sakos 2 minutes agorootparentprevIt's also a decent but annoying indicator for stress, lack of sleep and fatigue. If it's particularly loud, I know I'm tired. reply d1sxeyes 28 minutes agorootparentprevOh come ON reply matrix2003 17 minutes agorootparentI think information might be the wrong word, but there is a big attention component. reply iamjackg 8 minutes agorootparentI think the commenter you're replying to is just annoyed at having lost the game. reply wizzwizz4 15 minutes agorootparentprevIf you're suffering from potentially-unwanted social constructs, you should update your infohazard signature database. https://xkcd.com/391/ reply bob1029 1 minute agoparentprevI think your stress level can be a significant part of how you perceive it. Blood pressure and muscle tension seem to be primary factors in my perception of tinnitus. reply smusamashah 11 minutes agoparentprevIs this true for other things? I have started getting motion sick from some video games and noticed that it becomes worse if I am thinking/worrying about it. Last bad sickness hit was from Sable and Anti Chamber. There are few other indie games that effect me similarly. I thought it's lack of textures or crosshair in indie games, or may be it's the camera movement or both, not sure. But now, when playing first person games, i find myself worrying if it will make me sick. More I think, more I feel it. Yesterday I felt sick after playing Alien Isolation for the first time. It had textures, a crosshair, but camera was janky which made me worried and more I thought about it, worse it got and I end up deleting the game. reply TylerE 5 minutes agorootparentFor me it's a combo of bad camera, head bob, and tight indoor areas that are hard to navigate without lots of backtracking and turning around. reply nozzlegear 9 minutes agoparentprevI've got a strong pulsatile tinnitus¹, which means I can hear a strong whooshing sound (like wind) matched to the rhythm of my heartbeat. It's in my right ear and lasts all day, all night, day in and day out for the last three or four years. When I tell people about it, I like to compare it to the Edgar Allan Poe story \"The Tell-Tale Heart,\" where the narrator hears the heartbeat of a person he murdered coming from beneath the floorboards, and it grows louder and louder. Then I laugh it off and say \"at least I'm not that crazy!\" Jokes aside, my tinnitus used to bother me horribly. It used to be all I could focus on, not just because it's loud and distracting at times, but because it triggered a severe case of health anxiety². I went through my regular GP, who referred me to an otolaryngologist and audiologist; that person found nothing physically wrong with my ear (which is usually the case for tinnitus) and referred me for an MRI; the MRI threw a big wrench in the works by discovering a brain tumor which turned out to be benign – a simple pituitary tumor which I just need to keep an eye on every few years. But again, still not the cause of the tinnitus. Anyway, long story short, no cause was ever found for me suddenly developing this pulsatile tinnitus nearly overnight. It took a couple months to adapt to the new, constant sound in my ear, but I hardly notice it anymore unless I'm straining to hear something³. When I'm focusing on something like playing a video game, watching tv or listening to an audiobook, I don't notice the tinnitus at all. If the sound really starts to bug me, I've saved some brown noise tracks on YouTube that are particularly effective at drowning out the frequency of the whooshing sound. ¹ I also have the \"regular\", high-pitched whistling tinnitus, but I've had that most of my life. ² I've always had a health anxiety, learned behavior from my mother. This post makes me sound like a basket case, but I promise I'm a fully functional adult and these ailments I'm describing are trifles at the moment! ³ My wife and I have started to get into birding, so it does interfere a bit with my ability to hear some birds. More specifically, it interferes with my ability to locate where they are, I can't quite figure out if they're above me or behind me sometimes. reply ajkjk 8 minutes agorootparentprops for the effort on the footnotes reply jader201 37 minutes agoparentprevOr further, if you do read stuff about it and start thinking about it, don't sweat it. You'll eventually stop thinking about it, and life will go on. When I first started dealing with tinnitus, it sucked. Until I realized it didn't. Would I rather not have it? Sure. But life is not near as bad as I thought it would be at first, and I'm perfectly fine several years after it started. reply Twirrim 32 minutes agorootparent\"Don't sweat it\" is the most important thing, I've found, especially with tinnitus. As with a number of things, I've found I can actually move the needle when I give myself grace to fail, as long as I can genuinely try again and resolve to do better. A recent example being stopping telling myself I'm tired in the morning. Such a bad habit that self perpetuated. I was always feeling tired, in part, because I was telling myself I was tired. Chewing fingernails on the other hand, damn... that hasn't worked so far because I give myself grace, but never really resolve to do better. Someday I'll figure that out. reply kstrauser 14 minutes agorootparentRe fingernails: I bought keep a set of sharp fingernail clippers at my work desk. Any time I notice my fingernails being more than tiny, I neatly trim them. It turns out I mainly just couldn't abide having long fingernails. When I stopped having them, I stopped reflexively nipping at them. reply seper8 20 minutes agorootparentprevI've heard people apply a foul tasting chemicals to their fingers for a while to help condition yourself to not bite on them. reply kstrauser 16 minutes agorootparentprevMeditation helped me shift from resistance to acceptance. When it's quiet and still in the room and I hear The Whine, I greet it. \"Hi there, little Eeeeeee!\" For me, that demotes it to an innocuous background sensation no worse, or even different, than realizing that oh, my foot is pushing against the floor. Fighting it is futile. Accepting it let me stop caring about it. Of course I don't claim that's the universal fix for everyone. It sure helped me. reply neilv 47 minutes agoparentprevYep, of course I noticed mine as soon as I saw the article title. Some days it's noticeable on its own, and on rare occasions, it's annoying. Avoiding alcohol, caffeine, heavy sodium, and heavy stress might help. I also try to avoid really loud noises (but that's impossible where I live in the city right now, and is an almost daily 'adventure'). reply brainzap 54 minutes agoparentprevtrue, once you learn about something you tend to focus on it. Once I associated my neighbours noise with a positive thoughts I stopped to notice it. reply 101008 20 minutes agoparentprevThis applies to a lot of problems where you don't have control and can't do much as well. Sometimes ignoring some problems is the best solution. reply whimsicalism 33 minutes agoparentprevyes, like many common chronic diseases there is a significant psychological/anxiety component. i have noticed the exact same about my tinnitus reply baxtr 13 minutes agoparentprevYeah right. Thanks for reminding me that I have one too. Haven’t heard it for weeks… reply throwaway290 27 minutes agoparentprevTo add to that I remember some kinds of tinnitus are actually caused by tension in neck/shoulder muscles. So relaxation can literally fix it reply ilayn 27 minutes agoprevAs a former drummer who bashed way too many Chinas without proper ear protection, I had some scary tinnitus for quite a while. My advice; - First make sure that the frequency is not dancing around. If it is then probably it is one of those things your brain making up then it is relatively easier to fool yourself back again. Check it when it happens https://audionotch.com/app/tune/ (disclaimer I am not related to website, just first google result). - If it is constant then try to counter it with noise especially when trying to sleep. Just give yourself one of those nice YouTube colored-noise videos like this one https://www.youtube.com/watch?v=8SHf6wmX5MU - Avoid in-ears altogether, especially the bass-boost ones make sure that it does not fit airtight. More bass does not mean you pulsate your ear-canal with an airgun. If you want proper bass sound, invest in hi-fi stereo and listen to it in a good room. - As mentioned, distract yourself. Even if it is chronic and actually has a pathological cause, the brain finds a way to cope with it, like the glasses on your nose not noticing the weight. reply casenmgreen 29 minutes agoprevI had tinnitus as an adult, and it was cured. It turned out to be caused by an improperly filled root canal; some root material remained inside the tooth, the flesh above and around the tooth was inflamed and this was applying pressure inside the skull and bringing tissues which would otherwise not have been into contact, or firmer contact. I had the root canal re-made, and the tinnitus ended. reply focusedone 20 minutes agoparentHow was this discovered? Regular PCP, dentist or a specialist? Thanks! reply codesnik 18 minutes agoparentprevwow. It's a long way between jaw, sinuses and the inner ear. Was your inflamation that big? reply xoxxala 4 minutes agoprevI can ignore my tinnitus during the day fairly well. It just doesn't bother me that much. I've never really used earpods, no longer use over the hear headphones, and keep the volume down on my speakers. But at night it's a completely different story. With a quiet house and nothing to distract, it was causing a huge problem in my ability to get rest. The solution was to play Spotify all night long at a low volume. The music keeps the ringing to a minimum. The genre of music doesn't really seem to matter. It all works. reply jerlam 30 minutes agoprevOnly 20% of tinnitus cases caused by loud noises- seems like we have a lot more research to do. reply pkaye 38 minutes agoprevI have tinnitus and hearing loss. I've found that wearing the hearing aids itself silences the tinnitus. I've read that the hearing aids add enough of the background noise back so your auditory system is stimulated and tinnitus is drowned out. reply outside415 1 hour agoprevthe newer generations of AirPods absolutely trigger tinnitus for me. The gen 1 AirPod Pros are the best. I really have to crank the volume to trigger it. The Gen 2 AirPod Pros are the worst. Even low volumes rip apart my ears. Constant ringing all of the time. The USB-C Airpod Pros Gen 2 are ok at low to mid volumes, can't use them at high volumes what so ever though, they also let in a terrible amount of wind noise for outdoor activity which makes them unusable since turning up the volume to mute the wind noise causes tinnitus for me. The AirPod Pro Max also get too loud, they are ok at low to mid volumes, high volumes = extreme tinnitus. HomePods are similar, I can only have them on at volume levels I appreciate for short periods of time or I get tinnitus. Compare this with my old sennheisers and audeze headphones, 0 tinnitus even at extreme volumes. Similar for my in ear Mochi headphones. Or compare the HomePods to my Panasonic Surround Sound Speakers for my TV from 12 years ago that I still use, I can make the walls shake with no tinnitus. If I turn up my homepods to a volume close to that my ears will be ringing for hours or days after. It really bums me out, I wish I understood what is changing about the technology. Like are they going from Analog to Digital and is digital more harsh or something? I don't know. reply radicaldreamer 1 hour agoparentThere have been theories about ANC headphones, earphones, and in-ear-monitors causing tinnitus over the years, but nothing concrete with evidence. reply smokel 41 minutes agorootparentThere are also theories that they will prevent tinnitus because one can play music at a lower volume and still enjoy it. reply outside415 4 minutes agorootparentI don't have tinnitus on most of my devices at medium to high volumes. only apple devices and more recently built wireless in ear monitors. victim blaming is great though. I personally like to pick apart all of the layers of music and hear all of the nuance/production. At low volume this is generally not possible. Not all of us have audiophile tendencies though I suppose. reply cma 3 minutes agorootparentprevYeah, they should lower the noise floor and allow same dynamic range at lower volume, just like Apple touts here for the new AI noise cancellation (which also mentions they use eartips for part of the reduction). reply throw_pm23 40 minutes agorootparentprevThe burden of evidence should be on the side who wants us to put their electromagnetic devices in our body-cavities. To put it less bluntly: even if there is no evidence of harm, one can rationally decide to avoid these products out of caution. reply nabla9 38 minutes agoparentprev\"triggered\" tinnitus from in-ear headphones or normal headphones is not necessarily from the sound level. Try to massage ears (pull earlobes forward, back, up down, and massage muscles around ears) head and jaw muscles. Stretch your neck muscles. If you do it from time to time and tinnitus eases even a little, it might come from how the AirPods or headphones press your ear or head causing tensions. reply michaelteter 45 minutes agoprevMine started in one ear after a problem while ascending during a scuba dive. Something remained different in the region around my ear afterward. Then COVID did some sh*t to my sinuses which left them changed. Now I have relatively low tinnitus in one ear and very noticeable tinnitus in the other ear. The pitch is high... reminiscent to the squeal that an old CRT or tube TV would make if it had no signal. The tinnitus is some function of my blood circulation, because I can clearly hear my pulse in the worst ear... just this constant pulsing squeal. On occasion it is so loud that I wonder if my head is about to blow open. Blood pressure is good when tested though. Who knows... that's all so complex and interconnected, and then there's the possibility that some of it is imagined or phantom. reply matharmin 25 minutes agoparentIf it's pulsing, get it checked out. Pulsatile tinnitus is often a symptom of a bigger underlying issue. reply zzzeek 39 minutes agoparentprevim familiar with this kind of tinnitus (and many others), what happens if you try a good sinus decongestant? reply umpalumpaaa 38 minutes agoprevI had really bad Tinnitus for years. Then I took a hearing test. Doc concluded that I needed a hearing aid. Then I got the Lyric hearing aid which sits deep inside your ear canal 24/7 and it immediately did not only fix my hearing but also my tinnitus. Its an analoge but digitally programmable hearing aid which needs to be replaced every 3-4 months or so. reply lalalandland 4 minutes agoparentI think I suffer from this. My theory: There are muscles in the ear canal that try to modulate the sound and those muscles tense up and cause issues. I also have sore muscles that get a lot better from use of magnesium supplements and the tinnitus also get slightly better from this use. (It get a lot worse if I stop taking it) reply beefman 15 minutes agoparentprevDoes your tinnitus return when you're not wearing the Lyric devices (or when they're turned off, if that's possible)? reply umpalumpaaa 13 minutes agorootparentIt returns when I turn them off. But this happens only for like 5 minutes every 3-4 months or so (ideally). reply hooverd 5 minutes agoprevRelated, I wonder how YC23 Auricle is doing? reply cromka 1 hour agoprevWonder how will they deal with AirPods themselves losing ability to emit frequencies over time, to prevent false positives/negatives. AFIK they continue to replace AirPods after testing them for sound being out of range. reply jerlam 56 minutes agoparentDo the Airpods perform some kind of self-diagnostic to confirm that it can emit needed frequencies for the hearing test? It would be a PR nightmare if Apple was giving diagnoses for hearing loss, when in reality people were being asked to hear a sound that the Airpods could not emit. reply radicaldreamer 47 minutes agorootparentPretty sure they will simply tell you to have a professional hearing test done. reply fwip 46 minutes agorootparentprevAnecdotally, it can be a very distressing experience. When I was in elementary school I had a hearing test, and as the test went on I could hear fewer and fewer of the tones. The nurse got increasingly surprised and worried, \"really, you can't hear that? what about this one? really?\" I was nearly in tears, thinking that I was nearly deaf and somehow didn't know. Turns out my hearing was fine - the batteries in the hand-held device were nearly dead. (I do have some auditory-processing difficulties, but those were not measured by this test.) reply raverbashing 44 minutes agoparentprevThis sounds like it's caused by \"naturally\" accumulated gunk over time reply Eumenes 1 hour agoprevInteresting, so Apple is providing the cause (Airpods) and treatment (Airpods) for tinnitus! reply snakeyjake 23 minutes agoparentAll modern iPhones, when paired with Airpods, monitor listening volume levels and advise you if you are exposing yourself to too loud a volume too often. This information is tracked via the Health app. I do not know if other devices have the ability to monitor this, but I have had the feature turned on since it was released, at OSHA 8-hour exposure safe limits. reply acdha 58 minutes agoparentprevWhere did you see cause? The paragraph in that section mentions noise but that seems like a weird criticism of a product which has a bunch of features to prevent excessive noise exposure. What would be more interesting to me would be research into whether in-ear designs or active noise cancellation correlate with this at all since those are something humans didn’t evolve with. reply kmfrk 55 minutes agorootparentIt's in reference to conversations like this: https://news.ycombinator.com/item?id=35023808. reply ycombinete 36 minutes agoparentprevCynically I did wonder if they’re using Jennifer Lawrence’s arsehole trick to confound the search results for “Apple AirPods Tinnitus”. reply zzzeek 42 minutes agoprev [–] OK I'll bite (ha ha), why is Apple doing this? they think airpods cause tinnitus? eh reply micromacrofoot 1 minute agoparentthey're going to completely obliterate the hearing aid market for most general cases reply ttpphd 34 minutes agoparentprevTo work on customers who are interested in over-the-counter hearing aids reply evilfred 35 minutes agoparentprev [–] anecdotally, my ENT said he has had an onslaught of young patients w tinnitus who use them. but kids also tend to blast volume. unrelated: another big cause of tinnitus is Viagra. it drives too much blood to the sensitive ear arteries reply whimsicalism 32 minutes agorootparent [–] reported tinnitus is heavily anxiety correlated and younger generations are more anxous, it seems reply evilfred 27 minutes agorootparent [–] makes sense! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers from the University of Michigan released data from a large tinnitus survey involving over 160,000 participants, aiming to improve understanding and management of the condition.",
      "Key findings include that 77.6% of participants have experienced tinnitus, with 15% experiencing it daily, and prevalence increases with age, especially among those 55 and older.",
      "The study, part of Apple's Research app, highlights common management methods and causes, and aims to advance understanding of sound exposure and its impact on hearing health."
    ],
    "commentSummary": [
      "Apple's Hearing Study provides preliminary insights into tinnitus, highlighting the psychological aspects and personal experiences of users.",
      "Key advice includes avoiding overthinking, using distractions, and managing stress and muscle tension, with some users finding relief through hearing aids or addressing health issues.",
      "The study also explores the potential effects of modern audio devices like AirPods on tinnitus, with mixed opinions from users."
    ],
    "points": 132,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1725903382
  }
]
