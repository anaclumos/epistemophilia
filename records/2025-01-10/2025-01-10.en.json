[
  {
    "id": 42655870,
    "title": "I've Acquired a New Superpower",
    "originLink": "https://danielwirtz.com/blog/spot-the-difference-superpower",
    "originBody": "I’ve acquired a new superpower Daniel Wirtz 19 hours ago • Copy link Status Slug Desciption Featured Cover Video Cover Image Social Image Publish date Last edited time Created time URL Yesterday, I was browsing Reddit. Midway through my feed, I stumbled upon a video from a German TV show, where a 9-year-old girl demonstrated her ability to quickly identify the difference between two seemingly similar images. In one example, she was presented with two images of hundreds of coffee beans spread out on a table. To my eye, both images looked exactly the same. But it took her only a couple of seconds to call out that one coffee bean was missing on the left side of the image. She repeated this with other images, always almost instantly calling out the differences. I was baffled. How is this possible? I turned my attention to the comments to see how Reddit reacted to this genius of a kid. The top comment immediately caught my attention: Wait, what? This can't be the case. I paused the video, trying to cross my eyes, but without luck. To give it another chance, I googled for \"simple spot-the-difference puzzle\" and tried again. For a moment, the images just seemed to blur over each other. But then suddenly, the images seemed to lock together in the middle, creating a third image between the two others that I was able to focus on. And the top Reddit comment was right. In the third image, some parts of the image stuck out. They looked a bit like they were shimmering or being lifted from the page. I was instantly able to call out the differences. I went back to the more difficult images from the video. Having trained to cross my eyes helped. It took me a second, but I was able to cross my eyes and get the third image to lock in. And again, I instantly spotted a shimmering detail in the image—the one small difference between the two images. In high anticipation, I demonstrated my newly found skill to my fiancée. She wasn’t as impressed as I thought she would be, but it still felt like I had just drunk a magic potion and gained a special superpower. Try it out yourself Down below, you will find an easy, hard, and superhuman spot-the-difference puzzle. Practice on the easy one first, then work your way forward. Here is the step-by-step technique, that worked for me: Get a bit closer to the screen so you can see both images clearly. Now, cross your eyes and aim to overlap both images. When the images are overlapping, try to focus on the image in the middle. At one point, it should come into focus, and you can relax your eyes. Now look at the image, and the shimmering details should stand out to you. Easy mode Source: jagranjosh.com Solution Hard mode Source: worldofprintiables.com Solution Impossible mode Source: Imgur Solution Update: I’ve posted this on Hacker News and it sparked a thread of interesting comments and stories. Here a link to the post.",
    "commentLink": "https://news.ycombinator.com/item?id=42655870",
    "commentBody": "I've Acquired a New Superpower (danielwirtz.com)364 points by wirtzdan 4 hours agohidepastfavorite177 comments Workaccount2 2 hours agoAt a local bar they had a game machine, and if you got a high score on any of the games, your tab for the evening was free. One of the games was a \"spot the differences\" between two pictures with an ever decreasing timer for each round. Using this trick I was able to easily surpass the high score, and garner a crowd watching me perform this mind numbing feat. Probably my peak fame right there. reply duxup 59 minutes agoparent>Probably my peak fame right there. My son and I always make jokes about everyone's 5 minutes of fame. Some random person on the jumbotron at a sporting event \"Yup, there's his moment, it's over now.\" At least yours got you something ;) reply soco 2 hours agoparentprevI can't overlap the images to save my life - they get like halfway there and that's it... reply PaulHoule 1 hour agorootparentIt's like https://triaxes.com/docs/3DTheory-en/522ParallelCrosseyedvie... which some people struggle with, somebody posted a https://en.wikipedia.org/wiki/Autostereogram to HN yesterday which some people get and others don't. (That's different from the \"cross-eyed stereogram\" because one of them involves having two images and the other one has one image with two images hidden in it) reply mhitza 1 hour agorootparentI can understand why it's hard for some. I've landed on that wiki page a while ago and couldn't figure it out. Then found a similar thing on an itch.io page that was easier for me to figure out. In these later examples (starting with the easy puzzle of the OP, and your 3d examples), I find that I do the process in two stages. Unfocus my sight until the third image shows up in the middle at the correct size (as a blurry mess). Then try to focus the center image. reply PaulHoule 53 minutes agorootparentWhat's more a lot of people (maybe 20%) don't benefit from things like https://www.reald.com/ which is one reason why stereo movies have struggled. (That plus some people get sick... Having both a flat and 3-d movie in two different comes across as money grubbing to the consumer but it is really a money sink to the theater.) reply nis251413 4 minutes agorootparentYeah that's me. I lack stereoscopic vision so such tricks or 3d glasses etc do not work. tartoran 24 minutes agorootparentprevI have a big problem crossing my eyes too while having no problem with the parallel view way seeing stereograms. I am actually going to stop trying as my eyes started to hurt. reply DrSiemer 7 minutes agorootparentWhich one makes things become bigger? I learned that one first and then later figured out the one that makes the mixed image smaller (cross eyed I think?). Now I cannot do the big one anymore. reply waffletower 1 hour agorootparentprevThat happened to me too but I persisted and eventually succeeded. I think I needed to cross my eyes slightly more than I was initially. I have been diagnosed with a minor eye convergence issue which makes it difficult to focus on near field objects in motion -- gaining this superpower was difficult but I did it without a headache thankfully. reply rwmj 1 hour agorootparentprevI spent far too much time as a twenty-something generating autostereograms, which seems to have trained my eyes. I was able to \"cross\" the images on this page very quickly. reply KPGv2 30 minutes agorootparentNB autostereograms require you to move your eyes away from each other, the opposite of crossing them. To put it another way, crossing your eyes is what your eyes do when you're looking at something close to you, while the opposite is when you're looking far away. Which is why for ASGs people advise you to look past the picture. Or why you bring the pic close to your eyes (so close that you basically have no choice but to look beyond the picture) reply iforgotpassword 11 minutes agorootparentYou can easily generate inverted ones that require crossing your eyes to appear properly, but they don't look as nice since they pop out instead of going into the screen/book. reply jeffhuys 41 minutes agorootparentprevDon’t CROSS them. Relax them, like you’re tired and can’t focus on a computer screen. reply jeffhuys 40 minutes agorootparentAlso keep the size low. If you’re having a hard time at 20cm from a 4k 30” monitor, it won’t come easy. Zoom out. reply arka2147483647 26 minutes agorootparentprevYou can actually do it both ways, but which is easiest for whom is different. reply hk__2 30 minutes agorootparentprevThere are two methods, either you cross them either you do like you’re describing. reply jjk7 10 minutes agorootparentprevIt helps me to see the depth and then properly focus to cross them very slightly to start, then as I see the image my eyes adjust to pull it in focus properly. reply Taek 33 minutes agorootparentprevYou might be too close to the screen. reply physicles 2 hours agorootparentprevAre you crossing your eyes (focusing nearer than the object) or diverging them (focusing past it)? Diverging is a harder skill to learn. reply paulsmith 45 minutes agorootparentMy whole life I've been doing stereograms by diverging, but I couldn't get the three images in the post (the pairs would get closer but never fully overlap), so I tried crossing based on your comment. It was way easier than diverging (obviously, since I couldn't do it otherwise), but it took me a few tries, because I think it's actually /too/ easy to cross your eyes compared to diverging - I was way overshooting when I crossed my eyes. The trick was to notice this, and then control the un-crossing until they lined up. reply biomcgary 1 hour agorootparentprevIs diverging harder? I find it easier. Maybe it is from long ago practice on stereograms, but I'm curious if it could be due to neurological/physiological differences. reply leni536 4 minutes agorootparentIt depends on the image. If the two images are too far apart then it could require your eyes to diverge, and not to just converge slightly less. That might be impossible. reply grumbel 47 minutes agorootparentprevCrossing is easier because you can simply hold your finger in front of your eyes and look at that for practice. Diverging requires you to look past the image, meaning you have nothing to really look at, which makes it difficult to figure out what your eyes are even supposed to do. Those stereograms aren't helping much either, since they look like nothing until you get it right. With cross-eye you have instant double-vision that you just need to align. Cross-eye also works across much larger distances, diverging fails when the images are too far apart. reply titzer 1 hour agorootparentprevDiverging is definitely harder, and might be out of focus. To keep in focus I found it easier to focus on the right image and then cross my eyes, rather than staring in the center and then staring through the screen into the distance while trying to make them line up. I used to not be able to do the \"magic eye\" 3d images until recently, and this trick is pretty handy. reply soco 1 hour agorootparentprevNot even sure which one I should try :) but yes tried both to no avail. Maybe it's just not something to achieve in the first try... reply wruza 1 hour agorootparentFor crossing just focus on your finger and then remove it. Looking far away may be harder, and afaik it’s near impossible to look “past infinity”, iow pictures must be less wide than the distance between your eyes. Btw these two methods aren’t equivalent in watching stereograms. If you look at one and see something but it doesn’t really make sense, then it’s probably the opposite chirality. Personally I hate the crossing method because it makes your eyes feel strange for a while. reply unkulunkulu 1 hour agorootparentprevhow I approached crossing: first practice just crossing your eyes and observing how every object has two images in this case and when you slowly “uncross”, they merge back into one. you can use anything in your surroundings. then for the stereogram you do the same, observe the out of focus edges of the left and right pictures, then slowly uncross until left and right image occupy the same spot as though they were the same object. now its out of focus, but one (ok, actually three, because there were two, you “doubled” that by crossing, then merged two of them. but ignore the other two and focus on the merged pair) sometimes you will merge images of the same picture, in this case you are just back at your normal vision, repeat :) then you try to keep them overlapped and focus the vision, try to “believe” that you are really looking at a single object. reply ThrowawayTestr 2 hours agorootparentprevTry on mobile, it's easier if the images are smaller. reply pivo 20 minutes agorootparentWow, yeah it happened immediately for me on mobile while I couldn't get past half way on my monitor. Thanks! reply throwaway743 33 minutes agoparentprevJust got a funny visual of someone going crosseyed and focused on overcoming a challenge in front of them, with a crowd of people cheering them on. reply iforgotpassword 6 minutes agoprevWe got a magic eye book when I was maybe 6 - some time early elementary school. After learning how to do it, and also trying it by crossing my eyes to see an \"inverted\" image, I started doing it whenever I saw some repeating pattern IRL. It was most interesting when it was slightly uneven, for example a fence with sloppily applied vertical planks. Doing the magic eye would make it seem like some of them are closer to you than others. Eventually I tried the same on those \"spot the difference\" games since well it seemed kinda obvious to try, and I was blown away that it accidentally gave me that \"superpower\". I think that was pretty smart for a 6yo. Has only gone downhill ever since. ;-) reply nayuki 2 hours agoprevI discovered this trick independently about a decade ago, to use cross-eyed viewing to easily spot differences between two similar images. Like you said, the parts that mismatch appear to shimmer and be unstable, making them obvious. However, I feel eye strain from doing it, so I prefer other methods. 99% of the time, I do https://en.wikipedia.org/wiki/Blink_comparator instead, just switching between two images with zero flicker and zero displacement offset. Also with both eyes, it's easier to spot certain kinds of subtle differences like color shifts, JPEG-like compression artifacts, tiny differences in antialiased renderings, etc. One benefit of the cross-eyed method, though, is that you can difference videos. But the use case for that is rarer than differencing images. reply jeffhuys 32 minutes agoparentTo reduce eye strain, don’t cross your eyes, but relax them (so, the other way). Instantly clear and snaps together as if magnetic. reply tartoran 21 minutes agorootparentThe problem I have with this is that instead of the images completely overlapping they overlap a section in the middle. I can't get both images to completely overlap and am getting some eye strain from trying to force them. reply the__alchemist 2 hours agoprevIf you've done Magic Eyes, this is straightforward. Was able to get all 3 of the test images quickly. This is with focusing beyond the screen. Focusing in front of the screen is something I am unable to do, and not for want of effort. Also, your eyes might accidentally do this if looking at tiled patterns, e.g. wallpaper. Relative image size (e.g. view distance) is important. reply naet 3 minutes agoparentI'm great at magic eyes / stereograms and have a ton of posters around my house with them, but I still had trouble with seeing the differences in the test images. I easily locked in my focus on the overlapping cat images but only one difference stood out to me. I eventually got them all but it wasn't that easy (maybe with practice I could get there). The differences are noticeable when I focus right on it, but when I'm looking at the whole image it's harder to tell what is missing from one eye. reply johnthedebs 2 hours agoparentprevAs a kid, I got a Magic Eye book and learned to see it by crossing my eyes (ie, focusing in front of the screen). I thought it was pretty interesting when I realized that I was seeing all the images inverted (\"peaks\" were \"valleys\" and vice versa) due to the way I was focusing. Alas, I never was able to see the images \"correctly\". reply kayge 8 minutes agorootparentIt's funny because even if you do the Magic Eye pictures \"correctly\" (focusing past them) you can still get funky images by going too far and locking the surrounding pattern a second time. If I remember right the first time I did this was on a heart picture (similar to [0]), which ends up looking like a big puffy W stacked on top of a slightly larger puffy W :D [0] https://i0.wp.com/www.magiceye.com/wp-content/uploads/2018/1... reply ses1984 2 hours agorootparentprevInstead of crossing your eyes to focus in front of the image, you have to uncross them and focus on something behind the image. Put your finger about six inches in front of your face and then look at the horizon. If the horizon is in focus you should see two fingers. reply whatshisface 1 hour agorootparentFocusing behind is much easier because you can get yourself started by focusing on an actual object. reply aidenn0 1 hour agorootparentFocusing in front can be done by focusing on an actual object too? Many people e.g. put a finger between them and the picture and then remove it. reply whatshisface 57 minutes agorootparentThe finger method interferes more with the third image in my experience. reply andrewla 1 hour agorootparentprevSame -- much harder to get them to go the other way. I'm surprised that cross-eyed random dot stereograms never took off; so much easier to do. reply mikepurvis 2 hours agoparentprevI have a slightly lazy right eye, so this has always come naturally to me, but I will say it's considerably easier to achieve the false focal lock on printed material— something about screens, even quality ones with high refresh rates, just isn't the same. reply adeon 1 hour agoparentprevMaybe we are the opposite. As a kid, I could only do cross-eyed-focus-in-front-of-screen, but not \"focus beyond the screen\". Or a book at the time. So I was able to see the 3D in Magic Eyes, but the 3D effect was inverted. Today as an adult I am able to focus beyond the screen, but it's still much easier for me to do it cross-eyed. I also got all the images in the post almost right away. But my eyeballs focused in front instead > _but there is no natural reason for one’s eyes to diverge. When you’ve finished looking at something close to your face and your eyes need to uncross. So you do that eye movement while still holding the image close to your face. Note you are looking “past” where the image is. As long as the image is closer than your infinity focal view you can do this, it doesn’t have to be close to your face necessarily, Magic Eye posters on walls do work. reply satvikpendem 3 hours agoparentprevSee my other comment about cross view vs parallel view, looks like you can do one vs the other and the author can do the opposite. reply vault 2 hours agorootparentWow. Thanks to MarkusWandel I discovered I can focus images while crossing my eyes and finally understood your comment. I've always done the \"uncrossing\" since I was a kid. reply llm_trw 1 hour agoprevI've used this to quickly read through a few hundred page documents given to us only as a scanned pdf which was too low quality to run ocr (at the time) on. The sleazy counter party was very upset when I came back with notes on them not adding the changes we asked for on the drafts they sent back within minutes of them sending them back. reply layer8 1 hour agoparentI can’t parse your second sentence. reply whatshisface 1 hour agorootparentUncross your eyes! :-) (They're saying that the person who send the contract was trying to trick them, and that they were upset when the trick was caught.) reply fragmede 1 hour agorootparentpreva counterparty is the other person you're signing a contract with who sometimes lies to you and says they changed things when they didn't reply layer8 47 minutes agorootparentIt was the grammar I had trouble with, not the vocabulary. A comma before “within” would have helped a bit. reply codazoda 2 hours agoprevWeird timing. I dunno why this works but I've been using it to see mice. You see, I noticed that I have a mouse problem in my garage. I figure if I've seen one mouse, there are probably more. So, I stood on some stairs in my garage and crossed my eyes to sort of blur the scene. It allowed me to catch movement more quickly and I was quickly watching multiple mice run around the edges of the area. reply mncharity 51 minutes agoparentHmm. I noticed in lectures, if I stilled my eyes, most of the field of view would grey out, except for areas of motion (eg a lecturer's head or writing arm) which appeared normal. After motion stopped in an area, it would slowly grey out. When a motion started, its area would snap to normal, making it easy to spot onsets of motion. Eventually my eyes would twitch, and the whole field would refresh. reply idiotsecant 2 hours agoparentprevThat seems like not the same thing though, right? You're not doing a diff on two images, you're just losing resolution so you can direct more attention to movement. reply thunderbong 2 hours agorootparentI think it's because our peripheral vision is able to observe movements faster. reply Jzush 2 hours agoprevI didn't know this had a name or was considered a skill. I've done this since the 90's when those magic eye books became popular. I even managed \"impossible mode\" in 2 or 3 seconds. reply etaioinshrdlu 3 hours agoprevWho's been doing this since they we're maybe 7 years old :) reply tobr 2 hours agoparentChecking in! I’m frequently baffled by how unaware most people seem to be about the absolute basics of how their eyes work. Like, people don’t even seem to be aware of how their stereo perception is largely made from two images, or any of the implications that has. I actively think about the two images maybe dozens of times per day. reply graypegg 47 minutes agorootparentChill out, that's a bit of hyperbole isn't it? This is just a trick for doing a spot the difference puzzle. It's not exactly a daily task most people are thinking about. Most people at least understand that stereographic vision has something to do with 3D perception because we've all closed 1 eye before. reply jeffbee 1 hour agoparentprevI figured everyone because I had a puzzle book that instructed readers to do this. reply knallfrosch 49 minutes agorootparentWe've had these stereoscopic books with hidden images and I never saw any. So I've been failing at this since 7 years old – does that count? reply nullbyte 30 minutes agoprevMy Piano teacher used to have this book on her coffee table with images like this. You could blur and cross your eyes, and the image would combine to become 3D. But I never knew this technique could be used to spot the difference between images. Very cool discovery! reply jasperry 31 minutes agoprevClaims have been made (outside the medical mainstream) that regularly practicing crossing your eyes helps stave off presbyopia. One does get better at seeing stereograms with practice, so it seems like it at least improves some type of muscle control. reply dogman1050 1 hour agoprevThe stars puzzle helped me find a speck of dirt on my phone screen. reply dylan604 1 hour agoparentThe funny thing about the stars image is this is a common way to find asteroids, comets. It's not limited to just those. Only instead of a bunch of cross eyed astronomers, they overlay and align the images and do subtraction/difference filtering to see what's left. For comets/asteroids, the dot of interest will move between frames. Even just playing back the aligned images as a timelapse can reveal motion. reply voidUpdate 3 hours agoprevI've heard about this before, but I've never actually managed to do it until just now. I needed to sort of \"tune the parameters\" a lot so that my eyes were crossed but also focused, since I've had a lot of trouble actually getting them in focus when I'm doing it, and the effect isn't as pronounced as I expected it to be reply idiotsecant 2 hours agoparentI think you're still not doing it quite right. The effect is really quite obvious and pronounced. reply piva00 1 hour agorootparentIt took me some attempts but I agree, it's very obvious, the 2nd image made it glaringly obvious after I managed it well on the 1st one. My eyes get very watery after just a few seconds though, curious to hear from others how common this side-effect is. reply soperj 32 minutes agoprevAny recommendations for when you can't get the images to quite overlap? I feel like I can get 75% of the way there, but then they start going the other direction. I can do magic eye easily. reply kayge 0 minutes agoparentUse your browser to zoom out and make the images slightly smaller reply error404x 47 minutes agoprevI tried crossing my eyes, but it’s not working for me; I keep seeing things blurry. Maybe I’m doing it wrong. However, I solved the first two puzzles. For the last one, I just guessed randomly. My guess wasn’t exactly correct, but it was close, just a little distance away. reply bootwoot 40 minutes agoparentOne thing I noticed: because you're tricking your eyes into thinking they're observing at a different distance, your brain doesn't seem to correctly account for head tilt (my theory of the diagnosis). Anyway, I think you're head must be exactly level with the image or you'll get double-vision/blur reply sdwvit 37 minutes agoparentprevIt’s way trickier if you have astigmatism reply rstarast 3 hours agoprevthat's why they mirror one of the images in \"find the differences\" in puzzle competitions, e.g. USPC https://wpc.puzzles.com/uspc2024/ reply rererereferred 18 minutes agoprevThis is a game mechanic in one of the trials in Ace Attorney Chronicles. reply kiwiguy1 13 minutes agoprevThis is the coolest thing I have seen on here!! reply evan_ 1 hour agoprevI use this technique to get web layouts pixel-perfect with the mockup, just put both windows next to one another and superimpose them with your eyes. Works great. There are tools that do this by overlaying an image with 50% alpha but it doesn't work as well. Last year when there was a bunch of fuss about Kate Middleton not having made any public appearances there was a minor flap where people claimed that a photo she'd released was just an edit of an earlier photo. There was a tweet presenting two photos, one old and one purporting to be new, where she was holding strikingly similar poses. The claim was that the new one was just an edit of the older one. I used this technique and immediately the minor differences stuck out like a sore thumb- her hand was rotated more in one, her hair was laid differently, etc. reply throw7 50 minutes agoprevI always feel like I'll permanently see cross eyed if I keep doing that. It doesn't help that I was accidentally hit in the head by my double's partner racket in tennis and spent like a minute or two walking around seeing double. Not fun. reply not_a_bot_4sho 2 hours agoprevEvery Asshole knows this trick for targeting spacecraft https://youtu.be/XGdjKvivJA8?si=lRrfl6rHzAsE7nEO reply jogu 1 hour agoprevI remember doing this as a child on our TV that had a picture-in-picture setting. I would set the same channel twice and cross my eyes pretending that it was 3d TV. reply dylan604 1 hour agoparentHow does that work when the two images are different sizes and overlapping? Did your PiP mode have a split screen option? The ones I've seen only allowed moving where the insert was placed (which corner), but it was always a PiP and never a split screen. reply jogu 23 minutes agorootparentYes, it had a split screen option where the two images were the same size and side by side. Can't recall what kind of TV it was... perhaps a Sony? reply belowm 49 minutes agoprevAfter a short period of training, I got to the point where I can see the third image which I can focus on. However, the differences are very subtile and don't stick out at all :/ reply drdo 32 minutes agoparentTry the second test image (the one labelled \"Hard\", with the snow). That one was far easier for me than the first (the cat one). reply norswap 1 hour agoprevWow that's interesting — trying to cross my eyes produces hellish jitter. I suspect it's because my left eye is slighty lazy. But I was able to superimpose the right cat picture onto the left one (it's a lot harder for the more complex sky resort picture). It's pretty eerie, the right picture just slides right up the left one (I did need to figure out the right distance for it). It doesn't help me pick out the differences though, I mostly only see the right picture, and if try to focus my left eye, the right picture slides out. Still, intersting. reply woah 15 minutes agoprevDo people not know about this? reply xkcd-sucks 1 hour agoprevAutostereogramming it doesn't help me lol If it's perfect, the overlapping regions just merge in color, i.e. the cat's paw becomes off-white. If it's not perfect, I still have to attend to which parts are popping in and out. In both cases I still have to compare the merged view to the left and right hand sides. Although it is very nice for illustrating each eye's contributions to the merged view. Just not an attention-saver. reply intalentive 59 minutes agoprevValidates a claim in the predictive processing paradigm. The diff between actual and expected is what matters to error correction. That's where all the relevant information is. reply doctoboggan 3 hours agoprevI've been doing this since I was a kid as well. When I was younger some restaurants would have video monitors with games on them, and one of them was spot the difference. I essentially maxed out the score and still held the highest score when I came back in town years later. I wonder if its still around... reply timthorn 1 hour agoprevThis is effectively how Pluto was discovered (not cross-eyed, but with a tool to help): https://airandspace.si.edu/stories/editorial/finding-pluto-b... reply freecodyx 56 minutes agoprevI used to play this game with a friend when we were at the pub, once we start struggling spotting the differences we know we’re drunk reply manishfoodtechs 39 minutes agoprevI can overlap. But still need to match overlap with anyone.anyway interesting reply cyberax 8 minutes agoprevThis trick had been used in practice to detect fake banknotes and coins, with a device like a two-sided periscope. It allowed a bank worker to put a real coin on one side and the tested sample on the other, so that any differences can be immediately apparent. reply tobr 2 hours agoprevMore things you can use this for: ”Are these two things the same size?” ”Are these things that are supposed to be evenly spaced actually evenly spaced?” ”Are all these things straight/at the same angle?” ”Is the wallpaper pattern aligned everywhere?” ”Is that surface using a repeating texture?” reply klik99 1 hour agoprevI picked this up during the magic eye craze of the 90s, and I will never not find it hilarious how people get shocked at my ability to find the differences. I always share the skill too, it’s one of those things people find impossible until they get it and it’s easy. reply whywhywhywhy 2 hours agoprevThis same technique can be used for a 3D effect: https://old.reddit.com/r/CrossView/ reply breadsniffer 49 minutes agoprevWow. That’s insane! With the trick I can somehow solve them all! reply ChrisMarshallNY 3 hours agoprevDammit! I can't uncross, now! https://www.youtube.com/watch?v=qaz2hxZLycY reply valbaca 58 minutes agoprevLearned this at a young age with the Highlights magazines at the dentist. reply unkulunkulu 53 minutes agoprevHah, old trick, I read pull requests this way for years reply ktzar 3 hours agoprevI used to use that trick with some arcade game that was popular in bars in 00s Spain. People were just impressed! reply TonyTrapp 2 hours agoprevDoesn't work for me. Just like stereograms. I just don't know how to \"tell\" my eyes to cross. Maybe similar to how I didn't figure out until I was 20-something how finger snipping works. Maybe by the time I'm 50 I can cross my eyes...! reply dspillett 7 minutes agoparentThe never work for me because my eyes don't work well together. Just not team players. I'm almost always looking through just one or the other, annoyingly usually the one that would least be preferable. reply alt227 2 hours agoparentprevYou dont 'tell' your eyes to cross, you just look closer or further away. Try looking at the image at normal distance, whilst imaginging that you are looking into the distance at a beautiful view or at the horizon on an ocean. It is this difference in distance focusing which causes the illusion. reply s3krit 2 hours agorootparentActually, given enough practice, you can literally just get your eyes to do it. I started doing magic eye puzzles as a kid and loved it, so just eventually learned how to control my eyes in that way. Even today, if I see any repeating pattern, or even anything vaguely similarly shaped, I can’t help but do it reply jeffhuys 36 minutes agorootparentWhen they snap together it feels soo good… reply JTyQZSnP3cQGa8B 1 hour agorootparentprevWell actually, I've been telling my eyes to cross since I was a child. I can't describe it, it's like tensing a muscle in the eyes or something and you can control the angle with the tension. reply jeffhuys 36 minutes agorootparentI can even rotate my eyes! Did you know we have muscles for that? I trained it in the mirror - try tilting your head and look at your eyes REALLY closely: they rotate a bit to cancel out the tilt. reply Karawebnetwork 2 hours agorootparentprevWhenever I try to do this, the most I get is that the two images touch. The cats in the example are holding paws, but they never overlap. I've been trying to make this work since the old magic images from the 90s, but I've never managed it. I wonder if there isn a hardware limitation related to my eye configuration. reply dgacmu 2 hours agoparentprevTo do it with crossed-eye view, try looking at your finger and slowly moving the finger closer to your eyes until you see a third image come into view in between the two on the screen. At some point your brain will/might let you focus on that image. reply comonoid 1 hour agoprevI knew some people who used it to compare aerial photos (though generally a special device with mirrors was used). reply hk__2 16 minutes agoparent> though generally a special device with mirrors was used Maybe this: https://en.wikipedia.org/wiki/Blink_comparator reply OscarCunningham 2 hours agoprevThis is called 'vdiff' in the Jargon File: https://jargonfile.johnswitzerland.com/vdiff.html reply edelbitter 3 hours agoprevde-clickbaited: to spot minor differences between two images, view them like stereograms reply egypturnash 2 hours agoprevYou can also now free-view stereo image pairs. Congratulations. reply ayeeyeiiieee 25 minutes agoprevITT: everyone telling us their stories about how great they are at stereograms. We get it, you're all super special. reply yegle 2 hours agoprevI feel old knowing https://en.wikipedia.org/wiki/Magic_Eye reply j3s 1 hour agoprevall i managed to do was make myself very dizzy reply tunnuz 3 hours agoprevThat's so cool, thanks for sharing this. I managed to do all of them, in the impossible one I identified correctly the area, but couldn't pinpoint the difference. reply erwincoumans 3 hours agoprevThat's fun, it worked for me as well. I could spot the difference in all images, including the 'impossible mode'. reply chrisbrandow 2 hours agoprevEvery 80’s kid knows this trick from those old books where you cross your eyes to reveal images. reply Damogran6 3 hours agoprevI was really good at the random dot stereograms. This is a really cool recasting of that skillset. reply alt227 3 hours agoparentI think thats the best part of this skill. Everyone did 'Magic Eye' images as a kid, but to be able to take that useless skill and apply it to something more useful and interesting is really cool. reply twolf910616 1 hour agoprevdang it i just did this on a zoom meeting. hopefully no one saw me trying to cross my eyes reply foobarian 1 hour agoprevIt's a sailboat! reply etaioinshrdlu 3 hours agoprevWho's been doing this since they were maybe 7 years old :) reply FL410 3 hours agoprevVery cool. Even the impossible mode one was relatively easy! reply evandrofisico 40 minutes agoprevCome on, I've been doing this since I was like 4 years old, this can't be news for anyone, Am i right??? reply jeffhuys 38 minutes agoparentYes, I didn’t think people would be so amazed by it either. Like it’s mind-blowing that this works or has been thought of. But we first did it once as well, some people just discover it late in life I guess (or not at all). reply dreadlordbone 3 hours agoprevFor some reason when attempting this my neck starts cramping reply alfiedotwtf 55 minutes agoprevThis is also how I used to do Magic Eye images when I was a kid. Although the stereoscopic image was inverted on the z axis, it was a lot easier than to cross eyes by looking further out into the distance reply ThrowawayTestr 3 hours agoprevNeat. If this isn't working for you, try on mobile. It's easier if the images are small. reply dreadlordbone 3 hours agoparentThis helped, thanks. reply rufus_foreman 1 hour agoparentprevYou can also just back away from the screen. For the 3rd one I had to back up about 6 feet from my monitor before it clicked into place, once it is in focus you can move closer to see the difference better. reply 7bit 1 hour agoprevI've done this on these game machines 30 years ago when I was 10. I'm baffled that there are still people who have to figure this out. reply TheRealPomax 1 hour agoprevThis is how I help my family when they're stuck on \"spot the difference\" steam games. It also takes literally any fun out of them, the actual game has to come from not (just) spotting differences, because that task is trivial. reply anarticle 1 hour agoprevIncredible! This technique is also used for the 3d visualization of protein structures, it was called \"cross viewing\": https://imgur.com/cross-views-are-commonly-used-to-view-prot... You cross your eyes to get the two images to line up, hold it there and then try to adjust the focus of your eyes. It's a neat skill to have. reply Fauntleroy 3 hours agoprevI'm not sure this works if you have astigmatism reply ehayes 2 hours agoparentI have a small astigmatism (and wear glasses) and I was able to do it, but I feel like if I did any more today I'd have a headache. reply mikepurvis 2 hours agoparentprevI did until it was corrected with LASIK a year ago, and I could still do magic eyes, both with and and without my glasses on. reply SysComp 2 hours agoprevNot working for me reply robofanatic 2 hours agoprevThanks! now I have a migraine reply a1o 3 hours agoprev [–] A new way of doing git diff, leave both versions of the code side by side and cross your eyes. reply bicx 2 hours agoparentNew business idea: git diff, but it uses Mechanical Turk to hire an army of cross-eyed diff spotters. reply waffletower 1 hour agorootparentThere is a wonderful stable diffusion prompt here. reply justahuman74 2 hours agoparentprevI sure didn't expect there to be a plausibly work-related angle when I came to this thread reply satvikpendem 2 hours agoparentprevSomewhat related, but I've been using the Semantic Diff extension in VSCode, works better than standard git diff. reply louiechristie 3 hours agoparentprev [–] My thoughts exactly. Could be done with a git diff in a VR headset reply xattt 1 hour agorootparent [–] If you’re already spending the computing effort to create the VR image… /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Reddit user discovered a technique to easily spot differences in \"spot-the-difference\" puzzles by crossing their eyes to create a third, shimmering image.",
      "This method allows differences to become more apparent, and the user felt it was like gaining a new skill or \"superpower\" after practicing.",
      "The user shared this technique with others, encouraging them to try it by focusing on the middle image created when crossing their eyes."
    ],
    "commentSummary": [
      "A technique to spot differences between two images by crossing one's eyes, akin to viewing stereograms, was used to win a game machine high score and a free bar tab.",
      "This method highlights differences as shimmering or unstable areas, sparking discussions on its applications and challenges, including eye strain and vision difficulties.",
      "The conversation expanded to related topics such as stereoscopic vision and the use of similar techniques in different fields."
    ],
    "points": 365,
    "commentCount": 179,
    "retryCount": 0,
    "time": 1736519674
  },
  {
    "id": 42652577,
    "title": "Visualizing All ISBNs",
    "originLink": "https://annas-archive.org/blog/all-isbns.html",
    "originBody": "Anna’s Blog Updates about Anna’s Archive, the largest truly open library in human history. Visualizing All ISBNs — $10k by 2025-01-31 annas-archive.li/blog, 2024-12-15 This picture is 1000×800 pixels. Each pixel represents 2,500 ISBNs. If we have a file for an ISBN, we make that pixel more green. If we know an ISBN has been issued, but we don’t have a matching file, we make it more red. In less than 300kb, this picture succinctly represents the largest fully open “list of books” ever assembled in the history of humanity (a few hundred GB compressed in full). It also shows: there is a lot of work left in backing up books (we only have 16%). Background How can Anna’s Archive achieve its mission of backing up all of humanity’s knowledge, without knowing which books are still out there? We need a TODO list. One way to map this out is through ISBN numbers, which since the 1970s have been assigned to every book published (in most countries). There is no central authority that knows all ISBN assignments. Instead, it’s a distributed system, where countries get ranges of numbers, who then assign smaller ranges to major publishers, who might further sub-divide ranges to minor publishers. Finally individual numbers are assigned to books. We started mapping ISBNs two years ago with our scrape of ISBNdb. Since then, we have scraped many more metadata sources, such as Worldcat, Google Books, Goodreads, Libby, and more. A full list can be found on the “Datasets” and “Torrents” pages for Anna’s Archive. We now have by far the largest fully open, easily downloadable collection of book metadata (and thus ISBNs) in the world. We’ve written extensively about why we care about preservation, and why we’re currently in a critical window. We must now identify rare, underfocused, and uniquely at-risk books and preserve them. Having good metadata on all books in the world helps with that. Visualizing Besides the overview image, we can also look at individual datasets we’ve acquired. Use the dropdown and buttons to switch between them. All ISBNs [all_isbns] Files in Anna’s Archive [md5] CADAL SSNOs [cadal_ssno] CERLALC data leak [cerlalc] DuXiu SSIDs [duxiu_ssid] EBSCOhost’s eBook Index [edsebk] Google Books [gbooks] Goodreads [goodreads] Internet Archive [ia] ISBNdb [isbndb] ISBN Global Register of Publishers [isbngrp] Libby [libby] Nexus/STC [nexusstc] OCLC/Worldcat [oclc] OpenLibrary [ol] Russian State Library [rgb] Imperial Library of Trantor [trantor] ⬅ ➡ 🔄 There are lots of interesting patterns to see in these pictures. Why is there some regularity of lines and blocks, that seems to happen at different scales? What are the empty areas? Why are certain datasets so clustered? We’ll leave these questions as an exercise for the reader. $10,000 bounty There is much to explore here, so we’re announcing a bounty for improving the visualization above. Unlike most of our bounties, this one is time-bound. You have to submit your open source code by 2025-01-31 (23:59 UTC). The best submission will get $6,000, second place is $3,000, and third place is $1,000. All bounties will be awarded using Monero (XMR). Below are the minimal criteria. If no submission meets the criteria, we might still award some bounties, but that will be at our discretion. Fork this repo, and edit this blog post HTML (no other backends besides our Flask backend are allowed). Make the picture above smoothly zoomable, so you can zoom all the way to individual ISBNs. Clicking ISBNs should take you to a metadata page or search on Anna’s Archive. You must still be able to switch between all different datasets. Country ranges and publisher ranges should be highlighted on hover. You can use e.g. data4info.py in isbnlib for country info, and our “isbngrp” scrape for publishers (dataset, torrent). It must work well on desktop and mobile. For bonus points (these are just ideas — let your creativity run wild): Strong consideration will be given to usability and how good it looks. Show actual metadata for individual ISBNs when zooming in, such as title and author. Better space-filling curve. E.g. a zig-zag, going from 0 to 4 on the first row and then back (in reverse) from 5 to 9 on the second row — recursively applied. Different or customizable color schemes. Special views for comparing datasets. Ways to debug issues, such as other metadata that don’t agree well (e.g. vastly different titles). Annotating images with comments on ISBNs or ranges. Any heuristics for identifying rare or at-risk books. Whatever creative ideas you can come up with! You MAY completely veer off from the minimal criteria, and do a completely different visualization. If it’s really spectacular, then that qualifies for the bounty, but at our discretion. Make submissions by posting a comment to this issue with a link to your forked repo, merge request, or diff. Code The code to generate these images, as well as other examples, can be found in this directory. We came up with a compact data format, with which all the required ISBN information is about 75MB (compressed). The description of the data format and code to generate it can be found here. For the bounty you’re not required to use this, but it is probably the most convenient format to get started with. You can transform our metadata however you want (though all your code has to be open source). We can’t wait to see what you come up with. Good luck! - Anna and the team (Reddit, Telegram)",
    "commentLink": "https://news.ycombinator.com/item?id=42652577",
    "commentBody": "Visualizing All ISBNs (annas-archive.org)289 points by RyanShook 14 hours agohidepastfavorite84 comments graypegg 23 minutes agoI see that bounty at the bottom, so tossing away my chances here, but this visualization is just asking to be mapped onto a Hilbert Curve. [0] When you \"stripe\" the data like this, points that are sorted close together could end up pretty far apart, since a distance in the Y axis skips an entire row of data as you move down, rather than a distance in the X axis which is 1-to-1 with the source data. If you map it onto a hilbert curve, the X and Y axis mean nothing, but visually points that are close together in the sorted list, will be visually close together in the output image. Since the first part of an ISBN is the country, then the second part is the publisher, and the third part is the title, with a check sum at the end, I would remove the checksum and sort them each as a big number. (no hyphens) You should end up with \"islands\", where you see big areas covered by big publishing countries, with these \"islands\" having bright spots for the publisher codes. Bonus points for labeling these areas! I set up something a while ago [1] for an interview that does this with weather data. It makes the seasons really obvious since they're all grouped together. [0] https://en.wikipedia.org/wiki/Hilbert_curve [1] https://graypegg.com/hilbert (https://github.com/graypegg/hilbertcurveplayground code if anyone wants to go for the prize using this! Please at least mention me if you decide to reuse this code, but I can't stop ya lol) reply WillAdams 6 hours agoprevThe thing is, ISBNs aren't hierarchical --- they are bought in blocks (or even individually at an exorbitant markup, says the guy who bought one to reprint a single book), so this doesn't show anything really interesting/useful. A visualization using LoC or even Dewey Decimal would be far more useful, esp. if it also linked to public domain and copyright-free repositories/lists, say an interactive and visual version of John Mark Ockerbloom's: https://onlinebooks.library.upenn.edu/ reply est31 3 hours agoparentISBN's are hierarchical, what do you mean? Like Gaul, ISBNs are divided into multiple parts, where one part is for the language, another is for the publisher, and the last is for the title. The last part is a checksum. https://en.wikipedia.org/wiki/ISBN#Overview reply WillAdams 2 hours agorootparentYes, but this internal hierarchy for an issued number doesn't tell anything beyond those facts about a specific edition of a specific text. One can't use ISBNs alone to create a hierarchical listing of texts which is useful for anything beyond browsing by language/publisher/order in which the ISBN was generated. A visual and interactive representation of books by LoC or some other cataloging system would actually be useful. reply PaulHoule 56 minutes agorootparentI got into an argument with the manager of South End Press back in '94 about whether 'Futuresplash' (soon to be Macromedia Flash) had a future, he thought it did and he was right. Years later I was working at the library and got a little bit steamed because South End Press was reusing ISBN's after books went out of print which was allowed but, I think, lame. One of my strategies for researching a topic is looking a few up in the OPAC, finding them in the stacks, and finding more books on the topic in those areas. (In the Library of Congress system, machine vision could be under QA56 with the rest of computer science or around TA1630, thus \"areas\".) From time to time I've thought about trying to replicate the feel of this with some kind of UI given that our library moved a lot of the collection into deep archives and we have a very fast 'Borrow Direct' service with other peers) reply convolvatron 1 hour agorootparentprevtotally agree, but thats not in the data. however, since blocks are assigned to agencies associated with countries and publishers, you might find some utility in showing coverage by likely language and/or country of origin and date. reply MarceColl 6 hours agoparentprevIt shows what they want to show, which is mostly how much of the world books they have. Hierarchical has nothing to do with it. reply Finnucane 5 hours agorootparentIt only sort of shows that. ISBNs are issued by edition, not title, so many books would have more than one. And books published before 1970 or so might not be represented at all if they have no recent edition. reply NoMoreNicksLeft 3 hours agorootparentprevThey can't even have a tiny fraction of the world's books. Each edition of the book gets a new ISBN... if a book is released as a paperback, hardback, kindle edition, pdf, and epub then there are supposed to be five ISBNs. The vast, vast majority have only been released as dead-tree versions. They have none of those. The books they scan may have an ISBN, but the scans do not have them. Like all Project Gutenberg books, their books have no ISBNs at all. From a strict point of view, they've released new editions of these books. reply nickelpro 3 hours agorootparentWorthless semantics in the context of the mission of the project. What you've described is that the archived content can be mapped to multiple ISBNs. It's clear the only element of concern here is the content itself. The failure to preserve a particular binding or printer's choice of typeface is irrelevant. Failing to recognize this requires an almost malicious level of pedantry reply mmooss 2 hours agorootparentprev> The books they scan may have an ISBN, but the scans do not have them. Like all Project Gutenberg books, their books have no ISBNs at all. From a strict point of view, they've released new editions of these books. Are you saying they actively remove ISBN numbers from scans? If I downloaded one of the books, it wouldn't have an ISBN? Why? That seems like a bunch of extra processing per book, makes it harder for users to specifically identify a book, and probably does nothing for legality. Also, can people search by ISBN? reply Tomte 1 hour agorootparent> Are you saying they actively remove ISBN numbers from scans? No, he‘s playing the pointless „well, actually a scan of a book is a different thing from the book itself“ game. reply glimshe 41 minutes agoprevAnna's archive is one of the wonders of the world. If we almost destroyed our species but Anna's archive endured, there would be hope for a relatively expedient reconstruction. reply skrebbel 10 hours agoprevI thought it was my color blindness that made me not able to distinguish between the red and green pixels as described (i only see red and black ones), but even with a browser extension that counters color blindness i can't distinguish more colors. Is this just me, or is the graph weird? reply saithound 10 hours agoparentFwiw (not color-blind) I can see red, green and black pixels. The graph doesn't look weird to the naked eye. Find the interactive visualiser by scrolling down, and switch it to \"Files in Anna's Archive [md5]\". This will highlight the location of the green pixels in grey. reply Muehe 3 hours agoparentprevIf you have red-green blindness like me try this: - Right-click the image and select \"Inspect\". - Add a new CSS hue-rotate filter to the element: element { max-width: 100%; margin: 0 auto; filter: hue-rotate(-90deg); } Usually I use \"filter: saturate(100);\", but that didn't really work well for this image. You might have to adjust the rotation degree though, -90 worked best for me. reply superzamp 10 hours agoparentprevThe graph seems to be alright, there are indeed red and (some) green pixels, looks like an issue with your extension unfortunately. reply rendx 10 hours agoparentprevI see green dots and a few lines of green dots. Did you try zooming in? reply psychoslave 10 hours agoparentprevNo idea of were the issue might land, but I can see the difference in colors. reply thaumasiotes 2 hours agoparentprevI see red, green, and a bit of yellow. I assume the yellow is what happens when the red and green pixels come too close to each other. reply Finnucane 5 hours agoparentprevI am also color blind and the graph is not good. reply asfasdfasdfn 10 hours agoparentprevThe graphs are very easy to read, albeit depend on your ability to distinguish between red and green. Can you change the green channel to blue to better view it? reply jdblair 2 hours agoprevIt appears that the IP of the server is blocked in the EU. I get this from my ISP (Ziggo, in the Netherlands): Deze website is geblokkeerd Europese sancties De Raad van Europa heeft besloten dat de websites van RT (voorheen Russia Today) en Sputnik News niet meer mogen worden doorgegeven. De website die je probeert te bezoeken, valt onder deze Europese sanctie. VodafoneZiggo is verplicht de sanctie uit te voeren en heeft de website geblokkeerd. reply voytec 2 hours agoparentWorks in Poland, but here you go: https://web.archive.org/web/20250106112552/https://annas-arc... reply hk__2 1 hour agoparentprevNo issue here in France. reply quink 9 hours agoprevKind of hard to tell what corresponds to what in these graphs, maybe if someone could point out Bookland (i.e. 978), it would be a bit easier to orient oneself? reply seszett 8 hours agoparentMaking it easier to visualise is the whole point of the bounty announced by this post. reply billpg 6 hours agoprevAnyone else seeing this? \"This server couldn't prove that it's annas-archive.org; its security certificate is from *.hs.llnwd.net. This may be caused by a misconfiguration or an attacker intercepting your connection.\" reply masfuerte 4 hours agoparentYes. A DNS request for annas-archive.org to my ISP (EE in the UK) returns an address for www.ukispcourtorders.co.uk, which also gives a security warning. If I click through the warning on either site I get an HTTP 400 error. According to Wikipedia, www.ukispcourtorders.co.uk used to list the blocked domains and the court orders responsible. https://en.wikipedia.org/wiki/List_of_websites_blocked_in_th... reply c0balt 6 hours agoparentprevNo, sounds like you are being mitm for them. Though the domain appears like a legitimate CDN. reply swores 4 hours agoparentprevSame for me reply greenie_beans 6 hours agoprevis it illegal to download and use their isbn file? like what is wrong with having that information? reply karel-3d 5 hours agoparentI don't think this page, which links to libgen and sci-hub, is that concerned about copyright. reply greenie_beans 5 hours agorootparentannoying non-answer to my question. i already know all about anna's archive. i'm asking if a person can download these isbns and use them to make data visualizations without fear of breaking a law? https://software.annas-archive.li/AnnaArchivist/annas-archiv... reply karel-3d 5 hours agorootparentSorry, I misunderstood your question. reply salomonk_mur 5 hours agorootparentprevThey explicitly provide that data for you to do as you wish. They are in a grey area, not you. You can download it no problem. reply greenie_beans 5 hours agorootparentis there legal precedent for that? already asked LLMs so please don't copy/paste an LLM response. reply whataguy 10 hours agoprev> Each pixel represents 2,500 ISBNs. If we have a file for an ISBN, we make that pixel more green. What do you mean by \"more green\"? I don't see any shaded green. And I presume the black pixels are unregistered ISBNs? reply lmm 10 hours agoparentIf you look closely there are definitely some brownish pixels and some dim greens. reply eporomaa 9 hours agoprevHm, I got: \"... European sanctions The Council of Europe has decided that the websites of RT (formerly Russia Today) and Sputnik News may no longer be transmitted. The website you are trying to visit falls under this European sanction. ...\" reply reddalo 9 hours agoparentI think the website is censored at DNS level but they chose the wrong error page. In Italy it just errors out with a NS_ERROR_CONNECTION_REFUSED. reply flir 8 hours agorootparentYou're just cleared up a minor mystery I never bothered to investigate (BT, UK). Thanks. Flipping DNS to 8.8.4.4 fixed it for now but I really need to move this connection to A&A. reply TonyTrapp 9 hours agoparentprevWorks fine here from a European IP. reply jaapz 9 hours agorootparentIt's blocked at least in the Netherlands. Weirdly it mentions it being part of the sanctions against Russia, while from a cursory search I only found a judge ordering the site to be blocked because of copyright issues (thanks Brein). They probably just show the wrong error page? reply Cthulhu_ 9 hours agorootparentMust be ISP specific, I'm also in NL and can access it fine. reply rollulus 7 hours agorootparentprevI'm also in NL. Ziggo's DNS server blocks it: $ dig annas-archive.org @89.101.251.228 annas-archive.org. 360 IN CNAME unavailable.for.legal.reasons. unavailable.for.legal.reasons. 339 IN A 213.46.185.10 213.46.185.10 serves a generic page mentioning Russia Today and the Pirate Bay. Not sure which one applies here. reply seszett 3 hours agorootparent> CNAME unavailable.for.legal.reasons. Not really standards compliant, but an interesting use of DNS. reply Freak_NL 4 hours agorootparentprevSame for KPN: http://195.121.82.125/ Would Tweak have blocked this? Most households in the Netherlands currently have the choice of Ziggo, KPN, and Odido. Long live VPNs… reply xp84 1 hour agorootparentIs that three broadband providers serving the same address?? You guys are so lucky you don’t even know. In America we generally have a choice of one if you aren’t including Starlink or legacy slow satellite. And perhaps a joke of a 1-6Mbps DSL option in some parts. reply rchard2scout 9 hours agorootparentprevIt's blocked by my corporate networking filter for me, in the category \"Illegal downloads\". So the Russian sanctions message is probably incorrect indeed. reply powerhugs 9 hours agoparentprevSwitch DNS to like 1.1.1.1 (Cloudflare) or 8.8.8.8 (Google) reply Over2Chars 9 hours agoprevnext [22 more] [flagged] michaelt 9 hours agoparentSome people in the archiving / 'data hoarding' community feel it's simpler to just back up everything. This attitude is particularly prevalent in the communities that deal with data other people have already digitised. If you're paying $100 per book for someone to visit a major library, get the book out, scan it, check the OCR? Then you'd probably be selective, to get the most out of a limited budget. But if you're grabbing epubs and pdfs, and a book only needs $0.002 of space on a hard drive somewhere? Grabbing the useless 41% is probably cheaper and easier than exercising editorial control. reply jillesvangurp 9 hours agoparentprevThe problem with such judgment is that they are subjective and subject to biases that change over time. Almost every scrap of information from ancient civilizations is considered priceless at this point because so few is left of it. Anything from obscene graffiti, shopping lists, personal messages, etc. All of it. Many autocratic regimes editorialize and censure all forms of publications. But even in the US, which is nominally still a democracy you now have states like Florida forcing changes to literature works and banning books entirely for religious and ideological reasons. And this is not just a right wing thing. There have been a few publishers that took it upon themselves to editorialize literature from the 19th and 20th century to get rid of some things that are now considered sexist, racist or otherwise offensive. The whole cancel culture is not just about canceling people, but about limiting access to their work as well. I was at a Christmas market in Berlin a few weeks ago near the Opera. There's a nice little monument there for the book burning that happened in the 1930s. Anything that was vaguely intellectual or Jewish in origin was burned right there during the Kristallnacht. Nice place for a Christmas market and a grim reminder that those calling for things to be deleted/cancelled aren't necessarily very nice people. And of course Hitler himself got cancelled. Possession or distribution of his books is still not allowed in Germany. Anyway, imagine somebody in 5000 years finding their way to some archive of hacker news or some reddit thread might look differently at the value of some of the comments than the average moderator. reply heinrich5991 7 hours agorootparent> Possession or distribution of his books is still not allowed in Germany. AFAIK this has never been true in Germany (for the book Mein Kampf at least). AFAIK the German state of Bavaria inherited Hitler's copyright on the book, and did not republish it. This means that no one was allowed to print it for copyright reasons, but you could still own or trade existing copies of the book. After 2015, 70 years after Hitler's death, the book entered the public domain. Looking into Wikipedia, uncommented reprints have been forbidden: https://en.wikipedia.org/w/index.php?title=Mein_Kampf&oldid=..., which I didn't know before. reply jillesvangurp 5 hours agorootparentIt seems you are correct and I was only half right. Lets just say that quoting the man in public is still likely to get you in trouble. More than a few AFD politicians are finding that out the hard way. reply 9dev 4 hours agorootparentAnd rightfully so. Germany has a peculiar history in this regard, and that implies a federal obligation to account for it. reply Over2Chars 8 hours agorootparentprevAll action is \"subjective and subject to biases that change over time\". This would then imply I could never take any action, because it's just subjective and biased. Maybe that's an exaggeration of your position, but you do seem to be suggesting inaction or the impossibility of judgement. I reject this position 100%. I would suggest that judgement is a critical part of our civilization, and it's judgement that says those bits of obscene graffiti in Pompeii that makes it so. Or else they could say \"well, we can't claim ancient cave art is priceless, because we're biased and our biases will change over time. Maybe in a thousand years we'll discover that ancient cave art is worthless, so we'll do nothing\". In fact you have judged my opinions and shared your judgement with me. Good job! Your characterization of regimes as autocratic is judgmental, biased and will change over time. But right now that's your judgement and I applaud it, even if I disagree. Gosh, book burning. Not backing up a romance novel or cookbook is definitely analogous to book burning, but I'll play along. It was a symbolic act to show a rejection of ideas, not an attempt to eradicate the books, much in the same way Gandhi encouraged the burning of foreign made clothing and products. He wasn't going to rid the world of British cloth nor were the Germans going to rid the world of non-German ideas. So yeah, when all the badly written cook books, romance novels, and children's books are in a huge bonfire, you can blame me, personally. reply globnomulous 7 hours agorootparent> All action is \"subjective and subject to biases that change over time\". This is poppycock. Backing up all books -- the very action discussed by the person you're answering -- is by definition neither subjective nor subject to biases. > This would then imply I could never take any action, because it's just subjective and biased. And even if the first quoted claim were true, this, too, clearly isn't. Nowhere does the comment you're answering imply that the bias or subjective rationale of an action should, ipso facto, discourage a person from taking it. Your comment is replete with similar reasoning, so warped that it's difficult to characterize as anything other than in bad faith. Indeed, this is the snottiest, rudest, least constructive comment I've seen on HN in quite some time -- excepting a couple of my snotty remarks on language or the quality of someone's writing. I have no idea what response you expect, but the only one you deserve, I think, is one that just points out your dismissiveness, sarcasm, and breathtaking contempt. What an awful way to move through the world, let alone through HN. reply flir 2 hours agorootparent> least constructive comment I've seen on HN in quite some time But we should still archive it. Some day it might be useful to someone ;) reply jillesvangurp 1 hour agorootparentExactly :-) reply jillesvangurp 5 hours agorootparentprevThanks for this. I wasn't going to feed the trolls; but you are not wrong > this is the snottiest, rudest, least constructive comment I've seen on HN in quite some time I wish ;-). I see a lot worse here regularly. But it's certainly not nice behavior. Luckily, I have a thick skin. reply bqmjjx0kac 5 hours agorootparentprevI wouldn't be surprised if they're an LLM-powered bot. reply wizzwizz4 5 hours agorootparentprevI don't think you're doing it on purpose, but this is Holocaust denial. The Nazis did destroy all extant copies of several works – for example, research of the Institut für Sexualwissenschaft. (Edit: Some Judaica were sent to Prague instead of being destroyed – though apparently Hitler's planned Judaism Museum is an urban myth.) They absolutely were trying to utterly destroy – not just symbolically reject – vast swathes of culture. Please don't make stuff up about the Holocaust. It's the sort of mistake you shouldn't make even once. reply simpaticoder 7 hours agoparentprevSturgeon's Law (https://en.wikipedia.org/wiki/Sturgeon%27s_law) states \"90% of everything is crap\" so you're not too far off. reply lyu07282 5 hours agorootparentThis thread should've really summoned Jason Scott, I remember him causally mentioning that he has a backup of every single 4chan post ever made (99% crap in that case, but probably invaluable for future generations of sociologists/historians who want to piece back together where it all went wrong). reply flir 8 hours agoparentprevEveryone's 41% is different. Long tail, innit. reply Over2Chars 8 hours agorootparentGosh. reply flir 8 hours agorootparentYou mention the example of romance novels above. There's a schlocky Victorian pulp novel that's of no use to anyone - except that it happens to contain a fantastically detailed description of an abandoned saltings in my hometown that nobody ever thought to record in any way. For me, those two paragraphs are gold. If the novel hadn't been digitised as part of Google's Books Archive Project, I wouldn't have been able to find those two paragraphs. Digitisation not only creates backups, it enables completely new ways of interacting with those texts (eg Google's Ngram Viewer). reply Over2Chars 7 hours agorootparentWell I guess your one valuable paragraph that matters only to you justifies backing up millions (billions?) of human and soon to be AI generated books, because someone, somewhere, at some time will find a line or two valuable. Maybe. I retract my position, let's back up everything! reply xp84 1 hour agorootparentLet’s say there are ten billion such marginally-useful books published by the time the next few decades. Many epub books are like a couple MB. So 30 petabytes total. That’s something you could fit in one room. One rich guy could buy enough hard drives to do that today. Why not? reply tiagod 5 hours agorootparentprevI think that's the case. IIRC The British Library has copies of all published material in the UK, including flyers and such. What seems banal and useless to you, might be extremely important for future historians, and to be honest, books are pretty compressible and storage is cheap. reply lyu07282 5 hours agorootparentI think its a law in almost all nations in fact that forces publishers to sent a copy of everything they publish to a national archive like that (the US equivalent is the Library of Congress). If you bring up the topic of preservation, most people won't understand why, or even be opposed to the idea, goes to show that sometimes its a good idea to ignore the ignorant public. https://en.wikipedia.org/wiki/Legal_deposit reply sebstefan 9 hours agoprev [–] >$10,000 bounty >There is much to explore here, so we’re announcing a bounty for improving the visualization above. Unlike most of our bounties, this one is time-bound. You have to submit your open source code by 2025-01-31 (23:59 UTC). >The best submission will get $6,000, second place is $3,000, and third place is $1,000. >All bounties will be awarded using Monero (XMR). ? Why are they using crypto, and, weirdly enough, specifically the crypto people use for buying drugs, to award this? Is it some kind of scam? reply yawndex 9 hours agoparentBecause the efforts of Anna's Archive are unfortunately currently very much illegal, and XMR is one of the few cryptocurrencies that can actually offer some privacy to its users. reply sebstefan 8 hours agorootparentI've used XMR before. Just surprised seeing it to pay for legitimate & harmless visualization work. I see, that makes sense reply aprilnya 3 hours agorootparentSo what you’re saying is you think XMR is just for buying drugs, and you’re also saying you’ve used XMR before. Hmmmmmm /s reply fear-anger-hate 8 hours agoparentprevThey use monero because what they are doing (copyright infringement) will get you in to big trouble anywhere in the western world. Without cryptocurrencies much of the modern large scale archival efforts wouldn't be possible, or at the very least would significantly increase risks for the people participating in it. For me this alone is a good enough reason to admit that there are valid reasons for existence of privacy coins. The harm they may cause in the short term via tax avoidance or being used to buy drugs is minimal, but the possibility that because of them archivists are able to fund servers for data that future historians wouldn't have otherwise been able to get their hands on? Priceless. reply Klaus23 9 hours agoparentprevBecause it is a book download site, which is illegal in every country that has copyright, and revealing one's identity with a bank transfer would be a stupid way to go to jail. reply akimbostrawman 8 hours agoparentprev>Why are they using crypto, and, weirdly enough, specifically the crypto people use for buying drugs, to award this? You really have to ask why a illegal/grey site is using currency that is build to protect privacy and anonymity? is this some kind of sarcasm? reply friend_Fernando 9 hours agoparentprev [6 more] [flagged] thomasingalls 8 hours agorootparentMajor efforts at creating \"everything\" libraries are usually looked upon as a positive effort that benefits all of humanity, and we generally mourn the loss of any such effort, regardless of whether the effort is against the laws of the state at the time the effort was undertaken, or even if the collection was created in a morally reprehensible way. See: Library of Alexandria, Library of Congress, GenBank, the Svalbard seed vault, Google Books, Internet Archive and all its efforts, ...the Louvre, and most major museums. In general, we collectively recognize - without having to be told - that preservation of knowledge is a noble and worthy effort that transcends the fleeting whims of a population at a point in time. All that to say, people probably don't need to be tricked into liking such efforts. They're popular because of what they are. reply friend_Fernando 8 hours agorootparentNo one is objecting to knowledge preservation - when you just preserve it instead of abundantly replicating it with a wink. Reasonable people are objecting to copyright law violation, for the simple reason that it disincentivizes further knowledge creation. Even more reasonable people are objecting to weaponizing copyright law violation on behalf of the vilest dictatorship on the planet. reply myrmidon 4 hours agorootparent> Reasonable people are objecting to copyright law violation, for the simple reason that it disincentivizes further knowledge creation. Do you honestly believe that our current copyright framework is mainly aligned at maximizing incentives for knowledge creation? This sounds absurd to me. From my point of view, the copyright framework has been shaped (by continous lobbying efforts) into a system to maximize extraction of profits from existing IPs. That is very different from incentivizing \"knowledge creation\", because the lions share of income is spent on overhead or distributed to shareholders, with the \"knowledge creator\" (i.e. author), gettingweaponizing copyright law violation on behalf of the vilest dictatorship on the planet. How is Annas archive weaponizing copyright violation? How is it furthering Putins interests? reply greenie_beans 6 hours agorootparentprev> Reasonable people are objecting to copyright law violation, for the simple reason that it disincentivizes further knowledge creation. how? reply akimbostrawman 8 hours agorootparentprev [–] what a bunch of nonsense reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Anna's Archive, the largest open library, has visualized all ISBNs to track the progress of book backups, revealing that only 16% of books are backed up so far.",
      "A $10,000 bounty is offered for improving the visualization's zoomability, interactivity, and usability, with prizes for the top three submissions, and all code must be open source.",
      "This initiative is crucial for preserving rare books, as Anna's Archive holds the largest open collection of book metadata."
    ],
    "commentSummary": [
      "The discussion on Hacker News revolves around visualizing ISBNs (International Standard Book Numbers) from Anna's Archive, with suggestions like using a Hilbert Curve for improved data representation.",
      "Users debate the effectiveness of ISBNs for hierarchical data visualization, proposing alternatives such as the Library of Congress system.",
      "The conversation also addresses the legality and ethics of Anna's Archive, which hosts copyrighted material, and the use of Monero cryptocurrency for bounties due to privacy concerns."
    ],
    "points": 289,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1736484355
  },
  {
    "id": 42651275,
    "title": "Predictions Scorecard, 2025 January 01",
    "originLink": "https://rodneybrooks.com/predictions-scorecard-2025-january-01/",
    "originBody": "January 1, 2025 — Dated Predictions Predictions Scorecard, 2025 January 01 rodneybrooks.com/predictions-scorecard-2025-january-01/ [You can follow me on social media: @rodneyabrooks.bsky.social and see my publications etc., at https://people.csail.mit.edu/brooks] This is my seventh annual update on how my dated predictions from January 1st, 2018 concerning (1) self driving cars, (2) robotics, AI , and machine learning, and (3) human space travel, have held up. I promised then to review them at the start of the year every year until 2050 (right after my 95th birthday), thirty two years in total. The idea is to hold myself accountable for those predictions. How right or wrong was I? I have decided to change my rules for myself a little bit after this year, in response to the many many people who have said how much they enjoy seeing my updates. My predictions were mostly for the first few years, and by next year the density of due dates will be very low. So, on the eight anniversary of my first set of predictions, i.e., a year from today, I will be making a new set of predictions centered on the period January 1st 2026 to January 1st 2036, and that will give a new density of predictions where there will be real meat to see how accurately they turned out. What I Want to Achieve and a Changing Hype-driven Landscape The level of hype about AI, Machine Learning and Robotics completely distorts people’s understanding of reality. It distorts where VC money goes, always to something that promises impossibly large payoffs–it seems it is better to have an untested idea that would have an enormous payoff than a tested idea which can get to a sustainable business, but does not change the world for ever. It distorts what young researchers work on as they do not want to be seen as old fashioned even when the current hyped topic is sort of dumb–soon the dumbness is forgotten and the heat of the chase becomes all. It distorts what people think they need to get a degree in at college in order to have good career prospects. I want people to use rational thought processes when they hear about hyped ideas and be able to assess what is really going on, and what is just plain (to use the technical term) bullshit. My Color Scheme and Past Analysis The acronyms I used for predictions in my original post were as follows. NET year means it will not happen before that year (No Earlier Than) BY year means I predict that it will happen by that year. NIML, Not In My Lifetime, i.e., not before 2050. As time passes mentioned years I color then as accurate, too pessimistic, or too optimistic. This year I have added hemming and hawing. This is for when something looks just like what I said would take a lot longer has happened, but the underlying achievement is not what everyone expected, and is not what was delivered. This is mostly for things that were talked about as being likely to happen with no human intervention and it now appears to happen that way, but in reality there are humans in the loop that the companies never disclose. So the technology that was promised to be delivered hasn’t actually been delivered but everyone thinks it has been. I have not changed any of the text of the first three columns of the prediction tables since their publication on the first day of 2018. I only change the text in the fourth column to say what actually happened. This meant that by two years ago that fourth column was getting very long and skinny, so I removed them and started with fresh comments last year. I have kept last year’s comments and added new ones, with yellow backgrounds, for this year. If you want to see the previous five years of comments you can go back to the 2023 scorecard. Overview of changes this year There has been a lot of activity in both self driving cars (the demise of Cruise a big push by Waymo to scale human assisted deployments, and lots of smoke and mirrors from an electric car company) and in AI, where robotics has been pulled into the ultra hyposphere while in generative AI the end of scaling and the introduction of inference mechanisms (!!) have been hotly announced and disputed. The human spaceflight endeavor, as it did last year, has crawled along and again has stretched out dates that were probably too optimistic in the first place. But First.We all know about FOMO, Fear Of Missing Out. In late 2023, for a talk on generative AI that I gave at MIT, I coined another acronym, FOBAWTPALSL, Fear Of Being A Wimpy Techno-Pessimist And Looking Stupid Later. Perhaps that one is a little bit too much of a mouthful to catch on. These two human insecurities lead people to herd-like behavior in establishing and propagating the zeitgeist on almost any topic. They lead to people piling on the hype fiestas, rushing to invest (money, effort, or hope) in marginal ideas once they have become a little bit popular, or believing our airspace is being invaded by foreign drones. “Mounting evidence, and lack thereof, suggests that perhaps the whole craze has been a sort of communal fever dream fueled by crowd mentality, confirmation bias and a general distrust in all things official.” That quote is from the drone story linked to above, but it could well as been about the hype that we are moving towards AGI (Artificial General Intelligence). I want to be clear, as there has been for almost seventy years now, there has been significant progress in Artificial Intelligence over the last decade. There are new tools and they are being applied widely in science and technology, and are changing the way we think about ourselves, and how to make further progress. That being said, we are not on the verge of replacing and eliminating humans in either white collar jobs or blue collar jobs. Their tasks may shift in both styles of jobs, but the jobs are not going away. We are not on the verge of a revolution in medicine and the role of human doctors. We are not on the verge of the elimination of coding as a job. We are not on the verge of replacing humans with humanoid robots to do jobs that involve physical interactions in the world. We are not on the verge of replacing human automobile and truck drivers world wide. We are not on the verge of replacing scientists with AI programs. Breathless predictions such as these have happened for seven decades in a row, and each time people have thought the end is in sight and that it is all over for humans, that we have figured out the secrets of intelligence and it will all just scale. The only difference this time is that these expectations have leaked out into the world at large. I’ll analyze why this continues to happen below in the section on AI and ML. Here is a list of some of those hype cycles that I, personally, have perceived and lived through, as taken from my presentation at MIT in late 2023 that I referenced above re FOBAWTPALSL. Really, was there really hype about all these things? Yes, there was, within the circles that cared. Those circles have gotten wider and wider and when reigning world chess champion Garry Kasparov was beaten by I.B.M.’s Deep Blue computer under tournament conditions in 1997 it was widely reported in the popular press, And it was declared that it was all over for humans. Back in February 2011 a computer program named Watson played on the television game show Jeopardy against all time human champions. John Markoff, legendary technology reporter at the New York Times, wrote stories about this the day before the competition, and the day after, when Watson had indeed beaten the humans, with the same questions (fed as text to it as the same time as the humans heard the questions) all running on a cluster of machines not connected to an outside network. Here are three successive paragraphs from the second of those stories. For I.B.M., the future will happen very quickly, company executives said. On Thursday it plans to announce that it will collaborate with Columbia University and the University of Maryland to create a physician’s assistant service that will allow doctors to query a cybernetic assistant. The company also plans to work with Nuance Communications Inc. to add voice recognition to the physician’s assistant, possibly making the service available in as little as 18 months. “I have been in medical education for 40 years and we’re still a very memory-based curriculum,” said Dr. Herbert Chase, a professor of clinical medicine at Columbia University who is working with I.B.M. on the physician’s assistant. “The power of Watson- like tools will cause us to reconsider what it is we want students to do.” I.B.M. executives also said they are in discussions with a major consumer electronics retailer to develop a version of Watson, named after I.B.M.’s founder, Thomas J. Watson, that would be able to interact with consumers on a variety of subjects like buying decisions and technical support. My personal experience at that time was people I did not know, but who had heard about my role at MIT (as director of the MIT AI Lab, and then founding director of MIT CSAIL, the Computer Science and Artificial Intelligence Lab) would come up to me and ask about the future of medicine. The people were variously doctors or health industry executives. I reassured them that medicine as we knew it then would stay much the same and was not about to be rendered obsolete. And then in 2016 Geoff Hinton, one of the key architects of Deep Learning (which has had undeniable impact on the world) said: “People should stop training radiologists now. It is just completely obvious that within five years deep learning is going to be better than radiologists.” More people asking me whether this was true. It wasn’t in five years and it isn’t now. We need more radiologists than ever. And yes they do use deep learning tools to help them see some things they wouldn’t otherwise see. But they also understand anomalies using causal reasoning and we would be in a sorry state if all radiology was done by programs today. Now look at those plum colored paragraphs above again as you take yourself way back in time to a year or so ago when ChatGPT was just a baby AGI, You can find stories just like this one if you substitute “ChatGPT” for “Watson” and “Microsoft” for “I.B.M.” The things confidently predicted in 2011 (and in 1979, and in 2016) about the end of doctors didn’t happen then and it is not happening now. Nor are all the other jobs ending. Today I get asked about humanoid robots taking away people’s jobs. In March 2023 I was at a cocktail party and there was a humanoid robot behind the bar making jokes with people and shakily (in a bad way) mixing drinks. A waiter was standing about 20 feet away silently staring at the robot with mouth hanging open. I went over and told her it was tele-operated. “Thank God” she said. (And I didn’t need to explain what “tele-operated” meant). Humanoids are not going to be taking away jobs anytime soon (and by that I mean not for decades). You, you people!, are all making fundamental errors in understanding the technologies and where their boundaries lie. Many of them will be useful technologies but their imagined capabilities are just not going to come about in the time frames the majority of the technology and prognosticator class, deeply driven by FOBAWTPALSL, think. But this time it is different you say. This time it is really going to happen. You just don’t understand how powerful AI is now, you say. All the early predictions were clearly wrong and premature as the AI programs were clearly not as good as now and we had much less computation back then. This time it is all different and it is for sure now. Yeah, well, I’ve got a Second Coming to sell you…Self Driving Cars As with flying cars the definition, or common understanding, of what self driving cars really means has changed since my post on predictions seven years ago. At that time self driving cars meant that the cars would drive themselves to wherever they were told to go with no further human control inputs. Now self driving cars means that there is no one in the drivers seat, but there may well be, and in all cases so far deployed, humans monitoring those cars from a remote location, and occasionally sending control inputs to the cars. The companies do not advertise this feature out loud too much, but they do acknowledge it, and the reports are that it happens somewhere between every one to two miles traveled. These inputs are not direct control of the normal human mechanism of control the steering wheel, the brakes, and the accelerator. Rather they are advice that overrides some of the algorithms. For instance, “steer out into the next lane and go around this truck” as the human realizes that the truck is just not going to move (see an anecdote below on the first night I took the new Waymo taxis in San Francisco (I had previously last ridden a Waymo in 2012 in Mountain View)). Why is this difference important? One of the motivations for self driving cars was that the economics of taxis, cars that people hire at any time for a short ride of a few miles from where they are to somewhere else of their choosing, would be radically different as there would be no driver. Systems which do require remote operations assistance to get full reliability cut into that economic advantage and have a higher burden on their ROI calculations to make a business case for their adoption and therefore their time horizon to scaling across geographies. But wait, you might say, isn’t that electric car company that used to be based in California and is now based in Texas going to roll this out imminently and have a fully digital taxi service. They demoed it on a Hollywood movie studio lot just this year, and the cars were painted gold. Hmm. The location of the demo and the fact that the cars, even down to the tires, were painted gold tells you everything you need to know. Both the cars and the humanoid robots at that event were presented as autonomous but in reality they were all tele-operated directly by people (see below in the humanoid section for more details). And that same electric car company is actively hiring people into paying jobs as remote operators. There was a reasonably balanced appraisal from Reuters just after the event, though it does not go into details of the demos. Here is a direct quote from the story: “We do expect to start fully autonomous unsupervised FSD in Texas and California next year.” Musk said. The astute reader will note that this is the 11th year in a row that the CEO of Tesla has made this prediction of the same milestone happening the next year. We can admire the consistency. Actual self-driving is now generally accepted to be much harder than every one believed. The reason that this bait and switch is important to understand is that the promise of inevitable fully self driving technology upended a historical way that new transportation systems have been adopted. In the past whenever we have introduced new transportation mechanisms there have been large investments in infrastructure and that infrastructure is shared and used by everyone. The Romans built roads so soldiers and traded goods could travel long distances–in Europe those road networks are still the basis of today’s road networks. When steam engine driven trains were the new transportation technology vast networks of rails were built allowing goods to move long distances in mere hours or days. When Ford started mass production of automobiles he built roads and the local governments followed and the the Federal government followed, and those roads are what we use today. Actual fully self driving cars promised that no infrastructure changes would be needed to revolutionize how vehicles would be controlled. Each individual vehicle would do what was needed all by itself. As sensors and networks got better there was no need for expensive new infrastructure because of this promise. The promise was false. If government and private partnerships in building smart roads, which was a hot topic in the 1990s. had continued, every one of us would now have smarter safer cars, but still with onboard human drivers taking over in many situations. But we would have had smart freeways where once you were on it your car would be self driving. The road would have had lots of sensors effectively shared across all cars, as that data would have been transmitted to all passing cars. It would have been a fraction of the cost per car compared to the sensing on today’s almost but not really self driving cars like those of Waymo. And we would have had much more accurate congestion data where the root causes of local congestion would have been sensed with semantic understanding rather than just inferring it from the aggregate collection of location data from phones, individual cars, and historical data from roadside sensors. Instead we now have individual corporate actors using a mixture of partial self driving and remote human supervision. The big question is whether the economics of this works at scale, and whether the fake promises will drive out the human drivers in cheaper services and we’ll all end up paying more. Will the level of hype we saw push our decentralized transportation system into the hands of a few wealthy companies, and in effect make it a centralized system where everybody has to pay private companies to be part of it? As a reminder of how strong the hype was and the certainty of promises that it was just around the corner here is a snapshot of a whole bunch of predictions by major executives from 2017. I have shown this many times before but there is one new annotation here for 2024. The years in parentheses are when the predictions were made. The years in blue are the years are the predicted years of achievement. When a blue year is shaded pink it means that it did not come to pass by then. The predictions with orange arrows are those that I had noticed had later been retracted. The prediction that Jaguar and Land-Rover made that they would have fully autonomous cars by 2024 did not come to pass, so I have shaded it pink, Note that every single blue year up until now is shaded pink, and that every one that is shaded pink has still not come to pass. None of the predictions that were out there in 2017 for the next few years have happened. None. There are three more for 2025, and I am sure that a year from now they will all be shaded pink also. One of the big selling points of self driving cars was that they would be safer than cars driven by humans. So far that is not holding up with real data. One electric car maker with self driving software had it disengage when it sensed there would be an accident, supposedly so that the human could take over in a split second. And then the company did not report the incident as the fault of the software as it was no longer controlling the car when the impact occurred. It was reported, and I had this experience myself in my last ride in a Cruise in 2023, that Cruise vehicles would freeze when an accident looked likely, and then not report it as their software’s fault as the car was stationary and was hit by another car. In many reported cases, and in my case, simply continuing to move forward would avert any likely accident (fortunately for me the human driver of the other car slammed on the brakes and did not hit my robot vehicle). In this story from the Washington Post about Federal investigations into the safety incidents with self driving cars, they report that the companies involved claim they have vast amounts of driving on our roads under their belt. Not so. An industry association says autonomous vehicles have logged a total of 70 million miles, a figure that it compares to 293 trips to the moon and back. But it’s a tiny fraction of the almost 9 billion miles that Americans drive every day. The relatively small number of miles the vehicles have driven makes it difficult to draw broad conclusions about their safety. To put that into perspective, the total number of miles driven by all autonomous (sort of) vehicles over the last decade is less than 1% of the miles driven by humans every day in the United States. It is a tiny, tiny portion. Take a look at this embedded video from the Wall Street Journal about investigations of crashes (many of which have been fatal) involving autonomous driving systems. From the audio: “The kinds of things that tend to go wrong with these systems are things like it was not trained on, pictures of an overturned double trailer. It just didn’t know what it was. There were some lights there, but the lights were in unusual positions. A person would have clearly said something big is in the middle of the road. But the way machine learning works is it trains it on a bunch of examples and if it encounters something it doesn’t have a bunch of examples for it may have no idea what’s going on.” [[My own take is that the fetish of end to end learning leads people to leave out well known algorithms that might solve many of these problems (e.g,, the incredibly simple time to collision algorithms based on looming). Yes, end to end learning made speech understanding systems better, but that does not mean it is the appropriate fetish to apply everywhere.]] Pro tip: Think about this history of industry prognostications about fully autonomous driving being just around the corner when you read today’s prognostications about LLMs taking jobs, en masse, in the next couple of years, or humanoid robots being dirt cheap and being able to learn how to do any human manual task real real soon now. You know you have seen this movie before… My own experiences with Waymo in 2024 I have two sorts of experiences with Waymo vehicles. First, as a driver of my own vehicle and sharing road space with them every single time that I drive. And second, as a user of their ride service. The streets of San Francisco had been thick with Waymo vehicles with no driver in them especially in the second half of 2024. As I drive across the city every morning to head down to my robotics/AI startup half way down the peninsula I see them everywhere until I get on to 101. I see them in front of me and behind me and in adjacent lanes as I drive on multilane one way streets. Sometimes I see four of them in a single block. Twice I’ve seen four of them in a line, in my block and could see four of them in a line in the block ahead of me. When I am at four way intersections with no traffic lights I see them participating in the social ritual of taking your turn to drive through the intersection in the order you stopped, except when a pedestrian is crossing in front of you. They do that pretty well. They do less well when they accidentally get into a line of parents’ cars snaking around a corner for school drop off or pickup. Over the last few months I have noticed that in general they are getting more aggressive about stretching the rules, just like people do. Otherwise human drivers (including me) take advantage of their politeness. That aggression is not always welcomed. One morning I saw a workman with a group doing some digging on a road, and holding a sign with SLOW on one side and STOP on the other side have to jump in front of a Waymo to get it to do what he was trying to tell it to do with the sign. STOP. It wasn’t stopping for no stinking sign! The only time I have seen a Waymo go into reverse, ever, was when I was illegally driving the wrong way down a single lane street and we were heading straight at each other. As a rider I feel they are not quite aggressive enough with human drivers some time, so a ride in a Waymo takes longer than with an Uber or Lyft. It is hit and miss where they drop me off. Sometimes they take a place to pull over half a block from my house, even when it is raining. There is no way to adjust what they happen to decide that day, even though I know that they will always be able to pull in right in front of my house. The first time I took a Waymo this year, on the way home it picked me up at a restaurant and then was about to make a right turn. But at that corner there was an 18 wheeler with its lights flashing and surrounded by green cones. It pulled right in behind that truck and waited a long time before it drove forward. I am guessing a remote operator intervened told it to go around because eventually it pulled around it in the lane just to the left. Based on seeing Waymos interact with orange cones I suspect it would have done better if the cones had been orange rather than green. This easily illustrates that the learning that this robot does, and indeed any robot does, is nothing like the learning that people do (see my rant about the seven deadly sins and mistaking performance for competence in the section below on advances in AI and ML). I mostly feel safe when I am a passenger in a Waymo. Sometimes I don’t feel that my driver of an Uber that I am taking rides with Uber that are not as safe as I would prefer. Self Driving Taxi Services There have been three self driving taxi services in the US in various stages of play over the last handful of years, though it turns out, as pointed out above that all of them have remote operators. They are Waymo, Cruise, and Zoox. Waymo and Cruise are similar in that they use conventional cars adorned with lots of sensors. Zoox has purpose built vehicles that have no steering wheel or pedals for brake or accelerator. Waymo and Cruise went for deployments in large parts of two or more cities and have had ride services callable by apps, just as one can do with Uber or Lyft. Zoox is smaller scale, much more restricted in geography, and really not comparable. At this time last year Cruise was in trouble has it had suspended all of its San Francisco operations under pressure from regulators after some bad accidents that happened in a way that never would happen for human driven cars. Briefly, their cars were getting hit at night by emergency vehicles with lights flashing as the Cruise cars crossed intersections. Human drivers see the reflections of lights from such vehicles flashing even if they don’t see the vehicles themselves. The Cruise vehicles were only reacting to flashing lights that they could perceive directly. But the accident that tipped the scales was when a pedestrian crossing in front of a human driven vehicle was hit and went flying in the air landing right in front of a Cruise. The Cruise hit the person (who now disappeared from sight) as a human driver would most likely have done. But then it proceeded to drive 20 feet with the human underneath the vehicle being dragged along as it went into a mode where it was supposed to get off the road. A human driver would not have reacted that way to having been in a collision, even if it was not their fault. The hammer finally fell in December of 2024. General Motors shut down Cruise. The leading paragraphs from this linked story from the Wall Street Journal are: General Motors has scrapped its Cruise robotaxi program after nearly a decade and $10 billion in development, citing the time and costs needed to scale the business and rising competition. GM on Tuesday said it plans to realign its autonomous driving strategy and give priority to development of advanced driver assistance systems, which take over steering and other functions in certain situations and are common on new vehicles today. The automaker said it would continue to develop fully autonomous technology for personal vehicles, and build on the progress of its Super Cruise system, a hands-off, eyes-on driving feature that the company introduced several years ago. GM said it owns about 90% of Cruise and intends to buy out the remaining investors. It plans to combine the technical teams from Cruise and GM into a single effort to advance autonomous and assisted driving. “We want to leverage what already has been done as we go forward in this,” Chief Executive Mary Barra told analysts on a call Tuesday. The Detroit automaker said it expects the restructuring to reduce spending by more than $1 billion annually after the proposed plan is completed, which is expected in the first half of next year. While there are 40 companies that have permits to test autonomous driving in California, alone, the demise of Cruise leaves just one company, Waymo, trying to make an actual go of a digital taxi service in the United States. They have an enormous significant lead over anyone else who wants get into this business and have spent billions of dollars (probably very much north of $10 billion) on this endeavor over the last 15 years. In an email they sent me a couple of weeks ago as a user of their services they reported that they provided 4 million customer rides in 2024. That is approximately 4 million more than any other company in the United States. Waymo Despite being so far out in front it has not been all smooth sailing for Waymo. Early in the year the operations center for Waymo somehow neglected to realize it was Chinese New Year in Chinatown in San Francisco. So Waymo vehicles were routed through that area on the biggest night of celebration. Any human driver would have realized that the streets, i.e., the street surfaces where cars usually drive, were completely packed with humans, no doubt some of whom were intoxicated as well as just being out having a good time. Not so the Waymo vehicles. They tried pushing through the very very dense crowds, no doubt annoying many people. And what do people have at Chinese New Year? Fireworks. So some revelers decided to push back on this robot car invading their space. Here are a couple of pictures of the results. Not pretty. And an example of how taking away people’s agency is never a good idea for robots (see my second law of robotics). Throughout 2024 Waymo has been investigates for various accidents such as those described in this Wall Street Journal article. “Reports included collisions with stationary or semistationary objects, such as gates, chains or parked vehicles, according to the regulator.” In the middle of the summer Waymo added a feature where they would honk their horns at cars in their way. But this backfired when hundreds of Waymos were coming back to their parking lot in the very early hours of the morning, and they started honking at each other and waking up human neighbors. Eventually that got fixed. In late September a motorcade for Kamala Harris in San Francisco was brought to a halt by a Waymo that stopped in the middle of California Street doing a U-turn in front of it. I’m sure this incident was of great concern to the Secret Service. Eventually a San Francisco police officer got into the car and drove it out of the way–this is shown in a video included with the story above. I do not know how the officer got access to the vehicle and whether Waymo remote operations were cooperating. More disturbingly humans outside the Waymos started harrassing humans inside them. The most concerning cases come from the realization that if a woman is in a Waymo at night she will be dropped off, outside, on a public road at the end of her journey with no option but to get out of the car where it has stopped. So groups of men have followed Waymos with women in them and then harassing the woman when she gets out. If she was driving her own car she might be heading to an off road parking space or she might choose not to stop if she knows she is being followed. There are no such options in a Waymo so taking a Waymo at night is less safe than other means of transportation–just follow it and eventually the preyed upon woman will have to get out. Here is a very recent disturbing story about this practice. Meanwhile Waymo managed to raise $5.6B to expand to new cities in 2025. It already operates in parts of San Francisco, Los Angeles, and Phoenix. The new money will let it expand to Austin and Atlanta in the United States and to start operating in parts of Tokyo in Japan. That is expensive expansion. Here is the question for the future of watered down remote monitored “autonomous” driving systems (let’s call it “watered down autonomy”), and it is up to Waymo now. Can Waymo expand fast enough in these new markets in 2025 and take enough business from what is left of traditional taxi operators, along with those operating under the Uber and Lyft models, and do it in a way which is in sight of profitability, so that it has a case to raise the stupendous amounts of money needed to operate in all large cities in the US in the next 10 t0 20 years? If Waymo can not succeed at this in the next two years I think the idea of large scale use of watered down autonomy will be dead for at least a decade or two. Right now full autonomy everywhere is already dead. Prediction [Self Driving Cars] Date 2018 Comments Updates A flying car can be purchased by any US resident if they have enough money. NET 2036 There is a real possibility that this will not happen at all by 2050. Flying cars reach 0.01% of US total cars. NET 2042 That would be about 26,000 flying cars given today's total.Flying cars reach 0.1% of US total cars. NIML First dedicated lane where only cars in truly driverless mode are allowed on a public freeway. NET 2021This is a bit like current day HOV lanes. My bet is the left most lane on 101 between SF and Silicon Valley (currently largely the domain of speeding Teslas in any case). People will have to have their hands on the wheel until the car is in the dedicated lane.Such a dedicated lane where the cars communicate and drive with reduced spacing at higher speed than people are allowed to driveNET 2024 20240101 This didn't happen in 2023 so I can call it now. But there are no plans anywhere for infrastructure to communicate with cars, though some startups are finally starting to look at this idea--it was investigated and prototyped by academia 20 years ago. First driverless \"taxi\" service in a major US city, with dedicated pick up and drop off points, and restrictions on weather and time of day.NET 2021The pick up and drop off points will not be parking spots, but like bus stops they will be marked and restricted for that purpose only. 20240101 People may think this happened in San Francisco in 2023, but it didn't. Cruise has now admitted that there were humans in the loop intervening a few percent of the time. THIS IS NOT DRIVERLESS. Without a clear statement from Waymo to the contrary, one must assume the same for them. Smoke and mirrors. Such \"taxi\" services where the cars are also used with drivers at other times and with extended geography, in 10 major US citiesNET 2025A key predictor here is when the sensors get cheap enough that using the car with a driver and not using those sensors still makes economic sense. 20250101 Imminent dual use of personal cars was the carrot that got lots of people to pay cash when buying a Tesla for the software subscription that would allow thei car to operate in this way. Shockingly the CEO of Tesla announced in smoke and mirrors roll out of Cyber Cab in 2024, that the service would use specially built vehicles to be produced at some indeterminate late date. I got suckered by his hype. This is unlikely to happen in the first half of this century. Such \"taxi\" service as above in 50 of the 100 biggest US cities. NET 2028 It will be a very slow start and roll out. The designated pick up and drop off points may be used by multiple vendors, with communication between them in order to schedule cars in and out.20250101 Even the watered down version of this with remote operators is not gong to happen in 50 cities by 2028. Waymo has it in 3 cities and is currently planning on 2 more in the US in 2025. Dedicated driverless package delivery vehicles in very restricted geographies of a major US city.NET 2023The geographies will have to be where the roads are wide enough for other drivers to get around stopped vehicles. A (profitable) parking garage where certain brands of cars can be left and picked up at the entrance and they will go park themselves in a human free environment.NET 2023The economic incentive is much higher parking density, and it will require communication between the cars and the garage infrastructure.A driverless \"taxi\" service in a major US city with arbitrary pick and drop off locations, even in a restricted geographical area.NET 2032 NET 2032This is what Uber, Lyft, and conventional taxi services can do today. 20240101 Looked like it was getting close until the dirty laundry came out. 20250101 Waymo now has a service that looks and feels like this in San Francisco, 8 years earlier than I predicted. But it is not what every one was expecting. There are humans in the loop. And for those of us who use it regularly we know it is not as general case on drop off and pick up as it is with human drivers. Driverless taxi services operating on all streets in Cambridgeport, MA, and Greenwich Village, NY. NET 2035 Unless parking and human drivers are banned from those areas before then.A major city bans parking and cars with drivers from a non-trivial portion of a city so that driverless cars have free reign in that area. NET 2027 BY 2031 This will be the starting point for a turning of the tide towards driverless cars.The majority of US cities have the majority of their downtown under such rules. NET 2045 Electric cars hit 30% of US car sales. NET 202720240101 This one looked pessimistic last year, but now looks at risk. There was a considerable slow down in the second derivative of adoption this year in the US. 20250101 Q3 2024 had the rate 8.9% so there is no way it can reach 30% in 2027. I was way too optimistic at a time when EV enthusiasts thought I was horribly pessimistic. Electric car sales in the US make up essentially 100% of the sales. NET 2038Individually owned cars can go underground onto a pallet and be whisked underground to another location in a city at more than 100mph. NIML There might be some small demonstration projects, but they will be just that, not real, viable mass market services. First time that a car equipped with some version of a solution for the trolley problem is involved in an accident where it is practically invoked. NIML Recall that a variation of this was a key plot aspect in the movie \"I, Robot\", where a robot had rescued the Will Smith character after a car accident at the expense of letting a young girl die.Electric Cars Last year US manufacturers pulled back on their planned production of EVs. In data from this report we can see that sales dropped at the start of 2024 but have now picked up again. 2022 2022 2022 2022 2023 2023 2023 2023 2024 2024 2024 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 5.3% 5.6% 6.1% 6.5% 7.3% 7.2% 7.9% 8.1% 7.3% 8.0% 8.9% There is steady growth in sales but my prediction of 30% of US car sales being electric by 2027 now seems wildly optimistic. We need two doublings to get there in three years and the doubling rate seems more like one doubling in four to five years. Note that some sources include hybrids and hydrogen powered cars in electric vehicles but I am using the battery electric vehicle (BEV) numbers. To see how the trends are across brands you can see a breakout for Q2 of 2024 here. There appear to be two main headwinds for BEV adoption. Firstly, if one doesn’t have on property residential parking it is hard work in the US to find a place to recharge, and it takes hours for the charging to finish. This will stop many city dwellers from adopting. Secondly the increased tire wear adds up to real money. The maintenance requirements for BEVs are much less than for cars with an internal combustion engine. On the other hand tires do not last as long (I have had to buy four new tires in less than two years owning my first BEV), apparently due to the increased weight of the car. Flying Cars Flying cars are another category where the definitions have changed. Back when I made my predictions it meant a vehicle that could both drive on roads and fly through the air. Now it has come to mean an electric multi-rotor helicopter than can operate like a taxi between various fixed landing locations. Often touted are versions that have no human pilot. These are known as eVTOLs, for “electric vertical take off & landing”. Large valuations have been given to start ups who make nice videos of their electric air taxis flying about. But on inspection one sees that they don’t have people in them. Often, you might notice, even those flights are completely over water rather than land. I wrote about the lack of videos of viable prototypes back in November 2022. Nevertheless there have been wild predictions. I ended a longer version of this component in last year’s annual review with: Also note the size of this vehicle. There are many fossil fuel powered helicopters that are much smaller. This is not going to be a personally owned vehicle for the masses. Don’t hold your breath. They are not here. They are not coming soon. Nothing has changed. Billions of dollars have been spent on this fantasy of personal flying cars. It is just that, a fantasy, largely fueled by spending by billionaires. Robotics, AI, and Machine Learning So what happened in Robotics, AI, and Machine Learning this year? Many, many, many people got just a little bit over excited. That’s what happened. There have been a lot of party tricks and it is the researchers who often play the tricks on themselves without realizing it. This is not new, none of it is new. But there are orders of magnitude more people watching it now, and more people are out to make a buck by being hypesters, promising riches to those who will invest in their irrationally overpriced companies. How could this be? We are seeing mass sinning, lots and lots of people committing some of the seven deadly sins of predicting the future of AI which I wrote about back in 2017 here (or here you can see a professionally edited version of that blog post of mine). Four of those seven sins seem most relevant to today’s hyped up atmosphere around robotics, AI, and machine learning. Here now are short descriptions of these particular four sins, edited down from my earlier much more detailed descriptions. Then I will weave them together to explain how it is still pretty much business as usual, and I mean that in a good way, with steady progress on both the science and engineering of AI. Performance versus Competence One of the social skills that we all develop is an ability to estimate the capabilities of individual people with whom we interact. We use cues from how a person performs any particular task to estimate how well they might perform some different task. We are able to generalize from observing performance at one task to a guess at competence over a much bigger set of tasks. These estimators that we have all inherited or learned do not generalize well to other creatures or machines. We are not good at guessing which smart things other species might be able to do, and we are not good at guessing what an AI system can do when we have seen it do a few tasks in a limited domain. We get it wrong all the time. Indistinguishable from Magic When people cannot explain how something works they cannot know its limits as they do not have any sort of model (nor have they seen enough examples of it before). Arthur C. Clarke said that any sufficiently advanced technology is indistinguishable from magic. In our minds UFOs can do all sorts of amazing things as we have no way of knowing their limits–they may as well be magic, And that is what they become in speculation about them. Isaac Newton spent half his working life on alchemy as he did not know that the nucleus of atoms were not subject to mere chemistry. He would have been just as ignorant of the limitations of an iPhone screen (different sort of apple…), despite his own ground breaking work in optics. Remember, he was a really really smart dude. But even he was not able to develop all the theories needed to understand the world around him, despite his successes with calculus and gravity and the makeup of white light. He attributed properties to chemistry that were way beyond its limits. Exponentialism We have just lived through sixty years of the most phenomenal growth of a technology in the history of humankind. It is the story of silicon-based computation. Everyone has some idea about Moore’s Law, at least as much to sort of know that computers get better and better on a clockwork like schedule. This reality has trained people to think that probably a lot of other things in tech will change exponentially, especially when that thing has a strong computational component. The sin of exponentialism is to argue that some other process is going to follow a Moore’s-like law when it is unwarranted to so argue. Moore’s law worked for so long because in the starting technology of the 1960s the currents used to represent digital information were many many orders of magnitude beyond the minimal physical limit needed to determine whether they were present or not, and hence distinguish a 1 from a 0. Those currents could be halved many times without breaking physics limits. Speed of Deployment New technologies get deployed much more slowly than people imagine. Even software technologies. The old internet protocol, IPv4, can only address two billion, or 2×109, devices, which is way less than the number of people on our planet. A new protocol, IPv6, which can address more than 3×1038 devices was meant to replace it over a two year period of dual use by about 2003. But in 2024 IPv4 was still there and carrying over half the world’s internet traffic despite its inadequacies. Must functioning businesses that operate in the physical world are very averse to taking up new technology as it dramatically increases existential risk to their business. They must foresee immediate and incredibly high return on investment (ROI) to be tempted to move to new technologies. Even the military is slow to adopt new technologies. The US Air Force still flies the B-52H variant of the B-52 bomber. This version was introduced in 1961, making it 63 years old. The last one was built in 1963, a mere 61 years ago. Currently these planes are expected to keep flying until at least 2040, and perhaps longer–there is talk of extending their life out to 100 years. What does this all mean? Right now there is incredible hype for both Large Language Models (LLMs), and all their variations, and for humanoid robots, especially humanoid robots that are going to learn how to do things. The hype is driven by the four sins above. LLMs LLMs have proved amazing facile with language. They have been trained on pretty much all the text that is available on the Web and all the digitized historical books that exist. Miraculously LLMs seem to be able to infer a representation of some sort, that is somewhat independent of the particular human language that they read. So they are able to translate between human languages, and when you ask them just about anything they produce text in the language that you asked in, and that text often seems entirely reasonable and informative. I used the word “miraculously” as we do not really understand why they are able to do what they do. We, of course, know that the architecture for them is built around noticing correlations in vast amounts of text that connect some tens of thousands of tokens which are the components of words in each language that is digested. It is a surprise that they work as well as they they do, and produce coherent sounding language on just about any topic. Here is the original architectural diagram from the 2017 Attention Is All You Need paper: Each column from bottom to top is a pure feed forward network, with no search, no iteration, no conventional algorithm at all. There are inputs at the bottom and then layer upon layer of linear neurons that have numbers or weights stored in them that multiply and add their inputs and threshold that sum to provide an output. The detail in the architectural diagram is how the connections between layers are organized. On the left is an input or question, in a linear string of words, from a user. That gets injected half way up the network on the right and remains constant while a single iteration process runs. The stack on the right outputs a word (or token) and that gets fed back to the bottom of that stack, and a new token pops out the top. All the output tokens that have so far been produced remain in the right bottom input buffer as ordered input. What the network has been trained to do, is given the user input on the left, and what the network has output so far, choose a very likely next word, given the billions of examples it has seen in training. Some randomness is used to choose among a small number of very likely next words at each stage. There are hundreds of billions of weights that get learned and stored in the layers of network to act as multipliers for each individual input to each layer. So now us humans are faced with looking at this system running and our human nature just makes us commit the first two sins from above. It is in our nature and we cannot help ourselves. First, we see really impressive examples of responses to input questions, and if a human was giving those answers we would estimate that person to be quite clever and able to reason. Often though, because they have so many billions of examples on which they were trained LLMs are essentially looking up the question in the weights. The weight if gained from all of human knowledge that is out there on the network in language form. Invisibly the network is perhaps (but not in any intentional way) merging some similar questions, and then merging the answers which were already in the vast data that it has seen. But us dumb humans just think the damn thing is really really smart. Then, since we don’t have a real explanation in our heads for what it is doing we start thinking it is magic, and that there is no real limit to what it is extracting from all that data (that it used a significant portion of the energy budget for many different countries to compute) and how general its capabilities will be. It becomes magic. And then researchers try to show that it can reason, that it has inferred a spatial understanding of the world, that language can be used to do all sorts of things that Moravec’s paradox tells us it can’t. There is a lot of magical thinking that humans do about LLMs. Of course it can diagnose diseases like a doctor talking about them. Of course it can teach a student as well as a human teacher. Of course it can program as well as a human computer programmer. It is magic after all. But in reality the fact that it is just picking likely next words means that in fact we can’t trust its output. Some outputs are great. Some are pure confabulations (most people use the word “hallucinations” for this, but I prefer “confabulations”). And we do not know which we will get ahead of time, or more perniciously how much of each we will get, trustworthy pieces of output and confabulated pieces of output all jumbled together. Not to worry say the proponents, More learning will fix it. Fire up a nuclear power plant (I am not making this up–the tech companies are getting more nuclear power built or activated so that their LLMs can learn what a human learns using just 20 watts powering their brain; I am not confabulating this!!), and we’ll feed it more data and it will become more trustworthy. It is magic after all. But the magic is not going as well as the proponents imagined and promised as this Wall Street Journal story explains. Their imaginations were definitely encourage by exponentialism, but in fact all they knew was that when the went from smallish to largish networks following the architectural diagram above, the performance got much better. So the inherent reasoning was that if more made things better then more more would make things more better. Alas for them it appears that this is probably not the case. But rabid exponentialists have not yet given up. Expect a bunch of VCs to adversely affect the growth of pension funds around the world as pension funds are a prime source of capital that VCs spend. More serious academics are working on boxing in the LLMs with more external mechanism beyond just feeding the output tokens back in as a linear string of input. Many of these mechanisms look a lot like more conventional AI mechanisms, and we will see where these additions prove to be useful, how much of the wheel will be reinvented, and how long (months?, years?, decades?) to get there. And the answers to those last questions will tell us how much sinning has been done by companies in predicting fast deployments. Back in rant at the beginning of this post I gave the example of I.B.M. and Watson and their completely optimistic predictions of how any problems of applying Watson (which seemed extremely competent based on its performance on live TV) to the real world would be solvable. The areas that it was predicted to be applicable came from magical thinking. Surely no one today could be as dumb as that big company was back in 2011. Surely not. No, not us smart inhabitants of 2025. Its us. We are nowhere near as dumb as them!! Humanoid Robots The other thing that has gotten over hyped in 2024 is humanoids robots. The rationale for humanoid robots being a thing is a product of the four sins above and I think way less rooted in reality than the hype about LLMs. In fact I think it is pretty dumb. [[I suspect many people will reason that I cannot have a valid opinion about this precisely because I happen to have built more humanoid robots than anyone else on the planet. So read ahead with caution.]] My first law of robotics states: The visual appearance of a robot makes a promise about what it can do and how smart it is. It needs to deliver or slightly over deliver on that promise or it will not be accepted. The first sentence describes, I think, what is sucking people into believing that humanoid robots have a big future. It looks like a human, so its performance will be like a human, so it will be competent like a human. It’s the performance/competence sin without even waiting for the performance part! The second sentence describes how the humanoid fever will break, and how the hundreds of millions of dollars put into many of these companies (billions of dollars overall) will disappear. The puppets will not perform at acceptable levels. It is easy to see this as you hear all the things investors and CEOs of humanoid robots say they will be able to do. They have hardly even got to the lab demonstration phase. My third law of robotics is: Technologies for robots need 10+ years of steady improvement beyond lab demos of the target tasks to mature to low cost and to have their limitations characterized well enough that they can deliver 99.9% of the time. Every 10 more years gets another 9 in reliability. For real work, robots need to operate with four, five, or six nines. We are a long way from that. The zeitgeist is that we will simply teach the robots to do stuff and then they will be able to do it. BUT, we do not know yet whether that is going to work. In order for it to work you have to both collect the right sort of data and then learn the right things from that data. It is not at all clear to me that we know the answers to make either of those things true. I think it will be an active place for lots of good research for many years to come. There is an excellent survey paper of current research state of the art called Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes. Unfortunately I think the title of the paper is going to confuse many people. “Real-World Successes” to someone like me, who these days deploys robots that people pay for and that provide real ROI, sounds like it is about systems that have been deployed. But on reading the paper it turns out that they mean that it is learning and demonstrations done in a lab setting on physical hardware rather than just in simulations and simulators. And, to me the lab demonstrations are shakier (literally) than I imagined in my third law above. I think we are a long way off from being able to for-real deploy humanoid robots which have even minimal performance to be useable and even further off from ones that have enough ROI for people want to use them for anything beyond marketing the forward thinking outlook of the buyer. Despite this, many people have predicted that the cost of humanoid robots will drop exponentially as their numbers grow, and so they will get dirt cheap. I have seen people refer to the cost of integrated circuits having dropped so much over the last few decades as proof. Not so. They are committing the sin of exponentialism in an obviously dumb way. As I explained above the first integrated circuits were far from working at the limits of physics of representing information. But today’s robots use mechanical components and motors that are not too far at all from physics based limits, about mass, force, and energy. You can’t just halve the size of a motor and have a robot lift the same sized payload. Perhaps you can halve it once to get rid of inefficiencies in current designs. Perhaps. But you certainly can’t do it twice. Physical robots are not ripe for exponential cost reduction by burning wastes in current designs. And it won’t happen just because we start (perhaps) mass producing humanoid robots (oh, but the way, I already did this a decade ago–see my parting shot below). We know that from a century of mass producing automobiles. They did not get exponentially cheaper, except in the computing systems. Engines still have mass and still need the same amount of energy to accelerate good old fashioned mass. This Year’s Prediction Update There is only one new comment in my robotics, AI and ML predictions table this year. There are a bunch of well funded new companies in the home robot space, and perhaps they will come up with new mobility solutions, which in my experience is the big blocker for home robots. Prediction [AI and ML] Date 2018 Comments Updates Academic rumblings about the limits of Deep LearningBY 2017Oh, this is already happening... the pace will pick up.The technical press starts reporting about limits of Deep Learning, and limits of reinforcement learning of game play.BY 2018The popular press starts having stories that the era of Deep Learning is over.BY 2020VCs figure out that for an investment to pay off there needs to be something more than \"X + Deep Learning\".NET 2021I am being a little cynical here, and of course there will be no way to know when things change exactly.Emergence of the generally agreed upon \"next big thing\" in AI beyond deep learning.NET 2023 BY 2027Whatever this turns out to be, it will be something that someone is already working on, and there are already published papers about it. There will be many claims on this title earlier than 2023, but none of them will pan out. 20240101 It definitely showed up in 2023. It was in the public mind in December 2022, but was not yet the big thing that it became during 2023. A year ago I thought it would perhaps be neuro-symbolic AI, but clearly it is LLMs, and ChatGPT and its cousins. And, as I predicted in 2018 it was something already being worked on as the \"attention is all you need\" paper, the key set of ideas, was published in 2017. The press, and researchers, generally mature beyond the so-called \"Turing Test\" and Asimov's three laws as valid measures of progress in AI and ML.NET 2022I wish, I really wish. 20230101 The Turing Test was missing from all the breathless press coverage of ChatGPT and friends in 2022. Their performance, though not consistent, pushes way past the old comparisons. 20240101 The Turing Test was largely missing from the press in 2024 also, and there was a story in Nature commenting on that. So yes, this has now happened. Dexterous robot hands generally available. NET 2030 BY 2040 (I hope!) Despite some impressive lab demonstrations we have not actually seen any improvement in widely deployed robotic hands or end effectors in the last 40 years.A robot that can navigate around just about any US home, with its steps, its clutter, its narrow pathways between furniture, etc. Lab demo: NET 2026 Expensive product: NET 2030 Affordable product: NET 2035 What is easy for humans is still very, very hard for robots. 20250101 A bunch of startups in the home robot space got significant funding in 2024. Two of them are run by ex-CEOs of large companies: iRobot and Cruise (and he was also an intern at iRobot after we were already a public company). So this one may be in play for a lab demo in the next few years if they have this as one of their goals.. A robot that can provide physical assistance to the elderly over multiple tasks (e.g., getting into and out of bed, washing, using the toilet, etc.) rather than just a point solution. NET 2028 There may be point solution robots before that. But soon the houses of the elderly will be cluttered with too many robots.A robot that can carry out the last 10 yards of delivery, getting from a vehicle into a house and putting the package inside the front door. Lab demo: NET 2025 Deployed systems: NET 2028A conversational agent that both carries long term context, and does not easily fall into recognizable and repeated patterns.Lab demo: NET 2023 Deployed systems: 2025 Deployment platforms already exist (e.g., Google Home and Amazon Echo) so it will be a fast track from lab demo to wide spread deployment. 20240101 One half of this happened this year. ChatGPT has been connected to microphones and speakers so you can now talk to it. and It does not fall into recognizable patterns. BUT the other half is the half it does not have; it has no updatable memory apart from its token buffer of what it has just said. Long term context may be long term in coming. An AI system with an ongoing existence (no day is the repeat of another day as it currently is for all AI systems) at the level of a mouse. NET 2030 I will need a whole new blog post to explain this...A robot that seems as intelligent, as attentive, and as faithful, as a dog. NET 2048 This is so much harder than most people imagine it to be--many think we are already there; I say we are not at all there.A robot that has any real idea about its own existence, or the existence of humans in the way that a six year old understands humans. NIML A Parting Shot I recently read a research paper on humanoid robots working in built for human environments. It was based on the argument that the best form for a robot that is to operate in human environments is something tallish and skinny-ish, and probably dynamically balancing, with arms that can reach down to table tops etc., and with a sensor system that can look down from above, as that is what our human environments are optimized for. Here is the first paragraph of the paper: The past decade has seen an explosion of research in humanoid robotics. The stated motivations for this work have varied widely. Many teams have concentrated on bipedal locomotion, some have been interested in human level social interactions, understanding human intelligence, modeling human learning capabilities and others have been more interested in entertainment. Some humanoid robots have had manipulation capabilities on static humanoid platforms and some of that work is aimed at dexterity, plus there has been simple two armed grasping on mobile humanoid platforms. Overall there has been very little work combining dexterous manipulation with humanoid robots, static or mobile–much of that which has appeared, has been concerned with dynamic tasks like pole balancing and juggling rather than manipulation, or has used teleoperated manipulation. Apart from the weird references to pole balancing and juggling this all sounds pretty reasonable and consistent with what is happening today, and with recent history. In fact this is the very first paragraph of the very first paper in the very first issue of the very first volume of the International Journal of Humanoid Robotics. And it was published in 2004, with me as first author. Let me spell that out in case you thought there was a typo in the year. This is from a paper that I and my students and post-docs wrote in the year two thousand and four. Here is the beginning of the contents page for that first issue. You can download the text of that paper here. The journal is now in its 21st year of operation, an on its 21st volume of issues and papers. By the time this paper was written my research group at MIT had been working on and building humanoid robots for twelve years. This paper, about a robot named Cardea, was probably our sixth or seventh humanoid robot. [[In 2008 I started a company that built and shipped thousands of humanoid robots. The picture at the top of this post was taken in China with a line up of humanoids that we had built in Massachusetts and New Hampshire and sold to people in China (before a US initiated trade war with China put an end to it in 2018…irony can be personally hard to take at times…).]] The robot Cardea (Cardea was an ancient Roman goddess of door hinges and handles; these are still a challenge for modern robots…) was a two wheeled dynamically balancing robot that lived in a built-for-humans office environment. Cardea was able to open doors using existing door handles and then make its way through doors it had opened. Pro tip: Just because you heard about a new idea this last year or two doesn’t mean that people haven’t been working on that very same idea for decades. So temper your expectations that it must be about to transform the world. Ideas that transform the world take decades, or centuries of development, and plenty of people long before you have been just as excited about the idea and had thought it was on the verge of taking off. And none of us, including you and me, are likely to be special enough or lucky enough to come along at just the right time to see it all happen. Like all modern humanoid robots Cardea did not walk in a way that used passive dynamics to store energy, and basically modulate the behavior of a passive mechanism that had only low energy input, which is how all animals walk. So, like all modern mobile humanoid robots (and legged robots in general) when things were going awry its control algorithms tried to recover by pumping in large amounts of energy very quickly and sometimes that didn’t quite work and the energy needed to go somewhere. Cardea could be a little dangerous in those circumstances, if it fell on you having just increased its kinetic energy. Even the spring based deployment system for its stick-like legs that were engaged when it realized it was going to fall could be dangerous. This is still a problem with all modern humanoid robots. That is why the tele-operated humanoids that were in the Tesla movie lot theater show a couple of months ago operated in two modes. When they all walked out the human guests were kept away from them. Once they stopped walking and were operating in a very different mode people were allowed to approach them, and then get fooled into thinking they were talking to an AI powered robot when they were really talking to a remote human operator. But the robot was no longer moving its feet, and no longer a source of physical danger as a result. Another pro tip: Don’t stand anywhere near a walking or balancing wheeled humanoid when they are moving or doing any task. I have had some near misses for myself with my own humanoids twenty years ago and more recently with some of the humanoids from new start ups. And more generally never be below any sort of walking robot, no matter how many legs it has, when it is walking up stairs. HUMAN SpaceFLIGHT The numbers of flights in 2024 was not much different from those in 2023 (I neglected to include the flights by China last year). It does not feel like a golden age of human spaceflight, though there were other highlights from SpaceX. Orbital Crewed Flights Three countries put 28 people into orbit in 2024, the United States launched 16 people on five flights and Russia and China launched 6 people each with two launches. So there were nine crewed orbital flights total. Two were private and seven were government flights. The United States: There were four US flights to the International Space Station, starting with the private Axion-3 mission with a crew of four on January 18th. The launch vehicle for this was a SpaceX Falcon 9, and the crew vehicle was a SpaceX Dragon. The remaining US flights to the ISS were paid for by NASA. Two of them were SpaceX flights, with four people on March 4th, the Crew-8 mission, and two people on board Crew-9 on October 25th. The remaining US flight to the ISS was the inaugural crewed flight of Boeing’s Starliner, launched on June 5th atop an Atlas V rocket with two people aboard. They are still stuck in space and will be for a few more months–see the section on Boeing below. The other US mission was also a SpaceX launch and vehicle flight, this time known as Polaris Dawn. It was the second mission paid for by billionaire Jared Isaacman, with him as commander. There was a former US Air Force fighter pilot as mission pilot and two SpaceX employees as mission specialists, giving a total crew size of four. They stayed aloft for five days, launching on September 10th, This mission flew higher above Earth than any mission since Apollo 17, the last lunar landing mission, in 1972. Two of the crew “spacewalked” with their feet inside the Dragon capsule but with their bodies outside. This was the first private spacewalk ever. Now Isaacman has been tapped by the incoming US President to be the administrator of NASA. Russia: There were two Soyuz launches, each with three people, up and down, but different people coming back. The launch dates were March 23rd and September 11the. The six people that launched on Soyuz in 2024 were 3 Russian Cosmonauts 2 NASA Astronauts and one Belarusian commercial airline flight attendant who won a national competition with 3,000 applications. She was the only one not set for a long duration mission and was off the ground for slightly less than 14 days. So there were no space tourists per so, but the Belarusian flyer was most likely included as part of Russia’s efforts to keep in good favor with Belarus which has aided it in its war in Ukraine, and was certainly not part of the regular scientific program of the ISS. China: There were two flights of Shenzhou (a larger more modern version of Soyuz) that were crewed in 2024. Both flights were to the Tiangong Space Station and both took along three Taikonauts, first on April 25th and then on October 9th. Both crews were assigned long duration missions and now the crews are overlapping previous crews at Tiangong so it is now being continuously occupied. The first handover this year took about five days and the second about three and a half weeks. Both times there were six Taikonauts onboard Tiangong at the same time. Suborbital Crewed Flights There have been two companies providing space tourism flights on suborbital flights. Blue Origin launches a capsule on top of a reusable rocket, New Shepard, and the capsule lands using a parachute and a brief rocket blast right before hitting the ground (similar to how Soyuz lands). Virgin Galactic has a winged craft which is carried aloft by a bigger a jet engined airplane, it separates at high altitude within the atmosphere and rockets into space. It flies back and lands on a runway. Both companies are run by billionaires who made their money in other businesses. Both billionaires have flown to space on their own craft. Both companies have aimed to have regular launches with lots of tourists, but neither has gotten to that scale and so far only a very small number of the many people who have paid a substantial deposit have been able to fly. Blue Origin had a failure with an uncrewed version of the vehicle in 2022 and only flew one flight in 2023 which was also uncrewed. This year they flew three crewed flights on May 19th, August 29th, and November 22nd, each with six passengers (the system is automated and requires no pilots). In 2021 and 2022 they also had three flights, so there has now been nine crewed flights total. The first two took four passengers and the remaining seven have had six passengers, so altogether they have flown 50 people above the Karman line, 100 kilometers above Earth. This is not yet a regular cadence, nor a large scale tourist business. In 2024 Virgin Galactic had two flights, each with two crew from the company and four passengers. These flights were on January 26th and June 8th. Virgin Galactic flights are now on hiatus, awaiting a new bigger and better vehicle in about two years. Virgin Galactic has had a total of twelve flights since December 13th in 2018. Three have had two people on board and nine have had six people on board, for a total of sixty filled seats that have crossed the Karman line. The total number of different people is smaller as the two pilot seats on each flight have been occupied by a small number of people who have flown multiple times. So, in 2024 thirty people went on suborbital flights, and altogether there have been 110 people on these commercial suborbital flights. Space tourism on suborbital flights has yet to take off in a regular or scaled way. Prediction [Space] Date 2018 Comments Updates Next launch of people (test pilots/engineers) on a sub-orbital flight by a private company.BY 2018A few handfuls of customers, paying for those flights.NET 2020A regular sub weekly cadence of such flights.NET 2022 BY 202620240101 There were four flights in 2021, three in 2022, and seven, five with customers on board, in 2023--all of them by Virgin Glactic. Blue Origin did not fly in 2023. At this point 2026 is looking doubtful for regular flights every week. 20250101 Now 2026 is looking impossible given the data from 2023 and 2024, and one of the two companies being on hiatus for all of 2025, and well into 2026. Regular paying customer orbital flights. NET 2027 Russia offered paid flights to the ISS, but there were only 8 such flights (7 different tourists). They are now suspended indefinitely. 20240101 There were three paid flights in 2021, and one each in 2022, and 2023, with the latter being the Axiom 2 mission using SpaceX hardware. So not regular yet, and certainly not common. 20250101 There were two paid flights in 2024. Next launch of people into orbit on a US booster.NET 2019 BY 2021 BY 2022 (2 different companies)Current schedule says 2018. 20240101 Both SpaceX and Boeing were scheduled to have crewed flights in 2018. SpaceX pulled it off in 2020, Boeing's Starliner did not fly at all in 2023, but is scheduled to launch with people onboard for the first time in April 2024. 20250101 The second company did finally launch humans into orbit in June 2024, so it has happened three years later than I predicted and six years later than what had been promised when my prediction was made. Of course, everyone implicitly assumed that along with getting humans into space the companies would also be able to bring them back. Not so for Boeing. Two paying customers go on a loop around the Moon, launch on Falcon Heavy.NET 2020The most recent prediction has been 4th quarter 2018. That is not going to happen. 20240101 Starship launched twice in 2023 but didn't get to orbit either time. This is going to be well over six years later than the original prediction by the CEO of SpaceX. 20250101 The billionaire who signed up for this and paid a hefty deposit in 2017 gave up waiting and cancelled the contract in 2024. This fantasy is over, for now at least. Land cargo on Mars for humans to use at a later date NET 2026SpaceX has said by 2022. I think 2026 is optimistic but it might be pushed to happen as a statement that it can be done, rather than for an pressing practical reason. 20240101 I was way too optimistic, and bought into the overoptimistic hype of the CEO of SpaceX even though I added four years, doubling his estimated time frame. 20250101 I can now call this as orbital mechanics and Hohmann transfer windows dictate that the cargo would need to have been launched a few months ago for it to get to Mars in 2025. It has not been launched. Humans on Mars make use of cargo previously landed there. NET 2032 Sorry, it is just going to take longer than every one expects.First \"permanent\" human colony on Mars. NET 2036 It will be magical for the human race if this happens by then. It will truly inspire us all. Point to point transport on Earth in an hour or so (using a BF rocket). NIML This will not happen without some major new breakthrough of which we currently have no inkling. Regular service of Hyperloop between two cities.NIMLI can't help but be reminded of when Chuck Yeager described the Mercury program as \"Spam in a can\".20240101 Calling this one 26 years early. As of today no-one is still working on this in an operating company. Boeing’s Starliner First announced in 2010 Boeing’s Starliner was originally scheduled to fly a human crew in 2018. It carried out its second uncrewed flight in May 2022, and finally did make its first crewed flight on June 5th. The crew of two docked with the ISS, but there were problems with multiple gas thrusters for fine motion during the docking. The original plan was that the crew would stay on the ISS for about a week and then return to Earth for a touchdown on to hard soil (as all Russian and Chinese crewed missions end along with all Blue Origin sub-orbital flights). The option of that return was considered, but the thrusters were on a section of the vehicle which is discarded along the way before the landing so there was no possibility of getting a look at the hardware back on Earth. So a program of tests while docked to the ISS was started delaying the crew return. Eventually it was decided that it was too risky for the crew to return on the craft and so it returned empty on September 7th, landing in New Mexico. As it happened, although there were more anomalies with the thrusters the crew would have landed safely had they been on board. Now the crew was stranded in space with no designated ride home. It was decided to remove two crew from the Crew-9 launch and have the Starliner astronauts, Barry Wilmore and Sunita Williams, fly back on that SpaceX Dragon with the other two, which after additional delays is now scheduled to happen some time in March 2025. Their one week visit to the ISS will have stretched out to nine months by then. Boeing has committed to fixing the problems with Starliner. The boosters that it uses are no longer being built, but there are five existing ones reserved for the five additional contracted flights that Boeing has with NASA. They are supposed to happen once per year. We do not know at this point, but I think it would not be a huge surprise if Starliner never flies again. SpaceX Falcon 9 Once again the Falcon 9 launch system has broken all sorts of records for number of launches and reuse. During 2024 there were 132 single booster launches. For two of those flights no attempt was made to recover the first stage (there is a performance penalty for the primary payload in order to recover the first stage). One attempted recovery failed when the booster (on its 23rd flight) caught fire as it landed on the recovery barge. Another booster has since flown a total of 24 times. In terms of mission success all but one of these flights succeeded; one failed when the second stage failed during re-ignition for adjusting the orbit. There were also two Falcon Heavy, the three booster version, launches, both of which succeeded. One of the had successful landings for the two side boosters, but there was no attempt to recoer the central booster on that flight and no attempt to recover any of the three boosters on the other Heavy flight. This brings the total number of launches of the single booster version to 417 along with 11 launches of the three booster Heavy version. These numbers are way beyond the number of launches for any other orbital booster. Additionally it is the only flying orbital system that is reusable at the moment, though Blue Origin and Rocket Lab both plan on joining the club soon. It is worth, once again, looking at how long it has taken to get to a total (across both single booster and Heavy triple booster versions) of 428 launches, with only three failures to deliver the payload to where it was intended to go. The first launch occured in June 2010, and there were a total of 4 launches in the first three years. The first successful booster recover happened on the 20th flight, in December 2015, five and a half years in. The first reuse of a booster occured in 2017, in the 8th year of the program. Since 2021 there has been a steady increase in the number of launches per year, Year # of launches 2010 2 2011 0 2012 2 2013 3 2014 6 2015 7 2016 8 2017 18 2018 21 2019 13 2020 26 2021 31 2022 61 2023 96 2024 134 SpaceX had previously gotten satellites to orbit with its first rocket, the Falcon 1. Falcon 9 has been a spectacular success. But it was not instantaneous. It took time to build from the cadence of launches, about 10 years before the hockey stick curve showed up. Deployment is never sudden but comes after a long build. SpaceX Starship Starship is SpaceX’s superheavy two stage rocket, designed to put 150 tons of payload into orbit, but also be able to go to the Moon or Mars. There is the booster which is designed only to work in Earth atmosphere with 33 Raptor engines both to get the second stage high enough and fast enough and to let the first stage have a controlled return to the launch site. The second stage, called Starship, is both a booster and the payload. It has three Raptor engines and three Raptor vacuum engines. The Raptor engines are designed to get the Starship into orbit after the first stage drops away, and to guide the Starship as it returns to its Earth launch site. The Raptor vacuum engines are meant for breaking out of Earth orbit and going to the Moon or Mars, and to do soft landings on those two bodies where there is no or almost no atmosphere. In 2024 SpaceX made steady progress with four launches of the two stages coupled together. The first two launches lead to both stages blowing up. The third and fourth launches were a big improvement. As with earlier flights they launched from the coast of Texas. In both cases the second stage did a reentry burn on it first orbit and then did a soft landing in a target zone in the Indian Ocean. In the third flight the main booster returned to the launch site and hovered next to the launch tower betweeen two giant arms which then captured it and the engines shot down successfully. It was sifficiently damaged during flight however, that it was not reusable. In the fourth flight there were health anomalies to the first stage was ditched in the Gulf of Mexico. On the fourth flight there was both less heat shielding and much less damage from heat during reentry. This is definite forward progress. But it is still quite a long way from both being operational and both stages being reusable. And it is even further away from being human rated. This is the vehicle that the CEO of SpaceX recently said would be launched to Mars and attempt a soft landing there. He also said that if successful the humans would fly to Mars on it in 2030. These are enormously ambitious goals just from a maturity of technology standpoint. The real show stopper however may be human physiology as evidence accumulates that humans would not survive three years (the minimum duration of a Mars mission, due to orbital mechanics) in space with current shielding practices and current lack of gravity on board designs. Those two challenges may take decades, or even centuries to overcome (recall that Leonardo Da Vinci had designs for flying machines that took centuries to be developed…). The President of SpaceX may be taking a leaf out of the CEO’s always overly optimistic predictions. In November she said “I would not be surprised if we fly 400 Starship launches in the next four years”. Looking at the success of Falcon 9 it is certainly plausible that I may live to see 400 Starship launches in a four year period, but I am quite confident that it will not happen in the next four years (2025 through 2028). One more thing. Back when I first made the predictions there had been an announcement by the CEO of SpaceX that in 2018 the company was under contract to send a very rich paying customer in a trip around the moon in 2018, launched on a Falcon Heavy. I was completely skeptical. Over the years the date got pushed back and pushed back, and the proposed flight vehicle was changed to be Starship. As we all know the flight of the Japanese billionaire around the Moon still hasn’t happened. In 2024 Yusaku Maezawa finally gave up waiting and cancelled the contract. NASA Artemis NASA’s plan is that the second Artemis mission, using the Orion Capsule, Artemis II, will fly to the Moon with four people aboard, the first crewed Artemis flight. An uncrewed flight of Orion around the Moon flew in 2022. The crewed flight was scheduled to launch in May 2024, but it was first delayed by six months and then a little more and in the last year it has slipped another full year. It is now scheduled to fly in April 2026. Artemis III was scheduled to launch in 2025 with a return to the surface of the Moon. However that relied on using a Starship (itself refueled in LEO by 14 (yes, fourteen!!) other Starship launches) to land there. No one any longer believes that schedule, and willlikely delay a few years, given where Starship is in its development and current capability. The officieal schedule says mid 2027, but that seems unlikely. You can find the architecture of the Artemis III mission at this website. Blue Origin Orbital BE-4 Engines and New Glenn The suborbital tourist flights that Blue Origin operates are not its main business. It has ambitions to compete head to head with SpaceX. Another billionaire vs billionaire competition. It has developed the BE-4 engine designed to fly 100 times, and to power the first stage of its massive New Glenn rocket (see below). But in the meantime it has started selling the BE-4 to ULA (United Launch Alliance) to power their Vulcan Centaur heavy launch vehicle. It’s first stage uses two BE-4 engines, along with a variable number of solid fuel strap ons. Vulcan Centaur flew two times in 2024 and the BE-4 engines worked perfectly both times, on January 8th and again on October 4th. This is a solid validation of the engine’s capabilities. Blue Origin’s own first orbital class rocket, New Glenn, is massive, and comparable to the Flacon Heavy (three boosters) rather than the Falcon 9 in capability. It has been in development for a long time, but saw its first visits to a launch pad, fully stacked in 2024. The first stage uses seven BE-4 engines, and is intended to land on a barge and be fully reusable. The second stage uses two BE-3U engines, a variant of the single engine used on their New Shepard sub-orbital space tourism vehicle. There is a project underway to make a fully reusable version of the second stage. Launch seems imminent. Here it is at the launch pad in November 2024. On Friday December 27th, 2024, it was fully fueled in both stages and went through a countdown and fired its seven BE-4 engines for 24 seconds. Now it will leave the pad to have its payload installed. The launch could be as early January 6th. The very first launch will be an all up affair, attempting to get something to orbit and land the booster on its first flight. This is a very different development approach to that used by SpaceX. Let’s Continue a Noble Tradition! The billionaire founders of both Virgin Galactic and Blue Origin had faith in the systems they had created. They both personally flew on the first operational flights of their sub-orbital launch systems. They went way beyond simply talking about how great their technology was, they believed in it, and flew in it. Let’s hope this tradition continues. Let’s hope the billionaire founder/CEO of SpaceX will be onboard the first crewed flight of Starship to Mars, and that it happens sooner than I expect. We can all cheer for that. Categories: Dated Predictions",
    "commentLink": "https://news.ycombinator.com/item?id=42651275",
    "commentBody": "Predictions Scorecard, 2025 January 01 (rodneybrooks.com)224 points by timr 18 hours agohidepastfavorite166 comments sashank_1509 16 hours agoFeels too self-congratulatory when he claims to be correct about self driving in the Waymo case. The bar he set is so broad and ambiguous, that probably anything Waymo did, would not qualify as self driving to him. So he think humans are intervening once every 1-2 miles to train the Waymo, we’re not even sure if that is true, I heard from friends that it was 100+ miles but let us say Waymo comes out and says it is 1000 miles. Then I bet Rodney can just fiddle with goal post and say that 3.26 trillion miles were driven in US in 2024, and having a human intervene 1000 miles would mean 3.26 billion interventions, and that this is clearly not self driving. In fact until Waymo disables Internet on all cars and prices it never needs any intervention ever, Rodney can claim he’s right, even then maybe not stopping exactly where Rodney wanted it to, might be proof that self driving doesn’t work. Next big thing after deep learning prediction is clearly false. LLM is deep learning, scaled up, we are not in any sense looking past deep learning. Rodney I bet wanted it to be symbolic AI, but that is most likely a dead end, and the bitter lesson actually holds. In fact we have been riding this deep learning wave since Alex-Net 2012. OpenAI talked about scaling since 2016 and during that time the naysayers could be very confident and claim we needed something more, but OpenAI went ahead and proved out the scaling hypothesis and passed the language Turing test. We haven’t needed anything more except scale and reasoning has also turned out to be similar. Just an LLM trained to reason, no symbolic merger, not even a search step it seems like. reply benreesman 13 hours agoparentWaymo cars can drive. Everything from the (limited) public literature to riding them personally has me totally persuaded that they can drive. DeepMind RL/MCTS can succeed in fairly open-ended settings like StarCraft and shit. Brain/DeepMind still knocks hard. They under-invested in LLMs and remain kind of half-hearted around it because they think it’s a dumbass sideshow because it is a dumbass sideshow. They train on TPU which costs less than chips made of Rhodium like a rapper’s sunglasses, they fixed the structural limits in TF2 and PyTorch via the Jax ecosystem. If I ever get interested in making some money again Google is the only FAANG outfit I’d look at. reply vessenes 4 minutes agorootparentAgreed Waymo cars can drive. Also I don't believe that, say, when a city bus stops on a narrow street near a school crosswalk, that the decision to edge out and around it is made on board the car, as I saw recently. The \"car\" made the right decision, drove it perfectly, and was safe at all times, but I just don't think anyone but a human in a call center said yes to that. reply KKKKkkkk1 9 minutes agorootparentprevWhich structural limits of TF2 and PyTorch were fixed via the Jax ecosystem? reply tylerflick 13 hours agorootparentprevI can tell you as someone that crosses paths almost everyday with a Waymo car, they absolutely due work. I would describe their driving behavior as very safe and overly cautious. I’m far more concerned of humans behind the wheel. reply benreesman 13 hours agorootparentI especially love how they can go fast when it’s safe and slow when the error bars go up even a little. It’s like being in the back seat of Nikki Lauda’s car. reply Bootvis 11 hours agorootparentAs shown here: https://www.youtube.com/watch?v=hVZ8NyV4pXU reply benreesman 11 hours agorootparentPerfect clip out of all of YouTube. reply fouronnes3 1 hour agorootparentprevDoes Waymo run on JAX? reply tsimionescu 10 hours agoparentprevI think that, if it were true that Waymo cars require human intervention every 1-2 miles (thus requiring 1 operator for every, say, 1-2 cars, probably constantly paying attention while the car is in motion), then it would be fair to say that the cars are not really self driving. However, if the real number is something like an intervention every 20 or 100 miles, and so an operator is likely passively monitoring dozens of cars, and the cars themselves ask for operator assistance rather than the operator actively monitoring them, then I would agree with you that Waymo has really achieved full self driving and his predicitons on the basic viability have turned out wrong. I have no idea though which is the case. I would be very interested if there are any reliable resources pointing one way or the other. reply skywhopper 5 hours agorootparentI disagree that regular interventions every two trips where you have no control over pickup or dropoff points counts as full self driving. But that definition doesn’t even matter. The key factor is whether the additional overhead, whatever percentage it is, makes economic sense for the operator or the customer. And it seems pretty clear the economics aren’t there yet. reply laweijfmvo 15 hours agoparentprevWaymo is the best driver I’ve ridden with. Yes it has limited coverage. Maybe humans are intervening, but unless someone can prove that humans are intervening multiple times per ride, “self driving” is here, IMO, as of 2024. reply Denzel 15 hours agorootparentIn what sense is self-driving “here” if the economics alone prove that it can’t get “here”? It’s not just limited coverage, it’s practically non-existent coverage, both nationally and globally, with no evidence that the system can generalize, profitably, outside the limited areas it’s currently in. reply AlotOfReading 14 hours agorootparentIt's covering significant areas of 3 major metros, and the core of one minor, with testing deployments in several other major metros. Considering the top 10 metros are >70% of the US ridehail market, that seems like a long way beyond \"non-existent\" coverage nationally. reply Denzel 12 hours agorootparentYou’re narrowing the market for self-driving to the ridehail market in the top 10 US metros. That’s kinda moving the goal posts, my friend, and completely ignoring the promises made by self-driving companies. The promise has been that self-driving would replace driving in general because it’d be safer, more economical, etc. The promise has been that you’d be able to send your autonomous car from city to city without a driver present, possibly to pick up your child from school, and bring them back home. In that sense, yes, Waymo is nonexistent. As the article author points out, lifetime miles for “self-driving” vehicles (70M) accounts for less than 1% of daily driving miles in the US (9B). Even if we suspend that perspective, and look at the ride-hailing market, in 2018 Uber/Lyft accounted for ~1-2% of miles driven in the top 10 US metros. [1] So, Waymo is a tiny part of a tiny market in a single nation in the world. Self-driving isn’t “here” in any meaningful sense and it won’t be in the near-term. If it were, we’d see Alphabet pouring much more of its war chest into Waymo to capture what stands to be a multi-trillion dollar market. But they’re not, so clearly they see the same risks that Brooks is highlighting. [1]: https://drive.google.com/file/d/1FIUskVkj9lsAnWJQ6kLhAhNoVLj... reply AlotOfReading 11 hours agorootparentThere are, optimistically, significantly less than 10k Waymos operating today. There are a bit less than 300M registered vehicles in the US. If the entire US automotive production were devoted solely to Waymos, it'd still take years to produce enough vehicles to drive any meaningful percentage of the daily road miles in the US. I think that's a bit of a silly standard to set for hopefully obvious reasons. reply HenryBemis 5 hours agorootparentprev> ..is a tiny part of a tiny market in a single nation in the world. Calculator was a small device that was made in one tiny market in one nation in the world. Now we all got a couple of hardware ones in our desk drawers, and a couple software ones on each smartphone. If a driving car can perform 'well' (Your Definition May Vary - YDMV) in NY/Chicago/etc. then it can perform equally 'well' in London, Paris, Berlin, Brussels, etc. It's just that EU has stricter rules/regulations while US is more relaxed (thus innovation happens 'there' and not 'here' in the EU). When 'you guys' (US) nail self-driving, it will only be a matter of time til we (EU) allow it to cross the pond. I see this as a hockey-stick graph. We are still on the eraser/blade phase. reply dingnuts 7 minutes agorootparentif you had read the F-ing article, which you clearly did not, you would see that you are committing the sin of exponentiation: assuming that all tech advances exponentially because microprocessor development did (for awhile). Development of this technology appears to be logarithmic, not exponential. ivanbalepin 14 hours agorootparentprevSpeaking for one of those metro areas I'm familiar with: maybe in SF city limits specifically (where they still are half the Uber's share), but that's 10% of the population of the Bay Area metro. I'm very much looking forward to the day when I can take a robo cab from where I live near Google to the airport - preferably, much cheaper than today's absurd Uber rates - but today it's just not present in the lives of about 95+% of Bay Area residents. reply stouset 13 hours agorootparent> preferably, much cheaper than today's absurd Uber rates I just want to highlight that the only mechanism by which this eventually produces cheaper rates is by removing having to pay a human driver. I’m not one to forestall technological progress, but there are a huge number of people already living on the margins who will lose one of their few remaining options for income as this expands. AI will inevitably create jobs, but it’s hard to see how it will—in the short term at least—do anything to help the enormous numbers of people who are going to be put out of work. I’m not saying we should stop the inevitable forward march of technology. But at the same time it’s hard for me to “very much look forward to” the flip side of being able to take robocabs everywhere. reply kiba 5 hours agorootparentPublic transit would also remove lot of jobs and yet nobody suggesting we shouldn't build more public transit because it will remove jobs. This is just coming from using what we already know how to do better. reply stouset 3 hours agorootparentPublic transit has a fundamentally local impact. It takes away some jobs but also provides a lot of jobs for a wide variety of skills and skill levels. It simultaneously provides an enormous number of benefits to nearby populations, including increased safety and reduced traffic. Self-driving cars will be disruptive globally. So far they primarily drive employment in a small set of the technology industry. Yes, there are manufacturing jobs involved but those are overwhelmingly going to be jobs that were already building human-operated vehicles. Self-driving cars will save many lives. But not as many as public transit does (proportionally per user) And it is blindingly obvious they will make traffic worse. reply AlotOfReading 12 hours agorootparentprevPeople living on the margins is fundamentally a social problem, and we all know how amenable those are to technical solutions. Let's say AV development stops tomorrow though. Is continuing to grind workers down under the boot of the gig economy really a preferred solution here or just a way to avoid the difficult political discussion we need to have either way? reply stouset 11 hours agorootparentI'm not sure how I could have been more clear that I'm not suggesting we stop development on robotaxis or anything related to AI. All I'm asking is that we take a moment to reflect on the people who won't be winners. Which is going to be a hell of a lot of people. And right now there is absolutely zero plan for what to do when these folks have one of the few remaining opportunities taken away from them. As awful as the gig economy has been it's better than the \"no economy\" we're about to drive them to. reply stavros 7 hours agorootparentThis is orthogonal. You're living in a society with no social safety net, one which leaves people with minimal options, and you're arguing for keeping at least those minimal options. Yes, that's better than nothing, but there are much better solutions. The US is one of the richest countries in the world, with all that wealth going to a few people. \"Give everyone else a few scraps too!\" is better than having nothing, but redistributing the wealth is better. reply stouset 3 hours agorootparentI agree. But this is the society we live in now. We don’t live in one where we take care of those whose jobs have been displaced. I wish we did. But we don’t. So it’s hard for me to feel quite as excited these days for the next thing that will make the world worse for so many people, even if it is a technological marvel. Just between trucking and rideshare drivers we’re talking over 10 million people. Maybe this will be the straw that breaks the camel’s back and finally gets us to take better care of our neighbors. reply stavros 3 hours agorootparentYeah but it doesn't work to on the one hand campaign for not taking rideshare jobs away from people on an online forum, and on the other say \"that's the society we live in now\". If you're going to be defeatist, just accept those jobs might go away. If not, campaign for wealth redistribution and social safety nets. reply dullcrisp 12 hours agorootparentprevDo you ever drive yourself or would you feel guilty not paying a driver? reply danenania 2 hours agorootparentprevWaymo has approval to operate in San Mateo County so it’s likely coming pretty soon. reply rrr_oh_man 10 hours agorootparentprev> preferably, much cheaper than today's absurd Uber rates You haven’t paid attention to how VC companies work. reply AlotOfReading 14 hours agorootparentprevWaymo's current operational area in the bay runs from Sunnyvale to fisherman's wharf. I don't know how many people that is, but I'm pretty comfortable calling it a big chunk of the bay. They don't run to SFO because SF hasn't approved them for airport service. reply kccqzy 14 hours agorootparentI just opened the Waymo app and its service certainly doesn't extend to Sunnyvale. I just recently had an experience where I got a Waymo to drive me to a Caltrain station so I can actually get to Sunnyvale. reply AlotOfReading 14 hours agorootparentThe public area is SF to Daly City. The employee-only area runs down the rest of the peninsula. Both of them together are the operational area. Waymo's app only shows the areas accessible to you. Different users can have different accessible areas, though in the Bay area it's currently just the two divisions I'm aware of. reply riffraff 11 hours agorootparentWhy would you consider the employee-only area? For that categorization to exist it must mean it's either unreliable for customers or too expensive cause there's too much human drivers on the loop. Either way it would not be considered as an area served by self driving, imo. reply AlotOfReading 4 hours agorootparentThere are alternative possibilities, like \"we don't have enough vehicles to serve this area appropriately\" or \"we don't have statistical power to ensure this area meets safety standards even though it looks fine\", and \"there are missing features (like freeways) that would make public service uncompetitive in this area\" to simply \"the CPUC hasn't approved a fare area expansion\". It's an area they're operating legally, so it's part of their operational area. It's not part of their public service area, which I'd call that instead. reply modeless 13 hours agorootparentprevI wish! In Palo Alto the cars have been driving around for more than a decade and you still can't hail one. Lately I see them much less often than I used to, actually. I don't think occasional internal-only testing qualifies as \"operational\". reply jsnell 12 hours agorootparentprevWhere's the economic proof of impossibility? As far as I know Waymo has not published any official numbers, and any third party unit profitability analysis is going to be so sensitive to assumptions about e.g. exact depreciation schedules and utilization percentages that the error bars would inevitably be straddling both sides of the break-even line. > with no evidence that the system can generalize, profitably, outside the limited areas it’s currently in That argument doesn't seem horribly compelling given the regular expansions to new areas. reply Denzel 11 hours agorootparentAnalyzing Alphabet’s capital allocation decisions gives you all the evidence necessary. It’s safe to assume that a company’s ownership takes the decisions that they believe will maximize the value of their company. Therefore, we can look at Alphabet’s capital allocation decisions, with respect to Waymo, to see what they think about Waymo’s opportunity. In the past five years, Alphabet has spent >$100B to buyback their stock; retained ~100B in cash. In 2024, they issued their first dividend to investors and authorized up to $70B more in stock buybacks. Over that same time period they’ve invested believes their money is better spent buying back their stock, Alphabet has to buy back their stock because of the massive amount of stock comp they award. reply davedx 9 hours agorootparent> Alphabet has to buy back their stock because of the massive amount of stock comp they award. Wait, really? They're a publically traded company; don't they just need to issue new stock (the opposite of buying it back) to employees, who can then choose to sell it in the public market? reply yakz 7 hours agorootparentIt's much better comp if the value of the stock goes up. reply davedx 9 hours agorootparentprevThat's a very hand wavy argument. How about starting here: > Mario Herger: Waymo is using around four NVIDIA H100 GPUSs at a unit price of $10,000 per vehicle to cover the necessary computing requirements. The five lidars, 29 cameras, 4 radars – adds another $40,000 - $50,000. This would put the cost of a current Waymo robotaxi at around $150,000 There are definitely some numbers out there that allow us to estimate within some standard deviations how unprofitable Waymo is reply jsnell 8 hours agorootparent(That quote doesn't seem credible. It seems quite unlikely that Waymo would use H100s -- for one, they operate cars that predate the H100 release. And H100s sure as hell don't cost just $10k either.) You're not even making a handwavy argument. Sure, it might sound like a lot of money, but in terms of unit profitability it could mean anything at all depending on the other parameters. What really matters is a) how long a period that investment is depreciated over; b) what utilization the car gets (ot alternatively, how much revenue it generates); c) how much lower the operating costs are due to not needing to pay a driver. Like, if the car is depreciated over 5 years, it's basically guaranteed to be unit profitable. While if it has to be depreciated over just a year, it probably isn't. Do you know what those numbers actually are? I don't. reply YetAnotherNick 14 hours agorootparentprevHere in the product/research sense, which is the hardest bar to cross. Making it cheaper takes time but generally we have reduced cost of everything by orders of magnitude when manufacturing ramps up, and I don't think self driving hardware(sensors etc) would be any different. reply Denzel 12 hours agorootparentIt’s not even here in the product/research sense. First, as the author points out, it’s better characterized as operator-assisted semi-autonomous driving in limited locations. That’s great but far from autonomous driving. Secondly, if we throw a dart on a map: 1) what are the chances Waymo can deploy there, 2) how much money would they have to invest to deploy, and 3) how long would it take? Waymo is nowhere near a turn-key system where they can setup in any city without investing in the infrastructure underlying Waymo’s system. See [1] which details the amount of manual work and coordination with local officials that Waymo has to do per city. And that’s just to deploy an operator-assisted semi-autonomous vehicle in the US. EU, China, and India aren’t even on the roadmap yet. These locations will take many more billions worth of investment. Not to mention Waymo hasn’t even addressed long-haul trucking, an industry ripe for automation that makes cold, calculated, rational business decisions based on economics. Waymo had a brief foray in the industry and then gave up. Because they haven’t solved autonomous driving yet and it’s not even on the horizon. Whereas we can drop most humans in any of these locations and they’ll mostly figure it out within the week. Far more than lowering the cost, there are fundamental technological problems that remain unsolved. [1]: https://waymo.com/blog/2020/09/the-waymo-driver-handbook-map... reply shrubble 12 hours agorootparentprevDoes Wayne operate in heavy rain and any kind of snow or ice conditions? reply bhelkey 2 hours agorootparentThe author specifically calls out that the taxi service needs not operate in all weather conditions or times of day. > First driverless \"taxi\" service in a major US city, with dedicated pick up and drop off points, and restrictions on weather and time of day. However, their analysis this year is that, \"This is unlikely to happen in the first half of this century.\" The prediction is clear. The evaluation is dishonest. reply khafra 12 hours agoparentprev> So he think humans are intervening once every 1-2 miles to train the Waymo Just to make sure we're applying our rubric fairly and universally: Has anyone else been in an Uber where you wished you were able to intervene in the driving a few times, or at least apply RLHF to the driver? (In other words: Waymo may be imperfect to the point where corrections are sometimes warranted; that doesn't mean they're not already driving at a superhuman level, for most humans. Just because there is no way for remote advisors to provide better decisions for human drivers doesn't mean that human-driven cars would not benefit from that, if it were available.). reply tsimionescu 10 hours agorootparentTo apply this benchmark, you'd have to believe that Waymo is paying operators to improve the quality of the ride, not to make the ride possible at all. That is, you'd have to believe that the fully autonomous car works and gets you to your destination safely and in a timely manner (at the level of a median professional human driver), but Waymo decided that's not good enough and hired operators to improve beyond that. This seems very unlikely to me, and some of the (few) examples I've seen online were about correcting significant failures, such as waiting behind a parked truck indefinitely (as if it were stopped at a red light) or looping around aimlessly in a parking lot. You'd also have to believe that when you wished to change how your Uber driver drove, you'd actually have improved things rather than worsened them. reply lukeschlather 5 hours agorootparentLet's suppose Waymo's fully automated stuff has tenfold-fewer fatal collisions than a human. There's no way to avoid the fatal accidents a human causes, and the solution to Waymos getting stuck sometimes is simple. The point is that the Waymo can actually be described as superior to a human driver, and the fact that its errors can be corrected with review is a feature and not a bug - they optimize for those kinds of errors rather than unrecoverable ones. reply gwern 15 hours agoparentprevThe Waymo criticisms are absurd to the point of dishonesty. He criticizes a Waymo for... not pulling out fast enough around a truck, or for human criminals vandalizing them? Oh no, once some Waymos did a weird thing where they honked for a while! And a couple times they got stuck over a few million miles! This is an amazingly lame waste of space, and the fact that he does his best to only talk about Tesla instead of Waymo emphasizes how weak his arguments are, particularly in comparison to his earliest predictions. (Obviously only the best self-driving car matters to whether self-driving cars have been created.) \"Nothing ever happens\"... until it does, and it seems Brooks's prediction roundups can now be conveniently replaced with a little rock on it with \"nothing in AI ever works\" written on it without anything of value being lost. reply elicksaur 5 hours agorootparentIt’s interesting that in my reading of the post I felt like he hardly talked about Tesla at all. He calls out that Tesla FSD has been “next year” for 11 years, but then the vast majority of the self-driving car section is about Cruise and Waymo. He also minorly mentions Tesla’s promise of a robotaxi service and how it is unlikely to be materially different than Cruise/Waymo. The amount of space allocated to each made sense as I read it. For the meat of the issue: I can regularly drive places without someone else intervening. If someone else had to intervene in my driving 1/100 miles, even 1/1000 miles, most would probably say I shouldn’t have a license. Yes, getting stuck behind a parked car or similar scenario is a critical flaw. It seems simple and non-important because it is not dangerous, but it means the drive would not be completed without a human. If I couldn’t drive to work because there was a parked car on my home street, again, people would question whether I should be on the road, and I’d probably be fired. reply davedx 9 hours agorootparentprevInteresting, that wasn't my takeaway from the article at all! Direct quote from the article: > Then I will weave them together to explain how it is still pretty much business as usual, and I mean that in a good way, with steady progress on both the science and engineering of AI. There are some extremely emotional defences of Waymo on this comment thread. I don't quite understand why? Are they somehow inviolable to constructive criticism in the SV crowd? reply 4ndrewl 10 hours agoparentprevNonsense. If you spoke about self-driving cars a few decades ago you would have understood it to have meant that you could go to a dealer and buy a car that would drive itself, wherever you might be, without your input as a driver. No-one would have equated the phrase \"we'll have self-driving cars\" with \"some taxis in a few of US cities\" reply bhelkey 2 hours agorootparentThe prediction is: > First driverless \"taxi\" service in a major US city, with dedicated pick up and drop off points, and restrictions on weather and time of day. Their 2025 analysis is: \"This is unlikely to happen in the first half of this century.\" The prediction is clear. The evaluation is dishonest. reply Schiendelman 5 hours agorootparentprevThat's how all innovation works. Ford never said people asked for a faster horse, but the theory holds. It doesn't matter what benchmarks you set, the market finds an interesting way to satisfy people's needs. reply mvdtnz 15 hours agoparentprevYour objection to him claiming a win on self driving is that you think that we can still define cars as self driving even when humans are operating them? Ok I disagree. If humans are operating them then they simply are not self driving by any sensible definition. reply sashank_1509 15 hours agorootparentHuman interventions are some non zero number in current self driving cars and will likely be that way for a while. Does this mean self driving is a scam and in fact it is just a human driving, and that these are actually ADAS. Maybe in some pedantic sense, you are right but then your definition is not useful, since it lumps cruise control/ lane-keeping ADAS and Waymo’s in the same category. Waymo is genuinely, qualitatively a big improvement above any ADAS/ self driving system that we have seen. I suspect Rodney did not predict even Waymo’s to be possible, but gave himself enough leeway so that he can pedantically argue that Waymo’s are just ADAS and that his prediction was right. reply mvdtnz 14 hours agorootparentNo one said scam (although in the case of Tesla it absolutely is). It's just not a solved problem yet. reply jdminhbg 14 hours agorootparent> It's just not a solved problem yet. Human driving isn't a solved problem either; the difference is that when a human driver needs intervention it just crashes. reply tsimionescu 10 hours agorootparentThis is not about crashes. By all accounts, the Waymo cars are mostly fully self driving, I beleive even the article author agrees with that. This includes crash avoidance, to the extent that they can. The remote operation seems to be more about navigational issues and reading the road conditions. Things like accidentally looping, or not knowing how to proceed with an unexpected obstacle. Things that don't really happen to human drivers, even the greenest of new drivers. reply jdminhbg 2 hours agorootparentOk, but crashes are much worse than navigational issues or accidentally looping. It’s only status quo bias that makes us think driving is more solved if you get the accidental looping fixed before the crashing. reply skywhopper 5 hours agorootparentprevSome of them are scams, yes. For stuff like Waymo, it definitely doesn’t match the hype at the time he made the original predictions. As pointed out above, there were people in 2016 claiming we’d be buying cars without steering wheels that could go between any two points connected by roads by now. reply Spivak 15 hours agorootparentprevYeah, I think semi-autonomous vehicles are a huge milestone and should be celebrated but the jump from semi-autonomous to fully-autonomous will, I think, feel noticeably different. It will be a moment future generations have trouble imagining a world where drunk or tired driving was ever even an issue. reply fragmede 13 hours agorootparentThe future is here, just unevenly distributed. There are already people that don't have that issue, thanks to technology. That technology might be Waymo and not driving in the first place, or the technology might be smartphones and the Internet, which enables Uber/Lyft to operate. Some of them might use older technologies like concrete which enables people to live more densely and not have to drive to get to the nearest liquor establishment. reply munchler 13 hours agorootparentprevYou can make exactly the opposite argument as well: You think that we can still define cars as human-driven even when they have self-driving features (e.g. lane keeping). If the car is self-driving in even the smallest way, then they simply are not human-operated by any sensible definition. reply skywhopper 4 hours agorootparentNo one is making predictions or selling stock in the amount of “fully human controlled” vehicles. reply Animats 15 hours agoprev> That being said, we are not on the verge of replacing and eliminating humans in either white collar jobs or blue collar jobs. Tell that to someone laid off when replaced by some \"AI\" system. > Waymo not autonomous enough It's not clear how often Waymo cars need remote attention, but it's not every 1-2 miles. Customers would notice the vehicle being stopped and stuck during the wait for customer service. There are many videos of people driving in Waymos for hours without any sign of a situation that required remote intervention. Tesla and Baidu do use remote drivers. The situations where Waymo cars get stuck are now somewhat obscure cases. Yesterday, the new mayor of SF had two limos double-parked, and a Waymo got stuck behind that. A Waymo got stuck in a parade that hadn't been listed on Muni's street closure list. > Flying cars Probably at the 2028 Olympics in Los Angeles. They won't be cost-effective, but it will be a cool demo. EHang recently put solid state batteries into their flying car and got 48 minutes of flight time, instead of their previous 25 minutes. Ehang is basically a scaled-up quadrotor drone, with 16 motors and props. EHang been flying for years, but not for very long per recharge. Better batteries will help a lot. [1] https://aerospaceamerica.aiaa.org/electric-air-taxi-flights-... reply shlomo_z 15 hours agoparent> Tell that to someone laid off when replaced by some \"AI\" system. What are some good examples? I am very skeptical of anyone losing their jobs to AI. People are getting laid off for various reasons: - Companies are replacing American tech jobs with foreigners - Many companies hired more devs than they need - companies hired many devs during the pandemic, and don't need them anymore Some companies may claim they are replacing devs with AI. I take it with a grain of salt. I believe some devs were probably replaced by AI, but not a large amount. I think there may be a lot more layoffs in the future, but AI will probably account for a very small fraction of those. reply lolinder 14 hours agorootparent> I believe some devs were probably replaced by AI, but not a large amount. I'm not even sold on the idea that there were any. The media likes to blame AI for the developer layoffs because it makes a much more exciting story than interest rates and arcane tax code changes. But the fact is that we don't need more than the Section 174 changes and the end of ZIRP to explain what's happened in tech. Federal economic policy was set up to direct massive amounts of investment into software development. Now it's not. That's a real, quantifiable impact that can readily explain what we've seen in a way that the current productivity gains from these tools simply can't. Now, I'll definitely accept that many companies are attributing their layoffs to AI, but that's for much the same reason that the media laps the story up: it's a far better line to feed investors than that the financial environment has changed for the worse. reply Mistletoe 12 hours agorootparentBut I see so many devs typing here saying how vital AI is to their writing code efficiently and quickly now. If that is true then you need way less devs. Are people just sitting idle at their desks? I do see quite a bit of tech layoffs for sure. Are you saying devs aren't part of the workers being laid off? >In 2024: At least 95,667 workers at U.S. -based tech companies have lost their jobs so far in the year, according to a Crunchbase News tally. reply tsimionescu 11 hours agorootparent> Are you saying devs aren't part of the workers being laid off? No, they are saying that the reason for the layoffs is not AI, it is financial changes making devs too expensive. > If that is true then you need way less devs. This does not follow. First of all, companies take a long time to measure dev output, it's not like you can look at a burn down chart over two sprints and decide to fire half the team because it seems they're working twice as fast. So any productivity gains will show up as layoffs only after a long time. Secondly, dev productivity is very rarely significantly bounded by how long boilerplate takes to write. Having a more efficient way to write boilerplate, even massively more efficient, say 8h down to 1h, will only marginally improve your overall throughput, at least at the senior level: all that does is free you to think more about the complex issues you needed to solve. So if the task would have previously taken you 10 days, of which one day was spent on boilerplate, it may now take you, say, 8-9 days, because you've saved one day on boilerplate, plus some more minor gains here and there. So far from firing 7 out of every 8 devs, the 8h-to-1h boilerplate solution might allow you to fire 1 dev in a team of 10. reply lolinder 2 hours agorootparentprev> Are you saying devs aren't part of the workers being laid off? Of course not. The Section 174 changes are really only relevant to software devs—the conversation in the months leading up to them kicking in was all about how it would kill software jobs. But then when it happened the media latched onto this idea that it was the result of automation, with zero evidence besides the timing. Since the timing also coincided with a gigantically important change to the tax code and a rapid increase in interest rates, both of which were predicted to kill software jobs, I'm suggesting that blaming AI is silly—we have a proximate cause already that is much more probable. reply thefaux 2 hours agorootparentprevWhy would Jevon's paradox not apply to human labor? I am not sure what I expect for software developers besides that the nature if the work will change but it is still too early to say exactly how. We certainly cannot extrapolate linearly or exponentially from the past few years. reply jdminhbg 11 hours agorootparentprev> But I see so many devs typing here saying how vital AI is to their writing code efficiently and quickly now. If that is true then you need way less devs. Sure, in the same sense that editors and compilers mean you need way less devs. reply lukeschlather 5 hours agorootparentprevhttps://www.bls.gov/ooh/computer-and-information-technology/... BLS reports ~1.9 million software developer jobs and predicts 17% growth through 2033. Crunchbase is talking about \"tech workers\" not developers. And they don't even say that tech employment is down. I predict that when BLS publishes their preliminary job numbers for 2024 it will be at least 1.85 million, not 1.9 million as suggested by your Crunchbase News. I would lay 2:1 odds that it will be higher than 2023's number. reply baq 12 hours agorootparentprevInduced demand means we’ll need more devs than we have right now since every dev can produce more value (anyone using cursor for a longer while should be able to confirm that easily). The problem is different in the meantime: nobody wants to be paying for training of those new devs. Juniors don’t have the experience to call LLM’s bullshit and seniors don’t get paid to teach them since LLMs replaced interns churning out boilerplate. reply guappa 5 hours agorootparentprevA lot of devs are hacks. If an AI can do your job you had no value as a software developer. reply skywhopper 4 hours agorootparentprevIt just isn’t true that AI has made developers more efficient. Some might claim such on this site, but the vast majority of developers aren’t using it, or they find it to be a drag on their productivity (because for most tasks the median software engineer has to do, it actually can’t help), and the ones that do use it are (unknowingly maybe) exaggerating its impact. Devs are getting laid off, yes. AI is not the reason. Executive/shareholder priorities are the reason. reply rcpt 11 hours agorootparentprevIt's pretty much impossible to get work as a copywriter now reply RamblingCTO 10 hours agorootparentI was thinking about this. I think we have an overcorrection right now. People get laid off because of expected performance of AI, not real performance. With copywriting and software development we have three options: 1. leaders notice they were wrong, start to increase human headcount again 2. human work is seen as boutique and premium, used for marketing and market placement 3. we just accept the sub-par quality of AI and go with it (quite likely with copywriting I guess) I'd like to compare it with cinema and Netflix. There was a time where lost of stuff was mindless shit, but there was still place for A24 and it took the world by storm. What's gonna happen? No one knows. But anyway, I figure that 90% of \"laid off because of AI\" is just regular lay-offs with a nice sounding reason. You don't loose anything by saying that and only gain in stakeholder trust. reply harvodex 5 hours agorootparent90% might even be too low. If you look up business analyst type jobs on JP Morgan website they are still hiring a ton right now. What you actually notice is how many are being outsourced to other countries outside the US. I think the main process at work is 1% actual AI automation and a huge amount of return to the office in the US while offshoring the remote work under the cover of \"AI\". reply theLiminator 15 hours agorootparentprevI imagine there aren't really layoffs, but slowing/stopping of hiring as you get more productivity out of existing devs. I imagine in the future, lots of companies will just let their employee base slowly attrition away. reply davedx 9 hours agorootparentprevYeah, the AgentForce thing is a classic example. Internal leaks say Salesforce is using it as cover for more regular (cost cutting based) layoffs. People who've actually evaluated AgentForce don't think it's ready for prime time. It's more smoke and mirrors (and lots of marketing). reply davedx 9 hours agoparentprevI think what Waymo's achieved is really impressive, and I like the way they've rolled out (carefully), but there's a lot of non evidence based defense of them in this comment thread. YouTube videos of people driving for hours are textbook survivorship bias. (What about all the videos people made but didn't upload because their drive didn't go perfectly?) Nobody knows how many times operators intervene, because Waymo hasn't said. It's literally impossible to deduce. Which means I also agree his estimate could also be wildly wrong too. reply skywhopper 4 hours agoparentprevHe’s saying AI can’t do the work of humans, not that dumb executives won’t pretend it can. reply brcmthrowaway 15 hours agoparentprevWhat is the silver bullet for battery tech? reply Animats 15 hours agorootparentSolid state batteries. Prototypes work, but high-volume manufacturing doesn't work yet. The major battery manufacturers are all trying to get this to production. Early versions will probably be expensive. Maybe a 2x improvement in kwh/kg. Much less risk of fire or thermal runaway. Charging inTheir imaginations were definitely encourage by exponentialism, but in fact all they knew was that when the went from smallish to largish networks following the architectural diagram above, the performance got much better. So the inherent reasoning was that if more made things better then more more would make things more better. Alas for them it appears that this is probably not the case. I recommend reading Richard Hamming's \"The Art of Science and Engineering.\" Early in the book he presents a simple model of knowledge growth that always leads to an s-curve. The trouble is that on the left, an s-curve looks exponential. We still don't know where we are on the curve with any of the technologies. It is very possible we've already passed the exponential growth phase with some of these technologies. If so, we will need new technologies to move forward to the next s curve. reply sinuhe69 8 hours agoprevI always have a definitional problem with predictions. I mean, it's moot whether a specific prediction is right or wrong as long as it doesn't help us to understand the big picture and the trends. Take, for example, the prediction about \"robots can autonomously navigate all US households\". Why all? From the business POV, 80% of the market is \"all\" in a practical sense, and most people will consider navigation around the home as \"solved\" if they can do it for the majority of households and with virtually no intervention. Hilarious situations will arise that amuse the folks; video of clumsy robots will flood the internet instead of cats and dogs, but for the business site, it's lucrative enough to produce and sell them en masse. Another question of interest is how is the trend? What will the approximate cost of such a robot be? How many US households will adopt such a robot by which time, as they have adopted washing machines and dishwashers. Will we see a linear adoption or rather a logistic adoption? These are the more interesting questions than just whether I'm right or wrong. reply SavageBeast 13 hours agoprevIn reading this I come to wonder if the current advances in \"AI\" are going to follow the Self Driving Car model. Turns out the 80% is relatively easy to do, but the remaining 20% to get it right is REALLY hard. reply brisky 1 hour agoparentAgree, that is why the agent hype is going to bust. Agent means giving AI control. That means critical failure modes and the need of human to constantly oversee agent working. reply rexreed 17 hours agoprevI like Rodney Brooks, but I find the way he does these predictions to be very obtuse and subject to a lot of self-congratulatory interpretation. Highlighting something green that is \"NET2021\" and then saying he was right when something happened or didn't happen, when something related happened in 2024 mean that he predicted it right or wrong, or is everything subject to arbitrary interpretation? Where are the bold predictions? Sounds like a lot of fairly obvious predictions with a lot of wiggle room to determine if right or wrong. reply kragen 2 hours agoparent\"NET2021\" means \"no earlier than 2021\". So, if nothing even arguably similar happened until 2024, that sounds like a very correct prediction. Whether that's worth congratulating him about depends on how obvious it was, but I think you really need to measure \"fairly obvious\" at the time the prediction is made, not seven years later. A lot of things that seem \"fairly obvious\" now weren't obvious at all then. reply gcr 17 hours agoparentprevNET2021 means that he predicted that the event would take place on or after 2021, so happening in 2024 satisfies that. Keep in mind these are six-year-old predictions. Are you wishing that he had tighter confidence intervals? reply rexreed 17 hours agorootparentIf the predictions are meant to be bold, then yes. If they're meant to be fairly obvious, then no. For example, saying that flying cars will be in widespread use NET 2025 is not much of a prediction. I think we can all say that if flying cars will be in widespread use, it will happen No Earlier Than 2025. It could happen in 2060, and that NET 2025 prediction would still be true. He could mark it green in 2026 and say he was right, that, yes, there are no flying cars, and so mark his scorecard another point in the correct column. But is that really a prediction? A bolder prediction would be, say \"Within 1-2 yrs of XX\". So what is Rodney Brooks really trying to predict and say? I'd rather read about what the necessary gating conditions are for something significant and prediction-worthy to occur, or what the intractable problems are that would make something not be possible within a predicted time, rather than reading about him complain about how much overhype and media sensation there is in the AI and robotics (and space) fields. Yes, there is, but that's not much of a prediction or statement either, as it's fairly obvious. There's also a bit of an undercurrent of complaint in this long article about how the not-as-sexy or hyped work he has done for all those years has gone relatively unrewarded and \"undeserving types\" are getting all the attention (and money). And as such, many of the predictions and commentary on them read more as rant than as prediction. reply Denzel 14 hours agorootparentPresumably you read the section where Brooks highlights all the forecasts executives were making in 2017? His NET predictions act as a sort of counter-prediction to those types of blind optimistic, overly confident assertions. In that context, I’d say his predictions are neither obvious nor lacking boldness when we have influential people running around claiming that AGI is here today, AI agents will enter the workforce this year, and we should be prepared for AI-enabled layoffs. reply riffraff 11 hours agorootparentprevThe NET estimation is supposed to be a counter to the irrational exuberance of media and PR. E.g. musk says they'll get humans to Mars in 2020, and the counter is \"I don't think that will happen until at least 2030\". reply kqr 13 hours agoprev> Systems which do require remote operations assistance to get full reliability cut into that economic advantage and have a higher burden on their ROI calculations Technically true but I'm not convinced it matters that much. The reason autonomation took over in manufacturing was not that they could fire the operator entirely, but that one operator could man 8 machines simultaneously instead of just one. reply skizm 3 hours agoprevI don't have a pulse on how far self-driving has come from a tech standpoint, but from an outsider's perspective I'd say it is \"achieved\" when I can order a self-driving car from an app in all of the top 10 most populated cities in the US (since that's where it is being developed) with as much consistency as uber/lyft. The real final boss for self-driving will be the government red-tape that companies will need to get through. I doubt local governments will be a laissez faire with self-driving as they were with uber being an illegal taxi company. reply metalliqaz 3 hours agoparentthe final boss will be the first big lawsuit against a manufacturer for liability after someone is killed by a driverless car Of course, then we will eventually see infrastructure become even more hostile to non-drivers and people will have to sue their own governments for the right to exist in public without paying transport companies. Strong Towns tried to warn us reply skizm 2 hours agorootparentDidn’t that already happen in Phoenix? They paused the program there, but unsure if there was actually a lawsuit or settlement. reply metalliqaz 1 hour agorootparentWasn't there a human \"monitor\" in the car? reply vikrantrathore 15 hours agoprevFor me this predictions are kind of being aware of how progress can happen based on history, but this will not lead to any breakthrough. I am not in the camp of being skeptic so I still like the hype cycle, they create an environment for people to break the boundaries and sometimes help untested ideas and things to be explored. This might not have happen if there is no hype cycle. I am in the camp of people who are positive as George Bernard Shaw in his 2 quotes: 1. A life spent making mistakes is not only more honorable, but more useful than a life spent doing nothing. 2. The reasonable person adapts themselves to the world: the unreasonable one persists in trying to adapt the world to themself. Therefore all progress depends on the unreasonable person. (Changed man to person as I feel it should be gender neutral) In hindsight when we look back, everything looks like we anticipated, so predictions are no different some pans out some doesn't. My feeling after reading prediction scorecard is that you need a right balance between risk averse (who are either doubtful or do not have faith things will happen quickly enough) and risk takers (one who is extremely positive) for anything good to happen. Both help humanity to move forward and are necessary part of nature. It is possible AGI might replace humans in a short term and then new kind of work emerges and humans again find something different. There is always a disruption with new changes and some survive and some can't, even if nothing much happens its worth trying as said in quote 1. reply sgt101 3 hours agoparentI feel a counter is that hyping and going along with hype leads to substantial misallocation of capital and this leads to human misery. How much money has been burned on robo-taxis which could have been spent on incubators for kids. reply kookamamie 13 hours agoprevIt's far too rambly and vague to make any sense of the achieved results, I think. reply teractiveodular 10 hours agoprevAll that verbiage about robotaxis and not a single mention about China, which by all accounts is well ahead of the US in deploying them out on the road. (With a distinctly mixed track record, it must be said, but still.) reply barnabyjones 11 hours agoprev>Individually owned cars can go underground onto a pallet and be whisked underground to another location in a city at more than 100mph. I'm curious where this idea even came from, not sure who the customer would be, it's a little disappointing he doesn't mention mag-lev trains in a discussion about future rapid transit. I'd much rather ride a smooth mag-lev across town than an underground pallet system. reply qwertox 11 hours agoparentYet such an underground system should exist to transport deliveries. reply kweingar 2 hours agoparentprevElon Musk promised to build this tech. reply michaelbuckbee 3 hours agoprevOne interesting prediction category in the Venn diagram overlaps of Rodney's predictions that is missing is drone deliveries. Where I live (in suburbia Virginia), we now can get items from the local WalMart grocery via DroneUp, which kind of blows mind. reply qznc 11 hours agoprevIf someone wants to have a credible prediction scorecard, get it on some third-party platform like Metaculus, Manifold, GJOpen, Polymarket, ... reply dang 16 hours agoprevRelated. Others? Rodney Brooks Predictions Scorecard - https://news.ycombinator.com/item?id=34477124 - Jan 2023 (41 comments) Predictions Scorecard, 2021 January 01 - https://news.ycombinator.com/item?id=25706436 - Jan 2021 (12 comments) Predictions Scorecard - https://news.ycombinator.com/item?id=18889719 - Jan 2019 (4 comments) reply FabHK 10 hours agoprevLOL about the last paragraphs: > Let’s Continue a Noble Tradition! > The billionaire founders of both Virgin Galactic and Blue Origin had faith in the systems they had created. They both personally flew on the first operational flights of their sub-orbital launch systems. They went way beyond simply talking about how great their technology was, they believed in it, and flew in it. > Let’s hope this tradition continues. Let’s hope the billionaire founder/CEO of SpaceX will be onboard the first crewed flight of Starship to Mars, and that it happens sooner than I expect. We can all cheer for that. reply tontonius 5 hours agoparentHow am I supposed to read this ? a thinly veiled hatred for Mr. Musk? reply FabHK 2 hours agorootparentThat's how I interpreted it, yes. reply IAmGraydon 14 hours agoprevDoes it drive anyone else crazy when an author posts 15,000 words (yes, there are that many in this article) when 1,500 would have more than communicated the relevant information? The length of this article is almost comical. reply zetalyrae 12 hours agoprev> LLMs have proved amazing facile with language. If you took a transcript of a conversation with Claude 3.6 Sonnet, and sent it back in time even five years ago (just before the GPT-3 paper was published), nobody would believe it was real. They would say that it was fake, or that it was witchcraft. And whoever believed it was real would instantly acknowledge that the Turing test had been passed. This refusal to update beliefs on new evidence is very tiresome. reply tkgally 11 hours agoparentSimilarly if you could let a person from five years ago have a spoken conversation with ChatGPT Advanced Voice mode or Gemini Live. For me five years ago, the only giveaways that the voice on the other end might not be human would have been its abilities to answer questions instantaneously about almost any subject and to speak many different languages. The NotebookLM “podcasters” would have been equally convincing to me. reply harvodex 5 hours agoparentprevThe whole point of the post is that many have updated their beliefs too much. reply Upvoter33 7 hours agoprevThe next big thing beyond deep learning being LLMs is funny reply yearesadpeople 9 hours agoprevQuite an unreadable web page, and somehow rationalising there was 'everything before me', and 'everything after me' with regard technology and prediction. Unfortunate understanding of reality really. reply ripe 2 hours agoprevOn reading the negative commentary here on Rodney Brooks's post, I'm realizing that besides being a rambling article, it also assumes too much background from the reader. It isn't really understandable without knowing something about the author and about the business of robots. Disclaimer: I worked for years building robots, several of these years with Rod. I assure you, when it comes to robotics and AI, he knows what he's talking about. Here's my perspective. Also, he wrote his original predictions six years ago in a blog post [1], which is the basis for this latest post. If you don't have the time to read the old post, I provide a short summary from it about autonomous driving below, too. 1. Rod is not just an MIT professor emeritus and a past director of CSAIL. He has co-founded multiple robotics companies, one of which, iRobot, made loads of money selling tens of millions of consumer-grade autonomous robots cleaning floors in people's homes. Making money selling autonomous robots is a very, very difficult thing. Roomba was a true milestone. Before then, the only civilian, commercially successful mass-produced robots were the programmable industrial arms that are still used in auto manufacturing. If the author sounds self-important, maybe that's why. Yeah, he can get a little snarky sometimes when self-important CEOs run around with VC money in their pockets making tall claims and never being held accountable. That's just his style. Try to look beyond it. You might learn a thing or two. 2. The entire purpose of his annual \"predictions\" posts starting with [1] was to counter the hype and salesmanship about AI and robotics that's wasting billions of investment dollars and polluting the media landscape. About autonomous cars, he believes that the core technology has been demonstrated in the 1980s, but that instead of using it, we have squandered the decades since then. For autonomous robots, the interaction with their surroundings is critical to success. We could have enhanced our road and communications infrastructure to enable autonomous cars. Instead, we have chosen to give money to slick salesmen to chase the mirage of placing \"intelligent\" cars on existing roads, continuing to neglect our civil infrastructure. [1] https://rodneybrooks.com/my-dated-predictions/ reply bArray 3 hours agoprev> The level of hype about AI, Machine Learning and Robotics completely distorts people’s understanding of reality. It distorts where VC money goes, always to something that promises impossibly large payoffs–it seems it is better to have an untested idea that would have an enormous payoff than a tested idea which can get to a sustainable business, but does not change the world for ever. One thing to remember is that there is more than one target audience in these claims. VCs for example seem to operate on a rough principle of 5 tech companies, 4 make 0x and one makes 10x, for a total 2x on each investment. If you only promise 5x, with 4 failures of 0x and one success at 5x, total return is 1x on each (not worth the risk). You may say \"yes, my company is 2x, but it is guaranteed!\" - but they all sell this idea. VCs could be infinitely good at predicting success and great companies, but it's based on partial information. Essentially companies have to promise the 10x and the VCs assume they are likely incorrect anyway, in order to balance the risk profile. I do have a fundamental problem with this \"infinite growth\" model that almost everything seems based on. > There is steady growth in sales but my prediction of 30% of US car sales being electric by 2027 now seems wildly optimistic. We need two doublings to get there in three years and the doubling rate seems more like one doubling in four to five years. Even one doubling in 4-5 years might be too much. There are fundamental issues to be addressed: 1. What do we do about crashed EVs? They are dangerous to store and dangerous to dismantle. There have been quite a few EV fires at places like Copart now. There is little to no value in crashed EVs because they are so dangerous, which pushes insurance up because they cannot recover these funds. 2. Most car dealerships in the UK refuse to accept EVs for trade-in, because they sit on their forecourt until they eventually die. Those who can afford EVs typically get them on finance when the batteries provide the fullest range. Nobody I know is buying 10 year old EVs with no available replacement batteries. Commerical fleets are also not buying any more EVs as they essentially get no money back after using them for 3 years or so. 3. The electrical grid cannot scale to handle EVs. With every Western country decarbonising their electrical grid in favour of renewable energy, they have zero ability to respond to increased load. The truth is, when they push to remove fossil fuel vehicles, they simply want to take your personal transport from you. There is no plan for everybody to maintain personal mobility, it'll be a privilege reserved for the rich. You'll be priced out and put onto public transport, where there will be regular strikes because the government is broke and wages cannot increase - because who knew, infinite growth is a terrible investment model. > The other thing that has gotten over hyped in 2024 is humanoids robots. > The visual appearance of a robot makes a promise about what it can do and how smart it is. The real sin is not HRI issues, it's that we simply cannot justify them. What job is a humanoid robot supposed to do? Who is going to be buying tens of thousands of the first unit? What is the killer application? What will a humanoid robot do that it is not cheaper/more effective to do with a real human, or cannot be done better with a specialised robot? Anything you can think of which is a humanoid robot performing a single physical action repeatedly, is wrong. It would need to be a series of tasks that keeps the robot highly busy, and the nature of the work needs to be somewhat unpredictable (otherwise use a dedicated robot). After all, humans are successful not because we do one thing well, but because we do many not-well defined things good-enough. This kind of generalisation is probably harder than all other AI problems, and likely requires massive advances in real-time learning, embodiment and intrinsic motivation. What we need sub-problems for robots, i.e. like a smart vacuum, where robots are slowly but surely introduced into complex environments where they can safely incrementally improve. Trying to crack self-driving 1+ tonne high speed death machines in your first attempt is insanity. reply ynniv 14 hours agoprevIt's long, so I'm skimming a little and... flying cars. If you don't know why we don't have flying cars, you're not a good engineer. It really doesn't matter what prestigious lab you ran, as that apparently didn't impart the ability to think critically about engineering problems. [Hint: Flying takes 10x the energy of driving, and the cost/weight/volume of 1 MJ hasn't changed in close to a hundred years. Flying cars require a 10x energy breakthrough.] reply tsimionescu 10 hours agoparentThe article is responding to claims by CEOs of car companies, industry and business press, and other hype sources that keep predicting flying cars next year or so. It's predicting that, against this hype, it will not come to pass. Not sure why you've worded your comment in such a way as if the article was hyping up flying cars. Not to mention, since we do have helicopters, the engineering challange of flying cars is almost entirely unrelated to energy costs (at least for the super rich, the equivalent of, say, a Rolls Royce, not of a Toyota). The thing stopping flying cars from existing is that it is extremely hard to make an easy to pilot flying vehicle, given the numerous degrees of freedom (and potential catastrophic failure modes); and the significantly higher impredictability and variance of the medium (air vs road surface). Plus, the major problem of noise pollution, which gets to extreme levels for somewhat fundamental reasons (you have to diaplace a whole lot of air to fly; which is very close to having to create sound waves). So, overall, the energy problem is already fixed, we already have point-to-point flying vehicles usable, and occasionally used, in urban areas, helicopters. Making them safe when operated by a very lightly trained pilot, and silent enough to not wake up a neighborhood, are the real issues that will persist even if we had mini fusion reactors. reply ynniv 3 hours agorootparentI'm not sure that this disproves my original point that self driving cars and flying cars don't belong in the same list because they are fundamentally different engineering problems. reply FabHK 10 hours agoparentprevNot quite. It's about 3x. It also depends on whether you're talking fixed wing or rotary wings. A modern car might easily have 130 kW or more, and that's what a Cessna 172 has (around 180 hp). (Sure, a plane cruises at the higher end of that, while a car only uses that much to accelerate and cruises at the lower end of the range - still not a factor of 10x.) As another datapoint, a Diamond DA40 does around 28 miles per gallon ( Don’t hold your breath. They are not here. They are not coming soon. > Nothing has changed. Billions of dollars have been spent on this fantasy of personal flying cars. It is just that, a fantasy, largely fueled by spending by billionaires. It’s worth actually reading the article before trashing someone’s career and engineering skills! reply ynniv 14 hours agorootparentEngineering is about focusing on what matters. There's no point in talking about flying cars: they will exist when portable fusion exists, so just talk about that. reply cudgy 5 hours agorootparentSo you are saying that a true engineer doesn’t read articles and criticizes a successful engineer that wrote said article with hand-wavy arguments? reply lifeisstillgood 11 hours agoprev>>> [self driving cars are rmeote controlled] in all cases so far deployed, humans monitoring those cars from a remote location, and occasionally sending control inputs to the cars. Wait, What now? I have never heard this, but from the founder of CSAIL I am going to take it as a statement of fact and proof that basically every AI company is flat out lying. I mean the difference between remote piloting a drone that has some autonomous flying features (which they do to handle lag etc) and remote driving a car is … semantics? But yeah it’s just moving jobs from one location to another. reply tsimionescu 10 hours agoparentNote that even the examples he gives are related to things like an operator telling the car to overtake a stopped truck instead of waiting for it to start again. So occasional high level decisions, not minute-to-minute or even second-to-second interactions like you have when flying a drone. This is more like telling your units to go somewhere in a video game, and they mostly do it right, but occasionally you have to take a look and help them because they got stuck in a particularly narrow corridor or something. reply RamblingCTO 10 hours agoparentprevNitpick: he's not the founder, not by far. He's just a past director of CSAIL reply RAHIMUDEEN 2 hours agoprevBig Mumbai reply Over2Chars 11 hours agoprevI am always a fan of people who pretend to have psychic powers. Predict the future, Mr. Brooks! reply andyferris 14 hours agoprevSo... he's not a fan of Elon Musk, I take it? reply nurettin 8 hours agoprevI don't know the motivation behind making robotics and AI predictions, as these things have been done to death since the 70s, but I know people who bet for high inflation made a killing in financial futures. reply maverickmax90 10 hours agoprevWhat you marked as hype is a flaw in your skill to recognize real world cases vs wishful thinking. You are not predicting just daydreaming. reply richrichie 13 hours agoprev> It distorts where VC money goes, always to something that promises impossibly large payoffs–it seems it is better to have an untested idea that would have an enormous payoff than a tested idea which can get to a sustainable business But this is the whole point of VC investing. It is not normal distribution investing. reply zzzeek 15 hours agoprevwhat a weird writer, lots of interesting things to talk about but this very long essay continued to circle back to being author-self-obsessed with their own prowess and drawing out huge expositions and bullet lists on how well they are at predicting things. Call it self-referential-appeal-to-authority. reply fifticon 11 hours agoparentAnother perspective is that it is a person who takes great care/is very thorough, to examine and re-evaluate his reasonings, and makes an effort to explain the logic in his reasoning, which can be helpful if you are trying to figure out if you agree or disagree. reply mberning 14 hours agoparentprevIt is odd. The product of a mind which clearly thinks very highly of itself. reply lalaithion 13 hours agoprev [–] > A robot that has any real idea about its own existence, or the existence of humans in the way that a six year old understands humans It seems to me we’re at the very least close to this, unless you hold unproven beliefs about grey matter vs silicon. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author provides an annual update on predictions made in 2018 about self-driving cars, robotics, AI, machine learning, and human space travel, with plans to make new predictions for 2026-2036.",
      "Despite significant developments in AI and self-driving cars in 2024, full autonomy and the replacement of human jobs with AI remain distant, with remote human monitoring still necessary for self-driving cars.",
      "Human spaceflight progress is slow, with SpaceX's Starship and NASA's Artemis facing delays, while Blue Origin's New Glenn is nearing launch, and suborbital tourism shows limited progress."
    ],
    "commentSummary": [
      "The article examines Rodney Brooks' predictions on AI and robotics, particularly focusing on self-driving cars and AI advancements. - Critics claim Brooks' predictions are vague and self-congratulatory, especially concerning Waymo's self-driving technology, which some argue is not fully autonomous due to human interventions. - Brooks aims to temper the hype around AI and robotics by highlighting realistic timelines and challenges, while the discussion also considers the economic viability of self-driving technology and its impact on jobs."
    ],
    "points": 224,
    "commentCount": 166,
    "retryCount": 0,
    "time": 1736468875
  }
]
