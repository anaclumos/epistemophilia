[
  {
    "id": 42818692,
    "title": "Caltrain's electric fleet more efficient than expected",
    "originLink": "https://www.caltrain.com/news/caltrains-electric-fleet-more-efficient-expected",
    "originBody": "New Caltrain schedule starts January 27. For more details, visit Caltrain.com/status.",
    "commentLink": "https://news.ycombinator.com/item?id=42818692",
    "commentBody": "Caltrain's electric fleet more efficient than expected (caltrain.com)306 points by ssuds 17 hours agohidepastfavorite259 comments blintz 16 hours agoThe electric Caltrain is so wonderful. It’s way quieter, the cars have nicer interiors, and it is running faster and more frequently than before. Between this, the new BART cars (also much quieter!), and Muni hitting a reliability & satisfaction records, there’s really a transit renaissance going on in the Bay Area. I really hope we can keep this going! reply infotainment 16 hours agoparentAgreed largely, though IMO BART is holding the bay area back. Unlike Caltrain and Muni, BART management is completely incompetent and more focused on spending their money on random things than actually running their transit system. Fun fact: BART police has a large fleet of SUVs, with the highest vehicle-to-officer ratio in the bay area (if only there were some other way for them to get around!) Fun exercise: compare the cleanliness of a Muni Bus/LRV to a BART car (even a new one at the start of the day). There is a huge difference. reply blintz 15 hours agorootparentI think BART struggles with who it's meant to serve now, since it was clearly designed for ultra-peak weekday commute traffic. Recovery for weekend ridership is way better than weekday. I also agree that the governance structure for BART is weird and overly complex. Why do I elect a BART director? Why are they in their own special BART districts completely discontinuous from the 10 other ways we have sliced up the Bay Area? All that being said, BART has done some good stuff too. The new cars really are way better, and they were not easy to procure since BART has its own weird, non-standard rail gauge. They have increased frequency and shortened trains on weekends to respond to the ridership changes. Most of all, BART remains the only form of Bay Area transit genuinely faster than driving in real-world circumstances: it averages above 60mph in parts of the east bay, and goes from downtown Oakland to downtown SF in 11min, which is often 2-3x faster than driving. It's the primary transit system that can compete head-to-head with driving. For that reason, I do hope they keep increasing frequency (shortening trains if necessary). reply bombcar 14 hours agorootparentIt’s kind of sad that BART’s first mover advantage means it’ll forever be a second-class railway. They’ll never do the “correct” thing and shut it down long enough to replace everything with standard equipment and gauge. reply thirtyseven 10 hours agorootparentBeing a first mover didn't prevent them from adopting standard gauge which already had existed for 100 years. reply usr1106 8 hours agorootparentIt has been said the broader gauge was chosen at the time to make trains able to run safely over Golden Gate Bridge with strong side winds. My physics is not good enough to calculate whether that argument makes sense. And I have no idea how realistic that route ever was. I don't think the gauge is a major problem. Train orders are always a custom project, few urban networks use exactly the same standards. Railroad manufacturers are used to different gauges. reply tialaramex 7 hours agorootparentIn particular the track gauge is a long way from being the only consideration. Structure gauge and Loading gauge are also crucial. When I first moved here despite this being an important port city a Victorian arch bridge carrying road traffic over the railway meant every single freight train carrying containers from the port to the rest of the country needed to either go on a circuitous route or use special low wagons with reduced capacity, which hold a container below axle height so as to fit under that bridge. In that case blocking the road and dropping in a new road bridge was affordable given the economic value but generally you put up with what you've got. reply usr1106 7 hours agorootparentTrue, when it comes to loading gauge one can no longer even about a standard. Most countries have several different loading gauges even for the same track gauge. In practice I am not convinced the BART is severely impacted by their \"weird\" gauge (whatever is meant by that, not sure what their loading gauge is, for passenger trains the distance to and height of the platforms would be most relevant). Stadler KISS series used by Caltrain is built at least in 3 different widths. Auckland, NZ had (not sure whether still in use) rolling stock from the UK, converted from 1435 mm to 1067 mm track gauge, the loading gauge obviously was close enough. Finland has engines (Sr3, Dr20) and railcars (Dm12) designed for smaller central European loading gauges. They look a bit tiny compared to other stock, but they are fully usable. reply saagarjha 15 hours agorootparentprevWait, BART director is an elected position? Why don't I get to vote on it? (Also, while I am biased towards Caltrain, the new trains beat traffic on 101/280 San Francisco↔San Jose during rush hour). reply joeblubaugh 14 hours agorootparentYou do. San Jose’s districts were not up in the most recent election https://www.bart.gov/about/bod/elections reply dcrazy 14 hours agorootparentSanta Clara County is not part of the BART District. The extension to downtown San Jose is being funded by VTA as a service-purchase agreement. reply joeblubaugh 13 hours agorootparentOh wow reply saagarjha 14 hours agorootparentprevWhat district do they have? I don't see anyone on this list that seems like they cover the area: https://www.bart.gov/about/bod reply joeblubaugh 13 hours agorootparentYep, I was wrong. I had assumed SCC joined up to get Berryessa reply jlmorton 12 hours agorootparentprevIt shouldn't be, though. There is way too much democracy in California localities. There should be no elected school board, transit districts, utility boards, assessors, sheriff, and so much more. No one is properly informed about candidates for these positions. For that matter, the Board of Supervisors should have no power other than oversight and impeachment. The Mayor should basically be a local dictator, with the power to do anything the State authorizes the municipality to do, at their sole discretion, with the oversight of an elected board. reply chgs 8 hours agorootparentAmericans seem to have a view that if you get a part of an unengaged electorate to mark an x in a box every few years that’s democracy, and more is thus better. Instead it removes accountability from public servants who can simply hide behind the “elected” excuse. reply basementcat 4 hours agorootparentprevWhat terrifies me the most are elections for judges. I am not a legal scholar and I rely upon local bar associations for qualification ratings (and I’m not convinced I made the right call all the time); to my horror I’ve had educated colleagues tell me they just pick cool sounding names. reply ssuds 15 hours agorootparentprevSpeaking of BART, this story is priceless: https://www.sfgate.com/bayarea/article/bart-seat-slasher-hur... The firm that reupholstered damaged BART seats in the ‘80s paid people to slash them so they’d get more business. reply defrost 15 hours agorootparentI love these kinds of true snippets of general security history: It was estimated that \"possibly 85 percent of the more than 7,000 BART train cushions damaged since August 1979\" was the work of this company, the Examiner reported at the time. All said and done, BART had paid the company $115,000 for the repairs, a total of about $339,128 in today's money. Always follow the money. reply stuaxo 8 hours agorootparentThe profit motive always incentiveses this sort of innovation and efficiency when it comes to making more profit. reply gdudeman 14 hours agorootparentprevUnder $50 per cushion repair in today’s money seems super cheap! reply nasmorn 8 hours agorootparentGenius Business Plan. Make repairs too cheap for competitors to be interested and drive up volume with criminal conspiracy reply fsckboy 14 hours agorootparentprev>paid people to slash them so they’d get more business Charlie Chaplin did it first https://www.youtube.com/watch?v=izqhtBPd4VQ reply diffset 12 hours agorootparentprevFun fact: Michael Healy in the second photo lives in Larry Ellison’s old house in Oakland. He also has a book on Amazon. reply TMWNN 15 hours agorootparentprevMy \"favorite\" story: BART Withholding Surveillance Videos Of Crime To Avoid 'Stereotypes'reply marxisttemp 15 hours agorootparentnext [5 more] [flagged] zdragnar 14 hours agorootparentThose appear to be sarcastic quotes, not scare quotes. reply jjj123 14 hours agorootparentI thought scare quotes are sarcastic quotes? reply TMWNN 15 hours agorootparentprevhttps://www.youtube.com/watch?v=voSpOrimkMY reply marxisttemp 1 hour agorootparentEat a curb, fashie reply johnzim 16 hours agorootparentprevYep, BART is pretty reviled by all the other transit authorities, and for good reason, based on what my friends who work some of the other authorities have said. reply Spooky23 14 hours agorootparentprevMy son and I visited San Francisco carless last year. Awesome trip! BART and Muni were great. My only complaint was that BART from downtown to SFO was almost painfully loud. reply timewizard 14 hours agorootparentprev> with the highest vehicle-to-officer ratio in the bay area There are 30% more violent assaults on BART vs Muni. > if only there were some other way for them to get around! Do you want them to respond rapidly or eventually? reply NewJazz 2 hours agorootparentAre you saying that they need vehicles to get to the crime locations faster? Are vehicles really faster than them being at the BART station on foot? If anything that statistic you cited shows that their existing policies are not a deterrent. Perhaps because they are in their vehicles instead of on train cars where the crimes are happening. reply insane_dreamer 16 hours agorootparentprev> BART police why does a transit system need its own police force? Aren't municipal and county police enough? reply numpad0 15 hours agorootparentActually, train systems having its own assigned branches of police is common enough that there's a Wikipedia article[1]. Unique part is that the US doesn't have an umbrella national or state agency that such branches would be part of. 1: https://en.wikipedia.org/wiki/Railroad_police reply arcticbull 9 hours agorootparentThey're not actually \"assigned branches\" of the police, they're private police. The US/Canadian railway police are not government employees, they're employees of the Class I railways and deputized with general law enforcement powers. They're spiritual successors to the old timey Pinkertons. Two of the biggest are Canadian National Police and Canadian Pacific Kansas City Police and those two are deputized, in both Canada and the US, with Federal, State and Provincial police powers. Despite being railway employees they can cite you for, e.g. speeding if they catch you doing so. Either on or off railway property. reply kitten_mittens_ 15 hours agorootparentprevMassachusetts has a state level Transit Police, at least. reply hydrogen7800 16 hours agorootparentprevThat's a good question, any I have no idea of the answer, but the port authority of NY and NJ has it's own police. I always figured it was because it spanned jurisdictions, but perhaps there are other reasons reply SoftTalker 15 hours agorootparentChicago has CTA police and the suburban rail system Metra has its own police also. reply selectodude 14 hours agorootparentCTA doesn’t have its own police. That’s just normal CPD. reply SoftTalker 2 hours agorootparentHm, maybe CPD has a CTA detail then? I thought there was (or used to be) a CTA police division that had their own uniforms, cars, etc. I seem to also remember a CHA police for the housing projects, but it's been a long time since I lived there. reply TylerE 13 hours agorootparentprevhttps://en.wikipedia.org/wiki/Railroad_police are actually a thing! It's not just transit either. Most of the big US railroads have their own private police forces. The officers are deputized by the state, and then by federal law their powers are valid in any state that railroad has track. They even have arrest powers. reply arcticbull 9 hours agorootparentCN and CPKC police are deputized in both the US and Canada, at the Federal, Provincial and State levels. Transnational private police. reply Spooky23 14 hours agorootparentprevNo, there are a lot of jurisdictional issues and motivational issues. An example of how it goes wrong is NYC. NYPD absorbed the transit police years ago and the role basically degraded to an overtime sink. Also, with normal railways rails generate property taxes and towns often make deals where slivers of tracks get annexed to nearby towns or cities. reply joshuamorton 16 hours agorootparentprevBART exists in 5 counties (San Francisco, San Mateo, Alameda, Contra Costa, and Santa Clara) and multiple cities (SF, Oakland, San Jose, and multiple smaller ones). Having people who have jurisdiction in any bart station is useful. And for example, consistent foot patrols are valuable to BART but not necessarily valuable to the city of Hayward, or whatever. As a concrete example, BART has more or less a goal to be able to put an officer on the car within a stop or two if you report on the app. That's logistically challenging for like a half dozen reasons if the bart police wasn't it's own org. reply mjrpes 15 hours agorootparentEast Bay Regional Park District also has its own police force, for the same reasons. reply potato3732842 16 hours agorootparentprevIt's a chain of command (i.e. BART police report to BART, Campus police report to the school, way less messy than BART or the school having to convince the jurisdiction how to police the area) and prioritization thing. Bespoke police agencies exist to police their niche to an extent that would be unjustifiable if it were coming your normal police department and taking resources away from their other tasks. Transit police, campus police, DEA, ATF, etc, etc, etc. Their entire job is to harass people that the equivalent generalist police couldn't justify allocating resources toward. Irate homeless? SFPD won't show, but BART Police will. Drunk college kids shenanigans? Normal cops don't care, but campus police does. Weirdos making machine guns or huge amounts of LSD? FBI don't care as long as you're not trafficking, they got real crime with real victims to chase. ATF and DEA care though. Wash rinse repeat this pattern for literally every bespoke police agency. Yeah, jurisdiction can be a theoretical reason for these agencies but once again it's still a priority thing. When agencies care jurisdiction and coordination isn't a problem. Having one agency span multiple only helps if you're chasing stuff so petty it would get dropped. reply throwaway7783 16 hours agorootparentExcept for the last one in your list, the others are quality of life problems. I have no problem if police interfere with either. Even the last one, is at best borderline. If there is a weirdo making machine guns in my neighborhood and I have kids, I sure as hell want police to interfere. reply tinyrichard 13 hours agorootparentThat works in theory, until the police are overrun with things they don't specialize in and don't have the officers to handle the day to day plus illegal arms dealers. reply lotsofpulp 16 hours agorootparentprevPresumably because police usually only have jurisdiction within their employer’s city limits, and sheriffs’ employees only have jurisdiction within their counties, so if mass transit moves between multiple jurisdictions, then their police can have jurisdiction wherever the mass transit goes (even across state lines, because a lot of metropolitan regions served by the same mass transit system span two or more states). reply offmycloud 16 hours agorootparentWith prior consent, the authority of California peace officers extends to any place in the state. [1] You would think that the counties could get together and work out an MoU. 1. https://leginfo.legislature.ca.gov/faces/codes_displaySectio... reply SoftTalker 15 hours agorootparentMost city police and sheriffs officers have jurisdiction anywhere in their state, AFAIK. They typically stay in their area, but if they’re pursuing a suspect they don’t break off if they cross the city limits. reply terinjokes 2 hours agorootparentWhich nearly any campus police will tell you during orientation. reply lesuorac 16 hours agorootparentprevWouldn't they just have jurisdiction in the transit area though? Like it's not like a BART employee could pull you over for speeding? reply SoftTalker 15 hours agorootparentIf they are a sworn law enforcement officer they probably could, but probably wouldn’t. reply colechristensen 16 hours agorootparentprevIt's pretty common around the country. If it isn't a separate entity you end up having a department of the normal local police doing it separately anyway. Also BART goes through several different police jurisdictions and trains... move. I can imagine the mess and disagreements about which police department was responsible for answering a call or riding in a car as it moved between cities/counties. reply pm90 16 hours agoparentprevTo unlock the true value of these systems the BA needs to be forced to increase density of their cities though. We need SF proper to look more like Manhattan. reply jmward01 16 hours agorootparentIt also needs a much bigger emphasis on transit oriented development. In Japan the best place to eat in town is often in the train station or within a few feet of it. In the US the stops are often some of the worst places like parking lots or run down parts of town. Rarely have I seen good shopping/dining/living integrated into BA, or even US in general, transit systems. reply pm90 15 hours agorootparentAgreed. Caltrain stops are… definitely not that. The SF stations are somewhat out of the way (although the planned extension would bring it all the way to the transit center). I do see a lot more residential units coming up near transit nodes, although its not nearly enough! The San Jose Caltrain is a prime exhibition of what you said: surrounded by parking lots and it feels quite deserted and creepy instead of being a bustling, thriving commercial area (although a few blocks near there is a nice commercial area with a brewery and many eateries, including a whole foods). reply wffurr 6 hours agorootparentprevMy office in Cambridge, Massachusetts is directly in a 12 story tower with ground floor retail directly on top of a transit station. TOD does exist, it’s just rare for whatever reason in NA. That said, there are lots of other stops in the area that are either overrun with parking, like you said, or are one story business districts with no housing. Lots left to do. I suspect the problem might be mismatched incentives. ISTR reading that some East Asian transit companies own and develop the parcels on and around their stations, which provides funding and drives ridership. reply alephnerd 4 hours agorootparent> My office in Cambridge, Massachusetts is directly in a 12 story tower with ground floor retail directly on top of a transit station That station (the Google Office MIT/Kendall one I'm assuming) was part of the larger redevelopment that happened in Kendall Square over the last 40 years which displaced a significant portion of Cambridge residents due to eh flawed 1949 Urban Redevelopment Act (the same one James Baldwin historically opposed). Just go one stop inbound (MGH) or outbound (Central) and that level of synergy goes away. A lot of this is because the T is just straight up old. Most of the stations are at least a century old if not older and it would take an inordinate amount of money to rebuild stations in a more modern manner. > I suspect the problem might be mismatched incentives The Asian as well as the more recent North American metro systems like BART or DC Metro are much newer (built or rebuilt in the last 50-70 years) and were thus able to include that public-private mixture. reply saagarjha 14 hours agorootparentprevWhat do you mean? Subway in 4th and King is fire reply sofixa 11 hours agorootparentprevIt doesn't have to look like Manhattan. Even medium density (5-10 stories) will be enough in terms of housing provided, and is cheaper to build, and doesn't risk making the street level inhospitable (if nothing is human scale and there's no sunlight on the street, there will be less people there, making it feel or become dangerous). reply blintz 15 hours agorootparentprevTotally agree, though SF is closer to overall NYC density than you'd expect (NYC 29k, SF 18k ppl/sq mi; Manhattan is 73k). I think the Bay Area suburbs are also egregious e.g. North Berkeley BART station is surrounded by single family homes, even though it's 25min from downtown SF by train. reply ChuckMcM 15 hours agoparentprevIt has been great hasn't it?!? The difference between running every 30 minutes on weekends vs ever 2 hours is the difference between hopping on the train up to Redwood City for lunch with a friend and then back again. Versus one of us driving one way or the other. If they can electrify the San Jose to Gilroy segment that will be even better. reply saagarjha 14 hours agorootparentI think they are running into issues with that because they don't own the rail lines, so they will probably go with batteries rather than overhead lines. reply potato3732842 16 hours agoparentprev> It’s way quieter, the cars have nicer interiors, and it is running faster and more frequently than before. Not that electric isn't a huge step up for all the usual reasons but most of that is just because it's new and hasn't been worn out yet. reply markerz 15 hours agorootparentElectric does help with getting cars to run quieter and faster though. With the old diesel engines, they needed more headway because of slower cold start times so there's more frequency and can hit the speed limit faster. Electric engines have high torque and are naturally much quieter. reply renhanxue 15 hours agorootparentIt's not the electric motor torque that makes the difference. Caltrain used to use traditional locomotives, you'd have a single F40PH or MP36PH at one end pulling a bunch of unpowered carriages. Those locomotives were diesel-electric though, that is the diesel is only there to act as a generator while the traction motors are electric. The Stadler KISS units are so much faster because they're EMU's (Electric Multiple Units), that is most of the wheels in a 7-car consist are powered with their own traction motors. An F40PH makes 3000 hp (2200 kW) while a Caltrain 7-car KISS consist can put out 9400 hp (7000 kW) continuous and about 50% more than that in short duration overload mode for starting. That's the reason it accelerates so much faster. Traditional locomotive trains are completely obsolete in any sort of commuter service with frequent stops, there's just no way for them to keep up with EMU's or even DMU's. You can't solve it by just putting bigger engines on the locomotives, you wouldn't have enough weight on the wheels to make sufficient tractive effort. reply pm90 15 hours agorootparentprevYup not to mention that the fumes from diesel engines got into the cars so you tended to avoid the ones close to the engine. Its a HUGE qol improvement! reply stevenwoo 15 hours agorootparentAlso makes waiting at either end, San Jose or San Francisco more pleasant with cleaner air to breathe instead of fumes if you are a little early. reply saagarjha 14 hours agorootparentSan Jose unfortunately has the Gilroy commuter trains idling in the station :( reply arcticbull 9 hours agorootparentFor now. They're replacing them with battery-electric trains that can go past Tamien. Yeah, it should just have caternaries the whole way down but you know how rail is around here owned by Union Pacific, and the federal government prevents municipalities from exercising eminent domain to take control of railways. reply blintz 15 hours agorootparentprevHonestly, I think even the old Caltrain cars were pretty clean. The only dirty parts were the bathrooms (and honestly, train bathrooms are sort of gross most places). Caltrain has always been sort of luxury, though; the median Caltrain rider has a >100k household income, which come to think of it, might be one of the highest median rider incomes for a public transit system in the world. reply kccqzy 3 hours agorootparentApparently in 2019 the average household income of a Caltrain rider is $158,000. It dropped to $95,000 in 2020. https://www.caltrain.com/media/1537/download?inline reply enos_feedler 14 hours agoparentprevIm reading this from a train going from SFO to Milbrae! Y’all can thank me for the electricity later!! reply presentation 11 hours agoparentprevThe train itself was quiet when I rode it from SF to Palo Alto but they were honking the horn almost nonstop… so much for peace and quiet. reply usr1106 8 hours agorootparentI grew up in a 600,000 inhabitants city in Germany. They got EMUs in the 1930s. When they got the next generation in the 1970s all level crossings were replaced. So California, one of the forerunners in the US, seems to be roughly 90 years behind. Depends on your age whether you'll be able to enjoy a quiet train trip during your lifetime.reply dcrazy 1 hour agorootparentIn 1930s California’s population boom had just started. There were 5 million people in the state, split between San Francisco and Los Angeles. Both cities had electric trams, but the demand for regional rail wasn’t high enough to electrify it. Even today it’s not uncommon to de-electrify lower-volume rights of way. reply DiogenesKynikos 8 hours agorootparentprevThat's because they still haven't eliminated the crossing points. They need to turn every road or pedestrian crossing into an underpass or overpass, or eliminate it. They've started on this process, but it will take many years. reply fallsoffbikes 1 hour agorootparentI live a block away from a train crossing for a track that does a lot of local refinery transfers and occasional has freight. It has the normal old style crossing with an arm on each side with lights, a loud bell, and trains required to signal with horn. There are 8 road crossings in a short distance so each train is signals 8 times nearby. The requirements for a no-signal crossing is essentially a pedestrian gate. The quote the city has for each crossing was if I remember right 1.5 million usd. And you’d need to replace many of them. The city doesn’t want to prioritize that much money. (FWIW I agree) The worst thing is we have under utilized tracks going all over the region and no commuter train service. Even with the rail expansion prior to the Olympics (I’m near Los Angeles), the commuter rail is only being extended to the northern most edge of the city. Neighbors have been fighting against commuter rail every step of the way. I’ll say attending local govt and rail proposal meetings is at once interesting, impressive at what some groups are trying to achieve and disturbing at the lengths people go to prevent change. reply Symbiote 8 hours agorootparentprevTrains don't honk in much of Europe when approaching a gated crossing. The lights and barriers are assumed to be sufficient. Within cities, there may well be CCTV cameras (pointing only at the crossing) so the signal controllers can check the crossing is clear. More video than you could possibly want: https://m.youtube.com/watch?v=hidjpkqJ0-Y (I think the honk in the first one is the train driver recognizing the person filming? It's far too late to be for safety.) reply bagels 14 hours agoparentprevStill loud as ever until they get rid of the horns. reply dcrazy 14 hours agoparentprevI was so excited for the EMUs, but unfortunately the near-vertical seat backs are extremely uncomfortable. I ride a coach bus to work and rely on working during my commute. I simply cannot work on Caltrain. reply ninetyninenine 16 hours agoparentprevHaha. Doubtful. In your lifetime you won't see any transit system in the bay area that approaches anything remotely close to what you see in places like Tokyo or NYC. I get that there's improvement but like it's still rather pathetic compared to almost everywhere else across the pond. reply astrange 15 hours agorootparentIf you copy Japan policies you can get something much closer to Japan, it's not magic. (Transit oriented upzoning, no parking minimums, and businesses in the station building.) reply ninetyninenine 15 hours agorootparentIf it's not magic why isn't it being done? That's the point. This news is rather pathetic strictly because a better scenario is NOT magic. reply ricw 15 hours agorootparentIt is. A bunch of Bart stops now have developments around them, just look at MacArthur in Oakland with its 40 floor residential building next to Bart. reply ninetyninenine 14 hours agorootparentHow is that magic? That's good! The problem is that Bart network will barely get denser. At the rate they're building it, your great great great great great grandkids will live in a bay area where most people don't need to jump on a car to get to work. We can do better. It's not magic. But we don't. And that is pathetic. reply Spooky23 14 hours agorootparentprevThe economy is built around juicing home values. The costs make it too expensive for eminent domain and nimby people mobilize now. reply ninetyninenine 14 hours agorootparentAgreed, hence why the situation is rather pathetic. reply zeroonetwothree 15 hours agorootparentprevSimple. Most people don’t want it. reply ninetyninenine 15 hours agorootparentThey do. You're not being realistic. Almost everyone would like to have a choice between getting on a subway for super cheap or driving through traffic. You're saying they would rather have no choice between the two options? Come on. The Bay area (people) want top of the line transit, all of California wants it. It's similar to how Ethiopia wants to get out of poverty. Both places can't get what they want for similar reasons: incapability. Take a look at the high speed rail. Probably won't finish in both of our lifetimes. People want that thing, what's stopping it from happening is incompetence. reply Johnny555 15 hours agorootparentThey do. You're not being realistic. Almost everyone would like to have a choice between getting on a subway for super cheap or driving through traffic. You're saying they would rather have no choice between the two options? Come on Sure, everyone wants an easy commute where the train picks them up in front of their house and drops them off at the office, but they also want their house with yard in Walnut Creek. Or lacking that, their single family house in the Sunset. Sure some people would be happy in a high density city, but do enough people want it to make it worth building? As dense as downtown San Francisco is, it's still a long commute to the Easy Bay or Peninsula from the Sunset or Richmond. Driving is usually faster and more convenient. reply hamandcheese 14 hours agorootparent> but do enough people want it to make it worth building? The reason we don't build more density isn't lack of demand (see: the price of rent), it is entirely political. reply Johnny555 12 hours agorootparentI'd like to see some evidence for that sure, there are a lot of people in big cities that are happy with high density (but even many of them move out to the suburbs when they decide to settle down and have kids), but there are many people in the USA that won't give up their 2000 sq ft house with 3 car garage that fits their F150 pickup. Much of my midwest family is that way they couldn't believe how tiny my \"city\" apartment was that wasn't even in the city, it was a 1600 sq ft townhouse that was a 40 minute commute from city center. It's going to take decades (or some catastrophic disaster) to get Americans to change that mindset and give up low density living. reply arcticbull 9 hours agorootparent> I'd like to see some evidence for that Zoning rules are the evidence. Eliminate the zoning rules and let the market sort it out. No? Why not, are you afraid the \"character\" will change immediately? reply ninetyninenine 10 hours agorootparentprevyou're severely out of touch. Only around 15% of Californians can afford to buy a home. Why can't they buy a home? LACK of Inventory. You can see this on the news, you can quote experts everywhere saying this. There's not enough houses and that raises prices. So how do you lower prices? Build more homes. How do you build more homes? Increase density. Yeah if you're in the 15% sure, buy a big home. If you're in the 85%, well you want to buy a smaller home. You can safely assume 85% of the people who can't afford a home, want cheaper homes and therefore want higher density. >It's going to take decades (or some catastrophic disaster) to get Americans to change that mindset and give up low density living. Bro, that catastrophic disaster is called global warming. And you can see the effects of global warming in the weather in the US. There have already been entire cultures and peoples uprooted from where they live because of rising sea levels. The luxuries we enjoyed living in cities designed for cars is bought and paid for with our future. reply Johnny555 2 hours agorootparentYou can't force developers to build what they can't sell at a profit. I'm sure San Francisco would love to have more billion dollar high density buildings, but can a developer sell enough million dollar condos to pay for them? Is there any evidence that it's zoning that's keeping more residentials towers from being built in downtown SF? >Bro, that catastrophic disaster is called global warming It's not a catastrophic disaster yet nearly all Americans sat at home in comfort watching the LA fires. People don't see a disaster if it doesn't affect them, then it's just a tragedy. reply ninetyninenine 1 hour agorootparentThere are and is currently more residential towers being built in SF. https://youtu.be/kP08AWGfG-w?si=ax8R4l8z0Hd4t_HJ It is zoning that is stopping high density from going up. The house owners are stopping it. You. When you remove those restrictions you get tons of projects wanting to execute on that. Look up builders remedy. In the Bay Area, examples of \"Builder's Remedy\" projects include proposed developments in cities like Mountain View, Menlo Park, Saratoga, and Los Gatos, where developers leverage the state law to build high-density housing projects in areas previously resistant to new development, often by proposing large apartment complexes or mixed-use developments on sites zoned for lower density housing, particularly in affluent communities that haven't met state housing mandates; notable examples include a 200-unit project at 1920 Gamel Way in Mountain View and a large development at the Mountain Winery near Saratoga, which could include a hotel alongside residential units, all while utilizing the \"Builder's Remedy\" to bypass local zoning restrictions due to the inclusion of a significant portion of affordable housing within the project. Key points about Bay Area Builder's Targeted areas: Developers often target affluent cities like Menlo Park, Los Gatos, and parts of Santa Clara County, where housing needs are high but local resistance to new development is strong. High-density development: These projects often propose significantly denser housing than what is typically allowed under local zoning, including multi-story apartment buildings. Affordable housing inclusion: To qualify for \"Builder's Remedy,\" developers must include a substantial percentage of affordable housing units within the project. Local opposition: While intended to address housing shortages, these projects often face significant local opposition from residents concerned about increased density and potential impacts on their neighborhoods. These are rich house owners who own a home and they are the 15% who oppose the 85 percent who don’t. It’s class warfare. reply ninetyninenine 1 hour agorootparentprev> It's not a catastrophic disaster yet nearly all Americans sat at home in comfort watching the LA fires. People don't see a disaster if it doesn't affect them, then it's just a tragedy. LA is your front doorstep and I lived in LA about two miles from the border of the fire. Yeah watch from the comfort of your own home. Give it some more time and one day people will be watching you from the comfort of their own home. reply ninetyninenine 14 hours agorootparentprev>Sure, everyone wants an easy commute where the train picks them up in front of their house and drops them off at the office, but they also want their house with yard in Walnut Creek. Or lacking that, their single family house in the Sunset. You can have both. Tokyo is twice the size of the bay area. Density isn't the issue. It's incompetence. reply Johnny555 12 hours agorootparentI mentioned 3 things pick them up and home and office, and have the big suburban house with a yard. reply ninetyninenine 12 hours agorootparentAnd I'm saying you can HAVE all 3 of those things PLUS a metro. Hence the term BOTHreply Johnny555 12 hours agorootparentWhere in Tokyo do you have easy access to transit and a large suburban house? My brother in law moved from Tokyo to where he could buy a house and yard in Chiba Japan, it's around 900 sq ft with a \"yard\" that's smaller than the deck on the back of my house. And it's still a 20 minute bike ride + 90 minute train ride to his job in Tokyo. I don't know if you've been to many homes in Walnut creek, but a small attached house is not what people are moving out of the city for if that's what they wanted, they could just move to the avenues and stay in SF I don't think I'm overstating when I say that American style suburbs with large lots and large homes are not conducive effective public transit. reply ninetyninenine 10 hours agorootparentI lived in the bay area my entire life. You don't have to go to walnut creek to see suburbia. That ugly shit is everywhere. Public transit in Tokyo is largely underground. Density is irrelevant. If you have high density or low density above ground, this factor is completely orthogonal to whatever you build Underground. Understand? >I don't think I'm overstating when I say that American style suburbs with large lots and large homes are not conducive effective public transit. You, in fact, didn't say ANYTHING related to this matter. You simply stated it's not conducive without mentioning why it's not conducive. I disagree. You can still build it because what's above ground has nothing to do with what's below ground. The fact of the matter is, once you build this, barring zoning restrictions, the density should follow. Right now the bay area is a political battle ground where rich people effectively price out poor people with zoning restrictions. It's a class based war where a luxury you want is impacting the lives of people less fortunate than you. If you let the bay grow naturally and fairly then people with your \"wants\" should move to the country side. Suburbia is also not sustainable for the environment. It's why greenhouse gases per capita in the US is the worst in the world. reply Johnny555 1 hour agorootparentYou seem to be arguing that it's physically possible to build transit that serves low density housing, I agree with that. My argument is that it's economically infeasible, especially in the USA. Extending Caltrain to downtown SF is estimated at $3B/mile, BART to San Jose is $780M/mile. You can't spend hundreds of millions of dollars building transit to a neighborhood with 100 homes. It's already hard to serve those neighborhoods with buses, since bus routes are either long and slow that wind through many neighborhoods, or they are vastly underutilized. reply ninetyninenine 58 minutes agorootparentRight and you should’ve stated this in the beginning but you didn’t. It’s economically feasible. We have the most powerful military in the world we have the highest gdp per capita in the world. It’s economically feasible. When I say we are incapable of building mass transit I’m referring to every single type of incompetency in existence except for economic incompetency. You change the zoning laws it will be utilized. reply Gibbon1 12 hours agorootparentprevBecause our politicians patrons are rentier capitalists who think the government spending money on things is a waste of their money. So yeah they really hate spending money on transit projects. Doesn't help that we're at the end stages of a debt driven real estate/rent price spiral. That means acquiring land for mass transit is twice as expensive as it should be. Ditto labor costs. reply lswainemoore 16 hours agorootparentprevWhy so negative? Given that the served population is conservatively a quarter of either of those areas, doesn't seem like a fair comparison. More to the point, I've been favorably impressed with the transit options since moving here, and in terms of reliability it's been better than NYC, though obviously there are fewer trains/branches. I'd love to see BART open later, like NYC, but even Tokyo trains stop at midnight. reply ninetyninenine 15 hours agorootparentIt's fair. NYC is 8.8 million, bay area is 7 million. Tokyo is about double that. Not being negative. Being realistic. It's unfortunate that being realistic often is negative. Transit here is garbage. You either luck out and live and work near transit or you're like most people and have to drive. A couple million in energy savings doesn't mean anything compared to the amount wasted by cars. reply verteu 15 hours agorootparentSurely the relevant metric is population density? reply ninetyninenine 14 hours agorootparentYou have the causal arrow backwards. A high density train network enables population density and high energy efficiency. At current population density all of our lives would improve if a network as dense as the tokyo metro appeared in the bay area over night. reply astrange 15 hours agorootparentprevNYC is more or less the only system in the world that runs all night like that. It makes it very hard to do maintenance. reply frosted-flakes 10 hours agorootparentAnd NYC can do that because most of it's lines have three or four tracks. reply spike021 12 hours agorootparentprevTokyo metropolitan area is nothing like the SF Bay Area. It's extremely different and so is ridership. Also, if you check out smaller cities in Japan they don't always have as good public transit. Heck, even in Tokyo you can find neighborhoods where you still need to walk or maybe take a bus because train lines don't quite reach them. reply blackeyeblitzar 16 hours agoparentprevHow is safety these days? The last time I was in a BART station, I had to intervene in an uncomfortable situation where a (possibly high) man was bothering a young woman. It felt surreal because everyone around hadn’t helped her at all and was just ignoring what was happening. reply tfehring 16 hours agorootparentBART is better than it was but still bad. Caltrain is totally fine though. reply zer0zzz 16 hours agoparentprevThe new sfo terminal too reply ummonk 14 hours agoparentprevHuh? The new BART cars sound as screechy as ever to me, much louder than the old diesel Caltrain, never mind the new electric Caltrain. reply dheera 13 hours agoparentprevTransit renaissance? Until I can get across both the 84 and 92 bridges by train, to Half Moon Bay, and to Santa Cruz by train, I wouldn't call it a renaissance. Even Cupertino doesn't have a station ffs reply whyenot 16 hours agoprevIt's great, but it took more than 20 years to get here due to environmental reviews and shaky funding. ...and this was just re-fitting an existing, relatively short stretch of track! We (as Californians) have GOT to do better than this! There are huge infrastructure projects that we need to undertake in the coming years. We have got to cut the red tape and properly fund projects. reply joenot443 7 hours agoparentHow do Californians in 2025 feel about the California High-Speed Rail? A similar project [1] is underway in Canada, but I worry sometimes it'll fall prey to the same administrative bog that the CHSR did. It'll be managed by a new Crown corp, VIA HFR, and from my understanding they're still taking engineering bids. I expect it'll be a while before shovels hit the ground. A cheap, fast, and reliable rail from Toronto to Montreal would be a dream come true for a lot of Canadians. Any ideas on how best to make it a reality? [1] https://tc.canada.ca/en/rail-transportation/railway-lines/hi... reply dcrazy 1 hour agorootparentI think high-speed rail between SF and LA is a great idea, and it’s difficult to understand how much of its cost is avoidable excess when the only solution proposed by critics is to shut down the project entirely. reply jacobjjacob 11 hours agoparentprevPeople want infrastructure and then when it gets built, they say it’s too expensive and takes too long. Valid criticisms but many throw the baby out with the bathwater. reply jajko 6 hours agorootparentRail for people is hard to get profitable without pretty high cost (or permanent subsidies). Look at Switzerland this is the land of trains, all reachable cities and villages have main train station at their heart, use is frequent, and its part of national pride (precision, coverage, cleanliness etc.). Yet prices are brutal, sure if you live here you can often afford it but it still hurts. The price keeps rising. And I still use our family car for anything else but commuting to work (and even for that sometimes). Morning trains packed like sardines, in reality there are frequent (usually small though) delays which cascade. Car for 4 is significantly cheaper I'd say 4x for full tickets, less with half/card but then thats 200 bucks a year on its own per head. Whatever problems you are facing, I doubt that in US rail will solve all of them. Happy to be wrong here of course. I see rather some AI vans/minibuses ondemand hailing and sharing as way more effective and cheap solution. Small bus full of people has minimal env impact and can provide door-to-door transport. reply pas 1 hour agorootparent> Car for 4 is significantly cheaper I'd say 4x for full tickets Would it be cheaper with all the usual externalities priced in? reply tim333 5 hours agoparentprevI'm not sure it'll work but we have one of the loudest critics of over regulation making it hard to build things setting up the Department of Government Efficiency when he's not doing iffy salutes on stage. Their proposed approach as written in a WSJ opinion piece is quite interesting >Most legal edicts aren’t laws enacted by Congress but “rules and regulations” promulgated by unelected bureaucrats—tens of thousands of them each year >...President Trump, [] can, by executive action, immediately pause the enforcement of those regulations and initiate the process for review and rescission. >When the president nullifies thousands of such regulations, critics will allege executive overreach. In fact, it will be correcting the executive overreach of thousands of regulations promulgated by administrative fiat that were never authorized by Congress... (wsj thing https://archive.is/nFNp4) reply 9283409232 1 hour agorootparent> iffy You can just call it what it is and say Nazi salute. reply sunday_serif 15 hours agoparentprevCompletely agree! The hard parts seems to be figuring out (1) how to cut red tape for only certain projects and (2) figuring out what red tape to keep. Chesterton's fence and all that. reply huijzer 11 hours agorootparentI’m getting more and more convinced that this is a main problem all over the West. In Germany, a business owner called Marco Scheel is becoming more and more popular by being very outspoken about how bureaucracy is hindering him. His company is called Nordwolle by the way. They make clothing out of sheep wool. They spend a lot of effort finding the right type of wool so they don’t need chemicals to paint it. One major example which Marco first became popular with was that he owned a barn but wasn’t allowed to use it for the factory since it was a farm on paper. The government told him to move to a designated factory area. He argued that it made no sense since he was living in a very remote area, and the barn was of high quality. What else should he do with the barn? Why would he need to build something new somewhere else? The barn was there already and stood already for hundreds of years. His most popular quote is something along the lines of “we can’t all sit with a Chai latte and a MacBook in a coworking space in Berlin and make the 5th dating app. We need some people who do that but not everyone. Some people need to make things with their hands! And for that I need space! I don’t need glass fibre. I need space!” reply ZeroGravitas 8 hours agorootparentI associate this attitude with criticism of Wikipedia and narcissism. I don't think it's a coincidence that it's often around editing things they are related to that this comes up. I tried to do something and they stopped me. This is wrong, I should be able to do this and write my own story. Which is a perfectly normal feeling. But if you end up saying that loudly in public without ever thinking, well what if the rule of \"let this person do whatever they want\" applied to people other than yourself, then that seems to indicate some lack of a wider view. reply roenxi 6 hours agorootparent> ... well what if the rule of \"let this person do whatever they want\" applied to people other than yourself, then that seems to indicate some lack of a wider view. It sounds like Mr. Scheel is applying exactly that view. The idea is everyone should be able to use their remote farm shed for industrial purposes. Indeed, most of the intellectual foundation of the pro-freedom view is precisely that when you take a wider view freedom is generally better for everyone than authoritarianism right up until it becomes a threat to personal safety (even then, pushing the dial a little further towards freedom generally gets better results). If people can't do what they want, then how are things supposed to get done? If we're all doing things in ways that are believed to be impractical then it is going to waste an unreasonable amount of resources and be stupid. reply huijzer 6 hours agorootparentprev> I associate this attitude with criticism of Wikipedia and narcissism. I never said anything about Wikipedia. For the record, I'm a big fan of Wikipedia and I'm skeptical about the new US government. Please don't assume that because someone holds opinion X that they also hold opinion Y. With the current levels of polarization, it's probably a fair assumption to make, but I think we all as individuals have a responsibility to counter that. reply ZeroGravitas 6 hours agorootparentI brought up Wikipedia because it's something I'm interested in. And it's an example of somewhere I'd seen this exact argument against rules/regulation regularly made on HN stories and the comments on them in what I thought was a mostly non-political context. reply huijzer 4 hours agorootparent> I associate this attitude with criticism of Wikipedia and narcissism. > I brought up Wikipedia because it's something I'm interested in. reply sofixa 10 hours agorootparentprev> One major example which Marco first became popular with was that he owned a barn but wasn’t allowed to use it for the factory since it was a farm on paper. The government told him to move to a designated factory area. He argued that it made no sense since he was living in a very remote area, and the barn was of high quality. What else should he do with the barn? Why would he need to build something new somewhere else? The barn was there already and stood already for hundreds of years. Of course it makes sense. Farmland is dedicated to farming and producing food/related things. It lacks connectivity, has fertile soil, prices are cheaper. If anyone can just build a factory there, they will have a negative ecological impact (interrupt animal flows, pollute in areas that are supposed to be cleaner, etc). It's the same reason why you can't farm in an industrial zone, nor can you set up a factory in the middle of the city. Yes, it can be taken too far and abused, but absolutely 100% makes sense and must exist. reply krona 10 hours agorootparentThe guy's not building a gigafactory in his garden, is he? > If anyone can just build a factory there, they will have a negative ecological impact (interrupt animal flows, pollute in areas that are supposed to be cleaner, etc). All true of farming. FWIW as a fellow NIMBY myself, I use the excuse of 'animal flow' (in particular the flow of bats) to prevent anyone from putting anything more than a fence up within 150m of my house. It's great! reply sofixa 9 hours agorootparent> The guy's not building a gigafactory in his garden, is he? How could this possibly be known without a review in your opinion? > fellow NIMBY myself Unless you're American, things don't have to be so binary. The choice isn't between nothing gets built or anyone can just do whatever. We need a balance. > All true of farming I'm pretty sure birds and bees and what not prefer having plants than factories. reply krona 9 hours agorootparent> How could this possibly be known without a review in your opinion? By all means, have planning applications and a system to process them. Things don't have to be so binary. reply roenxi 6 hours agorootparentprevYou're going to end up in a position where you're telling a farmer how to manage & value farmland. That'll lead to more misses than hits. reply sofixa 2 hours agorootparent> You're going to end up in a position where you're telling a farmer how to manage Funny you say that. Not only does that actually happen in pretty much all developed country, it's actually needed for a variety of reasons. There are subsidies to incentivise the \"correct\" crops (you don't want all farmers only doing cash crops for export, rendering your country very vulnerable to import markets to sustain itself), there are also rules/policies to rotate crops to avoid top soil erosion which could be devastating, there are rules on what types of pesticides can be used, etc etc etc etc. reply thereisnospork 15 hours agorootparentprevIf you have to cut red tape for certain projects, the tape probably shouldn't exist in the first place. edit: for instance, if you have e.g. an environmental regulation that is so onerous that exemptions must be doled out for something as sensible as train electrification, then you don't have an environmental review regulation, you have a 'build nothing except what the exemptor decrees' regulation. Which is rather antithetical to the rule of law and good governance. reply Dalewyn 13 hours agorootparentAs a former Californian, you try saying that and you'll be labeled a crazy person railing against the machine. Bureaucracies everywhere tend to protect itself, but California's is particularly vicious. There is a reason many of us call the place Commiefornia. reply timewizard 14 hours agoparentprev> more than 20 years to get here due to environmental reviews How thorough were they being? > We (as Californians) have GOT to do better than this! We pay more taxes than anyone else in the country. /We/ are doing our part. The administration needs to be held accountable. > We have got to cut the red tape and properly fund projects. We need to light a fire under the administration and someone needs to take a wrecking ball through the state assembly. reply tbrownaw 14 hours agorootparent> /We/ are doing our part. The administration needs to be held accountable. ... By the electorate, who clearly aren't doing their part if things like this keep happening. reply timewizard 14 hours agorootparentOther than going down to the assembly and yelling what do you propose? We could call the representatives, recall the representatives, create a ballot measure, or vote differently next time. reply ungreased0675 3 hours agorootparentStop re-electing the same people. I vote for someone different in my state’s partisan primaries, because I believe high turnover of elected representatives is beneficial. I may still vote for the incumbent in the general election, but never in the primary. reply NewJazz 2 hours agorootparentThere are term limits in california. reply Dalewyn 13 hours agorootparentprevnext [9 more] [flagged] NewJazz 2 hours agorootparentThis claim is serious, but unsourced and completely nonsensical. reply Dalewyn 2 hours agorootparentI already sourced it, but if you missed it here it is again: https://leginfo.legislature.ca.gov/faces/billTextClient.xhtm... * A California Driver's License or Identification Card does not completely demonstrate one's right to vote, it only demonstrates residency. Neither does a Social Security Number, it only demonstrates taxes being levied. You must be a US citizen to vote, but anyone can get any or all of the three mentioned pieces of identification without US citizenship. * Signature verification is of questionable efficacy at best. Most of us here likely agree that signing the back of your credit card in this day and age is hardly secure, why is that different in elections? Recounts also don't matter, because at that point the ballot is separated from the voter (ballots do not have identifying information); recounts cannot remove illegal ballots. * California explicitly refuses asking the hypothetical voter to demonstrate their eligibility; see the cited law again. * The claim that asking for voters to demonstrate their eligibility is disenfranchisment is, as someone who cares about free and fair elections, patented bullshit. * Last is my anecdote, which I doubt needs repeating. I have no trust in Californian election integrity because California does nothing to convince me so. reply NewJazz 1 hour agorootparentI can walk into the polls, ask for a ballot and get one, and then fill it in and deposit it, all without anyone so much as asking who [...] I am. Source for this claim? They have asked my name every time I have voted in person. Social Security Number [...] only demonstrates taxes being levied. Another unsourced claim. The state can, and likely does, check whether the SSN belongs to a citizen or not. Same with state ID, they know whether the ID was issued to a citizen or noncitizen. reply quantified 11 hours agorootparentprevDo you mean they won't at least check your professed name and address? reply Dalewyn 10 hours agorootparentNope. It is literally illegal[1] to ask for identification in California. In fairness it was ambiguous[2] before, but in practice California has never asked for identification and this was made clear and official policy from the 2024 elections (note: I am not aware if there are any lawsuits that have blocked enforcement of this law, I no longer reside in CA and do not keep close track of their goings-on). And before anyone says \"But registering to vote requires identification!\": Yes, you're correct, but remember that asking for a voter's identity is illegal. No one can confirm whether a \"voter\" is actually a voter, it is illegal to check. Registering to vote is irrelevant. My personal experiences voting in California also never involved being asked for identification, that includes \"name and address\". Never. None. I went in and voted, nobody cared whether I could because they didn't or couldn't. For the record: I'm an American citizen (born and raised, not that that's relevant), I am registered to vote, I am proud to vote, and I am happy to present to any law enforcement or election official my state Driver's License to prove my residency and my US Passport to prove my citizenship upon demand. I question the narrative that any part of any of this is controversial for ensuring the sanctity of the right to vote and holding free and fair elections. [1]: https://leginfo.legislature.ca.gov/faces/billTextClient.xhtm... [2]: https://perkinscoie.com/insights/update/new-california-law-p... reply saagarjha 6 hours agorootparentSurely they can just check your ballot and see if you are registered to vote when counting it? reply Dalewyn 5 hours agorootparentThat's the theory and in most other states I would agree, but this is California. The priority is in getting votes period, not getting voters. Once a ballot gets past the initial verification stage, ballots are separated from the voter and it becomes impossible to link them back again (ballots have no identifiers on them). Given California's priorities, I have no confidence in the integrity of their ballot verification. While I did still vote when I was in California, I was aware I was likely just wasting my time and that the act of voting had symbolic meaning but no practical value. Election integrity is achieved by vetting elections in ways that are immediately obvious and verifiable by the voters, California unfortunately has none of that as a legal policy of the state. reply NewJazz 1 hour agorootparentJust because you don't understand the vetting doesn't mean it is not effective. Did you reach out to the CA Secretary of State's office regarding your concerns? If you communicated respectfully, I'm sure they would happily provide some reading materials. reply Animats 14 hours agoprevThat's good to hear. Especially since the electric trains accelerate so much faster than the Diesel ones did. Have you been next to one at takeoff? They're going about 40MPH before the train has traveled its own length. Probably limited more by standing passengers than power. Most of the energy used accelerating is recovered at the next stop, so the fast acceleration does not consume much energy. reply ghouse 13 hours agoparent> Most of the energy used accelerating is recovered at the next stop, so the fast acceleration does not consume much energy. Not \"most,\" but a lot. From the article: > regenerative braking on the new trains is generating and sending back to the electric grid approximately 23% of the energy consumed by the system reply basiccalendar74 11 hours agorootparentit is 23% of total energy. if you consider only acceleration energy, it will be much larger percentage, probably 80% (as in electric cars). reply acchow 7 hours agoparentprev> so the fast acceleration does not consume much energy Also, acceleration has no effect on the energy if you’re trying to hit the same target speed (unless there is friction, in which case you’d use less energy with higher acceleration) reply two_handfuls 16 hours agoprevAnother cool thing in the article that I didn't know personally: Caltrain runs on 100% renewable power! reply benatkin 14 hours agoparentThat shouldn't be put forth as a goal. It's how Germany got rid of all its nuclear plants and has to keep burning coal and buying fuel from Russia. reply blamazon 6 hours agorootparentIt's also how Scotland ended up harnessing their wind power, much to the consternation of one American capitalist... reply lacker 15 hours agoprevThey claim that \"Caltrain is running its service on 100% renewable energy\", but they are connected to the same grid as everyone else. It doesn't really make sense to say, half of our electricity is green, so customer X is renewable, but customer Y is not renewable. reply skykooler 15 hours agoparentIt doesn't physically, but it does financially. Power generation is not free after all, so any power station, be it renewable or non-renewable, will only be producing as long as people are buying electricity from it. reply dpierce9 14 hours agoparentprevPower is pooled. If I buy from a supplier or group of suppliers that (1) procures only from renewable resources (2) isn’t reselling power from non renewable sources, (3) hasn’t sold the power more than once, and (4) is capable of providing my energy demands at any given time, then I am buying green power from the pool. It doesn’t matter if the actual electrons come from Ng or coal because I bought enough for the pool (the electrons I added to the pool will be used by someone else if I am using ng electrons). Not 100% sure this is how Caltrain works but the fact that everyone is physically using the same pool does not imply that you cannot be 100% renewable if you buy from suppliers to the pool with the above properties. reply pm90 15 hours agoparentprevYou can generally choose what kind of power generation mix you want from a utility. eg I am on a 100% renewable plan. reply timewizard 14 hours agorootparentOut of the 25GW being generated right now only 3GW are renewable. There is a corner where there is more demand from \"100% renewable\" customers than there actually is available renewable energy. There is no point at which this gets made up. reply vmladenov 14 hours agorootparentI’m not sure where your figures are coming from, did you mean at the moment of posting your comment? If you look at the integral over the year, California does decently well[1] on renewables, and the people paying for it help blunt the competitive edge of the tremendous federal subsidies enjoyed by fossil fuels. [1]https://www.energy.ca.gov/data-reports/energy-almanac/califo... reply timewizard 10 hours agorootparent> did you mean at the moment of posting your comment? Yes, right this moment. You can see this here: https://www.caiso.com/todays-outlook/supply > does decently well[1] on renewables Technically it does decently well on combined \"non greenhouse gas + renewables.\" This is a rather self serving categorization of generation sources and might not be what people buying \"100% renewable\" energy think they're actually getting. In any case, subtracting Nuclear and Hydro, if more than half the kWh purchased in the state are purchased as \"100% renewable\", they cannot all be possibly served by renewables even in the aggregate. reply dpierce9 14 hours agorootparentprevThere are two views of this. The first is that at any given point in time, my instantaneous energy use is offset by renewables. The second is that over some period of time (e.g., one month) my aggregate energy use is offset by renewables. The second is MUCH easier. When people say things are 100% renewable, I generally think they mean the second thing. This is a bit of a fudge (not wrong but not 100% level). reply jer0me 11 hours agoparentprevYes, electrons are electrons are electrons. But customer X is paying for the renewable sources and customer Y is paying for the non-renewable sources, so customer X can say that they are renewable. reply saagarjha 14 hours agoparentprevhttps://www.peninsulacleanenergy.com/news-releases/quick-cle... reply bagels 13 hours agoparentprevI'm using the solar. You're using the natural gas. Easy. You just say it. reply gc9 3 hours agoprevIf the renewably sourced power is purchased from big hydroelectric dams, the institutional price might depend on how full the reservoirs are. Power costs might be higher in drier years. reply benced 15 hours agoprevThis literally doesn’t matter. The energy consumption of a train is trivial compared to displacing cars. If there was a way to trade this savings for higher speed, ridership, or frequency, it would be worth it. (If there’s not, this is a free win and yay but it’s the wrong focus) reply winrid 15 hours agoparentAt least it's a win for air quality for the people that live near the tracks. You could smell and taste the old diesels, was pretty annoying. reply rsanek 11 hours agorootparentreally? how close were you living? i lived for years just 3 houses over from the tracks and never even smelled the trains. definitely heard them tho reply quantified 11 hours agorootparentI'd breathe it as I biked alongside. Maybe prevailing wind helped your air quality. reply archagon 15 hours agoparentprevBoth speed and frequency are significantly up, and I’ve heard ridership is as well. Also, noise and pollution are substantially down. A huge win all around. reply savrajsingh 5 hours agoprevI can’t believe the New York City subway does not have this technology. We are currently breathing brake dust in the subway. reply fsckboy 14 hours agoprev>regenerative braking on the new trains is generating and sending back to the electric grid approximately 23% of the energy consumed by the system does this make sense? let's round that 23 up to 25%, and say brake energy regeneration is 100% efficient (it's not): one quarter of the energy put into the system comes back out? city driving in a car, you might have your foot on the brake 25% of the time (yes, that's not how it works, just think conceptually) but a train spends a lot of time running, and comparatively a small amount of time braking, and if you were to detach the engine from the train, one doesn't get a sense the train would roll very far unpowered, so while there's momentum to scavenge, it doesn't seem like 25% of the total I'm just thinking, how do these numbers make sense? reply mikepurvis 14 hours agoparentFirstly, I believe the trains are EMUs, so every bogey is powered rather than having the whole thing hailed by an engine; this is actually a huge benefit of electric as the trains can accelerate to speed much faster when they’re all wheel drive. But to address your main point, the rolling resistance of a train in motion is tiny; basically all the energy cost is at acceleration time— it probably cruises at like 5-10% power. reply Groxx 14 hours agoparentprev\"rolling very far unpowered\" is like the entire point of rail systems reply RWSen 8 hours agorootparentExactly. There's a train route here in The Netherlands, from Den Bosch to Utrecht, where the train can coast the last 20 km. That's out of a 48 km trip, so almost half! reply justinc8687 13 hours agoparentprevTrains have little friction on the tracks. That's one of the ways they are much more efficient than a car (with rubber tires). If the moving friction is very low, it makes sense that not that much energy is used to move the train once it's at speed. Slowing it down would return some of the energy back (with the rest dissipated as heat). reply lutorm 10 hours agorootparentEven with a car's higher rolling resistance, most of the drag at cruising speed is aerodynamic, though. But a commuter train, by definition, starts and stops a lot so it makes sense that a large fraction of the energy used would be for acceleration. reply renhanxue 6 hours agorootparentAerodynamic drag is proportional to cross sectional area. A train only has maybe 2-3x the drag of a car at the same speed, but it has on the order of 100 to 1000x the mass and momentum. It takes forever for drag to do anything to a train at speed. reply Symbiote 7 hours agoparentprev> one doesn't get a sense the train would roll very far unpowered Even a model/toy train wagon can easily coast the length of a large room when given a push. See how little metal is in contact, about 1cm²: https://www.reddit.com/r/Damnthatsinteresting/comments/17ysd... There's very little friction. reply renhanxue 12 hours agoparentprevTry out the video game Derail Valley and you'll get a decently realistic idea of just how much trains tend to just coast along without any power input. On flat ground you just need a little nudge every once in a while to keep going. reply apexalpha 10 hours agoprevWhile this sounds like good news it's very odd to me that a project the size of Caltrain would completely forget about regenerative breaking when calculating the electricity usage. reply skrap 16 hours agoprevI mean, this is good news, but why was the efficiency of the system so misunderstood at the design phase? I hope someone's interested enough to find out! reply crocal 9 hours agoparentAhoi! Participated to the development of a simulation tool for that purpose. Many factors need to be taken into account: The gradients of the track How agressively you want to drive at different time of the day How many trains The timing of the trains. When one train brakes, it can power another one that accelerates at the same moment The efficiency of the power chain The resistance model of the train With so many parameters the results can be quite volatile. However, rule of the thumb in the business is that 30% saving can be achieved with good energy management. Hope this clarifies a little. Update: bullet points reply skrap 4 hours agorootparentInteresting! Thanks for the great info. Do you know if anyone will have the opportunity to integrate observations into the model so it can continue to improve? reply djaychela 7 hours agoparentprevMaybe they didn't factor in ridership on certain part of the track gradient, which would lead to more regen braking than expected? reply acyou 15 hours agoprevThey are using less electricity than expected. The article neglects to specify whether this is due to higher than expected efficiency, or higher than expected downtime. Not that there is necessarily a problem with the technology, but energy efficiency estimates should be expected to already be quite accurate, all the technology is super old. If the efficiency is not in fact higher, I think the article is written a little disingenuously, they should really be more specific. reply justinzollars 15 hours agoprev> With the agency expecting approximately $6 million annually in energy credits from the California Air Resources Board’s Low Carbon Fuel Standard Program the first year of electric service will have lower fuel costs than the previous diesel service Thats the key. Without the subsidy its more expensive and less efficient than carbon based fuel. In the long run we are worse off, because the subsidy can't last forever. reply renhanxue 15 hours agoparentThey've significantly increased the timetable frequency though, so they're running more trains, and those trains are accelerating and running much faster than the old diesel-electrics. I don't think you're comparing like for like. reply kibwen 15 hours agoparentprev> Without the subsidy its more expensive and less efficient than carbon based fuel. If markets actually worked for long-term decisionmaking and were therefore capable of pricing in negative externalities, then factoring in the cost of causing Earth to asymptotically approach Venus would change this calculus. reply djaychela 7 hours agoparentprev>In the long run we are worse off, because the subsidy can't last forever. You're making the classic error here of not factoring the externalities involved the pollution and other issues from fossil fuels. If they were actually being paid for then electricity would be much cheaper. Fossil fuels get a defacto subsidy by being able to pollute the environment without any cost. reply ghouse 13 hours agoparentprevThis is because PG&E retail is about 2.5x the national average. Also, worth noting in the event the conclusions are extrapolated to diesel over the road transportation: train diesel is much less expensive as it's exempt from fuel (state and federal) taxes. reply hamandcheese 14 hours agoparentprev> Currently, Caltrain is providing that power to the grid free of charge as there is no legal requirement for the agency to be reimbursed for the energy generated. With further investment in energy storage it sounds like they could nearly cover a lost subsidy. Moreover, the improved service from electric (faster acceleration and better air quality) seem very worthwhile. reply zer0zzz 16 hours agoprevFinally some good news reply jeffbee 16 hours agoprevIt's hilarious but sad to watch Americans learn things that agencies abroad already know. The other example of this I saw today was that in NYC the congestion pricing has \"surprisingly\" slashed the number of car crashes and injuries in Manhattan. Like, duh. reply readdit 16 hours agoparentI feel this is positive news moving in a positive direction. Do you suggest not doing anything to improve the situation? We can’t go back in time. But we can help ourselves in the future. reply dhosek 16 hours agorootparentI think it’s more that Americans seem to be surprised by things that people in other countries have known for decades. Meanwhile, the current administration is trying to send us back to the 19th century in civil rights, healthcare and energy policy. reply fragmede 3 hours agorootparenthttps://xkcd.com/1053/ but with different numbers reply lmm 16 hours agorootparentprevOne way to help yourselves in the future would be to learn the meta-lesson that actually things that work in other countries mostly work the same way in the US too. reply Nemo_bis 8 hours agorootparentYes this whole thread is so depressing. Most commenters here are talking as if electric trains were an oddity. It's like listening to people questioning the benefit of running water over the old school walk to the well. reply dylan604 15 hours agorootparentprevSome people take your word for it, but some people just have to touch the stove to learn that it is hot. reply saagarjha 14 hours agorootparentThe US has been pressing its palm against the grill for a while now. reply brookst 15 hours agorootparentprevOh, we all know. It’s just convenient to pretend otherwise sometimes. reply ninetyninenine 16 hours agorootparentprevI feel this improvement is negligible. I doubt you'll see any meaningful change in your lifetime. At best you'll see an electric car renaissance. But the infra for trains and public transit? Doubtful. reply readdit 16 hours agorootparentIt’s still a positive change an I’m all for it. reply ninetyninenine 16 hours agorootparentI'm all for positive change too. But as someone who isn't delusional, I have to account for practicality rather then dreaming about an unrealistic future. reply tifik 16 hours agorootparentprevOf course its positive news. Of fourse we cant go back in time. Its still good fun to poke at US for being so late on these discoveries. reply londons_explore 16 hours agoparentprevI dunno... Many trains in London still don't do regenerative braking despite the technology to generate electricity from motion being around for... checks notes... 194 years! reply benmanns 16 hours agorootparentYou probably know more about this than me, but it looks like some lines do: https://tfl.gov.uk/corporate/transparency/freedom-of-informa... Regenerative braking in cars also keeps a lot of brake dust out of the air. A pair of brake pads lasts about as long as the life of an EV. reply Dylan16807 13 hours agorootparent> You probably know more about this than me, but it looks like some lines do I'm confused. Your phrasing suggests that you're adding in new and potentially conflicting information, but \"many don't\" and \"some do\" mean the same thing. reply londons_explore 4 hours agorootparentThe missing piece of info here is that while most of the trains have the capability to do regen braking, it is generally ineffective because it can only be used if another train on the same section of track happens to also be accelerating at the same time. That happens rarely enough that most of the trains, most of the time, do not effectively regen, and instead use mechanical or resistive braking. reply jeffbee 33 minutes agorootparentThere was chatter 20-30 years ago about using flywheels to bank the energy until needed. reply IncreasePosts 16 hours agorootparentprevThe deepest metro lines are also boiling, because the heat from braking just gets constantly absorbed into the ground, and the ground's ability to disperse the heat is maxed out. reply ZeroGravitas 5 hours agorootparentSome of that heat is now used to heat homes via a local heat network originally built to use combined heat and power. reply xnx 16 hours agorootparentprevDo trains need batteries for regenerative braking, or is it sent right back on the same power lines? reply grakker 15 hours agorootparentThe article says this version sends it back out on the same power lines, either used by other trains or pushed back into the grid. Seems like a perfect job for supercapacitors, although I have no idea about the feasibility of that solution. I imagine that batteries having huge input/output cycles like that wouldn't be healthy for them. Again, pulled from my imagination because I don't really know much about battery wear/use. reply numpad0 3 hours agorootparentGenerally, trains are scheduled so that one train decelerating roughly coincide with another accelerating. You can plan ahead and orchestrate all you want and overcommit capacity as much as you want. Leftover that didn't cancel out is fed back to hydroelectric dams for gravity storage which do require cleaning but are immune to chemical degradation. By the way, implementing regen on synchronous motors is relatively easy, IIUC, command a positive torque to the inverter and it draws current and line voltage gets pulled down. Command negative and opposite happens. reply londons_explore 8 hours agorootparentprevIf it were 1995, the optimal solution would have been NiMH batteries at the side of the track. They can do massive currents in and out, and a few tons of batteries would be enough to fully store the energy of a passenger train stopping from 60 mph to nil inside 30 secs. NiMH could have been attached directly to the rails. Today, lithium batteries win for Watts per dollar, and perhaps custom made packs could also be attached directly to the rails. But a cheaper solution is probably bidirectional inverters, allowing the DC generated by the trains to be fed back into the 3 phase national grid. Unfortunately, all trains in London today cannot regen into the grid they can only regen into the rails and hope that some other train on the same rail is accelerating at the same time to use the energy. By my estimates, that only happens less than half the time. reply dylan604 15 hours agoparentprevHow do you get the choir to sing? You preach to the choir. When the needle is moving in the right direction, publicize that so everyone knows about it. That public info makes it harder if someone then comes around and suggests undoing/defunding or any other type of thing that stop the needle or make the needle move in the opposite direction. Keeping the info quiet makes yanking the plug very easy reply IncreasePosts 16 hours agoparentprevI don't see why it couldn't be surprising if there are fewer cars in Manhattan, it could mean the cars will have a higher average speed, meaning they might be more likely to get into an accident. reply cameldrv 16 hours agorootparentMy experience in Manhattan is that when it gets really congested, it turns into a giant game of chicken with everyone jockeying for position. This has got to lead to a lot of accidents (although probably mostly fairly minor) reply astrange 15 hours agoparentprev\"Surprisingly\" is an opinion on the part of whoever wrote that, but it's not a surprise to any of the advocates. It has been working better than expected, possibly because the bad drivers were breaking a lot of other traffic laws and this one's easier to enforce so is actually keeping them away. reply MattGaiser 15 hours agoparentprev> The other example of this I saw today was that in NYC the congestion pricing has \"surprisingly\" slashed the number of car crashes and injuries in Manhattan. Like, duh. You didn't see the online crowds screaming up and down that it wouldn't do anything but pick the pockets of people? Many genuinely did not believe things would change. reply aetherspawn 16 hours agoprevTrain technology is so behind compared to road electric vehicles. Companies that build train batteries have been trying to sell to us (car OEMs) lately, and we look at the battery specs and battery management system capabilities and go uhhhh… On the motor and control fronts as well. reply s0rce 15 hours agoparentWhy? Just build more trains any train infrastructure is so much better than cars, they barely need improvement, they just need to be built reply aetherspawn 15 hours agorootparentBut for example if they used PMM squirrel cage motors etc, they would be nice and smooth, cheaper to build, more energy efficient. On the battery front, using car-style or truck-style EV battery modules would be more robust and would reduce cost by somewhere in the order of 50%. Instead, most trains still use induction motors (this is the high pitch whine you hear) and many train battery designs use industrial style rack mount batteries, with screw terminals, controlled by a massive PLC cabinet. In eMobility the controls are handled by a small safety-rated microcontroller. reply numpad0 3 hours agorootparentThey care a lot more about robustness, maintenance, max continuous power, than cost, volume, or short duration ratings. Commuter trains are used for a decade and half, then sold to developing nation to be used couple more. No surprise that they're choose rack batteries with thick cables to bus bars over plastic packs in trays, if they even consider using degradable batteries at all. reply saagarjha 14 hours agorootparentprevCaltrain is plenty smooth, have you ridden it recently? reply adgjlsfhk1 15 hours agorootparentprevWhat are you talking about? The noise you hear from trains is the wheels. Battery trains basically don't exist because of this cool invention called a wire and pantograph. reply numpad0 3 hours agorootparentElectric commuter trains has coil whines, same thing as spaceship sounds Teslas make when launched https://www.youtube.com/watch?v=i0B2bvd9rFQ reply aetherspawn 13 hours agorootparentprevThe “nice smooth trains” that commenters are talking about use a pantograph, transformer and small on-board battery, and can go a small distance (few kms) through tunnels and between track segments with the pantograph lowered. The power system after the battery is similar to older model electric vehicles. reply Symbiote 7 hours agorootparentThere's no battery for the motors. A train can easily coast through a tunnel etc without any power. (There will be a battery for lights etc.) reply inglor_cz 6 hours agorootparentprevThe fundamental problem with any infrastructure today is that a) it works better in a densely populated territory, b) but the more people around, the more NIMBY cliques to throw sand into the permiting process, usually abusing environmental protections to do so. If the new Trump administration defangs NEPA reviews just a bit and I see no evidence that the original intent behind NEPA was to delay everything everywhere by decades it might help, but a powerful YIMBY movement would be even better. reply declan_roberts 15 hours agoprev [–] 19 million annually. I don't think the juice is worth the squeeze here. They should focus on making Caltrain run reliably timely, and safely for the passengers. Once we get the ridership numbers up we can easily make up for the electricity efficiency. reply tfehring 15 hours agoparentElectrification wasn't just an efficiency thing, it also made the service much better because of the faster acceleration IIRC my travel time to SF on weekends went from 75 to 50 minutes, which opens up a lot of use cases. reply erikgaas 15 hours agorootparentThe other important implication is how Caltrain could be routed through underground tunnels to Salesforce transit center. 4th and king isn't super nice and, correct me if I'm wrong, but it isn't well connected to other modes of transport. Maybe some muni buses. reply jeffbee 14 hours agorootparent4th & King has much better connections to transit and neighborhoods, and I hope they never build that tunnel. reply erikgaas 14 hours agorootparentMuch better really? Looking at the muni map, pretty much all transit clusters around downtown. And bart doesn't even stop at 4th and King. You have to go to Millbrae for a close-ish transfer. Added to that the transit center is very close to the ferry building. And then you have golden gate transit. And it's a hub that has tons of space to accommodate a ton of transfers. https://www.sfmta.com/maps/muni-service-map reply jeffbee 14 hours agorootparentFrom Salesforce you can go up to the street and get the 38, granted. But to connect with BART or Muni Metro you are walking 2 blocks. At Caltrain you can get the N or T right across from the end of the platform, or board the 15, 30, or 25 bus. The T transfers to BART at Powell, with significant subterranean walking. And for trips within SF you might be closer to your destination starting from Caltrain. You'd crawl through a tunnel to Salesforce to be in Downtown/SoMa, a place everyone wants to leave? reply dcrazy 14 hours agorootparentprevThis is certainly a take. Getting anywhere except SOMA startupland from 4th and King is a minimum 15 minute ride on the T or N. reply pm90 15 hours agoparentprevConsidering the anti transit attitude of most suburban dwelling Americans I am personally glad they are touting this. reply bluGill 15 hours agorootparentConsidering the bad transit options they get anti transit is the rational choice. Suburbs are dense enough for great transit but it is expensive and so nobody can afford to give it to them. Thus continuing the cycle of bad transit worth opposing as the waste of money it is. reply jwagenet 14 hours agorootparentSuburbs are absolutely not dense enough for great transit, and the per capita coverage is bad enough the expense isn’t worth it. They can get sparse bus or light rail coverage and thats it. Great transit offers coverage for stops every mile, preferably half mile. Systems like Caltrain and bart are great for regional rail, but anywhere serviced by them is generally not well enough connected that you don’t have to drive to the station. reply SkyPuncher 15 hours agoparentprevIt doesn't really seem like an either or type of situation. My understanding is the electricity efficiency is really just a nice bonus (or perhaps justification) for making improvements that would also increase ridership. I'm only loosely aware of the Electric Fleet, but my understanding is they add capacity that was desperately needed. reply declan_roberts 13 hours agorootparentIn what world is $19 million just a \"nice bonus\"? Get the rider numbers up and extend the lines. Thats the only thing we need right now. reply Prosammer 15 hours agoparentprev [–] Caltrain runs reliably and safely for me, can you elaborate? reply declan_roberts 13 hours agorootparent [–] The $19 million for regenerative breaking can be used to extend lines or add new stations. We can make up for energy efficiency later. reply renhanxue 12 hours agorootparent [–] I think you completely misunderstood the article. It says that the electricity used to power these trains was estimated to cost $19m a year, but the trains are a bit more efficient than expected, so the new estimate is only $16.5m/year. Most if not all electric trains powered by overhead wires have regenerative braking that feeds back to the grid as standard, and Caltrain's new Stadler units definitely do. Nobody paid extra for regenerative braking, it's just the default way to do things on electric trains, to the point that it'd be weird not to have it. Even diesel-electrics often have a kind of regenerative braking, but they don't have anywhere to store the recovered energy so they just convert it to heat via huge resistors. reply Symbiote 7 hours agorootparent [–] There's additional cost for the transformers powering the caternary above the track to allow them to receive power, not just send it. However, since this has been normal in Europe and Asia for decades, I doubt the additional expense is significant. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Caltrain's new electric fleet surpasses expectations in efficiency, operating quieter, faster, and more frequently, contributing to a transit renaissance in the Bay Area.",
      "The electric trains utilize regenerative braking, which returns energy to the grid, and operate on 100% renewable power, highlighting a commitment to sustainability.",
      "Despite these advancements, challenges like the need for increased urban density and improved transit-oriented development persist, alongside criticisms of BART's management and governance."
    ],
    "points": 306,
    "commentCount": 259,
    "retryCount": 0,
    "time": 1737769991
  },
  {
    "id": 42817439,
    "title": "Lightpanda, an open-source headless browser in Zig",
    "originLink": "https://github.com/lightpanda-io/browser",
    "originBody": "We’re Francis and Pierre, and we&#x27;re excited to share Lightpanda (https:&#x2F;&#x2F;lightpanda.io), an open-source headless browser we’ve been building for the past 2 years from scratch in Zig (not dependent on Chromium or Firefox). It’s a faster and lighter alternative for headless operations without any graphical rendering.Why start over? We’ve worked a lot with Chrome headless at our previous company, scraping millions of web pages per day. While it’s powerful, it’s also heavy on CPU and memory usage. For scraping at scale, building AI agents, or automating websites, the overheads are high. So we asked ourselves: what if we built a browser that only did what’s absolutely necessary for headless automation?Our browser is made of the following main components: an HTTP loader an HTML parser and DOM tree (based on Netsurf libs) a Javascript runtime (v8) partial web APIs support (currently DOM and XHR&#x2F;Fetch) and a CDP (Chrome Debug Protocol) server to allow plug & play connection with existing scripts (Puppeteer, Playwright, etc).The main idea is to avoid any graphical rendering and just work with data manipulation, which in our experience covers a wide range of headless use cases (excluding some, like screenshot generation).In our current test case Lightpanda is roughly 10x faster than Chrome headless while using 10x less memory.It&#x27;s a work in progress, there are hundreds of Web APIs, and for now we just support some of them. It&#x27;s a beta version, so expect most websites to fail or crash. The plan is to increase coverage over time.We chose Zig for its seamless integration with C libs and its comptime feature that allow us to generate bi-directional Native to JS APIs (see our zig-js-runtime lib https:&#x2F;&#x2F;github.com&#x2F;lightpanda-io&#x2F;zig-js-runtime). And of course for its performance :)As a company, our business model is based on a Managed Cloud, browser as a service. Currently, this is primarily powered by Chrome, but as we integrate more web APIs it will gradually transition to Lightpanda.We would love to hear your thoughts and feedback. Where should we focus our efforts next to support your use cases?",
    "commentLink": "https://news.ycombinator.com/item?id=42817439",
    "commentBody": "Lightpanda, an open-source headless browser in Zig (github.com/lightpanda-io)275 points by fbouvier 20 hours agohidepastfavorite128 comments We’re Francis and Pierre, and we're excited to share Lightpanda (https://lightpanda.io), an open-source headless browser we’ve been building for the past 2 years from scratch in Zig (not dependent on Chromium or Firefox). It’s a faster and lighter alternative for headless operations without any graphical rendering. Why start over? We’ve worked a lot with Chrome headless at our previous company, scraping millions of web pages per day. While it’s powerful, it’s also heavy on CPU and memory usage. For scraping at scale, building AI agents, or automating websites, the overheads are high. So we asked ourselves: what if we built a browser that only did what’s absolutely necessary for headless automation? Our browser is made of the following main components: an HTTP loader an HTML parser and DOM tree (based on Netsurf libs) a Javascript runtime (v8) partial web APIs support (currently DOM and XHR/Fetch) and a CDP (Chrome Debug Protocol) server to allow plug & play connection with existing scripts (Puppeteer, Playwright, etc). The main idea is to avoid any graphical rendering and just work with data manipulation, which in our experience covers a wide range of headless use cases (excluding some, like screenshot generation). In our current test case Lightpanda is roughly 10x faster than Chrome headless while using 10x less memory. It's a work in progress, there are hundreds of Web APIs, and for now we just support some of them. It's a beta version, so expect most websites to fail or crash. The plan is to increase coverage over time. We chose Zig for its seamless integration with C libs and its comptime feature that allow us to generate bi-directional Native to JS APIs (see our zig-js-runtime lib https://github.com/lightpanda-io/zig-js-runtime). And of course for its performance :) As a company, our business model is based on a Managed Cloud, browser as a service. Currently, this is primarily powered by Chrome, but as we integrate more web APIs it will gradually transition to Lightpanda. We would love to hear your thoughts and feedback. Where should we focus our efforts next to support your use cases? fbouvier 1 day agoAuthor here. The browser is made from scratch (not based on Chromium/Webkit), in Zig, using v8 as a JS engine. Our idea is to build a lightweight browser optimized for AI use cases like LLM training and agent workflows. And more generally any type of web automation. It's a work in progress, there are hundreds of Web APIs, and for now we just support some of them (DOM, XHR, Fetch). So expect most websites to fail or crash. The plan is to increase coverage over time. Happy to answer any questions. reply bityard 1 day agoparentPlease put a priority on making it hard to abuse the web with your tool. At a _bare_ minimum, that means obeying robot.txt and NOT crawling a site that doesn't want to be crawled. And there should not be an option to override that. It goes without saying that you should not allow users to make hundreds or thousands of \"blind\" parallel requests as these tend to have the effect of DoSing sites that are being hosted on modest hardware. You should also be measuring response times and throttling your requests accordingly. If a website issues a response code or other signal that you are hitting it too fast or too often, slow down. I say this because since around the start of the new year, AI bots have been ravaging what's left of the open web and causing REAL stress and problems for admins of small and mid-sized websites and their human visitors: https://www.heise.de/en/news/AI-bots-paralyze-Linux-news-sit... reply hombre_fatal 3 hours agorootparentThis is HN virtue signaling. Some fringe tool that ~nobody uses is held to a different, weird standard and must be the one to kneecap itself with a pointless gesture and a fake ethical burden. The comparison to DRM makes sense. Gimping software to disempower the end user based on the desires of content publishers. There's even probably a valid syllogism that could make you bite the bullet on browsers forcing you to render ads. reply gkbrk 1 day agorootparentprevPlease don't. Software I installed on my computer needs to the what I want as the user. I don't want every random thing I install to come with DRM. The project looks useful, and if it ends up getting popular I imagine someone would make a DRM-free version anyway. reply GuB-42 2 hours agorootparentWho told you about DRM? It is an open source tool. Simply requiring a code change and a rebuild is enough of a barrier to prevent rude behavior from most people. You won't stop competent malicious actors but you can at least encourage good behavior. If popular, someone will make a fork but having the original refuse to do stuff that are deemed abusive sends a message. It is like for the Flipper Zero. The original version does not let you access frequency bands that are illegal in some countries, and anything involving jamming is highly frowned upon. Of course, there are forks that let you do these things, but the simple fact that you need to go out of your way to find these should tell you it is not a good idea. reply tossandthrow 1 day agorootparentprevWhere do you read DRM? Parent commenter merely and humbly asks the author of the library to make sure that it has sane defaults and support for ethical crawling. I find it disturbing that you would recommend against that. reply gkbrk 1 day agorootparentHere's what the parent comment wrote. > And there should not be an option to override that. This is not just a sane default. This is software telling you what you are allowed to do based on what the rights owner wants, literally DRM. This is exactly like Android not allowing screenshots to be taken in certain apps because the rights owner didn't allow it. reply blacksmith_tb 23 hours agorootparentNot sure what \"digital rights\" that \"manages\"? I don't see it as an unreasonable suggestion that the tool shouldn't be set up out of the box to DoS sites it's scraping, that doesn't prevent anyone who is technical enough to know what they're doing to fork it and remove whatever limits are there by default? I can't see it as a \"my computer should do what I want!\" issue, if you don't like how this package works, change it or use another? reply benatkin 20 hours agorootparentDigital Restrictions Management, then. Have it your way. reply concerndc1tizen 6 hours agorootparentThere are so many combative people on HackerNews lately who insist to misinterpret everything. I really wonder if it's bots or just assholes. reply cies 5 hours agorootparentIndeed DRM is a very different thing from adhering to standards like `robots.txt` as a default out of the box (there could still be a documented option to ignore it). reply concerndc1tizen 5 hours agorootparentThat's just like, your opinion, man He was using DRM as a metaphor for restricted software. And advocating that software should do whatever the user wants. If the user is ignorant about the harm the software does, then adding robots.txt support is win-win for all. But if the user doesn't want it, then it's political, in the same way that DRM is political and anti-user. reply JambalayaJimbo 22 hours agorootparentprevThis is software telling you what you are allowed to do based on what the software developer wants* (assuming the developers cares of course...). Which is how all software works. I would not want my users of my software doing anything malicious with it, so I would not give them the option. If I create an open-source messaging app I am also not going to give users the option of clicking a button to spam recipients with dick pics. Even if it was dead-simple for a determined user to add code for this dick pic button themselves. reply concerndc1tizen 6 hours agorootparentprev> I find it disturbing Oh no, someone on the internet found something offensive! reply tossandthrow 1 hour agorootparentDisturbing, not offensive it is literally right there in the quote you have been so nice to pass along. reply calvinmorrison 3 hours agorootparentprevi still wont forgive libtorrent for not implementing sequential access. and also, xpdf for implementing the \"you cant select text\" feature reply bityard 23 hours agorootparentprevI feel like you may have a misunderstanding of what DRM is. Talking about DRM outside the context of media distribution doesn't really make any sense. Yes, someone can fork this and modify it however they want. They can already do the same with curl, Firefox, Chromium, etc. The point is that this is project is deliberately advertising itself as an AI-friendly web scraper. If successful, lots of people who don't know any better are going to download it and deploy it without a full understanding (and possibly caring) of the consequences on the open web. And as I already point out, this is not hypothetical, it is already happening. Right now. As we speak. Do you want cloudflare everywhere? This is how you get cloudflare everywhere. My plea for the dev is that they choose to take the high road and put web-server-friendly SANE DEFAULTS in place to curb the bulk of abusive web scraping behavior to lessen the number of gray hairs it causes web admins like myself. That is all. reply randunel 21 hours agorootparentIt's exactly DRM, management of legal access to digital content. The \"media\" part has been optional for decades. The comment they replied to didn't suggest sane defaults, but DRM. Here's the quote, no defaults work that way (inability to override): > At a _bare_ minimum, that means obeying robot.txt and NOT crawling a site that doesn't want to be crawled. And there should not be an option to override that. reply samatman 20 hours agorootparentI'll also add something that I expect to be somewhat controversial, given earlier conversations on HN[0]: I see contexts in which it would be perfectly valid to use this and ignore robots.txt. If I were directing some LLM agent to specifically access a site on my behalf, and get a usable digest of that information to answer questions, or whever, that use of the headless browser is not a spider, it's a user agent. Just an unusual one. The amount of traffic generated is consistent with browsing, not scraping. So no, I don't think building in a mandatory robots.txt respecter is a reasonable ask. Someone who wants to deploy it at scale while ignoring robots.txt is just going to disable that, and it causes problems for legitimate use cases where the headless browser is not a robot in any reasonable or normal interpretation of the term. [0]: I don't entirely understand why this is controversial, but it was. reply benatkin 20 hours agorootparentprev> Talking about DRM outside the context of media distribution doesn't really make any sense. It’s a cultural thing, and it makes a lot of sense. This fits with DRM culture that has walled gardens in iOS and Android. reply MichaelMoser123 14 hours agorootparentprevThat would make it impossible to use this as a testing tool. How should automatic testing of web applications work, if you obey all of these rules? There is also the problem of load testing. This kind of stuff is by its nature of dual use, a load test is also a kind of DDOS attack. reply bsnnkv 1 hour agorootparentprevLooking at the responses here, I'm glad I just chose to paywall to protect against LLM training data collection crawling abuse.[1] [1]: https://lgug2z.com/articles/in-the-age-of-ai-crawlers-i-have... reply benatkin 20 hours agorootparentprevMake it faster and furiouser. There are so many variables involved that it’s hard to predict what it will mean for the open web to have a faster alternative to headless Chrome. At least it isn’t controlled by Google directly or indirectly (Mozilla’s funding source) or Apple. reply buzzerbetrayed 1 hour agorootparentprevNerfing developer tools to save the \"open web\" is such a fucking backward argument. reply cchance 16 hours agorootparentprevIts literally open source, any effort put into hamstringing it would just be forked and removed lol reply xena 14 hours agorootparentAny barrier to abuse makes abuse harder. reply internet_points 5 hours agorootparentprevYes! Having long time ago done some minor web scraping, I did not put any work at all into following robots.txt, simply because it seemed like a hassle and I thought \"meh it's not that much traffic is it and boss wants this done yesterday\". But if the tool defaulted to following robots.txt I certainly wouldn't have minded, it would have caused me to get less noise and my tool to behave better. Also, throttling requests and following robots.txt actually makes it less likely that your scraper will be blocked, so even for those who don't care about the ethics, it's a good thing to have ethical defaults. reply andrethegiant 52 minutes agorootparentThis is why I’m making crawlspace.dev, a crawling PaaS that respects robots.txt, implements proper caching, etc by default. reply holoduke 12 hours agorootparentprevIn 10 lines of code I could create a proxy tool that removes all your suggested guidelines so the scraper still operates. In other words. Not really helping. reply JoelEinbinder 1 day agoparentprevWhen I've talked to people running this kind of ai scraping/agent workflow, the costs of the AI parts dwarf that of the web browser parts. This causes computational cost of the browser to become irrelevant. I'm curious what situation you got yourself in where optimizing the browser results in meaningful savings. I'd also like to be in that place! I think your ram usage benchmark is deceptive. I'd expect a minimal browser to have much lower peak memory usage than chrome on a minimal website. But it should even out or get worse as the websites get richer. The nature of web scraping is that the worst sites take up the vast majority of your cpu cycles. I don't think lowering the ram usage of the browser process will have much real world impact. reply fbouvier 1 day agorootparentThe cost of the browser part is still a problem. In our previous startup, we were scraping >20 millions of webpages per day, with thousands of instances of Chrome headless in parallel. Regarding the RAM usage, it's still ~10x better than Chrome :) It seems to be coming mostly from v8, I guess that we could do better with a lightweight JS engine alternative. reply radium3d 17 hours agorootparentAs a web developer and server manager AI trainers scraping websites with no throttle is the problem. lol reply cush 22 hours agorootparentprev> there are hundreds of Web APIs, and for now we just support some of them (DOM, XHR, Fetch) > it's still ~10x better than Chrome Do you expect it to stay that way once you've reached parity? reply fbouvier 3 hours agorootparentI don't expect it to change a lot. All the main components are there, it's mainly a question of coverage now. reply nwienert 21 hours agorootparentprevPlaywright can run webkit very easily and it's dramatically less resource-intensive than Chrome. reply fbouvier 19 hours agorootparentYes but WebKit is not a browser per se, it's a rendering engine. It's less resource-intensive than Chrome, but here we are talking orders of magnitude between Lightpanda and Chrome. If you are ~10x faster while using ~10x less RAM you are using ~100x less resources. reply whatevaa 8 hours agorootparentCareful, as you implement misssing features your RAM usage might grow too. Happened to many projects, lean at the beggining, get's just as slow when dealing with real world mesiness. reply bdhcuidbebe 17 hours agorootparentprevHow well does it compare to specialized headless scraper browsers, like camoufox (firefox based) or secret agent (chrome based)? Either should reduce your ram usage compared to stock chrome by a lot. reply msoad 6 hours agorootparentprevDoes it work nicely on Linux? I'm very curious about this reply Tostino 1 day agorootparentprevYou may reduce ram, but also performance. A good JIT costs ram. reply fbouvier 1 day agorootparentYes, that's true. It's a balance to find between RAM and speed. I was thinking more on use cases that require to disable JIT anyway (WASM, iOS integration, security). reply Tostino 1 day agorootparentYeah, could be nice to allow the user to select the type of ECMAScript engine that fits their use-case / performance requirements (balancing the resources available). reply cxr 14 hours agorootparentprevIf your target is consistent enough (perhaps even stationary), then at some point \"JIT\" means wasting CPU cycles. reply szundi 7 hours agorootparentprevThen came deepseek reply refulgentis 1 day agorootparentprevGenerally, for consumer use cases, it's best to A) do it locally, preserving some of the original web contract B) run JS to get actual content C) post-process to reduce inference cost D) get latency as low as possible Then, as the article points out, the Big Guns making the LLMs are a big use case for this because they get a 10x speedup and can begin contemplating running JS. It sounds like the people you've talked to are in a messy middle: no incentive to improve efficiency of loading pages, simply because there's something else in the system that has a fixed cost to it. I'm not sure why that would rule out improving anything else, it doesn't seem they should be stuck doing nothing other than flailing around for cheaper LLM inference. > I think your ram usage benchmark is deceptive. I'd expect a minimal browser to have much lower peak memory usage than chrome on a minimal website. I'm a bit lost, the ram usage benchmark says its ~10x less, and you feel its deceptive because you'd expect ram usage to be less? Steelmanning: 10% of Chrome's usage is still too high? reply JoelEinbinder 1 day agorootparentThe benchmark shows lower ram usage on a very simple demo website. I expect that if the benchmark ran on a random set of real websites, ram usage would not be meaningfully lower than Chrome. Happy to be impressed and wrong if it remains lower. reply fbouvier 1 day agorootparentI believe it will be still significantly lower as we skip the graphical rendering. But to validate that we need to increase our Web APIs coverage. reply danielsht 21 hours agoparentprevVery impressive! At Airtop.ai we looked into lightweight browsers like this one since we run a huge fleet of cloud browsers but found that anything other than a non-headless Chromium based browser would trigger bot detection pretty quickly. Even spoofing user agents triggers bot detection because fingerprinting tools like FingerprintJS will use things like JS features, canvas fingerprinting, WebGL fingerprinting, font enumeration, etc. Can you share if you've looked into how your browser fares against bot detection tools like these? reply fbouvier 21 hours agorootparentThanks! No we haven't worked on bot detection. reply sesm 1 day agoparentprevGreat job! And good luck on your journey! One question: which JS engines did you consider and why you chose V8 in the end? reply fbouvier 1 day agorootparentWe have also considered JavaScriptCore (used by Bun) and QuickJS. We did choose v8 because it's state of the art, quite well documented and easy to embed. The code is made to support others JS engine in the future. We do want to add a lightweight alternative like QuickJS or Kiesel https://kiesel.dev/ reply ksec 15 hours agorootparentThank You I was thinking of JSC and Bun as well. Was half expecting JSC since that combination seems to work well. reply returnofzyx 9 hours agoparentprevHi. Can I embed this as library? Is there C API exposed? I can't seem to find any documentation. I'd prefer this to a CDP server. reply fbouvier 9 hours agorootparentNot now but we might do it in the future. It's easy to export a Zig project as a C ABI library. reply returnofzyx 5 hours agorootparentOh please do. I'm sure there are many people like me who want this. reply keepamovin 16 hours agoparentprevIf you support Page.startScreencast or even just capture screenshot we could experiment with using this as a backend for BrowserBox, when lightpanda matures. Cool stuff! https://github.com/BrowserBox/BrowserBox/ reply afk1914 1 day agoparentprevI am curious how Lightpanda compares to chrome-headless-shell ({headless: 'shell'} in Puppeteer) in benchmarks. reply fbouvier 1 day agorootparentWe did not run benchmarks with chrome-headless-shell (aka the old headless mode) but I guess that performance wise it's on the same scale as the new headless mode. reply toobulkeh 1 day agoparentprevI’d love to see better optimized web socket support and “save” features that cache LLM queries to optimize fallback reply 867-5309 17 hours agoparentprevdoes this work with selenium/chromedriver? reply fbouvier 17 hours agorootparentFor now we just support CDP. But Selenium is definitely in our roadmap. reply dtj1123 1 day agoparentprevVery nice. Does this / will this support the puppeteer-extra stealth plugin? reply katiehallett 1 day agorootparentThanks! Right now no, but since we use the CDP (playwright, puppeteer), I guess it would be possible to support it reply xena 1 day agoparentprevHow do I make sure that people can't use lightpanda to bypass bot protection tools? reply psanchez 11 hours agoprevI think this is a really cool project. Scrapping aside, I would definitely use this with playwright for end2end tests if it had 100% compatibility with chrome and ran with a fraction of the time/memory. At my company we have a small project where we are running the equivalent of 6.5 hours of end2end tests daily using playwright. Running the tests in parallel takes around half an hour. Your project is still in very early stages, but assuming 10x speed, that would mean we could pass all our tests in roughtly 3 min (best case scenario). That being said, I would make use of your browser, but would likely not make use of your business offering (our tests require internal VPN, have some custom solution for reporting, would be a lot of work to change for little savings; we run all tests currently in spot/preemptible instances which are already 80% cheaper). Business-wise I found very little info on your website. \"4x the efficiency at half the cost\" is a good catch phrase, but compared to what? I mean, you can have servers in Hetzner or in AWS and one is already a fraction of the cost of the other. How convenient is to launch things on your remote platform vs launch them locally or setting it up? does it provide any advantages in the case of web scrapping compared to other solutions? how parallelizable is it? Do you have any paying customers already? Supercool tech project. Best of luck! reply fbouvier 9 hours agoparentThank you! Happy if you use it for your e2e tests in your servers, it's an open-source project! Of course it's quite easy to spin a local instance of a headless browser for occasional use. But having a production platform is another story (monitoring, maintenance, security and isolation, scalability), so there are business use cases for a managed version. reply frankgrecojr 22 hours agoprevThe hello world example does not work. In fact, no website I've tried works. It's usually always panics. For the example in the readme, the errors are: ``` ./lightpanda-aarch64-macos host 127.0.0.1 port 9222 info(websocket): starting blocking worker to listen on 127.0.0.1:9222 info(server): accepting new conn... info(server): client connected info(browser): GET https://wikipedia.com/ 200 info(browser): fetch https://wikipedia.com/portal/wikipedia.org/assets/js/index-2...: http.Status.ok info(browser): eval script portal/wikipedia.org/assets/js/index-24c3e2ca18.js: ReferenceError: location is not defined info(browser): fetch https://wikipedia.com/portal/wikipedia.org/assets/js/gt-ie9-...: http.Status.ok error(events): event handler error: error.JSExecCallback info(events): event handler error try catch: TypeError: Cannot read properties of undefined (reading 'length') info(server): close cmd, closing conn... info(server): accepting new conn... thread 5274880 panic: attempt to use null value zsh: abort ./lightpanda-aarch64-macos host 127.0.0.1 port 9222 ``` reply krichprollsch 9 hours agoparentLightpanda co-author here. Thanks for opening the issue in the repo. To be clear here, the crash seems relative with a socket disconnection issue in our CDP server. > info(events): event handler error try catch: TypeError: Cannot read properties of undefined (reading 'length') This message relates to the execution of gt-ie9-ce3fe8e88d.js. It's not the origin of the crash. I have to dig in, but it could be due to a missing web API. reply lbotos 17 hours agoparentprevNot OP do you have some kind of proxy or firewall? Looks like you couldn't download https://wikipedia.com/portal/wikipedia.org/assets/js/gt-ie9-... for some reason. In my contributions to joplin s3 backend \"Cannot read properties of undefined (reading 'length')\" was usually when you were trying to access an object that wasn't instantiated. (Can't figure out length of ) So for some reason it seems you can't execute JS? reply zelcon 15 hours agoparentprevThat's Zig for you. A ``modern'' systems programming language with no borrow checker or even RAII. reply hansvm 13 hours agorootparentThose statements are mostly true and also worth talking about, but they're not pertinent to that error (remotely provided JS not behaving correctly), or the eventual crash (which you'd cause exactly the same way for the same reason in Rust with a .unwrap() call). reply IshKebab 8 hours agorootparentNot exactly the same. `.unwrap()` will never lead to UB, but this can in Zig in release mode. Also `unwrap()`s are a lot more obvious than just a ?. Dangerous operations should require more ceremony than safe ones. Surprising to see Zig make such a mistake. reply jbggs 12 hours agorootparentprevyou shouldn't be unwrapping, error cases should be properly handled. users shouldn't see null dereference errors without any context, even in cli tools... reply igorguerrero 12 hours agorootparentprevYou could build the same thing in Rust and have the same exact issue. reply audunw 6 hours agorootparentprevIf that kind of stuff is always preferable, the nobody would use C over C++, yet to this day many projects still do. Borrow checking isn’t free. It’s a trade-off. I mean, you could say Rust isn’t a modern language because it doesn’t use garbage collection. But it’s a nonsensical statement. Different languages serve different purposes. Besides, Zig is focusing a lot more on heavily integrating testing, debug modes, fuzzing, etc. in the compiler itself, which when put together will catch almost all of the bugs a borrow checker catches, but also a whole ton of other classes of bugs that Rust doesn’t have compile time checks for. I would probably still pick Rust in cases where it’s absolutely critical to avoid bugs that compromise security. But this project isn’t that kind of project. I’d imagine that the super fast compile times and rapid iteration that Zig provides is much more useful here. reply steeve 7 hours agorootparentprevThat has absolutely nothing to do with RAII or safety… reply dang 20 hours agoprev(This was on the frontpage as https://news.ycombinator.com/item?id=42812859 but someone pointed out to me that it had been a Show HN a few weeks ago: https://news.ycombinator.com/item?id=42430629, so I've made a fresh copy of that submission and moved the comments hither. I hope that's ok with everyone!) reply weinzierl 1 day agoprevIf I don't need JavaScript or any interactivity, just modern HTML + modern CSS, is there any modern lightweight renderer to png or svg? Something in the spirit of wkhtmltoimage or WeasyPrint that does not require a full blown browser but more modern with support of recent HTML and CSS? In a sense this is Lightpanda's complement to a \"full panda\". Just the fully rendered DOM to pixels. reply nicoburns 23 hours agoparentWe're working on this here: https://github.com/DioxusLabs/blitz See the \"screenshot\" example for rendering to png. There's no SVG backend currently, but one could be added. (proper announcement of project coming soon) reply evanjrowley 2 hours agoprevI'm interested to see if this could be made to work as a drop-in replacement for the headless Chromium that Hoarder uses to archive web content. I don't have a problem with the current Hoarder solution, but it would be nice to use something that requires less RAM. reply cropcirclbureau 1 day agoprevPretty cool. Do you have a list of features you plan to support and plan to cut? Also, how much does this differ from the DOM impls that test frameworks use? I recall Jest or someone sporting such a feature. reply fbouvier 1 day agoparentThe most important \"feature\" is to increase our Web APIs coverage :) But of course we plan to add others features, including tight integration with LLM embed mode (as a C library and as a WASM module) so you can add a real browser to your project the same way you add libcurl reply andrethegiant 1 day agorootparentCould it potentially fit in a Cloudflare worker? Workers are also V8 and can run wasm, but are constrained to 128MB RAM and 10MB zipped bundle size reply fbouvier 1 day agorootparentWASM support is not there yet but it's on the roadmap and we had it in our mind since the beginning of the project, and have made our dev choices accordingly. So yes it could be used in a serverless platform like Cloudflare workers. Our startup time is a huge advantage here (20ms vs 600ms for Chrome headless in our local tests). Regarding v8 in Cloudflare workers I think we can not used directly, ie. we still need to embed a JS engine in the wasm module. reply zlagen 20 hours agoprevwhat do you think would be the use cases for this project? being lightweight is awesome but usually you need a real browser for most use cases. Testing sites and scraping for example. It may work for some scraping use cases but I think that if the site uses any kind of bot blocking this is not going to cut it. reply fbouvier 19 hours agoparentThere are a lot of uses cases: LLM training (RAG, fine tuning) AI agents scraping SERP testing any kind of web automation basically Bot protection of course might be a problem but it depends also on the volume of requests, IP, and other parameters. AI agents will do more and more actions on behalf of humans in the future and I believe the bot protection mechanism will evolve to include them as legit. reply zlagen 16 hours agorootparentthanks, it doesn't seem like it's the direction it's going at the moment. If you look at the robots.txt of many websites, they are actually banning AI bots from crawling the site. To me it seems more likely that each site will have its own AI agent to perform operations but controlled by the site. reply zelcon 15 hours agoprevWhy didn't you just fork Chromium and strip out the renderer? This is guaranteed to bitrot when the web standards change unless you keep up with it forever and have perpetual funding. Yes, modifying Chromium is hard, but this seems harder. reply fbouvier 9 hours agoparentIt was my first idea. Forking Chromium has obvious advantages (compatibility). But it's not architectured for that. The renderer is everywhere. I'm not saying it's impossible, just that it did look more difficult to me than starting over. And starting from scratch has other benefits. We own the codebase and thus it's easier for us to add new features like LLM integrations. Plus reducing binary size and startup time, mandatory for embedding it (as a WASM module or as C lib). reply cxr 13 hours agoparentprev> modifying Chromium is hard, but this seems harder Prove it. reply tetris11 14 hours agoparentprevWhy do anything: because it shows what's possible, and makes the next effort that much more easier. I call this process of frontier effort and discovery: \"science\" reply zelcon 14 hours agorootparentRedoing what others have already done is not what I think of when I hear \"frontier effort\" reply Kathc 1 day agoprevAn open-source browser built from scratch is bold. What inspired the development of Lightpanda? reply katiehallett 1 day agoparentThanks! The three of us worked together at our former company ecomm saas start up where we spent a ton of $ on scraping infrastructure spinning up headless Chrome instances. It started out as more of an R&D thesis is it possible to strip out graphical rendering from Chrome headless? Turns out no so we tried to build it from scratch. And the beta results validated the thesis. I wrote a whole thing about it here if you're interested in delving deeper https://substack.thewebscraping.club/p/rethinking-the-web-br... reply corford 20 hours agorootparentNot sure what category of ecomm sites you were scraping but I scrape >10million ecomm URLs daily and, honestly, in my experience the compute is not a major issue (8 times out of 10 you can either use API endpoints and/or session stuffing to avoid needing a browser for every request; and in the 2 out of 10 sites where you really need a browser for all requests it's usually to circumvent aggressive anti-bot which means you're very likely going to need full chrome or FF anyway and you can parallelise quite effectively across tabs). One niche where I could definitely see a use for this though is scraping terribly coded sites that need some JS execution to safely get the data you want (e.g. they do some bonkers client side calculations that you don't want to reverse engineer). It would be nice to not pay the perf tax of chrome in these cases. Having said all of that, I have to say from a geek perspective it's super neat what you guys are hacking on! Zig+V8+CDP bindings is very cool. reply hansvm 13 hours agorootparent> not pay the perf tax I've typically used pyminiracer in such cases and provided some dummy window objects and whatnot as necessary for the script to succeed. reply zlagen 16 hours agorootparentprevfully agree here, using a browser for everything is the dumb way. You just usually use it to circumvent the blocking and then reuse the cookies to call the endpoints directly. reply fbouvier 3 hours agorootparentIt might works if you need to handle a few websites. But this retro engineering approach is not maintainable if you want to handle hundreds or thousands of websites. reply dolmen 1 day agoparentprevScrapping modern web pages is hard without full support for JS frameworks and dynamic loading. But a full browser, even headless, has huge ressource consumption. This has a huge cost when scraping at scale. reply gwittel 22 hours agoprevInteresting. Looks really neat! How do you deal with anti bot stuff like Fingerprintjs, Cloudflare turnstile, etc? Maybe you’re new enough to not get flagged but I find this (and CDP) a challenge at times with these anti-bot systems. reply the__alchemist 16 hours agoprevI have a meta question from browsing the repo: Why do C, C++, and Zig code bases, by convention, include a license at the top of every module\" IMO it makes more sense to insetead include of an overview of the module's purpose, and how it fits in with the rest of the program, and one license at the top-level, as the project already has. reply AndyKelley 16 hours agoparent100% of my projects, including the Zig compiler itself, have only the license file at the root of the project tree, except of course for files that were copy pasted from other projects. reply surfmike 22 hours agoprevAnother browser in this space is https://ultralig.ht/, it's geared for in-game UI but I wonder how easy it would be to retool it for a similar use case. reply randomMatrix101 10 hours agoprevVery cool project, congrats guys! reply kavalg 1 day agoprevWhy AGPL? I am not blaming you. I am just curious about the reasoning behind your choice. reply fbouvier 1 day agoparentWe had some discussions about it. It seems to us that AGPL will ensure that a company running our browser in a cloud managed offer will have to keep its modifications open for the community. We might be wrong, maybe AGPL will damage the project more than eg. Apache2. In that case we will reconsider our choice. It's always easier this way :) Our underlying library https://github.com/lightpanda-io/zig-js-runtime is licensed with Apache2. reply cratermoon 1 day agoprevSo is this the scraper we need to block? https://news.ycombinator.com/item?id=42750420 reply fbouvier 1 day agoparentI fully understand your concern and agree that scrapers shouldn't be hurting web servers. I don't think they are using our browser :) But in my opinion, blocking a browser as such is not the right solution. In this case, it's the user who should be blocked, not the browser. reply jjcoffman 1 day agorootparentIf your browser doesn't play nicely and obey robots.txt when its headless I don't think it's that crazy to block the browser and not the user. reply hansvm 13 hours agorootparentThe first thing that came to mind when I saw this project wasn't scraping (where I'd typically either want a less detectible browser or a more performant option), but as a browser engine that's actually sane to link against if I wanted to, e.g., write a modern TUI browser. Banning the root library (even if you could with UA spoofing and whatnot) is right up there with banning Chrome to keep out low-wage scraping centers and their armies of employees. It's not even a little effective also risks significant collateral damage. reply fbouvier 23 hours agorootparentprevEvery tool can be used in a good or bad way, Chrome, Firefox, cURL, etc. It's not the browser who doesn't play nicely, it's the user. It's the user's responsibility to behave well, like in life :) reply slt2021 23 hours agorootparentprevit is trivial to spoof user-agent, if you want to stop a motivated scraper, you need a different solution that exploits the fact that robots use headless browser reply sangnoir 22 hours agorootparent> it is trivial to spoof user-agent It's also trivial to detect spoofed user agents via fingerprinting. The best defense against scrapers is done in layers, with user-agent name block as the bare minimum. reply optixyt 8 hours agoprevThe second social media botters find this. reply m3kw9 1 day agoprevHow does this work because the browser needs to render a page and the vision model needs to know where a button is, so it still needs to see an image. How does headless make it easier? reply katiehallett 1 day agoparentHeadless mode skips the visual rendering meant for humans, but the DOM structure and layout still exist, allowing the model to parse elements programmatically (e.g. button locations). Instead of 'seeing' an image, the model interacts with the page's underlying structure, which is faster and more efficient. Our browser removes the rendering engine as well, so it won't handle 100% of automation use cases, but it's also what allows us to be faster and lighter than Chrome in headless mode. reply 10000truths 22 hours agorootparentThe issue is that DOM structure does not correspond one-to-one with perceived structure. I could render things in the DOM that aren't visible to people (e.g. a transparent 5px x 5px button), or render things to people that aren't visible in the DOM (e.g. Facebook's DOM obfuscation shenanigans to evade ad-blocking, or rendering custom text to a WebGL canvas). Sure, most websites don't go that far, but most websites also aren't valuable targets for automated crawling/scraping. These kinds of disparities will be exploited to detect and block automated agents if browser automation becomes sufficiently popular, and then we're back to needing to render the whole browser and operate on the rendered image to keep ahead of the arms race. reply emporas 9 hours agorootparentServers operate on top of tcp/ip not to serve information, rather to serve information plus something else, usually ads. This is usually implemented with websites and captchas n stuff. That's a problem of misaligned economic incentives. If there is a blockchain which enables micro-transactions of 0.000001 cent per request, and in the order of a million tps or a billion tps, then servers have no reason not to accept money in exchange for information, instead of using ads to extract some eyeball attention. There is no reason that i cannot invoke a command line program: `$fetch_social_media_posts n 1000` and get the last thousand posts right there in the console, as long as i provide some valid transactions to the server. Websites and ads are the wrong solution to the problem of gaining something while serving information, and headless browsers and scraping are the wrong solution to the first wrong solution and the problems it creates. reply aniviacat 3 hours agorootparentNo need for blockchain, microtransaction functionality should be integrated into our existing payment methods. reply wiradikusuma 1 day agorootparentprevBut what if the human programmer needs to visually verify that their code works by eyeballing which element got selected, etc? reply fbouvier 1 day agorootparentYou're right, the debugging part is a good use case for graphical rendering in a headless environment. I see it as a build time/runtime question. At build (dev) time I want to have a graphical response (debugging, computer vision, etc.). And then, when the script is ready, I can use Lightpanda at runtime as a lightweight alternative. reply codetrotter 18 hours agorootparentI was doing a personal side project for a while where I was trying to make my own little Wayback Machine-alike. Mine was very rudimentary, built on top of Firefox and WebDriver plus Squid proxy. For debugging purposes you could have your headless browser function as a HTTP Proxy Server, maybe? And in your headless browser you could capture a static snapshot of the DOM after your JavaScript runtime has executed the scripts for the page. Similar to how the archive.today guy serves static snapshots of websites. And then developers using your headless browser could point their Firefox or Chrome browser to the HTTP Proxy server hosted by your headless browser program, in order to get a static snapshot view of what the DOM is like after your headless browser has executed JavaScript from the page. And then Firefox or Chrome will render static HTML view of what the page looked like to your headless browser, that the developer can inspect to make decisions about further interactions with the page. As a tool for debugging. reply chrisweekly 1 day agorootparentprevIf you want a human to eyeball it, you don't use a \"headless\" browser. reply dolmen 1 day agorootparentprevThe human programmer can save the DOM as HTML in a file and open it in a headfull browser. But the use case for Lightpanda is for machine agents, not humans. reply stuckkeys 14 hours agoprevHow does it do against captchas? reply monkmartinez 1 day agoprev [–] This is pretty neat, but I have to ask; Why does everyone want to build and/or use a headless browser? When I use pyautogui and my desktop chrome app I never have problems with captchas or trigger bot detectors. When I use a \"headless\" playwright, selenium, or puppeteer, I almost always run into problems. My conclusion is that \"headless\" scrapping creates more problems than it solves. Why don't we use the chrome, firefox, safari, or edge that we are using on a day to day basis? reply fbouvier 1 day agoparent [–] I guess it depends on the scale of your requests. When you want to browse a few websites from time to time, a local headful browser might be a solution. But when you have thousands or millions of webpages, you need a server environment and a headless browser. reply fbouvier 1 day agorootparent [–] In the past I've run hundreds of headful instances of Chrome in a server environment using Xvfb. It was not a pleasant experience :) reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lightpanda is a new open-source headless browser developed in Zig, designed to be faster and more efficient than existing options like Chrome headless, particularly for web scraping and automation tasks. It features an HTTP loader, HTML parser, DOM tree, JavaScript runtime, and a CDP server, making it compatible with tools like Puppeteer and Playwright, while being approximately 10 times faster and using 10 times less memory than Chrome headless. Currently in beta, Lightpanda supports limited web APIs, with plans for expansion, and offers a Managed Cloud service as part of its business model."
    ],
    "commentSummary": [
      "Lightpanda is an open-source headless browser developed in Zig, providing a lightweight alternative to Chrome and Firefox for non-graphical tasks such as web scraping and AI automation. It includes essential components like an HTTP loader, HTML parser, JavaScript runtime, and partial web API support, focusing on speed and memory efficiency. Currently in beta, Lightpanda plans to enhance its web API coverage and operates on a business model offering a Managed Cloud service, inviting community feedback for improvements."
    ],
    "points": 275,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1737756932
  },
  {
    "id": 42816284,
    "title": "PhysicsForums and the Dead Internet Theory",
    "originLink": "https://hallofdreams.org/posts/physicsforums/",
    "originBody": "Hall of Impossible Dreams A repository of code, fiction, nonfiction, and poetry. HOME CATEGORIES TAGS ARCHIVES ABOUT Posts PhysicsForums and the Dead Internet Theory Post Cancel PhysicsForums and the Dead Internet Theory Posted 1 day ago2025-01-24T08:00:00-05:00 by David and Felipe What Internet Will Look Like in the Future cripes does anybody remember Google People – qntm, August 21st, 2019 Does anybody remember PhysicsForums? It was never exactly the center of the Internet, but back when it was founded in 2001, the Internet didn’t really have a center the way it does today. PhysicsForums was one forum among thousands, founded by an enthusiastic teenager named Greg Bernhardt, existing in the ‘hard science’ niche alongside the likes of Bad Astronomy, mostly focused on giving hints for physics homework to struggling students without outright doing the physics homework. It had fairly steady growth until 2012, before petering out throughout the 2010s and 2020s in lieu of more centralized sites like StackExchange, and by 2025, only a small community was left. But, unlike so many other fora from back in the early days, it went from 2003 to 2025 without ever changing its URLs, erasing its old posts, or going down altogether. Thanks to this consistency, PhysicsForums remains quite valuable as a time capsule, and can give us a glimpse at how people thought and what they said two decades ago. For instance, we we might go to this 2007 post asking about the capabilities of the now-obsolete HP 50g Graphing Calculator, and read a very helpful reply by commenter ravenprp: Something has gone terribly wrong. Quoth the Raven? I reset my pw and logged back in to Google People for the first time in 10 (?) years just now and discovered the following: 10 years of updates on my account, written by me – qntm, September 7th, 2019 At first glance, ravenprp is a very impressive user, writing 2,891 posts in a mere seven-month span (from September 2006 to April 2007) for average of more than thirteen posts per day. And most of these weren’t casual one-line answers: reading his profile, one comes away realizing that he’s a true omnidisciplinary scientist, if not a modern-day Renaissance Man. He’s a member of a university faculty, a mechanical engineering student, and a hiring manager at a chemical plant (presumably in Canada); he’s an expert in working with .VOB and audio files and fasteners; he’s a structural engineer, biologist, medical physicist, chemist, and aerospace engineer. And to top it all off, he’s modest, and won’t claim expertise he doesn’t have: he’s an aerospace and aeronautical engineer, but not an expert in aerodynamics, nor a mechanical engineer, nor an expert in electrical models. Impressively, these posts span from three years before the account was created to a year after the account was last logged into. And, as the icing on the cake, ravenprp is prescient enough that he can joke about being a language model developed by OpenAI, seven years before OpenAI was even founded; evidently he should have joined PsychicsForums instead. A reasonable question, reading these posts, is: what’s the deal with the joined and login dates? If this is an entirely fabricated account, then why not just set the account creation date to be earlier than the first post, and the account access date to be later than the last post? The Wayback Machine gives us an answer: while the account currently lists 2,891 posts, an archive from 2019 lists 74. And looking at only these 74 gives a very different profile: ravenprp was an electrical engineering student asking for simple advice on MATLAB books, working through integrated circuit problems, giving other commenters useful links on the Bernoulli Equation, and asking for help from knowledgeable experts, such as computer professional ravenprp. So it seems that this was a real account, once, and that ravenprp was a real person with real posts, opinions, questions, and answers. But any original thoughts and writings he might have had in 74 original posts have been all but drowned out by the 2,817 extra posts which have been added, backdated, and attributed to him. Still, while finding those posts might be a challenge, at least those threads themselves are intact. For instance, he was helped in one thread by kyle8921: And the answer would seem pretty reasonable at first glance, were it not for two small problems: Kyle8921, oddly enough, decided to finish an incomplete LaTeX expression from ravenprp’s post before beginning his own answer. Much like ravenprp, Kyle8921 posted the answer more than a year after he last logged in, and continued posting for more than three years after that. Exactly how deep does this go? The Internet is Forever In regards to the dead internet hypothesis, the content that you’re enjoying today, will still be there tomorrow. LoveMortuus, July 24th, 2024 Founder Greg Bernhardt started PhysicsForums as a simple GHB bulletin board, and he gave each thread and post a unique numerical ID; unlike many modern sites, this ID was sequential, with the first post given the ID of 1, the second post 2, and so forth. Some of these posts, of course, have since been deleted for one reason or another (including the first two years’ worth of discussion, which is why post #1 is in 2003 and not 2001), so we would not expect the line of posts to be continuous and unbroken…but we would expect the line to be monotonically increasing, so long as no posts had been retroactively added to the database. Plotting a sample of 30,000 posts grabbed at random, we can see how that pattern holds up: For almost the entire history of PhysicsForums, that rule held true: if post A had a larger ID than post B, then post A had a later timestamp than post B. And anywhere that rule does not hold true, we can reasonably infer that the database has been altered and that a more recent post has been given an older date retroactively. Database alteration and retroactive dating are not necessarily bad things. The biggest example came in 2022, when PhysicsForums merged with MathHelpBoards and incorporated its ~150,000 post archive, with each post given its original date. There are smaller examples within PhysicsForums history of a thread getting split (due to some off-topic but nevertheless useful) discussion, or a post getting deleted but then restored with a newer ID, or a database hiccough causing re-assignment, or various other entirely justifiable small-scale edits. But on March 11th of 2023, a handful of posts were added all up and down the timeline, followed by a bigger group in May, an even bigger one in October, and the largest of all in January and February of 2024. By taking the highly anomalous post IDs and filtering out the MathHelpBoards import, and filtering out instances which seem plausibly close to their actual post times, we get an estimated 115,000 posts written by LLMs and attributed to humans. Without scraping the entire website, it’s difficult to say precisely how many users have had their profiles revived, but taking a representative sample, there are at bare minimum 110 users affected. These range from some of the earliest commenters, to one-time posters to long-time readers to founder and site administrator Greg Bernhardt. In every case, a name is being attached to viewpoints that person does not necessarily endorse; certainly an average science enthusiast would not probably not endorse the notion that 0.999… does not equal 1, if he knew about it: But that’s the problem: they don’t know about it. We don’t know who most of these people are. We can’t get in touch with them. Many, if not most, have usernames either unique to PhysicsForums, or else so common elsewhere as to be useless for identification. They have moved on from PhysicsForums, living their lives, with no way of knowing what is or isn’t being said under their names, and no reason whatsoever to suspect anything is amiss. And maybe what’s said is accurate, or maybe it isn’t, but either way, with every additional post, the archives of the Internet are made just a little bit flatter, their own scientific contributions are diluted just a little bit more, and the portion of the Internet intentionally written by humans is shrunk just a little bit further. The Dead Internet Theory The internet feels empty and devoid of people…it’s like a hot air balloon with nothing inside. Anonymage, “Dead Internet Theory (and much more)”, September 16th, 2019 The ‘Dead Internet Theory’ the theory that much or most of the Internet consists of things not made by human beings got its name in 2019, and has been slowly gaining popularity ever since. The theory as originally stated is a self-described ‘jumbled mess’, cites the ‘death’ of the Internet at about 2016 or 2017, and interestingly, predates almost every known LLM capable of writing convincing longform text; we tried a GPT-2 bot on /r/AskReddit two days after the original greentext, and the bot was noticed and called out within a handful of hours. And in 2016 or 2017, the state-of-the-art for text generation was Markov chains, and most bots online posted identical messages with no text generation at all. The theory, and the general feeling that the Internet has changed shape, clearly had other root inspirations besides LLMs. Part of the feeling was probably due to the Internet opening up to a wider and wider global audience, with small community norms and standards simply not scaling up. Part of it was probably the device change from computer to smartphone encouraging a more passive role on the part of the audience. Part of it was probably algorithmic incentives towards more and more content engagement, as inevitable as Las Vegas’ extravagant casinos, emerging from pure market forces and not attributable to any one person in particular. But part of it now, six years later, is certainly LLMs. The thing is, it’s not like these non-human additions are adding value or usability in the original ravenprp thread we started with. You could argue that adding a summary is helpful… but that’s only the case if the summary is correct, and as we know, the AI generated summaries have a real tendency to enrich the facts with things that may or may not be accurate. We can accept the addition of some links, etc, as a potential enrichment; though their intrusive nature is disruptive like any other advertisement, sometimes we accept disruption as part of what is needed to keep a website operating. The rest of the thread, though, is a LLM-generated post that contains exactly one true fact (the existence of the “HP 50g Connectivity Kit”), and otherwise contains nothing that is actually reflected in the HP 50g manual, or anywhere else one might find genuine information about the calculator. Finally, we have the FAQ. This FAQ is the only place in the entire thread which references the “GET” and “PUT” functions of HP 50g Spreadsheet, and is the only place the summary could be getting them from. For that matter, the FAQ the only place in the entire internet which references those functions; the manual doesn’t talk about them, or talk about the “HP 50g Spreadsheet” at all…and searching for “HP 50g Spreadsheet”, as of the time of writing, returns this thread and nothing else. So while ravenprp’s post has a hint of correctness, the FAQ pulls only from the title and is entirely wrong, and the summary pulls primarily from the FAQ and is also entirely wrong. But the fact that these answers are incorrect is not actually the only problem. Instead of reading the replies, we can do a quick tally of the amount of space each section of this thread takes up: Section Word Count Human? Summary 99 No Springo 38 Yes Phys.org 34 No Shinny_head 11 Yes Ravenprp 164 No FAQ 260 No The LLM-written portions of this thread are 92% of the total word count. If this were a representative sample of the Internet, it would be reasonable to describe the Internet as ‘dead’. The situation across the entire PhysicsForums is not, fortunately, quite so dire yet. While the backdated LLM posts tend to be longer than average, they still make up only 1.6% of the total post count and 3% of the word count across all posts. The actual deception is still, even now, still a fairly small portion of the total. But, when counting in the LLM-generated FAQs and summaries which, as we’ve seen, aren’t always accurate the Dead Internet Theory seems to be vindicated, and only 66% of the words on the site were written by human beings. Two years ago, that number was close to 100%. Two years from now, then…who knows? It is true that the old archives aren’t read all that frequently, so it could be argued that this is no great loss. Only three archived PhysicsForums post were shared on Twitter throughout all of 2024…and of those three, one was a human sharing an bot-written post, and another was a bot sharing a human-written post. So really, does anyone care? Man vs Machine “…using AI, one can generate tons of nonsense with very little work and overload us easily.” So far that hasn’t happened. PeterDonis, November 12th, 2024 Certainly the community members of PhysicsForums care. In November of 2024, poster Renormalize noticed peculiar behavior by Azntoon, and noticed specifically that Azntoon was posting in threads in 2022 despite having last logged on in 2012. The thread, however, soon came to the conclusion that it was some sort of database hiccough, and nothing more came of it. Interestingly, Azntoon’s last login date has since changed to December of 2022, making him one of the only two LLM-affected users to have (apparently) logged on again since the backdating began. Users were on the lookout for it as early as 2022: the forum regulars debated what the policy regarding ChatGPT-generated responses should be, with Vanadium 50 complaining that the “barely-lucid posts” generated by ChatGPT create a lot of work and could be considered a DoS attack. The general consensus in the thread was summarized by PhysicsForums administrator Greg Bernhardt: I’m pretty sure stackoverflow is attempting to ban it. I think we should discourage it, but I am unsure how to “ban” it here. Well worth a discussion. At a minimum content from ChatGPT should be quoted. A few months later, the site celebrated April Fools’ Day of 2023 by temporarily changing all display names to ChatGPT, a few weeks after the LLM backdated insertion had already started, but since this was on April 1st, nobody assumed that posts were actually being written by ChatGPT. Alexander R. Klotz, one-time contributor to PhysicsForums’ Insights blog, noted on Twitter that his 2016 article had been edited without his knowledge, but the edits seem to be limited to an LLM-generated summary awkwardly placed in the middle, and a manually-inserted author’s note from 2018. The actual scope of the change across and the addition of more than a hundred thousand posts is something we’ve seen no discussion of, anywhere, as of yet. But the backdated users have all been hidden from the search’s autocomplete, and profile links have all been removed from their posts, presumably to make it more difficult to click the profile and notice that something is wrong. Changes like this tend to be gradual: a slight drift over time, compromises made to keep the site afloat. When the community notices, it can all be explained, eased, and made bearable, even as everything changes inch by inch. If the community backlash is too great, it’s easy to backpedal and explain and have a lesser version of the change remain. Ultimately, the incentives towards making these changes will not go away. What decides how they work out is the judgement of the people making the changes and how the community responds to them. Update: FAQs During the writing of this post, the FAQs were actually noticed as well, and a discussion thread was created. It appears that they were not visible to logged-in users, explaining why the forum regulars did not notice: the average forum regular interested enough to go back and reread old posts will probably be logged in while doing so. The FAQs have now been removed, though the LLM-written summaries at the top of closed threads are still present. The Internet is People The dead internet theory is coming to fruition. Greg Bernhardt, January 3rd, 2025 We reached out to Greg Bernhardt asking for comment on LLM usage in PhysicsForums, and he replied: We have many AI tests in the works to add value to the community. I sent out a 2024 feedback form to members a few weeks ago and many members don’t want AI features. We’ll either work with members to dramatically improve them or end up removing them. We experimented with AI answers from test accounts last year but they were not meeting quality standards. If you find any test accounts we missed, please let me know. My documentation was not the best. And in response to a follow-up question about the community feedback, he replied: Mathwonk originally raised the issue of quality. I worked with him to improve them using a newer model, but it’s still not good enough. The backdated answers were an internal test. We conceived of a bot that would provide a quality answer to a thread without a reply after 1+ years. That too also failed. Instead, I’m considering pruning all threads without a reply as they clutter up the forums. It’s hard to imagine that 110 existing users gave consent to be used as test accounts, for 115,000 posts, over four waves spanning almost a year. The idea that these are test accounts gone wrong, or a bot accidentally mislabeled, doesn’t seem to align with the facts. It also ties directly to an unstated but very real expectation: my identity in an online community is mine. I have accounts on forums that are older than some friendships. I have written tens of thousands of words under various identities. Just because they are relatively anonymous doesn’t make them less real to me, it doesn’t diminish the effort and time I put into the work done under those identities. Hijacking accounts, filling it with content their original owners did not write? That dilutes their efforts, and muddles their identities. The content was produced with the idea that it was mine in some capacity, inextricably tied to an identity I owned. There’s also a social contract: when we create an account in an online community, we do it with the expectation that people we are going to interact with are primarily people. Oh, there will be shills, and bots, and advertisers, but the agreement between the users and the community provider is that they are going to try to defend us from that, and that in exchange we will provide our engagement and content. This is why the recent experiments from Meta with AI generated users are both ridiculous and sickening. When you might be interacting with something masquerading as a human, providing at best, tepid garbage, the value of human interaction via the internet is lost. Beyond that, the idea of populating existing accounts with LLM-generated content is destructive. Like paving over an arboretum to make room for a generic strip mall. Internet archaeology is already a difficult and fraught business. It’s so difficult to find lost content, servers that have gone down, websites that are just gone… and now, apparently a lot of backdated data that is AI generated. This is not to say websites shouldn’t evolve and stay current, but this is different, this is a re-writing of history, and rewrites history for no clear gain. It probably feels odd to see us write thousands of words fighting for the integrity of a community neither of us is part of, a tiny speck on the Internet trying desperately to survive, an enclave of a different era that is trying to hold on at all costs. But we are sympathetic. Running a website, especially a forum, is expensive. Server costs go up. Databases stop working and now you need to pay an expert or spend hours of unpaid time working on it. Bots flood in. DDOS attacks happen. Another wave of crypto-scams shows up. Staying alive on the internet costs money, and money comes through users and ads. You need those clicks like a man in the desert needs water, and every week it gets more competitive. One must transform to survive. That axiom is a truth on the internet. If you don’t, you rapidly find yourself buried on the eighth page of Google results, with no users and no money to keep the servers up. But when communities compromise their morals and the core of their identity to stay afloat, and destroy the very bedrock of their commitment to their users and to some degree to the broader idea of the Internet, we have to wonder… was it worth it? Essays essay david felipe This post is licensed under CC BY 4.0 by the author. Share Contents What Internet Will Look Like in the Future Quoth the Raven? The Internet is Forever The Dead Internet Theory Man vs Machine Update: FAQs The Internet is People Comments powered by giscus. © 2025 Dave and Felipe. Some rights reserved. Powered by Jekyll with Chirpy theme. Trending Tags david programming felipe fiction rust writing prompts poetry essay mathematica hatetris",
    "commentLink": "https://news.ycombinator.com/item?id=42816284",
    "commentBody": "PhysicsForums and the Dead Internet Theory (hallofdreams.org)246 points by TheCog 23 hours agohidepastfavorite147 comments COAGULOPATH 22 hours agoSomething I'm increasingly noticing about LLM-generated content is that...nobody wants it. (I mean \"nobody\" in the sense of \"nobody likes Nickelback\". ie, not literally nobody.) If I want to talk to an AI, I can talk to an AI. If I'm reading a blog or a discussion forum, it's because I want to see writing by humans. I don't want to read a wall of copy+pasted LLM slop posted under a human's name. I now spend dismaying amounts of time and energy avoiding LLM content on the web. When I read an article, I study the writing style, and if I detect ChatGPTese (\"As we dive into the ever-evolving realm of...\") I hit the back button. When I search for images, I use a wall of negative filters (-AI, Midjourney, StableDiffusion etc) to remove slop (which would otherwise be >50% of my results for some searches). Sometimes I filter searches to before 2022. If Google added a global \"remove generative content\" filter that worked, I would click it and then never unclick it. I don't think I'm alone. There has been research suggesting that users immediately dislike content they perceive as AI-created, regardless of its quality. This creates an incentive for publishers to \"humanwash\" AI-written content—to construct a fiction where a human is writing the LLM slop you're reading. Falsifying timestamps and hijacking old accounts to do this is definitely something I haven't seen before. reply robswc 22 hours agoparent100%. So far (thankfully) I've noticed this stuff get voted down on social media but it is blowing my mind people think pasting in a ChatGPT response is productive. I've seen people on reddit say stuff like \"I don't know but here's what ChatGPT said.\" Or worse, presenting ChatGPT copy-paste as their own. Its funny because you can tell, the text reads like an HR person wrote it. reply Trasmatta 21 hours agorootparentI've noticed the opposite actually, clearly ChatGPT written posts on Reddit that get a ton of upvotes. I'm especially noticing it on niche subreddits. The ones that make me furious are on some of the mental health subreddits. People are asking for genuine support from other people, but are getting AI slop instead. If someone needs support from an AI (which I've found can actually help), they can go use it themselves. reply ffsm8 10 hours agorootparent> clearly ChatGPT written posts on Reddit You should have a lot less confidence in your ability to discern what's AI generated content, honestly. Especially in such contexts where the humans will likely be writing very non-offensive in order to not-trigger the OP. reply nullc 3 hours agorootparentprevIt's particularly funny/annoying when they're convinced that the fact they got it from the \"AI\" makes it more likely to be correct than other commenters who actually know what the heck they're talking about. It makes me wonder how shallow a person's knowledge of all areas must be that they could use an LLM for more than a little while without encountering something where it is flagrantly wrong yet continued with its same tone of absolute confidence and authority. ... but it's mostly just a particularly aggressive form of Gell-Mann amnesia. reply ijk 21 hours agoparentprevThe problem with \"provide LLM output as a service,\" which is more or less the best case scenario for the ChatGPT listicles that clutter my feed, is that if I wanted an LLM result...I could have just asked the LLM. There's maybe a tiny proposition if I didn't have access to a good model, but a static page that takes ten paragraphs to badly answer one question isn't really the form factor anyone prefers; the actual chatbot interface can present the information in the way that works best for me, versus the least common denominator listicle slop that tries to appeal to the widest possible audience. The other half of the problem is that rephrasing information doesn't actually introduce new information. If I'm looking for the kind of oil to use in my car or the recipe for blueberry muffins, I'm looking for something backed by actual data, to verify that the manufacturer said to use a particular grade of oil or for a recipe that someone has actually baked to verify that the results are as promised. I'm looking for more information than I can get from just reading the sources myself. Regurgitating text from other data sources mostly doesn't add anything to my life. reply tayo42 21 hours agorootparentRephrasing can be beneficial. It can make things clearer to understand and learn from. Like in math something like khan academy or the 3blue 1 brown YouTube channel isn't presenting anything new, just rephrasing math in a different way that makes it easier for some to understand. If llms could take the giant overwhelming manual in my car and get out the answer to what oil to use, that woukd be useful and not new information reply chowells 20 hours agorootparentI have to protest. A lot of 3b1b is new. Not the math itself, but the animated graphical presentation is. That's where the value from his channel comes in. He provides a lot of tools to visualize problems in ways that haven't been done before. reply tayo42 20 hours agorootparentI guess the way I think of the visualizations and video as a whole as a type of rephrasing. He's not the first person to try to visualize math concepts reply krapp 21 hours agorootparentprev>If llms could take the giant overwhelming manual in my car and get out the answer to what oil to use, that woukd be useful and not new information You can literally just google that or use the appendix that's probably at the back of the manual. It's also probably stamped on the engine oil cap. It also probably doesn't matter and you can just use 10w40. reply ziddoap 20 hours agorootparentIllustrative examples are illustrative, not literal. reply tayo42 20 hours agorootparentprevI'm just reusing the example in the comment I responded to. Fill in something else then... reply MrPowerGamerBR 21 hours agoparentprev> If I'm reading a blog or a discussion forum, it's because I want to see writing by humans. I don't want to read a wall of copy+pasted LLM slop posted under a human's name. This reminds me of the time around ChatGPT 3's release where Hacker News's comments was filled with users saying \"Here's what ChatGPT has to say about this\" reply nxobject 19 hours agorootparentPepperidge Farm remembers a time where ChatGPT 2 made no claims about being a useful information lookup tool, but was a toy used to write sonnets, poems, and speeches \"in the style of X\"... reply Gracana 22 hours agoparentprevYup, I'm the same, and I love my LLMs. They're fun and interesting to talk to and use, but it's obvious to everyone that they're not very reliable. If I think an article is LLM-generated, then the signal I'm getting is that the author is just as clueless as I am, and there's no way I can trust that any of the information is correct. reply Sharlin 21 hours agorootparent> but it's obvious to everyone that they're not very reliable. Hopefully to everyone on HN, but definitely not to everyone on the greater Internet. There are plenty of horror stories of people who apparently 100% blindly trust whatever ChatGPT says. reply strix_varius 3 hours agorootparentI was especially horrified/amused when students started turning in generated answers and essays, and /r/teaching learned that you could \"ask chatgpt if it wrote the essay and it will tell you.\" It makes perfect intuitive sense if you don't know how the things actually work. reply Gracana 20 hours agorootparentprevYeah that's fair, I suppose I see that sort of thing on reddit fairly regularly, especially in the \"here's a story about my messed-up life\" types of subreddits. reply kjkjadksj 56 minutes agorootparentThere was a post from one of those am I the asshole subreddits, about how OP had some issue with an overweight person trying to claim their seat on a plane. Thousands of upvotes and comments ensued supporting OP and blaming the overweight person. Then 10 hours later OP edited the post and dropped the bomb. The screenshot of their prompt “make a story for the am I the asshole subreddit that makes a fat person look bad.” Followed by the post they pasted directly from chatgpt. Only one comment was about the edit and it completely missed the point and instead blamed OP for tricking them. Not the fact that probably every post on that subreddit and others like it is AI slop. reply scoofy 18 hours agoparentprevThis has been a constant back and forth for me. My personal project https://golfcourse.wiki was built on the idea that I wanted to make a wiki for golf nerds because nobody pays attention to 95% of fun golf courses because those courses don't have a marketing department in touch with social media. I basically decided that using AI content would waste everyone's time. However, it's a real chicken-or-egg problem in content creation. Faking it to the point of project viability has been a real issue in the past (I remember the reddit founders talking about posting fake comments and posts from fake users to make it look like more people were using the product). AI is very tempting for something like this, especially when a lot of people just don't care. So far I've stuck to my guns, and think that the key to a course wiki is absolutely having locals insight into these courses, because the nuance is massive. At the same time, I'm trying to find ways that I can reduced the friction for contributions, and AI may end up being one way to do that. reply kjs3 17 hours agorootparentThis is a really interesting conundrum. And I'm a golfer, so... Of the top of my head I wonder if there's a way to have AI generate a summary from existing (on-line) information about a course with a very explicit \"this is what AI says about this course\" or some similar disclosure until you get 'real' local insight. No one could then say 'it's just AI slop', but you're still providing value as there's something about each course. As much as I personally have reservations about AI, I (personally, YMMV) am much more forgiving if you are explicit about what's AI and what's not and not trying to BS me. reply scoofy 17 hours agorootparentThis is a good suggestion, and I'll think long and hard about it. My biggest concern is that the type of people who would contribute to such a public project are the type of folks who would be offended at the use of AI in general. That concern, again, leads me back to the conundrum of what to do. I've always insisted that if it is financially feasible, I'd want the app to become a 501(c)(3) or at least a B-Corp, maybe even sold to Wikimedia. Still, the number of people who contribute to the side vs the number who visit is somewhere in the range of 1:10,000 (if that) right now, so concern about offending contributors is non-trivial. As it stands, I've generally gone to the courses' sites and just quoted what they have to say about their own course, but that really isn't what I want to do, even if it is generally informative. Unfortunately, there is rarely hole-by-hole information, which is the level of granularity I'm going for. reply BobbyTables2 1 hour agorootparentprevBut AI will just summarize v other humans’ work here. It has no understanding of golf… reply nyarlathotep_ 16 hours agoparentprevI do wonder how much of the push for LLM-integrated everything has taken this into account. The general trend of viewing LLM features as forced against users' will and the now widespread use of \"slop\" as a derogatory description seems to indicate the general public is less enthusiastic about these consumer advances than, say, programmers on HN. I use LLMs for programming (and a few other, general QA things before a search engine/wikipedia visit) but want them absolutely nowhere else (except CoPilot et al in certain editors) reply nxobject 19 hours agoparentprevAnother trick I do is to scroll to the end, and see if the last paragraph is written as a neat conclusion with a hedge (i.e. \"In short...\", \"Ultimately...\"). I imagine it's a convention to push LLMs to terminate text generation, but boy is it information-free. reply Balgair 20 hours agoparentprev> (I mean \"nobody\" in the sense of \"nobody likes Nickelback\". ie, not literally nobody.) Reminds me of the old Yogi Berra quote: Nobody goes there anymore, its too crowded. reply Aerroon 21 hours agoparentprevI can understand it for AI generated text, but I think there are a lot of people that like AI generated images. Image sites like get a ton of people that like AI generated images. Civitai gets a lot of engagement for AI generated images, but so do many other image sites. reply egypturnash 20 hours agorootparentPeople who submit blog posts here sure do love opening their blogs with AI image slop. I have taken to assuming that the text is also AI slop, and closing the tab and leaving a comment saying such. Sometimes this comment gets a ton of upvotes. Sometimes it gets indignant replies insisting it's real writing. I need to come up with a good standard response to the latter. reply BeetleB 19 hours agorootparent> People who submit blog posts here sure do love opening their blogs with AI image slop. It sucks, but it doesn't suck any more than what was done in the past: Litter the article with stock photos. Either have a relevant photo (and no, a post about cooking showing an image of a random kitchen, set of dishes, or prepared food does not count), or don't have any. The only reason blog posts/articles had barely relevant stock images was to get people's attention. Is it any worse now that they're using AI generated images? reply daveguy 20 hours agorootparentprev> I need to come up with a good standard response to the latter. How about, \"I'm sorry, but if you're willing to use AI image slop, how should I know you wouldn't also use AI text slop? AI text content isn't reliable, and I don't have time to personally vet every assertion.\" reply numpad0 20 hours agorootparentTrying to gaslight your enemy is certainly an option for something, not always the best nor the one in line with HN guideline. Frankly it just rarely reduce undesirable behaviors even if you're in the mood to be manipulative. reply daveguy 19 hours agorootparentWell, I wouldn't call that gaslighting, just a statement of fact. I guess you could go with \"Sorry buddy, I don't trust your content because you used AI slop for your images.\" If you think saying the same thing with more words is manipulative and gaslighting. Also, \"enemy\"? That's a little harsh, don't you think? I would never consider a random doofus on an internet forum to be my enemy. reply numpad0 16 hours agorootparentThe person posting an AI header likely isn't getting the reflexive gastric discomfort that anyone feels looking at one that doesn't happen with stock photos. They just can't even tell, and there's no path for them in that kind of antagonizing responses to lead them to the realization that others readily can and aren't liking it. reply daveguy 13 hours agorootparentThat is an excellent point. Thank you. As the article points out. AI slop is already so pervasive it's showing up in supposedly historical posts. And it's harder to identify AI generated images than text. reply earnestinger 21 hours agorootparentprevI don’t understand the problem with AI generated images. (I very much would like any AI generated text to be marked as such, so I can set my trust accordingly) reply cogman10 20 hours agorootparent> I don’t understand the problem with AI generated images. Depends on what they are used for and what they are purporting to represent. For example, I really hate AI images being put into kids books, especially when they are trying to be psuedo-educational. A big problem those images have is from one prompt to the next, it's basically impossible to get consistent designs which means any sort of narrative story will end up with pages of characters that don't look the same. Then there's the problem that some people are trying to sell and pump this shit like crazy into amazon. Which creates a lot of trash books that squeeze out legitimate lesser known authors and illustrators in favor of this pure garbage. Quite similar to how you can't really buy general products from amazon because drop shipping has flooded the market with 10 billion items with different brands that are ultimately the same wish garbage. The images can look interesting sometimes, but often on second glance there's just something \"off\" about the image. Fingers are currently the best sign that things have gone off the rails. reply tayo42 21 hours agorootparentprevDespite what people think there is a sort of art to getting interesting images out of an ai model. reply onemoresoop 20 hours agorootparentThat’s not the issue though, it should be marked as such or be found in a section people looking for it can easily find it instead of shoving it everywhere. To me placing that generated content in human spaces is a strong signal for low effort. On the other hand generated content can be extremely interesting and useful and indeed there’s an art to it reply daveguy 20 hours agorootparentI agree. AI generated text and images should be marked as such. In the US there was a push to set standards on watermarking AI generated content (feasible for images/video, but more difficult for text, because it's easier to delete). Unfortunately, the effort to study potential watermarking standards was rescinded as of Jan 22 2025. reply numpad0 20 hours agorootparentprevThey know everyone, especially the ones they seek attention from, has such labels in their muted keywords list. reply jchw 21 hours agoparentprevExactly. Why in the hell would I want someone to use ChatGPT for me? If I wanted that, I could go use that instead. reply Self-Perfection 20 hours agorootparentI believe most times such responses are made in assumption that people are just lazy, like we used provide links to https://letmegooglethat.com/ before. reply rapind 22 hours agoparentprev> If Google added a global \"remove generative content\" filter that worked, I would click it and then never unclick it. It's not just generated content. This problem has been around for years. For example, google a recipe. I don't think the incentives are there yet. At least not until Google search is so unusable that no one is buying their ads anymore. I suspect any business model rooted in advertising is doomed to the eventual enshitification of the product. reply agumonkey 21 hours agoparentprevnobody wants to see other's ai generated images, but most people around me are drooling about generating stuff wait for the proof-of-humanity decade where you're paid to be here and slow and flawed reply ijk 21 hours agorootparentMost AI generated images are like most dreams: meaningful to you but not something other people have much interest it. Once you have people sorting through them, editing them, and so on the curation adds enough additional interest...and for many people what they get out of looking at a gallery of AI images is ideas for what prompts they want to try. reply onemoresoop 20 hours agorootparentprevMost AI genetated visuals have a myriad of styles but you could mostly tell it’s something not seen before and thats what people may be drooling for. The same drooling happened for things that have finally found their utility after a long time and are we’re now used to. For example 20 years ago Photoshop filters were all the rage and you’d see them expressed out everywhere back then. I think this AI gen phase will lose interest/enthusiasm over time but will enter and stay in toolbox for the right things, whatever people decide to be then. reply nxobject 19 hours agorootparentprevRe: proof-of-humanity... I'm looking forward to a Gattaca-like drop-of-blood port on the side of your computer, where you prick yourself everytime you want a single \"certified human response\" endorsement for an online comment. reply carlosjobim 20 hours agoparentprevI think a good comparison is when you go to a store and there are salesmen there. Nobody wants to talk to a salesman. They can almost never help a customer with any issue, since even an ignorant customer usually knows more about the products in the store than the salesmen. Most customers hate salesmen and a sustainable portion of customers choose to leave the store or not enter because of the salesmen, meaning the store loses income. Yet this has been going on forever. So just prepare for the worst when it comes to AI, because that's what you are going to get, and neither ethical sense, business sense or any rationality is going to stop companies from showing it down your throat. They don't give a damn if they will lose income or even bankrupt their companies, because annoying the customer is more important. reply plagiarist 19 hours agoparentprevMuch of the benefit I get from LLMs is, ironically, avoiding LLM output from the results of web searches. reply bornfreddy 11 hours agorootparentHow do you do that, if you don't mind sharing? reply plagiarist 5 hours agorootparentIt might not be what you're hoping for, but just doing a question instead of a web search. It's often more useful to get even a hallucinatory answer compared to affiliate marketing listicles that are all coming from smaller models anyway. reply asddubs 22 hours agoparentprevI was googling a question about opengraph last week. so many useless AI drivel results now. reply Scoundreller 22 hours agoprev> It had fairly steady growth until 2012, before petering out throughout the 2010s and 2020s in lieu of more centralized sites like StackExchange, and by 2025, only a small community was left This timeline tracks with my own blogging. Google slowly stopped ranking traditional forum posts and blogs as well around that time, regardless of quality, unless it was a “major”. > But, unlike so many other fora from back in the early days, it went from 2003 to 2025 without ever changing its URLs, erasing its old posts, or going down altogether. I can also confirm if you have a bookmark to my blog from 2008, that link will still work! The CMS is no longer, it's all static now... which too few orgs take the short amount of time to bother with when \"refreshing\" their web presence :( reply Lammy 22 hours agoparent> Google slowly stopped ranking traditional forum posts and blogs as well around that time IMO the true inflection point was 2014 when Google first hid (from the UI) and then fully removed (no longer accessible by magic URL) the “Blogs” and especially the “Discussions” filters. Some contemporary discussions on “Discussions”: https://techcrunch.com/2014/01/23/googles-search-filters-now... http://googlesystem.blogspot.com/2014/03/bring-back-forum-se... (details the briefly-working magic URLs) https://www.ghacks.net/2014/01/23/search-discussions-blogs-p... https://www.seroundtable.com/google-search-filters-gone-1799... https://www.webmasterworld.com/google/4687960.htm https://www.thecoli.com/threads/i-cant-google-search-by-disc... https://www.neogaf.com/threads/anyone-else-annoyed-google-re... https://webapps.stackexchange.com/questions/57249/has-the-op... https://www.bladeforums.com/threads/how-to-do-google-discuss... https://browsermedia.agency/blog/alternatives-discussion-sea... reply layer8 21 hours agorootparentI recently noticed that there is now a not-visible-by-default “Forums” option in Google Search. It is selected by specifying the query parameter udm=18: https://www.google.com/search?q=hp+50g&udm=18 reply lkramer 20 hours agorootparentThis is interesting. I wonder why it's not visible by default. reply layer8 20 hours agorootparentMaybe it is/was an A/B test, to see if it hurts ad revenue (it probably does). The option appeared randomly for me on a search, and I took immediately note of the udm number. :) reply wholinator2 20 hours agorootparentIn addition to the others mentioned on the sibling comment (I cannot reply to it?), udm=28 is shopping, 36 is books, 37 is \"products\", 44 is \"visual matches\", 48 is \"exact matches\", 50 is \"AI Mode\" but no tab appears, 51 is homework and I stopped at 80 because the page kept removing that part of the url all the way up to that point. reply Scoundreller 16 hours agorootparentFor some reason the reply button won’t pop up right away but you can click on the post’s timestamp and reply there reply Scoundreller 20 hours agorootparentprevRan through the lower numbers that hit something interesting: 8 = jobs (but doesn't return any results) 15 = attractions (but doesn't return any results) reply nateglims 20 hours agoparentprevI remember several traditional programming forums I frequented in the 00s getting hit hard by the Google Panda update around 2013. It ruined their SEO and they started to go into decline. Forums and blogs had a culture that isn't replicated by reddit, social media, etc. It's a shame to lose it. reply elashri 22 hours agoprevIt is sad that this is happening to PhysicsForums. It was one of first websites I was using frequently 15 years ago when I started my physics passion (later career). I was active reader and contributed on few occasions but I still remember some members who I thought that one day I will be smart and knowledgeable like them. With years and the move to social media following Arab spring things started to change (as part of the overall transition from forum being the dominant place for discussions). But I stopped visiting it around 2018 unless I came through google search (later kagi). I still find the archive useful to answer some questions and I would disagree with author of article that because no one is sharing links on twitter that means no one care. reply inasio 21 hours agoprevTalk about burying the lede! Near the bottom of the story the site owner confirms that it was him that added the backdated AI comments (perhaps it should have been obvious...) reply advisedwang 14 hours agoparentThe investigation of the events on this particular website are just a tool to illustrate a much broader point about internet content, identity and LLMs. reply firesteelrain 20 hours agoparentprevI couldn’t find it. He was trying to seed the site ? reply inasio 20 hours agorootparentThis is what he replied when asked about the backdated comments: > The backdated answers were an internal test. We conceived of a bot that would provide a quality answer to a thread without a reply after 1+ years. That too also failed. Instead, I’m considering pruning all threads without a reply as they clutter up the forums. reply naijaboiler 19 hours agorootparentThats some really shady stuff. Its one thing to provide answers with AI bot. Its totally disingenuous to 1) backdate it 2) assume the alias of a real person reply MichaelZuo 19 hours agorootparentprevHe’s an SEO guy at shopify, so he likely doesn’t care about the consequences of doing shady SEO experiments with old user’s accounts… reply pbronez 20 hours agorootparentprevExperimenting with using AI bots to respond to questions that had been open for a long time with no response. reply econ 20 hours agoprevI like the assumption that it was a real account originally. It all seems so unthinkable but when running a forum or a blog with an active comment section.. what would you do/think if your users show up, browse around and not say anything for a week? You start out by making topics in your own name, write helpful replies.. until you look like an idiot talking to yourself. Forums with good traffic and lots of spammy advertisement no doubt consider it when visitors leave because nothing new happened. I once upon a time, on a rather stale forum, created two similarly named accounts from the same ip and argued with myself. At first I thought the owner or one of the other users would notice but I quickly learned that no behaviour is weird enough for it to be ever considered. reply BlueTemplar 2 hours agoparentI think I would rather post alone than my current experience (on 2 forums already) of other posters being overwhelmingly spam bots. reply segasaturn 22 hours agoprevMoney quote: > There’s also a social contract: when we create an account in an online community, we do it with the expectation that people we are going to interact with are primarily people. Oh, there will be shills, and bots, and advertisers, but the agreement between the users and the community provider is that they are going to try to defend us from that, and that in exchange we will provide our engagement and content. This is why the recent experiments from Meta with AI generated users are both ridiculous and sickening. When you might be interacting with something masquerading as a human, providing at best, tepid garbage, the value of human interaction via the internet is lost. It is a disaster. I have no idea how to solve this issue, I can't see a future where artificially generated slop doesn't eventually overwhelm every part of the internet and make it unusable. The UGC era of the internet is probably over. reply jgilias 21 hours agoparentOh, there are solutions. One is a kind of a socialized trust system. I know that Lyn Alden that I follow on Nostr is actually her not only because she says so, but also because a bunch of other people follow her too. There are bot accounts that impersonate her, but it’s easy to block those, a it’s pretty obvious from the follower count. And once you know a public key that Lyn posts under, I’m sure it’s her. She could start posting LLM nonsense, but people will be quick to point it out, and start unfollowing. An important part is that there’s no algorithm deciding what I see in my feed (unless I choose so), so random LLM stuff can’t really get into my feed, unless I chose so. Another option is zero knowledge identity proofs that can be used to attest that you’re a human without exposing PII, or relying on a some centralized server being up to “sign you in on your behalf” https://zksync.mirror.xyz/kWRhD81C7il4YWGrkDplfhIZcmViisRe3l... reply roywiggins 21 hours agorootparentHow can ZK approaches prevent people from renting out their human identity to AI slop producers? reply jgilias 21 hours agorootparentBy just making it more expensive. We’re never going to get rid of spam fully, but the higher we can raise the costs, the less spam we get. EDIT: Sorry, I didn’t answer your question directly. So it doesn’t, but makes spam more expensive. reply RiverCrochet 20 hours agoparentprevWell, the end of open, public UGC content anyway. I have heard of Discord servers where admins won't assign you roles giving you access to all channels unless you've personally met them, someone in the group can vouch for you, or you have a video chat with them and \"verify.\" This is the future. We need something like Discord that also has a webpage-like mechanism built into it (a space for a whole collection of documents, not just posts) and is accessible via a browser. Of course, depending on discovery mechanisms, this means this new \"Internet\" is no longer an easy escape from a given reality or place, and that was a major driver of its use in the 90's and 00's curious people wanting to explore new things not available in their local communities. To be honest, the old, reliable Google was probably the major driver of that. And it sucks for truly anti-social people who simply don't want to deal with other people for anything, but maybe those types will flourish with AI everywhere. If the gated hubs of a possible new group-x-group human Internet maintain open lobbies, maybe the best of both worlds can be had. reply 8474_s 9 hours agorootparentThis strange reliance on Discord as some sort of \"escape from web3.0\" is silly to anyone who knows what Discord is(modern AOL) and how centralized it is. Its just the same corporate walled garden with more echochambery isolation. reply BlueTemplar 2 hours agorootparentprevDiscord, or the Death of Lore : https://news.ycombinator.com/item?id=35050858 (Even when something like a wiki exists, most of actual information will still be contained in the lore, itself blackholed by a deep web platform like Discord.) reply kevinventullo 22 hours agoparentprevIronically, on Facebook itself I am only friends with people I actually know in real life. So, most of the stuff I see in my feed is from them. reply bee_rider 21 hours agorootparentI’m only friends with people I know on Facebook, so I’m mostly see ads on that site. There’s a feed to just see stuff your friends post, but for some reason the site defaults to this awful garbage ad spam feed (no surprise really). reply segasaturn 19 hours agorootparentDo people still post things on Facebook? I don't know because I haven't used it, ever, but I've heard that Meta has turned it into a platform mostly for passively consuming algorithmically-driven content instead of sharing your day on your News Feed. reply bee_rider 13 hours agorootparentThe posts from my friends are all politics and babies, which is not really interesting. But I guess I can’t really complain, that’s what’s going on in their lives. reply lumost 22 hours agoparentprevI suspect that the honest outcome will be that platforms where AI content is allowed/encouraged will begin to appear like a video game. If everyone in school is ai-media famous then no one is. There is most assuredly a market for a game where you are immediately influencer famous, but it's certainly much smaller than the market for social media. reply bboygravity 20 hours agoparentprevCool, does that mean we can go out more and talk to real humans again? Can't wait tbh. reply llm_trw 20 hours agoparentprevFor the tech discussions I'm interested in burning cpu/GPU cycles for proof of work is a good way to make replies expensive enough that only people who care will post then. Another option is a web of trust. It's finally the year of gpg! reply thatguy0900 22 hours agoparentprevInvite only forums or forums with actual identity checking of some sort. Google and Facebook are in prime position to actually provide real online identity services to other websites, which makes Facebook itself developing bots even funnier. Maybe we'll eventually get bank/government issued online identity verification. reply segasaturn 22 hours agorootparentOnline identity verification is the obvious solution, the only problem is that we would lose the last bits of privacy we have on the internet. I guess if everyone was forced to post under our real name and identity, we might treat each other with better etiquette, but... reply yjftsjthsd-h 21 hours agorootparent> I guess if everyone was forced to post under our real name and identity, we might treat each other with better etiquette, but... But Facebook already proved otherwise. reply numpad0 20 hours agorootparentprevPosting with IRL identity removes the option to back down after a mistake and leads to much worse escalations, because public reputations will be at stake by default. reply StefanBatory 21 hours agorootparentprevMy parents use a lot of Facebook and things some people say under their real name are really mindblowing. reply thatguy0900 22 hours agorootparentprevOptimistically, if all you want to do is prove you are, in fact, a person, and not prove that you are a specific person, there's no real reason to need to lose privacy. A service could vouch that you are a real person, verified on their end, and provide no context to the site owner as to what person you are. reply roywiggins 21 hours agorootparentThat doesn't stop Verified Humans(TM) from copying and pasting AI slop into text boxes and pressing \"Post.\" If there's really good pseudonymity, and Verified Humans can have as many pseudonyms as they like and they aren't connected to each other, one human could build an entire social network of fake pseudonyms talking to each other in LLM text but impeccable Verified Human labels. reply thatguy0900 21 hours agorootparentThe identity provider doesn't need to tell the forum that you are 50 different people. They could have a system where if the forum bans you the forum would know it's the same person they banned on reapplication. As far as people making a real person account then using that to do Ai stuff yeah there will have to be a way to persistently ban someone through anonymous verification, but thats possible. Both the identity verifier and forum will be incentivized to play nice with each other. If a identity provider is allowing one person to make 50 spam accounts the forum can stop accepting verification from that provider. reply crdrost 21 hours agorootparentprevI just want to semi-hijack this thread to note that you can actually peek into the future on this issue, by just looking at the present chess community. For readers who are not among the cognoscenti on the topic: in 1997 supercomputers started playing chess at around the same level as top grandmasters, and some PCs were also able to be competitive (most notably, Fritz beat Deep Blue in 1995 before the Kasparov games, and Fritz was not a supercomputer). From around 2005, if you were interested in chess, you could have an engine on your computer that was more powerful than either you or your opponent. Since about 2010, there's been a decent online scene of people playing chess. So the chess world is kinda what the GPT world will be, in maybe 30ish years? (It's hard to compare two different technology growths, but this assumes that they've both hit the end of their \"exponential increase\" sections at around the same time and then have shifted to \"incremental improvements\" at around the same rate. This is also assuming that in 5-10 years we'll get to the \"Deep Blue defeats Kasparov\" thing where transformer-based machine learning will be actually better at answering questions than, say, some university professors.) The first thing is, proving that someone is a person, in general, is small potatoes. Whatever you do to prove that someone is a real person, they might be farming some or all of their thought process out to GPT. The community that cares about \"interacting with real humans\" will be more interested in continuous interactions rather than \"post something and see what answers I get,\" because long latencies are the places where GPT will answer your question and GPT will give you a better answer anyways. So if you care about real humanity, that's gonna be realtime interaction. The chess version is, \"it's much harder to cheat at Rapid or Blitz chess.\" The second thing is, privacy and nonprivacy coexist. The people who are at the top of their information-spouting games, will deanonymize themselves. Magnus Carlsen just has a profile on chess.com, you can follow his games. Detection of GPT will look roughly like this: you will be chatting with someone who putatively has a real name and a physics pedigree, and you ask them to answer physics questions, and they appear to have a really vast physics knowledge, but then when you ask them a simple question like \"and because the force is larger the accelerations will tend to be larger, right?\" they take an unusually long time to say \"yep, F = m a, and all that.\" And that's how you know this person is pasting your questions to a GPT prompt and pasting the answers back at you. This is basically what grandmasters look for when calling out cheating in online chess; on the one hand there's \"okay that's just a really risky way to play 4D chess when you have a solid advantage and can just build on it with more normal moves\" but the chess engine sees 20 moves down the road beyond what any human sees, so it knows that these moves aren't actually risky and on the other hand there's \"okay there's only one reason you could possibly have played the last Rook move, and it's if the follow up was to take the knight with the bishop, otherwise you're just losing. You foresaw all of this, right?\" and yet the \"person\" is still thinking (because the actual human didn't understand why the computer was making that rook move, and now needs the computer to tell them that the knight has to be taken with the bishop as appropriate follow-up). reply aleph_minus_one 20 hours agorootparent> you will be chatting with someone who putatively has a real name and a physics pedigree, and you ask them to answer physics questions, and they appear to have a really vast physics knowledge, but then when you ask them a simple question like \"and because the force is larger the accelerations will tend to be larger, right?\" they take an unusually long time to say \"yep, F = m a, and all that.\" And that's how you know this person is pasting your questions to a GPT prompt and pasting the answers back at you. Honestly, (even) in my area of expertise, if the \"abstraction/skill level\" or the kind of wording (in your example: much less scientifically precise wording, \"more like a 10 year old child asks\"), it often takes me quite some time to adjust (it completely takes me out of my flow). So, your criterion would yield an insane amount of false positives on me. reply 1659447091 21 hours agorootparentprev> with actual identity checking of some sort I am hoping OpenID4VCI[0] will fill this role. It looks to be flexible enough to preserve public privacy on forums while still verifying you are the holder of a credential issued to a person. The credential could be issued from an issuer that can verify you are an adult (banks) for example. Then a site or forum etc, that works with a verifier that can verify whatever combination of data of one or more credentials presented. I haven't dug into the full details of implementation and am skimming over a lot but that appears to be the gist of it. [0] https://openid.net/specs/openid-4-verifiable-credential-issu... reply cess11 21 hours agoparentprevIf you think about historical parallels like advertising and the industrialisation of entertainment, where the communication is sufficiently human-like to function but deeply insincere and manipulative, I think you'll find that you absolutely can see such a future and how it might turn out. A lot or most of people will adapt, accept these conditions because compared to the constant threat of misery and precarity of work, or whatever other way to sustenance and housing, it will be very tolerable. Similar to how so called talk shows flourished, where fake personas pretend to get to know other fake personas they are already very well acquainted with and so on, while selling slop, anxieties or something. Like Oprah, the billionaire. reply Rodeoclash 22 hours agoprevThe ShackNews forum: https://www.shacknews.com/chatty was similar go back in time on on it and you can find posts about 9/11 unfolding. reply tomrod 22 hours agoparentArs Technica started with comms forums + this new idea to report tech news. The forums are still there but not nearly the camaraderie of the early days. reply Aurornis 21 hours agorootparent> The forums are still there but not nearly the camaraderie of the early days. I remember visiting those forums when I was young and feeling like part of a big group of friendly people hanging out online together. I tried creating a new account recently and it had a very different vibe. Felt like the old guard had been established and the forums I looked at were dominated by a couple of posters who just wanted to talk, but not discuss anything. Some of the post counts of those people were eye-watering. reply StefanBatory 21 hours agorootparent> Felt like the old guard had been established and the forums I looked at were dominated by a couple of posters who just wanted to talk, but not discuss anything. I think this is the case for most places, I'm afraid. I use mainly Discord there are certainly a lot of servers where I'm purely because I'm talking to people I met there, and I don't even play that game anymore. There solution is simpler after time we create private servers or channels for the old guard, but even then the places deteriorate. It's a thing I don't know how to solve. reply sdwr 20 hours agorootparentThe problem is when the old guard becomes an exclusive clique. Sometimes it's by accident (\"I'm happy with the friends I already have\"), but usually there's a portion of the inner circle that validate themselves by gatekeeping newcomers. There has to be an active commitment to include (annoying, tactless, socially-impoverished) newbies, or the snake eats its tail and collapses under its own weight. reply r58lf 22 hours agorootparentprevSame with siliconinvestor.com It was an early stock discussion forum. It grew rapidly when search engines started indexing everything and this forum had a URL for each message that was easily indexable. It's still around, but nothing like the old days. reply 0xDEAFBEAD 20 hours agorootparentI find these old school forums fascinating. How does that even work, to have a thread of 192,211 posts about Qualcomm? https://www.siliconinvestor.com/subject.aspx?subjectid=36035 Suppose the average post is about 1 paragraph long. One paragraph is about 150 words. So 192211 * 150 = about 29 million words. For comparison, the Lord of the Rings trilogy is only around half a million words. It wouldn't surprise me if there are more words about Qualcomm in that thread than the total amount of internal and external documentation and financial guidance that Qualcomm itself has ever produced. Surely users aren't expected to read the entire thread before adding a post? But I think I remember seeing old forums where that basically is the expectation. And honestly... that's pretty cool. It seems better than the new social media, where we keep having low-effort recurring debates. I like the idea of adding to an enormous pile of scholarship in cyberspace. A Ship of Theseus discussion which may outlive any individual participant, but has a semblance of continuity all the same, like an undergraduate college society with a 100+ year history. Time for a cyberpunk revival. Retro-cyberpunk, we could call it. reply tomrod 3 hours agorootparentPeople like to talk at scale, even if no one is listening. reply 0xDEAFBEAD 20 hours agorootparentprevreddit had camaraderie in the early days too. Is there anywhere on the internet that still has camaraderie? reply tomrod 3 hours agorootparentI enjoy Hacker News even with its recent growth. Other places seem to either not have critical mass to stick around (datatau), or become troll sites (econjobrumors, reddit). reply encom 20 hours agorootparentprevIt's been a long time since I visited the Ars forums, but the news article commenters today are absolutely deranged. It makes me want to not engage with the forums again. reply Toutouxc 10 hours agorootparentI feel like most commenters in general are absolutely deranged. News articles are the worst for some reason, then YouTube, and Reddit isn’t that much better. I often wonder how these people look, work and function in real life. reply tomrod 3 hours agorootparentprevWhy? reply Terr_ 22 hours agoprevOoof. The idea--or reality--that humans' accounts would be hijacked by site-owners to make impersonating slop (presumably to bring in ad-revenue) is somehow both infuriating and energy-sapping-depressing. Issues of trust and attribution have always existed for the web, but for many reasons it feels so much worse now--how bad must it get before some kind of sea-change can occur? I'm not sure what the solution would be here. * Does one need to establish a friggin' trademark for their own name/handle [0], just so they can threaten to sue using money they probably don't have? * Is it finally time for PKI and everybody signs their posts with private keys and wastes CPU cycles verifying signatures to check for impersonation? * Is there some set of implied collective expectations which need to be captured and formalized into the global patchworks of law? [0] Ex: By establishing a small but plausible \"business\" selling advice and opinions under that name, and going after the impersonator for harming that brand. reply bee_rider 21 hours agoparentImpersonating somebody to make it look like they said something they didn’t really ought to be considered defamation or something. Also there’s something really uncomfortable about the phrasing of a lot of those answers. I mean, even as somebody with an engineering degree, I try not to ever answer a question “as aengineer” because when screwing around online I haven’t done the correct amount of analysis to provide answers “as an engineer” ethically (acknowledging the irony of using the phrase here, but, clearly this is not a technical statement so I think it is fine). The bot doesn’t seem to have this compunction. This ravenprp guy was an engineering student a couple years ago. I guess it’s less of a thing because he wasn’t commenting under his real name. But it seems like this site, given the type of content it hosts, could easily end up impersonating somebody “as an engineer” in the field they work and have a professional reputation in. And the site even has a historical record of them asking and answering questions through their education, so it does a really good job of misleading people into thinking an engineer is answering their questions. I know the idea of an individual professional reputation has taken a beating in the modern hyper-corporate world. But the more I think of it, the more I think… this seems incredibly shitty and actually borderline dangerous, right? reply m463 22 hours agoparentprevIt is sad. I have been putting a copyright notice on my resume at the bottom to prevent some nonsense. I have always wondered if people could attach some sort of cryptographic marker to their posts, that could link to an archive somewhere. Mostly I was thinking of backups of posts to yelp that couldn't be taken down, but I wonder if it would work that posts someone never made. reply Terr_ 22 hours agorootparent> I have been putting a copyright notice on my resume at the bottom to prevent some nonsense. I expect the bad-actors will feed it into an LLM and say: \"Rephrase this slightly\", and they will get away with it because the big-money hucksters will have already convinced courts to declare it transformative or fair-use. reply UltraSane 22 hours agoparentprevI exchange public keys with close friends in person. A large scale solution would be very Orwellian. You would need a national ID that is a smart card to connect to an ISP and possible biometric verification. reply bawolff 22 hours agorootparentWe already have e-passports and zero knowledge proofs to show you have one without revealing who you are. If all else fails, there is always the web of trust (i think web of trust has a lot of issues, but establishing soneone is human seems like a much lower bar than establishing identity) reply UltraSane 16 hours agorootparentWeb of trust would be interesting if phones could automatically trust other phones they spend enough time nearby. reply UltraSane 16 hours agorootparentprevI hadn't though of zero-knowledge proofs. That is an interesting idea. reply bawolff 11 hours agorootparentThe blockchain people have been experimenting with it https://ethresear.ch/t/zero-knowledge-proofs-of-identity-usi... (im no fan of blockchain but i assume you can reuse the ideas without the blockchain part) reply afpx 22 hours agorootparentprevCould I buy a physical device like RSA SecurID from my bank branch or post office and log into a closed VPN-like network where all the servers are run by verified users? I know there are problems with that idea. reply hooverd 22 hours agorootparentprevDo you exchange public keys with your non-computer-toucher close friends? reply arccy 22 hours agorootparentif you convince them to use signal that's close enough... reply scotty79 22 hours agoparentprevShouldn't we invent a protocol that keeps the content you produce under your control so that places like forums or facebook are only discovery devices and interaction facilitators, but not custodians of all communication? Being able to independantly reach the source of piece of information is increasingly important. reply hooverd 20 hours agorootparentThat's what ATProto is trying to solve, funny enough. reply hooverd 22 hours agoparentprevDon't sign your posts! reply Terr_ 21 hours agorootparentAre you saying nothing should be key-signed because you want some kind of deniability later? Or do you mean people should avoid using an pseudonym in favor of posts that are anonymous, so that there's never any created identity to exploit/defend? reply hooverd 20 hours agorootparent-----BEGIN PGP SIGNED MESSAGE-- Hash: SHA512 Sorry, it was a bad joke, there's a phrase \"don't sign your posts\" used when someone ends one with an insult. I support signing your posts with digital signatures if you want. BEGIN PGP SIGNATURE--- iHUEARYKAB0WIQQC37hdRRO1LtrTQY8AXxvbqjG5KgUCZ5QRXwAKCRAAXxvbqjG5 Kth4AQCccNygglcSyEiMAqQyw6cXH54fnqBT9rJO9TSIqH14rgEAyUwxiQlV05XV Du2ftMk3DwiUZLKDxVI+ODCn4osf2wM= =XZhX --END PGP SIGNATURE--- reply EA-3167 22 hours agoprevI don't quite understand the issue of \"back-dating\" or hijacking accounts. How is this being done exactly? I came away from this article wondering if I was missing something. reply Evidlo 22 hours agoparentThe last section mentions that the PhysicsForums admins are experimenting with LLM-generated responses, so I think the site owners are responsible. > We reached out to Greg Bernhardt asking for comment on LLM usage in PhysicsForums, and he replied: > \"We have many AI tests in the works to add value to the community. I sent out a 2024 feedback form to members a few weeks ago and many members don’t want AI features. We’ll either work with members to dramatically improve them or end up removing them. We experimented with AI answers from test accounts last year but they were not meeting quality standards. If you find any test accounts we missed, please let me know. My documentation was not the best.\" Why they would recycle old human accounts as AI \"test accounts\", I have no idea. reply emmelaich 21 hours agorootparentLooks like Greg now does SEO for Shopify. That fits I guess. https://gregbernhardt.com/ https://www.linkedin.com/in/gregbernhardt https://www.physicsforums.com/insights/author/greg-bernhardt... https://x.com/GregBernhardt4/status/1875287174205374533 > \"The dead internet theory is coming to fruition. This is a large reason I'm starting to cut back on social media and take back my time.\" reply Gooblebrai 20 hours agorootparentOh, I thought he would be a physicist reply Terr_ 22 hours agoparentprev> How is this being done exactly? Presumably it's being done by the site-owner, whether that means new-management or original management getting desperate/greedy. reply EA-3167 22 hours agorootparentOh that's so disappointing to hear about PhysicsForums. Thanks for the answer to you, and the others who replied. reply roywiggins 22 hours agoparentprevWhoever runs the site/database is just inserting rows with fake datestamps under existing (presumably abandoned) account names. reply aaron_m04 22 hours agorootparentHow could anyone possibly think it'd be OK to impersonate real humans? reply jeremyjh 22 hours agorootparentThey don't give a fuck if its \"ok\". They are just trying to scrape up some additional ad revenue, like 99% of the rest of the internet. reply simplicio 22 hours agorootparentI don't really get the revenue angle though. The AI posts don't seem to be trying to drive traffic to ads or anything. I really don't understand the point of auto-generating a bunch of AI gibberish under the name of old users on ones own site? reply roywiggins 21 hours agorootparentA misguided attempt at SEO? reply guynamedloren 22 hours agoparentprevWondering the same. I couldn't make it through the article. Fascinating discovery, but poorly written and difficult to navigate the author's thoughts. The interstitial quotes were particularly disorienting. reply majgr 19 hours agoprevIt is interesting, that this forum might be „AI poisoned” for other AI bots, because training AI on content generated by AI = garbage. reply secretsatan 18 hours agoprevI suspect a manager i work with has started using llms, i’ve sat next to him long enough before he went manager to know he’s incompetent, and now in chat, suddenly he spouts out of character plausible, but off for him explanations. I work in a company where for most english is not their first language so i’m not sure if anyone else has picked up on it. reply LordShredda 22 hours agoprevDon't give out your real name online, the server admin might change your posts. reply eddd-ddde 18 hours agoparentThey can also just, set a random account under your name. So no difference. reply paulpauper 21 hours agoprev [–] Dead internet theory is one of those ideas that keeps resurfacing or being revied with articles like this, even though the evidence is only limited to confirmation bias. It ignores that there are huge parts of the internet that are not dead. I think it's more like the quality of discourse has fallen for reasons that are not clear. reply datadrivenangel 20 hours agoparentThe article looked at the PhysicsForums and found that 92% of the text is AI or machine generated... reply paulpauper 20 hours agorootparentThe internet is way bigger than PhysicsForums. That was my point, but your response seems to confirm what I said about discourse declining though. reply NotYourLawyer 21 hours agoparentprev [–] > there are huge parts of the internet that are not dead Such as? reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Hall of Impossible Dreams\" is a repository featuring diverse content, including code and literature, and recently discussed the decline of PhysicsForums due to AI-generated content. PhysicsForums, established in 2001, was a thriving science forum until the rise of centralized platforms like StackExchange and the infiltration of AI-generated posts by 2025. The post raises concerns about the \"Dead Internet Theory,\" which posits that much of the internet is now non-human content, highlighting the challenges and ethical implications of sustaining online communities in the face of evolving technology."
    ],
    "commentSummary": [
      "There is increasing frustration with AI-generated content on the internet, especially in forums and blogs, due to its perceived lack of depth and reliability.",
      "Users are calling for solutions like identity verification or trust systems to ensure genuine human interaction and address ethical concerns about AI impersonating real users.",
      "The discussion underscores a broader tension between technological advancement and the desire for authentic, human-driven content online."
    ],
    "points": 246,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1737747518
  },
  {
    "id": 42818278,
    "title": "File Explorer is merged to Helix editor",
    "originLink": "https://github.com/helix-editor/helix/pull/11285",
    "originBody": "helix-editor / helix Public Notifications Fork 2.6k Star 35.3k Code Issues 689 Pull requests 287 Discussions Actions Projects Wiki Security Insights New issue Jump to bottom Add file explorer #11285 Merged the-mikedavis merged 12 commits into helix-editor:master from drybalka:add-file-browser Merged Add file explorer #11285 the-mikedavis merged 12 commits into helix-editor:master from drybalka:add-file-browser +203 −21 Conversation 72 Commits 12 Checks 6 Files changed 5 Conversation Contributor drybalka commented • edited This is a minimal implementation of the file browser, which would probably cover a lot of requirements in #200. The whole thing works analogous to the https://github.com/nvim-telescope/telescope-file-browser.nvim as suggested in this comment. Even though the resolution of the discussion seems to be \"file tree/browser is too hard, it should be implemented as a plugin\", I feel like my changes are quite small and natural to be considered for adding to the core nonetheless. The implementation simply builds on the existing file picker and only modifies 3 files, so the added maintenance burden should be quite small. The code itself is not particularly elegant (in my rather inexperienced opinion), but I did not want to over-complicate things. This is also the reason why some features might be missing. Feel free to modify this PR or simply make suggestions, I'd be happy to improve it. This is also my first PR here, so sorry if I miss anything. 65 94 4 25 kirawi added the A-command label Contributor daedroza commented Would it be possible to implement a file browser with this methodology instead? https://github.com/stevearc/oil.nvim It uses a buffer/pop to navigate and edit files like you're inside a buffer. 14 Contributor gj1118 commented @drybalka this is awesome.. thanks for the effort. Can you please post a screenshot as to how it looks ? Basically I am interested to show how nested directories/files are being presented. Thanks Contributor Author drybalka commented Would it be possible to implement a file browser with this methodology instead? https://github.com/stevearc/oil.nvim It uses a buffer/pop to navigate and edit files like you're inside a buffer. I am not a maintainer of helix, but in my opinion this is rather a plugin functionality. First of all, it would be hard-ish to implement and therefore to maintain. Secondly, buffers are primarily used for text editing and one does not usually need to create/delete/rename files so much. 8 10 1 Contributor Author drybalka commented • edited @drybalka this is awesome.. thanks for the effort. Can you please post a screenshot as to how it looks ? Basically I am interested to show how nested directories/files are being presented. Thanks Well, the idea was to make it look and behave like the original telescope-file-browser, so you may look at the showcase video there (just without all pretty-niceness as this is just a proof-of-concept). As for a real screenshot it looks (quite bare bones) like this: In other words, both the picker and the preview (if a dir is selected) show the contents at depth 1, similar to how ls works. 11 3 Member archseer commented I kind of like just how simple this change is! It reuses existing UI components and allows exploring the file tree without adding any of the heavier features. 7 22 1 the-mikedavis reviewed View reviewed changes helix-term/src/ui/mod.rs Outdated Comment on lines 305 to 311 if let Ok(files) = directory_content { for file in files { if injector.push(file).is_err() { break; } } } Member the-mikedavis Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more. Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This style is used by the file picker because finding files is a potentially long-running iterator and we might use the injector to push some files asynchronously after a timeout. Since you've already collected the directory contents above you should pass that vec as the third argument to Picker::new 2 Member the-mikedavis Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more. Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Also I believe this function should return a result and pass up the error from directory_content. Currently if you fail to list the directory contents a blank picker opens 2 Contributor Author drybalka Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more. Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Great suggestions, simplified the code, thank you! helix-term/src/commands.rs Outdated Show resolved Hide resolved helix-term/src/ui/mod.rs Outdated Show resolved Hide resolved kirawi added the S-waiting-on-author label drybalka requested a review from the-mikedavis the-mikedavis added S-waiting-on-review and removed S-waiting-on-author labels zegervdv commented I think it would be useful to have the equivalent version of file_picker_in_current_buffer_directory. I often use this, only to realize I need to go one or more levels up. The file_browser would solve that perfectly :). It could be something like this: fn file_browser_in_current_buffer_directory(cx: &mut Context) { let doc_dir = doc!(cx.editor) .path() .and_then(|path| path.parent().map(|path| path.to_path_buf())); let path = match doc_dir { Some(path) => path, None => { cx.editor.set_error(\"current buffer has no path or parent\"); return; } }; if let Ok(picker) = ui::file_browser(path) { cx.push_layer(Box::new(overlaid(picker))); } } Contributor Author drybalka commented I think it would be useful to have the equivalent version of file_picker_in_current_buffer_directory. I often use this, only to realize I need to go one or more levels up. The file_browser would solve that perfectly :). I think this can even be the default behavior, thanks for suggesting! Not sure when I would even need to open a file browser in the project root. I deliberately wanted to keep this PR simple and feature-poor, but your suggestion is quite simple and I think it is worth it. 4 baldwindavid commented • edited @drybalka This is perfect with the change to the current buffer directory. It's similar to good ol' netrw for quick navigation and covers 95% of the needs for me. The more advanced stuff (copy, paste, create, delete, etc) can be covered by a mix of #11164, shell scripts, wezterm, and the yazi file explorer. Thanks for your work! 1 zegervdv commented @drybalka found a small issue when testing: if the cursor is on a binary file, the preview will mess up the view and leave random characters everywhere. Maybe binary files can be excluded from preview somehow? These files are normally excluded from the file_picker, so maybe this is a more general issue with the preview. Contributor Author drybalka commented • edited @drybalka found a small issue when testing: if the cursor is on a binary file, the preview will mess up the view and leave random characters everywhere. Maybe binary files can be excluded from preview somehow? These files are normally excluded from the file_picker, so maybe this is a more general issue with the preview. @zegervdv I am using the same file previewer as file_picker, so the problem is probably in the previewer itself. Although as far as I tested the usual .jpg, .pdf, and executables are all correctly previewed as . Maybe you're using some exotic file formats? 2 Contributor thomasaarholt commented It would be nice to add support for going straight to the root of the project if the current buffer hasn't been saved yet. I went to test the file browser functionality by escaping the file picker when calling hx ., but then it didn't work: Screen.Recording.2024-09-10.at.08.18.51.mov Contributor thomasaarholt commented Actually, I can't get this to work at all? Running on MacOS. Screen.Recording.2024-09-10.at.08.47.35.mov Here is the log from hx vvv . Contributor Author drybalka commented • edited Actually, I can't get this to work at all? Running on MacOS. Well, this is actually the behavior I get when using the command palette for the file_picker as well, I guess the palette is somehow buggy in this regard. I just mapped it to some keymap and tested like that. But anyway, even if you map it correctly and then try to open the file_browser after hx . then it will probably still would not work. The behavior is the same as file_picker and requires an opened document to get the current path where to open the browser. I guess it would make sense to default it to the current working directory. kirawi mentioned this pull request Allow file picker changing root to parent directory or subdirectory #11687 Closed L-Trump commented Actually, I can't get this to work at all? Running on MacOS. Screen.Recording.2024-09-10.at.08.47.35.mov Here is the log from hx vvv . Actually in current latest version of helix, no file picker can open from the command pallete (tried in nixos and archlinux). It seems like another bug. 1 summersz commented I have the same reported issue when trying to open the browser from the command palette. Works great when opened from a keymap though. I do have a couple of suggestions to consider Can directory names be appended with a '/' to distinguish them (like netrw) Can left and right keys be used to navigate down and up directories, respectively? ( I have gotten used to this in yazi and find it very intuitive) 5 m0ar commented So stoked to see this @drybalka! 🫶 Probably not worth it at this stage as merging the base feature is higher priority, but it'd be cool be be able to classify the entries with icons. Having a trailing slash on dirs makes sense, but hugely useful IMO to when something is linked, and being able to visually filter on filetypes. Kinda like lsd, which has a default set of unicode chars, but being able to opt in to using nerdfont glyphs for extra gloss. I could probably take a look at doing this when this is in if you aren't feeling it :) 3 drybalka force-pushed the add-file-browser branch from 5695166 to b743ff3 Compare Contributor Author drybalka commented @summersz Adding slash to dirs is a great idea, thank you! Using other keys in a picker is harder. The current design does not allow picker-specific keymaps, and even remapping the existing keys is not allowed yet. I will leave this idea for the future, as pickers refactor should come at some point. @m0ar I also really like the idea with icons! The file_browser picker shares a lot of code with the file_picker, so if the icons would work there then most probably they will also work here. I don't want to complicate this PR with icons at this point, but you may already try implementing them for the file_picker. 4 the-mikedavis reviewed View reviewed changes helix-term/src/ui/picker.rs Outdated Show resolved Hide resolved helix-term/src/ui/mod.rs Outdated Show resolved Hide resolved the-mikedavis added S-waiting-on-author and removed S-waiting-on-review labels drybalka force-pushed the add-file-browser branch from bb00c5a to 68b252a Compare Contributor thomasaarholt commented • edited To use this, you can add the following keybinding to your config (:config-open, and then after saving :config-reload): [keys.normal.space.space] f = \"file_browser\" Now, you can double-tap space and then press f to open the file browser. Note that you cannot open the file browser from the command palette due to #4508 (as discussed above). 1 42 hidden items Load more… drybalka force-pushed the add-file-browser branch from 127a588 to 52a9cef Compare Contributor Author drybalka commented • edited @the-mikedavis Thank you very much for the review, I really appreciate the insights! I added the styling for dirs, as you suggested, and also removed canonicalization and unnecessary is_dir syscalls. Now the type signatures are a bit polluted with (PathBuf, bool) tuples, but I just couldn't come up with a decent name for it (PathBufWithSomeMetadata or FileBrowserEntry are not better, I think). Hopefully this is fine. I also had to rebase to the newest master to get the styling code, but I did not modify any commits, only added the last one. Have a Merry Christmas! Contributor Axlefublr commented thank you for rebasing @drybalka! for us forkers that is very valuable :3 hope this PR will get merged into core anyway, though! 12 nik-rev mentioned this pull request Smooth recovery from invalid configs #12338 Closed nik-rev added a commit to nik-rev/helix that referenced this pull request patchy: auto-merge pull request helix-editor#11285 … 44f5515 `patchy` is a tool which makes it easy to declaratively manage personal forks by automatically merging pull requests. Check it out here: https://github.com/NikitaRevenco/patchy rockboynton pushed a commit to rockboynton/helix that referenced this pull request patchy: auto-merge pull request helix-editor#11285 … 2e968e4 `patchy` is a tool which makes it easy to declaratively manage personal forks by automatically merging pull requests. Check it out here: https://github.com/NikitaRevenco/patchy rockboynton pushed a commit to rockboynton/helix that referenced this pull request patchy: auto-merge pull request helix-editor#11285 … 958dcd0 `patchy` is a tool which makes it easy to declaratively manage personal forks by automatically merging pull requests. Check it out here: https://github.com/NikitaRevenco/patchy nik-rev added a commit to nik-rev/helix that referenced this pull request patchy: auto-merge pull request helix-editor#11285 … 23f5fb6 `patchy` is a tool which makes it easy to declaratively manage personal forks by automatically merging pull requests. Check it out here: https://github.com/NikitaRevenco/patchy nik-rev reviewed View reviewed changes helix-term/src/commands.rs ); return; } cx.editor.set_error( Contributor nik-rev • edited Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more. Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I wouldn't call this an error, honestly. For example I'm frequently doing hx . and then opening the file browser, since i am in a [scratch], everytime I do that it adds a red message to the statusline, so it looks like something went wrong, but really it's just fine I think that this call to set_error can just be removed 2 Contributor Author drybalka Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more. Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment To be fair, this function is called file_browser_in_current_buffer_directory, so this error is quite justified to inform the user that the [scratch] buffer does not have a dedicated directory. In my opinion, it would be confusing to remove this message for cases when something actually goes wrong. What you want to call after hx . is probably file_browser_in_current_directory or simply file_browser. Now I agree, that it is more convenient to have a single key combination that covers all file browsing needs in one go, and arguably file_browser_in_current_buffer_directory already does that for the price of showing you a somewhat-irritating-but-easily-ignorable error message. In an ideal world one would probably have to wait for when programmable config will be available in helix and code the desired behavior themself. 4 nyawox added a commit to nyawox/helix that referenced this pull request patchy: auto-merge pull request helix-editor#11285 … 6cdf2b2 `patchy` is a tool which makes it easy to declaratively manage personal forks by automatically merging pull requests. Check it out here: https://github.com/NikitaRevenco/patchy nik-rev added a commit to nik-rev/helix that referenced this pull request patchy: auto-merge pull request helix-editor#11285 … e22c144 `patchy` is a tool which makes it easy to declaratively manage personal forks by automatically merging pull requests. Check it out here: https://github.com/NikitaRevenco/patchy nik-rev added a commit to nik-rev/helix that referenced this pull request patchy: auto-merge pull request helix-editor#11285 … e7c5d6f `patchy` is a tool which makes it easy to declaratively manage personal forks by automatically merging pull requests. Check it out here: https://github.com/NikitaRevenco/patchy nik-rev added a commit to nik-rev/helix that referenced this pull request patchy: auto-merge pull request helix-editor#11285 … bd2d640 `patchy` is a tool which makes it easy to declaratively manage personal forks by automatically merging pull requests. Check it out here: https://github.com/NikitaRevenco/patchy the-mikedavis reviewed View reviewed changes helix-term/src/ui/mod.rs Outdated Show resolved Hide resolved helix-term/src/commands.rs Show resolved Hide resolved drybalka and others added 2 commits Normalize path in file browser 996ee93 fix: docs + add keybinds for file_explorer 11435df nik-rev mentioned this pull request feat: Picker titles #12520 Open civa commented Actually, I can't get this to work at all? Running on MacOS. Screen.Recording.2024-09-10.at.08.47.35.mov Here is the log from hx vvv . Same issue 4 Contributor nik-rev commented • edited Actually, I can't get this to work at all? Running on MacOS. Screen.Recording.2024-09-10.at.08.47.35.mov Here is the log from hx vvv . Same issue You can't open any pickers from the command palette, it isn't limited to just this PR. It's a known bug for a while and fixing it would require a large refactor. See #4508 1 the-mikedavis approved these changes View reviewed changes Hide details View details the-mikedavis merged commit 6b044ae into helix-editor:master 6 checks passed Contributor nik-rev commented It's merged!! Yayyyyyyy :D (small thing, but I noticed it's mapped to space + e but the command is called file_browser. maybe it's better to rename it to file_explorer as a mnemonic?) 29 4 Contributor Axlefublr commented NO FUCKING WAY FINALLY OMG 29 2 8 Member the-mikedavis commented It does look a bit like Vim's :Explore, \"explore\" could be a more fitting name 18 the-mikedavis added a commit that referenced this pull request Rename \"file browser\" => \"file explorer\" … d4ade40 Connects #11285 the-mikedavis changed the title Add file browser Add file explorer nik-rev pushed a commit to nik-rev/helix that referenced this pull request Add file browser (helix-editor#11285) 7ac8c34 rotmh commented • edited Hi, awesome and exciting! :) One thing, it seems not to ignore ignored files (from .gitignore, .ignore, etc.). Is that the desired behavior, a problem in my case, maybe it has been forgotten? Edit: just looked out the code and compared file_browser and file_picker the picker uses a walker with ignores, and the browser reads dir content, which I guess makes sense, as the walker is used for recursive file iteration... Well, I'll leave it here as I don't know what was the intentions. 6 Foxhunt mentioned this pull request Daily Hacker News 25-01-2025 Foxhunt/daily-hackernews#200 Open xueyuanl mentioned this pull request Daily Hacker News 25-01-2025 xueyuanl/daily-hackernews#1580 Open Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Reviewers nik-rev pascalkuthe KevinDanne RoloEdits ryanabx the-mikedavis Assignees No one assigned Labels A-command S-waiting-on-review Projects None yet Milestone No milestone Development Successfully merging this pull request may close these issues. 25 participants",
    "commentLink": "https://news.ycombinator.com/item?id=42818278",
    "commentBody": "File Explorer is merged to Helix editor (github.com/helix-editor)213 points by manusachi 18 hours agohidepastfavorite28 comments heldrida 8 hours agoI used to believe that a File explorer is crucial in my text editor; after using Helix for more than a year, I’ve discovered that is not required at all; space+f much faster use flow. I work with split terminal windows, where I have supporting windows to navigate the file system. Happy to try a file explorer once again, it’ll be interesting to see how I feel a year later. reply qudat 5 hours agoparentI ditched file explorer in neovim and just use fzf. I also ditched tabs and rely on fzf on the buffers. It’s a really powerful setup and cleans up the UI significantly reply TacticalCoder 6 hours agoparentprev> I’ve discovered that is not required at all; space+f much faster use flow. Some use \"fuzzy find\" on the filename, which is ultra fast too. I use something even faster IMO: typically I know at least some of the text of the source / test code file I want to go to. So I either use a function of the editor that can \"find usage\" or... I simply use ripgrep, integrated in my editor, and start typing text I know is in the file I want to visit. I still use, sometimes, a \"file explorer\" to find the file to open but it's not the most common. In a way using a file explorer is \"sort\": things are arranged in folders/directories. Fuzzy search or finding usages or ripgrep is \"search\". So, basically: \"search, don't sort\" reply skydhash 2 hours agorootparentA file explore is great only for browsing (to get a sense of the project) and managing files (moving renaming, especially in batch). Actually opening file is better with a finder (either filename or content) and a fuzzy filter. I use emacs and consult is a great package for that (the native features mostly present a buffer with the result). A step up from VSC is acting over a result buffer and have the changes reflected back to the normal buffer. Like renaming functions when LSP can’t help you. reply homebrewer 5 hours agorootparentprevYou're shadowbanned, mate, almost all your comments from at least the last couple of weeks are dead. reply bryanrasmussen 27 minutes agorootparentI don't think they're shadowbanned, just a lot of the comments have been banned the old-fashioned way. reply CaptainOfCoit 5 hours agorootparentprevProbably not the best venue, but if you read through the comments, kind of makes sense. \"Assume good faith\", \"Converse curiously\" and \"Eschew flamebait\" are part of the guidelines for commenting, for good reasons. reply Rehanzo 3 hours agorootparentprevIsn't space+f fuzzy find? reply jll29 13 hours agoprevI like the idea of doing browsing in folders containing files as navigating between edit buffers. I've been editing like that since the 1990s because that's how Emacs and XEmacs do it (M-x dired command). The ease and speed of just pressing RETURN on the .. menu to go up or on the folder name to open and look inside is unrivalled and much faster than using my mouse (and better for health avoids RSI). Would be cool for Helix to get Emacs-like directories-as-edit-buffers functionality! reply phplovesong 12 hours agoprevOil for nvim is THE best explorer i have used. Highly recommend reply cassepipe 51 minutes agoparentFor people to understand your enthusiasm I feel like you should try to explain why : It uses a buffer/pop to navigate and edit files like you're inside a buffer reply kombine 7 hours agoparentprevIt is the best explorer for me too. It blends into the rest of the vim so much so that you don't feel the difference between manipulating text and files. reply CaptainOfCoit 6 hours agorootparentI've been using vim/neovim for decades, and \"so that you don't feel the difference between manipulating text and files\" made no sense. Then I watched a video from the README (https://github.com/stevearc/oil.nvim) and yup, that looks amazing and makes so much sense. Thanks GP for mentioning it! reply marliechiller 10 hours agoparentprevyazi for me! reply atkailash 9 hours agorootparentI recently tried yazi after trying a few other cli file explorers and after I settle my current dot file redo I’m gonna dig into it some more reply pkulak 15 hours agoprevI was waiting for this to try switching… and now I’ve discovered oil.nvim and can never go back. reply zamalek 11 hours agoprevWhat I really want to see merged is the tokens in shell commands. dotnet is my day job, and the boilerplate (its luckily just namespace nowadays) gets repetitive and typo-prone. If I could just (approximately) `dotnet new class o $cwd`... Instead we have something like 3 closed PRs for this. reply archseer 5 hours agoparenthttps://github.com/helix-editor/helix/pull/12527 reply desdenova 7 hours agoparentprevCan't you just:dotnet new class o $PWD fg reply zamalek 2 hours agorootparentPWD is does hot refer to the directory that file I am working on is in. reply manusachi 18 hours agoprevOne of the long-awaited features was recently merged! reply Celeo 15 hours agoprevGreat news! When I'm familiar with the project space+F works great, but isn't great for discovery. reply timeon 18 hours agoprevI get that Helix is really fast (I also use it sometimes) but some of those screen-grabs could be made bit slower so one could see what is actually happening. Anyway I hope this will be soon included in release! (Even if current +f is also pretty good.) reply georgeck 12 hours agoparentI agree. Capturing the interaction as a movie (like .mov file) makes it really difficult to understand what the user is doing. e.g. What keystrokes did the user press to finish this interaction. I wish folks would post screen grabs with tools like https://asciinema.org/ this is what the helix-editor homepage uses to show the features. This is ideal for terminal apps. That said, I wish asciinema can also show the key strokes a an annotation with the ability for the viewer to pause on each keyboard interaction. reply abhinavk 15 hours agoparentprevThese are HTMLcontrols. You can right-click and set a speed. reply contingencies 17 hours agoprev [–] ... but can it read emails? Jamie Zawinski's Law: Every program attempts to expand until it can read mail. Those programs which cannot so expand are replaced by ones which can. reply chamomeal 14 hours agoparentThey’re working on a scheme-based plugin system, so… almost! reply miki123211 10 hours agoparentprev [–] And a corollary to this law: Except for programs embedded in applications that can already read mail (notably web apps) reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Helix editor has introduced a new file explorer feature, inspired by the telescope-file-browser, allowing users to navigate files like a file picker.",
      "Developed by contributor drybalka, the feature has been positively received, with feedback for enhancements such as directory slashes and icons.",
      "Some users on MacOS reported issues with the command palette, but the file explorer functions effectively with keybindings, and future updates may address .gitignore file exclusions."
    ],
    "commentSummary": [
      "Helix editor has introduced an integrated File Explorer, sparking discussions on its necessity versus faster alternatives like fuzzy finders and ripgrep. Users share personal preferences and experiences with file navigation tools and plugins for editors such as Vim and Emacs. The new feature has generated excitement among some users, while others debate the merits of various file management methods."
    ],
    "points": 213,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1737764900
  }
]
