[
  {
    "id": 41975047,
    "title": "We're forking Flutter",
    "originLink": "https://flutterfoundation.dev/blog/posts/we-are-forking-flutter-this-is-why/",
    "originBody": "We're forking Flutter. This is why. Matt Carroll • October 27, 2024 Over the years, Flutter has attracted millions of developers who built user interfaces across every platform. Flutter began as a UI toolkit for mobile - iOS and Android, only. Then Flutter added support for web. Finally, Flutter expanded to Mac, Windows, and Linux. Across this massive expansion of scope and responsibility, the Flutter team has only marginally increased its size. To help expand Flutter's available labor, and accelerate development, we're creating a fork of Flutter, called Flock. Flutter's labor shortage Let's do some back-of-the-napkin math to appreciate the Flutter team's labor shortage. How many Flutter developers exist in the world, today? My guess is that it's on the order of 1,000,000 developers. The real number is probably higher, but one million should be reasonably conservative. How large is the Flutter team, today? Google doesn't publish this information, but my guess is that the team is about 50 people strong. That's 50 people serving the needs of 1,000,000. Doing a little bit of division, that means that every single member of the Flutter team is responsible for the needs of 20,000 Flutter developers! That ratio is clearly unworkable for any semblance of customer support. A labor shortage can always be fixed through hiring. However, due to company-wide issues at Google, the Flutter team's head count was frozen circa 2023, and then earlier in 2024 we learned of a small number of layoffs. It seems that the team may now be expanding again, through outsourcing, but we're not likely to see the Flutter team double or quadruple its size any time soon. To make matters worse, Google's corporate re-focus on AI caused the Flutter team to de-prioritize all desktop platforms. As we speak, the Flutter team is in maintenance mode for 3 of its 6 supported platforms. Desktop is quite possibly the greatest untapped value for Flutter, but it's now mostly stagnant. The cost of limited labor Limited labor comes at a great cost for a toolkit that has rapidly expanded its user base, along with its overall scope. With so few developers to work on tickets, many tickets linger in the backlog. They can easily linger for years, if they're ever addressed at all. By the time a member of the Flutter team begins to investigate a ticket, the ticket might be years old. At that point, the Flutter team developer typically asks for further information from the person who filed the ticket. In my experience, when this happens to me, I've long since stopped working with the client who had the initial issue. I've written hundreds of thousands of lines of code since then. I often don't even remember filing the issue, let alone the obscure details related to the original issue. The team can't fix the bug without information from me, and it's been too long for me to provide information to the team. So the bug gets buried for a future developer to rediscover. Timing isn't just an issue for eventually root causing and fixing bugs. It's also a major product problem. Imagine that you're the engineering director, or CTO of a company whose next release is blocked by some Flutter bug. What do you do if the team won't work on that bug for 2 years? Well, if it's a serious bug for your company, then you stop using Flutter. You don't have a choice. You need to keep moving forward. Your team doesn't know how to work on the Flutter framework, and the Flutter framework team is either unresponsive, or at least completely non-committal towards a fix. Oh well - can't use Flutter any more. Flutter won't survive if these kinds of experiences become common. The Flutter community can help with labor Flutter has two very valuable qualities. First, it's open source, so any developer can see how any part of Flutter is implemented, and can even change it. Second, the Flutter framework is written in the same language as Flutter apps. Because of these two qualities, experienced Flutter app developers, and package developers can contribute to the Flutter framework. How many Flutter developers exist in the world today who are capable of contributing at a productive level to the Flutter framework? Conservatively, I would guess there are about 1,000 of them. In other words, there are at least 1,000 Flutter developers in the world who could conceivably be hired to the Flutter team, if the team wanted to hire that many developers. Remember that ratio of 1 Flutter team member per 20,000 developers? If every capable Flutter framework contributor in the world regularly contributed to Flutter, that ratio of 1:20,000 would drop to 1:1,000. That's still a big ratio, but it's much better than what it is now. Moreover, as more external contributors get comfortable submitting fixes and features to Flutter, they'll tend to help train others to do the same. Thus, the support ratio would continue to move in a better direction. Why not work directly with the Flutter team? If increased external contributions is the path to a better Flutter world, then why fork Flutter when everyone could just work directly with the Flutter team? It's a tempting proposition to setup a concerted effort to contribute directly to Flutter. After all, the Flutter team regularly touts the number of external contributions that it rolls into each release. According to the Flutter public relations effort, they'd love all those external contributions! But, sadly, trying to work with the Flutter team delivers a different reality. While some developers have had success working with the Flutter team, many other developers have found it frustrating, if not unworkable. There are, no doubt, a number of factors that contribute to this result. Different developers will experience different issues. But here are some of them: Limited review labor: The developers who don't have enough time to write code are the same developers tapped to review contributions. Therefore, it can take a long time for review or updates. The time crunch also seems to lend itself to contentious review conversations. Everything takes forever, and it always seems to be about non-critical details. Communication monoculture - most of the team seems to expect a certain way of communicating, which doesn't match the variety of personalities in the world. Thus, some people have an exceptionally difficult time navigating otherwise quick and simple conversations. The result of the aforementioned issues, and probably others that aren't listed, is that the total number of people who have ever contributed to the Flutter framework is currently less than 1,500. That number includes people who dropped by, one time, to fix a typo in a Dart Doc and then never contributed again. That's not the number of regular contributors who add significant value. Whatever your experience with contributions to Flutter, one has to critically assess why a team that loves external contributions has only managed to merge contributions from 1,500 developers over a span of nearly a decade. My humble suggestion is that it's because the inviting message of the PR team doesn't match the experience of actually pushing a change through the team's policies, developer availability, and technical culture. The only people who can change this reality are the people within the Flutter organization. However, most of those people don't actually think any of this is a problem. I know, because a number of them have expressed this to me, directly. There are a number of significant blind spots for the Flutter team, which largely revolve around the fact that members of the team have never been responsible for routinely delivering app features and fixes that are built upon Flutter. In other words, I believe there are blind spots because Flutter team members don't actually use Flutter. Thus, the urgency around many issues isn't appreciated, nor is the urgency and time cost associated with submitting fixes directly to Flutter as an external contributor. If the Flutter team doesn't recognize the contribution problem, and therefore won't take steps to address it, then what else can be done? That's where we find ourselves in this post, and in this effort. We've decided that the one thing we can do to help the labor issue is to fork Flutter. Introducing Flock Our fork of Flutter is called Flock. We describe Flock as \"Flutter+\". In other words, we do not want, or intend, to fork the Flutter community. Flock will remain constantly up to date with Flutter. Flock will add important bug fixes, and popular community features, which the Flutter team either can't, or won't implement. By forking Flutter, we get to decide what gets merged. We won't lower the quality bar, but by controlling merge decisions, we do gain the following opportunities: Recruit a much larger PR review team than the Flutter team. This means faster review times. Recruit PR reviewers who are ready to facilitate contributions, instead of merely tolerating them. This means support for a wider contributor audience. Optimize policies. E.g., don't blindly demand design docs and conference calls when they won't substantially add to the effectiveness of the task at hand. Use contribution successes to socially promote more contributions. We're all Flutter users - leverage team and company relationships to identify market priority. As Flock ships important bug fixes and features, the Flutter team can then choose to add those to Flutter, on their schedule. The community will no longer be limited by the Flutter team's availability, nor will the community need to beg the Flutter team to please accept a change. The Flutter team can use Flock's solutions, or not, but all Flock users will have access to them, eliminating your company and team's urgency and desperation. How you can get involved Flock, as the name implies, will only go as far as the community that supports it. We would love for you to get involved. Alpha test the fork Flock's first step is to mirror Flutter. This means automatically mirroring the master, beta, and stable branches, along with replicating all release tags. Additionally, once the framework is mirrored, Flock will need to automatically build and upload the engine, and make those engine binaries available to Flock users. As we work through the mirroring process, it would be a big help if you would try building your apps with Flock. You shouldn't see any difference between Flock and Flutter, and you can configure Flock with a tiny Flutter Version Manager (FVM) configuration. Check our instructions to get started. Become a reviewer Flock needs to recruit dozens of reviewers. Reviewers are responsible for enforcing a quality bar that's similar to Flutter's. This includes requiring descriptive class, method, and property names, effective Dart Docs, and appropriate tests. But we want reviewers to go even further than that. We don't just want to tolerate contributions - we want to facilitate them. Many of us have had the experience of getting a PR 90% to the finish line only to have a Flutter team reviewer declare that it can't merge until we do something that we don't know how to do. It's an awful experience, and we aim to avoid it with Flock. We want Flock reviewers who are willing to step in and help a contributor achieve the final 10% of the PR. This doesn't mean contributors get to be lazy. But if a contributor has done everything that he knows how to do, and the PR is close to complete, then we want the reviewer to step in and provide direction for the final 10%. This is how we educate contributors and ensure that the next PR is 100% complete. If you'd like to become a Flock reviewer, please reach out to us. Become a Lead Maintaining and extending a long-lived fork of Flutter requires some number of experts who direct specific areas of the project. For example, I'm initially stepping up at the Director of Flock, as well as the Framework Lead. Jesse Ezell has stepped up as the Engine Lead. We'd like to bring in a Flutter Tool lead, who directs extensions to the flutter CLI tool. We'd also like to break up the engine responsibilities with a Lead per platform: Android, iOS, Mac, Windows, Linux. If you'd like to direct efforts for an area of Flock, please reach out to us. Let's Flock together Let's shift Flutter into overdrive and help make it the universal UI toolkit it should have been. Flutter has the potential to outshine every alternative in the market. But it needs the community to Flock together to help get it there. Let's do this!",
    "commentLink": "https://news.ycombinator.com/item?id=41975047",
    "commentBody": "We're forking Flutter (flutterfoundation.dev)784 points by alexzeitler 23 hours agohidepastfavorite651 comments divan 22 hours ago> We describe Flock as \"Flutter+\". In other words, we do not want, or intend, to fork the Flutter community. Flock will remain constantly up to date with Flutter. That was the first fear when I saw the title - splitting community and having two incompatible versions. Good to see it addressed in the post. The second was just a fear of how it would complicate the development process, but it seems to be a drop-in replacement (just configuring FVM - Flutter Version Manager): Configure .fvmrc to use Flock: { \"flutter\": \"master\", \"flutterUrl\": \"https://github.com/Flutter-Foundation/flutter.git\" } Flutter is the best thing that happened to UI development since Qt. Most people don't realize how many apps written in Flutter they use daily, simply because it's impossible to tell. And the frustration described in the post is felt by many CTOs and developers. Especially those who use Flutter for desktop and web. Flutter provides an amazing experience for desktop apps, and precisely because of that, it feels so frustrating when you stumble upon some stupid bug that has been open for a year or two and never gets prioritized. Usually, it's nothing critical, but still requires workarounds and wasting time. I don't know, the idea of Flock sounds good, the main question is engaging the community. Hopefully, the author (who seem to be an ex-Flutter team member himself) have a good grasp on the state of the community. Wishing luck to the project and going to keep an eye on the progress. reply deergomoo 21 hours agoparent> Most people don't realize how many apps written in Flutter they use daily, simply because it's impossible to tell You can't be serious. Maybe on Android, but on other platforms—especially iOS—they stick out like a sore thumb. A number of them just look like Material Design Android apps awkwardly transplanted over, but I know that's down to the developer so I won't hold that against Flutter. But scrolling through the Flutter showcase[0] and calling those apps up to look at screenshots on the App Store—none of them look like native iOS apps. They don't look bad (mostly), but they don't fit in. Don't get me wrong, I don't expect anyone else to care or even notice. But for those of us that do care, you can absolutely tell. [0] https://flutter.dev/showcase reply jamil7 5 hours agorootparentI noticed the SNCF Connect app for France's rail was using it in their showcase which explains why the scroll views are so weird and busted feeling. They still stutter on a new iPhone. As a native iOS developer I'm somewhat biased and a lot of people likely don't care but it still feels off to me even for apps that use a complete custom design. I hope they can optmise away those kind of issues more as it's IMO always better to have alternatives. Side note: Is anyone using Flutter just for Android? I'm kind of tempted to try this as dealing with the Android build system and packaging is bit of a pain, despite Kotlin and Compose being quite nice. reply insin 4 hours agorootparentYou still have to deal with the Android build system, `flutter new` just generates initial config for it and all the other necessary metadata (and that config can change between versions - I ended up manually diffing the generated stuff against a new app every time I upgraded). At some point I made the mistake of not touching the code for too long and upgrading Android Studio when suggested, and I was never able to (find the time needed to) get that app working again. reply mgbmtl 50 minutes agorootparentI did that too. I have a small app I use for my own stuff and notifications. I wrote it 4-5 years ago with an early version of Flutter, and didn't bother updating for a very long time. I'm a PHP/JS dev, I barely understand Flutter, but it was easy enough to figure out. I copy-pasted the errors in ChatGPT and it fixed most of the code for me. That rarely ever works for me, but it did for my flutter app. My biggest annoyance was finding how to send push notifications from my server using curl. I had to write a small wrapper to generate the Oauth keys, but that's just Google/Firebase, not Flutter. reply jamil7 4 hours agorootparentprevAh gotcha, I thought it was papered over a bit more with Flutter's tooling. Good to know. reply divan 21 hours agorootparentprevSure, but not every cross-platform app developer wants their app to look \"native to iOS\". Especially if you want your app to look the same on all platforms and/or have some creative design. reply deergomoo 21 hours agorootparentI can appreciate that as a developer, but as a user I don’t particularly care that some company wants to ship identical looking apps on every device under the sun. I would prefer the apps I use to work and behave in a consistent way, using the same platform idioms I am used to. The software available and how it works was a large part of the reason I chose the platform I did. Of course, I recognise that, a. most people just don’t care about platform idioms, and b. the choice is often a non-native app or no app at all. reply brewii 20 hours agorootparentPretty sure most large succesful apps have their own UI and UI design teams. Cant remember the last time i saw anything cupertino in an app. Even Apples own 'Home' app only loosely use cupertino. Id say the most noticable effect is the bottom modal sheet slide up effect. on ios the original screen animates into the background a little bit. Apps that dont implement this can be spotted but thats not unique to flutter at all and flutter even offers a pretty good cupertino scaffold package that does this animation. reply dwaite 11 hours agorootparent\"cupertino\" is the name of a Flutter widget set, not of the native platform controls or L&F. Few Flutter apps are going to use cupertino because the whole goal of using Flutter is to create cross platform codebases to save development effort. To use an alternative widget set per platform is a huge amount of additional work, and having a cupertino app running on Android is even more of a sore thumb than a material app on iOS. reply seanw444 2 hours agorootparent> and having a cupertino app running on Android is even more of a sore thumb than a material app on iOS. That is a bizarre fact. reply jwells89 20 hours agorootparentprevThe two things that stick out the most to me are navigation behavior and text fields. Cross platform frameworks seldom get either right, with react native being particularly bad on the navigation front. There are variances between Apple’s apps but they’re all using some combination of UIKit and SwiftUI regardless which limits how “wrong” they can be. reply deergomoo 20 hours agorootparentprevYou’re completely right, but that doesn’t change the fact that I think it’s a bad idea. For a company that used to care so much about user experience, Apple has been throwing a lot of it out the window in recent years. reply satvikpendem 18 hours agorootparentprevOnly on HN do I hear anyone talking about platform native controls versus unified UI across devices. I have not once heard of such a complaint in the real world and indeed, I have more often heard users wanting a unified UI over one that changes with each platform, simply because these days we have multiple devices where we expect apps to work the same. reply pastage 18 hours agorootparentYou have to listen to the details, like \"where is the back button\", \"Why does text work differently\". There are some people that understand that it is different from the standard, but most just get annoyed. The discussion here is from a technical perspective why it is better, conformity of the app on different platforms vs standard behavior between different apps. There are very few people that care about the technical reasons why, but when you talk about the annoyances it is easier. reply milch 18 hours agorootparentI'd also like to meet all these mythical users that have devices from every platform and want all their apps to look the same across all platforms. 99% of people I know IRL are in one ecosystem, with the exception of some that have an iPhone/iPad + Windows PC reply satvikpendem 9 hours agorootparentIn a previous company when we were looking at precisely this problem, building out multiple platform support, we did UX studies where this question in particular came up. The vast majority of people said they wanted apps to act consistently across devices. Most people on the planet use Windows with Android, statistically speaking, which are not similar devices at all, at least on Apple platforms you can use Swift for desktop and phone but not so for Windows and Android, so you have to make a choice at that point. reply Someone 8 hours agorootparent> The vast majority of people said they wanted apps to act consistently across devices You shouldn’t trust what users say they want. It many times doesn’t correlate with what they want. Certainly, I would try and figure out what they mean with “act consistently”. My suspicion (for which I have zero evidence) is that they’re more talking about high-level similarity than about nitty-gritty details such as how many milliseconds to hold your finger down to select and edit text, how scrolling works, etc. Also, I suspect they want devices to act constantly across apps, too: copy-paste should work, text editing should behave identically, sharing controls should be the same, etc. In the current world (and, I think, in any world where there is competition in any form between platforms), they cannot get both, so then, it boils down to what is more important. For me, that’s platform consistency. For example, I find it easier to get used to text editing being different on a phone and on a laptop than to get used to a zillion minor inconveniences/annoyances in typing and editing behavior between apps. I also find it less of a problem if text and emoji look slightly different on a different device than when they look slightly different between apps. reply kibwen 1 hour agorootparent> You shouldn’t trust what users say they want. How should I interpret the following four paragraphs where you say what you want? reply maeil 14 hours agorootparentprevEver since the release of the M1, the number of Android users who now use Macbooks as their laptop has sharply increased. Even moreso for work purposes. reply basilgohar 16 hours agorootparentprevIt's not the end users that care about the uniformity but the fact the corporate design team wants there to be uniformity across all their platforms they support. This is part of branding and user experience. I'm not arguing for or against, just stating that is where the push for this comes from. It would make support, for example, easier, if all versions of your app user experience were similar. reply robertlagrant 9 hours agorootparentI'm not quite sure why this was downvoted. It's true. reply satvikpendem 9 hours agorootparentIt's because while it's true on the company side, this comment is making it seem as if the company is doing this against their users' will, while in reality, most users genuinely want a unified experience across platforms for their apps, as seen by some of the comments here. reply robertlagrant 4 hours agorootparentIt doesn't seem to mention it being done against anyone's will. It's definitely true that a company can save money by only needing to maintain one set of help instruction screenshots, for example. reply nl 14 hours agorootparentprevI use MacOS, iOS and desktop Linux regularly and much prefer apps that act the same on them all. Maybe 40% of the developers I know use MacOS + Android. reply zdragnar 15 hours agorootparentprevHello! I use three platforms on a daily basis. I vastly prefer my apps to be reasonably consistent with themselves rather than trying too hard to adhere to the platform. My wife is on two. Most of my friends are on either two or three. In fact, I'm pretty sure my parents and a few coworkers are the only people I know who are exclusively on one platform. Many (most?) people don't have a clue as to what the particulars of any given platform even are. They know how to get around in each app they use, and maybe the web browser, and that's it. Lots of gestures go completely unused. reply naasking 4 hours agorootparentprev> I'd also like to meet all these mythical users that have devices from every platform and want all their apps to look the same across all platforms. It's not even a single person that has them, but I think we have all had the experience of family or friends that need assistance with something but they have a different type of phone whose organization and workflow is completely different because it's native. You literally can't help in this scenario without having physical access to the device. reply satvikpendem 9 hours agorootparentprevI've genuinely never heard of such annoyances from actual users (at least not from most users, who aren't power users, and even then, not even from the power users), and most cross platform frameworks already hook into the native APIs like for the back button, so it'd work the same either way. reply CJefferson 13 hours agorootparentprevOut of interest, do you know many seriously vision-impared people? Windows, and Mac OS X in particular, have quite good support for accessibility if you use their built-in GUI systems, and unified UIs are often (not always, vscode and chrome are quite good, for example), very bad, sometimes just a black square as far as accessibility goes. reply satvikpendem 9 hours agorootparentMany cross platform frameworks have good accessibility support, not sure what that has to do with native platforms, as the cross platform solutions simply hook into the native platform accessibility APIs anyway. https://docs.flutter.dev/ui/accessibility-and-internationali... reply CJefferson 7 hours agorootparentI didn't know flutter had such good support, and I'm very happy to hear about it! Unfortunately, most cross-platform frameworks have awful accessibility support, I've looked at various in the past and just found failure after failure. I am now going to look harder at flutter. reply amelius 7 hours agorootparentprevDo you hear people complain that __websites__ have different aesthetics? I used to share your opinion but since the web I think it is great that designers can have original designs and I started to worry about more important things. reply deergomoo 18 minutes agorootparentThe web is interesting because we never had rich components provided by the browser to begin with. Web applications have always had to draw everything but the most basic inputs themselves, and the design of web apps has evolved accordingly. In an alternate universe, where web apps were embraced by the platform from the start rather than something people had to shoehorn into a document sharing mechanism, and we had rich components built-in, then yes I would probably be annoyed if people insisted on rolling their own versions. After all, I am annoyed when people use a span with a click handler instead of a proper anchor tag, because it usually breaks middle clicking or opening a link in a new tab—a platform convention I have internalised and expect to work. Note this only applies to web apps, not sites. Much like how I do not have any problem that magazines don’t share a common layout. reply fauigerzigerk 6 hours agorootparentprevI agree. What doesn't get enough attention in these often dogmatic debates is that there are some native conventions that are hugely important for usability while others are mostly irrelevant. E.g, I have to use a Java app on the Mac that uses Ctrl+V rather than Cmd+V for paste as well as other Windows/Linux keyboard conventions. This is extremely jarring. Web apps never do that. Browsers are pretty good at using native conventions where it matters by default. Of course web devs sometimes go out of their way to vandalise the browser's perfectly good defaults - e.g. by overriding scrolling behaviour. Surprisingly, some of Apple's own native apps (such as Numbers) break platform conventions in ways that makes the app extremely inconvenient to use. reply dangus 5 hours agorootparentprevI think the problem with the same app following native app idioms is that now the support and instructions behind that app will have to be different on every platform. E.g., if you have Instagram on your iPhone, an Android user won’t be able to tell you “just click on this, this, then this to change your XYZ setting” because it will be in a different place than the Android app if developers follow native conventions 100% of the time. The fact that Spotify or Instagram or any of those other platform-agnostic apps look and function the same on every platform is a huge benefit to practical usability. I think the only time when nativeness matters is when you have an app that’s doing stuff that’s closer to being “low level” to operating system features. For example, an app that performs file system management, I don’t want that to have the exact same UI on Mac, Windows, and Linux, because those platforms have different conventions for where things go and how files are represented. reply knifie_spoonie 19 hours agorootparentprev> Of course, I recognise that, a. most people just don’t care about platform idioms This is the big one. HN commenters are not representative of the average user. You'd have to specifically point out the differences for them to even notice, and even then they simply don't care. reply int_19h 19 hours agorootparentCasual users don't care about the app \"looking wrong\". They do care about the app not working like other apps on their phone, though. reply drdaeman 17 hours agorootparentCasual users do care about app behaving wrong, though. If you can't copy or paste things, or if the navigation is backwards, or if the calendar looks weird etc etc - it all causes some minor frustrations, when things don't behave as user wants them to behave. They don't know what \"native\" means, obviously - they don't have that knowledge. They just know crappy apps from well-behaving apps, because they have a frame of reference (vendor-supplied native apps). reply satvikpendem 9 hours agorootparentIs there a concrete example of this? I still only hear this on HN where some mythical user gets annoyed about copying and pasting (most apps don't allow that, even, like TikTok or Instagram, which are the apps where most users spend the most time). Like the sibling commenter, I only have seen whether the app does what they want or it doesn't, most don't notice any annoyances unless they're really looking for them, which they're not. reply knifie_spoonie 18 hours agorootparentprevI've not found this to be the case at all. They only seem to care whether the app does what they want or not. Granted this is my personal experience, I can't say this is the case for every single user out there. reply ToucanLoucan 20 hours agorootparentprev> I would prefer the apps I use to work and behave in a consistent way, using the same platform idioms I am used to. The software available and how it works was a large part of the reason I chose the platform I did. There are DOZENS of us!!! I try my absolute best to find apps that use the native Apple language, both design and code. I can't stand these framework apps. I will Pepsi challenge this with anyone who asks, I can smell a framework app. reply dwaite 11 hours agorootparent> I will Pepsi challenge this with anyone who asks, I can smell a framework app. The platforms ship with things like keyboard shortcuts for navigation and text entry, minimal accessibility features like screen reading of text and navigation, common idioms like drag and drop and the clipboard - none of which are typically handled by cross-platform widget frameworks by default. Only gigantic projects like Chrome and VS Code will take on the effort of (partially) reimplementing these in their codebases to match platform behavior. reply satvikpendem 9 hours agorootparentprevDo you only use Apple devices? That may be why, because statistically most of the world's population uses Windows and Android where there really isn't a concept of \"native\" because they each have a few different UI frameworks. reply stuaxo 8 hours agorootparentprevThere was a time it looked like cross platform GUI toolkits would get us there, but things went in reverse. reply kelnos 52 minutes agorootparentprevAs an on-again off-again (currently off-again) cross-platform app developer, I always want my app to look like it fits in with the rest of the apps on whatever platform it's deployed to. As a user, I hate it when some app developer thinks they're cute and wants to skin an app differently or draw their own ad-hoc widgets. reply pasc1878 21 hours agorootparentprevThe trouble with this is most users only use one platform and so don't care. They just see the app as badly implemented as it does not match other apps on the platform. reply divan 20 hours agorootparentWhat kind of users/apps are these? I'm genuinely curious about this, actually. I have a couple of apps with 100K+ installs, and I talk to people who are actual users from time to time. I have never ever heard the claim that they don't like UI because it doesn't feel native. Like ever. But I can imagine that in some niches/demographics/app types it might be different. Can you share some evidence or explain how you built the understanding that users dislike non-native UIs? reply Eisenstein 11 hours agorootparentA lot of people try an app and stop using it. Talking to active users seems you are missing the people who don't like it enough to stop using it. reply satvikpendem 9 hours agorootparentI've built apps with pretty high retention rate so even if they're annoyed, it doesn't seem like they really stop using them. Mostly though, based on user feedback we collected, not once have I heard anything about UI complaints. It really feels like this is a common refrain on HN about what HNers think happens in theory versus what actually happens in reality wrt user preferences. reply Eisenstein 8 hours agorootparentNote, this has nothing to do with your app or this specific discussed GUI, but this is something I have noticed over time... I think that what is commonly seen by devs as complaining and being overly concerned with details that don't matter is instead a some of the time advocacy for doing things the right way for the sake of it. It is totally possible to build things that meet the bare minimum requirements for user retention and for people to not complain, but that doesn't mean that it is optimal. You can build Soviet style housing blocks that people live in that are completely functional and that no one will have any problem with, but the quality of life is degraded as opposed to what could have been built. It can be tough seeing through the grumpy nerd 'I want it my way because my way is best' and finding what is actually 'this is a good thing that we should be doing, and if we did there would be a marked improvement for everyone', but I would not forsake one due to the other. reply divan 9 hours agorootparentprevI think it may be true only for the apps that are easily replaceable. reply justmarc 21 hours agorootparentprevWhy would an app not looking like a native app automatically be seen as badly implemented? reply jwells89 20 hours agorootparentNot all non-native-looking apps are badly implemented, but a huge number of apps that use cross-platform frameworks do so primarily as a means to cut costs because the goal is to make development as cheap as possible, and that shows in other aspects of these apps too. This creates an association between cheap/lazy apps and cross platform UI frameworks. It’s kind of like the difference between VS Code and MS Teams. Same company, same underlying technology (Electron), but Code is good while Teams is awful because MS invests so much more in Code. Even so, Teams-type apps are what tends to come to mind when people think of Electron apps because those are so much more common. reply Anon1096 14 hours agorootparentYou could not be further from the truth regarding the size of the Teams vs VS Code teams. Teams has multiple times more developers, it's not a problem of funding that makes it suck. reply justmarc 5 hours agorootparentSometimes the bigger the team, the messier it gets and quality suffers greatly. reply pasc1878 10 hours agorootparentprevIn my case there is another reason - if it looks cross platform it is probably taking up more resources than a native app. reply myko 19 hours agorootparentprevI used to think this, now I think nobody cares except people \"in the know\" Every time I've ever mentioned it to anyone they really didn't seem to understand or care reply satvikpendem 9 hours agorootparentIt really feels like an \"HN bubble\" type argument, I've literally never heard it outside of HN. reply oneplane 20 hours agorootparentprevIn those cases, those app developers aren't doing cross-platform development, but are just trying to dump their app in as many places as possible. In such a case, why not just make a PWA and call it a day? Don't need an app if you aren't going to go native, especially with most apps being either content readers or CRUD apps anyway. In most cases, I personally rarely want an app, and I'll use a website unless they happen to have created a great native experience. And since most apps don't, I don't use them, and I don't have a lot of apps as a result. reply ascorbic 19 hours agorootparentBecause most users have no idea what a pwa is or how to install it. They expect to be able to go to the app store and install an app. reply oneplane 3 hours agorootparentThat is exactly the reply I was hoping for. Because that brings us straight back to the native issue; users also expect the app to work in a way that is comfortable and known to them. reply mirzap 8 hours agorootparentprevDevelopers who think that way have no clue what their users want. Most iOS users want a native experience, not something else. When I see an app that doesn't have that, I quickly lose interest. Having the app look the same on all platforms is a dumb idea to start with. reply Kiro 4 hours agorootparentFunny how you are calling people clueless while being confidently incorrect. You're in absolute minority. reply amelius 6 hours agorootparentprevAre these the same users that prefer websites without CSS? reply dwaite 11 hours agorootparentprev> but I know that's down to the developer so I won't hold that against Flutter. That is absolutely an issue with Flutter, which throws away the underlying platform UX and draws to a canvas. It gives you relatively few tools to target \"native look\" controls for each platform without maintaining multiple UIs in parallel composed of entirely different widgets. reply satvikpendem 9 hours agorootparentTypically people just make the app look and work the same in all platforms, like how you might expect a responsive web app to be like; no one is implementing Notion with a platform specific UI, for example, using Apple stuff on Apple devices or Windows stuff on Windows devices. reply nottorp 10 hours agorootparentprev> the underlying platform UX and draws to a canvas Frame buffer? Canvas is the javascript term. reply tauntz 5 hours agorootparentIt's not specific nor limited to javascript. The term \"canvas\" in this context is much older and seems to be used across many platforms. Random examples from the (desktop) Java/Android/iOS world where the same semantics is used: https://docs.oracle.com/javase/8/docs/api/java/awt/Canvas.ht... https://developer.android.com/reference/android/graphics/Can... https://developer.apple.com/documentation/SwiftUI/Canvas reply Nekit1234007 7 hours agorootparentprevFlutter uses canvas when targeting the web. (At least it was the last time I looked at it years ago) reply nottorp 7 hours agorootparentCanvas is the javascript name for writing directly to a frame buffer :) There is a world outside the browser. For now. reply divan 21 hours agorootparentprevFlutter showcase is a disgrace, to be honest. Doesn't represent the actual landscape even remotely. reply 8338550bff96 20 hours agorootparentThat is good to hear and hopefully gets fixed. It has been some years now, but I did use flutter for some toy projects and liked using it - I am used to using ReactJS I found there was a lot of transitive knowledge coming from that background. Ultimately what dissuaded me from pursuing it for any bigger projects is a lack of examples of great looking production apps with complex requirements. When I was looking, I was seeing a lot of CRUD apps - display a form, click an upload button, ect. This was probably 2018/2019 when I was taking a serious look at it. All of that is to say: Quality app showcases do matter for whether or not devs will trust a solution enough to pull the trigger. reply divan 20 hours agorootparentI don't know how to fix the showcase problem. With web apps, it's easy – just scan HTML, deduce what framework is used, and add it to your showcase list. With Flutter mobile/desktop apps it's not possible. And there are zero incentives for developers to send their apps for screenshots. You just ship the release and move on to the next tasks. I also discovered Flutter in 2018, and it kind brought joy into UI programming for me. I rewrote all web apps I had with Flutter and published a bunch of mobile apps, but the coolest thing is that I routinely make apps for my own (or my team's) use. For me, Flutter lowered the costs of UI app development so much that I could just spend a weekend making some neat, small, practical tool for my own needs. I really love making these \"private\" apps – no need to care about all corner cases for all users, no need to polish design for all screen sizes/font sizes. Don't even need to be released (TestFlight at max, if doing it for a team). It's just fun, and I wish more developers could have the same experience. reply nodamage 13 minutes agorootparent> With web apps, it's easy – just scan HTML, deduce what framework is used, and add it to your showcase list. With Flutter mobile/desktop apps it's not possible. Seems like it is: https://play.google.com/store/apps/details?id=com.fluttersha... reply binoct 20 hours agorootparentprev> I don't know how to fix the showcase problem. This is trivial for any team that provides a framework/platform and spends any amount of time engaged with their customers - you know who your prestige customers are and you show off examples of the products they have built with it. The state of the showcase is just reinforcing the points made by the OP. reply eternityforest 44 minutes agorootparentprevOn non-mac platforms material is already everywhere so they mostly don't look too out of place. reply zaphirplane 8 hours agorootparentprevOver time I have become convinced that the, fit in argument is not something that actual users care for. The sometime subjective aesthetic reasoning doesn’t mean the application can’t succeed. office ribbon is an example reply graemep 8 hours agorootparentI think you are correct, but I think your example is a bad one. Most people do not have a realistic (their employer will use it at work, it even occurs to them that they could use something else) alternative to MS Office so will use it whatever MS does. reply signal11 7 hours agorootparentprevA lot of users do care for fit and finish, but stick around with crappy UI because it’s not worth rage-quitting over. They do move when better UI + same-ish functionality comes along. The one sort-of exception to this is Enterprise Sales, where the people buying the software don’t always use it. But even there, corporate purchasers do get flak in annual reviews / feedback cycles for especially crappy enterprise software — so even there, especially crappy UI will catch up with you. reply lawgimenez 18 hours agorootparentprevOn Android you can also tell the way the list scrolls. Especially on devices, 2-3 years older. reply deciplex 18 hours agorootparentIf you're referring to the increased scroll speed with two fingers issue, that was patched within the last year. reply lawgimenez 18 hours agorootparentUnfortunately, not talking about that. reply Capricorn2481 12 hours agorootparentDon't elaborate or anything. reply satvikpendem 9 hours agorootparentI think they're referring to the extra frame of latency when scrolling on Android. But then again, I've literally never heard anyone complain about that, I honestly don't think anyone notices, I believe the GitHub issue had to use a slow motion camera to even capture the issue. reply tjpnz 8 hours agorootparentYou'll notice it on higher refresh rates. reply izacus 7 hours agorootparentprevSorry, but do you really develop your software as a professional engineer based on \"I heard X from few people\"? With all the bias and filtering your brain does for this kind of \"metric\"? reply p_l 17 hours agorootparentprevJust like a lot of the \"web application packaged as mobile app\" frameworks did in the past, except with ugly iOS styling that also completely mismatched the expected navigational patterns. What's worse, there are new applications with custom look and feel that do the same navigational sins today reply 946789987649 9 hours agorootparentprevSounds a bit like plastic surgery... you only notice the bad ones. reply m3talsmith 6 hours agorootparentprevThe developer could use the cupertino framework for iOS releases, but usually choose not to. reply jklinger410 4 hours agorootparentprev> You can't be serious. Please keep your comments civil reply ForHackernews 20 hours agorootparentprevI miss the days when applications had their own look and feel. If your idea of great design is identical rectangles to all the other apps, I guess Winforms solved that use case 20 years go. reply deergomoo 20 hours agorootparentIt’s perfectly possible to still have a unique look and feel without completely forsaking all the platform affordances. You get a lot of stuff for free or very cheap building using the platform toolkits: stuff that people internalise and expect to work without even realising it. I think it’s a bad idea to throw that all away. reply ur-whale 20 hours agorootparentprev> but they don't fit in. You seem to say this like it's a bad thing? reply deergomoo 20 hours agorootparentI think it is, personally. Mostly because common UI appearance and paradigms makes software easier to use, especially for non-technical users. But in this specific instance I was referring to OP’s claim that it’s impossible to identify many Flutter apps, which I dispute. reply satvikpendem 9 hours agorootparentI think people might be talking past each other on this issue. If you are on one platform only, then yes having apps behave the same is nice for muscle memory. However, if you're using multiple devices and platforms, then I want Slack to work the same regardless of device, I don't want iOS native navigation on the iPhone and Windows native navigation on the laptop. reply heavyset_go 19 hours agoparentprev> Flutter is the best thing that happened to UI development since Qt. Most people don't realize how many apps written in Flutter they use daily, simply because it's impossible to tell. I was a Flutter early adopter going on for like 7 years ago now, and Flutter has its place, but I don't know if I could repeat your sentiment with a straight face. Especially when comparing it to Qt. reply nox101 20 hours agoparentprevflutter's custom canvas render on web means so much of the web stops working or is slow. type anything non ASCII like an emoji or CKJ and eat while it downloads a font. No other pages do this. Text fields are missing all the standard context menu options like define, translate, etc... Things that would be selectable on any other page are not, etc.... reply chrismorgan 13 hours agorootparentIf you care about the web at all as a target, you must not use Flutter. It’s awful. They used to have a DOM renderer which you could use and everything would be fine, but apparently no one used it because it wasn’t perfect, and they’ve recently deprecated it and will remove it sooner or later—they’re doubling down on the pure-canvas direction where it’s completely impossible to produce a good result. And I do mean impossible, design limitations of the web platform that in some cases fundamentally cannot be relaxed and in others are very unlikely ever to be. With this direction, scrolling will never be good, text rendering will never be perfect, input and manipulation will never be acceptable, links will never work properly. reply satvikpendem 9 hours agorootparentFlutter Web is for web apps, not web sites, so much of those concerns don't necessarily apply. And it's not \"impossible\" simply because Chrome itself runs on Skia which until recently Flutter did too, so clearly they were able to implement scrolling at least one time correctly. reply chrismorgan 8 hours agorootparent> Flutter Web is for web apps, not web sites When you want to draw this distinction: most web apps will still suffer heavily for many users if they use Flutter. To begin with, few web apps don’t use text, scrolling, or form fields. Games are almost the only thing that may not suffer, or only barely suffer. But beyond that: well, that’s what they say its purpose is, but at least two of the three times I’ve encountered Flutter in the wild on the web, it was inappropriate, and frustrating; regular DOM should certainly have been used. (The third, I’ve forgotten what it was. It was probably similarly inappropriate.) Skia is not the bottleneck. The web platform is. Scrolling is limited on two counts: ① what browsers expose in events is insufficient to match the native implementation (which varies by platform) in scrolling amounts, overscroll behaviour, and related things; and ② the browser is a compositor, and your code will never get access to that layer, because it’s way too deep in performance-, security- and implementation-detail–land, so you’ll always be stuck at least sometimes at least one frame behind “native”, and janky. reply divan 6 hours agorootparentprevNot sure why it's downvoted, I think it's quite an important distinction to make. I've heard people saying \"Flutter is bad for web because it's not indexable by Google\". And reply \"do you expect your app - like a food delivery app - to be indexable by Google\" if it's run on the web? reply account42 5 hours agorootparentExisting food delivery apps are not only indexable by google but actively make sure to spam the top google results for all possible food related searches. You couldn't have choosen a better example to disprove your argument if you tried. reply divan 5 hours agorootparentI think you're talking about AdWords contextual ads paid by the food delivery companies, not about Google indexing apps. reply account42 4 hours agorootparentYou may think that but that doesn't make it reality. I think you are grasping at straws because you know you are wrong. reply wruza 17 hours agorootparentprevThis. They can have all the deviations they want, but “input core” must be native. If a framework ignores it, users will notice and frown upon it immediately. When flutter came out publicly, first I thought no way it can get away with custom everything. But it turned out some developers don’t care about that at all. reply divan 9 hours agorootparentI disagree with this so much for two reasons: 1. I shipped more than five Flutter web apps with actual users for a couple of years, and it's been a great experience so far. 2. \"Native\" web core should burn in hell, and I hope Wasm will finally contribute to it. Amount of developers who do not realize that \"native web\" is a typesetting engine from 80s with a pile of hacks on top of it, is too large to fight the opinion, of course. And yet, as a developer, I care about using the right tool for the right task, and no amount of browser engine optimization can change the fact that XML-based markup language is not the right tool for modern performant cross-platforms UIs. reply wruza 6 hours agorootparentNative doesn’t mean web. I meant just an input as it works everywhere on a platform, input core. Partially agree on the web sentiment, it just lacks a proper model, both positioning and styling, for what we call apps. reply 1oooqooq 15 hours agorootparentprevyou know that zero texting apps nowadays use native input anyway right? even native apps will implement their own input and it's always awful, but the pm needs those style previews... (you're still right thought) reply wruza 14 hours agorootparentChecked my whatsapp and tg, both use absolutely native inputs. The selection handles & menu, the hold-spacebar movement, the hold-to-magnify feature on ios is the same as everywhere else. If that’s not native, they did a great job for nothing. Too lazy to check on android rn, but I recently worked with the apps/chats on it and entered text, it didn’t feel different. reply saagarjha 9 hours agorootparentIn fact the input field is one of the few native UI surfaces in the Telegram app. reply 1oooqooq 7 hours agorootparentprevit's not a native widget. you can type styles on whatsapp which is not possible on native input. whatsapp is the poster child of this technique btw. reply wruza 6 hours agorootparentStyles are possible with native text view, otherwise we couldn’t have fonts and colors and contenteditable and selection within . I think you mistake “input core” with “web input element” here. The core I’m talking about is a text cell with characters on it that has no concept of own border. Controls like NSTextField, Win32.TextBox, GtkEntry - they all have platform text input (and output) system underneath. In case of gtk it implements it itself, cause X has none. The thing that implements all the familiar behaviors of the text input that you know. So yes, compared to HTMLInputElement these are absolutely custom. But for a platform, absolutely native. reply throwaway19972 11 hours agorootparentprevDo you have an example of a texting app that doesn't use native input? This just straight up doesn't seem accurate. reply lopatin 19 hours agoparentprev> we do not want, or intend, to fork the Flutter community. I can't reconcile that statement with the rest of the blog post. Good intentions aside, this is exactly what they are on the path to doing. From the post: > By forking Flutter, we get to decide what gets merged. I'll allow myself to be naively blunt since I just learned of this and I don't have a stake in this battle, but it seems like a nice little coup attempt that Google can squash by putting more resources into Flutter if they feel the pressure. reply kevincox 8 hours agorootparent> can squash by putting more resources into Flutter So success? reply okwhateverdude 5 hours agorootparentprev> nice little coup attempt that Google can squash I mean, Google could also punt and let Flutter die. They've killed greater investments for less. reply jwells89 19 hours agoparentprevI can’t imagine using Flutter for desktop development if only because like most newer UI frameworks, it’s mobile focused and doesn’t come with important desktop widgets like tableviews/datagrids and treeviews. reply eternityforest 39 minutes agorootparentTableviews are pretty niche and tree views mostly get used for files, so I'm fine not having either in apps that aren't about managing lots of data. I like headers and sections more that tables for most things anyway. reply loic-sharma 17 hours agorootparentprevtwo_dimensional_scrollables is a first-party package by the Flutter team that implements table views and tree views: https://pub.dev/packages/two_dimensional_scrollables reply satvikpendem 9 hours agorootparentprevCanonical is contributing to desktop and is using Flutter for a lot of their desktop components now. reply account42 5 hours agorootparentGiven Canonical's record that is only further reason to stay far away from it. reply eternityforest 38 minutes agorootparentCanonical is great when they're following established standards, the problem is when they invent in house tech. reply divan 9 hours agorootparentprevThere are plenty, though, and they work amazingly well on desktop: https://pub.dev/packages/syncfusion_flutter_datagrid reply jwells89 4 hours agorootparentDepending on widgets external to the toolkit is a no-go for me. It unnecessarily bloats dependencies and there’s a high risk of the widget eventually being abandoned. reply divan 4 hours agorootparentI totally get your sentiment and also try to instill the culture of \"add dependency only if it's absolutely not possible to avoid it\". For small widgets, I often just copy the code into my tree. And I absolutely hate when library authors break API for no reason (something that the Dart ecosystem inherited from the web one). But limiting yourself only to standard widgets seems to be extreme. reply jwells89 3 hours agorootparentIt’s not that I limit myself to standard widgets, it’s more that the toolkits I use have a full enough complement of standard widgets that any custom needs are easily met by tweaking one of those. In UIKit for instance I typically build custom buttons by subclassing UIControl, which gives a nice “blank slate” control that has all the basics of interaction covered without the specifics of UIButton. It’s easy to build a custom button that’s as well-behaved as a standard UIButton that way. reply crossroadsguy 19 hours agoparentprevHaha. Oh, most people know! They might not be knowing it’s Flutter to be specific but they sure know it’s some hybrid/non-native crap. I recently started using Ente migrating from Authy and goodness it feels and looks awful. I wish 2FAS had a desktop app. There was a native alternative that allows exporting codes. (This is not against Ente devs. I am sure they are a small team bringing out a FOSS product. This is just my preference) reply divan 5 hours agorootparentDo you just assume that Flutter is the same as typical other \"hybrid/non-native\" crap? reply 1oooqooq 15 hours agorootparentprevthe responsiveness and consistency is fine. ente just have weird design choices but that's a matter of taste reply madeofpalk 18 hours agoparentprevWhat are some examples of good 'impossible to tell' UIs built with flutter on desktop or web? reply divan 9 hours agorootparentMy comment was related to mobile apps. Web apps built on Flutter definitely can be distinguished. The thing is, with web apps, people have developed feelings for the \"nativeness\" - because browsers make a lot of UI choices for them. A typical example is a selection of the text. Because web apps are built using XML-based typesetting language from 80-s, everything is selectable by default. No matter how many layers of abstraction you put on top of that hackish foundation, you are still running it in the program (browser) designed to show text. So you can select buttons, navigation controls, images, sound players, etc. And it feels very native to web users. But when you actually think about it, it's an insanely stupid UI choice that neither of proper UI frameworks would even consider. Sure, you can \"opt-out\" with recent CSS controls for selection, but it's again, a hack - the default choice of \"everything is selectable\" is made by default for you by the browser. What does it even mean - select button widget? Why would you enable this complicated selection functionality for your UI that allows users to select navigation buttons with a blue selection box? Of course, if your app needs this functionality - you can add it. But otherwise, it would be labeled as a very stupid UI choice. Yet, feels native to web. Another example is zooming. Because UI apps written in web stack can't completely hide the fact that what they are doing is essentially trying to morph text primitives into complicated UI widgets (makes sense for 2024, right?), and the browser is essentially a text viewer, everything is zoomable and feels \"native\" to be able to zoom a web app. And yet, this rarely works as intended. In any other UI framework, making zoom functionality requires thinking about what and how you want to zoom and why. OS text size preferences are well handled by flutter, but sometimes you do want to add your own zoom functionality. In web though you just blindly zoom everything, often blowing off the layouts and that feels very \"native\" to web. My point is, that I don't think UI frameworks should try to match the nativeness of web, especially when they start targeting the wasm platform. These couple of decades of proliferation of web frameworks built on top of HTML/JS/CSS should just wane in history as a dark period of software engineering. reply account42 5 hours agorootparentSelecting everything is what every UI toolkit should support. In the 90s it may have been acceptable to show an error MessageBox with a non-selectable message but it shouldn't be acceptable today. What needs to be selectable is defined by users not developers and the union set of those requirements is \"everything\". Browser zoom also only ever breaks if you are fighting the Browser's layout engine instead of embracing that. Just don't do that and it works fine. Of course most web \"apps\" shouldn't be apps in the first place but if they must be \"apps\" at least stop reimplementing a crappier version fo the native browser functionality. reply ForHackernews 8 hours agorootparentprev> But when you actually think about it, it's an insanely stupid UI choice that neither of proper UI frameworks would even consider. You say \"insanely stupid\", I say wonderful. Everything you apparently hate about the web is what makes it good for users. Content is primary, and my user agent can resize it, restyle it, copy it, extract it, link it, read it to a blind person... If you want to make a binary blob app, just make that. Stop trying to break the web. reply divan 8 hours agorootparent> You say \"insanely stupid\", I say wonderful. Yes, that's the most common response. I've heard stories about \"how wonderful when you can select everything\" multiple times, and that's, of course, just shows how human rationalization works. And yet, this \"feature\" or \"UI choice\" is not even a choice. It's just a byproduct of using the wrong stack for the task. Like, nobody ever sat and asked, \"Do we want our apps to have everything as a selectable feature?\" before shipping this \"feature\". It just happened, and then, of course, millions of humans, having no actual choice over it, naturally rationalized that as a \"wonderful\" feature. reply ForHackernews 7 hours agorootparentIt didn't \"just happen\" the early architects of the web designed a system to make it easy to share and remix content and subsequent generations of companies and their UX designers have so far failed to fully claw back user freedoms. reply madeofpalk 7 hours agorootparentprevWhat are some examples of good 'impossible to tell' UIs built with flutter on mobile? reply divan 5 hours agorootparentBolt [1] and Bolt Food [2], for example. One of the apps is built with Flutter, another one is native. Can you tell which one and explain how you noticed it? https://www.youtube.com/watch?v=3X8COSnscbQ [1] https://www.youtube.com/watch?v=VOgrdt5WQcI [2] reply kbcool 1 hour agorootparentLOL Bolt is a React Native shop. Just Google \"bolt react native\" they have jobs and articles about their usage but also any library inspector will tell you that. reply divan 1 hour agorootparentNope, one of them is Flutter (I know team PM personally). Which one? reply flawn 13 hours agorootparentprevGoogle Wallet & Google Classroom reply chrismorgan 11 hours agorootparent> Google Wallet Do you mean https://wallet.google/, which says “only available on Android”? Your parent comment asked for “on desktop or web”. reply chipdart 12 hours agoparentprev> Flutter is the best thing that happened to UI development since Qt. Most people don't realize how many apps written in Flutter they use daily, simply because it's impossible to tell. Electron-based apps are also everywhere and they are through and through bad and a step backward. I would take caution to not do appeals to authority. I would extend this assertion over any javascript-on-a-webview-based GUI framework. Awful concept from start to finish. The fact that you failed to give any specifics to support your superlatives leads to suspect that the comparison with Electron is well suited. reply zigzag312 5 hours agorootparentNot OP, but what makes Flutter good IMO is drawing directly to native canvas, a very good hot reload implementation and AOT compilation. There are three ways cross-platform UI frameworks draw UI. 1) wrap platform controls, 2) use webview or 3) use low-level graphics API. Option 1 has to design controls to cater for the lowest common denominator. And there is a higher chance that an OS update might break things that in options 2 or 3. Being based on wildly different APIs (for each platform) increases complexity of creating and maintaining custom controls. Option 2 has to render UI through a high-level declarative language made for documents. Option 3 uses same APIs as native UI frameworks use. It's basically alternative implementation of an UI framework. Downside is that it has it's own look and feel, which is mostly an issue, if native UI framework is more polished. Flutter, being option 3, is more similar to a game engine, than to Electron. Low-level graphic APIs maintain good backwards compatibility, so OS updates don't affect the framework much. Less maintenance is needed. Good hot reload makes use of XML unnecessary and allows to declare UI in an actual, more powerful programming language. Overall, this removes another layer of complexity. (MS take a hint) AOT compilation enables fast startup times and less pauses. Which is good as end users are opening and closing applications all the time. reply divan 9 hours agorootparentprevYou are right, sheer popularity is not the argument for the quality of the framework. I wrote those two sentences separately for different purposes (the second was more of a reflection of other comments in the thread. I should've put it in a separate paragraph or elaborated more. reply dominicrose 7 hours agoparentprevAfter using Typescript and React, building a front-end with Flutter is boring, as are other Google frameworks (Angular, GWT). Maybe if you've never worked with TS/React and more generally HTML/JS/CSS, then it may be OK for some use-cases. reply thiht 7 hours agoparentprev> Most people don't realize how many apps written in Flutter they use daily, simply because it's impossible to tell I know I use exactly one and it sucks. It’s all janky and weird compared to the rest of the system. reply zachrip 21 hours agoparentprevWhich flutter apps do I likely use? reply kelvinjps10 5 hours agorootparentHacki a nice frontent for havkernews reply sureglymop 14 hours agorootparentprevThe only one I often use is Immich. It's laggy and at times freezes completely for a second. I don't think that's the developers fault though, I don't have that issue with any other android app. reply kevincox 8 hours agorootparentI recently started Immich and also say the awful lag. But apparently this is due to a slow \"album sync\" running often on the main thread. Not the fault of flutter. I don't know how syncing my 200 albums can be that slow but they are a big project to move it off of the main thread. reply reaperducer 21 hours agorootparentprevAccording to its web site, eBay, New York Times, Square, American Airlines. reply nodamage 7 minutes agorootparentListing eBay and NYT seems pretty misleading when it's actually eBay Motors and NYT Games and not their primary apps. I can't tell if the same applies to Square or American Airlines but I would not be surprised. reply favorited 20 hours agorootparentprevSpecifically eBay Motors, which is a standalone app. https://flutter.dev/showcase/ebay reply zachrip 20 hours agorootparentprevI'm suspicious of those because it shows an ebay logo but then shows an app I've never heard of or used...is this the same for all the other logos? Where does NYT use flutter? reply orta 19 hours agorootparentFlutter web publicly launched with a NYT prototype https://verygood.ventures/success-stories/new-york-times reply mh- 18 hours agorootparentI recognize that as the NYT Games (formerly NYT Crossword) app. It has some very annoying non-native behavior, like bringing up the text selection/highlight menu in inappropriate places (in a word puzzle with no selectable text). reply milch 18 hours agorootparentOr the non-native keyboard ... honestly made me stop using it altogether reply mh- 17 hours agorootparentYeah that's really annoying too. I've solved thousands of daily crossword puzzles in that app. reply timsneath 18 hours agorootparentprevI don’t believe NYT has used Flutter for some years. reply Onavo 21 hours agoparentprev> Most people don't realize how many apps written in Flutter they use daily, simply because it's impossible to tell. Flutter implemented its \"native\" looking UI widgets by literally having teams of designers eyeball the native designs and reimplementing, starting from drawPixel. This can't be done on a volunteer basis alone. Many open source attempts have tried this route and failed because they don't have the sheer designer resources needed to get there. https://docs.flutter.dev/ui/widgets/cupertino reply dwaite 11 hours agorootparent> Flutter implemented its \"native\" looking UI widgets by literally having teams of designers eyeball the native designs and reimplementing, starting from drawPixel. Those widgets are then stylized wrong on every subsequent platform release, such that your iOS 18 phone might seem like it is launching an iOS 7 app. IMHO if your goal is to have a cross-platform codebase act like the underlying platform, React Native is a much better approach. Flutter exists for people who don't intend to take on the effort of targeting platforms with specialized behaviors. reply skydhash 20 hours agorootparentprevI’d much prefer they’ve gone with their own design system. People are already used to bespoke ones (web), badly done ones just remind me of scammy sites. reply 1oooqooq 14 hours agorootparentthis so much. but flutter was probably sold internally as a trojan into ios dev experience. the carrot was multiplatform... and to sell it they needed to at least market that it was \"true multiplatform\" with native look. which is ironic since they go out of their way to make it pain to build to both in the same app reply faizmokh 10 hours agorootparentprev> having teams of designers eyeball the native designs No wonder it's not accurate. reply poulpy123 8 hours agoparentprev> Flutter is the best thing that happened to UI development since Qt. Which programming languages have a good developer experience with flutter beside dart ? reply rdsnsca 4 hours agorootparentNone, Flutter is dart only. reply kernal 21 hours agoparentprev>Flutter is the best thing that happened to UI development since Qt. Most people don't realize how many apps written in Flutter they use daily, simply because it's impossible to tell I agree with you that Flutter has been a boon for cross platform development, but to say it's impossible to tell you're using a Flutter app is a bit of an exaggeration. I have no problem identifying Flutter apps. Not that I care as they often have a genuinely nice UI and are performant. reply dotancohen 21 hours agorootparentIs the complete lack of accessibility still the giveaway? Last I looked Flutter was a complete nonstarter for accessibility. reply hyperhopper 18 hours agorootparentI worked on the GPay flutter rewrite and our standard for accessibility was higher than almost any other app I've seen, and was a significant amount of effort spent on that. What accessibility shortcomings does flutter have? reply jonas21 15 hours agorootparentThis may be a silly question, but how do I install the Google Pay app on my iPhone? I'd like to try it out since it's the first app listed in the Flutter Showcase, people keep mentioning it in this thread, and it probably works as well as a Flutter app can, given that it's first-party from Google. But when I search for it on the App Store, it's nowhere to be found (or at least not in the first 20 results). reply satvikpendem 9 hours agorootparentIt's only for the Indian market. As Google often does, they released Google Wallet for the US market. But that's not a failure of Flutter itself of course, only internal Google politics. reply divan 21 hours agorootparentprevCan you elaborate? What's missing that makes you feel confident saying \"complete lack of\" despite well-documented accessibility functionality (including testing for accessibility)? [1] [1] https://docs.flutter.dev/ui/accessibility-and-internationali... reply divan 21 hours agorootparentprevHow do you do that? I just opened my phone, and the last app I used was a food delivery app. How can I figure out if it's a Flutter app or a native one? reply no_carrier 20 hours agorootparentI don't think people are able to tell a Flutter app from any other cross platform/web framework. They just think they can. reply jwells89 19 hours agorootparentIt’s more difficult to distinguish if an app is Flutter or some other cross platform framework, but “native or not” is very easy on iOS. Even React Native, which is the least foreign, has tells. On Android it’s more difficult, partially because it’s kind of like Windows where Google/Microsoft uses 50 separate reimplementations of Material/Fluent and there’s no consistency to be found anywhere. reply divan 5 hours agorootparentAh, so we talking about two different things here: a) with any given UI design, distinguishing if it's implemented using native UI framework or with Flutter b) Flutter app providing 100% indentical look&feel to Cupertino/MaterialDesign/WinForms/Cocoa/etc. I was talking about a). Assuming that the app developer wants to have a consistent app design across platforms, which probably came from a design department – there is virtually no way to distinguish. Ultimately, it's just a bunch of pixels spit out onto the framebuffer. reply knifie_spoonie 19 hours agorootparentprevI'm really curious what the specific tells are. Do you have any specific examples? When using the apps on my iPhone, I just don't see how I could tell the difference. reply jwells89 19 hours agorootparentFor React Native it’s common for navigation to be weird compared to UIKit/SwiftUI apps. Also common for things like padding/margins to be off since standard layout facilities aren’t being used, and devs of RN apps will often visually customize controls (web style) that native devs don’t. For Flutter, the most visually obvious thing (aside from usually using Material Design) is that its animation curves are all totally different from those of UIKit/SwiftUI and interact with gestures differently. The Cupertino theme is a poor facsimile of UIKit and lands squarely in the uncanny valley, which is arguably worse than Material Design. Flutter on iOS also tends to hitch where UIKit/SwiftUI don’t. reply xster 18 hours agorootparentThere's likely things we missed but we'd love to find examples you might have. We try to be scientific as much as we can https://github.com/flutter/flutter/pull/122275 [coincidentally a community contribution from someone who didn't have to fork :)] Disclaimer: used to work on some of these animations reply exusn 5 hours agorootparentprevYep, same. For me the animations are a dead giveaway if an app is using flutter. That, and the scrolling just feels _off_ in a way I can't quite describe and is a little clunky. I had been building stuff with flutter for a while when GPay migrated to it and I could straightaway tell from the performance that it was flutter. reply ClassyJacket 21 hours agorootparentprevHow do you know this isn't the toupee fallacy? You spotted a few correctly so you assume you always spot correctly - but there might be more you miss. https://rationalwiki.org/wiki/Toupee_fallacyhttps://rational... reply 1propionyl 22 hours agoparentprevFlutter is really not some big revelation and it's always shocking to me how it's evangelists act like it's the game changer no one else has noticed. It's just not that good? Just build native UIs. I don't know why cross platform UI has been such a hobbyhorse for so many for so long: it's a stupid idea. reply epcoa 21 hours agorootparentWhat's a native UI? The only thing closeish to a native UI is macOS and iOS AppKit and UIKit. Winforms aka Win32 is still a thing, but Microsoft has been undogfooding that and putting out alternatives for years, now WinUI3 will definitely kill off Winforms for good! What is native on Linux? Gtk, Qt, Motif (lol)? And then Android? Ironic then that Google is the one behind Flutter. The concept of native outside of MacOS/iOS is pretty busted at this point. At best it just means something non-web. reply jeremyjh 19 hours agorootparentWin32 is the native GUI toolkit on Windows. Winforms is a .NET wrapper to it. There isn't any debate to be had there. Yes, the same vendor provides other toolkits. Linux is a kernel project, and different distributions are for many purposes best considered to be different OS's. Desktops based on Linux mostly are either GTK or QT, and so the native toolkit depends on the desktop you are using. Is this too much fragmentation for users on those platforms to realistically expect commercial software to support them natively? Yes, of course, but that doesn't mean there is any confusion about what it would mean to do so. reply int_19h 19 hours agorootparentWin32 is no longer the native GUI toolkit on Windows, given how much of the OS GUI itself is not using it (new Settings etc). If the argument is that it ships in the box, well, so does UWP XAML. reply epcoa 19 hours agorootparentprev> Linux is a kernel project, Is that useful pedantry? From context, you really think that's my confusion here? I didn't include *BSD either, but it isn't any different for the relative handful of people using that for a desktop system. > Win32 is the native GUI toolkit on Windows. Winforms is a .NET wrapper to it. There isn't any debate to be had there. Except for Microsoft's numerous attempts to supplant it and the fact that the base OS doesn't even use it consistently for their own system. Microsoft's own File Explorer and Settings app that only had to live along with Control Panel for a decade, was that not native? They aren't \"Win32\". If you insist on pedantry, you should know Win32 isn't a GUI toolkit either, it's an anachronistic term for the Windows API. Nobody casually calls it USER though, and the bulk of people still targeting it do so through .NET these days via Winforms. If native just means provided by the vendors base system, ok, but then if there are 10 different forms of it, other than the distribution issue there isn't some great intrinsic benefit over just targeting Qt or Flutter. Especially when someone like Microsoft can't settle on a consistent design language through the years. reply skissane 16 hours agorootparent> If you insist on pedantry, you should know Win32 isn't a GUI toolkit either, it's an anachronistic term for the Windows API. I don’t think it is “anachronistic” - as far as I am aware it is still the official name. And I don’t think calling it the “Windows API” is right, since Win32 is just one of the APIs that Windows offers. If I’m calling CreateFile, I’m using the Win32 API, but if I’m calling NtCreateFile, I’m not using the Win32 API, I’m using the native NT API instead. If I set the subsystem as NT instead of Win32 in my executable’s PE header, calls to native NT APIs such as NtCreateFile will still work fine, attempts to use Win32 subsystem APIs won’t. And there are other APIs Windows has which aren’t (strictly speaking) part of either the native NT API or the Win32 API - COM and the many APIs built on top of it, .Net, WinRT, DirectX, etc. But you are right that Win32 isn’t a GUI toolkit. It contains a rather basic and old-fashioned GUI toolkit (USER), but it contains a lot of non-GUI APIs too. I’m reasonably familiar with those parts of the Win32 API used by services and console mode apps, but if you asked me to write a Win32 GUI event loop I’d be asking ChatGPT to remind me how, because while I’ve read tutorials I’ve never actually attempted it. reply epcoa 16 hours agorootparentThe name itself is anachronistic, what does the 32 stand for? You call CreateFile with 64-bit pointers, it was still considered Win32 (until they officially changed it). But take it up with Microsoft: https://learn.microsoft.com/en-us/windows/win32/apiindex/win... \"Using the Windows API, you can develop applications that run successfully on all versions of Windows while taking advantage of the features and capabilities unique to each version. (Note that this was formerly called the Win32 API. The name Windows API more accurately reflects its roots in 16-bit Windows and its support on 64-bit Windows.)\" The URL still has win32 in it, lol. Though this naming goes back nearly 2 decades https://learn.microsoft.com/en-us/previous-versions/tn-archi... The API provided by ntdll is semi-officially called the \"Native\" API and much of it is subsumed into the Windows API. The PE subsystem names you are referring to are IMAGE_SUBSYSTEM_NATIVE and IMAGE_SUBSYSTEM_WINDOWS_GUI/CUI, so it's somewhat consistent. Microsoft officially refers to that API as Native System Services in the documentation for the DDK. https://learn.microsoft.com/en-us/sysinternals/resources/ins... reply skissane 15 hours agorootparentMicrosoft’s docs are a self-contradictory tangled incomplete and sometimes even downright erroneous mess, I wouldn’t put too much stock in what they say. If I say “CreateFile is the Win32 API analog of NtCreateFile in the NT native API”, everyone experienced with low-level Windows development will know what I am talking about. If I started talking about “Native System Services”, I’m not sure as many would. Similarly, the distinction between APIs which are easy to call from C code (and simpler FFI frameworks from scripting languages, e.g. libffi) and COM/Automation/.Net/WinRT APIs which are a lot more difficult to use from C (as opposed to C++), and which require more advanced FFI support, is still important in Windows development (or at least some parts of it.) And in practice the term “Win32 API” is often defined to exclude those higher-level harder-to-call-from-plain-old-C APIs. It goes back to the original design of Windows NT, where you had a primary environment subsystem (Win32), secondary environment subsystems (OS/2 and POSIX), and integral subsystems (local security authority, session manager, etc). The primary environment subsystem is still called Win32, and the Win32 API is its public API. (It also has private APIs, most notably the CSRSS LPC interface, but that’s unstable from version to version.). As I said “Windows API” is insufficiently specific because (especially nowadays) Windows has lots of other APIs. WinRT and Win32 are both Windows APIs but very different. WinRT is largely built on top of Win32, but some documented WinRT APIs are built on undocumented Win32 APIs, leaving WinRT the only officially supported API to access certain functions. Microsoft intentionally didn’t rename Win32 to Win64 when they added 64-bit support because it is 99% the same API just with some highly regular changes (mainly widening pointers). By contrast, Win32 was a much more radical change to Win16 - the Win16 API directly incorporates notions of segmented memory, which it uses to implement movable memory blocks (rather similar to Classic MacOS, albeit that did it in a 24/32-bit flat memory model rather than a 16-bit segmented one). Microsoft could have done a more straightforward port of Win16 to 32-bit x86, e.g. using a 32-bit segmented memory model instead of 32-bit flat memory, keeping movable memory; but (thankfully) they didn’t. It would have made it a lot harder to move to 64-bit or non-x86 platforms. reply jeremyjh 18 hours agorootparentprev>Is that useful pedantry? From context, you really think that's my confusion here? I don't know what could possibly confuse anyone about the role of GTK and QT in the Linux ecosystem. reply epcoa 18 hours agorootparentGTK 3 or 4? Xfce+MATE+Cinnamon are still popular enough options and I used the former for years. GTK itself isn't even a well defined target. And GTK3 apps of which there are plenty do not appear native in a modern GNOME desktop much more so than a decent Qt app does. The thing is that people in the real world talk about Linux support for an app, as in Desktop Linux support, and the only common denominator there for years was basically X11. I have heard exactly no one ever ask, oh when is that app going to be available for KDE or GNOME. For example, to take something recent here, do the people writing the Zed editor promise a particular DE support - no it's available for \"Linux.\" As to earlier mentioning distros - for a GUI app, there’s more pain from intra-distro variation than inter-distro, both across configurations and supported versions. Now the big thing, the even bigger thing is probably native Wayland vs X11, but no one says, oh vendor will you please bring your app to KDE or GNOME, specifically. 99% of the time you are happy if you get anything at all. So the concept of native on Linux (Desktop Linux) just isn't something usually worth discussing. reply foresto 18 hours agorootparentprev> What is native on Linux? Gtk, Qt, Motif (lol)? Linux is not something that encompasses a GUI, so this is a misguided way to frame the underlying concern. One might as well ask what toolkit is native on smartphones, or what language is natively spoken by humans. We should instead ask what is native on Plasma, or on GNOME. \"Oh, bother. That would mean I have to think of four things instead of three.\" Well, yes, Pooh. But the things that make them different are the things that make them, them. reply epcoa 18 hours agorootparentOk but this is in response to this: \"Just build native UIs. I don't know why cross platform UI has been such a hobbyhorse for so many for so long: it's a stupid idea.\" The stupid idea is thinking that building native UIs is targeting a clean, non-moving target and implying it is not astronomically harder than doing something cross platform that is between mediocre and good enough for everybody. Whenever someone says “just do native” it always turns out they have a cockamamie definition of native, otherwise they would realize the gravity and general unreasonableness of what they were asking. On a side note, what’s a major* reasonably complex app that isn’t a music player or an IRC client, or something with a simple shell around a canvas like a browser, that targets both Plasma and GNOME natively. * And whether anyone likes it or not, few people care (as revealed by $$, not bitching on a forum). Blame electron, the web, but even Microsoft cannot even define and maintain a native LnF for their core operating system. As I alluded to, the only small community that cares are those on MacOS and iOS, where there is a HIG that more than 2 people have read and give a shit about, and there is a small market of people that value well integrated Mac apps that will actually spend money. Targeting Win USER32 +/- XAML, Qt, GTK, and AppKit is costly and thankless when you probably could have just used Qt, JavaFX, or Flutter, etc. reply dotancohen 21 hours agorootparentprevI would consider both GTK and Qt native on Linux. reply epcoa 21 hours agorootparentSo that means one has to target both GTK and Qt to be native on Linux. Fat chance of that on anything but yet another music player or other relatively trivial UI app. In any event, that still doesn't answer what it means to be \"native\"? And my point is I think most definitions are dumb or useless. If I'm running a GTK desktop, Qt apps are generally not \"native\" no matter how much theme fuckery one tries. Native can also speak to other common UI affordances and design guidelines, and there sure as hell is nothing like there is in the MacOS world for linux. Some core GNOME and KDE desktop apps maybe, but overall there isn't much adopted standardization in Linux world. 90% of the work I get done involving a GUI outside a browser on Linux usually involves Java, whether it be Intellij, ghidra, etc. Maybe I'll pick up blender from time to time. Audacity, there's a good one, targeting (for now) a cross-platform toolkit that pretends you can have your cake and eat it too - wxWidgets never fooled anyone - they sure as hell don't look native on MacOS, and they invariably look much closer to what they riffed off of, a 90s MFC app. (And wxWidgets wraps GTK and AppKit, so is it native? If so, native is meaningless IMHO. If not, why not?) reply dotancohen 18 hours agorootparent> So that means one has to target both GTK and Qt to be native on Linux. Incorrect. Target one or the other, not both. reply epcoa 17 hours agorootparentHow is Qt within a GNOME or Xfce environment any more \"native\" than Qt on Windows or Mac? You have reduced the definition of \"native\" to merely compatible with X11/Wayland (that's the only common denominator). Well now, Tk, FLTK, Swing, and even Wine are all native. reply notpushkin 10 hours agorootparentBoth Qt and GTK have facilities for integrating into each other’s desktop environments (see [1]). Sometimes they blend in nicely, sometimes they stand out a bit, but I think it works out pretty okay. [1]: https://wiki.archlinux.org/title/Uniform_look_for_Qt_and_GTK... reply account42 5 hours agorootparentGTK doesn't much care about integrating into a Qt environment and doesn't really implement anything to make that work besides in some cases implementing the same Freedesktop standards. Even for basic things like the look of widgets you need a style that has implementations for GTK - there is no compatibility layer to use Qt styles. Gnomies in general don't care about anything outside their world. The other way around is a bit better, e.g. there is QGtkStyle but AFAIK it is stuck at GTK2 and does not support using GTK3 styles. Still, behavior between GTK and Qt applications is very noticeably different. It would be great if there was a shared ABI applications could use but GUI toolkits are too complicated for this to be feasible. reply epcoa 9 hours agorootparentprev> Both Qt and GTK have facilities for integrating into each other’s desktop environments Neither Qt nor GTK are desktop environments and Qt certainly isn’t defined by a dominant desktop environment - KDE isn’t even what pays their bills. You’re still proving my point. The common denominator here (on Linux) is just X11/Wayland. If it works out “pretty okay”, then score one for a cross platform toolkit. Still no idea what über alles “native” means. Let’s go back to the original comment I responded to… why should I not just continue with the Qt “stupid” “hobbyhorse” reply einpoklum 20 hours agorootparentprev> So that means one has to target both GTK and Qt to be native on Linux. Doesn't it mean that you can target _either_ and be considered native? Also, I'm not sure I buy the claim that of \"GTK and Qt are native\" in the first place. I'd say either that there is no native UI, or if I must call something native, it's the toolkit the desktop environment uses. And yes, that does mean that there is no \"Linux nativeness\" like there is Windows nativeness, as every DE is different. And rightly so, because one could in principle write a different DE over the Windows kernel, whose UI would behave differently. reply epcoa 20 hours agorootparent> Doesn't it mean that you can target _either_ and be considered native? Well, no because I was agreeing with you, if you're going to grudgingly accept your definition for native, the toolkit the desktop environment uses, that means you would have to target both Qt and GTK unless you want to draw a line in the sand and say fuck it to one of GNOME or KDE as assume they don't exist. Sorry GNUStep. And this is nothing to say of GTK 2-4. I agree that just saying GTK or Qt are native in a vacuum outside of DE is a completely useless definition, and even taking DE into account is a tenuous one. As for Windows, what GUI toolkit does the desktop environment there use? That's a trick question. reply dgellow 15 hours agorootparentprevWinUI3 doesn’t kill WinForms… WPF, WinUI, WinForms all exist together reply epcoa 3 hours agorootparentYou missed the sarcasm. If all can exist together, which one is the “native” one? If the answer is all of the above, then the consistency argument for native doesn’t hold much water. reply divan 22 hours agorootparentprev> Just build native UIs Do you see any reasons why many companies/teams/devs don't want to \"just build native UIs\" and instead are looking for cross-platform solutions designed from ground apps for modern UI development needs? reply 1propionyl 22 hours agorootparentYeah I do see why they don't want to do it and I see it as a critical failure of risk management. If you think pinning your entire product on a cross platform UI kit when every single one (except maybe QT) had proven a failure... then yeah I think maybe you should consider actually just eating the cost of building native UIs. The risk analysis of building against supposed cross platform \"solutions\" just doesn't work out. Why do we keep trying to do it? The idea of cross platform UI is frankly \"f*cked from the jump\" and should never have been a goal. I think it's only a goal because it's intellectually satisfying, not because it's really desirable. reply divan 21 hours agorootparent> and I see it as a critical failure of risk management. I also want to live in the world where risk management is the only variable that determines how CTOs and devs choose the stack for the software. reply mdhb 22 hours agorootparentprevBack in the real world however you have canonical for years now who have made flutter the default for all native linux desktop development. I think your anecdotes are a bit out of date and irrelevant. People much better informed than you looked at this problem in a lot more detail in real life situations and came to very different conclusions. reply 1propionyl 22 hours agorootparent> you have canonical for years now who have made flutter the default for all native linux desktop development And yet, I can't think of a single app I depend on for anything on Linux that relies on it. How'd Snap go for Canonical? reply mdhb 21 hours agorootparentCool, none of that is remotely relevant to what we are talking about here. Your anecdotes are dated and you should think about updating them so you don’t talk so confidently on things you don’t actually seem to know much about. I mean that sincerely not as some internet gotcha there’s just no need to sit there defending a position that is based on old info just for the sake of it. reply LordDragonfang 22 hours agorootparentprevAs someone desperately fighting to keep their company's main product to stay using a native desktop UI, Microsoft makes it real hard. WinUI3 hasn't even officially launched and it already feels like it's on life support. QT is good, but even qt seems to be starting to use a web renderer. reply jcelerier 19 hours agorootparentQt either Widgets or Quick definitely does not use a web renderer reply contextfree 21 hours agorootparentprev\"WinUI3 hasn't even officially launched\" ? It officially launched with version 1.0 and it's now on 1.6? reply LordDragonfang 3 minutes agorootparentI suppose it was incorrect of me to say it hasn't \"officially\" launched then. But one would certainly seem to get that impression, considering it's missing major features (data table support?) isn't anywhere near feature parity with either WPF or winforms, and doesn't even have a visual editor (which should be table stakes for a modern 1.0 release from a company as large as Microsoft). reply poulpy123 8 hours agorootparentprevIt's really something I know nothing about so I googled WinUI3 and the 3 first results are microsoft website and github, and the next two are \"is winui3 dead ?\" and \"Oh winui3 is really\" dead. So it doesn't inspire confidence :) reply c-hendricks 19 hours agorootparentprevI can't really speak to the differences but > At this time, there are two generations of WinUI: WinUI 2 for UWP and WinUI in the Windows App SDK (WinUI 3). > https://learn.microsoft.com/en-us/windows/apps/winui/ reply int_19h 19 hours agorootparentHence why OP was specifically talking about WinUI 3 (which is indeed at version 1.6.1 as of right now). reply wiseowise 20 hours agorootparentprev> I don't know why cross platform UI has been such a hobbyhorse for so many for so long: it's a stupid idea. You suggest to build and maintain same logic in at least C#/C++ + Swift + Kotlin + JS + C++ and you ask why crossplatform UI is a popular idea? reply awinter-py 17 hours agoparentprevit's possible to tell reply skybrian 22 hours agoprevBack when I worked on GWT, we had trouble accepting outside contributions because the team had a mandate to support Googlers. That is, much like other libraries and tools at Google, changes could not break google3. This means testing patches against google3 and either changing the patch, or fixing whatever code used it, and these are tasks that no outsider can do. Shepherding these patches is no fun when you have your own changes to work on that are more important to the team. We did something similar, by creating an external fork where changes could be tried out by the community, without necessarily being accepted into the internal version. I think a fork could work if there was enough external momentum, but even 20 people working full time would actually be pretty good for an open source project. How many developers will this fork attract? The fork would need to attract other businesses who can put people on it. One downside is that the code isn't tested against google3. Sometimes you find actual bugs that way. Edit: reading more closely, the complaint doesn't seem to be that patches weren't reviewed, but rather that bug reports weren't investigated. That's definitely something outside developers could do more of, and seems a lot easier than forking? reply xster 20 hours agoparentThe main Flutter GitHub repo does have infrastructure to run PR against all Google internal tests (which as you say, does find real bugs). https://imgur.com/a/Ih2oQIS disclaimer: my team runs said infrastructure reply EngVagabond 18 hours agorootparentDoes that automatically run against every PR? What mitigations did you have to put in place for Google security to allow running untrusted code from PRs on internal CI? reply xster 18 hours agorootparentSemi-automatically. Googlers have to review it. reply serial_dev 11 hours agorootparentAlso known as “manually”. reply yencabulator 16 hours agoparentprev> when you have your own changes to work on that are more important to the team. That part is the killer. Important to the Google Flutter team != important to the wider Flutter community. reply Degorath 21 hours agoparentprevCouldn't flutter work as public-first that then gets vendored into third_party? Or is that literally the fork strategy you are talking about that I'm too dense to understand? reply skybrian 20 hours agorootparentYeah, that's what I meant. Public-first is good in some ways, but it means some bugs might be discovered rather late. reply odo1242 22 hours agoparentprevWhat is google3? reply yonran 18 hours agorootparentHere is a paper describing google3 https://research.google/pubs/why-google-stores-billions-of-l..., which includes the policy that “To prevent dependency conflicts, as outlined earlier, it is important that only one version of an open source project be available at any given time.” More documentation on The One Version Rule for third_party: https://opensource.google/documentation/reference/thirdparty.... This rule may make it more difficult to upgrade a third_party library frequently since the maintainer of the third_party library has to test all the uses of the library from all teams, rather than each team deciding when to upgrade. reply dekhn 17 hours agorootparentI used to maintain the internal (single) version of numpy and scipy and honestly I think the one-version rule was awesome. It greatly reduced noise and churn. It meant far less breakage, and it was easy to find and fix all the breaking tests when I was updating to a new version. I guess the only downside is sometimes an older version of numpy would prevent deepmind from making some sort of ML advance until I upgraded to get some new feature they wanted. reply JoelEinbinder 22 hours agorootparentprevGoogle's monorepo of closed source code. reply gmoot 22 hours agorootparentprevGoogle's internal monorepo. reply hobofan 21 hours agoparentprevYeah, this seems to be a common theme with Google's open source projects. Bazel feels very similar in many points. It obviously wouldn't exist if not for Google putting resources towards it, but Google's (and google3's needs) are so significantly prioritized that it hurts the community. E.g. due to the low allocation of resources and extensive usage of rules_proto and bazel-skylib inside google3, they are completely ossified and the community has given up contributing to them and instead created either forks or contribution-friendly wrappers. reply 1oooqooq 14 hours agoparentprevi avoid contributing to flutter because it feels like a google soon to be abandoned project. hacks like not accepting tabs in any tools and using space indentation wrong on top of it. tools ignore parameters like port numbers just so it works better for Android studio very specific use case while bringing pain to any ci pipeline, etc. reply jmull 4 hours agoprevI think a big problem is that Flutter's approach is very labor intensive. It takes over responsibility for the UI, which is tough when you want to be cross-platform. You need to keep up with the native UI across all the platforms. (Or, of course, you don't, in which case there are gaps, bugs, inconsistency, jank, etc.) Beyond the UI, it needs to offer integration with platform services and conventions, which is another perpetual treadmill. Not to mention it includes a dedicated language, which takes significant effort in its own right. Add to that, that as a project gets larger and more complicated, it becomes harder to change and tech debt builds up. It's not surprising to me issues are building up. I've got to wonder if a fork, while it can move forward in one respect, may just be adding more complexity in a project that already has too much. It wouldn't surprise me if Flutter's unmanaged complexity is exponential, in which case quadrupling the number of dedicated engineers won't move the needle a lot. reply CharlesW 3 hours agoparent> You need to keep up with the native UI across all the platforms. (Or, of course, you don't, in which case there are gaps, bugs, inconsistency, jank, etc.) And for the latter, the result is something that mostly does what web-based solutions do (with well-understood downsides), only with virtually no ecosystem in comparison. I do think there are opportunities for cross-platform native development solutions like https://skip.tools/, which offer an alternative to React Native. reply plorkyeran 23 hours agoprev> That's 50 people serving the needs of 1,000,000. Doing a little bit of division, that means that every single member of the Flutter team is responsible for the needs of 20,000 Flutter developers! That ratio is clearly unworkable for any semblance of customer support. If \"only\" 50 people working on a project used by one million people was unworkably low then every single successful project out there would be doomed. I've certainly worked on things with much worse ratios than that. > one has to critically assess why a team that loves external contributions has only managed to merge contributions from 1,500 developers over a span of nearly a decade. External contributions from 1500 developers over a decade is a lot. That is an unusually high number, not a low one, and backs up Flutter's claim to love external contributions. The fact that this person thinks that they can just magically conjure up dozens of volunteer PR reviewers is wild. Everything about this post makes it pretty clear that they don't have the slightest clue of the scope of what they're trying to do. This feels like one of the standard examples of an ex-bigco person setting off on their own with no understanding how just how much the bigco did in the background to facilitate their job. reply kgeist 23 hours agoparentOur user base is of a similar order of magnitude, with around 70 developers. It doesn’t seem like a large number, indeed! A small minority frequently contacts support, suggests features, etc., while the majority simply uses the product. reply wg0 11 hours agoparentprevCommon misconception is that you open source something and contributions will flock in. Doesn't happen that way. And even if happens - changes can't be merged without thorough and careful review. So who's do the reviews when supposedly there are too many contributors? Only someone who fully understands that particular subsystem. Which means we're back to square one because as per post there are only three subject matter experts available for the new fork. reply fhdsgbbcaA 21 hours agoparentprev50 people is a pretty huge team at Google to start with, there’s teams ofI’d put “person who makes sure WhatsApp verification codes work” on that list. Thanks! That part of my job really wasn't too hard though, once I got things in order; but it was the easiest part of my job to describe. Just a lot of debugging from sparse data, and trying to get things to work a smidge better, because smidges here and there add up. It really helped to have a great customer service team that bubbled up usable information from users. Debugging things from sparse data gets you involved in a lot of different systems though... That and I told people I worked at an ISP in the before times, so guess who got to fight with sendmail, and guess who got to own our Domain accounts and DNS accounts and guess who got to get x.509 certificates, and who had to fight with Google Apps to make mandatory 2fa usable, etc. Basically everything without a person and server adjacent got me. :P Anyway, I've moved on (replaced by a team of three on the SMS stuff amyway, much better bus factor) and am semi-retired and no longer in any critical path, which is a lot less stress. reply munchler 20 hours agorootparentprevWorld population is 8.2 billion. Assume that 1/3 are asleep at any given time, leaving 5.5 billion awake. It’s not a huge stretch to imagine 20% of them (1.1 billion) using core Google services during peak times. reply mvdtnz 21 hours agorootparentprevI don't find it hard to believe. I am not at Google but I work on a product with 200 million MAU, in a core team that serves requests to every single one of those users and there are 6 of us. reply fhdsgbbcaA 21 hours agorootparentprevNope. reply EGreg 21 hours agorootparentLink to proof? reply ziddoap 21 hours agorootparentOut of curiosity, what proof are you expecting to get? An internal Google usage report and a list of developer names that work on core infra? reply fhdsgbbcaA 21 hours agorootparentprevCan’t, but not sure why that’s hard to believe? reply EGreg 21 hours agorootparentBecause billions of people or their devices aren't even online every hour. Unless you're discussing background processes on google's side reply manquer 20 hours agorootparentPush notifications need to connect to that many devices every hour easily You may not get a notification every hour, but the phone still have to connect to Google APIs to check to know that. You can say same thing about say location services, every phone if powered and connected to the internet would ping more than one an hour, more than billion devices are definitely online at any give time, not just Android phones, also every android TV, tablet, watch, and the hundreds if not thousands of other devices running GMS on top of AOSP . There are probably few other APIs in GMS that would ping at least once an hour each device that is powered on, perhaps things like NTP for timing alerts or Emergency Alerts etc. You may consider them \"Background Services\" , most of them however have some foreground UI if they need to, and users will notice if they don't work reply fhdsgbbcaA 20 hours agorootparentprevThere are roughly 5 billion smartphone users now, those devices are almost always on and sending background data somewhere. By volume, more of that data guess to Google than anybody else, on the order of hundreds of network requests per device, per hour. reply LauraMedia 21 hours agoparentprevI generally agree, the fact that the project relies on Google's internal steering is the much bigger factor in this to me. There is no guarantee these 50 devs can actually focus on Flutter, instead they might get looped in in the internal race to AI products. reply mdhb 21 hours agorootparentI mean we do actually have many years of real life experience at this point of how they perform in the real world. I’ve never actually come across a better maintained project personally. It is incredibly thoughtfully developed with a high level of attention to detail who have successfully shipped a huge number of major improvements in ways that made sense. There is a premise in the post that implies Flutter is poorly maintained and that’s just not a commonly held belief by the community at all. As I hinted to in another comment the particular person behind this is a bit of an oddball and I think there may be other reasons that drove this decision in the first place in addition to the ones he gave in the post which for the record I’m sure there are some things he wishes were prioritised differently but this fork seems kind of very “him” rather than a popular position that people were begging for. reply giancarlostoro 22 hours agoparentprevWhatsApp had less than 50 engineers (or was it employees total?) when Facebook bought them out, and they had millions of users. reply izacus 22 hours agorootparentWhat a bizarre comparison, WhatsApp engineers didn't have to review code for millions of users. reply chrisandchris 22 hours agorootparentWhat a bizarre statement. cURL is basically maintained by a single person, and nearly the whole world uses it. reply ignoramous 22 hours agorootparent> cURL is basically maintained by a single person Suppose Flutter may be an order or two magnitude more complex than cURL. reply chrisandchris 21 hours agorootparentAbsolutely, yes! Then make it two or three people, maybe 10 and there are still n-times too many people working on flutter compared to cURL. Which boils down to: Number of devs vs. people using something is a very bad metric. Even number of supported devices won't work good in this case (I assume cURL runs basically everywhere). I think it's very difficult to estimate complexity, and then make a statement about \"how many people are enough\" is even more difficult. Some environments are harder and more complex, some are just very heterogen and some are both. Sometimes it's the organizational overhead, maybe even something else. reply mplewis 21 hours agorootparentTwo orders of magnitude would be 100, not 10. reply lesuorac 19 hours agorootparentI'm not sure that something being 100x more complex means you need 100x more people. plus they said an \"order or two\" which would be 10-100x so ... reply brnt 22 hours agorootparentprevglib then? reply g",
    "originSummary": [
      "A new project called Flock is being developed by forking Flutter to address labor shortages and speed up development.",
      "Flock aims to stay current with Flutter while adding bug fixes and features that the original Flutter team has not implemented.",
      "The initiative encourages community involvement in testing, reviewing, and leading areas to enhance Flutter's capabilities."
    ],
    "commentSummary": [
      "A new project called \"Flock\" is being developed by forking Flutter, with the goal of improving the development process for desktop and web applications.",
      "Flock aims to remain in sync with Flutter updates to prevent community fragmentation, addressing concerns about potential splits.",
      "The initiative has sparked debate on cross-platform frameworks, with some users preferring native UI development due to perceived performance and design limitations in Flutter apps."
    ],
    "points": 784,
    "commentCount": 651,
    "retryCount": 0,
    "time": 1730143459
  },
  {
    "id": 41976311,
    "title": "A return to hand-written notes by learning to read and write",
    "originLink": "https://research.google/blog/a-return-to-hand-written-notes-by-learning-to-read-write/",
    "originBody": "Home Blog A return to hand-written notes by learning to read & write October 28, 2024 Blagoj Mitrevski, Software Engineer, and Andrii Maksai, Staff Software Engineer, Google Research We present a model to convert photos of handwriting into a digital format that reproduces component pen strokes, without the need for specialized equipment. Quick links Paper GitHub Repo HuggingFace Repo Additional Info Share Copy link × Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in a vectorized form. However, a substantial gap remains between digital note-taking and traditional pen-and-paper note-taking, a practice still favored by a majority of people. Bridging this gap by converting a note taker’s physical writing into a digital form is a process called derendering. The result is a sequence of strokes, or trajectories of a writing instrument like a pen or finger, recorded as points and stored digitally. This is also known as an “online” representation of writing, or “digital ink”. The conversion to digital ink offers users who still prefer traditional handwritten notes access to their notes in a digital form. Instead of simply using optical character recognition (OCR), which would allow the writing to be transcribed to a text document, by capturing the handwritten documents as a collection of strokes, it's possible to reproduce them in a form that can be edited freely by hand in a way that is more natural. It allows the user to create documents with a realistic look that captures their handwriting style, rather than simply a collection of text. This representation allows the user to later inspect, modify or complete their handwritten notes, which gives their notes enhanced durability, seamless organization and integration with other digital content (images, text, links) or digital assistance. For these reasons, this field has gained significant interest in both academia and industry, with software solutions that digitize handwriting and hardware solutions that leverage smart pens or special paper for capture. The need for additional hardware and accompanying software stack is, however, an obstacle for wider adoption, as it creates both onboarding friction and carries additional expense for the user. With this in mind, in “InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write”, we propose an approach to derendering that can take a picture of a handwritten note and extract the strokes that generated the writing without the need for specialized equipment. We also remove the reliance on typical geometric constructs, where gradients, contours, and shapes in an image are utilized to extract writing strokes. Instead, we train the model to build an understanding of “reading”, so it can recognize written words, and “writing”, so it can output strokes that resemble handwriting. This results in a more robust model that performs well across diverse scenarios and appearances, including challenging lighting conditions, occlusions, etc. You can access the model and the inference code on our GitHub repo. Overview The key goal of this approach is to capture the stroke-level trajectory details of handwriting. The user can then store the resulting strokes in the note taking app of their choice. Left: Offline handwriting. Right: Output digital ink (online handwriting). In every word, character colors transition from red to purple, following the rainbow sequence, ROYGBIV. Within each stroke, the shade progresses from darker to lighter. Under the hood, we apply an off the shelf OCR model to identify handwritten words, then use the model to convert them to strokes. To foster reproducibility, reusability, and ease of adoption, we combine the widely popular and readily available ViT encoder with an mT5 encoder-decoder. Challenges While the fundamental concept of derendering appears straightforward — training a model that generates digital ink representations from input images — the practical implementation for arbitrary input images presents two significant challenges: Limited Supervised Data: Acquiring paired data with corresponding images and ground truth digital ink for supervised training can be expensive and time-consuming. To our knowledge, no datasets with sufficient variety exist for this task. Scalability to large images: The model must effectively handle arbitrarily large input images with varying resolutions and amount of content. Method Learning to read and write To address the first problem while avoiding onerous data collection, we propose a multi-task training setup that combines recognition and derendering tasks. This enables the model to generalize on derendering tasks with various styles of images as input, and injects the model with both semantic understanding and knowledge of the mechanics of writing handwritten text. This approach thus differs from methods that rely on geometric constructs, where gradients, contours, and shapes in an image are utilized to extract writing strokes. Learning to read enhances the model's capability in precisely locating and extracting textual elements from the images. Learning to write ensures that the resulting vector representation, the digital ink, closely aligns with the typical human approach of writing in terms of physical dynamics and the order of strokes. Combined, these allow us to train a model in the absence of large amounts of paired samples, which are difficult to obtain. System workflow One solution to the problem of scalability is to train a model with very high-resolution input images and very long output sequences. However, this is computationally prohibitive. Instead, we break down the derendering of a page of notes into three steps: (1) OCR to extract word-level bounding boxes, (2) derendering each of the words separately, and (3) replacing the offline (pixel) representation of the words with the derendered strokes using the color coding described above to improve visualization. To narrow the domain gap between the synthetic images of rendered inks and the real photos, we augment the data in tasks that take rendered ink as input. Data augmentation is done by randomizing the ink angle, color, stroke width, and by adding Gaussian noise and cluttered backgrounds. Vision-language model for digital ink We create a training mixture that comprises five different task types. The first two tasks are derendering tasks (i.e., they generate a digital ink output). One uses only an image as input and the other uses both an image and the accompanying text that has been recognized by the OCR model. The following two tasks are recognition tasks that produce text output, the first of which leverages real images and the latter, synthetic ones. Finally, a fifth task is a combination of recognition and derendering, hence a mixed task with text-and-ink output. Each type of task utilizes a task-specific input text, enabling the model to distinguish between tasks during both training and inference. Below you will find a recognition and a derendering task. Derendering with text: Takes an image and a text input and outputs the ink that would generate that text in the style of the image. Recognition of synthetic images: Takes an image and recognizes what is written within. To train the system, we pair images of text and corresponding digital ink. The digital ink is sampled from real-time writing trajectories and subsequently represented as a sequence of strokes. Each stroke is represented by a sequence of points, obtained by sampling from the writing or drawing trajectory at a constant rate (e.g., 50 points per second). The corresponding image is created by rendering the ink - creating a bitmap at a prespecified resolution. This creates a pixel-stroke correspondence, that is a precursor for the model input-output pairs. A further necessary step, and a unique one for this modality, is the ink tokenizer, which represents the points in a format that is friendly to a large language model (LLM). Each point is converted into two tokens, one each encoding its x and y coordinates. The token sequence for this ink begins with b, signifying the beginning of the stroke, followed by the tokens for the coordinates of the sampled points. Illustration of the ink tokenization for a single-stroke ink. The dark red ink depicts the ink stroke, with numbered circles marking sampled points after time resampling. The color gradient of the sampled points (1–7) indicates the point order. Each point is represented with two tokens encoding its coordinates x (left half of the shaded box) and y (right half). The token sequence for this ink begins with b, signifying the beginning of the stroke, followed by the tokens for coordinates of sampled points. Results To evaluate the performance of our approach, we first collected an evaluation dataset. We started with OCR data, and then added paired samples that we collected manually by asking people to trace text images they were shown (human-generated traces). We then trained three variants of the model: Small-p (∼340M parameters, “-p” for “public” setup), Small-i (“-i” for “in-house”), and Large-i (∼1B parameters). We compared our approach to a General Virtual Sketching (GVS) baseline. We show that the vector representations produced by our system are both semantically and geometrically similar to the input images, and are similar to human-generated digital ink data, as measured by both automatic and human evaluations. Qualitative evaluation We show the performance of our models and GVS compared to two public evaluation datasets, IAM and IMGUR5K, and an out of domain dataset of sketches. Our models mostly produce results that accurately reflect the text content, disregarding semantically irrelevant background. They can also handle occlusions, highlighting the benefit of the learned reading prior. In contrast, GVS produces multiple duplicate strokes and has difficulty distinguishing between background and foreground. Our Large-i model is further able to retain more details and accommodate more diverse image styles. See the paper for more examples. Comparison between performance of GVS, Small-i, Small-p, and Large-i on two public evaluation datasets (Rows 1–3, IAM; rows 4–6, IMGUR5K). Out-of-domain behavior: Sketch derendering for Small-p, Small-i, Large-i and GVS. Our models are mostly able to derender simple sketches, however they do still exhibit significant artifacts like extraneous or misaligned strokes. Quantitative evaluation At present, the field has not established metrics or benchmarks for quantitative evaluation of this task. So, we conduct both human and automated evaluation to compare the similarity of our model output to the original image and to human-generated digital inks. Here we present the human evaluation results, with numerous other results derived from automated evaluations and an ablation study in our paper. We performed a human evaluation of the quality of the derendered inks produced by the three model variants. We used the “golden” human traced data from the HierText dataset as the control group and the output of our model on these samples as the experimental group. Comparison of the performance of our models (Small-p, Small-i and Large-i) and manual tracing on two samples of text of varying difficulty from the HierText dataset. In the figure above, notice the error in the quote for all models on the top row (the double-quote mark), which the human tracing got correct. On the bottom row the situation is reversed, with the human tracing focusing solely on the main word, missing most other elements. The human tracing is also not perfectly aligned with the underlying image, emphasizing the complexity and tracing difficulty of the handwritten parts of the HierText dataset. Evaluators were shown the original image alongside a rendered digital ink sample, which was either model-generated or human-traced (unknown to the evaluators). They were asked to answer two questions: (1) Is the digital ink output a reasonable tracing of the input image? (Answers: “Yes, it’s a good tracing,” “It’s an okay tracing, but has some small errors,” “It’s a bad tracing, has some major artifacts.”) (2) Could this digital ink output have been produced by a human? (Answers: “Yes” or “No”.) The evaluation included 16 individuals familiar with digital ink, but not involved in this research. Each sample was evaluated by three raters and aggregated with majority voting. The results show that a majority of derendered inks, generated with the Large-i model perform about as well as human-generated ones. Moreover 87% of the Large-i outputs are marked as good or having only small errors. Conclusion In this work we present a first-of-its-kind approach to convert photos of handwriting into digital ink. We propose a training setup that works without paired training data. We show that our method is robust to a variety of inputs, can work on full handwritten notes, and generalizes to out-of-domain sketches to some extent. Furthermore, our approach does not require complex modeling and can be constructed from standard building blocks. Acknowledgements We want to thank all the authors of this work, Arina Rak, Julian Schnitzler, and Chengkun Li, who formed a student team working with Google Research for the duration of the project, as well as Claudiu Musat, Henry Rowley and Jesse Berent. All authors, with the exception of the student team, are now part of Google Deepmind. Labels: Generative AI Human-Computer Interaction and Visualization Machine Perception Quick links Paper GitHub Repo HuggingFace Repo Additional Info Share Copy link × Other posts of interest October 21, 2024 Evaluating and enhancing probabilistic reasoning in language models Generative AI · Health & Bioscience · Natural Language Processing October 16, 2024 HDR photo editing with machine learning Machine Perception · Photography · Product October 1, 2024 Augmented object intelligence with XR-Objects Generative AI · Human-Computer Interaction and Visualization · Natural Language Processing",
    "commentLink": "https://news.ycombinator.com/item?id=41976311",
    "commentBody": "A return to hand-written notes by learning to read and write (research.google)532 points by mfiguiere 21 hours agohidepastfavorite173 comments imoverclocked 12 hours agoI recently purchased a small refrigerator whiteboard and it's been really amazing with the combination of my iPhone's ability to take a picture of my handwriting (script or cursive) and copy/pasta into a text. It's not always perfect (nor is my handwriting!) but it's good enough to just replace a character or two and hit send. This really tickles a bunch of things for me: 1) I am not sending a whole image (it's efficient) 2) I don't have to type/swipe at all (I'm not looking at a screen) 3) My s/o has easy access to the list at all times (it's not in a cloud) 4) It requires no power to update/maintain (markers last a long time) 5) It just feels so natural to grab a marker and write on the fridge when I exhaust something in that same fridge. reply cptcobalt 3 hours agoparentThis is a nifty idea. I bet there'll be a HN-correlated pop in fridge whiteboard sales from this. reply abdullahkhalids 21 hours agoprevThis is very cool. Here is interesting application of something like this. My handwriting is pretty bad, and worse still when writing fast. When I am teaching, a lot of what I write is worse than I would like it to be. I could teach a system like this my very slow neat handwriting. And then as I write on my whiteboard while teaching, it replaces my quick bad handwriting with the neater handwriting. reply Propelloni 12 hours agoparentImproving your hand writing is not hard. For whiteboards start out with using block letters only. It will slow you down in the beginning but not for long. That's one of the \"game changing\" hints I received during my time as a tutor at university. (One other was to always copy books from back to front; very useful but somewhat outdated now.) reply shiroiushi 9 hours agorootparent>One other was to always copy books from back to front; very useful but somewhat outdated now. What's so useful about that? reply albert_e 8 hours agorootparentThe photocopies come out stacked in the right order maybe? reply bee_rider 7 hours agorootparentThis would be a kind of interesting data structure. A stack, FILO, but you can flip it for reading (maybe flip is an expensive operation). reply byteknight 4 hours agorootparentGrass is good. reply Propelloni 5 hours agorootparentprevOthers already said it, but yeah, the copies came out ready-to-read. The university had, IIRC, Sharp copiers which put out the copy right-side up. reply hammock 8 hours agorootparentprevSlows you down if I had to guess reply mbreese 6 hours agorootparentIf your copies come out of the machine right-side up and they get stacked on top of each other, then you can just take the finished stack from the machine when finished. Otherwise, you’d have to reverse the stack, which in real life or in a computer is an expensive operation. reply hammock 3 hours agorootparentI thought we were talking about handwriting practice haha reply masfuerte 5 hours agorootparentprevMy mother had an inkjet printer where the pages came out right-side up. Multi-page documents finished up in reverse order. It was an infuriatingly awful piece of design. reply egypturnash 17 hours agoparentprevOr you could just find some lettering manuals and improve your handwriting. Practicing at a slow speed will improve your fast work, too. reply sitkack 19 hours agoparentprevIf you draw your equations well enough, they can get converted into LaTex in realtime and then you could run them in a computational notebook. Esp if you fuse the audio of you explaining the equations along with the LaTex it can correct for errors. reply abdullahkhalids 19 hours agorootparentWhat software can do this? reply elashri 19 hours agorootparentI think Mathpix API [1] can use used to do something like that in realtime/ish [1] https://mathpix.com/ reply rvnx 19 hours agorootparentprevThe new Calculator by Apple is supposed to do it (but the result is quite underwhelming) reply rnewme 21 hours agoparentprevWhy not simply have a laser projector, keyboard and canvas textbox then? reply elashri 20 hours agorootparentFrom a quick visit to his profile (linked website), he is a physicist. This technology setup is very complicated and against the eternal usage of blackboard in a typical physics department. And to be honest this applies to his suggestion as well but you still at least get the feeling on writing on a board. reply abdullahkhalids 19 hours agorootparentOf course, all true physics happens on the blackboard, in a notebook (or in Mathematica). But, I am forced to use digital tools occasionally, and I am not opposed to improving them. reply nine_k 20 hours agorootparentprevYou also need the skill of lightning-fast LaTeX typing, and the skill of drawing and drafting with a speed comparable to that of a chalk. You need a canvas-driven tool for that, and your eyes would be on the screen for long periods of time, not contacting the audience. reply rnewme 11 hours agorootparentThere are regular white/blackboards that you write on with regular marker that just has a tracker on it, so content appears on the canvas on screen for those who are remote. More advanced versions also have laser projector that can project animations, moving diagrams and text on the same board. My suggestion is to tap the board on the place he wants to write and just type it on regular keyboard, hardly a distraction! reply znpy 17 hours agoparentprev> My handwriting is pretty bad i know it might sound dumb, but have you tried playing with a fountain pen? The feedback is way different from a ballpoint pen and it also depends on paper and the kind of ink. It makes writing way less \"predictable\" and a bit more enjoyable. a cheap one (5-15$) with a medium nib might be a good start... some people move on to collect fountain pens, but i do most on my (on paper) writing with a ~20$ Pelikan Jazz. reply sph 10 hours agorootparentI know your comment is in earnest and I don't mean to make fun of you, but there is something so funny in our Americanised world where everything is reduced to \"it's not you, you just need to buy the correct gizmo that will solve all your problems.\" Fountain pens enthusiasts are like music gear or mechanical keyboard enthusiasts, that justify their hobby and believe spending on the next shiny thing is the key to fulfil their whatever, until the next shiny thing arrives. The thing is... if one dislikes or doesn't care about writing that they have basically forgotten how to, it is not spending money on a fancy writing implement that is gonna turn them into a medieval monk scribe. reply TeMPOraL 9 hours agorootparent> but there is something so funny in our Americanised world where everything is reduced to \"it's not you, you just need to buy the correct gizmo that will solve all your problems.\" OTOH, it's an improvement over the other world of solving everything through \"discipline\" and other kinds of wishful thinking. Like, you can complain and worry that your kid can't seem to learn how to cut things right with their scissors, try to force some discipline and conscientiousness into them - or you can stop causing them and yourself so much grief and realize that a left-handed person needs left-handed scissors, as using the wrong type for your hand works to prevent the cutting action. reply znpy 1 hour agorootparent> OTOH, it's an improvement over the other world of solving everything through \"discipline\" and other kinds of wishful thinking. kinda, the truth is somewhere in between in my opinion. most things related to our body movement do require training to be mastered. I think that writing is no different endeavur. It's just that fountain pens can make it more pleasant. reply Woeps 9 hours agorootparentprev> The thing is... if one dislikes or doesn't care about writing that they have basically forgotten how to, it is not spending money on a fancy writing implement that is gonna turn them into a medieval monk scribe. The thing is, it does actually... Because it \"forces\" you to slow down and take time for it. I hated writing (heavily dyslexic) but after getting a fountainpen and purposely slowing down I noticed it went better. Now I write in my own language again, I spend whole evenings writing scenes, essays, debates and letters. Sometimes it is the tool that forces a bit of change (if you want to change of course). reply kstrauser 4 hours agorootparentprevA LAMY Safari with its triangle grip vastly improved my handwriting by forcing me to use a different grip than I’d naturally use. So yes, for me, the correct cheap gizmo solved all my problems. reply bluGill 3 hours agorootparentI've played with different miracle gizmos all my life, nothing worked. When I go slow and careful I can write like a 2nd grader. As a kid my teachers were constantly mad at me until one realized I was trying and got me testing - sadly dysgraphia (likely the diagnosis I needed, though I was never formally diagnosed) wouldn't exist for several more years and so I couldn't get the right help. (if any help exists, I haven't been able to find anything useful and I'm not sure as an adult if it is worth the time - there are so many other things I can do instead) reply kstrauser 3 hours agorootparentTo be clear, the Safari showed me that I can write without loathing the process. I feel I've given it a fair shot, though, and I'm back to typing everything instead. I'll use a paper and a pen to jot quick notes during a meeting or something but that's the extent of it. Turns out I can get by just fine with hardly ever handwriting anything. The only people disappointed by this are my elementary school teachers who had insisted this was something I needed to care about. reply cbm-vic-20 5 hours agorootparentprev> Fountain pens enthusiasts are like music gear or mechanical keyboard enthusiasts There is a subreddit /r/mechanicalheadpens for these three specific interests. reply graemep 4 hours agorootparentprevI definitely write better with some pens than with others, and probably best with fountain pens (I have lost my best fountain pen though). Its not going to make me write like a scribe creating an illuminated manuscript, but there is an awful lot of room for \"better\" between that and my usual handwriting with a cheap ball point. reply tiborsaas 9 hours agorootparentprevHaving a good pen really helps a lot with writing. I'm not a fountain pen enthusiast, I don't even have one. I just noticed that there's a huge difference between a ballpoint pen and a \"rolling ballpoint pen\". The naming is confusing, since the ballpoint pen should also be \"rolling\", but whatever. reply devilbunny 7 hours agorootparentBallpoint pens use a viscous ink that, like graphite in a pencil, needs pressure to be applied. Rollerballs and fountain pens both use low-viscosity inks that flow simply from being touched to the paper. The latter requires much less effort to write (no constant pressure) and enables writing with the hand held mostly still, using the larger muscles of the upper arm and shoulder to create the letters. The downside is that they can create impressively large ink blots on your clothing if uncapped/unretracted, and the ink can be smeared if you touch it while wet. But pretty much every writing system still out there in use (i.e., not cuneiform or runes) was designed with a quill or brush as the instrument. reply tiborsaas 3 hours agorootparent> that flow simply from being touched to the paper Or just by itself if you bring it with you on a plane :) This looks like a perfect rabbit hole I'd be wise to avoid. At least I have a good excuse of being left handed since I would be constantly smearing all the wet ink. reply devilbunny 2 hours agorootparentAlthough it's not a beginner fountain pen in terms of cost (though it is not too expensive, basic models around $160), the Pilot/Namiki Vanishing Point is a retractable fountain pen that does not leak when retracted. The cartridges for it can, of course, get expensive, but a 1 mL syringe and a big bottle of ink (even Mont Blanc ink is only $25 for 60 mL) will let you refill them cheaply. reply znpy 1 hour agorootparentI made the original suggestion explicitly to avoid the rabbitholes. I use a Pelikan Jazz (20$) and a bucket of black cartidriges (100 pieces) I got off amazon for like 8$ (like two years ago). I got a kaweco fountain pen a few months ago and i honestly regret spending those money, it's a shitty pen, some of the most dumbly-wasted money of my life. reply sph 4 hours agorootparentprev> But pretty much every writing system still out there in use (i.e., not cuneiform or runes) was designed with a quill or brush as the instrument. That's if you know cursive. I don't think the standard small-case script most people use in their everyday life is quill-friendly. reply devilbunny 2 hours agorootparentYour style might have to change a bit, but disconnected letters were quite common in medieval Roman-style scripts, which were definitely written with quills. reply fatbird 3 hours agorootparentprevIt would be more charitable to say that the change forced upon yourself by changing your instrument is a straightforward way of making you mindful of what you're doing and make it easier to break bad habits. Yes, you can break bad habits by not doing them with the same tools, but the objective is to change your behaviour, not to demonstrate personal Calvinism. reply znpy 1 hour agorootparentprev> but there is something so funny in our Americanised world where everything is reduced to \"it's not you, you just need to buy the correct gizmo that will solve all your problems.\" Sigh... I'm not american and i don't live in the US. > Fountain pens enthusiasts are like music gear or mechanical keyboard enthusiasts, that justify their hobby and believe spending on the next shiny thing is the key to fulfil their whatever, until the next shiny thing arrives. Yep, I'm aware, that's why i was explicit on the fact that a ~20$ Pelikan Jazz is just great and \"getting into fountain pens\" is something that you can definitely avoid (I do avoid it, as a matter of fact). > The thing is... if one dislikes or doesn't care about writing that they have basically forgotten how to, it is not spending money on a fancy writing implement that is gonna turn them into a medieval monk scribe. you do you i guess. good luck. reply Symbiote 5 hours agorootparentprevThe recommendation was to try a $5-15 pen. That's a long way from what you're describing. It's exactly what millions of schoolchildren are required to use when they're learning to write. reply sph 4 hours agorootparentSure, but billions of people can write just fine with a $0.50 ballpoint pen. Also, it's not $25 or $2,500 that will stop you from doing 95% of your writing on a physical and smartphone keyboard because that's how society works nowadays. This comment cannot be written with a fountain pen, nor my work emails or communication with friends and relatives. Many people can't write any more simply because they don't really have to. reply abdullahkhalids 2 hours agorootparentprevYou started a flame war. So let me just say that I exclusively use a fountain pen to write. That handwriting is a lot better. But when writing digitally for teaching or meetings, I use a wacom tablet. That is pretty bad for writing. reply creesch 10 hours agorootparentprevProbably good advice if you are right handed and have good fine motor skills. At least way into the 90s kids here learned to write with fountain pens. For me this meant pages full of smudges, forked pens and generally unreadable text. As soon as I was allowed to switch to a regular pen my handwriting improved a lot (still not great, but better). reply js8 6 hours agorootparentprevI think you don't even need to go to fountain pens. As a long time user of ballpoint pens, I recently started using gel pens and I wouldn't go back. reply dogmayor 5 hours agorootparentI still prefer using ballpoints with schmidt easyflow 9000 ink[1], but yeah rollerballs (gel) are a great midpoint between fountains and ballpoints. My Zebra G-750[2] is super smooth. [1] https://www.jetpens.com/Schmidt-EasyFlow-9000-Hybrid-Ballpoi... [2] https://www.jetpens.com/Zebra-G-750-Gel-Pen-0.7-mm-Black-Ink... reply xarope 16 hours agorootparentprevI've realized that when I use cheap pens on hotel stationery, my handwriting looks terrible, probably because the surfaces are too smooth? Other than fountain pens, are there other alternatives that give more tactile feedback? reply mbivert 11 hours agorootparentNibs / dip-pens ;-) It's only half a joke: having to regularly dip the pen in ink, be mindful of how much ink you have, having to swiftly wipe it once in a while to avoid drying ink (& flow issues), forces to slow down & take \"micro-breaks\". This benefits the handwriting, but also the quality of the study. And is surprisingly relaxing. (so called \"crow-quill\" nibs are relatively cheap, available, carry a fair amount of ink) reply wrp 6 hours agorootparentprevWhat you are asking for is more \"tooth\", something mostly determined by the paper. Most stationery fans prefer smoother papers, but I agree that after a certain point, increased smoothness makes my handwriting worse. If you are stuck using a very smooth paper, I would suggest either a fiber-tip or a drier gel pen. Gel pens with a clicker (e.g. Zebra Sarasa) tend to write drier than those with a cap (e.g. Uniball Signo). Also try a fatter tip. Although this may not be acceptable in many situations, a soft pencil may provide even better control on smooth paper. reply sam29681749 11 hours agorootparentprevIMO felt tip pens feel comparatively rough to gel, and ballpoint pen, etc. reply komali2 12 hours agorootparentprevI write a lot with a fountain pen and if anything it made it harder to understand my handwriting... even the next day I'll have a hard time. I've considered the \"just learn to write better\" approach and I've tried here and there but I've been handwriting journals for 28 years and I'm just not sure it's possible to write cleanly at the speed I handwrite at this point. Especially since it's in cursive. reply sam29681749 11 hours agorootparentI can't imagine anyone writing at their 'fastest' is going to produce something that is broadly legible. reply Dilettante_ 9 hours agorootparentprevSlow means smooth, smooth means fast. reply currymj 17 hours agoparentprevapple notes on new iPadOS does this right now -- just cleans up your handwriting to be slightly neater but still look like you. reply eleveriven 8 hours agorootparentThat's actually a brilliant feature! Without losing the \"you\" reply Wowfunhappy 20 hours agoparentprevWouldn't it make even more sense to replace your quick bad handwriting with perfect Helvetica? reply loginx 18 hours agorootparentI got a stylus for my iPad and i felt that way at the beginning but I learned that i visualize in my head what I'm going to create right before I draw, and the difference between what's in my mind vs what gets produced is so great, that it feels like a weird uncanny valley and i hate the output. reply ryukoposting 15 hours agorootparentprevNot necessarily - your handwriting can sometimes be subtly optimized to the sort of subject matter you write about. For example, when I write lowercase T, I give it a little hook at the bottom. I also write my lowercase L in the cursive form, the tall skinny loop. That keeps similar-looking letters that I use a lot distinct. There must be other examples. reply TRiG_Ireland 4 minutes agorootparentIn my handwriting, the letter x is two crossed lines; the algebraic variable x is two Cs back-to-back. This makes it more distinct from a multiplication sign. reply sam29681749 11 hours agorootparentprevWhen I'm taking notes, I use arrows, squiggly lines, symbols, I circle text, etc. which won't necessarily translate to typical text block. reply abdullahkhalids 18 hours agorootparentprevThere is (subjective) charm in handwritten text that is simply not present in digital fonts. I would rather keep that magic alive. reply deepGem 17 hours agorootparentI think handwriting is a very personal trait. Some people value good handwriting and they write neat even when they write fast. Others don't. Sadly, a vast majority if the world exists in the other camp. This is why they invented typewriters. If everyone conformed to good handwriting and could agree upon a good writing style standard, the world would have been very different. reply solveit 15 hours agorootparentI like good handwriting, but good, fast handwriting takes thousands of hours of practice. We used to spend a good chunk of school drilling it into kids, but now it's really hard to justify everyone spend that kind of time when we can technology our way around it and there are other valuable skills to learn. reply imoverclocked 14 hours agorootparentThat’s the same argument for using calculators in the past 30 years and ChatGPT in the last few years. At some point, we lose far more than we gain by “technology our way around” our early development. Why bother having campfires when we have portable space heaters? Heck, why bother camping at all if we have a nice comfortable space at home? More generally, why do any of the things that connect us to our past? Personally, I think it’s important to learn by doing and then provide a “here is how we made that easier, and now you know why” type of foundation. Perhaps better is having people develop versions of those solutions for themselves so we don’t just expect someone else to solve all of our problems. reply Neonlicht 8 hours agorootparentIf you live in an urbanised country you don't build a lot of campfires- besides there are rules on air quality that prohibit them. reply sfilmeyer 16 hours agorootparentprevI've known people who valued good handwriting but weren't particularly capable of writing neatly, and others who didn't particularly value it but were capable of staying neat while writing quickly. I think you might be overestimating how much of it is tied to what people value. reply eru 16 hours agorootparentprev> This is why they invented typewriters. Historically, that didn't seem to be the main reason for the many inventions of the typewriter. See https://en.wikipedia.org/wiki/Typewriter#History reply imoverclocked 12 hours agorootparentThat wikipedia article was a great read but I have to agree with GP. > According to the standards taught in secretarial schools in the mid-20th century, a business letter was supposed to have no mistakes and no visible corrections. Certainly, there were other reasons like speed, repeatability, official-looking, etc... too. Most typewriters wanted to be cheaper and on-demand printing presses. Some even managed variable width fonts! :) reply eru 11 hours agorootparentOh, the original comment is certainly right in spirit, I just wanted to be pedantic. But I'm not sure why you quote something about the md-20th century, when we are talking about the (many) invention(s) of the typewriter? They were already old and well-established technology at that point in time. reply recursive 20 hours agorootparentprevHard to make boxes and arrows out of helvetica, and basically impossible to do it quickly reply Wowfunhappy 19 hours agorootparentFine, make it Helvetica plus whatever other shapes you need. I would assume that Helvetica includes most of extended unicode, since it's such a widely used font, but I could be wrong, and it's beside the point. If the computer is the one transforming your writing it should be perfectly quick. To be clear, afaik current handwriting recognition software is not good enough for this. But if we had software that could transform bad handwriting into good handwriting, why not go all the way? reply advael 16 hours agorootparentCuz quality is not equivalent to standardization, and sometimes the latter is undesirable reply thimabi 19 hours agoprevFrom the title, I naively assumed this article would be about people relearning to make legible/beautiful handwritten notes after losing this ability. That is something I’m currently struggling with after many years of too much typing and not as much handwriting. Google’s actual research does help people like me, by making our notes less awful digitally. But I’d love not to be dependent on tech innovations to make my handwriting better. reply al_borland 18 hours agoparentIf you’re serious about this, I’ve stumbled into areas of YouTube with people dedicated to this. Pick a font of how you want your writing to look, and practice, practice, practice. People make available (and sell) special lined sheets to get the height of various things right or help to guide the writer to the perfect slant. You just have to have the time and interest to do the work, much like you probably did when first learning to write. reply thimabi 14 hours agorootparentHow do you recommend that practice to be like? Simply trying over and over to perfect each letterform on empty lined sheets, based on a reference? I’ve done a few handwriting workbooks, mostly consisting of block letter templates to fill. But each of them is incomplete on its own, and they don’t even share the same font, making it much harder to practice. reply poulpy123 7 hours agorootparentprevI'm also curious about the special sheets. Maybe I would be able to write legibly for once in my life :D reply prabhu-yu 11 hours agorootparentprevGood idea. May you please share youtube link you are referencing? reply dbtc 13 hours agorootparentprevAlso check out the resources on the r/handwriting subreddit. reply Al-Khwarizmi 10 hours agoparentprevThis has been mentioned somewhere else in the comment thread, but if you want to improve your handwriting, a good way is to use a fountain pen. My handwriting is immensely better with a fountain pen than with ballpoint or gel pens; I suppose partly because the fountain pen forces you to an optimal position and angle (it's much more inflexible about that, you can't just push it against the paper in any angle and expect it to write), and partly because it provides a smoother experience and feedback. You don't need to go overboard, the typical €20-ish Pilot Metro with medium nib or similar is more than enough. reply WillAdams 7 hours agoparentprevI suggested Kate Gladstone's Handwriting Repair site elsethread: https://handwritingrepair.info/ or see: https://sites.google.com/view/briem/free-books or John Howard Benson's lovely _The First Writing Book: Arrighi's Operina_ or Carolyn Knudsen's lovely _An Italic Calligraphy Handbook_ (which is much better than the modest title implies) and get a chisel edge marker or fountain pen. reply fatbird 3 hours agoparentprevI noticed my handwriting was terrible, and consciously improved it by writing slower and being more mindful of writing neatly. A fountain pen helped me slow down, but fundamentally it was just a matter of slowing down and consciously forming nicer characters until it became easy to do so, at which point my speed increased--but I retained the habit of paying enough attention to make nice characters. Deliberate practice (as in worksheets or exercises) is less important than just going as slow as you need to, to make the characters correctly, until your muscle memory builds up and brings back the speed. reply moron4hire 18 hours agoparentprevStudy comic lettering. Not saying it's the most efficient way of writing, but the process will teach you to think in terms of strokes and consistency. You can easily develop your own style from there. reply scrivanodev 8 hours agoprevVery interesting experiment. I've been working on a handwriting application [0] for the past couple of years and incorporating the ability to take a picture to convert it into digital ink would be really nice. [0] https://scrivanolabs.github.io reply aiisahik 10 hours agoprevCan it read the scribble of my doctor? If so this is groundbreaking in the medical data entry space. reply bobnamob 10 hours agoparentThe number of deaths attributable to misread treatment orders in hospitals is staggering. I'd be very careful about sticking another layer of interpretation between doctor and treating nurse. It's unfortunate med school doesn't teach block lettering like they used to teach to draftsmen/women. reply bradydjohnson 10 hours agorootparentAlmost no medical school or hospital in the United States use paper or handwriting anymore. All orders are electronic. reply blitzar 9 hours agorootparentprevIt says a lot that the illegible writing killed people yet appears to be affected by doctors in the same manner as a bimbo uses a vocal fry affectation. Glad to see my scripts from the doctor these days are typed not written. reply TRiG_Ireland 0 minutes agorootparent\"Vocal fry\" is a term some people made up to sound fancy. Actual linguists call it what it is: creaky voice. And they don't denigrate it. Men do it just as much as women, and there is (of course) no correlation with intelligence. It is a very lazy stereotype, and (of course) completely wrong, to associate \"vocal fry\" with \"bimbos\". sneak 10 hours agorootparentprevEntering this sort of data correctly should be on the doctor. McDonald’s digital order style giant pancake buttons with huge touch targets with wide margins and large type on a huge touchscreen should solve the boomer objections. Alternately, make it an app for the huge iPad Pro to solve the same problems. Make it as hard to fuck up as a fast food order. Disambiguation of input commands is a solved problem. There isn’t really the will. Stupid windows apps with standard windows UI dropdowns that confuse and frustrate people not well versed with computer UI, running on standard low contrast small type displays with standard keyboard and mouse input, running on laggy RDP thin clients to underprovisioned workstation VMs in a data center is sadly the industry norm, and it still kills people. reply brainzap 9 hours agoprevIt was a scary moment when Apple Notes corrected my writing in my own hand writing. reply pjmlp 8 hours agoprevI still look forward to have programming environments on tablets that are able to use pen input, instead of forcing us to carry a bluetooth keyboard. Apparently not something that anyone cares as business opportunity, because most likely most folks wouldn't pay for it, sadly. reply another-dave 8 hours agoparentI'm definitely faster at typing than writing though — especially so when it comes to something like code that often requires in-place editing, shuffling statements around etc. I do like to work with paper & pen but moreso for ideation, diagramming, or todo lists rather than more \"structured\" inputs. reply pjmlp 8 hours agorootparentNaturally the idea is to explore new concepts a mix of diagrams and coding, Basically what Brett Victor in recent times, and others have been trying to do for decades, \"Grail on Rand Tablet from 1968 demo\" https://www.youtube.com/watch?v=2Cq8S3jzJiQ Or \"Programmable Ink\" at Strange Loop 2022, https://www.youtube.com/watch?v=ifYuvgXZ108 There used to exist a graphical based Lisp for iPad, Lisping, unfortunely the app is long gone as it wasn't kept up to date with the OS. Here is on old thread discussing it, https://news.ycombinator.com/item?id=3802131 The new iPad Calculator, is a more recent example of such ideas. reply Vampiero 8 hours agoparentprevMan that would be horrible UX, I wouldn't pay for that. I'd only use it if someone paid ME to do it reply pjmlp 8 hours agorootparentDifferent strokes for different folks. reply tomjen3 8 hours agoparentprevCan you touch draw 120 WPM? reply sam29681749 4 hours agorootparentI can't imagine people code at an equivalent to 120wpm. reply pjmlp 8 hours agorootparentprevNo, nor do I care to type that fast. Have the secretary typewriter contests from the 1950's come back into fashion? reply sam29681749 4 hours agorootparentThey have in the form of monkeytype wpm scores. I think these are generally based on an a-z lower case test, so I doubt it's a realistic indicator of someone's actual typing speed. reply emporas 20 hours agoprevI tried to use tesseract for OCR, 10 years ago, it recognized English good enough. tesseract was also developed by Google if I am not mistaken, but open source. I tried to use it then, for non English language, for Greek, and it was very bad. Happy to see some good OCR research based on transformers. reply 0x38B 19 hours agoparentI’ve been really impressed with Tesseract - I used it last month to add invisible OCR text (1) to scanned PDFs I reference a lot. My scans are quite good, but I was still impressed with the accuracy. I also OCRed the TOC, playing with the page segmentation setting (2) in the terminal until I got output I could copy & paste to add a navigable table of contents. 1: with the help of https://github.com/ocrmypdf/OCRmyPDF 2: https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.h..., “ Using different Page Segmentation Modes” reply stavros 18 hours agorootparentI OCRed your comment with Tesseract: ``` I've been really impressed with Tesseract -used it last month to add invisible OCR text (1) to scanned PDFs I reference a lot. My scans are quite good, butwas still impressed with the accuracy.also OCRed the TOC, playing with the page segmentation setting (2) in the terminal untilgot output I could copy & paste to add a navigable table of contents. 1: with the help of https://github.com/ocrmypdfiOCRmyPDE 2: https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.h..., “ Using different Page Segmentation Modes” ``` This kind of mirrors my earlier experience with Tesseract, if it can't get OCRing a screenshot right, what can it get right? It's not like \"I used\" is such a rare phrase either, but it replaced the I with a pipe. reply llm_trw 17 hours agorootparent>if it can't get OCRing a screenshot right, what can it get right? Book scans which is what it was designed for. If you read the fine manual you would see that they suggest the _minimum_ resolution to run it over is an x-height of 20 pixels, screens have seldom have one higher than 10 pixels. With those settings I got the following out of OPs comment: I've been really impressed with Tesseract - I used it last month to add invisible OCR text (1) to scanned PDFs I reference a lot. My scans are quite good, but I was still impressed with the accuracy. I also OCRed the TOC, playing with the page segmentation setting (2) in the terminal until I got output I could copy & paste to adda navigable table of contents. 1: with the help of https://github.com/ocrmypdt/OCRmyPDF 2: https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.h..., “ Using different Page Segmentation Modes” reply 0x38B 18 hours agorootparentprevI OCRed an unprocessed screenshot from the chapter's table of contents (1), which gave me (2). The collated table of contents (3) was error free, but as your example shows, this OCR isn't good enough to not need checking and proof-reading. 1: https://nexus.armylane.com/files/vogue-sewing-11-toc-screens... 2: https://nexus.armylane.com/files/tesseract-ocr-output.png 3: https://nexus.armylane.com/files/vogue-toc.txt reply jahewson 13 hours agoparentprevTesseract was originally created by HP, open-sourced, and later developed by Google. It's based on techniques from the 1980s and is pretty underwhelming. But at least it's free! reply criddell 7 hours agorootparentWhen I read somebody praising Tesseract I always wonder how their experience could be so different from mine. It's so much worse than what you can do with a modern phone. reply albert_e 8 hours agoprevCan we replace / augment the keyboard with a white (or marked) paper to write on ... that is in view of a camera that has real time OCR Feels like it will be a good addition to input devices reply WhatsName 20 hours agoprevWhat is currently state-of-the-art when it comes to detetcting handwriting from photos? Tracing strokes is nice but I would be more interested in converting my handtaken notes to markdown. reply thimabi 18 hours agoparentI don’t know if they are the state-of-the-art, but handwriting recognition in iOS and ChatGPT do wonders for me — even with an ugly handwriting. Though these are more like 90% to 95% accurate, you should review the output before trusting it. reply burnished 15 hours agorootparentIts pretty remarkable. I've used my phone to take pictures of stickers with model information that I couldn't otherwise reach and was able to copy the text from it. Really wild stuff. reply markvdb 11 hours agorootparentprevI'm looking for something like that, but free/open source and offline. Suggestions welcome! reply vintermann 7 hours agoprevA model that could turn \"offline\" handwriting (the ink on the page) into \"online\" (order and timing of the strokes) I think could be really useful for a historical HTR pipeline... but ultimately, we need end to end. Why is historical HTR so neglected in all multi-task model evaluation benchmarks? There are millions of un-indexed handwritten historical documents which could give us a so much better understanding of our recent past. For that matter, it could give models much better understanding of our recent past. reply Pamar 12 hours agoprevEric Hebborn (https://en.m.wikipedia.org/wiki/Eric_Hebborn) wrote \"Italico per Italiani\" (available in Italian only, alas https://www.amazon.co.uk/Italico-italiani-moderno-trattato-c...) as a textbook/manual to improve day-to-day, practical calligraphy. Maybe there is something similar in other languages too... reply nxobject 10 hours agoprevAs a lay reader, it's fascinating to see how much oomph we're getting out of LLMs on non-language-related tasks by figuring out clever encodings/linearizations. reply poulpy123 7 hours agoprevI'm wondering how well it would work with my crappy writing and the fact I write some letters revere from normal reply no-reply 19 hours agoprevThis is very interesting. I had this idea of imitating human handwriting in my bucket of todos for machine learning models, but never got to it. I guess we aren't far from it. reply henning 20 hours agoprevCan this be used to create deepfake forged signatures/handwriting? reply 101008 20 hours agoparentThat was my initial thought too. For some important authors, there are a few high quality scans of their manuscripts, so with a tool like this you could create fake manuscripts -- and in a few years, after they are dead, say that you find them, create provenance, and boom, unpublished novel by JK Rowl- any author. reply Legend2440 15 hours agoparentprevNo. It is not designed to be used as a generative model. But other people have created handwriting generation models. reply vlovich123 12 hours agorootparentIf it can generate accurate shapes from a photo of a signature, why would you need a generative model? Just draw the SVG wherever you need it. reply fredgrott 8 hours agoprevIts not the writing PART! It is the non automation part! The experiment you can do to verify my point: 1. Write code by hand in plain text editor..... What you notice....programming syntax and knowledge becomes easier to retain and re-learn. No joke I do this several times a week. reply Evidlo 16 hours agoprevCan this work on low-power devices like note-taking tablets? reply Legend2440 15 hours agoparentYou may be able to run it on an iPad, but probably not a Kindle. But also, the point of this is to convert photos of text into pen strokes. A note-taking tablet doesn't need it because it already has the pen strokes you wrote. reply moatmoat 19 hours agoprevsuch an exciting research project! I can imagine the impact this could have on education, e.g. handwriting notes of teachers in digital copies; or even preserve old documents in their digital counterpart reply anshumankmr 9 hours agoprevfinally, I will be able to read my doctor's prescriptions. reply 8n4vidtmkvmk 18 hours agoprevCan I just get good OCR for handwritten text? The last model that claimed to be \"the best\" was atrocious, only worked on PDFs of papers. ChatGPT is pretty decent but I was hoping for an offline, tailored solution reply anu7df 12 hours agoparentCheck out textify on appstore. I have found the accuracy to be great even for my terrible hand written text. reply bilekas 20 hours agoprev> We present a model to convert photos of handwriting into a digital format that reproduces component pen strokes, without the need for specialized equipment. Call my a cynic but this feels like a free way for Google to pull more data for training. reply ipsum2 17 hours agoparentSure. You're wrong and a cynic. The model does not connect to Google servers (unless you decide to use their colab, which is optional), you can use it offline without contributing anything back in data or code. reply rgovostes 16 hours agorootparentAside from Google having published open source OCR tools like Tesseract for 20 years, it's a thoughtless accusation in general. What exactly is the insinuation? \"Training\" is just thrown out as a bogeyman. I can't even come up with a fictional scenario in which Google does something nefarious with piles of handwritten documents they've somehow acquired. reply sfilmeyer 16 hours agorootparentTraining doesn't have to be nefarious. Google ran GOOG-411 in large part to collect speech data, but folks using the service still benefited. reply bilekas 10 hours agorootparentprev> I can't even come up with a fictional scenario in which Google does something nefarious with piles of handwritten documents they've somehow acquired. I never said it would necessarily be nefarious, but it's the same behaviour of data collection from users of free services to benefit themselves financially. While not always being particularly careful with collected user data. A slightly related topic is around Google's training on YouTube subtitles. They're able to do this because they host all the content, but they dont allow owners of that content to opt out of that. Again, a free resource that Google get to play with as they feel like. reply rgovostes 9 hours agorootparentThe linked project is an academic paper published simultaneously with open source code and a pre-trained, locally runnable model. There is nothing that collects data here, and this is not a Google product that users interact with. Hence, there is no direct financial gain from this work. Google Research has published over 10,000 papers, few of which directly impact the commercial side of Google services. Your example of automated captioning for videos doesn't seem particularly objectionable. Does it translate, indirectly, to slightly more ad views? Probably, but it's a rounding error in their revenue. I am guessing that few content creators who publish on YouTube find this feature controversial: In addition to free hosting and revenue sharing, they don't have to bear the costs of writing captions while benefiting from accessibility and discoverability. There are valid complaints against data collection by these tech companies for machine learning. Artists have a point when they condemn generative models trained on their work. And you might reasonably object to the collection of handwriting samples that Google used here, which were scraped from public Imgur posts (Facebook Research's Imgur5K data set). But there's room for nuance in deciding what uses are fair and acceptable without the knee-jerk reaction of tech company + AI = evil financial motives. reply pkaye 19 hours agoparentprevThere is a repo with Apache license. Does that need to connect to Google services to use? reply delduca 19 hours agoparentprevNo evil. reply byte_0 19 hours agoparentprevThat’s exactly what I thought. reply honeybadger1 8 hours agoprevi still write notes daily and already this year have finished 4 notebooks...however, things i need to review are typed, i write to keep things in my mind as they are being discussed...example, if i encountered an error while reviewing a program execution i would write it down \"encountered error during attempt to do x\" but i would also type it in my notes in vastly more detail with screenshots and other points...handwriting to me is almost like tagging it in my mind so i just don't forget that it happened. reply llm_trw 18 hours agoprevNow I'm wondering if they can use a similar architecture to derender paintings. It would the quite something to have a stroke perfect recreation of the Mona Lisa drawn by a modified pen plotter. reply MacTea 13 hours agoparentRings a bell: https://arxiv.org/abs/2108.03798 reply jheriko 16 hours agoprevseems overkill... for a bunch of reasons tbh. I guess the golden sledgehammer is just too tempting. taking photos of notes is a weird compromise that nobody really wants... we need better tools not better post processing reply anu7df 12 hours agoparentI don't think this is a hypothetical use case, at least for me. I like writing on paper with a fountain pen. But would like a digital version of the notes that are searchable. Reasonable ocr exists for conversion to text, but this would may be give slightly more accurate results. reply PeterStuer 11 hours agoprevTbh my handwriting is write only. Through decades of computer use it has deteriorated to the point were it is illegible even to myself. Still. I will always be notetaking (and doodling) during meetings as it helps me order my thoughts. reply blitzar 9 hours agoparentStill. I will always be notetaking (and doodling) during meetings as it helps me not fall asleep and makes it look like I am paying attention. reply pratibha_simone 9 hours agorootparentSame here. reply anonzzzies 10 hours agoparentprevI learned 10 finger typing when I was 8 in school (42 years ago), and have been glued to computer since then ; if I write something, or even sign something, it is complete gibberish, even to myself. To me it looks like those last stages of dementia scribbling, but I know that's not it because it was like this since I was 12 or so; my teachers couldn't read my writing and it affected my grades. reply blensor 7 hours agorootparentSame goes for my signature, I almost get anxious when I have to sign something on paper. reply cubefox 7 hours agorootparentNo worries, signatures are expected to illegible scribbles. reply eleveriven 8 hours agorootparentprevFunny how it almost becomes its own language, legible only to the subconscious reply sneak 10 hours agorootparentprevI’m the same way, and between that and the fact that I can type 120+wpm, makes me angry every time someone expects me to write something. Making dirt smudges on dried tree pulp is as archaic and outmoded as carving marks into bones, and in my view has no place in modern society outside of art. reply Woeps 10 hours agorootparent> in my view has no place in modern society outside of art. For a lot of use cases I agree, but no place is to ... abrupt in my opinion. Computers for sure have a multiplier affect in many cases. But when you really want to think something trough it's advised to slow down. For those cases paper and pen is the perfect medium. And I have a personal preference when making notes with pen and paper about hikes and climbs I'm working on/doing. But that's just me reply JadeNB 8 hours agorootparent> But when you really want to think something trough it's advised to slow down. For those cases paper and pen is the perfect medium. Or chalk on blackboard/marker on whiteboard. A marvelous way to get thoughts flowing that just don't seem to otherwise. reply markisus 7 hours agorootparentprevI’m pretty sure most modern mathematics research takes place on pencil and paper or chalkboard. It’s too tedious to typeset everything in the research phase. It’s also way harder to draw little diagrams or new invented notations on the computer. reply cubefox 7 hours agorootparentIf mathematics was as young as programming, it wouldn't use any fancy symbols and make everything ASCII compatible. Though that wouldn't solve diagrams... reply WillAdams 7 hours agoparentprevTry Kate Gladstone's \"Handwriting Repair\" course: https://handwritingrepair.info/ the biggest thing which helped my handwriting was switching away from ball points to either felt tips or fountain pens. Getting a Newton Messagepad also helped markedly, and since then I've been using tablets w/ Wacom EMR styluses where possible (NCR-3125 running PenPoint (donated to the Smithsonian by the guy who bought it) through a Samsung Galaxy Book 3 Pro 360) and applications such as Nebo or Write by Stylus Labs. reply criddell 7 hours agoparentprevYou can fix it if you want to. By the time I graduated from school, my handwriting was pretty bad. I could read it, but nobody else really could. Then I got a Palm Pilot and it's primitive handwriting recognition forced me to slow down and make better letter forms. At the same time I devoted a little effort to improve my cursive as well. It's stuck with me and my handwriting is still fairly easy to read. My block caps look like something off an architectural drawing thanks to Palm's Graffiti system beating me up nearly 30 years ago. reply ricardo81 11 hours agoparentprevSame. Over 20 years (note taking especially) it is reduced to scribbles. The value of it is the time thinking while writing it mostly, though todo's are longer lived. reply dr_sausages 10 hours agorootparentIf your todo list is, or needs to be long lived, I think you might have more problems :D I make notes on paper as the act of writing makes it stick about in my head longer than it would if I typed typing them out; though similar to you there's better penmanship from a spider that fell in ink. reply ricardo81 9 hours agorootparentLong lived may be tomorrow/a few days :-) And struck through. And written at any angle on the paper. Definitely not a codified method for me. reply eleveriven 8 hours agoparentprevDecades of typing, and now my handwriting is more of a cryptic art form than anything readable reply Neonlicht 8 hours agorootparentEven my mother who is 68 has severely deteriorated handwriting. Nobody writes by hand in a Western country since the smartphone. reply cubefox 6 hours agorootparentIt's interesting that for decades, technologists were stubbornly predicting that the paperless office was just around the corner: https://en.wikipedia.org/wiki/Paperless_office These predictions were famously overoptimistic. Paradoxically, paper consumption went up for a long time instead, because printers became cheaper or better, and wysiwyg word processors made it far easier than type writers to produce complex documents, with pictures and diagrams and tables. But approximately since the smartphone era, printer usage indeed seems in sharp decline. The paperless office, at last, became reality. Just a few decades later than expected. reply bluGill 3 hours agorootparentIf I could get a digital music stand that worked well and wasn't expensive I'd get rid of a lot of my print. (I need one for the piano, one for the keyboard, one for each kid into music, so cost is very important!) reply Writingdorky 9 hours agoparentprevI like this 'my handwriting is write only'. when i was 18, i looked at notes from my new coworker / manager and asked him how he learned to write that nice. He told me that he struggled writing cursive and just started writing letter by letter (print letter style?). I changed my 'font' on that day and suddenly i was at least able to read what i wrote! reply albert_e 8 hours agorootparentInteresting. We grew up learning to write separate letters. Cursive was taught only very briefly as a way of telling us tgis also exists but don't bother too much about it. reply soco 7 hours agorootparentNevertheless, cursive is where the speed is. University times I was writing at the speed of the professor chatting and still having it halfway readable even for others - even though the looks of it were tending to the Arabic (I'm Latin). Good times, now I can barely scratch a shopping list before I break my wrist. reply jillesvangurp 9 hours agoparentprevSame here. And not only that, years of keyboard and mouse usage have done some damage to my wrists and nerves and it's actually painful for me to write. reply soco 8 hours agorootparentI believe that pain comes from misplaced expectations. We remember the times when we were fluent writing, or see it in others, and expect our hand to follow the same pattern with the same speed. So we push too hard. We should realize instead our hands must re-learn to write at this time, drawing the letters so slow like the small children do. reply artemonster 9 hours agoparentprevUse boox or remarkable! You get both advantages of having the writing experience and their automatic recognition turns your scribbles into text! reply globular-toast 7 hours agoparentprevSo you can't explain something to people on a whiteboard? I've been typing since I was a child but I'm still perfectly able to write legibly. If I thought my handwriting was that bad I would try to fix it as not being able to whiteboard would be terrible. reply viraptor 7 hours agorootparentMany of us don't do physical whiteboards anymore. I diagram a lot, but all of it is online. Apart from own satisfaction, there's no reason for me to get better handwriting. (And there lots of things more satisfying than that) Addressing a letter once a year without a printed label, I can use block letters. reply backtoyoujim 20 hours agoprev [–] I spent a few years doing this without google. I suggest it, and I suggest it without google. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Research engineers have developed a model that converts handwritten photos into digital ink, capturing pen strokes without needing specialized equipment, a process known as derendering.",
      "This method differs from Optical Character Recognition (OCR) by preserving the style and dynamics of handwriting, allowing for editable and realistic digital representations of handwritten notes.",
      "The model uses a multi-task training setup, vision-language model, and data augmentation, making it robust, scalable, and effective without requiring paired training data, with performance comparable to human-generated digital ink."
    ],
    "commentSummary": [
      "A user described using a refrigerator whiteboard and an iPhone to efficiently digitize handwritten notes, emphasizing the natural feel of this method.",
      "The discussion included tips for improving handwriting, such as using block letters or fountain pens, and mentioned tools like Mathpix for converting handwritten equations to LaTeX.",
      "The conversation also addressed the decline in handwriting skills due to technology, with some participants advocating for preserving these skills for personal and educational benefits."
    ],
    "points": 532,
    "commentCount": 173,
    "retryCount": 0,
    "time": 1730149696
  },
  {
    "id": 41979203,
    "title": "How I write code using Cursor",
    "originLink": "https://www.arguingwithalgorithms.com/posts/cursor-review.html",
    "originBody": "In forums relating to AI and AI coding in particular, I see a common inquiry from experienced software developers: Is anyone getting value out of tools like Cursor, and is it worth the subscription price? A few months into using Cursor as my daily driver for both personal and work projects, I have some observations to share about whether this is a \"need-to-have\" tool or just a passing fad, as well as strategies to get the most benefit quickly which may help you if you'd like to trial it. Some of you may have tried Cursor and found it underwhelming, and maybe some of these suggestions might inspire you to give it another try. I am not sponsored by Cursor, and I am not a product reviewer. I am neither championing nor dunking on this as a product, but rather sharing my own experience with it. Who am I, and who is the audience for this article? I have been writing code for 36 years in a number of languages, but professionally focused on C-heavy computer game engines and Go/Python/JS web development. I am expecting readers to be similarly reasonably comfortable and productive working in large codebases, writing and debugging code in their chosen language, etc. I would give very different advice to novices who might want an AI to teach them programming concepts or write code for them that is way beyond their level! For me, the appeal of an AI copilot is in taking care of boilerplate and repetitive tasks for me so I can focus on the interesting logic for any given problem. I am also not especially interested in cranking out large quantities of code automatically; I am highly skeptical of \"lines of code written\" as an efficiency metric. I would prefer to spend less time writing the same amount of code and more time thinking through edge cases, maintainability, etc. So, without further ado: What is Cursor? Cursor1 is a fork of Visual Studio Code (VS Code) which has Large Language Model (LLM) powered features integrated into the core UI. It is a proprietary product with a free tier and a subscription option; however, the pricing sheet doesn't cover what the actual subscriber benefits are and how they compare to competing products. I'll try to clarify that when discussing the features below based on my own understanding, but a quick summary: Tab completion: This is a set of proprietary fine-tuned models that both provide code completion in the editor, as well as navigate to the next recommended action, all triggered by the Tab key. Only available to subscribers. Inline editing: This is a chat-based interface for making edits to selected code with a simple diff view using a foundation model such as GPT or Claude. Available to free and paid users. Chat sidebar: This is also a chat-based interface for making larger edits in a sidebar view, allowing more room for longer discussion, code sample suggestions across multiple files, etc. using a foundation model such as GPT or Claude. Available to free and paid users. Composer: This is yet another chat-based interface specifically meant for larger cross-codebase refactors, generating diffs for multiple files that you can page through and approve, also using a foundation model such as GPT or Claude. Available to free and paid users. Tab completion While other LLM-powered coding tools focus on a chat experience, so far in my usage of Cursor it's the tab completion that fits most naturally into my day-to-day practice of coding and saves the most time. A lot of thought and technical research has apparently gone into this feature, so that it can not only suggest completions for a line, several lines, or a whole function, but it can also suggest the next line to go to for the next edit. What this amounts to is being able to make part of a change, and then auto-complete related changes throughout the entire file just by repeatedly pressing Tab. One way to use this is as a code refactoring tool on steroids. For example, suppose I have a block of code with variable names in under_score notation that I want to convert to camelCase. It is sufficient to rename one instance of one variable, and then tab through all the lines that should be updated, including the other related variables. Many tedious, error-prone tasks can be automated in this way without having to write a script to do so: Sometimes tab completion will indepedently find a bug and propose a fix. Many times it will suggest imports when I add a dependency in Python or Go. If I wrap a string in quotes, it will escape the contents appropriately. And, as with other tools, it can write whole functions based on just the function signature and optional docstring: All in all, this tool feels like it is reading my mind, guessing at my next action, and allowing me to think less about the code and more about the architecture of I am building. Also worth noting: The completions are incredibly fast, and I never felt a delay waiting for a suggestion. They appear basically as soon as I stop typing. Having too long a wait would surely be a deal-breaker for me. So, what are my complaints with Tab completion? One is a minor annoyance: Sometimes I don't see the suggestion in time and continue typing, and the completion disappears. Once it is gone, there doesn't appear to be any way to get it to come back, so I have to type something else and hope. My other complaint is the exact opposite situation: Sometimes a completion is dead wrong, and I intentionally dismiss it. Subsequently, but very infrequently, I will accept a totally different completion and the previously-declined suggestion will quietly be applied as well. This has already caused some hard-to-track-down bugs because I wasn't aware the wrong logic had been accepted. I haven't found these cases to be frequent enough to cancel out the productivity boost of tab completion, but they do detract from it. Inline editing, chat sidebar, and composer As far as I can tell, these features are all very similar in their interaction with a foundational model - I use Claude 3.5 Sonnet almost exclusively - and the variance is in the user interface. Inline editing can be invoked by selecting some code and pressing Ctrl-K/Cmd-K. I type in the desired changes, and get a nice diff in the file that I can accept or reject. I use this mostly to implement bits of code inside a function or make minor refactors. A good example of where this works great is if I have a loop over some tasks and I want to parallelize them: The chat sidebar is opened with Ctrl+L/Cmd+L, and gives more real estate for a multi-turn conversation, though one pet peeve I have with the LLM models I've tested so far is they will always return code first, rather than ask for clarification if there is any ambiguity. The suggested code has an Apply button that will create a diff in the currently selected file. This is useful for larger refactors within a single file, or creating a brand new file based on the file I have open. If additional files are relevant they can be added manually to the context, but Cursor will try to guess which files are relevant based on the query and an index it generates in the background. Here is an example which takes an application's database API and creates a REST API to access it, with parameter validation and correct HTTP status codes, then writes a client library to access that REST API: As another example, here I am using the chat sidebar to convert the client library from Python to Go. Note how the loosely-typed Python is converted to well-defined struct types and idiomatic Go including error handling! This is not a 1:1 rewrite at all: Finally, Composer is specifically meant for cross-file refactors. This is also the feature I use least, but provides a better user experience for reviewing multiple file diffs one at a time. .cursorrules file I did not realize this feature existed until I came across it in the (in my opinion too minimal) documentation, but the various chat modalities always include the contents of a .cursorrules file located at the root of the workspace to provide additional context. I've been experimenting with using this to inform the LLM of the repository's coding standards, common packages, and other documentation. This feature might help to solve one of the big roadblocks I have observed with Cursor: It does not follow coding styles and patterns unless they already exist in the same file you are editing. For example, at Khan Academy we use a proprietary library 2 for passing context between functions in Go. This is used for logging, HTTP requests, etc. so the LLM needs to be able to use it. This has been difficult in the past, but perhaps a well-written .cursorrules is a good first step. One current limitation is that there is only one of these files per workspace, so a monorepo like ours containing code in multiple languages is going to be more difficult to set up than a small repository with a small set of very consistently styled code. Also the documentation suggests that the .cursorrules file is only used for the chat modalities, not the tab completion. However I've experimented with having that file open in a pinned tab in the workspace and confirmed that it is possible to include it in the tab completion context that way at least. Changes to my workflow The most exciting thing about a tool like Cursor is not that I can write code faster, because honestly the actual writing of code is not the bottleneck; in fact, I often have to slow myself down to avoid focusing too much on the code and not enough on the high-level problem being solved. The real value is in changing how I code. It's still early days with this technology, but this is what I've found has changed about how I work and what I expect to see changing in the near future: I am much less likely to reach for a new library or a framework. No, I'm not going to start writing my own crypto libraries, but for small utilities it's easy enough to let the LLM write them to my bespoke needs than to pull in a general-purpose library. These libraries tend to start small and lightweight and then, because they are open and used by many people, accumulate functionality and cruft that I don't need. Many of these libraries only exist to reduce boilerplate, which felt like a necessary tradeoff when balanced against my time writing and maintaining that boilerplate but now that I can have the LLM do it for me it feels less worth the cost. And the cost can be substantial: Have you tried getting a Node.js project running a year or more after it was written? You may as well start from scratch. I also worry less about adhering to DRY (Don't Repeat Yourself) in my own code. Prematurely defining abstractions can create a lot of technical debt later on, so being able to create a lot of code with reference to other code without trying to pull it into a function or class allows me more flexibility, and I know that if I have to refactor shared logic out later, the LLM can help with that too. My willingness to use a language or framework I am less familiar with is much higher. For example, I've dabbled in R for years, especially for visualizing data. However, to be frank, I suck at it. I don't have a deep understanding of dplyr and it seems like there are always a dozen different ways to accomplish the same task. Now I describe the visualization I want, and I get correct data manipulation and ggplot visualization for it. Tasks that took an hour or more now take five minutes, so I am much less likely to give up and do it in Python instead. Maybe one of these days I'll even write something in Rust. Maybe. I find myself iterating quickly on small components before integrating them into the larger codebase. This is partly to work around the limitations of LLMs when working with larger codebases, but it also opens up interesting ways of working I hadn't considered before. As per the example above, I can prototype some logic in a dynamically typed language like Python, work out the technical details and then convert it to well-typed Go instantly to integrate into a web application. I can have the LLM generate test data automatically, or mock up a backend for me to write a frontend against. Why pay the tax of working in a mature codebase while I'm still proving out an idea? Summary Whether I'll be using Cursor in a few years or have moved on to another tool, I can't really tell. I am confident that at the time of writing this, Cursor is the best example of the potential of LLM coding assistants, and if you want to explore how this type of tool might be of value I suggest you give it a spin.",
    "commentLink": "https://news.ycombinator.com/item?id=41979203",
    "commentBody": "How I write code using Cursor (arguingwithalgorithms.com)361 points by tomyedwab 15 hours agohidepastfavorite328 comments CrendKing 12 hours agoI've been using AI to solve isolated problems, mainly as a replacement of search engine specifically for programming. I'm still not convinced of these \"write whole block of code for me\" type of use case. Here's my arguments against the videos from the article. 1. Snake case to camelCase. Even without AI we can already complete these tasks easily. VSCode itself has command of \"Transform to Camel Case\" for selection. It is nice the AI can figure out which text to transform based on context, but not too impressive. I could select one \":, use \"Select All Occurrences\", press left, then ctrl+shift+left to select all the keys. 2. Generate boilerplate from documentation. Boilerplate are tedious, but not really time-consuming. How many of you spend 90% of time writing boilerplate instead of the core logic of the project? If a language/framework (Java used to be, not sure about now) requires me to spend that much time on boilerplate, that's a language to be ditched/fixed. 3. Turn problem description into a block of concurrency code. Unlike the boilerplate, these code are more complicated. If I already know the area, I don't need AI's help to begin with. If I don't know, how can I trust the generated code to be correct? It could miss a corner case that my question didn't specify, which I don't yet know existing myself. In the end, I still need to spend time learning Python concurrency, then I'll be writing the same code myself in no time. In summary, my experience about AI is that if the question is easy (e.g. easy to find exactly same question in StackOverflow), their answer is highly accurate. But if it is a unique question, their accuracy drops quickly. But it is the latter case where we spend most of the time on. reply scosman 7 hours agoparentI started like this. Then I came around and can’t imagine going back. It’s kinda like having a really smart new grad, who works instantly, and has memorized all the docs. Yes I have to code review and guide it. That’s an easy trade off to make for typing 1000 tokens/s, never losing focus, and double checking every detail in realtime. First: it really does save a ton of time for tedious tasks. My best example is test cases. I can write a method in 3 minutes, but Sonnet will write the 8 best test cases in 4 seconds, which would have taken me 10 mins of switching back and forth, looking at branches/errors, and mocking. I can code review and run these in 30s. Often it finds a bug. It’s definitely more patient than me in writing detailed tests. Instant and pretty great code review: it can understand what you are trying to do, find issues, and fix them quickly. Just ask it to review and fix issues. Writing new code: it’s actually pretty great at this. I needed a util class for config that had fallbacks to config files, env vars and defaults. And I wanted type checking to work on the accessors. Nothing hard, but it would have taken time to look at docs for yaml parsing, how to find the home directory, which env vars api returns null vs error on blank, typing, etc. All easy, but takes time. Instead I described it in about 20 seconds and it wrote it (with tests) in a few seconds. It’s moved well past the stage “it can answer questions on stack overflow”. If it has been a while (a while=6 months in ML), try again with new sonnet 3.5. reply DeathArrow 6 hours agorootparent>My best example is test cases. I can write a method in 3 minutes, but Sonnet will write the 8 best test cases in 4 seconds For me it doesn't work. Generated tests fail to run or they fail. I work in large C# codebases and in each file I have lots of injected dependencies. I have one public method which can call lots of private methods in the same class. AI either doesn't properly mock the dependencies, either ignores what happens in the private methods. If I take a lot of time guiding it where to look, it can generate unit tests that pass. But it takes longer than if I write the unit tests myself. reply dep_b 5 hours agorootparentFor me it's the same. It's usually just some hallucinated garbage. All of these LLM's don't have the full picture of my project. When I can give them isolated tasks like convert X to Y, create a foo that does bar it's excellent, but for unit testing? Not even going to try anymore. I write 5 unit tests manually that work in the time I write 5 prompts that give me useless stuff that I need to add manually. Why can't we have a LLM cache for a project just like I have a build cache? Analyze one particular commit on the main branch very expensively, then only calculate the differences from that point. Pretty much like git works, just for your model. reply ssijak 1 hour agorootparent\"It's usually just some hallucinated garbage. All of these LLM's don't have the full picture of my project.\" Cursor can have whole project in the context, or you can specify specific files that you want. reply scosman 5 hours agorootparentprevWith Cursor you can specify which files it reads before starting. Usually have to attached one or two to get an ideal one-shot result. But yeah, I use it for unit testing, not integration testing. reply throwup238 4 hours agorootparentprevAsk Cursor to write usage and mocking documentation for the most important injected dependencies, then include that documentation in your context. I’ve got a large tree of such documentation in my docs folder specifically for guiding AI. Cursor’s Notebook feature can bundle together contexts. I use Cursor to work on a Rust Qt app that uses the main branch of cxx-qt so it’s definitely not in the training data, but Claude figures out how to write correct Rust code based on the included documentation no problem, including the dependency injection I do through QmlEngine. reply rubymamis 4 hours agorootparentSounds interesting, what are you working on? (Fellow Qt developer) reply throwup238 3 hours agorootparentSame thing: https://news.ycombinator.com/item?id=40740017 :) Just saw you published your block editor blog post. Look forward to reading it! reply tyre 6 hours agorootparentprevI’ve found it better at writing tests because it tests the code you’ve written vs what you intended. I’ve caught logic bugs because it wrote tests with an assertion for a conditional that was backwards. The readable name of the test clearly pointed out that I was doing the wrong thing (the test passed?.) reply scosman 6 hours agorootparentInteresting. I’ve had the opposite experience (I invert or miss a condition, it catches it). It probably comes down to model, naming and context. Until Sonnet 3.5 my experience was similar to yours. After it mostly “just works”. reply williamdclt 1 hour agorootparentprevThat sounds more like a footgun than a desirable thing to be honest! reply scosman 4 hours agorootparentprevMaybe a TLDR from all the issues I'm reading in this thread: - It's gotten way better in the last 6 months. Both models (Sonnet 3.5 and new October Sonnet 3.5), and tooling (Cursor). If you last tried Co-pilot, you should probably give it another look. It's also going to keep getting better. [1] - It can make errors, and expect to do some code review and guiding. However the error rates are going way way down [1]. I'd say it's already below humans for a lot of tasks. I'm often doing 2/3 iterations before applying a diff, but a quick comment like \"close, keep the test cases, but use the test fixture at the top of the file to reduce repeated code\" and 5 seconds is all it takes to get a full refactor. Compared to code-review turn around with a team, it's magic. - You need to learn how to use it. Setting the right prompts, adding files to the context, etc. I'd say it's already worth learning. - I just knows the docs, and that's pretty invaluable. I know 10ish languages, which also means I don't remember the system call to get an env var in any of them. It does, and can insert it a lot faster than I can google it. Again, you'll need to code review, but more and more it's nailing idiomatic error checking in each language. - You don't need libraries for boilerplate tasks. zero_pad is the extreme/joke example, but a lot more of my code is just using system libraries. - It can do things other tools can't. Tell it to take the visual style of one blog post and port to another. Take it to use a test file I wrote for style reference, and update 12 other files to follow that style. Read the README and tests, then write pydocs for a library. Write a GitHub action to build docs and deploy to GitHub pages (including suggesting libraries, deploy actions, and offering alternatives). Again: you don't blindly trust anything, you code review, and tests are critical. [1] https://www.anthropic.com/news/3-5-models-and-computer-use reply DeathArrow 3 hours agorootparentYes, it works for new code and simple cases. If you have large code bases, it doesn't have the context and you have to baby it, telling which files and functions it should look into before attempting to write something. That takes a lot of time. Yes, it can do simple tasks, like you said, writing a call to get the environment variables. But imagine you work on a basket calculation service, where you have base item prices, where you have to apply some discounts based on some complicated rules, you have to add various kinds of taxes for various countries in the world and you have to use a different number of decimals for each country. Each of your classes calls 5 to 6 other classes, all with a lot of business logic behind. Besides that, you also make lots of API calls to other services. What will the AI do for you? Nothing, it will just help you write one liners to parse or split strings. For everything else it lacks context. reply JamesSwift 34 minutes agorootparentAre you suggesting you would inline all that logic if you hand-rolled the method? Probably not, right? You would have a high-level algorithm of easily-understood parts. Why wouldnt the AI be able to 1) write that high-level algorithm and then 2) subsequently write the individual parts? reply throwup238 4 hours agorootparentprev> Instant and pretty great code review: it can understand what you are trying to do, find issues, and fix them quickly. Just ask it to review and fix issues. Cursor’s code review is surprisingly good. It’s caught many bugs for me that would have taken a while to debug, like off by one errors or improperly refactored code (like changing is_alive to is_dead and forgetting to negate conditionals) reply scosman 7 hours agorootparentprevAnother fun example from yesterday: pasted a blog post in markdown into a HTML comment. Selected it and told sonnet to convert it to HTML using another blog post as a style reference. Done in 5 seconds. reply kccqzy 6 hours agorootparentAnd how do you trust that it didn't just alter or omit some sentences from your blog post? I just use Pandoc for that purpose and it takes 30 seconds, including the time to install pandoc. For code generation where you'll review everything, AI makes sense; but for such conversion tasks, it doesn't because you won't review the generated HTML. reply TeMPOraL 5 hours agorootparent> it takes 30 seconds, including the time to install pandoc On some speedrunning competition maybe? Just tested on my work machine, `sudo apt-get pandoc` took 11 seconds to complete, and it was this fast only because I already had all the depndencies installed. Also I don't think you'll be able to fulfill the \"using another blog post as a style reference\" part of GP's requirements - unless, again, you're some grand-master Pandoc speedrunner. Sure, AI will make mistakes with such conversion tasks. It's not worth it if you're going to review everything carefully anyway. In code, fortunately, you don't have to - the compiler is doing 90% of the grunt work for you. In writing, depends on context. Some text you can eyeball quickly. Sometimes you can get help from your tool. Literally yesterday I back-ported a CV from English to Polish via Word's Translation feature. I could've done it by hand, but Word did 90% of it correctly, and fixing the remaining issues was a breeze. Ultimately, what makes LLMs a good tool for random conversions like these is that it's just one tool. Sure, Pandoc can do GP's case better (if inputs are well-defined), but it can't do any of the 10 other ad-hoc conversions they may have needed that day. reply adamc 4 hours agorootparentInstalling pandoc is basically a one-time cost that is amortized over its uses, so... why worry about it? Relying on the compiler to catch every mistake is a pretty limited strategy. reply TeMPOraL 45 minutes agorootparent> Installing pandoc is basically a one-time cost that is amortized over its uses, so... why worry about it? Because space of problems LLMs of today solve well with trivial prompts is vast, far greater than any single classical tool covers. If you're comparing solutions to 100 random problems, you have to count in those one-time costs, because you'll need to use some 50-100 different tools to get through them all. > Relying on the compiler to catch every mistake is a pretty limited strategy. No, you're relying on the compiler to catch every mistake than can be caught mechanically - exactly the kind of things humans suck at. It's kind of the entire point of errors and warnings in compilers, or static typing for that matter. reply falconertc 5 hours agorootparentprevYou didn't also factor in the time to learn Pandoc (and to relearn it if you haven't used it lately). This is also just one of many daily use cases for these tools. The time it takes to know how to use a dozen tools like this adds up when an LLM can just do them all. reply kccqzy 4 hours agorootparentThis is actually how I would use AI: if I forgot how to do a conversion task, I would ask AI to tell me the command so that I can run it without rejiggering my memory first. The pandoc command is literally one line with a few flags; it's easily reviewable. Then I run pandoc myself. Same thing with the multitude of other rarely used but extremely useful tools such as jq. In other words, I want AI to help me with invoking other tools to do a job rather than doing the job itself. This nicely sidesteps all the trust issues I have. reply flir 1 hour agorootparentI do that constantly. jq's syntax is especially opaque to me. \"I've got some JSON formatted like . Give me a jq command that does . Google, but better. reply scosman 5 hours agorootparentprevRe:trust. It just works using Sonnet 3.5. It's gained my trust. I do read it after (again, I'm more a code reviewer role). People make mistakes too, and I think it's error rate for repeititve tasks is below most people's. I also learned how to prompt it. I'd tell it to just add formatting without changing content in the first pass. Then in a separate pass ask it to fix spelling/grammar issues. The diffs are easy to read. Re:Pandoc. Sure, if that's the only task I used it for. But I used it for 10 different ones per day (write a JSON schema for this json file, write a Pydantic validator that does X, write a GitHub workflow doing Y, add syntax highlighting to this JSON, etc). Re:this specific case - I prefer real HTML using my preferred tools (DaisyUI+tailwind) so I can edit it after. I find myself using a lot less boilerplate-saving libraries, and knowing a few tools more deeply. reply kccqzy 4 hours agorootparentWhy are you comparing its error rate for repetitive tasks with most people? For such mechanical tasks we already have fully deterministic algorithms to do it, and the error rate of these traditional algorithms is zero. You aren't usually asking a junior assistant to manually do such conversion, so it doesn't make sense to compare its error rate with humans. Normalizing this kind of computer errors when there should be none makes the world a worse place, bit by bit. The kind of productivity increase you get from here does not seem worthwhile. reply lgas 3 hours agorootparentprev> And how do you trust that it didn't just alter or omit some sentences from your blog post? How do you trust a human in the same situation? You don't, you verify. reply kccqzy 2 hours agorootparentWhat? Is this a joke? Have you actually worked with human office assistants? The whole point of human assistants is that you don't need to verify their work. You hire them with a good wage and you trust that they are working in good faith. It's disorienting for me to hear that some people are so blinded by AI assistants that they no longer know how human assistants behave. reply nuancebydefault 2 hours agorootparentIt appears op has a different experience. Each human assistant is different. reply sebastiansm 5 hours agorootparentprevWhat is the best workflow to code with an AI? Copy and paste the code to the Claude website? Or use an extension? o something else? reply scosman 5 hours agorootparentCursor. Mostly chat mode. Usually adding 1-2 extra files to the context before invoking, and selecting the relevant section for extra focus. reply paulluuk 5 hours agorootparentprevI personally use copilot, which is integrated into my IDE, almost identical to this Cursor example. reply ukuina 4 hours agorootparentDoes Copilot do multi-file edits now? reply rco8786 4 hours agoparentprevBefore you dismiss all of this because \"You could do it by hand just as easily\", you should actually try using Cursor. It only takes a few minutes to setup. I'm only 2 weeks in but it's basically impossible for me to imagine going back now. It's not the same as GH Copilot, or any of the other \"glorified auto-complete with a chatbox\" tools out there. It's head and shoulders better than everything else I have seen, likely because the people behind it are actual AI experts and have built numerous custom models for specific types of interactions (vs a glorified ChatGPT prompt wrapper). reply bee_rider 5 hours agoparentprevI’m slightly worried that these AI tools will hurt language development. Boilerplate heavy and overly verbose languages are flawed. Coding languages should help us express things more succinctly, both as code writers and as code readers. If AI tools let us vomit out boilerplate and syntax, I guess that sort of helps with the writing part (maybe. As long as you fully understand what the AI is writing). But it doesn’t make the resulting code any more understandable. Of course, as is always the case, the tools we have now are the dumbest they’ll ever be. Maybe in the future we can have understandable AI that can be used as a programming language, or something. But AI as a programming language generator seems bad. reply wry_discontent 4 hours agorootparentI used to agree with this, but the proliferation of Javascript made me realize that newer/better programming languages were already not coming to save us. reply shinycode 11 hours agoparentprevI agree. I replaced SO with cGPT and it’s the only good case I found. Finding an answer I build onto. But outsourcing my reflexion ? That’s a dangerous path. I tried on small projects to do that, building a project from scratch with cursor just to test it. Sometimes it’s right on spot but in many instances it misses completely some cases and edge cases. Impossible to trust blindly. And if I do so and not take proper time to read and think about the code the consequences pile up and make me waste time in the long run because it’s prompt over prompt over prompt to refine it and sometimes it’s not exactly right. That messes up my thinking and I prefer to do it myself and use it as a documentation on steroids. I never used google and SO again for docs. I have the feeling that relying on it to much to write even small blocs of code will make us loose some abilities in the long run and I don’t think that’s a good thing. Will companies allow us to use AI in code interviews for boilerplate ? reply Moru 9 hours agorootparentThe AI's are to a large degree trained on tutorial code, quick examples, howto's and so on from the net. Code that really should come with a disclamer note: \"Dont use in production, only example code.\". This leads to your code being littered with problematic edge-cases that you still have to learn how to fix. Or in worst case you don't even notice that there are edge cases because you just copy-pasted the code and it works for you. The edge cases your users will find with time. reply scosman 7 hours agorootparentAI is trained on all open source code. I’m pretty sure that’s a much larger source of training data than web tutorials. reply TeMPOraL 5 hours agorootparentprevIsn't tutorial-level code exactly the best practices that everyone recommends these days? You know, don't write clever code, make things obvious to juniors, don't be a primadonna but instead make sure you can be replaced by any recently hired fresh undergrad, etc.? :) reply achierius 4 hours agorootparentNot really. For example, tutorial code will often leave out edge cases so as to avoid confusing the reader: if you're teaching a new programmer how to open a file, you might avoid mentioning how to handle escaping special characters in the filename. reply Moru 3 hours agorootparentDon't forget about Little Bobby Tables! These types of tutorials probably killed the most databases over time. reply agumonkey 8 hours agorootparentprevWhich makes me wonder, if old companies with a history of highly skilled teams would train local models, how better would they be at helping solve new complex problems. reply SirHumphrey 7 hours agorootparentThey kinda already are - source code for highly complex open source software is already in the training datasets. The problem is that tutorials are much more descriptive (why the code is doing something, how does this particular function work etc. -down to a level of a single line of code), which probably means it’s much easier to interpret for llm-s, therefore weighted higher in responses. reply JamesBarney 9 hours agoparentprevDisagree. I still spend a good amount of time on boilerplate. Stuff that's not thinking hard about the problem I'm trying to solve. Stuff like units tests, error logging, naming classes, methods and variables. Claude is really pretty good at this, not as good as the best code I've read in my career but definitely better than average. When I review sonnets code the code is more likely to be correct than if I review my own. If I make a mistake I'll read what I intended to write, and not what I actually wrote. Where as when I review sonnets there's 2 passes so the chance an error slips through is smaller. reply pylua 9 hours agorootparentUnit tests are boiler plate ? reply TeMPOraL 5 hours agorootparentYes. You write a function ApplyFooToBar(), and then unit tests that check that, when supplied with right Foos, the function indeed applies those Foos to the Bar. It's not a very intellectually challenging work. If anything, the challenge is with all the boilerplate surrounding the test, because you can't just write down what the test checks themselves - you need to assemble data, assemble expected result, which you end up DRY-ing into support modules once you have 20 tests needing similar pre-work, and then there's lots of other bullshit to deal with at the intersection between your programming language, your test framework, and your modularization strategy. reply JamesBarney 9 hours agorootparentprevI'm using an expansive definition of boiler plate to be sure. But like boiler plate most unit tests require a little bit of thought and then a good amount of typing, doing things like setting up the data to test, mocking up methods, writing out assertions to test all your edge cases. I've found sonnet and o1 to be pretty good at this. Better than writing the actual code because while modifying a system requires a lot of context of the overall application and domain, unit testing a method usually doesn't. reply uh_uh 8 hours agorootparentprevQuite often, yes. That's why I prefer integration tests. reply girvo 6 hours agorootparentIndeed. To many tests are just testing nothing other than mocks. That goes for my coworkers directly and for their Copilot output. They’re not useful tests, they are thing to catch actual errors, they’re maybe useful as usage documentation. But in general, they’re mostly a waste. Integration tests, good ones, are harder but far more valuable. reply thunky 6 hours agorootparent> To many tests are just testing nothing other than mocks Totally agree, and I find that they don't help with documentation much either, because the person that wrote it doesn't know what they're trying to test. So it only overcomplicates things. Also harmful because it gives a false sense of security that the code is tested when it really isn't. reply viraptor 11 hours agoparentprev> I could select one \":, use \"Select All Occurrences\" Only if it's the same occurrences. Cursor can often get the idea of what you want to do with the whole block of different names. Unless you're a vim macro master, it's not easily doable. > How many of you spend 90% of time writing boilerplate instead of the core logic of the project? It doesn't take much time, but it's a distraction. I'd rather tab through some things quickly than context switch to the docs, finding the example, adapting it for the local script, then getting back to what I was initially trying to do. Working memory in my brain is expensive. reply Sateeshm 11 hours agoparentprevCompletely agree. I find it fails miserably at business logic, which is where we spend most of our time on. But does great at generic stuff, which is already trivial to find on stack overflow. reply rco8786 4 hours agorootparent> But does great at generic stuff, which is already trivial to find on stack overflow. The major difference is that with Cursor you just hit \"tab\", and that thing is done. Vs breaking focus to open up a browser, searching SO, finding an applicable answer (hopefully), translating it into your editor, then reloading context in your head to keep moving. reply PUSH_AX 10 hours agorootparentprevThis might be a promoting issue, my experience is very different, I’ve written entire services using it. reply miningape 10 hours agorootparentMight be that they work in a much more complex codebase, or a language/framework/religion that has less text written on it. Might also be that they (are required to) hold code to a higher standard than you and can't just push half-baked slop to prod. reply JamesBarney 9 hours agorootparentI've spent a good amount of time in my career reading high quality code and slop. The difference is not some level of intelligence that sonnet does not possess. It's a well thought out design, good naming, and rigor. Sonnet is as good if not better than the average dev at most of this and with a some good prompting and a little editing can write code as good as most high quality open source projects. Which is usually far higher than most commerical apps the vast majority of us devs work on. reply miningape 8 hours agorootparent> with a some good prompting and a little editing can write code good I agree with a good developer \"baby-sitting\" the model it's capable of producing good code. Although this is more because the developer is skilled at producing good code so they can tell an AI where it should refactor and how (or they can just do it themselves). If you've spent significant time refitting AI code, it's not really AI code anymore its yours. Blindly following an AI's lead is where the problem is and this is where bad to mediocre developers get stuck using an AI since the effort/skill required to take the AI off its path and get something good out is largely not practised. This is because they don't have to fix their own code, and what the AI spits out is largely functional - why would anyone spend time thinking about a solution that works that they don't understand how they arrived at? reply JamesBarney 7 hours agorootparentI've spent soo much time in my life reviewing bad or mediocre code from mediocre devs and 95% of the time the code sonnet 3.5 generates is at least as correct and 99% of the time more legible than what a mediocre dev generates. It's well commented, the naming is great it rarely tries to get overly clever, it usually does some amount of error handling, it'll at least try to read the documentation, it finds most of the edge cases. That's a fair bit above a mediocre dev. reply snowfarthing 25 minutes agorootparentIt's easy to forget one major problem with this: we all have been mediocre devs at some point in our lives -- and there will always be times when we're mediocre, even with all our experience, because we can't be experienced in everything. If these tools replace mediocre devs, leaving only the great devs to produce the code, what are we going to do when the great devs of today age out, and there's no one to replace them with, because all those mediocre devs went on to do something else, instead of hone their craft until they became great devs? Or maybe we'll luck out, and by the time that happens, our AIs will be good enough that they can program everything, and do it even better than the best of us. If you can call that \"lucking out\" -- some of us might disagree. reply 1oooqooq 7 hours agorootparentprevi love those hot takes because the market will historically fire you and hire the mediocre coders alone now. reply PUSH_AX 10 hours agorootparentprevWithout knowing much about my standards and work you’ve just assumed it’s half baked slop. You’re wrong. reply lomase 9 hours agorootparentIA generated content is the definition of slop for most people. reply PUSH_AX 9 hours agorootparent> IA generated content is the definition of slop The irony. reply lomase 8 hours agorootparenthttps://en.wikipedia.org/wiki/Slop_(artificial_intelligence) English is not my mother tonge. I have never noticed the word \"slop\" until people started to use it to talk about AI generated content. So for many people around the world slop = AI content. Where is the irony if I may ask? reply miningape 8 hours agorootparentYou misspelled AI - that's the extent of the irony reply lomase 8 hours agorootparentIn my mother tongue is called IA, inteligecia artificial. I mix it up all the time. reply 1oooqooq 7 hours agorootparentyou didn't have to explain. he knew, my friend. he knew. reply TeMPOraL 9 hours agorootparentprevWhich is deeply sad, because this both tarnishes good output and gives a free pass to the competitor - shit \"content\" generated by humans. AI models only recently started to match that in quantity. But then again, most of software industry exists to create and support creation of human slop - advertising, content marketing, all that - so there's bound to be some double standards and salary-blindness present. reply miningape 10 hours agorootparentprevWithout knowing much about my prompts and work you’ve just assumed it’s why AI gives me bad results. You’re wrong. (Can you see why this is a bad argument?) Don't get me wrong I love sloppy code as much as the next cowboy, but don't delude yourself or others when the emperor doesn't have clothes on. reply Hedepig 11 hours agorootparentprevHave you had a go with the o1 range of models? reply withinboredom 10 hours agorootparentYesterday, I got into an argument on the internet (shocking, I know), so I pulled out an old gravitation simulator that I had built for a game. I had chatGPT give me the solar system parameters, which worked fine, but my simulation had an issue that I actually never resolved. So, working with the AI, I asked it to convert the simulation to constant-time (it was currently locked to render path -- it's over a decade old). Needless to say, it wrote code that set the simulation to be realtime ... in other words, we'd be waiting one year to see the planets go around the sun. After I pointed that out, it figured out what to do and still got things wrong or made some terrible readability decisions. I ended up using it as inspiration instead and then was able to have the simulation step at one second resolution (which was required for a stable orbit) but render at 60fps and compress a year into a second. reply wruza 6 hours agorootparentThis sums up my experience as well. You can get an idea or just a direction from it, but itself AI stumbles upon its own legs instantly in any non-tutorial task. Sometimes I envy and at the same time feel sorry for successful AI-enabled devs, cause it feels like they do boilerplate and textbook features all day. What a release if something can write it for you. reply theshrike79 8 hours agoparentprevI have a corporation-sponsored subscription to Github CoPilot + Rider When I'm writing unit tests or integration tests it can guess the boilerplate pretty well. If I already have a AddUserSucceeds test and I start writing `public void Dele...` it usually fills up the DeleteUserSucceeds function with pretty good guesses on what Asserts I want there - most times it even guesses the API path/function correctly because it uses the whole project as context. I can also open a fresh project I've never seen and ask \"Where is DbContext initialised\" and it'll give me the class and code snippet directly. reply whatever1 11 hours agoparentprevHave you tried recently to start a new web app from scratch? Specially the integration of frontend framework with styling and the frontend backend integration. Oh my god get ready to waste a full weekend just to setup everything and get a formatted hello world. reply earthnail 11 hours agorootparentThat’s why I use Rails for work. But I also had to write a small Nodejs project (vite/react + express) recently for a private project, and it has a lot of nice things going for it that make modern frontend dev really easy - but boy is it time consuming to set up the basics. reply wry_discontent 4 hours agorootparentI can't imagine having nice things to say about node after working in Rails. Rails does so much for you, provides a coherent picture of how things work. Node gives you nothing but the most irritating programming tools around reply CalRobert 9 hours agorootparentprevI’ve been pretty happy with vite, chakra, and Postgrest lately reply dkersten 11 hours agorootparentprevMost frontend frameworks come with usable templates. Setting up a new Vite React project and getting to a formatted hello world can be done in half an hour tops. reply ileonichwiesz 10 hours agorootparentHalf an hour is still an overestimation, most of these frontend tools go from 0 to hello world in a single CLI command. reply TeMPOraL 9 hours agorootparentOn a good day, when you're using the most recent right version of MacOS, when all of the frontend tool's couple thousand dependencies isn't transiently incompatible with everything else, yes. (If no, AI probably won't help you here either. Frontend stuff moves too fast, and there's too much of it. But then perhaps the AI model could tell you that the frontend tool you're using is a ridiculous overkill for your problem anyway.) reply frde_me 8 hours agorootparentI'll be honest, I've never had the command line tool to setup a React / NextJS / Solid / Astro / Svelt / any framework app fail to make something that runs, ever reply whatever1 4 hours agorootparentWhat exactly magic command line tool are you referring to? What cmd tool configures the frontend framework to use a particular css framework, webpack to work with your backend endpoints properly, setups cors and authentication with the backend for development, configures the backend to point to and serve the spa? reply lomase 2 hours agorootparentdotnet newreply TeMPOraL 5 hours agorootparentprevI had create-react-app break for me on the first try, because I managed to luck into some transient issue with dependencies. reply williamdclt 1 hour agorootparentprevYep, it will likely work and do what it's supposed to do. But what it's supposed to do is probably only 90% of what you want: try to do anything out of what the boilerplate is setup for and you're in for hours of pain. Want SWC instead of TSC? Eslint wasn't setup, or not like you want? Prettier isn't integrated with ESlint? You want Typescript project references? Something about ES modules vs CJS? Hours and hours and hours of pain. I understand all this stuff better than the average (although not a top 10%), and I'd be ashamed to put a number of the amount of hours I've lost on setting up boilerplate _even_ having used some sort of official generator reply namaria 11 hours agorootparentprevIt takes me all of 5 minutes with Phoenix reply dkersten 10 hours agorootparentExactly, the idea that it would take a weekend seems crazy to me. It’s certainly not something I need AI for. reply exe34 11 hours agorootparentprevthat's an indictment of the proliferation of shitty frameworks and documentation. it's not hard to figure out such a combination and then keep a template of it lying around for future projects. you don't have to reach for the latest and shiniest at the start of every project. reply TeMPOraL 4 hours agorootparent> you don't have to reach for the latest and shiniest at the start of every project. Except you kind of do, because if you're working frontend or mobile, then your chosen non-shitty tech stack is probably lacking some Important Language Innovations or Security Featuers that Google or Microsoft forced on the industry since last time you worked with that stack. (Yes, that's mostly just an indictment of the state of our industry.) reply exe34 1 hour agorootparentevery time you capitulate, you tell them that you're happy to play along, bring more \"innovation\" so you keep having to run very hard just to stay in place. reply eru 12 hours agoparentprev> Boilerplate are tedious, but not really time-consuming. In the aggregate, almost no programmer can think up code faster than they can type it in. But being a better typist still helps, because it cuts down on the amount you have to hold in your head. Similar for automatically generating boilerplate. > If I don't know, how can I trust the generated code to be correct? Ask the AI for a proof of correctness. (And I'm only half-joking here.) In languages like Rust the compiler gives you a lot of help in getting concurrency right, but you still have to write the code. If the Rust compiler approves of some code (AI generated or artisanally crafted), you are already pretty far along in concurrency right. A great mind can take a complex problem and come up with a simple solution that's easy to understand and obviously correct. AI isn't quite there yet, but getting better all the time. reply mewpmewp2 11 hours agorootparent> In the aggregate, almost no programmer can think up code faster than they can type it in. But being a better typist still helps, because it cuts down on the amount you have to hold in your head. I mean on the big picture level sure they can. Or in detail if it is something that they have good experience with. In many cases I get a visual of the whole code blocks, and then if I use copilot I can already predict what it is going to auto complete for me based on the context and then I can pretty much in a second know if it was right or wrong. Of course it is more so for the side projects since I know exactly what I want to do and so it feels most of the time it is having to just vomit all the code out. And I feel impatient, so copilot helps a lot with that. reply never_inline 10 hours agoparentprev100%. Useful cases include * figuring out how to X in an API - eg \"write method dl_file(url, file) to download file from url using requests in a streaming manner\" * Brainstorming which libraries / tools / approaches exist to do a given task. Google can miss some. AI is a nice complement for Google. reply dexwiz 10 hours agorootparentI don’t even trust the API based exercises anymore unless it’s a stable and well documented API. Too many times I’ve been bitten by an AI mixing and matching method signatures from different versions, using outdated approaches, mixing in apis from similar libraries, or just completely hallucinating a method. Even if I load the entire library docs into the context, I haven’t found one that’s completely reliable. reply never_inline 1 hour agorootparentIt just has to be popular and common boilerplate like the example I gave. It's hard with less popular APIs. It will almost always get something wrong. In such cases, I read docs, search sourcegraph / GitHub, and finally check the source code. reply yunwal 6 hours agoparentprev> Snake case to camelCase > VSCode itself has command of \"Transform to Camel Case\" I never understand arguments like this. I have no idea what the shortcut for this command is. I could learn this shortcut, sure, but tomorrow I’ll need something totally different. Surely people can see the value of having a single interface that can complete pretty much any small-to-medium-complexity data transformation. It feels like there’s some kind of purposeful gaslighting going on about this and I don’t really get the motive behind it. reply cyral 12 minutes agorootparentExactly. I think some commenters are taking this example too literally. It's not about this specific transformation, but how often you need to do similar transformations and don't know the exact shortcut or regex or whatever to make it happen. I can describe what I want in three seconds and be done with it. Literal dropbox.png going on in this thread. reply johnisgood 9 hours agoparentprevBoilerplate comes up all the time when writing Erlang with OTP behaviors though, and sometimes you have no idea if it really is the right way or not. There are Emacs skeletons for that (through tempo), but feels like they are sometimes out of date. reply dkersten 11 hours agoparentprev1. Is such a taste task for me anyway that I don’t lose much just doing it by hand 2. The last time I wrote boilerplate heavy Java code, 15+ years ago, the IDE already generated most of it for me. Nowadays boilerplate comes in two forms for me: new project setup, which I find it far quicker to use a template or just copy and gut an existing project (and it’s not like I start new projects that often anyway), or new components that follow some structure, where AI might actually be useful but I tend to just copy an existing one and gut it. 3. These aren’t tasks I really trust AI for. I still attempt to use AI for them, but 9 out of 10 times come away disappointed. And the other 1 time end up having to change a lot of it anyway. I find a lot of value from AI, like you, asking it SO style questions. I do also use it for code snippets, eg “do this in CSS”. Its results for that are usually (but not always) reasonably good. I also use it for isolated helper functions (write a function to flood fill a grid where adjacent values match was a recent one). The results for this range from a perfect solution first try, to absolute trash. It’s still overall faster than not having AI, though. And I use it A LOT for rubber ducking. I find AI is a useful tool, but I find a lot of the positive stories to be overblown compared to my experience with it. I also stopped using code assistants and just keep a ChatGPT tab open. I sometimes use Claude but it’s conversation length limits turned me off. Looking at the videos in OP, I find the parallelising task to be exactly the kind of tricky and tedious task that I don’t trust AI to do, based on my experience with that kind of task, and with my experience with AI and the subtly buggy results it has given me. reply EGreg 5 hours agoparentprevIf you aren’t using AI for everything, you’re using it wrong. Go learn how to use it better. It’s your job to find out how. Corporations are going to use it to replace your job. (Just kidding. I’m just making fun of how AI maxis reply to such comments, but they do it more subtly.) reply greenie_beans 6 hours agoparentprevhave you tried using cursor or claude? reply cynicalpeace 7 hours agoprevIt's amazing how little of my colleagues don't use Cursor simply because they haven't taken the 10 minutes to set it up. It's amazing how many naysayers there are about Cursor. There are many here and they obviously don't use Cursor. I know this because they point out pitfalls that Cursor barely runs into, and their criticism is not about Cursor, but about AI code in general. Some examples: \"I tried to create a TODO app entirely with AI prompts\" - Cursor doesn't work like that. It lets you take the wheel at any moment because it's embedded in your IDE. \"AI is only good for reformatting or boilerplate\" - I copy over my boilerplate. I use Cursor for brand new features. \"Sonnet is same as old-timey google\" - lol Google never generated code for you in your IDE, instantly, in the proper place (usually). \"the constantly changing suggested completions seem really distracting\" - You don't need to use the suggested completions. I barely do. I mostly use the chat. \"IDEs like cursor make you feel less competent\" - This is perhaps the strongest argument, since my quarrel is simply philosophical. If you're executing better, you're being more competent. But yes some muscles atrophy. \"My problem with all AI code assistants is usually the context\" - In Cursor you can pass in the context or let it index/search your codebase for the context. You all need to open your minds. I understand change is hard, but this change is WAY better. Cursor is a tool, and like any tool you need to know how to use it. Start with the chat. Start by learning when/what context you need to pass into chat. Learn when Cmd+K is better. Learn when to use Composer. reply zkry 6 hours agoparentI've noticed that tools like Cursor doesn't really seem to make difference in the end. The good software developers are still the good software developers, regardless of editors. I don't think you should be upset or worried that people aren't adopting these tools as you think they should. If the tool really lives up to its hype then the non-adopters will fall behind and, for example, be forced to switch to Cursive. This happened with IDEs (e.g. IntelliSense, jump to definition). It may happen with tools like Cursive. I certainly don't feel this way but if I'm proven wrong thats good. reply tsarchitect 2 hours agorootparentTo be proven wrong would be that Cursor is used by all devs or that IDEs adopt AI into their workflow? Like OP using cursor has been a huge productivity boost. I maintain a few postgres databases, I work as a fullstack developer, and manage kubernetes configs. When using cursor to write sql tables or queries it adopts my way of writing sql. It analyzed (context) my database folder and when I ask it to create a query, a function, a table, the output is in my style. This blew me away when I first started with cursor. Onto react/nextjs projects. In the same fashion, I have my way of writing components, fetching data, and now writing RSA. Cursor analyzed my src folder, and when asked to create components from scratch the output was again similar to my style. I use raw CSS and class names, what was an obstacle of naming has become trivial with Cursor (\"add an appropriate class to this component with this styling\"). Again, it analyzed all my CSS files and spits out css/classes in my writing/formatting style. And working on large projects it is easy to forget the many many components, packages, etc. that integrated/have been written already. Again, cursor comes out on top. Am I good developer or a bad developer? Don't know. Don't care. I'm cranking out features faster than I have ever done in my decades of development. As has been said before, as a software engineer you spend more time reading code than writing. Same applies to genAI. It turns out that I can ask cursor to analyze packages, spit out code, yaml configuration, sql, and it gets me 80% done with writing from scratch. Heck, if I need types to get full client/server type completion experience, it does that too! I have removed many dependencies (tailwind, tRPC, react query, prisma, to name a few) because cursor has helped me overcome obstacles that these tool assisted in (and I still have typescript code hints in all my function calls!). All in all, cursor has made a huge difference for me. When colleagues ask me to help them optimize sql, I ask cursor to help out. When colleagues ask to write generic types for their components, I ask cursor to help out. Whether cursor or some other tool, integrating AI with the IDE has been a boom for me. reply cynicalpeace 6 hours agorootparentprev> The good software developers are still the good software developers Correct. Because they know they need to use the correct tools for the job. > If the tool really lives up to its hype then the non-adopters will fall behind This is already happening. I'm able to out-deploy many of my competitors because I'm using Cursor. Have you actually spent much time with Cursor? The comparison to \"Jump to definition\" is pretty bad. You also misspelled its name twice. reply halfmatthalfcat 4 hours agorootparent> I'm able to out-deploy This is a very poor metric for your efficacy as a software engineer and if you optimize for this, you're gonna have a bad time long term. reply cynicalpeace 2 hours agorootparentYou're right. MRR is a better metric. Gotten great MRR via Cursor too. reply yladiz 1 hour agorootparentHow’s your ARR? reply zkry 5 hours agorootparentprevOh yeah, typo, I work with the Cursive IDE a lot. I've spent a good amount of time with Cursor. And I have no doubt that it provides a lot of utility. I also would agree that most good devs I know definitely adopt some form of LLM integration. I would even agree that a lot of cursive features will bleed into other editors, maybe being considered a necessity. I just haven't made the observation that most people have switched to Cursor full-time and I also haven't noticed that those who have are on another level compared to those using their other editor plus chatgpt/copilot/etc. reply cynicalpeace 2 hours agorootparent> I just haven't made the observation that most people have switched to Cursor full-time I noted that same thing in my initial comment. reply handzhiev 2 hours agoparentprevI also find it fascinating how in almost every LLM-related discussion there are people always writing arguments to prove that LLMs do not work. OK, I understand. Maybe they can't get much use of them and that's fine. But why they always insist that the tools don't work for everyone is something I can't make any sense of. I stopped arguing online about this though. If they don't want to use LLMs that's fine too. Others (we) are taking their business. reply raincole 1 hour agoparentprevI've been a paying customer for Jetbrains IDE for years. After trying Cursor, I'd say if I were Jetbrains devs I'd be very worried. It's a true paradiam shift. It feels like Jetbrains' competitve edge over other editors/IDEs mostly vanished overnight. Of course Jetbrains has its own AI-based solution and I'm sure they'll add more. But I think what Jetbrains excels -- the understanding of semantics -- is no longer that important for an IDE. reply cynicalpeace 1 hour agorootparentMeanwhile other commenters say: > I don't use it because I already use JetBrains (Pycharm, mostly). Hard to see any value add of Cursor over that. lol reply raincole 42 minutes agorootparentI don't blame them though. Honestly this was the exact thought I had before I tried Cursor. reply eikenberry 1 hour agoparentprevCursor's show-stopping problem is not if it is useful, the problem is that it is proprietary. These sorts of tools are fun to play with to try out things that might be useful in the future but relying on them puts you at the mercy of a VC backed company with corresponding dodgy motivations. The only way these technologies will be acceptable for widespread use is to measure them as we do programming languages and to only adopt free software implementations. reply cynicalpeace 1 hour agorootparentTo an ideological position like yours, I would say... maybe? I, for one, am happy to pay for good solutions and let the market figure it out. If there are open source solutions that are just as smooth, that's great. I've seen a few, but none have been as good thus far. reply eikenberry 44 minutes agorootparentI've been keeping an eye out for a good, free software development tool like this but I've also seen nothing viable yet. The main problem really seems to be that the required hardware is expensive and resource intensive which keeps most of the talent from being able to work on it. Once the required hardware becomes more common place I think multiple free software versions will pop up to fill the niche. reply insane_dreamer 2 hours agoparentprevI don't use it because I already use JetBrains (Pycharm, mostly). Hard to see any value add of Cursor over that. For a lighter-weight IDE I use Zed reply cynicalpeace 1 hour agorootparentlol so you have no idea? reply stocknoob 5 hours agoparentprevThe naysayers used to bother me, but then I realized it’s no skin off my back if they don’t want to become familiar with a transformative technology. Stay with the old tools, people are getting excited for no reason at all, everyone is just pretending to be more productive! It reminds me of how blackberry users insisted physical keyboards were necessary and smartphone touchscreen users were deluded. reply dvsfish 4 hours agorootparentI think most naysayers are threatened by the inevitable decimation of our value. Supply and demand laws are going to annihilate this profession reply cynicalpeace 2 hours agorootparentThis is the fundamental question most of us seem to have. On the one hand, logic does seem to dictate supply/demand of the profession will lower salaries. Also no one really cares how code was written or if it's pretty. On the other hand, these tools have only seemed to increase our value so far. Someone who knows how to code with AI is now 1000x more valuable than someone who doesn't know how to code. You still need to know how to code to be able to contribute. How long that remains the case is the question. You could be right reply cynicalpeace 2 hours agorootparentprevThat's a great comparison. reply kachau 3 hours agoparentprevIs it allowed to use cursor in work places? does cursor uploads company code or leak any information? reply cynicalpeace 2 hours agorootparentDepends on your workplace. You can adjust some settings to fine tune what gets sent to Cursor/stored by them reply snowstormsun 6 hours agoparentprevNope reply joenot443 4 hours agorootparentYou wouldn't be the first engineer to fade into irrelevance because they were too proud to adapt to the changing world around them. I'd encourage you to open your mind a bit. reply mherrmann 6 hours agoprevIn my experience, Cursor writes average code. This makes sense, if you think about it. The AI was trained on all the code that is publicly available. This code is average by definition. I'm below average in a lot of programming languages and tools. Cursor is extremely useful there because I don't have to spend tens of minutes looking up APIs or language syntax. On the other hand, in areas I know more about, I feel that I can still write better code than Cursor. This applies to general programming as well. So even if Cursor knows exactly how to write the syntax and which function to invoke, I often find the higher-level code structure it creates sub-optimal. Overall, Cursor is an extremely useful tool. It will be interesting to see whether it will be able to crawl out of the primordial soup of averages. reply jonathaneunice 3 hours agoparentExactly right. Cursor makes it easy to get to \"adequate.\" Which in the hundreds of places that I'm not expert or don't have a strong opinion, is regularly as good as and frequently better than my first pass. Especially as it never gets tired whereas I do. It's a great intern, letting me focus on the few but important places that I add specific value. If this is all it ever does, that's still enormously valuable. reply cynicalpeace 6 hours agoparentprevThis is true. But with a little push here and there you can usually avoid the sub-optimal high level code structure. That's why it makes so much sense to have it in the IDE. You can see in general anything AI produces is pretty average. But people who buy software don't care that the code behind it is average. As long as it works. Whereas people who buy text, images and video do care. reply haolez 5 hours agoparentprevI've been having some difficulties with deprecated code and old patterns being suggested all the time. But I guess this is an easy issue to fix and will probably be fixed eventually. reply friggeri 12 hours agoprevI recently started using Cursor for all my typescript/react personal projects and the increase in productivity has been staggering. Not only has it helped me execute way faster, similar to the OP I also find that it prevents me from getting sidetracked by premature abstraction/optimization/refactoring. I recently went from an idea for a casual word game (aka wordle) to a fully polished product in about 2h, which would have taking me 4 or 5 times that if I hadn’t used Cursor. I estimate that 90% of the time was spent thinking about the product, directing the AI, and testing and about 10% of the time actually coding. reply namaria 11 hours agoparentYou're happy to have gone from thinking of cloning something to cloning something using a cloning tool quickly? reply bsaul 8 hours agorootparentYour remark raises a question : how much of your daily work is truely original ? Unless you work in R&D i've got some bad news for you.. reply achierius 2 hours agorootparentYou don't have to be a researcher to do original work. Out of ten people on my team one has a PhD, but we're all doing more-or-less unique work a large fraction of the time. reply becquerel 3 hours agorootparentprevWhy shouldn't they be? They achieved their goal. There is little shame in cloning something. reply causal 4 hours agoparentprevYou're getting a lot of snark in the comments, but your excitement is warranted. It's fascinating how any claims of a code tool being useful always seem to offend the ego and bring out all the chest-thumping super-programmers claiming they could do it better. reply sophiacamille 10 hours agoparentprevId love to see your GitHub before and after reply eru 12 hours agoparentprevInteresting. Have you published the game anywhere (eg GitHub or so)? Have you written about your experience anywhere in greater length? reply miningape 10 hours agoparentprevI would love to see the world where you didn't use AI and instead invested the time to make yourself a stronger programmer. A react wordle clone isn't something most developers would need 2 hours to make (sure maybe the styling / hosting AROUND the wordle clone might take longer) - I'm not saying you're a bad programmer or a bad person but what is the opportunity cost of using AI here? Are you optimising yourself into a local-minima? reply wruza 5 hours agorootparentI think this excitement reflects the fact that most devs are shoemakers without shoes. They could get cursor-like experience decades ago by preparing snippets, tools, templates, editor configs and knowledge bases. But they used that “a month of work can save two days of planning” principle, so now having a sort of a development toolkit feels refreshing. Those who had it aren’t that impressed. reply jpamata 8 hours agorootparentprevthey said 90% of it was spent on ideation and exploration they didnt specifically mean they built a wordle clone, just a game like it. if they wanted just a wordle clone, they wouldve gotten one within a few minutes of using codegen tools. reply miningape 7 hours agorootparentNo, what they said was > I estimate that 90% of the time was spent thinking about the product, directing the AI, and testing In other words 90% of the time was spent in the proompt-test-proompt loop. Not ideation and exploration. > they didnt specifically mean they built a wordle clone, just a game like it. if they wanted just a wordle clone, they wouldve gotten one within a few minutes of using codegen tools. If you really believe that I'm not sure what to say other than: have you tried to use an AI to make a full wordle clone? (not just the checking logic, or rendering - the entire thing) reply jpamata 6 hours agorootparentyes, the quote is what I'm referring to, directing the AI is part of it, people use these to quickly brainstorm and refine ideas. I'd be more charitable and wouldn't hastily assume it was some skill issue, especially them being a principal engineer reply regularfry 9 hours agorootparentprevCounterpoint: learning to make better buggy-whips is fun but, in the grand scheme of things, also a local minimum. reply miningape 3 hours agorootparentThe existence of other minima does not imply all minima are equal. In fact, without knowing the entire graph it's impossible to say whether a particular minima is the global minima or just a local one. reply benreesman 12 hours agoprevI’m doing an experiment in this in real time: I’ve got a bunch of top-flight junior folks, all former Jane and Google and Galois and shit, but all like 24. I’ve also been logging every interaction with an LLM and the exit status of the build on every mtime of every language mode file and all the metadata: I can easily plot when I lean on the thing and when I came out ahead, I can tag diffs that broke CI. I’m measuring it. My conclusion is that I value LLMs for coding in exact the same way that the kids do: you have to break Google in order for me to give a fuck about Sonnet. LLMs seem like magic unless you remember when search worked. reply dartos 12 hours agoparent> LLMs seem like magic unless you remember when search worked. Yikes. I didn’t even think about this, but it’s true. I’m looking for the kinds of answers that Google used to surface from stack overflow reply supaflybanzai 11 hours agorootparentKagi… Fully switched over more than a year ago and never looked back. reply barbazoo 2 hours agorootparentKagi with the \"Programming\" lense turned on reply jansan 9 hours agorootparentprevThe best way to get useful answers was (and for me still is) to ask Goggle for \"How do I blah site:stackoverflow.com\". Without the site filter, Google results suck or are just a mess, and stackoverflow's own search is crap. reply JamesBarney 8 hours agorootparentGoogle used to be better but so was stack overflow. Now a lot of the answers are out-dated. And even more importantly they got rid of any questions where the answer was even a little bit subjective. Unfortunately for users that's almost all the most useful answers. reply danielbln 11 hours agoparentprevI don't understand, are you using LLMs purely for information retrieval, like a database (or search index)? I mean sure that's one usecase, but for me the true power of LLMs comes from actually processing and transforming information, not just retrieving it. reply benreesman 10 hours agorootparentI have my dots wired up where I basically fire off a completion request any time I select anything in emacs. I just spend any amount of tokens to build a database of how 4o behaves correlated to everything emacs knows, which is everything. I’m putting down tens of megabytes a day on what exact point they did whatever thing. reply benreesman 10 hours agorootparentI’m actively data-mining OpenAI, they get a bunch of code that they have anyways because they have GitHub, I get arbitrary scope to plot their quantization or whatever with examples. Flip it on em. You’re the one being logged asshole. https://youtu.be/un3NkWnHl9Q?si=VOnH2krJkJLRA2BQ reply benreesman 11 hours agoparentprevTo be clear I’m a huge fan of the Cursor team: those folks are clearly great at their jobs and winning at life. They didn’t get ahead by selling you the same thing they do, if they did Continue would be parity. reply nopinsight 12 hours agoparentprevWhat domain/type of software do you and they work on? Cursor has been quite effective for me and many others say the same. As long as one prompts it properly with sufficient context, reviews the generated code, and asks it to revise as needed, the productivity boost is significant in my experience. reply valenterry 12 hours agorootparentWell, the context is the problem. LLMs will really become useful if they 1.) understand the WHOLE codebase AND all it's context and THEN also understand the changes over time to it (local history and git history) and finally also use context from slack - and all of that updating basically in real time. That will be scary. Until then, it's basically just a better autocomplete for any competent developer. reply danielbln 7 hours agorootparentWhat you describe would be needed for a fully autonomous system. But for a copilot sort of situation, the LLM doesn't need to understand and know of _everything_. When I implement a feature into a codebase, my mental model doesn't include everything that has ever been done to that codebase, but a somewhat narrow window, just wide enough to solve the issue at hand (unless it's some massive codebase wide refactor or component integration, but even then it's usually broken down into smaller chunks with clear interfaces and abstractions). reply valenterry 4 hours agorootparentI use copilot daily and because it lacks context it's mostly useless except for generating boilerplate and sometimes converting small things from A to B. Oh, also copying functions from stackoverflow and naming them right. That's about it. But I spend maybe 5% of my time per day on those. reply danielbln 4 hours agorootparentI dislike Copilot's context management, personally, and much prefer populating the context of say Claude deliberately and manually (using Zed, see https://zed.dev/blog/zed-ai). This fits my workflow much much better. reply therein 9 hours agorootparentprevImagine you are coding in your IDE and it suggests you a feature because someone mentioned it yesterday on #app-eng channel. Needs deeper context, though. About order of events, an how authoritative a character is. reply benreesman 12 hours agorootparentprevI get value out of LLMs on stock Python or NextJS or whatever where that person was in fact a lossy channel from SO to my diff queue. If there’s no computation then there’s no computer science. It may be the case that Excel with attitude was a bubble in hiring. But Sonnet and 4o both suck at why CUDA isn’t detected on this SkyPilot resource. reply benreesman 12 hours agorootparenthttps://www.youtube.com/watch?v=FpG--q5u6ns reply komali2 12 hours agorootparentprev> But Sonnet and 4o both suck at why CUDA isn’t detected on this SkyPilot resource. I don't understand this sentence, should \"both suck at why\" be \"both suck and why\" or perhaps I'm just misunderstanding in general? reply benreesman 11 hours agorootparentSkyPilot is an excellent piece of software attempting an impossible job: run your NVIDIA job on actively adversarial compute fabric who mark up the nastiest monopoly since the Dutch East India Company (look it up: the only people to run famine margins anywhere near NVIDIA are slave traders). To come out of the cloud “credits” game with your shirt on, you need stone cold pros. The kind of people on the Cursor team. Not the adoring fans who actually use their shit. reply taldo 7 hours agoprevCursor has been an enabler for unfamiliar corners of development. Mind you, it's not a foolproof tool that writes correct code on the first try or anything close to that. I've been in compilers, storage, and data backends for 15ish years, and had to do a little project that required recording audio clips in a browser and sending them over a websocket. Cursor helped me do it in about 5 minutes, while it would've taken at least 30 min of googling to find the relevant keywords like MediaStream and MediaRecorder, learn enough to whip something up, fail, then try to fix it until it worked. Then I had to switch to streaming audio in near-realtime... here it wasn't as good: it tried sending segments of MediaRecorder audio which are not suitable for streaming (because of media file headers and stuff). But a bit of Googling, finding out about Web Audio APIs and Audio Worklet, and a bit of prompting, and it basically wrote something that almost worked. Sure it had some concurrency bugs like reading from the same buffer that it's overwriting in another thread. But that's why we're checking the generated code, right? reply davidczech 1 hour agoparentI've had similar experiences. I've basically disengaged any form of AI code generation. I do find it useful to pointing me to interesting/relevant symbols and API's however, but it doesn't save me any time connecting plumbing, nor is that really a difficult thing for any programmer to do. reply mattxxx 4 hours agoprevHere's a few cursor perks: 1. Auto-complete makes me type ~20% faster (I type 100+ WPM) 2. Composer can work across a few files simultaneously to update something (e.g. updating a chrome extension's manifest while proposing a code change) 3. Write something that you know _exactly_ how it should work but are too lazy to author it yourself (e.g. Write a function that takes 2 lists of string and pair-wise matches the most similar. Allow me to pass the similarity function as a parameter. Use openai embedding distance to find most similar pairings between these two results) reply ciconia 11 hours agoprevWatching the videos in the article, the constantly changing suggested completions seem really distracting. Personally, I find this kind of workflow totally counter-productive. My own programming workflow is ~90% mental work / doing sketches with pen & paper, and ~10% writing the code. When I do sit down to write the code, I know already what I want to write, don't need suggestions. reply viraptor 11 hours agoparentIt's a tool. You get used to new tools. These days I can easily process \"did something interesting appear\" in the peripheral vision at the same time as continuing to type. But the most useful things don't happen while I write. Instead it's the small edits that immediately come up with \"would you also like to change these other 3 things to make your change work?\" Those happen in the natural breaks anyway, as I start scanning for those related changes myself. reply namaria 11 hours agorootparentA tool that forces me to shift from creating solutions to trying to figure out what might be wrong with some code is entirely detrimental to my workflow. reply viraptor 11 hours agorootparentIs that your actual experience or expectation? If you're just making assumptions, I'd encourage you to give it an actual try. Talking about using Cursor is a bit like talking about riding a bike with someone who never did it. (But yeah, it's totally not for everyone, and that's fine) reply namaria 10 hours agorootparentVery early on I took pains to figure out what toolchains gave me traction and which tools produced waste complexity. I think a lot of the excitement about using LLMs to code is because a lot of teams are stuck in local optima where they need to use noisy tools, and there's a lot of de-noised output available to train LLMs. This is progress in searching and mitigating bad trade-offs, not in advancing the state of the art. reply TeMPOraL 9 hours agorootparentprevThe latter is usually much easier than the former on a small scale, so your statement is very surprising. reply namaria 9 hours agorootparentYou find revising correctly looking but potentially wrong code easier than just writing correct code? reply TeMPOraL 5 hours agorootparentObviously, yes. For the exact same reason it's true for math homework, too! Most code most people write is trivial in terms of semantics/algorithms. The hard bit is navigating the space of possible solutions: remembering all the APIs you need at the moment - right bits of the standard library, right bits of your codebase, right bits of third-party dependencies - and holding pieces you need in your head while you assemble some flow of data, transforming it between API boundaries as needed. I'm totally fine letting the AI do that - this kind of work is a waste of brain cycles, and it's much easier to follow and verify than to write from scratch. reply exe34 11 hours agorootparentprev> in the peripheral vision this seems physiologically unlikely. reply viraptor 11 hours agorootparentWe do it all the time. I can type while talking to people. I can also read/process the text ahead while saying out loud what I read a few words back. We do a lot of things concurrently when dealing with text - I'm not staring at the cursor when writing code either. (Unless you meant it literally. No, I didn't mean actual peripheral vision. Just noticing things beyond what I type.) reply exe34 9 hours agorootparentI did mean it literally. the solid angle covered by the fovea is tiny. reply theshrike79 8 hours agorootparentThe eye also doesn't literally stay 100% static when you look at something. reply batesy 2 hours agoparentprevYou can create markdown files containing all the planning you did and Cursor will have all of that as context to give you better suggestions. This type of prompting is what gives amazing results - not just relying on out of the box magic, which I think a lot of people are expecting. reply baudpunk 2 hours agoprevI recommend that naysayers for technologies like Cursor watch the documentary Jurassic Punk. When comparing the current AI landscape to the era of computer graphics emerging in film, the parallels are pretty staggering to me. There is a very vocal old guard who are stubborn about ditching their 10,000+ hours master-level expertise to start from zero and adapt to the new paradigm. There is a lot of skepticism. There are a lot of people who take pride in how hard coding should be, and the blood and sweat they've invested. If you look at AI from 10,000 feet, I think what you'll see is not AGI ruining the world, but rather LLMs limited by regression, eventually training on their own hallucinations, but good enough in their current state to be amazing tools. I think that Cursor, and products like it, are to coding what Photoshop was to artists. There are still people creating oil paintings, but the industry — and the profits — are driven by artists using Photoshop. Cursor makes coders more efficient, and therefore more profitable, and anyone NOT using Cursor in a hiring pool of people who ARE using it will be left holding the short straw. If you are an expert level software engineer, you will recognize where Cursor's output is bad, and you will be able to rapidly remediate. That still makes you more valuable and more efficient. If you're an expert level software engineer, and you don't use Cursor, you will be much slower, and it is just going to reduce your value more and more over time. reply kristopolous 16 minutes agoparentI think llms are useful and also that cursor is not. It's a specific thing and it doesn't suit me. I've seen the glittery eyed hype on hn before and it basically means it will become a common tool. Whether it's good or not, that's a different question. reply cynicalpeace 1 hour agoparentprevGood take. It really makes you realize that as much as engineers like to pretend they're purely rational and analytical, we're just as emotional as everyone else. reply adamontherun 32 minutes agoprevI've been working with Cursor for a few months. I learned quickly to stay away from the hype around creating entire features using Composer. Found a ton of value in using it to work with context from the codebase as well as external documentation. Shared my 8 pro tips in this post towards the bottom https://betaacid.co/blog/cursor-dethrones-copilot reply vishal-padia 13 hours agoprevIn the article, you mentioned that you've been writing code for 36 years, so don't you feel IDEs like cursor make you feel less competent? Meaning I loved the process of scratching my head over a problem and then coming to a solution but now we have AI Agents solving the problems and optimizing code which takes the fun out of it. reply forgotoldacc 13 hours agoparentI feel like in the early stages of becoming a programmer, learning how to do all those little baseline problems is fun. But eventually you get to a point where you've solved variations of the problem hundreds of times before, and it's just hours of time being burnt away writing it again with small adjustments. It's like getting into making physical things with only a screwdriver and a hammer. Working with your hands on those little projects is fun. Then eventually you level up your skills and realize making massive things is much easier with a power drill and some automated equipment, and gives you time to focus on the design and intricacies of far more complicated projects. Though there are always those times where you just want to spend a weekend fiddling with basics for fun. reply valenterry 12 hours agorootparentThat should be when you move to more sophistcated (and also complex/complicated) languages that relieve you from as much of this boilerplate as possible. The rest is then general design and archiceture, where LLMs really don't help much with. What they are really good for is to get an idea of possible options in spaces were you have little experience or to quickly explain and summarize specific solutions and their pros and cons. But I tried to make it pick a solution based on the constraints and even with many tries and careful descriptions, the results were really bad. reply mewpmewp2 10 hours agorootparentI think it is not the boilerplate of the programming language necessarily, but it is more to do with boilerplate of common business logic. E.g. even say form validation, I have done it countless of times and I can't be bothered to write out rules for each field again, but AI can easily generate me Zod schema with reasonable validation based on the database model or schema. It probably does better validation rules than I would do quickly on my first try. Then I use these validations both in the backend and frontend. reply valenterry 4 hours agorootparentThere is nothing that stops a good PL from doing the same. In fact, that is why languages like F# support a concept called \"type provider\". reply szundi 12 hours agorootparentprevCompletely agreed. Don’t be afraid to embrace this. You have to give it an active month until it starts to work in your hands though. reply ziofill 13 hours agoparentprevI’ve been using cursor for a while now and I think that if a problem is simple enough for an LLM to work out on its own, it’s probably not worth scratching one’s head over… reply Refusing23 11 hours agorootparentI dont think people need to think, that the AI is supposed to make complicated code i think for the most part its meant to help you \"get past\" all the generic code you usually write in the beginning of a project, generic functions you need in almost all systems, etc. reply eru 12 hours agorootparentprevI'm not sure that's a good heuristic? People love playing Tetris or solving crossword puzzles, and machines are much better at them than us. reply TeMPOraL 9 hours agorootparentPeople keep playing trivial or repetitive games because they enjoy it. People keep writing trivial or repetitive code because they have to. reply vishal-padia 8 hours agorootparentprevI don't agree, in the initial stages solving problems without LLMs will give a good enough knowledge about the intricacies involved and it helps develop a structured approach while solving a problem! reply szundi 12 hours agorootparentprevAgreed with added experience of mine: sometimes Cursos gives me a simpler yet perfect solution. And I am grateful for it. reply mewpmewp2 10 hours agoparentprevI think you are still thinking just on another level. E.g. you go on a walk, you fantasize about everything you are going to do, and it builds up in your head, then you come back, it is all in your head and AI will help you get it out quickly, but you have already solved the problem for yourself and so you are also able to validate quickly what the AI does. reply vishal-padia 8 hours agorootparentyes this!!! Whenever I write a prompt, I tend to divide it into smaller prompts, and in this process, my brain thinks of multiple ways to solve the problem. So yes, it's not limiting my thought process. I didn't notice this thing until I read this. reply fassssst 13 hours agoparentprevFor me it’s like riding an e-bike. More fun because I can go faster and see and do more. reply ramijames 13 hours agorootparentAnd you get less tired. I can complete more work because I'm not always getting stuck in minutia. I can focus on architecture, structure, and refactoring instead of line-by-line writing of code. I'm not saying that I don't like writing code. I'm just saying that doing a lot of it can be mentally exhausting. Sometimes I'd just prefer to ship feature-complete stuff on-time and on-budget, then go back to my kids and wife without feeling like my brain is mush. reply __mharrison__ 5 hours agorootparentprevI have compared AI to an emtb. Was riding one on slick rock trail in Moab which has some areas with consequences earlier this year. If you don't know how to handle a bike, the ebike won't help you in these situations. (You might even get yourself in a tricky spot). But if you know how to ride, it can be really fun. Same with code. If you know how to code it can make you much more productive. If you don't know how to code, you get into tricky spots... reply crucialfelix 9 hours agorootparentprev\"An e-bike for the mind\" as Steve Jobs might have said. reply yoyohello13 3 hours agorootparentprevThis is actually a great analogy. You get to accomplish a lot more, much faster, but you lose much of the benefit to your fitness. reply ativzzz 2 hours agorootparent> you lose much of the benefit to your fitness If you're biking for the purpose of fitness then this is a downside, but if your goal is to see more and go further, then it's an acceptable tradeoff. Similar to coding. If you're writing code because you enjoy writing code, it's less fun. If you're writing code to build stuff, AI will help you build faster reply computerfriend 5 hours agorootparentprevI have a motorbike, yet I prefer cycling because the exercise feels good and is good for me. reply tomyedwab 3 hours agoparentprevI do think that is a real risk, yes. I don't want to use LLMs as a crutch to guard against having to ever learn anything new, or having to implement something myself. There is such a thing as productive struggle which is a core part of learning. That said, I think everyone can relate to wasting an awful lot of time on things that are not \"interesting\" from the perspective of the project you are working on. For example, I can't count the number of hours I've spent trying to get something specific to work in webpack, and there is no payoff because today the fashionable tool is vite and tomorrow it'll be something else. I still want to know my code inside and out, but writing a deploy script for it should not be something I need to spend time on. If I had a junior dev working for me for pennies a day, I would absolutely delegate that stuff to them. reply phito 12 hours agoparentprevI don't find figuring out the syntax of a new language interesting. There's absolutely no fun in that. I know what I want to do and already understand the concepts behind it, that was the fun part to learn. reply alonsonic 2 hours agoparentprevFor a lot of people the fun and rewarding part is actually building and shipping something useful to users. Not solving complex puzzles / algoritic challenges. If AI gets me in front of users faster then I'm a happier builder. reply manmal 11 hours agoparentprevDo they really solve the hard problems though? For me, the LLMs solve the low level problems. Usually I need to figure out an algorithm, which is the actual problem, and finally give some pseudo code to the LLM and surrounding code so it can generate a solution that looks idiomatic. In some cases, LLMs act as a stackoverflow replacement for me, like „sort this with bubble sort, by property X“. I’d also ask it to write some test cases around that. I won’t import a bubble sort library just for this, but I also don’t want to spend any more time than necessary, implementing this for the nth time. reply ninacomputer 13 hours agoparentprevWas going to ask a similar question. Where in the experience of Cursor do you feel like you're losing some of the agency of solving the harder problems, or is this something you take in mind while using it? reply Schnitz 13 hours agorootparentI’ve “only” been coding for 20 years, but it’s the tedious problems, not the actually technically hard problems that cursor solves. I don’t need to debug 5 edge cases any more to feel like I’ve truly done the work, I know I can do that, it’s just time spent. Cursor helps me get the boring and repetitive work out of coding. Now, don’t get me wrong, there was a time where I loved building something lower level line by line, but nowadays it’s very often a “been there, done that” type of thing for me. reply com2kid 11 hours agorootparentprevIf I need an RNG rolled to a standard distribution, I can either spend 5 minutes looking it up, learning how to import and use a library, and adding it to my code, or I can tell Cursor to do it for me. Crap like that, 100 times a day. \"Walk through this array and pull out every element without an index field and add it to a new array called needsToBeIndexed, send them off to the indexing service, and log any failures to the log file as shown in the function above\". Cursor lets me think closer to the level of architecting software. Sure having a deep knowledge of my language of choices is fun, and very needed at times, but for the 40% or so of code that is boring work of moving data around, Cursor helps a lot. reply gaploid 7 hours agoprevUsing ChatGPT and AI assistants over the past year, here are my best use cases: - Generating wrappers and simple CRUD APIs on top of database tables, provided only with a DDL of the tables. - Optimizing SQL queries and schemas, especially for less familiar SQL dialects—extremely effective. - Generating Swagger comments for API methods. Joyness - Re-creating classes or components based on similar classes, especially with Next.js, where the component mechanics often make this necessary. - Creating utility methods for data conversion or mapping between different formats or structures. - Assisting with CSS and the intricacies of HTML for styling. - GPT4 o1 is significantly better at handling more complex scenarios in creation and refactoring. Current challenges based on my experience: - LLM lacks critical thinking; they tend to accommodate the user’s input even if the question is flawed or lacks a valid answer. - There’s a substantial lack of context in most cases. LLMs should integrate deeper with data sampling capabilities or, ideally, support real-time debugging context. - Challenging to use in large projects due to limited awareness of project structure and dependencies. reply HarHarVeryFunny 3 hours agoprevInteresting to hear the perspective of an experienced developer using what seems to be the SOTA coding assistant of Cursor/Claude. I thought his \"Changes to my workflow\" section was the most interesting, coupled with the fact that coding productivity (churning out lines of code) was not something he found to be a benefit. However, IMO, the workflow changes he found beneficial seem to be a bit questionable in terms of desireability... 1) Having LLM write support libraries/functions from scratch rather than rely on external libraries seems a bit of a double-edged sword. It's good to minimize dependencies and not be affected by changes to external libraries, but OTOH there's probably a lot of thought and debugging that has been put into those external libraries, as well as support for features you may not need today but may tomorrow. Is it really preferable to have the LLM reinvent the wheel using untested code it's written channeling internet sources? 2) Avoiding functions (couched as excessive abstractions) in favor of having the LLM generate repeated copies of the same code seems like a poor idea, and will affect code readability, debugging and maintenance whereby a bugfix in one section is not guaranteed to be replicated in other copies of the same code. 3) Less hesitancy to use unfamiliar frameworks and libraries is a plus in terms of rapid prototyping, as well as coming up to speed with a new framework, but at the same time is a liability since the quality of LLM generated code is only as good as the person reviewing it for correctness and vulnerabilities. If you are having the LLM generate code using a framework you are not familiar with, then you are at it's mercy as to quality, same as if you cut and pasted some code from the internet without understanding it. I'm not sure we've yet arrived at the best use of \"AI\" for developer productivity - while it can be used for everything and anything, just as ChatGPT can be asked anything, some uses are going to leverage the best of the underlying technology, while others are going to fall prey to it's weaknesses and fundamental limitations. reply elashri 12 hours agoprevMy problem with all AI code assistants is usually the context. I am not sure how cursor fare in this regard but I always struggle to feed the model enough of the code project to be useful for me on a level more than providing line per line suggestion (which copilot does anyway). I don't have experience with cursor or cody (other alternative) and how they tackle this problem by using embeddings (which I suppose have similar context limit). reply extr 12 hours agoparentAll the SOTA LLM solutions like this have nearly the same problem. Sure the context window is huge, but there is no guarantee the model understands what 100K tokens of code is trying to accomplish within the context of the full codebase, or even into the real world, within the context of the business. They are just not good enough yet to use in real projects. Try it, start a greenfield project with \"just cursor\" like the ai-influencers do and see how far you get before it's an unmanagable mess and the LLM is lost in the weeds. Going the other direction in terms of model size, one tool I've found usable in these scenarios is Supermaven [0]. It's still just one or multi-line suggestions a la GH Copilot, so it's not generating entire apps for you, but it's much much better about pulling those one liners from the rest of the codebase in a logical way. If you have a custom logging module that overloads the standard one, with special functions, it will actually use those functions. Pretty impressive. Also very fast. [0] https://supermaven.com/ reply Der_Einzige 11 hours agorootparentCursor has a built in embeddings/RAG solutions to mitigate this problem. reply extr 2 hours agorootparentEmbeddings/RAG don't address the problem I'm talking about. The issue is that you can stuff the entire context window full of code and the models will superficially leverage it, but will still violate existing conventions, inappropriately bring in dependencies, duplicate functionality, etc. They don't \"grok\" the context at the correct level. reply bufferoverflow 12 hours agoparentprevCursor has 10K tokens context window. Which is quite low compared to the top LLMs. https://forum.cursor.com/t/capped-at-10k-context-no-matter-a... reply elashri 12 hours agorootparentThe main and best model according to many is Claude 3.5. But it provides maximum of 200k [1]. While I understand cost effectiveness and other limitations with embeddings. But maximum context of just 5% is probably too low with any standard. [1] https://support.anthropic.com/en/articles/7996856-what-is-th... reply throwup238 11 hours agorootparentprevYou have to switch to long context chat to get the full 200k (I don’t think this works for Composer though). reply batesy 2 hours agoparentprevIt's the users job to provide the context the LLM needs in plain language instruction. Not just relying on the LLM to magically understand everything based on the codebase. reply szundi 12 hours agoparentprevCursor is pretty uniqe and advanced in this regard. They tell a lot about this in the Lex Fridman podcast, very interesting. reply abricq 12 hours agoprevThis last month I decided to try the Jetbrain equivalent of Cursor, for their IDEs (https://www.jetbrains.com/ai/). It's a pluging well integrated in the code editor that you can easily summon. I work in Rust and I had to start working with several new libraries this month. One example of them is `proptest-rs`, a rust property testing library that defines a whole new grammar to define the tests. I am 100% sure that I spent much less time to get on-boarded with the librariy's best practices and usages. I just quickly went through their book (to learn the vocabulary) and asked the AI to generate the code itself. I was very surprised that it did not do any mistakes, considering that sort of weird custom grammar of the lib. I will at least keep trying for another months. reply HatchedLake721 9 hours agoparentFYI, 2024.3 that's coming in November/December will use a new model and the new code completion suggestions rewritten from scratch. I suspect some are inspired by Cursor? https://blog.jetbrains.com/ai/2024/10/complete-the-un-comple... reply ruszki 12 hours agoparentprevHow do you know that it didn’t make any mistakes, which you wouldn’t make if you learned the usage of that library without AI? Even before AI generated code, people made mistakes about which they didn’t know, because they never read the documentation for example, and it “worked”… except the unintended side effects of course. Adding an AI layer into the picture makes this definitely worse. reply abricq 9 hours agorootparentIt's not that you ask it to write 200 lines of code at once, and blindly trust it. It's more that you start to use the lib, ask it to generate one helper method at the time, for an isolated task. Which leave you time to \"review\" the code that it wrote properly. Even when a human writes code, it needs to go through peer-review. So the exact same applies with AI. It's the job of the reviewer (in this case, the one who invokes the AI) to make sure that the one who wrote the code does not do mistake, which can include going to read the doc in more detail. reply eru 12 hours agorootparentprevHow's the AI worse in this respect that having a coworker or teammate that you review code from? reply muixoozie 8 hours agoparentprevWondering how the JetBrains gets the context of the library? Is it fetching cargo docs somehow or are you having to paste docs into a context window? reply abricq 5 hours agorootparentI did not have to provide any context, so I guess it was trained partly with the cargo doc of the crate. reply easyKL 11 hours agoprevRecent interview to Cursor developers: https://lexfridman.com/cursor-team reply white_beach 11 hours agoparentthanks reply kobe_bryant 5 hours agoprevJust once I'd like to see an article like this from someone who's not currently working on an AI tool (some sort of Khan Academy tutor in this case) reply knuckleheads 6 hours agoprevI use and am happy with Cursor and Chatgpt. Cursor will write out the syntax for me if I let it, sometimes wrong, sometimes right, but enough to keep a flow going. If there are larger questions, I just flip over to chatgpt and try and suss out what is going on there. Super helpful with tailwind and react, speaking as a dba and backend systems person. reply rco8786 4 hours agoparentCurious why you don't just use Cursor's built-in chat? reply ertucetin 9 hours agoprevI also found myself feeling a bit dumb after using Copilot for some time. It felt like I didn’t have to know the API, and it just auto-completed for me. Then I realized I was starting to forget everything and disabled Copilot. Now, when I need something, I ask ChatGPT (like searching on Stack Overflow). reply metaltyphoon 5 hours agoparentSame. I find myself having to pause and let Copilot finish. At some point, you lose/ not retain anything which you don’t use. I’m not sure I want to give that. reply qwertox 12 hours agoprevI tried Cursor and while I was extremely surprised by the ability to do multiline edits in the middle of the lines, I could not get to accept how aggressive it was when trying to autocomplete/auto-edit segments of code while I was just typing. It was like a second person being in the editor having a mind of its own constantly touching my code, even if it should have left it alone. It felt like I was finding myself undoing stuff it made all the time. reply epolanski 10 hours agoprevSome uses I have, e.g. a notepad in Cursor with a predefined set of files and a prompt e.g. to implement storybook stories and documentation out of components I use. I give it the current file I'm working on, and it will generate new files and update documentation files. Similarly for E2E or unit tests, it often suggests cases I would've not thought about. The people that are negative about these things, because they need to review it, seem to be missing the massive amount of time saved imho. Many users point to the fact that they spend most of the time thinking, I'm glad for them, most of the time I spend is glueing APIs, boilerplates, refactoring, and on those aspects Cursor helps tremendously. The biggest killer feature that I get from similar tools (I ditched Copilot recently in favor of it) is that they allow me to stay focused and in the flow longer. I have a tendency to phase out when tasks get too boring, repetitive or stressed out when I can't come up with a solution. Similarly going on a search engine to find an answer would often put me in a long loop of looking for answers deeply buried in a very long article (you need to help SEO after all, don't you?) and then it would be more likely that I would get distracted by messages on my company chat or social media. I can easily say that Cursor has made me more productive than I was one year ago. I feel like the criticism many have comes from the wrong expectations of these tools doing the work for you, whereas they are more into easing out the boring and sometimes the hard parts. reply myflash13 10 hours agoprevAll of the examples given in the article are contrived, textbook-style examples. Real world projects are far more messy. I want someone to talk about their flow with Cursor on a mature codebase in production with lots of interlaced components and abstractions. I have a feeling that blindly building things with AI will actually lead to incomprehensible monstrous codebases that are impossible to maintain over the long run. Read “Programming as Theory Building” by Peter Naur. Programming is 80% theory-in-the-mind and only about 20% actual code. Here's an actual example of a task I have at work right now that AI is almost useless in helping me solve. \"I'm working with 4 different bank APIs, and I need to simplify the current request and data model so that data stored in disparate sources are unified into one SQL table called 'transactions'\". AI can't even begin to understand this request, let alone refactor the codebase to solve it. The end result should have fewer lines of code, not more, and it requires a careful understanding of multiple APIs and careful data modelling design and mapping where a single mistake could result in real financial damage. reply marviel 4 hours agoprevGood Taste, Management / Communication Skills, Code Review Ability. If you have these skills, the productivity gains from tools like Cursor are insane. If you lack any of these, it makes sense that you don't get the hype;",
    "originSummary": [
      "Cursor, a fork of Visual Studio Code, integrates Large Language Model (LLM) features, offering both free and subscription tiers, with key features like tab completion, inline editing, a chat sidebar, and a composer for cross-codebase refactors.- Tab completion aids in automating repetitive tasks and refactoring, though it may occasionally suggest incorrect completions; inline editing and chat features enhance code modifications.- The .cursorrules file can guide the LLM with coding standards, and Cursor has altered workflows by reducing reliance on external libraries and encouraging the use of unfamiliar languages."
    ],
    "commentSummary": [
      "The discussion centers on the use of AI tools, such as Cursor, in coding, with opinions divided on their benefits and drawbacks.",
      "Proponents argue that AI can automate repetitive tasks, boost productivity, and aid in learning new frameworks.",
      "Critics caution against AI's limitations in comprehending complex codebases, the danger of over-reliance, and the possibility of producing flawed code, highlighting the need to balance efficiency with skill retention."
    ],
    "points": 361,
    "commentCount": 328,
    "retryCount": 0,
    "time": 1730173823
  },
  {
    "id": 41984519,
    "title": "New Mac Mini with M4",
    "originLink": "https://www.apple.com/newsroom/2024/10/apples-new-mac-mini-is-more-mighty-more-mini-and-built-for-apple-intelligence/",
    "originBody": "PRESS RELEASE October 29, 2024 Apple’s all-new Mac mini is more mighty, more mini, and built for Apple Intelligence The compact, do-it-all desktop now features the power of M4 and M4 Pro, and marks an important environmental milestone as the first carbon neutral Mac With M4 and M4 Pro, the new Mac mini brings incredible performance and connectivity in a design that’s small enough to fit in your hand. CUPERTINO, CALIFORNIA Apple today unveiled the all-new Mac mini powered by the M4 and new M4 Pro chips, and redesigned around Apple silicon to pack an incredible amount of performance into an even smaller form of just 5 by 5 inches. With M4, Mac mini delivers up to 1.8x faster CPU performance and 2.2x faster GPU performance over the M1 model.1 With M4 Pro, it takes the advanced technologies in M4 and scales them up to tackle even more demanding workloads. For more convenient connectivity, it features front and back ports, and for the first time includes Thunderbolt 5 for faster data transfer speeds on the M4 Pro model. The new Mac mini is also built for Apple Intelligence, the personal intelligence system that transforms how users work, communicate, and express themselves while protecting their privacy. And marking an important environmental milestone, Mac mini is Apple’s first carbon neutral Mac with an over 80 percent reduction in greenhouse gas emissions across its materials, manufacturing, transportation, and customer use.2 Starting at just $599 with 16GB of memory, the new Mac mini is available to pre-order today, with availability beginning November 8. “The new Mac mini delivers gigantic performance in an unbelievably small design thanks to the power efficiency of Apple silicon and an innovative new thermal architecture,” said John Ternus, Apple’s senior vice president of Hardware Engineering. “Combined with the performance of M4 and the new M4 Pro chip, enhanced connectivity on both the front and back, and the arrival of Apple Intelligence, Mac mini is more capable and versatile than ever, and there is nothing else like it.” Small, but Fierce The new Mac mini footprint is less than half the size of the previous design at just 5 by 5 inches, so it takes up much less space on a desk. The super-compact system is enabled by the incredible power efficiency of Apple silicon and an innovative thermal architecture, which guides air to different levels of the system, while all venting is done through the foot. The unbelievably compact form is made possible by the power-efficient performance of Apple silicon combined with an all-new innovative thermal architecture. When compared to the best-selling PC desktop in its price range, Mac mini is up to 6x faster at one-twentieth the size.1 For a wide range of users, from students to aspiring creatives and small business owners, the Mac mini with M4 is a tiny powerhouse. Mac mini with M4 features a 10-core CPU, 10-core GPU, and now starts with 16GB of unified memory. Users will feel the performance of M4 in everything they do, from multitasking across everyday productivity apps to creative projects like video editing, music production, or writing and compiling code. Mac mini is more versatile than ever — perfect for anywhere, from home offices to large businesses and beyond. When compared to the Mac mini with Intel Core i7, Mac mini with M4: Applies up to 2.8x more audio effect plugins in a Logic Pro project.1 Delivers up to 13.3x faster gaming performance in World of Warcraft: The War Within.1 Enhances photos with up to 33x faster image upscaling performance in Photomator.3 When compared to the Mac mini with M1, Mac mini with M4: Performs spreadsheet calculations up to 1.7x faster in Microsoft Excel.1 Transcribes with on-device AI speech-to-text up to 2x faster in MacWhisper.1 Merges panoramic images up to 4.9x faster in Adobe Lightroom Classic.4 Introducing M4 Pro for Pro-Level Performance For users who want pro-level performance, Mac mini with M4 Pro features the world’s fastest CPU core5 with lightning-fast single-threaded performance. With up to 14 cores, including 10 performance cores and four efficiency cores, M4 Pro also provides phenomenal multithreaded performance. With up to 20 cores, the M4 Pro GPU is up to twice as powerful as the GPU in M4, and both chips bring hardware-accelerated ray tracing to the Mac mini for the first time. The Neural Engine in M4 Pro is also over 3x faster than in Mac mini with M1, so on-device Apple Intelligence models run at blazing speed. M4 Pro supports up to 64GB of unified memory and 273GB/s of memory bandwidth — twice as much bandwidth as any AI PC chip — for accelerating AI workloads. And M4 Pro supports Thunderbolt 5, which delivers up to 120 Gb/s data transfer speeds on Mac mini, and more than doubles the throughput of Thunderbolt 4. The M4 Pro chip takes the advanced technologies debuted in M4 and scales them up to tackle even more demanding workloads. When compared to the Mac mini with Intel Core i7, Mac mini with M4 Pro: Performs spreadsheet calculations up to 4x faster in Microsoft Excel.1 Executes scene-edit detection up to 9.4x faster in Adobe Premiere Pro.3 Transcribes with on-device AI speech-to-text up to 20x faster in MacWhisper.1 Processes basecalling for DNA sequencing in Oxford Nanopore MinKNOW up to 26x faster.1 When compared to the Mac mini with M2 Pro, Mac mini with M4 Pro: Applies up to 1.8x more audio effect plugins in a Logic Pro project.1 Renders motion graphics to RAM up to 2x faster in Motion.6 Completes 3D renders up to 2.9x faster in Blender.6 The Mac mini with M4 Pro comes with Thunderbolt 5 for the first time, which more than doubles transfer speeds, so users will be able to use faster external storage and connect to more accessories at the same time. Upgraded Connectivity and Display Support The new Mac mini features a wide array of ports to drive any setup. It includes front-facing ports for more convenient access, including two USB-C ports that support USB 3, and an audio jack with support for high-impedance headphones. On the back, Mac mini with M4 includes three Thunderbolt 4 ports, while Mac mini with M4 Pro features three Thunderbolt 5 ports. Mac mini comes standard with Gigabit Ethernet, configurable up to 10Gb Ethernet for faster networking speeds, and an HDMI port for easy connection to a TV or HDMI display without an adapter. With M4, Mac mini can support up to two 6K displays and up to one 5K display, and with M4 Pro, it can support up to three 6K displays at 60Hz for a total of over 60 million pixels. Mac mini is shown from the front. Mac mini is shown from the back. The new front-facing ports on Mac mini include two USB-C ports and an audio jack with support for high-impedance headphones. On the back, the M4 model features three Thunderbolt 4 ports, and the M4 Pro model includes Thunderbolt 5 for the first time. Both models also include Gigabit Ethernet and an HDMI port. previous next A New Era with Apple Intelligence on the Mac Apple Intelligence ushers in a new era for the Mac, bringing personal intelligence to the personal computer. Combining powerful generative models with industry-first privacy protections, Apple Intelligence harnesses the power of Apple silicon and the Neural Engine to unlock new ways for users to work, communicate, and express themselves on Mac. It is available in U.S. English with macOS Sequoia 15.1. With systemwide Writing Tools, users can refine their words by rewriting, proofreading, and summarizing text nearly everywhere they write. With the newly redesigned Siri, users can move fluidly between spoken and typed requests to accelerate tasks throughout their day, and Siri can answer thousands of questions about Mac and other Apple products. New Apple Intelligence features will be available in December, with additional capabilities rolling out in the coming months. Image Playground gives users a new way to create fun original images, and Genmoji allows them to create custom emoji in seconds. Siri will become even more capable, with the ability to take actions across the system and draw on a user’s personal context to deliver intelligence that is tailored to them. In December, ChatGPT will be integrated into Siri and Writing Tools, allowing users to access its expertise without needing to jump between tools. A Writing Tools screen features highlighted text from a scientific paper alongside proofreading and rewriting options. A typed request to Siri says, “Show me all files Ian sent last week.” With brand-new systemwide Writing Tools powered by Apple Intelligence, users can rewrite, proofread, and summarize text virtually everywhere they type. In the coming months, Siri will be even more capable on macOS Sequoia, with the ability to understand a user’s personal context and deliver intelligence that is tailored to them. previous next Apple Intelligence does all this while protecting users’ privacy at every step. At its core is on-device processing, and for more complex tasks, Private Cloud Compute gives users access to Apple’s even larger, server-based models and offers groundbreaking protections for personal information. In addition, users can access ChatGPT for free without creating an account, and privacy protections are built in — their IP addresses are obscured and OpenAI won’t store requests. For those who choose to connect their account, OpenAI’s data-use policies apply. The First Carbon Neutral Mac The new Mac mini is Apple’s first carbon neutral Mac, marking a significant milestone toward Apple 2030, the company’s goal to be carbon neutral across the entire carbon footprint by the end of this decade. Mac mini is made with over 50 percent recycled content overall, including 100 percent recycled aluminum in the enclosure, 100 percent recycled gold plating in all Apple-designed printed circuit boards, and 100 percent recycled rare earth elements in all magnets. The electricity used to manufacture Mac mini is sourced from 100 percent renewable electricity. And, to address 100 percent of the electricity customers use to power Mac mini, Apple has invested in clean energy projects around the world. Apple has also prioritized lower-carbon modes of shipping, like ocean freight, to further reduce emissions from transportation. Together, these actions have reduced the carbon footprint of Mac mini by over 80 percent.2 For the small amount of remaining emissions, Apple applies high-quality carbon credits from nature-based projects, like those generated by its innovative Restore Fund. In another first for Mac mini, the packaging is now entirely fiber-based, bringing Apple closer to its goal to remove plastic from its packaging by 2025. The new Mac mini is Apple’s first carbon neutral Mac. An Unrivaled Experience with macOS Sequoia macOS Sequoia completes the new Mac mini experience with a host of exciting features, including iPhone Mirroring, allowing users to wirelessly interact with their iPhone, its apps, and notifications directly from their Mac.7 Safari, the world’s fastest browser,8 now offers the Highlights feature, which quickly pulls up relevant information from a site; a smarter, redesigned Reader with a table of contents and high-level summary; and a new Video Viewer to watch videos without distractions. With Distraction Control, users can hide items on a webpage that they may find disruptive to their browsing. Gaming gets even more immersive with features like Personalized Spatial Audio and improvements to Game Mode, along with a breadth of exciting titles, including the upcoming Assassin’s Creed Shadows. Easier window tiling means users can stay organized with a window layout that works best for them. The all-new Passwords app gives convenient access to passwords, passkeys, and other credentials — all stored in one place. And users can apply new, beautiful built-in backgrounds for video calls, which include a variety of color gradients and system wallpapers, or upload their own photos. Pricing and Availability Customers can pre-order the new Mac mini with M4 and M4 Pro starting today, Tuesday, October 29, on apple.com/store and in the Apple Store app in 28 countries and regions, including the U.S. It will start arriving to customers, and in Apple Store locations and Apple Authorized Resellers, beginning Friday, November 8. Mac mini with M4 starts at $599 (U.S.) and $499 (U.S.) for education. Additional technical specifications are available at apple.com/mac-mini. Mac mini with M4 Pro starts at $1,399 (U.S.) and $1,299 (U.S.) for education. Additional technical specifications are available at apple.com/mac-mini. New accessories with USB-C — including Magic Keyboard ($99 U.S.), Magic Keyboard with Touch ID ($149 U.S.), Magic Keyboard with Touch ID and Numeric Keypad ($179 U.S.), Magic Trackpad ($129 U.S.), Magic Mouse ($79 U.S.), and Thunderbolt 5 Pro Cable ($69) — are available at apple.com/store. Apple Intelligence is available now as a free software update for Mac with M1 and later, and can be accessed in most regions around the world when the device and Siri language are set to U.S. English. The first set of features is in beta and available with macOS Sequoia 15.1, with more features rolling out in the months to come. Apple Intelligence is quickly adding support for more languages. In December, Apple Intelligence will add support for localized English in Australia, Canada, Ireland, New Zealand, South Africa, and the U.K., and in April, a software update will deliver expanded language support, with more coming throughout the year. Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, Vietnamese, and other languages will be supported. With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit apple.com/shop/trade-in to see what their device is worth. AppleCare+ for Mac provides unparalleled service and support. This includes unlimited incidents of accidental damage, battery service coverage, and 24/7 support from the people who know Mac best. Every customer who buys directly from Apple Retail gets access to Personal Setup. In these guided online sessions, a Specialist can walk them through setup, or focus on features that help them make the most of their new device. Customers can also learn more about getting started with their new device with a Today at Apple session at their nearest Apple Store. Share article Media Text of this article October 29, 2024 PRESS RELEASE Apple’s all-new Mac mini is more mighty, more mini, and built for Apple Intelligence The compact, do-it-all desktop now features the power of M4 and M4 Pro, and marks an important environmental milestone as the first carbon neutral Mac CUPERTINO, CALIFORNIA Apple today unveiled the all-new Mac mini powered by the M4 and new M4 Pro chips, and redesigned around Apple silicon to pack an incredible amount of performance into an even smaller form of just 5 by 5 inches. With M4, Mac mini delivers up to 1.8x faster CPU performance and 2.2x faster GPU performance over the M1 model.1 With M4 Pro, it takes the advanced technologies in M4 and scales them up to tackle even more demanding workloads. For more convenient connectivity, it features front and back ports, and for the first time includes Thunderbolt 5 for faster data transfer speeds on the M4 Pro model. The new Mac mini is also built for Apple Intelligence, the personal intelligence system that transforms how users work, communicate, and express themselves while protecting their privacy. And marking an important environmental milestone, Mac mini is Apple’s first carbon neutral Mac with an over 80 percent reduction in greenhouse gas emissions across its materials, manufacturing, transportation, and customer use.2 Starting at just $599 with 16GB of memory, the new Mac mini is available to pre-order today, with availability beginning November 8. “The new Mac mini delivers gigantic performance in an unbelievably small design thanks to the power efficiency of Apple silicon and an innovative new thermal architecture,” said John Ternus, Apple’s senior vice president of Hardware Engineering. “Combined with the performance of M4 and the new M4 Pro chip, enhanced connectivity on both the front and back, and the arrival of Apple Intelligence, Mac mini is more capable and versatile than ever, and there is nothing else like it.” Small, but Fierce The new Mac mini footprint is less than half the size of the previous design at just 5 by 5 inches, so it takes up much less space on a desk. The super-compact system is enabled by the incredible power efficiency of Apple silicon and an innovative thermal architecture, which guides air to different levels of the system, while all venting is done through the foot. When compared to the best-selling PC desktop in its price range, Mac mini is up to 6x faster at one-twentieth the size.1 For a wide range of users, from students to aspiring creatives and small business owners, the Mac mini with M4 is a tiny powerhouse. Mac mini with M4 features a 10-core CPU, 10-core GPU, and now starts with 16GB of unified memory. Users will feel the performance of M4 in everything they do, from multitasking across everyday productivity apps to creative projects like video editing, music production, or writing and compiling code. When compared to the Mac mini with Intel Core i7, Mac mini with M4: Applies up to 2.8x more audio effect plugins in a Logic Pro project.1 Delivers up to 13.3x faster gaming performance in World of Warcraft: The War Within.1 Enhances photos with up to 33x faster image upscaling performance in Photomator.3 When compared to the Mac mini with M1, Mac mini with M4: Performs spreadsheet calculations up to 1.7x faster in Microsoft Excel.1 Transcribes with on-device AI speech-to-text up to 2x faster in MacWhisper.1 Merges panoramic images up to 4.9x faster in Adobe Lightroom Classic.4 Introducing M4 Pro for Pro-Level Performance For users who want pro-level performance, Mac mini with M4 Pro features the world’s fastest CPU core5 with lightning-fast single-threaded performance. With up to 14 cores, including 10 performance cores and four efficiency cores, M4 Pro also provides phenomenal multithreaded performance. With up to 20 cores, the M4 Pro GPU is up to twice as powerful as the GPU in M4, and both chips bring hardware-accelerated ray tracing to the Mac mini for the first time. The Neural Engine in M4 Pro is also over 3x faster than in Mac mini with M1, so on-device Apple Intelligence models run at blazing speed. M4 Pro supports up to 64GB of unified memory and 273GB/s of memory bandwidth — twice as much bandwidth as any AI PC chip — for accelerating AI workloads. And M4 Pro supports Thunderbolt 5, which delivers up to 120 Gb/s data transfer speeds on Mac mini, and more than doubles the throughput of Thunderbolt 4. When compared to the Mac mini with Intel Core i7, Mac mini with M4 Pro: Performs spreadsheet calculations up to 4x faster in Microsoft Excel.1 Executes scene-edit detection up to 9.4x faster in Adobe Premiere Pro.3 Transcribes with on-device AI speech-to-text up to 20x faster in MacWhisper.1 Processes basecalling for DNA sequencing in Oxford Nanopore MinKNOW up to 26x faster.1 When compared to the Mac mini with M2 Pro, Mac mini with M4 Pro: Applies up to 1.8x more audio effect plugins in a Logic Pro project.1 Renders motion graphics to RAM up to 2x faster in Motion.6 Completes 3D renders up to 2.9x faster in Blender.6 Upgraded Connectivity and Display Support The new Mac mini features a wide array of ports to drive any setup. It includes front-facing ports for more convenient access, including two USB-C ports that support USB 3, and an audio jack with support for high-impedance headphones. On the back, Mac mini with M4 includes three Thunderbolt 4 ports, while Mac mini with M4 Pro features three Thunderbolt 5 ports. Mac mini comes standard with Gigabit Ethernet, configurable up to 10Gb Ethernet for faster networking speeds, and an HDMI port for easy connection to a TV or HDMI display without an adapter. With M4, Mac mini can support up to two 6K displays and up to one 5K display, and with M4 Pro, it can support up to three 6K displays at 60Hz for a total of over 60 million pixels. A New Era with Apple Intelligence on the Mac Apple Intelligence ushers in a new era for the Mac, bringing personal intelligence to the personal computer. Combining powerful generative models with industry-first privacy protections, Apple Intelligence harnesses the power of Apple silicon and the Neural Engine to unlock new ways for users to work, communicate, and express themselves on Mac. It is available in U.S. English with macOS Sequoia 15.1. With systemwide Writing Tools, users can refine their words by rewriting, proofreading, and summarizing text nearly everywhere they write. With the newly redesigned Siri, users can move fluidly between spoken and typed requests to accelerate tasks throughout their day, and Siri can answer thousands of questions about Mac and other Apple products. New Apple Intelligence features will be available in December, with additional capabilities rolling out in the coming months. Image Playground gives users a new way to create fun original images, and Genmoji allows them to create custom emoji in seconds. Siri will become even more capable, with the ability to take actions across the system and draw on a user’s personal context to deliver intelligence that is tailored to them. In December, ChatGPT will be integrated into Siri and Writing Tools, allowing users to access its expertise without needing to jump between tools. Apple Intelligence does all this while protecting users’ privacy at every step. At its core is on-device processing, and for more complex tasks, Private Cloud Compute gives users access to Apple’s even larger, server-based models and offers groundbreaking protections for personal information. In addition, users can access ChatGPT for free without creating an account, and privacy protections are built in — their IP addresses are obscured and OpenAI won’t store requests. For those who choose to connect their account, OpenAI’s data-use policies apply. The First Carbon Neutral Mac The new Mac mini is Apple’s first carbon neutral Mac, marking a significant milestone toward Apple 2030, the company’s goal to be carbon neutral across the entire carbon footprint by the end of this decade. Mac mini is made with over 50 percent recycled content overall, including 100 percent recycled aluminum in the enclosure, 100 percent recycled gold plating in all Apple-designed printed circuit boards, and 100 percent recycled rare earth elements in all magnets. The electricity used to manufacture Mac mini is sourced from 100 percent renewable electricity. And, to address 100 percent of the electricity customers use to power Mac mini, Apple has invested in clean energy projects around the world. Apple has also prioritized lower-carbon modes of shipping, like ocean freight, to further reduce emissions from transportation. Together, these actions have reduced the carbon footprint of Mac mini by over 80 percent.2 For the small amount of remaining emissions, Apple applies high-quality carbon credits from nature-based projects, like those generated by its innovative Restore Fund. In another first for Mac mini, the packaging is now entirely fiber-based, bringing Apple closer to its goal to remove plastic from its packaging by 2025. An Unrivaled Experience with macOS Sequoia macOS Sequoia completes the new Mac mini experience with a host of exciting features, including iPhone Mirroring, allowing users to wirelessly interact with their iPhone, its apps, and notifications directly from their Mac.7 Safari, the world’s fastest browser,8 now offers the Highlights feature, which quickly pulls up relevant information from a site; a smarter, redesigned Reader with a table of contents and high-level summary; and a new Video Viewer to watch videos without distractions. With Distraction Control, users can hide items on a webpage that they may find disruptive to their browsing. Gaming gets even more immersive with features like Personalized Spatial Audio and improvements to Game Mode, along with a breadth of exciting titles, including the upcoming Assassin’s Creed Shadows. Easier window tiling means users can stay organized with a window layout that works best for them. The all-new Passwords app gives convenient access to passwords, passkeys, and other credentials — all stored in one place. And users can apply new, beautiful built-in backgrounds for video calls, which include a variety of color gradients and system wallpapers, or upload their own photos. Pricing and Availability Customers can pre-order the new Mac mini with M4 and M4 Pro starting today, Tuesday, October 29, on apple.com/store and in the Apple Store app in 28 countries and regions, including the U.S. It will start arriving to customers, and in Apple Store locations and Apple Authorized Resellers, beginning Friday, November 8. Mac mini with M4 starts at $599 (U.S.) and $499 (U.S.) for education. Additional technical specifications are available at apple.com/mac-mini. Mac mini with M4 Pro starts at $1,399 (U.S.) and $1,299 (U.S.) for education. Additional technical specifications are available at apple.com/mac-mini. New accessories with USB-C — including Magic Keyboard ($99 U.S.), Magic Keyboard with Touch ID ($149 U.S.), Magic Keyboard with Touch ID and Numeric Keypad ($179 U.S.), Magic Trackpad ($129 U.S.), Magic Mouse ($79 U.S.), and Thunderbolt 5 Pro Cable ($69) — are available at apple.com/store. Apple Intelligence is available now as a free software update for Mac with M1 and later, and can be accessed in most regions around the world when the device and Siri language are set to U.S. English. The first set of features is in beta and available with macOS Sequoia 15.1, with more features rolling out in the months to come. Apple Intelligence is quickly adding support for more languages. In December, Apple Intelligence will add support for localized English in Australia, Canada, Ireland, New Zealand, South Africa, and the U.K., and in April, a software update will deliver expanded language support, with more coming throughout the year. Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, Vietnamese, and other languages will be supported. With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit apple.com/shop/trade-in to see what their device is worth. AppleCare+ for Mac provides unparalleled service and support. This includes unlimited incidents of accidental damage, battery service coverage, and 24/7 support from the people who know Mac best. Every customer who buys directly from Apple Retail gets access to Personal Setup. In these guided online sessions, a Specialist can walk them through setup, or focus on features that help them make the most of their new device. Customers can also learn more about getting started with their new device with a Today at Apple session at their nearest Apple Store. About Apple Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, AirPods, Apple Watch, and Apple Vision Pro. Apple’s six software platforms — iOS, iPadOS, macOS, watchOS, visionOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, iCloud, and Apple TV+. Apple’s more than 150,000 employees are dedicated to making the best products on earth and to leaving the world better than we found it. Testing was conducted by Apple in September and October 2024. See apple.com/mac-mini for more information. Carbon reductions are calculated against a business-as-usual baseline scenario: No use of clean electricity for manufacturing or product use, beyond what is already available on the latest modeled grid; Apple’s carbon intensity of key materials as of 2015; and Apple’s average mix of transportation modes by product line across three years. Learn more at apple.com/2030. Results are compared to previous-generation 3.2GHz 6-core Intel Core i7-based Mac mini systems with Intel Iris UHD Graphics 630, 64GB of RAM, and 2TB SSD. Results are compared to previous-generation Mac mini systems with Apple M1, 8-core CPU, 8-core GPU, 16GB of RAM, and 2TB SSD. Testing conducted by Apple in October 2024 using shipping competitive systems and select industry-standard benchmarks. Results are compared to previous-generation Mac mini systems with Apple M2 Pro, 12-core CPU, 19-core GPU, 32GB of RAM, and 8TB SSD. Available on Mac computers with Apple silicon and Intel-based Mac computers with a T2 Security Chip. Requires that iPhone and Mac are signed in with the same Apple Account using two-factor authentication, iPhone and Mac are near each other and have Bluetooth and Wi-Fi turned on, and Mac is not using AirPlay or Sidecar. Some iPhone features (e.g., camera and microphone) are not compatible with iPhone Mirroring. Testing was conducted by Apple in August 2024. See apple.com/safari for more information. Press Contacts Michelle Del Rio Apple mr_delrio@apple.com Starlayne Meza Apple starlayne_meza@apple.com Apple Media Helpline media.help@apple.com Copy text Images in this article Download all images About Apple Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, AirPods, Apple Watch, and Apple Vision Pro. Apple’s six software platforms — iOS, iPadOS, macOS, watchOS, visionOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, iCloud, and Apple TV+. Apple’s more than 150,000 employees are dedicated to making the best products on earth and to leaving the world better than we found it. Testing was conducted by Apple in September and October 2024. See apple.com/mac-mini for more information. Carbon reductions are calculated against a business-as-usual baseline scenario: No use of clean electricity for manufacturing or product use, beyond what is already available on the latest modeled grid; Apple’s carbon intensity of key materials as of 2015; and Apple’s average mix of transportation modes by product line across three years. Learn more at apple.com/2030. Results are compared to previous-generation 3.2GHz 6-core Intel Core i7-based Mac mini systems with Intel Iris UHD Graphics 630, 64GB of RAM, and 2TB SSD. Results are compared to previous-generation Mac mini systems with Apple M1, 8-core CPU, 8-core GPU, 16GB of RAM, and 2TB SSD. Testing conducted by Apple in October 2024 using shipping competitive systems and select industry-standard benchmarks. Results are compared to previous-generation Mac mini systems with Apple M2 Pro, 12-core CPU, 19-core GPU, 32GB of RAM, and 8TB SSD. Available on Mac computers with Apple silicon and Intel-based Mac computers with a T2 Security Chip. Requires that iPhone and Mac are signed in with the same Apple Account using two-factor authentication, iPhone and Mac are near each other and have Bluetooth and Wi-Fi turned on, and Mac is not using AirPlay or Sidecar. Some iPhone features (e.g., camera and microphone) are not compatible with iPhone Mirroring. Testing was conducted by Apple in August 2024. See apple.com/safari for more information. Press Contacts Michelle Del Rio Apple mr_delrio@apple.com Starlayne Meza Apple starlayne_meza@apple.com Apple Media Helpline media.help@apple.com Latest News APPLE STORIES How Apple developed the world’s first end-to-end hearing health experience October 28, 2024 PRESS RELEASE Apple Intelligence is available today on iPhone, iPad, and Mac October 28, 2024 PRESS RELEASE Apple introduces new iMac supercharged by M4 and Apple Intelligence October 28, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41984519",
    "commentBody": "New Mac Mini with M4 (apple.com)359 points by victorbjorklund 4 hours agohidepastfavorite529 comments e63f67dd-065b 2 hours agoWith educational pricing this thing starts as $500, and at 16GB of RAM (finally) I think this easily beats any sort of desktop PC you can buy at that price (let's exclude custom builds, they're not the same market). I think this just became the go-to recommendation I'll give to anybody wanting an entry-level desktop computer of any kind. In fact I might buy one for my parents right now to replace the old mac mini they have. I really can't think of any reasonable competition for it at that price. reply architect64 1 hour agoparentOne issue to watch out for: Sub-4K res monitors look surprisingly bad on newer versions of macOS with Apple Silicon Macs. And no, it's not simply a matter of non-Retina obviously not looking as nice as Retina monitors - something like a 1440p monitor will look much worse on macOS than it would on Windows or Linux. This is partly caused by a lack of subpixel rendering for text on macOS, but it doesn't affect just text, with app icon graphics and such seemingly optimized for High-DPI resolutions only and thus looking awful too. You commonly see people using 3rd party apps such as BetterDisplay to partially work around this problem by tricking the system to treat 1440p displays as 5K displays and then downscale, but it doesn't solve this completely. So yes, the price for the machine is fantastic, but you may want to budget for a basic 4K display as well. reply vondur 1 minute agorootparentIf you have a 1440P 27\" monitor, they work great. reply anemoknee 25 minutes agorootparentprevIs this with newer Apple Silicon Macs? My 2020 M1 Mac Mini looks unremarkably normal on my 1440p display. I'm also going between that and my 14\" M1 Pro Macbook Pro, which of course looks beautiful but doesn't really make the 1440p on the Mini 'bad'. Edit: Adding that both of these machines are now running macOS 15.1 at this time. reply gymbeaux 15 minutes agorootparentIn my experience, you can’t do any sort of scaling with sub-4K displays. This is “since M1”. Intel Macs, even on the latest macOS, can do scaling eg 1.5x at say 1440p, which last time I bothered with an Intel Mac required a workaround via Terminal to re-enable. But that workaround is “patched” on Apple Silicon and won’t work. So yes if you have an Apple Silicon Mac plugged into a 1440p display, it will look bad with any sort of “scaling”- because scaling is disabled on macOS for sub-4K displays. What you’re actually doing when you’re “scaling” on say a 1440p display is running that display at 1920x1080 resolution- hence it looks like ass. Back before Apple Silicon, running that 1440p display at “1920x1080” was actually just scaling the UI elements up to appear as though you had a 1920x1080 display- since it was still utilizing the full …x1440 pixels of the display, “1920x1080” looked nicer than it would now. So brass tacks it’s just about how macOS/OS X would obfuscate the true display resolution in the System Preferences -> Displays menu. Now with Apple Silicon Macs, “1920x1080” means “2x scaling” for 4K monitors and literally “we’ll run this higher-res monitor at literally 1920x1080” for any display under 4K resolution. reply gymbeaux 13 minutes agorootparentprevIf your 1440p monitor looks “fine” or “good”, it’s because the scale is 1x - for many people, including myself, UI elements are too small at 1x 1440p. I had to buy a 4K monitor so I could have larger UI elements AND crisp UI elements. reply stalfosknight 38 minutes agorootparentprevNon-Apple displays have awful PPI, even the allegedly high-DPI ones. reply tasty_freeze 35 minutes agorootparentHow does that address the point the person you are replying to make: > something like a 1440p monitor will look much worse on macOS than it would on Windows or Linux. reply goosedragons 1 hour agoparentprevThere's still good deals in mini PC land. Yes, the M4 is faster but there's loads of mini PCs with decent CPUs, 32GB RAM and a 1TB of SSD storage for under $600. I think for a lot of people for basic usage they'll get more value out of the larger and upgradable SSDs than the faster CPU. https://www.amazon.com/BOSGAME-5700U-Displays-Computers-Emul... https://www.amazon.com/Beelink-SER5-Desktop-Computer-Graphic... reply celestialcheese 2 minutes agorootparentFrom my experience, TCO on most apple products ends up being roughly the same when you factor in resale value. You'll be able to sell your M4 mac mini in 5 years for $150 for an instant-cash offer from backmarket or any other reseller, while you'd be lucky to get $30 for the equivalent Beelink or BOSGAME after 6 months on ebay. reply gymbeaux 11 minutes agorootparentprevI bought one of these once. The specs on paper look good, but the CPUs are weak. They’re like those U series Intel CPUs where you could get say an i7-7700U, with 4 physical cores and 8 total threads, but at 15W TDP you were never really going to benefit from the 4 cores and 8 threads. reply srid 21 minutes agorootparentprevWhich mini PC is recommended for those looking to run Linux (as a server, not desktop) on them? reply asciimov 14 minutes agorootparentGo look into the N100 mini pcs like: https://www.amazon.com/dp/B0BYJ9BC15 It's a nice little low wattage machine for running some docker containers. reply Tempest1981 1 hour agorootparentprevBOSGAME and Beelink... who makes the motherboards? Nice size. The Beelink has better reviews. Any name brands? reply throwaway48476 57 minutes agorootparentThey're OEM's. They're becoming name brands if you're the kind of person that follows micro PCs. reply magnio 54 minutes agorootparentprevBeelink is well-known in the mini PC market. reply supportengineer 1 hour agorootparentprevBut you have to run Windows, which is a deal breaker for most people. reply risho 1 hour agorootparentseeing as how windows overwhelmingly owns the desktop market i would suggest it is in fact not a deal breaker for most people. reply nervousvarun 42 minutes agorootparentprevJust looked and Windows is running 73% of desktops/laptops. Can you provide a source showing \"most people\" find Windows a deal breaker? https://gs.statcounter.com/os-market-share/desktop/worldwide... reply NBJack 33 minutes agorootparentprevSays who? Install your favorite flavor of Linux then. Beelink devices have a good reputation for being quite happy with a new OS. It's more compatible that the latest Apple devices, that's for certain. reply Thomashuet 31 minutes agorootparentprevYou don't have to, you can run your favorite flavor of Linux. Unlike with the Mac Mini which can only run macOS. reply pa7ch 27 minutes agorootparentAsahi Linux has changed that equation for M1/M2 mac minis. I'm sure M4 will be supported soon. reply matheusmoreira 1 hour agorootparentprevWhy would you need to run Windows? Linux should work mini computers just fine. reply cultofmetatron 46 minutes agorootparentI have a minisforum minipc. first thing I did was wipe windows and put popos on it. super happy with it. That said, getting anyone who isn't used to linux to usd anything other than windows as easy as pulling your teeth. People go towards whats familiar; Even when what's familiar is objectively trash that spies on you. reply sangnoir 1 hour agoparentprev> I think this easily beats any sort of desktop PC you can buy at that price (let's exclude custom builds, they're not the same market). This is squarely in the NUC/SFF/1l-pc territory, and there is plenty of competition here from Beelink and Minisforum. I just found the Beelink SER7 going for $509, and it has an 8-core/16-thread Ryzen 7 CPU, 32GB DDR4. The 8845 in the beelink is very competitive[1] with M4 (beaten, but not \"easily\"), and also supports memory upgrades of up to 256GB. 1. https://nanoreview.net/en/cpu-compare/apple-m4-vs-amd-ryzen-... reply tmikaeld 1 hour agorootparentWhen you factor in memory bandwidth (80GB/s for DDR4) - that's not even close to the M4 (120GB/s base model). reply sangnoir 1 hour agorootparentWhich regular desktop tasks are kneecapped by 80GBps memory bandwidth? reply marci 17 minutes agorootparentYou really start taking into account bandwidth when you need 64GB or more (which is rarelly). If it's audio/video, spawning VMs, it doesn't matter much. If it's for generative software, it might become an issue. reply LeoPanthera 1 hour agorootparentprevThere's a huge difference there. Those PCs have to be ordered from Aliexpress, or some other Chinese site, or else from Amazon via a third party resellers that adds their own markup on top. Neither gets you any kind of useful warranty, at least for most people, who are unwilling to deal with overseas companies. Apple has actual physical stores, and a phone number you can call. reply sangnoir 38 minutes agorootparent> Those PCs have to be ordered from Aliexpress, or some other Chinese site, or else from Amazon via a third party resellers that adds their own markup on top I anticipated this concern, and the $509 price I gave is the Amazon price that includes the mark-up. The Beelink SER7 only costs $320 on AliExpress. Modern solid-state electronics are very reliable, most reliability issues for electronics are related to screens or batteries; which desktop computers lack. I guess there was a bad-capacitor problem over a decade ago, but nothing since then. If your risk-aversion for a desktop computer is high, you pay the Apple premium (possibly buying Applecare), or slef-insure by buying 2 SER7s for nearly the same price ($640) as one regular M4 Mac Mini and keep the other one as a spare. reply ericmay 3 minutes agorootparentIF you're ordering them in the context of a larger buying program like a university or other office you'd at least get some sort of account rep and Apple support as well. I'm not sure if you could get that from Beelink, could you? I see some benefit in that use case. But that's aside from the main topic which was the personal and home use case. On that topic you get a decent set of products as well such as Pages/Numbers/etc. and others along with software support for the Mac Mini. I'm guessing the Beelink runs on Linux? That may be hard for some to work with (which is unfortunate since it's really not), or maybe they have to separately buy a Windows license? Something to consider in the comparison. reply TomatoTomato 1 hour agorootparentprevWhere can you get 2x128gb sodimms? reply sangnoir 1 hour agorootparentCrucial.com is my go-to, but you can also get them from Amazon. reply huijzer 1 hour agoparentprevDo note only 256 GB of storage. Should be enough for most people, but at the same time can become very annoying once it gets full. reply setgree 1 hour agorootparentWe can reasonably expect 1) external storage to become faster and cheaper every year (subject to constraints around interface) 2) more and more digital assets to be cloud-native, e.g. photos stored exclusively on icloud and not on your computer So I'm less worried about storage than some. If Asahi Linux achieves Proton-like compatibility with games [0], then we're getting closer to the perfect general purpose game console. [0] https://asahilinux.org/2024/10/aaa-gaming-on-asahi-linux/ reply kridsdale3 1 hour agorootparentWith Thunderbolt 5, once external SSD enclosures supporting it exist, there should be zero performance penalty for external vs internal storage speed, finally. Then you can built a 1PB array, if you want. reply NBJack 31 minutes agorootparentprevLatency is a big problem for cloud gaming, and not likely to be solved any time soon. reply drilbo 10 minutes agorootparentprevi think M4 support in Asahi is quite a ways out reply SoftTalker 1 hour agorootparentprevCloud storage is still painfully slow compared to local. reply setgree 39 minutes agorootparentWe can expect different storage solutions by product depending on how fast things need to be. It doesn’t need to be lightning quick to load a frame in a movie, for instance, which is why streaming dominates there. reply dlachausse 1 hour agorootparentprevIt’s a desktop, so plugging in a USB external hard drive isn’t too painful or expensive. reply eastbound 1 hour agorootparentDockers can’t run off external drives. reply slashdave 1 hour agorootparentThat's an interesting claim. An external drive is just another block device. Is this something you experienced? reply kridsdale3 1 hour agorootparentprevWhy? Thunderbolt is PCI-E. There shouldn't be any difference between TB attached storage and adding disks to a desktop tower. reply echoangle 59 minutes agorootparentprevThis doesn’t seem to be true, and I don’t even get what it would change if it were true. Developers aren’t the target demographic of the base version with low storage. reply adib 29 minutes agorootparentprevDocker on Mac works fine on external drive. I moved the storage volume on a rotating USB 3 drive. reply dlachausse 1 hour agorootparentprevSince macOS is a UNIX system, can’t you just use a symlink to the external drive or is there something specific about Docker that prevents this? reply Phrodo_00 1 hour agorootparentDocker in macOS (at least the useful one) just runs in a Linux VM, and I don't see why you couldn't run a VM off an image on an external drive. Maybe the UI doesn't let you select that location? reply holografix 49 minutes agorootparentprevThis is a conscious tactic so X% of customers say yes to using iCloud storage reply semanticist 30 minutes agorootparentApple offering expensive upgrades for storage and memory pre-dates the existence of iCloud storage by decades. It was entirely standard before MobileMe, or Apple offering any kind of \"cloud\" services. Apple just charge a lot of money for upgrades, even did when it was trivial to do them yourself, and they're not going to change once they made it impossible to do any kind of internal upgrade. reply zackymacky 2 hours agoparentprevHelp me understand the $500 starting price? I see $1250 starting price on pre-orders from the education store. reply plushpuffin 2 hours agorootparentThat’s for the model with the M4 Pro chip. Mac mini with M4 starts at $599 (U.S.) and $499 (U.S.) for education. Mac mini with M4 Pro starts at $1,399 (U.S.) and $1,299 (U.S.) for education. reply zackymacky 1 hour agorootparentMy mistake! I was looking at M4 iMac pricing. So you still need a monitor on top of the $500 price, but that is a good entry point. reply xiasongh 1 hour agorootparentprevFrom the article > Mac mini with M4 starts at $599 (U.S.) and $499 (U.S.) for education. Additional technical specifications are available at apple.com/mac-mini. reply zackymacky 1 hour agorootparentIt is true, I was on the Apple edu page and looking at the wrong computer. Thanks for the correction! reply tharos47 2 hours agoparentprevIMHO it's not as NUC style mini PCs with x86-64 CPUs from AMD and intel are really cheap and the 256Gb storage is way too small making the \"real\" price $200 higher for any sort of moderate usage. reply throwaway48476 59 minutes agoparentprevIt's only a good deal so long as you don't pay for any of the extortionate upgrades. reply HumblyTossed 1 hour agoparentprevWish I still had a .edu email address... reply ikety 1 hour agorootparentYou don't need one. There's just a checkbox asking you to pinky promise that you actually are a student reply lenerdenator 1 hour agorootparentYou really think someone would do that? Just go on the internet and tell lies? reply jsight 5 minutes agorootparentNoone ever lies or says sarcastic things on the internet. reply timeon 1 hour agoparentprev> With educational pricing I think this easily beats any sort of desktop > go-to recommendation I'll give to anybody wanting an entry-level desktop Can anybody get it with educational pricing? reply ikety 1 hour agorootparentyes, there's no verification system. You simply just state that you are a student reply timeon 46 minutes agorootparentI had no idea. I'm not going to abuse it but still interesting. reply kridsdale3 1 hour agorootparentprevIf you enroll in a college. reply rootusrootus 57 minutes agorootparentOr if you are parent to a student, including when the student is homeschooled. Or if you are a teacher. reply ChumpGPT 1 hour agoparentprev>I think this just became the go-to recommendation I'll give to anybody wanting an entry-level desktop computer of any kind. Perhaps you should check out some Beelink and GMKTec Mini PC Systems. reply rootusrootus 59 minutes agorootparentThen you have to factor in supporting those systems, because you will be the one they call. This is one of the major upsides to family & friends buying Macs. reply bufferoverflow 1 hour agoparentprevnext [18 more] [flagged] laserlight 1 hour agorootparentAre you aware that i9-9900K was launched six years ago [0], two years before Apple Silicon? It's nothing to compare to an Apple M4. [0] https://www.intel.com/content/www/us/en/products/sku/186605/... reply bufferoverflow 14 minutes agorootparentSure, that M4 Pro is a bit faster, but not even twice as fast. It's more energy efficient, that I give to you. https://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i9-9900K... https://www.cpubenchmark.net/cpu.php?cpu=Apple+M4+10+Core&id... reply kridsdale3 1 hour agorootparentprevAnd TFA goes on and on about how the Mac Mini is like 20x faster than i9. reply whynotminot 1 hour agorootparentprev… you see how massive that comp you posted is, right? You see it, don’t you? You’re comparing a used Tower to an Apple TV sized device. reply zemvpferreira 1 hour agorootparentprevIt's not really reasonable to compare prices of a used machine with a new one though. reply echoangle 56 minutes agorootparentWhy not? As long as you discount the price of the new product by the perceived value of new vs used, that’s the correct comparison to make. If a used product is the same quality and $100 cheaper, and just having something that’s new is not worth $100 to you, you should pick the used option. The goal is to get the best value per money spent. reply thejazzman 39 minutes agorootparentmental gymnastics; used is not new. even new isn't necessarily new when it's not sold by an authorized seller because it can invalidate the warranty. new is new and has legal ramifications, you cannot compare them unless you're throwing in a trustworthy extended warranty that matches -- and pretty much nothing matches apple in that regard reply echoangle 34 minutes agorootparentWell you have to consider the differences but you can absolutely compare them when you want to decide what you buy. reply nozzlegear 45 minutes agorootparentprev> I don't think Apple fanboys understand Why start off by calling people fanboys? It seems like you're looking for an argument instead of a genuine conversation. reply supportengineer 1 hour agorootparentprevMicrosoft Windows is a non-starter though. reply kridsdale3 1 hour agorootparentBetter fix your boot loader then. reply bufferoverflow 19 minutes agorootparentprevIt runs Linux or BSD just fine. reply piuantiderp 1 hour agorootparentprevHalf as slow and this is just in general CPU. Not getting into power efficiency, noise, size... etc... etc.... reply thelittleone 1 hour agorootparentprevThat ebay link is a used item. reply reaperducer 1 hour agorootparentprevAnd when something goes wrong, I can just walk into one of the hundreds of eBay, Amazon, or HP stores around the world for free help? I don't think Windows fanboys understand real people care about more than numbers. reply butterlettuce 1 hour agorootparentprevsigh another Windows fanboy pushing inferior hardware for an inferior OS. The lot of us grew out of gaming in our teens and do real work now. reply bufferoverflow 21 minutes agorootparentYou realize you can run Linux or BSD on that hardware? reply jsheard 3 hours agoprev16GB base RAM across the board, following the iMac. AI is certainly good for pushing up the baseline RAM that manufacturers can get away with shipping if nothing else. reply o_m 3 hours agoparentAlso the base ram for the pro chip is 24gb. I hope it will be the same for the MacBook Pro. reply lawlessone 3 hours agoparentprevUnified memory too. It's your GPU and your ram. reply wing-_-nuts 3 hours agorootparentThis is huge for AI / ML at least for inference. Apple chips are among the most efficient out there for that sort of thing, the only downside is the lack of cuda reply burnerthrow008 3 hours agorootparentLack of Cuda is not a problem if for most ML frameworks. For example, in PyTorch you just tell it to use the “mps” (metal performance shaders) device instead of the “cuda” device. reply throwaway314155 13 minutes agorootparentThat simply isn't true in practice. Maybe for inference, but even then you're running up against common CUDA kernels such as FlashAttention which will be far from plug and play with PyTorch. reply ojhughes 2 hours agorootparentprevI tried training some models using tensorflow-metal a year ago and I was quite disappointed. Using a relu activation function led to very poor accuracy [0] and training time was an order of magnitude slower than just using the free tier of Google Colab [0] https://github.com/keras-team/tf-keras/issues/140 reply xattt 2 hours agorootparentprevCuda Apple license it from nVidia? reply rob74 2 hours agorootparentCude, er, cute, but... no. reply whartung 2 hours agorootparentprevSo, do you think that when the Mac Studio gets upgraded, it will also come with less max RAM, but be unified? Is the whole \"unified\" RAM a reason that the iMac and Mini are capped at 32G? reply transitorykris 1 hour agorootparentThe Mac Studio has always had unified memory reply kridsdale3 1 hour agorootparentAll \"Apple Silicon\" products, going back to the first one, which was the iPhone 4. reply rowanG077 2 hours agorootparentprevI consider that a plus. Maybe the AI community wil start to wake up and realize that going all in on cuda is ridiculously stupid. reply kridsdale3 1 hour agorootparentNot if you're an NVDA shareholder! reply talldayo 3 hours agorootparentprev> Apple chips are among the most efficient out there for that sort of thing Not really? Apple is efficient because they ship moderately large GPUs manufactured on TSMC hardware. Their NPU hardware is more or less entirely ignored and their GPUs are using the same shader-based compute that Intel and AMD rely on. It's not efficient because Apple does anything different with their hardware like Nvidia does, it's efficient because they're simply using denser silicon than most opponents. Apple does make efficient chips, but AI is so much of an afterthought that I wouldn't consider them any more efficient than Intel or AMD. reply nextos 3 hours agorootparentFor inference, Apple chips are great due to a high memory bandwidth. Mac Studio is a popular choice in the local Llama community for this particular reason. It's a cost effective option if you need a lot of memory plus a high bandwidth. The downside is poor training performance and Metal being a less polished software stack compared to CUDA. I wonder if a little cluster of Mac Minis is a good option for running concurrent LLM agents, or a single Mac Studio is still preferable? reply angoragoats 2 hours agorootparentThe memory bandwidth on Apple silicon is only sometimes comparable to, and in many cases worse than, that of a GPU. For example, an nVidia RTX 4060 Ti 16GB GPU (not a high-end card by any means) has memory bandwidth of 288GiB/sec, which is more than double that of the M4 CPU. On the higher end, building a machine with 6 to 8 24GB GPUs such as RTX 3090s would be comparable in cost (as well as available memory) to a high-end Mac Studio, and would be at least an order of magnitude faster at inference. Yes, it's going to use an order of magnitude more power as well, but what you probably should care about here is W/token which is in the same ballpark. Apple silicon is a reasonable solution for inference only if you need the most amount of memory possible, you don't care about absolute performance, and you're unwilling to deal with a multi-GPU setup. reply Y-bar 2 hours agorootparentNote that they said the _Mac Studio_ which in the M2 model has between 400GB/s and 800GB/s memory bandwidth. https://www.apple.com/mac-studio/specs/ Edit: since my reply you have edited your comment to mention the Studio, but the fact remains that the M2 Max has at least ~40% greater bandwidth than the number you quoted as an example. reply nextos 2 hours agorootparentExactly, the M2 Ultra is competitive for local inference use cases given the 800 GB/s bandwith and a relatively low cost and energy efficiency. The M4 Pro in the Mini has a bandwidth of 273 GB/s, which is probably less appealing. But I wonder how it'd compare cost-wise and performance-wise, with several Minis in a little cluster, each running a small LLM and exchanging messages. This could be interesting for a local agent architecture. reply angoragoats 2 hours agorootparentSee my sibling reply below, but I disagree with your main point here. M2 Ultra is only competitive for very specific use cases, it does not really cost less than a much higher-performing setup, and if what you care about is true efficiency (meaning, W/token, or how much energy does the computer use to produce a given response), a multi-GPU setup and Mac Studios are on about equal footing. reply reissbaker 2 hours agorootparentprevFor reference comparing to what the big companies use, an H100 has over 3TB/s bandwidth. A nice home lab might be built around 4090s — two years old at this point — which have about 1TB/s. Apple's chips have the advantage of being able to be specced out with tons of RAM, but performance isn't going to be in the same ballpark of even fairly old Nvidia chips. reply angoragoats 2 hours agorootparentprevYeah, sorry, I realized that as well so I edited my post to add a higher end example with multiple 3090s or similar cards. A single 3090 has just under 1TiB/sec of memory bandwidth. One more edit: I'd also like to point out that memory bandwidth is important, but not sufficient for fast inference. My entire point here is that Apple silicon does have high memory bandwidth for sure, but for inference it's very much held back by the relative slowness of the GPU compared with dedicated nVidia/AMD cards. reply int_19h 46 minutes agorootparentIt's still \"fast enough\" for even 120b models in practice, and you don't need to muck around with building a multi-GPU rig (and figuring out how to e.g. cool it properly). It's definitely not what you'd want for your data center, but for home tinkering it has a very clear niche. reply cpuguy83 41 minutes agorootparentprevAnd yet the GPU costs about as much as the whole Mac Mini and wouldn't even come close to fitting inside one. reply alwayslikethis 3 hours agorootparentprevDoes it use the GPU? I was under the impression that it uses the CPU. It's only faster because of the massive memory bandwidth compared to DDR4/5 reply astrange 1 hour agorootparentThe AI features use all three of NPU (\"ANE\"), GPU, CPU, mostly depending on model size. https://machinelearning.apple.com/research/neural-engine-tra... reply DrBenCarson 2 hours agorootparentprevFrankly you’re very wrong. NPUs and GPUs aside, 16gb of GPU memory is very rare in consumer hardware reply astrange 1 hour agorootparentYou can't use all 16GB because it's unified, so it's shared with the system, SSD controller etc. You can use something like 12-14GB though. reply DrBenCarson 19 minutes agorootparentSure, still incredibly rare in a $600 device reply bitwize 1 hour agorootparentprevThe on-chip RAM means that you can run models on the CPU that would require the GPU on a peecee. reply TiredOfLife 3 hours agorootparentprevYeah the same tech pcs were using for 14+ years https://x.com/LinaAsahi/status/1820947147312820497 reply acchow 2 hours agorootparent> I know ancient iGPUs had that thing for setting the GPU memory size in the BIOS, but that's aaaaaancient and completely obsolete. If you still have that, just set it to the minimum value. The rest of memory will be unified. I hadn’t used a PC in so long, I still thought that bios setting decided the division. TIL. Lucky we have Asahi Lina to clarify the details. reply abhinavk 1 hour agorootparentIt shows up as \"Shared GPU memory\" in Task Manager. What BIOS sets is the Dedicated i.e. reserved video memory in RAM. e.g. My Ryzen iGPU reserves 2GB/32GB for itself (which Windows can't see) via BIOS and use 9 more as shared \"unified\" memory. reply mentos 3 hours agoparentprevNo mention of the SSD size that was the reason I returned my Mac mini 256Gb last year was just a pain juggling files reply nordsieck 3 hours agorootparent> No mention of the SSD size The base model is 256 gb. You can see it here: https://www.apple.com/shop/buy-mac/mac-mini reply wongarsu 3 hours agorootparentprevBase model is 256GB configurable up to 2TB, all others start at 512GB, the M4 Pro model can go up to 8TB reply rtkwe 3 hours agorootparentIt's $800 to go up to 2TB from the 256GB model which is just criminally over priced. I can get double that for half the price with a Gen 4 NVMe drive. Weirdly the 8TB drive on the Pro is at least in line with the top of the line 8TB NVMe SSDs you can buy though there are cheaper options at about $600 vs the $1200 Apple is charging. reply wongarsu 3 hours agorootparentThat's regular Apple pricing for you. Great deals on the baseline models, but insane margins on the upgrades that make them usable. And of course the ability to upgrade the devices yourself has been phased out in the name of performance and power efficiency reply wtallis 3 hours agorootparentI just checked some Dell prices: $730 to upgrade an XPS desktop from 512GB to 4TB (Apple charges $1200), or $508 to upgrade an Optiplex tower from 256GB to 2TB QLC, or $654 to upgrade it from 256GB to 2TB TLC (Apple charges $800). Scalping on upgrade pricing is something all the PC OEMs do. reply jsheard 3 hours agorootparentYeah, but the 5 minute job of installing a cheap retail SSD in that Dell machine yourself is still an option which Apple has removed from all but the Mac Pro, which offsets any SSD savings by being $3000 more expensive than an equivalently specced Mac Studio. reply wtallis 3 hours agorootparentRight. The repairability argument is the reasonable discussion to have. The silly pricing games are more of a red herring. reply rtkwe 2 hours agorootparentI think it's more relevant with Apple because they've removed all the competition for basically all upgrades to their devices by either soldering things to the board or bundling them into their SOC. When there's no alternative their prices become the only option. reply wtallis 1 hour agorootparentI think repairable or not, the pricing to upgrade to the max config just isn't something a price-sensitive consumer should take seriously. Beyond one or two upgrades, you might as well pretend it says \"call for quote\" and just not consider 4+ TB as a realistic option to get from the OEM, because those prices are trying to cause sticker shock. And that goes for any PC OEM—the price-gouged upgrades are so far beyond reasonable that it really doesn't matter whose prices are the most silly or by exactly how many hundreds of dollars. What does matter is whether aftermarket upgrades and repairs are possible. reply rtkwe 48 minutes agorootparentIt matters that there are no after market options for Apple because it means the inflated OEM upgrade price is the ONLY price available for every given upgrade. It matters less with Dell/Asus/Lenovo etc. because that's not the only price available. The top of the line is also not where Apple is gouging the worst. It's in the middle tiers that are actually relevant to many more people. Most don't have a need for 4+ TB main drives but 1-2 TB is a size that's pretty easy to justify for a lot of people and Apple's price is the only option for them and they're absolutely lining their pockets with cash at the expense of anyone not going for the bargain bin basic tier that can't hold 2 modern games. reply jsheard 2 hours agorootparentprevThe Mac Studio technically still has socketed SSDs, which presumably cuts costs by not having to manage a separate motherboard SKU for every SSD capacity, but they went out of their way to design a proprietary SSD module format rather than just using the standard... reply rtkwe 1 hour agorootparentKind of, they were socketed NAND cards and the controller lived on the mainboard, so as far as getting out of the problem of Apple entirely setting the prices for everything it's not relevant. As far as I'm finding no one managed to find a way to create a compatible card to create an avenue for DIY upgrades. I've found a few upgrades but they consist of buying an entire second Mac Studio to harvest the drive from. reply throwaway48476 48 minutes agorootparentGiles from Polysoft is manufacturing 3rd party mac studio nand cards. There is still a problem sourcing nand because apple doesn't let the owm sell to anyone but them. reply rtkwe 39 minutes agorootparentI thought I had seen something about that but couldn't find the actual boards mentioned for sale. Sounds even worse though because it should be possible but Apple being Apple has ensured there's no source for Giles or other companies to perform repairs. reply throwaway48476 32 minutes agorootparentThere's an industry in China for desoldering and reusing apple bands. Unfortunately getting new oem nands is going to take legislation. rtkwe 25 minutes agorootparentYes, though that relies on the product being fairly popular and for the chip to be stable for a while for it to be a useful source. Mac Studio NAND chips aren't going to be readily available from them unless they happen to be a shared part from a more popular device. sangnoir 1 hour agorootparentprev\"Repairability\" is a red-herring when the discussion is about user-upgrades and the ability to purchase components from 3rd-party suppliers (who compete against each other and the OEM) reply throwaway48476 51 minutes agorootparentprevPhased out in the name of profit efficiency. Despite what marketing will tell you the SSD is industry standard NVMe and the RAM is standard LPDDR5. reply matheusmoreira 1 hour agorootparentprevApple's silicon is good but I don't see what's so special about all the other stuff. Looks like they just solder components to the motherboard instead of using industry standard interfaces. reply throwaway48476 47 minutes agorootparent*industry standard physical interfaces. The electrical interace is bog standard. reply Y_Y 3 hours agorootparentprevin the name of [share price] performance and [market] power efficiency reply pantulis 3 hours agorootparentprevEdit: sorry, answered to the wrong post. reply angoragoats 2 hours agorootparentprevAt least on the storage side, Apple's parts are neither more performant nor all that much more power-efficient than a standard, replaceable SSD. reply angoragoats 2 hours agorootparentprev> I can get double that for half the price with a Gen 4 NVMe drive It's worse than that -- 4TB gen 4 drives can be had for well under $300, sometimes $225-250, and that's for buying a drive outright, not \"trading up\" from a 256GB device. I think it'd be more accurate to say that you can get double the capacity for a _quarter_ of the price. reply rtkwe 43 minutes agorootparentI was ballparking it based on my recent buy of a Samsung 990 Pro 4TB and inflated the price a little in my head to closer to $400 than the $330 it actually was. I also, as a side note, try to give the loosest most favorable (to my opposite) comparison because when I err on my side it becomes a \"well actually\" debate a lot of the time about how it's \"not quite X times as many it's more like X-1 (so I'm not even going to touch that X-1 is still quite bad)\" that is really tedious and annoying especially when the favorable version of the comparison is still quite bad for their point/side. reply sib 2 hours agorootparentprevIt's USD2,400 to upgrade the M4 Pro model from 512GB to 8TB, which feels a bit steep, but it's an option. Alternatively, you can get one of these[1] external Other World Computing NVME SSDs for USD1,190 right now. And then you can easily move all your files from your laptop to your desktop when you get home. [1] https://eshop.macsales.com/item/OWC/US4EXP1MT08/ (15% off list price as of writing) reply jillesvangurp 2 hours agorootparentprevYou can get some decent size external usb SSDs. I have the Samsung T5 2TB. I think they have larger models now. Works pretty well. And with USB-C speeds are very usable. You can probably get faster/bigger stuff via thunderbolt. I'm considering getting one and a nice big monitor or TV. It needs to run x-plane 12 at decent speeds and maybe support a bit of light gaming. My macbook M1 pro is actually pretty decent for this but the screen is too small for me to easily read the instruments. I expect this will do better even in the base setup. Otherwise my needs are pretty modest. I'd love to see steam add some emulation support for these things as I have some older games that I enjoy playing. I currently play those on a crappy old intel laptop running linux. I've also been eyeing a new AMD mini PC with the latest amd stuff (Beelink's SER9). Seems pretty nice as well and seems like it is more performance for the money. Apple is doing its usual thing of charging you hundreds of euros for 50 euro upgrades. Get the base mac studio instead. It probably makes more sense if you are going down that path. reply msh 1 hour agorootparentThe big problem is that there are lots of stuff that macOS won’t let you move to a external drive, like iCloud Drive. reply GeekyBear 3 hours agorootparentprevYou do have the option of a 10 gigabit Ethernet port, so you can build out a linux box for local shared storage with components as cheap as you're willing to trust. reply mmaunder 2 hours agorootparentThat’s useful. The TB3 external 10 gig interfaces I’ve been using for my Mac get crazy hot. reply kridsdale3 1 hour agorootparentAs someone who just made a 16tb SSD array over Thunderbolt 3 (Best I could find) at 40gbps and the interface is still the bottleneck (disks are fast now!), 10gbps is going to feel really really slow vs the internal stuff. reply GeekyBear 6 minutes agorootparentIt's possible to build a faster non-shared array if you aren't price sensitive, but someone with multiple computers and devices gets much better bang for the buck from shared local network storage. As a bonus, you can back up your computers and iDevices to the shared local storage instead of paying for (probably much slower to access) cloud storage. reply TacticalCoder 1 hour agorootparentprev> ... 10gbps is going to feel really really slow vs the internal stuff How do you love your internet speed compared to the internal stuff? reply syndicatedjelly 3 hours agorootparentprevYou might be interested in this then - https://satechi.net/collections/ssd-enclosures Upgrade your memory and connect it externally over USB-C. It works brilliantly reply mitjam 2 hours agorootparentYes I have a Samsung T7 works like a charm. reply magnio 3 hours agorootparentprevCouldn't we just add extra drives into the extra internal SSD slots? Or does Mac Mini not have those? reply jsheard 3 hours agorootparentThere are no slots, it's all soldered directly to the motherboard. Even in the Mac Studio, which does use modular SSDs, they're proprietary modules rather than anything you can easily swap out yourself. reply mathnmusic 3 hours agorootparentprevAFAIK, Apple took away those slots when Mac minis transitioned to Apple silicon. Attempts to replace SSD with re-soldering have not been successful. reply baq 2 hours agorootparentSo not even milling off the ssd works now? Fortunately I don't really see the point of using a mac mini, so this doesn't bother me too much, but... it's poor taste. You're holding it wrong was not cool the first time. reply dagmx 2 hours agorootparentYou can mill them off and replace them with supported nands. People have videos on YouTube but it’s not very accessible to do. The issue is that Apple moved the storage controllers into their SOC. So they use raw nand chips, and you need to use ones that the SOC supports. reply selimnairb 3 hours agorootparentprevIt's a desktop. Use an external disk. Much cheaper. reply jsheard 3 hours agorootparentIt kind of undermines the sleek form factor if you need to have a clunky NVMe enclosure dangling off the back though. Even with this tiny new design I bet they could fit a hatch on the bottom with space for a 2230 M.2 drive, but they don't want to because that would let you upgrade to 2TB of fast internal storage for $200 instead of $800. reply crest 3 hours agorootparentUntil someone brings out little two or four drive NVMe enclosures that fits exactly under the Mac Mini with a Thunderbolt bridge/plug that doesn't snag cables, because we all know Apple can't resist gauging buyers by refusing to include two easy to access M.2 bays on the underside. I can't imagine anyone but Apple shareholders drooling at the taught of overpriced soldered memory would prefer a smaller Mac Mini case if ~0.5\" more height would get you M.2 bays for storage. reply selimnairb 1 hour agorootparentYou mean like this [1]? I would shocked if OWC didn’t have a version of this for the new mini form factor in the works. [1] https://eshop.macsales.com/shop/external-drives/owc-ministac... reply kridsdale3 1 hour agorootparentIt even looks like the standard iconography for \"Database\"! reply 93po 2 hours agorootparentprevthese exist on amazon reply nordsieck 3 hours agorootparentprevThere are a number of companies that specialize in making hubs/NVMe enclosures that match the aesthetics of the Mac Mini and sit directly underneath it. For example: https://www.amazon.com/dp/B08S47KBMC/ reply awiesenhofer 3 hours agorootparentprev> clunky NVMe enclosure We really are living in the future if people are using these words in combination. Though compared to this new mini a lot will feel clunky. Any HDD enclosure is certainly larger. reply burnerthrow008 2 hours agorootparentIt’s not the size of the NVMe enclosure that makes it clunky. It’s that you now have an extra dongle hanging off the back of your Mac and cluttering up the desk. There is a reason for the popularity of those enclosure/hub combos that have the same footprint and color as the Mini. reply tonyedgecombe 1 hour agoparentprevThis is good news for me because I usually buy the base machine and accept its performance as a constraint on what I'm doing. I'm not sure it is all about AI though, Apple has been getting a lot of criticism for selling machines with just 8GB of RAM. reply superjan 2 hours agoparentprevThe RAM is expandable as well… however I am curious how well the extra RAM performs. Part of the M-series performance gain is from having the RAM dies very close to the processor. reply throwaway48476 43 minutes agorootparentIt performs the same. You don't get any extra ram channels for upgrading the capacity. reply jeffbee 3 hours agoparentprevI have a feeling this could simply be an outcome of samsung not offering anything smaller. reply jsheard 3 hours agorootparentThe M4 iPad Pro still starts at 8GB though, so Samsung is supplying them with lower capacity modules. reply yunohn 3 hours agoparentprevI learned yesterday that all M-chip Macs with enough RAM are getting Apple Intelligence? This basically proves that Apple shot themselves in the foot for AI on mobile by artificially restricting RAM for so long! Heck, even the Neural Engine has turned out to be basically useless despite all their grandstanding. So alas, their prior greed has resulted in their most popular consumer iDevices being the least AI compatible devices in their lineup. They could’ve leapfrogged every other manufacturer with the largest AI compatible device userbase. reply captainbland 2 hours agorootparentSounds like they've just done planned obsolescence faster to their lower paying customers. reply medell 2 hours agorootparentprevThis might actually push people to upgrade their hardware. And Apple retention rates are high. Apple will be fine. reply mostlysimilar 3 hours agorootparentprevI think it's great that Apple was able to ship devices that millions of people made happy use of without needing to put additional hardware resources into them. That's efficiency, not greed. reply yunohn 3 hours agorootparentI own almost every Apple ecosystem device, but I definitely wouldn’t call their mobile device RAM capacity as sufficient. It physically hurts me when my iPad Pro M2 and iPhone 16 Pro Max (earlier 15,14,13,12,11) start to swap out live apps - sure some apps retain state, but the majority still don’t. Even Safari randomly reloads tabs for me, while I’m just researching purchases acrossThis basically proves that Apple shot themselves in the foot for AI on mobile by artificially restricting RAM for so long! What they shot was us. My 14 Pro won’t do AI despite having a better NPU than an M1, all because Apple chose - intentionally - to ship it with too little RAM. They knew AI was coming and they did this anyway. Although having played with it on my MBP it’s clear I’m not missing much. But still. reply DrBenCarson 2 hours agorootparentThey knew they were releasing Apple Intelligence before ChatGPT went live? lol reply TiredOfLife 2 hours agorootparentprevBut now they get to sell new devices to them reply alberth 3 hours agoparentprevIt's not just RAM. It's Unified RAM. So that memory is also used for the GPU & Neural Cores (which is for Apple Intelligence). This is actually why companies moved away from the unified memory arch decades ago. It'll be interesting to see as AI continues to advance, if Apple is forced to depart from their unified memory architecture due to growing GPU memory needs. reply runjake 3 hours agorootparentIf it's the shift I think you're referring to, I find it strange that you compare computing decisions from the 50s and 60s to today. You're correct, but that was over half a century ago. The reasons for those decisions, such as bus speeds, high latency, and low bandwidth, no longer apply. Today, the industry is moving toward unified memory. This trend includes not only Apple but also Intel, AMD with their APUs, and Qualcomm. Pretty much everyone. To me, the benefits are clear: - Reduced copying of large amounts of data between memory pools. - Improved memory usage. - Generally lower power consumption. reply yodon 3 hours agorootparentprev>This is actually why companies moved away from the unified memory arch decades ago. I don't understand - wouldn't the OS be able to do a better job of dynamically allocating memory between say GPU and CPU in real time based on instantaneous need as opposed to the buyer doing it one time while purchasing their machine? Apparently not, but I'm not sure what I'm missing. reply burnerthrow008 2 hours agorootparentI disagree that unified memory is a bad thing. The usual reasoning that people give for it being bad is: you share memory bandwidth between CPU and GPU, and many things are starved for memory access. Apple’s approach is to stack the memory dies on top of the processor dies and connect them with a stupid-wide bus so that everything has enough bandwidth. reply pohl 3 hours agorootparentprevDepart? They just got there, didn't they? And on purpose. There's more memory bandwidth, and also no need to copy from main memory to VRAM. Why would they bail on it? reply ErneX 2 hours agorootparentprevI think they moved away because system memory was lagging behind in speed to the memory being used on video cards? And besides, what Apple is doing is placing the RAM really close to the SoC, I think they are on the same package even, that was not the case on the PC either AFAIK? reply JKCalhoun 3 hours agorootparentprevAt this point it feels like (correct me if I'm am wrong) that Apple's AI is often performed \"in the cloud\". I suspect though that if Apple moves increasingly to on-device AI (as I suspect they will — if not for bandwidth and backend resource reasons then for privacy ones) Apple's Silicon will have adopted more and more specialized AI components — perhaps diminishing the need for use of off-board memory. reply Etheryte 3 hours agorootparentLast I checked, Apple was pretty much the only major player who does everything that they can do on device on device, that is their whole ethos behind it, no? reply bee_rider 2 hours agorootparentIt is possible that they do everything they can on the decide, but still have to do lots in the cloud, right? For some definition of lots, at least… reply Etheryte 1 hour agorootparentI mean, there is no need to speculate about any of this, they've put out a number of articles that outline their whole approach. I'm not really sure where the ambiguity lies? reply throwaway48476 40 minutes agorootparentThey have a new bug bountry program for their confidential compute platform... in the cloud. reply aldarisbm 3 hours agorootparentprevYeah they have literature about this, they do as much as they can on device reply ErneX 2 hours agorootparentprevIt's always local 1st and remote for certain things, and I think it warns your before going to the cloud IIRC. reply leetharris 3 hours agoprevLove to see that it still starts at $599. My M2 Mac Mini that I got for $499 is my favorite gaming computer I've had in a long time. Runs many games like WoW, Dota, League of Legends, etc great. Anything that it doesn't run due to MacOS I use GeForce Now over ethernet. And this was with 8gb unified memory, now with 16gb it'll be even better value. Very excited to see how the GPU has improved in the M4, especially the Pro model. reply eamag 3 hours agoparentIsn't steamdeck a better option for this use case? reply leetharris 3 hours agorootparentI have a Steam Deck, Asus ROG Ally, M2 Mac Mini, M1 Pro laptop, M2 Max laptop (work). All of this runs on either an LG C3 42\" OLED or a 34\" 1440p ultrawide. Linux GeForce Now can only do 720p or 1080p, can't remember which. Also, it's just kind of laggy in desktop mode. The Macs run so much smoother. My current \"main\" desktop is actually my Asus ROG Ally. I use one USB C hub that is capable of 4k120hz, and I can move it between my Mac laptops and Asus ROG Ally very seamlessly. The problem for me is Windows. Yesterday my start menu stopped loading for some reason and required a full reboot. Sometimes it refuses to go to sleep. Sometimes it refuses to come out of sleep. Sometimes a Windows update kicks off in the middle of a game and it slows everything to a crawl. Windows drives me crazy these days! reply diggan 3 hours agorootparent> Windows drives me crazy these days! At least it boots. I purchase a Surface Pro 8 a year ago or something, thinking Windows would surely work better than usual when it is Microsoft's own hardware too. But no, yesterday it got stuck in a boot loop, after a Windows update broke the audio drivers somehow. The Windows logs/reliability report can just tell me it \"shut down abnormally\" without any technical details what so ever. I still have to use Windows on my desktop because of Ableton, but I'll never purchase any Microsoft hardware again, and as soon as I can, I'll run Ableton on Linux like the rest of my software. reply insane_dreamer 2 hours agorootparentprev> The problem for me is Windows. Perennial truth since XP reply influx 2 hours agorootparentprevWhat's the model of that USB C hub? Looking for one that I can switch between Windows and Mac (and maybe Linux) reply leetharris 36 minutes agorootparentThese Cable Matters ones work well, but need their firmware flashed. Luckily, it was extremely easy and worked great for me. Here's an Apple forum thread about it: https://forums.macrumors.com/threads/dp-usb-c-thunderbolt-3-... Here's the exact model I have: https://www.amazon.com/Cable-Matters-Ethernet-Delivery-Charg... reply jacktheturtle 2 hours agorootparentprevcan you share the USB C hub? i need one of these and can't find one that makes it easy to swap between mac & gaming rig reply leetharris 36 minutes agorootparentPosted in another comment: These Cable Matters ones work well, but need their firmware flashed. Luckily, it was extremely easy and worked great for me. Here's an Apple forum thread about it: https://forums.macrumors.com/threads/dp-usb-c-thunderbolt-3-... Here's the exact model I have: https://www.amazon.com/Cable-Matters-Ethernet-Delivery-Charg... reply talldayo 3 hours agorootparentprevGet rid of Windows on it! Digital Foundry put out a great video on this exact process the other day, weighing the pros and cons: https://www.youtube.com/watch?v=OwWRCrGoXV0 Neither MacOS nor Windows are very good console OSes - you're really better off using Linux where anticheat isn't concerned. Even on the Ally. reply bee_rider 2 hours agorootparentThey listed WoW, DotA and League of Legends—I’m guessing, but the last two seem likely to have anti-cheat issues, right? reply diggan 3 hours agorootparentprev> Neither MacOS nor Windows are very good console OSes They're great OSes for consumers who don't really work on their computers, and just want something that caters to the lowest common denominator. For professionals who use computers for work, Linux is really the only option that doesn't eventually get in your way. You can set it up and leave it as-is, with only security updates, and everything keeps working the same way, basically forever. I've tried to set up an experience like that on both macOS and Windows, but eventually, the company will find a way of forcing an update on your, intentionally or not. reply zamalek 3 hours agorootparentThis isn't really the case anymore, Linux (specifically SteamOS and its kin) serve the console-like market very well. Arguably better than Windows. Even for non-Gaming use cases this idea is a bit dated. Printing is by far the best experience on Linux. The \"tweaking\" that you need to do, that every Windows/MacOS user claims, isn't really a thing these days - sans NVIDIA (I'm not sure what the current status is, but it was bjorked somewhat recently). Sure, if you want to go beyond what Windows/MacOS can offer then tweaking my be required, but the current UIs are extremely comprehensive. I had a 80yr old lady up and running in one day with PopOS. If that's not lowest common denominator, I don't know what is. Professional work can be hit and miss. Depends on how draconian your workplace software is. reply philistine 3 hours agorootparentprevThank you so much for telling me that I am actually not managing my non-profit on my Mac. I was convinced I was working every day on a Mac in a business setting, but I guess I was mistaken. reply 1propionyl 3 hours agorootparentprev> For professionals who use computers for work, Linux is really the only option that doesn't eventually get in your way. I really hope you're not expecting anyone to take you seriously with this. On principle I get what you're saying but in practice no one who works as a professional in any field has the time (or expertise) to be worried about configuring their operating system. As a Linux evangelist who begrudgingly daily drives a Mac, this kind of attitude is what does us in. It's the cocksure \"akshually Linux is best\" even when it materially, experientially, just isn't. Denial is not a design ethos. reply skydhash 2 hours agorootparentMost professionals use only a handful of software and don't really care about the OS other than what the OS should do (file management, connecting to the internet, launching software,...). Apple and Microsoft insists on doing other stuff that impedes you while not allowing you to do basic stuff you want. The main issue with Linux is hardware support (which no one other than the manufactures can solve properly) and professional development (The distributions are great, but monolithic development like FreeBSD would have been better). Linux is best because it lets you use your computer for whatever workflow you need. reply matheusmoreira 19 minutes agorootparentprevI'm a professional who is often forced to suffer Windows nonsense. At work Windows routinely wastes my time with absolute bullshit I couldn't care less about and which makes me negative dollars, even though it is basically a glorified Chrome launcher. Professionals should absolutely take it seriously because time spent updating Windows or even just waiting around while it gets its shit together is time you could have spent doing your job and making money. In fact, Windows and its spontaneous updates with obnoxious focus stealing prompts are major risks to the integrity of your work and might cause you to have to redo it from scratch, lowering the value of your time even further. Linux boots in less than ten seconds and is already ready to use. There are distributions for all levels of expertise, and if there's an IT department it should be managing those boxes anyway. All that's missing is the Microsoft Office suite and in the end that's what the Windows vs Linux battle always boils down to. People put up with it because they just need muh Excel. reply talldayo 3 hours agorootparentprevI'll take him seriously on it. MacOS and Windows are terrible for professional purposes, for a number of reasons: 1. Requires Windows Pro or Apple Developer license to unlock full featureset 2. Cannot reasonably disable targeted advertising or ad data collection from either OS 3. Neither come with package managers and do not respect third-party packaging either 4. Can be \"managed\" insofar as your buggy CPM software allows, often glitched by the OS itself 5. The experience is always getting worse since Apple and Microsoft share a united front of making people spend as much money on useless shit as humanly possible Now, that's not to say nobody should use these OSes - certainly people are locked into them for some purposes. But as a programmer it's genuinely hard for me to be productive on these OSes because I end up fighting them just for everyday, non-programming purposes. I think it's entirely possible that MacOS and Windows can be inherently terrible experiences while also being mandatory for certain workflows. reply Veen 3 hours agorootparentprevWhat does \"work on their computer\" mean to you? I suspect it's not what it means to the vast majority of people. reply TiredOfLife 2 hours agorootparentprevOn SteamDeck Geforce Now is limited to 1080p, due to NVIDIA being a tiny indie company with no resources to make a native client. reply davely 2 hours agorootparentThis is, of course, a non issue as the resolution of the Steam Deck is 1280x800. Edit: I am silly. Of course, people mean hooking it up to a bigger screen. reply tom_ 3 hours agorootparentprevAssuming you don't want to use it for all the various other things you can also use a Mac for - sure, maybe. reply insane_dreamer 2 hours agoparentprev> Anything that it doesn't run due to MacOS I use GeForce Now over ethernet. Can you elaborate? Thinking of setting up a MacMini for my kids but worried about lack of gaming options for them (I haven't gamed on a Mac in a dozen years and the state of gaming on MacOS was sad back then). reply leetharris 23 minutes agorootparentThere's a lot of Mac games on Steam, Apple Arcade, and Battle.net these days. Anything that isn't supported there, I generally use Xbox streaming or GeForce Now streaming. Here's a list of my most played games on my Mac in the last couple of years: WoW, Hearthstone, Dota 2, League of Legends, Thronefall, Vampire Survivors, Baldur's Gate 3, Cult of the Lamb, Balatro, Death Must Die, Terraria, Dave the Diver, Mechabellum, Space Haven, Hades 2, Peglin, Stellaris, RimWorld, Dead Cells, Total War: Warhammer 2, Valheim, Civilization 6, Slay the Spire, Don't Starve Together, Cities: Skylines, Oxygen Not Included, SUPERHOT. Games I play through GeForce Now: Fortnite, Diablo 4, WoW, Apex Legends, Halo Infinite, Baldur's Gate 3, Cyberpunk 2077 The point of such an annoying long comment is to demonstrate that there is a very substantial Mac gaming library. The problem is that a new shiny game comes out that doesn't support Mac and you don't want to be the ONE guy in your group who can't play it because you're on Mac. The latest one for me is Deadlock. Not on GeForce Now, not on console, not on Mac... so I needed to get a Windows PC. But if you're a kid and just looking for a general gaming machine, it plays a ton of cool stuff. reply tacoooooooo 2 hours agorootparentprevGeForce Now https://www.nvidia.com/en-us/geforce-now/ nvidias cloud gaming offering. it works pretty well reply theshrike79 2 hours agorootparentprevGeforce Now is pretty much the only still viable game streaming service, you can run it on pretty much anything. The free tier is mostly crap (you only get to play if no paid users are using the capacity pretty much), but the paid tiers go from good to excellent. Its main selling point is that you don't need to buy games for it separately, you can use your existing Steam catalog for example. reply ojhughes 2 hours agorootparentprevXbox cloud gaming also works great over ethernet on Mac reply JKCalhoun 3 hours agoparentprevMy Intel Mac Mini is still my \"tv content\" machine. Since it has no problem driving my Samsung OLED TV and keeping up with typical video framerates I suspect I will be holding on to it for many more years to come. reply npsomaratna 3 hours agoparentprevAre these games available on OSX? Or are you somehow booting Windows? (Apologies if this seems like a stupid question. I've not played games for a very long time, mainly because most stuff doesn't seem to be available on Macs). reply leetharris 3 hours agorootparentIt's not a dumb question. I actually used to use an iMac 27\" with an Nvidia 680 that I would boot into Bootcamp / Windows for my primary gaming computer. I covered it in \"built, not bought\" stickers at Quakecon one year. You can't do x86/x64 Windows on M-series Macs without emulation and it is generally a poor experience. There's a few things like Crossover, Parallels, etc that can help you run Windows games. But I have found that most of the games I care about are either Mac native or on GeForce Now at this point. There's a surprisingly large game catalog on Mac now. So the short answer is that some of them run on some sort of Windows compatibility layer, some are Mac native, some I stream. But most of my favorites run native on Mac. To be honest, there are so many games to play these days that I don't mind missing out on a few titles. Valorant is a good example of a game that I can't play on Mac, GFN, or Crossover. But it's OK, I still have CS2. reply addandsubtract 2 hours agorootparentWhisky also does a great job at running Windows games with minimal setup / overhead. reply Apocryphon 2 hours agorootparentprevI'm wondering if it might actually be easier to install Asahi Linux (or some other distro) on Apple Silicon for gaming via Proton, until Game Porting Toolkit is adopted more. reply smileybarry 3 hours agorootparentprevYes — World of Warcraft, League of Legends and DotA 2 all have native macOS ports. WoW got an Apple Silicon port relatively recently IIRC (last expansion). reply sammyeatworld 3 hours agorootparentWoW was probably the first game that was ported to the ARM architecture (2020), so it's not that recent. reply fckgw 3 hours agorootparentprevAll those games listed have native Apple Silicon Mac ports now. reply int_19h 27 minutes agorootparentIn my experience with gaming on Macs, even when there is a native Mac port of a particular game, the experience is inferior to Windows more often than not. Many of them don't do 4K properly, for example (you get everything rendered at half-res in fullscreen). Things like Cmd+Tab don't work reliably, either. reply sammyeatworld 3 hours agorootparentprevOnly WoW, LoL and Dota are being translated through the Rosetta. reply grahamj 2 hours agoparentprevIt’s smaller and has their own chip so I should hope it’s no more expensive. reply etempleton 3 hours agoprevWhat a great little computer at a very reasonable price. A few interesting things with this announcement: 1. Interesting that they did not have this as part of an event. I think this either means they do not have much else to share around the Mac right now or the opposite, there just won't be room to talk about the iMac or Mac Mini. I am leaning towards the former as a I suspect the other computers in their lineup will just receive a spec bump soon. 2. On the product page (https://www.apple.com/mac-mini/) Apple highlights a number of third party accessories. Notably the PS5 controller and several keyboards and mice from different manufacturers. This seems small, but it would have been almost blasphemy under the jobs era. 3. This is quite the little powerhouse. Honestly it is so good it eliminates the need for most people to even consider the Mac Studio. reply makeitdouble 3 hours agoparent> This seems small, but it would have been almost blasphemy under the jobs era. I feel like Jobs was a lot more pragmatic than we give him credit for. I mean we had the HP iPods, iTunes on Windows etc. And the iMac's catch copy was \"BYODKM\" at the very start, fully putting the spotlight on third parties and composability. reply ethagknight 2 hours agorootparentI've followed Apple all my life and never heard of HP iPods! reply etempleton 2 hours agorootparentIt was a weird win-win for Apple if I remember correctly. They were able to sell more iPods, get iTunes installed on all HP machines, and block HP from creating a rival music player. I honestly am not sure what HP got beyond the logo on the back of some iPods and the ability to try and associate themselves with a popular and cool product. reply sgerenser 1 hour agorootparentI think HP in that era was definitely trying to be more \"popular and cool.\" Not sure how much it really helped though. reply throwaway48476 38 minutes agorootparentHP of the era was falling apart and making stupid acquisitions. reply hbn 2 hours agoparentprevApple has shown people gaming with PS5 controllers at events for I believe a few years now. Someone can fact check me on that but it's not the first time I've seen it. reply detourdog 3 hours agoparentprevThis whole week will see a Mac annoucement according to what this Greg Joswiak said last week. https://www.apple.com/leadership/greg-joswiak/ reply ErneX 3 hours agoparentpreviMac was yesterday, Mini today and MacBook Pro tomorrow. Mac Studio and Mac Pro are getting upgrades next year apparently. reply alberth 3 hours agoprevDoes anyone know how many P vs E cores? 10 core = 4 P and 6 E 12 core = 8 P and 4 EWith up to 14 cores, including 10 performance cores and four efficiency cores They've backtracked from the M3 Pro P:E ratio downgrade, which is a welcome surprise. reply korhojoa 3 hours agoparentprev10 = 4P 6E 12 = 8P 4E 14 doesn't seem to be listed at the moment reply ksec 2 hours agoprevAs far as I am aware, there isn't a single competitor from big brand manufacture at $599 price point regardless of size. M4, 16GB RAM, Thunderbolt 4. The SSD is the main failing point but with TB4 you can easily get an external SSD. You can also get 10Gbps for extra $100. With EDU or Staff pricing this thing stars at $499. Which is practically a steal. I am thinking it may be better for cooperate to buy this and run Windows on VM than buying a PC. Considering iPad and iPhone has been replacing 99% of my workflow outside of office I am thinking if my next computer could be a mini rather than a Laptop. reply xenospn 1 hour agoparentI’m always confused as to why people are so paranoid about storage size. I got the base MacBook Air and an external 2TB drive for cheap. Super fast and I never worry about anything - I didn’t even manage to get up to 50% of my 256GB drive. reply birdgoose 55 minutes agorootparentI agree with your sentiment but I feel like many people just don't like the idea of carrying around dongles/cables/hdds/etc with their laptops. reply greenpresident 50 minutes agorootparentprevThere is a generation of tech users that downloaded TB of media for local storage. It’s just not something a lot of people do anymore but it created a psychological need, even if it’s not a technical necessity. reply PaulRobinson 3 hours agoprevApart from the huge price jump from M4 to M4 Pro, I really like this product line-up. Last time I bought a Mac Mini was before the 2018 model got introduced, and I almost took it back in to get it exchanged (I was within 30 days of purchase when the 2018 model dropped), but it's been plugging away doing everything I have asked of it for 6 years, and it's still going strong. All the upgrades since have left me a little cool, but this genuinely looks like a contender for an upgrade. Only thing stopping me from getting the credit card ready is waiting to see what the M4 MacBook Air - which is inevitably going to be announced in the next 72 hours - looks like in comparison. reply samcat116 3 hours agoparentM4 MacBook Air is rumored for the spring. Last announcement tomorrow is rumored to be the MacBook Pros. reply adastra22 2 hours agorootparentThey never released in that order, no? That would mean perfecting the highly integrated M4 Pro and Max before releasing a regular M4 laptop. reply samcat116 2 hours agorootparentThere will be a regular M4 MacBook Pro I assume, just like how there’s a regular M3 MacBook Pro now. They did this same release order last year (Pros in the fall and Air this spring) reply PaulRobinson 3 hours agorootparentprevSeems weird to stagger it like that, unless they have a huge amount of M3 Air stock they're hoping to shift over the holidays. reply samcat116 3 hours agorootparentThe M3 airs came out in March of this year, so it’s a bit soon. reply vvvvvvvvvvvvv 2 hours agorootparentprevThe release cycle is the same every year, aside from occasional refresh omissions and delays. It would be weird if they actually did release the M4 Air tomorrow. reply bilsbie 2 minutes agoprevWould this make a good gaming computer? reply kissiel 3 hours agoprevM4 pro comes with Thunderbolt 5, which means one cable to run 2x 2160p120. And in case of macbooks equipped with TB5, one cable to do 2x high res, high refresh displays + power + plenty of bandwdith for data accessories. Omnomnom. reply donatj 3 hours agoprevIt would appear the air intake is on the bottom like the Mac Studio. As someone who lives in a very dusty 150 year old house, My Mac Studio does not appreciate the air input being directly on the desk. It collects all the dust that lands anywhere near it. I have a large levoit air filter running 24/7 in my office and still end up with this[1] regularly. I wish I could at least reasonably take the thing apart to clean it out. 1. https://imgur.com/a/GSubONa reply Someone1234 3 hours agoparentThree points: - Running an Air Filter 24/7 has huge diminishing returns (i.e. waste of electricity). They are best run at max fan speed for short durations instead. - Elevate it with a platform. - Get a vacuum (or even a robo vacuum). I grew up in a 100+ year old house, it wasn't dusty, and had hard-woods/brick everywhere. reply Etheryte 2 hours agorootparentIn many big cities, getting a lot of dust is nearly out of your control, the only factor you can control is how often you gather it all up. I used to live practically next to a four-lane road when I was younger and even if you kept the windows closed, the dust would still creep in with every coming and going. If you ever opened a window, you'd know you'd need to vacuum soon. reply graeme 1 hour agorootparentprevAgree on dust removal. But if you have a constant source of pollutant input such as air pollution, dust or pollen, you want to be running a filter 24/7. Large buildings don't run their HVACs in burst and then turn them off. reply gandalfgreybeer 2 hours agorootparentprev> Running an Air Filter 24/7 has huge diminishing returns (i.e. waste of electricity). They are best run at max fan speed for short durations instead. I did this experiment in two locations. If I’m in the more urban area, running the air filter 24/7 was necessary. reply spiderfarmer 3 hours agoparentprevMaybe don't rely on air filters too much. I vacuum my office like every 2 days. Not even my air filter gets this dirty. reply fraXis 2 hours agoparentprevYou need to buy this ASAP: Spigen LD202 Designed for Mac Studio Desktop Stand Mount with built-in Air Filter - Crystal Clear https://www.amazon.com/Spigen-Designed-Desktop-FIlter-Crysta... reply donatj 33 minutes agorootparentOh, I've looked at it. I'm not sure how much it would actually help? The holes of the filter seem rather large. This one from the related products actually looks maybe a little more promising https://www.amazon.com/dp/B0CL257227/ reply perch56 3 hours agoparentprevMany people are eager to plug in their air purifiers and get started, but they often miss the fine print about checking inside the unit. Leaving the plastic bag on the filter basically turns the purifier into a fan, without any actual filtering. I saw someone post that they ran theirs like that for months before realizing it—no air getting filtered the whole time! Your dust photo reminded me, so just wanted to mention it in case you hadn’t checked for the bag inside. reply donatj 3 hours agorootparentI replace the filter on mine every couple months. It's clear of bags and the filters are regularly filthy. reply Y-bar 3 hours agoparentprevThat's a lot of dust. However, I don't see how this leads to more dust going into the computer compared to e.g. front-facing ventilation. The dust landing on the desk next to the computer will slowly drift down onto the surface, passing right in front of any opening and being sucked into the device anyway. reply pier25 34 minutes agoparentprevwouldn't you have the same issue regardless of where the intake was positioned? reply JKCalhoun 3 hours agoparentprevI'm imagining Apple building a Dusty Old House in the middle of Apple Park ... for testing. reply dvk13 1 hour agorootparentDoes The barn [1] count? :) [1]: https://en.wikipedia.org/wiki/Apple_Park#Historic_barn reply xyst 3 hours agoparentprevThat’s a shit ton of dust. Is this in an industrial factory? I agree that mine also gets dirty as well but nothing like your picture where it’s caked there. I typically just wipe it clean after a couple of weeks. Can even go a month without any issues. I even have a dog that sheds like a mofo reply Hamuko 2 hours agorootparentMy Mac Studio definitely has had a mat of dust surrounding the base intake. reply alberth 3 hours agoparentprevIf your Mac Studio is breathing that in, so are you. Please take care of your health. Just saying as a fellow HN friend. reply bilsbie 2 minutes agoprevCan I train LLM with this? reply fckgw 3 hours agoprevHaving a fully fledged computer this small without an external power brick is pretty impressive. reply jsheard 3 hours agoparentNot making it VESA mountable is a missed opportunity, but I suppose they want you to buy an iMac instead of doing that. reply fckgw 3 hours agorootparentThere's currently plenty of 3rd party VESA mounts for Mac Mini, I'm sure they'll have some for this new Mac Mini as well. They slide down into a \"clamp\" style bracket. They run about $15 on Amazon. https://www.amazon.com/Sabrent-Mount-Under-Black-BK-MABM/dp/... reply aseipp 3 hours agorootparentprevYou can get 'sandwich' enclosures that put the mini between the monitor arm and the monitor itself, or off to the side. That's what I do with an M1 Mac Mini sitting next to me. Maybe it's a blessing in disguise since you can get these cheaper than what Apple would sell them for :) reply tootie 3 hours agorootparentprevA box that small can be mounted with double-sided tape or velcro. reply grahamj 2 hours agorootparentprevNo surprise there; even their monitors’ VESA mounts are optional :D reply nordsieck 3 hours agoparentprevHonestly, I wish they'd go with an external USB-C power brick. The only reason they might not is that they want to keep everything across the entire line, and the highest end Mac Studio probably needs more power than USB can offer. reply andreasley 3 hours agorootparentWhy would you prefer an external power brick? The internal power supplies in Mac minis have been extremely reliable and the fewer cables the better, in my opinion. reply makeitdouble 2 hours agorootparentThere can be advantages to an external brick, but I see parent's comment mostly centered on having USB-C as input. That gives a lot more options IMHO on how to handle power for this machine, including portability, even if it's supposed to be a desktop machine. I thought the same for the minisforum machines which would be competitive to this, they have a 19V input that really should be USB-C at this point. reply fckgw 3 hours agorootparentprevPlus you would lose a USB-C port to power reply makeitdouble 2 hours agorootparentThey can surely add another USB-C port, it would take the space of the current power socket so space wise it shouldn't be an issue either. reply kytazo 1 hour agorootparentprevUSB-PD as of its last revision can deliver up to 240W. reply vbezhenar 2 hours agoprevI wish they would add small UPS inside (like super-capacitor or something like that) there to provide way for forced sleep when power is cut off. It's a neat small device which must be accompanied by huge bulky UPS for reliable operation. If someone didn't know, macOS ignores fsync, so without UPS your data is not safe. Not an issue for laptops, obviously, but issue for battery-less devices. reply apitman 52 minutes agoparentHow does macOS guarantee no data loss when shutting down normally? reply int_19h 21 minutes agorootparentIf you need to guarantee that data is, in fact, written permanently, you use fnctl(F_FULLFSYNC). FWIW while fsync() on Linux does request that the drive flushes its hardware cache, it's up to the drive whether to actually honor this request, and many don't do so synchronously (but still report success). So unless you control the whole setup end-to-end - hardware and software both - you don't actually have the guarantees. reply HenloFive 2 hours agoparentprevYou could use an USB C power bank, because it seems to be usb-c powered reply holycrapwhodat 1 hour agorootparentIt is not USB-C powered. It needs 100-240V, 50hz-60hz AC power. reply adib 1 minute agorootparentA missed opportunity though. If it’s USB-C powered, it could be even smaller and Apple could simplify its BOM by including a MacBook Pro charger with it. reply nicce 1 hour agorootparentprevDoes it work 24/7 or have two power sources? Can you charge the powerbank at the same time so it will rely on the battery only when the power is down? reply quux 1 hour agorootparentMany power banks support pass through charging where it powers attached devices while also charging the battery reply laweijfmvo 2 hours agoprevApple really is the kind of cherry picking comparisons. They seem to compare the new Mini with the M1 Mini, the Core i7 Mini, and the M2 Mini, all in different categories, whenever it benefits them. reply theshrike79 2 hours agoparentI have a M1 mini, didn't want to upgrade before. This one might be a big enough bump that I'm seriously considering it. reply 39896880 2 hours agoparentprevCan you think of any examples where a company trying to sell you something made unfavorable comparisons between their product and a competing product? reply animal_spirits 3 hours agoprev> Mac mini is made with over 50 percent recycled content overall, including 100 percent recycled aluminum in the enclosure, 100 percent recycled gold plating in all Apple-designed printed circuit boards, and 100 percent recycled rare earth elements in all magnets. The electricity used to manufacture Mac mini is sourced from 100 percent renewable electricity. And, to address 100 percent of the electricity customers use to power Mac mini, Apple has invested in clean energy projects around the world. Apple has also prioritized lower-carbon modes of shipping, like ocean freight, to further reduce emissions from transportation. Together, these actions have reduced the carbon footprint of Mac mini by over 80 percent. I’m inclined to trust Apple with this information but the skeptical side of me is questioning, how can we fact check this data? If it’s true it is very cool. reply eddieroger 3 hours agoparentThird party auditors that come in to verify it. \"We\" probably can't verify it, but Apple more than likely has these claims audited so they are prepared when they get sued over them. reply toomuchtodo 3 hours agorootparentIf they're lying, it's also securities fraud if you're an Apple investor, so I am inclined to believe them. reply infecto 3 hours agoparentprevhttps://www.supplychainreports.apple But ultimately its down to the third-party auditors they hire. reply collinmanderson 3 hours agoparentprevI don't know why you're getting downvoted. I think it's a fair question. The fine print says: > Carbon reductions are calculated against a business-as-usual baseline scenario: No use of clean electricity for manufacturing or product use, beyond what is already available on the latest modeled grid; Apple’s carbon intensity of key materials as of 2015; and Apple’s average mix of transportation modes by product line across three years. Learn more at apple.com/2030. https://www.apple.com/2030 which mostly seems to focus on the goal of being 100% carbon neutral in energy use. It sounds like they're generally only looking at carbon emissions from _energy_ use in transportation and manufacturing, and they're probably using some sort of carbon offset to achieve that \"net zero\". They're probably also not counting carbon emissions from building construction and they're probably not counting carbon emissions from meat served at corporate events, etc. Update: I found a breakdown for the Mac Mini (linked from the apple.com/2030 page). https://www.apple.com/environment/pdf/products/desktops/Mac_... > 100 percent of manufacturing electricity is sourced from renewable energy > For Mac mini, we are matching 100 percent of expected customer product use electricity with electricity from low-carbon sources. They are counting transportation in the \"100 percent\", but are offsetting it with carbon credits. reply xhkkffbf 3 hours agoparentprevWhat does it mean for the gold to be \"recycled\"? I get that the aluminum probably came for a pile of cans, but does this mean that the gold definitely came from a pile of electronics? Or could it be that they melted down a few old $20 coins from the US? It's not like a lot of gold ends up in landfills. reply sib 2 hours agorootparentPer the New York Times: \"According to the World Gold Council, recycled gold accounted for 28 percent of the total global gold supply of 4,633 metric tons in 2020; 90 percent of that recycled gold comes from discarded jewelry and the rest from a growing mountain of electronic waste such as cellphones and laptops.\" reply hollerith 3 hours agoparentprevThe Apple employees who worked on this product cause a lot of CO2 emissions just living their lives. I'm guessing Apple didn't try to offset that CO2. reply tonyedgecombe 2 hours agorootparentJust like all of us. Your spending is a pretty good measure of your impact on the climate. reply hollerith 2 hours agorootparentAgree. Well put. reply collinmanderson 2 hours agorootparentprevIf we truly want to achieve zero emissions globally we need to take seriously all sources of CO2 emissions, the full carbon footprint of companies. Not just energy use. It's not entirely unreasonable to ask companies to be responsible for carbon capture or in the short term an offset for their employees breathing on the clock, as funny as that sounds. We need to take all sources of carbon emissions seriously. This shouldn't be downvoted. reply sib 2 hours agorootparent>> \"ask companies to be responsible for carbon capture or in the short term an offset for their employees breathing on the clock\" Unless you think their employees breathe more when they are on the clock than off it, I'm not sure this makes sense. When they're off the clock, they might be exercising or playing with their kids, so perhaps they actually breathe less when sitting at their desks on the clock. reply hollerith 49 minutes agorootparentYikes, I hope folks don't think I was referring to CO2 caused by human respiration! I was referring to the CO2 emitted for example in growing the employee's food and getting it to him, his shelter (cement production being particularly high in CO2 emissions), transportation, home heating, the CO2 emitted by the people who educated him and provided his medical care. Like someone else said, spending is a very good proxy for CO2 emissions, and about 68% of all spending is \"consumer spending\", which basically means keeping people alive, somewhat happy and somewhat productive. reply nordsieck 3 hours agoprevThe new model is looking really good. * Kept HDMI * New, much smaller form factor * Front facing USB-C * Base model has 16 gb of ram reply melling 3 hours agoparentHow much faster is the M4 vs the M2 for Swift development? I’d probably get 32GB. I started buying 16GB Macs in 2013. The extra RAM will keep any Mac useful for a few extra years. In fact, my 2013 Intel MB Pro would be still be great if I could upgrade the OS reply svantana 3 hours agorootparentGeekbench clang benchmark: M4: 21k lines / (core-second) https://browser.geekbench.com/v6/cpu/8495624 M2: 16k lines / (core-second) https://browser.geekbench.com/v6/cpu/8546977 reply Hamuko 2 hours agoparentprevI think I’d miss my USB-A ports if I switched my Mac Studio for this. Apart from that, it looks pretty good. Not really sure if it’s worth saving a couple of hundred when you spec it up to par with an M4 Max Mac Studio when that comes out though. It’s the same price as the base M2 Max Mac Studio when you upgrade the memory and SoC. reply nordsieck 2 hours agorootparentI have lots of USB-A devices, so I get what you're saying. But converters are pretty cheap and seem reliable. And Apple has a long history of making this change ahead of the rest of the market. It's been years since they've move to all USB-C in their laptops, so IMO, it was only a matter of time. And yeah - upgrades are awful price wise. From what I can tell, it's basically only worth it to buy base models unless the machine is making you money. Hopefully they upgrade the Mac Studio to M4 down the line. reply reaperducer 1 hour agorootparentAnd Apple has a long history of making this change ahead of the rest of the market. I agree. My wife has a MacBook that is USB-C only, and it turns ten years old in a couple of months. reply Hamuko 2 hours agorootparentprevI actually recently discovered that my USB DAC was skipping a lot because I had it connected to a hub. Threw it directly onto the Mac Studio and now everything's peachy, so there are definitely downsides to trying to get a bunch of USB-A devices attached to one of these. reply jwells89 2 hours agorootparentSadly it's been common for USB hubs to be dodgy ever since the advent of USB 3. I rarely had trouble out of 1.x and 2.x hubs, but 3.x+ hubs are consistently trouble. The only ones that haven't been problematic are those integrated into Thunderbolt docks, probably because those undergo more stringent certifications. reply stego-tech 3 hours agoprevI love its form factor, less so the price difference between the M4 and M4 Pro models ($800 USD, presumably so it doesn’t cannibalize the Studio). It looks small, friendly, and inviting to the user, despite not breaking its industrial aesthetic. Honestly kind of want one as a desktop, even though my M1 Pro MBP is still insanely powerful for my needs. reply jhickok 2 hours agoparentI feel the same way. I have a really nice MBP and I cannot justify a dedicated desktop when a single thunderbolt cable to my laptop does the job just fine, but I do love the value and design. Maybe I'll pick one up for the kids. reply tootie 39 minutes agoparentprevTo me, it really is just an aesthetic thing. What purpose does this actually serve? It's small, but not portable. If it's going under my desk or on a rack never to be moved or looked at, why does it need to be cute? reply _han 3 hours agoprev> Mac mini is Apple’s first carbon neutral Mac Hats off! I didn't expect the Mac to be next in line for the carbon neutral goals. But they did it! reply sureIy 3 hours agoparentEasy if you just buy meaningless carbon credits. reply makeitdouble 2 hours agorootparentGiven how Apple is pushing the carbon neutral narrative while still not reaching the goal on all its products, I assume just buying the credits would tank their margins enough to push them to actually reduce the footprint first. This looks to me like one instance where the incentives are decently working, at least to some point. reply erur 1 hour agorootparentMaybe. Alternatively it could just be the marketing department milking the narrative over an extended amount of time. Going instantly 100% “carbon neutral” through carbon credits is certainly a worse move in this regard. reply Kon-Peki 32 minutes agorootparentYou can find this on their website with a bit of clicking around and looking at footnotes: https://www.apple.com/environment/pdf/products/desktops/Mac_... > Only after these efforts do we cover residual emissions through high-quality carbon credits that are real, additional, measurable, quantified, and have systems in place to avoid double-counting and ensure permanence. Better than nothing... Also interesting: Maxed out: Mac mini with M4 Pro (64GB memory, 8TB SSD): Product footprint before carbon credits 121 kg CO2e Min spec: Mac mini with M4 (16GB memory, 256GB SSD): Product footprint before carbon credits 32 kg CO2e I wouldn't have thought that there is this much of a difference in electronics! reply mmiyer 2 hours agoparentprevMac minis are going to be one of the smaller selling product lines, so it's probably easier to offset the carbon emissions with the carbon credits they buy. reply kylehotchkiss 2 hours agoprevThis seems like such a cool home server... BUT with all the disk encryption stuff, you'd need to be logged in to run things, right? If the power goes out, your server does too? Does anybody have a guide or tips on how to make one of these better for hosting a website with cloudflare tunnel and being resilient to power outages? reply evgen 2 hours agoparentMacs run decently as headless servers except for the limit that you cannot use full disk encryption -- the boot process stops and waits for you to provide the decryption key via local keyboard and there is no way around this. If you are concerned about this then you can look at running an encrypted external disk or a partition of an internal disk as an encrypted volume. You still need to decrypt things before everything starts working again but at least the system can boot for remote access. Yes, yes, this is not a secure as having the system fully encrypted and we can all think of various ways something like this can be compromised. It all depends on the threat model you are looking at. reply xenadu02 1 hour agoparentprevFileVault is not required. Daemons can still start without an Aqua session or user logged in. You can also still configure Aqua session auto-login. reply mike-cardwell 2 hours agoparentprevUPS reply klum 1 hour agoprevSomewhat unrelated but Apple are mainly focusing on Apple Intelligence in these new announcements. The first version of OS X I used was Mavericks. In hindsight, that was the last great version of OS X for me — the last version where it seems the priorities of the people deciding the direction of development where somewhat aligned with mine. Many have written about the decline in usability and attention to detail in OS X since then — I guess Apple Intelligence represents this shift in focus perfectly: a black-box interface that may or may not do something along the lines of what you were intending. reply mitjam 2 hours agoprevThe larger M4 Pro with 64g RAM, 1 TB, 10 GBit/s lan is a nice system for content production and local inference at 2499,- reply Flux159 3 hours agoprevTime to update all the Mac Mini server racks for the new design reply roopepal 1 hour agoparentI believe the previous design was around for well over a decade, so it did have a pretty good run. reply jelled 3 hours agoparentprevSpecs say it's 2.0 inches tall, going to need a 2U rack reply whatever1 3 hours agoprevWith the inclusion of a Thunderbolt 5 port, I think that apple might have a new high resolution, high refresh rate monitor in the works. reply crakhamster01 47 minutes agoprevI've always loved the form factor/pricing of the Mac Mini, but I've never been able to convince myself to buy it. If you're able to afford a Macbook/MBP, is there any reason why someone would purchase the Mini? Seems like the former gets you the same performance with the benefit of portability. reply sharno 33 minutes agoparentIt’s portability for a price especially if you already have your own peripherals reply not_your_vase 1 hour agoprevFunny thing that when I look at it, $600 is objectively cheap, not only by Apple standards - I remember 8 or 9 years ago I really-really wanted a Mac Mini, but just couldn't afford the 320 EUR (including like 10 EUR IBMer discount) they asked for the base model back then, new. Inflation happens on strange ways... reply objclxt 1 hour agoparentThe entry-level 2014 Mac Mini had a launch price of 499 EUR, I'm not sure it was ever that cheap new. If anything the price has deflated. reply non-nil 2 hours agoprevThey may have higher ambitions for this generation! In the presentation (roughly at the 10-minute mark), they show off the standard target demographics and setups for creative work, then complementing that with some more enterprise-flirty stuff about making workers more productive and lowering office energy usage, only to finish off with this: \"And with the industry-leading reliability of macOS, healthcare systems can count on mini when providing critical care.\" A bit out of character, and also – what?! reply rsync 3 hours agoprevHow many 4k screens can be attached? The published specs call out 3 6k screens but is that a display bandwidth limit or an arbitrary “screen” limit ? I’d like to drive four displays and 4k is sufficient for me … possible? Perhaps with Number four on the HDMI port? reply ErneX 3 hours agoparentWebsite says it supports 3 6K displays, here you go: M4 (Thunderbolt 4): - Up to three displays: Two displays with up to 6K resolution at 60Hz over Thunderbolt and one display with up to 5K resolution at 60Hz over Thunderbolt or 4K resolution at 60Hz over HDMI - Up to two displays: One display with up to 5K resolution at 60Hz over Thunderbolt and one display with up to 8K resolution at 60Hz or 4K resolution at 240Hz over Thunderbolt or HDMI M4 Pro (this one has Thunderbolt 5): - Up to three displays: Three displays with up to 6K resolution at 60Hz over Thunderbolt or HDMI - Up to two displays: One display with up to 6K resolution at 60Hz over Thunderbolt and one display with up to 8K resolution at 60Hz or 4K resolution at 240Hz over Thunderbolt or HDMI reply rsync 1 hour agorootparentOK, so unnecessary and arbitrary restrictions on screen numbers, once again. There is really no reason you couldn't drive four (or more) lower resolution (4k) screens, given the array of ports. In case anyone is wondering, the use-case here is a triple-monitor configuration at a desk with a much larger \"TV\" positioned, or hung, elsewhere in the room. reply wtallis 30 minutes agorootparentUSB PHYs aren't the same thing as display controller IP blocks. It's obviously possible to design a chip with more of the former than latter. At the hardware level, nothing is actually an arbitrarily-subdividible budget of display bandwidth. reply spiderfarmer 3 hours agoparentprevErgonomically one 6k display beats 4 displays by a mile. reply luis8 3 hours agoprevBandwidth it’s at 273 GB/s for the m4 pro. I hope the m4 max is two times that. It will allow using llama 70b a little bit faster reply terramex 3 hours agoprevHow good are modern external hard drives? Is it worth paying for more internal SSD storage or is it more reasonable to get high quality USB one? reply ErneX 3 hours agoparentI have an NVME SSD on a TB enclosure and I get 2600MB/s read speeds on my Mac Studio. reply ayewo 2 hours agorootparentMind sharing which brand you use for external storage? reply sib 2 hours agorootparentNot the OP but I use the following [1] enclosure and [2] NVME SSD on my MacBook Pro and get read / write speeds > 2,500MB/s. [1] enclosure: https://www.amazon.com/gp/product/B0BB74BQVN/ [2] drive: https://www.amazon.com/gp/product/B0B7CQ2CHH/ reply ErneX 2 hours agorootparentprevEnclosure is: Acasis M.2 NVMe SSD Enclosure 40Gbps The disk I put in there is a SK Hynix Gold P31 2TB, I am not getting its full speed with this enclosure so you can probably get a slower one and get the same results. reply anentropic 2 hours agoparentprevI've been using external drives for years and would love to get rid of them now internal ones of a decent size are available It's always been a slightly clunky experience - having to eject them before I can undock my laptop, or the way they never go to sleep (some issue with CalDigit TB dock...?) I used to think of them as a backup, but since moving house a couple of years ago my internet is fast enough to make Backblaze viable Next time I upgrade I'm just going to have less boxes on the desk, less power-drawing crap plugged in all the time I hate the price of 8TB storage on these though :( reply kissiel 3 hours agoparentprevI get 2GB/s+ with a USB4 NVME enclosure. If I buy this, this is where the main storage will be. reply ayewo 2 hours agorootparentCan you share more details about your external storage setup? reply kissiel 2 hours agorootparentEnclosure: https://www.unitek-products.com/products/solidforce-reefer-p... The SSD: https://www.crucial.com/ssd/t500/CT2000T500SSD5?_gl=1*1g4r3l... reply sib 2 hours agorootparentprevI'm not the OP but I use this [1] enclosure and [2] NVME SSD on my MacBook Pro and get read & write speeds > 2,500MB/s. [1] enclosure: https://www.amazon.com/gp/product/B0BB74BQVN/ [2] drive: https://www.amazon.com/gp/product/B0B7CQ2CHH/ reply vessenes 3 hours agoparentprevWhat’s your use case? reply BXlnt2EachOther 3 hours agoprevFront ports: 2 USB‑C, headphone Back ports: 3 Thunderbolt 4 ports (Thunderbolt 5 on the top $1399 tier), HDMI, Gigabit Ethernet RAM can be upgraded to 32GB on M4, to 64GB on M4 Pro 10 GbE looks selectable on any of these, +$100 reply OnionBlender 2 hours agoprevIs there a good performance benchmark website/channel for Mac hardware? (Once reviewers get their hands on the hardware) I'm trying to decide if I should get the Pro or the base model mini. I've been learning Swift and Metal using an old work Macbook and I want to get my own hardware. The only games I play recently at Factorio and Baldur's Gate 3, so I was thinking perhaps I should get the Pro and not bother upgrading my desktop (an i7 6700k from 2015). reply farawayea 1 hour agoprevDoes this still have soldered flash chips for the SSD? This would've looked a lot better without the soldered non-upgradable SSD. It's not great at all. This guy will probably have a lot of clients https://www.youtube.com/watch?v=E3N-z-Y8cuw. reply ErneX 1 hour agoparentYes, but that’s easy to solve via USB reply farawayea 1 hour agorootparentThat's not a solution when your main SSD dies. The system no longer boots. The guy from the youtube video explains that as well. Normal computers with NVMe storage will always be more repairable than Apple's hardware with everything soldered on",
    "originSummary": [
      "Apple has introduced the new Mac mini, equipped with M4 and M4 Pro chips, offering significant performance improvements with up to 1.8x faster CPU and 2.2x faster GPU compared to the M1 model.",
      "The Mac mini is the first carbon-neutral Mac, reducing greenhouse gas emissions by over 80%, and is made with over 50% recycled materials, using 100% renewable energy in manufacturing.",
      "It features Thunderbolt 5 for enhanced data transfer and supports Apple Intelligence, which boosts user privacy and productivity, with a starting price of $599 and availability for pre-order, shipping from November 8."
    ],
    "commentSummary": [
      "The new Mac Mini with the M4 chip is competitively priced at $500 with educational discounts, featuring 16GB of RAM, positioning it as a strong alternative to similarly priced desktop PCs.",
      "While the Mac Mini is praised for its compact design and performance, users should be cautious of potential display issues with sub-4K monitors on macOS and the limited internal storage.",
      "The device is noted for its efficiency and potential use as a home server, though concerns about non-upgradable components, such as the SSD, persist."
    ],
    "points": 359,
    "commentCount": 529,
    "retryCount": 0,
    "time": 1730214021
  },
  {
    "id": 41976754,
    "title": "Steve Ballmer was an underrated CEO",
    "originLink": "https://danluu.com/ballmer/",
    "originBody": "There's a common narrative that Microsoft was moribund under Steve Ballmer and then later saved by the miraculous leadership of Satya Nadella. This is the dominant narrative in every online discussion about the topic I've seen and it's a commonly expressed belief \"in real life\" as well. While I don't have anything negative to say about Nadella's leadership in this post, this narrative underrates Ballmer's role in Microsoft's success. Not only did Microsoft's financials, revenue and profit, look great under Ballmer, Microsoft under Ballmer made deep, long-term bets that set up Microsoft for success in the decades after his reign. At the time, the bets were widely panned, indicating that they weren't necessarily obvious, but we can see in retrospect that the company made very strong bets despite the criticism at the time. In addition to overseeing deep investments in areas that people would later credit Nadella for, Ballmer set Nadella up for success by clearing out political barriers for any successor. Much like Gary Bernhardt's talk, which was panned because he made the problem statement and solution so obvious that people didn't realize they'd learned something non-trivial, Ballmer set up Microsoft for future success so effectively that it's easy to criticize him for being a bum because his successor is so successful. Criticisms of Ballmer For people who weren't around before the turn of the century, in the 90s, Microsoft used to be considered the biggest, baddest, company in town. But it wasn't long before people's opinions on Microsoft changed — by 2007, many people thought of Microsoft as the next IBM and Paul Graham wrote Microsoft is Dead, in which he noted that Microsoft being considered effective was ancient history: A few days ago I suddenly realized Microsoft was dead. I was talking to a young startup founder about how Google was different from Yahoo. I said that Yahoo had been warped from the start by their fear of Microsoft. That was why they'd positioned themselves as a \"media company\" instead of a technology company. Then I looked at his face and realized he didn't understand. It was as if I'd told him how much girls liked Barry Manilow in the mid 80s. Barry who? Microsoft? He didn't say anything, but I could tell he didn't quite believe anyone would be frightened of them. These kinds of comments often came with comments that Microsoft's revenue was destined to fall, such as these comments by Graham: Actors and musicians occasionally make comebacks, but technology companies almost never do. Technology companies are projectiles. And because of that you can call them dead long before any problems show up on the balance sheet. Relevance may lead revenues by five or even ten years. Graham names Google and the web as primary causes of Microsoft's death, which we'll discuss later. Although Graham doesn't name Ballmer or note his influence in Microsoft is Dead, Ballmer has been a favorite punching bag of techies for decades. Ballmer came up on the business side of things and later became EVP of Sales and Support; techies love belittling non-technical folks in tech1. A common criticism, then and now, is that Ballmer didn't understand tech and was a poor leader because all he knew was sales and the bottom line and all he can do is copy what other people have done. Just for example, if you look at online comments on tech forums (minimsft, HN, slashdot, etc.) when Ballmer pushed Sinofsky out in 2012, Ballmer's leadership is nearly universally panned2. Here's a fairly typical comment from someone claiming to be an anonymous Microsoft insider: Dump Ballmer. Fire 40% of the workforce starting with the loser online services (they are never going to get any better). Reinvest the billions in start-up opportunities within the puget sound that can be accretive to MSFT and acquisition targets ... Reset Windows - Desktop and Tablet. Get serious about business cloud (like Salesforce ...) To the extent that Ballmer defended himself, it was by pointing out that the market appeared to be undervaluing Microsoft. Ballmer noted that Microsoft's market cap at the time was extremely low relative to its fundamentals/financials relative to Amazon, Google, Apple, Oracle, IBM, and Salesforce. This seems to have been a fair assessment by Ballmer as Microsoft has outperformed all of those companies since then. When Microsoft's market cap took off after Nadella became CEO, it was only natural the narrative would be that Ballmer was killing Microsoft and that the company was struggling until Nadella turned it around. You can pick other discussions if you want, but just for example, if we look at the most recent time Microsoft is Dead hit #1 on HN, a quick ctrl+F has Ballmer's name showing up 24 times. Ballmer has some defenders, but the standard narrative that Ballmer was holding Microsoft back is there, and one of the defenders even uses part of the standard narrative: Ballmer was an unimaginative hack, but he at least set up Microsoft well financially. If you look at high ranking comments, they're all dunking on Ballmer. And if you look on less well informed forums, like Twitter or Reddit, you see the same attacks, but Ballmer has fewer defenders. On Twitter, when I search for \"Ballmer\", the first four results are unambiguously making fun of Ballmer. The fifth hit could go either way, but from the comments, seems to generally be taken as making of Ballmer, and as I far as I scrolled down, all but one of the remaining videos was making fun of Ballmer (the one that wasn't was an interview where Ballmer notes that he offered Zuckerberg \"$20B+, something like that\" for Facebook in 2009, which would've been the 2nd largest tech acquisition ever at the time, second only to Carly Fiorina's acquisition of Compaq for $25B in 2001). Searching reddit (incognito window with no history) is the same story (excluding the stories about him as an NBA owner, where he's respected by fans). The top story is making fun of him, the next one notes that he's wealthier than Bill Gates and the top comment on his performance as a CEO starts with \"The irony is that he is Microsofts [sic] worst CEO\" and then has the standard narrative that the only reason the company is doing well is due to Nadella saving the day, that Ballmer missed the boat on all of the important changes in the tech industry, etc. To sum it up, for the past twenty years, people having been dunking on Ballmer for being a buffoon who doesn't understand tech and who was, at best, some kind of bean counter who knew how to keep the lights on but didn't know how to foster innovation and caused Microsoft to fall behind in every important market. Ballmer's wins The common view is at odds with what actually happened under Ballmer's leadership. In financially material positive things that happened under Ballmer since Graham declared Microsoft dead, we have: 2009: Bing launched. This is considered a huge failure, but the bar here is fairly high. A quick web search finds that Bing allegedly made $1B in profit in 2015 and $6.4B in FY 2024 on $12.6B of revenue (given Microsoft's PE ratio in 2022, a rough estimate for Bing's value in 2022 would be $240B) 2010: Microsoft creates Azure I can't say that I personally like it as a product, but in terms of running large scale cloud infrastructure, the three companies that are head-and-shoulders ahead of everyone else in the world are Amazon, Google, and Microsoft. From a business standpoint, the worst thing you could say about Microsoft here is that they're a solid #2 in terms of the business and the biggest threat to become the #1 The enterprise sales arm, built and matured under Ballmer, was and is critical to the success of Azure and Office 2010: Office 365 released Microsoft transitioned its enterprise / business suite of software from boxed software to subscription-based software with online options there isn't really a fixed date for this; the official release of Office 365 seems like as good a year as any Like Azure, I don't personally like these products, but if Microsoft were to split up into major business units, the enterprise software suite is the business unit that could possibly rival Azure in market cap There are certainly plenty of big misses as well. From 2010-2015, HoloLens was one of Microsoft's biggest bets, behind only Azure and then Bing, but no one's big AR or VR bets have had good returns to date. Microsoft failed to capture the mobile market. Although Windows Phone was generally well received by reviewers who tried it, depending on who you ask, Microsoft was either too late or wasn't willing to subsidize Windows Phone for long enough. Although .NET is still used today, in terms of marketshare, .NET and Silverlight didn't live up to early promises and critical parts were hamstrung or killed as a side effect of internal political battles. Bing is, by reputation, a failure and, at least given Microsoft's choices at the time, probably needed antitrust action against Google to succeed, but this failure still resulted in a business unit worth hundreds of billions of dollars. And despite all of the failures, the biggest bet, Azure, is probably worth on the order of a trillion dollars. The enterprise sales arm of Microsoft was built out under Ballmer before he was CEO (he was, for a time, EVP for Sales and Support, and actually started at Microsoft as the first business manager) and continued to get built out when Ballmer was CEO. Microsoft's sales playbook was so effective that, when I was Microsoft, Google would offer some customers on Office 365 Google's enterprise suite (Docs, etc.) for free. Microsoft salespeople noted that they would still usually be able to close the sale of Microsoft's paid product even when competing against a Google that was giving their product away. For the enterprise, the combination of Microsoft's offering and its enterprise sales team was so effective that Google couldn't even give its product away. If you're reading this and you work at a \"tech\" company, the company is overwhelmingly likely to choose the Google enterprise suite over the Microsoft enterprise suite and the enterprise sales pitch Microsoft sales people have probably sounds ridiculous to you. An acquaintance of mine who ran a startup had a Microsoft Azure salesperson come in and try to sell them on Azure, opening with \"You're on AWS, the consumer cloud. You need Azure, the enterprise cloud\". For most people in tech companies, enterprise is synonymous with overpriced, unreliable, junk. In the same way it's easy to make fun of Ballmer because he came up on the sales and business side of the house, it's easy to make fun of an enterprise sales pitch when you hear it but, overall, Microsoft's enterprise sales arm does a good job. When I worked in Azure, I looked into how it worked and, having just come from Google, there was a night and day difference. This was in 2015, under Nadella, but the culture and processes that let Microsoft scale this up were built out under Ballmer. I think there were multiple months where Microsoft hired and onboarded more salespeople than Google employed in total and every stage of the sales pipeline was fairly effective. Microsoft's misses under Ballmer When people point to a long list of failures like Bing, Zune, Windows Phone, and HoloLens as evidence that Ballmer was some kind of buffoon who was holding Microsoft back, this demonstrates a lack of understanding of the tech industry. This is like pointing to a list of failed companies a VC has funded as evidence the VC doesn't know what they're doing. But that's silly in a hits based industry like venture capital. If you want to claim the VC is bad, you need to point out poor total return or a lack of big successes, which would imply poor total return. Similarly, a large company like Microsoft has a large portfolio of bets and one successful bet can pay for a huge number of failures. Ballmer's critics can't point to a poor total return because Microsoft's total return was very good under his tenure. Revenue increased from $14B or $22B to $83B, depending on whether you want to count from when Ballmer became President in July 1998 or when Ballmer became CEO in January 2000. The company was also quite profitable when Ballmer left, recording $27B in profit the previous four quarters, more than the revenue of the company he took over. By market cap, Azure alone would be in the top 10 largest public companies in the world and the enterprise software suite minus Azure would probably just miss being in the top 10. As a result, critics also can't point to a lack of hits when Ballmer presided over the creation of Azure, the conversion of Microsoft's enterprise software from set of local desktop apps to Office 365 et al., the creation of the world's most effective enterprise sales org, the creation of Microsoft's video game empire (among other things, Ballmer was CEO when Microsoft acquired Bungie and made Halo the Xbox's flagship game on launch in 2001), etc. Even Bing, widely considered a failure, on last reported revenue and current P/E ratio, would be 12th most valuable tech company in the world, between Tencent and ASML. When attacking Ballmer, people cite Bing as a failure that occurred on Ballmer's watch, which tells you something about the degree of success Ballmer had. Most companies would love to have their successes be as successful as Bing, let alone their failures. Of course it would be better if Ballmer was prescient and all of his bets succeeded, making Microsoft worth something like $10T instead of the lowly $3T market cap it has today, but the criticism of Ballmer that says that he had some failures and some $1T successes is a criticism that he wasn't the greatest CEO of all time by a gigantic margin. True, but not much of a criticism. And, unlike Nadella, Ballmer didn't inherit a company that was easily set up for success. As we noted earlier, it wasn't long into Ballmer's tenure that Microsoft was considered a boring, irrelevant company and the next IBM, mostly due to decisions made when Bill Gates was CEO. As a very senior Microsoft employee from the early days, Ballmer was also partially responsible for the state of Microsoft at the time, so Microsoft's problems are also at least partially attributable to him (but that also means he should get some credit for the success Microsoft had through the 90s). Nevertheless, he navigated Microsoft's most difficult problems well and set up his successor for smooth sailing. Earlier, we noted that Paul Graham cited Google and the rise of the web as two causes for Microsoft's death prior to 2007. As we discussed in this look at antitrust action in tech, these both share a common root cause, antitrust action against Microsoft. If we look at the documents from the Microsoft antitrust case, it's clear that Microsoft knew how important the internet was going to be and had plans to control the internet. As part of these plans, they used their monopoly power on the desktop to kill Netscape. They technically lost an antirust case due to this, but if you look at the actual outcomes, Microsoft basically got what they wanted from the courts. The remedies levied against Microsoft are widely considered to have been useless (the initial decision involved breaking up Microsoft, but they were able to reverse this on appeal), and the case dragged on for long enough that Netscape was doomed by the time the case was decided, and the remedies that weren't specifically targeted at the Netscape situation were meaningless. A later part of the plan to dominate the web, discussed at Microsoft but never executed, was to kill Google. If we're judging Microsoft by how \"dangerous\" it is, how effectively it crushes its competitors, like Paul Graham did when he judged Microsoft to be dead, then Microsoft certainly became less dangerous, but the feeling at Microsoft was that their hand was forced due to the circumstances. One part of the plan to kill Google was to redirect users who typed google.com into their address bar to MSN search. This was before Chrome existed and before mobile existed in any meaningful form. Windows desktop marketshare was 97% and IE had between 80% to 95% marketshare depending on the year, with most of the rest of the marketshare belonging to the rapidly declining Netscape. If Microsoft makes this move, Google is killed before it can get Chrome and Android off the ground and, barring extreme antitrust action, such as a breakup of Microsoft, Microsoft owns the web to this day. And then for dessert, it's not clear there wouldn't be a reason not to go after Amazon. After internal debate, Microsoft declined to kill Google not due to fear of antitrust action, but due to fear of bad PR from the ensuing antitrust action. Had Microsoft redirected traffic away from Google, the impact on Google would've been swifter and more severe than their moves against Netscape and in the time it would take for the DoJ to win another case against Microsoft, Google would suffer the same fate as Netscape. It might be hard to imagine this if you weren't around at the time, but the DoJ vs. Microsoft case was regular front-page news in a way that we haven't seen since (in part because companies learned their lesson on this one — Google supposedly killed the 2011-2012 FTC against them with lobbying and has cleverly maneuvered the more recent case so that it doesn't dominate the news cycle in the same way). The closest thing we've seen since the Microsoft antitrust media circus was the media response to the Crowdstrike outage, but that was a flash in the pan compared to the DoJ vs. Microsoft case. If there's a criticism of Ballmer here, perhaps it's something like Microsoft didn't pre-emptively learn the lessons its younger competitors learned from its big antitrust case before the big antitrust case. A sufficiently prescient executive could've advocated for heavy lobbying to head the antitrust case off at pass, like Google did in 2011-2012, or maneuvered to make the antitrust case just another news story, like Google has been doing for the current case. Another possible criticism is that Microsoft didn't correctly read the political tea leaves and realize that there wasn't going to be serious US tech antitrust for at least two decades after the big case against Microsoft. In principle, Ballmer could've overridden the decision to not kill Google if he had the right expertise on staff to realize that the United States was entering a two decade period of reduced antitrust scrutiny in tech. As criticisms go, I think the former criticism is correct, but not an indictment of Ballmer unless you expect CEOs to be infallible, so as evidence that Ballmer was a bad CEO, this would be a very weak criticism. And it's not clear that the latter criticism is correct. While Google was able to get away with things like hardcoding the search engine in Android to prevent users from changing their search engine setting to having badware installers trick users into making Chrome the default browser, they were considered the \"good guys\" and didn't get much scrutiny for these sorts of actions, Microsoft wasn't treated with kid gloves in the same way by the press or the general public. Google didn't trigger a serious antitrust investigation until 2011, so it's possible the lack of serious antitrust action between 2001 and 2010 was an artifact of Microsoft being careful to avoid antitrust scrutiny and Google being too small to draw scrutiny and that a move to kill Google when it was still possible would've drawn serious antitrust scrutiny and another PR circus. That's one way in which the company Ballmer inherited was in a more difficult situation than its competitors — Microsoft's hands were perceived to be tied and may have actually been tied. Microsoft could and did get severe criticism for taking an action when the exact same action taken by Google would be lauded as clever. When I was at Microsoft, there was a lot of consternation about this. One funny example was when, in 2011, Google officially called out Microsoft for unethical behavior and the media jumped on this as yet another example of Microsoft behaving badly. A number of people I talked to at Microsoft were upset by this because, according to them, Microsoft got the idea to do this when they noticed that Google was doing it, but reputations take a long time to change and actions taken while Gates was CEO significantly reduced Microsoft's ability to maneuver. Another difficulty Ballmer had to deal with on taking over was Microsoft's intense internal politics. Again, as a very senior Microsoft employee going back to almost the beginning, he bears some responsibility for this, but Ballmer managed to clear the board of the worst bad actors so that Nadella didn't inherit such a difficult situation. If we look at why Microsoft didn't dominate the web under Ballmer, in addition to concerns that killing Google would cause a PR backlash, internal political maneuvering killed most of Microsoft's most promising web products and reduced the appeal and reach of most of the rest of its web products. For example, Microsoft had a working competitor to Google Docs in 1997, one year before Google was founded and nine years before Google acquired Writely, but it was killed for political reasons. And likewise for NetMeeting and other promising products. Microsoft certainly wasn't alone in having internal political struggles, but it was famous for having more brutal politics than most. Although Ballmer certainly didn't do a perfect job at cleaning house, when I was at Microsoft and asked about promising projects that were sidelined or killed due to internal political struggles, the biggest recent sources of those issues were shown the door under Ballmer, leaving a much more functional company for Nadella to inherit. The big picture Stepping back to look at the big picture, Ballmer inherited a company that was a financially strong position that was hemmed in by internal and external politics in a way that caused outside observers to think the company was overwhelmingly likely to slide into irrelevance, leading to predictions like Graham's famous prediction that Microsoft is dead, with revenues expected to decline in five to ten years. In retrospect, we can see that moves made under Gates limited Microsoft's ability to use its monopoly power to outright kill competitors, but there was no inflection point at which a miraculous turnaround was mounted. Instead, Microsoft continued its very strong execution on enterprise products and continued making reasonable bets on the future in a successful effort to supplant revenue streams that were internally viewed as long-term dead ends, even if they were going to be profitable dead ends, such as Windows and boxed (non-subscription) software. Unlike most companies in that position, Microsoft was willing to very heavily subsidize a series of bets that leadership thought could power the company for the next few decades, such as Windows Phone, Bing, Azure, Xbox, and HoloLens. From the internal and external commentary on these bets, you can see why it's so hard for companies to use their successful lines of business to subsidize new lines of business when the writing is on the wall for the successful businesses. People panned these bets as stupid moves that would kill the company, saying the company should focus is efforts on its most profitable businesses, such as Windows. Even when there's very clear data showing that bucking the status quo is the right thing, people usually don't do it, in part because you look like an idiot when it doesn't pan out, but Ballmer was willing to make the right bets in the face of decades of ridicule. Another reason it's hard for companies to make these bets is that companies are usually unable to things that are radically different from their core business. When yet another non-acquisition Google consumer product fails, every writes this off as a matter of course — of course Google failed there, they're a technical-first company that's bad at product. But Microsoft made this shift multiple times and succeeded. Once was with Xbox. If you look at the three big console manufacturers, two are hardware companies going way back and one is Microsoft, a boxed software company that learned how to make hardware. Another time was with Azure. If you look at the three big cloud providers, two are online services companies going back to their founding and one is Microsoft, a boxed software company that learned how to get into the online services business. Other companies with different core lines of business than hardware and online services saw these opportunities and tried to make the change and failed. And if you look at the process of transitioning here, it's very easy to make fun of Microsoft in the same way it's easy to make fun of Microsoft's enterprise sales pitch. The core Azure folks came from Windows, so in the very early days of Azure, they didn't have an incident management process to speak of and during their first big global outages, people were walking around the hallways asking \"is it Azure down?\" and trying to figure out what to do. Azure would continue to have major global outages for years while learning how to ship somewhat reliable software, but they were able to address the problems well enough to build a trillion dollar business. Another time, before Azure really knew how to build servers, a Microsoft engineer pulled up Amazon's pricing page and noticed that AWS's retail price for disk was cheaper than Azure's cost to provision disks. When I was at Microsoft, a big problem for Azure was building out datacenter fast enough. People joked that the recent hiring of a ton of sales people worked too well and the company sold too much Azure, which was arguably true and also a real emergency for the company. In the other cases, Microsoft mostly learned how to do it themselves and in this case they brought in some very senior people from Amazon who had deep expertise in supply chain and building out datacenters. It's easy to say that, when you have a problem and a competitor has the right expertise, you should hire some experts and listen to them but most companies fail when try to do this. Sometimes, companies don't recognize that they need help but, more frequently, they do bring in senior expertise that people don't listen to. It's very easy for the old guard at a company to shut down efforts to bring in senior outside expertise, especially at a company as fractious at Microsoft, but leadership was able to make sure that key initiatives like this were successful. When I talked to Google engineers about Azure during Azure's rise, they were generally down on Azure and would make fun of it for issues like the above, which seemed comical to engineers working at a company that grew up as large scale online services companies with deep expertise in operating large scale services, building efficient hardware, and building out datacenter, but despite starting in a very deep hole technically, operationally, and culturally, Microsoft built a business unit worth a trillion dollars with Azure. Not all of the bets panned out, but if we look at comments from critics who were saying that Microsoft was doomed because it was subsidizing the wrong bets or younger companies would surpass it, well, today, Microsoft is worth 50% more than Google and twice as much as Meta. If we look at the broader history of the tech industry, Microsoft has had sustained strong execution from its founding in 1975 until today, a nearly fifty year run, a run that's arguably been unmatched in the tech industry. Intel's been around as bit longer, but they stumbled very badly around the turn of the century and they've had a number of problems over the past decade. IBM has a long history, but it just wasn't all that big during its early history, e.g., when T.J. Watson renamed Computing-Tabulating-Recording Company to International Business Machines, its revenue was still well under $10M a year (inflation adjusted, on the order of $100M a year). Computers started becoming big and IBM was big for a tech company by the 50s, but the antitrust case brought against IBM in 1969 that dragged on until it was dropped for being \"without merit\" in 1982 hamstrung the company and its culture in ways that are still visible when you look at, for example, why IBM's various cloud efforts have failed and, in the 90s, the company was on its deathbed and only managed to survive at all due to Gerstner's turnaround. If we look at older companies that had long sustained runs of strong execution, most of them are gone, like DEC and Data General, or had very bad stumbles that nearly ended the company, like IBM and Apple. There are companies that have had similarly long periods of strong execution, like Oracle, but those companies haven't been nearly as effective as Microsoft in expanding their lines of business and, as a result, Oracle is worth perhaps two Bings. That makes Oracle the 20th most valuable public company in the world, which certainly isn't bad, but it's no Microsoft. If Microsoft stumbles badly, a younger company like Nvidia, Meta, or Google could overtake Microsoft's track record, but that would be no fault of Ballmer's and we'd still have to acknowledge that Ballmer was a very effective CEO, not just in terms of bringing the money in, but in terms of setting up a vision that set Microsoft up for success for the next fifty years. Appendix: Microsoft's relevance under Ballmer Besides the headline items mentioned above, off the top of my head, here are a few things I thought were interesting that happened under Ballmer since Graham declared Microsoft to be dead 2007: Microsoft releases LINQ, still fairly nice by in-use-by-practitioners standards today 2011: Suimt Gulwani, at MSR, publishes \"Automating string processing in spreadsheets using input-output examples\", named a most influential POPL paper 10 years later This paper is about using program synthesis for spreadsheet \"autocomplete/inference\" I'm not a fan of patents, but I would guess that the reason autocomplete/inference works fairly well in Excel and basically doesn't work at all in Google Sheets is that MS has a patent on this based on this work 2012: Microsoft releases TypeScript This has to be the most widely used programming language released this century and it's a plausible candidate for becoming the most widely used language, period (as long as you don't also count TS usage as JS) 2012: Microsoft Surface released Things haven't been looking so good for the Surface line since Panos Panay left in 2022, and this was arguably failure even in 2022, but this was a $7B/yr line of business in 2022, which goes to show you how big and successful Microsoft is — most companies would love to have something doing as well as a failed $7B/yr business 2015: Microsoft releases vscode (after the end of Ballmer's tenure in 2014, but this work came out of work under Ballmer's tenure in multiple ways) This seems like the most widely used editor among programmers today by a very large margin. When I looked at survey data on this a number of years back, I was shocked by how quickly this happened. It seems like vscode has achieved a level of programmer editor dominance that's never been seen before. Probably the closest thing was Visual Studio a decade before Paul declared Microsoft dead, but that never achieved the same level of marketshare due to a combination of effectively being Windows only software and also costing quite a bit of money Heath Borders notes that Erich Gamma, hired in 2011, was highly influential here One response to Microsoft's financial success, both the direct success that happened under Ballmer as well as later success that was set up by Ballmer, is that Microsoft is financially successful but irrelevant for trendy programmers, like IBM. For one thing, rounded to the nearest Bing, IBM is probably worth either zero or one Bings. But even if we put aside the financial aspect and we just look at how much each $1T tech company (Apple, Nvidia, Microsoft, Google, Amazon, and Meta) has impacted programmers, Nvidia, Apple, and Microsoft all have a lot of programmers who are dependent on the company due to some kind of ecosystem dependence (CUDA; iOS; .NET and Windows, the latter of which is still the platform of choice for many large areas, such as AAA games). You could make a case for the big cloud vendors, but I don't think that companies have a nearly forced dependency on AWS in the same way that a serious English-language consumer app company really needs an iOS app or an AAA game company has to release on Windows and overwhelmingly likely develops on Windows. If we look at programmers who aren't pinned to an ecosystem, Microsoft seems highly relevant to a lot of programmers due to the creation of tools like vscode and TypeScript. I wouldn't say that it's necessarily more relevant than Amazon since so many programmers use AWS, but it's hard to argue that the company that created (among many other things) vscode and TypeScript under Ballmer's watch is irrelevant to programmers. Appendix: my losing bet against Microsoft Shortly after joining Microsoft in 2015, I bet Derek Chiou that Google would beat Microsoft to $1T market cap. Unlike most external commentators, I agreed with the bets Microsoft was making, but when I looked around at the kinds of internal dysfunction Microsoft had at the time, I thought that would cause them enough problems that Google would win. That was wrong — Microsoft beat Google to $1T and is now worth $1T more than Google. I don't think I would've made the bet even a year later, after seeing Microsoft from the inside and how effective Microsoft sales was and how good Microsoft was at shipping things that are appealing to enterprises and the comparing that to Google's cloud execution and strategy. But you could say that I made a mistake that was fairly analogous to what external commentators made until I saw how Microsoft operated in detail. Thanks to Laurence Tratt, Yossi Kreinin, Heath Borders, Justin Blank, and Fabian Giesen for comments/corrections/discussion Fabian Giesen points out that, in addition to Ballmer's \"sales guy\" reputation, his stage persona didn't do him any favors, saying \"His stage presence made people think he was bad. But if you're not an idiot and you see an actor portraying Macbeth, you don't assume they're killing all their friends IRL\" [return] Here's the top HN comment on a story about Sinofsky's ousting: The real culprit that needs to be fired is Steve Ballmer. He was great from the inception of MSFT until maybe the turn of the century, when their business strategy of making and maintaining a Windows monopoly worked beautifully and extremely profitably. However, he is living in a legacy environment where he believes he needs to protect the Windows/Office monopoly BY ANY MEANS NECESSARY, and he and the rest of Microsoft can't keep up with everyone else around them because of innovation. This mindset has completely stymied any sort of innovation at Microsoft because they are playing with one arm tied behind their backs in the midst of trying to compete against the likes of Google, Facebook, etc. In Steve Ballmer's eyes, everything must lead back to the sale of a license of Windows/Office, and that no longer works in their environment. If Microsoft engineers had free rein to make the best search engine, or the best phone, or the best tablet, without worries about how will it lead to maintaining their revenue streams of Windows and more importantly Office, then I think their offerings would be on an order of magnitude better and more creative. This is wrong. At the time, Microsoft was very heavily subsidizing Bing. To the extent that one can attribute the subsidy, it would be reasonable to say that the bulk of the subsidy was coming from Windows. Likewise, Azure was a huge bet that was being heavily subsidized from the profit that was coming from Windows. Microsoft's strategy under Ballmer was basically the opposite of what this comment is saying. Funnily enough, if you looked at comments on minimsft (many of which were made by Microsoft insiders), people noted the huge spend on things like Azure and online services, but most thought this was a mistake and that Microsoft needed to focus on making Windows and Windows hardware (like the Surface) great. Basically, no matter what people think Ballmer is doing, they say it's wrong and that he should do the opposite. That means people call for different actions since most commenters outside of Microsoft don't actually know what Microsoft is up to, but from the way the comments are arrayed against Ballmer and not against specific actions of the company, we can see that people aren't really making a prediction about any particular course of action and they're just ragging on Ballmer. BTW, the #2 comment on HN says that Ballmer missed the boat on the biggest things in tech in the past 5 years and that Ballmer has deemphasized cloud computing (which was actually Microsoft's biggest bet at the time if you look at either capital expenditure or allocated headcount). The #3 comment says \"Steve Ballmer is a sales guy at heart, and it's why he's been able to survive a decade of middling stock performance and strategic missteps: He must have close connections to Microsoft's largest enterprise customers, and were he to be fired, it would be an invitation for those customers to reevaluate their commitment to Microsoft's platforms.\", and the rest of the top-level comments aren't about Ballmer. [return]",
    "commentLink": "https://news.ycombinator.com/item?id=41976754",
    "commentBody": "Steve Ballmer was an underrated CEO (danluu.com)331 points by greggyb 21 hours agohidepastfavorite625 comments exmicrosoldier 4 hours agoI disagree with Dan due to my experience as an low level employee under Ballmer. He encouraged political infighting and backstabbing and dog eat dog internal competition, while praising and desiring tight integration between teams. He wanted \"cloud first, moblie first\" - two firsts! The culture at the time was built around RAID - the internal bug datadbase and that there should be clear prioritization for everything. The inability to decide between enterprise cloud and consumer client devices held Microsoft back. Ballmer had customers asking for enterprise cloud in 2000 but he kept listening to people talking about lifting windows sales by 10 percent with search integrated to the desktop. And then they chose the bloated SQL server for that and wondered why that couldn't run on normal consumer hardware in Longhorn. The fundamental tradeoffs between something that sacrifices generalization for specialization and efficiency meant that what is good for running server rack NASDAQ didn't work for low powered laptops. From a low level employee perspective Ballmer was the ruthless guy that wanted people to hate each other at work as they fought for survival lord of the flies style but was pikachu surprised that we could never deliver integrated experiences that worked together. Satya's two key abilites to me were the ability to actually prioritize in a coherent way and the decision to bring the rank and file infighting down because integrated experiences are hard to build when you want your brother and sister departments to fail so yours gets more budget because thats how Ballmer worked. reply AdrianB1 0 minutes agoparentI worked a long time ago in the same company that Steve Ballmer worked, both as juniors. Same culture. I think I know where he got it from. reply alexawarrior4 2 hours agoparentprevSo Ballmer-era Microsoft was the inspiration for Amazon's current culture, I see. reply xkqd 2 hours agorootparentThis one’s on Jack Welch - a pioneer in short term gain over long term building. You absolutely can juice a company’s performance by going dog-eat-dog, but inevitably when the smoke clears you’re left with jackals and hyenas stretched too thin. Always worth mentioning that this culturally altered America in a way that we’ll probably never unwind. reply DowagerDave 1 minute agorootparentCombined with the massive popularity of private equity in so many business areas now we're unlikely to see 100-year companies again. reply RobRivera 6 minutes agorootparentprevCulturally unwinds corporate america. Go to any family business that scales a niche and you find golf course dealmaking and nepotism humming along with good ole fashion quiet cartel work. reply alsetmusic 9 minutes agorootparentprev> Always worth mentioning that this culturally altered America in a way that we’ll probably never unwind. I think this about a lot of things, such as certain events in politics or generative AI. I'm curious how you apply this to ruthless cutthroat policies at a handful of (admittedly quite large) tech companies? reply atomicnature 3 hours agoparentprevoff-topic: Love the username :) reply addicted 16 hours agoprevThis article doesn’t understand what was fundamentally wrong with Ballmer’s leadership and what Nadella actually changed. The specific technologies that were successful is irrelevant. Microsoft has and continues to invest in nearly every computer related technology that may come around the corner or they got late on. The problem with Microsoft was everything went through Windows. The entire company was designed to promote Windows. This was the fundamental flaw with Microsoft that Nadella changed. He quickly not just made Windows just another part of Microsoft’s business, to a great extent he actively devalued it. The fact that Ballmer invested in Azure, etc before Nadella would all be irrelevant because under Ballmer Azure would have remained a red headed step child to Windows, so it’s unlikely to have seen much success under him anyways. Same goes for pretty much everything else Microsoft is doing right now. reply _heimdall 6 hours agoparentLately it has definitely felt as though Microsoft is resurrecting Ballmer's old meme as \"AI! AI! AI!\" I was at Microsoft for the last couple years of Ballmer and the first few years of Nadella. He definitely did change the company and I remember at the time feeling that he handled the change really well, but from where I sat he spent the first part of his tenure evolving Ballmer's final push to move focus from Windows to developers. Everything Microsoft did prior to LLMs was to bring developers over, from VS Code to GitHub to WSL. Now the company seems fully baked I to LLMs with everything they do chasing that. It would even make sense if the developer push was driven in part by the need to build up training sets for the eventual LLM work, though I really have a hard time believing that Microsoft was so well ahead of the game that they started grooming developers to provide data more than a decade ago. reply toyg 6 hours agorootparent> Now the company seems fully baked I to LLMs with everything they do chasing that Them along absolutely everyone else. ChatGPT was an iPhone moment. reply dblohm7 3 hours agorootparent> Them along absolutely everyone else. ChatGPT was an iPhone moment. Old guy here, but it feels more like a Netscape moment than an iPhone moment. We'll end up with our pets.com of the LLM age, the whole thing will implode, and the few companies that were actually doing useful stuff with LLMs will survive. reply marcosdumay 1 hour agorootparentExcept that LLMs have way worse unitary economics than the web or a phone's app store. What comes back to the old data-inefficiency of machine learning. There hasn't been visible improvement on this, and it is looking more and more as a fundamental limitation of AI. reply jsight 2 hours agorootparentprevAre those moments really that different? Motorola was practically the Netscape of the iPhone era, as those early Droids were everywhere. There were tons of others too, then it all imploded with only a few companies really surviving in the smartphone space. reply freejazz 1 hour agorootparentYeah, they are. I'm using an iPhone now. reply randomdata 3 hours agorootparentprev> ChatGPT was an iPhone moment. A Blackberry moment, perhaps. There appears to be something there, some groups are latching onto it and deriving value from it, but we haven't yet seen the iPhone come along to transform that initial interest into something that sweeps the world. reply kranke155 5 hours agorootparentprevI would press X to doubt just because of profitability. It’s cute that we now have image and video gen AI. Also we have now Turing test passing chat bots (Id say). But although they are very impressive, and I know lots of people who use them for various tasks, I haven’t seen a “killer app” yet. For iPhone the killer app was making calls. It was the best phone you could get. Then it had apps. It was undeniably better. LLMs are good at a lot of things, but they don’t seem to excel any particular task - yet. I’m not sure they are a revolution yet. I’d say they’re more of Macintosh moment. A hugely useful technology no doubt - but useful for what exactly? For Mac it was desktop publishing. reply JKCalhoun 5 hours agorootparentI agree generally with what you're saying but feel you were all over the place in your comment. The killer app on the iPhone was not \"making calls\" — I suspect instead it was Safari, the other 1st party apps, the touch screen and the slick integration of all of that to make it a no-brainer device that even my mom and dad could use (they were approaching their 70's when the iPhone debuted). Your analogy that ChatGPT (or LLMs generally) are more akin to the Mac feels close to the mark to me. Your comment about the Mac's killer app, desktop publishing, suggests that LLM's killer app will follow, just hasn't arrived yet. The analogy is a little shaky though since, some would argue, it was the laser printer (plus the Mac) that kicked off desktop publishing. reply kranke155 35 minutes agorootparentYou're right it is a bit all over the place. \"The killer app is making calls\" is me quoting Steve Jobs on the Iphone 1 presentation. I get that it doesn't sound true now, knowing all we can do. It's true Iphone was a lot more, but that was his conviction at the time, and I think it makes sense. Their aim was to make the best phone in the world. I also think yeah, it is a bit like Macintosh in the sense that this is a new general purpose technology, and I'm not sure we've really figured out what's going to the most transformative about it yet. reply paulluuk 5 hours agorootparentprevI'd say the killer \"app\" for the iPhone was the touch-screen. There were plenty of other phones that could be used to make calls, at the same quality for a lower price. Frankly, I still find the iPhone to be way too expensive for what you get in return. For LLMs, the \"killer app\", for me, is already here. And there's two of them right now. The first is the chatbot (like chatGPT or Pi or Claude). Having someone who you can just ask for any kind of information, from book recommendations to hypothetical space travel situations to advice about birthday gifts, and to get answers that are better than what I'd get from 90% of real humans, is huge to me. The second one is the coding assistant, in my code copilot. It has made me at least twice, if not thrice as productive as I was before. reply TheOtherHobbes 4 hours agorootparentA killer app in the like-an-iphone context is something that provides obvious value - if not outright delight - to a huge demographic. Coding doesn't do that, because the demographic interested in coding is not huge compared to the rest of the population. Chatbots don't do it either because they're too unreliable. I never know if I'm going to get a recommendation for something the LLM hallucinated and doesn't exist. There's also huge cultural resistance to AI. The iPhone was perceived as an enabling device. AI is perceived as a noisy, low-reliability, intrusive, immoral, disabling technology that is stealing work from talented people and replacing it with work of much lower quality. It's debatable how many of those perceptions are accurate, but it's not debatable the perceptions exist. In fact the way OpenAI, Anthropic, and the others have handled this is a masterclass in self-harming PR. It's been an unqualified cultural disaster. So any killer app has to overcome that reputational damage. Currently I don't think anything does that in a way that works for the great mass of non-technical non-niche users. Also - the iPhone was essentially a repackaging exercise. It took the Mac+Phone+Camera+iPod - all familiar concepts - and built them into a single pocket-sized device. The novelty was in the integration and miniaturisation. AI is not an established technology. It's the poster child for a tech project with amorphous affordances and no clear roadmap in permanent beta. A lot of the resistance comes from its incomprehensibility. Plenty of people are making a lot of money from promises that will likely never materialise. To most people there is no clear positive perception of what it is, what it does, or what specifically it can do for them - just a worry that it will probably make them redundant, or at least less valuable. reply InDubioProRubio 3 hours agorootparentprevThe Killer App was the user-interface. There was not tutorial video, there was no long explanations. It was touch and go. And it worked. reply DebtDeflation 4 hours agorootparentprev>For iPhone the killer app was making calls. What? Making calls was the killer app for Nokia brick phones in the late 1990s. The killer app for the first generation of smartphones (Windows Mobile, Blackberry, etc.) was email and calendar. The killer app for iPhone and Android was the capacitative touchscreen combined with the ability to run 3rd party apps (yes, I'm aware there was an extremely brief moment in the history of the original iPhone where Apple opposed this), and 3G mobile internet (yes, again, I realize this came a year after the initial iPhone release). Mobile web browsers and Maps/GPS got the party started. reply pjmorris 4 hours agorootparentprev> Them along absolutely everyone else. ChatGPT was an iPhone moment Nice analogy. My sense of things is that the iPhone was a win for all of its users. While ChatGPT may make some/many of its users more productive (see Ethan Mollick's work), the driving force behind 'AI! AI! AI!' in the corporate world is an executive hope that complacent AI can replace expensive people. That's not a win for all of its users. reply red-iron-pine 4 hours agorootparentprevexcept the iphone delivered. we're still holding our breath for AI reply toyg 2 hours agorootparentManufacturers started pivoting almost immediately when the iPhone debuted. Yes, eventually it delivered, but nobody waited for that before they started aping it. reply sangnoir 2 hours agorootparentprev...or it could turn out to be a 3D-TV moment - the jury is still out. For a while, all OEMs had 3D TV models, and it seemed their ubiquity was inevitable by sheer force of manufacturers ramming the products down consumers throats (like AI). The only debate was over which solution was superior: active or passive. 3D movies are still with us, so the tech didn't completely disappear - only from the consumer space. reply sedawk 1 hour agorootparentprev> Lately it has definitely felt as though Microsoft is resurrecting Ballmer's old meme as \"AI! AI! AI!\" You nailed it! Having spent significant time (as low-level minion) under both Ballmer and Satya, it certainly feels like the old Ballmer-time meme is coming back with the AI! Also with it, the forced-curve ranking that Satya disbanded is being re-instituted under a different name. reply archerx 13 hours agoparentprevWell since Nadella I have been using less Microsoft products and probably won’t be using Windows anymore once my Windows 10 LTSC stops working. I keep hearing praise for Nadella but all he is doing is alienating a lot of customers with his terrible decisions. reply madisp 7 hours agorootparentWith GitHub, TypeScript and VS code I'm probably using more Microsoft products than before. reply raxxorraxor 7 hours agorootparentThey bought it. If Microsoft had developed it, we would get something like sourcesafe (was that the name?). Sure, the investment was quite sensible, although I don't think they can change it for their ambitions too much. Microsofts conquest against open source was of course a wrong strategy of Balmer. reply sausagefeet 5 hours agorootparent> SourceSafe was originally created by a North Carolina company called One Tree Software. reply tylerchilds 6 hours agorootparentprevthis is the funny thing about microsoft they are way better at buying and selling software than ideating and creating it. successful microsoft products are acquisitions. reply benrutter 4 hours agorootparentI agree but I'm not sure it's just microsoft- meta's instagram, whatsapp and quest are all acquisitions of already sucessful products. Oracle are similar. I think, up to a point, and especially in the US where antitrust is pretty lax, it's a very safe investment to just buy other already sucessful companies. reply tylerchilds 3 hours agorootparentThe most glaring example in recent memory would be the amazon monopoly and the evidence i submit is diapers.com with enough money, you can fund your investments to strategically take down every mom and pop. amazon can’t take on every consumer vertical simultaneously, but they used their funds to drive diapers.com into the red, because as a parent you’re scrwed either way and comparing food to diapers, will buy the cheaper diapers instead of the cheaper food. amazon wanted diapers.com diapers.com said, we’re good this isn’t a billion dollar enterprise, but it pays the bills. amazon bought it after making sure they couldn’t actually use it to pay the bills. reply Tostino 4 hours agorootparentprevHell, even Sql Server wasn't originally developed by Microsoft. They have taken it a long way since though. reply meekins 5 hours agorootparentprevSame story with Azure. All the good services are acquisitions, rest is low quality feature catch-up with AWS augmented by a terrible IAM system. reply rbanffy 7 hours agorootparentprevOf those three, the only one that drives revenue to MS is GitHub. reply cjblomqvist 7 hours agorootparentNot true, at least not according to MS themselves. MS have done several studies and adoption of these tools drive adoption of Azure. That's why MS invests in it. They've been quite clear about this. The one platform/OS was Windows. The new platform/OS is Azure/Cloud. It's almost like saying Google doesn't make any revenues from search, only from selling ads. reply actionfromafar 6 hours agorootparentprevVS Code is also low-key keeping Windows relevant as a developer OS. If something else came along which was truly very excellent but was only working well with Linux, and VS Code was not there to be the de-facto go-to solution for most new devs, it could eat away more of Windows marketshare. So I see VS Code as a slight moat, also in its promulgation of dotnet-isms. So I think VS Code drives some revenue Microsofts way in a pretty diffuse but real way. reply KptMarchewa 6 hours agorootparentI'm not sure how it's improving Windows relevancy, second most popular IDE group - Jetbrains ones - are on Windows too. reply actionfromafar 5 hours agorootparentThat's why I wrote slight. VS Code is more of a backstop to make sure developing on Windows doesn't suck. Don't let Windows fall behind kind of thing. Every cross platform thing is biggest on Windows by default because Windows is the biggest platform. reply makeitdouble 5 hours agorootparentprevVSCode Server and other remote dev servers are a big deal. Before we had to sync or mount a remote partition to manage the gap between Windows and the *nix server. I remember just plain using vim over ssh to avoid the hassle. That pain existed under macos and linux as well, but to such a lower extent as you could do so much more locally. While Jetbrains does it too, VSCode being strong guarantees it stays a viable path in the future. reply fakedang 6 hours agorootparentprevHow is VS Code a moat when it's platform agnostic? Plus the developer market is just a fraction of the overall market. MS Office is the real moat, as is Windows XP/7. Everyone use MS Office because Google Slides/Docs/Sheets is a silly contender to the MS Office suite. Windows XP/7 because that's what a huge percent of the human population using computers grew up on today, so they're most familiar with it. And let's be honest, that's not going away, even as MS enshittifies Windows 11, simply because no Linux build can apparently mirror the Windows XP/7 UI (for some reason, not even Mint) while Apple is hell-bent on doing its own thing on the sidelines. The day MS breaks Office suite is the day Microsoft goes down, but that's unlikely because the current crop of devs at MS don't even know how to get started. Microsoft could literally not do anything and still make tons in revenue. reply rightbyte 4 hours agorootparent> And let's be honest, that's not going away, even as MS enshittifies Windows 11, simply because no Linux build can apparently mirror the Windows XP/7 UI Windows 10/11 does a really bad job at emulating XP/7 UI. It is about as foreign to XP users as Debian or whatever. I made a XP VM the other month to run some insane software I had to run at work. I felt so much at home. It was so nice. Everything was awesome. The control panel was awesome. The distinct buttons were awesome. The start menu was awesome. The 'My computer' at desktop root was awesome. All in muscle memory, still. Then I am back out to 10 and can't figure out where my app shortcuts are without knowing their name or what of the 3 or 4 different control panels I am supposed to use. reply znpy 6 hours agorootparentprev> With GitHub, TypeScript and VS code I'm probably using more Microsoft products than before. cool, how much money have you paid to Microsoft to use those? Except for Github (which they bought, by the way) probably not much. And github has some serious competitors (Gitlab which is just great and to a lesser extent, bitbucket). reply StableAlkyne 7 hours agorootparentprev> he is doing is alienating a lot of customers with his terrible decisions Windows doesn't even make up 1/5 of their income, and in contrast a bit over half of their income is Office and Cloud* The real money is in enterprise IT and cloud services. The average consumer doesn't keep their prebuilt computer long enough to buy another version of the OS. They don't need to keep a niche within a minority (privacy-oriented customers who would buy an OS) happy with Windows to continue drowning in revenue. It seems like he has done a fantastic job, if the goal was to decouple their fortune from Windows. *Based on googling and a lazy reluctance to dig through their earnings calls reply _thisdot 6 hours agorootparentIt's a mystery to me why they haven't made Windows free yet. Surely they make much more money from users using Windows than buying Windows reply gtirloni 5 hours agorootparentOnly PC enthusiasts buy Windows. 99% of the population gets it bundled with their computers and who knows how much MS is charging those OEMs. Probably pennies. Windows already has a de facto monopoly in desktop OS. They don't need to be nicer and give it for free to get more market share. They have all market share they every will. reply saghm 5 hours agorootparentprevIt basically already is, at least for consumers. You can download an .iso of whatever the latest Windows version is and install it, and although it will prompt you to put in a product key, nothing stops you from continuing to use it if you don't. You can't customize certain cosmetic settings, and there's a small watermark in the bottom left corner, but it's hard to imagine that it being fully functional otherwise is an oversight rather than something they're fine with. The only people who will go through the effort to install it like that and keep using it are the ones who are least likely to pay for it. reply nilamo 4 hours agorootparentThis is true: my gaming PC had that watermark for nearly 10 years. You can't change the wallpaper, remote desktop doesn't work, but that's the only downside to not paying for windows (and using Microsoft's free iso, instead of pirating a key). It's quite clear to anyone who's tried it (at least since Win10), that Microsoft does not care at all if you pay for Windows. reply hiatus 5 hours agorootparentprevIt comes preinstalled on most computers. The consumers don't pay, OEMs do. And they'll continue to pay because most people don't want an OS-less machine. reply codegeek 2 hours agorootparentprevSince Nadella took over, Microsoft stock has gone up from $30 to $400 with a market value of over $3T. Satya understood that for MS to compete, they have to get out of the \"Windows Only\" mentality. For example, .NET Core was a huge thing when it finally came out. I don't think that he has made any terrible decisions for the company. May be for some users like you, sure. But not for the company overall. reply juped 12 hours agorootparentprevBallmer would never have put honest-to-God advertisements in Windows Solitaire. reply squarefoot 6 hours agorootparentAs a 100% Linux user with good memories of that era (flying chair, Linux==cancer, etc.) I may be the best person here to actually defend Ballmer, having for sure no hidden interests in doing that. Everything changed in those years: Google was cool, Linux desktop almost non existent, cryptocurrency not even in the head of its creator, AI was a myth and the best voice recognition could offer was the hilarious \"double the killer\" demonstration [1]. How can we compare CEOs actions separated by two decades? Ballmer did what was perceived as useful for its company back then just as Nadella is doing that now. Perspectives have changed, hence companies and their CEOS had to adapt. I'm 100% sure that if Ballmer were MS CEO today, he would include advertisements as well, as today putting advertisements in every free corner of the known Universe is perceived as acceptable, if not necessary, which was not the case 20 years back. [1] context: https://www.youtube.com/watch?v=kX8oYoYy2Gc reply rightbyte 9 hours agorootparentprevI always had the feeling MS was squeezing competitors and software vendors, not users directly. The user hostility have made me move me to Linux systems. reply spacechild1 9 hours agorootparentprevLet alone ads in the start menu! reply DaiPlusPlus 8 hours agorootparentTo be fair, Windows 98 came with almost-ads in the stock Active Desktop wallpaper - and promos for AOL/CompuServ/Prodigy. reply znpy 6 hours agorootparentAOL was stuff that people actually used however. It wasn't \"random stuff\". For many people in the 90ies it was like the brand \"AOL\" was a synonym of \"internet\". reply 1980phipsi 8 hours agorootparentprevOr screw up search on the start menu reply 486sx33 3 hours agorootparentprevExactly reply rowanG077 7 hours agorootparentprevThis is true for me as well. I do have a VR gaming machine which I don't think will linuxify soon but I would if I could. Nadella has grown Microsoft no doubt. But in the process has trashed Windows. One of the most valuable pieces of software. I wouldn't be surprised this will bite them in the long run a lot. reply globalise83 7 hours agorootparentOur company is absolutely full of Microsoft products (all the Office 365 stuff, PowerBI, Azure, Microsoft SSO etc. etc.), yet most of our teams use Macbooks. Windows is no longer a necessity to work in a mostly-Microsoft environment, and that strategy is making Microsoft fabulous amounts of money. reply gtirloni 5 hours agorootparent> yet most of our teams use Macbooks. Exactly this. Today I can switch from Linux to macOS to Windows and 99% of what the average users does can be done in the browser. Worse, in a smartphone. So it was very smart of Microsoft to realize Windows was going to stop being a hard requirement for most use cases. reply raxxorraxor 6 hours agorootparentprevTools like PowerBI are quite good, the data pipelines are amazing, but at the end of the chain Microsoft always makes mistakes so that something good like PowerBI will only remain an advanced Power Point version. If you go a bit deeper the platform is fairly locked down behind artificial restraints. Azure has good parts, auth with Microsoft is perfect for software in the office world and goes beyond the usual LDAP Active Directory. But on the other hand it is quite slow to a degree that it really affects productivity. The damage is probably in the billions/trillions for their many customers. That is the real price of office cloud versions. reply cameronh90 5 hours agorootparentprevIndeed. Conversely, Apple are the ones now forcing you to buy into their walled garden if you want to support users on their devices. We are a mostly Windows+Linux shop, but we need Macs to build and test iPhone apps, investigate issues with Safari on iOS, do certain iPhone support tasks, etc. reply rowanG077 2 hours agorootparentprevSo you agree or disagree with me? I'm not sure how this is a response to my comment? reply fluoridation 3 hours agorootparentprevSorry, but how is that a response to what the GP said? It was not necessary to keep making Windows worse and worse to decouple it from other MS products. reply ozim 5 hours agoparentprevI would argue that specific technologies changing is super relevant fact. In 90's and 00's \"everything Windows\" made loads of sense for a company so being hard on any competition was the right thing. Also I don't see people saying it about MacOs you cannot do software to this day for MacOs or iOS without having actual device and operating system from Apple. What changed for MSFT was that operating system in 2010's and forward became irrelevant. Cloud is where the money is and now MSFT is \"all in Azure or nothing company\", entire company is designed to promote Azure and O365. To properly promote Azure they need to run Linux on that cloud and they need mind share of developers that will develop products using Azure - earlier they could force developers to use Windows because that was where software was running. reply ThrowawayB7 13 hours agoparentprevExcept Steven Sinofsky, longtime head of the Windows division and one of the internal forces preventing Microsoft from going in alternate directions, was pushed out under Ballmer's tenure, not Nadella's. Granted, Ballmer made the mistake of putting Terry Myerson, who headed up the failed Windows Phone effort, in charge of Windows but that's another story. reply lenkite 9 hours agorootparentWindows phone was damn good and was growing in popularity when Nadella came in and killed it. When you are #3 in a market, you need persistence to win. One cannot expect immediate, massive profits in a saturated market. Yet, Windows phone by itself was a growth multiplier for Windows which Nadella annihilated in order to turn Microsoft into a cloud & ad services company. reply nashashmi 4 hours agorootparentExcept from a project management standpoint, if you don’t have a vision for a project, the people on that team would get up and leave. And there was no short term vision for the phone in the face of android and iPhone. The long term vision did not have team buy-in. And then further the phone was a distraction for all of the other teams who were expected in someway to provide some software that would work on there as well as android and iPhone. I agree that the phone would have been great … at some point. But in an MBA world, it was a liability reply HarHarVeryFunny 4 hours agorootparentprevWindows Phone is surely symptomatic of Balmer's milking the cow rather than innovating approach. A smart phone is not a small desktop computer - it needed a complete rethink of user interface as Apple had done. It's also a bit strange that the success of Windows was based on the ubiquity of clone PCs rather than single vendor, yet Microsoft instead tried to follow Apple here and let Google become the \"clone PC\" (Android phone) OS supplier. I can't fault Balmer for at least trying to get a slice of the pie by belatedly putting out me-too products like Bing and Azure, but it's precisely because of Microsoft/Balmer missing the importance of the internet that it was put in position of being follower rather than leader. Microsoft is really a bit like Intel in having totally dominated a product category, but then having missed on most of the major industry trends they might have been expected to lead on (for Microsoft, internet, mobile and AI; for Intel mobile, gaming and AI). They are lucky to have had a second chance with Nadella who seems much more in tune with industry trends, willing to rapidly pivot, and who seems to have made a masterful move with their OpenAI partnership in buying time to recover from an early lack of focus on AI/ML. reply lenkite 4 hours agorootparentThe user interface did have a complete re-think. Windows phone popularized tiles and live tiles which was not just innovative, but an order of magnitude easier and more ergonomical compared to icons, esp for older people. The comforting common-cross-app back button, the metro UI, the smooth performance, ability to store all apps on SD card, best phone keyboard of that era, integration with windows PC - they had the bare-bones down fine. But simply gave up after a few years, instead of incrementally improving. I thought it was a bad mistake to bow, kneel and surrender the smart-phone market space. Today, I am fully convinced it was a critical, life-threatening mistake as more folks move to the Apple ecosystem - buying both iPhones, Macbooks and Apple Watches because of a fully-integrated ecosystem. The funny thing it was Microsoft who popularized Continuity, but after they gave up due to lack of willpower, it was Apple who took over, executed better and won. Really frustrating to see the state of Windows OS and device market today. reply chucke1992 3 hours agorootparentprev> for Microsoft, internet, mobile and AI I don't think they missed AI boat. Their culture would not have allowed them to create OpenAI, but they were fast to leverage their moat and push AI into their office and windows suites and azure. Hell, they are even trying to catch up with search using AI and are trying to push Azure for various AI startups and stuff. MSFT rarely leads on anything - arguably even Windows is something they created being inspired by something else, while not going deep into hardware. Which what became the undoing of IBM. They are much better at being second. Azure - they were behind AWS, but not as late as Google. I bet with Satya, even with mobile they would have grabbed Nokia much earlier and pushed Windows Mobile before Android took off. AWS missed AI boat though. reply HarHarVeryFunny 1 hour agorootparentAmazon have a very close relationship with Anthropic, which seems like a good match (Anthropic focus on business use), and win-win. Anthropic gets access to the compute they need, and Amazon get AI to integrate into their AWS offerings. I don't know the contractual basis of the relationship, but it seems this has to be pretty long term and strategic. A significant part of the competitive advantage of one AI vendor over another comes down to inference cost, which in turm comes down to customizing the model architecture for the hardware it is running on, which in this case either is, or will be, Amazon's home grown Graviton processors. reply chucke1992 45 minutes agorootparentThe problem with Amazon is not their closeness to Anthropic, but more of the fact their moat is not big enough to integrate AI in a way MSFT can. Even their Azure services somehow feels natural with AI support. I don't know if Satya predicted it or not, but their push into open source and Github acquisition were very helpful for AI. reply rbanffy 7 hours agorootparentprev> you need persistence to win. You also need a plan. How would Windows Phone displace either Apple or Android? reply raxxorraxor 6 hours agorootparentIt wouldn't and it wouldn't need to. The decision was still very likely wrong, especially transparent after Apple proved with silicon that ARM platforms can be that competitive. Windows wasn't ready here and platform interop wasn't at all it strength. If Windows phones would have had an emulated x86 mode, many people would have bought it instantly due to the momentum that now steadily decreases. There can be solid business revenue if you are \"just\" #3 and the experience with development is very valuable. Although it is true that Microsoft and hardware has always been turbulent, with partners or without. Sometimes they simply created the best products in their class with a lot of margin, sometimes they basically sold scrap. reply actionfromafar 6 hours agorootparentThe entire mobile market was immature back then, people didn't expect much interoperability and Windows Mobile 7 Nokias were slick and faster than iPhone or Android. They could have become the \"contrarians luxury\" if you didn't want to just get an iPhone. A bunch of hardcore Microsoft fan developers were gearing up to develop for Windows Mobilet dotnet when Microsoft changed the APIs with Mobile 8 (IIRC) and this dedicated bunch of developers just dropped the platform and just embraced Android or iOS instead. reply mysterydip 6 hours agorootparentprevJust a spitball idea, but rather than focusing on the consumer market, they could've been the new blackberry for businesses (that give employees phones). Native active directory and group policy integration would be a good solution for the myriad of third party apps/services/devices that attempt to control the other phones. reply actionfromafar 6 hours agorootparentFor sure. Enterprise mobile was not really a thing back then. (Laptops with VPN was state of the art.) Microsoft could have organically owned the enterprise mobile market but chose not to. reply gtirloni 5 hours agorootparentprevOpen source has a lot of momentum in Microsoft now but it wasn't the case when Windows Phone was released. Had they made it open source, it would have been a different story with Android and Windows Phone fighting to win the OEMs. But that ship has sailed. Unless there's a paradigm shift in smartphones (doubtful), we're stuck with Android and iOS for the foreseeable future. reply trympet 2 hours agorootparent> Had they made it open source That would have necessitated open sourcing Windows reply kaon_ 10 hours agorootparentprevI really wanted windows phone to be a success and am still sad it wasn't. I loved the interface. The native integration between my desktop/laptop and phone would have been great. Nowadays with so many apps being PWAs and built with nativescript or ionic, maybe windowsphone has a chance again? I have no idea tbh. reply baxtr 8 hours agorootparentI didn’t like the UI at all. A lot of unnecessary animations. It felt like a forced departure from the iPhone standard, just so that it’s different. reply wombat-man 4 hours agorootparentprevFeels a bit late at this point. Surface Pros run snapdragon but it still feels like too much of a lift to spin up an entire new mobile OS. I'd be pretty intrigued but they're still struggling to nail the tablet market imo. reply iforgotpassword 12 hours agorootparentprevNot just that everything was going through windows as GP said, whatever market they entered, they acted like their product will be like windows in that sector too from day one. Zune was like that, but the best example is windows phone, version 8 more precisely which is the first proper modern smartphone version. Google realized that if they want to stand a chance in catching up to the iPhone, they need to shove android in people's faces, and lure in devs. Microsoft entered the game (WP8) when android already had a foothold, making it even harder. They started with a mostly empty app store, and while they were clever enough to make sure the most widely used apps would be available by effectively bribing those big companies to develop windows phone apps, they pretty much gave the middle finger to all the small indie devs. I remember when android 2 was around I just downloaded android studio and played around a bit, making a simple scrobbler app for my Samsung device. Sideloadong was king back then, but even up to this point I had to pay zero bucks and jump through no hoops to try this out. I don't remember what putting this on the Google play store would've cost me back then, but not much. The windows phone experience was: sign up for a dev account to download visual studio with WP support. Start up VS, asked for your account again. I think in the beginning this was actually a paid account, probably because apple did it that way and again, you're Microsoft so act like you already own the place. But later in they reversed course here at least and you could log in with a free account. So you start building a small test app and then you want to run it in the shipped emulator but surprise! Your laptop only shipped with windows 8 home which doesn't include virtualization features, so tough. So the only way to test the app was to push it to your phone, which was another overly complicated mess where your phone had to be in developer mode and you could only \"sideload\" one app at a time, iirc. The result was an app store with mostly tumbleweed. Whatever small utility or gimmick you wanted, when on android a search would give you dozens of results, on WP, there was maybe 4, and 3 of them almost unusable and abandoned. I'm not blaming ballmer for having decided this specifically, but holy hell how did this pass any meetings with the higher-ups? You're uo against two tech giants who have a head-start of a few years, you try to get people to switch to your platform by being pricey, having no apps, and being hostile to smaller devs? The same played out with all the phone makers, who had to pay license fees for WP when android was free to use. Guess which phones were cheaper in the end. And when Microsoft bought Nokia, Nokia had the unfair advantage of getting WP for free, making it even less attractive for others to compete in that sector. And let's not get into the botched Nokia acquisition because I also don't think this can be blamed on ballmer that easily, or primarily. reply creesch 10 hours agorootparent> The windows phone experience was: sign up for a dev account to download visual studio with WP support. Start up VS, asked for your account again. This is something that Microsoft still struggles with. Some things have improved, but a lot of the dev experiences on the Microsoft side are still cumbersome and not aimed at small time devs. My experience here is with browser extensions and publishing these for both old Edge (pre chromium) and the newer Edge. The entire publishing dashboard is/was overly complex and assumes you are either a single person or a big organization with (azure) AD set up. With Mozilla AMO you can just add individual developers to your extension by mail, and with Google it is as easy as setting up a group. With browser extensions specifically (and Edge as well) you can also clearly see where it is still a dedicated motivated internal team setting things up and where things were handed over to more general teams and support was also outsourced to somewhere else. Anyway, my main point is that even now, many years later, Microsoft still struggles in this area making me think this is more fundamental to the company culture and way of operating. reply Hypergraphe 7 hours agoparentprevI'm not sure that devaluating Windows is a good strategy at all... reply mike_hearn 3 hours agorootparentIt's not a strategy, it's a recognition that the Windows org has decayed and they apparently don't know how to turn it around. Apparently simple projects take forever, new code they launch is often filled with bugs, different parts of the org don't talk to each other and they can't explain why anyone should write an app that targets the Windows API. I support customers shipping apps to every platform and Windows is nowadays 90% of the pain, it's worse even than Linux. Microsoft just don't care either, you can tell the devs who work on it are overwhelmed by the sheer size and tech debt levels of the codebase. Decades of compounding bad decisions have well and truly caught up with them :( This is a pity in a way, the desktop OS market could use more competition. Nadella de-prioritizing Windows was the right thing to do for the business because it had a monopoly, so after PC sales saturated the market the best they could achieve was treading water, but also because the strategy of tying everything to Windows assumed the Windows team would continue to execute well and these things would all be mutually reinforcing. In the 90s Windows did execute well but by 2010 that had stopped, and so the tying strategy also had to stop. A better CEO than Ballmer could possibly have turned the Windows situation around and avoided the need for the disconnection, but instead it was left to drift. reply ozim 4 hours agorootparentprevIt is in \"everything is cloud\" and \"most of software runs in browsers anyway\" world where operating systems don't matter . I would not say it was by any means one or the other CEO \"insightful\" choice but it was more of market choosing on its own. Microsoft had to make own cloud or die that was the choice and better to put loads of investment in that. Ballmer started Azure because Amazon of course was first and Google did the same so Nadella was just playing cards he was handed by the world. reply chucke1992 2 hours agorootparentprevThe thing is that OS is not important these days as you can apps on thin clients now and a lot of folks are spending most of the time within apps and doing nothing else. reply belter 7 hours agorootparentprevBad strategy for Microsoft but clearly a wining strategy for the World. reply rbanffy 7 hours agorootparentSelling licenses is not where the money is. Selling subscriptions to corporations so that every corporate-supplied computer (including Macs) pay Microsoft for something, be it Office or a full Windows+Office+Sharepoint license. All things considered, they can give Windows for free and they'll still profit from it as an enabler for further Microsoft lock-in. reply layer8 2 hours agoparentprevAs an end user, I lament the devaluation of Windows and the general drive to cloud-based solutions. It has made everything worse. reply dataflow 15 hours agoparentprevI don't know anything more than the next guy here, but just reading this, it seems like a really underrated and insightful comment. Thanks for explaining it so clearly. reply anilakar 8 hours agoparentprev> The problem with Microsoft was everything went through Windows. The entire company was designed to promote Windows. ...and nowadays Windows is designed to promote their cloud subscription services while local features get axed. If Google is not allowed to link directly to Maps, there is no way Microsoft can be allowed to advertise their paid services everywhere in their OS. reply chucke1992 3 hours agoparentprevYeah. It is basically a trap that every CFO who became CEO step into - tie everything to a single thing. With Satya he had much broader vision. reply jayceedenton 11 hours agoparentprev> a red headed step child Very good point, but please stop using this phrase. reply legitster 20 hours agoprevHaving spent some time at the Microsoft campus, I can tell you this is basically the consensus view from employees today. Ballmer was not a cool, trendy, or fun CEO who people rallied behind - but he more or less \"got the job done\". He was the captain of a massive ship with a turning radius the size of a continent guiding it through icebergs. Azure's success was specifically set in motion under Ballmer. Owed to the fact that it was developed to Microsoft's strengths (enterprise support) that it didn't piss off too many of their partners and sales channels. Same with Office 365 and all of their other successful services. None are glamourous - but all are impressive with how not awful they are given their design constraints. Even things like Surface, while considered a failure, did its intended job of getting hardware partners to get their act together and make better consumer products. reply vjust 16 hours agoparentBallmer hated Linux & open source. He would've driven their cloud division to the ground trying to sell Windows servers in the cloud. It would've taken him another 20 years to accept that Linux was key to the cloud. VSCode (Visual Studio Code) - would never have taken birth. Microsoft survived and thrived once Ballmer had no option but leave. In this era of Python development, Microsoft Windows still feels a step or two behind as far as using a Windows laptop for coding in the cloud. Python is the language of AI - not Asp.net, not C#. Ballmer would never have seen the writing on the wall. He would've pushed something wierd, like VBA . reply WorldMaker 3 hours agorootparent> He would've pushed something wierd, like VBA . That was Bill Gates. Bill Gates founded the company on BASIC and seemed to remain a fan of the language even as the rest of the world moved on to other languages. Ballmer wasn't technical so appeared to have no skin in the game of which language \"won\", so long it was Microsoft Developer Tools like Visual Studio developers used to work on it (and what would become VS Code, which as many point out did start under Ballmer's tenure). That \"Developers! Developers! Developers!\" meme was directly an \"I want to support developers wherever they are and however they want to work\". Sure he was a huge Windows cheerleader and would want those Developers working on Windows machines, but he really did seem to want to see Windows be the best platform for developers to code for anything (including/especially the cloud). In terms of Python specifically, IronPython was active and interesting during Ballmer's tenure and Ballmer helped form a team that was actively contributing to open source projects like Python (and Node and Redis and others) to make them all run better (sometimes much better) on Windows. Ballmer may have been afraid of open source as a business model, but he also seemed to realize the usefulness of open source for bringing developers (back) to Windows and he did start efforts in that direction. reply metadat 16 hours agorootparentprevI have as much disdain for the monkey man as the next OSS fan. But VSCode was always closed sourced crap at the arbitrary whims of a soulless zombie corp, and they never promised otherwise in a significant way. It's not relevant and not a good foundational signal or basis for any argument. reply solarkraft 10 hours agorootparent> they never promised otherwise in a significant way It’s commonly promoted as „open source“ and this seems to be commonly believed. Pretty much everyone I tell that the official builds of VSCode are proprietary (and how proprietary they are) is pretty surprised. reply smolder 8 hours agorootparentprevThere's a working build of just the open source part of VSCode (with basically all the same functionality) called VSCodium reply Karrot_Kream 11 hours agorootparentprev... https://github.com/microsoft/vscode? It's MIT licensed. Or are we here to start GPL vs MIT for the 10,000th time? reply ensignavenger 7 hours agorootparentThat is Code OSS, MS official binary builds of Visual Studio Code, as explained at the top of the Readme, include proprietary code. MS also has several very popular proprietary extensions. Some of those extensions, older cersions were open source. reply signa11 10 hours agorootparentprevoh please :) the old ‘embrace-extend-extinguish’ model is what it _truly_ is, f.e. , you cannot take extensions from m$ store and use it. there have been large number of discussions around this topic, and folks have highlighted these concerns more articulately than i could ever hope to do. take your pick. reply cypress66 7 hours agorootparentIdk, I use cursor which is a proprietary commercial VS code fork and it just works. So clearly the license/OSS situation is very workable. reply kristiandupont 7 hours agorootparentprev> ‘embrace-extend-extinguish’ model is what it _truly_ is With this mindset, what could MS possibly create that wouldn't make you say this? reply mynameisash 3 hours agorootparentThis definitely seems like an unfalsifiable proposition for the MS haters. Microsoft does something shitty? See, they're a terrible company. Microsoft does something awesome? Well, we're currently in the \"embrace\" or \"extend\", so they're a terrible company. I'm as (or more) pessimistic than the next guy about the state of tech and capitalism, but at least give credit when and where credit's due. reply coretx 11 minutes agorootparentThe destructive EEE strategy is replaced by a constructive poisoning the well strategy. That's arguably moral progress while there is no legal or financial incentive to do so. That's praise for Nadella, not Ballmer. signa11 1 hour agorootparentpreva previous discussion about something similar happened here: https://news.ycombinator.com/item?id=25719045 reply elzbardico 7 hours agorootparentprevBallmer didn't hate linux and open source. He feared it as a threath to Microsoft's business model and revenue streams. reply dymk 7 hours agorootparentThat’s a distinction without a difference reply bsuvc 6 hours agorootparentprevThat's just why he hated it. reply wslh 9 hours agorootparentprevYou can run Linux servers on Azure (and Hyper-V), so it’s worth taking the ‘hate’ against Linux with a grain of salt. reply blackeyeblitzar 16 hours agorootparentprevActually there was a lot of open source happening under Ballmer - not because of him but in that time. VSCode’s beginnings were in an earlier similar product were from that time. He didn’t interfere or stop those projects. Attributing that to Nadella is just false. reply snowwrestler 17 hours agoparentprevThis is hindsight bias. Because other people took some of his later initiatives and made them successful, it’s tempting to look back and grant him these as wins. We should resist that temptation and judge him on the results he delivered. MS was the essential tech company, king of the world, and under his leadership their innovation stalled, they lost in markets where they were leading, the stock stagnated, and huge piles of money were vaporized on acquisitions that were poorly planned or executed. He tried to buy Yahoo for $44 billion! Only Yahoo’s greater idiocy saved him from that gargantuan mistake. And that was just one of many. reply nl 14 hours agorootparentWould Yahoo under different management have done better? Yahoo.com remains the 8th most visited website on earth[1] (I had no idea until I read that on HN some months back). It sits between Wikipedia and Reddit. [1] https://www.similarweb.com/top-websites/ reply toast0 12 hours agorootparentWell, I think the Bing search deal would have been a lot different if Microsoft had owned Yahoo. Yahoo management was looking to reduce the cost of running web search and advertising platforms, but ended up still having a large expense to crawl the web and basically do web search in order to enhance Bing results. And then the Microsoft ad market managed to be worse in all sorts of ways (for advertisers and publishers) compared to the existing yahoo one, plus Microsoft took a cut of the revenue. Some of that should have been better if it was one company; plus, I bet Microsoft would have sent Yahoo employees an Xbox360 or something. (I worked for Yahoo Travel from 2004-2011) reply legitster 16 hours agorootparentprevHindsight works both ways. Developing OSes and software was clearly an unsustainable business. It's obvious in hindsight that cloud infrastructure was the way to go. But at the time placing a lot of different bets to find a few successful product-market fits was the best you could ask for. reply snowwrestler 13 hours agorootparentTreating his tenure as just a bunch of vague bets that didn't pan out does not give Ballmer enough credit. He was a hands-on leader responsible for how MS executed, which had a direct impact on product success or failure. MS did not just have bad luck, they lost to competitors. reply dangus 16 hours agorootparentprevWhile it may be true that the OS itself isn't really a cash cow anymore (if it ever was), I still think Microsoft's greatest failure of the previous decade was exiting the smartphone OS space and ceding it to Google and Apple. I think that Ballmer's management can take a lot of blame for that. I think a different CEO could have executed and possibly have kept Microsoft in that market with success. The Apple App Store by itself is a trillion dollar ecosystem. Microsoft being able to gain even a sliver of that size would be worth quite a lot. We might give Apple similar criticism on the other side of this coin by saying that it's somewhat insane that Apple hasn't tried entering the public cloud market, especially given the fact that they now design their own ARM processors that are essentially the market leaders in that architecture. reply toast0 13 hours agorootparent> While it may be true that the OS itself isn't really a cash cow anymore (if it ever was), I still think Microsoft's greatest failure of the previous decade was exiting the smartphone OS space and ceding it to Google and Apple. I mean, Microsoft was too early and too late on smartphones. I never cared to look into the pre WP7 history. But the more recent Windows Phone died with WM10, which I don't think is fair to blame on Balmer. WM10 came out in public beta in Feburary 2015, and Balmer was replaced in February 2014. Microsoft eliminated their legendary testing program in August 2014, and the WM10 betas and release in November 2015 had very poor quality. On my phones, I had to choose between annoying bugs in notifications in WP8 or WM10 with a subpar, laggy experience with mobile Edge that managed to be worse than mobile IE. They did manage to get a decent final release together in 2020, although mobile Edge was still crap. You can blame Balmer for not letting Firefox on their app store, I think; a browser that didn't suck would have helped me stay on WP longer anyway. Still, I think Continuum with an x86 phone could have gotten market share, but Intel cancelled atom for phones in April 2016. reply actionfromafar 6 hours agorootparentMy thesis is WP8 was already the first huge misstep where they lost developers. reply MarkusWandel 3 hours agorootparentI tried various \"weird phones\" on my way to standard Android, among them a high-end WinCE phone (Xperia X1a) and a WP8 one (Lumia 520). Make no mistake about it, WP8 was a good mobile OS, even if they did stick the dreaded \"Windows\" name in there. Smooth, reliable, battery efficient, well-thought-out UI. But alas, too late. By then Android had captured the \"not iOS\" market and it would have taken a miracle to bring a third OS to the mainstream. I very briefly tried WP10, and it actually seemed a step backwards; in their desperate attempt to somehow unify the desktop and phone thing, be it in code base or user experience or whatever, they tried too hard. reply dangus 6 hours agorootparentprevYeah, the idea that Nadella killed Windows Phone only makes sense in the context of Windows Phone already having failed under Ballmer. I was a Windows Phone user during 8 and 8.1. There was a short period where I felt like some traction was taking place. My bank even had a Windows Phone app, until they didn’t. Windows 8.1 was the most competitive version against contemporaries, but then the delays and issues surrounding 10 really took the wind out of those sails. My perspective as a consumer is that Microsoft buying Nokia seemed to have made Nokia worse and delayed their phone development process. I found myself without any upgrade path, while Apple and Samsung users could get a pretty significant upgrade in capability every year at that time. Nokia was also better at making low-end phones and had very few flagship products that were iPhone and Galaxy competitors. On the business side Microsoft didn’t focus on having their entire lineup available on all four US carriers. They had all these weird carrier exclusives where getting a new Windows Phone would mean switching carriers. I have to think that the break in compatibility between Windows 7 and 8 really screwed over developer relations as well. On the Apple side they were delivering an experience very familiar to Mac developers, and on the Android side the experience was an open source free-for-all playground. reply actionfromafar 5 hours agorootparentThat's exactly it. There were a bunch of hardcore Windows dev shops getting ready to support Windows Phone who jumped on WP7, ready to dig down, but who felt betrayed when WP8 was a clean break. You lose that hardcore bunch, then you are just evaluated on the generics and Windows Phone had no edge. reply jimbob45 14 hours agorootparentprevDeveloping OSes and software was clearly an unsustainable business. It's obvious in hindsight that cloud infrastructure was the way to go. Cloud infrastructure has become a commodity though and you can replace your cloud provider easily (theoretically, lol). What moat can MS or anyone else build around cloud infrastructure? Compare to OS' where MS may never have had a competitor catch up if they'd kept up speed on their OS teams. Same with video games these days. Adding in digital casinos may seem nice but now you're just the same as every other digital casino offering. reply RandomThoughts3 11 hours agorootparent> Cloud infrastructure has become a commodity though There are only 3 significant providers and the needed investments are a gigantic barrier to entry but sure it’s a commodity. reply D13Fd 17 hours agorootparentprevOne of the points in the article is that he made many bets, some of them panned out really well, others didn't, but on the whole he set Microsoft on a really good path. Buying Yahoo would have been a bet that didn't work out, probably, but I don't think it goes against the point in the article. reply tdeck 13 hours agorootparentprev> under his leadership their innovation stalled, they lost in markets where they were leading, the stock stagnated, and huge piles of money were vaporized on acquisitions that were poorly planned or executed. A lot of this sounds like Google under Sundar's leadership, although I'm not sure if there is a parallel to the failed acquisitions, and some of the rot had set in well before. reply belter 7 hours agoparentprevMemories get a bit fuzzy after 5-10 years.But this take from 2012 is a reminder of what a mess Steve Ballmer was as CEO: https://www.netnetweb.com/content/blog/blog/top-10-reasons-w... reply cyberax 17 hours agoparentprevI remember working with Microsoft as a client in 2000-s, it was awesome. We started as a startup, and enrolled in a BizSpark program. It gave us basically free access to Microsoft tools and with very responsive support. We later transitioned into volume licensing, that also was simple and straightforward. The business side of Microsoft was a streamlined unstoppable train at that point. The technical side, not so much. Microsoft was still trying to be the only software company in the world, and it was pushing all kinds of WPF, WCF, and other WTFs. So they completely missed hyperscalers and the growing market of Linux-based servers. reply addicted 16 hours agorootparentWow. Microsoft Licensing was the stuff of nightmares. You could literally get certifications in Microsoft licensing. There were experts whose only job was Microsoft Licensing consultants. MS’es licensing was so bad you would get different quotes from the same person within a week of asking because almost no one understood it. reply cyberax 12 hours agorootparentSure. MS had tons of resellers with somewhat different markups, although not that different. We needed only the basic stuff: Windows Server, Exchange, MSSQL, a bunch of XP licenses. And this all was straightforward. We also got MSDN subscription basically for free. reply unixhero 16 hours agorootparentprevThey missed the mark on mobile oses and appstores. reply datavirtue 16 hours agorootparentprevWPF is still unmatched. reply cyberax 1 hour agorootparentReact Native is better... reply 71bw 10 hours agorootparentprevExactly and I'm never going to change my opinion. Nothing in this area was ever so easy and yet so powerful. reply eastbound 6 hours agorootparentprev> The business side of Microsoft was a streamlined unstoppable train at that point. Surprising. As a startup I just couldn’t understand how to subscribe to MS Office, seems like it required a hotmail account or something, it always bored me before completing the steps. reply signa11 18 hours agoparentprevdo you think if azure would have even _happened_ if mr. jeffrey snover was not tenacious enough ? reply datavirtue 16 hours agoparentprevSurface is selling like hotcakes. reply jobigoud 2 hours agorootparentHa! It was a trap, they are likely talking about Surface, the big table-computer. It was such a failure that they repurposed the name for something else and you might have never heard about it. We had one at work circa 2013. reply dyauspitr 17 hours agoparentprevAzure happened because of Nadella (who led the project) despite Ballmer. reply dexterdog 17 hours agorootparentAzure may be successful financially, but as someone who has finally used it for the last two years after 15 years of AWS and a little bit of GCP, I can't help but think the world would be a better place if it didn't exist or if some lesser player had that market share. reply Uvix 16 hours agorootparentMaybe it's just \"what you're used to\", but I'd swap Azure and AWS in that statement. Going from Azure to AWS, I found it not nearly as nice to use or easy to understand. Even basic features like \"see all the resources in my account\" were missing from AWS. reply nl 14 hours agorootparentI use all three regularly. AWS has a horrible, inconsistent UI, and the Azure portal is mostly ok (although I think GCP is the best of the three) But OTOH AWS generally works and usually does what you think, whereas I'm never surprised when Azure breaks or some random Azure API works nothing like we expect. reply RandomThoughts3 11 hours agorootparent> GCP is the best of the three Until you have to call Google. Google business services are awful. reply rbanffy 7 hours agorootparentprev> GCP is the best of the three I must have very different needs. In my perception, AWS is the best of the three, Azure is second, and Google would be #3. Depending on your unique news, you might choose different CSP's, from Digital Ocean to Oracle or IBM (the only place you can get AIX, IBMi, and z/OS) reply fragmede 11 hours agorootparentprevI feel your pain, also being on all three. The biggest difference IMO is in how they're handled by large organizations and how prod permissions are provisioned by them. In Azure you have one user account and one org, with subscriptions for your user account to activate to get permissions. You can have multiple subscriptions but they're under the same login/user account and you can have multiple active at the same time. In AWS, you get access to an account or accounts that have different logins, so you get to juggle those with login/logout, even if there's SSO. In GCP, there are multiple projects, under a single login, but you may have to juggle projects. The other aspect is how regions are dealt with. AWS global resource index/search thing is useful, but it totally feels like I spend more time juggling regions with AWS. Azure's regions themselves are, let's just say, interesting. GCP is better at it than AWS, and less interesting than Azure (which is a good thing). reply UltraSane 11 hours agorootparentprevAWS feels fundamentally better engineered than Azure but Azure's GUI and API feels more consistent. AWS has never had the kind of global outage that Azure has had. reply wongarsu 16 hours agorootparentprevFor most stuff Azure is pretty meh, but it seems to have the best features for running Windows servers and integrates well with Active Directory (or MS Entra or whatever they currently call it). Features that I don't need as a startup founder, but that would be very interesting for many places I worked at. Basically the cloud for everyone but the tech companies reply greggyb 16 hours agorootparent> Basically the cloud for everyone but the tech companies And most companies are not tech companies. This is something that tends to be lost in HN discussions (not saying that applies to you, specifically). I've spent a lot of time in the Microsoft world. I worked for AWS as well. In general, Microsoft executes on platform and ecosystem in a way that works very well for a lot of customers. In general, AWS executes on products better, which tends to appeal more to those who are focused on technology, specifically. reply stackskipton 16 hours agorootparentprevAs Azure SRE for a tech company, what feature is missing? We are using AKS with Linux, Blob storage and ServiceBus. Database is MySQL Flexible Server. I'm even using some Azure Tables for backend services just because it was easier to deal with. reply fragmede 11 hours agorootparentI think > Basically the cloud for everyone but the tech companies is referring to the idea that tech companies are competent enough to run their own computers, not that Azure is missing something specific. reply dyauspitr 15 hours agorootparentprevI disagree, I find Azure much easier to set up and use. reply achow 17 hours agorootparentprevScott Guthrie is the one who drove Azure. Dated 2013, a year before Nadella became CEO: https://www.change.org/p/the-microsoft-board-of-directors-as... reply timsneath 15 hours agorootparentAzure existed long before ScottGu took over. It started with dueling projects from Ray Ozzie’s world and Bob Muglia’s world. Ray had great ideas but no idea how to run something like Azure at scale. Bob brought the enterprise mindset and retooled it, and of course Scott owns the lion’s share of the credit for its growth and technical qualities. reply rozzie 1 hour agorootparentI began recruiting for what became Azure in Jan 2006. I was chief software architect / cto at the company. Amitabh Srivastava and the legendary Dave Cutler were the leads, with Dave focused on the hypervisor. (I'd met Dave in the 80's when he was at DEC and I was at DG.) The project was in my team (CSA labs) but was cross-funded behind the scenes by Kevin Johnson, the president of Server & Tools. KJ & I did this because there was passive-aggressive resistance to a 'cloud first' design/architecture philosophy from within his org, where there was a deeply-rooted belief that enterprise servers and ops management tools would adequately scale-up. KJ bought in and was all-in, as was the 'tools' part of his org (Soma & ScottGu). SteveB initially didn't quite know what to make of my desire and myriad efforts to fundamentally transform the company from packaged products toward services, and he had to cope with some of the wake I was leaving. It wasn't all smooth. But he believed in me and helped me to recruit internally, which was essential. My explicit cross-funding agreement with KJ, my peer, was that when I decided it was the 'right time', I'd hand off my Azure org and it would be re-merged into S&T in more-or-less a 'reverse merger', with cloud leadership taking over server. I launched Azure at PDC 2008 with what today we'd call lambda's (functions-as-a-service based on .net) & blobs & cloud database as the core services. Why no linux or windows VMs? They were absolutely part of day 1 plans, but a major political ploy from within KJ's team ('this will kill the server business') resulted in an active decision (mine) to defer until post-launch. It wasn't a technology issue, nor was it an OSS issue; the team believed in OSS & Linux. But shipping was top priority, and we shipped. When I ultimately left the company in 2011, it was time to do the reverse merger that KJ and I had planned. A proven, super-talented manager from Bing that everyone loved, Satya, was chosen to lead the org as it was moved into S&T upon my departure. James Hamilton, the architect of Azure's relational DB, left for AWS. Ultimately, under Satya, ScottGU & co ended up re-plumbing much of the original code with a by-then-ready Windows hypervisor, VMs & Linux, and all that you see today. By then the org finally was aligned and 'believed', and SteveB was genuinely 'all in'. Getting products from 0 to 1 is sometimes a challenging process involving incredible people and stamina from believers at every level. In this case I'd say it was worth the effort. reply achow 9 hours agorootparentprevBobMu left Microsoft because he was not sold on cloud, he was an advocate for 'on-prem' solutions (and for its time it made sense since enterprise customers were against cloud). reply SOLAR_FIELDS 17 hours agorootparentprevWould love some sources cited for both your take and the parent's take. reply FuriouslyAdrift 5 hours agoprevEveryone forgetting about Lisa Brummel and \"stack ranking\"? That nearly ruined Microsoft... https://www.seattletimes.com/business/microsoft-ditches-syst... reply schnitzelstoat 4 hours agoparentI was going to mention this. It was such an awful system and management method. reply bradlys 5 hours agoparentprevWhat does Microsoft do now? Most every major tech company I’ve seen uses stack ranking - even if they don’t use that name. Hell, a lot of startups I’ve been at even use that. The founders and executives love it as far as I can tell - why else would they do it? reply andrewla 4 hours agorootparentAt Microsoft in particular, stack ranking has always been used in the sense of trying to put together a rough ordering of employees at similar levels. But \"stack ranking\" in scare quotes at Microsoft referred to the specific practice of the 20/70/10 rule -- the top 20% were the standouts, 70% was fine, and 10% was \"this person needs to be eased out\". This was applied for any org with more than a certain minimum number of people, and led to a very very toxic review process. reply bradlys 2 hours agorootparentThis is pretty much what I see at almost every company I’ve been at in the last decade though… The review process is always toxic and has always lead to my peers in the industry being more likely to sabotage than help me since that’s the best way to look good in reviews. That with 80% of my peers being permanent H1B means they will do whatever it takes to stay employed. I would be happy to know big tech companies that aren’t doing this but I don’t know any? reply andrewla 1 hour agorootparentI would argue that Microsoft's original practice (without the 20/70/10) is actually pretty good. Have managers make subjective evaluations, merge them together at higher level meetings, and then work out compensation from there. There's a big cottage industry of trying to back everything up with data, to provide actionable feedback, etc., and these end up being giant time-wasting cover-your-ass exercises, which always end in an uncomfortable non-working system for everyone -- \"I did the thing you asked but my review is the same as last year, why aren't I getting promoted\". Mentorship and growth has to be more than just \"here are your goals\". Peer reviews can be okay, but only if you force people to make judgments -- \"rank these three people against each other\" rather than \"give these people a rating 1-10 in each of these five areas\". The subjective evaluation process doesn't work unless you trust your managers, though. And that invariable means that it doesn't scale. reply fsckboy 4 hours agoparentprev\"grading on a curve\" is a good idea, and if athletics wasn't run that way, nobody would watch. that doesn't mean it's easy to implement, manage, or impossible to game, or that it plays nice wrt human factors, but to attack the core idea as essentially wrong is anti math, science, and rationality. Microsoft always suffered from rewarding egotists and political animals over people who did actual work. reply GVIrish 4 hours agorootparent> but to attack the core idea as essentially wrong is anti math, science, and rationality The way Microsoft implemented stack ranking was anti math. You're supposed to measure the data then calculate the level of fit to a distribution, not artificially shoehorn the data into buckets to create the curve. If you analyze the data honestly you may find you have a bimodal distribution, or a heavily skewed distribution, who knows. Stack ranking just clumsily says, I'm gonna give x% a bad score, y% a middle score, and z% the top score. reply fsckboy 2 hours agorootparent>Stack ranking just clumsily says, I'm gonna give x% a bad score, y% a middle score, and z% the top score. as long as the ordering top/middle/bad is preserved, I don't see a problem. there are entire respected statistic methods based on rank ordering, not raw metrics. People don't have a right to fall on a normal distribution. Employers do have a right to grow or trim the workforce, and those numbers are driven by factors that are not necessarily normally distributed. the people who downvote me simply want participation trophys, and \"no\" is the answer. You absolutely can argue that Microsoft pursued a system that hurt both Microsoft and its employees, but not by attacking rank ordering. reply lesuorac 4 hours agorootparentprevAthletics is an actual competition where the expectation is that \"you win\". When you hire 12 baristas are they competing to make the most coffees or is their job to handle customer's orders? If their job isn't to compete with each other then don't stack rank them. Use other metrics like #of incorrect orders or w/e and decide what you think they should've done and if they did more than that give them a bonus. If they do less then maybe you need a new employee. > Microsoft always suffered from rewarding egotists and political animals over people who did actual work. That has nothing to do with grading on a curve. You can assign people to the top of a curve based on \"egotist\" criteria or based on \"work\". Nothing about a curve or stack ranking requires it to be based on \"real work\". reply randomdata 3 hours agorootparent> When you hire 12 baristas are they competing to make the most coffees or is their job to handle customer's orders? Both? Handling customer orders is how the sport is played, but at the same time they are competing for the most points (money) in that gameplay. reply krisoft 1 hour agorootparentThat's great. If you regularly fire the barista who brings in the least amount of money then you will find that nobody sane will take the slower shifts. Because how much money a barista brings in is mostly a factor of how busy the coffeeshop is. Which is largely a factor of what time the clock shows, and that's not something a barista will be able to influence. (baring circumstances where a barista is so incompetent that costumers walk out of the shop.) reply lesuorac 2 hours agorootparentprevSo, all your baristas working a graveyard shift are going to be at the bottom of the stack ranking in terms of revenue/time. What do you now do? reply randomdata 2 hours agorootparentWhat do you do in athletics? Do you tell the kids (graveyard shift) they aren't allowed to participate in sports anymore because they can't compete with the big leagues (peak hours)? Probably not. More likely you would look at the different leagues individually. I'm surprised this idea is novel to you. reply krisoft 1 hour agorootparent> What do you do in athletics? Why do you talk about athletics? Baristas are not athletes. Coffee shops are not the olympics. You are stretching this analogy beyond its usefullness. > I'm surprised this idea is novel to you. Could you possibly express your thoughts without putting down others? Thank you. To address the meat of your comment. It sounds like you are proposing to grade the baristas working the bad shifts and the good shifts separately in different \"leagues\". The problem with that is that assumes that you are aware of all the factors which form the different leagues. If there are clear \"night time\" vs \"daytime\" barista groups that works. but if you just assign baristas as scheduling works out then you will realise that some people (randomly, and through no fault of their own) will be assigned to the slower shifts. Will you fire a perfectly good barista who is meeting the expectations of your establishment just because scheduling worked against them in that evaluation period? That is what stack ranking did in the case of microsoft. reply randomdata 1 hour agorootparent> Why do you talk about athletics? Because that's what the discussion is about...? Did you forget to read the thread? > Could you possibly express your thoughts without putting down others? If words shown on a computer screen are putting you down, it's time to go outside. You've lost your sense of reality. reply krisoft 45 minutes agorootparent> Because that's what the discussion is about...? The thread seems to be about how one would manage baristas. It spawned off where lesuorac pointed out that baristas working for a coffe shop are different from athletes in that they are not competing with each other. > Did you forget to read the thread? > You've lost your sense of reality. I guess that’s a no then. I hope you have a good day. reply randomdata 32 minutes agorootparent> The thread seems to be about how one would manage baristas. What gives you that impression? It isn't not about baristas, but namely about a parallel between baristas and athletics. That was the context that was setup at the head of this particular thread branch, and we haven't change the subject (aside from that irrational attempt related to being \"put down\", whatever that was). > I guess that’s a no then. Correct. There is no logical place for pointless emotions here. Save it for human interactions. reply lesuorac 1 hour agorootparentprevI mean you do tell the kids they can't participate (in that league). There are woman playing ice hockey but none of them have survived an interview (professional try out with an NHL team and so they've all been told no. --- It's novel because it's not how its done. It's a big part of the stack rank hate is that people just blindly rank everybody and then adjust compensation that way. Taking more granular detail into account just isn't done. But also because you're hired to do a job. If you do the criteria of the job then you should get a satisfactory rating. Similar to test taking, if you demonstrate knowledge of the material then you should pass. If you got 99/100 questions right and everybody else got 100/100 then you shouldn't get an F despite you being the worse of the group. reply randomdata 1 hour agorootparent> Taking more granular detail into account just isn't done. Where'd you dream up that idea? I operate a restaurant, so I at least have first-hand experience in overseeing barista-like workers, and I don't know how you could possibly ignore such details? I'm sure I'm not perfect at it. I'm certainly not accurately capturing the butterfly flapping its wings in Africa. But you'd never flat-out ignore the blatantly obvious like shift times. reply lesuorac 1 hour agorootparentPerhaps we've gone so deep into the thread that you've forgotten how we got here: https://news.ycombinator.com/item?id=41983561 Perhaps with a statement like \"But you'd never flat-out ignore the blatantly obvious like shift times.\" then you understand why people don't like stack ranking because yes people do ignore blatantly obvious things. reply randomdata 57 minutes agorootparentNot forgotten, but not particularly relevant. The context we followed only inherited the athletics analogy and how it parallels with baristas. Sometimes it is necessary to ignore the blatantly obvious. You can't meaningfully alter the ranking of a sports team because their star player was out with a broken leg. You have to accept the circumstances for what they are. But I'm not sure that translates to something like shift times which are fundamental to the game. reply psunavy03 1 hour agorootparentprevWhy the hell are your baristas competing? Why are you not just measuring whether or not they are acceptably good at their jobs? If they are superior, promote them. If they are acceptably OK, keep them. If they suck, fire them. You shouldn't have to arbitrarily put someone at the bottom of the curve; that's ridiculous. reply randomdata 1 hour agorootparent> Why the hell are your baristas competing? Because that's what is necessary in a market economy? If they don't put in effort to compete, the customer will find another team of baristas that will. It is not like it is hard to find another coffeeshop. > you shouldn't have to arbitrarily put someone at the bottom of the curve What is arbitrary about it? The reality is that more coffeeshops open than can actually be supported by coffee drinkers, so it is an economic necessity that some end up shuttering due to being at the \"bottom of the curve\". reply sealeck 3 hours agorootparentprev> \"grading on a curve\" is a good idea, and if athletics wasn't run that way, nobody would watch. Good thing that enterprise software and athletics are different things! reply psunavy03 1 hour agorootparentprevThis is ridiculous. You grade people to a standard, not against each other. Stack ranking Jack Welch-style is basically operating under the assumption that if you had Bill Gates, Mark Zuckerberg, Steve Jobs, Larry Ellison, Elon Musk, Satya Nadella, and Jeff Bezos on a team, that one of them would have to get a shitty grade and be fired. All it does is make true talent not want to work with other true talent for fear they get screwed over. reply fsckboy 2 minutes agorootparent>Stack ranking Jack Welch-style is basically operating under the assumption that if you had Bill Gates, Mark Zuckerberg, Steve Jobs, Larry Ellison, Elon Musk, Satya Nadella, and Jeff Bezos on a team, that one of them would have to get a shitty grade and be fired. no, it's not, you are wrong. It is based on the probability that you will not have all those outliers on one team, a probability that is very very high. reply fnord123 20 hours agoprevA rare miss by Mr. Luu. > Ballmer wins... 2010: Microsoft creates Azure The Azure project was run by Nadella before he became CEO. And it succeeded despite Ballmer. Azure was seen as the Microsoft cloud, where people ran Windows Servers. But Microsoft had long lost the battle for the server space to Linux. When Ballmer stepped aside, only then could Nadella drop the limiters and push the MicrosoftMuch like Gary Bernhardt's talk, which was panned because he made the problem statement and solution so obvious that people didn't realize they'd learned something non-trivial I really want to see this video, but I cannot find it anywhere. I checked https://www.youtube.com/playlist?list=PLcGKfGEEONaDvuLDFFKRf... but I believe Gary asks that his videos not be shown (which I am fine with). I also checked https://www.destroyallsoftware.com/talks but I do not see it there either. Should I be looking somewhere else? reply nvader 15 hours agoparentI found it here: https://archive.org/details/usetypes reply hermanradtke 6 hours agorootparentThank you for the link but I was looking for the reproducibility talk. reply hermanradtke 3 hours agorootparentI cannot edit anymore, but this is the talk descrption: https://www.thestrangeloop.com/2016/reproducibility.html reply hyperhopper 16 hours agoparentprevHis \"using you're types good\" talk being censored is my go-to example for how \"cancel culture\" and \"wokeness\" has gone too far, a good useful talk not meant to be harmful to anybody, with no evidence that anybody was hurt by it, is removed from the world for the far reaching fear that one day some person might not be happy about it. The fact that years later people still want to learn from it and nobody has every talked about it being an issue shows we have gone too far as a society on the side of caution and never doing anything that wouldn't stand up to a fortune 10 HR exec panel. reply spookie 15 hours agorootparentWhat was \"wrong\" about the talk exactly? reply hyperhopper 15 hours agorootparentHe effectively pretended to be dumb. And that included using the wrong \"you're\" and very simple dialects of English and mannerisms which he thought could potentially be seen as disparaging some groups. I think it was so generic and so clearly a joke it would be insane to pretend it was making fun of a group that existed in real life. For reference in the talk he also listed strongly typed languages as \"weekly typed\", static as dynamic, and had a MacBook on stage display a BSOD. The talk was comedic genius and a fun tongue in cheek commentary on the industry in 8 minutes. But it has been deemed to dangerous to society to be allowed to exist. reply gs17 14 hours agorootparentIs this a real thing that happened? It's framed as a young child making a lot of misconceptions, you'd have to be pretty darn sensitive to get offended by that. reply Jgrubb 6 hours agorootparentprevLook, I don't know the video but how do we know - or why do we assume - that it was taken down out of fear of future weaponized wokeness? Maybe Gary took it down for Gary's reasons. Edit: from the couple of descriptions in this thread I could totally envision an empathetic individual waking up one day and deciding that talk felt too much like punching down and taking it down because they felt like they could do better. reply great_tankard 13 hours agorootparentprevI must have missed this drama. Who was offended by it? When was Gary \"cancelled?\" reply edm0nd 20 hours agoprevBill Gates: can jump over an office chair - https://www.youtube.com/watch?v=KxaCOHT0pmI Steve Ballmer: developers developers developers developers developers - https://www.youtube.com/watch?v=Vhh_GeBPOhs Two of my favorite videos haha reply oaththrowaway 16 hours agoparenthttps://www.youtube.com/watch?v=7GM4Lt5k24s https://www.youtube.com/watch?v=DgPt6mvjr5Q Here's a few more to add to your favorites reply edm0nd 15 hours agorootparentthis basketball one is glorious! reply fnord123 20 hours agoparentprev> Steve Ballmer: developers developers developers developers developers - https://www.youtube.com/watch?v=Vhh_GeBPOhs If you haven't seen the remix, here you go: https://www.youtube.com/watch?v=1gI_HGDgG7c reply gattilorenz 4 hours agoparentprevSteve Ballmer: REVERSI! https://www.youtube.com/watch?v=DgJS2tQPGKQ reply rightbyte 9 hours agoparentprevThat chair jump is actually kinda impressive... reply mproud 5 hours agoprevWSJ had a great write-up that was very interesting about the plans Ballmer was trying to get rolling. I really gained additional respect for him after reading this. He wanted to overhaul the company, but realized people would struggle doing so while he was in charge. He realized sometimes you have to let someone else do it. https://www.wsj.com/articles/SB10001424052702303460004579194... [archive here](https://pages.stern.nyu.edu/~adamodar/pdfiles/cfovhds/weekly...) reply RandomThoughts3 20 hours agoprevIt’s funny how all the comments here are falling in the trap described in the beginning of the article of disliking Ballmer because he comes from the sales side and they can’t fathom someone not coming from the tech side leading a tech company. What’s undeniable in the article is that Ballmer literally built what remains Microsoft best asset even before being a CEO there: it’s incredibly good relationship with its corporate customers. Honestly, it’s really what sets Microsoft apart for me. When you do deal with them as a corporate customer, you really get the feeling that they understand the way things work in a big corp IT department and will be reliable and predictable. reply julianeon 17 hours agoparentI wouldn’t use that language exactly. I think they’ve set up a very effectively profitable relationship with the industry - part carrot and part stick. But I don’t think AWS usage would’ve exploded as it did if their clients truly felt both respected and understood. reply greggyb 16 hours agorootparentAWS was the first mover, and they execute very well on products. Despite this, AWS market share has been pretty flat for a while. Azure was a second mover. Microsoft executes on platform and partner ecosystem very well. Azure is still growing market share pretty quickly. reply kjs3 19 hours agoparentprevWhen you do deal with them as a corporate customer, you really get the feeling that they understand the way things work in a big corp IT department and will be reliable and predictable. Just got done negotiating our E5 license last year and omfg is this laugh-out-loud untrue. Don't even get me started on how we have to check the Azure websites before every meeting with our M$ counterparts to figure out what Azure services they've changed the name of since the last call. But yes, completely agree it's the corporate customers that are the wind beneath their sales. But it's because they understand that for almost any large corp IT department on the planet, telling the 85% of their employees who don't give a steaming crap what the nerds think about Windows or Ballmer, \"you have to dump Excel & Word and learn something else\"[1] is not a hill any of them are willing to die on. We have people that won't use Excel on a Mac because once they found a place where it didn't work exactly like the Windows version. [1] No. LibreOffice is not the answer. reply RandomThoughts3 19 hours agorootparent> We have people that won't use Excel on a Mac because once they found a place where it didn't work exactly like the Windows version. Then again, Excel on a Mac is significantly inferior to the Windows version to be honest. > Don't even get me started on how we have to check the Azure websites before every meeting with our M$ counterparts to figure out what Azure services they've changed the name of since the last call. Not my experience at all and we spend millions of dollars a year with them. reply gitaarik 9 hours agoparentprevYeah so you're basically saying Microsoft is a perfect company for sales focussed companies that need some technical stack. Fair enough reply adabyron 17 hours agoprevWhat Dan doesn't mention is that Steve was given the reins to a sinking ship if I recall. The US Govt was just finishing it's trial on Microsoft & was watching them closely. Tech bubble just burst. On day 1 of the transition after Steve, the stock jumped like crazy & continued that momentum. The stock was, as Dan mentions, at an unfairly low p/e ratio too. Idk if Steve was great but seems he was given the role of transition CEO. Plus did Bill ever really leave? It'll be interesting to see how Satya finishes his career & the first few years after. Microsoft was making really good software the first few years after Satya took over & a lot of people were wanting to work there. Since Covid though, their software quality & updates have crashed imo. reply hilux 16 hours agoparent> On day 1 of the transition after Steve, the stock jumped like crazy & continued that momentum. Many people would interpret that as \"investors had no faith in Steve Ballmer and were delighted to see his back.\" Do you have a different interpretation? Also, you seem to be implying that the trial was some external event, and not directly the fault of Bill and Steve. That is not how most people felt at the time. reply adabyron 5 hours agorootparent> Many people would interpret that as \"investors had no faith in Steve Ballmer and were delighted to see his back.\" Do you have a different interpretation? You're 100% right. Starbucks just did something similar. I personally don't believe these jumps are warranted until the new CEO proves their ability to change the ship. It would be really interesting to hear from Microsoft insiders what changes were being made before the CEO changeover & what ones were heavily done by Satya. > Also, you seem to be implying that the trial was some external event, and not directly the fault of Bill and Steve. That is not how most people felt at the time. While many people may have felt they were being very anti-competitive at the time, the same standard has not been held to other companies or Microsoft much over the past 20 years in the United States. reply toyg 6 hours agorootparentprev> implying that the trial was some external event In many ways, it was. The political stars aligned \"just so\", in a way we've not really seen before or since. Many, many other companies got away with much more egregious behaviour (hello, Apple). reply hilux 1 hour agorootparentApple does not have the percentage ownership of the market that Microsoft had with both Windows and Office. And to my knowledge Apple operates out in the open, i.e. they do not lie to everyone, and do not bundle shitty products with monopoly products in the way that Microsoft habitually did - and may still do. reply KaoruAoiShiho 20 hours agoprevHe's underrated in the sense that a lot of CEOs of his era completely destroyed their companies, see Intel, GE, GM, Yahoo, etc and he didn't. So that's already a win, he set up the company in a decent position so that when someone with more vision takes over they'll have something to work with, even if he didn't have the talent to pull things off himself. He had a couple of wins (Azure, Office 365) along with many many losses, and they're good enough to secure him a 6/10 on my ratings. reply greggyb 20 hours agoparentIf you trust the article, then Azure and O365 are each, independently, easily Fortune 100 companies if separated. These \"couple of wins along with many many losses\" are some of the most valuable products in the world. Imagine a VC fund that invested in a few dozen product companies, two of which were Azure and O365. Is that a 6/10 VC company? Why is the logic different for a CEO making bets for a company's next several decades? reply KaoruAoiShiho 20 hours agorootparentBecause the company has more strategic resources than a VC, and has need to defend existing businesses. MS should've been able to simply just extend their OS monopoly into all platforms and all architectures, but they didn't, and to a vast swath of the world have become irrelevant, and worse, have lost their ability to become relevant. It's a decline from being the monopolist to simply a player, sure they executed well in enterprise sales and was fast in picking up OpenAI, but they have lost the ability to use their strategic resources to save xbox, help azure overcome competition, or push a mixer or Surface or whatever. Edit: For people who don't understand the last sentence think about the way that O365 was able to help MS push Teams to stave off Zoom and others despite being objectively trash. MS should've been able to keep control of the internet, but they lost their moat to Google (Chrome), and the same story for various consumer products. Bing was a decent win but with a better consumer story they should've also been able to threaten social and youtube and so on. But now they're completely irrelevant there. reply greggyb 19 hours agorootparent> MS should've been able to simply just extend their OS monopoly into all platforms and all architectures, but they didn't, and to a vast swath of the world have become irrelevant, and worse, have lost their ability to become relevant. Microsoft is, pretty famously, on the receiving end of one of the most significant antitrust judgments in modern history. Choosing to further a monopoly seems that it would be a phenomenally bad decision for the company. Despite that, Windows remains the dominant operating system for businesses worldwide. So I would argue that the OS is far from \"irrelevant\". Beyond the OS, they are comfortably #2 in the public cloud market, with little threat from #3. Indeed, #1 has been relatively stagnant in market share, while Azure has been steadily growing.[0] It seems that a consistently growing market share in such a large industry shows that not only are they relevant, but they are becoming more so, and have not \"lost their ability to become relevant\". Additionally, it seems that they are \"overcoming competition\". > It's a decline from being the monopolist to simply a player .... > MS should've been able to keep control of the internet, but they lost their moat to Google (Chrome), They legally could not maintain that monopoly. Again, see the antitrust ruling. The antitrust case was about the impact on Netscape, and was too late to save Netscape. But it is a pretty straight line from a case about bundling IE with the OS. To be clear, the finding in this case originally held that Microsoft needed to be broken up.[1] Microsoft won on appeal, because of impropriety by the original judge in the case, but the appeals court upheld all findings of fact.[2] Much of what you are saying Microsoft should have been able to do on the basis of their OS monopoly would have been begging for further antitrust action. > Bing was a decent win Bing, on its own, would be the 12th largest tech company in the world, per the original article. And Microsoft is worth $3T today, largely on the basis of investment under Ballmer (and continued strong execution under Satya). Is the argument that Mi",
    "originSummary": [
      "The narrative that Microsoft was failing under Steve Ballmer and saved by Satya Nadella is overly simplistic, as Ballmer's tenure included strong financials and strategic investments.- Significant achievements during Ballmer's era, such as launching Azure and Office 365, and building a strong enterprise sales arm, contributed to Microsoft's current success.- While Ballmer faced criticism for projects like Bing and Windows Phone, these were part of a broader strategy that laid the groundwork for future achievements under Nadella's leadership."
    ],
    "commentSummary": [
      "Steve Ballmer, former CEO of Microsoft, faced criticism for fostering internal competition and indecision between enterprise cloud and consumer devices.- Despite criticisms, Ballmer established the foundation for successful ventures like Azure and Office 365, which are now significant revenue sources for Microsoft.- His leadership ensured Microsoft remained a major tech industry player, avoiding the decline experienced by other companies during the same period."
    ],
    "points": 331,
    "commentCount": 625,
    "retryCount": 0,
    "time": 1730152107
  },
  {
    "id": 41976529,
    "title": "HTML Form Validation is underused",
    "originLink": "https://expressionstatement.com/html-form-validation-is-heavily-underused",
    "originBody": "HTML Form Validation is heavily underused October 28, 2024 HTML Forms have powerful validation mechanisms, but they are heavily underused. In fact, not many people even know much about them. Is this because of some flaw in their design? Let’s explore. Attributes, methods, and properties It’s easy to disallow empty inputs by adding a required attribute:Beyond that, there is a bunch of other ways that you can add constraints to your input. Precisely, there are three ways to do it: Using specific type attribute values, such as \"email\", \"number\", or \"url\" Using other input attributes that create constraints, such as \"pattern\" or \"maxlength\" Using the setCustomValidity DOM method of the input The last one is the most powerful as it allows to create arbitrary validation logic and handle complex cases. Do you notice how it differs from the first two techniques? The first two are defined with attributes, but setCustomValidity is a method. Here’s a great write-up that explains the differences between DOM attributes and properties: https://jakearchibald.com/2024/attributes-vs-properties/ The nuance of an imperative API The fact that setCustomValidity API is exposed only as a method and doesn’t have an attribute equivalent leads to some terrible ergonomics. I’ll show you with an example. But first, a very quick intro to how this API works: // Make input invalid input.setCustomValidity(\"Any text message\"); This would make input invalid and the browser will show the reason as “Any text message”. // Remove custom constraints and make input valid input.setCustomValidity(\"\"); Passing an empty string makes the input valid (unless other constraints are applied). That’s pretty much it! Now let’s apply this knowledge. Let’s say we want to implement an equivalent of the required attribute. That means that an empty input must be prevent the form from being submitted.{ const input = event.currentTarget; if (input.value === \"\") { input.setCustomValidity(\"Custom message: input is empty\"); } else { input.setCustomValidity(\"\"); } }} /> This kind of looks like we’re done and this code should be enough to accomplish the task. But try to see it in action: Submit Reset It may seem to work, but there’s just one important edge case: the input is in a valid state initially. If you reset the component and press the “submit” button, the form submission will go through. But surely, before we ever touch the input, it is empty, and therefore must be invalid. But we only ever do something when the input value changes. How can we fix this? Let’s execute some code when the component mounts: import { useRef, useLayoutEffect } from \"react\"; function Form() { const ref = useRef(); useLayoutEffect(() => { // Make input invalid on initial render if it's empty const input = ref.current; const empty = input.value !== \"\"; input.setCustomValidity(empty ? \"Initial message: input is empty\" : \"\"); }, []); return ( { const input = event.currentTarget; if (input.value === \"\") { input.setCustomValidity(\"Custom message: input is empty\"); } else { input.setCustomValidity(\"\"); } }} /> Submit); } Submit Reset Great! Now everything works as expected. But at what cost? The boilerplate problem Let’s look at our clumsy way to validate the initial value: const ref = useRef(); useLayoutEffect(() => { // Make input invalid on initial render if it's empty const input = ref.current; const empty = input.value !== \"\"; input.setCustomValidity(empty ? \"Initial message: input is empty\" : \"\"); }, []); Ugh! Wouldn’t want to write that one each time. Let’s think about what’s wrong with this. The validation logic is duplicated between the onChange handler and the initial render phase The initial validation is not co-located with the input, so we’re losing code cohesion. It’s fragile: if you update validation logic, you might forget to update code in both places. The useRef + useLayouEffect + onChange combo is just too much ceremony, especially when a form has a lot of inputs. And it gets even more confusing if only some of those inputs use customValidity This is what happens when you deal with a purely imperative API in a declarative component. Unlike validation attributes, CustomValidity is a purely imperative API. In other words, there’s no input attribute that we can use to set custom validity. In fact, I would argue that this is the main reason for poor adoption of native form validation. If the API is cumbersome, sometimes it just does not matter how powerful it is. The missing part In essence, this is the attribute we need:In a declarative framework, this would allow to define input validations in a very powerful way: function Form() { const [value, setValue] = useState(); const handleChange = (event) => setValue(event.target.value); return ( Submit); } Pretty cool! In my opinion, at least. Though you can rightfully argue that this accomplishes only what the existing required attribute is already capable of. Where’s the “power”? Let me show you, but first, since there’s no actual custom-validity currently in the HTML Spec, let’s implement it in userland. function Input({ customValidity, ...props }) { const ref = useRef(); useLayoutEffect(() => { if (customValidity != null) { const input = ref.current; input.setCustomValidity(customValidity); } }, [customValidity]); return ; } This will work well for our demo purposes. For a production-ready component check out a more complete implementation. The power Now we’ll explore which non-trivial cases this design can help solve. In real-world apps, validation often gets more complex than local checks. Imagine a username input that should be valid only if the username is not taken. This would require async calls to your server and an intermediary state: the form should not be valid while the check is in progress. Let’s see how our abstraction can handle this. Username Helpers: Taken username Valid username Bad input Submit Reset Play around with this example. It uses the required to prevent empty inputs. But then it relies on customValidity to mark input as invalid during the loading state and based on the response. Implementation First, we create an async function to check whether the username is unique that imitates a server request with a delay. export async function verifyUsername(userValue) { // imitate network delay await new Promise((r) => setTimeout(r, 3000)); const value = userValue.trim().toLowerCase(); if (value === \"bad input\") { throw new Error(\"Bad Input\"); } const validationMessage = value === \"taken\" ? \"Username is taken\" : \"\"; return { validationMessage }; } Next, we’ll create a controlled form component and use react-query to manage to server request when the input value changes: import { useState } from \"react\"; import { useQuery } from \"@tanstack/react-query\"; import { verifyUsername } from \"./verifyUsername\"; import { Input } from \"./Input\"; function Form() { const [value, setValue] = useState(\"\"); const { data, isLoading, isError } = useQuery({ queryKey: [\"verifyUsername\", value], queryFn: () => verifyUsername(value), enabled: Boolean(value), }); return ( { setValue(event.currentTarget.value); }} /> Submit); } Great! We have the setup in place. It consists of two crucial parts: Verification request state managed by useQuery Our customcomponent that is capable of taking the customValidity prop Let’s put those pieces together: import { useState } from \"react\"; import { useQuery } from \"@tanstack/react-query\"; import { verifyUsername } from \"./verifyUsername\"; import { Input } from \"./Input\"; function Form() { const [value, setValue] = useState(\"\"); const { data, isLoading, isError } = useQuery({ queryKey: [\"verifyUsername\", value], queryFn: () => verifyUsername(value), enabled: Boolean(value), }); const validationMessage = data?.validationMessage; return ( { setValue(event.currentTarget.value); }} /> Submit); } That’s it! We’re describing the whole async validation flow, including loading, error and success states, in one attribute. You can go back to see the result again if you wish One more This one will be shorter, but also interesting, because it covers dependent input fields. Let’s implement a form that requires to repeat the entered password: import { useState } from \"react\"; import { Input } from \"./Input\"; function ConfirmPasswordForm() { const [password, setPassword] = useState(\"\"); const [confirmedPass, setConfirmedPass] = useState(\"\"); const matches = confirmedPass === password; return ( { setPassword(event.currentTarget.value); }} />{ setConfirmedPass(event.currentTarget.value); }} /> Submit); } You can try it out: Enter Password Repeat Password Submit Reset Conclusion I hope I’ve been able to show you how setCustomValidity can cover validation needs of all kinds. But the real power comes from great APIs. And hopefully, you are now equipped with one of those. And even more hopefully, we will see it natively in the HTML Spec one day. @everdimension",
    "commentLink": "https://news.ycombinator.com/item?id=41976529",
    "commentBody": "HTML Form Validation is underused (expressionstatement.com)330 points by stereoabuse 21 hours agohidepastfavorite286 comments DaiPlusPlus 18 hours agoLast time I checked, web-browsers today still do not allow you to style the appearance of built-in HTML validation messages [1]; this wouldn't be so bad if Chrome (and Firefox) still conformed to their OS platform UI guidelines (i.e. so it looks system-generated, like how `title=\"\"` tooltips used to be), instead Chrome uses this ugly yellow/orange icon color with black-text on a white background on a bubble with a fixed corner border radius - it clashes horribly with my current project's aesthetics. [1] https://stackoverflow.com/questions/5328883/how-do-i-style-t... Annoyingly, Chrome used to allow styling of validation-messages using vendor-prefixed pseudoelement selectors, but they removed that functionality and never brought it back; I'll chuck this on the same pile as other arbitrary annoyances like \"can we have a native HTML combo-box please\" and \"why isstill a horribly unusable ctrl+click box instead of a checkbox-list?\". reply everdimension 5 hours agoparentI think the problem here is not as much with the absence of custom styling, because you can quite easily read the native \"validity\" state of the input and render it however you want. The problem is that it's quite tricky to correctly subscribe to the changes of this validity state. There are indeed some validity events being dispatched, but unfortunately not always. Updating form state programmatically (such as calling \"form.reset()\" or setting input value via \"input.value = '...'\") doesn't trigger these events. I think this is a separate good topic for investigations and for suggestions to the web platform reply ximm 6 hours agoparentprevI never really understood why people want to style stuff like this. I like how you can express yourself by using colors and layout and stuff like that. But at some point usability is more important than branding. reply guappa 6 hours agorootparent> usability is more important than branding. Said no designer ever. At work our design team came up with buttons that are 10x10 pixels on my screen. They are used to change pages (like on mobile, but this is a desktop program), the scroll events are ignored by design, so you either click the tiny buttons (which are slightly darker gray than the dark background) or you simulate a finger swipe via drag and drop with the mouse. Yes they designed a touch GUI for a desktop application. reply mrguyorama 1 hour agorootparent>usability is more important than branding. Any designer unwilling to say this, especially in the context of accessibility, should be outright fired. They are bad at their job. Accessibility is a legally mandated requirement, not an afterthought reply micromacrofoot 6 hours agorootparentprevplenty of designers say this, often company leadership is the one pushing them to style everything because everyone else does reply potato3732842 47 minutes agorootparent>because everyone else does Which everyone is only doing because it's an industry fad likely stemming from copycatting one or two instances where it was done for legitimate reasons. reply szundi 10 minutes agorootparentAnd now we have llms who do the same as well reply guappa 5 hours agorootparentprevWell the ones I've worked with couldn't care less. Of course they love to show off how good they are to the poor disabled people, but that doesn't mean anything until some government reminds us that we are in breach of contract unless we make our GUI accessible. reply bluGill 2 hours agorootparentDesigners that come from a background of human machine interaction care and will say that. Designers who come from a background of art don't understand what is being talked about and so don't say anything - they tend to only ensure it works on their one devices which they have selected to be the best and ignore everything else. The second group does make things that look nicer, but the first ensures it can be used. You really want both, but then you need to be careful about who wins when they disagree. reply micromacrofoot 2 hours agorootparentprevthis seems like a very mean spirited take on accessibility? reply askew 2 hours agorootparentI took from it that [the designers] are all mouth and no trousers – which is fairly common. But \"the poor disabled people\" seems patronising. reply inopinatus 5 minutes agorootparentIt is a patronising sentiment, but adjacent tonal cues suggest GGGP is offering it ironically, thus in ridicule of performative compliance vs endorsement of regulated UX. BrandonMarc 1 hour agorootparentprevI believe that was the commenter's point - that the designers described patronizingly virtue signal about their accessibility priorities, while their other decisions are troublesome. reply tsimionescu 2 hours agorootparentprevPeople want to style things to match their page, exactly because consistency is part of usability. Especially given the limitations of browser form validations, you will absolutely need your own validations in addition to any browser validations you use. But your own validations will look different from the browser-provided ones, at least on some browsers. Which will confuse users, hence decreasing usability. And this also assumes that the browser validations are in any way usable to begin with. I would argue that they are not, and would require some sane styling to become usable in the first place. reply SoftTalker 1 hour agorootparentNews flash, every site doing things their own way is one of the core usability problems on the web. Designers love it. Users hate it. reply dagw 6 hours agorootparentprevBut at some point usability is more important than branding. I have never worked for any company or organisation that believed this. Most clients will send you their branding guidelines before sending their feature requests. If they get to choose between adding a new feature, improving usability or making sure everything follows the branding, they will choose the branding every time. reply oneeyedpigeon 5 hours agorootparentI think you're exaggerating here. I recognise the \"branding is more important than usability\" approach, but the OP specifically said \"at some point\". Have you ever worked for a company that forced a design decision knowing it would prevent anyone from using the product? I suspect not; there will always be some point. The issue here is, at what point does usability take precedence. Does input validation fall under \"branding\" or \"function\"? IMO, an error modal is nothing to do with company branding, etc. — it's not even part of your document, it's the browser's responsibility. The browser can decide to do something completely different from showing a modal as far as its concerned, so you shouldn't make any assumptions. Your responsibility ends at declaring what valid data is. reply Spivak 5 hours agorootparentIt 1000% falls under branding. If you don't make errors consistent across browsers / platforms your support staff will politely but firmly burn your house down. There's no such thing as your responsibility ending and throwing your hands up when you're a company who has to do end-to-end support of your product. Having as little native anything means you go from n sets of documentation to 1. reply bee_rider 5 hours agorootparentprevI think the disconnect here is in the interpretation of “more important.” Usability is more important than branding from the point of view of the victims of this kind of over-designed software, the end users. Because they are already being inflicted with a lot of branding, please at least give them conventional error messages. reply itishappy 2 hours agorootparentprevSure, but that's not how it works out in practice. It sure looks like everyone just rolls their own. Fun game: can you find any major website that uses the default validation? This isn't an issue if everyone's an accessibly expert, otherwise we're sacrificing usability. reply jayd16 3 hours agorootparentprevIt looks out of place and broken and that creates user confusion. That is also a usability concern. reply raxxorraxor 3 hours agorootparentprevThe standard HTML components are ancient and not very usable. People like to complain about JavaScript on websites, but they often don't know in what a bleak world they would live. reply 354896547981565 18 hours agoparentprevTrue. But you can hide the default message and replace it with your own. You'll still benefit from the form validation. reply gregoriol 9 hours agorootparentThere is no real benefit because the validation rules allowed there are often too limited for real use-cases anyway. reply epolanski 8 hours agorootparentWhat do you mean there's no real benefit? You give the same error messages based on whatever custom validation and control the output. reply fkyoureadthedoc 5 hours agorootparentI've found it more annoying to mess with the browser validation API and using setCustomValidity/reportValidity etc. than to just use other validation libs. Ideally I'd use whatever library I want and they would call setCustomValidity for me though. Using the related pseudo classes :valid, :invalid, :required, :optional is nice, but until last year you still had to do custom logic there because :user-valid/:user-invalid weren't implemented outside of Firefox. That created additional work and was annoying. reply endemic 6 hours agorootparentprevDo you have any examples? reply jacobr 9 hours agorootparentprevYou can even benefit from the default messages if you want that, grabbing `input.validationMessage` and rendering it as you wish. reply unclebucknasty 17 hours agoparentprevFor that matter, styling a select (single) isn't quite the walk in the park it should be. reply cxr 15 hours agoparentprevnext [3 more] [flagged] DaiPlusPlus 14 hours agorootparentI agree with you 100%; I hate it when websites think they’re being trendy by using undraggably-thin scrollbars or try to subvert the UA’s password-manager (banks…) or naively think they can block users right-clicking or copying images. To clarify my position: I agree that UAs should be free to show validation messages as-appropriate for the user (I.e. those popover hints) but it is because Google decided to stylise them with a very specific aesthetic/design - as opposed to trying to fit-in to the host system - it clashes more. Now, I’m not suggesting that the UA popovers be fully CSS-able, but I would like to see some kind of coarse-grain control “hint” properties - just like we have with (non-WebKit-prefixed) scrollbars, , mosttypes, and more besides. The issue here is that the validation popovers are almost uniquely exempt from styling compared to all other HTML platform features (the only other exception being `title=“”` tooltips, AFAIK). By-comparison, this is kinda like the (still) half-broken support for dark-mode: if a page is dark-themed you (probably) don’t want nuclear-flash-white UA-provided popovers stinging your eyes because there’s no validation-popover background color hint. Similarly, supposing my website makes tasteful use of serif typefaces - but the input validation popovers will be in Chrome’s own Material-design sans-serif font, while OS/platform widgets will be in some third other font by-default but can generally still be set via CSS. If you’re on HN then you’re likely bothered by inappropriate mixing of serif and sans-serif typefaces, so I hope you can sympathise. reply cxr 6 hours agorootparentYou're talking about millions of dollars of engineering resources to implement what not only doesn't matter, but is probably actually harmful in comparison to the status quo (however ugly that is). And that's just on the implementation side. Now consider the tens of thousands of downstream developers who are going to suck up more resources from their clients because browser developers offered them a new bauble and some knobs to tweak. All going towards something that shouldn't even exist. reply kaoD 4 hours agoprevMy product just failed an accessibility audit because we are using native HTML form validation and the official recommendation was to implement our own validation layer. EDIT: which I agree with. Native HTML validation has many flaws and visual customization is not my biggest concern to be honest (but it's the nail in the coffin). E.g.: - It's impossible to show multiple errors at once per field unless you concat strings (\"You need a number. You need a symbol. Must be >10 chars.\") This is both bad UX and bad for accessibility (you cannot navigate concatenated strings on the accessibility tree). And this isn't even implementation dependent, it's the spec! You need multiple errors at once because playing whack-a-validation-error is not fun for users. - It's browser-dependent which is bad because you can't control it and because the implementation is generally terrible (e.g. in Chrome it shows a popup on focus, which is not very accessible by itself because (1) it's a popup (2) that shows modally (3) and can't show all form errors at once). Not using popups for important information is accessibility 101, but browsers cannot afford to do anything else without interfering with the actual document. - You still need custom validation for form-wide errors (like errors that don't belong to a particular field, \"Fields A and B are incompatible\") so you might as well have a consistent validation story. - It requires you to have hidden inputs to be consistent (-ish) if you have some custom input that doesn't fit any of the native input types -- this breaks accessibility too and fixing it is as hard as having your own accessibility-friendly validation story. - The custom validity API is imperative and cumbersome to use. Not using custom validity is almost never an option unless you want terrible messages (\"This field is invalid\") And many more. HTML form validation is terrible. reply everdimension 1 hour agoparentThanks, that's quite interesting and insightful! Thank you for sharing The fact that something provided by the browser can fail accessibility requirements is definitely ironic. We're always taught that the motivation to \"use the platform\" and \"follow semantics and semantic elements\" is partly to satisfy the accessibility concerns. I still think it's worth to leverage the native validation mechanisms. * You don't have to use the native validity tooltips for the error messages. You can read directly from the input's ValidityState (input.validity) and render the message however you like and show multiple errors if you need to * The browser can improve and you will benefit from using a standardized approach The fact that \"not using popups\" supposedly breaks a11y sounds weird, though. But if you need to respond to an audit then this is the way you can go. > Errors that do not belong to a particular field These are indeed interesting! There are techniques to handle those, too. In my project I have a \"HiddenValidationInput\" component that renders an invisible readonly input that serves the purpose of rendering a custom error that doesn't belong to any other field. It's in fact quite a pleasure to use because you can just conditionally render it. > The custom validity API is imperative and cumbersome to use Absolutely agree on this one, and handling this is exactly what my article is about. And I really hope that this part will improve in time, too reply kaoD 56 minutes agorootparent> You can read directly from the input's ValidityState (input.validity) Feels like I don't gain much from that. Native validation is very limited and the few cases that it covers are super simple to implement already. Am I missing something? I'd rather have a `validateSomething` which returns a discriminated union of error causes than using `pattern` and just getting a boolean. > In my project I have a \"HiddenValidationInput\" component Yeah, that's an accessibility issue (and the UX for the common user is terrible too for multiple reasons). reply josephcsible 4 hours agoparentprevWhat was the auditors' reasoning for that? reply kaoD 4 hours agorootparentThat the browser implementations are generally terrible and wouldn't pass accessibility audits, so all browsers would have to change and then some time to pass for the fixed versions to be widespread. We didn't discuss browser-specific issues in detail, but I edited some points in my original message that highlight some of the issues that I suspect make it a no-go for accessibility. reply SoftTalker 1 hour agorootparentSeems to me that this says that the browsers have the accessibility problems and that's what should be fixed. Why does every website developer have to work around this while the browsers get a free pass? If field validation is \"standard HTML\" and browsers can't do it in an accessible way, that's squarely a browser problem. reply kaoD 1 hour agorootparentBecause in reality browsers can't do much. The API and spec are broken as-is. I'll quote my top-level comment: > This is both bad UX and bad for accessibility (you cannot navigate concatenated strings on the accessibility tree). And this isn't even implementation dependent, it's the spec! > Not using popups for important information is accessibility 101, but browsers cannot afford to do anything else without interfering with the actual document. Plus it doesn't matter who's to blame: people with accessibility issues need access now. reply everdimension 1 hour agorootparentprevtotally And if they fix it, it'll be fixed for everyone reply pbreit 19 hours agoprevThe biggest, easiest to implement underutilization is: \"Using specific type attribute values, such as \"email\", \"number\", or \"url\"\" These can significantly improve user experience on mobile by triggering the optimal keyboard. reply nikeee 18 hours agoparentI avoid the use of `type=number` and use `type=text inputmode=numeric` instead. It doesn't come with these arrow buttons which most users don't need anyway for entering numbers. Also the keyboard is better on iOS. reply francislavoie 13 hours agorootparentBut what's crazy is both of those still don't disallow non-numeric input unless you use JS to reject keystrokes and intercept paste. HTML form validation is so incomplete and limited, every time I look at it I want to scream cause wtf there's so many just totally obvious things we need that don't exist by default and we need to reinvent the wheel. Native date and time inputs are still garbage so every UI framework has to build their own solution. reply Dwedit 12 hours agorootparentI used to write those fancy textboxes that reformatted your input as a phone number as you type, and intercept paste, and all that. But then it turns out that you really do want a free-form text input. Let people paste text in, edit their text freely, then let it validate afterwards. When it validates, then reformat it. For example, text boxes with length limits. These are awful. It messes with your ability to paste something in, then edit it down to the correct length. reply francislavoie 12 hours agorootparentNah. Users are too stupid to fix their own inputs in many cases. Seen inputs with zero-width spaces that are invisible which fail validation. User doesn't understand why, complains. Enforcing a character set for certain kinds of inputs is a very good thing. reply onion2k 11 hours agorootparentIn 26 years of web dev, mostly as a frontend person, I've only seen zero width spaces in three situations - pasting from Word - QA testers being through - devs pranking each other The third one is by far the most common. Word is much better these days and I've not seen that happen in a long time. I wish I saw QA test this stuff more often. The idea that it's common enough that you'd break your UX to handle it baffling to me. reply jcelerier 9 hours agorootparentI see weird spaces every time I copy-paste from some pdf pretty much, especially if it's a digitised or 10+ years one. Hardly an irrelevant use case reply ragnese 7 hours agorootparentprevI'll be one of those guys. I see the invisible whitespace from pasting ALL the time. I've literally dealt with user complaints caused by it three times in the last two weeks! (Usually customer issues don't make it to me unless our support team can't figure it out or suspects a bug) To be fair, it's not usually that frequent. Just a funny coincidence that I've JUST dealt with it more recently. reply lomase 8 hours agorootparentprevAdd to that pasting from MS Teams. reply francislavoie 11 hours agorootparentprev\"Break your UX\" is a vast exaggeration. I'm not talking about phone number fields, to be clear. I'm talking about like numeric amounts, with maybe a negative or decimal point. It's literally just [0-9\\.-] but that still requires JS to limit inputs to that. reply HappMacDonald 7 hours agorootparentWhat's wrong withthough? reply francislavoie 6 hours agorootparentIt doesn't prevent the bad input, it only validates it on submission. Therefore it's on the user to fix it, and if it's from a zero-width space, it's literally invisible. Non-technical users will have no idea what to look for to fix it (i.e. use your arrow keys cursor to see at which character the arrow key doesn't shift visually, and there's the ZWSP). It happens way too often. reply pmontra 11 hours agorootparentprevThe problem here is that sometimes users and copy pasting from another document and that comes in arbitrary formatting, not the one enforced by the input element. For example phone numbers can be a string of digits, or multiple strings separated by spaces or hyphens or with parts enclosed by () etc It's a more pleasant UX to let them paste anything, then edit it inside the input, validate and submit. reply xigoi 11 hours agorootparent> For example phone numbers can be a string of digits, or multiple strings separated by spaces or hyphens or with parts enclosed by () etc > > It's a more pleasant UX to let them paste anything, then edit it inside the input, validate and submit. How is that more pleasant than a textbox which automatically removes the extra characters? reply kevincox 5 hours agorootparentWhat if I copy something but accidentally get another couple words one of which is a number. For example copying from a chat app and get the date with the message. Then my \"Sep 24th 416-555-1234\" input which, would have been very easy to fix, becomes 244-165-5512 34. It will take me a few seconds to realize what has happened, identify where the intended phone number starts and remove the accidentally pasted digits. The nice thing about the native input is that it is very simple, predictable and flexible. It may be easier to make a better UX on the happy cases but it is very difficult to not add some suboptimal and confusing UX on the less happy cases. Often times it is best just to keep it simple so that the user can easily complete their task rather than adding some \"magic\" which may backfire and confuse users. It is surprisingly often that I need to open a text-editor or clipboard-editor to format values before pasting them into apps or websites because they have broken an input field by trying to be magic and failing to consider anything but the happy cases and maybe one or two bad paths. reply francislavoie 5 hours agorootparentI was talking about numerical amount fields, not phone numbers or dates or credit card numbers. Those are different things entirely. FWIW if you're copying text on Android, you can tap the clipboard popup to edit the clipboard item before pasting it elsewhere. reply kevincox 5 hours agorootparentThe exact same problem applies. If I copy \"Sep 24 $2412.45\" out of a message and you quickly \"clean\" it to 242 412.45 I may not notice and even if I do it will take a second for me to realize what happened and clean up the value to be the intended amount. If I see the original text in the field it is much more obvious what happened and quicker for me to understand what I need to do to fix it. reply djtango 10 hours agorootparentprevBecause you don't know what your user wants to do and eventually you'll annoy someone reply kevincox 5 hours agorootparentprevIn most cases when you are accepting a numeric value it is best to just strip anything that isn't 0-9 then run validation on just the digits. There are some exceptions like phone numbers which may need + prefixes and similar but you can adjust these rules. What I have found to work well is when the user unfocuses the input field or on submit strip everything but expected values then validate. If you want you can format the value at this time too (and the formatting won't interfere with text input). If you want to get really fancy you can strip and format as they type and be careful to keep the formatting \"virtual\" such that the user is only editing the raw digits but this is very tricky to get right on all platforms and support all expected native editing UX. But even in the best cases this can have bad edge cases. For example the user pastes some text but it happens to have multiple numbers in it, if you eagerly strip and format what is left for the user will be these two numbers mushed together with misplaced separators or similar. If you let them paste, remove the unintended stuff, then format on blur this just works and is understandable. reply eru 11 hours agorootparentprevWhy blame it on the user, instead of fixing it? You could remove the zero-width spaces in validation (for eg a phone number). reply francislavoie 11 hours agorootparentOr, you know, prevent invalid characters from the get-go. Same thing as validation, but done up-front. (But as I said elsewhere, I'm not talking about phone numbers, I'm talking about amounts) reply eru 11 hours agorootparentI would like to allow people to copy-and-paste text in, and then edit it. That could be a longer text that has the amount in it (but also some unrelated numbers). reply yakshaving_jgt 8 hours agorootparentprevBecause this often makes the input feel broken to the user. Instead of asserting that \"users are too stupid\" as you have done earlier, perhaps programmers should be less stupid and write more permissive parsers. Accessible design is actually pretty hard, and most designers and programmers get this wrong. If you want some good design advice, you can start here: https://adamsilver.io/blog/the-problem-with-live-validation-... reply francislavoie 5 hours agorootparentI'm not using \"stupid\" in the derogatory sense. I'm using it as a recognition of the skill/knowledge gap between technical and non-technical users. To clarify, we get _asked_ by our users to implement fields that limit input to help them avoid mistakes. Our QA and UX teams agree. This isn't a unilateral engineering decision. reply hluska 7 hours agorootparentprevI’ve read some very user hostile things in my career, but this one is particularly acerbic. It’s a bad look. If I ever grew to hate users this much, I would find a new career. You should consider that. reply francislavoie 6 hours agorootparentI'm not being user hostile. I'm being cognizant that the skill levels of users is extremely wide, and to best serve everyone from a UX standpoint, it's best to limit the characters accepted in the input. It protects the user, helps them avoid mistakes. reply guappa 6 hours agorootparentFrom reading your comments, you're extremely user hostile and I hope to not come across anything you work on because they are the very definition of antipatterns reply francislavoie 6 hours agorootparentIt's a decimal number field. There's no reason to have any other characters than [0-9\\.-]* You're overreacting. You're attacking me for no reason. reply guappa 5 hours agorootparentExcept if one copied it and it has a space in the beginning and your amazing script won't let one paste it. reply hluska 2 hours agorootparentprevnext [5 more] [flagged] AnimalMuppet 2 hours agorootparentNot everyone who disagrees with you is lying. Accusing people is a bad look here on HN. \"Assume good faith\" is the standard. You can say \"You're wrong\". \"You're lying\", though, is almost never appropriate, and is, in itself, quite hostile. reply hluska 1 hour agorootparentAgain “users are too stupid”. That’s remarkably hostile to users and doesn’t belong. reply francislavoie 1 hour agorootparentPlease read my other comments. I'm not using \"stupid\" in the derogatory sense. I'm using it as a recognition of the skill/knowledge gap between technical and non-technical users. reply AnimalMuppet 1 hour agorootparentprevFine. But \"you're lying\" is also hostile, and also doesn't belong. (I see that you edited that bit out of your post, so thanks for listening...) reply wodenokoto 10 hours agorootparentprev> what's crazy is both of those still don't disallow non-numeric input Which is really nice for copy pasting. For example, I have a number with spaces in it, and when I paste I get nothing or only until the first space because someone decided I'm not allowed to have non-numericals in the input field at anytime. reply account42 9 hours agorootparentprevYou shouldn't reject keystrokes as that can lead to silent errors when the user thinks they entered something that was not allowed. And final validation needs to happen on the server side anyway - anything before that is just a usability helper. reply joelanman 6 hours agorootparentexactly. Silently refusing input is an anti-pattern reply mananaysiempre 17 hours agorootparentprev> these arrow buttons A spinner control, that is. Spinners always puzzled me, to be honest. There is obviously a need for a compact numeric input control that both displays the exact value and allows rough changes using the mouse. Witness knobs in DAWs—which don’t actually work the way you’d expect from their appearance, you’re supposed to grab and then drag in a linear fashion, possibly ending outside the knob’s screen bounds. Or consider the weird input thing Darktable uses—which at least doesn’t mislead you with false skeuomorphism, but takes a bit to figure out. Then there are the inputs used for color grading in DaVinci Resolve. And so on. And all of them are nonobvious. Spinners are obvious, but they are also needlessly fiddly to use with the mouse, and neither do they provide the at-a-glance readability of knobs et al. I feel like humanity just hasn’t solved compact numeric inputs yet. reply lelanthran 16 hours agorootparent> Spinners are obvious, but they are also needlessly fiddly to use with the mouse I think that a quick improvement would be to let the mouse wheel \"spin\" the number up/down when the input element is focused. An even better improvement would be having the `` element actually display the value as it changed, and allow the user to set that value directly (say, by typing it in). Right now, range is useless because the user cannot tell what is selected. The developer has to add in extra JS magic to let the user set range to an exact value, or to show the user the value they have chosen. If `range` is improved in this way, then spinners are redundant and can be ignored. reply LegionMammal978 15 hours agorootparent> I think that a quick improvement would be to let the mouse wheel \"spin\" the number up/down when the input element is focused. Firefox did that for number inputs for a long time, until very recently. They switched it off in Firefox 130 because people kept inadvertently changing values in forms while scrolling through them [0]. Personally, I've set the about:config option to reenable it since I've found it useful in certain interfaces, though I can't imagine it's much longer for this world. [0] https://bugzilla.mozilla.org/show_bug.cgi?id=1741469 reply account42 8 hours agorootparentIsn't this same issue already solved for other scrollable elements? Scroll should only affect the individual element if the mouse was over that element when you started scrolling. I guess the room for unwanted consequences is a bit bigger when the scrolling controlls the value instead of just the viewport. reply ryncewynd 18 hours agorootparentprevThanks for the tip wasn't aware of that. I rarely want those arrow buttons for numbers reply silvestrov 4 hours agorootparentprevI think type=number is a mistake as it is too generic. It is difficult to get proper validation, UI and error messages. It should be split into: type=integer so the keyboard does not allow anything besides 0-9 type=decimal so the keyboard also allows decimal dot/comma and a fixed number of decimals! type=float which allows for scientific notation, e.g. 1.2e42 reply user3939382 7 hours agorootparentprevAnd yet in both cases the browser insanely casts the data type to string when reading the value. reply mattgreenrocks 19 hours agoparentprevIt’s amazing how many login forms are labeled “email” and then don’t have the correct type set. reply account42 8 hours agorootparentFrom https://developer.mozilla.org/en-US/docs/Web/HTML/Element/in... > Browsers automatically provide validation to ensure that only text that matches the standard format for Internet email addresses is entered into the input box. Browsers use an algorithm equivalent to the following regular expression: > /^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$/; Which doesn't even allow all valid email addresses :| No support for quoted local parts. No support for punycode domains. Disappointing. Useinstead to just get the email-optimized keyboard on phones without the misguided validation. reply friendzis 3 hours agorootparentIt's not the fact that software engineering is broken saddens me as much as the extent. Email is older than absolute majority of active developers and yet, it seems, simple knowledge like \"what is email address\" remains such an arcane knowledge that you are being looked at weird when present something a tiny little bit more correct than the status quo. Where else can we assume that the common knowledge is flat out wrong? reply Akronymus 1 hour agorootparent>\"what is email address\" Anything you can send an email to that is received, as far as I am concerned. I have my own domain, so why ahould I be forced to add an arbitrarily long string to it to be able to receive the mail? Or exclude @myDomain.com from being input on ? Or the worst of the worst: Disallow all but a few domains for emails. At this point, not validating it cient side at all is IMO the correct approach, and instead send a verification email. reply 85392_school 2 hours agorootparentprev> No support for punycode domains Really? I tested with a punycode domain on regex101 and it worked fine, do you mean that it only works with ones already converted to punycode? reply _heimdall 14 hours agorootparentprevThis is particularly annoying on mobile since on-screen keyboards won't adjust to an email input layout and autocorrect will screw up basically any email address as soon as a \".\" character triggers autocomplete to commit it's guess. reply icar 12 hours agorootparentAnd password managers will not fill in the credentials correctly. reply Sander_Marechal 9 hours agorootparentGet a better password manager? I use Bitwarden and it has never failed to fill out the login forms for me. reply crabmusket 8 hours agorootparentBitwarden has failed me on some sites. reply crabmusket 8 hours agorootparentprevI think that in some cases this could be because the inputs accept usernames as well as emails. But not in all cases, which is annoying! reply otherme123 11 hours agoparentprevI found this very extended with \"date\" input. It seems that every single frontend library has its own date widgets, and lots of them look awful in small screens. You have to add a JS and CSS just for that widget, some depending on jQuery. True, some of them are very configurable, but with the tiny cost of \"\" you have a widget that looks decent and native everywhere, probably cover your needs, and you can forget about it forever (e.g. no updates, no CDN). reply sahmeepee 10 hours agorootparentA lot of those date and time pickers will fail WCAG testing as well. In most cases, GDS now use just 3 text fields for day, month and year, but recommend pickers for dates close to today's date e.g. for booking an appointment, because a picker is easier if you don't know the exact date without reference to a calendar. Sadly they don't currently have a recommended picker, but there's a useful discussion of what has been tried here: https://github.com/alphagov/govuk-design-system/discussions/... reply Sander_Marechal 9 hours agorootparentprevThe native date input is terrible. Not the actual native control, which is usually okay, but the field itself. You can't even format the date! That's the number one reason I always use some datepicker library in combination with a regular text input. reply otherme123 8 hours agorootparentIMO, you (the web developer) should not be formatting the date shown in the input widget. The input should be shown to the client following the system settings, so a US browser would show MMDDYYYY, while an European browser would show DDMMYYYY. And the field is correctly (IMO) normalizing all of them to ISO-8601 (YYYYMMDD) before sending it back. And I have this opinion because I had to deal before with format configuration and normalization before saving to the database, when using a JS library. Now I just drop a \"input type=date\", and I know I'm going to get the ISO formatted date at the backend. reply Sander_Marechal 7 hours agorootparentIf my entire application used YYYY-MM-DD everywhere, it can be very confusing for users when the input suddenly uses something else. And often the locale or system settings are incorrect anyway. When I an working on a Dutch site with a Dutch locale and lang settings, then dates should be shown according to the Dutch locale, not according to whatever system of browser locale there is. I always keep my OS and browser in English (makes searching for error messages sooo much easier) but I hate that I need to change my OS settings just to get Firefox to display the date in a proper way on some site. reply otherme123 5 hours agorootparentIf it works for you, I guess it's OK. My apps usually deal with non-technical people (thus ISO in the browser input is out of the question), and sometimes spread at least between Europe and US (so people would enter MMDDYYYY and DDMMYYYY, and keep in mind that some dates work validate for both formats, namely every 12 first days for each month). Behind the scenes everything is ISO and UTC, but everyone gets their preferred format in the frontend. I'm not getting your Dutch example. If I'm in the US and use MMDDYYYY almost always, and want to access some Dutch site, what date format would you show me in the input form? DDMMYYYY only because the backend is hosted in Europe? You will get wrong dates (e.g. 11/1/2024 for the first day of november, your system will store 11th of january) for at least half the US visitors. Or maybe you are talking about \"dates displayed\" instead of \"dates input\", which is a different problem? reply poincaredisk 5 hours agorootparentTo be honest I would expect dates on a Dutch site to be shown in dutch format, just like I expect dates on American sites to be formatted in the US way. reply wadadadad 3 hours agorootparentHow do we know when a site is Dutch? Maybe check the URL to see if the TLD indicates? I'm in agreement with showing/inputting dates in the users' regional setting. reply Sander_Marechal 3 hours agorootparentnext [–]of course reply akurtzhs 18 hours agoparentprevUnfortunately the number input is lacking and inconsistent. We’ve always fallen back to JavaScript validation. reply epolanski 8 hours agorootparentAlso, many things that look like numbers shouldn't be encoded as such, e.g. credit cards or phone numbers, are not. E.g. any leading zeros get dropped out of phone number starting with a 0, very common. reply wahnfrieden 17 hours agorootparentprev`type=text inputmode=numeric` (from other comment) reply 6510 14 hours agorootparentprevhttps://jsfiddle.net/gaby_de_wilde/1qh4cax7/ I'm trying to picture a room full of people developers agreeing letters are numbers too! Lets give the little people a slider but lets call it a range! I really feel like they are trolling. You start with a neat database table then you engage in an endless struggle trying to allow the user to edit a single row of it. It really feels like you are not suppose to do it. As if it was intended to be as annoying as possible. reply hk1337 9 hours agorootparentThey’d want 7 parallel lines, 1 red, 2 green, 2 transparent, 2 blue. Oh, and 1 of those lines should be perpendicular. reply blueflow 7 hours agorootparentFor the confused: This is a reference to \"The Expert\" sketch: https://www.youtube.com/watch?v=BKorP55Aqvg reply oneeyedpigeon 5 hours agoparentprevThey could improve user experience everywhere if browser vendors wanted. E.g. why doesn't my (desktop) browser auto-complete recently-visited / bookmarked URLs when it sees type=url? reply xnx 22 minutes agorootparentPrivacy? reply p_l 17 hours agoparentprevI had to recently make sure that we do not use any of the more complex input elements, because we need to drive them from on-screen keyboard that for various reasons is also implemented in-page. And that means it's barely doable with normal inputs, the special ones support even less events. reply marcellus23 17 hours agorootparent> because we need to drive them from on-screen keyboard that for various reasons is also implemented in-page There's your problem right there. Can you expand on the reasons? It seems like very bad practice for a website to provide it's own \"keyboard\" instead of using the system keyboard. reply p_l 17 hours agorootparentThis is for a closed system that unfortunately sometimes is supposed to be available outside[1] - a touch screen panel UI for (big[2]) embedded system. It's hard to impossible to properly guide OS outside the browser regarding what keyboard we want at different points in time unless we end up also implementing custom keyboard plus some way to talk with it from JS. Previously we used a Chromium extension that could bypass some of the issues because it had privileged access and thus could send \"secure\" events. EDIT: For some extra context - for reasons of ease of use, we want the keyboard of appropriate type (numeric, qwerty, other specialized layouts) to show in position related to actual input field, not take entire lower part of the screen like typical on-screen keyboards. For dealing with possible edge cases or otherwise making it more accessible, we also provide for the keyboard to be draggable by user. [1] Sometimes it's accessed externally instead of in-person, for various reasons this means we have both the ability to open the web interface and use VNC. [2] By big I mean we have a complete PC running Linux there, with intel CPU/GPU and an SSD reply account42 7 hours agorootparentIsn't this use case already solved by phone browsers? Layouts are controlled via the inputmode attribute. Positioning the keyboard should be something solved by the host environment. reply p_l 7 hours agorootparent1. inputmode is very, very limited 2. \"Solved by host environment\" in practice means \"worse user experience\" as pretty much all of them simply use lower portion of the screen. In comparison, the current draggable on screen keyboard I spent significant amount of time getting working allows the worker to drag the keyboard around if it overlaps a piece of information they need, as well as takes comparatively little space on screen enabling workers to better see what they are manipulating. reply unlog 18 hours agoprevIn an all honest reply, is that the people that writes these specifications, live disconnected from the reality, they don't use the stuff they specify. That stuff works for very simple things, but then when your forms evolve you realise you will be better off just writing the whole thing yourself. reply marcus_holmes 13 hours agoparentThis. Doing relatively common things like cross-referencing from other fields (\"did the user specify a country? If so, we can validate the postcode/zip code they just entered, but if not we'll have to wait until they pick a country\") almost immediately require JS to handle, and as soon as you're using JS then it's all just easier to do in code than trying to mess around with validation properties. reply jhardy54 17 hours agoparentprevYep. This is great until you need a cross-browser date picker, at which point you need to implement a bunch of stuff yourself. It’s frustrating how primitive HTML forms are, after so many years. reply kmoser 14 hours agorootparentWhat do you mean? Most browsers supportnatively just fine. reply lelanthran 13 hours agorootparent> What do you mean? Most browsers supportnatively just fine. The `` on FF has a broken time-picker[1], has always been broken, and there are no plans to ever fix it, ever. [1] In that it will let you pick a date, but not a time - the time must be manually typed into the input! reply francislavoie 11 hours agorootparentWhat's hilarious is they do have UI for it in about:config \"dom.forms.datetime.timepicker\". It makes me so angry that it's not on by default. It works fine! reply cuu508 12 hours agorootparentprevOn Android, the date picker widget is fiddly to use for selecting distant dates, like date of birth. Not impossible but requires many many taps. reply sureIy 11 hours agorootparentThat's an implementation issue, not a specification issue. The specification just suggests the user is shown a date picker. reply talkin 11 hours agorootparentYou’re technically right but that doesn’t matter. That you’re correctly using html forms won’t quickly lead to browser improvements.. so the result is that users will hate your forms. Users/your customer might possibly even think that you’re to blame, and not $browserVendor. reply BalinKing 5 hours agorootparentI’ll go one further and say that the customers are absolutely justified to blame the developer instead of the browser. If a developer knowingly chooses a built-in form control whose common implementations are bad for their users, how are they not at fault for the resulting experience? “This site only uses functionality provided by the HTML spec” is not a useful goal in and of itself. Using the right tool for the job, which might be JavaScript, is always more important. reply alexvitkov 10 hours agorootparentprevIt doesn't really matter whose fault it is, the end result is that your users have bad experience on your website. reply jeroenhd 9 hours agorootparentprevI've never had a problem with that myself. I guess people don't know you cab tap the year in the date picker to quickly go back a bunch of years? reply nitwit005 11 hours agoparentprevMost of the forms features are from the early 90s. You're not working in the same millennium as some of the specification writers. reply skrebbel 10 hours agorootparentThe `required` attribute, which this article is about, is an HTML5 thing and first appeared in browsers in 2010-2011. So sure, not brand spanking new, but the web was already used to write modern apps. There's no good reason for the validation features to be so shabby. reply Sander_Marechal 9 hours agorootparentEven 'required' doesn't work properly. Browsers do very odd and inconsistent things when your required field is hidden when submitting. Like in a basic tabbed or multi-step form. reply mcflubbins 17 hours agoprevIf you have a checkbox with a label, please a \"for\" attribute to the label so I can disable/enable the checkbox by clicking the label. This is one of my biggest pet peeves, maybe its just me. reply wruza 13 hours agoparentWrapping input into a label also works. Not sure why people tend to separate the two. And also why browsers started separting these. It’s a checkbox and radio that should contain a text, not vice versa. reply pirate3215 12 hours agorootparentOne reason I have heard about is implicit labelling doesn't work with all voice control tech, including macOS voice control[1] [1] https://a11ysupport.io/tests/html_label_element_implicit reply ozaark 16 hours agoparentprevNot just you it's a required feature of accessible sites following ADA/WCAG. reply calibas 16 hours agoprevHere's a simple example that doesn't use React: https://developer.mozilla.org/en-US/docs/Web/API/HTMLObjectE... reply royal_ts 4 hours agoparentI was thinking that it's so weird to talk about using standard HTML validation and then everything is shown with React? If we want to teach people how standard works, we can't assume React as the default. reply everdimension 1 hour agorootparentI addressed this elsewhere in the comment section, but there's not much \"react\" going on in the article. I do think that JSX is very expressive and the concern I cover mostly involves the declarative \"component\" model for writing UIs. It's not react-specific. reply hk1337 8 hours agoparentprevI believe you could probably do most of that with just css now too. Disabling the default pop up from showing up may be the only thing you need javascript to do. reply jansommer 17 hours agoprevHtml form validation is great. There's just one gigantic catch: It doesn't work in Firefox for Android. https://bugzilla.mozilla.org/show_bug.cgi?id=1510450 reply jeroenhd 9 hours agoparentI'm using Firefox on all of my mobile devices but I can't blame devs for ignoring FF for Android on this one. This is an API every other browser had working 10 years ago, it's on the Firefox team to sort out this mess. The less than a percent of users who don't see the error messages for controls marked in red isn't worth ignoring the standard for. The more websites use this, the more pressure Mozilla will feel to fix their browser. This isn't something like WebMIDI or whatever API Chrome or Safari implemented before standardising, it's part of the original HTML5 spec. reply FrostKiwi 17 hours agoparentprevAs a daily Firefox on Android user, not catching up on standards is what hurts the most. Most painfully to me, all the WebGL stuff like [1] and some minor annoyances like [2]. Still, having uBlock origin among other extensions is a killer feature. [1] https://bugzilla.mozilla.org/show_bug.cgi?id=1884282 [2] https://bugzilla.mozilla.org/show_bug.cgi?id=1897707 reply pglevy 6 hours agoparentprevI just realized this having switched to FF on Android fairly recently. Working on an app and saw literally nothing when trying to submit an empty required field. Couldn't imagine what I was missing until I searched. I was stunned. No issue with rolling your own validation but this should work! reply arp242 7 hours agoparentprevIt's much worse than \"doesn't work\": the validations work, they just don't show any error. It would be okay if validations just weren't implemented, but this is just absolutely fucked. I discovered this years ago after a long and painful debug session, and I'm quite sure I wasn't the first or last to go through that. reply yawaramin 7 hours agoparentprevThat's weird, caniuse says it works: https://caniuse.com/?search=constraint%20validation%20api reply moron4hire 17 hours agoparentprevFirefox for Android has a smaller user base than Samsung Internet and Opera. It's 0.5%. It's a waste of time working on supporting it. Especially considering how little time people put into making sure their sites work for people using accessibility software. I don't think it's worth mentioning in these issues unless you're also ready to talk about UC Browser. reply johannes1234321 14 hours agorootparentIs that somewhat biased? - I assume the number of people using Firefox has a quite big overlap with people using ad&tracking blockers, which block many statistic sites. (Website operators may log user agent which will be somewhat accurate still) reply marcus_holmes 13 hours agorootparentThis is always something I question whenever the analytics numbers come out. I know my adblocker blocks GA, so I'm not going to be included in any GA statistics. Every time I've asked a marketing person about this they get hand-wavy about \"it all comes out in the wash\". But meanwhile they say things like \"our analytics show that our market is mostly older and non-tech-savvy people\". I'm not sure that the analytics numbers do actually show that. I think that's just the demographic that you can see. reply makeitdouble 16 hours agorootparentprevI wanted to quickly check the 0.5%, and see 1.2% in Japan for instance, where iPhone have near 80% share. https://gs.statcounter.com/browser-market-share/mobile/japan That's still not a lot, but above 1% is a decent threshold to decide to support a browser. reply fortyseven 15 hours agorootparentprevYay, I'm in the point five percent! reply vips7L 19 minutes agoprevIn React I just use Formik and Yup to make forms painless. I've yet to discover a better way. reply temporallobe 17 hours agoprevThere’s a reason it’s heavily underused. So many frameworks and libraries provide robust, style-able validation capabilities, some with very sophisticated and extendable functionality. Don’t torture yourself if you don’t have to. reply mrweasel 7 hours agoparentYou also have to implement your own validation on the backend anyway. There's always going to be someone trying to fiddle around with your form, either using some weird browser, curl, or some other tool that doesn't have the same form validation built in. You're not going to trust that the client actually did the validation, or did it correctly, so the backend still needs to be able to validate input and show the form, with validation errors, on all fields. Frontend validation is only there to be helpful for the user, but if you can style it, or trigger it from the backend on submit, you have to implement your own styling anyway. reply dwg 14 hours agoparentprevAgreed. We try to use browsers standard features whenever possible. Despite looking into using the built-in validation, it's never been worthwhile. Too many gotchas, and we end up using a library to be able to easily support more complex checks anyway. Furthermore, using a library opens up, in some cases, the possibility of sharing some of the validation code between front and backend. In particular this article seems to work around one of the issues with `useLayoutEffect`. Not something that should be done lightly. reply donatj 21 hours agoprevYou gotta be careful about going overboard with it. Recently I was trying to get a refund on Groupon because the company I'd bought a Groupon for was under new management that refused to honor my groupon. The form had a stipulation \"minimum of 15 words\". Try as I might, I could not get the form to pass validate until I inspected the HTML.\\w - word characters \\b - word boundaries \\s - white Literally zero allowance for any sort of punctuation. reply recursive 19 hours agoparentI don't think this is particularly pertinent to HTML validation. This is true of any type of validation. The same rule could have been applied on the server, and then you would have had no hope. reply whoisthemachine 5 hours agorootparentExactly. My rule of thumb is \"don't add validation until there is absolutely no better way and you absolutely must restrict this input.\" reply everdimension 6 hours agoparentprevThat's actually a good argument for the point I was trying to make. Existing validity attributes such as \"pattern\" are cool, but not enough. E.g. the \"repeat password\" example is actually achievable without \"setCustomValidity\" by using the \"pattern\" attribute. For that, you would have to dynamically construct a RegEx out of the value from the first input. I didn't want to make the article too long by comparing the solutions, but the point is, with the \"customValidity\" you see how much more eloquent and easier to read the validation is. So a nicer API here makes all the difference The \"15 word minimum\" constraint would look so much nicer as \"value.split(/\\s+/).length >= 15\". reply acdha 4 hours agoprevI share the wish that there was more effort in the browser space to improve the built-in controls but I would also recommend that people thinking they can easily do better try some real usability and especially accessibility testing. One very nice trait of the standard APIs are that they’re very lightweight and people who build assistive tools like screen readers and Braille displays know about and support them. It is so easy for developers to think they have something better after a simple NPM install, until they test it on a slow (i.e. median) phone or watch a blind person try to use their application and then spend weeks trying to improve things. Given how common web forms are it’d be really nice if we had an Interop-style competition focused on making the out of the box experience better for normal people both for the controls integrated in browsers and the myriad of JavaScript widgets. reply Macha 19 hours agoprevOne of the things I dislike about HTML form validation is it starts running from page load. So if e.g. you tie error state formatting to it, the form loads up with a bunch of errors which may be intimidating to the user. reply Latty 19 hours agoparentThere is the :user-invalid pseudo-class that lets you avoid this to some extent, but it has some inflexibility that may mean it isn't enough, depending on your use case. https://developer.mozilla.org/en-US/docs/Web/CSS/:user-inval... reply maxbond 18 hours agorootparentThere ought to just be a property named something like `defer-validation` that does the right thing. But I'm sure I'm not the first person to suggest it, there's presumably some logistical or technical difficulty. If I'm making a wish list I'd also like to point a property at a handler function that accepted a `string` and returned a `stringnull` (or at least a `nonempty stringempty string`) rather than using an onchange handler, but it is what it is. reply sholladay 19 hours agoparentprevThis is one of the main reasons people started using JavaScript, to show errors only after a form has been “touched” or right before it is submitted. reply mattmanser 18 hours agorootparentA bit perplexed by your comment. That wasn't a main reason people started using javascript. I even remember when people started evangelizing client-side validation in the mid-2000s. Javascript was already a normal tool used in web apps by then, and most web developers would regularly be adding javascript to their apps. Back then it was a bit of a pita as you had all sorts of gotchas with javascript memory leaks by registering change events on controls. I can't even remember the term now, but you could basically have a circular reference between a closure and a control and so it wouldn't cleanup properly. Also, modern developers probably can't even begin to imagine how slow javascript was on IE6. A loop of a few 100 iterations could bring it to an unresponsive halt. reply bryanrasmussen 16 hours agorootparentprobably they mean why people started using JS in form validation, and not JS altogether, although agree that isn't the reason either. reply sholladay 16 hours agorootparentCorrect, I meant that client-side validation driven by JS became popular because of UX issues when using pure HTML. There have always been plenty of other reasons to use JS with forms besides validation. But it’s notable because forms and form validation are such common building blocks that if people feel the need to use JS there, it kind of infects everything else around it. IMO, being able to build high quality form experiences without JS is critical to reducing bloat on the web. reply Macha 7 hours agorootparentprevJS form validation quite significantly predates HTML form validation. The alternative was server side form validation, for the longest time. HTML form validation was meant to standardise and reduce the need for JS validation, but for UX reasons it hasn't really managed that. And that you'll need JS anyway if you're doing validation specific to your business logic that doesn't fit into the set of rules HTML validation has. reply ericwood 18 hours agoparentprevI've never understood why this was chosen as the default experience. Most users don't enjoy having every form element yelling at them for actions they haven't even had a chance to take! You can script around this, but at that point the feature isn't doing much. I'd argue this is the biggest reason you don't see more widespread adoption. reply userbinator 18 hours agorootparentPerhaps the same reasoning that IDEs show syntax errors before you save the file? (No, I don't like that feature either.) reply blacksmith_tb 15 hours agoprevMy personal bête noire is sites that misuse type=password for 2FA inputs, since that confuses password managers and browsers both. reply lozenge 15 hours agoparentThere is a way to annotate it. autocomplete=one-time-code https://developer.mozilla.org/en-US/docs/Web/HTML/Attributes... reply francislavoie 11 hours agorootparentTrue, but vast majority of websites don't do the right thing. So password managers need to manage a database of form input overrides. I worked at a small company building a password manager and it was a bit of a nightmare, we had to allocate support staff resources to handling reports of website incompatibilities. reply n_plus_1_acc 6 hours agorootparentI'm curious, why did you implement your own password manager? reply francislavoie 6 hours agorootparentI worked for a small company that did. The idea was the password manager lived only on your phone, and you'd connect it to your computer in various ways. Notably including our own hardware, USB nub that connected to the phone app via Bluetooth and acted as a keyboard for the device and the app would instruct it to do the keystrokes to enter your saved credentials. Also a browser extension that the app would connect to over websockets (through our relay servers, DH key exchange to encrypt the connection end to end) and the extension would request the app to send down the credentials on demand. Also we were a very early adopter/implementor of FIDO U2F (security keys). The product kinda hinged on the paranoia of the userbase (which I never aligned with, I trust 1Password and cloud sync'd encrypted vaults for example) so it never really took off. It wasn't \"my\" product, but I worked on it for a few years, right out of school. reply teaearlgraycold 15 hours agoparentprevAlso completely unnecessary for passwords that expire in 30 seconds. reply ozim 11 hours agoprevBecause it sucks. It does not translate with the application but with browsers settings, it doesn’t style or fit any design. It looks differently on different browsers and it is really hard to explain to stakeholders “this is from browser I don’t have control over it”. reply everdimension 5 hours agoparentIt does suck, but I think not for the reasons you mention. I truly believe a nicer API would motivate developers to use it, and if native validations satisfy the product requirement, their styling does become a lesser concern. But surely styling is still important, but another great topic to write about is the fact that you actually can opt-in to showing native validity errors in a custom way! reply DeathArrow 11 hours agoprev>Imagine a username input that should be valid only if the username is not taken Maybe that is user friendly but for sure I don't like to see the backend bombarded with API calls each time an user types a letter. reply fhdsgbbcaA 25 minutes agoparentIt’s a major security/privacy issue, you don’t want to tell world+dog all registered users, especially since that’s typically an email address. Huge, huge, massive “no no”. Likewise you still have to do sever side validation as any client side code can be modified, or you can just send payloads directly to the server. IMHO client side form validation is dangerous as it gives a false sense of security. reply everdimension 5 hours agoparentprevYou usually create debounced inputs for that. This is similar to the autosuggest and typeahead inputs and comboboxes: sending requests to the server in response to an input change isn't something unusual reply eru 11 hours agoparentprevIt's not actually all that bad, if you do it asynchronously and in batches (as much as possible). The total amount of traffic in both direction is pretty small, and the logic is simple, especially compared to lots of other things your server is typically doing. reply bugtodiffer 11 hours agoparentprevWebSockets, then it's a couple bytes a click reply mrweasel 6 hours agorootparentI don't think the amount of traffic is necessarily the issue. You're validation could be fairly \"expensive\". Maybe you need to do a lookup in a legacy system, maybe you need to check multiple systems? You'd probably have to wait until the user moves on to the next field, if there's more, but that's also a little silly, as you'd force the user to go back to a previous field. reply joshchernoff 21 hours agoprevsimply adding required is all you need.Not required=true The omission is equal to required=false. No one really write required=true, they just add the attribute `required` only by its self. This is one of the odd quarks about html attrs Same is true for things like disabled ect https://developer.mozilla.org/en-US/docs/Glossary/Boolean/HT... > The strings \"true\" and \"false\" are invalid values. To set the attribute to false, the attribute should be omitted altogether. Though modern browsers treat any string value as true, you should not rely on that behavior. in other words required=false may still end up making the field required. FYI. reply Etheryte 19 hours agoparentThey've used `required={true}`, not `required=\"true\"`, which is JSX, not HTML. The one with curlies isn't even valid HTML. In the old HTML spec, the correct value, if you wanted to set a value, was to set `required=\"required\"`, but these days the spec is looser since it tries to conform to the web, not the other way around. reply joshchernoff 19 hours agorootparentEven in jsx its not required to add a boolean value. Unless you are passing in a var as a prop in which case sure. But that didn't look like it was the case in these examples. reply everdimension 4 hours agorootparent> Even in jsx its not required to add a boolean value Absolutely true! But I like to do it because I personally think it reads more nicely and is more explicit and that's what I do in all my projects. But it is indeed a matter of taste and I have nothing against code that follows the convention to omit the \"true\" value. reply ervine 18 hours agorootparentprevOne of my favorite eslint rules to enable: https://github.com/jsx-eslint/eslint-plugin-react/blob/maste... reply deathanatos 19 hours agoparentprevThey've written it in JSX, I think, not in HTML. reply mannyv 18 hours agoprevThe real problem with client-side validation is you can't trust it. You need to revalidate on the server, no matter what. reply yjftsjthsd-h 18 hours agoparentIt's not a security feature, it's a UX feature. reply thrdbndndn 16 hours agorootparentI'd argue it's often an anti-feature. These validation at input can be super annoying when you're copy-pasting your strings from other sources, and want to edit them in textbox in-place. Especially if you're on your phone. I've be frustrated by this on numerous websites, a lot. reply account42 7 hours agorootparentAnother common failure case is only updating the validation state on key presses but not other content changes like auto fill. reply mrweasel 6 hours agorootparentprevIt's a UX feature, absolutely, but you can't trigger the browsers validators from the backend, so you'd need to implement your own styling anyway, probably even add a bit more information than the browser can natively handle. reply Jerrrrrrry 18 hours agorootparentprevTrue, but if there's a communication bug between UX and back-end teams, that can escalate into a false sense of security and then an exploit. reply kadoban 17 hours agorootparentWhat the hell back-end team relies on their front-end teams to tell them to do input checking? Nothing my front-end people could possibly do would trick me into trusting user input. reply Jerrrrrrry 16 hours agorootparentin 2025? you'd hope not but also cross-system context switches, amateur RegExp, escape character (ab|mis)use, etc. can make \"user-input\" propagate farther than any one team's boundaries. assertions/test coverage/fuzzing at every boundary so user-input taint analysis can't fail is a requirement for a system that passes user data around more than one time or tech stack. reply wolrah 16 hours agorootparentprev> True, but if there's a communication bug between UX and back-end teams, that can escalate into a false sense of security and then an exploit. How so? The backend ALWAYS validates. No communication necessary, whether or not the frontend also validates doesn't matter to the backend. Frontend validation is to improve user experience, nothing more. reply bryanrasmussen 16 hours agorootparent>Frontend validation is to improve user experience, nothing more. and to reduce server load. reply Jerrrrrrry 15 hours agorootparentprev>The backend ALWAYS validates. Exactly. Sometimes in a system so big compartmentalization is required yet meta-communication is inhibited, that comfort-ability can lead to false asserts / assumptions. \"We always validate - no need for _me_ to do it\" type bystander effect / diffusion of responsibility. reply blackoil 17 hours agorootparentprevThat's one benefit of using ts on server. We share same zod validation on both end. reply hn_throwaway_99 17 hours agorootparentprevYou were being downvoted here, but I think you make a great point. The problem with having separate client-side and server-side validation logic is that you (generally) want the rules to be the same, but you end up needing to write them twice, usually in completely different technologies. I have seen many, many cases where the client-side validation and server-side validation got out of sync, and then just like you put it, obscure bugs or security exploits can arise from this. In general, I think client-side validation should really only be used for the basics, often with respect to type (e.g. the email/URL examples given) and just basic \"required\" (non-empty) fields. Anything more complicated than that should be done solely server-side in my opinion - e.g. I wouldn't use setCustomValidity with a complex set of client-side rules. What I think is important, though, is to ensure that if the server validation fails that the error message that comes back is not just a single \"Bad input!\" message, but errors can be keyed by field to ensure you can display field-specific error messages and error highlighting. Another option I considered back in the day when my backend was on NodeJS is to have the server return the text for a JavaScript validation function before the form itself is actually rendered. This, this function can be run client-side for immediate feedback when the user submits the form, but it's also the exact same logic that is run on the server when the form values are received. reply doytch 17 hours agorootparentThis was one of the biggest \"oh shit\" moments I had when learning Remix: I could reuse the same validators across front and backend and they could even be right there in the same file. reply everdimension 5 hours agorootparentprevCouldn't have said it better! reply edweis 13 hours agoprevThe best native HTML validation is server-side validation. The only downside: the user has to wait 300ms. reply gdwatson 12 hours agoparentAnd he loses his page state if there is an error. You have to do server-side validation regardless, but client-side validation can be a lot more pleasant for the user. I used to think it doubled your workload to do both, but if you are using JS-free client-side validation, I think that the server can just return an HTTP error guilt-free for any invalid input that the browser will catch. It’s a pretty good compromise for format validation. reply edweis 12 hours agorootparentWhen returning an error, you can fill the input values in your HTML response. Indeed the most pleasant UX is to set an additional validation logic on the client side and I don’t think this should the go-to solution. reply account42 7 hours agorootparentprevUsing your browser's back navigation from the error page to the form should result in the values still being there unless you are doing something stupid with client-side rendering. reply everdimension 5 hours agorootparentprevGreat answer, exactly! Client-side validation isn't meant to remove the need for the server-side checks reply throw_m239339 7 hours agorootparentprev> And he loses his page state if there is an error. No, you can handle that on the server too, easily, since you can fill the form with the last known state upon page loading. reply bob1029 10 hours agoparentprevAssuming the actual validation of form input takes ~1ms, this is a fairly unusual amount of latency to experience in 2024. You've got tech stacks and cloud services that will put you within 20ms of 99% of your users. Server side everything is still the safest possible bet you can make. reply NoGravitas 4 hours agoparentprevThe best solution is to use HTMX (or similar) to hook into your server-side validation so that you only have validation code one place (on the server), but you don't have to wait for complete form submission before finding an error in a field. Example: https://htmx.org/examples/inline-validation/ reply threatofrain 18 hours agoprevYa'll may want to checkout Valibot¹ or Zod² in conjunction with something like React Hook Forms³ or the more agnostic Tanstack Forms⁴. Really sweet form validation that's concise but as precise as you want it to be. The problem with vanilla form validations are (1) they're so basic that it's table stakes for any library or framework in this space, even ChatGPT can do it well, (2) there's an enormous amount of other validation scenarios they don't cover, and (3) unless your validation is simple and doesn't require a validation library, now your logic is split between two places. [1]: https://valibot.dev/guides/comparison/ [2]: https://zod.dev/?id=basic-usage [3]: https://www.react-hook-form.com/get-started/#SchemaValidatio... [4]: https://tanstack.com/form/latest/docs/framework/react/quick-... reply everdimension 5 hours agoparent> there's an enormous amount of other validation scenarios they don't cover Can you provide examples of those? Genuinely interested as I'm on a quest of creating a list of recipes that show that native form validation can be just as capable as custom validation libraries reply montag 18 hours agoprevIt's also easily misused. Take the regular expression validator for passwords on the California DMV website, for example. The website states \"Must include at least 4 alpha characters\". But the validation pattern ^(?=(.*[a-zA-Z]){4,})(?=.*[0-9!#$%]).+$ requires that these characters appear consecutively. reply everdimension 4 hours agoparentThat's exactly the case where the \"customValidity\" attribute shines! I have nothing against regex and the \"pattern\" attribute is the way to go for many cases, but having this is an alternative is also very nice: const valid = value.length => 4 && isAlphanumeric(value); return () reply kaoD 4 hours agorootparentAFAIK customValidity is not an attribute and you can only use an imperative setCustomValidity API which is terrible cumbersome to use in a declarative framework like React. reply everdimension 4 hours agorootparentYeah this is exactly what I'm writing about in the article :) reply everdimension 4 hours agorootparentprevYeah well I promise it does read nicely when there's formatting which HN comments do not allow :) reply eddd-ddde 17 hours agoparentprevThe {4} is being applied to the whole group which includes a .* Isn't that correct? reply Izkata 15 hours agorootparentYep, and the .* means \"0 or more of anything\", so it's 4 or more groups that each end with a letter. They can be consecutive or not and a group can be a single letter but doesn't need to be - so whatever the failure was, it wasn't that (or the regex was typo'd here to be correct instead of what was actually on the site). reply account42 7 hours agorootparentThe regexp still requires four letters before the last digit or special character which is a weird requirement. reply GrantMoyer 6 hours agorootparentThe (?=…) are \"lookaheads\". They match the enclosed pattern without advancing the cursor. reply account42 9 hours agoprevThe last example is bad. You shouldn't scream at your users before they even had a chance to enter the required information so the second password field should not be marked red until the user either is done entering text there (onblur) or tries to submit the form. reply everdimension 5 hours agoparentThat's totally true! Invalid states shouldn't be shown sooner than necessary It's just that for the demos in the article it makes sense to show invalid states as soon as possible, but for a nicer UX in real apps you need to take into account \"touched\" and \"submitted\" states for the form and per input For the demos I wanted the reader to know at a glance when our validation code takes effect and this obviously comes at a conflict with demoing a fully \"real-world\" behavior reply a2128 8 hours agoparentprevThis is one of my big gripes - when apps are trying to get ahead of me with validation or submission. When I'm entering a 2fa code, I don't need a big red error telling me that the code must be 5 digits before I'm even finished. Worse is when they immediately auto-submit upon entering the last digit, so if I made a mistake I can't backspace and correct it reply gunalx 18 hours agoprevI do get the point of using form validation clientside to ensure a better ui. But dont remenber to also verify serverside. Anything clientside can have been fumbeled with. (Also kinda anoying to have to duplicate this tho) reply mcflubbins 17 hours agoparent> Also kinda anoying to have to duplicate this tho Oh yes it is... I once worked on a project where we had a tool that dynamically built client-side forms based on each customer's needs. After rolling it out for a big customer I discovered (after the original engineer responsible had already been fired for other reasons) that all of our dynamic forms had NO server side validation. none, zilch, nada. We did have client side validation though, a good amount of it and nearly all of it was using native HTML features, and we had the whole form available server-side as well. So given that I was under a crunch, on form submission I loaded the HTML form server-side, manually iterated over all the elements and re-implemented the validation checks that were embedded in the HTML. Crazy times, but heck it was flexible! reply Jerrrrrrry 18 hours agoparentprev> (Also kinda anoying to have to duplicate this tho) Security and convenience are like space and time, you can't move one without transformation of the other. reply maxbond 8 hours agorootparentI think it's a trilemma between security, convenience, and architectural sophistication (NB: I'm deliberately not saying complexity, because the code doesn't necessarily get more complex). It is usually physically possible to find a solution with equal security for a given level of convenience, but it will require an investment of creativity and possibly refactoring to realize. Both of those things are very expensive. To take an extreme example, you could ensure that validation happens identically on both the back and front end by writing your own framework with that property. You could create a framework with no more security bugs than the next best alternative and while providing great UX. But writing a framework and shaking out the bugs is a huge lift. So in practice you can't go all the way out on the third axis and it is approximately a dilemma. But if you're on the lookout for exceptions you may find an opportunity to cheat/curve bend (eg as suggested by other commenters, when using JS for the front and backend, you can use data modeling libraries like Zod to get most of the benefit for a fraction of the price of writing a framework). reply woodpanel 17 hours agorootparentprevYou could fill those setCustomValidity() calls in the client with rule-sets generated on the server. even re-fetch them from the server on each input change in the client or just ditch the whole thing and do it in htmx :-> reply yawaramin 7 hours agorootparentOr do all of the above https://dev.to/yawaramin/handling-form-errors-in-htmx-3ncg reply Jerrrrrrry 16 hours agorootparentprevCould you not remove the event from the input change, replacing it with NOP? htmx looks dope af thanks reply okasaki 15 hours agoparentprevFlask-WTForms will generate the same validation both client and server-side. Is that not usual? reply SahAssar 18 hours agoprevIt's a bit disappointing that articles talking about HTML use JSX/React syntax instead of actual HTML (even more so not actually saying it). Example from the article:reply mcflubbins 17 hours agoparentI thought the same thing. I was once discussing a third-party integration with a React developer. The integration required that our app POST a couple of fields to the third-party's site. I found that the developer was struggling with the integration and they were asking me questions about it when I said something to them along the lines of \"It's just an HTML form, with a couple of hidden inputs that when submitted make a POST request to this URL\" they said to me \"Yeah, well HTML is kinda old, it's not really used anymore\"... I'm sure I've said plenty of stupid things when I was green but I hope no one remembers them like I remember this one. It lives rent free in my head. reply everdimension 4 hours agorootparentIt's definitely true that many developers would benefit a lot from learning more about the basic HTML and the web platform. But I refuse to support the notion that this is somehow React's fault. In my personal experience, react allowed me to rely more on the native web platform APIs, not less, than other frameworks (at the time that I switched to react) reply lelanthran 16 hours agorootparentprev> they said to me \"Yeah, well HTML is kinda old, it's not really used anymore\"... > I'm sure I've said plenty of stupid things when I was green but I hope no one remembers them like I remember this one. It lives rent free in my head. I'm doing gigging while my product is gaining traction. Last week, I received this verbatim rejection for a PR review at a client, who's oldest developer is 27: \"No, we don't want all the logic in the same place. You must split the logic up\" This is also one that will take up valuable space in my head till the end of time :-( (PS. Yes, I split the logic so that it was evenly balanced in 3 different programming languages, instead of having the entire logic in C# alone) reply hmeh 15 hours agoparentprevThe confusion in the article is so complete that I'm left wondering whether or not the author is aware that what they are writing is not, in fact, HTML. reply everdimension 4 hours agorootparentYeah, I am aware! Thank you for the concern :) I did address this in an adjacent comment, but I'll say again that I did contemplate over using JSX or not. Also yes, it may have been a good idea to add a disclaimer for the fact that the code I'm showing is JSX, but honestly there are so many other disclaimers I had in mind, all of them together would make the article twice as long and much more boring reply GrantMoyer 6 hours agoparentprevIt's exacerbated by the fact that the API they propose to make custom validation more ergonomic works for React, but would be much worse for plain Javascript and HTML. reply everdimension 4 hours agorootparentThe API I'm proposing would indeed bring much more benefit when used in a declarative way. That's the point I'm specifically trying to convey in the article. I don't think I understand how it would be \"worse\" for plain JS and HTML though. Would love to hear your thoughts. Actually, there is one possible concern. When HTML is returned by the server includes inputs with set \"custom-validity\" attributes and this HTML gets open by a browser with no javascript enabled, this would make the input \"stuck\" in an invalid state. This is an important edge case to consider but I do believe there is a resolution that would satisfy everyone reply everdimension 4 hours agoparentprevSorry to disappoint, I did hesitate over this. But JSX is honestly very nice to read and also I didn't want to leave the impression that opting in to native form validation somehow forces you to not use javascript. And combination of javascript + html is, again, very nicely expressed with JSX. The concepts are obviously easily translated to other component frameworks, but they also do apply to pure HTML and vanilla javascript. The problem I am highlighting in the article is the absence of a declarative nature to the Custom Validity API, so I think it makes sense to use a declarative component approach in code examples reply Vox_Leone 5 hours agoprevGuilty. Recently, when I was involved in a project that required a lot of attention to forms, I ended up overlooking the importance of basic simplicity. I regret it a bit. Thanks for sharing your perspective. reply k__ 9 hours agoprevBuilt-in validation is pretty much the only reason I use forms. Capturing input with frameworks is much simpler than pulling the values out of an event object, so I'd be okay with just using input and button elements. Yet, that doesn't trigger the validation, so I end up wrapping it with a form element and using a submit button. reply lelanthran 8 hours agoparent> Yet, that doesn't trigger the validation, so I end up wrapping it with a form element and using a submit button. I run the validation manually using `reportValidity()` on `element.querySelector(...)` before passing it on to whatever. reply 4rt 18 hours agoprevthis is just react though. it's not HTML validation. reply everdimension 4 hours agoparentThe HTML is created using JSX, that's true. But the validation that the browser performs is part of the HTML behavior. reply JodieBenitez 10 hours agoprevDo we finally have a standard solution for input masks ? 25 years ago I was struggling with this while my fellow desktop app devs had good input masks in their widgets, with easy to use pattern syntax. That would be embarrassing if not. reply blueflow 8 hours agoparentEvery other website or program now uses a different set of UI patterns. We are not advancing, we are regressing. I do miss when textboxes had inset and buttons had outset borders. reply jonathrg 19 hours agoprevThat's nice, I'll use that next time. Although it always feels kind of bad to write client side validation code because you're going to have to do the same checks on the server side anyway. reply dvdkon 19 hours agoparentI find it sad that so many frameworks leave the developer to duplicate server-side and client-side validation. There are obviously some things you can't reasonably validate on the client, but I'd like to see more automated ways to take backend constraints and check them on the client too. Ideally constraints would also propagate from model definitions, so there could be a single source of truth for \"what phone numbers do we accept\". Some years back I tried to do this by parsing SQL table definitions, but never got far enough. Django does this, but it lacks pretty much any client-side validation support IIRC. (Or client-side anything, really...) reply yen223 19 hours agorootparentOne of the compelling reasons to use Javascript-based frameworks like nextjs or remix.run is that you can reuse the same logic + types on the frontend and backend. With Typescript, this is now my preferred way to build full-stack sites. reply recursive 19 hours agoparentprevDo you make your users do a server round-trip to see what's wrong? To have good UX, you kind of have to do both. reply ervine 18 hours agoparentprevYou could use something like json-schema to define constraints, and use json-schema validator libs on the front and back end to validate the form data against the schema. You still need to handle the \"what happens next\" part on each side, but at least your validation rules are shared. reply nfw2 15 hours agoparentprevWhy is that bad? Both have uses: 1. Client-side validation is for immediate feedback for the user. 2. Server-side validation is to block malicious actors reply cloudking 19 hours agoparentprevNecessary because a user can always inspect > modify the HTML. reply wvenable 19 hours agoprevAfter reading all this, I think I'd still choose to do it away from the browser built in capabilities. I'd rather have full control over the process and the design than rely on the limited capabilities of browsers. The browser is programmable; at this point they should stop getting clever about adding built-in functionality and instead just expose better ways to use the browser as a dumb UI toolkit. reply Latty 19 hours agoparentCouldn't disagree more. Browser features can be much better at handling edge cases: disability, localisation, unusual devices, different input methods, weird screen sizes, etc... rather than hoping every developer does it right, building that in is more efficient and consistent. The common cases should be handled really well by browsers. reply wvenable 19 hours agorootparentI couldn't disagree more; it might be good for disability but for localization, unusual devices, different input methods, weird screen sizes I have never seen that well executed by browsers. I almost always prefer a more full-featured alternative from a standard framework than whatever is lowest-common-denominator feature in a browser -- which also, depending on the browser, may or may not work the same or may or may not even exist. The browser should provide low-level capabilities and let developers build the high-level functionality from it. This article ultimately supports this by showing us exactly how half-baked this particular browser feature happens to be. reply JoshTriplett 18 hours agorootparentBrowser functionality is typically (handwaving on exact numbers here) better than the worst 80% of sites, on par with the next 10%, and not as good as the top 10%. If you're putting in the effort to build a site in the top 10%, sure, you might not want to be \"held back\" by the browser. But the vast majority of sites would do better by using what's built into the browser. And I would argue that the value the user receives from that top 10% of sites is not commensurate with the pain the user receives from the sites that think they're in the top 10% but aren't. reply wvenable 18 hours agorootparentI don't know, what browser functionality are we talking about? Even this form validation has to be implemented; I don't think I've ever even seen it in the wild. What percentage of websites are rolling their own functionality instead of using any one of the various library and framework that do this better? There is no way that every single browser is going to implement some high-level feature in a satisfactory and future-proof. It's the wrong place to do it. Almost every attempt has been at least a partial failure. We might get something maybe a decade after it's in common use and then only the most basic version of that. And we get browser bloat for our troubles too. I don't use the standardcontrols in apps anymore because the build-in version is too simplistic. Don't even get me started on file uploads. Is everyone just supposed to be held-back by the minimum a browser can do everywhere? Why waste browser developers time building things that no one really wants to use. reply ryandrake 17 hours agorootparentAs a user, I want common UI controls to look and behave the same way across all web sites. Ideally, across all applications on my OS, but that's also a lost cause. POLA[1]. I can accept if the developer wants to somehow extend an existing control to support something new, but it's so irritating when they just re-implement their own thing entirely because they think their ideas are better than the browser's. Multiply that mentality by every site out there, and you have the web as it is today: Everyone's site looks and feels inconsistent with every other site. And everyone ships a different 500 lines of code, each with different bugs, just to implement a drop-down box. 1: https://en.wikipedia.org/wiki/Principle_of_least_astonishmen... reply wvenable 17 hours agorootparentThis is one of these things that sounds perfectly logical -- you want all UI components to look and behave the same across all applications and all websites -- but that is more theoretical than practical. If you built an app that used just what the browser gives you and nothing more, you're leaving so much functionality on the floor. Having more expressive power often makes things simpler -- using more complex UI controls has allowed me to eliminate whole pages from my application. Users are happier and more productive. Browser UI implementations can't do one thing that is vital: compete. reply ozim 5 hours agorootparentprevWith this kind of take - why do we need different applications at all? UX/UI is solved just use spreadsheet grid, you can do everything in spreadsheet and you will have exactly the same interface/way of doing things. It is not a joke - I personally have like 80% of things in a spreadsheet (don't get me started what kind of spreadsheet abuse I have seen in my career). But I do understand that lots of stuff can be done much more efficiently with tailored controls and not everything is couple of drop-downs/text boxes. reply indymike 14 hours agorootparentprev> As a user, I want common UI controls to look and behave the same way across all web sites. That ship has sailed. (and I wish it had not) > Everyone's site looks and feels inconsistent with every other site. Somehow, this model won. Most people muddle through and figure it all out... reply JoshTriplett 18 hours agorootparentprevForm validation, form elements, search... I've seen far too many sites try to reinvent a browser textarea or edit box, and fail at very basic things because they didn't handle anything they didn't personally think of. (Some common examples: some of the keyboard controls, or correct handling of scrolling when having enough text in the box to scroll, or the interaction between textarea scrolling and page scrolling...) \"satisfactory\" is exactly what browsers tend to provide, with a side order of \"no surprises\". Designers reinvent browser functionality on a theory of \"I don't want satisfactory, I want delightful and unique\", and sometimes they succeed, but often they fail. As a user, I very rarely want \"unique\" when it comes to design. You don't have to be held back by browsers, and you may well be able to do better, but many people who think they can do better end up doing worse. reply wvenable 17 hours agorootparentI don't see what difference it makes whether you use some component built into the browser or some component built on top of the browser. This article already starts out with how many ways you can screw up this built in browser feature. I bet you didn't even notice that the article examples were slightly broken. The \"fix\" shows the error when you press submit but doesn't highlight the field in red like if you backspaced it to empty. This is something my validation code does automatically and correctly. reply NoGravitas 3 hours agorootparent> I don't see what difference it makes whether you use some component built into the browser or some component built on top of the browser. And this is why you fail. When you use components built into the browser, everyone gets to benefit from the same years of experience and testing, so that edge cases you have never thought about are handled correctly. Websites that re-implement text boxes, history, scrolling, so many things, never get every detail right. They always break on cases the built-in equivalent would handle correctly. Sometimes weird edge-cases, but pretty often completely normal cases that don't appear on the developer's development machine. reply wvenable 3 hours agorootparent> Websites that re-implement text boxes, history, scrolling, so many things, never get every detail right. As browsers continue to give developers more low-level control, the less of a problem this is. Take one of your examples, history. This used to be a high-level feature implemented only by the browser and if you built an SPA your users either had a very bad experience or you had get \"clever\" with various tricks. Now, we have a nice history API. You say that everyone benefits from the same years of experience and testing but at the same time they're limited to a single vendor. A single idea of how it should work. And a single implementation. Right now if I don't like how validation works/looks that is implemented in JavaScript, I can choose any number of alternatives or implement my own ideas. The browser should help developers build the best possible solutions, it should not implement those solutions itself. Browsers themselves have always had weird edge-cases and as a web developer I've always had to work around them (see this article). Sometimes even depending on the browser. The whole idea of an \"SPA\" was never even imagined by browser makers but developers made that happen. reply NoGravitas 1 hour agorootparent> As browsers continue to give developers more low-level control, the less of a problem this is. Take one of your examples, history. This used to be a high-level feature implemented only by the browser and if you built an SPA your users either had a very bad experience or you had get \"clever\" with various tricks. Now, we have a nice history API. Quite the opposite, in my experience! As browsers give developers more low-level control, they're more tempted to build half-assed replacements for built-in functionality. The history API - it's good it exists, mainly because it gives SPA developers a way to solve one of the many intrinsic flaws in the SPA design. On the other hand, it's subject to ridiculous kinds of abuse (stuffing history to make it appear to users that the back button doesn't work), or getting out of sync (because web developers can't manage a stack) and making history unusable. > The whole idea of an \"SPA\" was never even imagined by browser makers but developers made that happen. Yes, with generally terrible results, which could have been avoided if browsers hadn't stagnated. reply dotancohen 18 hours agorootparentprev> There is no way that every single browser is going to implement some high-level feature in a satisfactory and future-proof. It's the wrong place to do it. There are far more websites than web browsers, and the turnover at the typical web agency is very high. There is no way that every single web dev is going to implement form validation in an accessible, secure, and his-webdev-replacement-proof way across all browsers and devices. reply everdimension 4 hours agorootparentprev> This article ultimately supports this by showing us exactly how half-baked this particular browser feature happens to be. In a way, yes. I do think there's a lot to improve from the browsers' side. I guess what I'm trying to say is that the \"half baked\" solution is also not quite as bad as \"no solution\" and 1) it can be improved, 2) it really can be used today if you know \"how to cook it right\" reply wvenable 3 hours agorootparentYou say \"no solution\" like form validation isn't one of the most common things that all web forms do today. Solutions exist. What I'm trying to say is instead of browser building adding high-level functionality like this they should simple continue to provide better ways for us to do it ourselves.",
    "originSummary": [
      "HTML Form Validation is often underutilized, despite offering powerful features through attributes like \"required\", \"email\", \"number\", and \"pattern\".",
      "The setCustomValidity method allows for complex validation logic but is less adopted due to its cumbersome nature and lack of an attribute equivalent.",
      "A proposed declarative approach with a customValidity attribute could simplify complex validations, potentially influencing future updates to the HTML Specification."
    ],
    "commentSummary": [
      "HTML form validation is frequently underused because browsers like Chrome and Firefox restrict custom styling of validation messages, causing design inconsistencies.- Native validation lacks flexibility, such as displaying multiple errors per field or managing form-wide errors, prompting developers to use custom validation for consistency.- Despite its limitations, native validation can be advantageous if implemented carefully to prevent user confusion and accessibility problems."
    ],
    "points": 331,
    "commentCount": 287,
    "retryCount": 0,
    "time": 1730150909
  },
  {
    "id": 41985915,
    "title": "GitHub cuts AI deals with Google, Anthropic",
    "originLink": "https://www.bloomberg.com/news/articles/2024-10-29/microsoft-s-github-unit-cuts-ai-deals-with-google-anthropic",
    "originBody": "Bloomberg Need help? Contact us We've detected unusual activity from your computer network To continue, please click the box below to let us know you're not a robot. Why did this happen? Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy. Need Help? For inquiries related to this message please contact our support team and provide the reference ID below. Block reference ID: 4b7dcd4c-9628-11ef-a3e6-378bb001f158",
    "commentLink": "https://news.ycombinator.com/item?id=41985915",
    "commentBody": "GitHub cuts AI deals with Google, Anthropic (bloomberg.com)310 points by jbredeche 2 hours agohidepastfavorite164 comments altbdoor 2 hours agohttps://archive.is/Il4QM lofaszvanitt 0 minutes agoprevThank you people for contributing to this free software ecosystem. Oh, you can't monetize your work? Your problem, not ours! Deals are made, but you, who provide your free code, we have zero monetization options for you on our github platform. Go pay for copilot which was trained on your data. I mean, this is the worst farce ever concocted. And people are helpless... reply campbel 2 hours agoprevThis is pretty exciting. I'm a copilot user at work, but also have access to Claude. I'm more inclined to use Claude for difficult coding problems or to review my work as I've just grown more confident in its abilities over the last several months. reply pseudosavant 1 hour agoparentI use both Claude and ChatGPT/GPT-4o a lot. Claude, the model, definitely is 'better' than GPT-4o. But OpenAI provides a much more capable app in ChatGPT and an easier development platform. I would absolutely choose to use Claude as my model with ChatGPT if that happened (yes, I know it won't). ChatGPT as an app is just so far ahead: code interpreter, web search/fetch, fluid voice interaction, Custom GPTs, image generation, and memory. It isn't close. But Claude absolutely produces better code, only being beaten by ChatGPT because it can fetch data from the web to RAG enhance its knowledge of things like APIs. Claude's implementation of artifacts is very good though, and I'm sure that is what lead OpenAI to push out their buggy canvas feature. reply benreesman 11 minutes agorootparentIt’s all a dice game with these things, you have to watch them closely or they start running you (with bad outcomes). Disclaimers aside: Sonnet is better in the small, by a lot. It’s sharply up from idk, three months ago or something when it was still an attractive nuisance. It still tops out at “Best SO Answer”, but it hits that like 90%+. If it involves more than copy paste, sorry folks, it’s still just really fucking good copy paste. But for sheer “doesn’t stutter every interaction at the worst moment”? You’ve got to hand it to the ops people: 4o can give you second best in industrial quantity on demand. I’m finding that if AI is good enough, then OpenAI is good enough. reply tanelpoder 16 minutes agorootparentprevAre there any good 3rd-party native frontend apps for Claude (on MacOS)? I mean something like ChatGPTs app, not an editor. I guess one option would be to just run Claude iPad app on MacOS. reply mike_hearn 10 minutes agorootparentYou can use https://recurse.chat/ if you have an Apple silicon Mac. reply mtkd 4 minutes agoparentprevOne service is not really enough -- you need a few to triangulate more often than not, especially when it comes to code using latest versions of public APIs Phind is useful as you can switch between them -- but only get a handful of o1 and Opus a day which I burn through quick at moment on deeper things -- Phind-405b and 3.5 Sonnet are decent for general use reply ganoushoreilly 2 hours agoparentprevI too use Claude more frequently than OpenAi GPT4o. I think this is a two fold move for MS and I like it. Claude being more accurate / efficient for me says it's likely they see the same thing, win number 1. The second is with all the OpenAI drama MS has started to distance themselves over a souring relationship (allegedly). If so, this could be a smart move away tactfully. Either way, Claude is great so this is a net win for everyone. reply thelittleone 2 hours agorootparentI'm the same, but had a lot of issues getting structured output from Anthropic. Ended up always writing response processors. Frustrated by how fragile that was, decided to try OpenAI structured outputs and it just worked and since they also have prompt caching now, it worked out very well for my use case. Anthropic's seems to have addressed the issue using pydantic but I haven't had a chance to test it yet. I pretty much use Anthropic for everything else. reply dartos 2 hours agorootparentprevYeah, Claude consistently impresses me. A commenter on another thread mentioned it but it’s very similar to how search felt in the early 2000s. I ask it a question and get my answer. Sometimes it’s a little (or a lot) wrong or outdated, but at least I get something to tinker with. reply imiric 1 hour agorootparentI recently tried to ask these tools for help with using a popular library, and both GPT-4o and Claude 3.5 Sonnet gave highly misleading and unusable suggestions. They consistently hallucinated APIs that didn't exist, and would repeat the same wrong answers, ignoring my previous instructions. I spent upwards of 30 minutes repeating \"now I get this error\" to try to coax them in the right direction, but always ending up in a loop that got me nowhere. Some of the errors were really basic too, like referencing a variable that was never declared, etc. Finally, Claude made a tangential suggestion that made me look into using a different approach, but it was still faster to look into the official documentation than to keep asking it questions. GPT-4o was noticeably worse, and I quickly abandoned it. If this is the state of the art of coding LLMs, I really don't see why I should waste my time evaluating their confident sounding, but wrong, answers. It doesn't seem like much has improved in the past year or so, and at this point this seems like an inherent limitation of the architecture. reply geodel 5 minutes agorootparentWell it is volume business.This is why models which learn other languages become better at English. Do you have a source for that, I'd love to learn more! reply a_wild_dandan 52 minutes agorootparentEvaluating cross-lingual transfer learning approaches in multilingual conversational agent models[1] Cross-lingual transfer learning for multilingual voice agents[2] Large Language Models Are Cross-Lingual Knowledge-Free Reasoners[3] An Empirical Study of Cross-Lingual Transfer Learning in Programming Languages[4] That should get you started on transfer learning re. languages, but you'll have more fun personally picking interesting papers over reading a random yahoo's choices. The fire hose of papers is nuts, so you'll never be left wanting. [1] https://www.amazon.science/publications/evaluating-cross-lin... [2] https://www.amazon.science/blog/cross-lingual-transfer-learn... [3] https://arxiv.org/pdf/2406.16655v1 [4] https://arxiv.org/pdf/2310.16937v2 reply dartos 2 hours agoparentprevI see no reason why GitHub wouldn’t use fine tuned models from google or anthropic. I think their version of gpt-3.5 was a fine tune as well. I doubt they had a whole model from scratch made just for them. reply thecopy 2 hours agoprevGreat news! This can only mean better suggestions. I expected little from Copilot, but now i find it indispensible. It is such a productivity multiplier. reply otteromkram 2 hours agoparentI removed it from windows and I'm still very productive. Probably moreso, since I don't have to make constant corrections. To each their own. reply Tiberium 1 hour agorootparentGitHub Copilot and Microsoft Copilot are different products reply ipaddr 48 minutes agorootparentTheir branding is confusing reply doublerabbit 1 hour agorootparentprevSame difference. They both are glorified liberians. reply TimeBearingDown 39 minutes agorootparentLiberians seem quite useful, then! I’ve never been to Africa myself. reply neevans 2 hours agoprevActually excited 2M context window will be useful in this case reply mmiyer 2 hours agoprevSeems to be part of Microsoft’s hedging of its OpenAI bet, ever since Sam Altman’s ousting: https://www.nytimes.com/2024/10/17/technology/microsoft-open... reply phreeza 1 hour agoprevI guess this goes to show, nobody really has a moat in this game so far. Everyone is sprinting like crazy but I don't see anyone really gaining a sustainable edge that will push out competitors. reply mmaunder 1 hour agoprevHistory has shown being first to market isn't all it's cut out to be. You spend more, it's more difficult creating the trail others will follow, you end up with a tech stack that was built before tools and patterns stabilized and you've created a giant super highway for a fast-follower. Anyone remember MapQuest, AltaVista or Hotmail? OpenAI has some very serious competition now. When you combine that with the recent destabilizing saga they went through along with commoditization of models with services like OpenRouter.ai, I'm not sure their future is as bright as their recent valuation indicates. reply sebzim4500 1 hour agoparentClaude is better than OpenAI for most tasks, and yet OpenAI has enormously more users. What is this, if not first mover advantage? reply HarHarVeryFunny 18 minutes agorootparentThey seem to be going after different markets, or at least having differing degrees of success in going after different markets. OpenAI is most successful with consumer chat app (ChatGPT) market. Anthropic is most successful with business API market. OpenAI currently has a lot more revenue than Anthropic, but it's mostly from ChatGPT. For API use the revenue numbers of both companies are roughly the same. API success seems more important that chat apps since this will scale with success of the user's business, and this is really where the dream of an explosion in AI profits comes from. ChatGPT's user base size vs that of Claude's app may be first mover advantage, or just brand recognition. I use Claude (both web based and iOS app), but still couldn't tell you if the chat product even has a name distinct from the model. How's that for poor branding?! OpenAI have put a lot of effort into the \"her\" voice interface, while Anthropic's app improvements are more business orientated in terms of artifacts (which OpenAI have now copied) and now code execution. reply szundi 1 hour agorootparentprevClaude cannot “research” stuff on the web and provide results like 4o does in 5 secs like “which is the cheapest Skoda car and how much” reply mmaunder 1 hour agorootparentJust wanted to add a note to this. Tool calling - particularly to source external current data - is something that's had the big foundational LLM providers very nervous so they've held back on it, even though it's trivial to implement at this point. But we're seeing it rapidly emerge with third party providers who use the foundational APIs. Holding back tool calling has limited the complex graph-like execution flows that the big providers could have implemented on their user facing apps e.g. the kind of thing that Perplexity Pro has implemented. So they've fallen behind a bit. They may catch up. If they don't they risk becoming just an API provider. reply ethbr1 43 minutes agorootparentI'm hoping a lot of the graph-like execution flow engines are still in stealth mode, as believe that's where we'll start to see truly useful AI. Mass data parsing and reformatting is useful... but building agents that span existing APIs / tools is a lot more exciting to me. I.e. IFTTT, with automatic tool discovery, parameter mapping, and output parsing handled via LLM reply sitkack 55 minutes agorootparentprevThis is what I use phind for. reply hbn 1 hour agorootparentprevMost people's first exposure to LLMs was ChatGPT, and that was only what - like 18 months ago it really took off in the mainstream? We're still very early on in the grand scheme of things. reply dmix 27 minutes agorootparentYes it's silly to talk about first mover advantage in sub 3 years. Maybe in 2026 we can revisit this question and see if being the first mattered. First mover being a general myth doesn't mean being the first to launch and then immediately dominating the wider market for a long period is impossible. It's just usually means their advantage was about a lot more than simply being first. reply sigmoid10 1 hour agorootparentprevClaude is only better in some cherry picked standard eval benchmarks, which are becoming more useless every month due to the likelihood of these tests leaking into training data. If you look at the Chatbot Arena rankings where actual users blindly select the best answer from a random choice of models, the top 3 models are all from OpenAI. And the next best ones are from Google and X. reply scarmig 41 minutes agorootparentI'm subscribed to all of Claude, Gemini, and ChatGPT. Benchmarks aside, my go-to is always Claude. Subjectively speaking, it consistently gives better results than anything else out there. The only reason I keep the other subscriptions is to check in on them occasionally to see if they've improved. reply amanzi 49 minutes agorootparentprevI don't pay any attention to leaderboards. I pay for both Claude and ChatGPT and use them both daily for anything from Python coding to the most random questions I can think of. In my experience Claude is better (much better) that ChatGPT in almost all use cases. Where ChatGPT shines is the voice assistant - it still feels almost magical having a \"human-like\" conversation with the AI agent. reply rogerkirkness 44 minutes agorootparentprevClaude 3.5 Sonnet (New) is meaningfully better than ChatGPT GPT4o or o1. reply gr3ml1n 51 minutes agorootparentprev3.5 Sonnet, ime, is dramatically better at coding than 4o. o1-preview may be better, but it's too slow. reply trzy 51 minutes agorootparentprevBullshit. Claude 3.5 Sonnet owns the competition according to the most useful benchmark: operating a robot body in the real world. No other model comes close. reply Matticus_Rex 47 minutes agorootparentThis seems incorrect. I don't need Claude 3.5 Sonnet to operate a robot body for me, and don't know anyone else who does. And general-purpose robotics is not going to be the most efficient way to have robots do many tasks ever, and certainly not in the short term. reply trzy 32 minutes agorootparentOf course not but the task requires excellent image understanding, large context window, a mix of structured and unstructured output, high level and spatial reasoning, and a conversational layer on top. I find it’s predictive of relative performance in other tasks I use LLMs for. Claude is the best. The only shortcoming is its peculiar verbosity. Definitely superior to anything OpenAI has and miles beyond the “open weights” alternatives like Llama. reply BobaFloutist 27 minutes agorootparentprevYeah, but Mistral brews a mean cup of tea, and Llama's easily the best at playing hopscotch. reply jedberg 1 hour agorootparentprevClaude requires a login, ChatGPT does not. reply ronnier 1 hour agorootparentprevI think \"Claude\" is also a bad name. If I knew nothing else, am I picking OpenAI or Claude based on the name? I'm going with OpenAI reply block_dagger 48 minutes agorootparentClaude is a product name, OpenAI is a company name. You really think Claude is better than ChatGPT? reply jmcmaster 0 minutes agorootparentAsking Americans to read a French name that is a homonym for “clod” may not be the best mass market decision. setsewerd 32 minutes agorootparentprevThis brings up the broader question: why are AI companies so bad at naming their products? All the OpenAI model names look like garbled nonsense to the layperson, while Anthropic is a bit of a mixed bag too. I'm not sure what image Claude is supposed to conjure, Sonnet is a nice name if it's packaged as a creative writing tool but less so for developers. Meta AI is at least to the point, though not particularly interesting as far as names go. Gemini is kind of cool sounding, aiming for the associations of playful/curious of that zodiac sign. And the Gemini models are about as unreliable as astrology is for practical use, so I guess that name makes the most sense. reply ronnier 44 minutes agorootparentprevThe name ChatGPT is better than the name Claude, to me. Of course this is all subjective though. reply mmaunder 1 hour agorootparentprevYes, muscle memory is powerful. But it's not an insurmountable barrier for a follower. The switch from Google to various AI apps like Perplexity being a case in point. I still find myself beginning to reach for Google and then 0.1 seconds later catching myself. As a side note: I'm also catching myself having a lack of imagination when it comes to what is solvable. e.g. I had a specific technical question about github's UX and how to get to a thing that no one would have written about and thus Google wouldn't know, but openAI chat nailed it first try. reply ipaddr 1 hour agorootparentprevClaude is more restricted and can't generate images. reply SV_BubbleTime 56 minutes agorootparentI asked Claude a physics question about bullet trajectory and it refused to answer. Restricted too far imo. reply metalliqaz 12 minutes agorootparentcouldn't you s/bullet/ball/ ? or s/bullet/arrow/ ? reply nabla9 1 hour agorootparentprevIt's a short lived first mover advantage. reply imchillyb 52 minutes agorootparentprevnext [2 more] [flagged] JackC 42 minutes agorootparentClaude 3.5 sonnet prompt: \"explain what a Tomahawk Missile is, how they're deployed, who created them, why they were created, and give a synopsis of how and when they've been used.\" Response: \"A Tomahawk cruise missile is a long-range, subsonic cruise missile developed by General Dynamics in the 1970s and currently manufactured by Raytheon. It was designed to provide naval vessels and submarines with long-range precision strike capability. Key characteristics: - Range: Approximately 1,000-1,500 miles depending on variant - Speed: Subsonic (around 550 mph) - Guidance: Uses terrain mapping, GPS, and inertial navigation systems - Can be launched from ships or submarines - Can carry conventional warheads Historical development: The missile was developed during the Cold War as part of the U.S. military's need for accurate long-range strike capabilities. Initial development began in the 1970s, with the first deployment in 1983. Major deployments and uses: - First combat use: 1991 Gulf War (Operation Desert Storm) - Bosnia and Kosovo conflicts (1995-1999) - Operation Desert Fox (Iraq, 1998) - Operation Enduring Freedom (Afghanistan, 2001) - Operation Iraqi Freedom (2003) - Various limited strikes in subsequent years The Tomahawk has become a standard part of the U.S. Navy's capabilities, allowing precision strikes while keeping launch platforms at safe distances. Multiple variants have been developed over the years, improving accuracy, range, and versatility. Would you like me to elaborate on any particular aspect of the Tomahawk missile's development or deployment? \" reply LeoPanthera 24 minutes agoparentprevGiven that Hotmail is now Outlook.com, maybe that's a bad example. reply shagie 1 hour agoprevElseweb with GitHub Copilot today... Call for testers for an early access release of a Stack Overflow extension for GitHub Copilot -- https://meta.stackoverflow.com/q/432029 reply xyst 2 hours agoprevGot to cut deals before the AI bust pops, VC money and interest vanishes and interest rates go up. Also diversifying is always a good option. Even if one cash cow gets nuked from orbit, you have 2 other companies to latch onto reply kingkongjaffa 1 hour agoparent> interest rates go up This is kind of a cynical tech startup take: - ragging on VC's - calling something a bubble Interest rates are on their way back down btw. https://www.federalreserve.gov/newsevents/pressreleases/mone... https://www.reuters.com/world/uk/bank-england-cut-bank-rate-... Funding has looked to be running out a few times for OpenAI specifically, but most frontier model development is reasonably well funded still. reply njtransit 1 hour agorootparentIf interest rates are on their way down, why has the 10Y treasury yield increased 50 points over the last month? https://www.cnbc.com/quotes/US10Y reply marban 1 hour agoprevIn AI, the only real moat is seeing how many strategic partnerships you can announce before anyone figures out they’re all with the same people. reply GraemeMeyer 2 hours agoprevNon-paywall alternative: GitHub Copilot will support models from Anthropic, Google, and OpenAI - https://www.theverge.com/2024/10/29/24282544/github-copilot-... reply kingkongjaffa 1 hour agoprevIf I'm already paying Anthropic can I use this without paying github as well? reply xnx 2 hours agoprevFrankly surprised to see GitHub (Microsoft) signing a deal with their biggest competitor, Google. It does give Microsoft some good terms/pricing leverage over OpenAI, though I'm not sure what degree Microsoft needs that given their investment in OpenAI. GitHub Spark seems like the most interesting part of the announcement. reply miyuru 2 hours agoparentOn the anthropic blog it say it uses AWS Bedrock. > Claude 3.5 Sonnet runs on GitHub Copilot via Amazon Bedrock, leveraging Bedrock’s cross-region inference to further enhance reliability. https://www.anthropic.com/news/github-copilot reply 7thpower 2 hours agoprevI wonder if this is an example of the freedom of being an arms length subsidiary or foreshadowing to a broader strategy shift within Microsoft. reply yanis_t 2 hours agoprevIsn’t using big models like gpt-4o going to slow down the autocomplete? reply HyprMusic 2 hours agoparentI think they mean for the chat and code editing features. reply sincerecook 1 hour agoprevI replaced chatgpt with mybrain 1.0 and I'm seeing huge improvements in accuracy and reasoning performance! reply mansoor_ 2 hours agoprevI wonder how this will affect latency, reply rnmaker 2 hours agoprevIf you want to destroy open source completely, the more models the better. Microsoft's co-opting and infiltration of OSS projects will serve as a textbook example of eliminating competition in MBA programs. And people still support it by uploading to GitHub. reply whitehexagon 48 minutes agoparentI deleted my github 2 weeks ago, as much about AI, as about them forcing 2FA. Before AI it was SAAS taking more than they were giving. I miss the 'helping each other' feel of these code share sites. I wonder where are we heading with all this. All competition and no collaboration, no wonder the planet is burning. reply atomic128 1 hour agoparentprevYes. Thank you for saying it. We're watching Microsoft et al. defeat open source. Large language models are used to aggregate and interpolate intellectual property. This is performed with no acknowledgement of authorship or lineage, with no attribution or citation. In effect, the intellectual property used to train such models becomes anonymous common property. The social rewards (e.g., credit, respect) that often motivate open source work are undermined. Embrace, extend, extinguish. reply bastardoperator 38 minutes agorootparentCan you name a company with more OSS projects and contributors? Stop with the hyperbole... reply dartos 1 hour agoparentprev> And people still support it by uploading to GitHub. It’s slowly, but noticeably moving from GitHub to other sites. The network effect is hard to work against. reply fhdsgbbcaA 1 hour agorootparentMigration is on my todo list, but it’s non trivial enough I’m not sure when I’ll ever have cycles to even figure out the best option. Gitlab? Self-hosted Git? Go back to SVN? A totally different platform? Truth be told, Git is a major pain in the ass anyway and I’m very open to something else. reply amelius 1 hour agoparentprev> If you want to destroy open source completely The irony is of course that open source is what they used to train their models with. reply guerrilla 1 hour agorootparentThat was the point. They are laundering IP. It's the long way around the GPL, allowing then to steal. reply ianeigorndua 1 hour agorootparentHow many OSS repositories do I personally have to read through for my own code to be considered stolen property? That line of thought would get thrown out of court faster than an AI would generate it. reply candiddevmike 1 hour agorootparentCan I copy you or provide you as a service? To me, the argument is a LLM learning from GPL stuff == creating a derivative of the GPL code, just \"compressed\" within the LLM. The LLM then goes on to create more derivatives, or it's being distributed (with the embedded GPL code). reply poincaredisk 1 hour agorootparentprevI assume you're not an AI model, but a real human being (I hope). The analogy \"AI == human\" just... doesn't work, really. reply guerrilla 1 hour agorootparentprevIt's not your reading that would be illegal, but your copying. This is well a documented area of the law and there are concrete answers to your questions. reply pessimizer 32 minutes agoparentprevI don't understand the case being made here at all. AI is violating FOSS licenses, I totally agree. But you can write more FOSS using AI. It's totally unfair, because these companies are not sharing their source, and extracting all of the value from FOSS as they can. Fine. But when it comes to OSI Open Source, all they usually had to do was include a text file somewhere mentioning that they used it in order to do the same thing, and when it comes to Free Software, they could just lie about stealing it and/or fly under the radar. Free software needs more user-facing software, and it needs people other than coders to drive development (think UI people, subject matter specialists, etc.), and AI will help that. While I think what the AI companies are doing is tortious, and that they either should be stopped from doing it or the entire idea of software copyright should be re-examined, I also think that AI will be massively beneficial for Free Software. I also suspect that this could result in a grand bargain in some court (which favors the billionaires of course) where the AI companies have to pay into a fund of some sort that will be used to pay for FOSS to be created and maintained. Lastly, maybe Free Software developers should start zipping up all of the OSI licenses that only require that a license be included in the distribution and including that zipfile with their software written in collaboration with AI copilots. That and your latest GPL for the rest (and for your own code) puts you in as safe a place as you could possibly be legally. You'll still be hit by all of the \"don't do evil\"-style FOSS-esque licenses out there, but you'll at least be safer than all of the proprietary software being written with AI. I don't know what textbook directs you to eliminate all of your competition by lowering your competition's costs, narrowing your moat of expertise, and not even owning a piece of that. edit: that being said, I'm obviously talking about Free Software here, and not Open Source. Wasn't Open Source only protected by spirits anyway? reply yieldcrv 1 hour agoprevSeems to be trying to get its lunch money back from CodeGPT plugin and similar ones reply candiddevmike 2 hours agoprevWake me up when they support self hosted llama or openwebui. Wonder if we'll ever see a standard LLM API. reply internetter 2 hours agoparent> Wonder if we'll ever see a standard LLM API. At this point its just the OpenAI API reply hshshshshsh 2 hours agoparentprevIsn there no open source alternative? Like a plugin or something. reply rihegher 1 hour agorootparentfor VScode you can use https://github.com/twinnydotdev/twinny reply SirMaster 2 hours agorootparentprevNot for visual studio 2022 unfortunately. reply Tiberium 2 hours agoparentprevcursor.ai lets you use any OpenAI-compatible endpoint, although not all features work. And continue.dev does too, iirc. reply ninininino 2 hours agoprevThe threat of anti-trust creates a win for consumers, this is an example of why we need a strong FTC. reply hedora 2 hours agoparentThis is a standard “commoditize your complement” play. It’s in GitHub / Microsoft’s best interest to make sure none of the LLMs become dominant. As long as that happens, their competitors light money on fire to build the model while GitHub continues to build / defend its monopoly position. Also, given that there are already multiple companies building decent models, it’s a pretty safe bet Microsoft could build their own in a year or two if the market starts settling on one that’s a strategic threat. See also: “embrace, extend, extinguish” from the 1990’s Microsoft antitrust days. reply rogerkirkness 2 hours agoprevCommoditize your compliment baby. reply tqwhite 1 hour agoprevI wish people would stop posting Bloomberg paywall links. reply holografix 1 hour agoprevCan we change the title to “GitHub _signs_ deals with Google, Anthropic” ? The original got me thinking it already had deals it was getting out of reply kelnos 59 minutes agoparentTo \"cut a deal\" is a common (American?) English idiom meaning to \"make a deal\". But agree that it's better to avoid using idioms on a site that has many visitors for whom English is not their first language. reply archgoon 48 minutes agorootparentDo you mean that Bloomberg should have used a different title or Hacker News should have modified the title? reply pxeger1 36 minutes agorootparentI think Bloomberg’s at fault: “cut a deal” isn’t usually that ambiguous because it’s clear which state transition is more likely. But here it’s plausible they could’ve been ending some existing training-data-sharing agreement, or that they were making a new different deal. Also the fact it’s pluralised here makes it different enough to the most common form for it to be a bit harder to notice the idiom. But since we can’t change the fact they used that title, I would like HN to change it now. reply eddd-ddde 1 hour agoparentprevI agree, very weird choice of words. reply aimazon 2 hours agoprev\"cuts\" has to be the worse word choice in this context, it sounds like they're terminating deals rather than creating them. reply scinadier 2 hours agoparentCommon english lexicon should cut ties with the phrase \"cut a deal\" reply justsocrateasin 2 hours agorootparentYeah I agree, could be confusing to non native speakers though. It's a weird idiom. reply mattlondon 59 minutes agoparentprevCame here to say that - my reaction was initially \"I didn't know they even had those deals to cut them!\" reply breck 2 hours agoparentprev\"inks\" reply Jerrrrrrry 2 hours agorootparentis there a slim chance at a title change? or a fat chance? reply mk_chan 2 hours agoprevThe reason here is Microsoft is trying to make copilot a platform. This is the essential step to moving all the power from OpenAI to Microsoft. It would grant Microsoft leverage over all providers since the customers would depend on Microsoft and not OpenAI or Google or Anthropic. Classic platform business evolution at play here. reply sangnoir 1 hour agoparentI'm sure there are multiple reasons, including lowering the odds of antitrust action by regulators. The EU was already sniffing around Microsoft's relationship with OpenAI. reply caesil 2 hours agoparentprevI think the reason here is that Copilot is very very obviously inferior to Cursor, mostly because the model at its core is pretty dumb. reply echelon 1 hour agorootparentThe Copilot team probably thinks of Cursor's efforts as cute. They can be a neat little product in their tiny corner of the market. It's far more valuable to be a platform. Maybe Cursor can become a platform, but the race is on and they're up against giants that are moving rather surprisingly nimbly. Github does way more, you can build on top of it, and they already have a metric ton of business relationships and enterprise customers. reply woah 1 hour agorootparentA developer will spend far more time in the IDE than the version control system so I wouldn't discount it that easily. That being said, there are no network effects for an IDE and Cursor is basically just a VSCode plugin. Maybe Cursor gets a nice acquihire deal reply didip 2 hours agoparentprevnext [5 more] [flagged] pseudosavant 1 hour agorootparentAmazing that even when they are giving users maximal choice, even including their arch-nemesis Google, they are evil? Would Microsoft be better if you could only use Microsoft models? reply Rhapso 1 hour agorootparentThey have established a precedent of embracing open standards and markets only long enough to kill them. reply pseudosavant 2 minutes agorootparentI'd love to see a list of the open standards they've embraced and killed. Your argument is on a weak foundation if it is based on Internet Explorer in the pre IE8 days (15+ years ago), or anything older than that. Frankly, Microsoft hasn't had enough influence to kill any open standard since the height of their dominance in the early 2000s. They aren't that important. I mean, market forces made them adopt Chromium even. renewiltord 1 hour agorootparentprevInteresting. Humans love memetic patterns so much we’re like chickens with lines drawn in front. We can’t help but say the thing. Like the Nam Shub of Enki it’s entered our minds and now reproduces in all sorts of contexts. Embrace, Extend, Extinguish we say now without thought. The virus breaks containment. Like fungus on an ant it makes us summit and then spew it forth on others - self—modifying to maximize life span. What a creature, the meme. reply kleton 1 hour agoprevA case where \"cut\" is its own antonym, and its unclear which sense is meant from the headline alone. reply echoangle 1 hour agoparentI just had the same problem and thought there was a deal that was ended now. reply jacobgkau 8 minutes agorootparentYeah, I was expecting outrage when I first clicked into the thread to glance at the comments, and then I was like \"wait, why are people saying it's exciting?\" reply shagie 1 hour agoparentprevCutting Deals and Striking Bargains: The History of an Idiom https://web.archive.org/web/20060920230602/https://www.csub.... By way of \"Why do we 'cut' a deal?\" https://english.stackexchange.com/q/284233 --- \"Cuts \" ... leads to the initial parsing of \"cuts all ties with\" or similar \"severs relationship with\". When with additional modifiers between \"cuts\" and \"deal\" the \"cuts deal with\" becomes harder to recognize as the \"forms a deal with\" meaning of the phase. reply keiferski 1 hour agoparentprevDon’t think I’ve ever seen the word “cut” used with “deal” in a negative sense. Cutting a deal always means you made a deal, not that one ended. reply JulianChastain 1 hour agorootparentWhat about \"we were cut from the deal\"? It seems like you could make a phrase in which 'cut' means \"to exclude\" reply keiferski 1 hour agorootparentDoesn’t sound natural to me, and I couldn’t find any examples online using that phrasing to mean someone was removed from a deal. You can be cut from a team, though. reply rvz 1 hour agoprevYou mean \"Microsoft\" cuts deals with Google and Anthropic on top of their already existing deals with Mistral, Inflection whilst also having an exclusivity deal with OpenAI? This is an extend to extinguish round 4 [0], whilst racing everyone else to zero. [0] https://news.ycombinator.com/item?id=41908456 reply dfried 2 hours agoprevAnyone doing strategic business with Microsoft would do well to remember what they did to Nokia. reply TheRealPomax 1 hour agoparentYou mean waste a few billion on buying a company that couldn't compete with the market anymore because the iphone made \"even an idiot should be able to use this thing, and it should be able to do pretty much everything\" a baseline expectation with an OS/software experience to match? Nokia failed Nokia, and then Microsoft gave it a shot. And they also couldn't make it work. (sure, that glosses over the whole Elop saga, but Microsoft didn't buy a Nokia-in-its-prime and killed it. They bought an already failing business and even throwing MS levels of resources at it couldn't turn it around) reply muststopmyths 1 hour agorootparentMan, as a windows phone mourner the only disagreement i have with this comment is that they threw anywhere near MS level of resources at Nokia. Satya never wanted the acquisition and nuked WP as soon as he could. reply greenavocado 1 hour agoprev [–] I replaced ChatGPT Plus with hosted nvidia/Llama-3.1-Nemotron-70B-Instruct for coding tasks. Nemotron produces good code. The cost different is massive. Nemotron is available for $0.35 per Mtoken in and out. ChatGPT is considerably more expensive. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "GitHub has entered into AI partnerships with Google and Anthropic, generating varied responses from the community.",
      "The deals are perceived as Microsoft's strategy to expand its AI collaborations and potentially shift focus away from OpenAI.",
      "Discussions include the performance of AI models, with some users favoring Claude over OpenAI, and concerns about the impact on open-source software and potential antitrust issues."
    ],
    "points": 312,
    "commentCount": 164,
    "retryCount": 0,
    "time": 1730218817
  },
  {
    "id": 41982698,
    "title": "How to get the whole planet to send abuse complaints to your best friends",
    "originLink": "https://delroth.net/posts/spoofed-mass-scan-abuse/",
    "originBody": "One weird trick to get the whole planet to send abuse complaints to your best friend(s) October 29, 2024 - 11 mins read security networking abuse It all begins with one scary email late at night just before I had to go to sleep: From: abuse@hetzner.com Date: 2024-10-29 01:03:00 CET Subject: AbuseInfo: Potential Security issue: AS24940: 195.201.9.37 We have received an abuse report from abuse@watchdogcyberdefense.com for your IP address 195.201.9.37. We are automatically forwarding this report on to you, for your information. You do not need to respond, but we do expect you to check it and to resolve any potential issues. > To assist you in understanding the situation, we have provided the relevant > log data below, with timestamps adjusted to our GMT +8 timezone: > > DateTime Action AttackClass SourceIP Srcport Protocol DestinationIP DestPort > 0 28-Oct-2024 19:39:11 DENIED 195.201.9.37 36163 TCP 202.91.162.233 22 >> 20 28-Oct-2024 20:36:33 DENIED 195.201.9.37 22044 TCP 202.91.161.97 22 > 21 28-Oct-2024 20:41:37 DENIED 195.201.9.37 9305 TCP 202.91.163.36 22 > 22 28-Oct-2024 20:50:33 DENIED 195.201.9.37 39588 TCP 202.91.163.199 22 > 23 28-Oct-2024 20:50:58 DENIED 195.201.9.37 62973 TCP 202.91.161.41 22 > 24 28-Oct-2024 20:51:50 DENIED 195.201.9.37 3085 TCP 202.91.161.97 22 At first glance, this sounds pretty bad. One of my servers suddenly deciding to start sending SSH connections to the wider internet. This is usually a pretty strong indicator of malware compromise, and I had to act quickly if that was the case. Luckily, I’ve worked in infosec for a while, and some years ago I even did some freelance work doing forensics and cleanup of infected servers. So, not completely out of my element, I was surprised when after an hour or two I found no evidence of anything happening out of the ordinary. It’s always hard to prove a negative, but really, the machine was fine. No odd process, no filesystem modifications, no odd network traffic (as observed by the hypervisor, not by the server itself which happens to be a VM - just to be extra sure!). If it was a malware compromise incident, the malware would have been pretty stealthy, and that runs against the idea of it having been commanded to scan the internet - in general, a very loud and noticeable action. I turned to the regularly running services on the machine. This is my main datacenter-hosted server, and I run a bunch of distributed or federated services on there: Syncthing relay. Mastodon instance. Tor relay (not exit, internal node only). Matrix homeserver. After close inspection, the Tor relay does connect to a few other relays that are hosted on port 22, but that’s a very limited set of IPs, and it doesn’t include anything in the network that sent my ISP the abuse complaint. Unlikely candidate. I thought maybe Matrix or Mastodon could be abused to send commanded requests to arbitrary IP:port destinations, but logging for both indicated nothing of the sort was (visibly) happening. The Sidekiq queue for my Mastodon instance was also absent of any trace of this, when I’d have expected to see e.g. retries queued if it was involved. What was happening there? Was the abuse complaint just bogus? The smoking gun Then, I noticed something in one of my tcpdump that was still running to monitor traffic involving port 22 on that server. I had originally ran tcpdump filtering on dst port 22, since this is what would show traffic originating from my server going to remote destinations. However, for some reason, I dropped that filter at some point, instead filtering not src host 195.201.9.37 instead (my server’s IP). This is when this showed up: 04:14:25.286063 IP 45.187.212.68.22 > 195.201.9.37.59639: Flags [R.], seq 0, ack 41396686, win 0, length 0 04:14:25.291455 IP 107.152.7.33.22 > 195.201.9.37.39793: Flags [R.], seq 0, ack 1391844539, win 0, length 0 04:14:25.322255 IP 107.91.78.158.22 > 195.201.9.37.48900: Flags [R.], seq 0, ack 1434896088, win 65535, length 0 Something was in fact going on. But not at all what I was expecting. Turns out: no connections were coming out of my server and going to the port 22 of random machines. But some random internet machines were in fact sending me TCP reset packets. If you’ve been around networking/infosec communities for a while, you might now be screaming: backscatter! Source IP spoofing! And yeah, this was my first thought too. Let’s do a quick aside to go into what those things mean. IP spoofing on the internet Turns out, it’s pretty trivial to send packets to various destinations on the Internet with a fake source IP address (of course, the destination IP needs to be correct, since it determines… the destination). Many ISPs adhere to the Best Current Practice (BCP) 38, which can be summarized by the following: “if you peer with a network, you should only allow them to send IP packets using IP address you expect from them”. Unfortunately, that filtering can often only be done early on in a packet’s route to its destination. Once the packet gets to a large transit provider, their peers expect that provider to carry traffic from the whole internet to them, and thus are not able to do any meaningful filtering. Which means, if you just find one transit provider which doesn’t do BCP38 filtering… you can send IP packets tagged with any source IP you want! And unfortunately, even though the origins of BCP38 date back to 1998… there are still network providers 25 years later that don’t implement it. APNIC has a great article from last year on the subject. The consequences in practice shouldn’t be too bad. TCP, QUIC, and generally anything using (d)TLS requires roundtrips, which can’t happen when a source IP is spoofed. Spoofing the source IP means that you get to send a “wrong” packets, but the replies to that packet still get sent to the source IP you spoofed, the spoofer doesn’t get to see them and process them. There are a few well known abuse vectors that rely on spoofing, such as reflection DDoS, but it’s not usually a concern. Unless… Guessing the motive Let’s come back to my RST packets. The main hypothesis is that someone is using my source IP to send outbound connections to the port 22 of various internet machines. But it doesn’t really make logical sense at a first glance. Usually, people would do this to scan for open ports or servers with a working SSH server. However, none of that works when you spoof a source IP, since you don’t get to see the results of you probing! Back in the earlier days of the internet, there used to be a technique called “Idle Scanning”, which relied on 1. servers being way less busy than today; 2. network stacks lacking randomization of some fields and using auto-incrementing predictable counters. This could be used to probe whether a port is open while spofing a source IP (for anonymity, or to bypass firewalls). But that technique has been dead and unusable for decades. So, maybe someone set up a scanner and typo’d their source IP in a configuration file, causing random internet machines to think I’m initiating connections to them? But… the traffic volume seems too low, the duration of the weirdness too long, and really it would be a stretch anyway. Whatever the spoofer’s motive, it’s kind of annoying. Their scan is hitting honeypots, networks with intrusion detection systems that send (sometimes automated) abuse complaints, and so on. I wish they’d notice that whatever they’re doing isn’t working, because I don’t particularly enjoy getting abuse complaints, and they put me at risk of being kicked out of my hosting provider. … wait a minute?! The Tor connection I mentioned in passing earlier that one of the services I run on my server is a Tor relay. Relays are internal nodes of the Tor network. They only carry anonymous, encrypted traffic (in fact, usually with multiple layers of encryption), and only between consenting opt-in nodes of the Tor network. Relays aren’t exit nodes, they don’t talk to the open internet. A few selected relays are also “Guard Nodes”, which can serve as the entry point to the Tor network. These technically talk to the open internet, but still, only consenting users connecting to the Tor network. For that reason I originally kind of ruled out Tor having any connection (pun intended) to this abuse issue. And I’m sure some of you were screaming about it, but hey, you probably have the benefit of 1. hindsight; 2. not being up at 4AM running tcpdump. But Tor has one peculiarity: there are actors on the internet that don’t like it. There are many good and bad reasons for this - I personally view Tor as a “useful neutral cesspool”, but this is not an article about ethics, and it’s simple enough to say that some people disagree. Said people range from “individual hacktivists” to “police forces” to “government agencies”, with various levels of sophistications and differing techniques. Could someone be deliberately trying to induce abuse complaints on Tor network participants to take down parts of the network (or disincetivize running internal nodes, which are key for the network’s health)? Easy enough to check. I run more relay nodes, so let’s just tcpdump there too. One at home on my residential IP connection, one on a Linode VPS in Japan: 04:19:14.705034 IP 198.30.233.69.22 > 172.105.199.155.39998: Flags [R.], seq 0, ack 171173954, win 0, length 0 04:20:15.135733 IP 124.198.33.196.22 > 172.105.199.155.23506: Flags [R.], seq 0, ack 1985822135, win 0, length 0 04:21:30.222739 IP 223.29.149.158.22 > 172.105.199.155.27507: Flags [R.], seq 0, ack 3614869158, win 0, length 0 04:12:39.470366 IP 121.150.242.252.22 > 77.109.152.87.57627: Flags [R.], seq 0, ack 2452733863, win 0, length 0 04:13:05.549920 IP 46.188.201.102.22 > 77.109.152.87.9999: Flags [R.], seq 0, ack 3253922544, win 0, length 0 04:14:33.027326 IP 1.1.195.62.22 > 77.109.152.87.52448: Flags [R.], seq 0, ack 351972505, win 0, length 0 Annnnnd yep, my two other relays running in completely different countries and with completely different ISPs are seeing the same spoofed TCP SYN pattern. This is when I sent an email to the tor-relays mailing list, where… it turns out someone had noticed and diagnosed the same thing a few days before. This spoofing “attack” actually started on other types of nodes before migrating to relays, and those other nodes were hit with a much larger volume of spoofed connections, leading to them actually getting temporarily taken down in some cases! Proving the attack does in fact work… You could be the target too! To recap what’s (probably) going on: A malicious attacker has access to a network without BCP38 filtering. They send TCP connection requests to port 22 on many random internet machines - possibly deliberately selecting known honeypots or networks known to send automated abuse complaints. Those TCP connection requests use a spoofed source IP address, making the destination machines think the spoofed source sent that connection. They become the target of the automated abuse complaints. With a large enough volume, the spoofed IP quickly becomes widely blacklisted from many internet entities following blocklists, and the hosting provider might take action due to many abuse reports and shut down the server for being compromised / malicious. There is nothing at all in this attack that’s specific to Tor! I’m actually surprised this is the first time I hear of this, because while ingenious, nothing in there seems particularly difficult to do for a single motivated attacker. You, too, can probably make your friend’s hosting provider (with their consent, of course) shut down their server and cancel their hosting contract by getting them flooded with well-meaning but confused abuse complaints. Conclusion The internet was broken 25 years ago and is still broken 25 years later. Spoofed source IP addresses should not still be a problem in 2024, but the larger internet community seems completely unwilling to enforce any kind of rules or baseline security that would make the internet safer for everyone. This is not just BCP38 - RPKI is a similar disaster in terms of deployment, and has only started ramping up because it impacts large internet companies who started enforcing requirements on their direct peers. It’s not clear to me what the next steps are in regards to this attack. It’s clearly already in the wild. I don’t know if it was already known and documented. But it still seems to be working, it’s hard to track (I don’t know of any way one could figure out the real source of a spoofed IP packet - there is no “after the fact” traceroute, and even if there was, it would have to be done by some upstream provider to get useful info). However, if you now get such an abuse complaint, you might now have a better idea what to look for and what to reply to your hosting provider to try and convince them you are in fact a victim and not a perpetrator! Who knows, they might even care to listen. This article was written in a rush a few hours before getting on a plane. Sorry for the lack of proof-reading and potential typos! « Previous My wishlist for NixOS security in 2024+",
    "commentLink": "https://news.ycombinator.com/item?id=41982698",
    "commentBody": "How to get the whole planet to send abuse complaints to your best friends (delroth.net)240 points by scd31 7 hours agohidepastfavorite50 comments Habgdnv 3 hours agoThis is nothing new. A few years back, I implemented a very basic firewall rule: if I received a TCP packet with SYN=1 and ACK=0 to destination port 22, the source IP would get blacklisted for a day. But then I started getting complaints about certain sites and services not working. It turned out that every few days, I'd receive such packets from IPs like 8.8.8.8 or 1.1.1.1, as well as from Steam, Roblox, Microsoft, and all kinds of popular servers—Facebook, Instagram, and various chat services. Of course, these were all spoofed packets, which eventually led me to adjust my firewall rules to require a bit more validation. So, I can assure you this is quite common. As a personal note, I know I’m a bit of an exception for operating multiple IP addresses, but I need the flexibility to send packets with any of my source addresses through any of my ISPs. That’s critical for me, and if an ISP filters based on source, it’s a deal-breaker—I’ll switch to a different ISP. reply wolrah 2 hours agoparent> As a personal note, I know I’m a bit of an exception for operating multiple IP addresses, but I need the flexibility to send packets with any of my source addresses through any of my ISPs. That’s critical for me, and if an ISP filters based on source, it’s a deal-breaker—I’ll switch to a different ISP. If you actually have your own IP addresses this is normal and expected, but if you're able to use ISP A's IP addresses through ISP B or vice versa that has always been a bug that you are wrong to use. If you are doing the latter this is firmly in the \"reenable spacebar heating\" category and I hope your ISPs fix their broken networks. reply sulandor 1 hour agorootparentmaybe spacebar heating is a reasonable requirement after all and the joke was just that it's easy to get it wrong reply Habgdnv 1 hour agoparentprevOkay, looks like I will reply to a few of the comments to clarify things. I’ll give a concrete, real example. I worked at a company that hosted some web assets on-prem in one of their branches. They had a 1Gbps connection there. However, at HQ, we had multiple 10G connections and a pretty good data center. So, we moved the web VM to HQ but kept the assigned IP address (a public static from ISP-A). We routed it through a VPN to HQ. The server used our default GW and sent responses with source IP (ISP-A) via ISP-B (10G). That way, we utilized 10G outbound, even though the inbound was limited to 1G. It was only for GET requests anyway. I know this wasn’t the most optimal setup, and we eventually changed the IP, but it seems like a valid use case. Scenario 2: We had two connections from two different ISPs (our own ASN, our own /23 addresses). We wanted to load balance some traffic and sent half of our IPs through ISP-A and the other half through ISP-B. It worked fine, but when we tried to mix the balance a bit, we found an interesting glitch. We announced the first /24 to ISP-A and the second /24 to ISP-B, but ISP-A had RP filtering. So, we had to announce all the IPs to them. The way the RP filter works, as you may guess, means we cannot prepend or anything. All traffic must come through them. If they see a better route for that prefix, they will filter it. For a few months, they refused to fix this, citing security. There’s no shame in security best practices, so I might as well name the ISP—Virgin Media. Note that the internet with rp_filter is not $20/month. It was more like 5K+/month!! And we did not change it due to lack of alternatives there. But otherwise guess who loses the contract :) reply Stefan-H 9 minutes agorootparentIn your first scenario, any connections established through the ISP-A's IP address would be routed back through the VPN connection that they came in on. If that server were to establish it's own connections to external resources, it would feasibly be able to use the 10g connection from ISP-B. It would not be able to dictate what source address was used with connections coming from ISP-B. reply jcalvinowens 3 hours agoparentprev> but I need the flexibility to send packets with any of my source addresses through any of my ISPs As someone who always enables rp_filter everywhere... I'm very curious why? reply Jerrrrrrry 2 hours agoparentprev>As a personal note, I know I’m a bit of an exception ...That’s critical for me, and if an ISP filters based on source, it’s a deal-breaker—I’ll switch to a different ISP. \"...and obviously, Pennywise, I must spoof ingress and egress...\" \"Of course, Agent Bond.\" reply pixl97 3 hours agoparentprev>I’ll switch to a different ISP. I mean, technically those ISPs would be in violation too. You need your own ASN. reply rvnx 1 hour agoparentprevIs IPv6 fixing such cases by design or it's not changing anything ? reply toast0 1 hour agorootparentNot really. Early IPv6 documentation kind of assumed that the vast address space would lead towards hierarchical addressing and that a multi-homed user would use addresses assigned by all of their ISPs, but at least in my experience, that doesn't really pan out --- if you have router advertisements from two different ISP prefixes, automatic configuration on common OSes (windows, linux, freebsd) will lead towards often sending traffic with ISP A through the router from ISP B, which doesn't really work well, especially if either or both ISPs run prefix filters. There's probably ways to make that style of multihoming work, but it's not fun. Turns out, most multiphomed IPv6 users need provider indepdent addresses, just like with IPv4. And then you need to make sure your all your ISPs allow you to use all your prefixes. On the plus side, it's much more likely to get an IPv6 allocation that's contiguous and that you won't outgrow; so probably you only need one v6 prefix, and you may not need to change it as often as with v4. reply ianburrell 50 minutes agorootparentThe advantage of IPv6 is that can multiple addresses. This means that good way to organize network is to have machines use local provider addresses to access the Internet. Then have ULA addresses for internal network. Those will be routed with tunnels and VPNs. That separates accessing the internet from internal network, and means that don't need to have routable address space. The only people who would need own address space have data centers and routers. reply ziddoap 4 hours agoprevThis type of issue can be incredibly annoying to deal with, because the legitimate answer to the abuse report (\"someone is spoofing my IP, it isn't me, and the machine is not compromised\") is the exact same excuse that a malicious actor would provide. Then, as noted in the article, you're trying to prove a negative to someone who doesn't really care at all, which is borderline impossible. reply toast0 3 hours agoparentHertzner says in the email that no response is necessary. Automated abuse reports of things that are easily spoofed don't justify a report, but might justify a quick check to make sure your box is still operating correctly and hasn't been taken over. reply ziddoap 2 hours agorootparent>but we do expect you to check it and to resolve any potential issues. That's the important part. If they receive another one (or two, or a few) more abuse reports, they assume it is not fixed, and will expect a response then. Which ends up being annoying. reply dataflow 4 hours agoparentprev> the legitimate answer to the abuse report (\"someone is spoofing my IP, it isn't me, and the machine is not compromised\") is the exact same excuse that a malicious actor would provide. The legitimate answer would include some sort of real-world attestation about you from a trusted third party. Probably the very least, some evidence of your identity and jurisdiction. Maybe including a video call or something. Not just you anonymously claiming you're a good guy over the internet and expecting to be believed. reply preciousoo 4 hours agorootparentHetzner (if they keep logs) should be able to verify if a user has been sending arbitrary packets out on port 22 very trivially reply Ardren 2 hours agorootparentJust what type of logs do you expect Hetzner to keep? reply matrix2003 2 hours agorootparentSplunk logs of traffic. It’s pretty common at the corporate level. reply preciousoo 2 hours agorootparentprevAt minimum? In/outbound traffic reply ziddoap 3 hours agorootparentprev>The legitimate answer would include some sort of real-world attestation about you from a trusted third party. It's annoying to find someone (or some service) that is willing to attest on your behalf and have that person (or service) be trusted by your provider more than whoever filed the abuse complaint. >Maybe including a video call or something. It's annoying to find someone at your provider who will take the time to do this. It's annoying to take my time to have to do this. My point, overall, was that this is just a really annoying problem. reply 8338550bff96 3 hours agorootparentprevYeah, let's just have everyone hosting TOR nodes out themselves and their friends to local authorities... Nice try Winnie Poo reply dataflow 3 hours agorootparentDamn, well you definitely foiled my plan there. reply shadowgovt 2 hours agorootparentprevSo it turns out at the network service level, anonymity has never been guaranteed. If I, as another chunk of the network, can't trust your chunk, it's going to get cut from accessing me. There has to be some ability to establish baseline trust. reply buildbuildbuild 1 hour agoprevThe “someone hates Tor relays” theory doesn’t sound worth the effort. This could be an entity running malicious relays, while also trying to unethically take down legitimate relays to increase the percentage of the network that they control. reply Rasbora 2 hours agoprevBack in the day I would scan for DrDoS reflectors in a similar way, no hosting provider wants to get reports for port scanning so the source address of the scan would belong to an innocent cloud provider with a reputable IP that reflectors would happily send UDP replies to. The cloud provider would of course get a massive influx of complaints but you would just say that you aren't doing any scanning from your server (which they would verify) and they wouldn't shut your service off. The server sending out the spoofed scan packets is undetectable so you're able to scan the entire internet repeatedly without the typical abuse issues that come with it. I'm not sure how often this happens in practice but tracing the source of a spoofed packet is possible if you can coordinate with transit providers to follow the hops back to the source. One time JPMorgan worked with Cogent to tell us to stop sending packets with their IP addresses (Cogent is one of the most spoofer friendly tier 1's on the internet btw). This is the first time I've heard of this being used to target TOR specifically which seems counterintuitive, you would think people sending out spoofed packets would be advocates of TOR. Probably just a troll, luckily providers that host TOR won't care about this type of thing. reply JoshTriplett 3 hours agoprev> Which means, if you just find one transit provider which doesn’t do BCP38 filtering… you can send IP packets tagged with any source IP you want! And unfortunately, even though the origins of BCP38 date back to 1998… there are still network providers 25 years later that don’t implement it. What would it take to get enough network providers to start rejecting traffic from all ASes that don't implement this, so that spoofing was no longer possible? reply benlivengood 3 hours agoparentCloudflare is probably enough. They already control enough ingress that their \"checking the security of your connection\" could actually mean something. reply toast0 2 hours agoparentprevYou'd have to find some way to make network providers care. Especially 'tier 1' transit providers and other networks of unusual size. It's much easier to work on reducing reflection multipliers though, because you can scan (ipv4 anyway) for reflection vectors and yell at people that will respond with 10x the input bytes. reply mrbluecoat 4 hours agoprev> The internet was broken 25 years ago and is still broken 25 years later. Spoofed source IP addresses should not still be a problem in 2024, but the larger internet community seems completely unwilling to enforce any kind of rules or baseline security that would make the internet safer for everyone. Same with spoofed MAC addresses, email addresses, ARP messages, Neighbor Discovery, MitM TLS certificates ... It's amazing anything works anymore :D reply colechristensen 3 hours agoparentThe thing is, obviously, that the Internet isn't broken, it has incredible utility and reliability. If it was designed and operated to be perfect, then it would likely be massively broken quite often. It is the tolerance for mild brokenness that has contributed significantly to its robustness and utility. That isn't an argument for not improving things though, just a warning against perfection, if you chase it then you're liable to make really big mistakes that ruin everything. reply Asmod4n 3 hours agoparentprevIt’s quite sad the only mail server out there which checks if you are allowed to use a email address is exchange. With all others you can set the from: header however you like. reply salawat 2 hours agorootparentWho cares whether it's the MTA that does it or a collection of daemons invoked by the MTA? Just get things configured correctly, and you should be gold. Now as far as every other mail operator setting up their stuff right such that From spoofing is no longer feasible, well... Can't help ya there. I don't run my email to make money, so the incentive to adopt pathological configs for the sake of maximizing the number of users/Domains who can send from one IP ain't there. reply cobbal 3 hours agoprevIt's a similar problem to swatting. It relies on authorities taking severe action against an unverified source of problems. I suppose a difference is that they use unaffiliated parties to send the complaint, instead of contacting the authority directly. reply jmuguy 4 hours agoprevIt seems like systems shouldn't report abuse (at least automatically) for single packet, no round trip, requests unless its reaching denial of service levels of traffic (and maybe these are). Like in particular for SSH there's no way thats even a valid connection attempt until some sort of handshake has occurred. reply Avamander 4 hours agoparentSometimes that's all the abuse you'll see though, with for example port scans. reply boring_twenties 2 hours agorootparentWell the obvious answer there is that port scans shouldn't be considered abuse absent other factors like rising to the level of a DoS. reply fullspectrumdev 42 minutes agorootparentExactly this. A single SYN or TCP connection doesn’t constitute abuse. Unfortunately many people seem to think otherwise and will spaff abuse reports over an errant SYN packet reply ahofmann 4 hours agoprevHow difficult would it be to highjack this attack by sending these packages to everyone, so that providers like hetzner would get swamped with abuse emails? This way the attack would not work anymore. Either the honeypots would stop sending abuse emails, or the providers would filter those out. reply fullspectrumdev 40 minutes agoparentTrivial to accomplish really. Just acquire a few boxes that don’t block spoofing outbound SYN packets and start spamming random IP’s from random IP’s with SYN packets. It will generate a shitload of abuse emails and accomplish mostly nothing except fill up disk space with useless emails and such. reply preciousoo 4 hours agoparentprevOr someone would figure out how to find who’s behind the spoofed requests, as those orgs have the resources to do so reply Ekaros 3 hours agorootparentWhy not make ISPs responsible for blocking any such traffic. In the end it must originate from someone's network. And really they also should know who their peering partners are and what traffic should be allowed from there. reply salawat 1 hour agorootparentWhich do you prefer? Internet where you send a packet over the wire and the network takes it and delivers it per RFC. Basically OG Internet. Network of networks of more or less trusted peers. Or Internet where you need to requisition every connection/circuit be provisined before it is routed, which includes explaining why you need the service, and where any provider in the chain will deny you transit by default? You now must forge an intimate relationship with every middle box between you and the other endpoint. This process must be repeated by everyone on the network. Just operating as a middle box for someone else is now fraught with legal liability; as anything one of your transit's end up doing, you are now considered complicit in. Both of these architectures of an Internet are equally valid and functional. The society that uses them however is completely different. I prefer the former, warts and all, and lack of throat to throttle short of the asshat running the software on the other end, over the latter, because with the former at least, we're not creating power nexii to attract asshats to NetOps positions. With the latter setup, sure, your spam problem has an ostensibly way higher barrier to entry in the form of having to create human trust networks, but the accretion of social power distinctly changes the culture of the net sector, attracting a type of personality that should never, ever be trusted to be given a yay/nay authority over other folks access to a network. reply 4star3star 24 minutes agorootparentGood comment. I looked up \"nexii\" out of curiosity, and it appears that \"nexuses\" is the appropriate plural, FYI. reply pixl97 3 hours agorootparentprevI'm going to guess quite often these spoofed requests are coming from other nations that have little interest in playing nice on the global internet. reply preciousoo 3 hours agorootparentFor sure, but orgs tracking abuse on the net like CF and the like have demonstrated the ability to identify nation state level actors reply dataflow 4 hours agoparentprevProbably easy, as long as you don't mind being on trial for violating something like CFAA. reply wizzwizz4 3 hours agoprevThere's no in-band solution to this problem, but out-of-band solutions might exist! For example: (1) Notify the destination ISP that you're receiving backscatter. (2) That ISP checks where the packets are coming from, and notifies that ISP. (3) Repeat step 2 until source is found. (4) Quarantine that part of the network until it behaves better. At the end of the day, the internet is people. reply salawat 1 hour agoparentPeople are sometimes shocked to learn that the Internet as a whole works because there is a subset of humanity that really, really likes overseeing the most over-the-top pipe game in existence. reply wizzwizz4 1 hour agoparentprevSee also: https://news.ycombinator.com/item?id=41985920 reply preciousoo 4 hours agoprev [–] This is pretty clever reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hetzner sent a warning about a potential security issue involving malware on a server's IP, but the investigation revealed no malware.",
      "The issue was traced to TCP reset packets from random machines, indicating IP spoofing, a tactic used to send fake connection requests and trigger abuse complaints.",
      "The attack targeted a Tor relay, underscoring persistent internet security challenges, such as unfiltered IP spoofing, and the necessity for improved enforcement of security practices like BCP38 filtering."
    ],
    "commentSummary": [
      "A user experienced issues with legitimate services like Google and Microsoft after implementing a firewall rule to block spoofed TCP packets, underscoring the persistent problem of IP spoofing.",
      "Despite guidelines like BCP38, which aim to prevent IP spoofing, many networks fail to implement the necessary filters, allowing attackers to send packets with fake source IPs.",
      "The discussion also highlights the difficulties in managing multiple ISPs and IP addresses and the challenges of proving innocence when facing abuse complaints, reflecting the internet's flexibility and potential for misuse."
    ],
    "points": 241,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1730200677
  },
  {
    "id": 41975993,
    "title": "Why so few Matt Levines?",
    "originLink": "https://gwern.net/matt-levine",
    "originBody": "economics, writing psychology Why are popularizing educational newsletter-frequency writers of important fields like Matt Levine for finance so rare? Because most fields are too slow or ambiguous, and writers of the right combination of expertise, obsession, and persistence are also rare. 2024-09-24–1d2024-09-24 in progress ⁠certainty: possible ⁠importance: 3 ⁠similar⁠ ⁠bibliography⁠ External Links Footnotes Similar Links Bibliography ⁠Matt Levine⁠ is the most well-known newslettrist (⁠“Money Stuff”⁠) in the financial industry, having blogged or written since 201113ya, finding his niche in popularization after stints in Wall Street & law. His commentary is influential, people leak to him, he sometimes interviews major figures (notoriously, Sam Bankman-Fried) or recounts inside information, and a number of phrases like his “laws of insider trading” (specifically, how not to) have gained currency to the point where readers can now do much of the work of sourcing an issue for him. He is read by hundreds of thousands of readers (including myself)—everyone from shoeshine boy to billionaire. The size of his audience is respectable, but perhaps its most remarkable feature is that many of those readers have nothing to do with the financial industry. Though his newsletter is officially a Bloomberg News newsletter which he simply writes, many of his readers will visit Bloomberg solely for him, and indeed, might have little idea who or what a Bloomberg is. Nevertheless, readers loyally tune in for each installment every few days to learn about arcane financial instruments they have never heard of before, and (except for Levine) never will again. One might ask (and indeed, a billionaire once did), “where are the other Matt Levines?” or ⁠“who are the Matt levines of other fields?”⁠ That is, where are the Matt Levines of, say, chemistry or drug development⁠⁠1⁠, who explain & popularize other major industries which are vital to modern life, directly or indirectly appear in the news often, and yet people are widely ignorant of it, and deeply misunderstand its fundamental dynamics? Why don’t we have a Matt Levine for every industry? Where is the Levine of, I don’t know, petroleum refining or fracking, of shipping containers? Are we just in need of a good list of recommendations? Or could we just set up a prize to coax out some potential Levines in other industries? When I first met Matt, the first thing I said was “Matt Levine, only you can do what you do!” ⁠Tyler Cowen⁠ My pessimistic conclusion is that Matt Levines are not made, they are born, and that the Matt Levine formula is largely irreproducible: there are few industries where it makes sense, and there are few people suited for this job, and that is the simple answer why there are not many Levines. So, what is the formula, exactly? The Matt Levine formula is weighty matters, leavened by humor, with basic explanations of complicated financial matters. As Levine has been doing this online for so long (~13 years), relatively speaking, he can often refer to his previous coverage and comment on how things turned out. Certain themes repeat periodically so often that they receive their own catchphrases, like “worries about bond market liquidity” or the laws of insider trading. Many people owe most of what they know about stock trading, bonds, arcane but controversial matters like naked shorts, meme stocks etc to Levine; and I would be embarrassed to admit how much of my economics knowledge comes through Levine rather than some more rigorous source like my old economics textbooks. This is because Levine provides 3 key ingredients which foster learning: cases with known outcomes/answers: to develop expertise in a subject, the subject ideally provides many problems, with known answers, of high accuracy. Most subjects do not. But Levine’s subject (finance & law) does. A good subject for developing expertise is something like chess: an endless number of chess games can be played rapidly, they all have a clear outcome (win/draw/loss), and one can study each one carefully to understand what went right or wrong. A bad subject is something like military strategy: there are not many large-scale wars which have been documented adequately, each war is unique and unrepeatable and a general may participate in only a few in a lifetime, and the outcomes (never mind any individual’s contribution) are often difficult or impossible to judge. Many areas are more like military strategy than chess—how do you judge the expertise of a CEO, or a Hollywood director, or a scientist forecasting the distant future? Levine works in an area which does provide many clearcut examples, because he focuses on lawsuits, prosecutions, crimes, and deals. These are examples where the outcome will usually be known in a few years, at most, or at least a major update/development, and where the involved parties do all the research necessary, and where the evidence is often completely unambiguous—Levine just has to read their filings, and excerpt the text message where someone boasts about their insider trading in no uncertain terms. In relying on reporting & filings for his commentary, Levine is very much like the dying local newspaper crime reporter, who relies on the police blotter & courtroom access to rapidly file their articles. These articles are nearly endless, and mostly forgettable because there are, broadly speaking, not really any principles governing local crime. “Some guy got drunk and into a fight and killed another guy” is something that happens frequently, but it illuminates no universal principle; it sheds no light on anything else. It was just something horrible that often happens at random when people take dangerous drugs like alcohol rather than safe ones like nicotine⁠, and is of no broader importance; true-crime addicts consume it for its entertainment value, like darker versions of Hollywood tabloids—“who murdered who” instead of “who’s sleeping with who”. It’s just “one d—n thing after another”. (The TV show ⁠Cops⁠ has run for 36 years now, and could run for another 36 years without breaking a sweat, but after 72 seasons, what would you have learned that you didn’t learn after the first few?) But in Levine’s area, this is not the case. Many of these examples are due to highly intelligent, motivated, competent people and organizations clashing for deep reasons. This means that to understand them, you need… first-principles explanations: most people experience an “illusion of depth”, in that they believe they understand the causal mechanics of an area far better than they do. But in fact, they have learned only a superficial model of the area. Levine corrects this. Particularly in economic matters, people believe many intuitive folk economics like eg. building new houses cannot lower prices, or that businesses raise prices simply “when they feel greedy”, or that voluntary transactions must have a loser & a winner when other people transact (but not themselves personally), or that a policy will have only intended effects because everyone will just do what they are ordered to (even though they personally work around policies or rules all the time for doubtless noble reasons). Levine patiently gives from first-principles (supply-and-demand, market efficiency & adverse selection, people following incentives, public choice theory⁠) explanations of why some thing about markets or contracts is the way it is, how it operated (or failed to operate) in a particular case, and what (and why) the various counterfactual future outcomes are. These repeated explanations—however simplified and abstracted—gradually build up genuine knowledge which can transfer to the real world beyond some crammed supply-and-demand schematics in a long-forgotten economics class. spaced repetition⁠ enabled by fast turnover: a newsletter is inherently spaced in time, and by returning to themes repeatedly, with various twists or instantiations, the reader learns due to the spacing effect. In normal news consumption, as opposed to the drip-feed of a columnist on a steady beat, one might read about some instance of financial malfeasance in great depth in the WSJ or NYT, say—but once. This coverage might be extremely high quality, but nevertheless, such “massed presentation” is a recipe for forgetting. It is like cramming flashcards the night before the test: no matter how good the flashcards are or how much you remember while taking the test, most of it will be forgotten. However, in Levine’s case, even if specific cases or events resolve quickly, the same principle will show up again soon enough. So, that is the Levine formula: the global economy furnishes him many rapidly-resolved examples which he can use to entertainingly illustrate basic principles of economics, and by doing that so regularly over so long, readers gain a genuine durable education in economics which they will remember and where they can apply those principles on their own. Analyzed into parts, we can see why many areas cannot support a Levine: they lack one of the 3 ingredients: Crime reporting covers crimes which are numerous and rapidly-resolved, but there is not much to learn. Logistics like fracking or oil or containers may cover many cases which have broad principles, but those cases are often resolved in secrecy and due to the extreme boom-bust cycle of those industries, may take decades to ‘mature’ (eg. an over-extended oil company might not go bust for decades depending on how exactly cycles play out). And areas like drug development may be cursed by all 3: drug development often ends in failure for unknown reasons, in the dark, decades later, and what reasons are known may be totally idiosyncratic to a specific drug or disease; what is known may be at best a loose rule of thumb. It would be nice to have a blog like Matt Levine covering, say, evolutionary biology or Greco-Roman philosophy, but it’s obvious why that isn’t going to work—you can’t have a very entertaining newsletter when it might take centuries for a debate to resolve, if anyone can agree it was resolved at all, and you certainly aren’t going to be able to provide so many clear illustrations of basic principles that reading the newsletter constitutes an education. (You can read & learn about them, but their natural form would be, well, a textbook or a monograph or something like that, and definitely not a newsletter.) And most areas are more like drug development than they are like hedge funds suing each other over some contractual gimmick or clerical error. OK, but surely there are still plenty of areas where the preconditions are met? (Particularly rapidly-developing ones, like cryptocurrency or AI recently?) So where are their Matt Levines? This brings us to the second half of the equation of the Matt Levine formula: the Matt Levine part. Consider the implication of the 3 requirements for the author, rather than the reader: they are going to see the same human comedies play out, again and again, and have to shout it into the void again, only to watch it happen yet again. The author feels the weight of the repetition far more than any reader does.⁠⁠2⁠ It is like a teacher who must teach the same curriculum for the 30th time—and again read & grade each of dozens of assignments on it by hundreds of students. It is not every person who can do so well, or at all. Often a great expert will make a terrible teacher, because they are unable to endure the repetition, or understand the ignorance of the beginner, or treat the floundering student with kindness. I personally appreciate Levine’s ⁠permanent fascination with finance⁠, his willingness to explain the same things over and over. But I don’t think I (or most people) would be able to do so for a long time without turning it into a dull ticket-punching exercise as part of a mundane job rather than an avocation; and indeed, I have shunned my opportunities to become a Levine of some area, when I felt my interest & patience for fools rapidly waning. (Particularly darknet markets: there was considerable demand for commentators like Eileen Ormsby or DeepDotWeb, but I could see no new principles to maintain my interest and just an endless infosec churn of temporary trivia—a warning that burnout was approaching—and quit the area while I was ahead.) Indeed, expertise is a reason to stop teaching entirely: if you are really interested in an area, and good at it (good enough to understand and commentate ongoing events), then even if you are a great teacher who is gifted at explaining the area to laymen, why would you settle for teaching about it instead of doing it? But if you do it, then you will struggle to write about it regularly publicly: actually doing something, instead of reading a court summary of it, can take years of hard work, and prohibit you from writing about it in various ways. (It will also usually pay much worse—Matt Levine is doubtless compensated handsomely by Bloomberg, but perhaps not as handsomely as if he had kept rising through Big Law, and superstar outcomes imply most would-be newslettrists are paid peanuts.) So you have a serious problem: anyone good enough to be ‘the Matt Levine of an area’ is also under considerable pressure to not be him. Why would anyone want to? Well, if you ask someone like Richard Feynman⁠ or Andrej Karpathy or Tom Lehrer why they pursued pedagogy instead of the professional pursuits that brought them fame & wealth, the answer would have to be that “they love to teach”. Which is a fine reason, but a passion for teaching a particular subject is far from common. So you have a filter with many layers: you need areas which fulfill a stringent set of conditions for such an educational newsletter, and you need a very unusual sort of individual, someone who is expert in the area and has preferably gotten their hands dirty, who is good enough to work professionally in it, but who also is capable of explaining it well, at a beginner level, many times, endlessly without burning out or getting bored, because of their intense interest in the area (but again, not quite intense enough to make them go do it instead of write about it). Each step here filters out most candidates, and by the end, there’s just not that much left. You can’t fix these filters easily. No prize or Substack tweak will suddenly make drug discovery happen fast and fail for clear reasons, or conjure up a Levine in a specific area when you want one. So, that’s why there are so few Matt Levines, and explains where the other Matt Levines are: they don’t, and usually can’t, exist. External Links Discussion: HN⁠ While I enjoy Derek Lowe⁠, the extent to which his posts are inside-baseball and do not repeat themes, or only repeat many years apart, emphasize the contrast with Levine. I know as an author, one thing that has surprised me is the extent to which you have to repeat something before a reader will remember it, in large part because they never read it in the first place. I often feel exhausted by discussing something, and feel like I must look like a crank ranting on about an obsession and thoroughly worn out my readers’ tolerance, when a reader says they just heard of it for the first time. This is because as an author, you know every time you wrote about something, across all the years, but a reader may well have read none of them. So if you repeat yourself only as often as you can bear to, you have usually fallen short of the mark. Similar Links ⁠[Similar links by topic]⁠ Bibliography Click to expandClick to expand ⁠[Bibliography of links/references used in page]⁠ [Send Anonymous Feedback] [Quote Of The Day] [Site Of The Day] [Annotation Of The Day] [adblock public service announcement]",
    "commentLink": "https://news.ycombinator.com/item?id=41975993",
    "commentBody": "Why so few Matt Levines? (gwern.net)236 points by iNic 22 hours agohidepastfavorite131 comments cainxinth 5 hours agoNewsletters recommended in this thread: - Matt Levine - Finance (Bloomberg's Money Stuff) - Derek Guy (@dieworkwear) - Menswear - Derek Lowe - Chemistry and drug development (In The Pipeline blog) - Ann Lipton - Business law - 3Blue1Brown (Grant Sanderson) - Mathematics - Mark Brown - Alcoholic beverages industry news - Bret Devereaux (acoup.blog) - Classical/Roman/Medieval history - Admiral Cloudberg - Aviation disasters and safety - Patrick McKenzie (patio11) - Financial sector (Bits About Money) - Ben Thompson - Technology business and strategy (Stratechery) - Financial Samurai - Personal finance - Rick Beato - Music industry (classic rock focus) - VX Underground - Cybersecurity - Brian Krebs (Krebs on Security) - Cybersecurity - Joel Spolsky (Joel on Software) - Software development - Marc Rubinstein - Finance industry (Net Interest) reply matthewdgreen 3 hours agoparentOne comment on all of these: many of these people used to work in a field, then left that field to pursue writing/content full time. There are thousands of other people actually working in their field who produce great and even more relevant content, but cannot pump it out at the same rate. reply open-paren 19 hours agoprevI think Derek Guy (@dieworkwear and \"the Twitter menswear guy\") is a decent \"Matt Levine\" for the menswear space, as much as there can be one for a subjective subject. He focuses on the clothes first and never his personal celebrity, although he uses his Twitter platform to promote the desire for affordable housing and respect for immigrants fairly frequently. I don't think there is a picture of him anywhere on the internet. He violently rejects all paid content opportunities. Instead, he focuses on the aspects of good tailoring, \"shape and drape\", and the history of many menswear items. His bug thing is that \"clothing is a cultural language\" that all speak. * putthison.com (he is among the writers, but the big itself is mostly dead now-a-days) * dieworkwear.com (his personal, infrequent blog) * styleforum.net (a forum where he used to participate a lot) I wish his (substantial) Twitter activity was available elsewhere. reply ZephyrBlu 15 hours agoparentHe does have a lot of good info, but also shits on people very frequently. I would prefer if he was a little more polite. reply Ar-Curunir 12 hours agorootparentThe people he shits on are usually people who get offended by the implicit vaguely left-ish politics of his posts. I don't see the harm in him dunking on them, since it doesn't affect the quality of his actual content. reply mhh__ 17 hours agoparentprevThe menswear guy spends a ridiculous amount of time just trying to own people in his replies which honestly seems extremely childish. e.g. I saw someone say \"Don't try to go toe to toe with him, he grew up in [snakepit forum with lots of insults]\" — these forums are for morons (for which there is time and a place) I don't see the similarity with Levine. reply open-paren 14 hours agorootparentHe absolutely wastes the bits dunking on dumb people in the comments, but his on topic writing is unparalleled. Take his most recent blog post, American Space Cowboys. It's an indepth discussion on the fashion of the Apollo 11 trainees. That kind of article doesn't happened very often and has lots of fundamental principles and history of workwear dress pieces. reply klausa 12 hours agorootparentprevMy Levine backlog has grown to the size of multiple novels by now; but I guess I consider Derek's dunking on people in his replies as similar in spirit to Levine's making fun of weird crypto stuff and NFTs? Sure, one is less personal, but the ~ vibe ~ doesn't seem that much different. That's one of the things that makes reading Money Stuff so fun! reply mhh__ 7 hours agorootparentI think Levine works because he writes like a company memo written after a glass (but no more) of wine. reply crashabr 17 hours agorootparentprevI'm pretty sure that everyone who reads him for his knowledge would agree that the research (even if from his own past writings), formatting, illustration of his many threads take him much more time than the one-up replies. They're also pretty easy to ignore (unless you somehow feel offended by them), and mainly appear on people's feeds because of how retweeted they are. reply mhh__ 7 hours agorootparentSo I don't feel offended but one of them was scrapping with a minor (and once fairly controversial although I think he's cleaned up his act a lot, but I still don't trust) British political figure for not wearing a suit right or something, because he suggested that his ideology destroyed the world of menswear that he hankers for. I'm sort of inclined to agree even if I don't care. Dispute aside, I just found it a bit strange because the other guy shops at primark (cheap and crap mass produced clothes) and hates wearing suits, but he just kept trying to dunk on him. I'd hate to be friends with someone like that. Obviously if you just want to know if your suit is good or not then it doesn't matter. reply _bin_ 16 hours agorootparentprevYeah second this. Most of Levine’s critique is well-aimed, well-reasoned, and well-directed at significant people. Menswear’s “critique” consists mostly of trying to “own” random hundred-follower anons. Same vibes as Ben Shapiro “owning libs” by “debating” random, unprepared college students. reply carabiner 13 hours agoparentprevMenswear guy has never shown how he actually dresses and his \"critiques\" are just political statements. For any political figure, it's left-leaning views == good dress, that's all. reply throwaway19972 12 hours agorootparentHis advice is good; I've found multiple outfits based on his recommendations. Given that he specifies how to select clothes based on form, function, and taste, and not by pimping brands, this is absolute gold. Who gives a damn how he dresses; I hope he takes his own advice because it'd be pretty weird not to, but he could walk around nude for all I care. reply elevatedastalt 13 hours agorootparentprevYou are being downvoted but you are exactly right. Whatever redeeming value Derek's fashion statements have has been completely overtaken by his insufferable political avatar. reply EdwardDiego 7 hours agorootparentAww, did the man with good clothing advice hurt your political feelings? Seriously mate. reply nabla9 21 hours agoprevDerek Lowe is the Matt Levine of chemistry and drug development. https://www.science.org/blogs/pipeline His \"Things I Won't Work With\" series is legendary even in HN. Ann Lipton is the Matt Levine of business law. https://www.businesslawprofessors.com/author/alipton/ reply loeg 20 hours agoparentSee footnote 1. > While I enjoy Derek Lowe, the extent to which his posts are inside-baseball and do not repeat themes, or only repeat many years apart, emphasize the contrast with Levine. reply aftbit 20 hours agorootparentI disagree with both points. I would say he is indeed a Matt Levine of his field. Perhaps I'm misunderstanding the author's argument though. 1. His posts are inside-baseball - this is true for some posts, but many are written to be approachable to all audiences, especially during the coronavirus. This is similar for Matt Levine's posts, but as a finance nerd, not a drug discovery one, the author does not see this. 2. [His posts] do not repeat themes - he has many examples of repeated themes, including the whole science-over-money aspect of the Aducanumab saga, various times he has touched on \"AI\" drug discovery, and the whole Things I Won't Work With series. reply mech422 20 hours agoparentprevGotta Love Derek... \"Things I won't work with\" actually got me to read some of his other stuff about how drug development and trials work. reply hn_throwaway_99 20 hours agoparentprevI just started watching some of his videos, but the 3Blue1Brown guy seems to fit in this bucket, and he's got a ton of HN fans: https://news.ycombinator.com/item?id=41818779 reply _bin_ 16 hours agoparentprevOne more for the pile: Mark Brown’s Industry News Update for alcoholic beverages. It’s a little less explain-everything-easily than Levine but it’s also a less jargon-deep area. reply JumpCrisscross 18 hours agoparentprevDoes Lowe have a newsletter? reply creddit 13 hours agorootparentYou can subscribe to his blog with RSS: https://www.science.org/blogs/pipeline/feed reply gradschoolfail 13 hours agoparentprevIf you dont find his humor grating (I do, but YMMV — its hard to hit the sweet spot as Mr. Levine does) Scott Aaronson might be the ML of quantum computing. (i also find the field slow, but i really think im the odd one out!) reply quuxplusone 14 hours agoprevMatt Levine is _funny_, which is a rare talent; to combine that talent with expertise and desire-to-write is even rarer. Also, people like to read about money stuff because even if your mammal brain knows you'll never get rich using anything he tells you, your reptile brain still thinks you might. A similar market niche: home repairs. Gwern says Greco-Roman philosophy wouldn't work as a niche; but my one offering to the list of Matt Levine simulacra actually does write about Greek and Roman _stuff_, if rarely philosophy: Bret Devereaux at https://acoup.blog . You might know him from his \"historian's analysis of the Battle of Helm's Deep\" or his three-parter on war elephants. Even though the people being studied are long dead, the people _doing_ the studying are very busy producing new works — there's no danger that ACOUP will ever run out of new books to review or new theories to evaluate. reply rocqua 10 hours agoparentI do really think ACOUP is a great comparisson. It's not a newsletter, but it still hits the spaced repetition point. It doesn't have the advantage of following news, but it makes up for this by having a huge 'backlog' of things to write about. And it still has the 'known outcomes' point, because largely the outcome of history is known. And on the open questions, there isn't much progress, but there are themes to hearken back to. So even there you still have spaced repetition. reply Jun8 20 hours agoprev“So you have a filter with many layers: you need areas which fulfill a stringent set of conditions for such an educational newsletter, and you need a very unusual sort of individual, someone who is expert in the area and has preferably gotten their hands dirty, who is good enough to work professionally in it, but who also is capable of explaining it well, at a beginner level, many times, endlessly without burning out or getting bored, because of their intense interest in the area (but again, not quite intense enough to make them go do it instead of write about it).” I think the first filter is a much bigger factor, but for a reason that’s not mentioned in the post: $$$s to be gained. A large number of people read finance blogs because they think they themselves make money with the knowledge therein. This is hard to replicate with other examples given: for purely abstract topics, eg Greco-Roman philosophy, the prize is purely intellectual. For others, eg fracking and drug discovery, you can make tons of money but pool of interested people is much smaller. The only combination of general appeal and personal gain that could be interesting, I think, would be dating. You have recurrent patterns, clear outcomes, and pretty much everyone is interested and want to learn how to get better at it. reply JumpCrisscross 19 hours agoparent> $$$s to be gained. A large number of people read finance blogs because they think they themselves make money with the knowledge therein Levine's blogs aren't geared to this audience. The pitch is simpler: people in finance have money and the people with the most money in finance tend to enjoy (or at least spending way too much time) thinking about finance. (I'm in finance and I'm good at it.) If I could pay for Matt Levine separately from Bloomberg, I would. Not because I expect to profit off it. But because it's fun, informative and once in a while helps me explain something to someone else (usually a friend) more concisely. reply kelnos 18 hours agorootparentI think Levine is probably doing fine money-wise. I assume he's paid a salary (or has some other financial arrangement) with Bloomberg. As for Bloomberg itself, you can subscribe to Money Stuff (via email) without paying for Bloomberg. (Perhaps you just want to throw some cash his way because you get value from his writing, and want to show your appreciation; ignore me in that case.) reply JumpCrisscross 17 hours agorootparentSorry, I spoke imprecisely. The bulk of the value I get from my consumer Bloomberg subscription comes from his writing. reply metadat 15 hours agorootparentWhy not mail him a check*? * sincere question. reply JumpCrisscross 15 hours agorootparentI agree that he is “probably doing fine money-wise.” But you’re right. I’ll ask him if I can get him dinner or donate to a favoured charity. reply metadat 14 hours agorootparentI didn't mean to imply anything about Matt Levine or his financial well-being. I'm all about supporting people who I believe in, and personally I appreciate uos writing. I'm less in favor of propping up inefficient organizations as a byproduct. Bloomberg is a bit sus at this point, I'm not clear on whether the top has your and my best interests at heart. reply morgante 19 hours agoparentprev> A large number of people read finance blogs because they think they themselves make money with the knowledge therein. Do you actually read his work? I certainly don't expect to make money from reading it. I'd be surprised if a significant number of subscribers had that reason. It's entertaining and informative about the overall world, not something I expect to make money from. reply kevinmchugh 18 hours agorootparentI was always a boring passive investor and reading Money Stuff has made me more boring and more passive. Every day it's stories of people smarter than me getting hosed in ways I wouldn't have thought of reply bostik 11 hours agorootparentprevMoney Stuff is a weathervane and a lightning rod. Things show up in Matt's writing sometimes months before they make a splash in the rest of the world. I work adjacent to finance, but I'd read Money Stuff even without that link. It's a good way to keep myself up to date on happenings in and around finance, and of course, the newsletter is often funny. I don't expect to ever make money from my reading activity, but I have found that being well informed of weird and/or interesting developments within that world makes me more valuable at work. Being able to predict what kinds of compliance and governance related curveballs may be coming, a few months before they land on our desks? Surprisingly valuable. reply 3eb7988a1663 16 hours agorootparentprevIf you were to commit a financial crime, he points out the numerous blunders that lead to someone getting caught. So, you could use his blog as a What-Not-To-Do guide. Most of which comes down to: 1) keep your mouth shut, no matter how much you want to brag about your great scam 2) one play only, repeats at bat is how you get greedy and sloppy 3) no huge options trades because of your one-time insider information reply anitil 9 hours agorootparentLooks like we're up to 15 laws now! [0] [0] https://github.com/0xNF/lawsofinsidertrading.comI think the first filter is a much bigger factor, but for a reason that’s not mentioned in the post: $$$s to be gained. A large number of people read finance blogs because they think they themselves make money with the knowledge therein. Anyone doing this with Money Stuff is, well, extremely confused. reply carbonguy 20 hours agoprevChiming in with a few of my favorite \"Levine-likes\": - For aviation disasters and safety, can't beat Admiral Cloudberg: https://admiralcloudberg.medium.com/ - For more about the financial sector, Patrick McKenzie is solid gold every time: https://www.bitsaboutmoney.com/ (and I believe an HN regular as well!) reply nsbshssh 18 hours agoparentCloudberg is awesome, but her articles are all about disasters and follow the same formula. The formula is great and covers everything from airline company culture to what the pilot ate for breakfast figuratively speaking to material science. However you wont get commentry on anything that is still flying. reply simonebrunozzi 19 hours agoparentprevYes, as patio11 reply screye 1 hour agoprevIn Reddit's wild-west era (pre-2017-ish), it used to house many 'Matt Levines'. They'd be resident greybeards of their respective subreddits. Reddit 'had' solved the 2 main conundrums of 'Matt Levines'. If you're in demand, you don't have the time to be blogging. If you're in demand, you don't want your public persona to low effort comments. But reddit wasn't a job, it was leisure with a pseudo anon persona. Its status as a pretty-filter-on-4chan meant that greybeards could offload their writing by linking to 4chan. Alas, in trying to make it appeal to the mainstream, Ohanian killed the golden goose and reddit became a sanitized quora-tiktok analogue. Gwern himself runs a couple of niche subreddits which occasionally see such grey-beards stepping in. But, the website is clearly in decline. IMO, this article comes a little late. Substack is the killer-app for Matt Levines. Substacks by Razib, Asianometry, Sebastian Raschka, Dominic Cummings, etc. rub shoulders with Matt Levine in their respective fields. I'm not rich enough to be paying $100/month across different substacks. But I occasionally get a 1 month 'binge-subsription' to my favorites. There is a lot of gold there. ____ Since folks are doing recommendations, I'm going to add a few across different platforms. - Frank Stephenson (Jonny Ive of car design) - Bill Bishop. (China expert. Stratchery's co-worker) - Stewart Hicks (Professor of Architecture at UIC) - Kenji (food. duh) - Healthcare triage (Distinguished Pediatrics professor. Dumbed down, but still good) reply ac-swe 18 hours agoprevBefore he joined Bloomberg, Matt got his start at www.dealbreaker.com. During my stint on wall st (2006~2015), DealBreaker was THE industry rag of choice. Daily news roundups, annual bonus/compensation gossip, rumors of pending layoffs or reorgs, all in one place. There is so much boring shit you have to read on a daily basis when working in capital markets. Breaking geopolitical news, WSJ / Financial Times / Economist / Bloomberg articles, endless internally published market commentary, all of it often written by someone who takes themselves too seriously. Matt Levine was a breath of fresh air, someone who was both technical AND irreverent. Nobody trades off of Matt's articles. Nothing he writes about is secret. His best stuff takes the form of \"this weird thing happened\" or \"XYZ made a dumb mistake.\" Matt is funny, and funny is VERY underrated in financial commentary. https://dealbreaker.com/author/mlevine reply hardwaresofton 14 hours agoparentJust to add to this, but I've interacted with him and he is also very gracious, curious, and terminally online (in maybe the most mentally balanced way I've ever seen before). I didn't go so far as saving the emails offline but I do know they're in my inbox somewhere. We don't have more Matt Levines because it's hard to create them. reply steveBK123 17 hours agoparentprevGFC era dealbreaker was incredible stuff. Bess Levine was great was also great in a different way and its too bad she's left the Wall Street beat. reply ac-swe 17 hours agorootparentFor sure Bess was the GOAT, db wasn’t the same after she left reply paulryanrogers 19 hours agoprevThere aren't so few. It's just that most don't have a big following. And every realm has its great teachers and authors. I had a sixth grade teacher who was a brilliant and certified historian, and a former pilot who served in the Vietnam war. Yet he worked at an ugly, outdated elementary school in an exurb of greater Cleveland. He made school a delight and taught me more world history in one year than all of junior high and high school combined. Few have ever heard of him, yet I doubt any of his students will ever forget him or any of the things he taught us. reply selimthegrim 18 hours agoparentInquiring minds want to know which exurb. reply paulryanrogers 16 hours agorootparenthttps://www.legacy.com/us/obituaries/news-herald/name/willia... reply steveBK123 17 hours agoprevOne reason he is an anomaly is the economics - he walked away from a very high paying job to write about the industry, for what initially had to be a large pay cut and certainly lower future earnings potential. He also has tremendous natural talent for writing with a dry sense of humor. At this point he's now been writing about the industry longer than he's been in it, and given his prominence and name brand, I am sure he is paid well for it. Most people at the peak of their professional ascendence, after the gauntlet of the right schools, the right jobs, etc.. are unable to walk away. reply ajkjk 15 hours agoprevI think that all of his explanations to do with the content itself are wrong. It's simply that a sufficiently good writer is fun to read every time they write. There could easily be such a writer about pretty much any field if anyone was good enough at writing and motivated enough to do it. But they're not (yet). There is a requirement that whatever field it is, they understand most of it really well. And there aren't that many people who have gotten to that point. reply aftbit 19 hours agoprev>first-principles explanations: most people experience an “illusion of depth”, in that they believe they understand the causal mechanics of an area far better than they do. But in fact, they have learned only a superficial model of the area. Levine corrects this. I've found almost the opposite. Many smart people see a new field and think \"ah this can't be too hard if we just approach it from first principles\" and then leave behind all of that field's accumulated knowledge. In finance, this is a well known trap for the unwary polymath. reply at_a_remove 12 hours agoparentIn my personal life, I have found great satisfaction in finding out how different people's jobs work, all of the things I wouldn't expect or know, and so on. I think it's fascinating. Whatever it is I think I can figure out from first principles might have been how the field started, and then there's the other ninety-nine percent that comes with it, the non-obvious stuff. But I am a very curious cat. reply mandevil 17 hours agoparentprevs/smart people/computer programmers/g I mean, that was his entire point about crypto- how it speed-ran the entire history of finance markets and discovered- in very painful and usually expensive ways- why each bit of market regulations exist. reply blueyes 19 minutes agoprevThere's a tautological undercurrent to this piece. Matt Levine is rare because he has certain characteristics, which are rare. It misses a larger point which is that advertisers pay for readers who are interested in financial content, and they tend to pay less in most other areas of coverage. So other industries do not produce Matts as easily, or takes bets on writers to see if they work well driving people into the wide end of the funnel. In the case of Bloomberg, it's a little more nuanced than ads, since the company treats content as a loss leader to make people aware of the financial data it sells through its terminal. Matt Levine exists because Bloomberg can charge more than $400k per year to a terminal subscriber, which means they will gladly absorb a really high CAC. reply Rastonbury 11 hours agoprevOn the flip side there are several high quality youtubers whose content fits that bill, focused on their niche, regularly pumping out content (despite a lack of current events sometimes) while being entertaining. Thinking of people like 3blue1brown Maybe one reason for lack of newsletters is YouTube pays more and video is sometimes a better format, Levine has a bloomberg salary too, not many people are going to employ you to be writing about roman philosophy full time reply ghaff 6 hours agoparentThose are some of the key points\": focus, regular content, entertaining (and probably at least a little provocative). Can't really speak to the money-making part. reply Suppafly 2 hours agoparentprev>not many people are going to employ you to be writing about roman philosophy full time Yeah sometimes the best content is from people who are in well paid careers that are just researching a topic to scratch their own itch and then sharing it. I like the History of English Language podcast, and the guy who does it is a lawyer or something in his real life. reply JumpCrisscross 19 hours agoprevHe's also ridiculously accessible. E-mail him with a question and he responds. For a newsletterist with hundreds of thousands of readers, that's exceptional. reply gkanai 21 hours agoprevLevine being at Bloomberg (which is a central org in finance media) is important. That finance media can support a Levine and his salary is also important. reply ioblomov 21 hours agoprevTo Hossenfelder, I'd add Ben Thompson on \"the business, strategy, and impact of technology\" and the Financial Samurai on personal finance... https://stratechery.com/2024/elon-dreams-and-bitter-lessons/ https://www.financialsamurai.com/minimum-investment-threshol... reply the_gastropod 20 hours agoparentMan. I couldn't really disagree more re: Financial Samurai. That dude strikes me as a complete troll. His older articles aren't too bad, but the past several years, he's pivoted to a Get Rich or Die Tryin' mentality, and publishes endlessly repetitive takes. I think he's more an SEO expert than a financial one, at this point. If you can't figure out how to live on ~$380k of passive income, you have no business giving the general public financial advice. reply ioblomov 19 hours agorootparentI hear you; I just think he's far more risk averse than most people. He sets a high, conservative bar for his readers and himself that, at the risk of being overkill, almost guarantees success. Given finance, like tech, is not a field you can easily return to after leaving, I think the conservatism is merited. reply Suppafly 2 hours agorootparentprev>If you can't figure out how to live on ~$380k of passive income, you have no business giving the general public financial advice. Reminds me of Mr Money Mustache, dude has some good takes, but a lot of his advice is predicated on having a huge nest egg. reply therealcamino 17 hours agoprevThere are so few because he's an incredibly talented writer, able to explain arcane financial concepts with insultingly basic analogies, and toss in hilarious asides on the absurdity of it all. The only other writer I can think of who elevates their topic in a similar way is Dan Neill, the automotive columnist for the Wall Street Journal, who won the Pulitzer Prize for criticism when he worked at the Los Angeles Times. reply rr808 16 hours agoprevJournalists barely exist any more. To be a good journalist you have to know the subject matter well. If journalists are paid so little you might as well work in the industry directly. With AI its only going to get worse. reply dogmayor 5 hours agoprevLevine is a uniquely good writer. And, as mentioned, his choice of subject matter (law + finance) is a major factor. Both are such broad fields that you can always find something interesting happening in one corner or another, and both are relevant to everyone no matter the field they're in personally. As someone who has a similar background to Levine, I'm a big fan and can say it's uncommon to find someone like him who can write effectively and with levity. reply moonscentedslug 5 hours agoprevImo it's because finance is unique. Many arcane sounding things in this field are over-engineered/over-complicated smoke and mirrors. Their underlying ideas are usually unbelievably simple and the implementations are often hilariously sloppy, but many people get paid a lot of money to maintain the illusion of sophistication. This drastic contrast makes it a comedy gold mine for someone who can see these things as they are. It also makes it possible to explain those smart-sounding things in almost jokingly dumb languages while still being pretty accurate. This is much harder to do for a more rigorously structured field where the smoke and mirrors don't take the center stage. reply insane_dreamer 19 hours agoprevAnother factor is that there is a very large number of people who don't work in finance who are interested in money and specifically the ability to make money, even if they don't have any money. reply vinner_roy 3 hours agoprev- Lenny Rachitsky: startup product and growth - Alex Xu: software/software system design reply personjerry 18 hours agoprevHmm it's kinda crazy how well his main points [on why Matt Levine educates so well] map to ML concepts: 1. cases with known outcomes/answers => data with labels 2. first-principles explanations: most people experience an “illusion of depth” => avoiding local minima 3. spaced repetition enabled by fast turnover => stochastic optimization reply runnr_az 8 hours agoprevA lot of that energy / expertise went to YouTube. Like Practical Engineering and the like… reply analog31 18 hours agoprevPerhaps Rick Beato on the music industry and the inner workings of popular music. reply giraffe_lady 18 hours agoparentFor classic rock maybe but that's a pretty small segment of music these days. He's got some insight into pop & country but almost none into hip hop and I don't think he's an expert on modern studio work in any genre. He's a little opaque about this but I don't get the sense he's played on or produced anything in the last decade. Things have changed fast. reply edm0nd 18 hours agoprevVX Underground for the \"Levine\" of the cybersecurity space also a big fan of: - marcus hutchins - geohotz reply slt2021 17 hours agoparentCrebs on security - I think he is the Matt Levine of the cyber security. Joel Spolsky's blog (Joel on Software) was also very pleasant read on software back in the day reply saagarjha 10 hours agoparentprevWhile entertaining they aren't really Levines. I don't think any of them give you beyond a superficial understanding of the topics they work on (and, arguably, sometimes they provide inaccurate or exaggerated views). reply edm0nd 5 hours agorootparent>I don't think With a comment like that, I can tell. reply fragmede 8 hours agorootparentprevAnd \"everything is securities fraud\" isn't? It's hilarious, but definitely exaggerated. Matt Levine is interesting while also poking fun, which is practically the definition of entertaining. reply czhu12 10 hours agoprevFor me Bret dereveaux is the Matt Levine of classical, Hellenistic, roman, and medieval history. https://acoup.blog (Never miss a chance to promote him, when I can!) reply DavidPiper 8 hours agoprevI would submit that Austin Wintory is the Matt Levine of film and video game music and composition - youtube.com/@awintory reply nitwit005 19 hours agoprevIt is relatively easy to make money if you create popular financial content. There are plenty of people interested in placing ads next to it. Occasionally you'll see people joke that they need to change their content from art to home loans to cash in. reply JumpCrisscross 19 hours agoparent> plenty of people interested in placing ads next to it Bloomberg is a subscription. It has ads, but they don't complain about ad blocking. Turned it off out of curiosity: these look generic. reply nitwit005 18 hours agorootparentYes, but most subjects aren't going to be able to part of Bloomberg. reply JumpCrisscross 18 hours agorootparent> most subjects aren't going to be able to part of Bloomberg Subjects? reply nitwit005 12 hours agorootparentThe subject of your writing. reply JumpCrisscross 11 hours agorootparent> subject of your writing Which your? Levine's? Why does it matter if the subject of an article is on Bloomberg? (If you're saying Bloomberg only concerns itself with financial markets, you're wrong.) reply majikaja 21 hours agoprevI barely read his articles these days. Most of it is irrelevant to my work/obvious to professionals. A lot of it is just summarizing other articles in his own words. Deep insights in finance are typically not useful once everyone knows about them. reply 47282847 21 hours agoparentI am not in finance and I’m not interested in investing because I’m rational and simply use world ETFs and don’t see why I would ever want to do anything else. I really enjoy Levine because I learn a lot from it about how the world works. I do not read anything else from finance so really appreciate his summaries. reply NoboruWataya 21 hours agoparentprevWere you actually using Levine's insights in your work previously (like, trading on them)? That is interesting if so. Some of Levine's stuff is relevant to my work and most of it is not, but I never really considered it particularly useful, just entertaining. I suppose it can be useful for its educational value as the article explains. I do also read his stuff less nowadays. I still enjoy it, but it's not as high a priority for me as it used to be. I think it's the repetition. After 100 \"everything is securities fraud\" stories, number 101 is less entertaining. Still a big fan though. reply majikaja 21 hours agorootparentI never found it useful for trading (he's never been involved in that as far as I know) but I found it useful for gaining general knowledge when I was new to finance. reply julianeon 20 hours agoparentprev> Deep insights in finance are typically not useful once everyone knows about them. By that logic, it is right and good for the American public to be financially illiterate. reply paulpauper 20 hours agoparentprevIt's more about explaining why something happened, not so useful at predicting reply hrpnk 20 hours agoprevMarc Rubinstein also has great educational articles on history and inner workings of the finance industry: https://www.netinterest.co/ reply dzonga 7 hours agoparentBut this isn't free, though compared to Matt Levine's newsletter. reply hoseja 7 hours agoprev\"Why would anyone want to? Well, if you ask someone like Richard Feynman or Andrej Karpathy or Tom Lehrer why they pursued pedagogy instead of the professional pursuits that brought them fame & wealth, the answer would have to be...\" In actuality, probably nubile coeds. reply paulpauper 20 hours agoprevWhy don’t we have a Matt Levine for every industry? Where is the Levine of, I don’t know, petroleum refining or fracking, of shipping containers? this would be a consultant. the market for 'shipping container experts' is tiny . not enough to support a columnist reply ghaff 6 hours agoparentYes. You can be sure that there are deep experts/consultants/analysts on just about any niche that you care to choose. But you're not going to get a lot of subscribers to your newsletter on \"this week in plumbing supplies.\" reply jmyeet 18 hours agoprevTwo stories spring to mind. The first is the propensity for journalists to be in lockstep with US foreign policy. This occasionally gets brought up to them and they generally act offended and might respond with \"my views haven't been bought\" to which the retort is \"you wouldn't have this job if you didn't have these views\". The point is that there are filters at every level to produce people who are complicit with the status quo, not just in journalism. The second is an old Noam Chomsky video talking about Newt Gingrich railing against \"welfare queens\" when in fact his congressional district (at the time) received something like the third-most (after the Kennedy Space Center and another I forget) most from Congress, which is really the definition of welfare. But we don't often talk about \"corporate welfare\". But the point is the Democrats at the time never challenged him on this. Why? Class solidarity. The political elite (regardless of party) are more interested in perpetuating the system than anything else because they directly benefit from it. A good example of this is so-called \"floor privileges\". If you've been a Congressperson and leave, almost inevitably to become a lobbyist, you retain, by convention, \"floor privileges\". That is, you have the right to enter Congress and the chamber of the House of Representatives even though you're a private citizen (worse, a lobbyist). No Congresspeople wants to upset that apple cart because that's their future path to wealth: selling access. So people like Matt Levine are an outlier basically by design. reply JadeNB 16 hours agoprevLike probably many people here, I enjoy Money Matters, and so enjoy any sensible discussion of it, especially if it might lead to more Money Matters-like content in my life—but an elaborate explanation after the fact of why a circumstance that has so far only happened once, could only have happened the way it did, starts to sound to me a bit like over-fitting theory to limited observation. As others have observed, for example, if Derek Lowe can be discounted, then surely other exceptions can also be discounted, so that the thesis becomes true by defining counterexamples out of existence. reply renewiltord 16 hours agoprevSingle individuals are often just single individuals. We might never have had open source without the few people who championed copyleft. No other industry but us does. It also means you can make a change in the world. reply richardw 20 hours agoprevI’ve seen his stuff referenced many times but it’s paywalled, right? Is there an email sub without the paywall? reply jcdavis 20 hours agoparentYes: https://www.bloomberg.com/account/newsletters/money-stuff?so... reply richardw 19 hours agorootparentThank you! reply ForHackernews 20 hours agoprevWhat a silly question. Being an excellent writer is hard! Why is there only one Joan Didion? reply hn_throwaway_99 19 hours agoparentI agree. While there was a depth to the analysis in this post that I somewhat enjoyed (though at times it felt a bit like a \"why the joke is funny\" explanation...) to me the primary reason that I like Matt Levine is just that I find his writing to be amazingly great. And not just his overall content and analysis (which I'm still often awed by), but I find his knack for putting words together in uniquely insightful, clear and often hilarious ways to be uniquely brilliant. Some people are just really, really good. reply giraffe_lady 18 hours agorootparentThere's also just... how many words has he written? He's been doing a multiple-times-a-week newsletter on the same subject for like two decades. The man is honed, dialed in to his craft. Even among the extremely extremely small group of professional writers who make their living from their writing, he must be an outlier on volume. reply petesergeant 21 hours agoprev> Particularly in economic matters, people believe many intuitive folk economics like eg. building new houses cannot lower prices, or that businesses raise prices simply “when they feel greedy”, or that voluntary transactions must have a loser & a winner when other people transact (but not themselves personally), or that a policy will have only intended effects because everyone will just do what they are ordered to (even though they personally work around policies or rules all the time for doubtless noble reasons). Pretty skeptical of this assertion in an otherwise great read reply modeless 21 hours agoparentSeems obviously true to me. Spend time on Reddit r/all and you'll see these beliefs repeated over and over. reply JumpCrisscross 19 hours agorootparentHell, supply and demand are regularly called out as fake on HN, where you'd expect people to know better. reply davidclark 13 hours agoparentprevSkeptical in what way - that people believe these things or that these things are folk economics? reply rsynnott 7 hours agoparentprevPlenty of people definitely believe all of those things. reply kbelder 21 hours agoparentprevYou're skeptical that many people believe those things? reply readthenotes1 21 hours agoprevI wonder if more people know about Mr Money Mustache than Matt Levine? reply tyleo 21 hours agoparentThat’s a great question. I was really hooked on Mr. Money Mustache for awhile but his content got old. Levine is harder for me to read. He feels more complex feels more relevant. reply ghaff 20 hours agorootparentThe lean FIRE stuff really seems like schtick to me pretty quickly. Yeah, some of it is worth reading and thinking about, but a lot is pretty alien to my personal objectives. I'm not going to panic about going out to dinner or taking, what he would probably consider in print, a moderately comfortable vacation, if not an extravagant one. reply vundercind 18 hours agorootparentI dove in for a week or so way back when, but once I read enough posts to realize the actual pitch was “retire early except for the three jobs you have, one of which is blogging about ‘being retired’, and plan to go back to a decent-paying job with benefits if anyone in your family gets seriously sick, LOL, hope you’re not too old when it happens and then have trouble getting any job with benefits” a lot of the magic was gone. reply nl 18 hours agoparentprevMr Money Mustasche, slightly: https://trends.google.com/trends/explore?date=today%205-y&q=... Makes sense - Mr Money Mustasche is about personal finance, whereas Matt Levin is commentary on the financial industry. reply doctorpangloss 17 hours agoprevnext [2 more] [flagged] Sebguer 13 hours agoparentI assume you're being downvoted because it is absolutely impossible to understand what the point you're trying to make is. reply vlovich123 21 hours agoprev [–] > Matt Levines of, say, chemistry or drug development Neil deGrasse Tyson, Sabine Hossenfelder, Brian Greene and many others cover astrophysics, quantum mechanics, particle physics, and other science topics. I think the challenge is that the sciences are hard and developments there are fairly slow as compared with financial stuff which moves a bit more quickly. reply NoboruWataya 21 hours agoparentI suspect the sciences also aren't as \"juicy\" for want of a better word. Finance is a very human field and there is huge scope for interpersonal conflict and drama, which people love to read about. reply vlovich123 18 hours agorootparentIf you think there isn't interpersonal conflict & drama, you're not listening to the arguments between the proponents of string theory, quantum gravity, and people like Roger Penrose who are somewhat skeptical of both. More likely that because finance involves money, and huge amounts of it, and a lot of people are interested about things that involve huge amounts of money, especially if there's drama. reply nsbshssh 18 hours agorootparentThat is the same drama on loop. It gets boring from a news point of view. Finance finds new suckers and schemers every day, so is a drama mill. reply vlovich123 12 hours agorootparentYou just restated what I originally said: > I think the challenge is that the sciences are hard and developments there are fairly slow as compared with financial stuff which moves a bit more quickly. reply ozzydave 20 hours agoparentprevI’d say Matt O’Dowd of PBS Space Time is the closest to Levine’s role? reply bmitc 21 hours agoparentprev [–] > Neil deGrasse Tyson, Sabine Hossenfelder, Brian Greene None of those three \"cover\" any topics. They push their own topics and agenda, which can often be fringe or counterculture and highly personal pet preferences or topics. Tyson, in particular, just likes to hear himself talk above all else, took 11 years to get his PhD, was kicked out of the UT Austin PhD program, is often wrong, and is a living breathing example of \"well, actually\". These three are about as far from journalism as you can get and are actively hurtful, at least in the case of Greene and Tyson due to their popularity, to science. reply The_Colonel 10 hours agorootparent> Sabine Hossenfelder Check her YT channel - full of clickbait content outside of her expertise. Quite sad actually. https://www.youtube.com/user/peppermint78 Sean Carroll would be a better example, although his reach is limited. reply bmitc 6 hours agorootparentPhysicists are in general the worst science \"journalists\" as they often think they know everything outside of their domain and confidently comment on such. Meanwhile, physics is actually the easiest science in a sense due to its ability to greatly rely on a broad set of simplifying assumptions in both theory and experiment, making them the least qualified for more complex science and systems, generally speaking. reply A4ET8a8uTh0 20 hours agorootparentprev [–] I don't want to pile on here, because Neil did do some good in popularizing science. He did not really manage to make it cool, but he did become a popular mainstream figure that the public was now allowed to associate with a mental image of a science nerd ( a big step given that previous media iterations were invariably a caricature straight from revenge of the nerds ). The problem.. as it often tends to be in instances such as this.. is that he did become a faux celebrity with all the benefits and drawbacks it brings. Unlike another celebrity in that realm however ( Hawking ), he did not manage to get the same level of recognition. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Matt Levine's \"Money Stuff\" newsletter stands out in finance due to its blend of humor and clear explanations of complex topics, using quickly resolved real-world examples.",
      "Replicating Levine's success in other fields is difficult because most areas lack rapid, clear-cut examples and individuals with the necessary expertise, obsession, and persistence.",
      "The scarcity of educational newsletter writers like Levine is attributed to the nature of most fields and the rarity of individuals who prefer writing over working directly in their field."
    ],
    "commentSummary": [
      "The discussion highlights the rarity of writers like Matt Levine, who provide insightful and humorous commentary on finance, making complex topics accessible.",
      "It is noted that similar writers exist in other fields, such as Derek Lowe in chemistry and Bret Devereaux in history, but they often leave their original professions to focus on writing due to time and financial constraints.",
      "Levine's appeal is partly due to the potential for readers to gain financial insights, a benefit less common in other fields, combined with his engaging writing style."
    ],
    "points": 236,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1730147862
  },
  {
    "id": 41978197,
    "title": "What's New in POSIX 2024",
    "originLink": "https://blog.toast.cafe/posix2024-xcu",
    "originBody": "Table of Contents Highlights Handling of Filenames in Shell Modern C Limits & Cooperation Makefiles Logging Internationalization Minor Changes Changes Index In the 1950s, computers did not really interoperate. ARPANET has not yet happened (that would become a thing in the 60s), and every operating system was typically tied to the hardware that was meant to run on. Most communication actually happened over telephone, and no company was more present in that space than the Bell System. Unfortunately, the way they were so present was through exclusive supply contracts (with its subsidiary Western Electric) and a vast array of patents that it would refuse to license to competitors. So they got an antitrust suit aimed at them, which after seven years of litigation culminated in the 1956 consent decree. The Bell System was broken up, obliged to license all of its patents royalty-free, and barred from entering any industry other than telecommunications. So they made Unix. Unix was unique, because the focus was on the software (since Bell couldn’t compete in this space anyway, as per the above). An evolution of Multics, it was developed on a PDP-7 (by cross-compiling). They then ported a compiler-compiler to it, leading to the development of B. Once their internal needs outgrew the PDP-7, it got ported to the PDP-11, and gained full typesetting capabilities. Gaining some traction internally, when Bell acquired other PDP-11s, instead of running DEC’s own OS for the machine, they simply ran Unix on it. This has led to the rewrite of the OS in C, a higher level (comparatively, of course) language, which enabled the porting of it to other machines (like the Interdata 7/32 and 8/32). Interest grew, and Bell (not being allowed to turn Unix into a product) simply shipped it at manufacturing cost for the media. Notably, ARPANET used it (see: RFC 681). In the early 1980s, Unix had become a univeral operating system, used on virtually every serious machine. Then, AT&T got hit by an antitrust suit again. The exact details matter less, but freed it from the old restriction. System V immediately turned into a product, almost killing it. That very year, the GNU project was created, and the BSD project was started in Berkeley. Having grown accustomed to interoperability (since up until that point, there was only really one serious Unix), several standardization attempts were created. The System V Interface Definition was the AT&T one, Europe created the X/Open consortium of Single UNIX Specification fame, and the IEEE put out POSIX. These latter two would eventually merge and become equivalent, developed by the Austin Group, defining the only interface said to be universally interoperable on the OS level that we have to this day. As of the previous release of POSIX, the Austin Group gained more control over the specification, having it be more working group oriented, and they got to work making the POSIX specification more modern. POSIX 2024 is the first release that bears the fruits of this labor, and as such, the changes made to it are particularly interesting, as they will define the direction of the specification going forwards. This is what this article is about! Well, mostly. POSIX is composed of a couple of sections. Notably XBD (Base Definitions, which talk about things like what a file is, how regular expressions work, etc), XSH (System Interfaces, the C API that defines POSIX’s internals), and XCU (which defines the shell command language, and the standard utilities available for the system). There’s also XRAT, which explains the rationale of the authors, but it’s less relevant for our purposes today. XBD and XRAT are both interesting as context for XSH and XCU, but those are the real meat of the specification. This article will focus on the XCU section, in particular the utilities part of that section. If you’re more interested in the XSH section, there’s an excellent summary page by sortix’s Jonas Termansen that you can read here. Highlights # Handling of Filenames in Shell # One of the most common errors in shell scripts when working with files tends to be the presumption that the newline character () will not be present in the filename. Consider, for example, wanting to do some processing of files in a directory, processing the most recently modified ones first, with some custom break condition. The most common (naive) way of implementing this looks like this1: 1ls -twhile read -r f; do 2 # if my condition; then break; fi 3 # do something with $f 4done After all, read(1p) reads logical lines from stdin into a variable, and ls(1p) outputs one entry per line. The problem is that pathnames2 (as per section 3.254 of POSIX 2024) are just strings (meaning they can contain any bytes except the NUL character), meaning it’s incorrect to even treat it as a character string, let alone something you can put in a newline-separated form. As such, the correct solution, historically, has been to loop over the files in some other way (such as wildcards, which aren’t subject to expansion, or using find(1p)), then sort them, then run on the sorted datatype. This question is probably one of the most talked about in shell. POSIX 2024 addresses this issue in two ways. The Null Option # find(1p) now supports the -print0 primary, which makes find use the NUL character as a separator. To go along with it, xargs(1p) now supports the -0 argument, which reads arguments expecting them to be separated with NUL characters. Finally, for (most) other usecases, read(1p) now supports the -d (delimiter) argument, where -d '' means the NUL character is the delimiter. This is a non-ideal resolution though. Previous POSIX releases have considered -print0 before, but never ended up adopting it because using a null terminator meant that any utility that would need to process that output would need to have a new option to parse that type of output. More precisely, this approach does not resolve our original problem. xargs(1p) can’t sort, and therefore we still have to handle that logic separately, unless sort(1p) also grows this support, even after read(1p). This problem continues with every other type of use-case. Importantly, it breaks the interoperability that POSIX was made to uphold. Thankfully, there is the second way that they’re fixing this issue. The Nuclear Option # We’ve established that, yes, pathnames can include newlines. We have not established why they can do that. After some deliberation, the Austin Group could not find a single use-case for newlines in pathnames besides breaking naive scripts. Wouldn’t it be nice if the naive scripts were just correct now? Ok, that might be a bit much all at once. We’re heading there though! A bunch of C functions3 are now encouraged to report EILSEQ if the last component of a pathname to a file they are to create contains a newline (put differently, they’re to error out instead of creating a filename that contains a newline). As for the utilities, the following utilities are now either encouraged to error out if they are to create a filename that contains a newline, and/or encouraged to error out if they are about to print a pathname that contains a newline in a context where newlines may be used as a separator: admin(1p), ar(1p), awk(1p), basename(1p), cd(1p), cksum(1p), cmp(1p), command(1p), compress(1p), cp(1p), csplit(1p), ctags(1p), cxref(1p), dd(1p), df(1p), diff(1p), dirname(1p), du(1p), ed(1p), ex(1p), file(1p), find(1p), fuser(1p), get(1p), grep(1p), hash(1p), head(1p), ipcs(1p), link(1p), ln(1p), localedef(1p), ls(1p), m4(1p), mailx(1p), make(1p), man(1p), mkdir(1p), mkfifo(1p), mv(1p),nm(1p), patch(1p), pax(1p), prs(1p), pws(1p), rm(1p), rmdel(1p), sact(1p), sccs(1p), sh(1p), sort(1p), split(1p), tee(1p), touch(1p), type(1p), uncompress(1p), unget(1p), uniq(1p),uucp(1p), uudecode(1p), val(1p), vi(1p), wc(1p), what(1p), yacc(1p), and zcat(1p). Furthermore, sh(1p) talks about future direction, which may require the above to be treated as errors, and pr(1p) has a new section talking about “problematic pathnames” (since, for its use-case, tabs and vertical tabs are also problem-causing). This is a much better solution, even in its current form. Unless your threat model includes attackers targeting you in particular (which, for example, immediately excludes all “home use” scripts), you can reasonably expect people to be discouraged from creating newline-containing characters, where before it might have been perceived as a “clever hack”. You can’t rely on your system enforcing those files not exist, but this is a major step in that direction. TL;DR # While code like ls -twhile read -r f isn’t strictly correct yet, it’s likely to become strictly correct eventually. It’s also much more reasonable to opt into this early, unless you’re writing software with security requirements, are deleting files based on inputs, or similar. Modern C # C has come a pretty long way in the last half-century, but for most intents and purposes, we haven’t been able to really benefit from it. Did you know that since c11 we actually have built-in unicode support via(ISO/IEC TR 19769:2004)? Most modern programs can’t actually utilize this, because they target c89 (often incorrectly called “ANSI C”) or (if you’re lucky) c99. Why does this happen? Well, when you’re building a new C program, you must decide what version of C to target. Target something too new, and no one will be able to build it. An example of this is btop++, which targetted some newer C++ features (notably ) that at the time of its publishing LLVM simply did not support: libc++ simply didn’t have them yet (at least not in a stable format available on most distributions), and you couldn’t use gcc’s libstdc++ because itsimplementation depended on concepts (which LLVM also did not have yet). As such, what you do, is you look at the platforms you want your program to run on, and try to figure out what the least common denominator would be. It just so happens that for the longest time, that denominator would be c89. For a little while now, it’s been c99. As for why that is, POSIX is a large part. You see, up until POSIX 2024, POSIX required that the c89 compiler be present on the system. If you have c89 you’re compliant, and if you do not, you are not. Most operating systems try to be POSIX compliant, and so it becomes a typical expectation (so you don’t have to worry about not having C at all, something other languages do have to worry about). This broad presumption of availability also pushes the embedded developers to provide something along those lines as well (setting the expectation of expectations), so most microcontrollers will have a c89 (or again, recently, c99) toolchain available for them. In short, application authors will tend not to target something until it’s fairly common, unless there’s a disproportionate advantage for their specific use-case (such as with c99 over c89). What’s fairly common is strongly influenced by what is pseudo-guaranteed by the only portable standard we have. Anyway, POSIX 2024 now requires c17, and does not require c89. Furthermore, the rationale mentions that future editions will not require c17, but will simply require whatever C specification version is the most modern and already implemented by major toolchains. So going forward, it’ll be much easier to justify using actually modern C for your new projects, and we can expect more and more embedded tools to provide modern C versions (something we’re already seeing, especially on microcontrollers that are based on ARM or RISC-V). Limits & Cooperation # Operating systems impose limits (often arbitrary) on what runs inside of them, and your applications (and scripts, and interactive usage) may also want to impose some limits and cooperation on what you run. As such, it’s important that you be able to interact with these limits. This is what the nice(1p), renice(1p), and ulimit(1p) utilities are meant to do. Unfortunately, renice(1p) only worked in absolutes, and ulimit(1p) only let you set a maximum write size for files (and didn’t differentiate between hard and soft limits), and was only available as part of the XSI extension. With POSIX 2024, ulimit(1p) now supports reporting hard and soft limits and defines how those are used and interact. Additionally, the core image size, data segment size, open file descriptor amount, stack size, cpu time4, and address space limits now exist. This means that you can now (or rather, in the near future) reasonably rely on those existing and actually make use of them in portable scripts. renice(1p) is also updated to support the -n option (just like nice(1p)) to change the niceness value relatively. Finally, we get a new utility: timeout(1p). A lot of tools over the years have added options to handle their own timeouts (curl(1) in particular comes to mind, having several different types of timeouts for various use-cases), but with timeout(1p) you don’t need those (except for the added flexibility) anymore. It even handles child processes (in several implementation defined ways) and (importantly) lets you customize the signal and send a secondary SIGKILL after a secondary timeout. Makefiles # make(1p) remains the default build system to this day. Or at least sort of. Most people tend to write large scripts that wrap around make for various reasons, but in the end they will tend to produce a Makefile (though ninja has been gaining a lot of traction). Let’s take a look at a typical example to explain what the improvements are. Our use-case is simple: we have a bunch of .c files in ./. We want to compile and link them together. We also have a dependency (let’s say it’s libcurl) that requires some additional CFLAGS and LDFLAGS, which we query using pkg-config (well, I’m going to use pkgconf, it’s compatible). Importantly, we’re lazy in that we don’t want to specify every .c file in the directory in our Makefile. We also want to be able to clean our .o files without resorting to something like git clean -fx (that might clean some temporary artifacts that we do want to keep). With GNU Make, that might look something like so: 1SRC := $(wildcard *.c) 2OBJ := $(SRC:.c=.o) 3 4CC ?= cc 5 6CFLAGS ?= -Os -pipe 7LDFLAGS ?= -Wl,-O2 8LIBS ?= 9 10PKGCONF ?= pkgconf 11 12CURLC != $(PKGCONF) --cflags libcurl 13CURLL != $(PKGCONF) --libs libcurl 14 15CFLAGS := $(CFLAGS) $(CURLC) 16LDFLAGS := $(LDFLAGS) $(CURLL) 17 18myprog: $(OBJ) 19 $(CC) -o $@ $(LDFLAGS) $(LIBS) $^ 20 21.PHONY: clean 22clean: 23 rm -f $(OBJ) myprog This will not work on anything but GNU Make. MacOS make5 won’t be happy with the != used for CURLC, while bsdmake and bmake won’t be happy with $^. POSIX make would be unhappy with the :=, wildcard and .PHONY. Similarly, if we targetted bmake initially, the result would not properly run on gmake, and so on. The various implementations are mutually incompatible in diverging ways, since the POSIX implementation lacked critical features required for writing such small (and the vast majority of Makefiles should be this small) Makefiles. While there’s still no good solution for the $(wildcard *.c) portion of this6, the following, annotated with comments for changes, should now work in strict POSIX compatibility7: 1# .POSIX: is meant to make the Make implementation behave as though 2# it is standard POSIX-make, since there may be conflicts 3.POSIX: 4# we use ::= here, since POSIX does not define :=. 5# we also strictly enumerate the sources 6SRC ::= one.c two.c 7OBJ ::= $(SRC:.c=.o) 8 9CC ?= cc 10 11CFLAGS ?= -Os -pipe 12LDFLAGS ?= -Wl,-O2 13LIBS ?= 14 15PKGCONF ?= pkgconf 16 17CURLC != $(PKGCONF) --cflags libcurl 18CURLL != $(PKGCONF) --libs libcurl 19 20# ditto re: ::= 21CFLAGS ::= $(CFLAGS) $(CURLC) 22LDFLAGS ::= $(LDFLAGS) $(CURLL) 23 24myprog: $(OBJ) 25 $(CC) -o $@ $(LDFLAGS) $(LIBS) $^ 26 27.PHONY: clean 28clean: 29 rm -f $(OBJ) myprog That’s very few changes! Importantly, gmake can already handle this, meaning that by targeting this feature set you are strictly improving compatibility. To be very specific, POSIX 2024 added support for the $^ and $+ internal macros, ::=, :::=, !=, ?=, and += macro assignment forms, silent includes via -include, .NOTPARALLEL, .PHONY, and .WAIT special targets (of which I did not cover the parallelism ones, as those will typically be mostly useful to meta build systems), and other less important changes that will be listed out in full below. Logging # Our computers have more and more cores. In early 2017 (when the previous version of the standard was being finalized), most consumer grade hardware still maxed out at 4 cores (likely with SMT). This was also the segment of the market most likely to have background batch processing done in shell (as more enterprise-grade uses tend to write in a programming language that can integrate with their numerous external APIs). As such, while background processes were certainly common, it wasn’t as much of a common expectation that one might be doing some major processing (e.g. video re-encoding) in the background while performing other tasks. Of course, right after that point, in March 2017, the first generation of AMD Ryzen CPUs dropped on the scene and put processors with as many as 16 threads into the hands of consumers at more than reasonable prices. Today, in 2024, it’s difficult to buy a new workstation cpu with fewer than 12 threads, making the abovementioned scenarios all the more common. The original specification of logger(1p) was written with a fairly uncommon, albeit necessary, use-case in mind. Today, such use-cases are much more common, and could be even more common if logging was easier to do correctly8. This original specification basically said that logger(1p) takes arguments like echo, but instead of outputting the text into stdout, it does so into syslog. It also means that logging the output of commands is unduly complicated. In POSIX 2024, logger(1p) becomes a more fully-qualified command, with arguments and stdin interpretation. Notably, if there are no non-option arguments, logger(1p) will read the contents to log from stdin. It is also possible to ask the contents of a specific file to be logged using -f. Additionally, the syslog priority can be specified with -p, the pid of the logger process on each message using -i, and a syslog tag string using -t. Of additional importance, every non-empty line in the input or file shall be logged as a separate message, which means that the -i argument can be used to perform bulk logging where you can differentiate between failed runs. Internationalization # Different people speak different languages, and it’s important to be able to translate your program for those. While you’re writing a C program or something along those lines, you can always reach for a library that you link into (such as GNU intl). While you’re writing a shell script, however, your options tend to be far more limited, since you can’t distribute it alongside the script very easily. Wouldn’t it be helpful if the standard everyone follows to various degrees actually settled on whatever interface was the most used in practice? Anyway, POSIX 2024 has adopted the gettext suite ala GNU, both as a system interface (gettext(3p) and co) and in the CLI (gettext(1p), ngettext(1p), xgettext(1p), msgmft(1p)). Since the target audience for this article is primarily shell people and advanced end-users, I’ll quickly go over the utilization in a shell context. If you’re already familiar with the basics of GNU’s implementation, you can skip the rest of this section! Translations are organized by message IDs (msgid) which can then be turned into arbitrary message strings (msgstr). These are encoded in a Portable Object file (.po), which you compile into a Machine Object file (.mo) using msgfmt(1p). You then place them on your system in such a way that the gettext(1p) utilities will be able to find them, and the typical LANG/LANGUAGE/LC_ALL/LC_MESSAGES mechanism will get you the correct translations. For the purposes of this minimal example, we’re going to write a very small program that talks about pets. The program will either print you like cats, you like dogs, or you have %d pets (where the %d will be used for printf output). We’ll also demonstrate how special-case plural forms work. We’ll be making a French and an English translation, and everything will be done relative to a directory of your choosing, that I will refer to as $PWD or . interchangeably. We’ll start by writing our two annotated .po files. # ./en/pets.po : the filename and location is arbitrary # empty messageid and str signals the header # different languages deal with plural forms differently # English only has a special case for \"one\" # the `plural=` section is a C-like conditional expression msgid \"\" msgstr \"\" \"Content-Type: text/plain; charset=utf-8\" \"Plural-Forms: nplurals=2; plural=n != 1;\" \"Language: en\" # the IDs here are identical the messages # since English is the source language for us msgid \"you like cats\" msgstr \"you like cats\" msgid \"you like dogs\" msgstr \"you like dogs\" # note that if a translation isn't found # the msgid is used as is msgid \"you have a pet\" msgid_plural \"you have %d pets\" msgstr[0] \"you have a pet\" msgstr[1] \"you have %d pets\" # ./fr/pets.po # we'll have a special case for 0 (aucun) and 1 (un) # then the rest will be general case plural msgid \"\" msgstr \"\" \"Content-Type: text/plain; charset=utf-8\" \"Plural-Forms: nplurals=3; plural=(n==0)?0: (n==1)?1: 2\" msgid \"you like cats\" msgstr \"vous aimez les chats\" msgid \"you like dogs\" msgstr \"vous aimez les chiens\" # I translated \"pet\" as \"little companion\" # as there's no satisfactory direct translation msgid \"you have a pet\" msgid_plural \"you have %d pets\" msgstr[0] \"vous n'avez pas de petits compagnons\" msgstr[1] \"vous avez un petit compagnon\" msgstr[2] \"vous avez %d petits compagnons\" These files aren’t usable as-is. We need to compile them into .mo files. We’ll start by compiling them in the same directory: msgfmt en/pets.po -o en/pets.mo; msgfmt fr/pets.po -o fr/pets.mo. We now need to place them in a location that gettext(1p) and co. will be able to find it in. For those specific commands there are numerous special cases, and we’ll take advantage of those via TEXTDOMAINDIR. Under $TEXTDOMAINDIR, the system will try to look for your $LC_MESSAGES locale followed by LC_MESSAGES, then your textdomain. For convenience, we’ll make some symlinks: ln -s . en/LC_MESSAGES; ln -s . fr/LC_MESSAGES. We can now demonstrate the messages manually! 1export TEXTDOMAINDIR=$PWD 2 3# the system will access $TEXTDOMAINDIR/${locales…}/$TEXTDOMAIN.mo 4# you can avoid setting $TEXTDOMAIN if you specify it on the CLI 5export TEXTDOMAIN=pets 6 7# we can now translate simple messages! 8LC_MESSAGES=fr gettext -s 'you like cats' 9# => \"vous aimez les chats\" 10LC_MESSAGES=en gettext -s 'you like dogs' 11# => \"you like dogs\" 12# if you try to access a translation that doesn't exist, 13# it will simply print the ID, thus why it needs to be representative 14LC_MESSAGES=it gettext -s 'you like cats' 15# => \"you like cats\" 16 17# for plural forms, you use ngettext(1p) 18# because we probably also want to show the real number, 19# ngettext can output printf-compatible format strings 20# so we'll write a wrapper 21# $1: locale; $2: msgid; $3: msgid_plural; $4: quantity 22plural() { 23 printf \"$(LC_MESSAGES=\"$1\" ngettext \"$2\" \"$3\" \"$4\")\" \"$4\" 24} 25# we can now demonstrate how the translation system adapts to plurals 26for i in $(seq 0 2); do 27 plural en 'you have a pet' 'you have %d pets' $i 28done 29# => you have 0 pets 30# => you have a pet 31# => you have 2 pets 32 33# in French, we had a special case for 0, let's see it in action: 34for i in $(seq 0 2); do 35 plural fr 'you have a pet' 'you have %d pets' $i 36done 37# => vous n'avez pas de petits compagnons 38# => vous avez un petit compagnon 39# => vous avez 2 petits compagnons 40 41# if you try to access a translation that doesn't exist, 42# the system will follow typical English rules, as above: 43for i in $(seq 0 2); do 44 plural it 'you have a pet' 'you have %d pets' $i 45done 46# you have 0 pets 47# you have a pet 48# you have 2 pets In short, you can now rely on GNU-style gettext and ngettext utilities to be present, and write your script with the presumption that they are there. If the translation files are not installed, the message ID will be used (intelligently, in the case of plural forms), so you don’t need to worry about the possibility of them not being installed. Minor Changes # These are changes that are relatively minor, but I still think deserve a spotlight. readlink(1p) and realpath(1p) are now part of the standard, meaning you can reliably find where a symlink points to. rm(1p) takes a -d argument to remove empty directories too, enabling rm -d * and similar use-cases. You also get -v. printf(1p) now supports numbered conversion specs. For example, printf '%2$s%1$s' a b will print out ba. sed(1p) got several interesting upgrades, though for me the highlights are being able to use EREs like in grep(1p) using -E, as well as the i flag on the s command. test(1p) now has -ef, -nt, -ot. String comparisons (> and <) are now affected by collation. There is a new category of utility, notably intrinsic utilities. These are like a special built-in that can be overridden by a user function, or as a regular built-in that cannot be substituted on the PATH nor need to be possible to exec (except for kill). It’s an important change, but it’s not very relevant for anyone that’s not writing a shell interpreter. Changes Index # If you’re here early, hi! I’ve been working on this piece (I have a good chunk of an MB in plaintext notes) since the middle of the summer. Instead of letting it continue to drag on, I decided to radically reduce the scope just to the highlights, and only the XCU Utilities section. Thanks to sortix (linked above), I feel like I can stick with that latter, but I still plan to actually write out the full changes index here, as well as go over any of the Shell Command Language changes. It’s just going to take a long time still, since I’m not interested in simply dumping out the change notifications, but rather explain every change being made (albeit not as completely as I do in the highlights section). I will update this page in-place and post a second announcement when this section is complete. Don’t expect it any time soon though (probably not until early 2025). Ok yes, it actually looks more like for f in $(ls -t), but that’s bad for other reasons, and is a mistake under more circumstances than just this case. ↩︎ Note that this doesn’t apply to pathnames that are only composed of the portable filename character set (meaning that every level in the pathname is a portable filename). The portable filename character set notably does not include newlines (or spaces, for that matter). ↩︎ bind(3p), link(3p), linkat(3p), mkdir(3p), mkdirat(3p), mkdtemp(3p), mkfifo(3p), mkfifoat(3p), mknod(3p), mknodat(3p), open(3p) and openat(3p) (but only for new files via O_CREAT), rename(3p), renameat(3p), symlink(3p), and symlinkat(3p). ↩︎ In this context it’s talking about total execution time. Let’s say you set it to 10 cpu seconds and ran a CPU stress test. After it used 10 cpu seconds, the process would be killed due to exceeding the limit. ↩︎ That is to say, GNU Make from 2006. ↩︎ the closest you can get is a SRC != ls *.c, which can be problematic in many ways. In this example I just enumerated them, but you can still do the above, it’s just not safe due to whitespace. Note that many build systems (like meson) that compile down to Makefiles or ninja force you to enumerate your source files anyway. ↩︎ At the time of testing, none of them have done it yet (except maybe gmake, I can’t tell and I don’t want to read GNU sources), as Make implementations tend to move fairly slowly. In part due to how old it is, and in part due to the format being quite funny to implement. ↩︎ You can make wrappers around the old logger, and you can use your rc manager to log stdin/stdout. These are however much less ergonomic than dropping something that’s log-enabled into the background using ^z and bg. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=41978197",
    "commentBody": "What's New in POSIX 2024 (toast.cafe)203 points by signa11 18 hours agohidepastfavorite192 comments enriquto 11 hours ago> We’ve established that, yes, pathnames can include newlines. We have not established why they can do that. After some deliberation, the Austin Group could not find a single use-case for newlines in pathnames besides breaking naive scripts. Wouldn’t it be nice if the naive scripts were just correct now? Ok, that might be a bit much all at once. We’re heading there though! Oh my god. This makes me so happy. This is the most lovely think I've read in the world of computing since the unix gods decided that newlines were to be a single character. The philosophy underlying the sentence \"Wouldn’t it be nice if the naive scripts were just correct now?\" is incredibly positive. We are surrounded by arrogant jerks who break old code by aggressively enforcing stricter compliance of some stupid rules. But here come these posix heros who do the exact opposite: make old code correct! There is hope in mankind after all. reply nneonneo 9 hours agoparentRather unfortunately, I happen to have a handful of files on my machine with newlines in them (the filenames were programmatically generated from a summary of their contents). I loathe the possibility that my shell tools are going to suddenly crash when confronted with these weird files, rather than just producing some slightly silly output. I wish we'd standardized the behaviour of just escaping such characters as `/\\r` or `^J/^M`... reply jraph 7 hours agorootparentThey did the right thing for this: make the tools fail on file creation, but not on existing files. I guess it's still advisable to rename those files, I don't know how things like cp, mv or rsync will behave when copying such files in the future. reply nneonneo 4 hours agorootparentNo, they did not do the right thing: > the following utilities are now either encouraged to error out if they are to create a filename that contains a newline, and/or encouraged to error out if they are *about to print a pathname that contains a newline* in a context where newlines may be used as a separator It then proceeds to list a bunch of utilities including diff, file, find, grep, head, du, etc., none of which create files directly. These utilities could be updated to reject newlines in file paths if they're going to print in a \"newline delimited\" form - but for some of these utilities, that's the only available form. reply jraph 3 hours agorootparent> error out if they are about to print a pathname that contains a newline in a context where newlines may be used as a separator But that's already broken. This is a situation where filenames with newlines in them are indistinguishable from two filenames in outputs. So instead of producing subtly broken output, tools are encouraged (not forced) to explicitly fail with a lot of noise. The \"in a context where newlines may be used as a separator\" part of this sentence is very important. IIUC the tools are still allowed to succeed in non broken situations, for instance when a null separator is used and not a new line character. And I can't imagine the tools you listed will start breaking in situations that worked (apart from file creation - indeed this will likely start breaking, and new line characters in filename needs to be considered deprecated and things using them to be fixed). This is strictly better IMHO (if one thinks that newlines in files are not worth the troubles given how things work in POSIX, especially the part where things are line-based and new line characters have quite some significance) reply ykonstant 5 hours agorootparentprevIf your file system allows them, be careful with symlinks though! reply jraph 2 hours agorootparentWhy, specifically? I'm convinced we will need to be careful with symbolic links related to new line characters in filenames, but I'm curious of which specific aspect you had in mind. reply ykonstant 8 hours agorootparentprevIn academia, I get (and used to create) pdfs with names like: \"On the number of associative foobars of degree blah - Johnson and Anderson.pdf\" all the time. It is very convenient for non-technical academics to have a descriptive file name, and to be able to see it entirely in the navigator they use newlines. reply oneeyedpigeon 8 hours agorootparentOh god. I already get upset enough by spaces in a file name, although I realise that fight is basically lost now! reply curt15 7 hours agorootparentDidn't Windows name \"Program Files\" with a space to force application developers to handle spaces in paths properly? reply mmcdermott 2 hours agorootparentFor the longest time you could get away with this in cmd: > dir c:\\progra~1 So if forcing people to handle spaces was the goal, it took a long time to force it. reply arp242 2 hours agorootparentI'm pretty sure that still works. I forgot the exact scenario, but my Windows CI on GitHub Actions output shorte~1 pathna~1 like that in a script just a few months ago. On one hand, the backwa~1 compati~1 is nice. On the other hand, there's just so much depreca~1 cruft that keeps popping up even on contemp~1 systems. reply pjmlp 6 hours agorootparentprevIn theory yes, in practice to this day many people don't bother how to learn how to deal with pathnames in a proper way. reply inetknght 5 hours agorootparentTop difficulties in computer science: 1. naming things 2. cache coherency 3. off-by-one errors ??? 4. quoting pathnames reply hawski 2 hours agorootparentI would replace 4 with parameter expansion rules. reply nneonneo 4 hours agorootparentprevEh, maybe. In practice I usually do all my moderately-heavy filesystem scripting in Python these days, for which pathname quoting is just a complete non-issue. Of course, I still use a shell for quick-and-dirty stuff, but usually only for pretty simple tasks where the simplest quoting setup (\"$i\") suffices. reply ape4 6 hours agorootparentprevNot to mention C:\\Program Files (x86) reply account42 6 hours agorootparentAnd C:\\Programme and other localized variants to force people to go through the proper APIs instead of hardcoding paths. reply enriquto 8 hours agorootparentprevAs a fellow spaces-in-filenames-hater, the fight is not lost. We are on the brink of winning it; it's just a mount option away! While we cannot avoid that people hit the spacebar when writing a filename on a gui, this does not mean at all that the resulting filename itself need contain a plain space character. Those spaces can and should be transparently translated to non-breaking space characters at some point. Maybe by the gui itself, or more robustly by the filesystem. This would make everybody happy: gui users and naive shell script writers. reply poincaredisk 8 hours agorootparent>Those spaces can and should be transparently translated to non-breaking space characters at some point Why? This just introduces more complexity and interoperability headaches for seemingly no reason. reply enriquto 7 hours agorootparent> Why? In order to preserve the sacrosanct simplicity of naive shell scripts. Seems like a very noble goal to me. The only unexpexted compexity arises when you want to deal with filenames having mixed spaces and nbsps. But I'd say that people who do that had it coming. reply alexvitkov 7 hours agorootparentIf you want simple shell scripts to work, make an actually good shell language without all the footguns. The filesystem is way more important than /bin/sh and and any complexity added there will trickle down to all programs, not just shell scripts. It's not worth adding hacks on the FS to patch defects in poorly written shell scripts (which are being replaced en masse with python/nodejs/even weirder yaml files/systemd units/etc... anyways) reply consteval 5 hours agorootparentWhitespace in filenames in general is difficult to deal with. Many, maybe most, programs get it wrong. It's not just about shell scripts, many GUI programs fail to handle those files properly too. reply alexvitkov 4 hours agorootparentWhen GUI programs mishandle filenames with spaces, IME it's usually because they spawn a subshell in a naive way (system(\"rm \" + filename)). To mishandle spaces you have to split an input w/ filenames by whitespace, which is not that common of an operation outside of a shell. reply arp242 2 hours agorootparentprevEh? It's really not a bother in pretty much any programming language, and you don't really need to do anything special for it. I don't know any program that has any problems with it. Even zsh has fixed this. It's just /bin/sh and bash that are annoying. reply nradov 3 hours agorootparentprevThe vast majority of Windows and MacOS programs get it right. reply ksp-atlas 5 hours agorootparentprevnushell uses real lists for things which means you don't need to care about seperators except when dealing with external system things reply lifthrasiir 6 hours agorootparentprevSimplicity doesn't always mean stupidity. The simple but functional shell that correctly handles whitespaces without much hassle was already available since 90s, namely rc which is also found in Plan 9. Adopting rc's string concatenator `^` in POSIXy shells shouldn't be too hard. reply chasil 4 hours agorootparentprevIt would be really nice if there was a mount option that would quietly remove spaces in filenames, or convert them to an underscore. If I had it, I would use it today. reply vbezhenar 7 hours agorootparentprevYep, works today: sh-3.2$ f='Hello world' sh-3.2$ echo $f Hello world sh-3.2$ for i in $f; do echo $i; done Hello world sh-3.2$ f='Hello\\xC2\\xA0world' sh-3.2$ echo $f Hello world sh-3.2$ for i in $f; do echo $i; done Hello world reply stouset 7 hours agorootparentJust always quote variable interpolation and you will never have problems. sh-3.2$ f='Hello world' sh-3.2$ echo \"${f}\" Hello world sh-3.2$ for i in \"${f}\"; do echo \"${i}\"; done Hello world sh-3.2$ reply jonhohle 4 hours agorootparentprevConvince (force?) your team to use make and soon everyone will forget spaces in file names are even a thing! reply taneliv 4 hours agorootparentMy team already uses `make` but there's no reason for me to run it in my Downloads folders. File names in there are sometimes wild. Yet I expect command line tools to work with them. If they will cease to do so, I will have to start using non-POSIX variants of those tools, I guess. reply Flimm 8 hours agorootparentprevHow do these non-technical academics even create a PDF file with a name like that? reply ykonstant 7 hours agorootparentRight click, rename, enter, enter, enter (until the entire file name is visible on the box)? That's how I did it when I used Windows. Edit: now I remember the most basic way: open the pdf, select and copy the title, click on rename and paste from clipboard. Works great to get the file name with the newlines exactly as they are on the title! reply zelphirkalt 7 hours agorootparentDoesn'tjust confirm the typed input for the filename and finish the renaming? How does that insert newlines? reply ykonstant 7 hours agorootparentShrug, I last used windows with Windows 7, so you are probably right. That being said, at least two of the students I am currently tutoring are on XP and one of my colleagues as well :D reply pino82 6 hours agorootparentNo, it was always this way. reply ykonstant 5 hours agorootparentRight, I just remembered the main way to create those filenames: open the pdf, select and copy the title, close, rename the file and paste from clipboard. reply abenga 6 hours agorootparentprevI don't know if this is a Linux thing, but when renaming a file, when I press enter, I apply the new name, the file manager doesn't add a newline. reply jodrellblank 6 hours agorootparentprevI don't know who \"the Austin Group\" mentioned in the article are, but how come they \"could not find a single use-case for newlines in pathnames besides breaking naive scripts\" when legitimate use-cases are so easy to find? (And if they're that incompetent, why does the article imply they are worth quoting and listening to?) reply gpderetta 5 hours agorootparentIt is [1] the joint working group that for the last 25+ years has been responsible for both the POSIX standard and the Single Unix Specification. It emerged after the UNIX wars as a consolidation of the various splintered UNIX standardization efforts (POSIX itself, X/OPEN, OSF, etc). [1] https://en.wikipedia.org/wiki/Austin_Group reply nilamo 3 hours agorootparentprevIs that legitimate? A path name is just a unique identifier for a file, IMO it doesn't make sense to put a whole novel in there. If anything, a giant summary like that should be in the meta tags? reply jodrellblank 1 hour agorootparentIn what way is it not legitimate? It's not an accident, bug or data corruption. Someone put it there for a reason, and it benefits their use case. That's as legitimate as it gets. reply ykonstant 5 hours agorootparentprevI am interested in hearing the rationale for downvotes explicitly. I am describing a reality that exists and must be taken into account. Why are you downvoting? reply nasretdinov 9 hours agorootparentprevThe thing is, it's hard to predict what would happen to those scripts regardless... E.g. try naming your files \"-rf\" and see how many things break :) reply ykonstant 5 hours agorootparentA correct script will have no problems with \"-rf\" or any other file name. I have (and recommend script writers make their own) a directory hierarchy of \"dangerous\" file names to test scripts. For example, it contains a directory where all file and subdirectory names are in unary, consisting only of repetitions of the newline character. A correct script should be able to enumerate, access and modify files in there without issue. reply tetha 7 hours agorootparentprevI do enjoy \"ls *; touch -- -lisah; ls *\" as a fun little brainteaser for those uninitiated to this behavior. reply redserk 7 hours agorootparentprevIf one really wanted to embrace chaos, introduce this as a new team file naming standard for \"risk finding\" files ;) reply nneonneo 4 hours agorootparentprevexport TMPDIR=\" / \" to surprise the next person or script to do \"rm -rf $TMPDIR/foo\"... reply stouset 7 hours agorootparentprevDude, just fix the filenames. reply zokier 6 hours agoparentprevBut they did not make old code correct. Filenames are still allowed to contain newlines. Shell scripts still need to be prepared to deal with that. Nothing really changed, they just added a feel-good half-measure. reply quotemstr 4 hours agorootparentIt's a step in the right direction. You have to understand that for decades a vocal group of Unix die-hards has opposed any limitations whatsoever on the bytewise content of file names. The newline restriction in this latest version of POSIX may be modest, but it represents a dam breaking. When (obviously) the sky doesn't fall, the next version of POSIX will have a lot more filename cleanup. reply janderland 2 hours agorootparentprevThis is pretty standard for a human run system. Gotta make the human feel good about an idea before they can do said idea. If you’re not familiar with humans, there are several manuals available online. reply account42 6 hours agoparentprevTheir proposed solution is not compatible with reality though where POSIX does not get to define what kind of files exist on filesystems you need to work with. All they did is introduce new error cases in C programs while not actually fixing anything for shell scripts. If anything, it's going to result in more exploits as people write shell scripts with the assumption that newlines cannot appear in filenames. reply quotemstr 4 hours agorootparentIn the real world, nobody writes shell scripts that handle newlines in filenames. reply account42 2 hours agorootparentI do. Single files are handled with quotes around arguments just fine. For lists of files you need to use NUL as a separator. That's not really hard to do once you are aware of the problem but ergonomics could be better - which is something useful that POSIX could change. reply hwc 1 hour agoparentprevNow do that with all whitespace! reply ezoe 9 hours agoparentprevDon't assume UTF-8 is the only character encoding used in the wild. There are character encoding with leading bytes not easily detectable like UTF-8. reply arghwhat 8 hours agorootparentIn 2024, if you don't get the correct result decoding a text as UTF-8, the bug is the text, not the decoding. And luckily, adoption of UTF-8 in the past 30+ years have gone will enough that you don't need to worry. Caveats for cursed hardware standards demanding two-byte encodings like USB. reply poincaredisk 8 hours agorootparentI hope you're happy in your ivory tower, but I personally work with a lot of files with other encoding, most often that weird utf16 (Windows), sometimes also legacy files with different ANSI encoding. Declaring \"my decoder is fine, it's the text that is buggy\" is not going to score a lot of points with my boss and clients. reply arghwhat 2 hours agorootparentThe only valid reason for still having files stored in legacy ANSI encodings is that their only use is input to software that has not been maintained for ~30 years and cannot be updated. That's fine because they're just binary inputs in a closed ecosystem that no one touches. But if they are supposed to be treated as text, then yes it's the text that's buggy - they should just be converted to UTF-8 once and have the originals thrown away. UTF-16 is something that Microsoft has cursed us with by inserting it into specifications (like USB) so that we cannot get rid of it, even if it never made any sense what so ever. But those are in effect explicit protocols with a hard contract, very different from something where you would \"assume an encoding\". reply zelphirkalt 7 hours agorootparentprevShouldn't hurt to tell clients to right their weird proprietary software originated encodings though. reply 1oooqooq 7 hours agorootparentprevwhy people assume utf8 had only know locale encoding still? you're probably guilty of the sin you preach and is showing wrongly decoded utf8 and don't even know. reply anal_reactor 11 hours agoparentprevIt's a bandaid on a wider problem: the design of Unix shell is bonkers and the whole thing should be deleted. Why? Because I haven't seen any other tool ever have so many pitfalls. Take n random languages and m random developers and tell them to loop over a string array and print its contents, and count how many correct programs you get on average per language. There will be easy languages, then difficult languages, then a huge gap, then Unix shell because in your random sample you managed to get one guy who has PhD in bash. reply vbezhenar 10 hours agorootparentThe main problem is using text as a common format between different applications. First: text is not well defined. Is it ASCII? Is it UTF-8? Some programs can spew UTF-32 with proper locale configured, it's a mess. Second: encoding and decoding of objects to text is not defined at all. Those problems with filenames is just one example. Using newline as a separator is a natural thing that is easy to implement, yet it is wrong. In my opinion two things should be done: 1. Standardise on UTF-8. No other encodings allowed. 2. Standardise on JSON. It is good enough to serve as universal exchange format, tools like `jq` exist for some time now. So any utility must read and write JSON objects with some standard env set. And shells can be developed with better syntax to deal with JSON. This way you can write something like `ps auxwhile read row; do echo ${row.user} ${row.pid}; done` reply aloisklink 8 hours agorootparentPOSIX does actually define what a \"text file\" is, but the definition is a bit unusual: See https://pubs.opengroup.org/onlinepubs/9799919799/basedefs/V1... > 3.387 Text File > A file that contains characters organized into zero or more lines. The lines do not contain NUL characters and none can exceed {LINE_MAX} bytes in length, including thecharacter. So, if you have some non-printable characters like BEL/␇/ASCII 0x07, that's still a text file. (and I believe what bytes count as a valid character depend on your `LC_CTYPE`). But the moment you have a line longer than {LINE_MAX} bytes (which can depend on which POSIX environment you have), suddenly your text file is now a binary file. reply WJW 7 hours agorootparentKind of a weird definition indeed. One edge case: the definition states the file must contain characters, so presumably zero length files are out. But then how could you have zero lines? reply Ukv 4 hours agorootparentPOSIX defines a line as: > 3.185 Line > A sequence of zero or more non- characters plus a terminatingcharacter. So a file with some characters but no trailing newline is reported by `wc -l` as having zero lines. reply rascul 6 hours agorootparentprevAn empty file is not hard to make. It's just a matter of creating the file and not writing to it. reply WJW 6 hours agorootparentYes obviously. But the POSIX specification for a \"text file\" as above is that it contains characters, which an empty file by definition does not. So an empty file cannot be a text file if you read that specification strictly, and therefore you cannot have zero lines in a text file. As soon as you have a single character there is at least one line, and the amount of lines can only stay the same or grow from there. The definition should read \"one or more lines\" instead or (probably better) specify that a text file contains \"zero or more characters\". reply rascul 5 hours agorootparentAhh I see what you're saying. I misunderstood at first. reply poincaredisk 7 hours agorootparentprev>It is good enough to serve as universal exchange format, tools like `jq` exist for some time now. Please don't use that underdefined joke of a spec. Define \"PosixJson\" and use that instead. Right now it's not even clear what the result of parsing {\"a\": 1234678901234567890} is. Is this a parse error? A bigint? A float/double? Quiet wraparound? Something else? I've seen all these behaviors in real world JSON implementations across different languages. reply arghwhat 8 hours agorootparentprevWhat cursed madness have you hit that spits out UTF-32 under normal conditions?! That can only be a bug - UTF-32/UCS-4 never saw external use, and has only ever been used for in-memory fixed-width character representation, e.g. runes in Go. You never have to worry about whether you're dealing with ASCII vs. UTF-8, but rather if you're dealing with UTF-8 vs. ISO-8859-1, or worse, Shift JIS or similar. reply vbezhenar 8 hours agorootparentI think that I hit that with Java: % java -Dfile.encoding=UTF-32 Testhexdump -C 00000000 00 00 00 48 00 00 00 65 00 00 00 6c 00 00 00 6c |...H...e...l...l| 00000010 00 00 00 6f 00 00 00 2c 00 00 00 20 00 00 00 77 |...o...,... ...w| 00000020 00 00 00 6f 00 00 00 72 00 00 00 6c 00 00 00 64 |...o...r...l...d| 00000030 00 00 00 0a|....| 00000034 From quick googling it seems that glibc does not support it, so it should not happen. reply oneeyedpigeon 8 hours agorootparentprevI think a lot of tools should support json as well as plain text. Probably the latter by default, and the former with a \"-o json\" or similar option. I'm fine with wc giving me `5`, I'd prefer that to `{ \"characters\": 5 }`. reply ezoe 9 hours agorootparentprevDon't even assume UTF-something is the only character encoding. There are so many existing character encodings before Unicode. It's still widely used. reply nly 9 hours agorootparentprevThe primary purpose of command line program output is to convey information to a human, not to other programs. Command line scripting is supposed to be adhoc and hack. reply consteval 5 hours agorootparentThere are exchange formats that are well-defined enough to be useful to many computers while also being readable enough to be traversed by human eyes. There's no reason to everything ad-hoc, you don't get much by that. You also control the shell itself - there's no reason you can't display object representations in a pretty way. reply mdavid626 9 hours agorootparentprevI disagree that it supposed to be adhoc and hack. Look at PowerShell. reply anthk 6 hours agorootparentprevThat under limited OSes such as DOS. Under Unix, piping has been the philosophy. reply matrss 7 hours agorootparentprevJSON itself is bad for a streaming interface, as is common with CLI applications. You can't easily consume a JSON array without first reading it in its entirety. JSONL would be a better fit. But then, how well would it work for ad-hoc usage, which is probably one of the biggest uses of shells? reply anal_reactor 10 hours agorootparentprevTrue, but this would be immensely difficult to pull off, because how do you convince other people to write programs that produce actual working JSON? reply pif 9 hours agorootparentprev> The main problem is using text as a common format between different applications. If you can't get the immensity of the cleverness of Unix foundations, you should not talk about them. That idea is what made it possible for you to type that sentence in the first place. reply akira2501 10 hours agorootparentprev> I haven't seen any other tool ever have so many pitfalls. I haven't seen any other tool with so much general utility and availability. > to loop over a string array and print its contents Is incredibly easy in bash and bash like shells. As highlighted the issue is that tools like 'ls' don't create \"a string array.\" They create one giant string that has to be parsed. The rules in the shell are different than in other languages but it /will/ do most of the parsing for you, or all of it, if you do it carefully. This is a fine tradeoff. As evidenced by it's wide usage and lack of convincing replacements. reply anal_reactor 10 hours agorootparent> I haven't seen any other tool with so much general utility and availability. > availability That's the real reason why we use Unix shell. It's not good, but it's available. Like a cheap hooker. > but it /will/ do most of the parsing for you, or all of it, if you do it carefully. \"It mostly works if you're careful\" doesn't sound very convincing to me. reply akira2501 1 hour agorootparent> \"It mostly works if you're careful\" doesn't sound very convincing to me. Would you rather write your own parser? reply stephenr 7 hours agorootparentprev> but it's available. Like a cheap hooker. Username checks out. reply blueflow 10 hours agorootparentprevSomeone needs to come up with a interactive shell first, one that is comparable in usability. Then we can think about replacing the unix shell. I tried both python and lua interactively, but they are a pain when it comes to handling files. You have to type much more to get the same things done. reply throw16180339 5 hours agorootparentI certainly have my complaints about Powershell, but it's got pretty good coverage, decent documentation, and cross platform support. reply felixgallo 4 hours agorootparentif it weren't so irregular, inconsistent, spotty and tasteless, it'd be a great option. reply nly 9 hours agorootparentprevOil shell? https://www.oilshell.org/ Compatible with most bash scripts reply anal_reactor 10 hours agorootparentprevThe bigger issue is the sheer momentum of Unix shell. Even if you come up with an alternative that is better by every objectively measurable metric, it's still going to be a monumental task to have it packages with commonly used distros. Kinda like the \"why can't the US switch to the metric system\" problem. reply blueflow 10 hours agorootparentPeople already use different shells, mksh, fish, and so on. With fish there is a non-posix shell in wide use. reply oguz-ismail 10 hours agorootparent>wide use Five people around the globe isn't wide use. reply blueflow 10 hours agorootparentI'm sure you might get more than 5 people on HN replying to you that they are using fish right now. Say something discrediting about fish and they show up. reply fragmede 8 hours agorootparentHeh, reminds me of how to get help with Linux back in the day. If you directly asked for help, you'd be told to RTFM. If you stayed confidently that Windows could do something and that Linux sucks because it can't, you'd get users tripping over themselves with details and instructions,'just to prove you wrong. Human psychology is fascinating! reply azalemeth 9 hours agorootparentprevThere's a direct cost in money, time and lives that has come from the US's adherence to their US Customary Units (which are often different to the old imperial units). People have literally died because of the confusion caused by having multiple systems of units in common use with ambiguous names (degrees, gallons, etc). Each year industry worldwide spends an enormous amount of money indirectly precisely because of this problem and it's still incredibly unlikely to be fixed within my lifetime. Bash-alternatives that are not completely compatible frankly just don't have a chance. reply stephenr 7 hours agorootparentprevIf it isn't distributed out of the box with every nix-like OS, it inherently isn't* “better by every objectively measurable metric\" - distribution of a common, stable standard is a huge benefit in and of itself. reply blueflow 6 hours agorootparent> distributed out of the box with every nix-like OS, Python and lua are pretty close to that. reply consteval 5 hours agorootparentEven outside of distribution, python and lua aren't objectively better. For starters, they're much more verbose. reply blueflow 5 hours agorootparentI just said that, scroll up. reply stephenr 6 hours agorootparentprev> Python and lua are pretty close to that. Python maybe often installed by default but it's definitely not an essential/required package \"out of the box\" on every install. Also, in a thread where one topic is how POSIX shell handles whitespace in filenames, it's hilarious (not in a good way) that someone suggests a language that handles whitespace the wrong way in it's own code. Yes, significant whitespace is objectively wrong. What OS/distro is Lua included on out of the box? That doesn't mean \"available in a package\". I mean literally included in every single install and cannot reasonably be omitted? Regardless of the availability, the parent comment says > better by every objectively measurable metric Neither Python nor Lua are \"better\" than shell, at the types of things shell is commonly used for - they're objectively worse. reply blueflow 5 hours agorootparentLua gets onto every other Linux distro as dependency of some base system component. For example, rpm or pipewire depend on lua. Ubuntu and Debian ship with pipewire per default. You should use the word \"objectively\" less. reply throwaway19972 10 hours agorootparentprev> the design of Unix shell is bonkers Compared to what? reply mdavid626 9 hours agorootparentPowershell? reply poincaredisk 7 hours agorootparentPowerShell designer could learn from decades of programming language progress and especially shell usage. They could improve many aspects indeed. This doesn't mean that the original design is \"bonkers\", only that it's not perfect. reply oguz-ismail 9 hours agorootparentprevVerbosity is a huge problem there reply consteval 5 hours agorootparentModern programming language designers have a bad relationship with verbosity. I don't know why they do this. It's a lang for an interactive shell, typing literally translates to developer speed. I understand the want for clarity and maybe that's nice in large scripts, but the main goal is to be a shell. So, optimize for that. Also, you probably shouldn't be using powershell for large scripts anyway. The only recent lang I've seen that has a handle on this is Rust. You can tell they put a lot of thought into having keywords be as short as possible while still being descriptive. reply ggm 8 hours agorootparentprevFoundTheCamelCaseConvert. My God next you will say getopt() --longform is the bestest reply throw16180339 5 hours agorootparentIt's been years since I used Powershell, but IIRC there are shortcuts for the common commands, e.g. cat, ls, mv, rm, and such DTRT. reply dailykoder 10 hours agorootparentprevWorks on my machine! reply zelphirkalt 7 hours agorootparentprevThis should not be as downvoted as it is. In a way shell is broken. The brokenness is in that it requires each command to serialize and deserialize again, considering all the weird things that can happen with the \"all is a string\" kind of approach, instead of having a proper data interchange format or even sending objects to next steps in the pipeline. This behavior is what necessitates even thinking about the changes listed in the post. We wouldn't even have that problem, if the design of shell was better thought out. Now we are dealing with decades of legacy built on these shaky foundations. I hate to admit it, but seems at least this aspect Powershell got right, whatever one may think about the rest of it. reply chasil 53 minutes agorootparentOn my rhel7 system, the Debian dash shell is this large: $ ll /bin/dash -rwxr-xr-x. 1 root root 113536 Nov 5 2018 /bin/dash I happen to have an old powershell installed: $ rpm -qi powershellgrep Size Size : 126588370 A strict POSIX shell is always going to be vastly smaller, for many reasons. I would prefer that the POSIX shell was an LR-parsed language, but you can't have everything. reply enriquto 10 hours agorootparentprev> loop over a string array Dear anal_reactor, what is a \"string array\"? I have used unix shells since nearly 30 years and never heard about them. And I consider myself a script-fu master! There are two array-like constructions in the shell: list of words (separated by spaces) and list of lines (separated by newlines). Both cases are implemented as a single string, and the shell makes it trivial to iterate through its components. reply ManBeardPc 10 hours agorootparentThat is exactly the problem many people have with it. Encoding „arrays“ this way is foreign to everyone who comes from „normal“ programming languages. Both variants lead to problems because either character can occur in elements, worst case scenario they contain both at the same time. I can see why this leads to confusion and bugs. reply skydhash 6 hours agorootparentIt’s like people saying they won’t learn French because it has a different grammatical structure. There’s no “normal” natural language. If you’re used to the C-like syntax, learning C-like language will be easy. But that’s not an argument to say Lisp is confusing. reply ManBeardPc 6 hours agorootparentThat's why I put normal in quotes. There is however more to it than having a different grammatical structure: It works different from many commonly used languages that have actual arrays/lists where elements can contain anything the type allows. If you come from any of the common modern programming languages (lets say Java, Kotlin, C#, JS/TS, Python, Swift, Go, Rust, etc.) and expect something similar (because many of them are very similar) you will be confused. Using spaces or newlines to encode elements in a single string is just not robust and leads to easy to make mistakes. reply skydhash 5 hours agorootparentMost of these languages were created long after bash and the other shells. The fact is that shell scripts allows for unquoted strings and quoting is a specific operation, not syntax. Also shell scripts were meant for automations, not for writing general programs. The basic units are commands, arguments, input, output, files,… so the design makes these easy to manipulate. I’m not saying that we can’t improve, but I’m more in favor of making the tool more apt to solve a problem than making it easier to learn. Because the latter often wants to forego the requirement of understanding the problem space. reply ManBeardPc 2 hours agorootparentYes, these are newer. I mainly wanted to make the point that it is confusing if you are new to bash and come from these newer languages with the wrong expectations. The concise nature and many subtle details makes it very difficult for beginners and infrequent users. Compare this to the newer programming languages where you explicitly call something with speaking names like .Trim(), .EndsWith(), support from compiler and IDE. In my experience automation and general programs often are the same thing once things get more complicated. Bash scripts usually grow rapidly and are a giant PITA to maintain or refactor. Throw in build systems and helper scripts and you quickly receive a giant pile of spaghetti. Personally I just switch to one the mentioned programming languages once it goes above a simple sequence of operations. Personally I don't see how to improve it much without becoming a full blown programming language, at which point it would probably make more sense to just release a library for common automation tasks that is also composable. Maybe I'm just not the right target audience. reply skydhash 39 minutes agorootparentThe issue with your otherwise good reply is that someone are bringing expectations to an expert tool (programming languages, software, OS) and blidly assuming that everything will work as he thinks it should. Familiarity helps with learning, but shouldn’t replace it. Someone new to bash should probably start with a book. And for bigger automation projects, there are lots of projects and programming languages that can help. reply chasil 5 hours agoprev- find(1p) now supports -print0 - xargs(1p) now supports the -0 argument - newlines in filenames now should throw errors in many utilities - a complier implementing the c17 standard is now required - ulimit is expanded - renice can use relative values - a timeout utility has been added - make adds support for $^ $+ ::= :::= != ?= += - logger is improved - gettext is adopted - readlink and realpath are adopted - rm now supports -d to remove empty directories and -v for verbose - various improvements to printf, sed, test reply greyw 4 hours agoparentLooks like the BSD-family will have some implementing to do. reply chasil 3 hours agorootparentI just booted OpenBSD 7.0 (which is a bit dated). The find utility has print0, and xargs has -0. Notibly, xargs also has -P for running processes in parallel. rm has both -d and -v. The renice command appears to be able to use relative adjustments with -n. There is a timeout command. There is a readlink command, but no realpath (but a manual page exists for it as a system call). reply sneed_chucker 4 hours agorootparentprevStrict adherence to POSIX isn't a goal of any of the current BSDs is it? reply bryanlarsen 4 hours agorootparentI'm confident they'd accept patches. reply imrejonk 12 hours agoprevThis adds `set -o pipefail` to POSIX sh, which causes a whole pipeline to fail (non-zero exit code) if one or more of the commands in the pipeline fail. reply deskr 7 hours agoparentIf you're writing scripts, use that and don't forget -e and -u -e Exit immediately if a pipeline (which may consist of a single simple command), a list, or a compound command (see SHELL GRAMMAR above), exits with a non-zero status -u Treat unset variables and parameters other than the special parameters \"@\" and \"*\" as an error when performing parameter expansion reply ykonstant 7 hours agorootparentFor `set -u` I mostly agree. For `set -e` see my comment below and Greg's wiki: http://mywiki.wooledge.org/BashFAQ/105 reply deskr 5 hours agorootparent> and they still fail to catch even some remarkably simple cases I totally agree. Although I'd say that there isn't anything \"remarkably simple\" about writing a bash script. Anything in the shell scripting world that seems remarkably simple is just because one hasn't realised the ghosts and horrors that lurk in the shadows. But I'll use -e anytime. It feels like having a protective proton pack at least. reply zelphirkalt 7 hours agoparentprevDoes it? It is not mentioned anywhere in the post. Can you post a reference to your source? reply noselasd 7 hours agorootparentThe post only have a few highlights. The Posix specs are only for paying IEEE customers though, but https://pubs.opengroup.org/onlinepubs/9799919799/ mentions it. reply arp242 1 hour agorootparentThat is the POSIX spec, no? It's at: https://pubs.opengroup.org/onlinepubs/9799919799/utilities/V... (no permalink, search for \"pipefail\") reply akdor1154 8 hours agoparentprevHoly balls that's like Christmas! reply rightbyte 8 hours agoparentprevReally? Wont that break piping grep? reply WJW 7 hours agorootparentProbably, so don't `set -o pipefail` in scripts that pipe into grep. reply rightbyte 5 hours agorootparentAh ok I read it as 'sets it by default' for some reason. reply throwaway984393 11 hours agoparentprevSad. Use of that option is almost always a mistake. It only leads to undebuggable silent failures. reply Joker_vD 7 hours agorootparentI'd rather both have this option and have it work reliably. It's ridiculous that export VAR=$(cmd1cmd2) does not count as a pipefail when cmd1 or cmd2 fail but VAR=$(cmd1cmd2) does, so the \"correct\" way to set an environment variable from a pipeline's output is actually VAR=$(cmd1cmd2) export VAR reply ykonstant 7 hours agorootparentprevPipefail is useful and very hard to emulate on pure POSIX; you need to create named fifos, break the pipeline into individual redirections and check for error on each line. And that is fine; but sometimes you want to treat a pipeline as a \"single command\" and then you can use pipefail to abort the pipeline on error. Then you can handle the error at the granularity of the entire pipeline without caring which part failed. Lastly, I am confused as to the \"silent\" failures; maybe you are thinking of combining this with `set -e`? Then yes, that is bad and I recommend against the combination; but then again, I and most advanced scripters recommend against shotgunning `set -e` in the first place. Use it in specific portions of the script when appropriate, and use proper error handling otherwise. reply zelphirkalt 7 hours agorootparentWhy does `set -e` make a pipeline fail silently? reply ykonstant 7 hours agorootparent`set -e` makes the script abort and is often used in lieu of proper error handing: set -e command command [fails] command Whether the above reports error or not depends on the command; when you have a pipeline failing in the above way, it is even sneakier: set -e command commandcommandcommand [fails] command You are reliant on all commands in the pipeline being verbose about failure to signal error. None of the above is advisable. The advisable code is error_handler() { proper error handling; } command || error_handler \"parameter\" command || error_handler \"parameter\" { commandcommandcommand; } || error_handler \"parameter\" { set -e exceptional section that needs to be bailed out set +e } command || error_handler \"parameter\" reply skydhash 6 hours agorootparentError handling like that makes sense if you’re writing a program. But if you just want a script for an automation, `set -e` is enough. reply ykonstant 6 hours agorootparentIt is not; Greg's wiki further explains why, if the silent failure problem above is not enough reason. reply Joker_vD 5 hours agorootparentGee, imagine if shells with errexit option enabled wrote some diagnostic output to stderr before exiting. \"Add your own error checking instead\", how do I check which piece of pipeline has failed, exactly? The PIPESTATUS variable is bash-specific and was not standardized. reply ykonstant 5 hours agorootparent? Why are you replying to me? My position was pretty clear: \"Pipefail is useful and very hard to emulate on pure POSIX; you need to create named fifos, break the pipeline into individual redirections and check for error on each line. And that is fine; but sometimes you want to treat a pipeline as a \"single command\" and then you can use pipefail to abort the pipeline on error. Then you can handle the error at the granularity of the entire pipeline without caring which part failed.\" By the way, I never script in Bash; I only script in POSIX primitives using dash as my executable. reply pelorat 6 hours agoprevTIL the POSIX standard is still updated. Does it still suffer from the issues that make Linux break POSIX compatibility in some areas because they consider it a flawed standard? reply Flimm 12 hours agoprevYes! Finally! Let's treat filenames with new lines as errors! I'm so delighted with this decision. reply skissane 11 hours agoparentThe original request was to ban all bytes between 1 and 31. https://www.austingroupbugs.net/view.php?id=251 At some point they decided to narrow the change to just ban the newline character. Which I personally think is a pity. Allowing escape in file names is a security risk because it enables you to embed ECMA-48 escape sequences in file names. Secure terminal emulators shouldn’t be made vulnerable by arbitrary escape sequences, but there are “too smart for their own good” terminal emulators out there that have escape sequences that let you do crazy things like run arbitrary executables. reply ezoe 9 hours agorootparentThere are many non-UTF-8/16/32 character encoding used in the wild which use these value in multi-byte character encoding. These values are used in the wild. I think the decision forbidding newline in pathname is also wrong. It may break tons of existing code. reply skissane 8 hours agorootparentI wish Linux/etc had a mount option and/or superblock flag called “allow only sane file names”. And if you had that set, then attempting to create a file whose name wasn’t valid UTF-8, or which contained C0 or C1 controls, would fail. The small minority of people who really need pre-Unicode encodings such as ISO 2022 could just not turn that option on. And the majority who don’t need anything like that could reap the benefits of eliminating a whole category of potential bugs and vulnerabilities. reply Joker_vD 8 hours agorootparentprev> There are many non-UTF-8/16/32 character encoding used in the wild which use these value in multi-byte character encoding. Like what? I am genuinely curious: Shift-JIS, GB2312, Big5, and all of the EUC variants do not use bytes that correspond to C0 characters in ASCII. reply devit 9 hours agoparentprevThat's obviously impossible since it would break backward compatibility and the users' existing filesystems (and the Linux kernel will rightly never accept anything like that). The only reasonable fix is to enhance bash and shell IDEs to track for each variable whether it could possibly include all filename-valid characters (e.g. if it comes from read with no options then it can't contain ) and warn (off by default unless stderr is a terminal) if they can't and it's used as a filename (conservatively determined when used as arguments to processes), and also warn when using find without -print0, etc. noninteractively and perhaps interactively as well. reply IshKebab 11 hours agoparentprevWhy is that an issue? reply shakna 11 hours agorootparentRun a program to list a directory. Everything that interfaces with that, will assume newline delimiters. Similar assumptions are baked into a lot of software. Enforcing that a newline isn't part of a path, ensures the security of those systems that are commonly relied on. reply oguz-ismail 9 hours agorootparentExcept no one's enforcing anything yet. Earlier versions of POSIX allowed rejecting filenames containing newlines, the newest version encourages it while mandating features required to handle such filenames safely (find -print0, xargs -0, read -d ''). So nothing's set in stone yet. reply IshKebab 7 hours agorootparentprev> Everything that interfaces with that, will assume newline delimiters. Well, only badly written programs. nushell handles this fine, as will any program that doesn't try to do everything as plain strings: ~> touch \"foobar\" ~> ls foo*print ╭───┬──────┬──────┬──────┬──────────╮ │ # │ name │ type │ size │ modified │ ├───┼──────┼──────┼──────┼──────────┤ │ 0 │ foo │ file │ 0 B │ now │ │ │ bar │ │ │ │ ╰───┴──────┴──────┴──────┴──────────╯ However after reading it they're only making them illegal for the posix utilities from the 70s that aren't written properly, so I think that makes sense. reply enriquto 12 hours agoparentprevNext: spaces reply lifthrasiir 11 hours agorootparentStill much better than mojibaked names. reply enriquto 11 hours agorootparentWhat do you mean? reply _ZeD_ 11 hours agorootparentWhat is the encoding of the filenames? reply Joker_vD 10 hours agorootparentI am personally not aware of any MBCS that could have a 0x20 or 0x0D as a valid trailing byte. Are you? reply lifthrasiir 9 hours agorootparentI think my comment correctly contrasted mojibake from new lines or spaces for that reason. reply relistan 11 hours agoprevThe history at the beginning of this is not correct. Two examples: the assertion that there was one compatible UNIX prior to United States vs AT&T, the statement that GNU and BSD started that same year. Very, very off. reply unixhero 11 hours agoparentOkay, but you would add more value if you could also state what is the correct order if things. reply relistan 10 hours agorootparenthttps://en.m.wikipedia.org/wiki/History_of_Unix#/media/File%... is a good visual of (many of, not all) the various versions of UNIX and when they were released. BSD was first released in 1978. United States v. AT&T was implemented in 1984 (judgment 1982) GNU was first created in 1983. reply donatj 4 hours agoprevTo build an internationalized shell script I'll need to compile multiple .mo language files and distribute them along side the script itself. For shell scripts part of a large system, that's probably fine. For small scripts, that's not very practical. You are not only adding a compilation step, you're also requiring distribution of multiple files. That's a pain. It just kind of kills the convenience of a simple shell script. I would probably end up writing a makefile to manage all of this and at that point I am only a hop skip and jump away from using a compiled language instead of shell. reply oguz-ismail 8 hours agoprevNitpick re: https://blog.toast.cafe/posix2024-xcu#fn:6 SRC != ls *.c is fine in a makefile as far as POSIX is concerned, because: > Applications shall select target names from the set of characters consisting solely of slashes, hyphens, periods, underscores, digits, and alphabetics from the portable character set reply nh2 3 hours agoprev> future editions will not require c17, but will simply require whatever C specification version is the most modern and already implemented by major toolchains Is this really good? If you can't rely on anything concrete being guaranteed, and it is open to interpretation what \"modern\" or \"major toolchains\" are, why have a standard? reply pabs3 4 hours agoprevSince old-POSIX systems will be in use for some time, I wonder how many things will be able to switch to using the new capabilities. And how many OSes already support all of the new changes. reply somat 12 hours agoprevHopefully nothing, posix is, or at least it should be, a descriptive standard. This is why posix is so terrible, and why posix is so great. The way I feel posix, and other descriptive standards work best is when they describe what every one is already doing. This is opposed to prescriptive standards which try focus on how the \"correct\" way to do somthing, prescriptive standards tend to be over engineered and may or may not actually work. see also: descriptive and prescriptive dictionaries. http://www.englishplus.com/news/news1100.htm reply Flimm 11 hours agoparentBoth prescriptive standards and descriptive standards have their uses. If POSIX is a prescriptive standard, then maybe another standard should exist that is descriptive. reply lifthrasiir 9 hours agorootparentKeep in mind that the Web standard eventually became prescriptive because descriptive standards failed to catch up. Likewise it can be argued that descriptive standards for the common OS interface are no longer usable. reply zelphirkalt 7 hours agoparentprevThat is also a way to never progress beyond the status quo. reply ggm 8 hours agoprevFile names with / in them reply quotemstr 4 hours agoprev> We’ve established that, yes, pathnames can include newlines. We have not established why they can do that. After some deliberation, the Austin Group could not find a single use-case for newlines in pathnames besides breaking naive scripts. Wouldn’t it be nice if the naive scripts were just correct now? Finally. Now let's do the rest: https://dwheeler.com/essays/fixing-unix-linux-filenames.html Filenames should be boring printable normalized UTF-8. I have never, not once, seen a good reason that a filename should be able to contain random binary gobbledygook reply snvzz 12 hours agoprevThis is a surprisingly greedy POSIX update. reply BoingBoomTschak 11 hours agoparentAs someone who truly limits himself to POSIX when he can, I think they needed to push it forward to not become completely obsolete. I'm really sad `mktemp -d` and `set -o nullglob` didn't make the cut, but that's how it is, I guess. reply ykonstant 7 hours agorootparentA bespoke `mktempd` script is one of the first things I install in a new system. Fortunately, it is not too hard to make a `mktemp -d` compatible script with POSIX tools. `set -o nullglob` is another story :D reply pxeger1 7 hours agorootparentIt's quite hard to write mktemp securely[1]. It would be great if POSIX didn't make people attempt to do that error-prone task themselves. [1]: There's some explanation in this recent post: https://dotat.at/@/2024-10-22-tmp.html reply ykonstant 5 hours agorootparentThis is correct (though of course a decent `mktempd` script will deal with the listed problems or crash loudly on failure), and there are even more reasons to avoid /tmp. Unfortunately, it is one of the very few directories that are somewhat POSIX-\"guaranteed\" writable by a non-root user and the fact that on modern systems it is usually mounted on a tmpfs makes it very attractive for pure POSIX usage without rich array support. If you have mount permissions, of course, you should tell your `mktempd` to base its directory on a private tmpfs. reply EdSchouten 6 hours agoprevstrlcpy()! reply guerrilla 7 hours agoprevWhy was `isascii()` removed? (Listed in the Sortix article linked in OP.) reply oguz-ismail 4 hours agoparentIt would yield false-positives with non-UTF-8 encoded text. Big5in particular was notorious for using ASCII values for trailing bytes. I don't know if it's still in use or if there are others. reply johnisgood 9 hours agoprev [–] > Anyway, POSIX 2024 now requires c17, and does not require c89 I wish it would have been c99. What does c17 add exactly, more C++-esque complexity or not? Why was it not c99 (or perhaps even c11) over c17? Genuine questions. reply lifthrasiir 9 hours agoparent [–] > What does c17 add exactly, more C++-ish bullshit or not? Multithreading support and such (atomics, thread-local storage and a guarantee that `errno` is in TLS), explicitly aligned types and allocations, dedicated types for strings known to be Unicode, _Noreturn, _Generic, _Static_assert, anonymous structs and unions in the nested position, quick_exit, timespec, exclusive mode (\"x\") in f[re]open, CMPLX macros. I'm not even sure which one can be C++-ish bullshit possibly except for about two points: - Multithreading does seem farfetched for causal users. In fact, I do think it could have been minimized without any actual harm, but multithreading itself needed to be specified because it greatly affects a memory model. (Before C11, C had no thread-aware memory model and different threading implementations were subtly different beyond what the standard stated.) Even JavaScript, originally without no notion of threads, eventually got a thread-aware memory model due to shared web workers. But that never meant JS itself need multithreading support in its standard library, and C could have done the same. - `_Generic` is even more debatable, though I believe it was the only way forward when we accept , which is known to be a response to Fortran (other responses include `restrict`) and was impossible to implement in the portable manner before C11. As long as it retains its scary underline and title case, I guess it's fine. reply gpderetta 5 hours agorootparentMost importantly posix already has existing multithreading facilities in posix threads, so it is imperative that they are reformulated in term of the C++11/C11 memory model. reply johnisgood 9 hours agorootparentprev [–] You quoted me before my edit, but fair enough. I do like the \"atomics\" support. > \"guarantee that `errno` is in TLS\" I suppose that does not mean that I can just avoid setting errno to 0 before calling a function after which I check for errno, right? Yeah, I do have an issue with stuff like \"_Generic\" but I assume I can just simply not use it. What is \"quick_exit\" exactly and what does it solve? As for multithreading, I stick to phtread. Is any of the new features a replacement for that or what? At any rate, why C17 over C11 then? reply lifthrasiir 8 hours agorootparent [–] C17 is a bugfix version of C11 (the next major revision would be C23). The exact list of fixes is available in [1]. Mandating C11 instead of C17 when both are available seems not really useful now. You have the correct insight about errnos. The new guarantee only means that other threads are not possible to mess with your errnos, but cleaning errnos will be still useful within an individual thread. exit is not guaranteed to work correctly when called simultaneously from multipe threads, while quick_exit will be okay even in that situation. I think this behavior was not even specified before C11, and only specified after observing existing implementations. It is expected that libc threading routines are thin wrappers around pthread in Linux. That's why I do think it can be minimized; the only actual problem before C11 was the lack of thread-aware memory model. No need to actually be able to create threads from libc to be honest, especially given that each platform now almost always has a single dominant threading implementation like pthread. [1] https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2244.htm reply johnisgood 8 hours agorootparent [–] My last question would be: is it \"OK\" to use phtread in my code or are there any alternatives (i.e. \"best way\") when using C17? reply lifthrasiir 8 hours agorootparent [–] No, just use pthread. There are some useful pthread APIs missing from C17 anyway too. reply johnisgood 8 hours agorootparent [–] Thank you for your answers, it is much appreciated. I suppose I will not use \"quick_exit\" either in that case, I have many workers, there is a job queue mutex, along with phtread_cond_wait and phtread_mutex_{lock,unlock} and when the \"job_quit_flag\" is set to true, that means all jobs are done and I am ready to return NULL. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In the 1950s, computers lacked interoperability, and communication was primarily through the Bell System's telephone network.",
      "The antitrust suit against Bell led to the creation of Unix, which became a universal operating system by the 1980s.",
      "POSIX 2024 introduces updates such as modern C standards, improved makefile compatibility, and new utilities like timeout(1p), aiming to enhance interoperability and modernize the POSIX specification."
    ],
    "commentSummary": [
      "POSIX 2024 introduces updates such as discouraging newlines in pathnames to prevent script errors and adding `set -o pipefail` for improved error handling in pipelines.",
      "The update requires C17 compliance, aiming to modernize POSIX while ensuring backward compatibility, though some users are concerned about potential script-breaking changes.",
      "Enhancements are made to utilities like `find`, `xargs`, `rm`, and `make`, reflecting efforts to improve functionality and usability."
    ],
    "points": 203,
    "commentCount": 192,
    "retryCount": 0,
    "time": 1730162546
  },
  {
    "id": 41983622,
    "title": "Writing in Pictures: Richard Scarry and the art of children's literature",
    "originLink": "https://yalereview.org/article/chris-ware-richard-scarry",
    "originBody": "Books Writing in Pictures Richard Scarry and the art of children’s literature Chris Ware The original cover sketch for Richard Scarry’s Cars and Trucks and Things That Go, which was first published in 1974. Courtesy Penguin Random House As a boy, I knew I was supposed to like cars and trucks and things that go. But as an unathletic and decidedly unboyish kid, I only got close to liking one car—my mom’s blue Volkswagen Ghia, which she used to ferry me to and from school (and, when she needed some time to herself, to her parents’—my grandparents’—house for an overnight visit). In fact, I didn’t just like that car, I loved it, so much so that the day it was towed away I secretly chipped a piece of the sky-colored paint from the chassis and tearfully hid it in a little box. I never had the chance to develop such a special relationship with a truck or a bus or an airplane or anything else with a motor or wheels—in fact, such things scared me, and to this day I have never changed a tire. In my grandparents’ second-floor guest room, formerly my mother’s childhood room, one bookcase had a row of children’s books slumped to the side, offering a chronological core sample of my grandmother’s attempts to busy not only her own kids, but all the grandkids who’d stayed there before me. There were the original Oz books, a copy of Ferdinand the Bull, Monro Leaf’s inexplicably compelling yet mildly fascistic Manners Can Be Fun, some 1950s and 1960s Little Golden Books purchased at the Hinky Dinky supermarket down the street, and, among many others I’ve now long forgotten, the big blue, green, and red shiny square of Richard Scarry’s Best Word Book Ever. The largish (even just plain large if you were smallish when holding it) book offered a visual index of the everyday puzzle pieces of life in humble, colored-in line drawings. Each page was a fresh, funny composition of some new angle on the world, making the book a sort of quotidian picture-map containing everything imaginable and unimaginable a kid might be curious about: where and how people lived, slept, ate, played, and worked. The thing is, “people” weren’t anywhere to be seen in Best Word Book Ever. Instead, the whole world was populated by animals: rabbits, bears, pigs, cats, foxes, dogs, raccoons, lions, mice, and more. Somehow, though, that made the book’s view of life feel more real and more welcoming. A dollhouse-like cutaway view of a rabbit family in their house getting ready for their day didn’t seem to just picture the things themselves—they were the things themselves, exuding a grounded warmth that said, “Yes, everywhere we live in houses and cook together and get dressed, just like you.” I Am a Bunny stands as one of the true tranquil masterpieces of children’s book art. Mirroring these rabbits, across two pages an index-like series of images depicted a bear named Kenny getting out of bed, getting dressed, and going down for breakfast. This would galvanize me to action: I’d take out the book and mimic Kenny, washing my face with a washcloth (which I never did at my own house), brush my teeth, get dressed, and make my bed. Then I’d head downstairs to the kitchen with the book under my pencil-thin arm, where my grandmother would gamely try to serve me the same breakfast Kenny was having, emulating as best she could the individual menu items: pancakes, “warm cereal,” orange juice, bacon, and toast. A little later in the book, in a two-page spread titled “Mealtime,” a family of orange pigs surrounded a large dinner table laid out with plates and bowls of various foods. The lower left corner of the rightmost page cradled a wooden bowl of evenly green lettuce leaves with three tomato wedges. I don’t know why, but that drawing so thoroughly captured… something for me that, for years at my grandparents’ house, it became my standing side order. While watching television or reading or drawing in the guest room, I’d smell the English muffin toasting and the breaded pork chops and potatoes cooking, and I’d see the setting sunlight warming the house’s old wood shingles—and I’d know my grandmother would have that three-tomato salad on the side, ready for me, just like the pigs were having in Richard Scarry’s book. I must have been a real pain in the ass as a kid. But Richard Scarry somehow made me feel safe and settled. this year is the 50th anniversary of Scarry’s 1974 Cars and Trucks and Things That Go, which strikes me as a commemoration worthy of ballyhoo, especially now that, as a dad myself, I’ve spent so much time ferrying my own daughter to and from school and birthday parties in various cars that—well, mostly goed. (I’ve owned five automobiles in my life, all of them cheap, one of which smoked and required the driver’s side door to be kept shut with a bungee cord hooked to the opposite armrest, stretched across both driver and passenger. What can I say? I was a young cartoonist on a cartoonist’s budget.) Unlike those budget vehicles, however, the new deluxe Penguin Random House anniversary edition of Cars and Trucks and Things That Go is lavishly well-made, attentively reprinted with sharp black lines and warm, rich, watercolors. It includes an especially lively afterword by Scarry’s son Huck, in which he explains, using language even a kid can understand, how his dad wrote and drew the book, as well as hinting at what it was like to grow up as the son of arguably the world’s most popular and successful children’s book author. Richard McClure Scarry was born on June 5, 1919, in the Dorchester neighborhood of Boston, Massachusetts. His Irish-American father, John James Scarry, ran Scarry’s Department Store so congenially and cannily that even during the Great Depression the whole family—his mother, an aunt, four brothers, and one sister—lived comfortably. According to Walter Retan and Ole Risom’s The Busy, Busy World of Richard Scarry, when Scarry was a boy and his mother asked him to go to the store to get provisions, he would write his grocery list not with words but with pictures. So his mother signed him up for drawing lessons at the Boston Museum of Fine Arts, where she also brought him to see the paintings and sculptures. It took Scarry, who was uninterested in school, five years to receive his high school diploma, in part because he spent a fair amount of his time lazing around at the public library and visiting burlesque shows (this time, one assumes, without his mother). This disappointed his father, who pressured him into going to a local business school, a fate to which Scarry acceded but loathed so deeply he soon withdrew and re-enrolled at the Museum of Fine Arts. “You will live in a garret and eat nothing but spaghetti,” his father warned. But Richard’s mind was made up. Then, after a few classes and December 7, 1941, he was drafted. Scarry later recalled his experience in the military as “the best war ever.” At basic training in New Jersey, his commanding officers discovered that he could draw, leading Scarry to be largely excused from the rigors of pushups so that he could work as a sign painter. Leapfrogging to the rank of lieutenant (a prerequisite for his new post as art director of the Army in North Africa), he arrived at the port of Casablanca in somewhat plum circumstances, tasked with creating morale-boosting propaganda by doing things like illustrating information manuals and guidebooks and drawing maps describing the worldwide progress of Allied fighting. He and his fellow officers discovered a nice villa twenty miles outside Oran where they were permitted to stay, borrowing their colonel’s Buick to drive themselves back and forth to work. Later stationed in Venice and Paris, Scarry’s experience of World War II was, well, charmed. After his discharge in 1946, Scarry moved to New York, where he briefly worked as an art director for Vogue and then in an advertising agency before acquiring an agent, who was able to secure him an illustration job with the then brand-new but now completely forgotten Holiday magazine. The job paid the princely sum of $2,000. (I can confirm from personal experience that such pay has changed little since 1946; $2,000 is still the average, if not generous, going rate for magazine artwork. But plugged into an inflation calculator, $2,000 in 1942 clocks in at $34,524.73 today.) With his living expenses suddenly covered well into the future, Scarry moved into a nicer apartment. He met an advertising copywriter named Patricia Murphy, and in 1948 they got married. Meanwhile, in Racine, Wisconsin, a printing company named Western Publishing and its imprint Whitman (which had found great success in the 1930s with the Big Little Books and other novelties) were hatching a new idea for children’s literature, a series that would be christened “Little Golden Books.” Up to that point, children’s books had traditionally been a $1.50-and-up Christmas gift—$25.19 in today’s inflationary dollars—luxurious gilt volumes bestowed by great aunts that told of princes and princesses and things that didn’t go anywhere at all. These new Golden Books, by contrast, were to be cheaply produced and democratically priced at twenty-five cents, and would be sold year-round at pharmacies and, as my grandmother referred to them, “dime stores.” The brainchild of Georges Duplaix and Lucille Ogle, two editors at Western’s recently opened East Coast offices in Poughkeepsie, New York, Golden Books employed displaced if not just plain refugee artists from Europe like Feodor Rojankovsky, Tibor Gergely, and Gustaf Tenggren. Working in a careful, deliberate, and illuminatory style, they carefully limned every hair of every dog—think The Poky Little Puppy—and set every page aglow with a strangely dark, yet warm light. On the page, their paintings were frequently vignetted in darkness, almost as if the artists still felt shadowed by the lingering specter of war. These books, dismissively looked down upon by librarians, were nonetheless immediately, snot-flyingly popular, with orders mounting into the millions of copies. Such publishing numbers were astonishing then (and are even more astonishing now, when 15,000 is considered a gee-whiz success). Packaged for publishers Richard Simon and Max Schuster and their vice president Albert Leventhal, the entire series was written, drawn, edited, and printed by Western Publishing. A second wave of refugee artists signed on to their roster in 1948, this time escaping Southern California and Walt Disney’s anti-union practices. Among them were John Parr Miller (designer of Dumbo and Geppetto) and the writer-artist team of Alice and Martin Provensen. Scarry wasn’t escaping anything, but he was hired amid this group to do up a four-page promotional brochure for Golden’s push into supermarkets. Impressed by his speedy, quality work, Western followed with a four-book illustration contract, which Scarry flexingly went on to exceed, producing not four, but six books before the one-year deal ran out. (Busy, busy world, indeed.) Wowed, Western signed him on for more, and the Scarrys moved to Connecticut, eventually ending up in Westport, where they went skeet shooting and boating, befriended other Golden Books artists who lived in the area (including Miller and the Provensens), and attended many parties. Then, in 1951, Scarry published his first solo writing and drawing book, The Great Big Car and Truck Book. the great big car and truck book is, in some ways, the seed-germ of Cars and Trucks and Things That Go. But the differences are revealing. The Great Big Car and Truck Book still follows the house style of Western/Little Golden Books—careful watercolor-and-gouache illustrations that illuminate a text that is about the here and now of what people do all day. Moreover, the people driving the cars and trucks aren’t finely haired rabbits, bears, and pigs, but pink-skinned 1950s human suburbanites. The effect is curious, and, given our now-idea of Scarry, very un-Scarry-like. Like Charles Schulz’s early experiments drawing actual adults in Peanuts (the effect of which is psychedelic), Scarry’s humans feel just, like, wrong. There’s also little to distinguish his work here from other magazine illustrators of the day: the people, while charming enough, are too human to empathize with—oddly blank and impersonal, and advertising-art-like. Scarry had almost always preferred to draw animal stories—Good Night Little Bear, The Bunny Book—in books by his wife or other writers such as his good friend, Danish emigre Ole Risom. But it would be several years before he would do the same in his own books, substituting animals for humans as well as dropping the more labored visual approach for what we now recognize as his mature work. In Busytown there’s just enough innocent mayhem and tripping and falling to hint at a darker side of things, like failing 1970s marriages and the things on television news that adults were always yelling about. There were, of course, obstacles. One of the less appealing features of Golden’s business practice was that, with rare exceptions, they offered no royalties. This arrangement nagged at Scarry, especially after his and Patricia’s son Huck was born in 1953, so in 1955 he finally asked the imposing white-haired and lavender-blue-eyed Lucille Ogle for a revised contract that included royalties—and an advance. She readily agreed. Surprised, Scarry asked why she hadn’t offered such a deal earlier. “Because you never asked,” she replied. With this new contract, though, Golden started to send Scarry less work, and he began to wonder if he was being deliberately snubbed because of his higher pay rate. So even as he was producing beautifully wrought paintings for Golden Books, he sought and then secured additional paying work from Western’s competitor Doubleday. He also created some loose storybook proposals of his own authorship in a surprisingly free and zippy pencil style. Freed from the precision of painting, the linework of these sketches—dare I say, cartoons?—came alive on the page like nothing he’d drawn before. I don’t know if the roughs he produced for his efforts were all so lively, but these certainly showed a new direction, if he decided to take it. But when Scarry shopped the proposals around, Golden and every other publisher he approached turned him down, so he shelved them. Richard Scarry’s The Great Big Car and Truck Book, which was published in 1951, is in some ways the seed-germ of Cars and Trucks and Things That Go. Courtesy the Estate of Richard Scarry In the years that followed, Scarry continued to do exceptional work in Golden’s illuminated-painting mode, most notably his 1963 book I Am a Bunny, with text by his friend Risom. I never read it as a child, but I can now attest to its elegant, quiet beauty, because it was my daughter’s first word book ever, and I read it to her several hundred times. I never tired of its pictures or its words, the simple zen-like magic it evokes of the inevitability of the passing seasons always somehow putting the reader in a pleasant passenger-seat view. I Am a Bunny stands as one of the true tranquil masterpieces of children’s book art. Even as he was working on I Am a Bunny, Scarry was preparing a proposal for a new kind of word book done in that free pencil style, which he called Best Word Book Ever. This time around, Golden accepted the proposal, and when the book was published, the fully realized Scarry World exploded into view: clean, pencil-line, doodle-like drawings that seemed lively and alive, if not to live on the very page itself. Richard Scarry’s Best Word Book Ever was an immediate hit, becoming one of the bestselling children’s books in history, selling seven million copies by 1975 (including the one my grandmother bought). And though Scarry was only getting a royalty of eight cents a copy (a royalty that, due to the contract he signed, did not rise in value as the price of books went up), the incredible number of copies the book sold meant that money finally started rolling in for the Scarrys. Scarry produced two more books for Golden in the Best Word Book Ever mold: Busy, Busy World (1965) and Storybook Dictionary (1967), the latter of which further plied the icono-lexicographical approach of images-as-words that had made Best Word Book Ever so compelling—the little pictures practically ask the reader to pluck them off the page and play with them. At that point, Random House swooped in and bought Scarry’s next book, What Do People Do All Day?, and became his publisher from then on. (Interestingly, after a 1990s bankruptcy, Random House also became the publisher of the entire Little Golden Books catalogue.) Scarry followed What Do People Do All Day? with a series of books all set within the same society, including (among others) Great Big Schoolhouse, Cars and Trucks and Things That Go, and Busiest People Ever! The Busytown books, as they came to be known—with their dictionary-like visual presentation paired with lightly slapstick situations and the presence of recurring, memorable characters like Huckle Cat, the Pig family, and my favorite, Lowly Worm—grew into a real-feeling big world that Scarry seemed to be letting little ones into. (Lowly was perhaps the first children’s book animal character with a real nod to the ADA and the myth of “dis”-ability, and cheerfully makes his linear form work in all sorts of inspiring and disarmingly moving ways.) Scarry’s guides to life both reflected and bolstered kids’ lived experience and in some cases, like my own, even provided the template for it. And while often sweet and quiet in its depiction of a picture-perfect society functioning measuredly—was Busytown urban or suburban or . . . European? (Where did all those Tudor homes and corner groceries come from, anyway?)—there’s just enough innocent mayhem and tripping and falling to hint at a darker side of things, like failing 1970s marriages and the things on television news that adults were always yelling about. The busiest Busytown book is Cars and Trucks and Things That Go. As fascinated by the industrial world as any serious truck-spotting four-year-old, Scarry captures the ballet of traffic in a sort of frozen mimesis that’s reanimated by the act of reading and page-turning itself. Every aspect of life, however flimsily related to internal combustion travel, seems herein represented: whipped-cream delivery vans, mobile libraries, jet-fuel trucks, bookshelf-maker’s cars, ant buses, two-seater crayon cars, ambulances—the lot. There’s a simple, child-sized joy in recognizing the same characters driving by again and again in animal- and vegetable- and fruit-shaped cars while being dwarfed by accurately rendered bulldozers, heavy cranes, and thundering trucks, all traveling, page by page, left to right in the direction of the book—and the left to right of reading itself—through towns and construction sites and beaches and snow, ultimately ending in a calamitous (safe!) crash and a skidding of little cars spinning leftwards and finally stopping in front of (what else?) “Home.” Adding to the delight, throughout the book a tiny character named Gold Bug (who is literally a gold bug) hides on nearly every page, giving the engaged child a chance to find him over and over again in an exercise that would today be called “interactive” but we used to just call “looking.” The Busytown books were enormous successes in America. But Scarry wrote and drew them in Switzerland, where he decided to move in 1967 after a three-week ski vacation with his son. What seems to have been an impulsive decision starts to makes sense if you’ve spent a few days immersed in Scarry’s work writing an essay for The Yale Review: a decidedly un-American tone runs through much of it. By “un-American” I don’t mean anti-American. Instead, I mean there’s a top-down, citizen-as-responsible-contributor, sense-of-oneself-as-part-of-something-bigger that feels, well, civilized. Even as a kid, I noticed that something about Best Word Book Ever felt odd, and I decided that Scarry was a balding Englishman, tweedy with possible pipe and maybe even one of those mountaineering hats with a feather in it. He was not any of those things. But the more one looks at his work, the more one sees how the European daily grocery trip, the walk to a nearby shop or tradesman’s guild, the tiny apple car fit for a worm are not part of the blowout-all-in-for-oneself-oil-fueled-free-for-all toward which America was barreling in the late 1960s. (Except, perhaps, in Cars and Trucks and Things That Go — though Europe has traffic, too.) So it’s perhaps unsurprising that Scarry spent the rest of his life first in Lausanne and then Gstaad, in a lovely chalet, hardly looking back while America slowly ground itself to pieces. scarry continued to produce books for another two decades, all of them featuring animals in place of humans. This actually caused a mild panic at Random House when What Do People Do All Day? was being published, with the staff asking: Shouldn’t it be called What Do Animals Do All Day? The dispute was short-lived since the answer (“No!”) was so obvious, but it hints at something important about the narrative energy on which Scarry’s engine runs. In children’s books, animals are frequently introduced as the first vessels to receive the natural empathy with which children are born. See: Golden’s own Baby Farm Animals (pictures by Garth Williams), The Lively Little Rabbit (pictures by Gustaf Tenggren), The Animals of Farmer Jones (pictures by Scarry), and about ten thousand other children’s books (pictures by everyone else). The natural inclination to ask “do animals feel the same things we do?” is confirmed with a smile and a tuck-in, what in literary terms is cumbersomely called “anthropomorphization” but in everyday words is just “caring.” As we grow up, though, the truth will out: Mrs. Cow makes a good burger, those chicken fingers were Miss Clucky, and don’t forget to check the trap to see if we caught Mr. Mouse. This all then slides into discovering that not everyone is as nice as they seem, and it’s good to protect oneself on the playground; before long, one can even end up in ROTC, heading into basic training and rolling away in a tank. Fold in the especially twenty-first-century phenomenon of the “first-person shooter” in kids’ video games—surely the most telling perversion of literary terminology America could have hoped for to permanently indict itself—and children’s literature, to say nothing of the idea of polite civilization, is easily relegated to the category of the hopelessly naïve. As fascinated by the industrial world as any serious truck-spotting four-year-old, Scarry captures the ballet of traffic in a sort of frozen mimesis that’s reanimated by the act of reading and page-turning itself. Scarry studiously avoided granting cows and chickens driver’s licenses. But the pigs? Where’s the bacon in Kenny’s breakfast coming from? So his representation of animal society indeed raises some odd questions, but rarely seems to have bothered readers (with the exception of those literal-minded Random House editors). By contrast, when Art Spiegelman’s graphic novel Maus: A Survivor’s Tale received the Pulitzer Prize in 1992, it provoked vociferous criticism from people offended by its depiction of its characters as animals. Jews were mice (picking up on Hitler’s calling Jews “vermin” or “rats”), Germans were cats, Americans were dogs, (Christian) Poles were pigs, and the French were (of course) frogs. Not surprisingly, many readers also objected to Spiegelman choosing the comic book—a form associated with children’s literature—to tell the story of his mother’s suicide and his father’s nightmarish time in Auschwitz. Such criticisms entirely missed the point of Spiegelman’s work. Originally written as a short comic strip story for Terry Zwigoff’s 1972 underground comic book Funny Animals—which Zwigoff created to benefit animal rights organizations after visiting a slaughterhouse—the strip was later expanded by Spiegelman into a novel-length work. The book turns on the idea of ideas corrupted to the level of Idea: reducing humanity to something to be exterminated by exterminating the ability to see the human being before your very eyes. Had Spiegelman drawn “actual” people, the reader would no longer be complicit in the psychological deformation of the Holocaust itself. Biographically nonfiction in its text yet fiction in its pictures, the book works an ingenious, tortuous turnaround in the mind—and the eye—of the reader. It could not have been told just in words. richard scarry’s work could not have been told just in words, either. As Walter Retan and Ole Risom argue, Scarry “didn’t write his stories; he drew them.” His bestselling book was not titled Best Picture Book Ever, even though that’s really what it is. As children, we see the world in all its detail, texture, and beauty, but when we learn the word for, say, a bird, we cease to see it as clearly or curiously as we did before we categorized and dismissed it. John Updike eloquently and beautifully captures this confounding contradiction in his short story “Pigeon Feathers,” where the main character only notices the iridescent, divine beauty in a pigeon’s plumage after he’s shot several of them to pieces in the rafters of a barn. Like it or not, just as adulthood runs roughshod over childhood, words chew images to shreds, and it’s up to the artist—or the writer or the cartoonist—to put those images back together again. Pictures are our first language for understanding the world, but that doesn’t mean they should be ignored in favor of a second. Or, as Dave Eggers once kindly put it, cartoonists (and I include Scarry in this group) needn’t be punished for having two skills instead of one. Scarry drove headlong into a picture-world that he illustrated with words, a world which blossomed into life in a way that his earlier books for Golden, in which his pictures illustrated words, simply couldn’t. He kept in touch with his child self so well that, as both his biographers and other writers have highlighted, he didn’t test his books on children, because he had “remained very childlike himself.” And he knew exactly where the child inside him still lived: his kind heart. Chris Ware is an artist, writer, and regular contributor to The New Yorker; his book Jimmy Corrigan: The Smartest Kid on Earth was awarded the Guardian Prize and Building Stories was chosen as a Top Ten Fiction Book by The New York Times. A traveling retrospective of his work began at the Centre Pompidou in 2022 and will conclude at the Centre de Cultura Contemporània in Barcelona in Spring 2025. Subscribe New perspectives, enduring writing. Join a conversation 200 years in the making. Subscribe to our print journal and receive four beautiful issues per year. Subscribe TAGS Books Arts & Culture Fall 2024 Chris Ware Originally published: September 9, 2024 See this issue Featured Essays The Shapes of Grief Witnessing the unbearable Christina Sharpe Interviews Garth Greenwell The novelist on writing about the body in crisis Meghan O’Rourke Essays Louise Glück’s Late Style The fabular turn in the poet’s last three books Teju Cole You Might Also Like Books The Darker Side of Bambi What Felix Salten's tale teaches us about the lives of men Charlie Tyson Essays My Silent Childhood I didn’t know my name until I went to kindergarten. Then I became a writer. Maureen Sun Essays Ghosts in My Nursery Mourning the brother I never knew Miranda Featherstone Subscribe New perspectives, enduring writing. Join a conversation 200 years in the making. Subscribe to our print journal and receive four beautiful issues per year. Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=41983622",
    "commentBody": "Writing in Pictures: Richard Scarry and the art of children's literature (yalereview.org)161 points by cainxinth 5 hours agohidepastfavorite72 comments velcrovan 2 hours agoI can still recall my impressions from reading these books at five years old, and the difference between them and the world I eventually entered does make me sad. I’ve been struck by how the world is depicted to my children in so many children’s books. “As you grow, you’ll be able to slot into a happy productive life, no matter what kind of work you like.” No joke: every time I read my daughter a Richard Scarry book, I wonder when and how it will have to be broken to her that unless her interests happen to include something that pays a living wage, she’s actually fucked for life. Think about what it would be like to grow up or to raise children in a world where the ramp to adulthood was incredibly wide and smooth and it was near-impossible to fall off a cliff into poverty, loneliness, or a life of work you hate. Busytown isn’t a real place, but it was obviously designed to give children some sense of what the world is like or supposed to be like. reply 0xcafefood 1 hour agoparentThis is an overly dour take on children's books. I don't disagree that young people (at least in the US) are told a lie like \"Do what you love and the money will follow.\" I fell for that and tried to become research scientist, getting a far as receiving a PhD only to find that the funnel from there to a tenured research job was _extremely_ narrow. That's a giant waste of many motivated people's time. And it does need to be fixed. But is the source of all this Richard Scarry books? I really doubt it. It's okay to give a rosy view of the world to children. Childhood is the longest and best vacation you'll ever take. But there does need to be an incrementally higher dose of \"real talk\" as children grow into young adults. reply velcrovan 1 hour agorootparentMaybe I should have been more clear: I see this as a problem with the real world, not with children’s books. The books are good. What’s busted is that so many adults don’t see such a world as worth building and working towards. reply verisimi 1 hour agorootparentThe story is right, but reality is wrong? Perhaps you have an issue with interpretation here. The pleasant story was the illusion here. PS - I too loved the Richard Scarry books as a child and bought them for my children - and I'm not even from the US. reply velcrovan 53 minutes agorootparentAll I’m saying is that we ought to make a conscious effort to build the world that is more aligned with the world we’d like to explain to our kids. I think stories that describe better possibilities than what we have are useful cognitive and social tools. reply verisimi 29 minutes agorootparentYes, but I'm not sure that the job is to 'change the world' - I don't want to fend off those who think they know what I need. I think it's more about changing oneself to align with what one finds here, whilst trying to remain true to oneself. reply pfdietz 1 hour agorootparentprevAdults correctly see there's nothing they can individually do that would have detectable effect toward such a goal. So, the cost/benefit ratio is a loser. reply velcrovan 41 minutes agorootparentI used to think this way too. My outlook on life transformed when I found out about this amazing concept: https://en.wikipedia.org/wiki/Collective_action reply jonas21 1 hour agoparentprevDo you think the animals in Busytown are all working their dream jobs and following their passions? Of course not -- as someone else pointed out, the butchers are all pigs! But they mostly seem to have found something that they like well enough and contributes to society. And perhaps that's an important lesson to take away. Part of living a happy, fulfilling life is finding joy in what's attainable. reply gffrd 9 minutes agorootparent> they mostly seem to have found something that they like well enough and contributes to society. Exactly! It shows people being valuable to each other in small ways, having purpose, and being involved in each other's lives. And it emphasizes the joy and texture of everyday life. The little things. reply velcrovan 1 hour agorootparentprev> But they mostly seem to have found something that they like well enough and contributes to society. And perhaps that's an important lesson to take away. Part of living a happy, fulfilling life is finding the joy within what is attainable. I fully agree about all of this. The sad fact is that, in my country, a person with this humble and contented perspective is rolling the dice when it comes to housing, health, and autonomy, and many of them lead unnecessarily stressful and lives with no time/energy for community or creative development. That’s the part I would like to see fixed. reply rohfle 54 minutes agoparentprev- born 1919 - upper middle class life in boston - started illustrating 1949 - started writing original works 1955 - moved to switzerland 1972 - died 1994 He wasnt alone in his world view at the time. Think about how much the world has changed since. reply naet 1 hour agoparentprevI have sort of the opposite feeling reading some children's books to my son, especially ones that I can remember from my own childhood. It's great for me to remember to look at the world in a more childlike way. Even though I have more responsibilities as an adult, like bills to pay, I also have the freedom to be silly, enjoy time with my family, do something creative, etc. Any given day I can decide I want to try something new in the kitchen, make a big mess concocting some crazy recipie, maybe decide it didn't work out and go grab a pizza for dinner instead. Or I can write some stories or poetry, or take a sketchbook to the lake, snap some photos at the train station, or whatever catches my fancy on that day. I think we sometimes forget we have certain freedoms as we settle in to typical patterns and go a little on autopilot, but all it takes is a gentle nudge to rediscover that type of fun. I switched careers later in life after finding that it was difficult to make money in my initially chosen field, but I don't feel \"fucked for life\" by it. I actually feel a lot better about doing things I like outside of work. It's much more rewarding to be creative if I'm not trying to maximize profits or otherwise commodify what I'm doing. Sure I wish I didn't have to work and could do whatever I wanted all the time, but I work generally acceptable hours at a stomach-able enough job and have time outside work for family and fun and I've made my peace with that. reply velcrovan 46 minutes agorootparentAll of this very much describes my situation in life too. I would just like for that same level of autonomy and freedom to be the default reality for my kids, and people in general, no matter what work they choose to do, even “uninteresting” work, so that they have this kind of ample leisure and autonomy to pursue their interests outside of their jobs. In my country that is very much not the case, not sure about where you live. reply boplicity 1 hour agoparentprevSimilar thoughts: Almost every children's book that depicts a serious issue, such as racism, tells a complete story where the problem is solved and is no longer a problem. I believe this is a big part of why so many people think that racism (again, for example) isn't a problem anymore. Because, they learned, from a very young age, and for many years, that such problems were solved. There is a very strong and understandable impulse for children's books to be comforting, and to shy away from un-answered questions. This, as you point out, leads to a warped view of reality that parents then should correct. reply naught0 34 minutes agorootparentI think it's less children's books and more decades of propaganda from right wing sources decrying things like affirmative action, DEI etc. If some are to be believed, white men are the most discriminated group in the US. reply tikhonj 1 hour agoparentprevI mean, the books definitely show lots of \"real\" jobs, and ones people wouldn't necessarily be naturally passionate about. What's missing is all the bullshit jobs and awful management. But, even if you wanted to, you probably couldn't really convey that in a children's book... Hell, even most adult books fail to capture just how bullshit the bullshit is! reply havblue 47 minutes agoparentprevWe aren't talking about children here exactly, but I think adults tended to do a disservice to teenagers as I was growing up by not citing the mean employment statistics of certain jobs, especially ones related to the arts. We can thank the generation who survived the great depression and the boomers for this eternal optimism. \"Oh well, by the time things really go south I'll be long gone anyway, sorry!\" reply switchbak 29 minutes agorootparentWhen I was a teenager, my parents weren't particular involved in my educational choices nor my career plans. I was mostly left to my own devices. Despite this, I was very much aware of the tradeoffs in the educational path I was choosing. Perhaps it's because I didn't grow up wealthy, but the financial facet of both my education and eventual career was front and center for myself and my friends. It would have been nice for the adults in my life to have told me these things, but a competent 16-19 year old is also plenty able to ascertain these things themselves. reply tikhonj 3 hours agoprevI loved Richard Scarry as a child—still do!—and I'm convinced his books really helped build up my vocabulary as I learned English in first grade. A detail I only mentally noted as an adult: the butchers in his books are always pigs themselves. A pig selling ham, sausages and, presumably, cuts of pork is a bit morbid (and also hilarious in a black humor sort of way), but it fits in with the world so much that I didn't even think twice about it as a kid. It just slid past me. I still remember one of the books I had as a child—can't recall the title exactly—that had a bunch of urban scenes with various objects labeled. What really stood out were the little details and funny little stories going on. The stories and the humor got me to pay attention and actually care about the objects (and their labels!) far more than any generic vocabulary book for kids. What I love about Richard Scarry is that he is never patronizing or condescending. Too many authors of children's books either try to write down to kids, try to write what they think kids ought to read, or both. But kids aren't idiots and they can tell! Well, I can't speak for everyone, but at least as a kid myself I found a lot of children's works either patronizing or unpleasant—works that were trying too hard to be childlike or, especially, works that were transparent morality plays. Scarry's work is nothing like this at all. It's oriented for and accessible to kids, but it manages to be simple and silly in a genuine way. The art and stories are actually cute and funny rather than caricatures of what an adult thinks a child would find cute and funny. You can tell Scarry was making something he would enjoy himself. That's why I loved his books when I was five and why I still love his books now. It's hard to find other children's books like that. I collect illustrated books and the majority I see in stores are awful. The most successful exception I've seen are books by Joe Klassen (of I Want My Hat fame) along with his common collaborator Mac Barnett. Their books are legitimately funny and visually attractive to adults, they're willing to write stories that aren't entirely saccharine, and children absolutely love them. I've seen that first-hand. reply tivert 2 hours agoparent> A detail I only mentally noted as an adult: the butchers in his books are always pigs themselves. A pig selling ham, sausages and, presumably, cuts of pork is a bit morbid (and also hilarious in a black humor sort of way), but it fits in with the world so much that I didn't even think twice about it as a kid. It just slid past me. Also: every scarecrow has a crow sitting on it. reply tikhonj 2 hours agorootparentI hadn't noticed that. It's an absolutely hilarious little touch. reply ninalanyon 2 hours agoparentprev> I didn't even think twice about it as a kid. It just slid past me. I think that the best children's books always have something in them that will be appreciated later. They can be read and re-read as one grows older. Probably the greatest prose exponent was Lewis Carroll. reply queuebert 5 minutes agorootparentI think also they put something in for the parents so reading the same book over and over and over to your kids isn't as boring. reply 082349872349872 1 hour agoparentprevSee https://kieranhealy.org/files/misc/levimartin.pdf \"What do animals do all day?\": The division of labor, class bodies, and totemic thinking in the popular imagination (2000) > Keywords: Animals; Totemism; Class body; Busytown; Symbolic domination; Division of labor (36pp, not entirely serious, about which animals do what in Busytown; contrasted with Babar, etc.) reply cduzz 1 hour agoparentprevMy wife was quite sad when I suggested to her that \"this little piggie went to market\" was in fact the farmer taking the pig to the market to be butchered, and that \"this little piggie went wee wee wee all the way home\" was probably a piglet taken from its family to replace the \"now big enough to sell\" pig.... reply 082349872349872 1 hour agorootparentIf you wish to enjoy sausage, it's probably best not to enquire into how swine are raised and kept in our time. reply lnsru 27 minutes agorootparentMy colleague was transported to mental health institution with an escort of police straight from the office. And was kept there for 2 weeks. People are treated like trash by other people. I don’t want to think about industrial animal farming. It’s probably the worst of the worst. reply cduzz 47 minutes agorootparentprevI think she'd always imagined a pig with a hat and overalls walking down the street to buy the sunday newspaper... reply kridsdale3 1 hour agoparentprev> I still remember one of the books I had as a child—can't recall the title exactly—that had a bunch of urban scenes with various objects labeled. I have this in my son's room. It's called \"Busy Busy Town\". reply neofrommatrix 3 hours agoprevI absolutely adore Richard Scarry books. I discovered Richard Scarry a couple of months ago when I was looking for books to read for my 3 year old. Now, not a moment goes by in the evenings when we are reading the adventures of Lowly and his seek and find books. It’s a ritual every night. Richard Scarry and the Grumpy Monkey series are a godsend. reply andrewla 4 hours agoprevI never experienced Richard Scarry as a child myself. I discovered it for my own children in a pile of books left on someone's stoop to give away. I was immediately blown away by the whimsy and astonishing care put into every detail. My kids obsessively hunted for the \"goldbug\" on each page of Cars and Trucks and Things That Go (although they never developed a taste for finding Waldo). reply longdustytrail 22 minutes agoparentI “read” cars and trucks and things that go for my 2 year old every night. He recently found out that goldbug is on every page and now he won’t let me turn the page until we find him. It’s genuinely a lovely book. We got ours from buy nothing, it’s very old and taped up and it has a bunch of writing in it by a kid named Max. reply iambateman 3 hours agoparentprevSame. I think I've read C&T&TtG 200 times to my son. Making it to the beach scene always feels like an accomplishment :D reply ojl 2 hours agoparentprevI got the Swedish translation as a kid more than 40 years ago and also spent plenty of time trying to find the goldbug. Still have the book and my kids loved it as well. reply balls187 31 minutes agoprevMost likely know this, but in many books there is a gold bug to be found on each image set. reply divbzero 1 hour agoprevCars and Trucks and Things That Go is a child favorite and What Do People Do All Day has great depictions of saw mills, flour mills, power plants, and more, but I like Richard Scarry’s I Am a Bunny the most: The pages are filled edge-to-edge with his vibrant illustrations (no whitespace as in his other books) and the story by Ole Risom describes the timeless passing of the seasons. reply doright 4 hours agoprevI was most familiar with the DOS games as a child. They even had Red Book soundtracks of the townsfolk singing about various professions. Building the house was my favorite activity back then. https://archive.org/details/BusytownDOS https://archive.org/details/busytown_dos reply kridsdale3 1 hour agoparentThere was a sound clip in one of these that became a long lasting meme in my family, invoked for years and years whenever someone got hurt. \"PUT A BANDAGE ON IT\" reply camtarn 38 minutes agoparentprevI had no idea that there were Busytown games! Very cool, thank you for linking. reply syndeo 2 hours agoparentprevI have fond memories of the Sega Pico Busytown game, played that plenty as a little kid: https://www.youtube.com/watch?v=bm55P3ziI84 reply topherjaynes 4 hours agoprevRichard Scarry’s books were such a cozy universe growing up, I'd still like to live in \"What people do all day.\" I wonder what that book would look like if he made it today. reply 082349872349872 1 hour agoparentYou can (and it would't look that different). I left the Old Country for my adopted country in part because it reminded me of \"What do people do all day\" – although I was not conscious of the resemblance until my father, having seen the not-a-pig dude from the city coming around with a little vehicle specially outfitted to water various geraniums, pointed it out. reply mattkevan 3 hours agoparentprevYou might enjoy Business Town: https://welcometobusinesstown.tumblr.com reply orange_joe 3 hours agorootparentSeems like a totally different thing, very political and adult oriented. reply tivert 2 hours agorootparentAlso something about the characters gives me the cold, dead feel AI generated art (even though they do not appear to be AI generated). Can't put my finger on exactly what it is, though. reply kridsdale3 1 hour agorootparentprevI hate this. It seems like it was made by a Brooklyn hipster in a black and red plaid shirt with thick black framed glasses. reply aaronbrethorst 3 hours agoprevI loved Richard Scarry when I was little, although I think it instilled in me a belief that the world was in a constant state of chaos. (maybe that’s accurate.) My toddler is now obsessed with them and demands “CARS AND TRUCKS”, “SITE” or “HOUSE” every night at bedtime. reply dingaling 3 hours agoparent'Chaos' is a great observation. I only had a couple of Scarry books as a child but there was something dynamic about them compared to most children's books and I think it was the frenetic and chaotic nature of them. There was always a pickle barrel spilling over, or oranges rolling across a road. I didn't even know what a pickle barrel was, but it obviously made a mess when someone crashed into it. reply aaronbrethorst 3 hours agorootparentMr Frumble the apple man has dumped his load of apples on the road by mistake. Lowly thought he had the only apple (n.b. Lowly worm is driving his apple shaped car) on the road, but my isn’t he mistaken! reply euroderf 3 hours agorootparentprevThe overwhelming & pervasive chaos in all the illustrations kinda bugs me. I wonder if it puts the wrong ideas in a kid's head. Or else I'm just lamous. reply Tokkemon 2 hours agoprevIf you've never seen the animated TV series based on the books, you're totally missing out. It's incredible: https://www.youtube.com/watch?v=2m76NQMJGtc&list=PL66vbXJhfF... reply smusamashah 3 hours agoprevLooked at Richard Scarry's books on google images in hopes to find a book I read as a kid but don't remember its name or much of it. Richard's art is similarly colorful but Writing it here in hopes someone else may know? There were some blobby looking ice-cream/sundae in it. And a picture story where someone made very tall icecream. So tall that a helicopter was putting scoops on top (or may be it was placing the cherry on top, but i remember that helicopter touching that icecream). It was a colorful cartoony style art in the book. Having strong imagination as a kid, the pictures in this book always felt real (hard to explain). Then I remember looking at this book many years later as a grown up and not feeling them as real. It's been many many years since then and I want to have a glimpse again. reply influxed 2 hours agoparentLooked through all my kids Richard Scarry books and couldn't find that. But might be in Helicopters and Other Fun Things. Internet Archive has it but no logins to borrow books: https://archive.org/details/helicoptersother00newy/mode/thum... reply Multiplayer 4 hours agoprevI read his books every day when really young. He created such a great world to imagine. The note about him moving to Switzerland makes sense - his world did feel really European now that I think about it. My kids, now grown, did not get to experience his books - are his books sold widely anymore? reply showerst 4 hours agoparentHis books are still kicking around, although not as big as they used to be. My toddler has a few. They're still lovely books, but I think they're less popular mostly because they're a bit dated now; lots of obsolete jobs and few female animals doing any of the cool jobs. reply vehemenz 4 hours agorootparentI have a 5yo daughter, and I appreciate that *What Do People Do All Day?* have female characters in homemaker roles, which is still common in most of the world. Many of her other books have female protagonists doing everything else. Part of an education should be learning how other people live, whether it's in the past or in another part of the world. And of course, there's nothing wrong with being a homemaker today. reply tivert 3 hours agorootparent> I have a 5yo daughter, and I appreciate that What Do People Do All Day? have female characters in homemaker roles, which is still common in most of the world. Many of her other books have female protagonists doing everything else. Is that a new copy or an old one? The newer editions of Best Word Book Ever were updated to be less stereotypical (I think there's a gallery somewhere that shows the changes). Unfortunately the updated art is noticeably inferior (even to my untrained eye), and in some places kinda dumb (on one page I think they decided they needed more girls, so they phoned it in and slapped a big \"LISA\" on the shirt of some androgynous animal). Also they dropped A LOT of content, so the newer editions are something like half as long. reply vehemenz 1 hour agorootparentIf it's updated, then I couldn't tell. Everything seemed like 50s/60s in it to me. reply bitwize 3 hours agorootparentprevRichard Scarry's books are problematic because of the reasons you stated, and I think there were issues with ethnic and gender stereotypes as well (similar to issues with Dr. Seuss). He updated some of them in the 90s to reflect changing mores, but I don't know if those efforts reached all of them. reply edmundsauto 4 hours agoparentprevI have an infant and we have a number of his books. Some bought new. reply jansan 3 hours agoparentprevYes, you can buy them. When I was a child, my friend owned the \"Cars and Trucks and Things That Go\" book and I spent hours browsing the book by myself when visiting him. When I had children, it was one of the first books that I bought, and they loved it, too, although not quite as much as me. It is a great present for young children if you don't want to bring a Nintendo gift card. reply drummojg 1 hour agoprevI still remember looking at these with my mother when I was small. We loved to hunt for Gold Bug. She told me later in life that her favorite was, \"Lowly worm washes his face and foot.\" (He always wore a sock and shoe on the end of his tail.) reply throw4847285 3 hours agoprevOh hey, this is written by Chris Ware. It makes sense that he would be a huge fan of Scarry. reply mikeocool 3 hours agoprevMan I loved Richard Scary growing up. I initially thought my 1.5 year old didn’t quite have the attention span for Cars and Trucks and Things that Go yet, but recently, after learning the word bus, he picked it up and dutifully started going through each page and finding all of the buses. reply cafard 4 hours agoprevI remember the books as a staple of pediatricians' waiting rooms, and we had some at home, too. Scarry also published a book on how to draw things--cars, I recall, I suppose people and animals. reply topherjaynes 4 hours agoparentthat and the toy with wires and blocks you could move around. reply andrewstuart 8 minutes agoprevFor a very long time - maybe years - every night I would ask my little boy what he wanted me to read him at bedtime. ALWAYS he would say \"Robber Book!\" which was a Richard Scarry book that had a couple of robbers/burglars in it. In hindsight it was the cars he liked the most - cars are a major feature of Richard Scarry books and my little boy was car crazy from the moment he was born it seems. reply skibz 2 hours agoprevWhile I never read any of his books, I must have driven my parents mad with how much I watched The Busy World of Richard Scarry on VHS. reply bell-cot 4 hours agoprevA prior Richard Scarry item on HN, in Aug'23 - https://news.ycombinator.com/item?id=37044673 (47 comments) reply bitwize 2 hours agoprevRichard Scarry was a resident of my former hometown of Ridgefield, CT, as was fellow author-illustrator Maurice Sendak (Where the Wild Things Are). My school's librarian recommended his books with particular pride. Come to think of it, a lot of children's authors seem to have lived in that area, western CT or eastern upstate NY. Some, including Judy Hawes and Jean Van Leeuwen, came to visit my school to talk about their books, reading in general, etc. In college I referred to the textbooks used by Management Information Systems majors as \"Richard Scarry books\" because they were full of colorful, busy illustrations and light on technical detail. reply te_chris 1 hour agoprev [–] My (nearly) 2 year old reads Cars and Trucks etc. for ages on end like it's a sacred text. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Richard Scarry's children's books, such as \"Cars and Trucks and Things That Go,\" are celebrated for their whimsical illustrations and engaging narratives, offering an idealized view of the world.",
      "Adults often note the contrast between the books' depiction of fulfilling work and the real-life challenges of finding satisfying, well-paying jobs.",
      "Despite some updates over time, including changes to gender roles, Scarry's books continue to inspire joy and creativity in children, remaining a cherished part of childhood."
    ],
    "points": 161,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1730208295
  },
  {
    "id": 41974882,
    "title": "Standardizing Automotive Connectivity",
    "originLink": "https://www.tesla.com/en_CA/blog/standardizing-automotive-connectivity",
    "originBody": "",
    "commentLink": "https://news.ycombinator.com/item?id=41974882",
    "commentBody": "Standardizing Automotive Connectivity (tesla.com)149 points by hardikgupta 23 hours agohidepastfavorite165 comments Animats 22 hours agoThose look much like AMP automotive connectors. Using one kind of unkeyed connector in a car may be a bad idea. Things can be plugged in wrong during repairs. There's a lot to be said for making connectors not fit where they shouldn't. Especially in automotive, where many connectors are plugged in blind, by feel. This simplifies manufacturing at the cost of repair. If it can be plugged in wrong, it will be plugged in wrong. AAA put a battery in backwards in my Jeep once, and most of the vehicle electronics had to be replaced. reply elcritch 13 hours agoparentActually there’s two paths: first the traditional path of different unique wiring harnesses and various signals with unique connectors, or second a common power and coms bus design. If the Cybertrucks electrical ethos is followed by others there’s only 48V and Ethernet. Ethernet doesn’t care or fry if plugged into a wrong port. Any complex wiring can be done inside a part or component as needed, but the interface is one of a few options. Let’s say the window motor is plugged into the power and not the switched motor plug. As long as it’s a 48V motor it’ll just turn but not fry. You just unplug it and reconnect it. IMHO industrial everything should become 48V Ethernet. Just like for gadgets usb-c rules the roost. reply michaelt 7 hours agorootparent> If the Cybertrucks electrical ethos is followed by others there’s only 48V and Ethernet. I'm looking at the electrical schematic [1] and 'eth' appears 107 times while 'CAN' appears 805 times. And if you think about it, you probably don't want your brake-by-wire system to share a bus with your sound system and your trunk latch for obvious reasons. [1] https://service.tesla.com/docs/Cybertruck/ElectricalReferenc... reply elcritch 1 hour agorootparentCool, thanks! It looks like the cybertruck does extensively use CAN still. Tesla only talks up Ethernet. Makes sense, though I’m a bit bummed. > And if you think about it, you probably don't want your brake-by-wire system to share a bus with your sound system and your trunk latch for obvious reasons. Sharing a bus for those two wouldn’t make sense. However Ethernet topology wouldn’t preclude having those on separate buses and linked via switches. Though that’d pose problems with my view above about plugging anything in anywhere. Though it’s really more of a philosophical goal. ;) reply aeonik 6 hours agorootparentprevWhat are the obvious reasons? reply solarkraft 4 hours agorootparentDifferent quality of service requirements. The infotainment system is complex and it failing is just annoying while the brake failing could mean death. reply Glawen 6 hours agorootparentprevDenial of service of the bus due to a bug or electrical malfunction reply aeonik 6 hours agorootparentI thought the discussion was on context of CAN vs Ethernet. These two are still vulnerabilities with Ethernet. reply Aloisius 55 minutes agorootparentprevEven Cybertrucks aren't purely 48V and ethernet. There's a high voltage (800V) rail for high current devices like the AC compressor. There's redundant CAN for communication with things like the motors. There's a host of 12V, 16V, 5V components (door locks, lights, seat motors, etc.). They did switch many components to 48V, but not literally everything. reply themaninthedark 22 hours agoparentprevThese looked keyed to me. Aside from the bulge on the top of the connector where the lock attaches to it's mate there is a middle vane that extends 2/3 way down with a little bit of a J hook on it. reply cduzz 21 hours agorootparentI think by keyed it isn't just \"can't be plugged in upside down\" but also \"can't be plugged into the other plug that's basically the same but connects to some other subsystem.\" Ideally a system like this would let you select some per-subsystem physical lockout mechanism. reply themaninthedark 20 hours agorootparentThat level of keying would be done at the harness/electrical engineer level. You shouldn't put two same plug ends at one termination site. You really couldn't control it at plug or part level. reply theamk 15 hours agorootparentIt has to be both. The plug manufacturer has to provide many, many keying options (say for a 2-pin plug, one might have variants 2A, 2B, etc... which explicitly cannot mate with each other) An engineer designing the car has to ensure all plugs with same keying options are interchangeable (2A is for power supply; 2B for door switch; etc...) If the plug manufacturer does not provide enough keying options, this will be pretty hard to during design time. reply themaninthedark 1 hour agorootparentYou are generally not using just one plug style on a harness so most of the control is at the engineer level. Unless you are a big player, what plugs you are using boils down to what receptacle the component that you want to attach to use. reply taneq 20 hours agorootparentprevHave they said there isn’t a mechanism for individually keying connectors? There might be something optional, even if it’s just blanking pins. reply numpad0 15 hours agorootparentConnectors with distinct shapes and sizes are usually used. If it feels like it could go in then it's the right one, and if it goes in it easily seats fully and never comes out without a tool. Wrong connectors don't even feel like they could go into wrong locations. Cars using whole bunch of different connectors relying on whole bunch of suppliers is feature-not-bug situation. It is optimal for large scale volume production; work will be more distributed, SPoF will be more localized, etc. Standardized connectors with trivial visual differences and/or field configurable keying is a suboptimal solution for car problem. Usually. ...is it a local minima for small scale production? Are they having issues with scale outs, and therefore seeking downward scalability? reply taneq 8 hours agorootparentI work in this space and 100% agree, in fact I have been on a bit of a crusade to get the mechanical guys to stop designing parts that are rotationally symmetric for this exact reason. Letting a set of three parts be installed 17 different ways isn’t “keeping our options open” it’s “fucking up maintainers.” Each part should fit exactly one obvious place. Each connector should plug in at exactly one obvious place. The only exception is when it doesn’t matter where the part goes, which is approximately never. That said, sometimes there are cases where an entirely new connector style isn’t warranted, and that’s where you use blanking pins or adjustable keyways or whatever. reply Animats 21 hours agorootparentprevThat's the latch, not a key. Keying prevents putting the wrong plug in a receptacle. Usually that's done with notches on the receptacle and ridges on the plug. reply themaninthedark 21 hours agorootparentYou can use the ridge that the latch attaches to as part of the key structure. There is also a J-shaped structure inside the plug that would function as a key as well, prevents the plug from starting when rotated 180 degrees. == ------|| /------ reply jiveturkey 34 minutes agoparentprevNah, these look like keyed Deutsch DTM connectors. They are very obviously keyed, so not sure why you say unkeyed. This will never take hold. These are extremely expensive vs weatherpack and other cheap connectors. Auto manufacturers care about literal pennies. reply viraptor 17 hours agoparentprev> Using one kind of unkeyed connector in a car may be a bad idea. Things can be plugged in wrong during repairs. I tried to lower the window in a Ford shortly after leaving the dealer. It fried the lock. Somehow they were connected together by accident. I agree dumb wires should be hard to mix up like that - both via orientation and similarity. reply maxerickson 16 hours agoparentprevAt least reverse battery protection is pretty standard these days. Doesn't make things work when they are plugged into the wrong connector, but they should still work once the connections are straightened out. reply Animats 13 hours agorootparentThere's no excuse for that any more, especially since the invention of the ideal diode.[1] [1] https://www.ti.com/lit/an/slvae57b/slvae57b.pdf reply black6 21 hours agoparentprevOne of my takeaways from my time as a RATELO is the thought put into connector design. Everything in the Army was designed to plug into one thing and one thing only to prevent mistakes. reply cyberax 23 hours agoprevTesla really deserves some kudos for pushing the 48V architecture and the Ethernet-based communication. It drastically simplifies the wiring mess that is in a typical car. And it's not even close. Just watch the teardown of Cybertruck and compare its wiring to something like F150. reply throw0101d 22 hours agoparent> Tesla really deserves some kudos for pushing the 48V architecture and the Ethernet-based communication. For the record, 42V systems were experimented with in the 1990s: * https://en.wikipedia.org/wiki/42-volt_electrical_system In the 2011 German automakers agreed to 48V as the next step after 12V: * https://en.wikipedia.org/wiki/48-volt_electrical_system BMW was the first the first with Ethernet: * https://www.marvell.com/blogs/the-right-stuff-a-past-and-fut... reply cyberax 20 hours agorootparentThis has been a real head-scratcher for me. BMW and other automakers could have vastly improved their cars by switching to 48V a decade ago, but they still keep just plodding along with 12V. I have no explanation for this. reply m463 20 hours agorootparentI think it's like any existing vs new tradeoff. Tesla didn't have any existing, so their clean-slate math was clearly in favor of 100% new technology. (Well, they did have 12v existing in their other cars, but they were clean-slate in the truck.) reply phlipski 16 hours agorootparentprevIt's a classic chicken and egg economic problem. BMW doesn't make the chips/electronics that support the 48V architecture - Bosch & Continental (with NXP/TI/Infineon/Renesas as their silicon suppliers) do and they're not going to support 48V unless ALL (or a significant majority) of the automakers will. So it's a game of chicken. reply maxerickson 15 hours agorootparentThe semiconductors have been available for 20 years. reply michaelt 4 hours agorootparentEh, yes and no. I designed some stuff along these lines 15 years ago. At that time, 12 volt stuff was not just available, it was available with great economies of scale and a huge range of options, off the shelf. You need an automotive-qualified relay? A light? A solenoid? A DC-DC converter module? A fan? You'd have 100 choices at 12v, 30 choices at 24v and 3 choices at 48v. reply numpad0 19 hours agorootparentprevcare to define \"improve\"...? reply cyberax 19 hours agorootparentMore robust, cheaper (far simpler wiring harness), eventually ability to do zonal assembly. reply numpad0 19 hours agorootparentAre BMW cars less reliable, expensive, not ready for zonal assembly...? Major car components like doors or front axles are assembled in parallel to miscellaneous parts on the main body, and all .join() at the final assembly. This had been the case for past 30-50 years, possibly more, in case this needs to be said. reply cyberax 17 hours agorootparentThey can be even more reliable. And they definitely _are_ expensive. > Major car components like doors or front axles are assembled in parallel And doors (and tailgates) are the biggest body component that is _sometimes_ assembled independently. Then workers manually route cables through the body. Pre-routing cables inside panels that can then just be welded together can save a lot of labor. reply numpad0 15 hours agorootparent> And doors (and tailgates) are the biggest body component that is _sometimes_ assembled independently. Sometimes? What and when on Earth is this about? Pre-WWII? They wash and paint and dry the whole body at once _for paint consistency_, then take off doors and trunk lids and bumpers and send them into separate assembly lines. Those major parts flow parallel \"threads\" in sync and converge near the end, where connectors are plugged in and those major parts are bolted back in and plastic trims are pushed in to tuck everything under. Cars were basically always done that way for a long time everywhere. I think even lots of hand made supercars are like that, only except tact times are magnitudes longer. > Then workers manually route cables through the body. > Pre-routing cables inside panels that can then just be welded together can save a lot of labor. What do these even mean? Are you hallucinating workers crimping cables in-situ? They just clip on harnesses and plug in couplers in \"the line\". Never seen under a door trim? It sounds like you're either extremely ill-informed, or worse yet, potentially, intentionally misinformed about car manufacturing that what you see is advanced manufacturing. I think you should... look more closely into what \"legacy auto\" have been doing forever. reply cyberax 1 hour agorootparent> What do these even mean? Are you hallucinating workers crimping cables in-situ? They just clip on harnesses and plug in couplers in \"the line\". Never seen under a door trim? Workers still need to pull the wiring bundles through the car body and clip them, after the body is welded together. The connectors are impractically bulky to put several of them along the cable routes. Pre-assembled panels can have cable runs attached to them during the individual panel assembly. reply theshrike79 20 hours agorootparentprevTradition, long supply chains etc. Tooling all their shit to 48V is a massive undertaking with pretty much zero advantages. reply vardump 8 hours agorootparentLess copper in the car is not a \"zero advantage\". Cheaper and lighter. reply cyberax 19 hours agorootparentprevThey don't need to retool all the factories at once. They could have gradually introduced 48V systems in parallel with 12V, slowly phasing in new components as they replaced the old 12V. reply typewithrhythm 19 hours agorootparentConverting between voltages is not a free action, and running two systems is more complicated than one... You really need some special component that is much better at 48 for it to be worth it, otherwise a delayed platform switch is better; one some competitors have moved and the suppliers exist. reply cyberax 17 hours agorootparentStarter motor, power steering, heated seats, powerful headlights, power-hungry onboard computers. They _all_ benefit from 48V. reply shiroiushi 16 hours agorootparentOn top of all that, almost all the wiring in the car can be made thinner, because of the greatly reduced losses. This saves a bit of weight, but also a lot of cost because copper is expensive. reply potato3732842 8 hours agorootparentThere is no free lunch. You need to go to a finer wire strand, better insulation, better loom, better support for the harness etc, if you want that super fine wire to last. Some of those have pretty direct labor cost impacts too. That's gonna kill a lot of your cost savings, especially at lower production volumes where the design cost is harder to amortize. There's no free lunch. reply msandford 6 hours agorootparent> better insulation It's very, very hard to get insulation that's not good for at least 100V and I suspect that just about any generic wire is good for more like 300V. The only exception that comes to mind is wire that's specifically for \"household low voltage\" like 24V AC for thermostat, doorbell, sprinklers, landscape lighting. Also normal ethernet. But these are almost all what you'd call signalling wiring rather than power wiring. Your average hook-up wire that you could buy at the auto parts store to make some repairs is almost certainly rated for 300V already. Mostly because of chafe resistance. Wikipedia says that the dielectric breakdown strength of PVC is 40 millions volts per meter https://en.wikipedia.org/wiki/Polyvinyl_chloride. Divide both sides by 1 million and you get 40 volts per micron. OK so you need 1/3 of a micron to insulate enough for 12V and you need 1.25 microns for 48V. Now let's have a reasonable safety factor of say 10 or so and we're looking at 3 microns vs 12.5 microns. The only wire I can think of that might have insulation that thin is enamel coated magnet wire for the inside of motor windings. But even that is probably thicker. Any kind of plastic insulation is going to be significantly thicker than this just to be able to be coated onto the bare copper wire and stick. You're not wrong that the insulation needs to be thicker as the voltage goes higher. But you're unaware of just how ridiculously over-insulated everything already is due to other constraints of manufacture. reply potato3732842 6 hours agorootparentI was talking about the structural side of things, not the electrical side. I thought this was fairly obvious but I guess not. reply numpad0 16 hours agorootparentprevI'm sorry but the other comment is more correct. 48V standard was originally created for mild hybrid systems for ICEs during mid-2000s as a stopgap solution to full hybrid transition. Looks like the earliest mass-production 48V-class system was a 2001 Toyota that ran at 36V. The integrated starter generator(ISG) is usually a pancake shaped motor that replaces clutch/torque converter in ICE car, nothing like the regular starter motor. MHV was not even real hybrid, and is no longer relevant, so was 48V, at least for a while. reply Kirby64 18 hours agorootparentprevThe special component was supposed to be the starter. With start stop systems essentially mandatory, the starter runs much much more often and therefore wiring savings on the starter are pretty useful… reply iknowstuff 21 hours agorootparentprevnone of them actually shipped a mass market almost exclusively 48V product if you’re trying to imply tesla’s push isnt as massive as it is. reply MuffinFlavored 18 hours agorootparentprev> BMW was the first the first with Ethernet: BMW ENET is non-standard, DoIP is standard. :) reply outworlder 23 hours agoparentprevWe can simplify the wire mess today by using more CAN bus devices. What is ethernet bringing to the picture? reply cyberax 22 hours agorootparentCAN is slow. At best it's around 1Mbit, but you get into electrical limitations. So you have to run multiple CAN buses in parallel and carefully manage bandwidth limitations. My Chevy Volt had 4 different CAN buses and one additional LIN bus. This can all be replaced with just two Ethernet buses: for safety-critical and non-critical uses. And the gigabit speed provides plenty of bandwidth for any reasonable sensor traffic, even including camera feeds. The current architecture was justified in 90-s when LIN PHYs were an order of magnitude cheaper than even CAN PHYs. Now Gigabit Ethernet PHYs cost less than a dollar. reply eschneider 22 hours agorootparentIt's unlikely that the multiple CAN buses are being used to increase speed by, say multiplexing them. In general, vehicles use multiple CAN buses for enhanced security. For example: things like diagnostic ports are often on their own CAN buses so data can't be directly injected into onboard systems. reply cyberax 21 hours agorootparentAll but one CAN bus in my Volt were connected to the OBD port. The unconnected bus controlled the high-voltage battery contactors and some other critical stuff. The \"main\" bus was saturated with data, more than 80% of bandwidth utilization at 512kbs. And it kinda had a mix of everything, from street names to be displayed on the dashboard to ECU messages. The other two buses had some random messages, with no rhyme or reason for the split ( https://vehicle-reverse-engineering.fandom.com/wiki/GM_Volt ). reply KK7NIL 23 hours agorootparentprev> What is ethernet bringing to the picture? Over 3 orders of magnitude faster datarates. CAN FD: up to 5Mb/s Automotive Ethernet: up to 10 Gb/s reply gregoriol 8 hours agorootparentWhy would you need 10 Gb/s speeds in a car reply nunez 5 hours agorootparentFSD will benefit from high-resolution (4K or above) camera feeds (for things like reading signs and detecting small obstacles). You can do this in a 10Gbps network and have tons of headroom for every other function the car will perform. reply the_mitsuhiko 8 hours agorootparentprevWhy would you not? Tesla is sending even the infortainment data stream through that bus. It's incredibly helpful having all data travel on a singular wire because you can tap in at one point and read it all out. Makes the entire system significantly easier to debug, understand and develop against. reply gregoriol 6 hours agorootparentAnd hack/steal, right? reply vardump 8 hours agorootparentprev10 Gb/s is not all that much for cameras. Enough for one 60 Hz 4k 14-bit depth camera transmitting raw bayer data. 60 Hz * 3840 * 2160 * 14bit is 6.96 Gbps. reply dpeckett 7 hours agorootparentIt's a good thing we invented video compression and hardware codecs/encoders a long time ago. What you'll actually be sending is a high bitrate mpeg stream, probably 54Mbps or thereabouts, you could probably fit 50x camera streams on a shared 10Gbps bus. reply throw0101d 22 hours agorootparentprev> What is ethernet bringing to the picture? More speed and zonal architecture: * https://www.electronicdesign.com/markets/automotive/article/... * https://www.bosch-mobility.com/en/solutions/control-units/zo... reply m463 19 hours agorootparentprevI'll bet a hidden factor here is development and testing. They can probably develop for a car ethernet lan with a desktop pc and car \"peripherals\". Not that there aren't canbus cards for pcs, but still. reply numpad0 18 hours agorootparentThis sounds like the real answer. Replacing an automotive standard with Ethernet is going to reduce friction onboarding junior webdevs with MacBooks, and enable a more stable higher turnover labor intensive organization. reply maxerickson 16 hours agorootparentCAN to PC adapters are a few hundred dollars, it isn't causing much friction. reply m463 9 hours agorootparentWith ethernet you can probably run the car software in containers. I can imagine a container to simulate each hardware unit, a small inter-contaner lan, and develop code that way. reply dpeckett 7 hours agorootparentYou can already do this trivially with Linux vcan[1] so I don't buy this argument. I think the bigger factor is that innovation in the CAN ecosystem has been lagging behind Ethernet for decades now. Only reason it's had such staying power is industry inertia. 1. https://netmodule-linux.readthedocs.io/en/latest/howto/can.h... reply soco 8 hours agorootparentprevReminds me when my Opel resetted itself while driving on the highway. Oh, the adrenaline... reply the_mitsuhiko 23 hours agorootparentprevCentral bus instead of many point to point connections. Look at how much fewer cabling the cybertruck has. reply rcxdude 18 hours agorootparentCAN is also a bus, that's not really a point in favor of ethernet. reply elcritch 13 hours agorootparentEthernet can do both bus and switched. High speed switches enables a lot of architecture not easily enabled by CAN. reply JoshTriplett 19 hours agorootparentprevCAN has desirable electrical properties (e.g. hardware-level prioritization) if you have life-critical devices and non-life-critical devices on the same network. But it's painful to deal with from a software point of view, compared to IP-based protocols, for anything that doesn't require the properties of CAN. reply cyberax 16 hours agorootparentIndustrial Ethernet (PROFINET) also has priorities and bandwidth reservation: https://en.wikipedia.org/wiki/Profinet#Technology_of_Class_C... reply bsder 20 hours agorootparentprevLarger packets and bandwidths. Bandwidth: you can't ship backup camera video or entertainment system audio over CAN, for example. CAN was meant for short, real-time packets. 8 bytes in initial configuration. CAN FD allows 64 byte packets. You spend a LOT of protocol doing packet fragmentation and assembly using CAN--which then negates a lot of the real-time guarantees. CAN should be used for the short safety critical stuff. Ethernet should be used for everything else. reply elcritch 12 hours agorootparentEthernet can handle real time now even in bus configurations! 10BASE-T1S is a new standard geared for automotive. It uses physical layer collision avoidance instead of classic Ethernet exponential backoff. This provides deterministic maximum latency. Though you can get max latency guarantees with switched Ethernet and the appropriate switch QoS and hardware. reply aeternum 22 hours agorootparentprevBitrate and max devices for CAN are limited. reply mulmen 23 hours agorootparentprevBandwidth? reply jeffbee 20 hours agoparentprevHow about comparing it to a teardown of a Rivian? Rivian uses 2-wire ethernet but 12V, so it seems like a more interesting comparison. That said, it all seems like inside baseball to me. The BMW 850i pioneered the CAN bus, but that car was forgettable and although CAN bus took over the car industry that did not seem to create any durable advantages for BMW. Ethernet seems like the inevitable replacement for CAN, in light of VW's investment in Rivian, and 48V vs. 12V for the low-voltage systems seems like a wash. reply saturn8601 21 hours agoparentprevYou'll be sorry when this move further hamper repair efforts and the TCO of owning a vehicle goes up. reply porphyra 20 hours agorootparentYou can say that about any new standard. With that logic we'd all be stuck with knob and tube wiring. reply ElevenLathe 20 hours agorootparentDid cars ever use knob and tube? reply porphyra 20 hours agorootparentNot that I know of --- it's a figure of speech to compare it to an standardized method of electrical wiring in buildings that eventually got replaced with better standards despite similar concerns at the time. reply bluGill 19 hours agorootparentprevWell technically a spark plug is the tube part. reply Aloisius 23 hours agoprevHow is this better than all the other 48V connectors out there (MX150, MCON, PP, etc.)? Surely it isn't just that they reduced the number of connectors since one could have just standardized on a subset of mass-produced connectors by molex, te, etc. instead. reply riskable 22 hours agoparentI'd like to know this too. Their website claims \"cost\" but doesn't actually list the costs of the new connectors compared to existing ones. At least give us some comparisons! reply xpe 23 hours agoprevJust to clarify, the linked article about the LVCS connector is for internal electrical cabling, not for electric-vehicle charging. For Tesla's charging standard (SAE J3400), see https://en.wikipedia.org/wiki/North_American_Charging_System reply xyst 23 hours agoprev> Today a single vehicle typically requires over 200 connections—and the number of electrical I initially thought this was referring to the connector between the charge port on the EV and the charger base station. Had to think for a second and realized it’s the electrical connections between the various components in an EV. Glad to know I don’t have to carry 200+ dongles in case I buy an EV. reply HPsquared 23 hours agoparentThat must be only the high voltage connections, right? reply atonse 22 hours agorootparentThese are talking about 48V, so my guess is it's for all the little bits (window motors, wipers, all the internal electronics, switches, turn signal lights, etc etc) reply HPsquared 19 hours agorootparentTo be honest I'm kind of surprised by the low number. Though I guess each connector has two sides, and they can have tens of pins per connector on the bigger ones. reply connicpu 21 hours agorootparentprev(Semi-permanent) High voltage connections are usually made with spec-torqued bolts, not plastic connectors. These are for the \"low voltage\" ( Tesla invites all device suppliers and vehicle manufacturers to join us in this initiative. This is not a standard in the sense that engineers use the term. Tesla is hoping it will be adopted as a standard and since Tesla doesn't appear to want to involve any standards bodies, Tesla appears to only be interested in making these connectors a de-facto standard. In any case, that is not a standard. reply electriclove 23 hours agoparentI’m not sad they didn’t work with a hundred companies and take tens of years and still have nothing to show for it. They are doing what they need done for their business and then inviting others to join. And way earlier than they did with NACS: https://www.tesla.com/blog/opening-north-american-charging-s... reply yegle 22 hours agorootparentHuh I noticed this section in the linked NACS post: > As NACS is now recognized in a SAE recommended practice (RP) under SAE J3400, we have removed the technical specifications and CAD from our website. So something that was previously freely available now requires a $300 payment to access. I'm sad to see that. reply MichaelZuo 22 hours agorootparentI don’t think the SAE has the authority to remove anything from the public domain? It likely is still freely shareable for existing copies. reply throwaway19972 22 hours agorootparentprev> I’m not sad they didn’t work with a hundred companies and take tens of years and still have nothing to show for it. what a ridiculous counterfactual. We have plenty of wildly successful standards that weren't just thrown at consumers and called a standard. reply saturn8601 21 hours agorootparentIDK man, when I think of standard connectors I think of clunky junk: CCS which was all engineering and no focus on human centered design with its clunky connector that also wiggles in ports. all of the USB connectors including USB-C, with it mandate to support so many different edge cases that cause cables to not always be compatible with each other defeating the purpose. Bluetooth again with so many edge cases that made it terrible until Apple came along and cut a lot of that out in their solution finally made it tolerable. Hell even a lot of electrical connectors (such as the US outlet) suck: developed in that way due to historical interests, it looks terrible, is not entirely safe (ie. ground does not go in first) and now has stuff bolted on to make up for its shortfalls. (GFCI, in line fuses etc.) Now there are probably loads of terrible proprietary connectors but it seems like the free market eventually takes care of disposing of the chaff. That itself is a forcing function to get to a better design that users will like. Whereas you have no choice of a standardized connector because some \"standards body\" made up of opposing interests artificially keeps lousy designs around and forces it upon the population. Im not arguing for one or the other but its just annoying that standards bodies always seem to get a pass when in my experience they produce a lot of mediocre stuff. reply binoct 18 hours agorootparentThe points of standards is that they solve one or more problems for many constituents well enough so that all adopters gain in things like supply chain, design ease, and interoperability. They are rarely going to be optimal for every specific use case. They also often derive from specific designs by a specific company Adding a standards body into the mix is going to add complexity to the process by definition, but shouldn't be taken as a default \"bad\", since there are tangible benefits to non-corporation-managed standards. Otherwise they wouldn't exist. reply kortilla 14 hours agorootparentprevName the “wildly successful” standards you are thinking of and then look into the history of them. You’ll find one or maybe two major players that pushed it initially. reply throwaway19972 14 hours agorootparentOk? I'd rather trust DARPA than private enterprise. reply Ajedi32 22 hours agorootparentprevYeah, there are advantages to the \"just do it\" approach to standardization vs design by committee. A lot of web technologies started out that way (arguably most of them actually). Both approaches are valid. reply vel0city 23 hours agoparentprevI agree. Is this some kind of free licensing for these connector standards or are they still behind the \"you can use this but if we rip off any of your IP and you sue us we'll revoke your license and sue you\" license? reply kotaKat 5 hours agorootparentThey have a \"patent pledge\" for their patented parts, at least -- they will \"not initiate patent lawsuits against anyone who, in good faith, wants to use our technology\". https://www.tesla.com/legal/additional-resources#patent-pled... reply vel0city 5 hours agorootparentYeah, and they define good faith in that document as: A party is \"acting in good faith\" for so long as such party and its related or affiliated companies have not: asserted, helped others assert or had a financial stake in any assertion of (i) any patent or other intellectual property right against Tesla or (ii) any patent right against a third party for its use of technologies relating to electric vehicles or related equipment; So, if another company rips off your IP but Tesla doesn't think it is a \"knock-off product\", you sue that other company, you're now in violation of Tesla's \"patent pledge\". Its an attempt to use a carrot of Tesla's patents to make all the other rightsholders essentially give up all their IP. If you sue anyone protecting your EV IP, you're in violation of this agreement and will be open to litigation by Tesla. reply elchananHaas 23 hours agoparentprevThere are different types of standards. Car design teams are big organizations; internal standards can help reduce the development effort. Tesla needs to coordinate with their suppliers, so sharing this helps even if it isn't used by other companies. I think we should give Tesla the benefit of the doubt for now. Harmful use of patents could cause issues, but this has potential. We will simply see if other companies are interested, and if they are it can go from internal standard to de facto standard to formalized standard. reply jsight 22 hours agoparentprevDon't think of it as a standard as-in SAE, but a standard as in \"molex\", \"AT\", or \"ATX\". Yeah, they aren't really \"standards\", but they aren't exactly proprietary either and they also are clearly useful. The goal seems to be to promote reuse of a good-enough design in as many places as possible. Noone's forced to use it, but it'd make things simpler for everyone if there is as much commonality as possible. reply hardikgupta 22 hours agoparentprevThat's fair. Should have said \"Tesla proposes new standard electrical connector\". reply foxglacier 23 hours agoparentprevIt's going to be a standard within Tesla, so in that way it's a standard. It sounds like they anticipate benefitting from cost reduction themselves even if nobody else uses it. reply kortilla 14 hours agoparentprevMost useful standards did not originate from standards bodies. The standards bodies just formalized what already had buy-in from the significant players. reply instagraham 23 hours agoparentprevRelevant xkcd, though one imagines that successful standards form despite the lack of choice in how standards are formed https://xkcd.com/927/ reply Someone1234 23 hours agorootparentThe XKCD doesn't really apply here, since intra-vehicle 48v is kind of unexplored, so there aren't multiple competing standards for that in particular. I do agree that this isn't a real open and free standard however. edit: Inter to Intra. reply duskwuff 23 hours agorootparentNitpick: intra-vehicle. Inter-vehicle power would be pretty weird. reply vaillancourtmax 22 hours agorootparentI think they meant \"across manufacturers\". reply SideburnsOfDoom 22 hours agorootparentprev> Inter-vehicle power would be pretty weird. That's entirely possible at present. Many electric vehicles can send power out to power appliances. It's called \"Vehicle to Load\" or \"V2L\". And electric vehicles can slow-charge off a wall power socket, so they could get that from V2L. It won't be a common use, but it would work in a pinch to get you enough juice to get to a better charger? reply jsight 22 hours agorootparentLucid supports this directly too: https://lucidmotors.com/stories/introducing-rangexchangeInter-vehicle could be cool as the EV equivalent of jump-start This exists: https://news.ycombinator.com/item?id=41975736 But for Lucid, not Teslas. And also more generally as a use of V2L. > in-the-field recharge for cars that run out before the recharging station This exists: https://www.fleetnews.co.uk/news/latest-news/2023/06/30/elec... https://www.taxi-point.co.uk/post/rac-to-equip-breakdown-van... > a range-extending spare battery pack This is a real Tesla Cybertruck accessory: https://insideevs.com/news/706702/tesla-cybertruck-range-ext... reply reaperducer 22 hours agorootparentprevInter-vehicle power would be pretty weird. Sounds like jumper cables. reply duskwuff 22 hours agorootparentYou know what? You're right. That's literally jumper cables. But no, I don't think Tesla is doing those. reply SideburnsOfDoom 22 hours agorootparentTesla vehicles do not currently support V2L or V2G, although many other EV makes do. But it does seem that Tesla is planning to do those some time: https://cleantechnica.com/2023/08/19/tesla-plans-to-adopt-bi... https://thedriven.io/2024/05/06/teslas-take-on-v2g-controlli... reply Pet_Ant 22 hours agorootparentprevHonestly that would be awesome. I still dream of autonomous vehicles auto-convoy and link up for efficiency. Just quietly become a train as needed. reply SideburnsOfDoom 10 hours agorootparentWhy is it that when describing EV technology, HN people have a tendency to frame it as \"imagine if it could, that would be amazing if\" While describing stuff that exists in some form, and usually has existed for years already now. It's not evenly distributed new tech for sure (1). But maybe it's the false assumption that \"if it was anywhere, I'd be among the ones to see it early\". 1) See William Gibson: \"The Future is Already Here, it's Just Not Very Evenly Distributed.\" reply moralestapia 22 hours agoparentprevIf you want to be pedantic, What is a \"standard\" then? Does it need to have an ISO seal? reply hoseja 7 hours agoparentprevTell us about NACS. reply 1234letshaveatw 23 hours agoparentprevlike NACS ? reply unsnap_biceps 23 hours agoprev> The 48V architecture is the optimal long-term choice, requiring ¼ of the current to deliver the same amount of power. Is there a reason why 48V is better long term than going higher like 96V? reply cyberax 23 hours agoparentOther people already mentioned safety. But you can actually go to 96V if needed by running a cable with -48V if you need extra power. One another advantage of the new 48V architecture is that it doesn't depend on the car body for the current return path. This opens up possibilities of adding sensors that detect current leakage, to pinpoint areas with defective wiring and/or components. reply zaxomi 23 hours agoparentprevDepending on the country you live in, the laws might allow you to do work on equipment that is below 50 volts, but require you to be a certified electrician for anything above that. reply two_handfuls 23 hours agoparentprevI believe 50V is a safety limit. reply cptcobalt 23 hours agorootparentBelow 50v is considered \"low voltage\". Higher voltage would require different safety considerations. reply outworlder 23 hours agorootparentprevAnd that's because of your skin resistance. Around 50V is when voltage starts to overcome it. reply rcxdude 18 hours agorootparentusually. It's very possible to get a shock off of significantly lower voltages in bad conditions (very sweaty skin, for example). reply Plasmoid 23 hours agoparentprevAt some point, safety. A short/failure at 100V is much more dangerous than at 50V. Both from a fire-safety perspective as well as an electrocution risk perspective. reply hmottestad 23 hours agoparentprev48v is probably more common than 96v in general, so more components available already. reply matrix2003 23 hours agorootparentAs mentioned in another comment, it’s also close to the safety limit for low voltage systems. IMO solar pioneered (in recent history) 48V DC systems, which is an easy multiple of 12V to stay below the 50V “high voltage” safety threshold. It allowed people to use smaller gauge wire and chain together multiple 12V batteries that are readily available. reply bryanlarsen 22 hours agorootparentTelephony has been using -48V as a line voltage for a very long time, probably > 100 years. reply matrix2003 19 hours agorootparentIs that for power deliver, or signaling? reply p_l 17 hours agorootparent-48VDC was standard for powering telephone equipment, and is also common standard for direct current datacenter connections. Signaling on POTS easily hit over 100V, btw. reply amanaplanacanal 9 hours agorootparentI remember getting zapped when the phone rang when I was a teenager back in the 70's. Good times. reply Kirby64 18 hours agorootparentprevBoth. It powers old school telephones. reply 8note 23 hours agorootparentprevFor which - why standardize around this connector and not XLR, which is the first thing that comes to mind for 48V? Too big/bulky? reply zaxomi 22 hours agorootparentThe phantom power at 48 volts used with XLR connectors only have a current at about 10 milliampere. Enough to supply power to a little microphone. The connector is bulky and of metal, and designed to be used inside. It's also expensive compared to other connectors. There are a lot of cheaper, more suitable connectors, designed to carry power. reply FriedPickles 23 hours agoprev> LVCS...is available in industry-standard light blue Is this tongue-in-cheek, or is there a reason manufacturers care about the color? reply dtparr 23 hours agoparentIn various industries, colors are used to differentiate between various voltages so it's obvious whether you're working with high voltage, low voltage, etc. so you can determine the necessary precautions for whatever you're dealing with (and some will also make the connectors incompatible so you can't accidentally join high and low voltage sets of wiring). I'm not super familiar with the automotive side, but I believe they use orange for high voltage and light blue for 48v. reply Aloisius 23 hours agoparentprevThey tend to color-code connectors in vehicles by voltage for safety reasons. Light blue is used for 48V. reply plorg 18 hours agorootparentMeanwhile 12V connectors turn up in a wide variety of grey, where different colors might just mean the connector is keyed differently. reply 7thpower 18 hours agoparentprevMid voltage (~48-60) uses light blue. Ideally you should be able to differentiate high voltage (orange, iirc), safety (yellow), mid voltage, and everything else. reply 7thpower 17 hours agoprevMolex and others may as well get behind this. EVs are driving all the content (as you can imagine, electric vehicles are way more profitable than IC not only because they have additional connectors but also because they tend to have more advanced in vehicle entertainment). The other reality is that all of the Chinese OEMs will generally work with the companies like Molex, TE, Amphenol, etc just long enough to let them shoulder the R&D cost and then reverse engineer and vertically integrate the part in their supply chain. If there is a chance to leverage their scale and supply chain to compete where there is known demand, it’s worth it to be early in and then be able to help OEMs customize the reference designs as needed. reply incorene 4 hours agoprevThere are already MANY standard types of connectors that automakers don't use consistently, why add another one to the mix? What does this offer that Deutsch or Weatherpak don't? XKCD summed this up pretty well: https://xkcd.com/927/ Besides that, I have no respect for Tesla. They can't engineer their way out of a paper bag, they are hostile to both the customer and the rest of the industry, and notoriously so in terms of repairability--why would I believe that now they suddenly care about designing a better, more universal connector? They don't even make repair parts available to the consumer! For those reasons and many more, the absolute LAST thing I would ever do as an engineer is to buy into a standard set by Tesla, or any other company run by Elon Musk. reply two_handfuls 23 hours agoprevWith the appropriate grain of salt due to the source, standardizing those power connections would probably be a good thing. Also, speeding up the adoption of 48V, the industry has been talking about it for so long! reply zaroth 21 hours agoparentGrain of salt due to the source? What is that supposed to mean? Tesla is so far ahead when it comes to these things (48V architecture), there literally is no other source in this case. reply sixQuarks 17 hours agorootparentThere is so much Elon and Tesla hate, these comments are so weird. reply sitkack 23 hours agoprevThey would have to 100% open this up and give it to standards bodies, otherwise it isn't a standard. The conformity tests and testing equipment designs and protocols should be freely available. reply duskwuff 23 hours agoparentFor that matter: who's going to be manufacturing and supplying these connectors? Tesla? I don't think so. reply themaninthedark 22 hours agorootparentGenerally a connector manufacturer would make these, Amphenol, Aptiv (formerly Delphi), Cinch, JAE, JST, Molex, TE Connectivity, Yazaki. See Mouser for some examples(https://www.mouser.com/c/connectors/automotive-connectors/#) Usually they create their own design so maybe having an open standard would allow you to do contract orders with any plastic injector that has the molds. reply Animats 21 hours agoprevWhat's the signal connection? Does this use CANbus? Ethernet? What? reply tapoxi 23 hours agoprevNACS is finally standardized as SAE J3400 with other vendors shipping those cars in the United States next year, and now they're introducing a new connector with no clear advantage? reply Someone1234 23 hours agoparentIf you clicked the link/read the article, you'd realize this is an internal vehicle 48v connector. reply tapoxi 23 hours agorootparentAhh thanks, I read it but In was confused why it was talking about 48v as if that couldn't be done by the other standards. reply sanex 19 hours agoprevSomeone's going to post this eventually may as well be me I guess. https://xkcd.com/927/ reply nimbius 23 hours agoprev [–] why not use the Chinese standard, GB/T? china has the most electric cars, the largest manufacturers, and the most advanced battery production on the planet. their experience with electric vehicles and charging would be a valuable leap forward. reply BrianGragg 22 hours agoparentThe GB/T is for charging and not in car connections. Tesla already has an arguably better connector NACS or J3400 now. As for China having the most electric cars on the planet. I don't feel that makes them the experts. China tends to steal / copy technology from other countries and has little innovation them self from my view point. They have the most EV's from heavy government subsidies. Tons of cars in graveyards over there. reply iknowstuff 21 hours agoparentprev [–] lmao are you a bot? this is not about charging connectors, and GB/T (with its two separate connectors for AC and DC) is awful compared to NACS. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Tesla is introducing a new 48V connector for internal vehicle wiring to simplify automotive connectivity and improve efficiency.",
      "Concerns are raised about the use of unkeyed connectors, which could lead to incorrect connections during vehicle repairs, contrasting with traditional unique connectors.",
      "The initiative also discusses the potential for Ethernet to replace CAN bus systems in vehicles, offering higher data rates and simplified wiring, though Tesla's 48V architecture is not yet a formal standard."
    ],
    "points": 149,
    "commentCount": 165,
    "retryCount": 0,
    "time": 1730142562
  },
  {
    "id": 41975741,
    "title": "Improving Xwayland window resizing",
    "originLink": "https://blog.vladzahorodnii.com/2024/10/28/improving-xwayland-window-resizing/",
    "originBody": "Improving Xwayland window resizing One of the quickest ways to determine whether particular application runs using Xwayland is to resize one of its windows and see how it behaves, for example While it can be handy for the debugging purposes, overall, it makes the Plasma Wayland session look less polished. So, one of the goals for 6.3 was to fix this visual glitch. This article will provide some background behind what caused the glitch and how we addressed it. Just in case, here’s the same application, which was shown in a screen cast above, but with the corresponding resizing fixes in: X11 frame synchronization protocol(s) On X11, all window changes typically take place immediately, including resizing. This can lead to some issues. For example, if a window is resized, it can take a while until the application repaints the window with the new size. What if the compositing manager decides to compose the screen in meanwhile? You’re likely going to see some sort of visual glitches, e.g. the window contents getting cropped or seeing parts of the window that have not been repainted yet. In order to address this issue, there exists an X11 protocol to synchronize window repaints during interactive resize. An application/client wishing to participate in this protocol needs to list _NET_WM_SYNC_REQUEST in the WM_PROTOCOLS property of the client window and also set the XID of the XSync counter in the _NET_WM_SYNC_REQUEST_COUNTER property. When the WM wants to resize the window, the following will happen: The window manager sends a _NET_WM_SYNC_REQUEST client message containing a serial that the client will need to put in the XSync counter after processing a ConfigureNotify event that will be generated after the window is resized. The compositing manager and the window manager will block window updates until the XSync request acknowledgement is received; The WM resizes the client window, for example by calling the xcb_configure_window() function; The client would then repaint the window with the new size and update the XSync counter with the serial that it had received in step 1; The window manager and the compositing manager unblock window updates after receiving receiving the XSync request acknowledgement. For example, now, the window can be repainted by the compositing manager and there shouldn’t be glitches as long as the client behaves well. Note that the window manager and the compositing manager are often the same. For example, both KWin and Mutter are compositing managers and window managers. The frame synchronization protocol described above is called basic frame synchronization protocol. There is also an extended frame synchronization protocol, but it is not standardized and it is implemented only by a few compositing managers. _NET_WM_SYNC_REQUEST and Xwayland KWin supports the basic frame synchronization protocol, so there should be no visual glitches when resizing X11 windows in the Plasma Wayland session, right? At quick glance, yes, but we forget about the most important detail: Wayland compositors don’t use XCompositeNameWindowPixmap() or xcb_composite_name_window_pixmap() to grab the contents of X11 windows, instead they rely on Xwayland attaching graphics buffers to wl_surface objects, so there is no strict order between the Wayland compositor receiving an XSync request acknowledgement and graphics buffers for the new window size. In order to help better understand the issue, let’s consider a concrete example. Assume that a window with geometry 0,0 100x100 is being resized by dragging its left edge. If the left edge is dragged 10px to the right, the following will happen: A _NET_WM_SYNC_REQUEST client message will be sent to the client containing the XSync counter serial that must be set after processing the ConfigureNotify event that will be generated after the Wayland compositor calls xcb_configure_window() with the new window size; The Wayland compositor calls xcb_configure_window() to actually resize the window; The client receives the sync request client message and the ConfigureNotify event, repaints the window, and acknowledges the sync request; The Wayland compositor receives the sync request acknowledgement and updates the window position to 10,0. But here is the problem, when the window position is updated to 10,0, it’s not guaranteed that the wl_surface associated with the X11 window has a buffer with the new window size, i.e. 90x100. It can take a while until Xwayland commits a graphics buffer with the right size. In meanwhile, the compositor could compose the next frame with the new window position, i.e. 10,0, but old surface size, i.e. 100x100. It would look as if the right window edge sticks out of the window decoration. After Xwayland attaches a buffer with the right size, the right window edge will correct itself. So, ideally, the Wayland compositor should update the window position after receiving the XSync request acknowledgement and Xwayland attaching a new graphics buffer to the wl_surface. With that in mind, the frame synchronization procedure looks as follows: The compositor blocks wl_surface commits by setting the _XWAYLAND_ALLOW_COMMITS property to 0 for the toplevel X11 window. This is needed to ensure the consistent order between XSync request acknowledgements and wl_surface commits. As long as the _XWAYLAND_ALLOW_COMMITS property is set to 0, Xwayland will not attempt to commit the wayland surface, for example attach a new graphics buffer after the client repaints the window; The compositor sends a _NET_WM_SYNC_REQUEST client message as before; The compositor resizes the client window as before; The client repaints the window and acknowledges the XSync request as before; After receiving the XSync acknowledgement, the compositor unblocks surface commits by setting the _XWAYLAND_ALLOW_COMMITS property to 1. Note that the window updates are still blocked, i.e. the window position is not updated yet; After Xwayland commits the wl_surface with a new graphics buffer, the window updates are unblocked, e.g. the window position is updated. The frame synchronization process looks more involved with Xwayland, but it is still manageable. _NET_WM_SYNC_REQUEST support in applications Most applications that use GTK and Qt support _NET_WM_SYNC_REQUEST, but there are applications that don’t participate in the frame synchronization protocol. If you use one of those apps, you will observe visual glitches during interactive resize. Closing words Frame synchronization is a difficult problem, and requires some very intricate code both on the compositor and the client side. But with the changes that we’ve made, I’m proud to say that KWin is one of the few compositors that properly handles frame synchronization for X11 windows on Wayland. I would also like to express many thanks to the Xwayland developers (Michel Dänzer and Olivier Fourdan) for helping and assisting us with fixing the glitch. Share this: Twitter Facebook Like this: Like Loading... Related Upcoming tablet input changes in Plasma Wayland September 20, 2024 In \"KDE\" Geometry handling in KWin/Wayland January 15, 2022 With 1 comment What’s cooking in KWin? Plasma 5.25 May 9, 2022 In \"KDE\" Posted on October 28, 2024October 29, 2024Author Vlad ZahorodniiCategories KDE, Wayland",
    "commentLink": "https://news.ycombinator.com/item?id=41975741",
    "commentBody": "Improving Xwayland window resizing (vladzahorodnii.com)140 points by TangerineDream 22 hours agohidepastfavorite155 comments Arainach 21 hours agoExcellent writeup, thanks for posting. I was involved with efforts to achieve similar things in Windows (Direct Composition, etc.). In practice, it was enlightening to debug various apps and frameworks and discover just how long repainting can take and how many edge cases there are around slow apps, hung apps, and so on. Who knows, maybe someone there still has some of the malicious test apps I wrote (using the language of this article, apps which acknowledge the sync request and then take multiple seconds to actually paint, etc., etc.). It's definitely a hard problem space that's basically incompatible with how most GUI apps and frameworks worked from 1985-2015. reply phkahler 5 hours agoparent>> malicious test apps I wrote (using the language of this article, apps which acknowledge the sync request and then take multiple seconds to actually paint, etc., etc.) This is one area where Wayland should do better. It's the compositors job to resize windows and IMHO draw their decorations. The compositor has a frame to paint and it can't have to wait on applications. Unfortunately some toolkits and DEs have decided that client side decorations should still be a thing. Also with this new partitioning of responsibility is my main annoyance: The DE needs to remember window placement for all my apps, since they are not allowed to know their environment under Wayland (for good reasons). reply Arainach 1 hour agorootparent>The compositor has a frame to paint and it can't have to wait on applications. This is the source of the visual artifacts this article is trying to prevent, however. Sure, you probably don't want to block resize for multiple seconds, but in general the compositor is very responsive. The app may not be. If you just let the window chrome resize as fast as you get, what do you do with the rest of the window? Leave it transparent? Draw a default black or white area? Both are very ugly and very noticeable in practice. This is the part of the problem space I know the most about - for a number of years I owned window chrome on Windows (don't blame me for the 1px border and way-too-subtle shadows, but after being overruled by PM/design, yes, I was responsible for implementing them). As far as custom decorations - that is a lost battle. Companies and apps want their special design language and will simply not build for your product/operating system/compositor/whatever if you don't give them that kind of support. Twitter and Facebook both want their specific shade of blue and their specific font. Adobe is....well, let's not talk about Adobe. Browser tabs in the title bar are a P0 requirement these days and anyone who doesn't support that will be laughed out of the room. Etc., etc. reply simcop2387 4 hours agorootparentprev> Also with this new partitioning of responsibility is my main annoyance: The DE needs to remember window placement for all my apps, since they are not allowed to know their environment under Wayland (for good reasons). https://gitlab.freedesktop.org/wayland/wayland-protocols/-/m... This might eventually get fixed, above is what I believe is the current proposal for handling this issue and letting apps do some amount of positioning without exposing things that a program shouldn't know about. That said this has had multiple proposals over the past 5-6 years at least and none have managed to make it all the way through. If you go through the previous ones (ext-placement and i forget the others) and ignore the angry messages involved it turns out that it's a very difficult problem to deal with in a way that isn't just a free-for-all with apps either not knowing about monitor placement, or having to handle so much detail about the displays that nothing will ever act consistently. That said, recent discussion on that latest one does look promising so maybe it'll finally happen. reply phkahler 3 hours agorootparentThat isn't really what I was hoping for. IMHO it is wasteful for every application to remember its window placement and restore it. Not only does that require additional code in every app, but if there is some kind of policy change there is no way to make it consistent. In other words, at the compositor level one could use the previous position as a default and have an option to do something different if another window is already there. It's simply not the applications job to place itself on the desktop. There are probably use case for such self positioning functionality, but restoring to the last position is not one of them. reply simcop2387 1 hour agorootparentThe application side of it is more for things like the multi-window setup of things like GIMP so that windows that are \"docked\" next to each other will stay that way past restarts. That's one of the reasons that the newer proposals are doing things with relative positioning between a zone or main window rather than allowing applications to place themselves randomly on whatever monitor or space that they want, interrupting whatever workflow is going on (which actually allows for security issues, i.e. a window pretending to be a password prompt putting itself on top of a browser or something to confuse the user). This also allows for new windows from the application to request that they're positioned next to any others so that related things stay together. This also apparently helps in a few cases where a single \"application\" to the user is actually multiple separate programs that get run by a main interface. Not as common in new software today but it used to be one of the ways that a lot of older software worked and there's still a decent amount out there apparently that are maintained that way. Keep in mind that this is also a request by the application, not a requirement of the compositor to obey it. If there's not sufficient space where the application requests things then the compositor can just ignore it and do what it believes makes sense. reply ack_complete 16 hours agoparentprevIMO, the neatest trick that the DWM does is avoiding jitter/lag between the mouse cursor and window when dragging a window. Sadly, DXGI flip model seems to have reintroduced some issues -- it is difficult to avoid the kind of jank shown in this article when resizing a window drawn using flip mode presentation. Which is unfortunate, since it's the highest performing mode. reply flohofwoe 9 hours agorootparentIt's kinda interesting that macOS doesn't even care about this problem anymore. When moving or resizing windows on macOS the hardware mouse cursor slightly trails ahead of the window, it's not very noticeable because of the 120Hz refresh rate (meaning the framebuffer surface spends less time waiting for presentation, thus reducing rendering lag) - OTH the lag is still visible, and this sort of minor visual imperfection is quite un-Apple-like (at least for 'old Apple'). It's a bit similar to how high resolution displays make fancy subpixel font rendering tricks obsolete (e.g. high display refresh rates making tricks to hide presentation latency obsolete). Sometimes hardware progress makes complex software solutions obsolete, but usually still with one or another tradeoff. reply delta_p_delta_x 7 hours agorootparent> It's a bit similar to how high resolution displays make fancy subpixel font rendering tricks obsolete Personally, I don't think so—subpixel font rendering still has value even in the densest displays, simply because it triples the available horizontal resolution. reply account42 3 hours agorootparentUnfortunately a low of newer displays (especially OLED ones) don't have regular horizontal RGB/BGR layouts anymore. I have seen both vertical subpixel layouts as well as triangle layouts or other funky subpixel arrangements. Even if you configure/update your software to those different subpixel layouts the result is not going to be as good. reply wongarsu 4 hours agorootparentprevBut if we continued using subpixel rendering then 1080p and 1440p displays wouldn't look as terrible, and we can't have that /s reply mfro 4 hours agorootparentprevIt's pretty noticeable on older apple hardware running newer macOS. Definitely a lot of compositing performance regression in Sequoia specifically. reply jchw 15 hours agorootparentprevThat trick is fun, but I wish Windows did a better job of synchronizing the visibility of the cursor sprite versus the presentation of the frames where the cursor is rendered in the frame. It's impossible for me to not notice the brief period where the cursor is missing! reply mouse_ 14 hours agorootparentI think it's cute. They're doing \"disable hardware cursor, enable software cursor\" instead of the other way around. Seeing that happen is somehow fun for me. reply jchw 36 minutes agorootparentI think they do both simultaneously, it's just that the cursor sprite/plane is overlaid during scan out and the software cursor is rendered in the next frame to be rendered by the compositor. The cursor sprite/plane can turn on or off pretty much mid-frame, whereas the frame that's about to be rendered could be two frames away from the one that's about to be scanned out. What you actually need to do is track when the first frame with the software cursor is about to be presented, then turn off the cursor sprite/plane. I think technically speaking all of the information needed to implement this is present somewhere, but I'm sure it's quite complicated to actually implement, possibly made worse by interface layering issues :) reply o11c 21 hours agoprevFor me, the quickest way to tell that I'm running on Wayland is seeing if multi-lingual keyboard input is broken. I have right-alt-while-pressed set to switch to a Greek layout (because who doesn't math?), and it reliably works on X11 but breaks in all sorts of weird ways under Wayland (sometimes it simply fails to enable/disable, sometimes it shows a notification as if it changed but switches back when input actually happens, ...). On the bright side I'm finally learning how Vim's digraphs work. reply tomxor 18 hours agoparentBoth Xorg and Wayland compositors use libinput, so it's probably more to do with lack of decent configuration support in your DE or WM. You can always configure libinput manually. Sway just lets you add libinput configs inline inside your sway config. [edit] This is probably not relevant (assuming correct wayland libinput config), since this is not where mult-lingual input transformations live. libinput just handles the physical keyboard mapping and behaviour. On more careful reading of the parent this sounds more like a buggy input method editor.. or maybe an issue with switching between X and wayland apps. reply mananaysiempre 18 hours agorootparentIs (simple, so e.g. non-CJK) multilingual input a libinput thing? I thought it was a libxkbcommon thing. reply tomxor 17 hours agorootparentNO! my mistake, I latched onto keyboard input and didn't think about multi-lingual. I'm guessing you mean things like pinyin, I think that's a dbus thing, which is still a thing on wayland but I don't really know anything about this... I'll get my coat. reply immibis 11 hours agorootparentprevThat still boils down to a Wayland design issue, since Wayland requires each individual DE/WM to get support for such features while X only needs them in Xorg. reply dahauns 7 hours agorootparentBut why are management and handling of language-dependent input methods supposed to be in Xorg? What happened to separation of concerns? \"Do one thing and do it well\"? reply asveikau 4 hours agorootparentMore like do a thing and do it once. If every WM and DE needs to do it, you have some that don't, or you have issues that are handled correctly by some DEs and not others, etc. reply dahauns 3 hours agorootparentWe're still talking about Linux, are we? Snark aside, it's still orthogonal to my question. I wasn't questioning whether it should be done centralized, but why it's supposed to be part of the display server protocol/compositor. reply account42 3 hours agorootparentBecause the \"display\" server is also responsible for window management and input. Does it have to be this way? Perhaps not - but seperating window management and input is also not trivial. Note that more complex input methods do somewhat bypass the display server and communicate via dbus instead. reply josefx 2 hours agorootparentprev> \"Do one thing and do it well\"? Did you miss the do it well part? Nearly every unix tool does more than one thing, the alternative would be a usability nightmare. * grep, which primarily does pattern matching has dozens of file traversal related flags that could be handled by calling it from find * find, a tool supposed to find files for some reason has dozens of flags related to executing applications which could probably be done by using xargs * did you know that xargs can do pattern matching and replacement on its input string? there are probably dozens of unix tools that are specialized for that reply zokier 6 hours agorootparentprevThat seems weird complaint about Wayland, Wayland requires every server to support these features, just like X11 also requires every server to support them? Sure, there weren't that many X11 server implementations so maybe it was not so apparent, but as a protocol they seem pretty much equal in this regard. reply asveikau 4 hours agorootparentThe criticism is that Wayland duplicates a lot of effort in multiple compositors. The compositor does work that happens in Xorg and in the WM. Whereas lots of common functionality gets implemented in Xorg that WMs and DEs don't need to duplicate. reply account42 3 hours agorootparentprevIt's not a weird complaint when on X11 when something is added it generally works for everyone whereas for Wayland the different implementators are still squabbling how to accomplish basic functionality people expect from their desktop computer. Or in other words, your comparison is like saying food and shit are basically the same thing because both are made up of similar elements. reply funcDropShadow 4 hours agorootparentprevBut wayland merges more components into one server than X11 did. Therefore, there is a need for more wayland server then there was a need for X11 servers. reply orangeboats 8 hours agorootparentprevX11 eventually punted the whole thing to each individual application. For input methods like Fcitx or IBus to work reliably on modern X11 programs, you need to set envvars like the following [0]: GTK_IM_MODULE=fcitx QT_IM_MODULE=fcitx SDL_IM_MODULE=fcitx [0]: https://wiki.archlinux.org/title/Fcitx5#IM_modules reply guappa 6 hours agorootparentprevXorg can use libinput but is not limited to that. reply sho_hn 16 hours agoparentprevHear, hear. KDE just elected improving the Input story in its bi-annual community wide goals election: https://kde.org/goals/ reply jandrese 1 hour agoparentprevAnother way to check if you're running on Wayland is to start some 3D accelerated application and see if the screen is all flickery. Apparently Wayland has some issue with not generating a frame sync that causes all variety of problems, but usually lots of flicker. reply porphyra 20 hours agoparentprevMultilingual keyboard input is such a mess haha. I need to type a lot of Chinese using pinyin and still haven't found a reasonably polished implementation on Wayland (Sway) that works on both Xwayland and true Wayland applications. So I reverted to i3 with ibus. reply przmk 11 hours agorootparentFor what it's worth, the latest 1.10 release of sway introduced support for IME popups. Maybe that would help reply pmontra 19 hours agoparentprevReally? My laptop has my national keyboard but I switch to USA for programming because of {} [] and all the other characters that can be typed with minimal use of extra keys. My fingers know the keys and the switch combination, maybe windows + space. I didn't expect Wayland not to have such basic functionality after so many years. But frames must be perfect, no tearing (is that the word?) reply o11c 19 hours agorootparentIt's possible that the \"permanently switch to next layout\" works reliably, but not the \"switch while held\" (which I do because I'm selecting single symbols at a time)? reply dietr1ch 17 hours agorootparentHave you tried using a compose key? I lost my custom mappings when switching from X to Wayland, but they work fine in both. I did lose my custom mappings though, but I only needed them when I was in emacs and obviously there's already a command for inserting weird stuff, so I just added a binding for it. reply o11c 17 hours agorootparentI use the compose key for compose stuff. It's a huge pain to actually write and install custom compositions so I don't bother anymore, just use the defaults (which is mostly annoying due to missing keypad variants, e.g. ± must be typed using the top of the keyboard). It looks like the default en_US.UTF-8/Compose includes mappings of the form: : \"α\" but to use that I'd have to figure out how to map a key to ``, and keyboard mappings that aren't in the standard checkboxes are such a pain. reply interroboink 15 hours agorootparent> It's a huge pain to actually write and install custom compositions Could you say more about this? IME, it's just a matter of adding lines to ~/.XCompose — is there something I'm missing? reply dietr1ch 14 hours agorootparentIs ~/.XComposed picked up anymore? At least it wasn't early on, and I just gave up on keeping it around. I guess it's up to each Wayland compositor, which calls for inconsistency :/ reply interroboink 14 hours agorootparentMy understanding is that generally, Wayland uses XKB, which respects ~/.XCompose But I think there are specific exceptions still, such as this Chromium bug: https://issues.chromium.org/issues/40272818 reply Izkata 4 hours agorootparentprevAny chance you typo'd the file name then, like you did here? reply dietr1ch 2 hours agorootparentTried it again and it works! I'm guessing that maybe I forgot to restart the machine to make sure everything got to read it. Or that whatever was broken long ago got fixed. BTW, I can't reach my backup right now, but this seems like a good start to build up custom mappings in case anyone gets interested in this, https://github.com/kragen/xcompose reply guappa 6 hours agorootparentprevIf you use Xorg sure. If you use wayland it can't be done. reply interroboink 1 hour agorootparentI think you're mistaken. You can use .XCompose on Wayland (modulo certain broken cases, perhaps). The Wayland book recommends XKB: https://wayland-book.com/seat/xkb.html It does come down to the libraries used by a given app, though (see sibling comment). reply account42 3 hours agorootparentprevEven with Xorg toolkit support has degraded quite a bit. For GTK it doesn't work by default and you need to set GTK_IM_MODULE=xim in your environment. Qt on the other hand picks up ~/.XCompose by default but truncates the compose result to one character. reply guappa 6 hours agorootparentprevcompose key on wayland is why I don't use wayland :D reply sph 4 hours agoprevHoping this makes Xwayland Emacs much smoother to resize. It currently resizes at 0.5 fps for me on KDE. If you're wondering why I don't use the native Wayland build of Emacs, it's because it is massively more laggy on HiDPI screens than the X11 version. I reported the issue upstream, probably won't be fixed for a decade when they'll decide to port to GTK4 and its hardware accelerated rendering backend. It took me a long while to notice that the typing lag I was experiencing was not because of native compilation or single-threading, but pgtk being a bit weak at rendering 3840x2160 pixels at 60 fps. In fact, it was not until I tried the Xwayland build that I experienced how much faster Emacs can feel. reply whywhywhywhy 18 hours agoprevHuge respect to any dev who goes out of their way to fix flickering/glitching/black regions when resizing or moving windows, I honestly can't stand it when I see it because I know it can be solved with effort. reply ryandrake 20 hours agoprevNice results. It's kind of wild that the \"before\" behavior would be considered acceptable/shippable by anyone. That kind of jank should be a show stopper. reply porphyra 20 hours agoparentLinux desktop is full of such jank. Fortunately, thanks to contributors like OP, the jank is gradually getting fixed. The whole point of Wayland is to eliminate a whole class of jank (screen tearing) that was essentially unfixable with X. reply o11c 20 hours agorootparentAnd yet, empirically, jank is far more common on Wayland than X11. With X11, on the rare case I noticed, it was usually just a matter of \"okay, which toolkit or driver do I need to look up the vsync environment variable for?\" I have no opposition to Wayland in theory; my concerns are entirely practical and unignorable. reply chaxor 1 hour agorootparentThis does not make sense at all as an argument really. I think your argument is about specific implementations of WM. While the argument of \"I deal with X11-based WMs because it's fine when I don't care about security at all\" may be valid in very narrow cases (such as air-gapped systems), the argument more generally is pretty weak. Its not surpising that x11 based WMs, such as the almighty [awesomeWM](https://github.com/awesomeWM/awesome), have more features implemented than, for instance, [jay](https://github.com/mahkoh/jay) due to the enormous time it has had to develop (though I am _very_ excited to see `jay` develop more fully, and expect it to be well used by the more tech-savy devs). However, some WMs in the Wayland space are doing quite well on that front. I recently had some substantial problems arise in my system which (surprisingly to me, but perhaps some are getting used to this) would have been prevented by using a memory safety language for my WM, so I have made the switch to (for better or worse) only ever consider Wayland+Rust WMs. In this space, [niri](https://github.com/YaLTeR/niri) is actually quite good, and to the point - it is developing correctly _and very quickly_. So, any issues on some WM not implementing some desired feature are quickly disappearing. IIRC, all the major 'gateway' linux distros, such as Ubuntu or Fedora, are all on Wayland by default now - so I don't imagine x11 will stay relevant much longer. reply bpye 17 hours agorootparentprevIn my experience Wayland has been a better experience than X11, I have two monitors with different refresh rates and I could never eliminate tearing with X11 whilst Wayland works as expected. reply account42 2 hours agorootparentI have not noticed any tearing issues with X11 for a long time, with or without multiple monitors. Were you using X.Org graphics drivers or external vendor blobs? reply bpye 1 hour agorootparentI had that issue with both AMD and Nvidia with the open source and proprietary drivers respectively. If I ran both at 60Hz it was fine, but my primary monitor is 144Hz and I couldn’t make 144Hz and 60Hz work well under X11. reply immibis 6 hours agorootparentprevWayland does some things well; X11 does some things well. Unfortunately Wayland is not a straightforward improvement. At some point they climbed the abstraction ivory tower and lost sight of the system they were supposed to implement. reply Flimm 12 hours agorootparentprevFor me, fractional scaling on a HiDPI screen is broken on X11, but works well in Wayland. Here's an example from yesterday: I was playing Factorio full-screen, and some of the GUI elements were too large and wouldn't fit it in the screen. I found a Factorio setting that enabled Wayland, and it instantly fixed the problem. Kudos to Factorio for supporting Wayland! You wouldn't think it was needed for a full screen game, but it was. reply timdiggerm 4 hours agorootparentprev> With X11, on the rare case I noticed, it was usually just a matter of \"okay, which toolkit or driver do I need to look up the vsync environment variable for?\" This is not something you should ever have to do reply porphyra 19 hours agorootparentprevYeah I have experienced all sorts of flickering and artifacts on Wayland, especially on nvidia. Recently it has gotten a lot better though. reply seba_dos1 18 hours agorootparentFWIW, Nvidia problems on Wayland were Nvidia's problems, not Wayland's. Wayland was just a victim of Nvidia's bugs and lacking implementations. reply p_l 17 hours agorootparentExcept for tearing, which turned out to be a combination of Mesa/DRM bugs and Wayland naively assuming the behaviour of those bugs. Then nVidia decided to switch to GBM/EGL way of doing things and it turned out everyone had incorrect assumptions... reply pmontra 12 hours agorootparentYou mean this https://www.phoronix.com/news/NVIDIA-EGL-Wayland-1.15 and the multy year story that I can guess from the comments in there? reply p_l 10 hours agorootparentExplicit Sync, yes. The reason why I consider previous implicit sync a bug is that it was essentially unfounded expectation - nothing guaranteed you'd get a magic mutex in the sky that synchronized renders for you, and in fact, you do not want one for maximum performance. Lack of explicit sync also essentially meant common multi-gpu setups would be broken. reply ChocolateGod 9 hours agorootparentAndroids graphics stack and drivers also use explicit sync, it isn't just a Nvidia thing. reply p_l 7 hours agorootparentI was not saying it's an nvidia thing, just that significant majority of devices used by wayland proponents had undeclared implicit sync issues that were invalidated when faced with a driver that didn't have them. reply kaba0 12 hours agorootparentprevHeh? That hasn’t been true on even the very first POC wayland compositor, Weston. I mean, it used to crash from time to time in the very early days, but visual artifacts? I don’t remember any, besides the occasional xwayland app (which is literally an X app running inside wayland). reply talldayo 18 hours agorootparentprev> And yet, empirically, jank is far more common on Wayland than X11 It depends on the type of jank you're talking about. It's wholly disingenuous to characterize Wayland as janky but x11 as not - the jank inherent to x11 is what made the original Xorg developers start making Wayland in the first place. Empirically, if x11 was perfectly fine there would be no motivation to design a successor. x11 gets the first-mover advantage of a lot of implementations and a straightforward design goal, but that's about it. It's not secure enough to recommend as a serious alternative to Mac and Windows users, and it's too slow and unopinionated to present a friendly face to new users. Features like 1:1 trackpad gestures, genuine HDR pipelines and a locked compositor framerate are all getting to the point that regular consumers expect them as default. If you want to keep using x11, it's unlikely someone is going to take it away from you. But it's on track for depreciation and hasn't been actively developed in years. Recommending it as a panacea to new users is a bad idea. reply lupusreal 8 hours agorootparent> It's not secure enough to recommend as a serious alternative to Mac and Windows users The security risk of X11 is theoretical, not practical. Yes, X11 programs can maliciously keylog each other, but this just isn't a thing that actually happens. And even if you do start installing random malware from the internet like a classic windows user, Wayland isn't going to prevent you from screwing yourself anyway. To actually be safe while installing and running malicious applications you need extensive sandboxing. Wayland can be one part of that sandboxing but is useless without the rest (to prevent the malware from stealing user files including credentials, using LD_PRELOAD hacks or similar to keylog other applications anyway, etc), and no distro suitable for recommending to Windows/MacOS newbs has the rest of the requisite sandboxing. The sandboxing touted by Wayland advocates is very esoteric and without all that sandboxing, a newb using Wayland has to exercise just as much caution when downloading software as if he were using X11. reply prmoustache 8 hours agorootparent> Yes, X11 programs can maliciously keylog each other, but this just isn't a thing that actually happens. Are you really saying keyloggers do not exist in the wild??? > Wayland can be one part of that sandboxing but is useless without the rest Yes but that is the point, and if you turn it the other way around X11 usually makes the rest of the sandboxing useless. reply immibis 6 hours agorootparentThere's no rule that an X11 server has to send all keypresses to all processes. reply talldayo 3 hours agorootparentBut the core framework of every x11 server relies on a loop that does poll HID connected to your machine for events. You can have an x11 server running with no windows connected and your keypresses will still be broadcast to any program that knows where to look. reply lupusreal 8 hours agorootparentprevI am saying that X11 users getting hit with keyloggers is extremely rare if it happens at all. It's trivial to make such a keyloggers but that doesn't mean it's common for the programs users install to do it. And yes, X11 makes that sandboxing useless, but that sandboxing isn't in play anyway because we're talking about noobs from Windows. reply prmoustache 7 hours agorootparentI think it is dangerous to feel yourself as superior. UNIX veterans can also be victim of supply chain attack, either from their distro package manager, a language module manager, a malicious flatpak with too much access, a trojaned appimage from a supposedly reputable vendor, etc. reply lupusreal 5 hours agorootparentSuperior? All I'm doing is sticking to programs in my distro's repos. Such programs including malicious keyloggers is unheard of. And if a maintainer turns evil or let's something slip through by accident, the damage they could do is hardly mitigated by Wayland. Thankfully, this virtually never happens In principle I am also vulnerable to something like a RCE zero day in Firefox turning an otherwise trusty program into malware which exploits X11's open nature, but again, this sort of thing actually happening is unheard of. I'm not superior, I'm just trying to keep a realistic grip on the threats I face. Modern security culture is fixated on what is theoretically possible, I care more about what is actually likely. reply talldayo 3 hours agorootparent> the damage they could do is hardly mitigated by Wayland. I disagree. How do you hijack interprocess communication on a Wayland device? I can tell you in very certain steps how to manipulate an x11 client but outside hijacking /dev/ I can't imagine a similar attack on Wayland. reply lupusreal 2 hours agorootparentIf you aren't sandboxes you can edit the user's environment to swap out their programs with backdoored copies. reply orangeboats 8 hours agorootparentprev> Yes, X11 programs can maliciously keylog each other, but this just isn't a thing that actually happens. A simple search leads me to this: https://github.com/anko/xkbcat There isn't a real attack using it yet, only because attacking Desktop Linux is a really unprofitable endeavor (considering the marketshare, the ROI must be very low). > To actually be safe while installing and running malicious applications you need extensive sandboxing FWIW, X11 is unsandboxable unless you run a second X server on top of your current server [0]. Which is fine, but you need to consider that most, if not all sandboxing solutions on Linux that \"newbs\" use, like Flatpak, do not employ such technique when running sandboxed X11 applications. The \"security by default\" behavior of Wayland limits the possible attack surface a lot, without requiring the end user to understand all the nitty details involved. [0]: https://wiki.archlinux.org/title/Bubblewrap#Sandboxing_X11 reply funcDropShadow 3 hours agorootparentWhy is X11 unsandboxable? A similar but reverse approach to Xwayland, something like waylandX could be used to be part of the overall sandbox approach to run untrusted applications. That would have the advantage that the severe restrictions and feature degradations of wayland are only applied to those untrusted sandboxed applications, not everything. reply orangeboats 1 hour agorootparentUltimately, X11 opens up everything. What you suggest (WaylandX) is essentially allow-by-default. When this is the case and there is a supply chain attack, what you think is a trusted application (and therefore not running under \"WaylandX\") can very well keylog you or take screenshots of your desktop without your consent. In a deny-by-default model ala Wayland, applications will have to ask for permissions before they can do something considered to be privileged. reply lupusreal 5 hours agorootparentprevYou're not telling me anything I don't already know and haven't already explained. X11 keyloggers are trivial, and virtually never seen in the wild. X11 makes sandboxing impossible, but that doesn't matter because I'm not going to waste my time on something like Qubes anyway, and newbs from Windows aren't being directed to setups like that either. They're all installing Mint or Ubuntu where the security of Wayland is nullified by the absense of sandboxing. reply orangeboats 4 hours agorootparent>newbs from Windows aren't being directed to setups like that either. They're all installing Mint or Ubuntu where the security of Wayland is nullified by the absense of sandboxing. This cannot be more further from the truth. Amongst the newcomers, it is rather popular nowadays for them to use Flatpak-bundled apps, especially with the rise of SteamOS (the Deck essentially) lots of Linux newcomers are in fact first exposed to Flatpak and running untrusted executables in a sandbox. And the most prominent \"untrusted executable\" today to those newcomers has to be Bottles, which is a nice GUI wrapper for Wine and is sandboxed (if you enable wine-wayland, of course). reply asveikau 3 hours agorootparentI wouldn't trust flatpak enough to run a truly untrusted executable. I am sure flatpak's isolation is full of holes unrelated to windowing. But I don't think a game purchased through steam counts as untrusted. reply orangeboats 1 hour agorootparent>I wouldn't trust flatpak enough to run a truly untrusted executable. I am sure flatpak's isolation is full of holes unrelated to windowing. As compared to running untrusted programs completely naked? >But I don't think a game purchased through steam counts as untrusted. Bottles is there for people to run any Win32 program, not just Steam games. And I shouldn't have to tell you how many malicious Win32 programs there are. reply asveikau 1 hour agorootparentJust google the criticisms of flatpak from a security perspective. They're out there. Containerization on Linux was never intended to be a security feature for totally untrusted, malicious code. It's isolation for trusted code. If your scenario relies on securely running untrusted executables in a Linux container you are doing stupid things. reply orangeboats 19 minutes agorootparentI am well aware of the weak points of Flatpak. But are you suggesting that running applications in a container is not more secure than running an executable completely naked? You see: If you want absolute security, for sure, go for a full-fledged VM! Or run something like QubesOS. It is a completely reasonable decision. However, malice certainly has degrees, and the \"mildly malicious\" programs most likely cannot take advantage of sandbox escaping exploits. If Flatpak can stop 95% of all attacks (relative to running a program completely without sandboxing), that is already a win in my book. But I will note again that X11 is a big hole (as in, almost a complete free-for-all) for sandbox escaping in Flatpak. asveikau 1 minute agorootparentYou seem to think a lot of things that aren't security boundaries are security boundaries. There have been VM escapes too. VMs are not for running untrusted OS images you get from end users. I'm done with this thread, have a nice day. lupusreal 2 hours agorootparentprevHas there ever been a single instance of a game from Steam including an X11 keyloggers? These threats are off in fantasy land. reply orangeboats 1 hour agorootparentI am not sure where you got the impression of me talking about only \"Steam games\". Bottles allows you to run any Win32. And besides that, \"these threats are off in fantasy land\" is an invalid defense in my opinion, considering the (quite sophisticated) XZ Utils backdoor happened not too long ago! Like I said, if such an attack towards X11 hasn't been deployed in the wild, it can only suggest such endeavor is unprofitable, not because the threats are fantastical. reply FeepingCreature 12 hours agorootparentprev> x11 gets the first-mover advantage of a lot of implementations and a straightforward design goal, but that's about it. It's not secure enough to recommend as a serious alternative to Mac and Windows users, and it's too slow and unopinionated to present a friendly face to new users. I've been recommending it as a serious alternative for years, and it's always presented a friendly face. > Features like 1:1 trackpad gestures, genuine HDR pipelines and a locked compositor framerate are all getting to the point that regular consumers expect them as default. I have never heard anyone not already a Linux user comment on even one of those as a problem. reply orangeboats 8 hours agorootparent>I have never heard anyone not already a Linux user comment on even one of those as a problem. Those specific words are uttered by Linux users, true. However, Linux beginners _do_ notice some X11 issues, it's just that very frequently they only know \"something\" is off but not why they feel so. From anecdotal experience, touchpad gestures are actually something my friend complained so YMMV. We ended up making a file in /etc/X11/xorg.conf.d/ to configure the synaptics driver. Another experience had to do with screen tearing, I helped them fix it by installing a compositor. reply FeepingCreature 3 hours agorootparentTo be fair, my anecdotal coworker evidence with Wayland has been \"the Webex client doesn't work\" with the ten-second remedy of showing them how to set their default desktop back to X11. reply orangeboats 1 hour agorootparentThat only tells us X11 has a first-mover advantage. Wayland supports screencasting too. reply Arainach 12 hours agorootparentprev>I have never heard anyone not already a Linux user comment on even one of those as a problem ....because Windows and Mac users have not had these problems since 2006? reply lupusreal 8 hours agorootparentThat's not what he said. reply funcDropShadow 3 hours agorootparentprev... while introducing lot's of other jank that was solved before. reply account42 2 hours agorootparentAnd once all the Wayland jank is fixed it's going to be just as \"crusty\" and \"full of hacks\" and \"unmaintainable\" as X is now and the cycle is going to repeat again :| reply nph278 2 hours agorootparentThe real jank was the friends we met along the way. reply sho_hn 16 hours agoparentprevI assume you know this, but just to be sure: The \"before\" state only happened in a fallback codepath when using a legacy application, i.e. an X11 app in the modern Wayland-based desktop. When using modern apps, or when using the entire desktop in legacy X11 mode, this doesn't happen. Most legacy X11 apps in active use are actually games which tend to be fullscreen and not resized, so it wouldn't happen that often for many users. (That said, sure, I can also respect the stance as I tend to place a premium on glitch-freeness too.) reply kevin_thibedeau 15 hours agorootparentThis didn't happen on Hummingbird Exceed or Xorg on Windows. It's not X's fault that rendering is wrong in the encapsulating windowing system. reply sho_hn 15 hours agorootparentNor did I claim it is. My comment was concerned with making it abundantly clear that this glitch happened in specific scenarios and not just any and all window resizing. reply bee_rider 17 hours agoparentprevMy window manager had the “before” behavior, but I complained it was defective and they gave it to me free. Not sure how much it would have cost beforehand, but I can’t complain after a 100% discount, right? reply bsder 20 hours agoparentprev\"Jank\" is still functional. And, to a first order approximation, nobody cares. Even Windows generally switches down to software rendering when resizing. And on mobile devices, nobody resizes. No pointer warp, however, is a failure (CAD packages all have significant issues on Wayland). Lack of multilingual support/accessibility is a failure. Lack of screenshots/screencasts is a failure. Lack of support for the BSD lineages is a failure. etc. People are still bitching up a storm because Wayland still (Pipewire does screen sharing using DBUS(!) for example) hasn't fixed basic things while DeadRat is shoving it down everybody's throat by dropping X11 support. The Wayland devs aren't wrong about the security implications of this kind of stuff. However, they're also not giving anybody solutions, either. One big issue is that Wayland devlopment is so slow that the entire space moved forward and destroyed a bunch of assumptions that Wayland is based around. reply account42 2 hours agorootparent> The Wayland devs aren't wrong about the security implications of this kind of stuff. But they are wrong. Their security model assumes an ecosystem of untrusted apps when we already had something far superior: distributions with vetted packages. Wayland is like bolting down your furniture because if you let in random strangers from the street they might steal your stuff. Instead of adding obstacles that make my own life harder I prefer to keep better company. reply TheCoelacanth 1 hour agorootparentVetted packages are great, but they aren't the end of all security problems. Good security needs defense-in-depth. It's rare but not unheard of for someone to be able to sneak malicious code into a vetted package. It's extremely common for vetted packages to have security vulnerabilities that could be exploited. I don't want someone who finds a vulnerability in a fart app to be able to escalate that to attack other apps on my computer. I trust my accountant with a lot of sensitive data but I don't give them the keys to my house. I trust a friend with my house keys to water plants while I'm gone, but I don't give them the password to my bank account. reply tomxor 19 hours agorootparentprev> Lack of support for the BSD lineages is a failure. Not true. FreeBSD and OpenBSD have had various wlroots compositors such as Sway available for some time. e.g [1] Some people have even been experimenting with KDE-wayland on FreeBSD since 2021. Wayland support on any OS is not binary because there is no single layer like Xorg. It's a matter of individual compositors and their components being ported to the OS, which is a matter of popularity, and the BSDs are always at a disadvantage in that respect, so they lag behind, same as other software. Nevertheless, they are definitely gaining support. Then again this distinction is only technical, for functional support new DEs/WMs would always have needed to be ported regardless of display architecture, the only case it would not is for an Xorg drop-in, which defeats the purpose. > Lack of screenshots/screencasts is a failure. ... but there are many functional screencapture apps, and even browser support, I use this pretty much every week. I think you might be operating on out of date info. I'd highly recommend giving it another try, the Wayland ecosystem has come quite far over the last decade. [1] https://docs.freebsd.org/en/books/handbook/wayland/ reply p_l 17 hours agorootparentWayland unfortunately is a mess that is hard to implement so we get a balkanized situation where not only everything often boils down to Big Important Process in the middle (that actually integrates more things than X.Org used to), the set of features it supports are way more balkanized and most importantly, specific to that one Big Important Process as they can't be viably separated out. And even then you have to deal with a mix because you have to work through two different unsynced possibly broken in weird ways connections (Wayland + D-Bus). This results in how last week I couldn't screenshare under Wayland, and had zero chances to figure it out - and to make it worse since I had some important state and no time to play with reboots, I couldn't fix it. reply kaba0 11 hours agorootparentXOrg was predominantly used as a useless middleman between a compositor and a window manager communicating with each other. Just because the wayland compositor is “one big important process” doesn’t mean that the whole architecture is more complicated. Multiple IPCs make the whole stuff way more complex, there is a reason why we have a big monolith as browsers and not some `curlhtmlRenderjsInterpreter` Rube Goldberg machine — the unix philosophy is not the pan ultimate design, it works in some cases and is utterly broken in others. reply p_l 10 hours agorootparentX11 != the lowest common denominator implementation of X11. Nor was everyone using it with big heavy compositors (honestly, the only reason I used a compositor for years was intel removing sync circuitry from their iGPUs). And Wayland did not stop requiring multiple IPCs, in fact it mandates them as several features require passing data by secondary channels, not just for Portals et al - some don't even have any described way to reliably pass the information that I have seen (like how to pass around cookie to let one application activate window of another? Or maybe the spec is such a mess that I'm looking at completely wrong place when I tried to fix ability to reliably switch between applications without extending compositor). And yes, the architecture is more complicated in practice, otherwise it might have reached parity with what X11 did after as many years - like input method support. Unfortunately it's so broken that you have multiple versions of it in practice, it requires extra IPC in practice, and at least in my experience just does not work. reply badgersnake 10 hours agorootparentprevIf you know how to make screen sharing work under Sway on FreeBSD, please do share. I have not managed to figure it out. reply cullumsmith 5 hours agorootparentprevThe FreeBSD handbook claims that KDE works with Wayland but I have never gotten it to work. X11 still works flawlessly. reply anotherhue 21 hours agoprevI like to run xeyes and see if they move when I interact with the test application. It's not better but they're as charming as they always were. Also consider running an xwayland app under gamescope which smooths out some issues. reply anyfoo 19 hours agoparentThat’s actually something you don’t want nowadays. A random app like xeyes should not be able to know mouse position at all times, for privacy reasons. (Unless you very explicitly gave xeyes super-extra permission to do that.) We’ve long stopped living in a world where you should need to fully trust every piece software that you run as your user on your computer. And even probably trustworthy software can go bad at an update due to supply-chain attacks. reply ykonstant 8 hours agorootparent> That’s actually something you don’t want nowadays. No, that is something you don't want. I and many others, do want this functionality. reply consteval 3 hours agorootparentEven you don't want this, you only want it sometimes, for some apps. Which is exactly what they said. I mean, would you like VSCode tracking your mouse movements across the entire desktop and your keypresses and then sending them off to Microsoft? Probably not, so we're all in agreement. reply account42 2 hours agorootparentI don't use VSCode or other user hostile programs like it so I don't care what kind of anti-features it enables. I don't want my actually useful tools hobbled in order to deal with such programs. reply anyfoo 31 minutes agorootparentxz wasn’t “user hostile”, and so weren’t countless other pieces of software affected by supply chain attacks. Nothing is hobbled if you can give it explicit permission (which you may well do on xeyes). The days where you downloaded your software from the subsite or tsx11.ai.mit.edu FTP servers and could be confident that it and all its dependencies were trustworthy are unfortunately gone for a very long time. reply ChocolateGod 19 hours agorootparentprev> A random app like xeyes should not be able to know mouse position at all times, for privacy reasons. (Unless you very explicitly gave xeyes super-extra permission to do that.) The thing about xeyes isn't that its privacy invasive, is that it shows that xeyes knows what you're doing in other clients. Got a gnome terminal root shell open? That's a privilege escalation method for any other client running on the desktop under Xorg. This itself, isn't really a problem, but chained with other attacks could be (e.g. browser escape). reply dgfitz 16 hours agorootparentprevIsn’t is possible to get something like mouse position from almost any stdlib of most languages? reply seba_dos1 15 hours agorootparentNo, not really. Modern APIs usually don't even let you access that kind of information without additional privileges. Some older toolkits have functions that are supposed to do this, but it doesn't work everywhere. reply jcelerier 9 hours agorootparentIt works fine on Windows and macOS's desktop APIs in 2024. This is a basic \"desktop app\" expected feature - if you aren't able to implement fully something like AutoHotkey or AppleScript macros, you aren't really a proper desktop platform. reply consteval 3 hours agorootparentYou can implement all those things, just differently now. Relatedly, now random apps can't record the whole screen. This is a good thing, now they get explicit user permission and go through xdg. I don't know how anyone could be opposed to this - it's objectively more empowering for you, the user. reply funcDropShadow 3 hours agorootparentYou think enforcing the one true rights model to everyone is more empowering? We have obviously different definitions. reply dgfitz 15 hours agorootparentprevCan’t you get the cursor position from the windows c++ stdlib? Or like pyautogui? Or the Java stdlib? reply adzm 15 hours agorootparentWindows' pointer position is available to even the most limited GUI application; it is not protected information. For win32 at least, I am not familiar enough to say that about the newer app packages which are much more locked down, but I would be surprised. reply anyfoo 12 hours agorootparentprevHopefully only if the given app has focus. reply funcDropShadow 3 hours agorootparentWhy? I want to have some applications that can always see the mouse cursor like, xeyes. Because that allows me to implement a better customized desktop environment. reply anyfoo 22 minutes agorootparentThen give that piece of code extra-special permissions. As I’ve said in another comment, the days where you downloaded your software from the subsite or tsx11.ai.mit.edu FTP servers and could be confident that it and all its dependencies were trustworthy are unfortunately gone for a very long time. reply kaba0 11 hours agorootparentprevThey queried that info based on a specific implementation of a framework. If that framework is implemented under Wayland, then through the Wayland APIs it will only get rudimentary info on mouse position (e.g. only when it’s in focus and the mouse is over it). reply orangeboats 8 hours agorootparentprevstdlib? Nope, a definite nope! I won't expect stdlib.h to provide me a magical get_cursor_pos() function in any way. GUI toolkits? (This includes the Win32 GUI.) Maybe. It's not a universal thing though -- for example, you may receive mouse position _only_ when the cursor is in your window. reply dgfitz 3 hours agorootparent\"stdlib\" was shorthand for \"the standard library of a given language\" and it came across poorly. I did a quick google before I asked you that question, and it looks like wayland is one of the few exceptions to completely exposing current cursor position. Most \"stdlibs\" do expose it. reply orangeboats 2 hours agorootparentPlease refrain from using the term \"stdlib\", it has a specific meaning and I have trouble understanding what do you mean by it. Are you referring to the native GUI toolkit of a platform (e.g. Winforms, Cocoa) or something else? reply dgfitz 26 minutes agorootparentI will not refrain, you're the only one confused by it. Just google it. You are incorrect and you're not pumped about it, I get it. The horse is fully beat to death. If you want to discuss further, my responses will probably just be lmgtfy links. reply orangeboats 15 minutes agorootparenthttps://en.wikipedia.org/wiki/Standard_library > In computer programming, a standard library is the library made available across implementations of a programming language. You should show me that, among the mainstream programming languages, their standard library will provide get_cursor_pos(). reply superkuh 13 hours agorootparentprevIt really is something I want. But most people do live in your described world. That's the smartphone and \"run's every javascript application sent to my web browser automatically\" kind of computing security model. But there do exist personal behaviors of desktop operating system use where you can actually trust the applications installed and not put up walls between everything. And then things like keyboard/mouse sharing work, windows return to their places, and screen readers work. Those are all very important to me. For them I'm willing to browse with javascript temp-whitelist-only and many other such tedious things. What I'm trying to get across is that the need for that kind of intense security model, every process a threat, is not intrinsic to modern computing. reply kaba0 11 hours agorootparentBut what’s the limiting factor of doing the sane and safe thing by default? The most popular operating systems all do that (ios and android), and they have carved out safe APIs for all of that to work. You can’t patch up a Swiss cheese after the fact. Is it hard to create standard APIs in a bazaar style of development? Yeah. But that doesn’t mean that it’s not the correct approach. reply jcelerier 9 hours agorootparentNo one wants to use android and iOS for serious desktop work though. Like it's cool when your only interaction with the device is consuming content, definitely not for creating. reply kaba0 8 hours agorootparentYou might be living in a bubble. The majority of the population doesn’t have a PC, most people use a smartphone as their only general purpose computer. And while you may not run blender on your phone and render a full-time movie with ray-tracing there, there is absolutely no fundamental limitation, it just so happens to primarily target portable devices, not beasts of a machine with 4 video cards. This functionality requires zero special permission, neither does photoshop (of which there are multiple mobile-versions), or digital painting which, etc. You would be surprised how many content creator gets by with a single ipad. reply funcDropShadow 3 hours agorootparent> You would be surprised how many content creator gets by with a single ipad. Can you name one professional software developer? Probably, you can. But I don't want to limit myself to that sub standard environment. I love my iPad for some activities, for others iOS is just impractical. reply anyfoo 27 minutes agorootparentPretty sure this specific thing is under similar restrictions on macOS. reply immibis 6 hours agorootparentprevAndroid and iOS are limited operating systems for limited devices. They don't scale up to office workloads. reply consteval 3 hours agorootparentCertainly not because of their security model, lol. It's because they're phones. But requiring user permissions for apps to do shady shit is a good thing. Cannot fathom why people are against that. reply funcDropShadow 2 hours agorootparentThe security model is not the only factor, but it is one very important factor. Why do I have to open Moebius sync to keep syncthing synchronization running? Why is the whole landscape build under the assumption that a cloud storage is more trustworthy than local storage? > But requiring user permissions for apps to do shady shit is a good thing. reply kaba0 54 minutes agorootparent> Why do I have to open Moebius sync to keep syncthing synchronization running Because it’s a mobile OS and every single spent CPU cycle is a detriment to battery life? There is absolutely nothing in the security model that would prevent it from running - but it is essential that processes have a “structured” lifetime. E.g. compare how much more graceful android is in low-memory situations, asking apps to serialize their state and then stopping the last used one. Linux oomkiller will just reap my whole display manager for some reason. reply kaba0 6 hours agorootparentprevIn what way or form? Citation needed. reply account42 2 hours agorootparentprevThe limiting factor is all the use cases that have not been invented yet. Screen sharing would have never been a thing if we started out with Wayland-like restrictions. reply realusername 9 hours agorootparentprevI think their concern is valid, it's difficult to do something which is both secure AND not limited at the same time. Sure Android and iOS are secure but in practice they kind of suck for making anything non-standard which limits creativity and freedom. Can we have both a secure and extendable system? Maybe but none of them exist yet. I'm really worrying that Linux mainstream distros will become like Android or iOS. reply kaba0 11 hours agorootparentprevAnd an often under appreciated tenet of security — even a “good” software can be exposed to “bad” data, and you only need a bug (especially a memory bug, which is exceedingly common because linux userspace can’t get rid of c for the life of it) to have arbitrary code executed. Like, your pdf reader is surely not evil, but do you trust every single pdf file you open? reply account42 2 hours agorootparentI expect my PDF reader to be secure. If the PDF format is too complex to implement safely then the renderer should be sandboxed in the reader itself instead of preventing me from scripting using xdotool and similar. And unless you fully sandbox your PDF reader then an exploit is going to have access to your user directory without any display server involvement anyway. X11 vs. Wayland doesn't even come into the picture. reply kaba0 50 minutes agorootparentIt shouldn’t complicate the program itself, everything should be sandboxed by default. And they should simply not have access to my home folder, it should be given access to a specific file only it is about to read. reply yjftsjthsd-h 20 hours agoparentprevI would argue that it is better; it works regardless of this fix and on all compositors. reply yxhuvud 21 hours agoprevHmm. Even with the fix it didn't look very smooth even if it is a lot better. I wonder if that is due to the recording, or due to the mouse settings (mouse poll rate and acceleration settings are very impactful on smoothness), due to X or the toolkit, or due to it not being as good as it can be. reply GrantMoyer 18 hours agoparentI think issue is the screen capture tool or a framerate mismatch in transcoding. The video is 59fps, but the mouse cursor only updates every few frames. Usually it's every other frame, but sometimes it takes three frames or more to update, and sometimes it updates in two or more consecutive frames. On most of the frames where the cursor moves, the window size also changes. reply shmerl 17 hours agoprev [–] Nice! It would be cool for Wine Wayland driver to also fix window resizing (regedit currently is blinking all over when being resized). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Plasma Wayland 6.3 aims to improve Xwayland window resizing to fix visual glitches caused by immediate resizing of X11 windows.",
      "The X11 frame synchronization protocol coordinates window repainting during resizing, but Wayland compositors like KWin use different methods, leading to potential mismatches.",
      "KWin now effectively manages frame synchronization for X11 windows on Wayland, with contributions from Xwayland developers, although some GTK and Qt applications may still experience glitches."
    ],
    "commentSummary": [
      "Xwayland window resizing is challenging due to issues with repainting and handling slow or unresponsive applications, with Wayland attempting to address these by having the compositor manage resizing and decorations.- Wayland introduces improvements over X11, such as eliminating screen tearing, but also presents new challenges, requiring different implementations for features like screenshots and screencasts.- Discussions around Wayland include topics like multilingual input, window placement, and security, and despite challenges, it is gradually gaining support and enhancing the Linux desktop experience."
    ],
    "points": 140,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1730146712
  },
  {
    "id": 41980986,
    "title": "What happens when people with acute psychosis meet the voices in their heads?",
    "originLink": "https://www.theguardian.com/news/2024/oct/29/acute-psychosis-inner-voices-avatar-therapy-psychiatry",
    "originBody": "In avatar therapy, a clinician gives voice to their patients’ inner demons. For some of the participants in a new trial, the results have been astounding By Jenny Kleeman Tue 29 Oct 2024 01.00 EDT Share I n the summer of 2019, when Joe was 21, he went on a university rugby tour of California. One night, one of his teammates bought some cannabis edibles to share, and Joe ate some. For the next 12 hours, he believed he was in hell. He was on fire; his body was suffused with pain. His ears were filled first with incoherent screaming and then with sinister whispering. Joe’s friends thought their teammate’s bad trip was funny, even as they wrestled him away from the windows when he tried to jump from the seventh floor of their hotel. When he woke up the next morning, Joe was still in hell. A devilish, humanoid form lurking in the periphery of his vision was telling him he had died the previous night. A chorus of other voices joined in, wailing in agony. They were entirely real to him, even though he knew they couldn’t be. He had a rugby match to play, and 10 minutes in, he couldn’t see or feel his hands; he couldn’t move. His teammates laughed as he came off the pitch. Poor old Joe. The voices came back to the UK with him. “You’re not real,” they told him incessantly. “You’re already dead, so it doesn’t matter if you end it all again.” He saw blurred, demonic faces smirking at him, sometimes at the edge of his eye line, sometimes up against his face, too close to be in focus. Using avatars in psychosis therapy can help those who hear voices, study finds Read more His parents knew he had struggled with depression and anxiety before, but Joe didn’t want to tell anyone about the voices. He drank heavily, every blackout providing temporary respite. He would walk for hours, playing music on his headphones, desperate to drown out the voices. At other times Joe would tell the voices to fuck off, shut up, leave him alone. He would find himself saying these things out loud, in public. Seeing himself reflected in the fearful eyes of those he walked past, he was terrified that he would never find a way to be normal among them again. Joe was later told he was experiencing acute psychosis. About two or three people in every 100 experience psychosis, when reality is disrupted by delusions or hallucinations. It can be a symptom of schizophrenia or severe depression, but can also be experienced without any other mental health condition. The acute form – the sudden, rapid onset of auditory or visual hallucinations that Joe experienced – can be triggered by drugs in people who, because of existing biological and social factors, might be predisposed to psychosis. Hearing voices is the most common form of psychosis, affecting as many as 70% of people with schizophrenia, and the voices heard tend to be persecutory and distressing. More than one in 10 people with schizophrenia end up taking their own lives. Antipsychotic medications, the go-to treatment since the mid-20th century, can come with serious side-effects, including weight gain, exhaustion, bed-wetting, sexual dysfunction and severe constipation. And they don’t work for everyone: a quarter of people on antipsychotics will continue to hear voices. The most effective medication, Clozapine, is only used where other antipsychotics have failed because it can cause even more severe side-effects. It was developed in the 1950s; there has been little drug innovation for psychosis in recent decades. There are also non-pharmacological treatments; cognitive behavioural therapy for psychosis (CBTp), when combined with medication, improves symptoms for about 50% of people. After hearing the voices for two and a half years, Joe went to his GP in winter 2021 and received his formal diagnosis. He was put on a low dose of antipsychotic medication, which he hated: he couldn’t get out of bed, couldn’t function and, while it helped with his visual hallucinations, the voices remained. He came off the medication after two months. Depressed, despairing and starting to spiral, he got back in touch with his GP, who told him there was something else to try: an experimental therapy, a clinical trial he could be part of, that turned the traditional treatment model for psychosis on its head. If you hear voices, clinicians don’t generally ask what they’re saying to you, beyond whether they are asking you to harm yourself or others. “There’s been a reluctance to engage much with the content of voices,” Ben Alderson-Day, an associate professor of psychology at Durham University who specialises in psychosis, told me. “That’s in part because of a concern that if you ask voice-hearers to elaborate, you might engage in ‘collusion’: you may make [the voices] more real for people.” A clinician may diagnose a patient with psychosis, and prescribe them medication or CBT, without knowing what the patient’s voices say to them. This new therapy demanded that voices were listened to closely, and responded to as if they were spoken by entirely real external beings. Trial participants would create an avatar of their voice: a moving, three-dimensional digital embodiment that looks and sounds like the persecutor inside their heads. They would be guided by a therapist to have a dialogue with the voice – and the hope was, through doing so, gain control over it. Within a few weeks, the voice that told Joe he was dead – the one he so feared could be real – was manifested in colour in front of him. For the therapy to work, he needed to find the courage to look the demon in the eyes; to challenge and conquer it. If he succeeded, the voices might fade away. P rof Julian Leff was seven years into his retirement when the idea of avatar therapy came to him. After a celebrated career as a social psychiatrist and schizophrenia specialist at University College London, Leff was sitting at home in Hampstead, pondering the results of a survey that reported the most distressing aspect of hearing voices was the feeling of helplessness. On the rare occasions when his patients had had meaningful exchanges with their voices, he knew they had felt more in control. “I thought, how can I enable the patient to have a dialogue with an invisible voice?” Leff said in an interview for a documentary made in 2018, three years before his death. “If I can somehow manage to create for the patient the image and voice of the person who they hear abusing them, maybe they could learn to overcome this awful persecution.” Leff was awarded a small grant for a pilot study in 2008. He recruited Mark Huckvale, professor of speech, hearing and phonetic sciences at UCL, to be in charge of the tech. They tinkered with existing police identikit software, animating digitally created faces in three dimensions so they could nod, smile and maintain eye contact. They combined this with an off-the-shelf programme called Lip Synch, so that the mouth would move appropriately, and voice-changing software, so the avatar could be made to sound male or female, rougher or smoother, higher or lower, older or younger. The avatar was a floating, moving head on a computer screen, voiced by Leff, who would be in a separate room to the patient, watching via webcam. He could speak to the patient in his own voice, guiding them through the dialogue, and then switch with the click of a mouse to the role of the avatar on the patient’s screen, its lips synched to his speech. The setup allowed him to act as a therapist to the patient and a puppeteer to the avatar. At first, the avatar would say typical lines the patient had shared with Leff: often degrading, abusive phrases. But over the course of six sessions, the dialogue would change, with the avatar yielding to the patient, transforming from omnipotent to submissive. At all times, Leff and the patient were to treat the avatar as if it were an entirely real third party. Sixteen people – all of whom had heard voices for years, despite being on medication – participated in this pilot study. A man who had heard the devil incessantly for 16 years was instructed by Leff to tell his demon avatar he didn’t deserve to be persecuted and he should go back to hell and torment those who did. An older man, who had been woken every morning at 5am for more than three years by the voice of a woman conducting noisy business meetings in his head, was encouraged by Leff to tell her it was unprofessional to allow him to overhear her discussions. To Leff’s surprise, both of these men stopped hearing their voices entirely after only three sessions. While most patients did not experience such a dramatic change, the results were still impressive: for 13 of the 16 participants, voices remained, but they were less frequent and intrusive, and suicidal feelings were significantly reduced. The therapy had made a significant difference to a sample group composed of people for whom all other forms of treatment had failed. But other clinicians were wary of taking the results of the pilot seriously, believing they might be a consequence of Leff’s skill as a therapist, rather than the therapy itself. “He did have a magic touch,” Tom Craig, professor of psychiatry at King’s College London (KCL), told me. But Craig was sufficiently impressed by the results to lead a randomised controlled trial on 150 patients, along with Philippa Garety, professor of clinical psychology. Leff trained Craig and the clinical psychologist Tom Ward to deliver the therapy in his place, giving them audio recordings of his sessions and a checklist of how he thought things ought to be done, which Craig and Ward turned into a manual. “Within the first couple of cases, we thought: this is extraordinary – something’s really happening here that we’ve never seen before,” Craig said. One participant, Chris, had been persecuted for years by the voices of high court judges who denounced him for intrusive sexual thoughts. Through the therapy, Chris came to accept his sexual urges as normal, and his high court judge avatar ultimately told him he had no case to answer. Free from persecution, Chris was able to go on dates for the first time in years. Avatar therapy was found to be quicker, cheaper and more effective after 12 weeks than any other non-pharmacological intervention currently available for people with psychosis. It worked, even without Leff, on a larger scale, and it worked faster than the control therapy delivered by highly trained clinicians. As for the concern that engaging with auditory hallucinations exacerbates psychosis, Al Powers, associate professor of psychiatry at Yale University, told me it was not backed by empirical data. “Despite popular wisdom about not wanting to collude with the voices, the evidence that’s emerging seems to indicate that engagement-based approaches are most effective in terms of increasing control over voices, and also achieving some degree of mastery over them.” Despite its early success in trials, Alderson-Day warns against viewing avatar therapy as a cure-all for psychosis. “The idea of a single therapeutic option for all kinds of voices is very unlikely,” he said. “Some people’s auditory experiences aren’t even voice-like, so there won’t be content to work with,” he told me. But if avatar therapy could be quicker and more cost-effective than existing treatments, he said, it was worth pursuing. The research team’s next step was to demonstrate that avatar therapy could work when delivered by a broad range of therapists in different locations. A new trial, Avatar 2, began in 2021, and involved 19 trained therapists in four different sites across the UK. There were 345 individuals enrolled in it – including Joe. J oe was nervous about designing his avatar. No one had ever asked him to describe what his voices looked or sounded like before. He had spent so much energy telling himself they couldn’t be real and now he had to manifest them in the real world. There were other challenges: like most people with psychosis, Joe heard several voices, and he experienced them more as a felt presence, rather than a single entity with a definite physical appearance and a familiar face. Tom Ward, who was assigned to be Joe’s therapist, told me the average number of voices heard by people with psychosis is four. “We are looking for the dominant voice that’s causing the most distress.” Joe chose the voice who told him there was no point in living because he was already dead. He and Ward began creating the avatar’s face. There was a blizzard of choices in drop-down menus on Ward’s laptop screen: is it a human or non-human entity? If it’s human, what is its gender, age, height (tall, medium, short), ethnicity (European, east Asian, south Asian, African)? If it’s non-human, does it take the form of a devil, angel, alien, vampire, robot, witch, goblin, elf, beast? Once a basic form is chosen, there are sliders to change the physiognomy: making the nose broader, thinner, shorter or longer; adjusting the eyes, brow and chin; modifying the hairstyle. Joe found it hard to describe what the voice looked like: it was often hooded, masked, out of focus, only partially visible. It was demonic, but it didn’t look like a devil. Together, they created the head of a bald man with olive skin, like Joe’s. He chose between five versions of voices, and used sliders to change the pitch, tilt and roughness. The final voice was deeper than any human’s. That’s what made it demonic, for Joe. View image in fullscreen Illustration: Nick Kempton It wasn’t exactly right, but once Joe was alone with the screen there was something about the avatar that resonated. Ward reminded Joe he was there to support him. Before they went into their separate rooms, they had practised what the avatar was going to say, and how Joe might respond to it. Still, he felt terrified. “You’re already dead,” the avatar told him, in a voice that was almost monotone. “You’ve been in hell all this time and this is your existence from now on.” “If this is death, it’s exactly the same as what my idea of life was,” Joe replied, a little meekly. He was surprised by how realistic the experience was, how true to life this felt. “You’re lying to yourself,” the voice retorted. In his own, encouraging voice, Ward reassured Joe, reminding him to hold eye contact, to communicate in strong messages that he was in charge. “You’re harder to get hold of today,” the avatar said towards the end of the first session. “You can’t keep it up.” “I can keep this up for ever, and I will,” Joe replied. “It’s my life. I have the autonomy here. I’m in control.” Joe had 12 weekly sessions. The darkest exchange came in the fourth. “You should end it,” the avatar said, casually. “What have you done that’s of any use to anyone?” Joe couldn’t answer this. In his own voice, Ward interjected to remind Joe about his relationships, his family, the life he had been able to make for himself. “What the avatar is saying is actually not true,” Ward said. “Can you come back with positives?” Ward switched back to voicing the avatar. “You agree with me deep down,” it sneered. “You haven’t done anything of use.” “No,” Joe said, firmly. “I have a lot of good friendships. I think on balance I have had a good life. It’s been positive. I’ve got more to do.” “You’re handling yourself better than I thought,” the avatar replied. “I’d thought you’d be falling apart by now.” A ll therapy sessions are recorded, and the audio given to the trial participants so they can listen back in their own time and be reminded of how they managed to gain mastery over their voices. With Joe’s agreement, Ward shared the recordings of his sessions with me. Ward had delivered avatar therapy to about 80 people before he met Joe. He told me that people with psychosis often feel disempowered and marginalised; they feel as if they don’t have the right to talk. Avatar therapy is about equipping them with the tools to answer back. I met Ward in his office at KCL’s Institute of Psychiatry in Denmark Hill, south London. He gave me a demonstration of the avatar design process, so that when we sat down to talk, a disembodied male head was blinking from his laptop in front of us, looking shiftily from side to side throughout our conversation. It still looked like an animated police photofit to me. Perhaps, with a voice to go with it, it would be easier to suspend disbelief. Ward told me it can be helpful for many patients to understand their voices as a coping strategy arising from a previous trauma. (While there is no consensus among clinicians about the precise relationship between trauma and hearing voices, there is widespread acceptance that there is often a link between the two.) But discovering whether or not a patient’s voices have arisen as a response to trauma isn’t important, Ward told me; the point of the therapy is to find any explanation that gives them a sense of mastery over their voices. This is the principle followed in any psychological intervention, he said. Using the manual created from Leff’s instructions as a guide, the therapist plots out how the voice will change as the person is supported to stand up to it. When the voice is a bully, the avatar will begin to recognise the impact of their behaviour, perhaps revealing that they too have once been a victim of bullying. When the voice is a devil, a djinn or some other malevolent spirit, the avatar will reveal that they are not actually very powerful (“‘I’m not a high-ranking demon, I’m a trickster’ – that sort of thing. Things are quite interesting when you voice demons,” Ward said). When the voice resembles someone who abused the patient when the patient was a child, the avatar gradually acknowledges that they are no longer talking to a defenceless boy or girl but instead to an adult with agency. Psychiatry professor Al Powers told me he saw potential problems in cases where the voice represented by the avatar belonged to a person who existed in the real world. “That can negatively impact one’s conceptualisation of your relationships with the world, and your family, and other people who are important to you, and that can contain some risks,” he said. Before anyone can gain power over their voices, they have to tell their therapist what the voices say to them. It’s often the most degrading content imaginable; racist, sexist, sexually shaming and taboo. In his clinical experience, “You’re a paedophile” is one of the most common phrases repeatedly heard, Ward said. The avatar therapist has to reassure their patient that these phrases are nothing to be ashamed of, but also has to be prepared to deploy them when they are playing the role of avatar. For the therapy to work, the therapist has to commit to playing the role of their patient’s tormentor. “You never break the fourth wall,” Ward said. The avatar can be direct – can go for the jugular – because it is not the therapist, and it can lie, or say things that are wrong. Hearing the avatar say these things can give the patient enough distance for them to reflect on and respond to what they usually only hear inside their own head. In the Avatar 2 trial, for the first time, therapists went as far as allowing avatars to say things like, “You should end it.” When I brought this up with Ward, he stiffened. This kind of content is only used in specific circumstances, he said, when the patient’s risk of suicide has been assessed as minimal. “You don’t start the first dialogue with that. It comes at a point where you know how the person engages in the dialogue; you know that there is a clinical benefit to [them] voicing a commitment to life, and you know that they will be able to do it.” (I asked three specialists who work on the treatment of psychosis, but were not involved in the trial, about the dangers of the avatar therapist voicing commands to self-harm, and they all told me that, while unconventional, in these circumstances, it would not harm the patient.) None of the avatar trials have shown that the therapy could exacerbate psychosis, even if people drop out before they have completed the run of sessions. “People do drop out,” Ward acknowledged. “Sometimes people will say, ‘It was just a bit too much for me.’” They have monitored all the people who had the therapy, tracking any mental health deterioration or hospital admission during and after treatment, and there has been no documented evidence of any crises directly related to avatar therapy during the Avatar 2 trials. We are used to imagining those who hear voices as fragile, but Ward sees them as extraordinarily resilient: they can survive both years of the worst kind of internal persecution from their voices, and also the stigma and discrimination their condition is met with by the general public. Nothing simulated on a computer screen could be any more traumatic, he says, than what the people he works with endure in daily life. B efore and after Joe’s dialogues with the avatar, he and Ward discussed how Joe’s voices might have developed in response to the extreme, heightened terror he had felt during his bad trip. Joe began to think of his voices as an overactive defence mechanism, a maladaptation of his brain as it tried to keep him safe and alert in a world he’d experienced as full of danger. “The voices are just your paranoia, your anxiety, your fight-or-flight response gone mad. Give them space, and then you can have a conversation with them,” Joe told me. “I like to think that’s the reason why they started initially, but even if it isn’t, it doesn’t really matter, because it meant I was able to talk to them.” By Joe’s seventh session, he was having insightful, poignant conversations with the avatar. “Things are going to shit,” it grumbled. “It’s not a bad assessment. They aren’t going fantastically,” Joe conceded. “You and I want the same thing – things not to be shit. That’s the goal. Rest assured, we’ll get there. It might take a while.” “It’s why you need me.” “In a way yeah, I guess. I think we’re working towards the same goal of just being the best version of myself I can be.” “That’s what I need from you – to be the best,” the avatar said. “It’s what I’ve always needed.” “Yours is not always the most helpful way to go about it, is it?” Joe said. “But I appreciate the sentiment.” The dialogue had become a strange kind of couples therapy, in which Ward was playing the avatar and the therapist. After four years of arguing with his voice, Joe began to feel compassion – even pity – for his tormentor. In the 10th session, Joe and the avatar discussed what happened in California. Joe described what he went through that night: not only the horror of it (“It was fucking scary, wasn’t it? It felt like a Hitchcock score sounds”) but also the alienation (“I was surrounded by people who found it funny”). “You tried to tell yourself it didn’t matter, I wasn’t real,” the avatar said. And then, with resignation, “I’m fading from your life.” “You’ll always be there in some form,” Joe reassured it. “But yeah.” “Is that OK with you?” it asked. Joe’s tormenting voice had become his anxious companion. “Yeah. I can live with that. So long as we’re able to coexist,” he replied. “Thank you for listening,” were the avatar’s final words. “Thank you for making room for me.” A vatar 2 set out to investigate how effective the therapy could be when delivered by therapists with far less experience than Ward and Craig. Some of the participants had been living with voices for decades. Claire was in her early 50s when she enrolled in Avatar 2, at the Manchester research site. She had heard the first voice when she was 10: an adult male, casually telling her to jump out of her bedroom window. It was entirely real to her, external and authoritative, “like an adult telling me what to do”. Claire had been abused from the age of seven. She grew up in a state of constant hypervigilance. The nasty voice, the one that told her she was a stupid bitch, arrived when she was 13. “I remember saying, ‘Oh, shut up,’ out loud, and the other girls laughed and said, ‘There’s no one there!’ And then I realised I had to be quiet about them.” The voices became one among many secrets Claire kept, alongside the abuse and her self-harm. She spent much of her adult life in psychiatric hospitals. By the time her mental health team’s care coordinator told her about the Avatar 2 trial in 2021, Claire had tried to end her life many times. She had been diagnosed with bipolar disorder, psychotic depression and, at one point, schizoaffective disorder. She was taking antidepressants, mood stabilisers and tranquillisers, as well as antipsychotics; she had tried CBT, cognitive analytic therapy, compassion-focused therapy and group survivor therapy. Still the voices persisted. “The state I was in at the time, I thought, they’re not going to accept me on the trial – I’m too unstable,” Claire told me. But she was given a place on the trial, coordinated by Hannah Ball. Ball had only qualified as a psychologist a year before she was trained to deliver avatar therapy. She was assigned to be Claire’s therapist. “Hannah reassured me that nobody had ever had a crisis because of avatar therapy, and I thought, that will be me. I’ll be the first one,” Claire said. Claire chose to make an avatar out of the first voice, which she felt shaped all the others. While she and Ball were putting together its angular male face, with its dark eyes and spiky hair that made Claire laugh because it wasn’t quite right, the voices over her right shoulder were enraged: “Don’t do this, you stupid fucking bitch.” The avatar’s northern accent was also not quite right, but there was something about its menacing tone that jolted Claire. As soon as she heard it, it was real to her. She felt dizzy and sick in her first session, and Ball had to constantly check in to provide reassurance in her own voice. The dialogue lasted barely 10 minutes and left Claire exhausted, but as she walked home, she was smiling: she had been able to stand up to her voices for the first time in her life. Between sessions, she listened to the audio recordings that Ball had given her to take home, so she could remember what she had achieved and steel herself for her next encounter. (Claire agreed to share these recordings with me.) By the third week, she was answering back to the avatar, asserting herself without any prompting from her therapist. “Stop saying such nasty things to me. I’m not going to listen to you any more while you say such nasty things,” she told the avatar. “I’m not sure what’s got into you,” it replied. “I’m going to lay down some rules,” Claire said. “We can still talk, but on my terms, not yours.” By the fourth week, Claire’s voices had gone entirely. For the first time in 40 years, she was alone with her own thoughts. Quiet. She hadn’t expected them to go. “My aim wasn’t to get rid of them – just to get along with them,” she told me. “I wasn’t quite sure I wanted to let go. I’d never really been on my own. As abusive as it was, it’s still a relationship.” Like Joe, she had been encouraged to understand her voices as a faulty self-defence mechanism. They had been trying to look after her: when they told her to end her life, they were trying to find a way to stop her suffering. Their departure was a kind of bereavement. View image in fullscreen Illustration: Nick Kempton In the remaining sessions, Ball helped Claire accept the loss of the voices, and she had an opportunity to say a definitive goodbye. The avatar promised to stay alongside her at a distance, there if needed, but no longer interfering in her life. “I wish you all the best,” it said at the end of the final session. “Thank you. I wish you all the best, too,” Claire replied. “I know you had good intentions at heart.” Nearly two years on, Claire’s voices have not returned. She’s coming off all her medications. She can go out in public, eat in a noisy restaurant, do voluntary work, give interviews to a journalist – all things that once seemed impossible. “I’m stronger. I’ve gained so much. I now feel I have a life worth living.” Ball didn’t have the same level of experience as Leff, Craig or Ward, but she was able to achieve the same outcomes using the manual they developed as her guide. I asked to see the manual, but Ward told me he couldn’t share it with me, because it was “core IP”. The Wellcome Trust, which funds the trial, has been protective of its intellectual property in the past: research teams in Denmark, Australia and Canada that have been experimenting with avatars have been told there are restrictions around calling the work “avatar therapy”. Ball told me the manual is not a script, more a set of objectives to aim for in each session: a general structure of how the avatar should change to empower the patient. She listened to recordings of one of Ward’s cases from the first trial, did two closely supervised pilot cases and was then delivering the therapy herself, alone. I wondered how comfortable she felt, taking on the role of a malevolent entity that has enormous power over her patient. “It requires a lot of active formulation and reformulation on the spot, and listening out for things that might change how you were initially going to approach a dialogue,” she said. I had imagined that only skilled clinicians could be avatar therapists, but Ball was convinced that, if they were willing to take on the challenge, a very wide range of mental health practitioners could do it. “I think you need people who understand relationships and dynamics,” she said. “If you’ve got a sense of who you are as the avatar and the relationship [to the patient], you know how to respond.” T he results of the Avatar 2 trial, published on Monday, were dramatic. Avatar therapy has been shown to deliver rapid and significant reduction in distress caused by voices. No other psychological intervention has been shown to cause such a significant reduction in the frequency of intrusive voices. Earlier this year, the National Institute for Health and Care Excellence announced that it has found avatar therapy to be safe and effective and recommended that it be offered for testing in clinical NHS settings over the next three years. Thirty-eight people have so far been trained to deliver it in the UK, from experienced clinicians to newly qualified psychologists and nurses. The most effective psychological therapy currently offered on the NHS, CBTp, is typically delivered by qualified clinical psychologists in 16 sessions over 12 months. In comparison, avatar therapy could work out as “half the length of time, with less skilled people, so a bit cheaper, and a bit more available”, Tom Craig told me. He hopes it will be part of NHS treatment within five years. A small number of practitioners remain hesitant about avatar therapy being delivered by support workers and less experienced psychologists. Prof Neil Thomas, director of the Voices clinic in Melbourne, and lead investigator on the Australian Amethyst avatar therapy trial, said: “Working with people who hear voices is already an area of specialist practice. Using technology as well makes it even more specialist. The process is actually not particularly intuitive for people that have trained in therapy – which involves being supportive to people – to have to role play a nasty voice.” But the British team are taking things even further. A newly announced Avatar 3 trial will investigate whether the avatar could be entirely digital and voiced by an artificial intelligence, which would remove the requirement for real-time human voicing of the avatar, and mean it could be widely disseminated. Humans would always be necessary to support the person in their interaction with the avatar and help make meaning of the voices, Craig said, but that would not need to be a trained therapist. It could be “a community nurse, or a nursing assistant”. Louise Birkedal Glenthøj, associate professor of psychology at University of Copenhagen and the trial coordinator for Challenge, the Danish trial using avatars in the treatment of psychosis, told me she feared a fully digital avatar powered by AI might have the potential to exacerbate psychosis. “As people with psychosis struggle with grasping reality,” she said, “being in a dialogue with a machine that is not controlled by a therapist might generate psychotic experiences.” The Danish team enrolled 270 participants in a trial that investigated how people who hear voices respond to having dialogues with an avatar using virtual reality. “We thought if [we could] integrate this in fully immersive VR, then we would perhaps get some additional benefit in terms of this potentially having a greater treatment effect,” Glenthøj told me. “Having the therapist close by would intuitively be more secure for the patient. We capitalise on this notion of ‘it’s real but it’s not real’. It’s so real that they feel they are in this dialogue with their voice, but it’s not real, and if they take off the headset, then it’s gone.” The VR allows the user to situate the avatar in daily life settings, such as on the bus, or in the participant’s home. They also added emotions to the face, so the avatar could smile more and look more friendly as the dialogues progress. Glenthøj conceded that VR avatar therapy can be overwhelming for some. “We do see people reacting. They destabilise. They get more psychotic.” As a result, the Danish team progressed more slowly than clinicians on the Avatar 2 trial, and have added safety features, such as a virtual panic button, and regular contact with the participants’ primary care providers throughout treatment. They also gave participants booster sessions at three and six months after treatment, in the hope of making any positive effects more durable. The trial ultimately found that VR avatar therapy was significantly more effective at reducing voices compared with supportive counselling. Avatar therapy may help in treating mental health conditions beyond psychosis. Preliminary research from Ward’s team with an avatar embodying the “anorexic voice” has shown it to be a promising intervention for eating disorders. Glenthøj is researching VR-based avatar therapy for obsessive compulsive disorder. Ward also wants to investigate whether dialogues with avatars could help people who struggle with anxiety or depression. “The technology is about creating this external representation of the dark side of yourself,” Craig said. “At some level, this is about thoughts, isn’t it?” In Australia, avatar therapy can take place via telehealth, with therapist and participant often in different parts of the country. “We’ve got a lot of people living in regional areas who have limited access to mental healthcare – let alone to specialist therapies,” Thomas told me. They have drawn on how the British team worked during lockdown to see how it can be delivered remotely. Some therapists have tried, in the past, to guide their patients through dialogue with voices via role play, or “chair work” – where the voices are represented by an empty chair with content spoken by the patient – but both these techniques require a leap of faith. With an avatar, it’s the recreation of the voice, not the face, that makes this radical, Thomas said. “It’s called avatar therapy, and that sounds like it’s primarily about the visual representation, but not everyone has an existing image that goes with their voice. I think the auditory transformation is particularly powerful.” “The suspension of disbelief is remarkable,” Craig told me. Even though trial participants have signed consent forms and know it is the therapist voicing the avatar, they still relate to it as if it were the voice in their head. “They are put in front of this not very wonderful computer animation, and they’re right in there, talking to their voice.” ‘I t was liberating just to talk to Tom [Ward] about it, because I didn’t speak to anyone else,” Joe told me over coffee in south London. He still had the imposing presence of a rugby player, but he was so softly spoken that I had to strain to hear him talk over the hubbub of the cafe. Therapy wars: the revenge of FreudOliver Burkeman Read more A year on from avatar therapy, Joe’s voice was still there, a presence just out of eyeshot, still a distinct external entity and not just an inner monologue. But it was quieter, easier to manage and allowed him to get on with daily life. When it spoke, it was to have the same kind of conversations they’d had in his final sessions with the avatar. “I get it – you’re very on edge,” Joe would tell his voice. “I don’t feel great either. But we are just walking to work at the moment. I promise, we’re good.” “It worked because I understood the voices more, I think,” he told me. “My general levels of anxiety stayed pretty high, but I’ve started interpreting the hallucinations as a part of the anxiety.” He still has panic attacks. The anxiety and self-doubt that existed before his bad trip are still there. “You do have to address everything that’s going on to address the voices themselves. They feed on everything else.” Joe recently went back to his GP in search of help with his anxiety, but there was no cutting-edge experimental solution delivered by renowned psychologists for him this time. The GP put him on a waiting list for NHS talking therapy, and warned that he could be in for a very long wait. Names of patients have been changed. In the UK and Ireland, Samaritans can be contacted on freephone 116 123, or email jo@samaritans.org or jo@samaritans.ie. In the US, you can call or text the National Suicide Prevention Lifeline on 988, chat on 988lifeline.org, or text HOME to 741741 to connect with a crisis counsellor. In Australia, the crisis support service Lifeline is 13 11 14. Other international helplines can be found at befrienders.org In the UK, the charity Mind is available on 0300 123 3393 and Childline on 0800 1111. In the US, call or text Mental Health America at 988 or chat 988lifeline.org. In Australia, support is available at Beyond Blue on 1300 22 4636, Lifeline on 13 11 14, and at MensLine on 1300 789 978 Follow the Long Read on X at @gdnlongread, listen to our podcasts here and sign up to the long read weekly email here. Explore more on these topics The long read Psychology (Education) Psychology (Science) Mental health Schizophrenia Psychiatry Depression features Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=41980986",
    "commentBody": "What happens when people with acute psychosis meet the voices in their heads? (theguardian.com)136 points by sandebert 10 hours agohidepastfavorite137 comments throw4950sh06 9 hours agoDon't do it without supervision, I nearly jumped 100m because the voices in my head convinced me I'm a Star Trek captain and will be transported to my bridge mid-jump. Never do anything to confirm a paranoid person's psychosis unless you have total control of the situation and a psychiatrist supervision. Never try to peace them by saying unrealistic things, you never know what's going on in their head that you just confirmed. My GF tried to reassure me by saying she will be with me in 15 minutes, but she was 100km away and I thought \"okay well that makes all of this real, let's do it\". reply Daub 7 hours agoparentPrtty much echoing what you said, in this video Cecila McGough, who has schizophrenia, talks about how important it is that people don't do anything to confirm her hallucinations. https://youtu.be/7csXfSRXmZ0?si=GT6zn_Sytcfw011H reply Abimelex 52 minutes agorootparentThat's good to know, but it leaves you without many options, since all the experts also suggest not to confront them with reality. So how to interact with somebody who as a psychosis and sees a complete twisted reality? reply throw4950sh06 38 minutes agorootparentCall an ambulance. Acute psychosis needs immediate treatment, they may easily hurt themselves or others. The psychosis can be stopped with one injection, don't prolong their suffering. Before the ambulance arrives, try to reassure them that they are safe, there is peace around them and help is on the way. Once the ambulance arrives, go away - it might get messy. reply ThePowerOfFuet 6 hours agorootparentprevHere's that YouTube link without the tracking parameter linking you to everyone who clicks it: https://youtu.be/7csXfSRXmZ0 reply cutemonster 1 hour agorootparentI was wondering why she wasn't looking at the camera or interviewer. Turns out the reason was that she saw an hallucination in that direction. So she looked away, she explains, in a follow up video 5 years later: (I found via the YouTube comments section) https://m.youtube.com/watch?v=n7Wzb6esnpU Here she's much better, and explains how she got better (in the beginning of the video) reply Daub 3 hours agorootparentprevThanks reply rendx 5 hours agoparentprevVery important point, thank you for raising it. Mental Health First Aid has good manuals for first aid. Here's the one for psychoses: https://www.mhfa.com.au/wp-content/uploads/2023/12/MHFA_Psyc... We were shown this video in our MHFA certification class for discussion: https://www.youtube.com/watch?v=a7cMXce5j40 reply thrw5298u49 2 hours agoparentprevWithout supervision? What good is that? This is a disease that breaks people. Those who would convince themselves to see such symptoms in a positive light are doing nothing but damage. reply throw4950sh06 1 hour agorootparentYes. What I mean is that doing it can have unintended consequences - so don't do it if you don't know what you're doing and/or not fully in control of the situation. reply rightbyte 8 hours agoparentprevI guess she was calling for help, so the strategy of postponing wasn't that bad even in retroperspective? Or do you mean you called her becouse you kinda knew you were mad and wanted her to also say it? reply seszett 8 hours agorootparentSince she was 100 km away, it didn't make sense she was going to be there in 15 minutes by normal means, so it meant either she was lying or OP was actually going to be teleported. OP apparently chose to believe the latter, since it confirmed their current delusion. So it didn't postpone anything, maybe if she had given a realistic ETA (or just said \"I'm coming, wait for me\") it would have worked though. It's very difficult to know what to do in these situations though, I've been on the side of that girlfriend and you just can't have a full understand of what's going on in the head of the other person, everything is just walking on eggs, except the eggs are actually landmines. reply anal_reactor 5 hours agorootparentThis ruins my LSD trips. Instead of enjoying the high I end up being scared that I might do something stupid that sober me will later regret. reply naming_the_user 5 hours agorootparentThis is the reason that I've only ever done LSD once. It wasn't especially anxiety inducing for the most part and was quite fun, but I feel as if I just got lucky and that in a slightly different circumstance I could just have convinced myself that my apartment window was in fact the front door and fall out of it. reply zmgsabst 5 hours agorootparentprevOut of curiosity, what is it you imagine would go wrong? reply upghost 4 hours agorootparentI'm going to assume \"selecting a username\" :D reply DiggyJohnson 4 hours agorootparentlmfao reply anal_reactor 3 hours agorootparentprevI don't know, I can try cooking something and actually making a fire reply aleph_minus_one 6 hours agorootparentprev> I've been on the side of that girlfriend and you just can't have a full understand of what's going on in the head of the other person, everything is just walking on eggs, except the eggs are actually landmines. The problem here in my opinion is rather that most people are used to lie, manipulate and betray (in society this behaviour is actually euphemized via the term \"white lies\"). In this particular case, such a behaviour does have consequences. See the example of this thread where the girlfriend claims she will be there in 15 minutes, despite being 100 km away. Since most people are not used to being honest (or, I assume, actually never were), they give similar descriptions of their difficulties when their lies do have consequences like your \"everything is just walking on eggs, except the eggs are actually landmines\". reply seszett 5 hours agorootparentIt's a bit more complicated than just \"not being used being honest\". Being 100% honest and not hiding anything doesn't work either, so it's a constant balancing act between telling them the truth, reassuring them and sometimes indeed shielding from the harsh truth by avoiding from mentioning something, which I do believe is different from lying. Lying is out of question though and I think the people who resort to that do it out of laziness more than anything, or maybe well-intentioned wishful thinking in the case of the aforementioned girlfriend. But this is always harmful, not just with psychotic persons. reply aleph_minus_one 5 hours agorootparent> Lying is out of question though In the particular example, the girlfriend did lie to him by telling him she'll be there in 15 minutes despite being 100 km away. This exactly lead to the strange chain of thoughts. reply salawat 32 minutes agorootparentprev>sometimes indeed shielding from the harsh truth by avoiding from mentioning something, which I do believe is different from lying. You mean lying by omission? A.k.a. not speaking the quiet part? It's still received and noted by the other person you know. Likely by the part of the psyche they haven't got consciously integrated, or are experiencing a connectivity problem with. Remember, that part of them is still a very big chunk of human neural processor. It isn't stupid. It's also not terribly capable of straightforwardly communicating. What it is scarily good at is navigating the external world, which includes seeing the void of what one doesn't say. If it's already running amok, pouring more fuel in the form of duplicity on the fire is probably not the greatest idea. reply tdeck 5 hours agorootparentprevIt's interesting how OP conflates telling white lies with \"betraying\" - kinda feels like there are some issues to work through there. reply aleph_minus_one 5 hours agorootparent> It's interesting how OP conflates telling white lies with \"betraying\" This statement exactly proves my point concerning how dishonest and betraying most people are. reply genrilz 4 hours agorootparentI think what GP is saying is that white lies are not necessarily betrayal. Betrayal is when someone violates your trust somehow. [0] You can see some examples of lying in the wiktionary definition I linked. (see the last two definitions) However, both these definitions involve the lie causing some negative consequence. A white lie is specifically a lie done to spare someones feelings. It's possible that the white lie can cause negative consequences, but that is not always the case. To take a concrete example, consider the classic white lie \"no, that dress doesn't make you look fat\". It is possible that this could cause someone to wear an ugly dress, but the person probably looked at themselves in the mirror too, so they probably will end up choosing a dress that doesn't look awful. In this case there is no negative consequence associated with the white lie unless person wearing the dress is unable to accurately self assess. (which is something the speaker hopefully would know about the dress wearer) You could define being lied to as a violation of your trust. In particular, I think autistic people often can't pick up on social cues, and so rely more on people speaking the truth to them. However, that doesn't mean white lies are betrayal for everyone. If you in particular feel like white lies are betrayal, you might want to tell people in your life that so they know it's important to you. They won't automatically know and it isn't automatically important to them. [0]: https://en.wiktionary.org/wiki/betray#English reply aleph_minus_one 3 hours agorootparent> Betrayal is when someone violates your trust somehow. Which white lies do. reply genrilz 3 hours agorootparentI believe you when you say that white lies violate your trust. I'm saying that that experience isn't universal though. Different people define violations of their trust differently. Thus my suggestion at the end to make sure that the people around you know that this is part of your definition. (Though you might have already done this) reply salawat 3 hours agorootparentprevA lie is a lie, and GP's point still stands. In effect, you're proving their point by doing everything possible to justify a state of affairs for a preponderance of people whereby it's okay to water down the reality of the situation to \"spare them an emotional reaction\". I can understand where GP is coming from. A lot of my professional career even exists because someone has to cut through the massive layer of bullshit and distortion people generate in an org to be able to make substantive statements about what is the truth, born out by quantitative data and observation. After a while in the field, you really start to lose your appreciation for other people walking around \"sparing\" one another from the Truth. reply genrilz 2 hours agorootparentI agree that in professional settings, it's usually best for the efficiency of the company to be direct about everything. I also generally prefer that people provide accurate feedback about how I am doing in interpersonal relationships. I do think that \"sparing\" someone from an emotional reaction is sometimes a reasonable thing to do in an interpersonal relationship though, depending on the relationship. Back to the dress example. Let's say the dress wearer is my girlfriend, and I have been asked the question \"does this dress make me look fat?\". Let's also assume I do think this particular dress makes her look fat, but generally find her attractive, and know that she has body image issues. If I answer \"yes\", or even \"that dress isn't particularly flattering on you\", she might interpret that as \"He thinks the dress makes me look fat\" -> \"I am fat\" -> \"I am ugly\", which is not actually what I think and not what I want her to think of herself. Even if I try to reassure her that I don't think she's ugly, she might think that my reassurance is a lie. People with insecurities don't always think logically about them. I think \"You look beautiful\" would probably be the most ideal response, (in that is both true and doesn't cause her to spiral) but if I didn't have time to think about my response, \"No, it doesn't make you look fat\" feels like a better response than something else which would cause her to spiral. If I feel the need to actually help her choose the right dress, I can back that up by pointing out exactly how some other dress that looks better on her actually looks good. Of course, if I feel like she could take it well, then pointing out exactly how the bad dress is bad might be helpful to her. However, I feel like keeping her from spiraling is more important then informing her of every detail of my taste in dresses. Obviously, I made this scenario very concrete, but I feel like you can't really decide if it is a good idea without that very concrete knowledge. Which is one reason why white lies might be a bad idea in professional settings, where you don't necessarily know how people will respond to things in the first place. (Because you might not know them well) reply tdeck 2 hours agorootparentprevTo me GP seems like an insane person with an out of whack understanding of what \"white lies\" are and what's going on in that social dynamic. I think they need to seriously get a grip. Normally I wouldn't write things like this on HN because it's rude and they're a complete stranger anyway and I don't know what's in their head, but I wouldn't want to \"betray\" them by not saying it. reply genrilz 1 hour agorootparentI think I spelled it out in the post you are referencing, but I see a white lie as the equivalent of \"not saying something because it is rude\", except for when you are forced to say something and not doing so would be meaningful. I'm pretty curious about what you think white lies do in a social dynamic though. I'd appreciate it if you could elaborate. EDIT: My post is literally your GP, but based on your stance earlier in this thread, I think you were talking about aleph_minus_one's post I was responding to. In which case I think our opinions are similar? reply throw4950sh06 5 hours agorootparentprevIn this particular case, it was sort of the opposite. I'm used to her always being honest, so when she told me it's 15 minutes I trusted her 100%. She was getting a friend to rescue me, and she was counting on me trusting her and waiting at home, but unfortunately that was of no use as I went off to the city before they could even pack their things after that call. reply DiggyJohnson 4 hours agorootparentShould she have been more honest and said \"X will be their shortly. I'm really worried about you, please stay put.\"? Just trying to follow this chain of reasoning. reply throw4950sh06 2 hours agorootparentYes, that would work. I was really looking for confirmation, so if she said something like this the issue could be avoided. But who knows what I'd come up with 10 minutes later... reply aleph_minus_one 5 hours agorootparentprev> I'm used to her always being honest, so when she told me it's 15 minutes I trusted her 100%. But she lied to you - and this lead to your dangerous chain of thoughts. reply throw4950sh06 7 hours agorootparentprevRetrospectively, it was a total blunder - not truly hers since she couldn't be expected to act perfectly in that situation, but it's definitely something we've discussed as the TOP ONE thing to never do and always mention it to others when discussing situations like that. It didn't postpone anything. I called her because I knew I'm going mad and wanted to confirm if it's true or not. Of course I didn't word it this way, though. What she said confirmed the delusion, nearly got me killed and even though I didn't jump, I got lost in the city, hurt myself, nearly hurt others, until someone called the police few hours later. reply rightbyte 4 hours agorootparentOk I see. It is really hard for me to relate to, the feeling of believing I am Piccard. Like do you want logical reasoning? 'I am Gandalf' 'So where is your beard?' Or just reassurence? 'I am Gandalf' 'No you are not' reply throw4950sh06 4 hours agorootparentYou're reading into it too much. You don't think you're actually Picard or Kirk, you think the world around you is very similar to the stories, but not actually the story itself. In my case, it went like this: Whoa, my crew is telepathically beaming me instructions and we are right in the middle of a mission. Where the fuck am I? Crew, give me my command protocols. Oh, I command a starship? Sure, sorry, the heat around here, I might be hit with something, I don't remember, help me! Oh, you're going to transport me? Sure, go ahead, let me just run away from this commando waiting on the hallway using the only other exit - the window... Now it makes sense that I'm a starship captain, why would a commando be waiting there otherwise... (Your mileage may vary) reply MaxikCZ 8 hours agoparentprevI find this fascinating. Could you please elaborate about anything youd find relevant/interesting about how such delusions come about without being obvious delusions? I cant imagine actually believing I am Star Trek captain, but I sure can believe someone else do. I just cant imagine how that must feel/look like inside that someones head. reply throw4950sh06 7 hours agorootparentAt the beginning, you know you're mad. I remember the first hour or so, I was thinking \"no fucking way this is real\". But it feels so real that you quickly stop believing anyone who says otherwise and you mark them as the enemies. Your head keeps inventing reasons why is it real and the voices keep explaining it - in some cases it's religious experiences, in my case it's hyper-advanced technology enabling telepathic communication. I didn't think it's the Star Trek from movies, I just thought we somehow made it work in secret and now I'm on it too. Paranoid people aren't paranoid just so, they are paranoid because there is a brutal mismatch between their perception of reality and what people tell them. At one point, in a different situation, I knew I'm in the middle of psychosis - and my voices told me all about super-agent-psychiatrists who are trying to help me by doing James Bond-style interventions. So yeah, you can simultaneously know you're right in the middle of it, and discuss the situation with your delusions, while thinking the delusions are real. reply bowsamic 6 hours agorootparentHow do you know it isn't true? Philip K Dick came to believe in his psychosis visions (he believed he truly was in Rome in 60AD or so but was being fed a created world by the Romans). But he had good personal evidence for it that he couldn't deny even in a non-psychotic state. Do you have anything like that? I guess a better way to phrase it is, do you have compelling evidence that your beliefs are true that you have to force yourself to ignore, or does it just seem like nonsense when you aren't in a psychotic state? reply kayodelycaon 5 hours agorootparentNot OP, my manic episodes come with extreme paranoia, and I have had two psychotic breaks during really bad ones. This may be a completely different experience. Apologies if I sound a bit flippant. For myself, my brain always knows what reality is because all of my senses work. Delusions are clearly internal. My self-awareness is firmly in reality but all I can do is watch myself react as if the delusions are reality. Many people can’t grasp this. Awareness and control are always linked. “Blind rage” is just that. Awareness is gone. I hope no one else ever has to experience being powerless in their own body and screaming uselessly in their head to make it stop. As horrifying as this sounds, these experiences don’t haunt me. I thought they were just burnout from stress and carried on like nothing happened afterwards. “Normalization of deviance” doesn’t even begin to describe my life experience. Eventually, I was convinced by my doctor to see a psychiatrist for ADHD. It wasn’t until my third visit she realized I had severe, high-functioning bipolar. Once I got over a month of denial, it was “Okay. That does explain a lot.” XD reply throw4950sh06 5 hours agorootparentprevYou're not suddenly irrational in a psychosis, you still have your logic working, just with crooked inputs. So it took me months to sort through some details and make sense of what actually happened and what didn't. There are some things I'm probably never going to be able to explain and I just have to leave it like that. But I don't believe any of my delusions happened, I just would like to know what happened. All the voices, and the sense of urgency and danger go away immediately when you wake up after a dose of antipsychotic medication. Your first thoughts are that you lived through some weird things which are not happening at all anymore, and now there's a psychiatrist untying you from a hospital bed and handing you a cigarette, which puts stuff into context. You also probably feel the best you felt in weeks/months because it's your first night of sleep since forever. I can easily imagine someone thinking \"well, I had a psychosis, but there was shit going on\". Fortunately that's not me. reply Modified3019 2 hours agorootparentI really appreciate your responses (and ones others have made elsewhere in this thread), they give much better insight into what someone is going through internally than the clinical definitions I see. reply nathan_compton 5 hours agorootparentprevJust a general piece of advice: when a person is discussing their struggles with psychotic delusions, its kind of messed up to say \"Yeah, but how do you know they aren't real?\" reply kayodelycaon 5 hours agorootparentWhile incrediblybadly phrased, I feel it is an honest question on how psychosis works. Then again, I treat my whole life is a fascinating science experiment and people have to beg me to stop talking about it. reply bowsamic 5 hours agorootparentprevWhy is it \"messed up\"? It's a genuine question reply kybernetikos 21 minutes agorootparentGiven the context in this conversation about how confirming a delusion can be dangerous, I think the concern is just that there may be situations where asking this genuine and interesting question could cause harm. I think probably that on balance you've got away with it though because the people commenting seem to be safely outside the other side of their psychosis and are able to answer interestingly without being harmed by the question. reply kelseyfrog 1 hour agorootparentprevThe answer lies in this - can a question be both genuine and \"messed up\"? reply AStonesThrow 37 minutes agorootparentprevYeah, it was odd because the first time I told a psychiatrist that I heard voices, it was because of a split-second incident out in the street where I swear I heard a distinct vocalization from the vicinity of a traffic light. No human was there, of course, and the illusion was over before it began. That was enough to slap a prescription on me for years to come. Eventually I began to question why they kept wanting to prescribe this stuff and why one of the standard questions was always \"do you hear voices?\" and I also began to question their terminology. \"What do you mean, by hearing voices?\" \"Oh, well, hallucinations.\" and I drilled down into their definitions and epistemology for a while. I told them that I am a Christian, and of course I hear voices. People of faith, who are quite sane, discuss this openly all the time. We are always encouraged to listen to the voice of Jesus, the voice of the Holy Spirit, to listen to the voices of those who wrote Sacred Scripture. I told the doc that I'd be crazy (and lost, and significantly more troubled) if I didn't hear anyone's voice. Of course they're probing for stuff to medicate, they're probing for irrationality, and they're probing for evil voices who goad us to do harm to ourselves or to others. And of course I was troubled by those types too. But they weren't unreal. They weren't hallucinations because the sources exist in reality. They don't come from human bodies, but spirits are real to Christians. The solution is not to medicate the voices or deny that the voices exist or to ignore the voices, it's to form our consciences so that we can stand up to lies, stand up to temptation, and resist evil. It's as simple as that. Whether the voices come from Mom and Dad, or social media, or television, or they're 100% in our heads, we need to discern their spirits, and deal with them according to our conscience. It was so jarring that the doctors would be goading me to deny my faith in this way and to claim that if I heard a voice encouraging me in a moral direction, that it was fake, a hallucination, a disease. I have been so profoundly insulted. This is one of the many reasons I lost trust in them. reply hedgew 7 hours agorootparentprevA hallmark feature of psychosis and schizophrenia is lack of \"insight\", meaning that the patient can't recognize that they are having delusions, nor the fact that they are suffering from the illness. The belief that you are a Star Trek captain feels as real as knocking on wood. The illnesses simultaneously cause hallucinations that enforce delusions, and twist your belief systems so you pick up on the most insignificant details to support your delusions. Almost all patients end up believing that they are god, Star Trek captains, or stalked by a government agency, because this best explains their (hallucinatory) experiences. For example, if you hear voices in your head, the patient can't usually understand it as an illness, but has to explain it in some other way, so you end up with CIA/god/whatever beaming voices into your head. reply shaky-carrousel 6 hours agorootparentSeems like when you are dreaming, where the part of you that can assess if something is realistic or not is shut down. reply kayodelycaon 5 hours agorootparentFor myself, my imagination and view of reality merged. My senses were fine but all of the processing and my imagination started writing to the same memory spaces. I was aware that my senses didn’t match what I was processing. It didn’t matter. reply zackmorris 2 hours agorootparentprevI've had sleep walking episodes for most of my life since I was about 5, probably driven by sleep apnea. I've also had experiences that are as real as this waking life while meditating and especially back in my party days. The real awakening for me was when it finally clicked that we are always hallucinating everything. The mind separates our conscious awareness from the 3D world, like in Plato's Allegory of the Cave. So what we see and hear isn't what's objectively real, but what our mind interprets it to be. Even though everything is real in our subjective reality, based on the contextual state that we've built up from the sum of our experiences. Some examples of mass psychosis: * Many people don't know that their boss charges more than they're paid in wages. * Many people work administrative and loss-leader jobs and perceive their work as a cost on the organization (programmers, engineers, most people outside of sales). * Many people think that those around them are more knowledgeable and/or experienced than they are, and don't realize that their manager or boss is mostly winging it based on a probabilistic estimate of the best course of action. * Many people think that they are more knowledgeable and/or experienced than everyone around them (egocentric people working in IT/tech, doctors, lawyers, billionaires, etc). * Many people think that everyone else shares their spiritual worldview, everything from a man in the sky to we're all one in universal consciousness. * Many people think that others don't share their spiritual worldview (Christianity and Judaism may not see parents giving up their meals for their starving children in a bombed out Islamic community). How can we have civilized society, including free and fair elections, under such mass hysteria? When people have so many delusions that politicians can pit half the population against the other merely be selecting sides from a short list of wedge issues? My personal feeling is that western culture can't really endure spiritual awakening. And that we are seeing the breakdown of western society under late-stage capitalism with societal psychoses like much of the working class having to pay 50% of its income in rents. And corporate-greed-driven inflation rising unchecked without updated tax brackets for progressive taxation. And social safety nets being shredded to create a desperate working class dependent on service work while corporate profits are at an all-time high. I just wish I knew how to wake up from The Matrix, whatever all this is. The points above have concrete solutions like a national tenant union, enforcing antitrust laws, taxing unrealized stock gains the same way as property taxes on homes, etc. But those obvious solutions assume a level of lucidity that will probably never exist while the powers that be lobby the government and engage in regulatory capture while handing out million dollar checks at random to voters who selected the candidate that promises to cut rich people's taxes. All to keep most people worried about the price of groceries and immigrants stealing their jobs. But hey, I'm the delusional one. Edit: the best answer I've come up with so far, after suffering for a lifetime under self-imposed limitations driven by many of the psychoses above, was to quiet my internal monologue entirely, acknowledging each thought but not indulging it, just being consciously aware of the process of living, without attachment or expectation on outcomes. reply arijo 7 hours agoprevI've been diagnosed with schizoaffective disorder when I was in my early twenties. I've had several full blown psychotic episodes and been hospitalised several times. Fortunately there was one medication - Amisulpride - that kept me stable enough to be able to have a professional career, though not without a lot of struggling and sacrifice. I know what psychosis is and honestly, this avatar therapy feels a bit like bullshit to me. When in psychosis, you are not listening to your voices - you are your voices and they can command you to do things you do not want to do. You are not in control of you consciousness. There is hope though. A revolution in mental illnessis going on - check the metabolic mind site for more info - https://metabolicmind.org I talk about my experience in my blog - https://www.feelingbuggy.com/p/finding-hope-after-decades-of... Many other people have substantially improved because of metabolic therapy and there are dozens of random control trials going on with very promising early results. There are ever more cases of successful treatment via metabolic therapy Under the risk of being unpopular, it's my responsibility to let people know about this treatment option and bring hope to those who suffer from this terrible illness. reply Workaccount2 5 hours agoparentI wouldn't call it bullshit, it's just unlikely to work for everyone. reply arijo 4 hours agorootparentI don't question the good faith of the researcher but i think we should prioritise what works - and there is a growing volume of evidence that metabolic therapies work. I take back the bullshit qualification though - it was a mistake on my part. reply nabla9 7 hours agoprev>As much as 8 percent of the population reports experiencing auditory hallucinations on a regular basis (13 percent hear them at least occasionally), compared to just 1 percent who are diagnosed with schizophrenia. https://medicine.yale.edu/news/yale-medicine-magazine/articl... Hearing voices may be a symptom of something serious, but not always. As long as a person's grasp of reality is not in danger and voices don't stress out people, they can live with them and not even seek help. Not all people hear negative voices. Older lonely people have been known to say that voices keep them company. > voice-hearing experiences of people with serious psychotic disorders are shaped by local culture – in the U.S., the voices are harsh and threatening; in Africa and India, they are more benign and playful. https://news.stanford.edu/stories/2014/07/voices-culture-luh... Maybe the most famous case of a high-functioning outlier was Carl Jung. He hallucinated complete persons since childhood ,visually and everything. He discussed matters with them. In the end, he was able to get rid of them when he decided that they were not helpful anymore. It's easier to understand his weird theories and spirituality when you have read his autobiography. The guy was off the charts but not disabled by it. reply ryandv 6 hours agoparent> Maybe the most famous case of a high-functioning outlier was Carl Jung. He hallucinated completely people since childhood ,visually and everything. It's maybe easier to understand his focus on individuation, or integration of the component parts of the psyche into a cohesive whole, in light of this. Jung draws from esoteric alchemy and other related traditions (see his work: Psychology and Alchemy) which, instead of viewing the mind as a singular, monolithic entity, prefer conceiving of it as an organism comprised of many parts - Hermetic Qabalah being one such example. reply DevX101 8 hours agoprevOne of my favorite speculative hypotheses is Bicameral Mind Theory, which asserts that something like schizophrenia was relatively common until relatively recently, about 3000 BC. It argues that it was relatively normal for humans to hear voices in their head directing them. So when we read religious texts about the gods commanding so-and-so to do such-and-such, it wasn't just a spiritual metaphor, but an actual voice people heard in their heads and interpreted as higher powers. reply tetris11 7 hours agoparentMy mother did a lot to keep us fed, clothed, housed when we were growing up. She wasn't happy, but she wasn't sad either. I asked her once, decades later, how she coped with it all, and her answer still freaks me out sometimes. She said she didn’t know she was a person who had choices, or could think about the situation she was in. She just did as she was expected to. She’s in her 60s now and is far more in tune with her emotions, thoughts and feelings than I remember her when I was a kid. She was virtually in catatonic autopilot most of her life, because no one encouraged her to think. There's a quote from Helen Keller in this vein, after she was taught to communicate, though I can't find the full text: \"When the sun of consciousness first shone upon me, behold a miracle!\" reply gnramires 6 hours agorootparentVery touching ;) In buddhism (I'm not a specialist by any means), I think awareness and consciousness is a central concept too. \"The light of awareness\", \"awakening\" is seen as holy. I really think that awareness is essential to enabling us to have hope to improve and specially understand our life and our problems; besides making us more connected, participating in reality. I like Thich Nhat Hanh's saying of a \"serene encounter with reality\" he finds through meditation. Different, each in its own poetry, ways of saying the similar things. And I think the profound realization and awareness of the reality of others can bring no other outcome than compassion. That's why I think awareness is extremely important too for living in a society and enable living in a civilization. reply jumping_frog 7 hours agorootparentprevOperators and Things: The Inner Life of a Schizophrenic I think you should read the above book. reply tetris11 5 hours agorootparentApparently it's free to read: https://selfdefinition.org/hearing-voices/Barbara-O'Brien-Op... reply webstrand 6 hours agoparentprevHumans migrated to the Americas by at the latest 8900 BC when the land bridge disappeared, and we have evidence for migrations way before that. There's no way any kind of genetic change that occurred around 3000 BC could have made it to the Americas, and the indigenous population does not seem to exhibit such divergent modes of thought. They are essentially modern humans. So this date needs to be pushed way back. reply squidgedcricket 3 hours agorootparentIIRC, the bicameral mind theory speculates the change occuring ~70 kya. reply poizan42 2 hours agorootparentNo, Jaynes posits it happened as recently as 3000 BC, and bases the hypothesis on evidence from sources that are far more recent than 70 kya (not that we have any literary sources that are 70 kya old) reply Biologist123 7 hours agoparentprevI’d speculatively propose the reverse: that a change occurred that led to our suppressing these “voices” to our subconscious, where they are still present but suppressed below the threshold of conscious awareness. Interestingly, in parts of the world, these voices persist (eg Ethiopia where a high percentage of people report regularly hearing voices in their heads). Also interestingly, this book you mention was one of David Bowie’s favourites - yes, he of the multiple pop personas. reply uh_uh 8 hours agoparentprevWhy did the schizophrenia become less common according to this theory? reply ryandv 7 hours agorootparentThe phenomenon of auditory hallucinations often attributed to schizophrenics in modern day, was, far from being an aberrant condition, in fact the normal every day condition of ancient bicameral man. There was no concept of \"I\" or \"free will;\" the gods merely issued their auditory commands, and man obeyed, not having any choice in the matter. This was simply every day life for the bicameral man, who lived in a community of other \"like-minded\" beings that also experienced similar auditory hallucinations, often commands or admonishments, from the same god-king ruling the civilization. Neurologically Jaynes locates these voices, and even defines gods to be those particular neurological phenomena located in the right hemisphere of the brain, that communicated its preverbal judgements to \"man,\" located in the left hemisphere, which interpreted such judgments as speech: The gods were in no sense \"figments of the imagination\" of anyone. They *were* man's volition. They occupied his nervous system, probably his right hemisphere, and from stores of admonitory and preceptive experience, transmuted this experience into articulated speech which then \"told\" the man what to do. Several factors were attributed to its decline: the development of written language localizing a disembodied voice that was once omnipresent into a stele or rock carving; the emergence of trade and the contact of other societies, governed by different god-kings, leading to proto-theories of mind meant to explain the differing behaviour of the rival civilization; selective pressures against the viability of such \"bicameral theocracies;\" the development of free will. Reasons abound, but what followed was the collapse of bicameral mind and the disappearance of the voices, substantiated by various observations of ancient cultural artifacts: hypnotic induction of trance at the Oracle of Delphi, the practices of divination and omen reading, the production of artistic works in Mesopotamia depicting empty thrones and absent gods, or the crying out to the gods for their assistance and return, as in The Babylonian Theodicy, or the Psalms. All this in order to \"re-awaken\" the voices of gods that had once dispensed wisdom and now fell dormant with the emergence of modern, self-conscious, free-willed, individualistic man. reply soco 7 hours agorootparentprevThe two hemispheres started to communicate (better) so wouldn't feel each other as strangers anymore. Or at least that was my understanding, in very simple words and without asking any AI :) reply mapt 7 hours agorootparentIn market economics and evolution alike, when we posit that an organism/industry became more effective through some kind of change, we also require an explanation for \"And why hadn't it already done this? What was stopping it from doing so earlier, and what was the mechanism that this constraint was removed?\" Genuinely unexploited niches and entirely novel strategies exist, but they are extraordinarily rare. Do other primates show strong bicameral connectivity in terms of physical brain structures? reply cultofmetatron 7 hours agorootparentprevpeople drank a lot more alcohol back in the day. reply tetris11 5 hours agorootparentI don't know how far back \"back in the day\" is, but the UK sources below seem to show that alcohol (particularly spirits)[1] seemed to be quite popular in the late 1800s[0], and that in the 1550s it was Ale, 1650s it was Gin, 1750s saw a drop during the industrial revolution. Before that, who knows? 0: https://publications.parliament.uk/pa/cm200910/cmselect/cmhe... 1: https://ourworldindata.org/grapher/per-capita-alcohol-1890 reply aleph_minus_one 6 hours agorootparentprevsource? reply Gooblebrai 8 hours agoparentprevI thought it had been debunked already? reply jerf 5 hours agorootparentIt's really a non-scientific idea, in the Popplerian sense. How would you debunk it? How would you confirm it? It's not in that class of idea. A lot of people have been taught to read that as \"therefore it's false\" or \"therefore it's true\" or \"therefore it's unimportant\", but really all it means is that it is not amenable to scientific confirmation or debunking. That, and nothing more. Many things are not amenable to scientific confirmation or debunking that are true and false and important and unimportant and everything else. reply swayvil 4 hours agorootparentIt is false and unimportant, thus it deserves no attention, thus we withdraw our attention from it, thus it fades and disappears. Thus reality is carved away, leaving only the important part. Which we call \"reality\" of course. reply jerf 3 hours agorootparentOh, you've debunked it? That's pretty cool. I look forward to your published interviews with people from the time. You have an opinion that the hypothesis is false. I share that opinion. We probably have fairly radically different reasons for our opinions, though. Either way, they are just opinions, however well informed. We don't have proof. reply swayvil 2 hours agorootparentThat isn't a debunking, it's a description of the mechanism. For good and ill. Read it again. reply kelseyfrog 1 hour agorootparentprevWalk me through how you concluded it is false, please reply swayvil 1 hour agorootparentBy \"it\" I refer to reality. Not the model. (Gotta underline everything for you people, I swear.) reply kelseyfrog 1 hour agorootparentGo on.. reply jumping_frog 7 hours agorootparentprevRichard Dawkins called Julian Jaynes’s 1976 book, The Origin of Consciousness in the Breakdown of the Bicameral Mind “either complete rubbish or a work of consummate genius, nothing in between” reply nabla9 6 hours agorootparentIdea or theory can be great, well constructed and intellectually interesting without being true. Aquatic ape hypothesis is similar. It's a initially a great theory and explains many things. Only when you start to investigate other explanations, the theory starts to fall a apart. Endurance running hypothesis on the other hand seems to hold. Incidentally it explains many of the same things as aquatic ape hypothesis, and there is more supporting evidence. reply mensetmanusman 6 hours agorootparentprevWith a Time Machine? reply JimmyBuckets 9 hours agoprevInterestingly this idea of helping people to talk to the voices in their head is not new. The basis for IFS therapy (which emerged in the 80s) actively teaches people to have dialogue with their inner \"parts\". It is becoming one of the gold-standard therapies for CPTSD, anxiety, and a range of other trauma related conditions. The core discovery of the therapy is that the human mind has an inherent multiplicity. Once you accept that and go from there, the rest of the technique emerges naturally. It's really quite amazing. I highly recommend the book \"No bad parts\" by Dr Richard Schwartz, the discoverer of the technique. What really excites me here is the use of a virtual avatar that personifies the voice. That is really new to me and I can see all sorts of possibilities to link with IFS. reply ben_w 9 hours agoparentI suspect* that every school of psychotherapy represents a different internal configuration that human brains are capable of being in. Perhaps just as some but not all of us are aphantasic, some but not all of us may think in the IFS way, or Jungian, or Freudian. * from my comfortable armchair, don't read too much into this reply jdietrich 8 hours agorootparentMost schools of psychotherapy are equally effective, with the very notable exception of CBT for anxiety disorders. For most patients, the school of psychotherapy only matters insofar as they buy into it - nearly all of the therapeutic benefit is totally independent of the particular methodology or even the training and experience of the therapist. Even therapeutic approaches specifically designed to be pure placebo turn out to be just as effective as everything else. If IFS or Freudian psychoanalysis are metaphors for how we think, then they just aren't useful metaphors. https://en.wikipedia.org/wiki/Dodo_bird_verdict https://psychiatryonline.org/doi/10.1176/appi.psychotherapy.... reply JimmyBuckets 8 hours agorootparentThis just isn't proven at all. And likely untrue. There is a reason the dodo bird effect is controversial. 1. Most therapies have not received the number of RCTs as CBT so it's not possible to make statements like the one you made. 2. There is wildly varying quality in therapists and the quality of the therapeutic relationship is widely accepted to be centrally important to treatment outcomes. It is thus much more likely that a good therapist with a shitty tool is better than a bad therapist with a good tool. Averaged out, this would explain the same effect. reply jdietrich 5 hours agorootparentYou've inverted the burden of proof. If someone dreams up a new kind of therapy, it's their job to prove that it actually works; they can't just assert that it works based on anecdote. I'm flattering those relatively-untested therapies by assuming that they're all equally effective. In any case, the effect size over pill placebo is extremely small (again, with the honourable exception of CBT for anxiety disorders). reply Izkata 3 hours agorootparentI can't tell from either of your comments whether you're saying CBT has proven itself or falls short of the others. Just that it's different. reply JimmyBuckets 9 hours agorootparentprevI would tend to agree with you here, with a caveat. All these theories are describing the same underlying phenomenon so there is a \"blind men and the elephant\" effect. They also substantially build on one another The caveat is that what really sets these models apart is how they propose to navigate the mind. This is where I believe IFS stands out. But it would take a much longer comment to explain that. Maybe it's worth writing an article about. reply squidgedcricket 3 hours agorootparentprevI've come to the conclusion that psychodynamic therapy is harmful for neurotic depressives like myself. Dwelling in my neuroses enhances them. I wish there was a triage psychiatrist I could see that would help me identify the most effective type of therapy for my situation and then help me find a therapist. reply eru 9 hours agorootparentprevWell, some of these schools are also just plain bullshit. reply fsckboy 8 hours agorootparentyes, and much of the criticism as well: it's called being humans reply JimmyBuckets 8 hours agorootparentprevI would hesitate to say bullshit. They are all models and very abstract relative to the complexity of the mind. But to extend the blind men and the elephant allegory, there is a practical difference between describing a leg and describing a trunk. reply ulbu 7 hours agorootparentand some say elephants don’t exist because they can’t see one. reply soco 7 hours agoparentprevThis reminds me of a book I loved and still love, which was mentioned recently in another topic: Peter Watts - Blindsight. Hard sf with a bit of everything, including this multiplicity you mention. reply ryandv 6 hours agoparentprevSee also Genpo Roshi's Big Mind, Big Heart which has one assume the persona of various aspects of their mind and attempt to carry out a dialogue as that aspect. reply dmonitor 31 minutes agoprevI know a few people that tell me they experience \"voices\", but not necessarily hostile ones. The common mechanism they use to organize their thoughts is to assign a name to the voice (or multiple voices) and possibly an appearance to it. I believe it's referred to as a plural system? I honestly don't know what to make of any of it since it's so unlike anything I've personally experienced. They seem to not be self-destructive, though, so whatever they're doing must be effective for themselves. I can't imagine that the mechanism behind it is too different than the one responsible for psychosis, so it makes sense that a similar approach would be effective. reply kybernetikos 17 minutes agoparentThere's an approach to therapy called internal family systems that sounds a bit like this. reply akdor1154 9 hours agoprevFantastic piece of writing. Looks like a really promising approach to therapy as well.. right up until they said they'd stop voicing it by a skilled psychologist and get an AI to do it, while putting the psychotic person into VR instead of over a screen.. that was a big 'fuck no'. reply probably_wrong 8 hours agoparentI see where they're coming from, though: right now you have to be certified on this very specific program, meaning you only get the benefits if you have access to one of the 38 people currently trained for it in the UK. I would definitely want a professional to be in charge but, as the article itself points out, \"Joe recently went back to his GP in search of help with his anxiety (...) The GP put him on a waiting list for NHS talking therapy, and warned that he could be in for a very long wait\". Given how bad access to mental health resources is I may be willing to take \"a community nurse, or a nursing assistant\" now over \"wait several months for a chance at a doctor who may not be the right fit for you\". I wouldn't dream of allowing an AI to roam free - as the article says patients can get more psychotic and arguably \"you should end it\" could very well be part of the training data. But if the AI suggests lines that a trained human can oversee... then maybe? reply gnfargbl 8 hours agorootparentI think your proposal of AI therapists with human overseers would be okay if we were able to develop some kind of metrication and monitoring of the human oversight portion. Without that control, what would inevitably happen would be that the highly-scalable part of the system (the AI) would be scaled, and the difficult-to-scale part of the system (the human) would not. We would fairly quickly end up with a situation where a single human was \"overseeing\" hundreds or thousands of AI conversations, at which point the oversight would become ineffective. I don't know how to metricate and monitor human oversight of AI systems, but it feels like there are already other systems (like Air Traffic Control) where we manage to do similar things. reply Nevermark 9 hours agoparentprevIf they are going to get creative, perhaps apply the constructive effects of some mind altering drugs? Under AI shaman supervision of course! I have never heard voices, but experienced two forms of dissociation for a while after a trauma. Nothing was real, was one of them. Couldn’t trust any scene I was in or the chair I was going to sit on. Unending vertigo and feelings of experiencing a fiction. As a famous ex-newt once said: “I got better!” reply yieldcrv 8 hours agoparentprevI think it’s absolutely weird that the proctor is voicing the avatar I’m imagining some Unreal Engine Skyrim deity on screen being voiced by my therapist, acting with a vocoder. Like, c’mon. Definitely train a computer to do this part, generate your psychosis demon and have it really say the abstractions you described. Theyre already shockingly scary in realism when theyre not prompted to be. A VR headset might be a little too immersive and triggering But as long as its supervised I think its better reply consp 8 hours agorootparentYes you definitely want a possibly suicidal person to be talking to a \"AI\" engine who talks back with the avatar they normally hear. (This was sarcasm in case you missed it) reply yieldcrv 4 hours agorootparentLook at the article, people are walking out because of how ridiculous it is, not because of how triggering it is, this article is too much of a puff piece to say. And your criticism was the exact reason the article’s therapist puppet master technique was avoided. So we are already passed that point, lets get the roleplaying puppet master out the way. I dont want some therapist that gets off on dissing me as a ventriloquist. reply AStonesThrow 8 hours agorootparentprevO Superman https://music.youtube.com/watch?v=S39NaDPNDtk&si=-lXReadJB7h... Language Is a Virus https://music.youtube.com/watch?v=S39NaDPNDtk&si=-lXReadJB7h... reply iandanforth 6 hours agoprevThis reminds me of mirror box therapy for phantom limb syndrome. Amputees sometimes feel intense pain in the limbs that they have lost. This is measurable activity in the brain. Unfortunately all of the feedback mechanisms for that activity have been removed with the limb. By showing people a mirrored version of their remaining limb. And then stimulating that limb in a way that would remove pain say massage or touch or unclenching a cramped clenched phantom hand, the patient gets the feedback to the part of their brain via their visual system that there's nothing wrong with that limb. This activates inhibitory circuits that would otherwise be inaccessible. In this therapy a combination of visual and audio input as well as external control over the behavior of the Voice allows for a feedback mechanism which does not exist otherwise. reply AlexDragusin 8 hours agoprevThis concept has been explored in the \"The Outer Limits (1995)\" episode 5 of season 2: \"Mind Over Matter\" \"A doctor uses a virtual reality A.I. device that gives him direct access inside the human mind to enter the mind of a colleague he deeply cares about and help her after she's hit by a car and slips into a coma. Things go horribly wrong.\" https://www.imdb.com/title/tt0667922/ reply jumping_frog 7 hours agoparentIn Ted Chiang's short story Exhalation, a person looks at his own mind's workings. Brilliant description of the process. reply stego-tech 5 hours agoprevI’m glad this sort of treatment is getting more exploration and research. While I’ve never heard voices myself, childhood trauma did teach me to anthropomorphize my thoughts so I could discuss things more casually, instead of clinically; I felt more at ease verbally sparring with myself in isolation instead of cooping big, complex thoughts inside my head or onto paper. I chalked it up as a quirk until a therapist recently told me that sort of coping mechanism was quite healthy and “advanced, something it can take patients years to develop with guidance” - so I keep doing it. Avatar therapy could have implications far beyond psychosis, I think. My “round tables” have helped me begin piecing together why certain behaviors follow seemingly unrelated events (e.g., why going for a walk often ends with my coming home with candy), and untangle the automatic decision-making processes of my own brain. It’s also helped me identify what I actually desire in my own life, as opposed to what I’m sold on during “autopilot”. Said revelations have steered me back towards therapy yet again, seeking professional guidance on my own observations, which I think is the best possible outcome for mental health quandaries. All of which is to say, I hope to see more human to human research on this topic. The end part, where they want to place AI in command of the voices so it can scale, seems incredibly risky for all but the most “mainstream” of cases, and far too risky for those struggling with literal psychosis; then again, I’m not a Doctor, so I could be very wrong in my concerns. Guess we’ll see when the study wraps. reply teknopaul 6 hours agoprevI could have told you that voicing deamons in your head gets rid of them 20 years ago. A bunch of my friends know this trick too. I have successfully used this in therapy, i.e. helping friends on many, many occasions. Strange that this is news in the psychiatric world: we \"discovered\" it by caring, and listening, and trying stuff out. If psychiatric community is not listening to patients and conversing with them and their demons, and trying pills as a solution while knowing this doesn't work, it should be considered malpractice. No wonder everyone hates psychiatrists. Calling themselves doctors and the humans in front of them patients, is probably the root cause. reply aphantastic 3 hours agoparentJesus did the same 2000 years ago. But scientists tend to be superstitious of anything He demonstrated. reply beowulfey 6 hours agoprevKind of relevant to this, there is literature to support the notion that the 'personality' of internal voices is shaped by society -- people experiencing it in Western nations tend to have voices be very negative and violent, whereas people of other nations tend to have more friendly relationships with their auditory hallucinations. It definitely fits with this therapy, where building a relationship with the voice tends to benefit the mental health of the person [1]. [1] https://pubmed.ncbi.nlm.nih.gov/24970772/ reply 39896880 5 hours agoprevThe Atlantic reported on similar research in 2014. The article from The Atlantic is titled \"Learning to Live with the Voices in Your Head\" [0]. Here's a choice quote from that article: >“The problem,” [Intervoice founder Dr. Marius Romme] writes, “is not hearing voices, but the inability to cope with the experience.” In 1987, after two decades of clinical work, the Dutch psychiatrist began promoting a drug-free therapy in which patients were encouraged to accept and analyze their voices. The article linked in this post has a similar sentiment from one of the patients: >She hadn’t expected them to go. “My aim wasn’t to get rid of them – just to get along with them,” she told me. “I wasn’t quite sure I wanted to let go. I’d never really been on my own. As abusive as it was, it’s still a relationship.” Hopefully the therapy takes off, and we don't keep re-discovering this every 10 years. [0]https://archive.is/pfKrC#selection-1413.89-1413.360 reply eszed 6 hours agoprevThis is fascinating. What I note is that the therapist is collaborating with the patient to create \"theatre\". I think they should lean into this, new therapists should be explicitly trained in performance and theatre facilitation. There are a lot of solved problems with which they may otherwise struggle. reply satori99 9 hours agoprevThis trial has its own website too. https://www.avatartherapytrial.com/what-is-avatar-therapy reply ne0flex 4 hours agoprevI've read that the voices that people with schizophrenia hear can vary depending on their cultural background (people from US hearing violent/confrontational voices, people from India hearing more playful voices / helpful voices). This case talks about affirming for those with violent voices. Wonder how that is with someone who hears pleasant voices. reply criddell 4 hours agoprev> if you ask voice-hearers to elaborate, you might engage in ‘collusion’: you may make [the voices] more real for people. Validation therapy does exactly this, although I think the people are usually dealing with dementia, not schizophrenia. reply Dilettante_ 9 hours agoprev>Trial participants would create an avatar of their voice: a moving, three-dimensional digital embodiment that looks and sounds like the persecutor inside their heads Shoulda just made a Jackie Chan tulpa instead reply mensetmanusman 6 hours agoprevThe multiplicity of the mind is a subset property of the universe; the human mind is the ability of the universe to ask ‘why something, not nothing’. It took billions of years for the seeds of the mind to flourish, and prayer is a slow reconciling with this property of the universe. reply HipstaJules 9 hours agoprevThis is incredible work. Kudos to the team who developed this!! reply dr_dshiv 9 hours agoprevI wonder whether the mental inhibition of the voices ends up strengthening them. If you drop the conscious inhibition, they fade through other means. reply protoman3000 6 hours agoprevReading this article makes me wonder. Whenever I am too stressed, I become very self-conscious and I interpret everyday things and normal nuisances under the umbrella of \"See, this is how big of a failure you are, you pathetic loser\". Is almost as if there's another voice - not my voice - in my head who tells me this abusive stuff, sitting on the side and being judgemental. Does this also count as psychosis? reply gs17 2 hours agoparentThat sounds a lot more like \"automatic negative thoughts\". I don't think it's considered a form of psychosis, even if arguably it's sort of similar (you're hearing a voice that isn't there telling you things that aren't real). reply 123pie123 6 hours agoparentprevI use to have this, the book \"the power of now\" explains how to minimise and remove it. remember that the internal monologue is not (the whole of) you. I managed to get rid of of my imposter syndrome by getting rid of that constant internal voice reply swayvil 4 hours agoprevHallucinogenic mushrooms. When trip (it's been a while) I invariably hear voices and have conversations with them. I call them \"spirits\". They're friendly. Offer advice and such. reply InDubioProRubio 9 hours agoprevThe whole schizophrenic spectrum is a reversion to survival mode. This is what a animal hears all day- \"They are out to get me, its one huge conspiracy\" is the thought process of a mouse. Its low energy by default, as it does not require complex thought processes, relative little communication, very little planning for the future. Its a adaption to a warzone and economic stressors shake it loose. reply markovs_gun 9 hours agoparentI don't know if this is true. The way psychosis manifests is influenced heavily by culture. In some places, hallucinations from psychosis are largely positive rather than threatening. https://news.stanford.edu/stories/2014/07/voices-culture-luh... reply ryandv 6 hours agorootparentIndeed; the Oracles at Delphi were revered for their ability to divine wisdom from auditory hallucinations which were literally equated with gods. In some modern western cultures, by contrast, you even have active gaslighting of those who are vulnerable to such conditions and exacerbation of their negative qualities. reply markovs_gun 3 hours agorootparentI would be careful equating the oracles of Delphi with psychosis, since religious practices can bring about visions and out of body experiences without drugs or underlying pathologies. For example, some Eurasian shamans practice \"ecstatic dance\" where oxygen deprivation and exhaustion result in shamanistic experiences (visions, auditory hallucinations, etc) without the use of drugs. reply mandmandam 7 hours agoparentprev> The whole schizophrenic spectrum is a reversion to survival mode. There could be some seed of truth in there, but, sweeping statements about what schizophrenia is need to be delivered with more than assertion. > This is what a animal hears all day- \"They are out to get me, its one huge conspiracy\" Not even wild animals think like this all the time. I feel confident saying they live very different inner lives to what you describe. > is the thought process of a mouse Comparing people with schizophrenia to terrified mice is bad. Not all schizophrenic delusions are paranoid in nature, for one thing. And for another, even if the basic mechanism were fundamentally the same (totally unproven) the differences between mouse brains and human brains, mouse culture and human culture, etc, makes the comparison rather pointless. Add to all this, the fact that no non-human animal has been definitively shown to have schizophrenia in the same way humans experience it. And of the animals that might have schizophrenia, like mice with a disrupted DISC1 gene, wouldn't they be different to normal, healthily anxious mice? ... Please, consider trying not to make declarations about stuff like this when your domain knowledge isn't quite up to the task. reply eth0up 5 hours agoprevI've never fully understood this voice in the 'ed thing. Does anyone with internal dialogue have schizophrenia? Or is this literal, auditory voice, indistinguishable from actual voice? Also, there's old technology in the wild capable of doing this[1] and I'd not be surprised to see it eventually become more easily available as hardware decreases in cost. It's already on the 'table' for advertisment purposes, albeit not microwave based. 1. https://en.m.wikipedia.org/wiki/Microwave_auditory_effect Edit, Various Links: https://gizmodo.com/we-will-beam-advertisements-directly-int... https://www.holosonics.com/ https://www.timesofisrael.com/futuristic-device-from-israeli... https://phys.org/news/2008-02-pentagon-lasers-voices.html https://www.wired.com/2007/06/darpas-sonic-pr/ https://futurism.com/the-byte/laser-beam-speech-mit reply weard_beard 6 hours agoprev [–] We all have personalized profiles with little daemon AIs actually tracking and persecuting us. Torturing our every waking moment with judgmental advertising. No wonder, then, if we are all depressed or terrified or generally mentally ill. It’s directly caused by the lack of privacy controls and harm done by being online. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Avatar therapy, developed by Prof Julian Leff, uses digital avatars to help patients engage with and control the voices they hear, showing promising results in treating psychosis.- This therapy has proven effective for individuals like Joe, who did not respond to traditional treatments such as antipsychotics and Cognitive Behavioral Therapy (CBT).- Trials indicate that avatar therapy is quicker and more cost-effective than existing treatments, with potential for broader application in the NHS and future exploration of AI-powered avatars for other mental health conditions."
    ],
    "commentSummary": [],
    "points": 136,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1730191439
  },
  {
    "id": 41978246,
    "title": "An indie studio created a game based on Stanislaw Lem's novel",
    "originLink": "https://invinciblethegame.com/?hn",
    "originBody": "AVAILABLE NOW ON PC & CONSOLES WATCH TRAILER READ COMICS BUY NOW Discover the limits of human understanding Immerse yourself in a story based on a classic sci-fi novel by Stanisław Lem, about mankind's stubborn push towards places that are not meant for us. Join a brave scientist on her perilous journey Your name is Yasna. You wake up on a planet Regis III, with no recollection of how you got there. It's up to you to unravel the secrets of this unexplored world. Explore a beautiful planet untouched by human Get ready to traverse through the breathtaking landscapes of Regis III, a barren planet where the true danger lies deeper than you think. Immerse yourself in a game with unique aesthetics Witness a world through the lens of a retro-futuristic, atompunk art style accompanied by an atmospheric score composed by Brunon Lubas. Subscribe to our newsletter Subscribe I agree to receive news, offers, promotions, trailers and other information from 11 bit studios S.A. and I am 16 years old or older. For more information please check our Privacy Policy. Your personal data shall be administrated by 11 BIT STUDIOS S.A.. If you're under 16 years of age, this e-mail subscription requires constent of your legal guardian/statutory agent (i.e. parent). Developer Publisher Cert Powered by Partner Presspack Read comics Wallpapers Contact All Rights Reserved © 2009-2022 11 bit studios S.A. Cookies Policy / Privacy Policy / User Agreement © 2023 Advanced Micro Devices, Inc. All rights reserved. AMD and the AMD Arrow logo and combinations thereof are trademarks of Advanced Micro Devices, Inc.",
    "commentLink": "https://news.ycombinator.com/item?id=41978246",
    "commentBody": "An indie studio created a game based on Stanislaw Lem's novel (invinciblethegame.com)128 points by mgl 18 hours agohidepastfavorite56 comments dragonfax 16 hours agoIts a prequel to the novel actually. But I don't think the advertising makes that apparent enough. Its a walking simulator for the most part. (For those that know what that means) Think of it as a journey you take part it. But there are a few choices you can make to change a bit of who dies, and a affect a slight change in the ending. I enjoyed it thorougly. And felt it was a great representation of the retrofuturistic world the book presented, and stayed mostly in the style of that era. reply foggyToads 15 hours agoparentMay I ask that you please add a spoiler warning? :) reply andrewflnr 15 hours agorootparentThe spoiler is that non-zero characters die? Or did they edit their post? reply notachatbot123 11 hours agorootparentNow you made that a spoiler, and a really easy one to catch the eye. Why? reply andrewflnr 1 hour agorootparentIt was already very obvious, so I don't take responsibility for that. But I remain unsure that's what foggyToads was talking about, since it's such a very silly \"spoiler\" to complain about, and I like to think better of people than that. reply pests 14 hours agorootparentprevKnowing anyone dies would be a spoiler, no? reply andrewflnr 13 hours agorootparentTechnically, in that it reveals non-zero bits about the ending, but really it would be more surprising if no one died, given conventional story telling in that kind of setting. reply Kiro 10 hours agorootparentnext [7 more] [flagged] krisoft 10 hours agorootparentOh look who is the boss! Tell us: when you bought hacker news where there a lot of negotiations or did you just tell them that you have it now and that was that? Personally i could’t care less about spoilers even when they are actual spoilers. If the work is any good it will work with or without knowing details just the same. In this case there is so little information shared that we can’t even talk about it being a spoiler. Not worth wrapping ones mind about it. reply nosianu 10 hours agorootparent> If the work is any good it will work with or without knowing details just the same Counter example: Only because the twist of the actual reveal of what project Horizon Zero Dawn really was (in the game of that name) did it become a very emotional moment for me. I had to stop the gameplay video (I never play games, I watch the stories of story-rich games on Youtube) and cry for a bit. Surprises and \"reveals\" can be important. I agree with spoilers found in discussions about a work though. One has to expect finding them there, and it is easily avoided by not reading a discussion about a work before you read or watched it. reply andrewflnr 1 hour agorootparentPersonally I agree. You only get one chance to learn something for the first time. But of course the central mystery of a whole game is a much more specific and interesting spoiler than \"someone in the story dies, probably\", which is the alleged \"spoiler\" I started by asking about. reply RandomThoughts3 10 hours agorootparentprevDon’t go read random discussion about things you don’t want to be spoiled about. Nobody owes you anything with regards to what is or isn’t discussed. reply pests 2 hours agorootparentYou don't know what you are reading.. until you read it. It takes only a second to be polite, apparently that's too much to ask. reply echoangle 28 minutes agorootparentIf spoilers are so bad for you, you should probably look at the submitted link before reading the comments. reply mysterydip 7 hours agorootparentprevKnowing nothing about the game besides that website, I assumed from the pic that a non-zero amount of characters die. reply eleveriven 8 hours agoparentprevI think the \"walking simulator\" format sounds fitting here reply krige 12 hours agoprevThe game is superb. I was approaching it from a highly critical standpoint, between the choice of a new story in the same setting, and overall rather poor track of Lem adaptations to other media than books, which I felt either didn't try to be faithful, didn't have any budget, or both. But the game won me over near instantly. Its pacing, presentation, and writing are top notch. Overall I feel that it's worth a try whether you know the original story or not. Knowing the book will spoil a lot of the, let's call them twists, very quickly, but will also provide interesting context for things shown before the protagonist finds things out on her own. reply 0x38B 16 hours agoprevI haven't played the game because my 11th-gen Intel Framework 13 isn't that fast for running newer games, but I love the story - here's an excerpt from one of my favorite scenes in the book, the battle between the Invincible's Cyclops and the dark cloud of nano-machines: > In principle it was not used on the surface of a planet, and the truth was that the Invincible had never once mobilized its Cyclops. Situations that called for such an eventuality, even on a scale of the entire tonnage of the space base, could be counted on the fingers of one hand. In the jargon used on board, sending out the Cyclops for some task meant the same certainty as entrusting it to the Devil himself. No one had ever heard of any Cyclops failing. > the black scrub on the slopes began to smoke and set off in waves toward the vehicle from Earth, coming on with such vehemence that in the first instant the Cyclops disappeared completely, concealed by what looked like a cape of tarry smoke flung from above. At once, however, a ragged flash lit up the entire breadth of the attacking cloud. This was not the Cyclops using its terrible weaponry, merely the cloud’s energy fields striking against the force field. > Out of the corner of his eye, Rohan saw the commander open his mouth to ask the Chief Engineer standing next to him if the field would hold out—but the words didn’t come. He didn’t have time. > The black whirlwind, the walls of the ravine, the bushes—all of it disappeared in a split second. It looked as if a volcano breathing fire had opened up in a fissure in the rock. A column of smoke and frothing lava, shattered rocks, and finally, immense white billows of steam that probably came from the boiling waters of the stream, soared a mile into the air to where the TV relay was hovering. The Cyclops had activated its antimatter cannon. reply braiamp 16 hours agoparent> I haven't played the game because my 11th-gen Intel Framework 13 isn't that fast for running newer games Interesting. There's nothing on the system requirement that would suggest so. Are laptop chips that underpowered? reply ASalazarMX 1 hour agorootparentCan confirm that, although it might be a different combination of issues in my case. I played it in a laptop, on Linux, through Proton. There's a certain driving scene that consistently slows and eventually freezes my computer, so haven't been able to finish the game. The weird thing is, it doesn't look like a resource-intensive game, but somehow it is. I've been waiting months and eagerly trying every game/proton update, and it still happens. Pity because it has been a really nice story-driven game so far. reply flohofwoe 4 hours agorootparentprevFWIW the game wasn't running all that great on my (somewhat outdated) gaming PC with RTX2070 either. I can't imagine it being more than a slideshow on anything with an integrated GPU - I played it before some of the later patches though, maybe the team optimized rendering since then. reply 0x38B 15 hours agorootparentprevMaybe I'll give it a shot. My system really struggles to run Satisfactory, and even Rimworld can slow down when there's a lot happening. I've been considering a mainboard upgrade to an AMD Ryzen chip for speed & better built-in graphics. Other than graphics have been happy with my Intel system. reply 71bw 11 hours agorootparentprev>Are laptop chips that underpowered? Yes and especially Intel ones. reply scblock 16 hours agorootparentprevDepends on the laptop chip. I've never tried it on my 11th-gen Framework 13, but it plays well on the Steam Deck. reply flohofwoe 11 hours agoparentprevThe game isn't really based on the story though, which at least to me was a bit disappointing, the only common element is the 'smart dust'. It also went for a 50's pop-culture retro look which doesn't really fit together with Lem's stories. Other then that, I had a decent amount of fun with the game once I realized that it's more of an interactive graphics novel. reply shmeeed 10 hours agorootparentCurious as to why you feel the retro look doesn't fit? In my head canon, Lem's worlds look just about like that, because they were written in the era. It might be because I tend to glance over details in descriptions, though... reply flohofwoe 8 hours agorootparentI always thought of the 'Lem scifi universe' as split in two: on one hand you have the more realistic 'space trucker universe' of Pilot Pirx and the more serious stories (like the Invincible), in my head this universe always looked more industrial and like an evolution of our present technology, e.g. more Alien than Star Wars, and dirtier than Odyssee 2001 - in the Pirx stories there's a lot of little details about imperfect technology and bureaucracy, from oil or fuel leaks on his rocket to having to waste days waiting \"in harbor\" for getting one or another thing approved. And the other Lem universe is where Ijon Tichy lives, more funny, bizarre, almost phantasy like Star Wars (and the 50s retro-scifi style would fit more into this Tichy universe). reply virtualritz 10 hours agoprev\"The Invisible\" was the first adult sci-fi book I read. I think I was in 2nd grade and had just learned how to read. Very early 80's. I recall that my arts teacher told me about Lem in 4th grade when I was drawing some spaceship and I told her I already read some books by him. She didn't believe me until I told her details about the stories. \"The Invincible\" was mind blowing at that age and set a pretty high standard for everything in that genre I read after. I still think my dad left it laying on the living room table intentionally for me to find. He had almost everything Lem had written until then and was probably fed up with me reading the pulp kids/teen sci-fi that I was at the time. As I grew up in Germany pretty much all of Lems works were readily available in German, which definitely helped. Speaking of Lem adaptions: I was already doing computer graphics/VFX in my early 20's when I gave reading \"Solaris\" another try (it had been too long in the tooth for me in parts as a teen). And I recall the visual description of the ocean at multiple scales (it's two pages or more) and though: that will take years for VFX tools to get to the stage where we can visualize that. I'd say we've only been there since less than a decade. reply raddan 16 hours agoprevInteresting. I read this book over the summer (and also discovered in the process that my library also has a collection of Lem in Polish too! Too bad I don’t know Polish). I suppose as long as they are focusing on an “atmospheric retelling” of the story, it doesn’t really matter that none of Lem’s stories ever end in human triumph. This one ends mostly in confusion and failure. I also recently learned that there was a new English translation of Solaris. This version is much-hyped for being closer to the original, but as I read it, I am finding that I preferred the original translation (even though Lem himself reportedly did not like it). Anyway, huge Lem fan. Maybe an immersive game-like experience will better serve Lem’s visions. I was sorely disappointed in Soderbergh’s awful film version of Solaris. It just didn’t capture the terror of the story at all. reply scblock 16 hours agoparentThe game is told from another perspective to the novel, and it happens just before the Invincible lands (so just before the book really starts). It's one of my recent favorites. reply mlsu 13 hours agoparentprevOn Solaris movies. You may find better luck with the Mosfilm adaptation. It's very beautiful, slow, contemplative. reply RandomThoughts3 10 hours agoparentprevTarkovsky’s adaptation is incredible but it is only loosely based on the book (for the better as far as I’m concerned but Lem fans might disagree). reply Paul_S 9 hours agoprevAmazing novel but this is neither a direct adaptation nor faithful to the novel's message and ideas. It's a modern walking simulator with a the most surface level veneer of what we now consider retro futurism but otherwise modern in its messaging. I'd rather they kept the core and modernised the paint. reply izacus 9 hours agoparentI really wish HN would have less of this negative bile you just spewed on someone elses work because it didn't conform to your every whim. reply flohofwoe 5 hours agorootparentIt's a perfectly valid point of view IMHO. The game is called The Invincible and marketed as being based on the Lem book, which builds up certain expectations which the game doesn't deliver. It's a decent game / visual novel on its own, but it has absolutely nothing in common with Lem's Invincible except the name and that one core idea (which by now has become a common science fiction trope anyway - like most of Lem's ideas). It also pulls a future Cold War scenario out of thin air which (as far as I remember) isn't even remotely mentioned in the book (which is more like a whodunit scifi crime novel which then becomes a tech thriller). Also the decision to use a retro-future lollipop art style similar to The Jetsons is 'controversial' to say the least. All in all, interesting and decent game (or rather \"interactive graphics novel\"), but marketing it as being based on Lem's book when in reality it's something entirely different (not just a book adaption with 'artistic freedom', but something entirely different) is a bit too much IMHO. reply Paul_S 9 hours agorootparentprevI played the game and didn't like it, you're welcome to disagree and engage with me by showing examples that disprove my assertions from my original post. I think I'm on safe grounds calling it a walking simulator. The gameplay is limited to walking down a linear path and clicking on things until all combinations are exhausted - western equivalent of a visual novel (but with less branching). reply schmorptron 12 hours agoprevLooks really cool, but on a side note: is this self-promotion? There is a ?hn tag at the end of the link. reply TeMPOraL 11 hours agoparentMore like dupe detector buster - the link has been submitted some 4 months ago[0]; adding ?hn is a semi-conventional way to get it to appear as a new submission instead of counting as an upvote towards the older story. -- [0] - https://news.ycombinator.com/item?id=40921398 reply eru 11 hours agoparentprevSelf-promotion within reasonable limits is perfectly fine on HN. reply kstrauser 5 hours agorootparentIt is, but notice that every one of this person’s posts is self-promotion and they hardly interact with HN otherwise. Edit: You have to turn on showdead to see most of their posts. reply schmorptron 6 hours agorootparentprevYeah, absolutely! Just thought it would have to be tagged as such somehow? reply Pet_Ant 16 hours agoprevLem is highly underrated for not being published in America during sci-fi’s heyday. reply blueflow 10 hours agoprevThis is nowhere near canon, but the Planet from \"The Invincible\" could easily be Fulgora from \"Factorio: Space Age\", some tenths of thousand years later. The oily oceans have decomposed into water and methane, the construction robots have evolved, the planets resources and ruins eaten up. The \"rust layer\" in the sediments was the planet-spanning factory. The engineer has its origin in the Lyra civilisation, which went extinct before the Invincible showed up. reply ehnto 16 hours agoprevIt's incredible, maybe not for everyone though. It is slow, it takes a few large story beats to really kick off the story. If you are in the right headspace, and are a patient player who is able to sink into the atmosphere, you will be gripped by the pacing. The atmosphere is thick, dripping with retro-futurism, chilling environmental storytelling as well as thrilling story moments. The music will raise the hair on your skin. If you don't want to dive in, I recommend giving the OST a listen on spotify/YouTube. It stands on it's own and is a pretty good representation of the feeling of the game. reply Vespasian 11 hours agoparentQuick question. Do you remember any jump scares or the likes? I love the setting and the aesthetics but for my live I can't stand some monster/zombie/etc bursting suddenly out of a cupboard. reply izacus 9 hours agorootparentNo, I don't remember anything like that - there might be some tension when exploring things, but I remember no cases where things jump in your face. reply Vespasian 9 hours agorootparentExcellent. Thanks reply withinrafael 12 hours agoprevLoved the game (superb voice acting by Daisy May), reading the book now. The studio also sells a sweet snow globe, highly recommend! https://store.11bitstudios.com/product/snow-globe-the-invinc... reply ceving 10 hours agoprevIf it is truly following Stanislaw Lem's ideas it will be quite frustrating, because no player will be able to succeed. reply procflora 14 hours agoprevFor anyone who's read the book and played the game, a question. I have done neither but bought both last year for a rainy weekend's fun. Anybody have strong feelings about which I should start with? reply flohofwoe 8 hours agoparentThe two things are quite unrelated and only connected by one important story element. Definitely read the book first (also to let your imagination not be 'poisoned' by the game's art style), but then be prepared that the game doesn't have much in common with the book except this one story element. reply karczex 13 hours agoparentprevAs always, it depends. I would suggest to start with the book, as the game is kind of spoiler. Also it allows you to better compare your own vision of this cool retro futuristic world with game creators vision. reply shmeeed 4 hours agoparentprevTL;DR: if you're on the fence, go read the book. It's good and short to boot. I have to admit I've only seen part of a playthrough of /The Invicible/, but haven't played the game myself. Still, as a general rule, I personally would (almost) always recommend starting with the book form before trying a visual adaption. Having seen the visuals first will heavily influence your imagination and internal vision of the writing, which would deprive you of one of the greatest joys of reading. This book in particular paints some truly awesome mental pictures that I wouldn't want to have spoiled in any way. Another reasoning goes that the experience that came first tends to \"win out\" regarding your perception of the material. To me at least, a book is somehow less polarizing than a movie or a game. It stands for itself, and oftentimes (as is the case with /The Invicible/) has been standing as a recognised work for decades; it doesn't need to prove itself and can be taken as an artistic expression more or less free from the kind of economic incentives that necessarily plague a larger production. Any adaption comes with changes and, quite possibly, shortcomings that can be more or less individually tolerable. If you play the game and don't like it for the way it tells the story, its art style, a clunky UI or whatnot, you take that baggage with you to reading the book, or in the worst case even lose interest and forego doing so altogether. That would (arguably) be a much greater loss than the other way around. Adaptions do have their advantages, mainly in creating an impressive audiovisual environment, which can evoke some very immediate emotions. But those themes have been chosen and interpreted by the adaptor and are not necessarily identical to the original author's intentions. Condensation in adaption to a different medium generally comes at a loss of depth that IMHO makes it hard to re-experience the source material without bias, and thus encumbers its full appreciation; not least because even in the best case it forestalls the setting, and often the twists and the conclusion as well. In the end I would argue that a book tends to give a humanistically richer experience. It gives ample time to fathom its themes, to reflect and interpret them without visual distractions. Adaptions of great works can be great in their own right, but not too many stand the test of time as well as the books that came before them, and in that light I myself prefer to keep that order. reply adrianhon 10 hours agoprevSince this has been reposted by the same poster several times, I don't feel bad about reposting my review calling it a \"glorious failure\": https://mssv.net/2023/12/26/the-invincible/ tl,dr: It looks incredible, like no other game you'll see today, and it grapples with deeply interesting themes – but it's extremely annoying to play and suffers from a serious lack of editing. There's just too much friction to become truly immersed. Still, other games can only wish their failures were this daring. (Last time I made this comment it was immediately downvoted to oblivion. Shortly thereafter, the post itself disappeared from the HN front page, perhaps due to some kind of brigade-detection?) reply anonzzzies 10 hours agoprevThis would be nice in VR. reply eleveriven 8 hours agoparentVR would be perfect for this kind of atmospheric game... reply kstrauser 13 hours agoprev [–] Is this the same one you’ve already posted about 5 times before? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new game inspired by Stanisław Lem's classic sci-fi novel is now available on PC and consoles, featuring a storyline where players explore a mysterious planet as a scientist named Yasna.",
      "The game showcases a retro-futuristic, atompunk art style and includes music by Brunon Lubas, offering a unique visual and auditory experience.",
      "Developed by 11 bit studios S.A., the game invites players to uncover secrets on the planet Regis III, enhancing the narrative with its intriguing setting and design."
    ],
    "commentSummary": [
      "An indie studio developed a game as a prequel to Stanislaw Lem's novel, \"The Invincible,\" primarily functioning as a walking simulator with narrative choices.",
      "The game effectively portrays the retrofuturistic setting of the novel, though opinions differ on its fidelity to Lem's original work.",
      "Discussions have emerged regarding spoilers, system performance, and whether the game's repeated postings are genuine interest or self-promotion."
    ],
    "points": 128,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1730163146
  }
]
