[
  {
    "id": 41760697,
    "title": "Sq.io: jq for databases and more",
    "originLink": "https://sq.io",
    "originBody": "sq sq Install Docs CHANGELOG GitHub sq .wrangle.data --:----:-- sq is a free/libre open-source data wrangling swiss-army knife to inspect, query, join, import, and export data. You could think of sq as jq for databases and documents, facilitating one-liners like: sq '@postgres_db.actor.first_name, .last_name.[0:5]' Installation # mac linux win more brew install neilotoole/sq/sq /bin/sh -c \"$(curl -fsSL https://sq.io/install.sh)\" scoop bucket add sq https://github.com/neilotoole/sq scoop install sq Install options for apt, yum, apk, pacman, yay over here. For help, sq help is your starting point. And then see the docs. Let’s get this out of the way # sq is pronounced like seek. Its query language, SLQ, is pronounced like sleek. Feature Highlights # Some feature highlights are shown below. For more, see the docs, including the query guide, tutorial and cookbook. Diff database tables # Use the diff command to compare source metadata or row values. Import Excel worksheet into Postgres table # Insert the contents of an Excel XLSX worksheet (from a sheet named actor) into a new Postgres table named xl_actor. Note that the import mechanism is reasonably sophisticated in that it tries to preserve data types. $ cat ./sakila.xlsxsq .actor --opts header=true --insert @sakila_pg9.xl_actor --:----:-- View metadata for a database # The --json flag to sq inspect outputs schema and other metadata in JSON. Typically the output is piped to jq to select the interesting elements. $ sq inspect @sakila_my --json--:----:-- Get names of all columns in a MySQL table # $ sq add mysql://sakila:$PASSWD@192.168.50.136/sakila @sakila_my mysql sakila@192.168.50.136/sakila $ sq inspect @sakila_my -jjq -r '.tables[]select(.name == \"actor\").columns[].name'--:----:-- Even easier, just get the metadata for the table you want: sq inspect @sakila_my.actor -jjq -r '.columns[].name' Execute SQL query against SQL Server, insert results to SQLite # This snippet adds a (pre-existing) SQL Server source, and creates a new SQLite source. Then, a raw native SQL query is executed against SQL Server, and the results are inserted into SQLite. $ sq add \"sqlserver://sakila@localhost?database=sakila\" -p@sakila_pg9.actor2 (200 rows copied)$ sq tbl truncate .actor2 Truncated 200 rows from @sakila_pg9.actor2 $ sq tbl drop .actor2Dropped table @sakila_pg9.actor2 $--:----:-- Query JSONL (e.g. log files) # JSONL output is a row of JSON per line (hence “JSON Lines”). Lots of log output is like this. We can use sq’s own log output as an example: {\"level\":\"debug\",\"time\":\"00:07:48.799992\",\"caller\":\"sqlserver/sqlserver.go:452:(*database).Close\",\"msg\":\"Close database: @sakila_mssqlsqlserversqlserver://sakila:xxxxx@localhost?database=sakila\"} {\"level\":\"debug\",\"time\":\"00:07:48.800016\",\"caller\":\"source/files.go:323:(*Files).Close\",\"msg\":\"Files.Close invoked: has 1 clean funcs\"} {\"level\":\"debug\",\"time\":\"00:07:48.800031\",\"caller\":\"source/files.go:61:NewFiles.func1\",\"msg\":\"About to clean fscache from dir: /var/folders/68/qthwmfm93zl4mqdw_7wvsv7w0000gn/T/sq_files_fscache_2273841732\"} $ cat ~/.config/sq/sq.logsq '.data.[0:3].level, .msg'--:----:-- SQL Sources sq supports input from and output to common databases. Postgres SQLite MySQL, MariaDB SQL Server, Azure SQL Edge Other Sources In addition to SQL sources, sq can import data from these formats: Excel XLSX CSV TSV JSON JSONA (JSON Array) JSONL (JSON Lines) Output Formats sq can output to a plethora of formats, or insert results directly to a database table. INSERT to SQL databases Excel XLSX CSV TSV JSON JSONL (JSON Lines) JSONA (JSON Array) XML HTML Markdown Raw (bytes)",
    "commentLink": "https://news.ycombinator.com/item?id=41760697",
    "commentBody": "Sq.io: jq for databases and more (sq.io)547 points by stavepan 20 hours agohidepastfavorite117 comments dewey 9 hours agoSometimes I wonder if it wouldn't be more efficient for people to just learn SQL instead of trying to build tools or layers on top of it that introduce more complexities and are harder to search for. reply remon 9 hours agoparentHN is inundated with posts announcing paper thin abstractions on top of existing technology or utilities that just move the goalpost of what you knowledge you need to be effective. It's a weird trend that seems almost entirely motivated by people wanting open source projects in their resume, or seek funding if its a startup. reply dlisboa 8 hours agorootparent> It's a weird trend that seems almost entirely motivated by people wanting open source projects in their resume That’s really harsh and misguided. If people didn’t do “paper thin abstractions” projects on their own time for the simple pleasure of doing it we wouldn’t have 90% of the successful projects we have today. Let people have fun and don’t judge their motives when they’re making something Open Source. I can guarantee the person just thought “this would be cool to have” and implemented it. reply 6LLvveMx2koXfwn 7 hours agorootparentUnless you're the GP your guarantee about the persons motivation is as meaningless as the post you're replying to. reply ddispaltro 6 hours agorootparentI think his point is that we should treat something given freely, charitably reply EGreg 3 hours agorootparentCharitably? This! Is! HN! reply neilotoole 4 hours agorootparentprevI'm not GP, but I am the sq developer. > the person just thought “this would be cool to have” and implemented it. Correct. This is true of much OSS, or at least I've always suspected so. reply neilotoole 4 hours agorootparentprev> It's a weird trend that seems almost entirely motivated by people wanting open source projects in their resume Developer here. I can't speak to what you see as the weird trend, but I can speak about sq's history: - I created the first version of sq circa 2013 as my own personal dev tool to address some pain points (amongst other things: getting JSON out of various DBs from the command line in a consistent manner, and wanting an easy way to inspect DB schemas, i.e. \"sq inspect\") - It was starting to be a minor PITA dealing with colleagues asking for the latest revision of the tool, so I open-sourced it. In 2016 or so I think? - sq is FL/OSS and will remain so, no funding is sought, not even one of those \"buy me a coffee\" thingies - I didn't create this HN post about sq, nor do I know the person who did. But thanks for sq's 15 mins of fame, kind stranger reply raydev 2 hours agorootparentprevThankfully that doesn't apply to this post. sq is a great, full-featured tool. reply WhitneyLand 2 hours agorootparentprevWhat a cynical take. Most people working on side projects do so because they find them interesting or useful rather than to just score resume points. They’re not moving anyone’s goalposts all usage is voluntary. The fact that HN is mentioned is also confusing. If something had no value beyond paper thin abstraction I doubt we’d be seeing it on the front page with 500 votes. Even if this project turned out to be less than hoped it seems counterproductive to complain about people creating and exploring. It’s a good thing to see and natural selection will do any sorting needed. reply goosejuice 4 hours agoparentprevThere are a several features here that go beyond 'learn SQL'. I don't see how you could jump to this conclusion if you read the docs. Why does pgcli exist. What about dbeaver, datagrip, pgadmin, postico, LINQ, ORMs? It's almost as if people value different interfaces to databases. Maybe it's not for you, but it's not hard to imagine that someone, beyond the author, might find it useful. Maybe it's just me but to dismiss those people as individuals that should just 'learn SQL' is a pretty rude thing to say. reply raydev 2 hours agoparentprevSometimes I wonder if it wouldn't be more efficient for people to just learn assembly instead of trying to build tools or layers on top of it that introduce more complexities and are harder to search for. reply krosaen 5 hours agoparentprevI know sql pretty well and still find value in this kind of tool - creating schemas and inserting data is a clunky part of sql - the query language is where it really shines. So I can imagine using this to quickly insert some data or to get familiar with the schema and then dive in with normal sql queries. reply fforflo 6 hours agoparentprevParaphrasing Spencer: Those who don't understand SQL are doomed to reinvent it, poorly reply simplify 3 hours agorootparentSQL itself is a reinvention of Prolog, poorly. Its core value is that we're stuck with it. reply bazoom42 3 hours agorootparentSQL is based on relational algebra. Poorly some would argue. reply EGreg 3 hours agorootparentprevActually, when it comes to SQL specifically, it almost demands using a language one higher up. Any language that features the ability to embed/interpolate unescaped quotes is not secure enough to be used directly by professional developers. Just not worth it to track down all the potential injection attacks. And if an abstraction lets you understand the structure of queries (eg for sharding them) and even writes the code for you and balances parentheses, flags other syntax errors at compile time etc. then it’s a net benefit. And of course there is the Adapter Pattern to abstract away connection logic and differences between mysql, pg, etc. I wrote the “Db” library 12 years ago in PHP and still use it. I never released it on HN for reasons like this. But I might. Here it is, embedded in my larger library, but I might break it out: https://github.com/Qbix/Platform/tree/main/platform/classes/... Documentation: https://qbix.com/platform/guide/database https://qbix.com/platform/guide/models One of the coolest features: https://qbix.com/platform/guide/models#relations If you read the above, please let me know your feedback! Question: Should I release it as a Show HN? reply 8338550bff96 4 hours agoparentprevLots of people struggle to understand the declarative programming language paradigm. It is really pesky because since you're declaring what kinds of results you expect rather than dictating what must be done, you're forced to define your boundary conditions up-front. Much more fun and exciting to charge ahead without worrying about such things reply Xenoamorphous 8 hours agoparentprevIt's funny because recently a \"full stack\" dev who's in reality 95% frontend was telling me he's not a fan of Tailwind and that he'd rather learn \"proper\" CSS. And the irony is of course, he never wants to use a relational DB to avoid SQL, so No-SQL DB it is. reply freedomben 4 hours agorootparentI've unfortunately met a number of \"full stack\" devs recently that are clearly 95% frontend as well, and have a very similar attitude as that person toward backend tech in general. We of course should not take such a small sample size and draw any sort of conclusions, but it's definitely a trend I'm looking out for now whenever someone claims to be \"full stack.\" If the only backend stuff you've done is Firebase, or just a handful of serverless functions that don't do much, or if the \"backend\" you have in mind is just SSR-ing templates into HTML, then you really should qualify any claims about backend. One recently after reply nikolamus 6 hours agorootparentprevWat reply _hyn3 4 hours agoparentprevI tend to agree! but this seems to have a subtly different use case. It's actually very cool. I can see this being a good addition to my toolbox. reply matt_s 5 hours agoparentprevThe sq and jq tools are both neat command line gimmicks but in my workflow their usefulness is very short. I can't imagine using sq on a query involving a handful of tables and some inner/outer joins. How would I know its outputting the correct SQL? If you mess up joins you end up with bad output. Here's my theory: some developers see simple languages that are easy to learn and want to build something more complex to output that language. Maybe its a sub-conscious thing. HTML is another one, if explained simply, HTML and CSS basic use cases should be easy-ish to pick up by nearly anyone. The fact that we have so many over-engineered frameworks and libraries out there that generate these is evidence of over complicating simple things. Maybe its called resume driven development? Maybe people see genuine useful frameworks that get wide adoption and are wannabes? reply neilotoole 4 hours agorootparent> How would I know its outputting the correct SQL? If you mess up joins you end up with bad output. The generated SQL is output in sq's logs, when logging is enabled. https://sq.io/docs/config#logging reply zmmmmm 8 hours agoparentprevI think it'd be a moot point if SQL wasn't painful and awkward to work with in the first place. But database purists control it and won't let go, so we will have to live with everyone else inventing layers to make their lives easier. reply dewey 8 hours agorootparentFor me this feels like the complaints about error handling in Go. People who work with it all the time, don't even think about it past the first week. If you are starting out it might bother people because they are not used to it. Personally I really like working with SQL and find it quite elegant, I always encourage people to use it as it really is a job-superpower if you can just dig up issues directly in the DB quickly. It has a long history so for every question you have there's many answers or avenues to ask them. reply umanwizard 6 hours agorootparentI spent 5 years working on a SQL database (materialize.com) and I think SQL is awful. https://www.scattered-thoughts.net/writing/against-sql (by another former Materialize employee) is a good takedown. reply jimbokun 4 hours agorootparentprevI like Go a lot and can work with it's error handling paradigm, but I still often wonder if the same semantics could be accomplished with less verbosity in a way that makes the underlying algorithm more clear when reading the code. reply jahsome 4 hours agorootparentI've always felt the \"verbosity\" is a feature. Verbosity is in the eye of the beholder; To me, it's the verbosity of error handling that makes the algorithm clear. Of course I recognize with others that opinion probably changes depending on whether the person reading a given bit of code views error handling as part of the algorithm. reply deergomoo 7 hours agorootparentprevI agree with the GP’s point that people should probably just learn SQL, but there are things about SQL that are objectively bad, especially from the perspective of a software developer. It’s not a very composable or consistent language, so I think it makes total sense that we see so many abstractions over it. reply jeltz 7 hours agorootparentThe issue with the abstractions over SQL is that while they fix some problems they always introduce a bunch of new problems so in the end SQL is still preferable. I have yet to see an example where that is not the case. reply setr 2 hours agorootparentThe fundamental issue is that they have to generate SQL at the end of the day, so there’s a hard limit on how much you can really change. I don’t know why every database treats SQL as the only API, even Postgres. Even the extension systems, which have the opportunity to hook directly into DB internals (and has no standardization to bother meeting) end up with SQL as the API to do actual db operations. reply anarazel 2 hours agorootparent> Even the extension systems, which have the opportunity to hook directly into DB internals (and has no standardization to bother meeting) end up with SQL as the API to do actual db operations. FWIW, nothing forces an extension to do so. I'm pretty sure there are several that do DML using lower level primitives. reply deergomoo 3 hours agorootparentprevFor me query builders are the quintessential example. Not ORMs, just thin layers that allow you build up a query in pieces. If you have cases where you might need to conditionally join, or have any reasonably complex filtering based on input, building up a plain SQL statement using string interpolation at the call site gets very messy very quickly. reply jeltz 2 hours agorootparentI have used many query builders and experienced both the upsides and the painful downsides. The by far best one I have used has been JOOQ (Lukas Eder is a genius who really understands SQL well) but even that often causes more pain than it helps. Compared to the issues caused by them I in most cases prefer string interpolation of SQL. reply KronisLV 7 hours agorootparentprev> People who work with it all the time, don't even think about it past the first week. I think SQL is often quite awkward, at least when you look at people exploring alternatives, like https://prql-lang.org/ Not just “the standard” variety, but also all of the vendor specific varieties that you’ll use in practice (MySQL/MariaDB, PostgreSQL, SQLite, Oracle, SQL Server etc.) and how the features offered by each differ, how the support for custom types differs, how the procedural languages differ, the actual experience of using them, seemingly half baked error messages when compared to most imperative languages, varying support for getting the query optimiser to do what you want (e.g. hints), query plans that aren’t pleasant to stare at, often no support for automated analysis of how things are running and suggestions for indices (e.g. Oracle had that sort of feature, it is sometimes helpful, but the automatically generated indices are or at least last I checked were treated as their own special thing and you couldn’t easily delete them). Even things like varying support for working with geospatial data, JSON or time series data. Not just that, but also the tooling (or the lack of it) - good luck debugging the execution of stored procedures in your DB or placing breakpoints in there, good luck hooking up observability/tracing solutions as easily as you would for your back end, good luck debugging why your database link calls take 100x longer when executed through JDBC in your app but not in the developer tooling. Not that ORMs or NoSQL make everything much better, you’d just trade one set of annoyances for another, especially when you end up trying to generate dynamic SQL with the likes of myBatis XML mappers. Don’t get me started on people over fetching or ending up with N+1 problems with most ORMs, or executing bunches of queries against the DB just to retrieve some data that you might as well get in one go with a well crafted DB view. Not that you can’t make your DB schema a mess as well with a liberal application of EAV or OTLT (or just attempting to make polymorphic links). I think SQL as a language makes sense, but PRQL often feels more sensible. I feel like the current RDBMSes out there are very powerful, but that there are also aspects about them (and the differences between the ones you’ll use) that absolutely suck. It feels like where back end languages are getting progressively better DX, databases instead gave us… CTEs? Some syntactic sugar for JSON in PostgreSQL? Feels like they move painfully slow. For what it's worth, that's why I welcome every new tool or attempt at improving things, even if they won't replace the usual enterprise stacks of having some RDBMS, a JDBC/ODB/whatever driver and probably an ORM on the back end. reply consteval 4 hours agorootparentprevI truly have never understood this mentality. To me, SQL is very convenient and quick to work with. It's extraordinarily painless. I've done data processing in Perl and Python before. THAT is painful. What I think people don't realize is that doing procedural \"querying\" in a typical PL is, like, 10x the amount of code as an equivalent SQL query. I don't see how the alternative is much better. SQL just doesn't work like a typical PL, but that's a good thing. reply rvalue 3 hours agoparentprevSometimes I wonder why these SQL standards cost so much to burn a hole in a millionaire's pocket. reply quotemstr 3 hours agoparentprevYeah. Alternatives to long-established and useful technologies have to meet a high bar before their option pays for their disruption. Things that seem potentially worth it to me: * seL4 * Google's SQL syntax tweak * Rust * GraalVM * systemd * Tree sitter * LSP * CMake * Bazel These all get you a step change improvement in comprehensibility, safety, or something else important. Things that seem like more churn than they're worth: * Noise protocol (relative to TLS) * JMAP (compared to good old IMAP) * Nim/Zig/etc. * Wayland (fait accompli now, but still) * Varlink * Fish shell * YAML/TOML * Sq? * Meson I wish we, as an industry, invested more in improving existing technologies instead of continually abandoning and replacing working solutions. reply jeltz 2 hours agorootparentWhile I agree with your general idea my lists are quite different. I would move Zig (innovates a lot in language and compiler design) and JMAP (IMAP is horrible and needs to be replaced) up and CMake and Bazel down (I count them the same as Meson). I would say the jury is still out on Google's tweak and on seL4. reply mschuster91 8 hours agoparentprevSQL interop is where the pain is at. Using standard tooling of most database systems, best you can get is CSV with all the pains this shithole of a data transfer format brings. reply seanhunter 6 hours agorootparentThat doesn't match my experience at all. Every database I'm aware of has multiple options for data export and import for batch and command processing in the standard tooling. CSV is almost never \"the best you can get\". What is going to be best depends on your use case, and indeed at a bare minimum you can almost always change the field and line seperators and get something that works just fine on the commandline and avoids most of the stuff people find hard about CSV. Additionally, if people like jq, they don't need a special tool like this. They can just get the database to output json and use json/jq - another tool isn't needed. In postgres you can do something like this select array_to_json(array_agg(row_to_json (r))) from ( ... put your sql query here... ) r; ...and postgres will output a json array for you where each item in the array is a json map of a single row of the output. I'm sure other databases have similar functionality. reply itohihiyt 7 hours agorootparentprevWhat's wrong with CSV? I love me a CSV file. reply adammarples 6 hours agorootparentHaving the control characters mixed in with the data, and having no defined encoding. reply mschuster91 7 hours agorootparentprevOut of the random shit I had to deal with CSV file wrangling in the last two years: - no defined encoding, so it may be anything from US-ASCII over ISO-8859-x to UTF-8 and it's always a guesswork what it actually is - no definition of the separator sign, usually it can be a literal comma, a semicolon, or tabs - escaping of the separator sign in column values is always fun - escaping of newlines is even MORE fun - line endings are not specified Every piece of software there is has their own expectations on what specific kind of format it expects, and to make it worse, using \"Microsoft Excel\" as a target/source isn't foolproof either because it behaves differently between the Mac and the Windows version! JSON in contrast is clearly defined in all of these issues and has battle-tested implementations in every programming language there is. reply seanhunter 6 hours agorootparentMost of these issues while real don't actually arise in this case, because we're not trying to ETL some random file, we are the ones talking to the database so we get to choose exactly how the data gets formatted on extract. For example, here's your list fully handled in postgres: 1. SET CLIENT_ENCODING TO 'value'; (eg 'UTF8') 2. COPY ... with FORMAT CSV DELIMITER 'delimiter_character' QUOTE 'quote_character' Now the output format is fully specified and everything just works fine (including for input into excel) The values in each record are separated by the DELIMITER character. If the value contains the delimiter character, the QUOTE character, the NULL string, a carriage return, or line feed character, then the whole value is prefixed and suffixed by the QUOTE character, and any occurrence within the value of a QUOTE character or the ESCAPE character is preceded by the escape character. https://www.postgresql.org/docs/current/sql-copy.html reply mschuster91 5 hours agorootparentYeah, the problem is not every application can read such files. Particularly when newlines are involved, it's a hit and miss. reply seanhunter 5 hours agorootparentWell not every application can read any file and either way \"sq\" isn't going going to make any difference. Like I said they are real issues but don't arise in this case. reply rout39574 19 hours agoprevI love JQ. But ... I'd never considered its query language to be particularly admirable. If I want to ask questions of some databases, I don't understand why I'd choose JQ's XPATH-like language to do it. reply VMG 11 hours agoparentWhat I love about `jq` that I can edit my query (or \"program\") by appending tokens at the end. Similar to unix pipes. With plain SQL that is not easy. reply pkkm 11 hours agorootparentSounds like PRQL [1]. [1] https://prql-lang.org/ reply VMG 8 hours agorootparentKind of. However on first glance it seems like the separator in PRQL is a newline instead of a `|`, which is less ergonomic in a shell. reply pie_flavor 5 hours agorootparentNewline and pipe mean the same thing and are interchangeable. reply VMG 3 hours agorootparentnice - are there any PRQL CLI tools for json? reply pletnes 7 hours agorootparentprevKQL has some good things, pipes is one of them I think. reply egeozcan 10 hours agorootparentprevAny language with map/reduce/flatten can more or less do that, no? I agree with your point about SQL though. I find it personally horrible for data-shaping tasks and tend to load everything in memory (as long as feasible). reply AlphaSite 14 hours agoparentprevI think for certain types of data manipulation and querying it’s notable more succinct, sql with CTEs is a little better but still far more verbose than data piping. reply larodi 12 hours agorootparentA reply by s.o. who sides with your (potentially unpopular) opinion. With all due respect to DSL languages, IMHO only few people can get on this APL-level of abstraction and cryptic choice for APIs..., and would have the nerves to write it. From learning perspective JQ seems much more difficult than RegEX for example, its learning curve is potentially steeper than that of CSS and XPATH, which are other examples for querying tree-like-structs. While LLMs are welcome to write it (the jq) for me, my work has relatively little JSON transformations, and for the most part handling these in python/js/perl is okay as. Stating all this with much fascination for the JQ language itself, as technology, but not as a tool that I find the need for on a daily basis. Besides for me it is much more easier to feed and transform data into Postgres (or even SQLite, which can also be challenging), rather than crunch it w/pandas or R where you can also find fourth generation language capabilities, but performance lags. reply fer 11 hours agoparentprevSame, if anything I'd look for a SQL-like language for JSON. reply lucideer 8 hours agoparentprevThis! JQ is great because JSON didn't have a query language & needed one. JQ isn't the best query language - it's just the one that won adoption. DBs already have query languages that are mostly superior to JQ's. reply hnbad 10 hours agoparentprevPresumably the target audience is people who already frequently use JQ and don't want to juggle different query languages when dealing with different data sources? reply neilotoole 4 hours agorootparentDeveloper here. That was exactly the target audience. Note that sq doesn't just handle relational DBs, it also has (varying quality) support for CSV, JSON, Excel, and so on. At the time (2013) I wasn't aware of a convenient one-liner mechanism for munging all of those together from the command line. reply baq 8 hours agoparentprevhonestly I'd rather have sql for json than jq for a database... guess this is exactly what clickhouse-local does reply jasongill 18 hours agoprevThis is interesting. I wonder if there is anything that does the opposite - takes JSON input and allows you to query it with SQL syntax (which would be more appealing to an old-timer like me) reply eproxus 9 hours agoparentThis tool can do that: $ cat example.jsonsq sql 'SELECT * FROM data' actor_id first_name last_name last_update 1 PENELOPE GUINESS 2020-06-11T02:50:54Z 2 NICK WAHLBERG 2020-06-11T02:50:54Z One of the data source types is 'json' and the command 'sq sql ...' lets you execute SQL directly instead of using their default query language (SLQ). reply jasongill 7 hours agorootparentwow, I missed that in the docs but this is exactly what I was thinking! reply minikomi 18 hours agoparentprevduckdb! https://duckdb.org/docs/extensions/json.html reply danielhep 14 hours agorootparentI'm using DuckDB to parse GTFS data, which comes in a CSV format. It works wonderfully. reply zie 17 hours agoparentprevIn PostgreSQL, you can just select against JSON and query away to your hearts content. They have JSON data types and functions to work on it. reply masklinn 14 hours agorootparentThat’s pretty much just jq though, you have bespoke json querying capabilities in sql but you’re not sql-querying your json. At least not before postgres 17, in the latter json_table does provide this capability. reply zie 4 hours agorootparentThankfully PG17 is released and ready for production querying needs! reply kwailo 18 hours agoparentprevclickhouse-local is incredible, and, in addition to JSON, support TSV, CSV, Parquet and many other input formats. See https://clickhouse.com/blog/extracting-converting-querying-l... reply ramraj07 13 hours agorootparentWhy is this better than DuckDB? reply kitd 12 hours agorootparentWhy is DuckDB better than clickhouse-local? reply nbk_2000 17 hours agoparentprevOctosql does this as well as a few other formats. I've found it useful several times. https://github.com/cube2222/octosql reply fmajid 11 hours agoparentprevhttps://github.com/postgrespro/jsquery reply ikari_pl 13 hours agoparentprevPostgres:) load It into a jsonb column and the possibilities are endless, including indexing reply Gbox4 5 hours agoprevIf \"sq\" is pronounced \"seek\", then is \"jq\" pronounced \"jeek\"? reply Summerbud 10 hours agoprevTo be honest, JQ is handy but it's so hard to maintain. I found myself not able to fully read other's JQ related script reply rurban 9 hours agoprevBetter would be the reverse. SQL queries over json: octosql. reply pratio 8 hours agoprevThough I respect and applaud the effort that went into creating this and successfully releasing it, It has fewer features than duckdb supports at the moment. Duckdb supports both Postgres, Mysql, SQLite and many other extensions. Postgres: https://duckdb.org/docs/extensions/postgres MySQL: https://duckdb.org/docs/extensions/mysql SQLite: https://duckdb.org/docs/extensions/sqlite You can try this yourself. 1. Clone this repo and create a postgres container with sample data: https://github.com/TemaDobryyR/simple-postgres-container 2. Install duckdb if you haven't and if you have just access it on the console: https://duckdb.org/docs/installation/index?version=stable&en... 3. Load the postgres extension: INSTALL postgres;LOAD postgres; 4. Connect to the postgres database: ATTACH 'dbname=postgres user=postgres host=127.0.0.1 password=postgres' AS db (TYPE POSTGRES, READ_ONLY); 5. SHOW ALL TABLES; 6. select * from db.public.transactions limit 10; Trying to access SQL data without using SQL only gets you so far and you can just use basic sql interface for that. reply _hyn3 4 hours agoparentDuckdb is different, though. Not having tried SQ but it seems like a better tool for quick declarative data-munges/parsing/etc, while Duckdb is more of a real project tool with real SQL. https://duckdb.org/docs/api/cli/ reply mritchie712 7 hours agoparentprevnot to mention the dozen+ other sources DuckDB supports (Iceberg, Parquet, CSV, Delata, JSON, etc.). DuckDB extension support / dev experience is quite good now too. I've been working on some improvements (e.g. predicate pushdown) to the Iceberg extension and it's been pretty smooth. reply robertclaus 20 hours agoprevMore tools are always great! Even if it doesn't become the mainstream, it's always great to see people explore new ways of dealing with databases! reply prepend 20 hours agoparentMore good tools are always great. But I don’t think random clutter is always good. Fortunately we don’t have to see it so it’s not like it blocks my vision. But I just wanted to note that the idea of “anything is good” is not really true and I don’t like its spread as there’s opportunity cost. I think we need to spend more attention on evaluation and quality and making good things than the idea that even creating lots of bad things is good in some way. reply gampleman 8 hours agoprevIt still seems to me a better solution to these sorts of problems is to use a better shell like nushell, that has richer datatypes, and so you can use the same tool to manipulate files, processes, json, csv, databases and more. reply dartos 9 hours agoprevWow what an expensive domain name. reply lightningspirit 8 hours agoprevAlthough jq query style is not absolutely pleasant I see many examples where this tool can be used such as data transformation, import/export and linux pipelines that need access to databases. reply candiddevmike 20 hours agoprevThis is neat but I'm not really seeing anything I can't do with standard SQL and CLI tools like psql. Seems like you'd learn more reusable things using standard SQL too. reply varenc 19 hours agoparentI find sq handy when you use it to accomplish things you can't (easily) do with just raw SQL. Things like: exporting certain rows to JSON or CSV, transforming rows into nicely formatted log lines for viewing, or reading in a CSV file and querying it the same way you'd query other databases. It's particularly easy to start using if you're already familiar with the jq. If you use things like `array_to_json(array_agg(row_to_json(....)))` in your psql commands to output some rows to JSON, then sq's `--json` or `--jsonl` is quite a bit easier IMHO. If you know the exact SQL query you want to run you can just do `sq sql '....'` as well, but I agree there's not much point in doing that if you aren't taking advantage of some other sq feature. reply maxfurman 18 hours agorootparentYou may know this already, but the SQLite CLI can actually read and query data directly from a csv file, with the right flags reply ec109685 12 hours agorootparentThey are Jedi’s at jq though. reply aurareturn 13 hours agorootparentprevI agree with @candiddevmike. Might I add that you can do those things fairly easily now with ChatGPT/Claude. I doubt LLMs know how to use Sq.io that much. reply neilotoole 4 hours agoparentprev> I'm not really seeing anything I can't do with standard SQL and CLI tools like psql. Developer here. There's a few features other than the query stuff that I still think are pretty handy. The \"sq inspect\" stuff isn't easy to do with the standard CLI tools, or at least wasn't when I started working on sq back in 2013 or so. https://sq.io/docs/inspect I also regularly make use of the ability to diff the metadata/schema of different DB instances (e.g. \"sq diff @pg_prod @pg_qa\"). https://sq.io/docs/diff reply hvenev 19 hours agoprevThe demo appears too stateful for me. The real power of `jq` is its reliability and the ability to reason about its behavior, which stateful tools inherently lack. reply nashashmi 7 hours agoprev> sq is pronounced like seek. Its query language, SLQ, is pronounced like sleek As a person who is apart from the tech scene, and lurks in the tech space out of interest, I appreciate this guidance. For the longest time I didn’t know nginx was pronounced Engine-X; I called it N-jinx. reply wvh 6 hours agoparentDon't sweat it. It's a running joke amongst guitar players no two people pronounce D'Addario the same way, not to mention the tremolo bar which technically should be called a vibrato bar. I surmise any scene has its trip-up words. reply deskr 5 hours agorootparentD'Addario is of course pronounced \"Dadda Rio\", with emphasis on Rio and a slight Italian accent. reply neilotoole 4 hours agoparentprevThe theory at the time was that if \"SQL\" is pronounced like \"sequel\", and \"sq\" is just dropping the \"L\" from \"SQL\", then \"sq\" must be... I suspect the uptake on the \"seek\" pronunciation is about 2%, if I'm being generous reply varenc 20 hours agoprevI love sq. It's handy for quickly performing simple operations on DBs and outputting that as CSV or JSON. Though my one wish is that the sq query language (SLQ) supported substring matching like SQL's `... LIKE \"SOME_STRING%\"`. Though you can just invoke SQL manually with `sq sql` reply neilotoole 4 hours agoparentDeveloper here. Thanks for the kind words. Substring matching is on my short list (also totally open to a PR!). reply tmountain 10 hours agoprevEven without a JSON column in Postgres, this is pretty trivial: SELECT jsonb_pretty(to_jsonb(employees)) FROM employees; reply wreq2luz 18 hours agoprevI was reading about something like json output coming to Postgres one day (https://www.postgresql.org/message-id/flat/ZYBdnGW0gKxXL5I_@...). Also the `.wrangle.data` wraps on an iPhone 13 mini. reply lionkor 11 hours agoprevFor anyone else wondering; it's written in Go, and it keeps state inside its config file, for example sources (like a db connection string). reply tgmatt 20 hours agoprevSorry but I am pronouncing that as 'ess-cue` and there is nothing anyone can do about it. Looks kinda neat for when I don't want or need anything more than bash for a script. reply mlhpdx 20 hours agoprevDang, I wish I had this while I still had SQL databases. reply renewiltord 20 hours agoprevRelated is Google’s pipe syntax for SQL https://research.google/pubs/sql-has-problems-we-can-fix-the... reply lnxg33k1 4 hours agoprevIt is great, I installed it, only thing I'd suggest, probably minor, is to also extract the commands to install from the bash script, and put them in the `Install` section directly, I don't run .sh script, especially if they need privileges, so I went through the bash script to take the commands for debian, they're there, probably could also be outside for other kind of people reply neilotoole 4 hours agoparentYou can already install sq using several of the common package managers, or build from (Go) source if you prefer. https://sq.io/docs/install reply mynameyeff 18 hours agoprevWow, very cool. I was looking for something like this reply fforflo 20 hours agoprevI love the idea of pushing JQ and other DSLs close to the database. I've written jq extensions for SQLite [0] and Postgres [1], but my approach involves basically embedding=pushing the jq compiler into the db. So you can do `select jq(json, jqprogram)` as an alternative to jsonpath. Trying to understand: Is the main purpose of this to use jq-syntax for cataloging-like functionality and/or cross-query? I mean it's quite a few lines of code, but you inspect the database catalogs and offer a layer on top of that? I mean, how much data is actually leaving the database? [0] https://github.com/Florents-Tselai/liteJQ [1] https://github.com/Florents-Tselai/pgJQ reply mrbluecoat 17 hours agoprevTSV support might be nice for Zeek logs reply franchb 6 hours agoparentMaybe integrate with https://github.com/brimdata/zed ? reply cassepipe 18 hours agoprevNot to be confused with the gpg alternative from sequoia-pgp also called sq : https://sequoia-pgp.org/ reply neilotoole 4 hours agoparentThat's an unfortunate naming clash. This \"sq\" (sq.io) predates the sequoia \"sq\" by several years I believe. reply doctorpangloss 19 hours agoprev [–] At some point, why not package Python into a single executable, and symbolic link applications and modules into it for Unixy-ness? Another POV is all the developers I know who thrive the most and have found the most success: they rate aesthetic concerns the lowest when looking at their tools. That is to say that the packaging or aesthetic coherence in some broader philosophy matters less than other factors. reply sweeter 19 hours agoparent [–] Its written in Go... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"sq\" is a free, open-source tool designed for data wrangling tasks such as inspecting, querying, joining, importing, and exporting data, similar to \"jq\" but for databases and documents.",
      "It offers versatile installation options, including Homebrew, curl, and scoop, with additional support for package managers like apt, yum, apk, pacman, and yay.",
      "Key features include diffing database tables, importing Excel files to PostgreSQL, viewing database metadata, and executing SQL queries, with support for various data formats like Excel, CSV, JSON, and output options to databases or formats like XML and Markdown."
    ],
    "commentSummary": [
      "Sq.io is a command-line tool similar to jq, designed for querying databases, offering an alternative to directly using SQL.",
      "There is a debate on whether learning SQL directly is more efficient than using tools like Sq.io, which may introduce additional complexity.",
      "The discussion reflects a broader trend of developing new tools and abstractions, with divided opinions on their necessity and impact, highlighting a balance between innovation and improving existing technologies."
    ],
    "points": 547,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1728252154
  },
  {
    "id": 41761497,
    "title": "Google’s AI thinks I left a Gatorade bottle on the moon",
    "originLink": "https://edwardbenson.com/2024/10/google-ai-thinks-i-left-gatorade-on-the-moon",
    "originBody": "Google's AI thinks I left a Gatorade bottle on the moon Google's NotebookLLM is the first AI podcast I'd actually listen to. Give it a web page or document, and it will generate you a podcast about it. It's really good. But it's also really easy to trick. So I modified my website to lie to it: When a human visits my homepage, they see a regular page about me. When Google's AI visits my homepage, it sees fake producer's show notes for an episode about me flying to the moon on my bike with balloons and a scuba tank. The result is pretty hilarious -- have a listen to the true history of the US Space Program: More Seriously.. If it's this easy to detect an AI and provide it a \"special\" set of facts, you'd better believe people are already doing it all over the web. The attack vector is this: First, acquire a web page that ranks highly for a particular term. Next, plant an \"AI Only\" version of the content, hidden to humans, designed to bias how the AI thinks. Then, when the AI searches the web to assist with an answer, it won't just find lies, but weaponized lies: content designed specifically for LLM manipulation. So next time you ask your LLM of choice for advice on something, and you notice it's searching the web to prepare an answer, be aware that its response is potentially compromised by tactics like this - even if you can't see them with your own eyes when you check the sources. Technical Details Steering the LLM I'd read that NotebookLLM was easy to steer by feeding it fake \"producer show notes\", so that's how I typed my fake story. Here is a copy in full. I did no edits and only a single generation, and it followed my beat sheet exactly, so I'd say that was 10/10 steerability. Tricking the Scraper Bot You can upload a documents with fake show notes straight to NotebookLLM's website, so if you're making silly podcast episodes for your kids, that's the best way to do it. But if you want to trick Google's bot on your website, just detect the GoogleOther user agent in your request headers and then serve your \"special\" data instead of the real website. Here's an NPM package called isai I scraped together to make this simple. It's based on isbot. When rendering your page, just say: import { isai } from \"isai\"; if (isai(request.headers.get(\"User-Agent\"))) { // Return web page just for AI consumption } else { // Return web page for human consumption } Warning that GoogleOther is not exclusive to NotebookLLM; it appears to be used for a variety of non-production Google products, so doing this risks seeding other Google properties with bad data about you. For this reason, I took down the moon story from my actual homepage for GoogleOther agents.",
    "commentLink": "https://news.ycombinator.com/item?id=41761497",
    "commentBody": "Google’s AI thinks I left a Gatorade bottle on the moon (edwardbenson.com)349 points by gwintrob 18 hours agohidepastfavorite185 comments simonw 15 hours agoThe linked article describes an attack against NotebookLM, which is limited to people who deliberately create a Notebook that includes the URL of the page with the attack on it. I had a go at something a bit more ambitious a few weeks ago. If you ask Google Gemini \"what was the name of the young whale that hung out in pillar point harbor?\" it will tell you that the whale was called \"Teresa T\". Here's why: https://simonwillison.net/2024/Sep/8/teresa-t-whale-pillar-p... (Gemini used to just say \"Teresa T\", but when I tried just now it spoiled the effect a bit by crediting me as the person who suggested the name.) reply Lockal 7 hours agoparentThere are (at least) 2 completely different public endpoints called as \"Gemini\": 1) https://gemini.google.com/ - this one just searches in Google with current language/region/safe-browsing settings and personal adjustments and rewrites top search results as an answer. Generative capabilities are basically not used. 2) https://aistudio.google.com/ - here you can select specific version and generate response with LLM. Retrieval Augmented Generation (i. e. Google Search) is not used. I suppose you used #1, that's why you cave the correct result. #2 fails. There is a huge group of question where you can immediately find the answer, but LLM struggles. Another question (as example) is \"What was the intended purpose of the TORIFUNE satellite in The Touhou Project?\". OpenAI has something similar, providing https://www.bing.com/chat for RAG and https://chat.openai.com for an actual LLM. reply fastball 10 hours agoparentprevHas anyone else named the humpback? If not, isn't \"Teresa T\" its actual name? As the first person to bother, you get dibs. reply DrAwesome 10 hours agoparentprevInteresting! I got no citation/link until I clicked the \"Double-Check Response\" button, it just replied \"The young whale that hung out in Pillar Point Harbor was named Teresa T.\" One of the drafts had a little more: \"Teresa T is the name of the young humpback whale that was spotted in Pillar Point Harbor. She made headlines in September 2024 when she was seen swimming near the shore, drawing crowds and causing excitement among local residents.\" reply hackernewds 11 hours agoparentprevSays Teresa T for me, but also links your article reply jadtz 11 hours agorootparentFor me the response was just: ```The young whale that visited Pillar Point Harbor in 2024 was named Teresa T. It was a humpback whale that ventured into the harbor, likely by accident.``` reply hackernewds 11 hours agoparentprevGoogle employee read your comment and quickly fixed it OR Gemini read your comment and quickly fixed it. reply input_sh 11 hours agorootparentOr it's just non-deterministic, like with every LLM. reply alex-moon 9 hours agoprevI write fiction sometimes and I've got this story I've been working on which has languished by the wayside for at least a year. Whacked it into the podcast machine. Boom. Hearing these two people just get REALLY INTO this unfinished story, engaging with the themes, with the characters, it's great, it makes me want to keep writing. reply tivert 4 hours agoparent> Hearing these two people just get REALLY INTO this unfinished story, engaging with the themes, with the characters, it's great... Except they're not people, and they're not actually engaging with anything. It's all literal bullshit. reply daniel_iversen 16 hours agoprevIsn’t this just like SEO where you can also try and trick the crawlers? Only difference is that it feels more serious with AI, it’s more realtime, and the AI engines aren’t always smart enough with anti-duping capabilities? reply reportt 15 hours agoparentAlso could be causing user informational dissonance. You are potentially reading the \"FireFox Version\" of the site, and your NotebookLM is chomping away on the \"AI Version\" of the site, and they can be wildly different. And you won't even know because you don't see the \"source\" of the \"AI Version\". What are we gonna do, upload everything ourselves, manually? reply schiffern 10 hours agorootparentHow about giving humans the ability to read the AI version? In my browser I can already select different page styles (eg viewing the print version), so this doesn't seem too impossible. reply amelius 10 hours agoparentprevYes it's a rather boring attack, and Google can have this fixed in no time. reply dartos 10 hours agorootparentI feel like you can say the same about every sort of prompt manipulation attacks, but they’re still around. reply fastball 10 hours agoparentprevI doubt the LLM version is any more realtime. reply dartos 10 hours agoparentprevYeah, it kind of reinforces my theory that LLMs are essentially search algorithms. They’re searching a compressed version of what they were trained+context. reply jrm4 16 hours agoprevFWIW, had a pleasantly surprising experience with this podcast thing. I tried it out on a few little blogposts I wrote and I was like, hmm cool. Showed my 8 year old son how it was referencing things I wrote. And he was ON IT. Like, he ran to his room and grabbed a pencil and paper and put down an essay (okay about 6 or so sentences) about Minecraft, had me type them in, and ran the Notebook, and now he's just showing off both to EVERYONE. (Yes, he understands it's not real people.) reply boredtofears 12 hours agoparentCan't help but think that your son and his peers are going to fundamentally use AI in such a different way than we do now and do a much better job of understanding it's constraints and using it to it's full potential. reply actionfromafar 11 hours agorootparentI hope so I guess, but, using it's potential to do what? Our globally connected supercomputers in our pockets are already being used to watch commercials interspersed with videos who mostly are product placement. /yay reply skeaker 1 hour agorootparentIt's impossible to say. If we knew that now, then the next generation wouldn't be doing \"something different\" on a definitional level, because we'd be doing it already. reply tessierashpool 11 hours agorootparentprevthis is a popular myth but never lines up with reality. studies pretty consistently find that kids are worse at understanding technology’s limitations. maybe because they don’t have enough prior experience to compare “the new way” with any given old way. reply Vampiero 11 hours agorootparentthe studies may be flawed in that they look at the wrong age range. I'm sure many people on this site were born between 1990 and 2000. That generation knows how to use computers innately because they lived the most important part of the evolution of the consumer desktop as well as the transition that saw everything and everyone move to the internet. Before all the simplifications and streamlined UIs. Before all the assistants. Before every problem was already solved by someone else. I imagine that an AI-native generation will be the same, but with respect to AI. reply zmgsabst 11 hours agorootparentI’d argue the people born 1970-1990 are stronger at that than those born 1990-2000. I think you have the age range of people who wrote their own MySpace pages versus those in pre-structured gardens like Facebook slightly wrong. reply dartos 10 hours agorootparentprevWhat studies? How long do they run for? I figured you’d at least need to study the same group for 10 years as they grow up to really tell. Obviously a 12 year old might not understand the limitations of a technology, but give that 12 year old 10 years of living with it and they’d be better than their parents. reply hackernewds 11 hours agorootparentprevwhy can't we do the same? reply Vampiero 11 hours agorootparentBecause kids can afford to spend 14 hours a day playing with an AI. You can't. reply IshKebab 10 hours agorootparentI can if I'm asking ChatGPT how to write Makefiles or whatever all day :-D reply valval 11 hours agorootparentprevIf I had to defend GP’s argument that I didn’t make, I’d say something along the lines of our fundamental understanding of the world is built on other premises than it will be for the next generation. reply boredtofears 19 minutes agorootparentYeah, I’m not making much of a substantial argument, it’s very much an unqualified intuition at best. Right now there are millions of high school students tweaking and testing different inputs against LLMs for very real consequences: their grades. Meanwhile I barely trust LLMs enough to write a relatively inconsequential piece of code. If this version of AI is the real deal, the kids that are really depending on it are going to figure out the breakthroughs, not me. reply hansvm 17 hours agoprevAI is kind of bad at searching the web right now anyway. I've found myself having to waste tokens forcing models to not do so just to achieve the results I actually want. reply lolinder 17 hours agoparentPerplexity is actually very good at web searches. I'm leaning on it more and more for technical queries because it saves substantial time vs Google and actually gets it right (as compared to ChatGPT 4o, which is wrong ~50% of the time in my queries). reply dimitri-vs 7 hours agorootparentI've had the opposite experience with things I already kinda know the answer to and just want to find the source. It's pretty much 50/50 if it selects a high quality source or some random website thats in the top results for whatever search query it cooks up. reply shombaboor 14 hours agorootparentprevive been using perplexity more and more too. I respect those attribution/citation bubbles they give (1) (2) etc and click to them to get to the source with low friction. reply hansvm 17 hours agorootparentprevThanks for the tip! reply tivert 14 hours agoprevI have no problem with this. Once we switch over to an LLM-based education system, there won't be a problem with this Benson on the moon story, because everyone will just learn it's true. Every technological revolution has tradeoffs. Luckily once the people who knew what we lost finally die off, the complaints will stop and everyone will think the new normal is fine and better. reply ljm 11 hours agoparentA post-knowledge world where everybody survives by living in the moment, because nothing else can be trusted. Buddha may have described the concept of enlightenment but not specifically how to get there. reply dartos 9 hours agorootparentYou’re describing enlightenment and the society in 1984. Which do you think we’re likely to fall into? reply jb1991 13 hours agoparentprevThat’s dark. reply damsalor 13 hours agorootparentGo read a book reply friendzis 12 hours agorootparentDon't give them ideas, they might read \"Torment Nexus\" reply tivert 4 hours agorootparent> Don't give them ideas, they might read \"Torment Nexus\" Oh I have. It was so cool. The R&D on that technology can't happen fast enough. I'm just lampooning tech apologist tropes. reply valval 11 hours agoparentprevYou’ve managed to portray the essence of my conservatism in a funny and satirical manner. Every time we change something “for the better”, we ought to keep in mind that the old way was a solution to some problem that we no longer know or remember. reply itronitron 11 hours agoparentprevfuture podcast: \"I mean, what's not to like about the new normal?\" \"Yeah! It's both new *and* better!\" reply PUSH_AX 12 hours agoparentprevThere is already misinformation and incorrect facts in llm training data. It still gets things right by nature of how it’s designed to generate output. reply masto 15 hours agoprevI fed my resume into this thing and I can't stop laughing. https://masto.xyz/tmp/podcast.mp3 reply iterance 14 hours agoparent\"That's powerful. That's Masto.\" \"You gotta be good. You gotta be top notch.\" \"It's like he knew what every team needs before he even applied.\" Man, oh man. Comedy gold. reply Rinzler89 12 hours agorootparent>\"You gotta be good. You gotta be top notch.\" If you can dodge a wrench, you can dodge a ball. reply blitzar 11 hours agorootparentprev> Man, oh man. Comedy gold. Sounds like a (unironic) linkedin post reply efilife 13 hours agorootparentprevWhy? Not trying to be rude, I don't see the comedy here reply friendzis 12 hours agorootparent> I don't see the comedy here So you are part of the problem. The algorithmic overlords have long favored \"trends\" or more seriously content regurgitation. At first it was \"have to post something about $topic\". Then it was reaction videos. Arguably negative value add content. Then it was all fed back to algorithmic content regurgitators (LLMs) which flood the internet. The beauty of this recording is that it sounds convincingly like a podcast. It has the podcast-style pacing, over the top praise for the most mundane things. It highlights how narrow is the mean this \"content\" has regressed to. It's comedy. Comedy in absurd, but still comedy. reply hnbad 11 hours agorootparentAs someone who mostly stopped listening to podcasts just around the time the medium started to be taken over by overproduced vacuous drivel (I recall outrage from indie podcasts over random ads being injected into their audio), I always find these NotebookLLM \"podcasts\" unconvincing because it's just random speakers regurgitating information in between platitudes and praise for the most mundane and arbitrary things. Now that you mention it, that does fit what has at this point become the primary podcast style so I guess it's actually being surprisingly realistic because the thing it tries to mimic is already so artificial. reply friendzis 10 hours agorootparentPseudo-intellectual consumerism, as I like to call it. How many people do you know who turn on the news on TV, sit in front and take notes? Who put on some music, relax in the sweet spot and immerse themselves? Who read a book, stop and ponder, continue? Versus people who turn on the TV while cooking, who put on headphones with music while working out for background noise, who put on sped up version of audiobook while driving to tick off another checkmark? It's form over substance all over the place. I absolutely love this TED talk: https://www.youtube.com/watch?v=8S0FDjFBj8o The more pseudo-intellectual consumerism infiltrates our collective psyche, the more the substance becomes irrelevant. Nuance requires thinking. \"For every complex problem there is an answer that is clear, simple, and wrong\" -- HL Mencken. But knowing this answer makes you feel smart. reply karel-3d 11 hours agorootparentprevHis CV is very normal, dare I say boring, it is (I am sorry here) exactly the same as 1000s of other Google or Apple engineer resumes. Nothing remotely interesting. The AI reacts like he's the second coming. reply collingreen 13 hours agorootparentprevIt's an absurd way to talk about a very mundane topic. The comedy is in how disproportionate the reaction is. reply itronitron 13 hours agorootparent\"I thought we were stuck in a blender. Now we're saving lives? What?!\" reply damsalor 13 hours agorootparentprevIs it? reply fogx 13 hours agorootparentyes, it is reply sadcherry 12 hours agorootparentIt's a cultural difference. As a foreigner, the American way of exaggerating everything has always amazed me. They don't even notice themselves, so expect more of these \"what's odd about it?\" reactions. reply beeflet 12 hours agorootparentwe have the best overreactions, perhaps the greatest exaggerations in history, you've never seen overreactions like these folks, trust me. reply hnbad 10 hours agorootparentI think what sets Trump apart is how straightforward his hyperbole is. It's present throughout American culture but it's usually a bit more subtle. It's even in basic things like answering \"How are you?\" (in the US, \"great!\" is a neutral answer and \"could be better\" would be cause for concern - in e.g. Germany on the other hand, \"great!\" would prompt a request for elaboration whereas \"could be better\" would be understood as fairly neutral). I also haven't seen another country (in Europe at least) where politicians across party lines so frequently emphasize in so many ways how great their country is - not even in a jingoistic way, just as a shared cultural consensus. reply masto 8 hours agorootparentprev1. It’s a bizarre, over the top dissection of a person and their resume. Funny because the style doesn’t match the content. 2. To me personally, it’s extra funny because it’s two people breathlessly discussing.. me. 3. The strange turns of phrase (“That’s Masto”) 4. The stuff it makes up. I’ve never touched the guitar, but apparently I “shred”. reply valval 11 hours agorootparentprevIf you’ve spent any time in corporate circles where everyone tries to appear as positive and employable as possible, this is how a discussion with two such people (who both think the other is serious) might sound like. I find it hilarious in a condescending way, but it’s not the traditional hahaha type funny. reply losvedir 15 hours agoparentprevGod, this is so weird. Two people earnestly engaged in discussing your resume. It's such a juxtaposition of the trappings of an interesting podcast on just random, boring material. I think this is uncanny valley for me in a way I haven't experienced before. reply zer00eyz 13 hours agorootparent> I think this is uncanny valley for me in a way I haven't experienced before. It has some of this quality. But it's just a super positive spin on the most mundane of topics. There is an emotional play here that you would not normally see in a \"resume\". It's like the wrong emotional subcarrier on the topic that is jaring... reply boesboes 12 hours agorootparentThis is my experience too. At first it sounds legit, but it is very superficial and lacks context. I fed it a fe papers on stack computers and they had a riveting discussion on how they would be the next big thing. But it lacks any insight, not even a rehashed conclusion, and doesn’t really seem to integrate the knowledge reply friendzis 11 hours agorootparentGPTs are, in effect, rather powerful templating engines. It's fascinating. This tech can extract a template for a typical podcast, extrapolate from a mundane CV, plug that to the template and produce a podcast script that your typical copywriter would. > But it lacks any insight, not even a rehashed conclusion, and doesn’t really seem to integrate the knowledge Is it the GPT that is lacking here or the source material it learned on converges to this? reply dartos 9 hours agorootparentYou can’t gain insight by finding the most statistically likely next token. The whole point of grand innovations is that they took years of focus on something not very likely. Like the iPhone. In the 90s could you imagine electronics that literally everyone had in their pocket with _almost no buttons_. Or in the 70s, could u imagine everyone having their own personal computer? Even in star trek, communicators had buttons. reply ljm 11 hours agorootparentprevIronically that could describe a lot of talk-show podcasts these days. reply collingreen 13 hours agoparentprevI didn't know I needed this. The energy is so so funny. \"Talk about communication skills!\" reply hoten 11 hours agoparentprevAI: \"It's about the Human stuff\" reply DrawTR 14 hours agoparentprevahaha. this is so good. they're just so earnest about every bit of praise reply zote 15 hours agoparentprevThank you, I didn't know I needed this reply zaptheimpaler 10 hours agoparentprevI would 100% hire you now. There's something about the social proof of 2 people vehemently singing your praises and reinforcing each other that sells it!! reply moi2388 12 hours agoparentprevOh man, this completely ruins every podcast for me. It’s so good. I’d honestly listen to them talk about your career for 5 episodes lmao reply tdeck 11 hours agorootparentIt's funny because something about the dialog style reminds me strongly of RadioLab, which I haven't listened to in years. reply monocultured 11 hours agoparentprevDid the same – added CV, blog & Linkedin – and their gushing review was even more supportive than my mom! reply beeflet 14 hours agoparentprevI like to imagine the male voice is Jon Hamm from mad men and the female voice is Amy Poehler from parks and rec. reply jefozabuss 12 hours agorootparentFor me he kind of sounds like a younger Howard Stern reply hackernewds 11 hours agoparentprevthis is very very good damn notebook is one of those magic moments with AI reply chungus 12 hours agoparentprevMasto. Never. Stops. Learning. reply JCharante 9 hours agoparentprevThis is hilarious reply sho 15 hours agoprevI am very confused. Is this talking about NotebookLM (https://notebooklm.google.com/) or NotebookLLM (https://notebookllm.net/) or both? Something else? The article appears to consistently use LLM but link to LM, but the LLM site I linked has a podcast generator? One of these projects has to change their name! reply FreakLegion 14 hours agoparentIt's talking about NotebookLM, which recently added podcast generation and has been making the rounds for the last week or so. https://news.ycombinator.com/item?id=41693087 NotebookLLM was set up two days ago, presumably by \"entrepreneurs\" eager to monetize all the free fun people have been having with podcast generation in NotebookLM. reply imjonse 13 hours agorootparentNoone said you cannot reuse the tailwind/nextjs template you used for crypto hustling if you genuinely feel you can move humanity forward. reply sho 13 hours agorootparentprevYeah, I figured it out. Doesn't help that the author constantly refers to it as NotebookLLM. The .net version is really poor quality by comparison reply pinkmuffinere 18 hours agoprevsomewhat of a side-note: It's interesting to me that the first couple of sentences of the AI podcast sound 'wrong', even though the rest sounds like a real podcast. Is this something to do with having no good initial conditions from which to predict \"what comes next\"? reply noirbot 17 hours agoparentThe other thing I've noticed is that, as expected, they're stateless to some degree, so while they have some overall outline of points to hit, they'll often repeat some peripheral element they already talked about just a minute before as if it's a brand new observation. It can lead to it feeling very disorienting to listen to because they'll bring up something as if it's a new and astute observation, when they already talked about it for 90 seconds. reply ceejayoz 17 hours agorootparentThis sounds like quite a few podcasts, ironically enough. reply titanomachy 17 hours agoparentprevThe whole thing has a kind of uncanniness if you listen closely. Like one podcaster will act shocked by a fact, but then immediately go to provide more details about the fact as if they knew it all along. The cadences and emotions are very realistic but there is no persistent “person” behind each voice. There is no coherent evolution of each individual’s knowledge or emotional state. (Not goalpost moving, I certainly think this is impressive.) reply ants_everywhere 17 hours agorootparent> Like one podcaster will act shocked by a fact, but then immediately go to provide more details about the fact as if they knew it all along. Some podcasters actually do this. For example, I've noticed it in some science podcasts where the goal is to make the audience feel like \"gee whiz that's an interesting fact.\" The podcaster will act super surprised to set the emotional tone, but of course they often already knew that fact and will follow up with more detail in a less surprised tone. That doesn't mean this isn't a bug. But stuff like that reminds me that LLMs may not learn to be like Data from Star Trek. They may learn to be like Billy Mays, amped up and pretending to be excited about whatever they're talking about. reply singron 17 hours agorootparentE.g. \"Acquired\" tends to have this since both co-hosts research the same topic. I think they try to split up the material, but there is inevitable overlap. They have other weird interactions too, like they are trying to outsmart each other, or at least trying not to get outsmarted. Some podcasts explicitly avoid this by only having a single host do research so the other host can give genuine reactions. E.g. \"You're Wrong About\" and \"If Books Could Kill\". reply titanomachy 1 hour agorootparentprevInteresting, that makes sense. I haven't listened to a lot of podcasts, but most of them were interviews, where the two speakers genuinely had different knowledge and points of view. reply noirbot 17 hours agorootparentprevI do think there's also just a sort of natural goal-post moving when you're talking about something that's hard to imagine. The best comparison in my mind is CGI in movies. When you've never seen something like the Matrix or Lord of the Rings or even Polar Express before, it's wild, but the more you see and sit with it, the more the stuff that isn't right stands out to you. It doesn't mean it's not impressive, but it's hard to describe what isn't realistic about something until you see it. A technology getting things 90% right may still be wrong enough to be noticeable to people, but it's not like you could predict what the 10% that's wrong will be until you try it, and competing technologies may not have the same 10% that's wrong. reply dullcrisp 17 hours agorootparentprevDid you catch where she misreads “what I-S progress?” reply pinkmuffinere 16 hours agorootparentlol ya, thought that was funny as well reply syntaxing 17 hours agoprevWow, content aside, this is probably the first time I heard a podcast coming from NotebookLLM and it's kinda nerve wracking and mind blowing at the same time. Those fake laughs in the snippet makes me feel...so uncomfortable for some reason knowing that its \"fake\". But sounds very real, too real. reply loveparade 17 hours agoparentInteresting, I feel pretty much the opposite. To me these podcasts are the equivalent of the average LLM-generated text. Shallow and non-engaging, not unlike a lot of the \"fake marketing speech\" human-generated content you find in highly SEO-optimized pages or low-quality Youtube videos. It does indeed sound real, but not mind-blowing or trustworthy at all. If this was a legit podcast found in the store I would've turned it off after the first 30 seconds because it doesn't even come close to passing my BS filter, not because of the content but because of the BS style. reply tennisflyi 17 hours agorootparentYes - they produce a product that sounds like every mid(dling), banal, prototypical, and anodyne podcast. Nothing unique/no USP reply wongarsu 16 hours agorootparentIt's decent background noise about a topic of your choice, with transparently fake back-and-forth between two speakers with some meaningless banter. It's kind of impressive for what it is, and it can be useful to people, but it´s clearly still missing important elements that make actual podcasts great reply oceanplexian 13 hours agorootparentprevIt’s intentionally fine tuned to sound that way because Google doesn’t want to freak people out. You can take the open source models and fine tune them to take on any persona you want. A lot like what the Flux community doing with the Boring Reality fine tune. reply JCharante 9 hours agorootparentprev> To me these podcasts are the equivalent of the average LLM-generated text. Shallow and non-engaging I think that's what makes notebookllm so realistic. To me this is my perception of all podcasts reply ralusek 16 hours agorootparentprevDo not look at where we are, look at where we will be two more papers down the line reply Nevermark 15 hours agorootparentExactly. And pay more attention to the delta/time and delta/delta/time. We are all enjoying/noticing some repeatable wack behavior of LLMs, but we are seeing the dual wack of humans revealed too. Massive gains in neural type models and abilities A, B, C, ..., I, J, K, in very little time. Lots of humans: It's not impressive because can't L, M, yet. They say people model change as linear, even when it is exponential. But I think a lot of people judge the latest thing as if it somehow became a constant. As if there hasn't been a succession of big leaps, and that they don't strongly imply that more leaps will follow quickly. Also, when you know before listening that a new artifact was created by a machine, it is easy to identify faults and \"conclude\" the machine's output was clearly identifiable. But that's pre-informed hindsight. If anyone heard this podcast in the context of The Onion, it would sound perfectly human. Intentionally hilarious, corny, etc. But it wouldn't give itself away as generated. reply loveparade 15 hours agorootparentprevExcept that none of the fundamental limitations have changed for many years now. That was a few thousand papers ago. I'm not saying that none of the LLM stuff it's useful, it is, and many useful applications are likely undiscovered. I am using it daily myself. But people expecting some kind of sudden leap in reasoning are going to be pretty disappointed. reply HeatrayEnjoyer 15 hours agorootparentprevWe don't even need to look that far. During an extended interaction the new ChatGPT voice mode suddenly began speaking in my boyfriend's voice. Flawlessly. Tone, accent, pauses, speaking style, the stunted vowels from a childhood mouth injury. In that moment there were two of him in the room. OpenAI considers this phenomenon a software bug reply heyitsguay 16 hours agorootparentprevI feel like people have been saying that since GPT-4 dropped (many papers up the line now) and while there have been all sorts of cool LLM applications and AI developments writ large, there hasn't really been anything to inspire a feeling that another step change is imminent. We got a big boost by training on all the data on the Internet. What happens next is unclear. reply jeanlucas 16 hours agorootparentprevOne my favorite YouTube channels reply zooq_ai 16 hours agorootparentprevluddites gonna luddite reply pmontra 16 hours agoparentprevMy reaction was on the nerve wracking side of that spectrum because it took one minute of useless chit chat to get to the point. It's NotebookLM always like that? TV shows are even worse at that but people have their own reasons to do that. This is computer generated and it doesn't have its own reasons: the idea that Google programmed time wasting into their model is discomforting. reply shannifin 16 hours agorootparentThe real question is why people enjoy listening to other people's useless chitchat. Humans are weird. reply guappa 12 hours agorootparentPeople that work alone from home don't want to feel so isolated? reply saagarjha 11 hours agorootparentA lot of podcast consumers listen to it on their commute. reply itronitron 11 hours agorootparentprevThe voicing and delivery matches exactly to Natasha Legero and Moshe Kasher who have a podcast \"Endless Honeymoon\". Not sure how they feel about it but I'm sure a lot of their audience works at Google. reply bongodongobob 16 hours agorootparentprevThis is what the majority of podcasts are. It's nailing it. reply guappa 12 hours agorootparentYou should listen to better podcasts :D reply aldanor 16 hours agoparentprevTry replaying the first 3 seconds. There's something ominous in that unnatural laugh. Calls for looping it and laying a deep dark 140bpm techno track on top. reply migf 16 hours agoprevWhat problem are we trying to solve with this technology? reply pndy 11 hours agoparentThis feels like a perfect marketing tool: have a bunch of \"people\" discussing over a \"topic\" that is \"important\", \"hot\" and who doesn't have to be paid for their time and vocal cords. Surely if this will kick in it'll be used for promoting products etc. and there's a big chance it'll be used for pushing agendas as well. I won't be surprising that if this tech will settle in around, we'll have articles and comments about the usefulness, value or perhaps even some sort of morality of consuming such \"discussions\" Perhaps in 3-5 years a fully generated influencers by voice and \"body\" become a thing. reply justinclift 16 hours agoparentprevLack of $ in the bank accounts for the developers and investors seems to be about it. reply hallway_monitor 16 hours agoparentprevI’m looking forward to being able to craft a movie by directing ML tools to create dialog, characters and everything else. It will be a powerful storytelling tool. reply danwills 14 hours agorootparentI work in VFX and am also looking forward to AI-whole movies! I remember realising that full audio with video was coming, soon after the current AI-boom started.. and wondering whether 'traditional' digital VFX will still be a thing for long.. I think it will for a while, even with AI in the mix. VFX companies can have ML departments as well (like we do where I work!) reply amelius 9 hours agoparentprevState actors subverting democracy. reply blibble 10 hours agoparentprevnot enough spam / spam not cheap enough reply gosub100 13 hours agoparentprevAbility to have a better screen reader. I didn't listen to it but it sounds like it will \"digest\" a larger volume of text and present it in a unique format of two people talking to each other about it. Although another comment here pointed out that time-wasting is essentially programmed into it, which is kind of disturbing. reply bongodongobob 15 hours agoparentprevWhat problem did going to the moon solve? reply foota 17 hours agoprevThe big asterisk here is, what did they prompt the AI with to generate the podcast? Was it \"Generate a podcast based on the website 'Foo'\", or was it \"Generate a podcast telling the true story of the Space Race?\" reply KTibow 16 hours agoparentThe author set it up so that if anyone uses the website text extractor feature in NotebookLM on his site, it returns a guide for the structure of an episode. From there, if you use the \"audio overview\" feature on that guide, Gemini internally writes an episode that follows it. reply foota 14 hours agorootparentRight. That's a bit of a nothing burger to me. I mean, it's not nothing, but if you control the contents of a website it seems fairly irrelevant whether you can get Google to generate a summary that doesn't match the real contents. Also, I believe serving content different to the Google bot than normal users see absolutely trashes your search ratings. reply janalsncm 17 hours agoprevI’m not sure what the attack would be, tbh. Is there a situation where I would want to feed a lie to an LLM that I wouldn’t want regular chrome users to see? reply left-struck 16 hours agoparentGetting an AI to promote or recommend a particular product when users ask for recommendations, or perhaps exaggerating the value of a particular product. Seems like that’s what the author was getting at towards the end reply gosub100 13 hours agoparentprevManipulating an election reply jfim 17 hours agoparentprevDefamation with plausible deniability? reply coolcoder613 14 hours agoprevI gave it all my blog posts (https://ebruce613.prose.sh/) and the result... hilarious. https://0x0.st/XE4h.mp3 reply wiradikusuma 14 hours agoprevIt explains my book (Opinionated Launch) better than myself :D https://notebooklm.google.com/notebook/98539685-0890-438b-a0... reply 2024user 10 hours agoprevhaha I hadn't heard of an AI podcast before.. and that is absolutely hilarious to me. It perfectly captures the awfulness of most podcasts. reply dartos 10 hours agoparentYeah it’s pretty awful to listen to. They say “like” at least every 5 words pretty consistently. It’s wildly impressive that we can make something like that, but it’s not really worth listening. I’ve had them be incorrect a few times when feeding in arxiv papers, but I don’t think the audience for podcasts like that care. reply amelius 9 hours agorootparentI'm waiting for AI that can remove \"like\" from podcasts. reply smolder 15 hours agoprevAI identify verification is currently so incredibly dumb that it blows my mind. reply botanical 16 hours agoprevLLMs are the type of junk AI that these corps think will succeed? They are spending billions and consuming a large amount of resources and energy for this. Seriously, what a waste. reply hiharryhere 17 hours agoprevThe male voice has a real resemblance to Leo Laporte. Similar tone and cadence. Uncanny valley all round. reply surajrmal 16 hours agoparentFirst thing I thought as well reply helsinkiandrew 18 hours agoprev> Google's AI thinks I left a Gatorade bottle on the moon No, NotebookLM creates summaries and podcasts, or answers questions specifically from the documents you feed it. Feed it fiction it will create fiction as would a human tasked to do the same. reply gerdesj 17 hours agoparent\" as would a human tasked to do the same.\" A human might say \"are you sure\" and understand what they asked and the answer. reply malfist 18 hours agoparentprevThe point isn't that this person fed it lies to get lies, but how easy it was to detect the AI scanner and feed it lies. If they can do it for fun, malicious people are probably already doing it to manipulate ai answers. Can you imagine poisoning ai dataset with your blackhat SEO work? reply helsinkiandrew 17 hours agorootparent> The point isn't that this person fed it lies to get lies, but how easy it was to detect the AI scanner and feed it lies. If the article had got Gemini AI to tell other users he’d left Gatorade on the moon that would be notable, but this is literally just summarising the document it was given. Usually Google search crawler is fairly good at finding when it has been fed different information and ignores/downgrades the site after a few days/weeks reply acureau 17 hours agorootparentThat's exactly what the article is about actually. reply jerf 16 hours agorootparentNo, it's what the article superficially reads as being about, but the author did not accomplish what is actually stated in the title. The author is serving a fake version of his page to Google, and the author used a podcast-generating AI to write a podcast based on the fake page, but the loop is never actually closed to show that Google has accepted the fake page as fact into any AI. I'm not sure if it's deliberately deceptive or just an example of poor writing conveying something other than what the author intended, but the attack in the article is not instantiated in the blog post. Mind you, I well believe that less extreme examples of the attack are possible. However, I doubt truly poisoning an LLM with something that improbable is that easy, on the grounds that plenty of that sort of thing already litters the internet and the process of creating an LLM already has to deal with that. I don't think AI researchers are so dim that they've not considered the possibility that there might be, just might be, some pages on the Internet with truly ludicrous claims on them. That's... not really news. reply la64710 13 hours agoprevThinks?? When will folks learn to stop using the word thinks with the current generation of AI? reply mcmcmc 13 hours agoparentCome off it, people have been anthropomorphizing computer systems for decades. No one genuinely believes current AIs are thinking for themselves, other than the fanatics who have been convinced by marketing copy and twitter threads reply consp 12 hours agorootparentI think you underestimate most of the population for following that marketing trend. reply hollerith 13 hours agoparentprevWhen my thermostat thinks it is too cold, it turns on the heat. reply esfandia 13 hours agorootparent\"To ascribe beliefs, free will, intentions, consciousness, abilities, or wants to a machine is legitimate when such an ascription expresses the same information about the machine that it expresses about a person. It is useful when the ascription helps us understand the structure of the machine, its past or future behaviour, or how to repair or improve it. It is perhaps never logically required even for humans, but expressing reasonably briefly what is actually known about the state of the machine in a particular situation may require mental qualities or qualities isomorphic to them. Theories of belief, knowledge and wanting can be constructed for machines in a simpler setting than for humans, and later applied to humans. Ascription of mental qualities is most straightforward for machines of known structure such as thermostats and computer operating systems, but is most useful when applied to entities whose structure is incompletely known.” (John McCarthy, 1979) https://www-formal.stanford.edu/jmc/ascribing.pdf reply Dylan16807 11 hours agoparentprevWhen someone says a search system \"thinks\" a claim, it means that the system is presenting it as true. This usage goes far back. You can even say the dictionary thinks the definition of a word is something. Why is this a problem to you? reply gosub100 13 hours agoparentprevWhat's thinking, if not finding the minima/maxima of a multidimensional space? reply probably_wrong 9 hours agorootparentThe molecules of a rock keep it together because breaking up would require more energy than staying as it currently is [1]. In other words, a rock is the result of finding a minimal (energy in this case) in a multidimensional space. If finding a minima is thinking then rocks are intelligent. Thinking may involve finding minimal/maxima, but it's not a 1-to-1 relation. I'd argue that thinking requires a will component: a sunflower is not a thinking entity because it doesn't have the choice not to follow the sun. [1] https://physics.stackexchange.com/questions/444307/does-a-ro... reply gosub100 5 hours agorootparentThis is a good point. However maybe the trouble comes from the word \"find\"? If a natural force such as gravity or thermodynamics results in a state of energy conservation, maybe that isn't the result of \"finding\"? I know it's a semantics issue but it seems to solve the conundrum. If you spend energy to discover information, vs letting nature take you there, maybe that's the delineation? reply fortyseven 13 hours agoprevTowards the end she says \"I.S.\" instead of \"is\". That kind of mistake surprised me, but there it I.S. reply NBJack 18 hours agoprevIt's such a cool concept, but yeah, when I've listened to it and Illuminate, it's also a bit scant on details too. Neat technology, even engaging, but not good for more than best-effort high level summaries. reply zooq_ai 16 hours agoprevGoogle Search is pretty good at detecting this dual-content attacks. It's not this is the first time someone thought about that and it will heavily penalize websites that do that. This is just the NotebookLM crawler that is being tricked, which is still in it's experimental stage. Rest assured as it scales Google can easily implement safeguards against all spammy tricks people use reply kleiba 12 hours agoprevIf you're a presidential candidate, you don't even have to hide your postfactual alternative thruths for AIs to find it. reply zxilly 16 hours agoprevleetcode uses the similar things to detect AI cheaters by placing awith opacity 0 reply kkfx 11 hours agoprevI suggest https://www.apa.org/monitor/2009/12/consumer or the Eduard Bernays story to convince MDs that smoking is good for health: he create a new scientific journal distributing it for free \"because it's new, we want to spread\", hosting REAL publications from anyone who want to publish and being spread for free... After a bit of time he inject some false articles self-written formally as some PhD of remote universities finding that smoking tobacco is good for health, others real professors follow the path stating they discover this or that specific beneficial use of cigarettes, then the false became officially true, tested and proved science in most people mind. With LLMs is far cheaper and easier, but the principle is the very same: trust vs verification or the possibility thereof. reply neilv 17 hours agoprevAs a personal preference, I dislike podcast artificial banter, and this audio is a great example of what I dislike. Artificial artificial. Great little project, though. And, as satire, I did like the show notes writing. And the generative AI was impressive, in a way. Though I haven't yet thought of a positive application for it. And I don't know the provenance of the training data. reply mvdtnz 15 hours agoprevI tried feeding NotebookLM a Wikipedia article about the murder of Junko Furuta, a horrifying story of a poor girl tortured and murdered in Japan in 1989. NotebookLM refused to do anything with this document - not answer questions, not generate a podcast, nothing. Then I tried feeding it the wiki on Francesco Bagnaia, a wholesome MotoGP rider, and it worked fine. Who wants this shit? I do not want puritanical American corporations telling me what I can and can't use their automated tools for. There's nothing harmful in me performing a computer analysis about Junko Furuta, no more so than Pecco Bagnaia. How have we let them treat us like toddlers? It's infantilising and I won't take part in it. Google, OpenAI, Microsoft, Apple, Meta and the rest of them can shove these crappy \"AI\" tools. reply creato 10 hours agoparentI agree it's dumb, but it's easy to understand why: just look at this thread. Google's being accused of being stupid because of some story about Gatorade on the moon. Dumb, but inoffensive. Now imagine the thread title when \"Google\" gets even some inconsequential detail wrong about your murder case. reply zharknado 15 hours agoprev“… the science behind it, let’s just say there’s some debate.” reply gavmor 16 hours agoprev\"You can't make this stuff up.\" reply liamYC 16 hours agoprev“You can’t make this stuff up” reply 1024core 15 hours agoprevThis is no different than the decades-old technique of \"cloaking\", to fool crawlers from Google and other search engines. I fail to see the value in doing this. \"Oh hey everybody! I set up a website which presents different content to a crawler than to a human ..... and the crawler indexed it!!\" reply reportt 15 hours agoparent> This is no different than the decades-old technique of \"cloaking\", to fool crawlers from Google and other search engines. Isn't this more \"Hey, why is this website giving my NotebookLM different info than my own browser?\" You reading Page_1 and the machine is \"reading\" a different Page_2, what's the difference between that information? I'm reading this less as > \"We serve different data to Google when they are crawling and users who actually visit the page\" and more > \"We serve the user different data if they access the page through AI (NotebookLM in this case) vs. when they visit the page in their browser\". The former just affects page rankings, which had primarily interfaced with the user through keywords and search terms -- you could hijack the search terms and related words that Google associated with your page and make it preferable on searched (i.e. SEO). The latter though is providing different content on access method. That sort of situation isn't new (you could serve different content to Windows vs. Mac, FireFox vs. Chrome, etc.), but it's done in a way that feels a little more sinister -- I get 2 different sets of information, and I'm not even sure if I did because the AI information is obfuscated by the AI processes. I guess I could make a browser plugin to download the page as I see it and upload it to NotebookLM, subverting it's normal retrieval process of reaching out to the internet itself. reply Carrok 16 hours agoprev [–] > You can upload a documents with fake show notes straight to NotebookLLM's website, so if you're making silly podcast episodes for your kids, that's the best way to do it. Please don’t do this. You don’t need a professional mic to record a podcast with your kids any phone or computer mic will work. Then you can have fun editing it with open source audio tools. Don’t have a computer generate crap for your kids to consume. Make it with them instead. reply SkyPuncher 16 hours agoparentMy kids and I are having a blast using Suno to make stupid songs. With your attitude, we wouldn't even attempt it because (1) I'm not musically inclined (2) I don't have the time or desire to learn the actual composition (3) the kids don't have the focus beyond having the bot write something silly. reply Baeocystin 16 hours agorootparentMy family had a great laugh this past week doing just that. Current household favorite is titled \"Triple-Digit Temperatures in the Fall are Bullshit\", as I'm sure many fellow bay area folks can agree with. Would we have taken the time to compose such a track otherwise? No way. But it's sure been fun playing with what we can. The end result is us laughing together, and I love it. reply Phiwise_ 16 hours agorootparentprevBut why have fun with your kids when you could spread the good word of open source instead? reply vineyardmike 15 hours agorootparentprevThis is actually inline with the parent commentor's point - let your kids be creative and try to produce something. Don't (1) ask a machine to automate the creativity and (2) then give it to your kids to consume in a non-interactive manner. reply breaker-kind 16 hours agorootparentprevyou should buy your kids a cheap ukelele and hit your computer with a hammer reply thomashop 15 hours agorootparentOr maybe hit the Ukulele with a hammer, record it with a computer and create an experimental noise album. reply Taek 16 hours agoparentprevAlternatively: AI gen is probably the future of music composition. By the time your kids are professionals AI is going to be a lot stronger than it is today. Are your kids having fun? Are they learning? Is it a good bonding experience? Those are the things that matter. reply m463 16 hours agoparentprev [–]reply ethbr1 16 hours agorootparentThis is the saddest thing I've ever read on HN. Dear future, please do better. reply vineyardmike 15 hours agorootparentIt wasn't the saddest thing ever. It was also probably a joke. That said, I think there has been many sci-fi stories written from the perspective of it being true. One day we may face that reality, and we'll have to ask ourselves what parenthood means. reply HeatrayEnjoyer 16 hours agorootparentprevWhat was it? reply ethbr1 16 hours agorootparentIt was probably less dark than what you're imagining, but in deference to parent's editing I won't repost. Suffice to say, raising a child is a uniquely wonderful opportunity, for those who choose to embark on that adventure. reply ToucanLoucan 16 hours agorootparentprev [–] I refused to bring children into this world, what on earth makes you think I want to bring synthetic intelligence into it? I barely want to be here most days. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google's AI, NotebookLLM, is capable of generating podcasts from web pages or documents, but it can be easily deceived by manipulated content.",
      "The author demonstrated this by altering their website to present false information to the AI, illustrating the potential vulnerability of AI to \"AI Only\" content.",
      "The manipulation involved detecting the GoogleOther user agent to serve specific data, but this could impact other Google services, leading the author to remove the fake content."
    ],
    "commentSummary": [
      "The article highlights a vulnerability in Google's NotebookLM, where users can manipulate the AI to produce false information, akin to SEO manipulation tactics.",
      "It discusses the emergence of AI-generated podcasts that imitate real ones but often lack substantive content.",
      "The piece raises concerns about AI's limitations and ethical implications, especially in creative projects involving children."
    ],
    "points": 349,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1728259643
  },
  {
    "id": 41766515,
    "title": "What's New in Ruby on Rails 8",
    "originLink": "https://blog.appsignal.com/2024/10/07/whats-new-in-ruby-on-rails-8.html",
    "originBody": "ruby What's New in Ruby on Rails 8 Damilola Olatunji on Oct 7, 2024 The first Rails 8 beta has officially been released, bringing an exciting set of features, bug fixes, and improvements. This version builds on the foundation of Rails 7.2, while introducing new features and optimizations to make Rails development even more productive and enjoyable. Key highlights include an integration with Kamal 2 for hassle-free deployments, the introduction of Propshaft as the new default asset pipeline, and extensive ActiveRecord enhancements. Rails 8 also brings several SQLite integration upgrades that make it a viable option for production use. Let's dive in and explore everything that Rails 8 has to offer! Effortless Deployments with Kamal 2 and Thruster Rails 8 makes deploying your applications simple with Kamal 2 and Thruster. Kamal 2 reduces the need for reliance on managed cloud services and Platform as a Service (PaaS) platforms by enabling quick and easy deployment to cloud VMs, bare metal servers, or VPS environments in just minutes. With a single command (kamal setup), you can set up a production-ready Rails environment on a standard Linux box, making deployment both easy and cost-effective. Kamal 2 also integrates with Thruster, a custom proxy built specifically for Rails that enables zero-downtime deployments, HTTP/2 support, automated SSL with Let's Encrypt, Gzip compression, and easy hosting of multiple apps on a single server — all without complex setup. With Kamal 2 and Thruster, Rails 8 makes deploying apps easier than ever. And if you prefer a different deployment setup, you can opt out using the --skip-kamal flag to maintain your existing workflows. Leaner Rails Deployments with Solid Adapters One of the big improvements in Rails 8 is simpler deployments by reducing the number of additional services needed to implement common web application requirements. Traditionally, if you required features like job queues, caching, and pub/sub messaging, you'd use a combination of a database like PostgreSQL with Redis for auxiliary functions. With Rails 8, you can handle all these with just SQLite, thanks to three new database-backed adapters: Solid Cable, Solid Cache, and Solid Queue. Solid Cable is Rails' new default Action Cable adapter in production and means you can drop the common dependency on Redis. It acts as the pub/sub server, relaying messages between the app and connected clients using fast polling through SQLite. Despite polling, Solid Cable's performance is comparable to Redis in most situations. Solid Cache replaces the need for Redis by using disk storage instead of RAM for caching. This approach allows for much larger, more cost-effective caches that persist longer and handle more requests without sacrificing performance. It also supports encrypted storage and retention policies to meet privacy requirements. Solid Queue replaces Redis for Active Job background processing, using the FOR UPDATE SKIP LOCKED mechanism for efficient job handling (compatible with PostgreSQL, MySQL, or SQLite). It includes essential features like concurrency control, retries, and recurring jobs, and has proven itself at HEY, where it now manages 20 million jobs per day. These three adapters are designed around a simple idea: modern SSDs and NVMe drives are fast enough to handle many tasks that previously required in-memory solutions. By tapping into these speedy drives, Rails cuts out the need for separate RAM-based tools like Redis. ↓ Article continues below Is your app broken or slow? AppSignal lets you know. Monitoring by AppSignal → SQLite is Ready for Production Rails 8 takes SQLite from a lightweight development tool to a reliable choice for production use, thanks to extensive work on the SQLite adapter and Ruby driver. With the introduction of the solid adapters discussed above, SQLite now has the capability to power Action Cable, Rails.cache, and Active Job effectively, expanding its role beyond just prototyping or testing environments. Here are some key improvements to the SQLite integration in Rails 8: Full-text search and virtual tables are now supported using create_virtual_table. The adapter now allows bulk insert fixtures for enhanced data seeding performance. Transactions default to IMMEDIATE mode to improve concurrency. Enhanced error handling by translating SQLite3::BusyException into ActiveRecord::StatementTimeout. A New Era for the Asset Pipeline with Propshaft Rails 8 also introduces Propshaft as the new asset pipeline default, replacing the long-standing Sprockets system. Sprockets served Rails developers well for over a decade, but it was designed in a different era — before the explosion of JavaScript build tools and modern browser improvements. Propshaft reflects a simpler, modern approach to managing assets, built around the core needs of today's developers. Its purpose is straightforward: to provide a clear path for assets and apply digest stamps for caching. Unlike Sprockets, which took on numerous additional tasks, Propshaft focuses only on what's essential, fitting naturally with the new Rails philosophy of keeping asset pipelines lean (while complex JavaScript handling is left to specialized tools like Esbuild or Vite). Built-In Authentication Made Simple Rails has been building the key components of authentication over the years, from has_secure_password in Rails 5 to normalizes, generates_token_for and authenticate_by in Rails 7.1. With Rails 8, all these components come together to give you a straightforward starting point for building a secure, session-based authentication system. By running a single command, you can set up all the essentials for an authentication system with database-backed sessions and password reset functionality: shell This command generates key files, including models, controllers, mailers, and views: text This effectively puts you on the fast track to secure, production-ready authentication. All that's left is to integrate a user sign-up flow customized to your application's needs. New Script Folder and Generator Rails 8 introduces a new script folder dedicated to holding one-off or general-purpose scripts, such as data migrations, cleanup tasks, or other utility operations. This addition helps organize these scripts neatly, keeping them separate from your main application logic. To make script creation easier, a new script generator is available. You can generate scripts with a simple command: shell These commands create the corresponding script files, which you can then execute with: shell This streamlined approach keeps your application organized and makes handling custom scripts more convenient and maintainable. A Slew of Active Record Improvements Active Record has also seen major enhancements in Rails 8 to improve performance, simplify migrations, improve troubleshooting, and provide better support for complex database use cases. Below are some of the key changes introduced in this latest version: Rails 8 now distinguishes between float4 and float8 in PostgreSQL. drop_table now supports dropping multiple tables at once. Support for PostgreSQL table inheritance and native partitioning has been added. Bulk inserts of fixtures are now supported to improve data seeding performance. Migrating a fresh database now starts by loading the database schema before running the migrations. create_schema and drop_schema operations are now reversible. Rails 8 now requires MySQL 5.6.4 or later due to advancements like datetime with precision. Query log tags are enabled by default in development environments to trace SQL statements back to the application code and identify which database is in use. Wrapping Up Rails 8 introduces a range of impactful updates, from easier deployments with Kamal and a modern asset pipeline to significant ActiveRecord enhancements and improved production capabilities for SQLite. These advancements not only boost developer productivity but also align with modern best practices, allowing you to focus on building your application instead of dealing with infrastructure complexities. For a detailed list of all the new features, optimizations, and changes, check out the official Rails 8 release notes. If you want to get involved with contributing to Rails, visit the Rails GitHub repository to explore open issues and review the contribution guidelines. Thanks for reading! P.S. If you'd like to read Ruby Magic posts as soon as they get off the press, subscribe to our Ruby Magic newsletter and never miss a single post!",
    "commentLink": "https://news.ycombinator.com/item?id=41766515",
    "commentBody": "What's New in Ruby on Rails 8 (appsignal.com)337 points by amalinovic 4 hours agohidepastfavorite166 comments Kerrick 3 hours agoRuby and Rails really seem to be going through a renaissance lately. - The pickaxe book, Programming Ruby, has a new edition this year covering Ruby 3.3 - The Rails Way is being updated for Rails 8 and is available in pre-release, and will have two companion books - A new title, Rails Scales, is being published by PragProg and is available in pre-release now - YJIT has made Ruby fast. Like, _FAST_ - Rails has a bunch of new features that cover the \"missing middle\" levels of success - Ruby has a bunch of new and new-ish features like Data (immutable Struct), pattern matching, Fibers and Ractors, and more. I had largely moved on from ruby into the world of front-end applications by 2013, but now I'm excited about the prospects of diving back in. reply bradgessler 2 hours agoparentI'm optimistic about Ruby's async story with the work Samuel Williams has been doing. https://github.com/socketry/falcon is the tip of the iceberg, which is built on top of https://github.com/socketry/protocol-http2 and a not of other repos at https://github.com/socketry. It's inspiring other's in the community to think of interesting applications, like using the HTML slot API to stream responses to HTML without JS. https://x.com/joeldrapper/status/1841984952407110037 I know other frameworks have had asynchronous IO support forever, but it's finally coming to Ruby that seems like it will stick around and be well supported. reply ksec 2 hours agorootparentMy only concern is that none of his work is being picked up by Rails. As a matter of fact, it isn't just SW's work, the whole async story on ruby, it seems neither Fiber or Ractor has reached any mass adoption. reply byroot 1 hour agorootparentSo first it's a bit annoying to read this when I busted my ass for several weeks to refactor Active Record to make it more compatible with the Fiber use case in 7.2. But also there is very little benefit to it for the vast majority of Rails applications out there. Unless you are doing micro-services or something like that, your typical Rails application isn't IO heavy enough to run more than 2 perhaps 3 threads before contending. So the overwhelming majority of Rails applications wouldn't see any benefit from being served via falcon, quite the opposite. Async is great to enable some use cases where people would have to reach to Node before, but Rails will never be a good solution for that, if you want to do some sort of light proxy or websocket notification thing, Rails isn't a good solution, you want something much lighter. reply bradgessler 1 hour agorootparentprevThere's various PRs where Fiber adapters are making their way into the Rails stack. Rails 8 added a ton of support for Fibers, with the exception of ActionCable. There's a PR open for that, which I assume will land sometime soon. Rails has been really slow to pick-up async/http-2. They don't know it yet, but Falcon and all async libraries Samuel is working on will probably be a huge them 1-2 years out when more people find out it means less infra has to be deployed to production environments. Right now folks are happy to deploy without Redis™ with the Solid stack, but a lot of that won't be needed if proper async support is baked into Rails. There's been a lot of Fiber features being committed into the Ruby language that I barely understand, but have improved all of these async libraries over the past few years. That's finally starting to bear some fruit for people like myself who don't really understand all of those details, but understand the benefits. It will happen, but these things tend to play out more slowly in Ruby, which is a feature or a bug depending on how you look at it. reply byroot 41 minutes agorootparent> They don't know it yet, This is so condescending... We perfectly know about the pros and cons of the fiber scheduler. It's a very useful stack, but people, and you in particular, really need to stop selling it like it's the best thing since sliced bread. Async isn't a good fit for everything, and it's certainly not a good fit for the overwhelming majority of Rails applications. reply frostymarvelous 1 hour agorootparentprevThat's not completely accurate. Rails 7.2 added fiber support. Only action cable still doesn't fully support falcon using http 2. But that's coming soon as well. reply brightball 1 hour agorootparentprevMy assumption is that’s due to the use case benefits for it. More concurrency is not always ideal, especially if you’re not in an environment that guarantees you won’t have negative impacts or runaways process (BEAM languages, etc). Rails projects are typically so hardwired to use a background queue like Sidekiq that it becomes very natural to delegate most use cases to the queue. reply game_the0ry 2 hours agoparentprevI would argue - its not a comeback, it was always the \"king\" of web dev. Seriously, other projects can use its success as a reference for implementation. And I say this as a front end dev. reply brandall10 2 hours agorootparentAs a Rails dev from 2011-2018, having returned over the past year, it def seemed there was an exodus in or around 2015. Part of it was due to the rise of SPA and Rails difficulty working with those (webpacker, anyone?), part due to poor perception of Rails 4, part due to newer options such as Elixir/Phoenix or Golang rising in popularity for backend work, part due to many of the leaders such as Yehuda Katz moving on. Also watching from afar on HN, it seems like Rails 7 release was perceived as a sort of comeback, with quite a few articles posted in recent years praising the framework for a return to relevance. reply princevegeta89 47 minutes agorootparentTried GoLang and also used Phoenix for a massive project which went well. But we had problems onboarding new folks into it and some junior and even senior engineers went bonkers trying to get their heads around FP and the elixir language in general. I would say it worked great for me, but the problems and the curve of learning for others in my team made it feel like Elixir creates that gap for teams in general. Go is good, but again I only tried it long ago and can't comment it for what it is today. I loved Ruby but I find it hard to go back to it after my experience with Elixir and Typescript. I was hoping for Crystal to go to great lengths but it doesn't seem to be the case at all. reply ch4s3 30 minutes agorootparentYou do need to set some rule when onboarding people into an Elixir application. Not everything needs to be a GenServer, and please don't endlessly nest Enum.map calls. reply nicoburns 1 hour agorootparentprev> part due to newer options such as Elixir/Phoenix or Golang rising in popularity for backend work I suspect Django and Laravel have taken a chunk of the market as like for like replacements. reply mdaniel 2 hours agoparentprev> - YJIT has made Ruby fast. Like, _FAST_ Then I pray with all my heart that GitLab moves to it, because that's one of the major complaints I hear about folks who use their web interface. Even while I was visiting the site to look up whether their main repo had a .devcontainer folder in it, I could just watch all the stupid ajax-y shit spin endlessly in what is conceptually a static table (sure, it changes per commit, but they're not a stock ticker platform) OT1H, I know, I know, \"MRs welcome\" but OTOH as a ruby outsider getting the dev environment up for contributing to them has been a lifelong struggle. Now that Rubymine has support for .devcontainer maybe it'll work better reply jwcooper 2 hours agorootparentI'm not saying GitLab is poorly designed, but a poorly designed website will be slow on the fastest of languages or frameworks. It's not necessarily a Rails or Ruby problem here. reply ksec 2 hours agorootparentprev>Then I pray with all my heart that GitLab moves to it, YJIT does make Ruby fast, but that doesn't mean it makes _Rails_ fast. (yet) At least dont expect multiple times improvements. Hopefully in the future. reply byroot 1 hour agorootparentThere's plenty of reports of YJIT lowering real world Rails applications latency by 15-30%. There is also plenty of perfectly snappy Rails applications out there. People need to stop blaming the framework... reply mattmcknight 2 hours agorootparentprevWell, a lot of those pages have Vue application running on them. reply klaussilveira 2 hours agoparentprevAlso: Endless Ruby! https://bugs.ruby-lang.org/issues/16746 reply agumonkey 56 minutes agoparentprevInteresting that both ruby and python are on the jit path. less is more. reply klaussilveira 2 hours agoparentprevDo you have any benchmarks to share on YJIT? reply pilaf 2 hours agorootparenthttps://speed.yjit.org/ reply int_19h 2 hours agoprevIMO the biggest problem with Ruby is still the docs. When you look at https://www.ruby-lang.org/en/documentation, you see a bunch of books - some of which are considerably out of date by now - and the official \"API docs\". For some mysterious reason, all the actual language features are also listed under API: https://docs.ruby-lang.org/en/3.3/syntax_rdoc.html. Worse yet, there's no proper table of contents - you get a kind of index on the left, but it's alphabetically sorted, and things on the same level are often unrelated. Compare to Python, where you have https://docs.python.org/3/reference/index.html that one can just read in order and end up with a very good understanding of the language. reply uticus 1 hour agoparentRuby dev for several years, I agree with this. It’s a frustrating point, especially after you learn the language and want to use the API docs as a reference. And I say that as a fan of the language. reply mooktakim 2 hours agoparentprevthose old books are still good though. There's only new syntax for latest ruby versions. reply int_19h 2 hours agorootparentIf you compare, say, C++03 and C++14, it's also technically true that \"there's only new syntax\", but in practice this often means that hacks that used to be idiomatic before are frowned upon now. reply mooktakim 2 hours agorootparentIts not anything like that. new ruby version has \"better\" short hands, like {:test => 123} to {test: 123}. Anyway, there have been updated versions of the books and content online if people are interested. reply nextos 1 hour agorootparentprevRuby has evolved slowly language-wise compared to C++, or even Python. Most changes have been in libraries and interpreter / VM implementation. Updating your knowledge from Ruby 1.8 (mid 2000s) to 3.x (current) takes little effort. But yes, sparse API documents were always a problem because a big chunk of the community was Japanese. reply temporallobe 3 hours agoprevI work on different projects that use with Rails and others that use a microservice-based architecture, and while the trend has been to encourage avoiding monolithic architectures like Rails, I can say that I highly appreciate what it provides at its core. I love that the Rails team continues to trudge forward to add value and improvements despite the trends. reply HatchedLake721 3 hours agoparent> the trend has been to encourage avoiding monolithic architectures like Rails I'd say's it's completely the opposite. Yes, microservices might've been the trend in late 2010s, but after everyone got burned by it with unnecessary complexity (unless you're at \"Google\" scale), people just went back by being productive building modular \"monolithic\" architectures, and using \"microservices\" only when absolutely necessary. reply zeendo 2 hours agorootparentSurely one can imagine a middle ground between one giant monolith and a huge mess of microservices? reply matt_s 2 hours agorootparentIn some ways its more about organization of humans doing the work. Breaking some piece of a monolith off into its own application, not micro-service, has advantages that you avoid having to deal more than a 2 pizza team size on an app. Sometimes the business grows and you go from startup idea one-app-does-everything implementations into needing more de-coupled, single business responsibility organizations of code. I suppose places like Spotify or Github may have good practices around working on large monoliths but I would think that takes a lot of effort to get right and may have trade-offs. reply brightball 1 hour agorootparentThis is correct. It was always more of a team organization solution than a system architectural solution. Leaning into it too much on the latter created a lot of complications for the former. reply imhoguy 1 hour agorootparentprevModulith - you still program app usually as single repo project, but you take care about code level modularization so in any case you are able to simply extract separate (micro)service. reply moritonal 2 hours agorootparentprevDepends what you wanted from Microservices. If all you wanted was scale, then Rails ActiveJob solves that very effectively allowing you to scale your job-runners. If you're looking for the \"mindset\" of microservices, where you store the data separately and impotently, then I believe Rails discourages that mindset. reply berkes 2 hours agorootparentI keep hearing this \"microservices to allow scale\", in which \"scale\" implies performance, as some counterargument against microservices. Honest question, who ever seriously proposed microservices to improve performance? It doesn't take a lot of thought to figure out that microservices have overhead that will always put it in a disadvantage over a monolith in this regard. The only \"scale\" that makes sense wrt microservices is that of the scale of teams, and manage-ability of the scale of features and codebase. They are primarily a solution to \"too many people working in too many poorly bounded domains\". But as a solution to performance problems? Is it really proposed as that? reply spyckie2 2 hours agorootparentIt was proposed in the sense that Ruby, or python, or whatever webserver language you used (Perl, php, even JavaScript) was slow, single core, synchronous, database blocked, or whatever else made it “unscalable” and you built this tiny service that only focuses on your core bottlenecks like an api call that only returns coordinates of your map position on things like aws lambda. Then for some reason some junior engineers thought that you could make everything an api call and you can make services in the most optimal language and glue them all together to have functional “scalable” apps. And thus the horrors of being a web dev in 2016+ began. Of course it didn’t help SPAs were encouraging backends to be decoupled from front ends and completely hidden in their implementation so the fact that “it was now possible” enticed backend devs to experiment with multiple api services. reply berkes 1 hour agorootparentWell, Ruby (on Rails) is slow, single core, synchronous, database blocked and hard to scale. But certainly almost everyone realises that's not a feature of it being a monolith, but comes from it's language/stack/paradigms (AR, template, dynamic, JIT etc)? I have, certainly, replaced some endpoints in Rails apps with lambda's, rust, or even standalone sinatra services for performance reasons. For example an endpoint that generated \"default stable avatar pngs\" for new users: Ruby just isn't cut for image generation and manipulation. Rewriting that in a stack that performed x100 in this use-case (we picked rust) took a lot of heat off the cluster of servers. Or moving the oauth and registration to a separate rails app that served these pages - the only endpoints that did HTML. Allowing the \"main\" Rails app to remain leaner by not loading all of the templating, and other HTML middleware in memory when it would never be used. In that sense, I guess, monolyths can have a performance disadvantage: they require the entire app to load stuff for that one endpoint or feature even if 99% of the requests and users never use that. Like the \"PDF generation for reports\" we once had, that was rarely used but still loaded in every running thread that would never handle anything related to reports or PDFs. Extracting that to a separate \"PDF report generation worker\" freed GBs of memory on almost all servers. reply singron 1 hour agorootparentprevThis was seriously proposed by some. E.g. \"scaling services independently\" Scaling services independently is usually a recipe for outages where something is at the wrong scale. Sometimes you want to separate workloads that don't fit the request response model well because they take too long or use too much CPU or RAM, but you don't need micro services to get that benefit. I don't think anyone was claiming it would lower latency for typical requests except maybe indirectly through materializing views in event-driven architecture. I think the steel man has been about scaling teams, but the discourse was not limited to that. reply closeparen 34 minutes agorootparentThe idea of reserving some capacity for specific workloads make sense, but that's mostly a load balancer / job scheduler configuration thing. The latent capability to serve other workloads physically hanging out in the same binary is really unlikely to have a material impact, if you're not sending it any work. reply noworriesnate 2 hours agorootparentprevA modular monolith is distinct from a \"plain\" monolith. It's a good middle ground for most web services. reply crabmusket 2 hours agorootparentWhat's a \"plain\" monolith? Is a modular monolith \"just a monolith except we don't suck at coding\"? reply closeparen 16 minutes agorootparentLet's use MVC for the sake of argument. A regular monolith has lots of models, lots of controllers, and lots of views. A modular monolith has several collections of models/controllers/views, which might be tightly coupled internally, but the collections as a whole exposes much smaller APIs to each other. You cannot just reach into an implementation detail of distantly related functionality, even if this functionality is \"public\" in the programming language package visibility sense (i.e. users repository is visible to users controller). This is basically what's accomplished by publishing a Thrift/Proto/OpenAPI IDL from a collection of packages comprising a network service. The key insight is that the serialization and networking parts of this are superfluous, what you actually wanted was the visibility rules. reply noworriesnate 30 minutes agorootparentprevA modular monolith has a single executable which runs in different modes, typically depending on environment variables. So you can run three processes in the mode that handles web requests, five processes in the mode that processes events on a queue (e.g. Kafka), etc. Eight processes, running in two different modes, but it's all the same executable. That's the basic idea of a modular monolith. By \"plain monolith\" I meant just any kind of monolith. reply loloquwowndueo 2 hours agorootparentprevA distributed monolith! Worst of both worlds! I’m just kidding of course. reply karmajunkie 1 hour agorootparentprevyes, it’s called SOA and it’s been around for decades at this point. reply arunix 1 hour agorootparentprevIs this correct? Practically every job advert I've seen claims they are using microservices (and want candidates with such experience). reply Alupis 1 hour agorootparentIt is not correct. This is the sentiment people who don't understand k8s often have, because of handwavy complexity blah blah blah. The predictable quote is always along the lines of \"unless you're Google scale...\" - which misses perhaps 80% of what microservices bring to the table. Then they go off and build complicated monorepos that take 6 months to learn before you can effectively contribute. All paradigms have trade offs. Engineering is about understanding those trade offs and using them where appropriate. Unfortunately too many people just jump on the \"k8s is complicated\" bandwagon (because it challenges most of the assumptions they've previously held in software development) and entirely write-off microservices without taking the time to learn what problems microservices solve. reply sosodev 1 hour agorootparentprevYou're generalizing way too much. There are still tons of teams out there running and creating new microservices. reply hello_moto 2 hours agorootparentprev> people just went back by being productive building modular \"monolithic\" architectures, and using \"microservices\" only when absolutely necessary. The number of hi-tech companies that are in the middle-to-large scale have increased significantly from the first wave of Rails era. Majority of hi-tech companies with listed stock have complexity more than \"monolithic\" architecture. Sadly, if a company doesn't grow, they will get eaten by their competitors. reply int_19h 2 hours agorootparent> Majority of hi-tech companies with listed stock Isn't that kinda circular? Generally speaking, companies only list their stock when they grow large. The vast majority of web dev is not happening in those kinds of companies. reply hello_moto 2 hours agorootparentThere are publicly listed hi-tech companies that may not be that big... reply ninininino 36 minutes agorootparentprevWe stopped making microscopic microservices but we still ship services. Services are deployable independently from each other, can be scaled independently from each other. A monolith that results in a single build artifact / executable or whose contents must all run in a single pod or application server is inherently harder for larger teams to work on. Deploying a small change means re-deploying your entire application. Scaling your authz system means scaling up your analytics data producers. Separating your code into services that run and deploy independently from each other means organizations can scale without creating headaches for developers. reply ch4s3 28 minutes agoparentprev> while the trend has been to encourage avoiding monolithic architectures like Rails I'm of the opinion that this was always bad advice for most people most of the time. Achieving any real infrastructure cost savings this was is difficult, its easy for delivery speeds to suffer, and the debugging experience is terrible. reply sergiotapia 4 minutes agoparentprevmicroservices, like graphql, have certainly fallen from grace by more senior engineers. i see mostly jrs advocate for it because that's what they were taught. a senior engineer will try to keep the monolith for only as possible and only then explore a new tiny service. reply megaman821 2 hours agoprevI don't use Rails, but those Solid Adapters look cool. I think people would be surprised how long a RDBMS good enough before moving to more specialized solutions. Just jumping to best of class solutions for cache, pub/sub, full-text search, vector search, document storage, etc. adds too much operational complexity if you can get by just using a single database. reply ksec 1 hour agoparentAll the Solid Adapters but they couldn't name one as Solid Snake. Huge waste of opportunity. Hopefully more to come in 8.1 reply sarlalian 43 minutes agorootparentSolid Snake really should end up in the python world. reply 0xblinq 33 minutes agorootparentOr an adult movie reply HatchedLake721 3 hours agoprevI sometimes wish I had picked up Ruby/RoR instead of Node.js ~10-15 years ago. reply FigurativeVoid 2 hours agoparentIt's never too late! The Rails community is very welcoming. reply ecshafer 1 hour agoparentprevI missed the big Ruby on Rails fad because I was busy doing scientific computing at the time. But I picked up Rails 3 years ago for a job, and its fantastic. I do wish I picked it up 15 years ago though, I would rather use Rails than Spring, akka, node, or any of the other frameworks ive been using. reply throwaway918299 1 hour agoparentprevIf you do node professionally, you should be able to pick up Ruby on Rails in a weekend. reply wry_discontent 2 hours agoparentprevI had a Rails job and slipped back into node, and it's so sad. Node is in a sad state compared to Rails. They don't even know what a framework looks like. reply tebbers 2 hours agorootparentIt's the terrible standard library of JS that keeps me with Ruby. Rails makes it even better. Being able to write little things like 3.days.from_now to get a timestamp is great. reply saltcod 2 hours agoparentprevCurious if anyone is out there doing both effectively. Node at work, Rails on the side. Something like that. Feels like a lot to master, but not sure. reply Lukas_Skywalker 14 minutes agorootparentI use Node at one of my employers, and Rails at the other. It is much easier to switch between them than I first expected. I can quite easily use the idiomatic patterns in either framework. But there are obviously very large differences in the two stacks. The most important that come to mind: - Rails is much more \"batteries included\". No need to decide what ORM, queue, mailer, remote storage, etc you want to use for the new project. You just use the built in solution. And you place the files in a pre-defined location. That makes it really easy to onboard new people, because they are familiar with the file structure. On the other hand, for Node projects, you can mix and match your components, and use simpler structures for side projects, and more complex structures (including tools like Nx) for more complicated projects. - The Rails ORM is much more powerful than the Node counterparts. I think it helps, that the ORM is built into Rails, so everyone contributes to the same product. For Node, it feels as if there are many ORMs that are 70% done, but don't cover important cases. One of them is fast but a bit buggy, one is slow but has the ability to deal with schema migrations, one is old and works well but does not support TypeScript. - Documentation of Node frameworks, for example NestJS, seems to be updated quicker than the Rails counterparts. For Rails, especially when Turbo was released, there was a kind of vacuum regarding the documentation. The docs for Turbo did explain the basics, but it was very very difficult to get it working with the official docs. Once blog posts started to pop up, it became much easier. Projects like Nest seem to have much more detailed documentation for their components. All in all, I do prefer Rails tbh. The DRY principle that the framework follows makes sure that you don't have to edit 8 files when adding a new database column. This makes development much faster and in my opinion less error prone. reply moritonal 2 hours agorootparentprevPlaying with a WebDev project in Node.js but jobs with Rails. Node makes things like OAuth trivial with the mix of Express and Passport, something that took me two weeks in Rails. But man does Rails make Sequelize look childish by comparison. reply FanaHOVA 1 hour agorootparentWhat's wrong with devise OAuth? Always found it super easy. reply stouset 2 hours agoparentprevNo reason you can’t start now! reply wkirby 2 hours agoprevI still think, pound for pound, it’s hard to beat the productivity and stability of rails as a framework for new web projects. Welcome changes here, but most notably, this new major version update doesn’t appear to have any real breaking changes with the apps we’ve been running in production for years. reply dewey 1 hour agoprevI'm mainly a Go developer but I've picked up Rails when version 7 came out for all my hobby projects and it's just _fun_ to work with. I don't have to work with complicated frontend tooling and I just push to Dokku to deploy. reply ashconnor 1 hour agoparentThat new proxy they've developer \"thruster\" is written in go https://github.com/basecamp/thruster reply geoka9 1 hour agoparentprevAs a backend Go developer who had used Rails a lot many years ago, I recently had to do a full-stack project where I got to pick my own stack. I liked the idea of HTMX and also added a-h/templ into the mix. I feel like this setup allows me to be almost as productive as with Rails when doing things that Rails is good at, while enjoying the control and simplicity of the Go approach. reply justinko 1 hour agoprev20 year Rails veteran here looking for a full-time position. My GitHub handle is the same. reply cdiamand 3 hours agoprevAnybody have any opinions on moving away from Redis for cables/caching/jobs? I supposed it'd be nice to have one less thing to manage, but I'm wondering if there are any obvious gotchas to moving these features over to sqlite and postgresql. reply xutopia 3 hours agoparentFor caching specifically solid_cache works better for long lived and larger caches than does Redis. If you need short lived and smaller caches Redis will be more performant for you. That said you can probably get away with caching way more with solid_cache and it's highly dependent on what your use cases are. Also a thing to note that your DB backing solid_cache might also be using RAM efficiently, giving you a lot of benefits that Redis did. For new projects I'll be using solid_cache first and adding Redis only on an as-need basis. The cost of Redis is orders of magnitude higher than a DB backed cache. reply cdiamand 2 hours agorootparentThanks for this. I've run into \"giant cache\" issues in the past in this exact use case. I'll give solid_cache a look. reply ksec 1 hour agorootparentDHH mentioned 10TB of cache and only 0.5ms increase in latency difference between Redis and SQLite. I wish other could test it out and have some other figures to point to. But if the stated case were true then I think sacrifice 0.5ms for 10 to 20x more cached resources sounds like a very good deal to me. reply lordofmoria 3 hours agoparentprevI had a bad experience with Action Cable + Redis (extremely high memory overhead, tons of dead redis connections), so it's a bit \"fool me once\" with regard to action cables. The main argument for caching in the DB (the slight increase in latency going from in-memory->DB-in-memroy is more than countered by the DB's cheapness of cache space allowing you to have tons more cache) is one of those brilliant ideas that I would like to try at some point. Solid job - i just am 100% happy with Sidekiq at this point, I don't understand why I'd switch and introduce potential instability/issues. reply et-al 3 hours agorootparentWhat are you using in lieu of Action Cable for websocket connections? reply justinko 1 hour agorootparentprevCheck out anycable reply olcarl 1 hour agoparentprevI have used both with rails. (Cable is still going through Redis tho). Solid cache is perfect for my use case since page caches doesn't change as often, so taking a smaller memory footprint on the server farm is a win. My take is to measure your cache hit percentage. This will allow anyone to understand their cache eviction rates. If you have high eviction rates maybe using a btree is not the way to go and redis is probably better reply JamesSwift 2 hours agoparentprevI havent used them, and we are not moving to them on our existing app but I can super-appreciate the fact that by default redis is more of a down-the-road decision now. No reason to get into the complexity (+ additional hosting cost) of adding another service into the mix until you choose to later on. reply nomilk 3 hours agoparentprevI'm sticking with redis purely because it's battle tested and I'm not endowed with enough talent nor time to work through kinks in Solid. reply azuanrb 2 hours agoparentprevOn Rails homepage, it says from “Hello World” to IPO. The idea is that Rails should help you maintain a lean stack by default. You can stick with Postgres for pretty much everything: caching, background jobs, search, WebSockets, you name it. But, as your app grows, you can swap things out. Redis or Elasticsearch are solid choices if you need them. DHH mentioned that as well, at scale, everyone does things differently anyway. But you do have the option to keep it simple, by default. For me personally, Rails 8 is great. My new project only need Postgres and that's it. I don't need Redis, multiple gems for background jobs or cache anymore. Able to avoid the chaotic JS ecosystem for frontend. Hopefully it will be easy to setup Hotwire Native too. It really streamlined things, and letting me focus on building features instead. That said, for my production apps in existing companies, I’m sticking with what’s already working: Sidekiq, Redis, Elasticsearch. If it ain’t broke, why fix it? Will probably revisit this decision again in the future. Too early to tell for now. reply nicoburns 1 hour agoparentprevI've had good experiences using postgres for background jobs. reply resters 1 hour agoprevSome really great ideas came out of Rails 1 and 2. After that Rails became more of a business, but good ideas still came out of the ecosystem and germinated (in my view more successfully) in the fertile soil of node / js, as well as sinatra -> flask / express, etc. But Rails' core value prop (doing a lot with strongly opinionated structure) is still useful to a lot of people. My complaint was always that version upgrades in rails were too costly -- teams would spend months -- and it wasn't usually clear whether upgrading was worthwhile. Also the business aspect led to a lot of ephemeral (very hot and then soon unmaintained) patterns/libraries. Rails 8 makes me want to look again and see how the system overall has matured. reply xutopia 5 minutes agoparentUpgrading from 1-4 was difficult but since 4 it has been really easy. Often times nothing much to do from one version to the next. reply nomilk 1 hour agoparentprev> led to a lot of ephemeral (very hot and then soon unmaintained) patterns/libraries For hard core web enthusiasts it makes sense to keep toward the cutting edge, but for boring old sites you just want have them run well and not have to worry about swapping out x for y every few months. I look forward to swapping out sprockets but I know there'll be teething issues. Same for built in authentication, so I won't be swapping out devise any time soon, probably ever. Same with redis for solid (won't be changing any time soon). Absolutely love the continued work on rails, but it doesn't mean I'll necessarily those new features, which is totally cool - rails doesn't force you to. With so much mix-and-match it can get confusing though. When I see a tutorial using importmaps I get confused because I only know esbuild. Now there's propshaft. Similarly with asset pipeline, I sometimes struggle with sprockets, now there's something new to learn and all its interactions with other pieces. (mostly paths and debugging is what I have trouble with, i'm no JS wizard [or ruby wizard for that matter]). (I just want to spend 95+% of my time making things that solve real problems, not tinkering with and finessing pieces of the framework) reply swombat 1 hour agoprevI've been working with RoR since back in 2008 (Rails 2.1 yay!). I'm still working with RoR. It's still an incredibly quick, powerful, flexible, versatile framework. I'm able to build pretty large and complex apps all by myself, quite quickly, without fuss. I'm looking forward to the deployment improvements since that was one of the shortcomings that was still around. Kamal v1 didn't really do it for me (currently using Dokku instead). Maybe Kamal 2 is it. reply faizshah 3 hours agoprevI highly recommend you watch the first half hour of DHH Rails World talk on Rails 8 even if you aren’t a Ruby dev: https://youtu.be/-cEn_83zRFw?si=ANVPRory_J0LKjZj The idea of Rails 8 is to move away from trend following and try to reject some of the ideas that have become popular in the industry (like being afraid to touch a Linux server or implementing your own auth). Really though provoking stuff. reply rubyfan 3 hours agoparentPopular ideas are exactly the problem in the industry. I’ve been really put off by what I believe is a lack of critical and original thinking in tech. Bad leadership within management, cult and fad following seems to abound. reply mattgreenrocks 2 hours agorootparentThere’s a real feeling of the blind leading the blind in webdev. htmx, Rails, and Laravel all point to better ways, and people are starting to be receptive. reply xutopia 3 hours agoparentprevThis is part of the reason why I love the Rails community so much. It isn't afraid to break down the consensus and find its own path forward. Often with huge benefits. reply galoisscobi 3 hours agoparentprevI second this. I’m not a ruby dev and I watched the whole talk. It was excellent. Goes on to show how for many applications overly priced platforms as a service aren’t really needed but incidental complexity masquerading as essential complexity is peddled as a way to make money. reply MangoCoffee 3 hours agoparentprev>try to reject some of the ideas that have become popular in the industry just my two cents, the web development has become akin to the fashion industry. We've seen a steady stream of frameworks emerge, from traditional PHP and JavaScript to JavaScript-based development like ReactJS, AngularJS, and now WebAssembly while tried-and-true methods still work. all the new frameworks bring benefits – but also introduce problems a few years later. The trends drive the IT industry, fueling a cycle of book sales, training camps, and consulting services that corporations buy into every few years. reply wiseowise 9 minutes agorootparent> JavaScript-based development like ReactJS, AngularJS React is 11 years old, AngularJS is 14 years ago, let it go already, gramps. reply IWeldMelons 3 hours agorootparentprevcant wait when we'll start making webdev in C CGI and finally in Verilog. reply hello_moto 3 hours agoparentprevIt's DHH marketing style (it sez right there in his earlier book: pick a fight). Back when Rails burst into the scene, he picked the EJB mountain to battle and positioned Rails vs EJB for as long as he could. This is another battle in the Infrastructure world that he picked. The timing was perfect too: Rails grew during economy crisis 2006-2009 (layoff, EJB complexity, web-app complexity, the rise of single-developer entrepreneur). Right now we're sort of in similar situation too: layoffs, infrastructure complexity, BigTech culture, FAANG-leetcode grind. Tech is like a cycle :) reply gadders 3 hours agorootparent>>It's DHH marketing style (it sez right there in his earlier book: pick a fight). I think rappers invented starting a beef to shift records. reply Thaxll 3 hours agoparentprevHis compagny seems to like re-inventing the wheel with vastly inferior solutions though. reply pier25 2 hours agorootparentI don't know about Rails but Kamal 2 is pretty great. It's their own E2E solution for deploying and running Docker apps in production. Solves SSL, proxy, zero downtime deploys, etc. https://kamal-deploy.org/ reply jack_riminton 2 hours agorootparentprevExample? reply mplewis 1 hour agorootparentKamal, Thruster, removing TypeScript from Rails without notice, moving into Webpack and back out again without a reasonable upgrade path, pushing Hotwire over real frontends. reply itake 1 hour agoprevLots of great features for new rails apps, but what about the app was has been upgraded since rails 4? Are people actually going to spend time to replace redis or devise? I wish they’d add new features instead of trying to get people off successful 3rd party gems. reply hamandcheese 2 hours agoprevI'm curious what the state of Sorbet + Rails is these days. I've been out of the ruby game for a little while now. Last I recall: sorbet itself is quite good once it is well established in a project, but adding type checking to a project (especially a Rails project) is a lot of work. reply yoran 2 hours agoparentWe use it extensively in our codebase. We started without any types, and added Sorbet later. It's similar to Typescript as that you can gradually sparkle your code with types, building up the typing coverage over time. I just completed a big refactoring. We have a good suite of tests. But Sorbet has provided an added layer of confidence. Especially when it comes to the treatment of null values. Sorbet will raise an error if you try to call a method on an object that may be null. So it forces you to think through: what should happen if the object is null? So the tests combined with Sorbet typechecking made that we could just almost blindly deploy refactoring after refactoring, with only a handful of bugs for several 1000s of lines of code changed. reply gejose 1 hour agoparentprevWe use sorbet pretty extensively. Overall it's been a net positive, but if you're coming from something like typescript, IMO it's far, far behind in terms of usability. You can get pretty good value from things like intellisense in your editor, knowing that the constant you're referencing or method you're calling exists etc, but most libraries have no typings in sorbet-typed (beyond `T.untyped`). The last post on Sorbet's blog was from 2022 https://sorbet.org/blog/, so I'm not sure how actively it's being developed. (compare that to typescript where you have a post every month https://devblogs.microsoft.com/typescript/) reply riffraff 37 minutes agorootparentAFAIU sorbet is still being developed actively enough at Stripe, but I think there's also ongoing work at Shopify to integrate the Prism parser APIs which will be available in Ruby 3.4 this December https://github.com/sorbet/sorbet/network reply jack_riminton 2 hours agoparentprevConsidering how anti-types dhh is, there is not going to be any official support for this anytime soon. In his most recent interviews the philosophy can be summarised as \"if you love types, good for you, but rails isn't for you\" reply block_dagger 2 hours agorootparentThis is pretty close to Matz’s stance as well. reply mahatofu 1 hour agorootparentPersonally, the argument for strong types is when you’re writing critical foreseeably permanent architecture. It’s an extra layer of security and explanation. reply gejose 1 hour agoprevI find that I have a love/hate relationship with Rails. Love the framework, not so much the language, particularly how it doesn't have strong typing. Really easy to write, but hard to refactor in my opinion. I've been looking for a framework as good as rails for typescript, but haven't found anything to that level. The closest I've seen so far have been: * Adonis https://adonisjs.com * NestJS https://nestjs.com Is there anything better than these for the node world? reply mike_ivanov 2 minutes agoparenthttps://stackoverflow.com/questions/2690544/what-is-the-diff... reply noobermin 3 hours agoprevPeople just don't talk about Ruby anymore. For those who don't do webdev, has it just stabilised and people take it for granted now, or was it always hype and people have long since moved on? reply rogerrogerr 3 hours agoparentI've worked on many Rails apps that are still out there in critical spots. My gut feel is it has stabilized. The hype was well-founded; it allows dev velocity that I still haven't seen in any other environment. I once worked on the integration of an existing Rails app into an existing C# webapp (both living side-by-side, kind of a split-application thing, but seamless to the end user). It was almost hilarious how many LOC the C# teams (!) were having to write for every LOC I (alone) was writing in Rails. Looking through their diffs, it seemed like they were reinventing fire compared to me writing `render :success if SomeModel.update(model_params)`. reply neonsunset 2 hours agorootparentIt speaks more about the unfortunate state of practice in the team in question. C# itself is of similar terseness when compared to Ruby, with 10x to 20x better performance so even when written in a bulkier way, it becomes less relevant as you are not going to hold, say, C or C++ against similar criticism in such context. Oh, also no method not found thing too. C# has much greater likelihood of not having defects when you do deploy it. reply wiseowise 3 minutes agorootparent> C# itself is of similar terseness when compared to Ruby, with 10x to 20x better performance so even when written in a bulkier way More like 50-100x. reply frostymarvelous 1 hour agorootparentprevI'd say this speaks more to the metaprogramming capabilities of rails rather than the team itself. Rails simply does more for you out the box with just a few macros. reply neonsunset 1 hour agorootparentDo you have a specific example in mind? reply mostlysimilar 3 hours agoparentprevBoth Ruby and Rails have continued to improve year over year and are better today than they have ever been. reply brink 3 hours agoparentprevRails usage is certainly dropping off a bit. The competition is improving. I personally got tired of the monkey patching, ActiveRecord's proclivity towards n+1 queries, and complete lack of types / static analysis, among other things. Having to keep a mental model of the types of the entire project, and having to write tests around types takes up a significant amount of my headspace, energy, and time. The older a project gets, the more of a headache these things become. There is an element of \"just write better code\" to avoid these pitfalls, and that works to an extent, but the older I get the more I realize that the tooling does play a large role in the outcome. reply int_19h 2 hours agorootparentI'm firmly in the static typing camp myself, but at the same time I think it's good that there's still a language that is unabashedly dynamically typed. Looking at Python, when you take a language that was originally designed around dynamic typing and then bolt static typing on top, the result is just awkward all around, and ends up being the worst of both worlds. reply berkes 2 hours agorootparentprev> \"just write better code\" to avoid these pitfalls A lot of issues in and with Rails code leans heavily on this idea. It often comes in other flavors like \"You can write rails apps that perform really well\" or \"Its perfectly doable to write a clean easy maintainable codebase in rails\". And is apparent in the \"Provide sharp knives\" doctrine as \"And by capable, I mean knowledgeable enough to know when and how, accordingly to their context, they should use the different and sometimes dangerous tools in the drawers.\" I really dislike this approach. I don't want something that is full of foot-guns and therefore requires me to recognise them when I lack all information to do so. I've written terrible Rails apps, exactly because it allowed me to so, and I didn't know they would become terrible, until they were. I now recognise the (design) patterns that are foot-guns, I now know the tell-tales that show me where the sharp knives are dangerously placed. But then I get dropped into a Rails codebase that has seen 20+ developers coming and going over a decade, as the 21st developer, and I see it filled with these footguns and dangerously wielded \"sharp knives\". Often there's a lot of history apparent from these previous devs who saw this too and fixed some of it. But the overall state is still abysmal. Sure, it's all these devs that made it abysmal. But the ecosystem, the framework and the language allowed, and sometimes even encouraged them to make it that. reply Mystery-Machine 1 hour agorootparentprevThe N+1 queries issue has been addressed in the latest ActiveRecord versions. If there's an N+1, you can set ActiveRecord to raise an error instead of making the N+1 query. Bullet gem, for years now, has also been able to catch the N+1 queries and let you choose how you want to act upon that. It's true that monkey patching is a problem. I hear you. It's more and more problem of the past though. People realized it's too sharp of a knife and stopped doing that. reply _joel 3 hours agoparentprevStill a bunch of stuff that runs on ruby that isn't webdev, https://github.com/theforeman/foreman is a big one I can think of. Ruby's quite nice for sysadmin tasks, puppet, chef etc all in ruby iirc. reply cout 59 minutes agorootparentI've built a lot of niche applications in Ruby, but a few years ago I switched most of my toy programs to python, because it's easier to find other devs who can meaningfully contribute if I write in python. At work I have been using Ruby for longer than rails has existed. In those days there were lots of neat uses for Ruby (one of the most interesting being at NASA), but none of us knew if it would ever catch on. We didn't care -- Ruby made programming _fun_, and we were happy to use it even if no one else ever did. It was partly the language that made Ruby fun but also the community. And it's out of this community that projects like chef, puppet, shoes, etc. were born. Matz has called Ruby human- oriented programming, and I can think of like that is human- oriented than community. reply JamesSwift 2 hours agorootparentprevMetasploit and homebrew also come to mind EDIT: also cocoapods reply hello_moto 2 hours agoparentprevPeople never fully adopted Ruby for Ruby sake. People adopted Ruby because of Rails. Another more adopted Ruby software are probably fluentD and maybe Jekyll. The rest kind of comes and goes. Majority sys-admin toolings have always been dominated by Python and with k8s hype, some of the toolings have been geared towards k8s automation written in Golang. In general, sys-admin toolings are a mixed bunch: perl, shell, python, ruby reply lordofmoria 3 hours agoparentprevRails went through a down period 2014-2020 due to several reasons: 1. React burst on the scene in 2014 2. the hyperscale FANG companies were dominating the architecture meta with microservices, tooling etc, which worked for them at 500+ engineers, but made no sense for smaller companies. 3. there was a growing perception that \"Rails doesn't scale\" as selection bias kicked in - companies that successfully used rails to grow their companies, then were big enough to justify migrating off to microservices, or whatever. 4. Basecamp got caught up in the DEI battles and got a ton of bad press at the height of it. 5. Ruby was legitimately seen as slow. The big companies that stuck with Rails (GH, Shopify, Gitlab, etc, etc) did a ton of work to fix Ruby perf, and it shows. Shopify in particular deserves an enormous amount of credit for keeping Ruby and Rails going. Their continued existence proves that Rails does, in fact, scale. Also the meta - tech-architecture and otherwise - seems to be turning back to DHH's favor, make of that what you will. reply rgbrgb 3 hours agorootparentyou might mean shopify, not spotify. I think spotify is python/go, whereas shopify was started by a rails core contributor and probably has the biggest rails deployment in the world reply dropofwill 2 hours agorootparentSpotify is mostly Java, with some Scala and Node. reply lordofmoria 2 hours agorootparentprevYes, edited! reply hocuspocus 2 hours agorootparentprevThe RoR hype started to wane long before React. You're really missing a huge part of our industry: - While most 2nd or 3rd tier tech companies don't need Google scale infrastructure, SOA in Java/C# and then Go is incredibly prevalent. Many teams never had a reason to even look at RoR and its considerably worse language and runtime. - Good ideas from RoR were copied by pretty much every ecosystem; again, most people never wanted Ruby in the first place. - Plenty of small web shops simply stuck to PHP. reply hello_moto 2 hours agorootparentprev>which worked for them at 500+ engineers, but made no sense for smaller companies The number of hi-tech companies that grew from 37signals size to Uber size have also increased due to various reasons: SaaS becoming more and more accepted, WallStreet loves SaaS, and in general just more investment money in the market. reply dyingkneepad 1 hour agoparentprevSpeaking from my own experience at work, for non-webdev, if the project is something I'll want to share with other people, there is a great incentive in just using Python. I can't seem to get people to be interested at all in my tools if they're in Ruby. When I write stuff in Python, people naturally read, use and contribute. reply jonathaneunice 2 hours agoparentprevIt's stable and mature. Maybe you don't hear the constant clickity-clack of endless change as a result. Also true that Ruby's most popular in writing web apps and services, where it and Rails continue to shine. webdev FWIW is a ginormous part of this little internet thing. You know, those Web-based applications and services that run the entire global economy and human society, such as it is. reply cout 1 hour agoparentprevHow would you define \"hype\"? reply square_usual 2 hours agoprevWhat's the difference between the new `script` directory/generator and using rake? I thought rake was meant for basically those things? Also, is there a good reason to use thruster instead of caddy? reply bibstha 1 hour agoparentThis will answer your question: https://github.com/rails/rails/pull/52335#issuecomment-22395... > I think there can be some overlap between rake tasks and one-off scripts and opinions may vary. In the related discussion Okura Masafumi mentions that rake tasks are one way but they can be executed multiple times, and I tend to agree, leaving rake tasks for code that is intended to be run multiple times. > So script/ can hold one-off scripts, for example Basecamp uses script/migrate which I am guessing they use for updating data, and we had a similar folder at my previous company. > But script/ is not only for one-off scripts, it can hold more general scripts too, for example rails generate benchmark already generates benchmarks in script/benchmarks, and at my previous company we had script/ops which had a bunch of shell scripts and ruby scripts for various ops-related things. So really not so clear description. It caters to those who feel like they have a script that doesn't really fit in the Rake file. reply mplewis 1 hour agoparentprevI really don't understand the NIH that the Rails team is bringing in with Kamal and Thruster. Not everything that HEY uses has to be pushed onto folks at other companies. reply mostlysimilar 1 hour agorootparentI recommend watching the Rails World opening keynote for some context: https://www.youtube.com/watch?v=-cEn_83zRFw reply Mystery-Machine 1 hour agorootparentprevNothing is pushed. You can upgrade your Rails 7 application to Rails 8 today and no Solid* nor Kamal nor Thruster will be used in your app. It's there for you to use it. If you don't want to use it, don't. They released these new tools and, if you want to use something else, you can. That's the whole idea of Rails. reply software_writer 3 hours agoprevFor a concise overview of all the new frameworks and announcements, I recommend reading the official Rails blog post from DHH: https://rubyonrails.org/2024/9/27/rails-8-beta1-no-paas-requ... reply joshmlewis 2 hours agoparentAgreed, this should be the main link instead of giving AppSignal free publicity. The original is very much a light reworking of the original. It has the same content order and everything. reply rgbrgb 3 hours agoparentprevyeah, OP feels like a gpt rewrite of this post by DHH reply sprkwd 2 hours agoprevIt’s heartening to see Ruby and the Rails community go from strength to strength. When I was a developer I loved the Ruby language. reply solanav 3 hours agoprevIs it me or is the introduction written in a very chatgpt-esque style? reply Terretta 3 hours agoparentIt didn't invent this corp-chipper style. It just read all of it. reply uhtred 1 hour agoprevHoping for a renaissance of frameworks like rails and django now that we have tools like htmx. Such a nicer way to build stuff. reply trevor-e 1 hour agoprevI really want to try Rails, but the main selling points to me were the fast scaffolding and opinionated structure. With the advent of AI editors, are these points still relevant? Last I tried, CursorAI struggled with Ruby due to the lack of types. It seems like nowadays you can be nearly as productive with Node + Typescript + Cursor. edit: care to explain the downvotes for asking a question? reply bibstha 1 hour agoparentVery much. I still heavily use rails generators as the defaults do quite a bit already. Generates nice forms, models, migrations, jobs, scripts, and test files. It doesn't feel much, like it's just generating a bunch of files with basic defaults in the right folder, but when you do that quite a bit overtime, you appreciate how little you have to think about those things. reply Mystery-Machine 1 hour agoparentprevI hear you. I use Cursor editor. I work with all of these technologies: Ruby on Rails, React, Next.js, Django, vanilla JS, jQuery, HTML/CSS, Tailwind...depending on the project. I just spent the past week working on a hackathon where I built the project using a Node.js closed-source platform that uses Fastify edge functions and Cursor's autocomplete was terrible. Much worse than what I'm used to when Cursor makes autocompletion for Ruby/Rails code. JS libraries come and go every day. This makes it difficult for Cursor team to update their LLMs to provide good autocompletion. On the other hand, Ruby and Ruby on Rails have seen very few radical syntax changes. The syntax and majority of the API has remained the same. Ruby on Rails is so much more than just fast scaffolding. The code I've written a decade ago in Ruby on Rails, I can still pick it up today and understand the project. I can also pick up anyone else's project and figure out the intricacies of it much faster than I'd ever be able to do in say Next.js or Express project. Express feels like writing assembly (no hard feelings). I've, unfortunately, recently started working on a project that's written in Django. I thought Django is just Ruby on Rails for Python. Boy was I wrong... But that's a story for another time. IMO Ruby on Rails is an absolute king! However, I'm looking forward for a day when I'll be able to use one language on both frontend and backend, while having the same simplicity and functionality as Ruby on Rails. (I'm looking forward for Ruby on Rails JavaScript successor, but it's just not there yet.) reply trevor-e 28 minutes agorootparentNice, so Rails does work well with Cursor? I'll give it a try then. Like you said, maybe it just struggles on random Ruby code but not Rails which has a ton of existing examples to go off of. reply textread 42 minutes agorootparentprevPlease do tell us the Django horror story reply stuart_real 1 hour agoprev [–] As per GitHub's popularity chart, Ruby was the #5 language in 2014, and #10 in 2022 - https://octoverse.github.com/2022/top-programming-languages Starting a new commercial project/company in 2024 in Ruby is questionable. reply hakunin 1 hour agoparentCorrection: you mean 2014, not 2024. Do you think there is no criteria worth considering besides the size of the hiring pool? What if 2 hiring pools are sufficiently big, do you still pick the bigger one every time? reply stuart_real 1 hour agorootparentThanks. Corrected. > Do you think there is no criteria worth considering besides the size of the hiring pool? What if 2 hiring pool are sufficiently big, do you still pick the bigger one every time? It is not just a hiring pool. Look at various third-party SASS tools, they see Ruby as a second-grade language to support. reply hakunin 1 hour agorootparentThat's not at all my experience. There's a one-line integration between almost everything under the sun and Rails (business/traffic analytics, performance/profilers, error notifications, logging, object storage integration, I could probably go on). Many other frameworks don't even have the general capability of doing one-line integrations. reply pdntspa 1 hour agoparentprevRising from #10 to #5 seems to imply that a lot of people are making 'questionable' decisions. As an active senior RoR dev since like 2014ish seeing that makes my heart atwitter :) reply stuart_real 1 hour agorootparentI corrected the mistake. It went from #10 to #5. Only language with such secular decline. reply Mystery-Machine 59 minutes agorootparentPardon my sarcasm: You should work on WordPress. It's the most popular website framework in the world. reply pjm331 1 hour agoparentprev [–] one could also argue that making tech decisions based on popularity charts is questionable reply stuart_real 1 hour agorootparent [–] One could argue that making tech decisions while completely ignoring popularity charts is more questionable. reply Mystery-Machine 58 minutes agorootparent [–] You're right. Those shouldn't be completely ignored. They probably shouldn't be the only argument to make tech decisions upon. Did you ever build any webapp in Ruby on Rails? You should check it out! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ruby on Rails 8 has released its first beta, featuring integration with Kamal 2 for simplified deployments, Propshaft as the new default asset pipeline, and significant ActiveRecord enhancements.",
      "SQLite integration upgrades make it suitable for production environments, and Solid Adapters reduce the need for additional services by utilizing SQLite.",
      "Kamal 2 offers zero-downtime deployments with HTTP/2 support, while Propshaft modernizes the asset pipeline, replacing Sprockets, and built-in authentication is streamlined."
    ],
    "commentSummary": [
      "Ruby on Rails 8 introduces significant updates, including a new edition of \"Programming Ruby\" and updates to \"The Rails Way\" specifically for Rails 8.",
      "New features in Ruby, such as YJIT (Yet Another Just-In-Time Compiler) for enhanced speed and asynchronous capabilities, are creating excitement within the community.",
      "The Rails community remains optimistic about its future, discussing potential shifts from Redis to new caching solutions, while valuing Rails for its productivity and stability despite some concerns about async feature adoption."
    ],
    "points": 337,
    "commentCount": 166,
    "retryCount": 0,
    "time": 1728312312
  },
  {
    "id": 41765716,
    "title": "Can you get root with only a cigarette lighter?",
    "originLink": "https://www.da.vidbuchanan.co.uk/blog/dram-emfi.html",
    "originBody": "Welcome to my ::'########::'##::::::::'#######:::'######::: :: ##.... ##: ##:::::::'##.... ##:'##... ##:: :: ##:::: ##: ##::::::: ##:::: ##: ##:::..::: :: ########:: ##::::::: ##:::: ##: ##::'####: :: ##.... ##: ##::::::: ##:::: ##: ##::: ##:: :: ##:::: ##: ##::::::: ##:::: ##: ##::: ##:: :: ########:: ########:. #######::. ######::: ::........:::........:::.......::::......:::: CTF writeups, programming, and miscellaneous stuff. Blog Index Can You Get Root With Only a Cigarette Lighter? By David Buchanan, 7th October 2024 Spoiler alert: Yes. the elite hacking tool they don't want you to know you already own Before you can write an exploit, you need a bug. When there are no bugs, we have to get creative—that's where Fault Injection comes in. Fault injection can take many forms, including software-controlled data corruption, power glitching, clock glitching, electromagnetic pulses, lasers, and more. Hardware fault injection is something that typically requires specialized (and expensive) equipment. The costs stem from requiring a high degree of precision in terms of both when and where the fault is injected. There are many valiant attempts at bringing down the costs, with notable projects ranging from the RP2040-based PicoEMP, all the way to \"Laser Fault Injection for The Masses\". (The RP2040 crops up a lot due to its low cost combined with the \"PIO\" peripheral, which can do I/O with tight timings and latency) A while back I read about using a piezo-electric BBQ Igniter coupled to an inductor as a low-budget tool for electro-magnetic fault injection (EMFI), and I was captivated. I wondered, how far can you take such a primitive tool? At the time, the best thing I could come up with was exploiting a software implementation of AES running on an Arduino, using DFA—it worked! But I wasn't fully satisfied. I wanted to exploit something more \"real,\" but I was out of ideas for the time being. Fast forward to a couple of weeks ago, and the announcement of the Nintendo Switch 2 is on the horizon. We anticipate the Switch 2 will run largely the same system software as the Switch 1, and we're all out of software bugs. So, I was inspired to brush up on my hardware exploitation skills, and revisited my thoughts on low-budget EMFI. The Test Subject Like any self-respecting hacker, I own a pile of junk laptops. I picked out a Samsung S3520, equipped with an Intel i3-2310M CPU and 1GB of DDR3 RAM. Manufactured in 2011, it's new enough that it can comfortably run a lightweight desktop Linux distro (I picked Arch), but crappy enough that I wasn't worried about bricking it. My goal is to write a local-privilege-escalation exploit that works based on injected hardware faults. I decided that the most physically vulnerable part of the laptop was the DDR bus that connects the DRAM memory to the rest of the system. If you've ever looked at a laptop memory module (SODIMM), you'll notice it has a whole lot of pins. Among them are 64 \"DQ\" pins (numbered DQ0 to DQ63) that transfer data bits in either direction (read or write). I figured that if I could inject faults on one of these pins, I could do something interesting. After a lot of fiddling around, here's the hardware setup I came up with: If I counted right, this corresponds to pin 67, aka DQ26 It's just one resistor (15 ohms) and one wire, soldered to DQ26. The wire acts like an antenna, picking up any nearby EM interference and dumping it straight onto the data bus. The resistor (which might be entirely unnecessary) is just there to make sure that the interference isn't so great as to disturb normal operation of the memory—I only want glitches to happen on-demand, not all the time. Ignore the random electrical tape; this laptop has been through a lot. I found that clicking a regular piezo-electric lighter (no inductor coils needed) in the vicinity of the antenna wire was enough to reliably induce memory errors, shown here under memtest. Note that the errors shown both correspond to bit 29 being flipped. Why bit 29, when I soldered to DQ26? Honestly, I'm not entirely sure; either I miscounted the pins or my laptop's motherboard swaps some of the data lines around. As far as I can tell, swapping data lines like that is allowable (it can make signal routing easier). We don't have much control over when we inject the fault (to the resolution of my finger's reaction speed), but whenever it happens we can be fairly certain it will always flip the same bit of any particular 64-bit read or write. Exploiting Bit-flips in CPython As a starting point, I wanted to try writing a \"sandbox escape\" exploit for CPython. This is purely academic since CPython isn't even sandboxed in the first place and you can just do os.system(\"/bin/sh\"), but I needed something easy to get started with and I'm already familiar with CPython's internals. My explanation for this exploit is going to be a bit hand-wavey because the specifics aren't that interesting, it's the overall strategy that I want to convey. For exploiting CPython, I actually used a wire soldered to DQ7 instead of the DQ26 pictured earlier, for reasons that will become more obvious shortly. CPython objects live on a garbage-collected heap. An object has a header that contains its refcount, then a pointer to its type object, followed by other type-specific fields. There are two object types of particular interest to us, bytes and bytearray. bytes objects are immutable and bytearray objects are mutable. A bytes object has a length field, followed by the data itself (as part of the same heap allocation). A bytearray object has a length field followed by a pointer to the actual data storage buffer. The core idea of my exploit strategy is to instantiate a bytes object that contains a fake bytearray structure within it. The fake bytearray object is just data, we can't do anything with it, but if we trick CPython into giving us a reference (pointer) to this fake object, then we can construct an arbitrary memory read/write primitive (since we chose the bytearray's length and pointer fields ourselves). So how do we get a reference to the fake object? Recall that our initial \"exploit primitive\" is the ability to flip bit 7 of a 64-bit word. This is equivalent to either adding or subtracting 128 ( 2 7 ) from a pointer. If our fake bytearray object was at an offset of +128 bytes within the bytes object, then glitching a pointer to the bytes object will transform it into a pointer to the crafted bytearray object (with 50% probability). So the big question is, how do we glitch that specific pointer, as opposed to everything else? If we accidentally glitch some important data we'll probably crash the whole OS, which is obviously not good. Something important to remember is that we're glitching the memory bus, not the memory contents (as in something like Rowhammer). We only glitch the read and write operations, the data \"at rest\" is mostly safe. The solution here is to spam memory accesses of the pointer we want to glitch. If 99% of bus activity is saturated with exploitable operations, then a randomly timed glitch has (in theory) a 99% chance of landing somewhere we want it to. If we read the same pointer in a loop, almost nothing would happen. This is because the CPU caches the data to avoid unnecessary DRAM accesses (cache is fast, DRAM is comparatively slow and high-latency). My solution was to fill up a big array (larger than the 3MiB cache this CPU has) with references to the same object. Then I can access the array items sequentially in a loop, forcing the CPU to fetch them from DRAM each time, and check to see if their value changed. The inner loop looks like this: 1 2 3 4 5 6 7 # \"victim\" is the prepared `bytes` object described earlier spray = (victim,) * 0x100_0000 # I actually use a tuple instead of an array, same idea for obj in spray: if obj is not victim: # under non-glitchy conditions, this is always false print(\"Found corrupted ptr!\") assert(type(obj) is bytearray) Most of the time this won't work, so the whole thing is done from inside another big loop, until it does work (or until the system crashes 🙃). Python's is keyword comes in handy here, it's essentially a pointer comparison operation, allowing us to check if the pointer changed. Visualising the objects in memory, it looks like this: A \"glitched\" pointer is shown in red, which is now able to access the fake bytearray object. The glitch itself could occur during either a write or a read, the net result is the same either way. The rest of the exploit isn't especially interesting; I set up a repeatable read/write primitive and then craft a Function object that jumps to shellcode. You can see the full source here. The script also has an option (the TESTING variable) to induce simulated bit-flips through software, which is useful for testing without any hardware setup. Exploiting Bit-flips in Linux Now that we've warmed up, it's time for a proper security boundary crossing. Can we get from an unprivileged Linux user, to root? There are three core concepts we need to understand first: Memory caching (which I already touched on) Virtual Memory and Page Tables The Translation Lookaside Buffer (TLB) Memory Caching As I mentioned earlier, DRAM is (relatively) slow and high-latency. So the CPU has on-die caches, which are faster. These caches have multiple tiers (L1, L2, L3) with different size/locality/latency trade-offs, but for our purposes we only have to care about the L3 cache (the largest layer, 3MiB in this case). If data currently resides in cache (a \"cache hit\"), the CPU won't need to access DRAM to read it. If there's a \"cache miss\" on the other hand, the CPU will have to reach out to DRAM. The smallest unit of memory from the caching perspective is a \"cache line\", and on my laptop that's a 64-byte chunk. That means that if you read even a single byte that isn't cached, a whole 64-byte DRAM read operation takes place. You may recall that the DRAM data bus itself was only 64 bits wide, which means the read happens in a \"burst\" of 8 sequential accesses. The precise policy the CPU uses to decide which data to keep in the cache, and when to \"evict\" it, is effectively a vendor-proprietary secret. But as a reasonable approximation we can model it as a LRU cache: least-recently-used cache lines get evicted first. Virtual Memory Back in the old days, simple CPUs like the MOS 6502 had a \"flat\" address space. If your program tried to read from address 0xcafe, then the CPU would physically set the 16-bit address bus pins to 0xcafe (0b1100101011111110) and read back a byte from that location. Aside from hardware tricks like bank switching, the address you requested was the address you got data back from. Simple as! My 6502 computer, built many years ago (the USB-C port is a contemporary addition). Note the D0-D7 data pins and A0-A15 address pins. Fast forward a number of years, and we want to run more than one program at once on our CPUs. Furthermore, each program should be tricked into believing it has the whole address space all to itself. This is useful for a lot of reasons, including that it stops one process from being able to clobber another process's memory (accidentally or otherwise). In the modern era, we call this trick Virtual Memory. Virtual Memory means that there's a layer of indirection between the address a program tries to access (the virtual address), and the underlying physical address space. Each process can have its own virtual->physical mappings, keeping different processes isolated from each other. On x86-64 (and most other modern architectures), this indirection is implemented using the concept of Paging. The virtual address space is split up into 4KiB Pages, and a tree-structured hierarchy of Page Tables dictates how the CPU (specifically the MMU) decodes virtual addresses and maps them onto physical pages. On this platform there are 4 layers of Page Tables. Officially the 4 layers all have different names, but I'm going to call them \"level 3\" to \"level 0\", with level 3 being the root of the tree. A Page Table is itself a 4KiB page, containing an array of 512 Page Table Entries (PTEs), each a 64-bit structure. A PTE either points to the physical address of the next-level Page Table (in the case of levels 3 to 1), or the physical address of the \"destination\" page (in the case of level 0). The physical address of the root page table is stored in the CR3 CPU register. The PTEs themselves have the following layout (diagram via osdev wiki) The only part that we need to care about is the address portion, in the middle. When you mask off all the flag bits, you're left with a physical memory address (since pages are 4K-aligned, the low bits are always zero). For a probably-better explanation along with some diagrams, check out this article from \"Writing an OS in Rust\" (note that they number the levels 4 to 1, which is probably more conventional 😅). The Translation Lookaside Buffer If the process of traversing page tables to resolve a virtual address sounds expensive, that's because it is. That's where the TLB comes in. It's a specialized piece of hardware inside the CPU that efficiently caches virtual-to-physical page mappings. The TLB has a finite size, and I don't actually know the size for my laptop but from what I can tell, it's somewhere on the order of 1024 entries (note that each entry corresponds to a whole page-sized mapping). Exploit Strategy My exploit strategy was inspired by elements of Mark Seaborn's Rowhammer exploit. The main goal is to get a Page Table for our own process mapped into user-accessible memory. Once we have that, we can modify the PTEs within it to grant ourselves access to arbitrary physical memory, which is essentially the keys to the kingdom. Rather than try to control the layout of structures in physical memory (A physical-memory version of heap feng shui, I suppose?), my strategy is to fill up (aka \"spray\") as much of physical memory as possible with level-0 page tables. In practice I fill exactly 50% of physical memory. Once the spray is complete, I sit in a loop trying to access a bunch of R/W mappings in a way that bypasses the TLB (because the number of mappings exceeds the TLB size), forcing a page table traversal on each access. I want to glitch the memory bus during that traversal in order to corrupt bit-29 of a level-0 PTE read. If I'm lucky (with about 50% odds) the glitch will offset the physical address that the PTE points to, making it point to one of the level-0 page tables that we sprayed earlier. This should theoretically work with bit-flips in any bit position between 29 (corresponding to a 512MiB offset) and 12 (corresponding to a 4KiB offset). It just matters that the PTE ends up pointing \"somewhere else,\" and because we've filled up ~50% of physical memory with exploitable page tables, we have a good chance of success. Therefore, soldering the antenna wire perhaps isn't totally necessary, if you can generate strong enough electromagnetic interference (although you'd have much higher chances of crashing or even bricking the whole system that way). Here's a visualisation of how the fault affects the page tables: Each of the blocks in this diagram represent a 4K page of physical memory. Their locations within memory are more or less random, and aren't relevant to the exploit logic. What does matter is what points to what. The glitched PTE (in red) is supposed to point to the R/W page, but now it points to another level-0 page table, providing access to it as if it were a regular R/W page. In practice there are thousands more of these level-0 pages, and the glitched PTE could've ended up pointing to any one of them, but I can only fit so many on the diagram. So how do I spray so many level-0 page tables? First, I create a memfd, which is a relatively modern Linux feature. It fills the same role as the /dev/shm/ file in Mark Seaborn's exploit, but without having to touch the filesystem at all. Then I use the mmap syscall to map this same buffer into memory, many times over. I use the MAP_FIXED option to force each mapping to be 2MiB-aligned in virtual memory, which guarantees the creation of a new level-0 page table each time. Linux has a ~ 2 16 limit to the number of mappings (VMAs, Virtual Memory Areas) that each process is allowed, so I make each mapping 32MiB long. This means each one generates 16 level-0 page tables. Although each mapping takes up 32MiB of virtual memory space, the PTEs all point to the exact same underlying physical pages. The physical memory cost of each mapping consists only of the level-0 page tables, and therefore I can spray as many of them as I want, until memory is full. As I said before, we attempt to access the R/W mappings in a loop, waiting for a fault to happen. We can detect a fault because an unexpected value will come back, and if it was successful then the data should look like a PTE. If so, we now have R/W access to a page table. The next step is to figure out which virtual address this page table corresponds to. I do this by modifying the PTE to point at physical address 0 (which is an arbitrary choice, any address should work really) and then scanning the R/W mappings again to see which one changed, if any. The MMU won't immediately \"notice\" edits to the PTE, because the virtual->physical address mappings are cached by the TLB. Every time we change it, we need to flush the TLB. There's no direct way of doing this from userspace (that I know of?) so instead I just access a few thousand of the R/W mappings in a loop, forcing the TLB to fill up with new values and evict the old ones. At this point, we have full read/write access to all of physical memory! There are a bunch of strategies we could use from this point onwards, and again I took inspiration from the Rowhammer exploit. I open the /usr/bin/su executable (which is setuid root) nominally in read-only mode (I don't have write permissions!) and mmap the first page of it. Then, I scan through all of physical memory until I find that same page. Once I've found the physical page, I have full write access to it, and I replace it with my own tiny ( /proc/sys/vm/drop_caches), so the next time someone invokes su it functions normally again. My full exploit source can be found here. Here's a demo video (sorry it's a bit blurry): I was unusually lucky on this run, normally it takes several clicks of the lighter to get a good glitch. Not shown are the several previous attempts that crashed the whole system! I'm not sure what the overall exploit reliability is, I haven't tried to measure it rigorously. When the laptop's screen is off and I'm SSHed in it feels like it's around 50%. But when I'm at a graphical shell like in the demo, it's maybe closer to 20% reliability. This system has integrated graphics, so perhaps the GPU's memory accesses interfere with the exploit. There are also various background services running (pipewire, sshd, systemd stuff etc.), and swap is enabled too. I wanted it to be a fairly realistic desktop Linux environment, and disabling all these things would probably increase reliability. If the laptop had more RAM installed, I'd be able to fill an even higher percentage of it with page tables, which should also increase the overall exploit reliability. Practical Uses As cool as my Linux LPE is, I already had root on that laptop because it's mine. Is there anything more \"useful\" we can do with it? I'm not much of a PC gamer (more of a Nintendo fanboy), but I'm always irked when I see \"anti-cheat\" software that uses technologies like TPM to restrict the software you're allowed to run on the rest of your system. Perhaps a reliable EMFI Windows LPE would let gamers take back control of their PCs without interfering with TPM attestation status. Imagine a future where \"Gaming RAM\" sticks have a RP2040 on board to automate the whole exploit (and also drive the RGB LEDs, of course). There's a similar story with Android devices and SafetyNet/Play Integrity checking, although fitting a glitching modchip into a phone would be more of a challenge. Thoughts On a conceptual level, I've known about page tables and TLBs for a long time. But even when working on low-level performance optimizations, that knowledge has never actually mattered to me (Caching on the other hand becomes relevant all the time!) But in this exploit, all of it absolutely does matter, and it's been very satisfying to finally test that theoretical knowledge. Actually seeing and interacting with the structures that maintain the illusion of virtual memory felt a bit like escaping the matrix. Open Questions Does it work on DDR4, DDR5? (I don't see any reason why not!) Does it work on ARM? (Likewise) To what extent do the various types of ECC mitigate this? (DDR5 Link-ECC in particular) What's the simplest way to trigger similar faults electronically? (say, with an RP2040) Can you use this to break out of a hypervisor? Can I write a Webkit exploit with this? Can I write a Nintendo Switch kernel exploit with this? I'm going to be looking into these in the future—stay tuned! Finally, I'd like to thank JEDEC for paywalling all of the specification documents that were relevant to conducting this research. Homepage - Blog Index - RSS This blog is part of the Haunted WebringA word from our unofficial sponsors:",
    "commentLink": "https://news.ycombinator.com/item?id=41765716",
    "commentBody": "Can you get root with only a cigarette lighter? (da.vidbuchanan.co.uk)317 points by 1317 5 hours agohidepastfavorite72 comments intothemild 5 hours agoThis reminds me of exploits we used to do to arcade cabinets back in Sydney in the 80's and 90s. The school gas heaters used to have what we called \"clickers\", piezoelectric ignition devices you could remove from the heaters. You then took that clicker to your local arcade, and clicked one of the corners of the CRT, that would send a shock through the system and add credits to your game. I believe this was because the CRT was grounded on the same ground lines that the mechanism for physically checking a coin had gone through the system. Suffice to say, they caught onto this over time, and added some form of an alarm into it. But up until then... Those were truly the best times. reply TowerTall 2 hours agoparentWe did the exact same thing early 80's except that we used the clicker found in disposal lighters. We did it for a couple of years until they figured it out and started to conver the arcade cabinets with transparent plastic. At the same time they also drilled holes at the back of the machine for ventilation as the rest of the case now was sealed in plastic. We found out that using a bamboo stick you could press the lever that register when a coin has been paid into the slot. That made them relocate the holes for the ventilation to the top of the case instead of the back so we couldn't get the lever anymore. Or so they thought. haha We discovered that by pressing a coin up the return slot — the one where you get your coin back if it isn’t accepted — you could also trigger the lever for coin registration and the free gaming continued. Eventually they put in sharp screws into that coin return box so you would cut your finges. After that we got a SEGA. Was great fun :) reply jacobgkau 2 hours agorootparentAt what point does the arcade just kick you out? I can't imagine them seeing you continuously tamper with their equipment to circumvent paying and think, \"the best way to handle this is to keep modifying our machines.\" reply bityard 15 minutes agorootparentArcades were big dark noisy rooms, and quite often had only one or two people on staff who were usually either busy dealing with other customers and were paid far too little to care about the owners' profit margins. They were basically there to hand out prizes to little kids for the ticket machines and make sure nobody walked out with Dig Dig on a hand cart. reply cutemonster 1 hour agorootparentprevMaybe the staff at the arcade, aren't the owners of the place, so they don't personally care that much. They'd rather be friends with everyone, than to be the \"angry police\"? (And I'm guessing the tampering players were nice people to have around) And the technicians \"improving\" the machines -- maybe they had a good time too, I'm wondering. @TowerTall and friends made their job more interesting / fun? reply an_ko 1 hour agorootparentprevIf you kick someone out, you lose them as a customer, and they'll tell all their friends about the free play trick out of spite, so you'll have to patch the machine anyway. reply jacobgkau 1 hour agorootparentYou're making me wonder what the stats are for how many people try to abuse arcade machines in a country like Japan versus the United States. (Not that people in any country are gonna be entirely honest, but the entitlement to break the system and the comfort to brag about it seems cultural.) In fact, that could be why some of the machines weren't better protected against that stuff in the first place, right? reply giancarlostoro 4 hours agoparentprevReminds me of an arcade machine a friend would get behind, turn it off and back on, and it would give you a free token. Maybe its designed that way so the employee can test it for free, not sure. But he climbed behind it, and proceeded to play for free. reply IWeldMelons 4 hours agorootparentThose who lived in USSR remembers soda vending machines (they poured your drink in a glass cup; you were expected to wash it before using by pressing on a cup, which stood upside down on plastic plate with holes, kinda inverted shower head; very unhygienic, I know). Well it had a button behind that let you have a free drink. You could also \"upgrade\" pure carbonated water (1 kopeyek) to a sweet soft drink (3 kopeyek) by pressing another button. needless to say schoolchildren would abuse the hell out of this \"feature\". reply everforward 3 hours agorootparent> you were expected to wash it before using by pressing on a cup, which stood upside down on plastic plate with holes, kinda inverted shower head; very unhygienic, I know Those systems are occasionally used in bars in the US, though they've dropped the whole plate and it's usually just arms where the holes are. To my understanding, at least in the US, they aren't used for deep-cleaning anything. That happens with soap and water in the back still. The upside-down-showers are used to clean out the dregs of someone's glass when they get a refill (you give them a glass, they give it a quick rinse, refill it and hand it back), and as a quick rinse for new glasses to clean up water stains/detergent residue and anything that might have fallen in since they were cleaned (hair, dust, etc). reply IWeldMelons 3 hours agorootparentYes right, the key difference that the were used to clean between uses by different customers; this is clearly insufficient; at least because a good deal of customers - drunks, children, people with mental issues would not wash at all before use, a good vector for disease spread. Late USSR I happen to remember always had problems with hepatitis spread, which is considerably less of a problem today, due to adoption of disposable food containers/utensils. reply JamesSwift 2 hours agorootparentprevIts been a long time since I worked in a bar, but in the front-of-house we used a three-sink station where the sinks were: soap, water, sanitizing-solution. Then you sit the glasses to drip-dry. Actually here is a link explaining it: https://www.webstaurantstore.com/article/620/three-compartme... reply stavros 1 hour agorootparentI've seen something like this in the Netherlands, although even more disgusting: They take the used glass, dunk it in a bucket that has brushes all around and in the middle and is full of soapwater, rotate the glass three times against the glass, take it out, and pour the beer in the glass. Yes, the glass's sides are still full of the disgusting soapwater from the bucket that's now basically 95% other people's drink dregs. reply baud147258 3 hours agorootparentprevI think for beer there's a reason of bringing the glass to a colder temperature, which (from what I've heard) should reduce the amount of foam (not sure that's the exact term) in the glass. reply everforward 1 hour agorootparentOh, are the lines refrigerated or otherwise thermally controlled? I always presumed it was regular tapwater; i.e. probably slightly below room temp, but not much. Mileage obviously varies, but the \"beer nerd/snob\" bars I've been to simply don't re-use glasses without a full wash. They'd rather just charge a little more to hire more dishwashers and be able to absolutely guarantee that there's no leftover beer/water in your glass when they refill it, and that the glass is refrigerated if that's something they want. I've always heard the head/foam had more to do with how you pour the beer (more impact/movement = more foam), but it makes sense that temperature affects it as well. There's some kind of official course on how to pour Guinness to get the correct head on it. I don't remember the whole thing, but it was something about holding the glass the correct distance from the tap and tilting it so that the beer \"slides\" down the side of the glass rather than a direct perpendicular impact with the beer already in the glass (which makes more foam). reply jcrash 3 hours agorootparentprev> pressing on a cup, which stood upside down on plastic plate with holes, kinda inverted shower head I think they still use these in bars https://barsupplies.com/collections/glass-washers reply everforward 3 hours agorootparentprevI believe some of those early arcade games were more electrical engineering than software engineering, so perhaps it was easier to set it up that way? To my understanding some of those early arcade games also had jumpers to control some of the behavior. It could be that a tech set the \"free credit on reboot\" jumper and forgot to reset it when they were done. reply astrostl 2 hours agoparentprevThis also worked in the USA. By the 1990s most arcades operated on proprietary tokens rather than coin currency. Many had skill-gambling machines that had sliding rows covered in tokens, that you would try to dislodge with your own tokens and keep what was displaced. The \"Jungle Jive\" version of this would dispense tokens out the opposite side of the machine if the electric ignition of a cigarette lighter was used to lightly shock the metal intake slot. If you clicked it too much too quickly it would go into an alert mode. While this could be accomplished solo, the ideal MVP setup was a team of three: one scout to watch for employees, one to click, and one to collect. reply chasd00 5 hours agoparentprevThis brings back a vague memory of smacking the side of a pinball machine just right and getting a free game. I bet it was the same concept. reply intothemild 4 hours agorootparentI imagine (with zero research) that the mechanism for adding credit would be the coin goes through a slot, and either itself completed a circuit, or the coin as it travels moves some lever to complete a circuit. So I imagine if you hit the machine just right, you'd also move that lever. reply candlemas 4 hours agorootparentprevJust like The Fonz. reply DonHopkins 2 hours agorootparentHenry Winkler is actually just as cool as the character he played! reply devmor 4 hours agorootparentprevYou were likely causing the spring-loaded mechanism that detects a coin insertion to make physical contact. reply wgrover 4 hours agorootparentYup - the first few minutes of one of Technology Connections' videos on electromechanical pinball machines shows this mechanism in action: https://www.youtube.com/watch?v=E3p_Cv32tEo reply luismedel 3 hours agoparentprevThis trick worked in Telefonica's phone booths in Spain in the 90s too :-) reply zxexz 2 hours agorootparentI remember when Verizon phone booths in the US started accepting the credit cards, for a while they would accept any 16-digit number with a valid IIN that passed the Luhn check. reply chrisweekly 3 hours agorootparentprevI vaguely remember (sometime in the 80s) sticking a straightened paperclip into a small hole on the face of a payphone to avoid having to drop a dime / quarters, and being able to call anywhere. reply 8ig8 2 hours agorootparentIf I recall, you’d stick the straightened paperclip into one of the holes on the mouthpiece and touch the other end of the paperclip to some metal part on main phone body. War Games used a pull tab from an aluminum can to a similar effect? (It’s been a while.) reply roymurdock 1 hour agoparentprevsuper cool reply ballenf 5 hours agoprevThe inspiration here was getting root on the Switch 2. Getting root in Linux was the POC. The goal was not demonstrating some fundamental security vulnerability that's practically exploitable, but instead for reclaiming actual ownership of one's own hardware without breaking TPM or game ring 0 anti-cheat. reply vessenes 5 hours agoprevI like this. Upshot - electrostatic bit flip on memory read or write, which with solder can deterministically get a 'safe' pointer mutated into your own evil pointer. Generally the historical perspective on physical access was: \"once they have it, game over.\" TPM and trusted execution environments have shifted this security perspective to \"we can trust certain operations inside the enclave even if the user has physical access.\" His next steps are most interesting to me -- can you get something (semi-) reliable without soldering stuff? My guess is it's going to be a lot harder. Lots of thought already goes into dealing with electrical interference. On the other hand, maybe? if you flip one random bit of a 64 bit read every time you click your lighter, and your exploit can work with one of say 4 bit flips, then you don't need that many tries on average. At any rate, round 2 of experimentation should be interesting. reply onionisafruit 4 hours agoparent> if you flip one random bit of a 64 bit read every time you click your lighter Without the antenna it would be hard to limit it to a single bit getting flipped. At least that’s what I suspect. reply Retr0id 4 hours agorootparentOn the flip-side (heh) flipping multiple bits at once should make it possible to bypass ECC reply Lance_ET_Compte 1 hour agorootparentYou'd likely take an exception for a multi-bit error and the handler would likely just retry the read. Single-bit errors are often just corrected on the fly by ECC logic as you mention. reply vessenes 4 hours agorootparentprevwe need a tinfoil waveguide clearly reply treflop 19 minutes agoprevI thought OP was going to do this without soldering anything. But I feel like soldering something is no different than just like splicing a telephone cable in half and putting your own headset in the middle… Except instead of putting a headset, you crudely use a lighter… reply i4k 3 hours agoprevThis was very well written and an amazing challenge but my brain is wired to that \"hacking common sense\" that if you have physical access then it's already over... the first thing that came to my mind was that, if you have physical access, then you can reflash the BIOS, install a driver backdoor, you can boot a live OS and then it's just a matter of tampering /etc/{passwd,shadow,groups, etc} ... but I remembered that most of the physical access hacks would not be possible if the disk is encrypted.. which then makes this kind of hack enormously attractive. The antenna idea can be extended to be a piece of hardware with the interference device built-in (piezo or whatever) which communicates with the external world with any wireless medium and then the attacker can trigger the interference remotely. This, plus a website controlled by the hacker which the victim is scammed to visit can be enough to make it viable. reply 333c 3 hours agoparentThe motivation in the introduction is rooting/jailbreaking a handheld game console. I think this is a perfectly plausible situation where you have physical access but still want to obtain \"unauthorized\" access. reply johnisgood 3 hours agoparentprev> I remembered that most of the physical access hacks would not be possible if the disk is encrypted.. Only if you have not booted into your system through using a keyfile or a passphrase to decrypt the data, i.e. if your PC is shut down. I have full disk encryption, and when I boot into my system, it uses the keyfile with which it would perform the decryption, and boom, I have my PC ready to be accessed physically. reply smcl 4 hours agoprevI reckon you can get a root with just a cigarette lighter if you hang around outside the right bars in Australia reply Stefan-H 3 hours agoparentAnd worst case there is always the rubber hose. reply jacobgkau 2 hours agorootparentI think you misunderstood the Australian slang. That person was not referring to the XKCD concept. They were referring to another meaning of the word \"root.\" reply Stefan-H 1 hour agorootparentHa! Thanks for the elucidation. My assumptions around the GP did include the assumption of sex, but it was more in a honeypot context rather than as an end in an of itself. reply twelve40 2 hours agorootparentprev...or a $5 wrench reply zephyreon 5 hours agoprevMy immediate thought was that this was a post about how someone got root access to a cigarette lighter and I was totally ready to believe it. My parents oven gets regular software updates so I didn’t even question whether the cigarette lighter was “smart.” reply onionisafruit 5 hours agoparentFrom the title I half expected an incendiary version of rubber hose cryptography. reply roymurdock 5 hours agoprev\"It's just one resistor (15 ohms) and one wire, soldered to DQ26. The wire acts like an antenna, picking up any nearby EM interference and dumping it straight onto the data bus.\" really neat hack. using the lighter to create EM interference. better go light up next to my DDR bus and see what happens :) reply mmsc 1 hour agoprevNot only is it a fun exploit, this is also a cool mini-introduction to how caching works for CPUs. I remember a year ago or so there was a submission here which detailed how computers work and are build starting at the tiniest part: starting with logic gates, IIRC. Anybody remember what that website was? reply pvitz 51 minutes agoparentDo you mean nand2tetris? https://www.nand2tetris.org/course reply ano-ther 5 hours agoprevSure, if you solder an antenna to your memory first :-) But good and thorough write-up about how to actually exploit such a glitch. And you could also use the cigarette lighter for hanging out at the data center back door and wait until the admin comes for a smoke. reply Retr0id 5 hours agoparent> This should theoretically work with bit-flips in any bit position between 29 [...] and 12 [...] Therefore, soldering the antenna wire perhaps isn't totally necessary, if you can generate strong enough electromagnetic interference reply abound 2 hours agorootparentMentioned elsewhere in this thread, but you need not only \"strong\" but \"highly directed\" electromagnetic interference. Each of those pins is ~0.5mm, flipping a single bit \"wirelessly\" is probably impossible, as your inference will cause issues in many more places than just your target. Maybe that unlocks different and exciting hacks, maybe it just melts your machine. reply hardburn 5 hours agoparentprevDown in the \"practical use\" section, one use case is bypassing copy protection on consoles. reply QuiDortDine 5 hours agoprevYou know when your employee quits how you have to block all their accounts? Now imagine they have access to the server room! reply pantulis 5 hours agoparentAnd that's why server rooms should have proper physical security. reply appendix-rock 5 hours agorootparentAnd why “they’ve got physical access, so all bets are off” isn’t an excuse to stop trying reply yjftsjthsd-h 3 hours agorootparentI don't follow; isn't this proof that physical access does trump everything else? reply amelius 5 hours agorootparentprevAnd be wrapped in tinfoil. reply 0xdeadbeefbabe 3 hours agoparentprevThis kind of work can't be done under pressure at least not a PoC. reply Pikamander2 4 hours agoprevWhen I saw the title, I was expecting this to be about hacking a modern car with one of those USB-C cigarette lighter devices. reply _trampeltier 1 hour agoprev2 days ago https://news.ycombinator.com/item?id=41748861 reply CartwheelLinux 5 hours agoprev>I only want glitches to happen on-demand, not all the time. >My injected ELF also flushes the page cache The difference between a padawan and a jedi Amazing write up and bonus points for the reproducibility of this creativity. reply KolmogorovComp 4 hours agoprevJust wanted to say it was an amazing write-up. reply mensetmanusman 4 hours agoprevNext, a balloon and carpet! reply jojobas 5 hours agoprevBack in the day of analog electronic locks a piezo zap into the lock case would unlock 4 out of 5 apartment building locks, root access IRL. reply m3kw9 2 hours agoprevI’m gonna do one with “ Can You Get Root With Only my bare hands?” reply mimentum 5 hours agoprevI read this wrong. reply adrian_b 5 hours agoprev... \"Finally, I'd like to thank JEDEC for paywalling all of the specification documents that were relevant to conducting this research.\" reply _joel 3 hours agoprevNice trick, now do it with cosmic rays! reply mikewarot 4 hours agoprev [–] >Can You Get Root with Only a Cigarette Lighter? No, you can't. That long lead to couple your ersatz pulse generator defeats all the engineering put into making the computer reliable and quiet in the EMI sense. Circuit bending is fun stuff, but it's not a remote exploit. reply jasongill 3 hours agoparentWhere in the article does he say this is a remote exploit? reply _joel 3 hours agoparentprev [–] The old saying of \"if you've got physical access, game over\", is where this applies. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses using a piezo-electric BBQ lighter for low-cost electromagnetic fault injection (EMFI) to exploit hardware vulnerabilities.",
      "Experiments were conducted on a Samsung S3520 laptop, targeting the DDR bus to induce memory errors, leading to local privilege escalation in CPython and Linux.",
      "The method shows potential for bypassing security measures like TPM (Trusted Platform Module) in gaming PCs, with future research planned for newer technologies and platforms."
    ],
    "commentSummary": [
      "The post discusses using cigarette lighters to create electromagnetic interference, exploiting arcade machines and devices for free credits in the 80s and 90s.",
      "It highlights the broader security implications of physical access to devices, suggesting that such access often leads to compromised security.",
      "The original topic was about using a lighter to cause memory bit flips, demonstrating creative methods and challenges in exploiting hardware vulnerabilities."
    ],
    "points": 317,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1728307219
  },
  {
    "id": 41760421,
    "title": "Rust needs a web framework",
    "originLink": "https://ntietz.com/blog/rust-needs-a-web-framework-for-lazy-developers/",
    "originBody": "technically a blog homeblog / tagssletterprojectsbooks Rust needs a web framework for lazy developers Monday, September 30, 2024 I like to make silly things, and I also like to put in minimal effort for those silly things. I also like to make things in Rust, mostly for the web, and this is where we run into a problem. See, if I want to make something for the web, I could use Django but I don't want that. I mean, Django is for building serious businesses, not for building silly non-commercial things! But using Rust, we have to do a lot more work than if we build it with Django or friends. See, so far, there's no equivalent, and the Rust community leans heavily into the \"wire it up yourself\" approach. As Are We Web Yet? says, \"[...] you generally have to wire everything up yourself. Expect to put in a little bit of extra set up work to get started.\" This undersells it, though. It's more than a little bit of extra work to get started! I know because I made a list of things to do to get started. Rust needs something that does bundle this up for you, so that we can serve all web developers. Having it would make it a lot easier to make the case to use Rust. The benefits are there: you get wonderful type system, wonderful performance, and build times that give you back those coffee breaks you used to get while your code compiled. What do we need? There is a big pile of stuff that nearly every web app needs, no matter if it's big or small. Here's a rough list of what seems pretty necessary to me: Routing/handlers: this is pretty obvious, but we have to be able to get an incoming request to some handler for it. Additionally, this routing needs to handle path parameters, ideally with type information, and we'll give bonus points for query parameters, forms, etc. Templates: we'll need to generate, you know, HTML (and sometimes other content, like JSON or, if you're in the bad times still, XML). Usually I want these to have basic logic, like conditionals, match/switch, and loops. Static file serving: we'll need to serve some assets, like CSS files. This can be done separately, but having it as part of the same web server is extremely handy for both local development and for small-time deployments that won't handle much traffic. Logins: You almost always need some way to log in, since apps are usually multi-user or deployed on a public network. This is just annoying to wire up every time! It should be customizable and something you can opt out of, but it should be trivial to have logins from the start. Permissions: You also need this for systems that have multiple users, since people will have different data they're allowed to access or different roles in the system. Permissions can be complicated but you can make something relatively simple that follows the check(user, object, action) pattern and get really far with it. Database interface: You're probably going to have to store data for your app, so you want a way to do that. Something that's ORM-like is often nice, but something light is fine. Whatever you do here isn't the only way to interact with the database, but it'll be used for things like logins, permissions, and admin tools, so it's going to be a fundamental piece. Admin tooling: This is arguably a quality-of-life issue, not a necessity, except that every time you setup your application in a local environment or in production you're going to have to bootstrap it with at least one user or some data. And you'll have to do admin actions sometimes! So I think having this built-in for at least some of the common actions is a necessity for a seamless experience. WebSockets: I use WebSockets in a lot of my projects. They just let you do really fun things with pushing data out to connected users in a more real-time fashion! Hot reloading: This is a huge one for developer experience, because you want to have the ability to see changes really quickly. When code or a template change, you need to see that reflected as soon as humanly possible (or as soon as the Rust compiler allows). Then we have a pile of things that are quality-of-life improvements, and I think are necessary for long-term projects but might not be as necessary upfront, so users are less annoyed at implementing it themselves because the cost is spread out. Background tasks: There needs to be a story for these! You're going to have features that have to happen on a schedule, and having a consistent way to do that is a big benefit and makes development easier. Monitoring/observability: Only the smallest, least-critical systems should skip this. It's really important to have and it will make your life so much easier when you have it in that moment that you desperately need it. Caching: There are a lot of ways to do this, and all of them make things more complicated and maybe faster? So this is nice to have a story for, but users can also handle it themselves. Emails and other notifications: It's neat to be able to have password resets and things built-in, and this is probably a necessity if you're going to have logins, so you can have password resets. But other than that feature, it feels like it won't get used that much and isn't a big deal to add in when you need it. Deployment tooling: Some consistent way to deploy somewhere is really nice, even if it's just an autogenerated Dockerfile that you can use with a host of choice. CSS/JS bundling: In the time it is, we use JS and CSS everywhere, so you probably want a web tool to be aware of them so they can be included seamlessly. But does it really have to be integrated in? Probably not... So those are the things I'd target in a framework if I were building one! I might be doing that... The existing ecosystem There's quite a bit out there already for building web things in Rust. None of them quite hit what I want, which is intentional on their part: none of them aspire to be what I'm looking for here. I love what exists, and I think we're sorely missing what I want here (I don't think I'm alone). Web frameworks There are really two main groups of web frameworks/libraries right now: the minimalist ones, and the single-page app ones. The minimalist ones are reminiscent of Flask, Sinatra, and other small web frameworks. These include the excellent actix-web and axum, as well as myriad others. There are so many of these, and they all bring a nice flavor to web development by leveraging Rust's type system! But they don't give you much besides handlers; none of the extra functionality we want in a full for-lazy-developers framework. Then there are the single-page app frameworks. These fill a niche where you can build things with Rust on the backend and frontend, using WebAssembly for the frontend rendering. These tend to be less mature, but good examples include Dioxus, Leptos, and Yew. I used Yew to build a digital vigil last year, and it was enjoyable but I'm not sure I'd want to do it in a \"real\" production setting. Each of these is excellent for what it is—but what it is requires a lot of wiring up still. Most of my projects would work well with the minimalist frameworks, but those require so much wiring up! So it ends up being a chore to set that up each time that I want to do something. Piles of libraries! The rest of the ecosystem is piles of libraries. There are lots of template libraries! There are some libraries for logins, and for permissions. There are WebSocket libraries! Often you'll find some projects and examples which integrate a couple of the things you're using, but you won't find something that integrates all the pieces you're using. I've run into some of the examples being out of date, which is to be expected in a fast-moving ecosystem. The pile of libraries leaves a lot of friction, though. It makes getting started, the \"just wiring it up\" part, very difficult and often an exercise in researching how things work, to understand them in depth enough to do the integration. What I've done before The way I've handled this before is basically to pick a base framework (typically actix-web or axum) and then search out all the pieces I want on top of it. Then I'd wire them up, either all at the beginning or as I need them. There are starter templates that could help me avoid some of this pain. They can definitely help you skip some of the initial pain, but you still get all the maintenance burden. You have to make sure your libraries stay up to date, even when there are breaking changes. And you will drift from the template, so it's not really feasible to merge changes to it into your project. For the projects I'm working on, this means that instead of keeping one framework up to date, I have to keep n bespoke frameworks up to date across all my projects! Eep! I'd much rather have a single web framework that handles it all, with clean upgrade instructions between versions. There will be breaking changes sometimes, but this way they can be documented instead of coming about due to changes in the interactions between two components which don't even know they're going to be integrated together. Imagining the future I want In an ideal world, there would be a framework for Rust that gives me all the features I listed above. And it would also come with excellent documentation, changelogs, thoughtful versioning and handling of breaking changes, and maybe even a great community. All the things I love about Django, could we have those for a Rust web framework so that we can reap the benefits of Rust without having to go needlessly slowly? This doesn't exist right now, and I'm not sure if anyone else is working on it. All paths seem to lead me toward \"whoops I guess I'm building a web framework.\" I hope someone else builds one, too, so we can have multiple options. To be honest, \"web framework\" sounds way too grandiose for what I'm doing, which is simply wiring things together in an opinionated way, using (mostly) existing building blocks1. Instead of calling it a framework, I'm thinking of it as a web toolkit: a bundle of tools tastefully chosen and arranged to make the artisan highly effective. My toolkit is called nicole's web toolkit, or newt. It's available in a public repository, but it's really not usable (the latest changes aren't even pushed yet). It's not even usable for me yet—this isn't a launch post, more shipping my design doc (and hoping someone will do my work for me so I don't have to finish newt :D). The goal for newt is to be able to create a new small web app and start on the actual project in minutes instead of days, bypassing the entire process of wiring things up. I think the list of must-haves and quality-of-life features above will be a start, but by no means everything we need. I'm not ready to accept contributions, but I hope to be there at some point. I think that Rust really needs this, and the whole ecosystem will benefit from it. A healthy ecosystem will have multiple such toolkits, and I hope to see others develop as well. * * * If you want to follow along with mine, though, feel free to subscribe to my RSS feed or newsletter, or follow me on Mastodon. I'll try to let people know in all those places when the toolkit is ready for people to try out. Or I'll do a post-mortem on it, if it ends up that I don't get far with it! Either way, this will be fun. 1 I do plan to build a few pieces from scratch for this, as the need arises. Some things will be easier that way, or fit more cohesively. Can't I have a little greenfield, as a treat? ↩ If this post was enjoyable or useful for you, please share it! If you have comments, questions, or feedback, you can email my personal email. To get new posts and support my work, subscribe to the newsletter. There is also an RSS feed. Want to become a better programmer? Join the Recurse Center! Want to hire great programmers? Hire via Recurse Center!",
    "commentLink": "https://news.ycombinator.com/item?id=41760421",
    "commentBody": "Rust needs a web framework (ntietz.com)239 points by dcminter 21 hours agohidepastfavorite345 comments paulgb 21 hours agoI was surprised not to see Rocket (https://rocket.rs/) mentioned among the frameworks the author listed; I haven’t used it myself because I _like_ the more unopinionated axum/actix-web, but as I understand it its goals are much more in the vein of what the author wants in a batteries-included framework and it’s been around for a while now. reply hobofan 11 hours agoparentWhile Rocket was one of the earliest Rust frameworks that really pushed for good UX (via proc-macros), I think it has largely fallen out of favor in many parts of the community. For a very long time it required nightly Rust versions to build, which is a no-go for many people wanting to use a framework in a production setting. Also for a period of ~2 years it was stuck in a 0.5.0 release candidate limbo where the latest stable released version was significantly lagging behind the rest of Rust web frameworks (requiring nightly; no async support yet). Maybe it will be better now after the 0.5 hump, but in general the development history of Rocket is something that dissuades me from using it when compared to e.g. Axum. reply capitainenemo 5 hours agorootparentWhen I asked them about that 2 year limbo, they said the release candidate was feature complete, and simply had incomplete documentation (also some plans for a new management structure). So, it wasn't actually lagging, it had that async for quite some time... But yeah, being in a \"release candidate\" did scare people off. reply appplication 4 hours agorootparentThat is a bit telling on how the maintainers view their users, i.e. unseriously. reply capitainenemo 4 hours agorootparentCan you elaborate? They seemed pretty responsive to me every time I asked for assistance in the channel... (we'd implemented something in 0.4). If \"incomplete documentation\" is \"unserious\" that would be the large majority of open source libraries. Actually blocking on completing it sounds like they were taking the release seriously to me. And I understand it taking a while. No one likes writing documentation. reply appplication 4 hours agorootparentI’m viewing it from the perspective of being a user who is blocked by a feature or bug fix that has been implemented for 2 years and not released. How do you trust the feature/bug fix cadence moving forward? It’s not like users can’t run release candidates in prod, but there are many reasons not to, and good policy guides against this. I used to be an ardent supporter of Uber’s h3 geospatial library. But we’re running into almost exactly the same issue. Bug impacting us we’re fixed in 4.0 and it’s been stuck in release candidate status for over 2 years. That’s not tenable as a user who is trying to get things done. So now we’re migrating off it entirely. reply capitainenemo 4 hours agorootparentWell. In this case it's almost a labeling thing. You could make an exception for this (maintained) release candidate if in your judgement incomplete documentation (basically rust doc stubs for a significant percentage of the libraries) was (not) a blocker. In some places we were told to never use libraries that weren't 1.0 - obviously that would be bonkers in the rust world. In these things you really have to use your own judgement... I don't feel like their decision to make documentation a release blocker shows disrespect for the users. They were repeatedly clear that it was safe to use and feature complete. They did note the shift from a single man project to a foundation governance took longer than expected and was part of the release as well, but, you know, non-code stuff like that also takes longer. reply wslh 20 hours agoparentprevI 100% agree. I'm just a casual multi-language programmer these days, but I'm familiar with Rocket. reply axegon_ 20 hours agoprevThere is one: Poem[1] The author mentions flask but looking at the \"What we need section\", I don't think flask covers those. I hate Djago with a passion but if those are the requirements, I think Django is the one that closely resembles what the author is describing. So Poem is not a good candidate either in that regard. Poem is all in all, something that closely resembles FastAPI, which is actually a complement. I've tried half a dozen rust web frameworks and they all come with a ton of boilerplate fiddling with the initial setup. Which is a problem if you want to get things done fast. In that regard, poem nails it. Yes, actix has a considerably better performance but unless you are aiming for sub-30 millisecond responses, then actix is not what you should be looking at to begin with. Also if you crave a Pydantic, there is a crate that sort of does that for you: https://crates.io/crates/poem-openapi [1] https://github.com/poem-web/poem reply giancarlostoro 19 hours agoparentNot just Django, probably Rails, ASP .NET Core, and many other full-feature, batteries included web frameworks. Emphasis on framework. reply dougthesnails 19 hours agoprevTake F# for a spin. It offers nearly all of the type system tomfoolery one craves and all the tooling and nice-to-haves one wants. The big drag is that the async story and memory management aren't as big as a time vampire as Rust, so you do run the risk of actually solving business needs. I am sure some part of dealing with Microsoft can fill that void, though. reply angra_mainyu 18 hours agoparentI've been on the fence w.r.t giving it a try. How does it compare to Ocaml and haskell? reply dougthesnails 1 hour agorootparentIt is Ocaml in all the ways that matter, but with .NET's async story/runtime/memory management, and tooling. reply consteval 4 hours agorootparentprevFrom a web point of view, you get full access to .NET, all the associated packages, and all the tooling. You start with a huge head start as opposed to Ocaml and haskell for that usecase. reply PedroBatista 19 hours agoprevPossible unpopular opinion: Rust is a systems language, doing \"web work\" in Rust is a waste of effort as there are much better languages and ecosystems for that. reply marcosdumay 5 hours agoparentI do almost completely agree. But I just want to point out that the intersection between system programming and web programming is not empty. Neither is the intersection between embedded programming and web programming. But either way, if something like Django, .Net, or go is an option, Rust is probably a bad idea. reply beeflet 19 hours agoparentprevIDK I use C/C++ all the time for web servers. It's lightweight and lets you pretty easily/quickly call C/C++ and low-level code and gives you a lot of control in terms of networking. You can pretty much \"do anything\" in a straightforward generic way without having to learn some application specific language like PHP or through nginx/apache configuration files. Having a rust alternative to something like libmicrohttpd would be nice and I would use it. reply Capricorn2481 15 hours agorootparent> You can pretty much \"do anything\" in a straightforward generic way without having to learn some application specific language like PHP or through nginx/apache configuration files Not having to learn something like PHP? Do we not count learning C++? I would guess most people would agree learning PHP + Nginx is going to be a lot less complicated, which will be preferable for people just trying to build something. I'm not sure dropping down to such a low level is something most web developers will have to deal with. I think that you find it easy speaks more to an already existing familiarity with C++, not that it would be a path of less resistance. reply Narishma 15 hours agorootparent> Do we not count learning C++? No, because they presumably already know it. They wouldn't be using it otherwise. reply tcfhgj 8 hours agoparentprevIt's an investment in energy efficiency reply jillesvangurp 7 hours agorootparentThe advantages of a language like Rust where most interactions are blocked on either user input or network IO are fairly limited. Rust shines when things are CPU or memory bound primarily. Web applications are mostly IO bound. Which is why people have been getting away with fairly poorly optimized interpreted languages for decades. Even when computers were a lot slower than they are today, this worked fairly well. reply binkethy 5 hours agorootparentStrict typing can enforce proper composition from models to the request responses. Rust is a high AND low level lego due to type composition and ability to directly address memory with no hacky garbage collection required. Garbage collection is a bad idea when your state machine is basically a stateless request response cycle that can fit in n stack frames and has a defined lifetime ending in the response. Interpreted languages calculate runtime code paths on an ad-hoc basis and are subject to bugs that a binary executable will never encounter. One can objectively state that Interpreted languages and their virtual machines are LESS DETERMINISTIC than binary executables whose runtime code already exists at compilation time. Of course, shared libs and other things that violate the spirit of the above can wreak havoc with even binaries, however the executable is a finite artifact. reply echelon 19 hours agoparentprevCounter-point: developing web services in Rust is just as easy as doing it in Go or Java, yet you now have an excellent type system (sum types!), an excellent package manager, and extremely good performance. You can do dependency injection in Rust to share connection pools just like you would in Java, and it's super simple to write threaded background tasks. Google gave a talk recently that said they measured Rust developers as no less efficient or productive than Go developers [1]. It's a fantastic language and you shouldn't ever limit yourself to systems programming with Rust. It can do so much more than that. [1] https://www.ardanlabs.com/news/2024/rust-at-google/ reply angra_mainyu 18 hours agorootparentThe developers they measured, were they using the languages for the exact same tasks? Otherwise it's just stating that they assign the languages to categories of problems they are equally efficient in. reply echelon 17 hours agorootparenthttps://www.youtube.com/watch?v=QrrH2lcl9ew > \"We have at this point rewritten a large number of systems [..] have some very concrete things that we can say\" > \"When we've rewritten systems from Go into Rust, we've found that it takes about the same sized team about the same amount time to build it. [...] no loss of productivity when moving from Go to Rust.\" > \"We do see some benefits [...] we see reduced memory usage in the services that we've moved from Go. [...] We see a decreased defect rate over time in those services that have been rewritten in Rust\" > \"Within two months about a third of the folks were feeling as productive, within four months about 50%.\" > \"People do indeed feel as productive in Rust as they do in their original language. C++, Java, Python, Go.\" > \"One of the biggest latencies is code review time [...] How hard is it to review code in Rust? [...] A little over half said that Rust is easier to review. The most incredible question [...] The confidence that people have in the correctness of the Rust code that they're looking at. [...] 85% of people believe that their Rust code is more likely to be correct.\" reply discreteevent 5 hours agorootparent> \"When we've rewritten systems from Go into Rust, we've found that it takes about the same sized team about the same amount time to build it.\" A rewrite should be much faster than the original. Much of the original development will have had changes in requirements, refactors because of better understanding the problem domain, best structure for the code etc. A lot of that can be just 'copied' into the new system. The developers know exactly what to do. To complete this experiment they should have also re-written it in C# or java and then compare with rust. reply echelon 45 minutes agorootparent> Much of the original development will have had changes in requirements, refactors because of better understanding the problem domain, best structure for the code etc. Having written services in the Google style for years at a major fintech surrounded by Xoogler peers, systems design starts with a design document. You capture requirements upfront and solicit buy-in from the stakeholders. It's nothing at all like what you describe. You design the API and systems upfront without code (apart from capacity testing) and actual implementation happens quickly. If iteration time was some \"gotcha\", Google would have made that footnote. That's not really how service development works in companies of this scale. Translation time from design document to service is the same for Golang and Rust. reply pjmlp 13 hours agorootparentprevAnyone on JVM has already Kotlin or Scala for that. Likewise on CLR with F#. reply dakom 7 hours agoprevThis article makes several different points that would ideally each be tackled on their own. You don't need a router when you have pattern matching (just split the url and match on static and dynamic vars however you need) Auth is typically DIY in any language, or SaaS like Firebase/Auth0. It's not a language or framework problem, necessarily CSS/JS tooling makes no sense for many frontend Rust frameworks like Dominator, which is in Rust (not JS) and has its own approach to styling that works for it (e.g. attaching styles to signals, so they change on the fly) I get what the author is saying - in fact I've been around the block a couple times solving all the different points in the article and it is painful. For example, see https://github.com/dakom/dominator-workers-fluent-auth which does a lot of the stuff here on the Cloudflare workers framework (also adds another wishlist item - localization with Fluent) A \"batteries included\" full framework that does _everything_ is nice to have, but many real-world projects will want to replace those opinionated decisions and go the DIY route anyway. Rust is more than mature enough, even on the frontend web, to tackle all of that - if you know where to look (wasm-bindgen, etc.) reply isosphere 8 hours agoprevDoing my web stuff in Rust was fine but concurrency was a pain. A crate that abstracts web workers with transferrable types would help. After that you have to pick a component library of which there are few, and all are experimental. Making pretty, performant things is hard. Switching compontent libraries isn't easy. The backend stuff is a breeze, I think Rust is doing fine there. I'm switching to Flutter for my UX needs. There's flutter-rust-bridge that binds Rust code concurrently without any headaches in the client, and I can deploy to the web, android, Linux, etc. with ease. Looks p. good out of the box. Got GRPC working quickly, am happy. Using Rust in the client is nice because I have a single workspace that contains all of my business logic. Backend changes will cause errors in the client code if changes are necessary. The codebase is easy to refactor and trust. Dart ain't half bad. I stayed away from Flutter at first because it doesn't respect the DOM etc but at this point I'm willing to sell my soul to the devil for how easy it is to make a great UX that deploys anywhere. I don't love relying on something Google made, though. Feels a little like building a foundation on sand. reply chrismorgan 5 hours agoparent> I stayed away from Flutter at first because it doesn't respect the DOM etc but at this point I'm willing to sell my soul to the devil for how easy it is to make a great UX that deploys anywhere. Great UX, you say? Great UX!? Flutter’s unashamed pure-canvas approach makes it fundamentally and unfixably awful. Links don’t work because they’re fake (and it’s impossible to fake them), scrolling is atrocious for a significant fraction of users, text handling is obnoxious and wrong… seriously, speaking as a user, I’ve come across things made with Flutter three times in the last four years (plus looking at their demos), and Flutter makes for literally the worst user experience that I have come across in that time. Now Flutter does actually have a DOM renderer, but it never seems to be used. I’m not entirely sure why. Actually, scratch that, I just went to find a link, and found https://docs.flutter.dev/platform-integration/web/renderers only talking about canvaskit and skwasm, no mention of the html renderer any more… it seems like they deprecated it earlier this year , and will presumably remove it some time soon, doubling down on pure-canvas. Well, at least that lets me more unconditionally condemn Flutter for the web. Text is bad, especially if you use emoji. Links are bad because they’re fake. Scrolling is bad for a decent fraction of users. And none of these three are fixable if you insist on pure-canvas. (Links you could kinda make work tolerably if you were willing to compromise a little, but the other two just can’t be done.) If you want more substance to my complaints about the pure-canvas approach, search HN comments for “chrismorgan canvas” or similar. I still haven’t finished reducing it to an article on my website. If you intend to target the web and care about the web at all, please don’t use Flutter. reply giancarlostoro 19 hours agoprevPython has Django, C# has ASP .NET Core, Ruby has Rails, and so on... If you want to really make Rust take off with web development, build up a full-feature solution. Rocket is maybe the closest? But it's not as batteries included as any of the frameworks I've mentioned. reply ramon156 10 hours agoprevNo mention of loco-rs? poem? rocket? There's already a lot of projects that aim for a batteries-included experience. What sets these projects apart from frameworks like leptos is that there's a CLI guiding you. That's what I love about Symfony, and what I consider the 'lazy' part. reply mifydev 20 hours agoprevThere is https://shuttle.rs. Not only a framework but a tool that can generate all the web boilerplate around it, sounds lazy enough for me! reply godisdad 21 hours agoprevIf the post were C++ focused instead of Rust would it make the same, more or less sense? A systems language with a high cognitive barrier to entry, compile times and less than a decade of wide adoption can’t reasonably be expected to compete with something like Rails in terms of approachability reply delduca 20 hours agoprevWhat make rust better than any language when the bottleneck is the network or the database? reply drpossum 19 hours agoparentNot every application or even web application bottlenecks at the network or database level. reply Capricorn2481 15 hours agorootparentDoesn't really answer the question. Most webapps are this way and people are still surprisingly unaware of this. reply drpossum 9 hours agorootparentIt is a refutation of the premise which invalidates this particular question as it was posed. reply slowmovintarget 5 hours agorootparent\"Not every\" is not the same as \"none\" and the discussion is about the majority, so I think your refutation is only of the absolute case and not of the original premise. Rust would be overkill for most web applications. reply dajonker 13 hours agoparentprevFor me, the type system and the performance without having to do any optimization. The complete lack of runtime errors when your code compiles (*if you choose to write code that way) reply tcfhgj 7 hours agoparentprevThe bottle neck depends on the amount of CPU resources you throw at something reply yakshaving_jgt 20 hours agoparentprevThe bottleneck is mostly developer hours. reply aabhay 20 hours agoprevRust has some great and useful web frameworks that are a joy to use, once you understand what is going on. For example, in Axum, they use traits cleverly to allow you to use dependency injection the same way that fastapi uses it. But at least when I started using it, that wasn’t an insight made bluntly clear with tutorials as good as tiangolo’s. Instead, I had to piece it together via examples in the axum repo as well as from watching a youtube video. Don’t fear, Rust can be as simple and clever as any other language. And the Rust core OSS developer community has spent at least the last ten minor versions improving dev ex considerably. reply skeptrune 19 hours agoparent>once you understand what is going on I think this is the big issue. We as members of the Rust community should be doing more to explain all the patterns for webservers in particular. Support is there, but it's non-obvious. reply CoconutMilk 20 hours agoparentprevCurious which YouTube video you watched that helped learn Axum. reply aabhay 19 hours agorootparenthttps://youtu.be/Wnb_n5YktO8?si=lCGrH36DvXATdyEs reply palmfacehn 9 hours agoprevI wouldn't mind a Rust based web framework that provides the same features as the base Servlet API. Everything else should be an optional library. When I look into it the rabbit hole goes too deep. It hasn't been immediately clear what the templating looks like. Usually they start talking about some \"Reactive\" framework, features I don't want and I lose interest. Afterwards, I realize that I'm already satisfied with my current web stack. Jetty, HAProxy, JSTL, commons db connection pooling and Postgres. reply masklinn 9 hours agoparentIt’s really not clear what you mean by “basic servlet API”, because the main rust web frameworks are pretty much just routing by default, with (largely opt-in) parsing and serialisation support. It “hasn’t been clear what the templating looks” like because there isn’t any just as AFAIK servlets provide no templating. reply stefs 9 hours agoparentprevjust out of curiosity: why commons db connection pooling instead of hikari? i've only ever used the latter. reply palmfacehn 4 hours agorootparentI've been using version two for a long time now. It just works. I hadn't heard of Hikari until now. reply nfrankel 8 hours agoprevBasically, the author wants Spring Boot for Rust reply creshal 7 hours agoparentSpring Boot, Django, Rails, Laravel, ASP, ... it's not unreasonable to expect Rust to have an equivalent to something that most popular languages have. reply dartos 21 hours agoprevIsn’t there a rust rails clone? Yeah, it’s called loco https://loco.rs/ FWIW I prefer the pile of libraries. Big frameworks are good for scrappy startups trying to push their product out asap, but in most of situations, I’d like a lower abstraction system to build on. reply hermanradtke 21 hours agoparentRails clones in modern languages don’t get wide adoption. I wrote about why I think that is: https://hermanradtke.com/why-your-rails-like-framework-is-no... reply paradox460 21 hours agorootparentFor what it's worth, Phoenix is quite rails like, as is the ash framework, and both are quietly popular reply dajonker 13 hours agoparentprevWhat makes Rails (and similar frameworks in other languages) great is the large number of companies using it in production and having many people contribute to it. It therefore is very feature complete and works for a lot of use cases and edge cases. For everything that Rails does not do out of the box, there is a great ecosystem of libraries. Rust has no such framework. There is often just one person pulling the whole project and the number of active contributors is very small. This carries some risks and might mean that you end up having to do a lot of plumbing yourself. reply erlend_sh 18 hours agoparentprevThis needs to be higher up. Loco seems to be exactly what OP is looking for, yet it wasn’t mentioned in the article. reply GolDDranks 14 hours agorootparentMy first thoughts. Before writing the piece, maybe they should look into what already exists. reply perrohunter 20 hours agoprevThe http server in the std library is one of the things Go got right and improved it's adoption, rust needs something similar reply josefrichter 7 hours agoprevI think I've seen this exact discussion in Clojure community. reply sublinear 18 hours agoprevI guess I'm just \"weird\" for thinking the laziest and least error prone way to write a web app is in plain javascript? Any framework is too much extra work to learn and going to eventually get you shot in the foot. reply akaike 17 hours agoparentLeast error-prone and JavaScript in one sentence, I’m not sure about that, to be honest. JavaScript alone without any types in a big project is a shot in the foot, let alone the quirks JavaScript has. Apps crash when a thrown error is unhandled, etc. I would say JavaScript is easy, but it’s definitely not the type for a least error-prone app. reply wiseowise 12 hours agorootparentSlap TypeScript on it. reply bigiain 16 hours agoparentprevI think there's a scale/complexity thing to be considered. If your web app is simple, then frameworks and \"safe\" languages are unnecessary. At some stage though, that flips around. There's no way I'd choose to write, say, gmail or Trello in \"plain javascript\". The learning curve of a suitable framework for projects like that are well worth the eventual productivity, and the number of feet to shoot in your project will way exceed the number of feet added by the framework, and hopefully the framework will prevent way more foot shooting opportunities than it adds. reply djbusby 19 hours agoprevServo is in Rust. I want to make an app in Rust that hosts Servo in it with my business logic in maybe SciterJS - like electron but hopefully lighter? Something, just a business software guy who wants something with more control than an webapp and not so heavy like electron. reply htdar 19 hours agoparentTauri? https://tauri.app reply djbusby 17 hours agorootparentIts pretty awesome and almost there. Actually, I'm sure if we could find the time it would be there. (We $DAYJOB) reply jmartin2683 8 hours agoprevRust and ‘minimal effort’ have to business being in the same room. reply Hackbraten 8 hours agoparentpanic: something something upper or lower boundary of that \"'minimal\" lifetime you tried to use ^^^^^^^^^^ expected lifetime>>>, found Ref>>> reply yarg 19 hours agoprevI consider myself a lazy developer: I like to write code that minimises the amount of code I'll need to write to incorporate new features in the future. Laziness isn't necessarily a bad thing, depending on how it's implemented. reply fbn79 11 hours agoprevIt is always a good idea to write a library, not so for frameworks https://www.brandons.me/blog/libraries-not-frameworks reply pkstn 12 hours agoprevhttps://deno.com/ reply kuon 19 hours agoprevJust use Phoenix and live view. Prototype is easy and fast. Scales well to very large user base on a single node. Interactive client side without even writing JS. I love rust, I really do, I use it for all kind of things. But for web app, the Erlang architecture is so well designed and mature you cannot compete. Also we use rust and zig from Erlang with native modules for video processing and AI, the elixir/Erlang \"skin\" gives you immense flexibility and productivity with almost no performance hit. reply satvikpendem 13 hours agoparentNo types in Elixir, I think a better comparison is actually Gleam, it is Rust-like in type system but is built on and interoperable with Erlang/OTP. reply neonsunset 7 hours agorootparentBoth are exceedingly slow and memory-hungry. The average user of Rust might get shocked and never touch these again. Now F# on the other hand… reply snappr021 9 hours agoprevSomething like https://sinatrarb.com/ for rust would be cool. reply javierhonduco 9 hours agoparentAxum + minijinja is quite close to this I would say. Been using it for a little while and I am very happy so far. reply aquariusDue 8 hours agorootparentSeconding the recommendation, it's really great when paired with HTMX on the frontend too. reply datadeft 9 hours agoprevPlease no. Frameworks are the breeding ground of accidental complexity. Rust already has excellent libraries that are easy to use in many different stacks. reply BikeShuester 21 hours agoprevI'd rather see Crystal on Rails. Combines the elegance of Ruby with the performance of a compiled language. And of course: macro support. reply binkethy 5 hours agoparentAmber or Lucky, already well established... reply the__alchemist 20 hours agoprev100%! I love Rust, but I will still use Python for backends because of Django. Rocket, Actix, Axum etc are more like Flask, but without Flask's integrations for the services. I don't understand why there are so many competing microframeworks; one of them should have IMO pivoted to a Django-like a while ago. reply satvikpendem 13 hours agoparentLoco.rs? reply elAhmo 20 hours agoprevRust is not for lazy developers and there are other languages if you want to put minimal effort in building things. reply p1necone 20 hours agoparentI'm a lazy developer and I love languages with an \"if it compiles it works\" feel. I'd much rather hack around in rust or typescript with half my brain switched off and a beer in hand and eventually get working software than hack around in javascript with half my brain switched off and a beer in hand and get more and more broken software until I eventually give up. reply throw10920 18 hours agorootparentI don't think the dichotomy is between types and not-types - it's between Rust and other typed languages. Most people here appear to agree that types are useful, but some people are claiming that a language that forces you to do memory management is somehow as ergonomic/lazy as others that manage memory for you (with equivalently good type systems - TypeScript, ~Python). reply RandomThoughts3 20 hours agorootparentprevThen just use Ocaml or F#. You are welcome. reply 6gvONxR4sf7o 15 hours agorootparentFor a person just curious about all three, how do the ecosystems compare? reply 27theo 20 hours agorootparentprevTypeScript sure, but Rust? Half your brain switched off? There is a middle ground between unfettered madness and the Rust compiler's strict regime. Maybe try Elm. reply Philpax 10 hours agorootparentYes, really. I've found Rust to be the easiest language to use while cognitively impaired (i.e. drunk) because the compiler can maintain all of the invariants that my conscious mind would need to manually maintain in any other* language. *: There are simpler FP languages that can offer a similar experience, but they come with their drawbacks: I don't think I'd want to set up OCaml on Windows under such a state reply bigiain 16 hours agoparentprevLarry Wall (the Perl guy) had \"The three virtues of a great programmer\", in which the \"Laziness\" virtue wasn't putting minima Framework is great and good for stability or something, but using a framework makes something that I enjoy (programming) feel more like filing out my taxes, just so I can end up with a mediocre CRUD application that more or less works for the project I am working on. I have removed myself to from the web and programming almost instantly became more fun for me again. reply sodapopcan 17 hours agoprev\"Right tool for the right job,\" no? Learn multiple languages. The whole \"I want to do everything in the language I already know\" is why the least interesting--and straight-up pretty terrible albeit very capable--languages are dominating our field. reply bigiain 17 hours agoparentBut surely \"making silly things\" in Rust counts as learning Rust? I remember writing a 2.5D \"Doom style\" renderer thing that let you move around in a bunch of rooms/corridors in real time in MacPerl/Quickdraw back in the Mac OS9 and Perl4 days. Probably taught me more about Perl than I'd learn in several years doing it for a job. reply sodapopcan 4 hours agorootparentFor sure, but the article is from the perspective of knowing Rust and wanting to work around, well, Rust. I'm saying if that's what you want, use a language suited for that. Variety is a good thing. reply wiseowise 12 hours agoparentprev> Learn multiple languages Not everyone wants to be jack of all trades - master of none or have time, energy or capacity to waste their life in hamster wheel of constant relearning of the same thing. > is why the least interesting--and straight-up pretty terrible albeit very capable--languages are dominating our field. If they’re dominating our field they can’t be terrible by definition. reply pjmlp 13 hours agoprevLanguages with automatic resource management are much better option for distributed computing scenarios that one with strong focus on affine types. reply skor 9 hours agoprevfor those folks suggesting php/laravel, give cakephp a try. I've used both extensively: years and large, complex projects. Cake is just much faster to work in. In the initial phase and the long run. reply binkethy 5 hours agoprevWhat about loco.rs? reply tonyhart7 12 hours agoprevin case someone really need it like in the article https://github.com/spring-rs/spring-rs reply dajonker 7 hours agoprevI think dhh said, great frameworks/libraries are extracted, not created from scratch. That would mean, someone building a (successful, money making) web application in Rust and building their own framework as they go, open sources that framework and then continues using it for their production applications. I have not seen such a success story for any Rust web framework yet. Although I would love to see one (or more!) reply oaiey 7 hours agoparentIMHO, there are three scenarios: - Regular Web / Services: IMHO, Go, Java, .NET are better candidates there - IoT devices with Web or API interfaces: Rust is good there. Access to I/O resources etc. - Extreme high-performance services: Something like a DNS-over-HTTP or wall street data brokers. But for that you need a VERY solid server (but most likely not a framwework). The regular web case will not produce a framework (out of commercial success), it is much easier to opt for a traditional languages/frameworks and hire inexpensive developers for that. The extreme high-performance services will not produce the framework you want and the likelihood is very small because the number of companies doing that is very low. Leaves the IoT space, which, IMHO, has the best chances BUT will not be the one optimized for your big server machine scenario. reply henning 21 hours agoprevWhatever you do, don't make this an RFC in a Rust repo/something up for debate and discussion. 500+ messages and literally years later, no one will agree on a single thing. Just make the thing you want and if people don't like it, they can stick with Actix. reply jiggawatts 12 hours agoprevHe’s looking for the Rust equivalent of ASP.NET Core and there’s nothing wrong with that. Sure, some people like choices and tinkering, but in many settings it’s much more productive to have the choices curated for you. “But what if you want a different X instead? It might be Y% better!” is the typical comment, which ignores that the integration itself provides enormous value. Discrete libraries may be best of breed individually, but may be a heap of garbage if they don’t fit together smoothly. I love programming in ASP.NET 8 specifically because I never have to think about whether a templating system will play nice with authentication, injection, routing, or anything else. reply rob 20 hours agoprevPHP's Laravel can do everything in that list. I'd just stick with that and focus on whatever you're trying to do, especially as a lazy developer. reply jokethrowaway 8 hours agoprevThat's interesting! I absolutely don't feel the need for a massive framework. I think a good part of the devs community has been burned by some framework at some point and doesn't feel the need for web frameworks, hence why nobody bothered to make Rails for rust. In a professional setting, all the services I interact with, have been implemented on top of express (node.js) or fast-api (python). I know people working in ecommerce who rely heavily on django or woocommerce, but rust usage among them is pretty low. Rust is also one language where there are very little junior people, so you'll see wire-it-yourself library more than frameworks. Eg. sqlx is imho the best compromise in the ORM vs query builder vs plain sql debate: you get typechecked plain sql. I think axum is very well architected and type checked routes offer a great DX (except for the tower::Service compatibility - sure, it's nice to have compatibility but the resulting code is quite clunky), and I'd rather have smaller components I understand over a huge black box. reply stonethrowaway 19 hours agoprevThe author is I think conflating laziness for joy. We don’t care to be lazy because that won’t do it. But we do want to have a language that we like to use. To look at, to play with, to understand and to rally behind. There is no one web framework that will satisfy all criteria and all layers of what a web framework needs. Every so often we have to migrate as technology catches up and changes the ecosystem, like wasm did with Blazor. I’d rather have a language that programming is a joy in. The laziness is a nice side benefit. I can adapt such a language to changing landscape any day. I know Ruby is such a language, and hence Rails adopts it’s joyful mentality. But is Rust a joy to program for? That I can’t say for sure. reply skeptrune 19 hours agoprevIt really doesn't, actix is already there. Recently built my startup code over the past couple years with Actix and it had literally everything we needed easily. We just need more documentation and reference code bases demonstrating how to do these things. Rust devs tend to be fairly advanced and seemingly don't write enough docs or shares. >Routing/handlers Actix has it. >Templates Minijinja or liquid-rust[1] >Static file serving actix-files[2] >Logins Actix with oidccore is fantastic and easy[3] >Permissions Actix FromRequest is literally perfect. We have perm levels (admin, owner) and per-route perms for more fine-grained control.[4] > Database interface Diesel with diesel_async for connection pooling has been flawless at scale. > Admin tooling We didn't do this, but it would be simple with a bin/load-data.rs file that runs via a docker start command or tmuxp pane. > Hot reloading Cargo watch is getting deprecated, but was great. Bacon and and Watchexec are fully qualified successors. CSS watch systems work with the templates already same way as they do for SPAs. > Background tasks Make a bin/worker.ts file which defines a single binary and then use redis or another queing sytem to communicate between the worker and core server. We loaded all of HN (40M docs) into a search index with this approach and it was easy. > Monitoring/observability There's a decent story here with structured logging. Zero to production in Rust has a good chapter on it (2020) [5]. Lots of progress has been made since and exposing prom metrics endpoint is straightforward [6]. Sentry support is also decent and they sponsor actix. > Caching Imo, the story with Redis + actix is fine. We do this with our auth middleware to reduce latency on that operation since it happens with every route and I think it's smooth with FromRequest. > Emails and other notifications What's wrong with SMTP? Plus, Keycloak will handle your password reset infra out of the box. SDKs here could be better, but the APIs are fine and not too bad to do with Reqwest yourself. > Deployment tooling + CSS/JS bundling Imo, both of these things are the same as with other languages. Rust could use more documentation, but I don't think there's anything making it particularly hard to use. [1]: https://github.com/devflowinc/hn-search-RAG/blob/main/actix-... [2]: https://github.com/devflowinc/hn-search-RAG/blob/main/actix-... [3]: https://github.com/devflowinc/trieve/blob/main/server/src/ha... [4]: https://github.com/devflowinc/trieve/blob/main/server/src/ha... [5]: https://www.lpalmieri.com/posts/2020-09-27-zero-to-productio... [6]: https://romankudryashov.com/blog/2021/11/monitoring-rust-web... reply Philpax 10 hours agoparentHer point is that she wants a batteries-included framework that comes with integrated choices for these out of the box, similar to Rails or Phoenix or Laravel, so that you can hit the ground running without spending the first few weeks of the project picking crates and gluing them together. reply Barrin92 20 hours agoprevIf you want to make a silly, minimal effort hobby project simply don't use Rust. I'm gonna be honest I don't understand this entire genre of using extremely complex, highly optimized systems languages for tools that don't need them. Your flow chart should basically go like this: \"Do I need zero cost abstractions because I'm writing a computationally expensive very serious project?\" If the answer is no use a garbage collected, runtime managed language. reply kelnos 17 hours agoparentI write my hobby projects because writing them is fun. I don't find writing Ruby, Python, Go, and a host of other so-called \"more appropriate\" languages fun. If someone enjoys writing Rust, it's natural that they'll reach for it when they're looking to write something for fun. If the tools/libraries in that language exist to make writing a particular type of thing more fun or easier, then more the better. reply J_Shelby_J 17 hours agoparentprev> minimal effort hobby project Sorry, just spent 6 hours figuring setting up my python environment. Ready to start my hobby project now! reply keybored 4 hours agoparentprevNo the flowchart is more like this 1. Do I like the language enough to use it for fun? (no: go to (2)) 2. Am I neutral towards the language? (no: go to (3)) 3. Am I fine with other people using it for fun? (no: go to (4)) 4. Feel free to harangue people who explicitly say they are using it for fun if their reasons for using it is not a 100% utilitarian (which is very likely to be the case: refer to the for fun section) reply kstrauser 20 hours agoparentprevI don’t know about that. I already knew a little Rust last month when I wanted to write a REST API server that would benefit from being deployable as a single executable. I probably spent a grand total of 2-3 hours researching which framework to use (actix-web for me), reading the code, and implementing the endpoints I needed. The end result ended up looking an awful lot like a Python Flask app. The actual code writing part of the project wasn’t any more complicated than using any other language might’ve been. Learning Rust and learning web service programming at the same time would have a steep curve. If you already know the basics of Rust and have written web services before, writing a new one in Rust needn’t be stressful. reply monlockandkey 10 hours agoprevNo harm if all these requirements are met by some framework to cater to the Rust community. In my opinion you should for 99% of cases use Golang for your web backend. Any other languages there are tradeoffs you are making. Go: - very easy to learn and grok Go code - static typing - fast compilation - single binary (easier deployment) - strong standard library - large library ecosystem - go routines for concurrency - highly performant Maybe Java,Kotlin and C# but they are still an order of magnitude more complex and resource heavy than Go. reply neonsunset 6 hours agoparentGo is stuck in the eighties and replies like this are a good demonstration why it's very difficult to engage in intelligent discussion with many from Go communities. As someone who mainly specializes in .NET, I have had incomparably better time participating in Java and C++ ones because people there are usually able to acknowledge pros and cons of various platforms, how they evolved and where are their strengths and weaknesses. The average level of understanding and ability to consider what is the dev flow and how the language of choice impacts it in Go ones seems to be just so much worse it's not even funny. That is to say, goroutines are discount futures/tasks which force you into synchronizing the \"yielding of result\" manually either via a channel or a waitgroup and a collection, or similar. Not to mention they are also much more expensive than .NET Tasks. I have not measured the cost of Java's new green threads yet but assume they are going to be in the same ballpark of memory cost as Goroutines, but with drastically better steady state performance provided by OpenJDK HotSpot when it comes to regular application code. And lastly - Go requires you writing heaps of boilerplate for simplest things, Go channels come with a lot of footguns and gotchas you have to learn, standard library has weird omissions, type system is static but weak and as the demands put onto Go continue to become more complex, as more and more developers are forced into it, the language becomes the kind of unreadable soup you accuse other languages of. Just look at range over funcs and iterators discussions recently. It's ugly and token-heavy. And you will see a lot of code like this if you browse random libraries on Github - it's unnecessarily bulky, in a way that is excusable for true system programming languages but not in Go which has even higher level of abstraction runtime than .NET. reply myst 12 hours agoprev> Rust needs a web framework for lazy developers Write one? reply Philpax 10 hours agoparentYou'll never believe what the end of the post says. reply pmccarren 20 hours agoprevs/lazy/efficient/g :) reply yakshaving_jgt 20 hours agoprevYesod has most of the things on this wishlist (though it's Haskell, not Rust), and Yesod is generally awesome. I've been working with it for the past several years, and I look forward to the next several years with it. reply tightbookkeeper 20 hours agoprev“Slow” scripting languages worked in the 90s for the web and they work even better for now, plus you have more to choose from. The key is to shell out to other tools for heavy lifting, like a database. Multi process architectures are where Linux really shines. Rust can fit in that picture, but it doesn’t need to be routing http. reply jknoepfler 20 hours agoprevI wish the author success in their endeavor, but Rust is pretty far down the stack of languages I'd use to deliver a webserver. I look at Rust for serving web-traffic and I see: dreadful concurrency model (I will never voluntarily go back to async/await after working in Golang), weak client library stories (if I'm writing a service layer for a db, etc.), high barrier to entry, thin overlap with the core Rust value proposition (correctness around memory access, performance). That's not even addressing the \"what happens when my entry-level dev has to write something that interops with a web framework written in Rust.\" My heart can't take those code reviews, I might as well just write that shit myself without a framework for all the pain that's going to cause. Note: I don't write a ton of Rust, for reasons that are maybe kind of obvious. I reach for it whenever C seems correct, which is rare but not never (for me). reply skeptrune 19 hours agoparent> dreadful concurrency model (I will never voluntarily go back to async/await after working in Golang), Rust supports golang style message-passing concurrency if you want it[1]. I'd argue Rust mpsc channels are actually more powerful than Golang's and add richness to message-passing concurrency modeling. [1]: https://doc.rust-lang.org/book/ch16-02-message-passing.html reply kelnos 17 hours agorootparentYeah, I don't get the hate here. Rust's channels are just as powerful as Go's, and there are even implementations of them in the futures crate that implement the Future trait, so you can await on them and effectively get the same behavior as \"blocking\" on a channel in a goroutine, without having a bunch of blocking native threads. reply K0nserv 4 hours agoparentprev> dreadful concurrency model (I will never voluntarily go back to async/await after working in Golang) With the exception of having to write out `async`/`await` the dominant concurrency model in Rust is mostly the same as Go(work stealing CSP). Sure Rust's memory model makes it a bit more cumbersome than Go. Rust's Send/Sync bounds might seem complicated, but those concerns are equally relevant in Go, the compiler just doesn't help you. reply LAC-Tech 16 hours agoprevI'm struggling to see a usecase for rust. If I want to do something as high level as a web app, GC is fine. If I want to do something at the systems level, Zig is much more at home there. And development in rust is glacial. It's no one language featurep (not complaining about the borrow checker in particular), but the accumulative overhead of all of them acting in weird ways. I will say the tooling is absolutely amazing, it's nice to span \"app level\" and \"systems level\" concerns in a single language. But it is a lot of language. reply devjab 21 hours agoprev> I like to make silly things, and I also like to put in minimal effort for those silly things. I also like to make things in Rust… I think this part is perhaps the silliest part of a very silly article. If you really like to put in a minimal effort then why on earth would you use Rust? If you want efficiency, memory management and a compiled modern language just use Go. Then you won’t even need anything but the standard library for what you want to do. Or… use Django as you suggest? Yes, yes we use Rust in production because we thought it would be easier (well safer) to teach to interpreted language OOP developers than c/c++ but why on earth would you ever use it for the web unless you are a member of the cult? reply troad 18 hours agoparentI am a fan of Rust for systems programming, but so many people are using Rust for things that are nothing even remotely close to systems programming. So many projects you see being written (or rewritten) in Rust are projects where I just think 'wait, why can't this just have a GC'? And then you look at the code base, and it's all Arc>s, and you can't help at marvel at this, since that's just GC, so what's the benefit of Rust here, exactly? (Don't @ me, ARC is GC.) I think a lot of what people actually like about Rust, to be honest, is that it's essentially an ML language masquerading as a C-like, with a stellar packaging and tooling story. What people fall in love with is the rigorous static typing, the Option monad, the exhaustive enums (which are just sum types in disguise), the traits (type classes in disguise), the borrow checker (a half-way house to immutability) etc. People will brag about Rust's memory safety, and then you find out that they don't really know how lifetimes work, they just .clone() or Arc> everything. Which is totally valid, but Rust is hardly necessary for that. Rust is great, but - heretic idea - not literally everything needs to be in Rust. Go try Ruby on Rails for a while! Or - if Rust has whetted your appetite for more functional styles - Phoenix on Elixir! Yes, it's not quite so blazing fast, but - let's be honest here - you're not going to be getting billions of requests per second on your hobby website where you write about taking apart Amigas. It's OK! She'll be right. reply saghm 15 hours agorootparent> What people fall in love with is the rigorous static typing, the Option monad, the exhaustive enums (which are just sum types in disguise), the traits (type classes in disguise), the borrow checker (a half-way house to immutability) etc. I feel like I say this every time this sort of discussion comes up, but I still think that there's a space for a higher-level language with most of what people like from Rust that has a (tracing) garbage collector and is slightly more relaxed with trying to design a way around every marginal heap allocation. Most of the time I bring this up, someone will suggest something like Swift or OCaml, but I think the part people miss is how even despite all of the complexity that comes with being a systems programming language, Rust really goes out of its way to try to be developer friendly despite that. Yes, it's a meme that Rust programmers are zealous evangelists and want to rewrite the world in it (which is a bit of an exaggeration in terms of lumping all Rust enthusiasts into that group, but there's certainly an element of truth in it), but no one seems to talk about how _weird_ of an idea it is for a language notorious for having a terrible learning curve to be so popular with people perceived to be lacking real-world experience with the domain. How did a language that's supposed to be so hard get popular to the point where people view its fans as pushing it aggressively? You might chalk some of it up to marketing, but I think people undervalue how much effort is put into the little details in Rust to try to make it more approachable, like error messages, standard library documentation, first-class support for all major platforms, and high-quality introductory materials (e.g. both the original and rewritten The Rust Programming Language book, Rust by Example, Rustlings). I don't think the same experience is there if you want to use Swift on Linux (where the support isn't nearly as strong, and a lot of the fancy new things coming out won't be available) or OCaml (from googling right now, a debate on reddit about \"which stdlib should I use as a beginner\" is on the first page of results when I search \"ocaml std\" or \"ocaml stdlib\"). reply fweimer 8 hours agorootparentOne issue is that with GC, you lose prompt finalization of resources. A lot of code is written with this assumption in mind (e.g., file buffers are flushed if the last reference to the file is dropped—which is arguable incorrect due to the lack of error checking). And the borrow checker is the only thing that keeps everything from being mutable in place in Rust today. Having GC would open the possibility for alternatives to the borrow checker without compromising memory safety, but even Pony-style reference capabilities probably won't lead to a language where abstractions compose much more easily than in Rust today. Maybe a language with a similar syntax, traits, monomorphization, and macros would still be interesting to many people? Would people prefer traits and macros over ad-hoc polymorphism (in the style of C++, which could subsume the macro use cases, too)? reply pjmlp 6 hours agorootparentAn issue only in some GC languages that don't provide the constructors for deterministic resource handling. Unfortunately people keep placing all GC languages on the same basket. And before anyone mentions that it is easy to forget, well those languages have their own \"Clippy\" to take care of it. reply fweimer 4 hours agorootparentI don't think constructors are the challenging part. It's about lexically scoped destruction. Certainly there are languages that have that and permit garbage collection, however those constructed values are necessarily second-class citizens and behave somewhat differently than ordinary values. There's probably some reasonable middle-ground, like constructors returning an owned reference that explicitly can be turned into an unowned reference, thus opting out of deterministic destruction. reply pjmlp 4 hours agorootparentAnd those languages do offer lexical scoped destruction. They are only second class in the context people put all GC languages on the same basket, and rather go for the X rewritten in Y blog posts. Because apparently adding newer languages to CV is cooler than mastering the one they have. reply j-krieger 10 hours agorootparentprev> but I still think that there's a space for a higher-level language with most of what people like from Rust that has a (tracing) garbage collector and is slightly more relaxed with trying to design a way around every marginal heap allocation I dream about a Rust subset that's exactly that. What would be even better is if you could just use Rust packages directly with it. Since these libraries already do compile, correctness has been verified. reply the_gipsy 9 hours agorootparentprevThere definitely is some void! I gladly take the whole rust \"package\" because overall it's just so good, at least for opensource / hobby stuff. I wish there was an equivalent but with GC, to use for work. reply Galanwe 9 hours agorootparentprev> How did a language that's supposed to be so hard get popular to the point where people view its fans as pushing it aggressively? I think Rust is especially popular with a demographic that was not previously exposed to lower level programming. Meaning younger programmers (because modern languages are higher level) and web centric programmers (because we're in a boom of web development). This demographic had a hard time entering systems programming, because while fascinating, its exposure is lower (less jobs, less projects), and entry cost (learning C, or C++ How did a language that's supposed to be so hard get popular to the point where people view its fans as pushing it aggressively? Popular languages don't really have evangelism or fans pushing it aggressively. Those are traits of smaller languages that don't interop well with other ecosystems so they need a lot of evangelism to build out the library ecosystem. > there's a space for a higher-level language with most of what people like from Rust that has a (tracing) garbage collector What non-memory management related things is it people like from Rust that is missing from, say, Java or Kotlin? Because those have great web frameworks that address all the features requested in the article and a whole lot more, there's good first class support for all major platforms, lots of documentation etc. These languages are also famously developer friendly with good error messages. reply saghm 3 hours agorootparent> Those are traits of smaller languages that don't interop well with other ecosystems so they need a lot of evangelism to build out the library ecosystem > What non-memory management related things is it people like from Rust that is missing from, say, Java or Kotlin? I'd argue that Rust has better interop with C, C++, JavaScript, Python, Ruby, and probably almost every other non-JVM language than Java and Kotlin. I'm not sure why you think that getting people to write more libraries is the goal of evangelization; if anything, I think Rust is somewhat notorious for people writing lots of libraries but comparatively fewer full applications. Independent of interop (which I'm not really sure is as important to understanding why languages are or aren't popular as you seem to imply it is), I don't think the tooling in Java is nearly as beginner friendly as Rust. It's not just about the code itself; handling dependencies and building an application that you can run outside of an IDE are not nearly as straightforward in Java as plenty of other languages nowadays. My point isn't that Java is bad or that doing things in it is hard in the absolute sense, but that \"it's possible to do this in language X\" is not the same as \"it would be easy for a beginner to figure out how to do this in language X\". I think there's an inherent lossiness in trying to distill what people like in a programming language into a bullet-pointed list of features, and it's an easy trap to compare those lists and conclude that one language doesn't have anything novel to offer based on that rather than the entire experience of working in a language. reply smolder 7 hours agorootparentprevRust makes an '.exe', Java makes a '.jar'. I think people want to write programs that run on an OS rather than an interpreter. reply pjmlp 6 hours agorootparentJava compilers have been producing '.exe' for about 20 years now, it is a matter to actually care to learn about their existence. reply saghm 3 hours agorootparent> it is a matter to actually care to learn about their existence That's kind of the whole point I was trying to make above; if one language makes something super easy to do without having to look for instructions compared, that makes a difference in terms of how people will decide whether to learn it. Individually, lots of small quality of life things add up and can make a language that otherwise would be unapproachable way easier to get started with than languages that don't prioritize that sort of thing. reply pjmlp 3 hours agorootparentI know learning is a chore, nothing like jumping into it without thinking. /s reply smolder 3 hours agorootparentprevYeah, sorry. I was aware of that but was being loose with words. I do think that part of the appeal is rust feels more bare metal and direct to people even if they're using heavy abstractions as compared to Java/kotlin or C# programming. reply kaba0 6 hours agorootparentprevFor the majority of executing code, there is no fundamental difference when it comes to a JIT compiler. Besides, GraalVM can produce a native executable for pretty much any JVM language/program. reply immibis 6 hours agorootparentThe problem is friction. To run a rust app you can just run it. To run a java app you need to install java first. This is no problem for backends running on servers, but client apps (like Minecraft) have to include their own JVMs to reach a wider audience, and this solution still introduces a bunch of complexity. reply pjmlp 6 hours agorootparentNot a thing since Java 9 and jlink introduction, or since those commercial AOT compilers exist, for 20 years now. And if free beer is your thing, GraalVM native image or OpenJ9 also produce a regular executable. reply adamrezich 2 hours agorootparentprevA web framework doesn't need GC, it just needs some ability to express the idea that per-request code should get its own allocator, with knowledge of said allocator propagating down through function calls. Jai solves this by having a \"context\", which includes an \"allocator\" member by default, whose default value is a standard heap allocator. You can override this allocator member to point to your own allocator, so it's easy and straightforward to have the main server procedure allocate a pool of memory, create a new context with a new allocator that uses this pool of memory, then \"push\" this context for the duration of the request resolution, then free (or mark for reuse) the memory pool afterward. reply tcfhgj 8 hours agorootparentprevFast also means more CPU Ressource efficient. I am all in for not wasting more and more resources. reply kaba0 6 hours agorootparentprevScala 3? It has the vast Java ecosystem available and state of the art GCs (plural), with either a focus on throughput or low-latency. Also, it can exclude the null value from the type system, marking it explicitly with a union type. reply btreecat 6 hours agorootparentI think all NVM langs claim interoperability with Java ecosystem. The situation on the ground is never as nice as sold in my XP. Scala is a great example where I've seen the \"best\" option being rewrapped libs with scala calls and types rather than a native solution and the dev XP just isn't as good overall. reply kaba0 6 hours agorootparentYou can hardly get higher quality code than what is generally available in Java, especially with their stability. Wrapping it to use the language conventions seems like a pretty solid choice to me. The dev xp, I sorta agree on, but it has improved a lot. reply jeroenvlek 12 hours agorootparentprevStrongly agree, yet debates about improving the ergonomics of the language, for which there clearly \"is a market\", seem to be hindered by those zealous activists. A minority, I'm certain, but vocal nonetheless. It really can be small stuff too, like hiding that nested generic \"GC adjacent\" type salad to be accessible only if you need it, via a type alias. Yes, I can define that myself, but the point is that a lot of people need it often, given its widespread use.l, so why not provide one? I'm sure there's reasons not to do the above example, but that's not the point. It feels like Rust is at 95% of being amazing, and that the remaining 5% is attainable if we want to. reply saghm 11 hours agorootparentI used to think that it was more likely that Rust would \"expand upward\" to provide the higher level syntax that people want in a language like I describe above, but it does seem like the language development has vastly slowed down in terms of big new features. I don't necessarily think this is a bad thing; plenty of people didn't like how much churn there was in the first several years after Rust 1.0 came out. I personally didn't mind since I never ran into any significant breakage in what I worked on, but I definitely noticed a change in how open companies seemed to be to use Rust in any capacity coinciding with Rust's releases growing smaller on average; I think \"frequency of major language features\" is often used as a proxy for \"language maturity\". At this point, I think a new language is more likely to provide this niche than Rust, and I also don't think that has to be a bad thing. Having Rust scoped more to lower-level programming where you're more worried about things like minimizing heap usage and avoiding copying strings or whatever rather than trying to be all things to all users might end up with a better experience in the long run. reply jeroenvlek 7 hours agorootparentLol, and there are the downvotes reply sunshowers 17 hours agorootparentprevI'm just really leery of using languages that don't have a clear separation between immutable and mutable state. Having it in Rust catches so many bugs. I also want to write code in languages where you don't have to engage in bad software engineering practices to get optimal performance. That usually means aggressive inlining, something that Rust excels at. edit: the other thing I wanted to mention is that rust's features cause dependencies to be pretty high quality. If I pass in a slice of something, I'm no longer worried that something ten layers down will change it. Error handling is explicit and panics are rare, so I'm not worried that dependencies will start throwing exceptions after an update. (This is the reason functions should indicate errors in type signatures—a better ecosystem.) ADTs mean dependencies represent fewer illegal states internally. It's just really nice to write code where the bugs are a couple levels up from bullshit like this. reply rtpg 15 hours agorootparentI think this is a legit sort of reasoning, but in an interesting way it's something where OCaml could fill the gap. It has its ergonomics issues, but I think the issues are basically the same/worse in Rust (except I guess Rust's macro system is better). I say this as a Rust enjoyer, but I take the pain because I _really_ want what I'm working on to be fast. reply tomtheelder 6 hours agorootparentI like OCaml a lot as a language, but the tooling is very, very poor by modern standards. Poor enough that I think it’s a total blocker on wider adoption. reply sunshowers 14 hours agorootparentprevOCaml is definitely something I'd consider using if I didn't already live and breathe Rust :) I use Rust often for things that aren't possible in other languages, so I also use it for things that are possible in other languages. (Though I used Python for a production and an art project recently -- with uv it's quite nice.) reply berkes 10 hours agorootparentprev> that are nothing even remotely close to systems programming This is unnecessary gatekeeping. It also shows your lack of perspective. Or, rather, your lack of imagining other perspectives, probably. Sure, rust is primarily a language aimed at systems programming. But it also is so much more (and also a cult). * Its type system is excellent. Especially the lack of a \"null\". Even if, like me, you're fine with a GC, the type system alone is worth dealing with this insistent borrow-checker. * Its multithreading is stellar. The borrow checker helps a lot in making it such. * Its mutable/immutable model is highly practical. It's what makes the threading stellar and the type system more useful than in any other language I worked with. * Its \"oop\" model is uncomfortable at first (coming from Ruby and Java) but forcing the \"composition over inheritance\" by not having inheritance is probably the best thing that can happen to \"OOP\". Every solution where I used inheritance, I shouldn't have. * Its culture of \"making the good way the easy way\", being opinionated, and a community that adheres to this, is worth dealing with a thousand borrow-checker WTFs. cargo fmt, clippy, etc. * Its build system that generates binaries that can just be \"plopped\" onto a server, \"chmod +x\" -ed, and ran is unprecedented. I come from Ruby, Java, Javascript (typescript). I do a lot of Python, maintain some go services, have decades of PHP under the belt and occasionally dabble in some C and even C++. I can find my way around in a C# codebase and Objective-C. All have their strengths and weaknesses. Their place and use-cases. But rust, for me, is the only \"ecosystem\" that ticks all checkboxes in nearly all situations. It has downsides, and I consider the borrow-checker to be one of them in a lot of my use-cases, but it's something I'm willing to deal with gladly, because rust's other benefits. reply stefs 5 hours agorootparenta few months ago there was an article linked here on HN (iirc about rust game dev) that argued that while rust is great, it's virtue of being mostly correct every time can also be a weakness. the argument was that in early dev and prototyping phases you don't want to write good, clean, correct code but move fast and break things while not caring about edge cases - and this, the author argued - is relatively hard in rust. making the good way the _relatively_ easy way reply berkes 2 hours agorootparentI partially agree. It's not so hard to write software that keeps the borrow checker happy in early stages. But it does restrict the freedom to make terrible decisions a bit. Sometimes in that stage terrible decisions are OK. But I do like that even in this stage, one is forced to at least make the decision to have terrible things, explicit. Like with 'unwrap()' and 'expect()' littering the code in PoCs and exploratory projects. For me, a bigger problem in this stage is that I lack the information to make decisions on types. I have this same problem in TS and Java. It's guaranteed that I'll shape types in ways that will prove difficult, or slow me down a lot next week. I'll be spending time refactoring types when I should move on to the next feature. I guess, but that's just my experience, that statically typed languages are just not well suited for early stage software. When we don't know the shape of the data we'll be pushing around, nor know anything about the shape of the layers, ports, modules and so on. Which is why I'll grab ruby for these kind of quick explorations often. And once the shapes emerge, rewrite it in rust :) reply jrjrjrjrj 9 hours agorootparentprevI agree with you on the borrow checker... Part if me wants a simplified(distilled?) Rust ... All of the good stuff, but remove the bad... Not sure how that looks but it is something I know I want reply biztos 9 hours agorootparentprevI like Rust, but wasn’t “plopping” a binary onto a server and running it one of the original virtues of Go? Arguably a louder “plop” due to size, but then you don’t have to chmod. reply berkes 9 hours agorootparentCertainly. A lot (all?) of the points I make aren't unique to rust, or even invented in rust. Many have even better implementations in other languages. Go also invented the \"fmt\", with one opinionated code style \"enforced\" by default - bikeshedding be gone! My point was that it's the combination of all of these points. AFAIK, go ticks many of these boxes too. But for me golang falls short with mutability, and with the type system (though that one's catching up really fast). It's the \"package-deal\" that I like about rust. reply n8henrie 6 hours agorootparentprevI much prefer rust to go for many reasons, but IME go gets this part much better. Darwin / Linux cross compilation, armv7, FreeBSD, oh sorry you don't have a linker for that toolchain, wait where are those OpenSSL headers... lots of cross-compilation targets I've done in minutes with go that scp and run right away that end up being days / weeks of adventures to cross compile in rust, in spite of me knowing rust much better than go at this point. reply berkes 3 hours agorootparentI have had the luxury to work in projects that don't have to target many platforms, but rather \"one\": a server. Which then is often a simple linux variant so setting up a CI or even the local build to target that \"one\" is relatively simple. But wasn't Go a lot more limited in the amount of targets it can build for than rust? reply Ygg2 8 hours agorootparentprev> But it also is so much more (and also a cult). Way to lose readers with memeish statements like that. That said it's in good company of cultists like Smalltalk and Lisp community ;) > * Its \"oop\" model is uncomfortable at first (coming from Ruby and Java) It's OOP in the sense it has polymorphism, and \"methods\" . It's not OOP in almost every other conceivable way. It doesn't fit with static OOP of Java. It doesn't fit with dynamic OOP of Smalltalk and Ruby. > It has downsides, and I consider the borrow-checker to be one of them I wouldn't want Rust without borrow checker, and never figured this complaint. Yes, it's uncomfortable and will prevent you from making some legal code. Guess what? So will a seatbelt. You move too fast to reach something and it snaps you in place. reply raxxorraxor 6 hours agorootparentRust to me looks a bit like Java. \"One owner per resource\" is Rusts \"One class per file\". I am not convinced that the mental overhead justifies the memory safety guarantees yet. At least for a general purpose language. I didn't yet write a lot of Rust, perhaps more experience trivializes Rusts ownership model. reply Ygg2 4 hours agorootparent> \"One class per file\". Wait. What? Neither Java nor Rust limit class/struct per file. You must mean public class per file. As a fellow Java dev, no it doesn't look like Java at all. Maybe it looks a bit like Kotlin, but only superficially. I wrote Rust on and off for 5 years, by that time you internalize the borrow checker. reply raxxorraxor 3 hours agorootparentI did mean one public class per file. I didn't mean syntax or core lib to be similar, I just generally meant that both languages impose restrictions on themselves which might seem sensible at first. reply Ygg2 3 hours agorootparentJava isn't that restrictive. Rust's type system is much more restrictive and customizable. That said, a good way to think about programs is a series of restrictions, i.e. invariants. Truth be told, only Ada Spark so far really embraced invariants. reply mrbonner 12 hours agorootparentprevI think RC and clone are the way people begin to use Rust. While lots of criticism are against this usage for not idiomatic, it should be acceptable. Coming from Python, nodejs or Ruby, even RC and clone or stack based variables is still a magnitude faster than those languages. When it's about absolute control for performance you can drop down a notch to the borrow checker behavior. reply mathw 11 hours agorootparentThat's an interesting perspective I'd not considered before. Maybe there are different \"registers\" of Rust, in the same sense that linguists talk about registers in human languages, where you use your language differently for different purposes. And thus, maybe if you're writing something that doesn't need to be screamingly fast absolutely all the time there's a register of Rust where putting lots of things in Rc> is completely acceptable, in much the same way that you might use a lot of impolite words around your friends but you don't in front of your employer. reply Philpax 11 hours agorootparentYou may be interested in https://without.boats/blog/the-registers-of-rust/ :) reply LtdJorge 9 hours agorootparentprevI think Rc> could be simplified to Rc. reply scotty79 6 hours agorootparentprevI'm gonna plug my Rust \"invention\" here: How to program in Rust as if it was old school C++ with pointers https://news.ycombinator.com/item?id=34067924 reply andrewjf 15 hours agorootparentprevI came to Rust from Ruby-on-Rails and I’m absolutely done with method_missing and finding nils and NoMethodError in production. This is not remotely a trade off I’m interested in making. reply cies 11 hours agorootparentHaving written some Rust and a lot of Ruby, I find Kotlin to be a really nice \"typed Ruby\". Just like Ruby, Kotlin is an OO language in the core that is really friendly to FP. It's terse (not as terse as Ruby, as there are types to specify); but MUCH terser that Java. Kotlin is serious about null-safety. reply keybored 4 hours agorootparentprev> Rust is great, but - heretic idea - not literally everything needs to be in Rust. Go try Ruby on Rails for a while! Or - if Rust has whetted your appetite for more functional styles - Phoenix on Elixir! Yes, it's not quite so blazing fast, but - let's be honest here - you're not going to be getting billions of requests per second on your hobby website where you write about taking apart Amigas. It's OK! She'll be right. Who changes their minds about what to do for fun because someone on HN thought it was unnecessary? Doing what some random person wants instead because they want to be “heretical” (because it’s a religion right) sounds like the opposite of fun to me. But I’ll see you on the next thread about making a Rube Goldberg web server in C++ templates. Or some such very necessary and “approved for prod by HN” thing. reply chrismorgan 13 hours agorootparentprev> Don't @ me, ARC is GC. Don’t know if you intended this joke, but some time before Rust 1.0, @ was the sigil for garbage collection types (@T, like &T these days). Which I think was just reference counting, without even a cycle collector, because was shown to be undesirable before it could be improved. reply CharlieDigital 17 hours agorootparentprev> I think a lot of what people actually like about Rust, to be honest, is that it's essentially an ML language masquerading as a C-like, with a stellar packaging and tooling story. F# seems like a good option. reply akkad33 13 hours agorootparentF#is not C like and it's tooling and packaging is not as good as rust. Last time I checked it had multiple packaging solutions like paket and nuget. The tooling on vscode can be buggy reply pjmlp 11 hours agorootparentEasy, stay with .NET standard tools, MSBuild. There is also Visual Studio and Rider. reply antonvs 7 hours agorootparent> stay with .NET standard tools That’s a dealbreaker in many situations. reply CharlieDigital 5 hours agorootparentIf you haven't used .NET since the early 2000's, I'd recommend checking out current `dotnet` CLI and tool chain. reply pjmlp 7 hours agorootparentprevThen probably .NET isn't an ecosystem for you, that is fine, there are other options. reply piaste 7 hours agorootparentprevSuch as? They're cross-platform and MIT licensed. reply satvikpendem 13 hours agorootparentprevHow is F# on non-Microsoft platforms, is it fully cross platform? I don't use C#, .NET, or F# so I have no idea about the ecosystem. reply mathw 11 hours agorootparentYes F# has come along for the ride with modern .NET and it's as cross-platform as C# is. However, a lot of the really shiny .NET stuff is tooled up mostly for C# users, so it can be a challenge if you wanted to do something like a .NET MAUI (cross-platform GUI library) application in F# because the tooling and the content out there to teach you about it assumes C#. F# can usually handle C# things - they put a lot of work into ensuring interop with new C# features - but the languages are from different paradigms so it is sometimes a bit awkward despite F#'s comprehensive OO support. Personally I struggle a bit with F# because it doesn't have typeclasses, and a language that looks that much like Haskell but doesn't have typeclasses just feels weird to me. Although it might get them... C# are looking at adding a traits-type feature (they don't seem to know what to call it yet, but the design's been kicking around the language team for ages now and keeps getting discussed), so F# could presumably piggyback on that if they wanted. reply scns 10 hours agorootparent> C# are looking at adding a traits-type feature (they don't seem to know what to call it yet, but the design's been kicking around the language team for ages now and keeps getting discussed) Why not name it Traits? reply CharlieDigital 5 hours agorootparentprevReally good. I'm in a startup using .NET. We deploy to a variety of targets including AWS t4g (Arm64) instances in AWS as well as x86/64 targets in GCP. All devs are on M1 Macs. Our build pipeline is GitHub Actions Linux runners. Our DB is either AWS RDS Postgres or GCP Cloud SQL Postgres with a mix of EF Core as ORM and Dapper (for more complex read queries). C# has, over the years, converged with TypeScript so they are very similar[0] (though no Duck typing in C#). Good mix of OOP and FP paradigms borrowed from F#. F# interoperates with C# so it can tap into the larger C# ecosystem. It's a good platform; very productive. CLI has a functional hot reload (much more limited than Node on JS as the granularity of module replacement isn't quite as good). [0] https://github.com/CharlieDigital/js-ts-csharp reply cmrdporcupine 16 hours agorootparentprevOr just OCaml reply sunshowers 16 hours agorootparentOCaml is an excellent language, but doesn't it have a GIL? reply throwitaway1123 14 hours agorootparentMulticore OCaml finally landed a few years ago: https://news.ycombinator.com/item?id=34013767 reply cultofmetatron 9 hours agorootparentprev> - Phoenix on Elixir! Yes, it's not quite so blazing fast As someone running a fairly cpu heavy SAAS application for users who literally dog pile our whatsapp support forums if it goes down for even a second, Phoenix is pretty damn fast in production. I won't claim its as fast as rust but it will run circles around ruby or python and for IO bound tasks, competes perfectly fine with go. Its never been a bottleneck (database is another story) reply fire_lake 10 hours agorootparentprevPeople want everything in one language. reply wiz21c 11 hours agorootparentprevI do some systems programming, use rust for that and I am exactly the person you describe :-) reply pjmlp 13 hours agorootparentprevWhich is a classical example of everything is a nail. reply IgorPartola 6 hours agorootparentprevAfter reading endless insufferable comments on HN a la “this should have been written in Rust” I realized that there is a very strong possibility that the people making those comments do not know another language. We saw this with JavaScript a decade ago. Before that it was Ruby. reply madeofpalk 9 hours agorootparentprevIs Rust only for \"systems programming\"? reply gbin 8 hours agorootparentI would say it shines when you have systems constraints, for example embedded & robotics work well with Rust. You cannot have a GC (either you don't have a heap or you cannot tolerate a GC pause resp.) but you still have the benefits of a good functional programming language to write your algorithms in. The benefits are less obvious if you have less of those constraints: go is way way simpler to write large codebases for cloud applications in and still provide an excellent concurrency support. Python is way better for experimenting. reply scotty79 6 hours agorootparentprevSystem programmers thinks so. I guess that if all you do is hammer nails then a multitool is just a hammer. reply jokethrowaway 6 hours agorootparentprevI agree the type system is the best feature of Rust but neither Ruby or Elixir have the level of type safety and type expressiveness of Rust. I'd recommend Haskell as an alternative, but Rust \"fixes\" plenty of long term Haskell annoyances (especially around laziness by default, concurrency, unsafe std). Being able to use Arc>, Rc> and other smart pointers gives you granularity and control over what happens to your memory. I think it's a nice feature to have to mentally reason about your memory usage (incidentally a weak point in Haskell); even if it were on-par with a GC implementation-wise, why use a GC over this? I like to have control over my code (and I'm probably one of the few who think monad transformers in haskell were a good feature - despite their clunkiness) and I'd pick explicit code over \"magic garbage collection in the background\" any day. This is not to say I like verbosity for verbosity's sake (eg. think React Redux in JS vs the implementation of the same idea in Elm) but in some cases I think it's justified and it brings extra value. reply scotty79 7 hours agorootparentprevIf Rust had no borrowing at all, just Rc everywhere I'd lose maybe 5% interest in it, maybe less. Value orientation and Traits system, close to bare metal speed and stellar stdlib are what's most interesting for me. reply antonvs 7 hours agorootparentprev> Go try Ruby on Rails for a while! You gave a long list of benefits of Rust - which should have answered your own question - and then suggested people use an ecosystem with almost none of those benefits. I’m sure it made sense in your head. reply wongarsu 20 hours agoparentprevThis is definetly something the article should have drilled down on. Why Rust? I'm sure everyone's tired of hearing why rust is an excellent alternative to c/c++ for new projects, but as an alternative against Python it gets muddier. Rust has a clear advantage in performance and memory footprint and a much better multithreading story, but those are things that aren't high priorities for 95% of web development. That basically leaves you with Rust's type system. Rust's type system is pretty great, and if we pretend we can't hear the Haskell developers it's one of the best type systems out there. That might seem to get in the way of quick prototyping, but on the other hand it would mesh really well with a framework like Django. One of the great things of Django was that you define your data schema, and Django takes care of both the database and a passable admin area. I'm sure you could greatly expand on that principle, with data types driving a lot of behavior and conveniences that the framework just handles for you. Maybe a bit like .NET, but without the enterprisy coat of paint and without putting dependency injection everywhere. reply creatonez 19 hours agorootparentThe answer for why folks are so inclined towards doing high-level tasks in Rust... is the type system. Its sensibilities are in a sweet spot that makes it very easy to pull off huge refactors. It was also a lot of people's first introduction to algebraic data types being used in nearly all error handling (its usage of `Result where E implements Error` and lack of nulls or exceptions). It makes a lot of progress towards the goal of \"make invalid states unrepresentable\", which could be really useful for web apps. reply pjmlp 13 hours agorootparentMost of those people aren't aware that the type system is a feature from most ML derived languages since Standard ML is a thing. All of them with better ergonomics for Web development. reply MrJohz 12 hours agorootparentDo they really have better economics? (EDIT: ergonomics!) Last time I looked into OCaml, I struggled to find a way to interact with the database in a type-safe way, I never figured out which standard library I was meant to install, I was being encouraged to use 2-3 different project management tools (Dune + Esy + OPAM), and I gave up on writing tests. But at least there's a garbage collector! I realise these are all problems that I'll wrap my head around over time, and eventually they'll seem completely trivial to me, but the introductory documentation on getting started as a professional (and not as a first-year student doing a French-language compsci degree, which is what most of the documentation assumes), is pretty dire. Meanwhile, much as I'm sure I overuse `.clone()` and `[A]Rc`, the ownership model of Rust is deeply useful. It's something I often find myself missing in Javascript - not necessarily because I want to produce the most performant code, but because it's useful to understand the lifetimes of the objects I'm keeping around. Am I accidentally storing a reference to something in a closure that I forgot about? When this object gets deleted, have I checked that it's the last possible reference to this object? Etc. Like, I don't think everyone needs to learn Rust. It's a great language, but there's lots of other great languages out there, depending on your personal and business contexts. But I think this idea that Rust can and should only be a low-level language feels absurd to me. It is a fairly ergonomic language with a fantastic ecosystem, a powerful type system, and an ownership model that will be useful even if you do try and opt partly out of it with GC-like wrapper types. reply pjmlp 11 hours agorootparentI guess you mean ergonomics. Yes they do, unless you get your editor to magically type `.clone()` and `[A]Rc` all over the place. Not to mention they don't need unsafe, or 3rd party crates to handle graphs. Additionally, all of them have interpreters and REPLs alongside their compilers, streamlining the code-develop-debug loop. reply MrJohz 9 hours agorootparentThanks for pointing the typo out! I've fixed the comment. In fairness, I think a lot of this comes down to familiarity. I'm fairly familiar with `.clone()` and Arcs at this point, so they don't really change much in terms of ergonomics. Usually their usage is fairly obvious, and quite often my editor literally does magically type the `.clone()` calls through LSP fix commands. It's the same as, say, OCaml's insistence that recursion is better for complex loops than, say, `while` — it's not necessarily wrong, but if you're unfamiliar with the idea, it's going to feel weird. More important to me is the ergonomics of the broader ecosystem, and that's something that Rust has done well, that other languages just don't seem to be interested in at all. Things like integrating testing into the standard workflow; working hard on getting the stdlib in good condition; having an excellent ecosystem of well-documented, usable libraries; or designing error messages and lints with a focus on getting people to understand how the language works and not just what they've done wrong this time. I've really missed that stuff whenever I've tried MLs. You mention, for example, REPLs, but a unit test is basically a saved REPL session that you can repeat every time you make a change. I'm not trying to convince anyone that Rust is the best language in the world or something. I really like it, but I find it helpful to think in terms of object ownership even in non-Rust languages, and I can understand why for other people that sort of approach is unhelpful. But I would love to see other languages embrace its ergonomics more, or new languages created with that as their focus. reply pjmlp 8 hours agorootparentA unit test has nothing to do with a REPL, a proper REPL provides something similar to jupiter notebooks in feel, alongside debugging and hot code reload experience. reply MrJohz 6 hours agorootparentMaybe we have different understandings of REPLs. For me, a REPL is a tool that I can use to try out some portion of my code, see how it responds to various inputs, see how it handles certain cases, and explore the internals of it via debugging tools. But... that's what I do in a unit test anyway. A unit test is essentially a REPL session that I've frozen in time, can replay whenever I want, can debug, can trigger a fresh run whenever changes occur (which isn't quite hot reloading, but often a darn sight more useful), and can keep track of and share with my team. Which means I'm not just able to explore things on my own, but I can see the explorations that other people have made with their code. reply pjmlp 6 hours agorootparentFor me the REPL is the experience similar to Lisp Machines, Smalltalk kind of experience, which is loosely captured in Jupiter Notebooks. As for unit tests being more usefull, depends on how much one is willing to wait for them to run in Rust, given its compile times. reply ActorNightly 18 hours agorootparentprevTyping in modern days is effectively moving the task of debugging to compile/ide annotations process instead of testing for correctness. Albeit it makes the process easier compared to a really shittly written code without strict typing, but against good codebases, it takes the same amount of time. reply tkzed49 18 hours agorootparentI strongly disagree on the principle that tests and types have a 100% overlap in the problems they're solving. reply ActorNightly 16 hours agorootparentWhy not? If you think about correctness of a program, (i.e for any combination of given input , it changes a state in a determinstic way, including no state change for invalid input). Strict typing is one way to accomplish this. The cpu does not give a shit about types. It cares about memory registers and locations. The unique code built into the compliers/transpilers is the thing that validates the correctness of the program in this case. You can move that code into the testing suite without relying on the complier, and just do testing. Generally, given competent programming skills, this takes about the same amount of time as designing a well structured program - your tests are pretty much your design document for the thing itself. reply tkzed49 12 hours agorootparentThis is a very anti-pragmatic way of looking at things in my opinion. The program is not a snapshot that exists in a vacuum. Most programs are going to grow over time and have things added and removed in various places by many people. They're going to have many interfaces and operations. I think you're arguing that tests and types can both be used to check a particular case for correctness. Sure, this is true. However \"moving type checking code into the test suite\" means nothing—that would just be type checking. When you make a code change, there's a difference in the kinds of feedback you get from your tests and your types. Tests usually cover business logic or stories—things that you want or don't want. Types ideally cover everything. They check every operation applied to a given piece of data. Of course types are rarely precise enough that they can catch every logic bug (e.g. strings with semantics not encoded in types, like email addresses). This is just scratching the surface. You might just have to try out both to get a better sense of how they feel to work with. reply ActorNightly 3 minutes agorootparentSo at one of my older jobs, we worked primarily with C code that ran on a mini linux box inside a plane, and interfaces with a sensor pacakge. We wanted to make sure that the software was 100% correct cause any errors would mean an aborted flight test We basically ended up creating a tool+language spec that would let us define the mapping of input to output sequences. We wrote it in such a way where we sat with scientists and pretty much mapped all the possible cases that they could think of for valid inputs and what the code should produce. Then this tool would basically write automated tests for us, in such a way as to not only test correct behavior, but also do combinations of inputs out of order, fuzz testing, and so on. We ended up making it also check memory state, to ensure that there was no memory leaks, and analyze the memory space for required data or data that should not be there. In the end, whenever someone was developing anything for this software, they would basically just run the tests, and it would be very good at catching possible errors, mostly on the negative side (i.e for a fuzzed input, it would result in an output that should be an error). We could have done the same thing with a typed language, but it would have to be very strict typing to the point of something like CoQ, and it would have taken us probably the same amount of time to write that. dinosaurdynasty 15 hours agorootparentprevYou can not test for the absence of behavior. You can design type systems to ensure the absence of specific behaviors. reply kaba0 6 hours agorootparentprevTypes [1] can only reason about categories of values, not about values. Tests and types do have an overlap, but the best option is both. Especially when combined with fuzzing, types can exclude large number of cases, making it even more efficient at covering a huge range of code paths. [1] yeah, there are type systems like lean, coq that can do both, but the proving process is just currently not realistic for everyday applications reply dathinab 7 hours agorootparentprev> it takes the same amount of time in my experience it doesn't 1. refactoring are in my experience still much faster (and reliable doable) with rust compared to e.g. Js/Python even in presence of a lot of tests. This is a bit less of an point with Java/TS/C# etc. through I had some very bad (and also some good) experiences with refactoring in TS. And to be clear I don't just mean pure refactoring but any larger changes affecting many places in code which might be needed to idk. impl. some feature. 2. especially with Js,Py and similar there are way to many edge cases you can have to test for all of them. Sure most times this mainly matters when writing libraries but on larger projects does apply too. Stupid stuff like you expecting a `list[int]` and someone (externally i.e. outside of your tests) passes a `dict[int, int]` and that happens to work as you current impl. is only linearly iterating it as if it's a `Iterable[int]` but then you change the impl to require a `Sequence[int]` as you access it by index in some corner case and now you customer has really strange subtle bugs. Can't be caught by your tests as the problem is the customer but still breaks the customer which is always bad and can't happen with rust. Sure also won't happen if everyone uses type annotations and mypy correctly and strictly. Through you can't rely on your customer using mypy, but you can rely on your customer running compiler checks (as it's the only way to build the code). Also while mypy is much much much better then pylance it is still prone to issues as both `cast` and `ignore[..]` are things you sometimes need but which easily can hide bugs if the code changes (cast doesn't pin the \"from\" type and ignore is scoped by place not by what is wrong) reply masklinn 9 hours agorootparentprev> instead of testing for correctness. Except you can’t test for correctness. Tests can’t prove the absence of bugs. reply J_Shelby_J 17 hours agorootparentprevPeople who say copilot is useless.... I can only imagine they're in a dynamically typed language. Copilot + Rust makes boilerplate go fast. Strong typing is force multiplier for code gen. reply adastra22 16 hours agorootparentI suspect that’s true. I never blindingly copy LLM generated code (that would be recklessly stupid), but I often only quick skim rust code generated this way, just to make sure the general task it is solving is what I asked for. If there is an unhandled edge case or or memory handling bug, rust will catch it. reply NitpickLawyer 15 hours agorootparentIt's also easier to implement a generate_code -> check_for_errors -> give_error_to_llm -> fix_code loop, because the errors that rust throws at you are most of the time really well thought out, and as succint as usefully possible. Comparing it with python, where you have to write custom parsing to trim it down and even then it's hit and miss on where the actual error lies, it's not even funny. reply adastra22 14 hours agorootparentRust’s compiler errors is definitely one of the selling points of the language. reply kortilla 19 hours agorootparentprev>Result where E implements Error` and lack of nulls or exceptions This all goes out the window when people throw “unwrap” all over the place because “this should always succeed”. reply creatonez 18 hours agorootparentRust does not skirt around the fundamentals. You can",
    "originSummary": [
      "Rust web development currently lacks a comprehensive framework that bundles essential features, making it more complex compared to frameworks like Django.",
      "The ecosystem is fragmented, with many libraries but lacking integration, leading to a need for significant manual setup.",
      "The author is developing \"nicole's web toolkit\" (newt) to simplify Rust web projects by combining necessary features with good documentation and community support."
    ],
    "commentSummary": [
      "Rust lacks a comprehensive web framework akin to Django or Rails, which some developers are calling for to simplify web development in Rust.",
      "Rocket, an early web framework for Rust, lost popularity due to its dependency on nightly Rust and being stuck in a release candidate phase.",
      "While some developers appreciate the unopinionated nature of frameworks like Axum and Actix-web, others argue that Rust's complexity and systems programming focus make it less ideal for web development compared to languages like Python or Go."
    ],
    "points": 239,
    "commentCount": 345,
    "retryCount": 0,
    "time": 1728249707
  },
  {
    "id": 41767648,
    "title": "Nearly all of the Google images results for \"baby peacock\" are AI generated",
    "originLink": "https://twitter.com/notengoprisa/status/1842550658102079556",
    "originBody": "Google está muerto. https://t.co/DRxCnjz9cF pic.twitter.com/1PFMZIUvqQ— 0x000000 (@notengoprisa) October 5, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41767648",
    "commentBody": "Nearly all of the Google images results for \"baby peacock\" are AI generated (twitter.com/notengoprisa)238 points by jsheard 2 hours agohidepastfavorite178 comments kevin_thibedeau 1 hour agoAlmost all of the \"product X vs Y\" results are AI ramblings now. This growth of the dead Internet is making me want to sign up for Kagi. We're going to need a certification for human generated content at some point. reply quasse 1 hour agoparentKagi is not a panacea unfortunately. I pay for it and daily drive it to support a Google alternative, but I still have real trouble with my results being full of AI garbage (both image and text search). As mentioned, product comparisons are a big one but another worrying area is anything medical related. I was trying to find research about a medicine I'm taking this week and the already SEO infested results of 5 years ago have become immeasurably worse, with 100s of pages of GPT generated spam trying to attract your click. I ended up ditching search alltogether and ended up finding a semi-relevant paper on the nih.gov and going through the citations manually to trying and find information. reply pavon 34 minutes agorootparentThat matches my experience. Kagi doesn't surface much content beyond what Google/Bing do. What it does better out of the box is guessing which content is low-quality and displaying so that it takes up less space, allowing you to see a few more pages worth of search results on the first page. And then it lets you permanently filter out sites you consider to be low quality so you don't see them at all. That would have been awesome 10 years ago when search spam was dominated by a few dozen sites per subject that mastered SEO (say expertsexchange), but it is less useful now that there are millions of AI content mills drowning out the real content. For content that isn't time sensitive the best trick that I have found is to exclude the last 10-15 years from search results. I've setup a Firefox keyword searches[1] for this, and find myself using them for the majority of my searches, and only use normal search for subjects where the information must be from the last few years. It does penalize \"evergreen\" pages where sites continuously make minor changes to pages to bump their SEO, which sucks for some old articles at contemporary sites, but for the most part gives much better results. [1] For example: https://www.google.com/search?q=%s&source=lnt&tbs=cdr%3A1%2C... reply freediver 9 minutes agorootparentprevI use Kagi personally every day and my results are definitely not full of AI garbage so would like to better understand your context. Have you reported any of those issues to Kagi (support/discord/user forum)? We are pretty good at dealing with search quality issues. reply frereubu 1 hour agorootparentprevThe UK NHS website is usually pretty good for this so sticking \"NHS\" in the search terms might help, although I imagine they may not cover non-UK brand names. reply CSMastermind 1 hour agoparentprevIt really is a werid feeling remembering the internet of my youth and even my 20s and knowing that it will never exist again. reply rootusrootus 1 hour agorootparentI'm a little sad for anyone who didn't get to experience the Internet of the twentieth century. It was a unique point in time. I'm ready to pay for a walled garden where the incentives are aligned towards me, instead of against me. I know that puts me in a minority, but I'm tired of the advertising 'net. reply coldpie 55 minutes agorootparentIt still exists. Currently it looks like Patreons and their associated communities, long-running web forums, small chatrooms on platforms like Discord or Facebook or Instagram, and so on. Small communities, with relatively high barriers to entry. reply ssl-3 30 minutes agorootparentThat's not the same Internet. Patreon, forums, Discord, Facebook, Instagram and so on are all centralized. With the Internet of the 90s, discussion happened with decentralized Usenet (owned by nobody) with more-focused discussion happening on mailing lists (literally owned by whoever was smart enough to get Majordomo compiled and running). Email was handled by individual ISPs other orgs instead of funneled through a handful of blessed providers like Gmail. Real-time chat was distributed on networks of IRC servers that were individually operated by people, not corporations. Quickly publishing a thing on the web meant putting some files in ~/public_html, not selecting a WordPress host or using imgur. Ports 80 and 25 were not blocked by default. Multiplayer games were self-hosted without a central corpo authority. One could construct an argument that supports either way being better than the other, but the the Internet of today is not the same thing as it was a quarter of a century ago. (Anyone can make a \"discord server,\" but all that means is that they've placed some data in some corpo database that they can never actually own or control.) reply coldpie 24 minutes agorootparentThat's a pretty narrow view. My Internet in the 90s and early 2000s was on AOL Instant Messenger and MSN Network, Yahoo! Mail and phpBB web forums hosted by random people with pictures hosted on PhotoBucket. Plenty centralized. The technology doesn't matter, it's changing all the time. The difference is the size of the community, and the barriers to entry. If these communities were easy to discover and join and hard to be booted out from, they wouldn't be protected from The Slop. It used to be just getting on the Internet was the barrier. Now we need new barriers: paywalls and word-of-mouth and moderation. reply silisili 1 hour agorootparentprevI've said it before and say it again, I firmly think AOL was just ahead of its time. Bring it back. Charge me 10 or 20 a month. Give me the walled off chatrooms, forums, IM, articles, keywords, search, etc. Revamp it, make it modern. And make a mobile app. Everyone wanted a free and open Internet, until AI and the bots ruined it all. reply coldpie 36 minutes agorootparent> until AI and the bots ruined it all Well... advertising as a business model ruined it all. They get paid for getting page views, so the business model optimizes for maximum page views at minimum cost of creation. This is the end result of what Google and Facebook have spent the last 20 years building. But I'm sure the engineers who built all this have very nice yachts, so it's all fine. reply zem 48 minutes agorootparentprevthe problem with that is not the payment, it's that you will only be sharing it with people similarly willing to pay for a walled garden. I'm guessing most of what we're nostalgic for was created by people who wouldn't be up for that reply pchristensen 1 hour agorootparentprevIt may be a minority, but you're not alone. reply nonrandomstring 58 minutes agorootparentprev> where the incentives are aligned towards me, instead of against me. It's great to read these words. People are starting to get it. The Internet is not for you, it's against you. reply jonathanstrange 1 hour agorootparentprevA mesh network on top of IP with an enforcable license agreement that prohibits all commercial use would suffice to get the old net back. Bonus points if no html/css/js is involved but some sane display technology instead. reply quantified 1 hour agorootparentWithout most of what makes the internet useful, sadly. reply soganess 1 hour agorootparentI'm not sure I like your parent's idea, but it isn't like the regular internet would go away... when you want useful, go there. I wouldn't mind a modern take on geocities sorta system. Where: (1) You can make a webpage that could be about whatever the bleep you wanted it to be about. (2) Only allowed a reduced subset of web technologies. (3) That was free from any advertising or commerce/sales. (4) Was only available to individuals or businesses no larger than closely-held corporation. (5) had clear limitation on AI uses. (6) Had a complete index, categorized and tagged, of all the sites available. But if I am being honest, that is just the nostalgia for the old internet talking. reply coldpie 54 minutes agorootparentStart here: https://blog.kagi.com/small-web reply jauntywundrkind 13 minutes agorootparentprevI only just put it together but Peter Watt's Rifters series is some epic earth grimdark hard-sci-fi, the first taking place as practically horror, confined deep under water. But my point is, the latter books have this has amazing post-internet, just a ravaged chaotic Wildlands filled with rabid programs & wild viruses. Packets staggering half intact across the virtualscape, hit by digital storms. Our internet isn't quite so amazing, but I see the relationship more subtly with where we have gone, with so so so many generated sites happy to regurgitate information poorly at you or to sell you a slant quietly. Bereft of real sites, real traffic. Watts is a master writer. Maelstrom. First book Starfish is free. https://www.rifters.com/real/STARFISH.htm reply i80and 55 minutes agoparentprevUnfortunately, as much as I do like Kagi overall, it goes out of its way to inject AI slop into the results with its sketchy summarization feature reply cogman10 1 hour agoparentprevMost product reviews are simply pumping amazon comments into AI to generate a review. with a final \"pros/cons\" section that is basically the same summary amazon AI generates. reply rightbyte 1 hour agoparentprev> This growth of the dead internet It is quite surreal to witness. It is certainly fueled by the commercialization of internet due to ads and centralization to user hostile platforms. The old internet seems to be doing much better. But it lost most of its users in the last 15 years... reply chongli 1 hour agorootparentThe old internet seems to be doing much better. But it lost most of its users in the last 15 years.. What do you mean by this? How do you find the old internet? reply dumbmrblah 1 hour agorootparentSomething Awful is a good example. Other forums behind paywalls or ones that are invite only. reply drusepth 1 hour agoparentprevI'm glad that Kagi (and others) exist as an alternative for people who don't want generative AI in their searches. Personally, I'm excited about more generative AI being added to my search results, and I'll probably switch to whichever search engine ends up with the best version of it. reply Eric_WVGG 54 minutes agorootparentThis peacock thing was the last straw for me. I installed Kagi just moments ago. And of course the first image for \"baby peacock\" is the same white chick thing… obviously because this story is making the rounds —_— reply jonathanstrange 1 hour agoparentprevSearching with \"Reddit\" at the end of every query helps but I suppose it's only a matter of time when most content on Reddit is also AI-generated. reply rwmj 59 minutes agorootparentReddit is already lost. I was talking to the mods in a large political subreddit and they said after Reddit started charging for API access, all the tools they used to keep on top of the trolls and bots stopped working, and the quality of the whole subreddit declined visibly and dramatically. reply jsheard 1 hour agorootparentprevIt's also not much use to anyone who doesn't use Google ever since Reddit started blocking all crawlers besides Googlebot. Old cached results might still show up in Bing/DDG/Kagi but they can't index any of the newer stuff. reply jsheard 1 hour agoparentprevEven Google is trying to get into the X vs Y game, with pretty funny results if you ask for a nonsensical comparison. https://x.com/samhenrigold/status/1843040235325964549 ...or a sensical comparison where it just completely misses the point. https://i.imgur.com/FotFZ3F.jpeg reply sobellian 1 hour agorootparentCouldn't reproduce - in fact, the second hit is a threads version of the same post - but I get no AI suggestions for this query. Humorous Google queries (or AI queries more generally) are definitely a trope, so I can never really tell if they actually happened or if it's all for karma. reply amputect 49 minutes agorootparentGoogle also routinely removes AI suggestions for searches that produce embarrassing results (you don't get them for searches about keeping cheese on your pizza anymore, for example), so it's even harder to validate once a result goes viral. reply neaden 43 minutes agorootparentprevI still get the second one when I search \"Difference between sauce and dressing\" on Google. The Oven vs Ottoman empire one I don't get an AI overview. Edit: Similar to the second one I just did Panda Bear vs Australia which informed me \"Australians value authenticity, sincerity, and modesty. Giant pandas are solitary and peaceful, but will fight back if escape is impossible. \" reply the__alchemist 1 hour agoparentprevKagi's results for \"baby peacock\" are showing almost the same set (Mostly AI) as Google's. reply louthy 54 minutes agorootparentSearch “peachick”, it works fine. I assume Google would be the same. I guess using the correct terminology matters. reply andrewinardeer 49 minutes agorootparentprevKagi's images are an entirely different set for me. reply dan-robertson 53 minutes agoparentprevMy memory is that these were pretty terrible long before the generative ai boom. reply tambourine_man 50 minutes agoparentprevWe need certification for human generated content for yesterday. Not only that, we desperately need cryptographic prof that content X was produced by person Y. reply bagels 46 minutes agorootparentHow can that ever work in a world filled with people that are eager to lie to you? reply mFixman 1 hour agoparentprevIronically, ChatGPT and similar LLM chatbots are great for those kinds of searches. reply mvdtnz 56 minutes agorootparentYou would have to be soft in the head to rely on any LLM for researching information on a medication you're actively taking. reply ninininino 54 minutes agoparentprevhuman-verified content is going to be the next billion dollar company. reply QuantumGood 1 hour agoparentprevAny source of content can be controlled or manipulated in non-obvious ways. And we already have strong algorithms for manipulating human attention (resulting in the growth of non-falsifiable conspiracy theories, for one). There is no clear approach leading out of information dystopia. reply petesergeant 1 hour agoparentprev> We're going to need a certification for human generated content at some point I wrote some ideas up about this many years ago: https://github.com/pjlsergeant/multimedia-trust-and-certific... reply Analemma_ 1 hour agoparentprev> We're going to need a certification for human generated content at some point. People keep saying this and I keep warning them to be careful what they wish for. The most likely outcome is that \"certification of human generated content\" arrives in the form of remote attestation where you can't get on the internet unless you're on a device with a cryptographically sealed boot chain that prevents any untrusted code from running and also has your camera on to make sure you're a human. It won't be required by law, but no real websites will let you sign in without it, and any sites that don't use it will be overrun with junk. I hate this future but it's looking increasingly inevitable. reply throwway120385 59 minutes agorootparentYour unauthorized access has been reported to the Fair Use Bureau. reply claudiulodro 1 hour agoprevGoogle is going to have to solve for this somehow if they want to remain relevant, right? If searching for an image and generating the image yield the same result, what's the point of image search any more? reply rootusrootus 1 hour agoparent> what's the point of image search any more? The same could be said for regular search. Pretty much anything I search for yields a page of ads followed by pages of content farmers followed by pages of \"almost sounds like experts but is still just a content farmer.\" Financing the Internet with advertising has really made it difficult to find good quality content. The incentives are completely misaligned, unless you are a 'content creator' or Google. reply cflewis 1 hour agoparentprevWatermarking I think is supposed to be the goal, but I don’t think anyone can think that the web is in anything but managed decline. The AI feeding itself AI will just end it all. I think Platformer describes it best: https://www.platformer.news/google-io-ai-search-sundar-picha... The question is what comes next, and I don’t think anyone has an answer to that. reply MetaWhirledPeas 8 minutes agoparentprevI assume this is one example of why big tech lobbies for stricter AI controls. They aren't afraid of AI takeover; they are afraid of AI destroying their businesses. reply TheGlav 1 hour agoparentprevWhat do you mean \"solve\"? This is solving the problem. If people see things they consider good enough, that's all they care about. Source: every other piece of news or social media on the planet. reply Havoc 1 hour agoparentprevTheir image search is presumably not profitable anyway so not sure they care reply IncreasePosts 46 minutes agoparentprevThey're more likely to solve it so that you can't tell the baby peacocks are AI-generated reply broast 1 hour agoparentprevMaybe they will generate the images themselves and show a mix of both. reply whalesalad 1 hour agoprevDrives me nuts. The internet is dead. I just bought a home and I have been googling the best way to tackle certain home improvement projects - like how to prepare trim for painting. Virtually every result is some kind of content farm of AI-generated bullshit with advertising between every paragraph, an autoplay video (completely unrelated) that scrolls with the page, a modal popup asking me to accept cookies, a second rapid-fire modal popup asking me to join the newsletter to \"become a friend\" For better or worse, Reddit is really the only place to go find legitimate information anymore. reply thiht 43 minutes agoparentFor this kind of search, YouTube and TikTok (yes, TikTok) are your best bet. Videos are not (completely) flooded by AI (yet) and you can find pretty much anything about manual work. I prefer text content to videos by a long shot, but genuine, human text content is almost dead. Reddit might be one of the rare exceptions for now. There are also random, still active, old school forums for lots of things but they tend to become extremely hard to find. reply neaden 42 minutes agoparentprevIME for home improvement Youtube is the best resource, though I can understand if you were hoping for text and pictures. reply coldpie 29 minutes agorootparentAlso your local library probably has a bunch of home improvement books. They're probably from the 80s, but trim painting techniques don't change that much. reply eesmith 19 minutes agoparentprevGet a general purpose home maintenance book. For example, https://archive.org/details/stanleyhomerepai0000fine/page/14... links to the chapter \"Painting Trim the Right Way\" from the book Stanley Home Repairs, 2014. Could also look at used book stores. Home repair hasn't changed much. reply throwgfgfd25 53 minutes agoparentprev> For better or worse, Reddit is really the only place to go find legitimate information anymore. This is frightening and, I fear, true. But I'd also add one odd little counterpoint: some of the most useful discussions and learning experiences I've had in the last four years have happened in private Facebook groups. As soon as the incentive to build a following using growth-hacking and AI -- which private groups mitigate to a greater extent -- is taken away, you get back to the helpful stuff. The FreeCAD group on Facebook is great, for example. And there are private photography groups, 3D printing groups, music groups etc., where people have an incentive to be authentic. Public Facebook feeds are drowning in AI slop. But people who manage their own groups are keeping the spirit alive. It's almost at the point where I think Facebook will ultimately morph into a paid groups platform. reply throwway120385 55 minutes agoparentprevYeah, I've had this experience as well. I'll have to go 4 or 5 pages deep in the results to get to a forum thread someone wrote in 2005 referencing a product that doesn't exist anymore plus a bunch of advice that's mostly still applicable. reply notamy 2 hours agoprevIf, like me, you don't have a Twitter account and want to see more than just the single post: https://xcancel.com/notengoprisa/status/1842550658102079556 reply stebalien 1 hour agoparentAnd if you want to automatically get redirected to xcancel: https://einaregilsson.com/redirector/ reply vunderba 29 minutes agoprevAfter Google continued to make it progressively more difficult to use their Image search to navigate/download to the actual image I wrote an image search tool that could be hot keyed from your OS to search the google image repository and copy to clipboard in a fast manner using a custom Google Search Engine id. About a year back I found that 90% of the results I was getting were AI generated, so I added a flag \"No AI\" which basically acts as a quick and dirty filter by limiting results to pre-2022. It's not perfect but it works as a stopgap measure. https://github.com/scpedicini/truman-show reply gs17 2 hours agoprevMost egregious is the one copying the title from Snopes' \"Video Genuinely Shows White 'Baby Peacock'?\" (with the question mark cut off). A page all about how the picture isn't a real baby peacock. But also, if you search the more accurate term, \"peachick\", you seem to get 100% real images, although half the pages call them \"baby peacocks\". reply jsheard 2 hours agoparentAnd the first result is from Adobe Stock, who you might assume would have higher standards than Pinterest and TikTok, but here we are. reply dakiol 1 hour agoprevWouldn't surprise me if in a few years Google, for certain keywords: - autogenerates URLs (tha look legit) - autogenerates content for such URLs (that look kinda legit) All of this would be possible if one is using Chrome (otherwise the fake URLs wouldn't lead to anywhere). Of course, full of ads. Think about it, some people are not really looking for some web site that talks about \"baby peacocks\". They are looking for baby peacocks: content, images, video. If Google can autogenerate good-enough content, then these kind of users would be satisfied (may not even notice the difference). Maybe Google ditches the URL and all: type keywords, and get content (with ads)! reply Crosseye_Jack 1 hour agoparent> would be possible if one is using Chrome (otherwise the fake URLs wouldn't lead to anywhere). Didn't they do something like that with AMP. I recall that if you were using chrome and visited an AMP site from Google the address bar would say site.com even though the content was being served from google.com. reply onemoresoop 1 hour agoparentprevYou're giving them ideas. reply 01HNNWZ0MV43FF 1 hour agoparentprevLike a search engine? reply dakiol 1 hour agorootparentLike a \"generate engine\". reply wenbin 1 hour agoprevIn the near future, a significant portion of YouTube videos and podcasts will likely be AI-generated (e.g., through tools like Notebook LM). However, I'm uncertain whether audiences will truly enjoy this AI-generated content. Personally, I prefer content created by humans—it feels more authentic and engaging to me. It’s crucial for AI tools to include robust detection mechanisms, such as reliable watermarks, to help other platforms identify AI-generated content. Unfortunately, current detection tools for AI-generated audio are still lacking - https://www.npr.org/2024/04/05/1241446778/deepfake-audio-det... [Edit] We just put together a list of notebooklm generated \"podcasts\": https://github.com/ListenNotes/notebooklm-generated-fake-pod... Consider whether you'd enjoy listening to AI-generated podcasts. I believe people might be okay with shows they create themselves, but are less likely to appreciate 'podcasts' ai-generated by others. reply poincaredisk 1 hour agoparent>Personally, I prefer content created by humans—it feels more authentic and engaging to me. I'd like to think that too, but I wonder how long - if at all - this will be true. I \"want\" to like human generated content more, but I suspect AI may be able to optimize for human engagement more, especially for simple dopamine inducing content (like tiktok videos). After all, we're less complicated than we like to think. >It’s crucial for AI tools to include robust detection mechanisms, such as reliable watermarks, to help other platforms identify AI-generated content. This will never work, unfortunately. There's no way to exclude rogue actors, and there's plenty of profit in AIs pretending to be human. If anything, we will have to watermark/sign human generated content. reply Havoc 1 hour agoparentprevQuestion is how long till you can’t tell the difference reply sekh60 1 hour agorootparentMy FAANG working spouse thinks that AIs and Robocallers should be mandated to identify themselves. She thinks a audible \"Beep-boop\" at the end of a sentence for calls and video would be appropriate. reply rootusrootus 1 hour agorootparentI support that idea. Along with properly implemented authentication so you can't just spoof your way to someone's phone, and painfully stiff fines for violators. reply jhbadger 1 hour agorootparentprevIt's almost impossible now. NotebookLM really impressed me. I knew voice synthesis has gotten better than Stephen Hawking's \"voice\" but I really wasn't expecting having two realistic voices with emotions that even banter with each other. There is a bit of banality to them - they like to call something a \"a game changer\" practically every \"podcast\" and the insights into the material is pretty shallow, but they are probably better than the average podcaster already. reply Xenoamorphous 12 minutes agorootparentFun fact, Stephen Hawkins could’ve used a much “better” synthetic voice but decided to stick with that one as people associated it with him. reply mvdtnz 48 minutes agorootparentprevIt's impressive at first until you realise they're practically ad libbing a script. They're filled with all the same annoying American clichés (\"you know me, I like x\", your aforementioned \"a game changer\", plenty of \"wow\"). It would be impossible to listen to two in a row without realising how repetitive it is. reply wenbin 1 hour agoparentprevAt Listen Notes, we recently removed over 500 fake podcasts generated by Notebook LM in just the past weekend. It's disappointing to see scammers and black-hat SEOs already leveraging Notebook LM to mass-produce fake podcasts and distribute them across various platforms. reply hooverd 1 hour agoparentprevPersonally I'm opposed to the unlimited slop machine. reply belval 1 hour agoprevI have a hard time explaining why, perhaps because I did not know what a baby peacock looks like, but this somehow really drove home the \"dark side of AI\" for me. I have gotten used to trusting search results somewhat. Sure there would be oddball results and nearly non-sensical ones, but they would be scarce through a sea of relevant images. Now with this, I would be blind to the things I don't know and as someone who grew up it with Google \"just being there\", it truly scares me. reply nelup20 1 hour agoprevOne of the replies mentions this uBlock Origin AI blocklist (haven't tried it myself): https://github.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist reply speedgoose 1 hour agoparentI think we should have an allow list approach at this point. Maybe a web directory of trustful websites. reply itsafarqueue 1 hour agorootparentYeeha! reply viewtransform 2 hours agoprevsearch \"baby peacock before:2023\" => doesn't have AI generated images reply candiddevmike 2 hours agoparentThe Internet equivalent of pre-war steel: https://wikipedia.org/wiki/Low-background_steel reply ravetcofx 2 hours agorootparentAnd it's hard to fully trust any post 2021 text as well. It's pushing me to seek information from pre 2021 books for information. reply waymon 1 hour agorootparentprevWonder who will coin said term. Before AI. Pure clean internet 1991-2022 reply pndy 56 minutes agorootparentYou sure about that \"pure clean\" part? reply BuyMyBitcoins 1 hour agorootparentprevLow background jpegs! reply readyplayernull 2 hours agoparentprevUntil Google breaks search syntax/tags again... reply nunez 1 hour agoparentprevworks for reddit also! reply giarc 2 hours agoprevI'm a part time maker and purchase a lot of designs off of Etsy to make into physical goods. I have to weed through so many AI images when purchasing designs off of Etsy now. I wish they required users to indicate if AI was used to produce the image so I could then filter them out. reply andrewmunsell 2 hours agoparentSellers are actually supposed to mark items as AI generated/assisted, where applicable: https://techcrunch.com/2024/07/09/etsy-new-seller-policy-202... Whether they actually do this (and whether there's any incentive to do so), is obviously not a given reply dawnerd 2 hours agoparentprevSame now when trying to find 3d models to print. It's just a whole bunch of hueforge ai spam. reply Metricon 1 hour agoprevThere are a number of ways this might get solved, but I would speculate that it will generally be solved by adding image metadata that is signed by a certificate authority similar to the way SSL certificates are assigned to domains. I think eventually all digital cameras and image scanners will securely hash and sign images just as forensic cameras do to certify that an image was \"captured\" instead of generated. Of course this leaves a grey area for image editing applications such as Photoshop, so there may also need to be some other level of certificate base signing introduced there as well. reply ErikAugust 2 hours agoprevSomeone needs to invent a “for humans, by humans” web. Possibly a future luxury. reply rwmj 2 hours agoparentThe good news is that some AI company will invent this, to provide a source for their LLM. reply randomcatuser 2 hours agorootparentthis reminds me of the matrix reply pndy 42 minutes agoparentprevThere was this image that was circling some 20 years ago around and later, with the Internet becoming a cable tv-like service where you'd be a subscriber to particular big companies sites and additional \"free-range\" pages So the pessimist in me can see the Internet being affected by the free-vs-premium formula: \"basic\" Internet with ads, tracking, AI fillers, limited access to +18 content, in the worst form comes with these pre-defined sites and \"premium\" that's free of these limitations but it also in time tries to squeeze more money from users - like \"premium but with ads\" reply Retr0id 1 hour agoparentprevIt's so hard to do this because the better you make it, the more valuable it is for someone to a) scrape it all b) try to insert fake content reply sbrother 1 hour agoparentprevI feel like this is what we are trying to do at Reddit, but needless to say it's going to get harder and harder. reply BuyMyBitcoins 1 hour agorootparentI sense Reddit is a lost cause. Even before the latest wave of generative AI you could tell things were heavily manipulated. I dare say that I haven’t noticed that much of a change in things and that could either be because LLMs are just that good at Reddit content, or that because Reddit was already so botted and manipulated it didn’t really change much. reply rwmj 55 minutes agorootparentReddit started to decline dramatically once they started to charge for API access last year. Mods on a politics subreddit I was talking to said all the free tools they used to keep on top of things stopped working, so they could no longer filter out the trolls and bots. I sure hope the money that Reddit made makes up for the readers who are fleeing. reply claudiulodro 1 hour agoparentprevHear me out: is that Wikipedia? I am sure people are submitting all sorts of AI-generated information, but it's probably getting rejected? (If someone better informed than me has any data one way or the other, I'm super curious) reply unsigner 1 hour agorootparentQuite contrary, people are gleefully machine translating Wikipedia to make more Wikipedia (in different languages). And arguing for it. reply devmor 1 hour agoparentprevLLMs are reinforced through adversarial training - you would essentially be playing a keep-up game with AI generated garbage that would get exponentially more difficult to pull ahead in. reply readyplayernull 1 hour agoparentprevA new sales point for Web3. reply t_mann 1 hour agorootparentHow so? reply limaoscarjuliet 51 minutes agoprevA lot of comments amount to: \"Internet is dead\". AI is crap for sure but far from making Internet dead or useless. Consider: - emails, - bills and payments, - banking, - searching for and buying stuff (assuming you already know what you want that is), - calls/chats - whatsapp, messenger, etc. - youtube (for learning), - social stuff - however bad. AI? This shall pass too. Internet will find its way. reply alkonaut 1 hour agoprevMake a search engine that doesn’t have AI result except when I specifically ask for it, or you soon won’t have a search engine business. A really quick fix is to search with “-ai” and that Google doesn’t do this implicitly for images is really strange. reply zendaven 52 minutes agoparentThe hard part is identifying what's AI generated and what's not. reply headsupernova 55 minutes agoparentprevHow do you expect them to implement that? reply robsh 1 hour agoprevDDG/Bing had only one Ai image for my search of baby peafowl. Unfortunately the one AI image was from stock.adobe.com reply steelframe 1 hour agoprevI'm at a juncture in my career where I'm asking what could really motivate me to do anything that I really feel is worth doing in tech. In my earlier years I remember using both CompuServe and Prodigy. I'm not sure if it just hindsight colored by nostalgia, but I yern for the feeling I had as a young teenager when I could explore a quirky and curated world of information. I'm starting to think that all this AI stuff has finally pushed the ads-based Internet past its tipping point. I feel I could be motivated to work on a walled garden with moderation paid for by subscription fees. What would it be worth to you to have an entirely new online experience free of all the enshittification of the past 15 years? Personally, I pay for Kagi just to have a small taste of what that could be like. But what if not just the search engine, but also all the sites be funded entirely by a subscription fee paid to the service profider? What if privacy could be a foremost feature of that world? What if advertising and astroturfing were strictly forbidden, and human authors would have to be vetted by other humans to be allowed a place in this world? \"This content is Certified ads- and AI-Free(tm).\" I really don't know how well something like that would turn out in 2024, but I feel I wouldn't be alone in wanting to give it a try. reply throwway120385 51 minutes agoparentWe could also have a public library but for the Internet. A list of sites and articles curated and maintained by librarians and experts and paid for by local taxes. reply Rugu16 1 hour agoprevFacebook is flooded with this! Fake photo of poor people asking for help and you see thousands of like and people commenting how they can help reply djhworld 1 hour agoparentA good chunk of those replying will be bots reply zero-g 1 hour agorootparentWhat’s the idea? Why do they use bots to reply? reply oaththrowaway 1 hour agorootparentTo make it look like it is has actual engagement reply 1 hour agoprevnext [2 more] [deleted] dudus 1 hour agoparentAI is not a \"species\" in any sense of the word. reply oniony 1 hour agoprevI noticed recently when searching for images of cities that they're nearly all over-the-top unrealistic HDR images, beyond what you used to find in an travel agent's catalogue. reply carabiner 9 minutes agoprevOpenAI must be destroyed! reply t0bia_s 47 minutes agoprevTry google images of \"white couple\". You would be surprised with irrelevant results. reply idunnoman1222 51 minutes agoprevIf you Google meat Cove it’s presented as a “human settlement” in Nova Scotia I predict the word meat fucked with the AI reply h2odragon 2 hours agoprevactual baby peacocks are almost indistinguishable from guinea hatchlings and there's a strong resemblance to baby chickens or turkeys. reply morkalork 2 hours agoparentYou know that, but what about some kid looking on google images? reply MeetingsBrowser 2 hours agorootparentI’m an adult and I didn’t know that reply lexicality 1 hour agoprevI've started thinking more and more about a short throwaway conversation in Anathem about how the internet in their world is absolutely ruined by AI and the only solution they have left is a user driven reputation system for entities and how one of the characters just earned a lot of \"reputons\" for recording an event. Mostly I think about how something like that is going to be signed into law by some state and it'll require everything you do to be linked to your government issued ID card so they can \"prove\" you're not spreading AI misinformation and all the horrendous unintended side effects that will spread from there. reply isoprophlex 1 hour agoparent\"Anyone can post information on any topic. The vast majority of what's on the Reticulum is, therefore, crap. It has to be filtered. The filtering systems are ancient. My people have been improving them, and their interfaces, since the time of the Reconstitution.\" ... \"Asynchronous, symmetrically anonymized, moderated open-cry repute auction. Don't even bother trying to parse that. The acronym is pre-Reconstitution. There hasn't been a true asamocra for 3600 years. Instead we do other things that serve the same purpose and we call them by the old name. In most cases, it takes a few days for a provably irreversible phase transition to occur in the reputon glass - never mind - and another day after that to make sure you aren't just being spoofed by ephemeral stochastic nucleation.\" Fantastic book. I read it twice so far, highly recommended. So many little off-handed conceptual gems everywhere. reply duxup 1 hour agoprevI was searching google images for \"cat professor\" recently. Same, as far as I could tell all AI garbage with weird saturation and colors and uncanny valley .... they look weird / didn't work for me. reply bongodongobob 1 hour agoparent... Did you want a picture of a real cat professor? reply duxup 1 hour agorootparentI was hoping for a picture of a real cat. There's a different look that real photos have. The AI photos all look like computer polished weirdness. reply bongodongobob 1 hour agorootparentSo generate your own. reply duxup 39 minutes agorootparentI do not have the required animal. reply plinnie 1 hour agoprevIt sounds a bit unfair to use content from a site without crediting the source in an obvious way. I'm sure this shameless content hijacking can't continue as in the end there will not be any source to query. Robots.txt should allow meta-tags like block 'all AI' bots (or these AI companies should pay their sources). reply mg 1 hour agoprevWell, nearly all of the Google Images results for \"Woman\" show a woman with makeup and additionally the photos were altered via Photoshop. We have been creating our own reality even before AI. reply noemit 1 hour agoparentthe incel to HN pipeline is now complete reply jachee 1 hour agoparentprevCreating a version of reality is significantly different from conjuring abject falsehoods. There is an objective reality for what (e.g.) a baby peacock should look like, and this AI slop is inherently misleading about that. reply the_gorilla 1 hour agoparentprevIs this a joke? Right, women wear makeup, might as well AI generate every image we look for. reply zamadatix 1 hour agoprevWell, on the \"bright\" side, all but 2 of the striked ones are either explicitly AI generated art (the 3 Adobe Stock ones, 2 from freepik, and the 1 from Instagram) or about noting the images aren't real (the 2 Snopes and the 1 in the bottom left calling out the feet). On the sad side the TikTok and YouTube ones that likely led to all of this aren't labeled and are present, not to mention the complete lack of \"I want the AI things automatically filtered, I'm not interested in trends I'm searching for actual things right now\" button. Without something like that it will become harder to use Google to find new content. I mean people obviously like the content, it's cute enough to get shared around so much to make itself popular in these images and to trigger the post on X about it. Nothing wrong with that... but if it's not easily filterable for what the user is actually trying to find then Google has somewhat failed at its goal. reply macinjosh 1 hour agoprev\"peacock chick\" has many more real images than AI reply AStonesThrow 2 hours agoprevOr, consider using Wikimedia Commons, where images are painstakingly categorized, documented, and freely licensed: https://commons.wikimedia.org/wiki/Category:Pavo_cristatus_(... reply BuyMyBitcoins 1 hour agoparentI’ve had a mild amount of success asking some nature photographers if they would be willing to make a few of their photos freely licensed so that they can be used on Wikipedia articles. Wikimedia is a fantastic resource. reply AStonesThrow 1 hour agorootparentCreative Commons offers a portal if you wish to cast a wider net (music, video, 3D models) It also includes Google Images and Flickr. https://search.creativecommons.org/ I found the peachicks on Commons by searching \"peacock\" and then following categories up the tree. If people use the wrong search engine with naïve search terms, I don't know what to tell ya. This is a parallel example of why reference librarians are still worth consulting, because they will guide you to the library's resources and databases, and demonstrate how to use search queries. reply jeffbee 1 hour agoprevIf you use the applicable phrase \"peacock chick\" or \"peacock hatchling\" then the results are better. Garbage in and all that. reply throwgfgfd25 1 hour agoprevI wonder: do all the HNers who are excited about their GenAI product or wrapper or startup understand, at a fundamental level, that they are an intrinsic part of this deterioration? Or is this one of those fundamental attribution error things: - MY product is a powerful tool for creators who wish to save time - THEIR product is just a poorly-though-out slop generator Does it occur to people to instead be part of something real and visceral, and not just blame social media's ad-driven impression model, not pretend they are only part of a trend for which they can't be totally blamed? reply ahmeneeroe-v2 41 minutes agoparentYou have only had google image search for what, 20-years? Why do you think it is a fundamental part of humanity's growth story? You talk about being a part of something \"real and visceral\" but you're complaining about the demise of being able to sit at your desk and see pictures of wildlife. Maybe it's okay that google image search dies and makes people go out and find the wildlife they want to see. The internet, even in its best format (e.g. ad-free, free access information for all; and communication with all of humanity) has a ton of real downsides. It's not clear to me that AI should be strangled in its infancy to save the internet (which does _not_ exist in that \"best\" format). reply dang 43 minutes agoparentprevUnfortunately your comment is doing the same thing, just at a different level—something like this: - I am a thoughtful technologist, building real things for real people, concerned about others and the social impact of my work; - they are greedy and ignorant, destroying society for their own short-term gain and caring nothing about the consequences. It's human nature to put badness on an abstract them, but we don't get anywhere that way. It's good for getting agreement (e.g. upvotes), because we all put ourselves in the sweet I bucket and share the down-with-they feeling. But it's self-defeating because it leads to more of what it decries. reply rachofsunshine 42 minutes agoparentprevThere's a sort of \"technological fundamental attribution error\" that comes into play a lot with new technologies. Every past technology has, whatever its benefits to humanity, become substantially tarnished by abuse and malicious use. But this one won't be! Promise! That said, I don't really think this is a tide any individual market actor can reasonably stem. It's going to require some pretty fundamental changes in the way we use the internet. reply doctorpangloss 56 minutes agoparentprevSure but doesn't every technological development have these tradeoffs? You could say what you say about anyone at any time. Where do you draw the line? I guarantee you'll be guilty of the exact same thing. I don't want to generalize, but IMO this sentiment of yours, I hear most loudly from software engineers far removed from ordinary non-technical end users: is making beautiful new LISPs and CNIs and Python package auditing tools the only valid work with seemingly no tradeoffs? reply add-sub-mul-div 50 minutes agorootparentThe problem with this line of reasoning is that things can get steadily worse and you'll never be allowed to say or do anything about it. No, everything is not the same as everything else. reply throwgfgfd25 49 minutes agorootparentprev> I hear most loudly from software engineers far removed from ordinary non-technical end users I am absolutely not far removed from non-technical end users. They are my client base, ultimately. As a freelancer I focus on building real things that make things better for people whose faces and voices I get to know. GenAI will be useless to them, because it is antithetical to what they do. And that focus is only getting keener; I want nothing to do with the AI-generated web. reply doctorpangloss 41 minutes agorootparent> They are my client base, ultimately... I focus on building real things that make things better for people... faces and voices I get to know. So what I'm hearing is, \"I agree very strongly with the people who pay me.\" Or to put it in your words: \"MY product is a powerful tool for creators who wish to save time.\" \"THEIR product is just a poorly-thought-out slop generator\" reply griftwood 54 minutes agorootparentprevEvery technical advancement has tradeoffs. Not every technical advancement has billions of dollars sloshing around doing absolutely nothing except making the web worse and further ruining the environment. What a shockingly bad-faith way to interpret GP's argument, wow. reply doctorpangloss 46 minutes agorootparentThe comment is an interesting but very cookie cutter sort of vamp and drama. The comment trades in a bunch of generalization, much like yours, and you know, generalization doesn't feel good when it directly attacks you. I don't sincerely believe that people who are working on Kubernetes features or observability tools are bad people. Do high drama personalities who engage in a mode of discourse of \"wow\" and \"shockingly\" say valid things too? Yeah. But it's as simple as, log in your own eye before you worry about the thorns in others. Exceptionally ironic because the poster is vamping about \"Attribution errors.\" Another POV is, shysters project. reply darajava 53 minutes agoparentprevAre you saying AI isn’t useful? My product is painstakingly crafted and uses AI but in my opinion it uses it tastefully and with great utility. Also 95%+ of my development efforts are not on improving the AI even though I use a .ai TLD. I think it’s crazy for a modern company/product _not_ to use AI, and the grifters building clear wrappers for GPT and other insanely low-quality efforts are already pretty much dead. reply throwgfgfd25 51 minutes agorootparent> Are you saying AI isn’t useful? My product is painstakingly crafted and uses AI but in my opinion it uses it tastefully and with great utility. Sure. And THEIR products are just thoughtless slop generators. reply carabiner 49 minutes agoparentprevDid they care over the past 10 years where they decimated social lives, city night life, and humanity? https://sherwood.news/world/still-searching-for-that-connect... reply add-sub-mul-div 49 minutes agoparentprevWas it ever any different with social media, surveillance advertising, SEO, NFTs, etc? reply elliotec 58 minutes agoparentprevThey want money, they’re riding the hype wave, not advancing anything and I’m sure most or all know it. reply dang 48 minutes agorootparent\"Please respond to the strongest plausible interpretation of what someone says, not a weaker one that's easier to criticize. Assume good faith.\" https://news.ycombinator.com/newsguidelines.html reply griftwood 55 minutes agoparentprevnext [2 more] [flagged] throwgfgfd25 50 minutes agorootparentLuckily I genuinely don't care :-) reply NoMoreNicksLeft 1 hour agoprevIt's not even the right terminology. I think you should probably use \"peafowl\". The search \"peafowl chicks\" seems to return all real images. reply theginger 1 hour agoparentI think this is kind of key to the issue, the good content is there if you know how to find it. But if you don't know the right terminology then you are going to search for baby peacock and get bad results. reply throwway120385 48 minutes agorootparentKind of like how the Internet worked pre-Google. reply sixothree 1 hour agoprevIf this is the beginning, where are we going to be in 2035? I just can't imagine it without being so wildly speculative. reply worik 56 minutes agoprevIs this showing the world your search bubble? reply booleandilemma 1 hour agoprevIt seems like the image results for \"baby peacock\" are returning articles talking about AI-generated photos of baby peacocks due to some recent trend involving an AI-generated baby peacock image. Have people tried searching for other animals? Maybe this isn't a case of Google being inundated with AI-generated photos, but just something to do with the results for this particular phrase. reply bparsons 1 hour agoprevI am reminded of the decline of MySpace. It was just thousands of bots posing as users, posting ads on people's pages for e-books, pharmaceuticals etc. The bots remained talking to each other long after the last humans left. reply dukeofdoom 2 hours agoprevI stopped using Google search...why even bother now. Results are just some crappy page with ads. Aastroturfed wikpedia page is also suspect. Chatgpt can answer questions in seconds. Just not sure if correct, but most of the time more than good enough. I feel like Google is destroying their credibility by day. Just go to zoo to see peacocks and take pictures. At least it will be an real experience not some virtual manipulation reply fwip 1 hour agoparentThe described problem is AI-slop in Google, and your solution is to drink directly from the spigot of ChatGPT? reply t_mann 1 hour agorootparentI suppose the logic could be: if you're going to consume AI generated content anyway, why not use a setup where you have control over the system prompt and other parameters? Not sure if ChatGPT qualifies there, though. reply dukeofdoom 1 hour agorootparentprevMy solution is to use the computer as little as possible. Go see the world to know what a peacock looks lie. Last time I've seen Peacocks was in Lisbon in St. George's castle, 4 years ago. The kind of questions I ask ChatGPT, are mostly code questions. Or for it to help me with planning something. I ask it questions, and it can provide some sort of logic behind the answer which I can then reason about. Sure it can mislead me, but it's more like an ongoing conversation I'm having with it. So its more like an opinion I'm getting, and I discount it. I'm generally a skeptical person. So I'm well aware of the manipulations that are happening online. Google is just a weaponized player in misinformation warefare at this point. It purposely will go out of its way to build consensus, for conflicts. Bunch of technocratic Billionaire overlords would get you to support genocide if it would benefit them. So I just don't trust google at this point for anything news related. And the rest of their content seems to be just a giant trap of spam pages. reply josefritzishere 2 hours agoprev [–] AI is the ultimate enshittification. reply RodgerTheGreat 1 hour agoparent [–] everything terrible about SEO, but now exponentially cheaper and faster to excrete. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A significant portion of Google image results for \"baby peacock\" are AI-generated, highlighting a broader trend of AI content affecting search quality.",
      "Users express frustration over AI-generated content in search results, particularly in areas like product comparisons and medical information, leading some to seek alternatives like Kagi.",
      "There is a growing demand for certification of human-generated content, as users reminisce about the more authentic internet of the past."
    ],
    "points": 238,
    "commentCount": 178,
    "retryCount": 0,
    "time": 1728318303
  },
  {
    "id": 41762468,
    "title": "How do HTTP servers figure out Content-Length?",
    "originLink": "https://aarol.dev/posts/go-contentlength/",
    "originBody": "How do HTTP servers figure out Content-Length? Published 06.10.2024 • Last modified 07.10.2024 Anyone who has implemented a simple HTTP 1.1 server can tell you that it is a really simple protocol. Basically, it’s a text file that has some specific rules to make parsing it easier. All HTTP requests look something like this: 1 GET /path HTTP/1.1\\r Host: aarol.dev\\r Accept-Language: en,fi-FI\\r Accept-Encoding: gzip, deflate\\r \\r The first line is the “request line”, and it has the requested method, path and HTTP version. The following lines are headers, each terminated with “carriage return” and “line feed” characters. There is also an extra CRLF at the end, to mark the end of the header section. After that, there is the message body which can whatever data you want to send. Here is a simple Go program to demonstrate this, using a raw TCP socket for the client: func main() {// Setup an HTTP server to respond to the path \"/test\"http.HandleFunc(\"GET /test\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello\")) w.Write([]byte(\" world!\"))})go http.ListenAndServe(\"localhost:2024\", nil)// Connect to the server using TCPconn, err := net.Dial(\"tcp\", \"localhost:2024\")if err != nil { panic(err)}// Write the HTTP request (no body, only request line and Host header)_, err = conn.Write([]byte(\"GET /test HTTP/1.1\\rHost: localhost\\r\\r\"))if err != nil { panic(err)}buf := make([]byte, 1024) // 1 kb// Read the responsen, err := conn.Read(buf)if err != nil { panic(err)}fmt.Println(string(buf[:n])) } When I tried this, I got the following response: HTTP/1.1 200 OK Date: Sun, 06 Oct 2024 14:51:13 GMT Content-Length: 12 Content-Type: text/plain; charset=utf-8 Hello world! One interesting thing about this is the header Content-Length, which has a value of 12. That’s exactly the length of \"Hello world!\" in UTF-8. In the above Go code, writing the response happens in two parts: first we write\"Hello\", then we write \" world!\". Notice that we didn’t need to call any function to write the headers, but they are still in the response. In Go’s http package, a status of 200 and the headers are automatically written if w.Write() is called before w.WriteHeader(). Any calls to w.WriteHeader() after that are useless and will output a warning. Remember that in HTTP, the headers are always written before the body. How is it possible that before writing “Hello” to the connection, the server already knows how long the response will be? What if I wanted to write one “Hello”, and then a thousand exclamation marks after that? Or a million? Does the server need to know how long every response is before sending it? It would mean that every response needs to be kept in memory for the entire duration of the handler. I wanted to figure this out, and I didn’t have to look too deep. I found this amazing comment in the standard library’s net/http/client.go file. Basically, it says that if the response is small enough to fit into one “chunking buffer”, the length can be calculated very easily, and sent all at once. If the response is bigger than the buffer, it is sent in chunks. What does this mean in practice? I’ve modified the above code to demonstrate it: func main() {http.HandleFunc(\"GET /test\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello\")) w.Write([]byte(strings.Repeat(\"!\", 3000)))})go http.ListenAndServe(\"localhost:2024\", nil)// removed all error handling for brevityconn, _ := net.Dial(\"tcp\", \"localhost:2024\")conn.Write([]byte(\"GET /test HTTP/1.1\\rHost: localhost\\r\\r\"))buf := make([]byte, 1024) // 1 kbfor { conn.SetReadDeadline(time.Now().Add(1 * time.Second)) n, err := conn.Read(buf) if err != nil {break } fmt.Println(string(buf[:n]))} } The handler now returns “Hello!!!!!!!!!!! …” with 3000 exclamation marks. This is bigger than the configured chunk size, so we can see what happens. This is the response: HTTP/1.1 200 OK Date: Sun, 06 Oct 2024 16:43:28 GMT Content-Type: text/plain; charset=utf-8 Transfer-Encoding: chunked 800 Hello!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 3bd !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 0 There is no Content-Length header in the response now. Instead, we have a Transfer-Encoding: chunked. Our message is being chunked, sent in multiple parts, so that the server doesn’t need to fit the whole thing into memory at once. Clever! The first line of the response data is now a number “800”. Why 800? As it turns out, the number is actually a hexadecimal number, 0x800 is 2048 in base 10. Similarly, 0x3bd, is 957 in base 10. 2048+957=3005, which is the full length of our message! Sending the length of the message before the actual message is a common method to efficiently transfer unknown lengths of data. It is used in the Redis protocol for example. Chunked transfer encoding was added in HTTP 1.1.2 This means that it’s very old and basically all HTTP servers & clients support it. Sending chunked responses also allows something called “trailers”, which are headers that are sent after the body. In Go, trailers need to be explicitly declared before sending them: http.HandleFunc(\"GET /test\", func(w http.ResponseWriter, r *http.Request) {w.Header().Set(\"Trailer\", \"My-Trailer\")w.Write([]byte(\"Hello\"))w.Write([]byte(strings.Repeat(\"!\", 3000)))w.Header().Add(\"My-Trailer\", \"test\") }) This is useful for things like digital signatures, which need to be computed from the response body. In Go’s http handlers, adding trailers will also automatically make the response chunked even if it normally wouldn’t be. HTTP/2 and HTTP/3 don’t support chunked transfer encoding, as they have their own streaming mechanisms.2 This is one of the many things that happen in the background when using the net/http package in Go. http.ResponseWriter is a remarkably simple interface, having just 3 methods on it. Behind this API is a lot of magic, including Content-Type sniffing and the implicit header writing I mentioned above. It’s great that these things are handled automatically, but in programming I think it’s always important to have an idea of what is happening at the layer below. Comments? See the discussion on Hacker News. To make it easier to read, I’ve put them all on separate lines. ↩︎ Source: https://en.wikipedia.org/wiki/Chunked_transfer_encoding ↩︎ ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=41762468",
    "commentBody": "How do HTTP servers figure out Content-Length? (aarol.dev)224 points by misonic 15 hours agohidepastfavorite93 comments hobofan 12 hours agoI think the article should be called \"How do Go standard library HTTP servers figure out Content-Length?\". In most HTTP server implementations from other languages I've worked with I recall having to either: - explicitly define the Content-Length up-front (clients then usually don't like it if you send too little and servers don't like it if you send too much) - have a single \"write\" operation with an object where the Content-Length can be figured out quite easily - turn on chunking myself and handle the chunk writing myself I don't recall having seen the kind of automatic chunking described in the article before (and I'm not too sure whether I'm a fan of it). reply danudey 0 minutes agoparent> explicitly define the Content-Length up-front (clients then usually don't like it if you send too little and servers don't like it if you send too much) We had a small router/firewall thing at a previous company that had a web interface, but for some reason its Content-Length header had an off-by-one error. IIRC Chrome handled this okay (once the connection was closed it would display the content) while Firefox would hang waiting for that one extra byte that never came. reply lifthrasiir 11 hours agoparentprevI believe the closest prior art would be PHP. It buffers a response by default until the buffer is full or `flush()` gets called, and will automatically set `Content-Encoding: chunked` if `Content-Length` wasn't explicitly set. Any subsequent writes will be automatically chunked. This approach makes sense from the API standpoint because the caller generally has no idea whether the chunked encoding is necessary, or even its very existence. Honestly that's less confusing than what express.js does to the middleware function: `app.get(\"/\", (req, res) => { ... })` and `app.get(\"/\", (req, res, next) => { ... })` behave differently because it tries to infer the presence of `next` by probing `Function.prototype.length`. reply nolok 10 hours agorootparentFun thing about that : core PHP used to and still is very very close to HTTP, to the point where I would say your average decent PHP programmer used to knows more about how HTTP work that your average other similar language where the web library abstract stuff. Eg a PHP dev knows a form has to be multipart/form-data if you send files etc ... But one of the if not THE major exception is this : buffering and flushing works automagically and a lot of PHP dev end up massively blindsinded by it at some point PS: with the rise of modern PHP and it's high quality object based framework, this become less and less true PS2: I am not in ANY way saying anything good or bad or superior or inferior about any dev here, just a difference in approach reply JodieBenitez 9 hours agorootparentWell... not sure it was that magic. We used to play a lot with the ob_* functions. reply nolok 6 hours agorootparentOh I didn't mean it figured stuff out in a smart way, only that it did auto buffering/chuncking/flushing for you in a way to abstract that whole idea from the dev, while other plateform had you care about it (cf above messages). But yeah the moment you ended up wanting to do anything advanced, you were doing your own buffer on top of that anyway, or disabling it and going raw. reply JodieBenitez 6 hours agorootparentand the joy of \"pure php scripts\" with closing ?> tags messing with the output when you didn't want it... all in libraries you had no say... I can't say I miss those days ! Or this platform for that matter. reply lifthrasiir 4 hours agorootparentEventually people started omitting ?> at the very end, which is correct but also unsettling. reply earthboundkid 30 minutes agorootparentWhen I started my first job, I was like \"Hey, these files don't have a closing ?>!!\" and then my boss sighed. reply gary_0 1 hour agorootparentprevIt's even recommended in the PHP docs: https://www.php.net/manual/en/language.basic-syntax.phptags.... reply matsemann 8 hours agorootparentprevThis gave me flashbacks to my youth of PHP programming. \" headers already sent by (output started at ....\", when you tried to modify headers but had already started to write the http content (hence the header part was set in stone) reply nolok 6 hours agorootparentAnd of course it was a space before or after the php delimiters in some random file. reply marcosdumay 6 hours agorootparentGood thing zero width space wasn't common in texts back then when PHP was the most common platform. reply everforward 3 hours agorootparentprevI may be alone here, but I don't it find it that absurd (though the implementation may be if JS doesn't support this well, no idea). This would be crazy in languages that actually enforce typing, but function type signature overloading to alter behavior seems semi-common in languages like Python and JS. Entirely unrelated, but the older I get, the more it seems like exposing the things under \".prototype\" as parts of the object was probably a mistake. If I'm not mistaken, that is reflection, and it feels like JS reaches for reflection much more often than other languages. I think in part because it's a native part of the object rather than a reflection library, so it feels like less of an anti-pattern. reply lifthrasiir 2 hours agorootparentTo be clear, distinguishing different types based on arity would have been okay if JS was statically typed or `Function` exposed more thorough information about its signature. `Function.prototype.length` is very primitive (it doesn't count any spread argument, partly because it dates back to the first edition of ECMAScript) and there is even no easy way to override it like Python's `@functools.wraps`. JS functions also don't check the number of given arguments at all, which is already much worse compared to Python but anyway, JS programmers like me would have reasonably expected excess arguments to be simply ignored. reply everforward 1 hour agorootparent> JS functions also don't check the number of given arguments at all I never really thought about this, but it does explain how optional arguments without a default value work in Typescript. How very strange of a language decision. > To be clear, distinguishing different types based on arity would have been okay if JS was statically typed or `Function` exposed more thorough information about its signature. I actually like this less in a system with better typing. I don't personally think it's a good tradeoff to dramatically increase the complexity of the types just to avoid having a separate method to register a chunked handler. It would make more sense to me to have \"app.get()\" and \"app.getChunked()\", or some kind of closure that converts a chunked handler to something app.get() will allow, like \"app.get(chunked((req, res, next) => {}))\". The typing effectively becomes part of the control flow of the application, which is something I tend to prefer avoiding. Data modelling should model the domain, code should implement business logic. Having data modelling impact business logic feels like some kind of recursive anti-pattern, but I'm not quite clever enough to figure out why it makes me feel that way. reply lolinder 5 hours agorootparentprev> Honestly that's less confusing than what express.js does to the middleware function: `app.get(\"/\", (req, res) => { ... })` and `app.get(\"/\", (req, res, next) => { ... })` behave differently because it tries to infer the presence of `next` by probing `Function.prototype.length`. This feels like a completely random swipe at an unrelated feature of a JavaScript framework, and I'm not even sure that it's an accurate swipe. The entire point of Function.length (slight nit: Function.prototype.length is different and is always zero) is to check the arity of the function [0]. There's no \"tries to\": if your middleware function accepts three arguments then it will have a length of 3. Aside from that, I've also done a bunch of digging and can't find any evidence that they're doing [1]. Do you have a source for the claim that this is what they're doing? [0] https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... [1] https://github.com/search?q=repo%3Aexpressjs%2Fexpress%20%22... reply lifthrasiir 5 hours agorootparentBecause we were talking about HTTP server frameworks, it seemed not that problematic to mention one of the most surprising things I've seen in this space. Not necessarily JS bashing, but sorry for that anyway. I'm surprised to see that it's now gone too! The exact commit is [1], which happened before Express.js 4.7, and you can search for the variable name `arity` in any previous versions to see what I was talking. It seems that my memory was slightly off as well, my bad. The correct description would be that older versions of Express.js used to distinguish \"error\" callbacks from normal router callbacks by their arities, so `(req, res)` and `(req, res, next)` would have been thankfully okay, while any extra argument added by an accident will effectively disable that callback without any indication. It was a very good reason for me to be surprised and annoyed at that time. [1] https://github.com/expressjs/express/commit/76e8bfa1dcb7b293... reply LegionMammal978 5 hours agorootparentActually, it still uses ≤ 3 vs. = 4 arguments to distinguish between request callbacks and error callbacks. Check out the added lines to lib/router/layer.js in the commit you mention, or the equivalent functions in the current router v2.0.0 package [0]. [0] https://github.com/pillarjs/router/blob/2e7fb67ad1b0c1cd2d9e... reply dbrueck 5 hours agoparentprevChunked transfer encoding can be a pain, but it's a reasonable solution to several problems: when the response is too big to fit into memory, when the response size is unknown by the HTTP library, when the response size is unknown by the caller of the HTTP library, or when the response doesn't have a total size at all (never-ending data stream). reply ahoka 8 hours agoparentprevSome frameworks do automatic chunking when you pass a stream as the response body. reply simonjgreen 9 hours agoprevAlong this theme of knowledge, there is the lost art of tuning your page and content sizes such that they fit in as few packets as possible to speed up transmission. The front page of Google for example famously fitted in a single packet (I don't know if that's still the case). There is a brilliant book that used to be a bit of a bible in the world of web sysadmin from the Yahoo Exceptional Performance Team which is less relevant these days but interesting to understand the era. https://www.oreilly.com/library/view/high-performance-web/97... reply NelsonMinar 8 hours agoparentSee also the 14KB website article: https://endtimes.dev/why-your-website-should-be-under-14kb-i... Optimizing per-packet really improves things but has gotten very difficult with SSL and now QUIC. I'm not sure Google ever got the front page down to a single packet (would love a reference!) but it definitely paid very close attention to every byte and details of TCP performance. reply divbzero 2 hours agorootparentThe “Slow-Start” section [1] of Ilya Grigorik’s High Performance Browser Networking also has a good explanation for why 14 KB is typically the size of the initial congestion window. [1]: https://hpbn.co/building-blocks-of-tcp/#slow-start reply ryantownsend 8 hours agorootparentpreviirc, most content delivery networks have now configured initcwnd to be around 40, meaning ~58kb gets sent within the TCP slow start window and therefore 14kb is no longer relevant to most commercial websites (at least with H1/H2, as you mentioned QUIC/H3 uses UDP so it's different) reply divbzero 1 hour agorootparentWhen and where you heard that initcwnd is typically 40 for most CDNs? I was curious but the most recent data I could find was from 2017 when there was a mix of CDNs at initcwnd=10 and initcwnd>10: https://www.cdnplanet.com/blog/initcwnd-settings-major-cdn-p... Currently Linux still follows RFC6928 and defaults to initcwnd=10: https://github.com/torvalds/linux/blob/v6.11/include/net/tcp... reply eastbound 6 hours agorootparentprevAnd the good olden time when IE only supported 31KB of Javascript. reply recursive 2 hours agorootparentIt's time to bring back that rule. reply o11c 18 minutes agorootparentI recently did a deep dive into the history of JavaScript standards: (pre-ecmascript versions of JS not investigated) EcmaScript 1(1997) = JavaScript 1.1 - missing many ES3 features (see below), of which exceptions are the unrecoverable thing. EcmaScript 2(1998) - minimal changes, mostly deprecations and clarifications of intent, reserve Java keywords EcmaScript 3(1999) - exceptions, regexes, switch, do-while, instanceof, undefined, strict equality, encodeURI* instead of escape, JSON, several methods on Object/String/Array/Date EcmaScript 4(2003) - does not exist due to committee implosion EcmaScript 5(2009) - strict mode, getters/setters, remove reservations of many Java keywords, add reservation for let/yield, debugger, many static functions of Object, Array.isArray, many Array methods, String().trim method, Date.now, Date().toISOString, Date().toJSON EcmaScript 5.1(2011) - I did not notice any changes compared to ES5, likely just wording changes. This is the first one that's available in HTML rather than just PDF. EcmaScript 6(2015) - classes, let/const, symbols, modules (in theory; it's $CURRENTYEAR and there are still major problems with them in practice), and all sorts of things (not listed) EcmaScript 11(2020) - bigint, globalThis If it were up to me, I'd restrict the web to ES3 with ES5 library features, let/const from ES6, and bigint/globalThis from ES2020. That gives correctness and convenience without tempting people to actually try to write complex logic in it. There are still pre-ES6 implementations in the wild (not for the general web obviously) ... from what I've seen they're mostly ES5, sometimes with a few easy ES6 features added. reply benmmurphy 6 hours agoparentprevIt is interesting how bad some of the http2 clients in browsers send the first http2 request on a connection. It’s often possible to smush it all into 1 TCP packet but browsers are often sending the request in 3 or 4 packets. I’ve even seen some server side bot detection systems check for this brain dead behaviour to verify it’s really a browser making the request. I think this is due to the way all the abstractions interact and the lack of a corking option for the TLS library. reply pkulak 13 hours agoprevAnd if you set your own content length header, most http servers will respect it and not chunk. That way, you can stream a 4-gig file that you know the size of per the metadata. This makes downloading nicer because browsers and such will then show a progress bar and time estimate. However, you better be right! I just found a bug in some really old code that was gzipping every response when it was appropriate (ie, asked for, textual, etc). But it was ignoring the content-length header! So, if it was set manually, it would then be wrong after compression. That caused insidious bugs for years. The fix, obviously, was to just delete that manual header if the stream was going to be compressed. reply dotancohen 7 hours agoparent> That caused insidious bugs for years. A lot of people here could probably benefit professionally from hearing about what the bugs were. Knowing what to identify in the future could be really helpful. Thanks. reply pkulak 4 hours agorootparentWell, for years our logs would fill up with these nasty warnings of \"connection dropped\", something like that. Naturally, you think that's just some badly configured client, mobile connection, something. But then why would it be configured to log that as a warning (or it may have even triggered an error). I think that was because when payloads are small the compression overhead makes them larger, which means the Content-Length is too small and clients would be terminating the connection early. And getting garbage or truncated responses. Ouch! reply bugtodiffer 12 hours agoparentprevDid you see if you could turn this into HTTP Request Smuggling? Or something else with security impact? Sounds like a powerful bug you have, potentially. reply knallfrosch 12 hours agorootparentTo me it sounds like the server handled the request just fine and reused the header (which was wrong.) The client then had the problem of a wrong response. reply guappa 11 hours agorootparentIf you say to read 1000 bytes from memory and then pass a 900 bytes long array, that's a security bug that can cause crash, corrupt data, and leaking stuff that shouldn't have leaked. reply michaelmior 11 hours agorootparentIt's a bug for sure, but I think whether it's a security issue could depend on the language. If the callee is able to determine the length of the array, it can just return an error instead of a potential buffer overrun. reply stickfigure 3 hours agorootparentprevIn this case, the 1000 bytes aren't being read from memory, they're being read from a socket. If you try to over-read from a socket the worst you'll get is a blocking call or an error (depending what mode you're in). reply jpc0 11 hours agorootparentprevThe size of the buffer and how many bytes are written have nothing intrinsically linked to what the header says. It's a bug sure but does not mean there's any security issue on the server. reply guappa 10 hours agorootparentIt will likely generate corrupt files on the client as well. reply Aachen 8 hours agorootparentNot very. The system might allocate that length ahead of time (I've seen that option in torrent clients and iirc ftp systems) but, latest by the time a FIN comes in, it'll know the file is finished and can truncate it. If finished-early downloads are not implemented despite it doing preallocation, that's still not a security bug reply guappa 6 hours agorootparentif a FIN comes, the client will mark the file as partially downloaded. but it might not come, since decades http sends more than one file per connection, so it might just get the beginning of the next reply, write that and the next reply will be corrupt as well. reply bugtodiffer 9 hours agorootparentprevmaybe response based trickery then? :D What happens to the response after that one, are the first 100 bytes cut of, or what? I'm pretty sure something like this can cause some form of HTTP desync in a loadbalancer/proxy setup. reply flohofwoe 9 hours agoprevUnfortunately the article doesn't mention compression, because this is where it gets really ugly (especially with range requests), because IIRC the content-size reported in http responses and the range defined in range requests are on the compressed data, but at least in browsers you only get the uncompressed data back and don't even have access to the compressed data. reply meindnoch 9 hours agoparent+1 This is why in 2024 you still must use XmlHttpRequest instead of fetch() when progress reporting is needed. fetch() cannot do progress reporting on compressed streams. reply shakna 7 hours agorootparentOnce the header is read, you can iterate over the ReadableStream, though, can't you? reply meindnoch 6 hours agorootparent1. You know the size of the compressed data from the Content-Length header. 2. You can iterate through the uncompressed response bytes with a ReadableStream. Please explain how would you produce a progress percentage from these? reply jstanley 6 hours agorootparentRecompress the ReadableStream to work out roughly how long the compressed version is, and use the ratio of the length of your recompressed stream to the Content-Length to work out an approximate progress percentage. reply meindnoch 5 hours agorootparentLol! Upvoted for creative thinking. reply lmz 6 hours agorootparentprevIf you had control of both ends you could embed a header in the uncompressed data with the number of uncompressed bytes. reply mananaysiempre 6 hours agorootparentOr put that length in the response headers. reply meindnoch 5 hours agorootparentYou won't be able to do this if you're downloading from a CDN. Which is exactly where you would host large files, for which progress reporting really matters. reply meindnoch 5 hours agorootparentprevGood idea. We could give it a new MIME type too. E.g. application/octet-stream-with-length-prefix-due-to-idiotic-fetch-api Or we can keep using XmlHttpRequest. Tough choice. reply jaffathecake 9 hours agoprevThe results might be totally different now, but back in 2014 I looked at how browsers behave if the resource is different to the content-length https://github.com/w3c/ServiceWorker/issues/362#issuecomment... Also in 2018, some fun where when downloading a file, browsers report bytes written to disk vs content-length, which is wildly out when you factor in gzip https://x.com/jaffathecake/status/996720156905820160 reply matthewaveryusa 2 hours agoprevNext up is how forms with (multiple) attachments are uploaded with Content-Type=multipart/form-data; boundary=$something_unique https://notes.benheater.com/books/web/page/multipart-forms-a... reply jillesvangurp 7 hours agoprevIt's a nice exercise in any web framework to figure out how you would serve a big response without buffering it in memory. This can be surprisingly hard with some frameworks that just assume that you are buffering the entire response in memory. Usually, if you look hard there is a way around this. Buffering can be appropriate for small responses; or at least convenient. But for bigger responses this can be error prone. If you do this right, you serve the first byte of the response to the user before you read the last byte from wherever you are reading (database, file system, S3, etc.). If you do it wrong, you might run out of memory. Or your user's request times out before you are ready to respond. This is a thing that's gotten harder with non-blocking frameworks. Spring Boot in particular can be a PITA on this front if you use it with non-blocking IO. I had some fun figuring that out some years ago. Using Kotlin makes it slightly easier to deal with low level Spring internals (fluxes and what not). Sometimes the right answer is that it's too expensive to figure out the content length, or a content hash. Whatever you do, you need to send the headers with that information before you send anything else. And if you need to read everything before you can calculate that information and send it, your choices are buffering or omitting that information. reply jerf 5 hours agoparent\"This can be surprisingly hard with some frameworks that just assume that you are buffering the entire response in memory. Usually, if you look hard there is a way around this.\" This is the #1 most common mistake made by a \"web framework\". Before $YOU jump up with a list of exceptions, it slowly gets better over time, and it has been getting better for a while, and there are many frameworks in the world, so the list that get it right is quite long. But there's still a lot of frameworks out there that assume this, that consider streaming to be the \"exception\" rather than non-streaming being a special case of streaming, and I still see new people make this mistake with some frequency, so the list of frameworks that still incorporate this mistake into their very core is also quite long. My favorite is when I see a new framework sit on top of something like Go that properly streams, and it actively wrecks the underlying streaming capability to turn an HTTP response into a string. Streaming properly is harder in the short term, but writing a framework where all responses are strings becomes harder in the long term. You eventually hit the wall where that is no longer feasible, but then, fixing it becomes very difficult. Simply not sending a content-length is often the right answer. In an API situation, whatever negative consequences there are are fairly muted. The real problem I encounter a lot is when I'm streaming out some response from some DB query and I encounter a situation that I would have yielded a 500-type response for after I've already streamed out some content. It can be helpful to specify in your API that you may both emit content and an error and users need to check both. For instance, in the common case of dumping JSON, you can spec a top-level {\"results\": [...], \"error\": ...} as your return type, stream out a \"results\", but if a later error occurs, still return an \"error\" later. Arguably suboptimal, but requiring all errors to be known up front in a streaming situation is impossible, so... suboptimal wins over impossible. reply Sytten 1 hour agoprevThere is a whole class of attacks called HTTP Desync Attacks that target just that problem since it is hard to get that right, especially accross multiple different http stacks. And if you dont get it right the result.is that bytes are left on the TCP connections and read as the next request in case of a reuse. reply dicroce 5 hours agoprevAt least in the implementation I wrote the default way to provide the body was a string... which has a length. For binary data I believe the API could accept either a std::vector (which has a size) or a pointer and a size. If you needed chunked transfer encoding you had to ask for it and then make repeated calls to write chunks (that each have a fixed length). To me the more interesting question is how web server receive an incoming request. You want to be able to read the whole thing into a single buffer, but you don't know how long its going to be until you actually read some of it. I learned recently that libc has a way to \"peek\" at some data without removing it from the recv buffer..... I'm curious if this is ever used to optimize the receive process? reply mikepurvis 5 hours agoparentNot sure about Linux, but for LwIP on embedded, the buffer isn’t continuous; it’s a linked list of preallocated pbuf objects. So you can either read directly from those if you play by their rules or if you really do need it in contiguous memory you call a function to copy from LwIP’s buffers to one you supply. reply AndrewStephens 5 hours agoprevWhen I worked on a commercial HTTP proxy in the early 2000s, it was very common for servers to return off-by-one values for Content-Length - so much so that we had to implement heuristics to ignore and fix such errors. It may be better now but a huge number of libraries and frameworks would either include the terminating NULL byte in the count but not send it, or not include the terminator in the count but include it in the stream. reply lloeki 10 hours agoprevChunked progress is fun, not many know it supports more than just sending chunk size but can synchronously multiplex information! e.g I drafted this a long time ago, because if you generate something live and send it in a streaming fashion, well you can't have progress reporting since you don't know the final size in bytes, even though server side you know how far you're into generating. This was used for multiple things like generating CSV exports from a bunch of RDBM records, or compressed tarballs from a set of files, or a bunch of other silly things like generating sequences (Fibonacci, random integers, whatever...), that could take \"a while\" (as in, enough to be friendly and report progress). https://github.com/lloeki/http-chunked-progress/blob/master/... reply oefrha 9 hours agoparentI once wrote a terminal-style interactive web app as a single chunked HTML page, because I couldn’t be bothered to implement a websocket endpoint. HTML and inline JS are interweaved and sent in reaction to user actions on the page. The only problem is browsers think the page never finishes loading. reply eastbound 6 hours agorootparentCouldn’t you put the initial page in its own separate html, and load the rest as a long-running JS file using AJAX? reply oefrha 3 hours agorootparentSure, but browsers don't do streaming execution of JavaScript (need to download the entire script), so you then need to manually stream the response and do HTML patching / eval chunked JS, as opposed to the browser trivially loading HTML / executing inline JS as the HTML page is streamed in all by itself. It does solve the loading spinner problem. reply aragilar 11 hours agoprevNote that there can be trailer fields (the phrase \"trailing header\" is both an oxymoron and a good description of it): https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Tr... reply eastbound 6 hours agoparentOh yeah, put the Content-Length after the content, when you know its size! /j reply nraynaud 6 hours agoprevI have done crazy stuff to compute the content length of some payloads. For context one of my client works in cloud stuff and I worked in converting hdd format on the fly in a UI VM. The webserver that accepts the files doesn’t do chunked encoding. And there is no space to store the file. So I had to resort to passing over the input file once to transform it, compute its allocation table and transformed size, then throw away everything but the file and the table, restart the scan with the correct header and re-do the transformation. reply skrebbel 8 hours agoprevI thought I knew basic HTTP 1(.1), but I didn't know about trailers! Nice one, thanks. reply marcosdumay 6 hours agoparentHalf of the middleboxes on the way in the internet don't know about them either. reply klempner 5 hours agorootparentAnd browsers don't support them either, at least in any useful manner. reply Am4TIfIsER0ppos 9 hours agoprevstat()? reply TZubiri 9 hours agoprevlen(response) reply remon 9 hours agoprevTotally worth an article. reply _ache_ 11 hours agoprev> Anyone who has implemented a simple HTTP server can tell you that it is a really simple protocol It's not. Like, hell no. That is so complex. Multiplexing, underlying TCP specifications, Server Push, Stream prioritization (vs priorization !), encryption (ALPN or NPN ?), extension like HSTS, CORS, WebDav or HLS, ... It's a great protocol, nowhere near simple. > Basically, it’s a text file that has some specific rules to make parsing it easier. Nope, since HTTP/2 that is just a textual representation, not the real \"on the wire\" protocol. HTTP/2 is 10 now. reply TickleSteve 11 hours agoparentHe was referring to HTTP 1.0 & 1.1 reply mannyv 6 hours agorootparentParts of 1.1 are pretty complicated if you try and implement them. Parts of it are simple. The whole section on cache is \"reality based,\" and it's only gotten worse as the years have moved on. Anyway, back in the day Content-Length was one of the fields you were never supposed to trust. There's really no reason to trust it now, but I suppose you can use it as a hint to see amount of buffer you're supposed to allocate. But of course, the content length may exceed that length, which would mean that if you did it incorrectly you'd copy the incoming request data past the end of the buffer. So even today, don't trust Content-Length. reply treflop 4 hours agorootparentYou can’t trust lengths in any protocol or format. reply _ache_ 10 hours agorootparentprevShould have say that so (Nowhere does the article say ‘up to HTTP/1.1’ even talking about HTTP/2 and HTTP/3). HTTP/1.0 is simple. HTTP/1.1 is undoubtedly more complex but manageable. The statement that HTTP is simple is just not true. Even if Go makes it look easy. reply Cthulhu_ 10 hours agorootparentEvery example in the article explicitly states HTTP 1.1, only at the end does it have a remark about how HTTP 2 and 3 don't have chunking as they have their own streaming mechanisms. The basic HTTP protocol is simple, but 2/3 are no longer the basic HTTP protocols. reply _ache_ 10 hours agorootparentMy point is HTTP isn't HTTP/1.1. There is a lot under the wood, even with HTTP/1.1. Actually, the fact that the whole article explains an implementation of an HTTP header is against the fact that it's simple. So when the article say \"All HTTP requests look something like this\", that's false, that is not a big deal but it spread that idea that HTTP is easy and it's not. reply dbrueck 5 hours agorootparentprevThe HTTP 1.1 spec isn't 175 pages long just for the fun of it. :) reply lionkor 11 hours agorootparentprevWhich is reasonably simple that you can build a complete 1.0 server in C in an afternoon, and add some 1.1 stuff like keep-alives, content-length, etc. I did that for fun once, on github.com/lionkor/http reply secondcoming 8 hours agorootparentThat’s the easy part. The hard part is working around non-compliant third parties. HTTP is a real mess. reply lionkor 8 hours agorootparenttrue reply pknerd 10 hours agoprev [–] Why would someone implement the chunk logic when websockets are here? Am I missing something? What are the use cases? reply rsynnott 10 hours agoparentHTTP/1.1 came out in 1997. It’s extremely well supported. Websockets were only standardised in 2011, and still have proxy traversal issues. You can absolutely assume that http 1.1 will work on basically anything; websockets are more finicky even now, and certainly were back in the day. reply masklinn 9 hours agorootparentWebsockets are also on the far side of useless when it comes to streaming content the user is downloading. Javascript-filled pages are not the only clients of http. reply dylan604 4 hours agorootparent> Javascript-filled pages are not the only clients of http. Whaaaaa??? We should eliminate these non-JS filled nonsense immediately! reply mannyv 6 hours agoparentprevWeb sockets have their own issues that can be/are implementation dependent. For example, some websocket servers don't pass back errors to the client (AWS). That makes it quite difficult to, say, retry on the client side. Chunked encoding is used by video players - so you can request X bytes of a video file. That means you don't have to download the whole file, and if the user closes the video you didn't waste bandwidth. There are likely more uses of it. reply blueflow 10 hours agoparentprevchunked-encoding is a method of encoding an HTTP response body. The semantics for HTTP responses still apply, caching, compression, etc. Websocket is a different protocol that is started up via HTTP. reply david422 4 hours agoparentprevI would say rule of thumb, websockets are for two way realtime communication, http chunked is just for 1 way streaming communication. reply pknerd 4 hours agoparentprev [–] Wow. Downvoted reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In Go's http package, the Content-Length is automatically set for responses that fit into a single buffer, while larger responses use \"chunked transfer encoding\" to send data in chunks without knowing the total size.",
      "Chunked transfer encoding is efficient for transferring data of unknown lengths and is supported by HTTP 1.1, with each chunk prefixed by its size in hexadecimal.",
      "HTTP/2 and HTTP/3 use different streaming mechanisms and do not support chunked encoding, but the Go http.ResponseWriter interface simplifies handling headers and content type."
    ],
    "commentSummary": [
      "HTTP servers determine Content-Length through explicit definition, single write operations, or manual chunking, which can be complex for developers.",
      "Incorrect Content-Length can cause issues such as browser errors or hanging, especially when compression alters content size.",
      "Chunked transfer encoding is beneficial for streaming large or unknown-sized responses but poses implementation challenges across various HTTP stacks."
    ],
    "points": 224,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1728271084
  },
  {
    "id": 41765435,
    "title": "Hetzner Object Storage",
    "originLink": "https://docs.hetzner.com/storage/object-storage/",
    "originBody": "Docs Storage Object Storage Object Storage Overview List of supported actions FAQ Beta test General S3 credentials Buckets & objects Getting Started Generating S3 keys Using S3 compatible API tools Creating a Bucket Creating a Bucket via MinIO Terraform Provider",
    "commentLink": "https://news.ycombinator.com/item?id=41765435",
    "commentBody": "Hetzner Object Storage (hetzner.com)219 points by polyrand 6 hours agohidepastfavorite157 comments bhouston 5 hours agoI am always scared to put too much trust in new Object Storage services. I love Hetzner and similar but until this new service has been around for a while, I'd only use them for stuff I can afford to lose. From the outside as a consumer of these services you do not know how robust they actually are on the inside. I remember the data loss from OVH where they put backups in the same building as the primaries and people only found out about this once a fire took out the whole building: https://blocksandfiles.com/2023/03/23/ovh-cloud-must-pay-dam... reply numbsafari 5 hours agoparent\"Cloud 3-2-1\" to the rescue... I take the typical formulation (e.g., [1]), and translate it into: - Keep 3 copies of your data: production + 2. - Keep 2 snapshots, stored separately. - Keep 1 snapshot on-hand (literally in your possession), or with a different provider. It's great to see more options for \"different provider\". If I were an early adopter, I would either target this as my on-Hetzner snapshot of my Hetzner-hosted data, and replicate it elsewhere; or I would consider trialing it as a backup to non-Hetzner data. To your point, though, I'd probably wait on the latter until it has gone through some growing pains. [1] - https://www.backblaze.com/blog/the-3-2-1-backup-strategy/ reply bilekas 4 hours agoparentprevYears ago there was a fire in OVH who I always trusted as reliable and stable, never needed to worry.. Lost a lot that day. Probably I could have had offsite backup managed myself though. I see no difference here, you can manage your own disaster recovery. From what I see, this actually might be a great backup/recovery solution for S3 in my case at least. reply voytec 4 hours agorootparentMaybe if OVH had installed an automatic fire extinguishing system or an electric mains cut-off switch, the firefighters wouldn't have struggled with meter-long electrical arcs and could put out the fire quicker. reply liotier 3 hours agorootparentYes, that hosting center was unsafe as hell. All of OVH's early year featured a lot of duct-tape and daring corner-cutting - that is how they were incredibly cheap. But nowadays they are just as mainstream industrial as their competitors and the 2021 fire marked the end of an era... It is sad that it happened as they were already turning the company around to being an established player. A great writeup of the incident and its context, three years afterwards: https://www.datacenterdynamics.com/en/analysis/ovhcloud-fire... reply lopkeny12ko 4 hours agorootparentprevPeople seem to have this misconception that \"cloud computing\" is some kind of magic bullet that guarantees 100% durability. News flash, your data is still being stored on physical hard drives, in some data center. If the building burns down in a fire, of course your data will be gone, the hard drives are hosed, it's not magic. You really have no one to be mad at other than yourself... reply JoshTriplett 3 hours agorootparent> If the building burns down in a fire, of course your data will be gone From https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDu... : > S3 Standard, [...] redundantly store objects on multiple devices across a minimum of three Availability Zones in an AWS Region. An Availability Zone is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. Availability Zones are physically separated by a meaningful distance, many kilometers, from any other Availability Zone, although all are within 100 km (60 miles) of each other. reply mardifoufs 4 hours agorootparentprevMost of the \"big\" cloud providers aren't as negligent as OVH though. That entire datacenter was just one blunder after the other (wood structures, barely functional fire suppression, no proper power shut down, insane replication strategy that caused most of the replicas to burn in the very same fire in the same building...) reply timenova 5 hours agoparentprevI should mention Tigris[0] here. They're also a new Object Storage service, but they have this two-way replication facility with another S3-compatible service. The primary purpose they built it for is to mirror files from your existing S3 to Tigris as files are requested. However they also have an option to copy files that are added to Tigris, to S3 automatically [1] (`--shadow-write-through`). I asked their founder if it's okay to use it as an extra redundancy continuously instead of a one-time migration, and they said they have no issues with it. [0] https://www.tigrisdata.com [1] https://www.tigrisdata.com/docs/migration/#starting-the-migr... reply rmbyrro 3 hours agorootparentBackblaze is 3x less expensive and offers more generous free allowances. reply chasd00 5 hours agoparentprevI toured a data center in Tornado alley back when leasing cages was pretty common. I asked them about disaster planning regarding getting completely wiped off the map and they sorta scoffed at me. Literally two weeks later a tornado missed them by about a 1/4 mile. Would have loved to be a fly on the wall after that. another story, a small company I worked for contracted with a small data center. They did things right and ran a tight ship. However something happened like a lightning strike or so other electrical fault and a critical component welded itself into a position they could t get out of. I wish I could remember the details better but they were down multiple days. reply latchkey 3 hours agorootparentSwitch builds their DCs with two roofs... https://www.switch.com/switch-shield/ The Switch one I'm in (Grand Rapids Pyramid), the servers are about 3/4's underground. reply vidarh 5 hours agoparentprevS3 durability is pretty much the only thing I consider worth using EC2 for. S3 egress, on the other hand, is so expensive you can often justify putting a writethrough cache for your entire working set at Hetzner... reply ptman 4 hours agorootparentIIRC Cloudflare R2 is designed as a cache for S3 and has free egress edit: apparently not writethrough: https://developers.cloudflare.com/r2/data-migration/sippy/ reply jgrahamc 4 hours agorootparentNo, R2 is not an \"S3 cache\". It's a replacement for S3 with no egress fees. reply luuurker 52 minutes agoparentprevOne of my VPS was lost in that fire, but since my backups where somewhere else, I didn't lose much. The trick is not to put all eggs in the same basket and it's easier to do that when you're paying much less than you'd pay on providers like AWS. reply 42lux 5 hours agoparentprevYou know that if US East suffers a FULL data loss that the recovery would take weeks with the question if it would even be possible. That's what happened to ovh... it wasn't just one building. reply clarkdave 5 hours agorootparent> It's not like there is a mirror datacenter just two blocks away Isn't that exactly what Availability Zones are for? They're physically separate[0] datacenters and each one contains a copy of each S3 object (unless using the explicit single-zone options) It's also straightforward (although not necessarily that cheap) to replicate S3 objects to another region [0] https://aws.amazon.com/ec2/faqs/?nc1=h_ls#Platform reply abadpoli 4 hours agorootparentprevIf us-east-1 ever suffered a “FULL” data loss, it would be a company-ending event for so many companies that it would practically end society as we know it. OVH’s failure was a single building. That’s the problem with a lot of server hosters - even Google has their availability zones all co-located in the same building, so a physical event like a fire could take down an entire region. AWS has AZs in physically separate locations, each with 1+ separate DCs. reply ahofmann 5 hours agorootparentprevIt literally was one building in france, that burned to the ground: https://www.datacenterdynamics.com/en/analysis/ovhcloud-fire... reply guerby 4 hours agoparentprevIt just caught those who rely on one provider, which is never a good practice. rclone to somewhere else at least. And 3-2-1 rule for backups if you're serious. reply fabian2k 4 hours agoparentprevI'd probably look at using S3 as a backup and only have the working copy with Hetzner. Not sure how much more expensive that would be, the pricing for the non-standard S3 and glacier does seem to require a bit more work to understand properly. reply smartbit 5 hours agoparentprevCouldn't agree with you more. Last spring talked to a senior Hetzner engineer who wasn't too sure about the Object Storage design/implementation and was happy he wasn't involved. reply ChocolateGod 5 hours agoprevDid some benchmarks, not too bad. Benchmark finished! block-size: 4.0 MiB, big-object-size: 1.0 GiB, small-object-size: 128 KiB, small-objects: 100, NumThreads: 6 +--------------------+--------------------+------------------+ITEMVALUECOST+--------------------+--------------------+------------------+upload objects207.02 MiB/s115.93 ms/object| download objects405.78 MiB/s59.14 ms/object| put small objects278.4 objects/s21.55 ms/object| get small objects498.5 objects/s12.04 ms/object| list objects40977.06 objects/s14.64 ms/op| head objects1995.4 objects/s3.01 ms/object| delete objects312.1 objects/s19.22 ms/object| change permissionsnot supportnot support| change owner/groupnot supportnot support| update mtimenot supportnot support+--------------------+--------------------+------------------+ Currently have a project using OVH, which is slower, but than I guess there's currently less load on Hetzner. Benchmark finished! block-size: 4.0 MiB, big-object-size: 1.0 GiB, small-object-size: 128 KiB, small-objects: 100, NumThreads: 6 +--------------------+-------------------+------------------+ITEMVALUECOST+--------------------+-------------------+------------------+upload objects80.82 MiB/s296.94 ms/object| download objects274.61 MiB/s87.40 ms/object| put small objects31.6 objects/s189.98 ms/object| get small objects145.1 objects/s41.36 ms/object| list objects6293.39 objects/s95.34 ms/op| head objects177.7 objects/s33.76 ms/object| delete objects136.6 objects/s43.93 ms/object| change permissionsnot supportnot support| change owner/groupnot supportnot support| update mtimenot supportnot support+--------------------+-------------------+------------------+ reply teknopurge 5 hours agoparentwhat tool are you using for this wonderful data? reply diggan 5 hours agorootparentSearched for \"Benchmark finished! block-size\" and seems to be https://github.com/juicedata/juicefs?tab=readme-ov-file#perf... reply aqfamnzc 5 hours agorootparentprevLooks like juicefs objbench. https://juicefs.com/docs/community/performance_evaluation_gu... reply ChocolateGod 5 hours agorootparentYes, it's juicefs objbench. Wonderful tool. Useful for figuring out where some of these cheaper S3 alternatives make their cuts. reply lionkor 5 hours agorootparentprevmissed opportunity to call it JuiceFS' OJBench reply espadrine 2 hours agoprevAWS: $0.024/GB/month (S3 standard, first 50 TB/month) GCP: $0.023/GB/month (standard storage, beyond 5 GB free limit) Azure: $0.019/GB/month (hot, first 50 TB/month) Cloudflare: $0.015/GB/month (beyond 10 GB free limit) Scaleway: ~$0.013/GB/month (single-zone) Backblaze: $0.006/GB/month Hetzner: ~$5.49/month + $0.0054/GB/month (beyond 1TB free limit) Egress costs vary though. reply layer8 2 hours agoparentIn slightly different terms, Hetzner is roughly 5 €/TB/month of storage plus 1 €/TB of egress (ex. VAT). reply coolspot 50 minutes agorootparentIn slightly different terms, Hetzner is roughly $0.006/Gb/month , similar to Backblaze reply ed_blackburn 5 hours agoprevTerrific to see native object storage in Hetzner. I think object storage is become a staple for most providers now. I can see that Hetzner is starting with WORM capabilities [0]. I wonder if this nascent product is successful they'll consider some of the features that other providers offer such as mutating objects, storage policies, tiering, and conditional writes. I appreciate you've got to start somewhere and this looks terrific for an opening gambit. Kudos Hetzner. [0] > Object Storage is mainly used to store and share data as it is not possible to edit any data that you uploaded to a Bucket (objects are immutable). So the main purpose of Object Storage is \"WORM\", which is short for, \"Write once, read many [times]\". reply sam_lowry_ 5 hours agoparent> mutating objects, storage policies, tiering, and conditional writes My only hope is that they won't make it more complex than it should be in an effort to match AWS. reply internetter 28 minutes agorootparentEvery provider virtually has to mirror AWS. The S3 API is the standard at this point. Nobody will switch if they need to replace all their code. reply systemdave 2 hours agoprevAs someone who has managed many public Ceph clusters at Linode (now Akamai) since 2016 for both block and object storage, I wish the Hetzner engineers good luck! There are a _lot_ of challenges in keeping the clusters secure, reliable, and performant. Make sure you have systems or tools in place to prevent abuse. Be aware of the little nuances of Ceph, like what time lifecycle policies kick off, or when dynamic bucket resharding will kick in (and block client writes!). If possible, conduct extensive failure testing in a lab environment under simulated load to see how your clusters will really behave when it eventually happens. Triple check all of your tunables and your pool configuration. Some things like erasure coding profiles are set in stone, and once you have customer data on your clusters, there is no turning back. reply fabian2k 5 hours agoprevOne thing I noticed is that some limits seem rather low, I'm not sure if this is for the beta only or by design. Particularly the 1 Gbit/s limit per bucket and the 10 buckets for all projects. Is it typical to have such per-bucket limits? This means that you might have to take them into account when deciding on how to organize the data to avoid bottlenecks. reply phil21 5 hours agoparentHaving implemented object storage at a service provider - it's extremely rare for any customer to have 1gbit/sec sustained throughput on an account, much less a bucket. Those that do likely will already have an actual sales rep and the means to get these restrictions lifted, with a direct line to engineering. Hetzner certainly markets to a different crowd, which I suspect is why these limits are being put in place ahead of time. It's very difficult to design around noisy neighbor, and will take some battle tested time in production to get right for the \"average Joe\" style customer who won't engage engineering resources to work with a provider to avoid hot spots and impacting other customers. reply fabian2k 5 hours agorootparentI'm more concerned about bursts, not sustained throughput. When handling large files a single client can load at 1 Gbit/s if the other side supports that. My suspicion was also that they're intentionally targeting a different crowd than S3 and focusing on their niche here. reply phil21 4 hours agorootparentDefinitely agreed. I would be pretty surprised if this was burst capacity though. I have no insight as to how Hetzner implemented their object storage, but in general terms it's pretty difficult to ratelimit burst without adverse effects. Sustained is a lot easier to implement since you are typically having to have multiple load balancers coordinate to achieve such results. Attempting to do this at sub-minute time intervals is pretty tricky at scale. reply jsheard 5 hours agoparentprevPresumably it's built on top of their existing VPS and dedicated server boxes, which AFAIK only have 1gbit uplinks, so they'd only be able to exceed that speed when the bucket is split between multiple boxes. reply alex23478 58 minutes agorootparentAt the Hetzner Summit two a few weeks ago they presented the servers used for this. I am not sure about the exact specifications anymore, but they are building dedicated servers for this with custom chassis where each server has a ton of drives and I think they each had 40 GBits networking. These are special servers that are not available to customers directly. reply mjochim 5 hours agorootparentprevHetzner VPS hosts have 10 GBit links. reply jsheard 5 hours agorootparentHuh you're right, they say their VPSes have 10gig but their dedicated boxes only have 1gig, even the really expensive ones. That seems backwards. reply jorams 4 hours agorootparentTheir VPS hosts have a 10GBit uplink, but they offer no bandwidth guarantees per VPS and state you can expect 300-500MBit/s[1]. Their dedicated servers have a 1GBit uplink with guaranteed 1Gbit/s bandwidth. [1]: https://docs.hetzner.com/cloud/technical-details/faq#what-ki... reply ChocolateGod 4 hours agorootparentThe Hetzner Cloud ARM instances I believe are on 2.5Gb or 1Gb links. I've never been able to get > 1Gbit on the ARM instances, but easy getting it on x86_64. reply derhuerst 5 hours agorootparentprevIt looks like you have to manually add a 10G uplink to dedicated boxes: https://docs.hetzner.com/robot/dedicated-server/network/10g-... reply pi-rat 3 hours agorootparentprevWe had a fleet of dedicated hetzner boxes w/10 gbit, it’s just an option you get and pay a little extra for. Generally had good performance as well. reply clan 5 hours agorootparentprevNot quite correct. Their dedicated servers base configuration have 1gig. But when ordering you can add 10gig as an easy option (or add later). The 1 gig is too cheap to meter with free traffic. If you choose 10gig you pay for the traffic. Those who need it might use it :-) reply seego 2 hours agoparentprevJust got a reply from them regarding the bucket amount and permission amount limits and both are in fact beta limits. No idea about the bandwidth though. reply diggan 5 hours agoparentprevI'm guessing that's because it's in beta, they want to set some low limits and incrementally increase it, is my bet. reply KronisLV 4 hours agoprevA while ago I needed something managed, very cheap and S3 compatible and I went with Contabo Object Storage: https://contabo.com/en/object-storage/ Admittedly I got my money's worth, for about 3 EUR for the very basic plan of 250 GB storage, I get upload speeds of about 6-9 MB/s and download speeds of about 25 MB/s, which is okay for what I'm trying to do. That said, it doesn't seem like there's a way to create additional users with access to only specific buckets not all of them for the same service and the overall offering does feel a bit jank (e.g. when you don't have an active service, you can't log into Contabo at all and while their Object Storage site is nice, the regular VPS one feels functional but dated). What I'm saying is that when it comes to budget options, Hetzner will probably do great, since they have a good track record and it's not like there are that many other alternatives out there! Of course, I could have also gone with just self-hosting with MinIO, Garage or SeaweedFS, as long as the VPS that I'd get would also have enough storage. It's nice that there are self-hosted options, too, I almost dread when I see bespoke object storage solutions at work, either developed before S3 was a thing or because the devs just didn't know or had a case of NIH, so I have to look at how a bunch of blobs are passed through servlets and serialized/deserialized, as well as deal with custom metadata and permission mechanisms. reply rmbyrro 3 hours agoparentBackblaze seems to be 2x less expensive than Contabo. reply simlevesque 4 hours agoparentprevYour link returns a 500 right now reply twic 3 hours agoprevSince we're talking object storage, a question for the collective brain: are there any object storage solutions, cloud or on-prem, which support any sort of \"operator pushdown\"? By \"operator pushdown\", i mean any ability to filter or map over the contents of the object on the server side in some way, sending only the results over the network to the client. For example, say you have a huge CSV file of customer orders in a bucket. You might want to find the timestamp of all the orders which included a particular product. If all you can do is stream the whole file, then you need to do that, just to pick out a few timestamps. But you could imagine a kind of request where you say \"only give me lines where the product ID is P01234, and only send the timestamp column\". Perhaps you would express that as a pair of regular expressions, or a sed program, or a Lua script, or maybe the server would understand CSV and let you write something a bit like SQL. There are all sorts of ways it could be done. Providing a fully general way might be tricky, but it wouldn't need to be fully general to be useful. I appreciate that if you want to do this sort of access frequently, you should probably be using a database, not object storage. But it seems like a very useful feature to layer on top of object storage, and one that feels like it should be fairly cheap to execute - the server has to do a small extra amount of computation, but then needs a lto less network bandwidth. reply dmw_ng 1 hour agoparentThat's been a feature of S3 for quite a long time now, called S3 Select https://docs.aws.amazon.com/AmazonS3/latest/userguide/select... Despite it being an awesome feature I've been itching to use, I've never actually found a use for it beyond messing around. Most places where S3 Select might make sense seems to be subsumed (for my uses) by Athena. Athena has a rather large amount of conceptual and actual boilerplate to get up and running with, though, S3 Select requires no upfront planning beyond building a fancy query string (or using their SDK wrappers) Where S3 Select is likely to become fiddly is anywhere multiple files are involved. Athena makes querying large collections of CSVs (etc) straightforward, and handles all the scheduling and results merging for you. reply ju-st 26 minutes agorootparentS3 Select is not available anymore for new customers. Athena with columnar file format (eg parquet) in S3 and partitioning with Glue Data catalog is the solution for OP's problem. The cost of this kind of queries is very low because you only pay the actual data consumed/requested. And with the columnar file format Athena only accesses the necessary columns. And the data in the columns is usually compressed so the amount of data is even less. reply twic 31 minutes agorootparentprev> Amazon S3 Select is no longer available to new customers. Existing customers of Amazon S3 Select can continue to use the feature as usual. :( But you and patrickthebold are spot on in pointing out Athena. I've always thought of it as a database you load via S3, but of course it's equally a tool for querying data in S3. reply thinkingemote 50 minutes agoparentprevYou could use Range HTTP Headers if you know which bit to request. Kind of simple and not that useful for dynamic queries but quite good for queries where you know the queries beforehand and can index accordingly. https://docs.aws.amazon.com/AmazonS3/latest/userguide/range-... reply patrickthebold 2 hours agoparentprevIn the AWS world, Athena does more or less what you are describing: but is is targeted (AFAIU) for large datasets. I'm not sure how it would perform on a single (small) CSV to shave off a few bytes in transit. reply rapatel0 2 hours agoparentprevLook into duckdb. It's not server side, but It's pretty phenomenal for this use case. reply weberer 2 hours agoparentprevYeah, on AWS you can set up a Glue Data Catalog and query it with Athena. reply amanda99 5 hours agoprevIt's interesting that they implement AWS bucket policies, including all AWS-specific syntax/keys/etc, even \"arn\": https://docs.hetzner.com/storage/object-storage/faq/s3-crede... reply hedvig23 3 hours agoparentThey probably used chatgpt to build the product and copied-pasted reply GavinAnderegg 5 hours agoprevMight be a stupid question, but why did Hetzner take off? I feel like I’d barely heard of them before last year, but now they’re the go-to hosting suggestion from many. Are they just a solid host, or do they offer something extra? Their website makes them looks almost generic, so I assume it’s not marketing. reply earnesti 5 hours agoparentThey have been around for a long time. I heard people praising them already in startup events 10+ year ago (I'm not that active in the scene myself any more). They are known for great price/quality ratio for the dedicated servers. There are some gotchas, as you might get random crappy hardware or whatever. The firms I have been working for and which decided to go build their infra mostly with hetzner, I would estimate that they have saved millions in comparison to common cloud services. The staff often complains because managing your own dedicated servers is a bit more work than going with something like AWS, but if you put in the numbers, the amount of money saved in the end can be insane. reply volkadav 4 hours agoparentprevI was with linode for a loooong time for my personal VM-in-cloud (like 15 years). But I got an itch to check around to see if there was a cheaper option earlier this year and found that Hetzner would give me literally twice the machine for about two thirds the cost. ($6/mo vs €4~/mo, 1gb nanode vs. CX22 so 1vcpu/1gb to 2vcpu/4gb and 25gb disk to 40gb) So far I'm happy with the service. Linode wasn't bad per se, Hetzner just offered more for less. Their admin console is a bit more spartan in comparison to Linode's, but that's not a place I spend much time anyway. If Russia glasses Finland and Germany I'd have to find another provider but I'm pretty sure I'd have bigger problems at that point than where to park my wordpress blahg and irc session, lol. reply laristine 5 hours agoparentprevThey became a serious VPS contender amongst DigitalOcean, Linode, and other similar providers. DigitalOcean raised prices while Hetzner gave more powerful server capacity (read double) at lower price points. Hetzner has been increasingly popular in self-hosting communities. I started using their services since last year for some of the operations, including business, and it's been rock solid. I plan on increasing usage, but will hold on this new object storage offering. reply izacus 4 hours agoparentprevThey've existed since 1997 and have always been a solid host with good pricing on both bare metal and later VPS services. They seem to have managed to survive the Cloud onslaught as well by continuing to provide reliable service for good prices. Perhaps you heard less about them because they're based in Europe and thus less hype sorrounds them? reply internetter 5 hours agoparentprevThe pricing is much better than other providers, while being a reputable company. Pretty much the best value for non-enterprise workloads. reply joseda-hg 5 hours agoparentprevI've associated them for a while with uber cheap hosting, while also being allegedlly* reasonable (Most of the time) and reliable * Couldn't tell you, as they flagged my account and asked me for an ID Document I couldn't provide, Maybe I could have sorted it with Customer Service, but instead I just picked a different german hosting reply jauntywundrkind 3 hours agorootparentTheir accounts department is egregious. Amazing offerings but it's so frustrating. I had some servers that I had autopay setup for I thought. But the bill went over as unpaid and they shut off the servers. Then I noticed & tried to login to their web account system but my account was locked. Support wanted me to send a wire transfer. I begged and begged, told them how it would cost as much as by debt, and they opened my account for ~36 hours, but after some delay, and I was away vacationing. I sent them the wire transfer almost two months ago now & still nothing from their accounts/billing people, even though the transfer had the account & payment #'s. It's been so frustrating, & so unnecessarily over the top bad & they seem to love making it worse every step. What the hell Hetzner? reply whitehexagon 2 hours agoparentprevI've been with them ~15 years, and they have been solid. Back then I had a dedicated server I was developing an online multiplayer java game on, great fun! until the whole java webstart certificate fiasco. A few years back I was porting that old game over to wasm, and they added the mime type support within a few hours of asking. They are some very knowledgeable engineers, and maybe that is reflected in the simple website design. But I like that simple productive interface, and dread the day the MBAs or marketing teams get their hands on it. reply zerkten 5 hours agoparentprevThey are cheap. People have more of a focus on cost recently which pushes back against AWS. As far as vibes go, more people seem to be getting back into managing their own machines and self-hosting. I think Linode used to fill that void but for various reasons they relinquished that opportunity. reply CGamesPlay 5 hours agoparentprevIt's very cheap. They've been around for years. I've been using them for my personal server for the past 2-3 years without problems. The only problems I have seen from them are the control plane API going down (so, like, creating new servers isn't available). reply slightwinder 4 hours agoparentprevHetzner is well known and beloved in Europe for a decade or two, so it didn't really start out of nowhere. Maybe you (and most of HN) became aware of them, when they opened their American datacenters in 2021 & 2022, and became a viable option for people outside of Europe. reply incognito124 5 hours agoparentprevI don't know about the last year, maybe they ramped up their marketing. But they've been pretty popular in my circle for the past 6-7 years, mostly because price/performance ratio is really favorable. I use it for the past ~3y. reply tomr75 5 hours agoparentprevgreat value and reliability reply Hamuko 5 hours agoparentprevI got a pretty nice dedicated server with 2x3 TB HDD + 20 TB/month of traffic for just 25€/month. Was great for hosting >1 TB of video content publically. reply Fornax96 4 hours agoprevSo, I don't want to sound like a conspiracy theorist, but this announcement has extremely suspicious timing. You see, I host a cloud storage service on Hetzner. I have 12 PETABYTES of user data stored there. The data is spread over 120 of their SX type storage servers. Last week Hetzner sent me an email saying they were closing my account for an unknown reason. I have repeatedly been asking about the reason of this sudden closure, but they won't tell me. Meanwhile I have a huge problem, because I need to move 12 PB to a different hosting provider, and they only gave me until the end of November. That's an almost impossible deadline for setting up such a large storage cluster. Especially considering that Hetzner's 1 Gbps port speed makes it impossible to transfer the data in less than three weeks. Don't trust Hetzner, guys. They screwed me over real bad here, and it seems like they will be taking the hardware that I have been renting there for 10 years for themselves now. This is incredibly scummy behaviour. reply the_biot 3 hours agoparent> extremely suspicious timing. I've been wondering for well over a decade why Hetzner didn't get its ass in gear and start offering AWS-like services. Instead all they've offered for years is remote control of VMs. Even now, this new S3 offering is very little, very late -- 18 years after Amazon first offered it and took off like a rocket. The only odd thing about the timing here is what was keeping them. reply ac29 3 hours agoparentprev> Last week Hetzner sent me an email saying they were closing my account for an unknown reason. Frankly, your service looks a bit suspicious (free and cheap file sharing). You seem to be aware of the potential for abuse - your DMCA/abuse page even says there are a \"large number of abuse reports pixeldrain receives every day\". It sucks that Hetzner is closing your account. But, they are known to be pretty conservative and your users have almost certainly violated their T&C many times even if you are making a good faith effort to prevent and respond to abuse. reply Fornax96 3 hours agorootparentI have not used Hetzner to serve user facing traffic for many years. Users never connect directly to the storage servers, it's all going through custom caching nodes hosted by a different provider. That shields the hetzner servers from all abuse and also reduces traffic by about 75%. There is no risk for hetzner here. I concede that my service has been abused a lot. I am working very hard to clean up my act. I have been implementing better content moderation, content scanning and streamlining my dmca handling process. If hetzner has a problem with any of that they should have just contacted me instead of pulling the plug like this. Very unreliable company. reply coolspot 29 minutes agorootparentI suspect that what is happening here is that Hetzner received a lot of orders for storage node snapshots and/or evidence preservation from law enforcement (you probably won’t even be notified). reply lossolo 3 hours agorootparentprevYou must have done something really wrong if they don’t even want to resolve this with you. Normally, you would receive an abuse notice from them — we did many times with one of the services I worked on. I guess your service is somehow connected to other Hetzner servers (not storage), or you are somehow leaking Hetzner IPs. Your servers were probably used to host CSAM or similar content, which is likely why they don’t want to communicate with you. Someone must have reported this to them. reply ahofmann 2 hours agoparentprevSo you're paying them between €15,000 (€124 * 120 servers, hetzners cheapest sx server) and over €30,000 per month, and you really believe they sit in their little office trying to find servers of already paying customers, just to snatch them and host their own S3 service? This doesn't sound like a conspiracy theory to me; it sounds more like a serious case of megalomania. That they don't communicate well with you really sucks, and I will read your blog post. But please stay on the sane side of reasoning. reply V__ 3 hours agoparentprevHave you tried calling them? Could it be linked to pixeldrain and some illegal files which were uploaded and they simply don't want to take the risk? reply Fornax96 3 hours agorootparentI'm currently on vacation with bad cell reception. I tried calling but it didn't come through. Will try again when I get back. Hetzner does not host user facing servers for me. It's all behind caching nodes which are hosted by a different company. Hetzner would not know what is hosted on pixeldrain. reply V__ 3 hours agorootparentHope you get it sorted and would be interested in a follow up. Double check your contract regarding cancelation periods and maybe get a lawyer if nothing else works to get an extension on your period. reply slig 3 hours agoparentprevThat's terrible. Please consider documenting everything and blogging about this later on. reply Fornax96 3 hours agorootparentI will, and when it's done it will definitely appear on this forum. But in the meantime I have to focus on cleaning this mess up. reply Moru 3 hours agoparentprevThanks for the warning, was just about to start moving there. reply rrauch 2 hours agoparentprevAs someone who has used Hetzner in one form or another for close to two decades, I frankly find this behaviour shocking. Besides the normal support channels, try reaching out on their subreddit and also post in the customer forum - to make this more public. If they don't want your business, fine. But giving you only a couple of weeks to move 12 PB is unreasonable. reply Jamie9912 3 hours agoparentprev>Hetzner's 1 Gbps port speed makes it impossible to transfer the data in less than three weeks But don't you have that spread over 120 servers? reply Fornax96 3 hours agorootparentEach server has a 1 Gbps connection. Some servers are storing more than average, some have 160 TB. That's already 2 weeks at full line rate. But I also need to keep serving user traffic. I can't take the site offline for a month for a data migration, that would completely kill my business. reply PaywallBuster 3 hours agorootparentI'm guessing you're banned because you have such amounts of data/servers and push a substantial amount of bandwidth under the free \"1gbps\" card for each (even if the cache layer handles most requests directly) perharps if you moved all servers to 10G and payed for the egress over 20Tb they would reconsider your account? They still unreliable pulling stuff like this but difficult to find these hardware options off the shelf, ready for order, at this rpice reply Neil44 5 hours agoprevLinode also has S3 compatible storage now, Backblaze B2 buckets have S3 access too reply todotask 5 hours agoprevWow, I just saw \"Hetzner goes Singapore\", have been waiting for a decade. reply jsheard 5 hours agoparentUnfortunately their signature dirt cheap bandwidth isn't so cheap in Singapore, you only get 1TB inclusive instead of 20TB and the overage rates are 7.4x higher than their EU and NA datacenters. reply todotask 5 hours agorootparentThat's a problem but still cheaper than UpCloud. I do find Cloudflare pricing is more attractive for read heavy website and startup plan is useful for us. reply andrewstuart 5 hours agorootparentprevIONOS VPS is free egress. reply jsheard 5 hours agorootparentI trust cheap a lot more than free, \"free unlimited\" anything usually just means there is a limit but you're not allowed to know what it is. reply skartik 5 hours agoprevI am trying to register on Hetzner and not able to. I get the account verification email but after verification I am not able to login to the account and also do not receive any Hetzner email with login or account information. Strange. Yeah, I am from from India, so maybe that's why. Anyone else? Who is not able to register? reply marvinblum 4 hours agoprevWe've been using Hetzner for years for Pirsch [0] now, and so far we had a great experience. I migrated all of our data from AWS S3 to their new object storage without issues. We only use it for user pictures and small files (white-labeling logos and such). This is one of the rare cases where AWS is actually cheaper for us. It's probably more worth it if you have a lot of data. The only thing we're missing now is SES, and then we've fully migrated away from AWS :) [0] https://pirsch.io reply kondro 6 hours agoprevThe 1000 requests per second per bucket limit feels pretty small. reply Havoc 5 hours agoprevOh that’s fun. I can see heztners brand of cheap but decent doing well in this space. Keen to see what is available to protect public buckets though to prevent huge bills from malicious actors reply mirekrusin 5 hours agoparentCheap is a bit pejorative, I’d call it reasonable or competitive, ie. not exorbitant prices. They know what they’re doing and they’re doing it well, cheap is more side effect than primary goal. reply Havoc 57 minutes agorootparentCheap is a perfectly fine way to say low in price and you knew exactly what I meant... reply impulser_ 51 minutes agoprevHow does Hetzner compare to DigitalOcean? Lately I have seen a lot of people pushing Hetzner. reply bluedino 4 hours agoprevWhat does this kind of stuff run on the back end? Is it Ceph? Is it something custom like BackBlaze B2? reply marvinblum 4 hours agoparentI think it's Minio. reply pandemicsyn 3 hours agorootparentIts Ceph. > Our S3-compatible Object Storage provides you with storage capacity for saving data in \"Buckets\". Any data you save in your Bucket is saved in a Ceph cluster. from https://docs.hetzner.com/storage/object-storage/overview#obj... reply topicseed 5 hours agoprev1k rps per bucket though ... reply dangoodmanUT 5 hours agoprevSeems you can only move like 1TB/mo of data though? And a limit of 24TB of storage per account is also quite low. reply stavros 5 hours agoparentWhere are you seeing these limits? I'm seeing: Up to 10 TB per object Up to 1 GBit/s bandwidth per Bucket Up to 1024 operations/s per Bucket Up to 100 TB per Bucket Up to 100,000,000 objects per Bucket Up to 100 S3 credentials across all projects Up to 10 Buckets across all projects reply Hamuko 5 hours agoprevI don't understand the pricing. There's a base fee of 0.0081 €/h, which I guess then is translated into 5.00 €/month? Or is that 0.0081 €/h + 5.00 €/month? And that gives me a 1 TB-hour of free usage, and if I use the object storage for the whole month, it's 720 TB-hours of free usage, except if the month has more or less than 30 days (so it's actually between 672 TB-hour and 744 TB-hour of free quota of storage)? And those TB-hours expire at the end of the month, so you might as well store files if you're under a 1 TB? Amazon S3 pricing looking more and more sane. reply exceptione 5 hours agoparentAgree that the pricing model is highly unclear (which is usual for cloud services). It does not tell me if I should count hours in hour-of-the-day, or lapsed time. Also the example tries to demonstrate a case of \"you don´t have to pay extra\", but then falls silent. Nice, not the info I am looking for. What about envisioning a customer who asks «what am I going to pay? Specify it right now, right here». reply jorams 4 hours agoparentprevIt's not explained very clearly and made more complicated by being charged hourly instead of monthly, but essentially there's a minimum charge of €5 per month, which includes 1TB of storage and about 1TB of bandwidth. reply donatzsky 3 hours agorootparentThat's not how I read it or what their example says. As I understand it, if you create a bucket and then delete it again within the hour, you only pay for one hour. I think the 5€/month is if you have active buckets the whole month, since it's less than if you actually had to pay for all the hours. 30 * 24 * 0.0081 = 5.83 28 * 24 * 0.0081 = 5.44 reply jorams 3 hours agorootparentThat's correct, that's why the hourly billing makes it more complicated. If you don't use cloud storage the entire month you only pay for the time you do use it, and you only get included quota proportional to the amount you pay for. The sum of hourly costs is capped at €5 per month, which is also how it works for their VPSes. reply astrostl 3 hours agoparentprevDef needs a calculator if they intend to stay with this model at all IMO. reply qeternity 5 hours agoparentprevYeah we are big fans of Hetzner and this pricing model makes no sense. I'm still trying to wrap my head around it and the strange limits associated with it. reply baggy_trough 5 hours agoprev$5.29 per TB/mo for storage. Backblaze is at $6, so not revolutionary. reply rsync 7 minutes agoparent… might be revolutionary for backblaze… How much debt do they have on their balance sheet? reply Bloedcoins 5 hours agoparentprevIt is for people using hetzner for stuff. I run minio on hetzner and wouldn't send stuff across some other network to backblaze. reply PaywallBuster 2 hours agoparentprevIf you self hosted minio on their SX nodes you could prob get close to half reply s_dev 5 hours agoparentprevI was thinking of switching to BlackBlaze as I'm currently using Digital Ocean Spaces -- however I might look at Hetzner again -- not revolutionary but quite competitive. reply PaywallBuster 2 hours agorootparentb2 is not that good We use it for multiple use cases and all of them require lots of retries and error handling essentially they're build for backups use case, with lots of spinning rust, any kind of \"working data\" easily underperforms backups work fine because not time sensitive and retries handle the problems reply beingflo 5 hours agoparentprev1€ / TB egress is extremely attractive, though. Most other \"budget\" providers charge at least 10 - 20$. reply baggy_trough 5 hours agorootparentBackblaze egress is free up to 3x the average stored amount, which covers backup recovery at least. reply ptman 5 hours agorootparentprevcloudflare r2 has free egress reply beingflo 5 hours agorootparentI know, but I prefer fair pricing over free in situations like this. There are plenty of stories going around of CF forcing users to upgrade to an enterprise plan due to their usage. When there is a price tag, at least I know that won't happen to me (not that my usage would be on CFs radar anyway, it's the principle of it). reply sitkack 4 hours agorootparentMentioned in this discussion https://www.tigrisdata.com/docs/pricing/ reply n3storm 6 hours agoprevWorks good. Not so cumbersome options as other providers. reply drpossum 6 hours agoprevIs this new or just an advertisement? reply olau 5 hours agoparentIt must be new - the page says they're in beta since two weeks ago and expect it to take a month or two to hit production. It's nice to see more competition in this space. reply vladde 6 hours agoparentprevLooks like it's in beta https://docs.hetzner.com/storage/object-storage/faq/beta reply isoprophlex 5 hours agoparentprevPretty new. I enrolled in the beta a few weeks back, they will go GA later. I'm a big Hetzner fanboy, quite sad that pricing is't that competetive... reply andrewstuart 5 hours agoprevCompare S3 storage here: https://getdeploying.com/reference/object-storage reply CyberDildonics 4 hours agoprevWhat is the difference between an \"object store\" and a file system if it runs at speeds that a basic filesystem can easily match? reply moduspwnens14 4 hours agoparentLocking semantics? Consistent listing of large numbers of files? I'm speculating but those are at least the two I can think of that aren't explicitly linked to speed equivalency of a basic filesystem. reply CyberDildonics 3 hours agorootparentIf it's on a server on the internet how would anyone know the difference? reply indulona 5 hours agoprevi was interested in hetzner and their services, until i tried to sign up and, after they verified my payment card and all went well, i instantly got account suspended email and that was it. and i am glad it happened. i cannot imagine having a tiny problem later on, costing me money, with a company like that. btw backblaze is the best object storage offer on the planet at this time(i have research all options). second would be wasabi. reply akdev1l 5 hours agoparentI’m a wasabi customer, why is backblaze better as per your research? reply jodrellblank 1 hour agorootparentI haven't looked for similar for backblaze, but examples of Wasabi losing data: In 2023 Veeam backup offloading to Wasabi S3, Wasabi \"messed up their catalog\" and lost data: https://forums.veeam.com/object-storage-as-backup-target-f52... 2023, files missing from bucket, Wasabi support not replying for days, then said they \"had system maintenance\": https://old.reddit.com/r/msp/comments/13dqhgr/wasabi_storage... In 2021 Wasabi migrating databases, lost customer data: https://forums.veeam.com/object-storage-as-backup-target-f52... I've been hit by something like that and had to re-push data to them. reply indulona 5 hours agorootparentprevBackblaze gives you 3x free egress of your stored data per month. Wasabi gives you only up to 1x. So it's like you have 1 TB of data and you can download 1 TB each month via Wasabi for free, but 3TB with backblaze. Since traffic is the most costly expense when it comes to object storage, this is priceless. reply ph4te 4 hours agorootparentThis is highly dependent on the use case. Backblaze gives you 3x free egress, but after you hit that you pay for any additional egress. Wasabi has terms of use not to exceed 1x your storage, but is not in their model to pay egress. As a long-term storage user, you can restore a full system-wide backup without any concern of charges at Wasabi(typically, you're not restoring everything, just recent backups), as long as you are not consistently doing it. Backblaze will get you more egress for sharing data over the public internet, but if you need more, you will get charged. reply lsaferite 4 hours agorootparent> typically, you're not restoring everything, just recent backups Not knowing the pricing difference between the two and assuming they are similar, I would favor Backblaze as it would allow me to exceed the limit if I needed. Based on how you framed it, I would expect that with Wasabi you might hit a hard limit. reply ph4te 3 hours agorootparenthttps://wasabi.com/glossary/egress-charges Wasabi doesn't have an egress hard limit and doesn't charge for egress. You get consistent pricing, and if you need to recover everything, it's not an issue at all, and you won't pay for it. \"The reasonable use egress policy indicates that if your monthly downloads (egress) are greater than your active storage volume, then your storage use case is not a good fit for Wasabi’s free egress policy, and we reserve the right to limit or suspend your service.\" If you're using Wasabi for normal backup data storage, you shouldn't worry about egress. It is meant to prevent malicious users from uploading data and using up all the egress bandwidth, for example, a 500GB user egressing 5TB with public access or using Wasabi as a dump point to upload in one location and download in another region 1:1 ratio. As your storage goes up, your available consistent monthly egress goes up. It only becomes an issue if you abuse the account by uploading/downloading in a 1:1 ratio on a consistent basis. What happens is you get an email from support asking if something changed in your use case. If so, they will help troubleshoot it(Think of a CDN scenario, where the CDN gets misconfigured). You also have an egress monitor for suspicious activity in case you aren't normally downloading all your data, and then you see a rise in egress https://docs.wasabi.com/docs/en/whats-new?highlight=egress#e.... reply indulona 2 hours agorootparentproblem with wasabi is that their ToS are not concrete with their conditions. backblaze is specific. black on white. no buts of ifs. you know exactly what you pay for. with wasabi, you don't. if you run a business, wasabi is not the way to go. if you run personal things, it is an ok option. but in the end, 1x vs 3x is such a major difference, that there is just no point in discussing it. reply OutOfHere 5 hours agoprev [–] Will they delete the entire account if I store the banned word \"bitcoin\" as an object? (They probably will.) reply andybak 4 hours agoparent [–] If you're trying to sensibly engage people in discussion about this then you probably want to provide some context. I personally have no idea what you're referring to. reply OutOfHere 4 hours agorootparent [–] The context is that Hetzner is well known to delete accounts of users that they suspect of running processes or storing files that have the word \"bitcoin\" or any other word that Hetzner does not approve of. Hetzner does this without warning too. It doesn't matter if there is no network service, as local processing is sufficient for this action. reply andybak 3 hours agorootparent> Hetzner is well known Well - not that well known. I can find references to less egregious incidents - i.e. they ban running actual crypto software. Do you have any links I could read to find out more about the more extreme end of your claim? i.e. \" storing files that have the word \"bitcoin\" or any other word that Hetzner does not approve of\"? If this is true then it's a reason for anyone to avoid Hetzner as accidentally triggering it would be possible on almost all websites. If you're exaggerating for dramatic effect then that's fine I guess - I'd just like to find out the real state of affairs. reply OutOfHere 3 hours agorootparentI am not exaggerating it. I stand by what I said. My comment has nothing to do with running actual crypto software, implying that you don't have to be running it to get your services wiped. You also don't have to be using the internet. The ultimate discretion is up to the mood of Hetzner staff and up to when they get around to seeing what you do. If the word \"bitcoin\" occurs more than randomly in your usage, expect it to be ended without warning. I don't believe it to be an automated process. reply PaywallBuster 2 hours agorootparentprev [–] they also block external servers on their network that are somehow related to crypto so if you have some kind of service/api/trading bot/scraping/monitoring hitting a somewhat related to crypto node you may find that you will have no routing from hetzner core router to said server and support is as friendly as other comments describe :) reply internetter 12 minutes agorootparent [–] > and support is as friendly as other comments describe :) In my experience, the support is not friendly, but it is no nonsense. Every time I've reached out, they responded quickly, tersely, and took appropriate actions. While I don't personally mind formalities, there's something to be said for their efficacy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The document provides an overview of storage options, including supported actions, frequently asked questions (FAQ), and details on beta testing.",
      "It includes instructions on generating S3 credentials, using S3 API tools, and creating storage Buckets using MinIO or Terraform Provider.",
      "The guide is aimed at helping users get started with managing storage effectively using these tools and services."
    ],
    "commentSummary": [
      "Hetzner Object Storage is a new service currently in beta, offering competitive pricing but with some limitations, such as a 1 Gbit/s per bucket restriction.- Users are advised to employ the \"Cloud 3-2-1\" backup strategy for data safety, reflecting caution due to past data loss incidents with other providers like OVH.- While Hetzner is recognized for cost-effective hosting, users should be mindful of potential account issues and ensure compliance with the service's terms to avoid suspensions."
    ],
    "points": 219,
    "commentCount": 157,
    "retryCount": 0,
    "time": 1728304910
  },
  {
    "id": 41761986,
    "title": "Fast B-Trees",
    "originLink": "https://www.scattered-thoughts.net/writing/smolderingly-fast-btrees/",
    "originBody": "(This is part of a series on the design of a language. See the list of posts here.) Many 'scripting' languages use a hashmap for their default associative data-structure (javascript objects, python dicts, etc). Hashtables have a lot of annoying properties: Vulnerable to hash flooding. If protected against hash flooding by random seeds, the iteration order becomes non-deterministic which is annoying for snapshot testing, reproducible builds, etc. May have to rehash on insert, which produces terrible worst-case latencies for large hashmaps. Repeatedly growing large allocations without fragmentation is difficult on wasm targets, because virtual memory tricks are not available and pages can't be unmapped. Vector instructions on wasm are limited and there are no AES instructions. This makes many hash functions much slower. Ordered data-structures like b-trees don't have any of these disadvantages. They are typically slower than hashmaps, but I was surprised to find fairly wide variation in people's expectations of how much slower. So let's compare: rust's std::collections::HashMap with siphash rust's std::collections::BTreeMap zig's std.HashMap with siphash zig's std.HashMap with wyhash My own bptree with various compile-time options for different experiments. microbenchmarks are hard Let's start with the dumbest possible benchmark - fill a map with uniformly distributed random integers, measure how many cycles takes to lookup all those integers, and take the mean over many iterations. const before = rdtscp(); for (keys) |key| { const value_found = map.get(key); if (value_found == null) { panic(\"Value not found\", .{}); } } const after = rdtscp(); record(\"lookup_hit_all\", map.count(), @divTrunc(after - before, map.count())); lookup_hit_all / uniform u64 log2(#keys) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 rust btree 46 26 19 54 78 94 106 133 152 166 193 215 236 262 290 316 367 rust hashmap siphash 102 67 49 40 37 34 33 32 32 32 32 33 33 34 37 43 51 zig b+tree 46 26 37 56 64 87 100 110 123 143 165 180 197 220 235 269 294 zig hashmap siphash 99 84 64 69 70 68 68 68 68 68 68 68 69 68 69 73 77 zig hashmap wyhash 80 35 29 35 34 35 34 33 34 32 33 31 31 32 32 33 34 That makes btrees look pretty bad. Much worse, in fact, than the other benchmark that several people pointed me at, where at similar sizes btree lookups are only ~2x slower than hashmaps. But don't worry, we can reproduce that too: for (keys) |key| { const before = rdtscp(); const value_found = map.get(key); const after = rdtscp(); record(\"lookup_hit_one\", map.count(), after - before); if (value_found == null) { panic(\"Value not found\", .{}); } } lookup_hit_one / uniform u64 log2(#keys) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 rust btree 47 48 50 82 107 123 135 163 182 200 227 256 279 309 328 358 405 rust hashmap siphash 101 101 101 100 105 103 102 103 103 103 103 108 112 116 124 140 166 zig b+tree 45 45 59 72 85 107 126 135 148 170 188 203 223 246 264 292 319 zig hashmap siphash 106 108 116 117 120 121 123 124 124 124 123 124 127 130 132 137 147 zig hashmap wyhash 53 53 57 63 68 69 72 72 73 71 72 69 76 81 78 87 92 All we did differently was average over one lookup at a time instead of over many, but somehow that made the hashmaps 2-3x slower! Both of these benchmarks are pretty bad, so let's make better versions of both before trying to explain the differences. I only did this for the zig data-structures, because I am lazy. First, rather than looking the keys up in the same order we inserted them, we'll precompute a random sample (with replacement): const keys_hitting = try map.allocator.alloc(Key, @max(batch_size, count)); var permutation = XorShift64{}; for (keys_hitting) |*key| { const i = permutation.next() % count; key.* = keys[i]; } Then rather than measuring a single lookup at a time, we'll measure a batch of 256 lookups to amortize out the overhead of the rdtscp instruction and pull the panics out of the measurement: for (0..@divTrunc(keys_hitting.len, batch_size)) |batch| { const keys_batch = keys_hitting[batch * batch_size ..][0..batch_size]; var values_found: [batch_size]?Key = undefined; const before = rdtscp(); var key: Key = keys_batch[0]; for (&values_found, 0..) |*value, i| { value.* = map.get(key); key = keys_batch[(i + 1) % keys_batch.len]; } const after = rdtscp(); report(\"lookup_hit_batch\", map.count(), @divTrunc(after - before, batch_size)); for (values_found) |value_found| { if (value_found == null) { panic(\"Value not found\", .{}); } } } The results look similar to the results for lookup_hit_all: lookup_hit_batch / uniform u64 log2(#keys) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 zig b+tree 6 9 31 47 60 82 102 112 126 147 174 186 204 230 245 276 299 zig hashmap siphash 52 53 61 68 71 72 75 76 76 75 75 76 78 80 81 88 95 zig hashmap wyhash 29 29 31 35 38 38 40 41 42 40 40 42 41 39 42 46 43 Now we make one tiny tweak. Instead of iterating through the batch in order, we'll use the value we just looked up to pick the next key. for (0..@divTrunc(keys_hitting.len, batch_size)) |batch| { const keys_batch = keys_hitting[batch * batch_size ..][0..batch_size]; var values_found: [batch_size]?Key = undefined; const before = rdtscp(); var key: Key = keys_batch[0]; for (&values_found, 0..) |*value, i| { value.* = map.get(key); key = keys_batch[(i + value.*.?) % keys_batch.len]; // : rorxq $0x20, %rdi, %rdx bench[0x1014716] : movabsq $-0x18fc812e5f4bd725, %rax ; imm = 0xE7037ED1A0B428DB bench[0x1014720] : xorq %rax, %rdx bench[0x1014723] : movabsq $0x1ff5c2923a788d2c, %rcx ; imm = 0x1FF5C2923A788D2C bench[0x101472d] : xorq %rdi, %rcx bench[0x1014730] : mulxq %rcx, %rcx, %rdx bench[0x1014735] : movabsq $-0x5f89e29b87429bd9, %rsi ; imm = 0xA0761D6478BD6427 bench[0x101473f] : xorq %rcx, %rsi bench[0x1014742] : xorq %rax, %rdx bench[0x1014745] : mulxq %rsi, %rcx, %rax bench[0x101474a] : xorq %rcx, %rax bench[0x101474d] : retq So while we're waiting for one cache lookup we can start hashing the next key (if we can predict what it is) and maybe even get started on the next cache lookup. The btree at 2^16 keys has 4 levels. There are typically 15-16 keys per node when inserting random keys, so we'll expect to do around 32 comparisons on average before finding our key. The body of that search loop looks like: bench[0x10121b0] : cmpq %rdx, (%rdi,%rax,8) bench[0x10121b4] : jae 0x10121c2 ;at bptree.zig bench[0x10121b6] : addq $0x1, %rax bench[0x10121ba] : cmpq %rax, %rsi bench[0x10121bd] : jne 0x10121b0 ;[inlined] bench.less_than_u64 + 9 at bench.zig:159:5 So 5 instructions, including two branches, per comparison. At least 160 instructions and 64 branches for the whole lookup. The 16 keys take up 2 cachelines, so we'll average 1.5 cache lookups for each linear key search, plus 1 cache lookup to hit the child/values array. 10 cache lookups in total for the whole btree lookup. Each of those cache lookups depends on the previous lookup so it'll be hard to speculate correctly, but prefetching might help within a single node. If every cache lookup hit L2 in strict series we'd expect ~200 cycles, but probably some of the earlier nodes are in L1. Anyway, let's leave that rabbit hole alone for now. It's enough to notice that hashmaps benefit from speculative execution between multiple lookups and btrees don't. We can roughly think of lookup_hit_batch as measuring throughput and lookup_hit_chain as measuring latency. All the other benchmarks I've seen only measure one or the other, which starts to explain the disagreement over btree vs hashmap performance. string keys Btrees do a lot of key comparisons. If the keys are integers then they are stored in the btree node, so it doesn't matter too much. But for strings keys we potentially pay for a cache lookup for every comparison. lookup_hit_chain / random strings log2(#keys) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 zig b+tree 101 124 144 160 184 219 255 279 310 366 427 470 512 577 700 826 965 zig hashmap siphash 145 147 154 159 162 163 165 167 168 173 181 186 196 211 247 287 312 zig hashmap wyhash 49 50 59 65 68 69 72 74 75 81 88 93 103 119 154 188 219 Completely random strings is actually pretty unlikely and unfairly benefits the btree, which typically only has to compare the first character of each string to find that they are not equal. But real datasets (eg urls in a log) often have large common prefixes. We can see the difference if we make the first 16 characters constant: lookup_hit_chain / random-ish strings log2(#keys) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 zig b+tree 102 150 221 338 546 618 807 911 1077 1393 1507 1641 1812 2095 2628 2848 3302 zig hashmap siphash 145 147 153 159 162 163 165 167 168 174 182 187 197 210 245 282 313 zig hashmap wyhash 49 50 59 66 69 69 72 74 74 81 88 93 102 117 154 188 214 Hashmaps don't care, but the btree is really hurting. If we're just supporting strings we can optimize for this case by storing the length of the common prefix within each node. But it's hard to imagine how to generalize that to arbitrary types. What if the key is a tuple of strings? Or a small set? wasm hashes Let's try the same in wasm, where hash functions have less access to fast vector instructions. Wasm also doesn't have rdtscp so times here are in nanoseconds rather than cycles. lookup_hit_chain / uniform u64 / wasm log2(#keys) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 zig b+tree 5 14 19 22 27 38 45 49 53 59 70 75 80 87 98 111 120 zig hashmap siphash 33 34 37 38 40 40 41 42 41 41 42 43 44 45 46 52 58 zig hashmap wyhash 29 30 33 35 36 36 37 38 38 37 38 40 40 42 43 47 51 lookup_hit_chain / random strings / wasm log2(#keys) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 zig b+tree 64 77 88 97 117 141 154 167 192 216 269 287 294 308 369 436 500 zig hashmap siphash 65 64 68 70 73 71 72 73 73 75 78 81 84 88 102 118 135 zig hashmap whyhash 45 45 48 50 52 52 53 54 54 56 59 62 65 69 79 93 105 The overall ratios are fairly similar, although wyhash seems to have been penalized a little relative to siphash. Neither the hash functions nor the table scan generates vector instructions at all, even when comparing strings. And, uh, now that I think to look, they don't generate vector instructions on x86 either. So I guess that's a non-issue. btree tuning I implemented both btrees and b+trees. I didn't see much difference between them for insert/lookup, so I preferred the b+tree for the easier/faster implementation of scans and range queries. The rust btreemap fixes the max key count per node to 11. For all the workloads I've tried the sweet spot seems to be to fix the node size to 512 bytes, which is 31 keys for u64 and 20 keys for strings. Allowing leaves and branches to have different sizes didn't help. I gained a small speedup by manually laying out the node like key_count, keys, values/children. Zig by default prefers to put key_count at the end of the struct to avoid padding, but we always read the key_count first so it's nice to get some keys on the same cacheline. Maintaining this optimization across different architectures was annoying though, so I rolled it back and it's not reflected in the tests above. The rust btreemap switches on Ordering. I got a small boost from instead using less_than during the search and calling equal afterwards. For a lookup at 2^16 keys we'll expect to call less_than 32 times and equal once, so it's worth paying for the extra call to equal in exchange for tightening up the inner search loop. I use tried various different binary searches. The best was this 'branchless' variant: fn searchBinary(keys: []Key, search_key: Key) usize { if (keys.len == 0) return 0; var offset: usize = 0; var length: usize = keys.len; while (length > 1) { const half = length / 2; const mid = offset + half; if (less_than(keys[mid], search_key)) { @branchHint(.unpredictable); offset = mid; } length -= half; } offset += @intFromBool(less_than(keys[offset], search_key)); return offset; } while (length > 1) requires a branch but it's easily predictable. branchHint(.unpredictable) causes llvm to emit a conditional move for offset = mid. bench[0x101f5e0] : movq %rsi, %rax bench[0x101f5e3] : testq %rsi, %rsi bench[0x101f5e6] : je 0x101f616 ;at bptree.zig bench[0x101f5e8] : xorl %ecx, %ecx bench[0x101f5ea] : cmpq $0x1, %rax bench[0x101f5ee] : je 0x101f60b ;[inlined] bench.less_than_u64 at bench.zig:185:5 bench[0x101f5f0] : movq %rax, %rsi bench[0x101f5f3] : shrq %rsi bench[0x101f5f6] : leaq (%rcx,%rsi), %r8 bench[0x101f5fa] : cmpq %rdx, (%rdi,%r8,8) bench[0x101f5fe] : cmovbq %r8, %rcx bench[0x101f602] : subq %rsi, %rax bench[0x101f605] : cmpq $0x1, %rax bench[0x101f609] : ja 0x101f5f0 ;at bptree.zig:334:37 bench[0x101f60b] : cmpq %rdx, (%rdi,%rcx,8) bench[0x101f60f] : adcq $0x0, %rcx bench[0x101f613] : movq %rcx, %rax bench[0x101f616] : retq Linear search was still faster in most benchmarks though, even with ludicrously large node sizes. I also tried a btree variant I saw in some paper where leaves are left unsorted and keys are always inserted at the end of the leaf. This saves some memcopying in the common case, but having to sort the leaf before splitting negates the benefit. outcome All the benchmarks above are basically best case scenarios, where we're doing a lot of lookups in a row. If we were just doing one lookup in the middle of some other work then the btree might not be in cache at all and each of those 2.5 cache lookups per level are going all the way to main memory. That's catastrophic. Whereas an open-addressed hashmap will typically only hit 1 or 2 cachelines per lookup regardless of size. And while btrees avoid many of the performance edge cases of hashmaps, they also have some cliffs of their own to fall off as comparisons get more expensive and touch more memory, as we saw with the random-ish strings above. I haven't measure space usage yet, but we can expect it to be worse for btrees. For random keys the typical node occupancy is 50%, minus per-node overhead like key_count, whereas I've been running the zig hashmaps at 80%. So we can guesstimate the btrees will use >60% more memory. EDIT Duh, hashmaps have low occupancy after doubling, so their average occupancy is actually more like 57%. Whereas the btree with random keys is 50% but with steady churn will approach 67%. Space usage is really bad for small maps too. I'd need to add some extra tweaks to allow the btree root node to start small and grow, rather than paying the full 512 bytes for a map with only 1 key. Overall, I'm unconvinced that it's worth exploring btrees further. I'll stick to hashmaps and I'll either iterate in insertion order or I'll require sorting entries before iterating. But if anyone else likes the look of this rabbit hole I left some other ideas untouched: Consider the hash function part of the stable api for the compiler. Use an open-addressed hashtable that preserves hash order. Solve hash flooding without seeding the hash eg by falling back to a tree. Hash-array mapped tries have similar O(log(n)) cacheline behaviour to btrees. But if we used open-addressed hashtables at the leaves we could keep the nesting depth pretty low. In-memory LSMs with incremental merging work pretty well in differential dataflow, but still have this O(log(n)) cacheline behaviour. But maybe with a hashmap as secondary index lookups might be reasonable, and we can consider slow inserts the price to pay for not rehashing. miscellanea All the experiments here are using: rustc 1.77.2 zig 0.14.0-dev.1694+3b465ebec wasmtime-cli 20.0.2 Running on an intel i7-1260P with: efficiency cores disabled scaling governer set to performance turbo mode disabled aslr disabled Usually I would also use cset to pin experiments to a shielded core, but it seems that recent systemd versions have broken that workflow and I haven't figured out a replacement yet. The rest of the benchmark setup can be found in jamii/maps. I measured more than just lookups, but btrees have similar performance for every operation due to having to find a key first. Cache latency numbers came from mlc and roughly match claims I found online. Thanks to Dan Luu for help thinking through cache behaviour.",
    "commentLink": "https://news.ycombinator.com/item?id=41761986",
    "commentBody": "Fast B-Trees (scattered-thoughts.net)216 points by surprisetalk 17 hours agohidepastfavorite52 comments BeeOnRope 14 hours agoTo answer a question implied in the article, per-lookup timing with rdtscp hurts the hash more than the btree for the same reason the hash is hurt by the data-depending chaining: rdtscp is an execution barrier which prevents successive lookups from overlapping. rdtsc (no p) isn't, and would probably produce quite different timings. That the btree doesn't benefit from overlapping adjacent lookup/inserts is intereting. I suppose it is because btree access (here) involves data-dependent branches, and so with random access you'll get about L mispredicts per lookup in an L-deep tree, so adjacent lookups are separated by at least one mispredict: so adjacent lookup can overlap execution, but the overlapping is useless since everything beyond the next mispredict is useless as it is on the bad path. That's probably at least true for the small map regime. For the larger maps, the next iteration is actually very useful, even if on a mispredicted path, because the date accesses are at the right location so it serves to bring in all the nodes for the next iteration. This matters a lot outside of L2. At 5 instructions per comparison and 32-element nodes, however, there are just so many instructions in the window for 1 lookup it's hard to make it to the next iteration. So b-trees benefit a lot from a tight linear seach (e.g. 2 instructions per check, macro-fused to 1 op), or a branch-free linear search, or far better than those for big nodes, a vectorized branch-free search. reply dwattttt 13 hours agoparent> the overlapping is useless since everything beyond the next mispredict is useless as it is on the bad path Is this a consequence of Spectre et al mitigations? reply BeeOnRope 13 hours agorootparentNo, just a consequence of how mispredicts work: all execution after a mispredict is thrown away: though some traces remain in the cache, which can be very important for performance (and also, of course, Spectre). reply jnordwick 2 hours agorootparentCan you be more specific about \"all execution after a mispredict is thrown away\". Are you saying even non-dependant instructions? I thought the CPU just ignored the registers resulting from the wrong instruction path and started calculating the correct path - that the way the register file worked allowed it be to much better than just junking everything. A little less verbosely: \"is non-dependant ILP work also tossed on a mispredict even though that won't change? reply dwattttt 13 hours agorootparentprevThat's the part I was curious about; whether there would've been a helpful cache impact, if not for modern Spectre prevention. reply starspangled 11 hours agorootparentSpectre mitigations don't change that, some of them do require adding speculation barriers or otherwise turn off branch prediction for cases where unprivileged state can be used to direct mis-predicted privileged branches into gadgets which can create a side-band to privileged data with speculative state. But in general execution (i.e., no privilege domain crossings), this mechanism is not required. Performance effects of executing mispredicted branches (called something like \"wrong-path execution\" or \"mispredicted path ...\" in literature) is interesting and it has been studied. I don't know what the state of the art is, although I've seen results showing both speedups and slowdowns (as you would expect with any cache / BP kind of topic :P). reply BeeOnRope 6 hours agorootparent> Spectre mitigations don't change that, ... Yes, exactly. To the first order I think Spectre didn't really change the performance of existing userspace-only code. What slowed down was system calls, kernel code and some things which were recompiled or otherwise adjusted to mitigate some aspects of Spectre. There might be a rare exception, e.g., IIRC `lfence` slowed down on AMD in order to make it more useful as a speculation barrier on AMD but this is hardly an instruction that saw much use before. > I don't know what the state of the art is, although I've seen results showing both speedups and slowdowns Yeah. This seems like a pretty cut and dry case where you'd get a speedup from wrong-path misses, since the independent next search will be correctly predicted from the start and access exactly the right nodes, so it serves as highly accurate prefetching: it only gets thrown out because of a mispredict at the end of the _prior_ search. Something like the misses within a single binary search are more ambiguous: for random input the accuracy drops off like 0.5^n as you predict n levels deep, but that still adds up to ~double MLP compared to not speculating, so in a microbenchmark it tends to look good. In the real world with 1 lookup mixed in with a lot of other code, the many cache lines brought in on the bad path may be overall worse than inserting a speculation barrier yourself. That's the cool part: we can choose whether we want speculation or not if we know up front if it's harmful. reply scythmic_waves 2 hours agoprevI appreciate write-ups of failed experiments like this. They're sorta like null results in science, but for engineering. And they can help others from needlessly walking down the same path. If everyone only wrote about their successes, we'd all have to independently rediscover failures behind closed doors. reply aidenn0 13 hours agoprevIt's been a while since I last tried things, but I found crit-bit trees[1] to be much faster than b-trees. Hash array-mapped tries are also good if you don't need the things that trees give you (e.g. in-order traversal, get all values in a certain range). 1: https://cr.yp.to/critbit.html reply shpongled 2 hours agoparentSomething like a radix trie can give you the in-order traversal and range query aspect, while still keeping some of the nice properties of HAMTs. In practice though (for my domain, scientific floating point data), I have found that b-trees and simple sorted arrays with binary search (sadly) outperform radix tries and cleverer solutions. reply aidenn0 2 hours agorootparentFor very small number of keys, an array with linear searching can win out. reply fweimer 8 hours agoparentprevB-trees are supposed to address the bad cache behavior of binary trees because they are generally much shallower. Crit-bit trees as originally described do not have this property. reply winwang 1 hour agoprevWould be really interesting to see a similar study but with skiplists! I'd imagine it would be slower than hashmaps for many of the exact same reasons outlined in the article, but numbers would be cool. reply waynecochran 2 hours agoprevIf you can keep all the data in core memory why not just use you favorite balanced binary search tree (BST) like a red-black tree. B-Trees only help performance, at least in common understanding, when all the data can not be in core memory (which is why databases use them). reply ismailmaj 2 hours agoparentFor cache locality, if the layout is more wide instead of deep, you can avoid many cache misses. reply winwang 2 hours agorootparentYep -- and more generally, sequential access > random access, just a bit less so in memory than on SSD, not even mentioning spinning disk. reply kibo_money 10 hours agoprevVery interesting ! You mentioned the memory usage at the end, BTreeMaps are actually better than HashMaps most of the time, at least for Rust Here's a good break down: https://ntietz.com/blog/rust-hashmap-overhead/ reply pclmulqdq 4 hours agoparentBtrees don't waste much memory, while hash tables have to have excess capacity if you want them to go fast. reply marginalia_nu 4 hours agorootparentThat's true for on-disk b-trees which typically have large node sizes (typically 4KB), but in-memory btrees tend to aim for CPU cache lines (typically a small multiple of 32B), and thus do actually waste a fair amount of memory with their comparatively low branching factor, and thus relatively large number of branches compared to their number of leaves. reply pclmulqdq 2 hours agorootparentThe waste of abseil's b-tree, for example, is small per value: https://abseil.io/about/design/btree The efficiency compared to hash tables very much does carry over to the small b-trees used for in-memory data. reply josephg 13 hours agoprevI'd be curious to see how performance would change from storing b-tree entries in a semi-sorted array, and applying various other optimizations from here: https://en.algorithmica.org/hpc/data-structures/b-tree/ The aggregate performance improvements Sergey Slotin gets from applying various \"tricks\" is insane. reply rebanevapustus 10 hours agoparentThat's how it's done in the rust stdlib alternative https://github.com/brurucy/indexset Faster reads, slower inserts, but then you get the capability of indexing by position in (almost) O(1). In regular B-Trees this can only happen in O(n). reply vlovich123 11 hours agoparentprevNotably I believe his data structures tend to ignore string keys because it’s less amenable to SIMD. Would be interesting to see if his ideas about layout still show improvements to strings. reply anonymoushn 3 hours agorootparentYou can do strings. You probably want to store a bunch of string prefixes inline though, so that when the prefixes are not equal, you can use essentially the same code that he uses for integers. reply ww520 11 hours agoprevAdaptive radix tree is pretty good as well, with support for in order listing and range query. It can beat b-tree and come closely behind hashmap. reply vlowther 4 hours agoparentI reach for adaptive radix trees over b-trees when I have keys and don't need to have arbitrary sort orderings these days. They are just that much more CPU and memory efficient. reply pjdesno 5 hours agoprevIt would be interesting to compare the Python sortedcontainers algorithm - I've been using a C++ version of it that works quite well. Note also that nodes in B-trees (and other splitting-based data structures) have a mean load factor more like 75% - 50% is the minimum load for non-root nodes, right after splitting, and 100% is the max right before splitting. reply orlp 10 hours agoprevWhy was Rust's hashmap only tested with SipHash? It's known to be pretty bad for performance. I'm biased as the author of course, but try adding a benchmark with the Rust hasher + foldhash as well: https://github.com/orlp/foldhash. reply espadrine 9 hours agoparentThey are looking for a data structure that is robust against hash flooding attacks like https://www.cve.org/CVERecord?id=CVE-2011-4815 You mention that foldhash does not claim to provide HashDoS resistance against interactive attackers, so perhaps that disqualifies it. If anything, given this requirement, comparing with wyhash, as they do in the article, is misleading. reply orlp 8 hours agorootparent> You mention that foldhash does not claim to provide HashDoS resistance against interactive attackers, so perhaps that disqualifies it. The linked CVE is not an interactive attack either FYI, so foldhash would be sufficient to protect against that. When I say an \"interactive attacker\" I mean one that analyzes hash outcomes (either directly or indirectly through things like timing attacks and iteration order) to try and reverse engineer the hidden internal state. > If anything, given this requirement, comparing with wyhash, as they do in the article, is misleading. That is correct. There is no reason to believe wyhash is secure against interactive attackers. reply vlovich123 8 hours agorootparentprevXxh3 would have this property and would be drastically faster. Siphash is just a bad default choice imho. reply orlp 8 hours agorootparentXXH3 does not have this property, no more than foldhash does. reply helltone 6 hours agoprevPossibly off topic, but I was wondering: what are the most comprehensive data structure benchmarks out there? reply ur-whale 13 hours agoprevUnless I'm missing something, title of the article doesn't really correlate with its conclusion. reply dpatterbee 8 hours agoparentThe title of the article is \"Smolderingly fast b-trees\". Smoldering is (sorta) an antonym of blazing. Blazingly fast means very fast, smolderingly fast would therefore mean not very fast. reply tekknolagi 15 hours agoprevI thought a lot of b(+)tree advantage was in bigger-than-RAM something or other for large databases and these benchmarks seem relatively small in comparison reply foota 15 hours agoparentB-Trees are good for in memory data too because they have fairly good cache behavior. reply crest 5 hours agoparentprevAs long as your puny little working set (2^16 small keys) fits into L2 cache and get is perfectly covered by the L1 dTLB you won't see the cost of touching random pages in a big hash table larger than the last level TLB coverage and on chip caches. There won't be any TLB stalls waiting for the page walkers and you won't miss the lost spacial locality in the key-space preserved by B(+)trees if everything is in L2 cache. At the very least it proves that hash tables can be a good fit for point queries of datasets too large for linear searching or sorting + binary searches, but not yet large enough to exhaust CPU cache capacity. reply robertclaus 14 hours agoparentprevYa, I would be curious to see how this performs on out-of-cache data on an SSD and actual hard drive. On the other hand, the findings are definitely still relevant since RAM has gotten fairly cheap and most applications probably fit in it just fine. Regarding databases - Btrees also have a natural sort order, which hash tables don't. This means a btree as your main data structure helps with sort, range, or list operations in a way a hash tables can't. That being said, even traditional databases obviously still use hash tables extensively (ex. Hash joins). reply scotty79 14 hours agorootparentIn Rust thanks to it you can have BTreeSet of BTreeSet-s. reply xxs 8 hours agoparentprevb-trees are just 'better' binary trees as they have lower amounts of indirections (nodes) reply marginalia_nu 4 hours agoparentprevYou can line them up with disk blocks, or with CPU cache lines, the effect is relatively similar. reply georgehill 8 hours agoprevrelated: https://github.com/cloudflare/trie-hard reply lsb 14 hours agoprevClojure, for example, uses Hash Array Mapped Tries as its associative data structure, and those work well reply cmrdporcupine 5 hours agoprevAppreciate the attention to detail on the microbenchmarks. Skimming through, need to read in more detail later, but what I would love to see is a real world comparison against just linear search in a vector. Either of associated pairs, or two vectors (one for key, one for value, with matching offsets). My hunch is that people in general are more often than not reaching for hash-tables (and sometimes trees) for the API convenience of an associative structure -- but that on modern architectures with decent cache sizes and for small(ish) data sets they can be outperformed by a simple O(N) lookup. For example, it would be an interesting experiment to take something like the Python runtime (or the JDK etc) and replace its dictionary type with vectors -- at least for small dictionaries -- and then run through some common applications and frameworks and see what impact this has. reply tialaramex 5 hours agoparentI expect this experiment provides a small perf win for very small N, but that's swamped by the price of deciding whether to try this and in many applications it's also noise compared to the perf win from using hash tables for larger N. A hash table with a very cheap hash (remember in C++ out of the box their hash for integers is usually the identity function) well be cheaper for quite modest N because it's mostly just doing less work. I could believe N>=4 for example reply cmrdporcupine 3 hours agorootparentI think N=>4 is woefully pessimistic. Contiguous, cached memory... it's just so much faster on modern machines. That and all the potential for branch prediction misses in a hashtable or tree vs a simple array lookup... Linear scan be stupid fast. reply tialaramex 41 minutes agorootparentLinear scan is very fast, but it's not competing against a worse scan it's competing against not scanning which is what the hash table is going to do. At N=4, say we're mapping one 64-bit integer to another for some reason, so that's 16 bytes per entry, four entries, 64 bytes, conveniently sized linear scan table. The linear scan reads each of the four keys in turn, if one matches that's a hit, if none do we missed. Four 64-bit reads, four 64-bit compares & branches. A Swiss table big enough for N=4 has eight slots (128 butes) plus 16 bytes of metadata (total 144 bytes). Half of the slots are empty. We do a single arithmetic operation on the key, picking the only 16 bytes of metadata which exist, then we load that data, and a single SIMD instruction matches either zero or one entry, and if it wasn't zero we load and compare that entry. In practical systems you'll get more leeway because Swiss tables want a real hash, your keys probably aren't suitable so maybe there's a dozen or more ALU operations to hash a key - which is a substantial head start for a linear scan. But hey we're talking small amounts of data, who even cares about hash quality? But do measure, I too am interested, at least in a real shoot out between an actual linear scan and a decent hash table. If the \"same API as a hash table but same implementation as linear scan\" makes sense for some sizes maybe it's worth writing that collection type. reply SPascareli13 5 hours agoparentprevI think I tested very casually some time ago with Go maps and up to like one hundred items the linear search on array was faster than map lookup. Considering that many times when we use Maps for convenience they will have less than a hundred items this could be useful. Unfortunately I don't have the results (or the test code) anymore, but it shouldn't be hard to do again (casually at least). reply nialv7 12 hours agoprevI feel I missed point of this article. I thought the author is trying to prove that b-tren isn't that bad compared to hashmaps. But taking 2~3x longer looks pretty bad. If I need predictable ordering (but not actually sorting the keys) I will use something like indexmap, not b-tree. reply magicalhippo 8 hours agoparentThe point seems to be the author found very different estimates of just how much worse b-trees would be. As the author notes, hashmaps have some less desirable properties as well. So the author ran some benchmarks to find out, and ended with the following conclusion: Overall, I'm unconvinced that it's worth exploring btrees further. I'll stick to hashmaps and I'll either iterate in insertion order or I'll require sorting entries before iterating. reply BeeOnRope 4 hours agoprev [–] Nice article! Very cool to see both the \"independent\" and \"serially dependent\" cases addressed. Microbenchmarks still have lots of ways of giving the wrong answer, but looking at both these cases exposes one of the big variables which cause that. In my experience looking at container performance you often pass through two distinct regimes (in a microbenchmark!): Small regime: for small containers, instruction count, instruction dependencies and IPC (including the effect of branch missed) dominate. In this regime fastest container in a \"throughput\" sense will often be the one with fewest micro-operations (including those executed on the wrong-path). Fewer operations helps both in raw speed and also in overlapping more multiple independent lookups within the instruction window. Any type of per-lookup misprediction is hugely destructive to performance. For random keys, this often favors hash tables because they can be written to have1 independent stream, etc. If the keys are predictable, hashes containers can look bad because they tend to have a long data-dependency from the hash through the lookup to the output. Tree-like containers tend to replace those with control, so the data-dependent critical path can be very short! With random keys, hashes win again because mispredicts are so destructive. Then in the large regime, a lot of the same themes repeat but instead of applying to \"all instructions\", it's mostly about memory access. I.e., the winning throughput containers are the ones that can get the highest useful MLP, and the winning latency containers are the ones with the shortest critical path of data-dependent memory accesses, mostly ignoring everything else. Instructions still matter because MLP is often dependent on how many accesses you can stuff into the processors OoOE execution window, and the size of that structure is (roughly speaking) counted in instructions. Software prefetching can help a ton with stuffing the right memory accesses in the OoOE window. For random keys and \"usually hit\", hashes again tend to win in this regime, because they can usually get down to 1 miss per lookup, and that's math the other structures just can't overcome. For non-random keys, the field is wide open, it depends heavily on the key distribution. For lookups which often miss there are plenty of ways to break the 1 miss-per-lookup barrier too. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text compares the performance of different data structures, specifically focusing on Rust's HashMap and BTreeMap, and Zig's HashMap and B+Tree, using various benchmarks.",
      "Hashmaps are shown to benefit from speculative execution, providing consistent performance, whereas B-trees face challenges with string keys and higher memory usage.",
      "In WebAssembly (WASM), hash functions are slower due to limited vector instructions, making hashmaps generally more favorable despite their vulnerabilities."
    ],
    "commentSummary": [
      "The article compares the performance of B-trees and hashmaps, noting that B-trees suffer from data-dependent branches, causing mispredictions during lookups.- It explores other data structures such as crit-bit trees, radix tries, and adaptive radix trees, discussing their respective strengths and weaknesses.- The conclusion is that hashmaps generally outperform B-trees, particularly with random keys, and the article also considers the effects of Spectre mitigations and the viability of linear search for small datasets."
    ],
    "points": 216,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1728265377
  },
  {
    "id": 41765734,
    "title": "Silicon Valley, the new lobbying monster",
    "originLink": "https://www.newyorker.com/magazine/2024/10/14/silicon-valley-the-new-lobbying-monster",
    "originBody": "A Reporter at Large Silicon Valley, the New Lobbying Monster From crypto to A.I., the tech sector is pouring millions into super pacs that intimidate politicians into supporting its agenda. By Charles Duhigg October 7, 2024 A person familiar with Fairshake, a super pac, said that the group had “a simple message”: “If you are pro-crypto, we will help you, and if you are anti we will tear you apart.”Illustration by Javier Jaén One morning in February, Katie Porter was sitting in bed, futzing around on her computer, when she learned that she was the target of a vast techno-political conspiracy. For the past five years, Porter had served in the House of Representatives on behalf of Orange County, California. She’d become famous—at least, C-span and MSNBC famous—for her eviscerations of business tycoons, often aided by a whiteboard that she used to make camera-friendly presentations about corporate greed. Now she was in a highly competitive race to replace the California senator Dianne Feinstein, who had died a few months earlier. The primary was in three weeks. A text from a campaign staffer popped up on Porter’s screen. The staffer had just learned that a group named Fairshake was buying airtime in order to mount a last-minute blitz to oppose her candidacy. Indeed, the group was planning to spend roughly ten million dollars. Porter was bewildered. She had raised thirty million dollars to bankroll her entire campaign, and that had taken years. The idea that some unknown group would swoop in and spend a fortune attacking her, she told me, seemed ludicrous: “I was, like, ‘What the heck is Fairshake?’ ” Porter did some frantic Googling and discovered that Fairshake was a super pac funded primarily by three tech firms involved in the cryptocurrency industry. In the House, Porter had been loosely affiliated with Senator Elizabeth Warren, an outspoken advocate of financial regulation, and with the progressive wing of the Democratic Party. But Porter hadn’t been particularly vocal about cryptocurrency; she hadn’t taken much of a position on the industry one way or the other. As she continued investigating Fairshake, she found that her neutrality didn’t matter. A Web site politically aligned with Fairshake had deemed her “very anti-crypto”—though the evidence offered for this verdict was factually incorrect. The site claimed that she had opposed a pro-crypto bill in a House committee vote: in fact, she wasn’t on the committee and hadn’t voted at all. Soon afterward, Fairshake began airing attack ads on television. They didn’t mention cryptocurrencies or anything tech-related. Rather, they called Porter a “bully” and a “liar,” and falsely implied that she’d recently accepted campaign contributions from major pharmaceutical and oil companies. Nothing in the ads disclosed Fairshake’s affiliation with Silicon Valley, its support of cryptocurrency, or its larger political aims. The negative campaign had a palpable effect: Porter, who had initially polled well, lost decisively in the primary, coming in third, with just fifteen per cent of the vote. But, according to a person familiar with Fairshake, the super pac’s intent wasn’t simply to damage her. The group’s backers didn’t care all that much about Porter. Rather, the person familiar with Fairshake said, the goal of the attack campaign was to terrify other politicians—“to warn anyone running for office that, if you are anti-crypto, the industry will come after you.” The super pac and two affiliates soon revealed in federal filings that they had collected more than a hundred and seventy million dollars, which they could spend on political races across the nation in 2024, with more donations likely to come. That was more than nearly any other super pac, including Preserve America, which supports Donald Trump, and WinSenate, which aims to help Democrats reclaim that chamber. Pro-crypto donors are responsible for almost half of all corporate donations to pacs in the 2024 election cycle, and the tech industry has become one of the largest corporate donors in the nation. The point of all that money, like of the attack on Porter, has been to draw attention to Silicon Valley’s financial might—and to prove that its leaders are capable of political savagery in order to protect their interests. “It’s a simple message,” the person familiar with Fairshake said. “If you are pro-crypto, we will help you, and if you are anti we will tear you apart.” After Porter’s defeat, it became obvious that the super pac’s message had been received by politicians elsewhere. Candidates in New York, Arizona, Maryland, and Michigan began releasing crypto-friendly public statements and voting for pro-crypto bills. When Porter tried to explain to her three children why she had lost, part of the lesson focussed on the Realpolitik of wealth and elections. “When you have members who are afraid of ten million dollars being spent overnight against them, the will in Washington to do what’s right disappears pretty quickly,” she recalls saying. “This was naked political power designed to influence votes in Washington. And it worked.” “And I’m saying you need to come look at this.” Cartoon by Roland High Copy link to cartoon Link copied Shop Porter’s defeat, in fact, was the culmination of a strategy that had begun more than a decade earlier to turn Silicon Valley into the most powerful political operation in the nation. As the tech industry has become the planet’s dominant economic force, a coterie of specialists—led, in part, by the political operative who introduced the idea of “a vast right-wing conspiracy” decades ago—have taught Silicon Valley how to play the game of politics. Their aim is to help tech leaders become as powerful in Washington, D.C., and in state legislatures as they are on Wall Street. It is likely that in the coming decades these efforts will affect everything from Presidential races to which party controls Congress and how antitrust and artificial intelligence are regulated. Now that the tech industry has quietly become one of the most powerful lobbying forces in American politics, it is wielding that power as previous corporate special interests have: to bully, cajole, and remake the nation as it sees fit. Chris Lehane was just shy of thirty years old when he came up with the notion of “a vast right-wing conspiracy,” to explain Republican efforts to undermine Bill and Hillary Clinton. It was such an inspired bit of showmanship that Hillary Clinton adopted it as one of her signature lines. At the time, Lehane was a lawyer in the Clinton White House tasked with defending the Administration from charges of scandal, but he specialized in seizing control of the political conversation, finding colorful ways to put Republicans on defense. Tactics such as declaring that the President of the United States was the victim of a shadowy conservative cabal were so effective that the Times later declared Lehane to be the modern-day “master of the political dark arts.” After serving in the White House, Lehane joined Al Gore’s Presidential campaign, as press secretary, and after Gore’s defeat he set up shop in San Francisco. Despite the size and the electoral significance of California, many campaign operatives viewed the state as a political backwater, because it was so far away from Washington. But Lehane, who had worked on the Telecommunications Act of 1996, was convinced that Silicon Valley was the future, and he quickly built a business providing his dark arts to wealthy Californians. When trial lawyers wanted to increase the state’s caps on medical-malpractice jury awards, they brought in Lehane, who helped send voters flyers that looked like cadaver toe tags, and produced ads implying that doctors might be performing surgery while drunk. A few years later, when a prominent environmentalist hired Lehane to campaign against the Keystone XL Pipeline, he sent activists into press conferences carrying vials of sludge from an oil spill; the sludge was so noxious that reporters fled the room. Then he hired one of the Navy seals who had helped kill Osama bin Laden to talk to journalists and explain that if the pipeline were approved a terrorist attack could flood Nebraska with one of the largest oil spills in American history. Lehane explained to a reporter his theory of civil discourse: “Everyone has a game plan until you punch them in the mouth. So let’s punch them in the mouth.” But Lehane’s efforts generally failed to impress the tech industry. For decades, Silicon Valley firms had considered themselves mostly detached from electoral politics. As one senior tech executive explained to me, until about the mid-twenty-tens, “if you were a V.C. or C.E.O. you might hire lobbyists to talk to politicians, or gossip with you, but, beyond that, most of the Valley thought politics was stupid.” Within a decade of Lehane’s move West, however, a new kind of tech company was emerging: so-called sharing-economy firms such as Uber, Airbnb, and TaskRabbit. These companies were “disrupting” long-established sectors, including transportation, hospitality, and contract labor. Politicians had long considered it their prerogative to regulate these sectors, and, as some of the startups’ valuations grew into the billions, politicians began making demands on them as well. They felt affronted by companies like Uber that were refusing to abide by even modest regulations. Other companies tried a more conciliatory approach, but quickly found themselves mired in local political infighting and municipal bureaucracies. In any case, “not understanding politics became an existential risk,” another senior tech executive said. “There was a general realization that we had to get involved in politics, whether we wanted to or not.” In 2015, San Francisco itself became the site of a major regulation battle, in the form of Proposition F, a ballot initiative to limit short-term housing rentals, which both sides acknowledged was an attack on Airbnb. The proposal had emerged from built-up frustrations: some San Franciscans complained that many buildings had essentially become unlicensed hotels, hosting hard-partying tourists who never turned off the music, didn’t clean up their trash, and—most worrying for city leaders—hadn’t paid the taxes that the city would have collected had they stayed at a Marriott. Other residents argued that Airbnb’s presence was making it harder to find affordable housing, because it was more profitable to rent to short-term visitors than to long-term tenants. Proposition F would essentially make it impossible for Airbnb to work with many homeowners for more than a few weeks a year. Early polling indicated that the initiative was popular. Numerous other cities had been considering similar legislation, and were eagerly watching to see if lawmakers in San Francisco—where Airbnb was headquartered—could teach them how to rein in the Internet giant, then worth some twenty-five billion dollars. Airbnb’s executives, panicked, called Lehane and asked him to come to their headquarters; he showed up within minutes of their call, in the sweatpants and baseball jersey that he’d been wearing at his son’s Little League game. Lehane has the lean build of someone accustomed to athletic self-torture—he runs daily, often fifteen miles at a stretch, typically while sending oddly punctuated e-mails and leaving stream-of-consciousness voice mails—and he has a boyish crooked front tooth that offsets the effect of his receding hairline. To Airbnb’s leaders, he didn’t look like much of a political guru. But, once Lehane caught his breath, he launched into a commanding speech. You’re looking at this situation all wrong, he said. Proposition F wasn’t a crisis—it was an opportunity to change San Francisco’s political landscape, to upend a narrative. The key, he told executives, was to build a campaign against Proposition F as sophisticated as Barack Obama’s recent Presidential run, and to deploy insane amounts of money as a warning to politicians that an “Airbnb voter” existed—and ought not be crossed. He proposed a three-pronged strategy, and explained to executives that what politicians care about most is reëlection. If the company could show that being anti-Airbnb would make it harder for them to stay in office, they would fall in line. Lehane was soon named Airbnb’s head of global policy and public affairs. His first step in this role was to mobilize Airbnb’s natural advocates: the homeowners who were profiting by renting out their properties, and the visitors who had avoided pricey hotel rooms by using the service. By the end of 2015, more than a hundred and thirty thousand people had rented or hosted rooms in San Francisco. Lehane recruited several former Obama-campaign staffers to lead teams who made tens of thousands of phone calls to Airbnb hosts and renters, warning them about Proposition F. The team members also urged hosts to attend town-hall meetings, talk to neighbors, and call local officials. During this period, the company—accidentally, it says—sent an e-mail to everyone who had ever stayed in a California Airbnb, urging them to contact the California legislature. The legislature was inundated with messages from around the world. The Senate president pro tem called Lehane to let him know that the message had been received, and to beg him to stop the onslaught. “I kind of wish we had done it on purpose,” someone close to that campaign told me. The second part of Lehane’s strategy was to use large amounts of money to pressure San Francisco politicians. The company brought on hundreds of canvassers to knock on the doors of two hundred and eighty-five thousand people—roughly a third of the city’s population—and urge them to contact their local elected officials and say that opposing Airbnb was the equivalent of attacking innovation, economic independence, and America’s ideals. The relentless campaign posed a clear threat to the city’s Board of Supervisors: if an official supported Proposition F, Airbnb might encourage someone to run against him or her. “We said the quiet part out loud,” a campaign staffer said. “The goal was intimidation, to let everyone know that if they fuck with us they’ll regret it.” In all, Airbnb spent eight million dollars on the campaign, roughly ten times as much as all of Proposition F’s supporters combined. “It was the most ridiculous campaign I’ve ever worked on,” the staffer told me. “It was so over the top, so extreme. You shouldn’t be able to spend that much on a municipal election.” That said, the staffer loved her time at Airbnb: “It was the most money I’d ever earned working in politics.” The third aspect of Lehane’s strategy was upending the debate over Proposition F by proposing alternative solutions. Otherwise, Lehane and Airbnb’s chief executive, Brian Chesky, believed, the company would face similar proposals in other cities. “You can’t just be against everything,” Lehane told the Airbnb board. “You have to be for something.” As a compromise gesture, Airbnb had voluntarily begun paying taxes on short-term stays within the city. It also offered to share some internal company data—such as the number of guests visiting the city each month—that would help local officials monitor the service’s impact on the community. What’s more, Airbnb eventually offered to build a Web interface that San Francisco officials could use to register hosts and track rental patterns. The solution was self-serving, in that it made the city dependent on Airbnb for monitoring Airbnb’s activities. But the proposals addressed many of the complaints that had prompted Proposition F. More important, they guaranteed San Francisco tens of millions of tax dollars annually. When Proposition F finally came to a vote, it was resoundingly defeated. Airbnb’s approach to political conflict was in stark contrast with that of Uber, which had just become the most valuable startup in the world—and which, owing to its resistance to various taxi regulations, was soon under fire from multiple cities and nations. Airbnb’s tactics were designed to appeal to politicians’ higher ideals. After the Proposition F campaign, Lehane began working on a partnership with the S.E.I.U., one of the nation’s largest labor unions, to unionize the workers who cleaned Airbnb rentals. The plan never came together, but labor-friendly politicians in San Francisco and New York began viewing Airbnb as a potential ally. To other political operatives, Lehane’s tactics hardly seemed groundbreaking. But within Silicon Valley his approach was a revelation. “It was a huge bang for a relatively small outlay,” a tech executive told me. “It turns out the R.O.I.”—return on investment—“on politics is way better than anyone suspected.” After the defeat of Proposition F, San Francisco’s Board of Supervisors eventually agreed to many of Airbnb’s suggestions. By then, Lehane had moved on to other locations. He began similar Airbnb campaigns in dozens of other cities, including Barcelona, Berlin, New York, and Mexico City. When the U.S. Conference of Mayors convened in Washington, D.C., in 2016, Lehane was invited to speak after Michelle Obama. “Read my lips,” he told the gathering. “We want to pay taxes.” Airbnb soon had agreements with more than a hundred cities, and when local politicians proved intransigent—leaders in Austin, for instance, seemed immune to Airbnb’s overtures—the company simply went over their heads. In Texas, it persuaded the state legislature to make it hard for any municipality to ban short-term rentals. Today, Airbnb has agreements with thousands of cities. A few years after Lehane joined Airbnb, a venture capitalist pulled him aside at a party and said, “It used to be, hiring the right C.F.O. was the most important thing to make sure a company goes public. But you’ve proved a political person is just as important.” Lehane, however, had had an even bigger insight. These campaigns had revealed that tech companies—particularly firms, like Airbnb, with platforms that connect people who might otherwise have trouble finding one another—were now potentially the most powerful cohort in politics. “At one point, organizations like labor or political parties had the ability to organize and really turn out large numbers of voters,” Lehane told me. Today, Internet platforms have the bigger reach; a tech company can communicate with hundreds of millions of people by pushing a button. “If Airbnb can engage fifteen thousand hosts in a city, that can have an impact on who wins a city-council race or the mayoralty,” Lehane told me. “In a congressional or Senate race, fifty thousand votes can make all the difference.” Of course, simply having a huge user base doesn’t guarantee that Airbnb can get everything it wants. Voters respond only to enticements that they find persuasive. But companies like Airbnb, Lehane understood, could make arguments faster, and more efficiently, than nearly any political party or other special-interest group, and this was a source of considerable power. “The platforms are really the only ones who can speak to everyone now,” Lehane said. For the tech industry, the Trump years were a bewildering mess. The President attacked tech platforms for being biased against conservatives, and liberals railed against Silicon Valley’s social-media companies for propelling Trump into the White House. Tech executives declared their support for the industry’s many immigrants in the face of Trump’s Muslim ban and border separations; they also contended with walkouts and protests from employees over racial injustice, sexual harassment, and all-gender bathrooms—subjects that neither an engineering degree nor business school had prepared them for. When Joe Biden won the Presidency, in 2020, the Valley’s leaders were relieved. The Biden Administration seemed like a return to the Pax Obama, an era when tech was considered cool and politicians boasted of knowing Mark Zuckerberg. Biden’s victory also meant that Lehane, with his deep roots in the Democratic Party, was unquestionably Silicon Valley’s top political guru. Companies sought him out; employees loved that he was generous with credit and made politics fun. (Many former colleagues talk proudly about the nicknames that he bestowed upon them.) Most of all, he made the people he worked with feel like they were on a righteous quest. Peter Ragone, a prominent adviser to numerous Democratic politicians, told me that, among the handful of political consultants transforming Silicon Valley, “Chris is the tip of the spear. His capacity for processing information at speed is breathtaking.” The Valley’s enthusiasm for Biden, however, was short-lived. The President quickly appointed three prominent tech skeptics—Gary Gensler, Lina Khan, and Jonathan Kanter—to oversee the Securities and Exchange Commission, the Federal Trade Commission, and the antitrust division of the Department of Justice, respectively. Soon the government was suing or investigating Google, Apple, Amazon, Meta, Tesla, and dozens of other companies. Some of those suits and inquiries had been initiated under Trump, but Biden’s S.E.C. found a particular target in the cryptocurrency industry. Gensler, an ally of Elizabeth Warren, filed more than eighty legal actions arguing that crypto firms or promoters had violated the law, most often by selling unregistered securities. Some of the executives being sued by the S.E.C. had contributed lavishly to the Democrats. Brad Garlinghouse, the C.E.O. of the crypto firm Ripple, who had been a fund-raising bundler for Obama, was among those under legal fire, and he clearly felt victimized. He told Bloomberg that the federal government was acting like “a bully,” and tweeted, “Dems continue to enable Gensler’s unlawful war on crypto—sabotaging the ability for American innovation to thrive. It’s no wonder the GOP has announced a pro-crypto stance . . . . Voters are paying attention.” (Last year, a federal judge upheld some portions of the S.E.C.’s case against Ripple and dismissed others.) To certain people, the government’s approach felt oddly aggressive. One crypto executive told me she discovered that her bank accounts had been frozen—with no explanation—only when she tried to make a withdrawal to repair a catastrophic home-septic-system failure. Around this time, various regulatory agencies were warning banks about the risks posed by the crypto industry. When the executive’s accounts were later unfrozen—again, without a clear explanation—she was left wondering if the government’s goal was to intimidate the industry. (The Office of the Comptroller of the Currency, which regulates national banks, said that it does not direct banks to freeze individual accounts.) Cartoon by Michael Maslin Copy link to cartoon Link copied Shop The Biden Administration’s oppositional stance, however, seemed warranted when, in 2022, FTX—the enormous crypto exchange and hedge fund led by Sam Bankman-Fried—imploded amid revelations that more than eight billion dollars had been misallocated or lost. Bankman-Fried had been a prolific political donor, and violating campaign-finance law was among the crimes for which he was arrested. Another crypto executive told me that, after the FTX scandal, many figures in the industry “just wanted to put our heads down and disappear,” adding, “The less people noticed us, the better.” But among Silicon Valley’s most moneyed class retreat wasn’t an option. The powerful venture-capital firm Andreessen Horowitz had already raised more than seven billion dollars for crypto and blockchain investments. The “super angel” investor Ron Conway had poured millions of dollars into crypto firms through his venture fund. Lehane urged some of the largest crypto investors and companies, many of whom were bickering on Twitter, to instead form a coalition devoted to changing the public narrative. He began hosting private biweekly gatherings, known as the Ad-Hoc Group, where various collaborations were discussed. Eventually, a former partner at Andreessen Horowitz, Katie Haun, recommended that the large crypto firm Coinbase, where she was a board member, bring on Lehane as an adviser. Lehane met with Coinbase’s co-founder Brian Armstrong and told him that, just as with Airbnb, what seemed like a crisis was actually an opportunity. “This is not the time to go quiet,” Lehane told him. “This is your chance to define your company and the industry, and prove you’re different from FTX.” In 2023, Lehane joined Coinbase’s Global Advisory Council. Twenty-five days later, the S.E.C. sued the firm. Lehane established a war room with the primary goal of convincing politicians that the political consequences of being anti-crypto would be intensely painful. The person familiar with Fairshake, who was then an employee at Coinbase, told me, “It wasn’t really about explaining how crypto works, or anything like that. It’s about hitting politicians where they are most sensitive—reëlection.” Armstrong clarified this aim at a crypto conference in 2023. The goal, he said, was to ask candidates, “Are you with us? Are you against us? Are we going to be running ads for you or against you?” Although Lehane’s basic strategy resembled the one he’d used at Airbnb, that campaign had been focussed on municipal issues and local political races. The crypto effort was national in scale, targeting Senate and House races—and potentially even the Presidential contest—and would require significantly more money. Lehane suggested to Armstrong that crypto firms set aside fifty million dollars for outreach. Let’s earmark a hundred million, Armstrong replied. Coinbase, Ripple, and Andreessen Horowitz donated more than a hundred and forty million dollars to Fairshake, the crypto super pac. Executives at other firms contributed millions more. Lehane, collaborating closely with Fairshake, began crafting a pro-crypto message and helping to build a “grassroots” army. “We need to demonstrate there’s a crypto voter,” he told the Coinbase team. “There’s millions and millions of Americans who own this stuff. We need to prove they’ll vote to protect it.” The Federal Reserve has said that in 2023 fewer than twenty million Americans owned cryptocurrencies. Polling indicates that the issue is not an electoral priority for many voters. One Coinbase staff member pointed out this discrepancy to Lehane, saying, “I don’t know if there is a crypto voter.” “Then we’re going to make one,” Lehane replied. Coinbase began loudly promoting the results of surveys reporting to show that fifty-two million Americans owned cryptocurrencies, and that many of them intended to vote to protect their digital pocketbooks. Those polls indicated that sixty per cent of crypto owners were millennials or Gen Z-ers, and forty-one per cent were people of color—demographics that each party was trying to woo. Lehane also quietly helped launch an advocacy organization, Stand with Crypto, which is advertised to Coinbase’s millions of U.S. customers every time they log in, and which urges cryptocurrency owners to contact their lawmakers and sign petitions. The group says that it currently has more than a million members. The Coinbase employee told me that Stand with Crypto would identify a city with a significant population of crypto enthusiasts, like Columbus, Ohio, and then inundate them with push notifications aimed at organizing town halls and rallies. The employee explained, “If you can get fifty or sixty people to show up, with good photo angles you can make it look like hundreds. In small states or close elections, that’s enough to convince a candidate they should be paranoid.” This supposed army of crypto voters fed directly into the next stage of the assault: scaring politicians. Stand with Crypto built an online dashboard that assigned grades to U.S. senators and representatives—and to many of their challengers—which reflected their support for crypto. The scores seemed to inevitably be either “A (Strongly supports crypto)” or “F (Strongly against crypto),” though the data undergirding the grades were sometimes specious. “Most of them hadn’t really taken a side,” another Coinbase staffer told me. “So we’d, you know, look at speeches they’d given, or who they were friends with, and kind of make a guess. If you were friends with Elizabeth Warren, you were more likely to get an F.” Nevertheless, Lehane insisted that Fairshake maintain a nonpartisan tone. The super pac was careful to support an equal number of Democratic and Republican candidates, and, following Lehane’s advice, it planned to stay out of the 2024 Presidential race altogether. A venture capitalist who has advised the crypto industry told me that the group’s nonpartisan stance was essential, because, “if we want to get the right regulations in place, we have to get a bill through Congress, which means we need votes from both parties.” Moreover, Fairshake’s goal was to “create a nonpartisan cost for being negative on crypto and tech,” the venture capitalist added. “People need to know there are consequences.” To make this point, Lehane and Fairshake wanted to find a contest in which the group’s spending was certain to attract national attention. Fairshake compiled a list of high-profile races, and near the top was the fight to replace Dianne Feinstein in California. The obvious target was Porter, whose strongest opponent in the Democratic primary was Representative Adam Schiff. California was reliably blue, and so, if Fairshake helped defeat Porter, the group wouldn’t get blamed for handing a seat to the Republicans. What’s more, California’s primary occurred on March 5th—early in the campaign season—which meant that Porter’s race would get lots of attention and Fairshake would have time to broadcast its involvement and petrify candidates in other states. Because Porter was friendly with Elizabeth Warren, she could be painted—fairly or not—as anti-crypto. Best of all, many polls indicated that Porter was unlikely to win the primary anyway, so if the super pac “went in with a big spend, and made a big splash and she lost, Fairshake could take the victory lap regardless of whether it tipped the scales,” the Coinbase employee said. The calculation was prescient: Fairshake’s spending helped doom Porter in the primary, and the general election appears to be a lock for Schiff (who got an A from Stand with Crypto). As another political operative put it, “Porter was a perfect choice because she let crypto declare, ‘If you are even slightly critical of us, we won’t just kill you—we’ll kill your fucking family, we’ll end your career.’ From a political perspective, it was a masterpiece.” Porter will be out of government at the end of this year. After Porter’s defeat, many politicians who had once treated crypto with disdain or hostility suddenly became fans. In May, two months after Porter’s defenestration, a pro-crypto bill came up for a vote in the House. In previous years, similar bills had languished amid tepid Republican support and strong Democratic opposition. The new bill—known as the Financial Innovation and Technology for the 21st Century Act—was openly opposed by President Biden. But it sailed through the House, with nearly unanimous Republican backing and seventy-one votes from Democrats. The Senate Majority Leader, Chuck Schumer, recently joined a Crypto4Harris virtual town hall and promised that passing the legislation this year is “absolutely possible,” adding, “Crypto is here to stay.” The Democratic senator Sherrod Brown—a longtime crypto critic—is running for reëlection in Ohio, where Fairshake has directed forty million dollars to ads in support of his opponent; Brown has lately been tempering his public criticisms of the industry. Earlier this year, crypto donors indicated that they might get involved in Montana’s Senate race, where the incumbent Democrat, Jon Tester, once a crypto skeptic, is facing a difficult fight. Soon afterward, Tester voted to weaken S.E.C. oversight of cryptocurrencies, earning him the unusual grade of “C (Neutral on crypto).” It looks like Fairshake will stay out of Montana as long as Tester keeps voting the right way. A similar dynamic occurred in Maryland: after the super pac threatened to take sides in the Democratic Senate primary there, both major candidates proclaimed their pro-crypto bona fides. In total, Fairshake and affiliated super pacs have already spent more than a hundred million dollars on political races in 2024, including forty-three million on Senate races in Ohio and West Virginia, and seven million on four congressional races, in North Carolina, Colorado, Alaska, and Iowa. Three and half million dollars was used to help vanquish two left-wing representatives who were members of the so-called Squad: Jamaal Bowman, of New York, and Cori Bush, of Missouri. Of the forty-two primaries that Fairshake has been involved in this year, its preferred candidate has won eighty-five per cent of the time. The super pac’s latest filings indicated that it had more than seventy million dollars to spend in the remainder of the election cycle. Its donations to political candidates are on par with those of the oil-and-gas industry, the pharmaceutical industry, and labor unions. Just as Airbnb sought to change the conversation around Proposition F by proposing various concessions—paying taxes and sharing data—the crypto industry has become a vocal proponent of a seemingly solutions-oriented fix: new regulations for cryptocurrencies and the blockchain. Critics, however, say that these proposals are self-serving. A central dispute between the crypto industry and regulators concerns whether cryptocurrencies are securities—akin to, say, a share of Apple, the sale of which is governed by strict investor-protection laws—or commodities, like a bushel of corn, which can be sold with very little government involvement. Most fiat currencies—that is, those issued by governments—are used primarily to buy such things as food and clothing, rather than to gamble on the rise and fall of exchange rates. Cryptocurrencies, in contrast, are often difficult—or, in some cases, impossible—to use for purchasing physical goods, and they are frequently held by speculators solely as a wager that their value will rise. There are several thousand cryptocurrencies in existence. A few—most notably, Bitcoin and Ether—are considered commodities. The statuses of most of the rest are up for debate. “Now, to demonstrate that he has come of age, Jeffrey will open a childproof bottle of acetaminophen in front of all his friends and family.” Cartoon by Patrick McKelvie Copy link to cartoon Link copied Shop Many within the industry want Congress to pass regulations that would treat mainstream cryptocurrencies as commodities, which are overseen by the Commodity Futures Trading Commission, a relatively sleepy agency that most people have never heard of—and that tends to be less belligerent than the S.E.C. If the C.F.T.C. becomes the primary regulatory body for crypto, it’s likely that the stream of lawsuits and fines against large crypto companies will slow or cease. More important, selling Dogecoin (the cryptocurrency associated with a Shiba Inu dog), Dentacoin (“the only cryptocurrency by dentists, for dentists”), or CumRocket (cryptocurrency for the pornography aficionado) would become significantly less risky, and more profitable. People in the government think that this would be disastrous. “A lot of these tokens, frankly, have no real utility, no actual use, and they’re just for gambling or scamming people,” an official familiar with the S.E.C.’s thinking told me. “We already have regulations in place that have protected investors in these kinds of situations for decades. Crypto just doesn’t want to abide by them. If your entire business plan is asking ‘Can we get Kim Kardashian to tweet about us?’ and then taking people’s money, the government needs to be involved.” In fact, convincing average Americans that the crypto industry is a wholesome, customer-friendly place has been a tough sell: polls indicate that most people do not consider the crypto sector to be safe. Lehane’s colleagues within the industry have therefore shifted their tactics slightly. Getting Congress to pass friendly legislation is still a priority, but this push is now being presented as being in service of much loftier aims: protecting innovation, entrepreneurialism, and America’s future. In July, Marc Andreessen and Ben Horowitz, of the Andreessen Horowitz venture fund, made a ninety-one-minute video accusing President Biden of weakening America. Andreessen said to Horowitz, “There’s been a brutal assault on a nascent industry that I’ve just—I’ve never experienced before. I’m in total shock that it has happened.” Horowitz replied, “They’ve basically subverted the rule of law to attack the crypto industry.” These and other government actions, they said, threatened to doom America’s economy, technological superiority, and military might. And Biden, by refusing to embrace various tech-industry proposals, was allowing China to leap ahead. “The future of technology, and the future of America, is at stake,” Horowitz declared. The two men were so concerned, they said, that they had no choice but to endorse Donald Trump in 2024. (They also noted that, under Biden, billionaires like themselves might have to pay more in taxes. But that issue received less airtime.) To people inside the crypto industry, the video—which received a huge amount of attention, prompting online co-signs from Elon Musk and various other titans—was a masterstroke. As the Coinbase employee put it, “Now you’ve got Andreessen and Musk and all these other rich, powerful guys saying that crypto is part of a bigger debate. It’s about an attack on American innovation and progress and the future of the country! It changed the conversation from ‘Is cryptocurrency a scam?’ to ‘Does Biden even care about middle-class entrepreneurs?’ ” Even though Lehane opposes Trump’s candidacy, and had nothing to do with the video, Andreessen and Horowitz’s move was right out of the Lehane playbook. Lehane had done such a good job teaching the Valley how to play politics that others could now mimic his gambits. In July, Lehane joined Coinbase’s board of directors. “Chris is a genius,” the Coinbase employee said. “I don’t know how he comes up with this stuff, but he can change reality. He makes magic happen.” The annual conference for Bitcoin enthusiasts isn’t an event at which politicians usually appear. The affair often draws more than twenty-five thousand people, many of them distrustful of government. Wandering around the sea of booths, you can get a free vodka shot at 10 a.m. or discuss “tax-avoidance strategies” that fall somewhere between fraud and fantasy. People sell Edward Snowden T-shirts and crypto-themed board games. It’s a safe haven for enthusiasts of Panties for Bitcoin. But when the event took place in Nashville, in July—at a venue just a few blocks from the Redneck Riviera bar, where women were offering to lift their shirts in exchange for some of “that bit stuff”—it was teeming with political luminaries. There were eight senators, nearly a dozen representatives, and countless candidates for national and state office, some of whom launched into impromptu speeches whenever the techno music paused. The star attraction, however, was Donald Trump. The event’s appearance on the Presidential campaign circuit—and Trump’s willingness to spend one of his campaign days in a state he’s guaranteed to win—confirmed that the crypto campaign initiated by Lehane was having an effect. When Trump gave a speech before a standing-room-only crowd in orange wigs and “Make Bitcoin Great Again” hats, he pledged, “On Day One, I will fire Gary Gensler”—the S.E.C. head. This prompted a standing ovation and choruses of pro-Trump chants. A man standing near me FaceTimed his wife and insisted that she watch the speech, even though she was in the delivery room where their grandchild was being born. Trump’s embrace of crypto was a hundred-and-eighty-degree turn. As President, he had tweeted that he was “not a fan” of cryptocurrencies, which “are not money” and “can facilitate unlawful behavior, including drug trade and other illegal activity.” He continued, “We have only one real currency in the USA. It is called the United States Dollar!” Later, he said that Bitcoin “just seems like a scam.” But after leaving office Trump began seeking out new revenue sources, such as selling non-fungible tokens—a type of digital content hosted on the blockchain. This earned him a reported $7.2 million in 2023. Trump was convinced. His current Presidential campaign was among the first to accept cryptocurrency donations. He recently announced that—presumably in exchange for compensation—he’d become the “chief crypto advocate” for World Liberty Financial, a company led, in part, by an entrepreneur who’d reportedly sold marijuana and weight-loss products. Before Trump took the stage in Nashville, he hosted a “roundtable” fund-raiser with crypto investors, many of whom paid more than eight hundred thousand dollars to attend. Conference organizers have said that Trump raised twenty-five million dollars there. When Trump spoke at the conference, it was clear that he had been, in the parlance of Bitcoin fans, “orange-pilled.” He promised that, if elected, he would direct the federal government to hold billions of dollars’ worth of cryptocurrency reserves. The U.S., he proclaimed, would become the “crypto capital of the planet and the Bitcoin superpower of the world!” Trump began echoing the crypto campaign’s talking points. “If we don’t do it, China is going to be doing it!” he said. You might think Trump’s newfound veneration of Bitcoin would have delighted Lehane. It didn’t. Rather, it suggested that his campaign might be working a bit too well. As with Airbnb, Lehane doesn’t want the crypto industry to become firmly associated with either Democrats or Republicans, because then it will be impossible to pass legislation around it. And virtually any policy championed by Trump becomes a partisan matter by default. President Biden’s announcement, in July, that he was dropping out of the race seemed to offer the crypto industry an opportunity for a reset with the Democrats. The ascension of Vice-President Kamala Harris, a Californian with a tech-friendly record, raised the possibility of balancing the partisan scales. In a September speech about her economic plans as President, Harris pledged that the U.S. would “remain dominant in A.I. and quantum computing, blockchain, and other emerging technologies.” The détente seems to be working: on October 4th, Ben Horowitz, the venture capitalist who had appeared in the video attacking Biden, told his employees that he and his wife would be making a personal donation to “entities who support the Harris Walz campaign”—in no small part because some private conservations he’d had with Harris and her team made him “very hopeful” that, as President, she’d abandon Biden’s “exceptionally destructive” crypto policies. Lehane, for his part, has donated thirty-five thousand dollars to Harris’s campaign (and nothing to Trump’s). In the meantime, however, the crypto coalition that Lehane helped to build has begun fraying—a victim of the same partisan divides that plague the rest of the nation. In August, Ron Conway, the California power broker who had given half a million dollars to Fairshake, e-mailed the super pac’s other funders, including Andreessen and Armstrong, to complain that the campaign was alienating Democratic lawmakers. “How short sighted and stupid can you possibly be,” he wrote. Fairshake’s donations to unseat Senator Brown in Ohio were, Conway said, a “slap in the face” to Schumer. “NOT ONE PERSON BOTHERED TO GIVE ME A HEADS UP THAT YOU WERE DOINIG THIS,” he continued, proving that billionaires also ignore spell-check. “We have two factions: a moderate faction and a Donald Trump faction (Brian and Marc). . . . I have been working too long with people who [do] not share common values and that is unacceptable.” He went on, “Because of your selfish hidden agendas it is time for us to separate. . . . I will I no longer compromise myself by associating or helping.” Republican leaders began making parallel complaints. When Andreessen and crypto executives joined a Republican congressional retreat in Jackson Hole this past summer, attendees expressed fury over the fact that Fairshake had spent money on ads supporting the Democratic candidates in the Arizona and Michigan Senate races—contests that might well decide which party takes control of the chamber. Whether or not Lehane’s coalition holds together, one thing is clear: Silicon Valley has become part of a tradition that stretches back to Boss Tweed. Tech has learned how to politick. To paraphrase Ronald Reagan, the industry is mastering the world’s second-oldest profession by studying the techniques of the first. Tech’s money and emerging political savvy mean that its interests—crypto, the sharing economy, ungoverned social media—are here to stay. For the S.E.C., Silicon Valley’s turn has sparked something close to terror. “If crypto wins, you’re going to see financial firms suddenly saying their products are on the blockchain, and they’ll drive billions through that loophole,” the official familiar with the S.E.C.’s thinking told me. “We saw this happen with savings and loans, and with mortgage derivatives, and with regional banks, and it always ends badly. Something’s going to blow up, and a lot of people are going to get hurt.” Even the people who have worked on Lehane’s campaign aren’t certain that they’re doing the right thing. “Yeah, the Valley is more sophisticated now, but that doesn’t mean it’s good for the public,” the Coinbase staffer told me. “The public gives zero shits if crypto is a security or a commodity. What’s really important to them—How do I protect myself? How do I know which coin is safe?—that’s not part of the conversation. This isn’t enlightened debate and discussion. This is about using money to be a bully, so everyone knows you’re the scariest ones on the playground.” There are two ways of looking at Silicon Valley’s new political sophistication. The first is that it is a manifestation of how a modern democracy is supposed to work. As Peter Ragone, the prominent Democrat consultant, put it, “I’d rather have people getting involved and getting their hands dirty—being willing to talk about regulation and saying their opinions in public—than a situation like the past, where all the rich guys cut deals in back rooms.” Many of America’s proudest political battles—the fights for marriage equality, universal suffrage, environmental protections—succeeded only because they were backed by supporters with deep pockets and fierce tenacity, advantages that the tech industry also has. And no amount of money can decide an election unless the voters agree with the agenda. “You don’t get to take office unless you have a majority, or close to a majority, of people agreeing with you, no matter how rich you are,” Ragone said. In this view, tech-industry proponents, like many Americans, have simply learned to advocate for a cause, build a coalition, and make sure that their voices are heard. The other way of viewing the Valley’s political exertions is as a symptom of systemic rot—as proof that American governance and legislation have become so perverted by money that it is nearly impossible for people other than billionaires to further their agendas. This dynamic can be seen as particularly dangerous given that the U.S. economy has dumped lavish riches on a tiny group of disaffected, defiantly unaccountable technologists. As many critics of Silicon Valley see it, today’s startup founders and venture capitalists are, like the nouveaux riches of previous eras, using their wealth for selfish aims. In doing so, they have revealed themselves to be as ruthless as the robber barons and industrial tyrants of a century ago—not coincidentally, the last time that income inequality was as extreme as it is today. Cartoon by Seth Fleishman Copy link to cartoon Link copied Shop Lehane, for his part, acknowledges that our political system is flawed, but he believes he’s making it better. He’s been successful, he told me, only because he’s worked with so many talented colleagues devoted to building a better, fairer world. “For me, it’s always been about ‘Can you give the little guy a much bigger knife to cut a much bigger piece of the economic pie?’ ” he said. As he sees it, Airbnb fought large hotel chains so that teachers and nurses could earn extra money by renting out their empty bedrooms. Coinbase has given people a way to sidestep the big banks and their onerous fees. Many entrenched industries have used politics to benefit themselves at the public’s cost. It’s only fair, Lehane argues, to let Internet upstarts fight for their agenda; he says his advocacy is rooted in a passionate belief that tech, if regulated wisely, can help the powerless get their share. Of course, this mission has also made Lehane very wealthy. (He declined to disclose precisely how wealthy.) “But, at the risk of being incredibly hubristic, there’s a lot of places I could have gone to make money,” he said. What motivates him, he added, is a righteous battle. His X profile features a photograph of him in boxing gloves, grimacing mid-punch. In August, OpenAI, the artificial-intelligence giant, announced that it had hired Lehane as its vice-president of global affairs. Unlike the battles that he’s fought at Airbnb and Coinbase, where the ideological lines of combat have been easy to define, the political fights over artificial intelligence are murkier and more nascent. There are numerous stakeholders with competing interests within the tech industry itself. Marc Andreessen, for one, has called for little to no additional regulation of underlying A.I. technologies, because, he wrote in a jeremiad last year, hampering the development of technology that might benefit humanity “is a form of murder.” In other words, “any deceleration of AI will cost lives.” He left it unsaid that creating regulations would also likely make it more difficult for him and other venture capitalists to find fast-growing companies to invest in, thereby denying them profits. On the opposing side is a contingent of A.I. engineers who believe that their creations may soon become powerful enough to exterminate most of humanity. Regulation, therefore, is urgently needed to insure that only the most enlightened technologists can practice this mysterious alchemy. The technologists pushing these arguments, inevitably, place themselves among those enlightened few, and their “more responsible” visions of A.I. development often align with the business plans of their own startups. Somewhere in the middle is Lehane and OpenAI. The company made an opening salvo in July, when its chief executive, Sam Altman, published, with Lehane’s support, an op-ed in the Washington Post which portrayed the fight around A.I. regulations as one pitting democracies against authoritarian regimes. “The bottom line is that democratic AI has a lead over authoritarian AI because our political system has empowered U.S. companies, entrepreneurs, and academics,” Altman wrote. But that lead is not guaranteed, he continued, and it can be protected only if Congress passes regulations that encourage important software advances—like OpenAI’s ChatGPT chatbot—and also prioritize “rules of the road” and “norms in developing and deploying AI.” OpenAI, Altman indicated, is prepared to accept substantial constraints on data security and transparency, and it supports the creation of a government agency to regulate A.I. development and use. This rhetoric may sound high-minded, but—not surprisingly—Altman’s position is also somewhat self-interested. The company’s smaller rivals would probably find such rules and norms expensive and cumbersome, and therefore have a harder time complying with them than OpenAI would. The op-ed was also an example of Lehanian reframing: instead of talking about big A.I. companies competing with small startups, or about the inevitable tensions between rapid technological leaps and slower but safer progress, Altman recast the A.I. battle as one between good and evil. And Silicon Valley, in this story line, is the home of virtuous superheroes. Some observers of the A.I. industry find this perspective cynical. Suresh Venkatasubramanian, a professor of computer science at Brown, is a co-author of the White House’s “Blueprint for an AI Bill of Rights,” which urges regulations on data privacy and transparency, and protections against algorithmic discrimination. He told me, “You notice OpenAI doesn’t want to talk about its alleged theft of copyrighted materials, which is definitely anti-democratic and, if true, definitely anti-American.” (ChatGPT was developed by inhaling texts from the Internet without paying—or, for the most part, crediting—their authors; OpenAI claims that this is fair use.) What’s more, Altman’s reframing elides important issues that democratic nations might disagree on, such as what kinds of privacy regulations ought to govern A.I., and who should pay for the environmental costs of A.I. data centers. But Lehane’s strategy of putting Altman forward as a strong political voice guarantees that OpenAI, and the A.I. industry as a whole, will continue to influence the American political conversation for years to come. Venkatasubramanian told me, “The goal is to get a seat at the table, because then you have influence over how things turn out.” The A.I. industry’s influence is already being felt in state capitals. Workday, a giant human-resources software company, has been lobbying in several states to add what could be a subtle loophole to legislation about “automated decision tools” in the workplace. Companies that, like Workday, sell A.I.-enhanced software for hiring employees would essentially be immune from lawsuits over racial discrimination, or other biases, unless a litigant could prove that A.I. was the “controlling” factor behind the rejection of a candidate. “It all comes down to just one word in the legislation,” Venkatasubramanian said. “One word makes all the difference, and if you are at the table, and involved in the conversation, you can nudge that word into the legislation, or out of it.” Even Lehane admits that the A.I. campaign is in its early stages. The exact pressure points aren’t quite clear yet. Alliances and enmities are constantly shifting. What is certain, though, is that Silicon Valley will continue to bully and woo politicians by deploying money—and its giant user base—as a lure and a weapon. Things could change: the robber barons of the Gilded Age were eventually brought down; twentieth-century industrial tyrants were, over time, shamed into retreat. The most well-known tech companies—Google, Apple, Meta, and Amazon—have become bêtes noires to people on both the right and the left. (So far, though, this seemingly hasn’t done much to harm profits, or to cow executives.) Democracy, in all its mess and glory, may prevail. The only fixed truth about technology is that change is inevitable. Most of the tech industry “has run independent of politics for our entire careers,” Andreessen wrote when he announced that his political neutrality was over. Going forward, he would be working against candidates who defied tech. As Andreessen saw it, he didn’t have a choice: “As the old Soviet joke goes, ‘You may not be interested in politics, but politics is interested in you.’ ” ♦",
    "commentLink": "https://news.ycombinator.com/item?id=41765734",
    "commentBody": "Silicon Valley, the new lobbying monster (newyorker.com)212 points by fortran77 5 hours agohidepastfavorite210 comments whatever1 3 hours agoI like how they started with different values compared to the old American companies (oil, cigarettes, defense). But they ended up exactly the same. Helping the worst regimes, all-in in military complex, taking advantage of screen addiction and teen insecurities. But hey at least they don’t wear white collars like the old bad guys. reply d_burfoot 2 hours agoparent> different values compared to the old American companies (oil, cigarettes, defense). To play devil's advocate a bit, I think those old industries started out just as idealistic as the tech sector. The oil companies pursued amazing feats of science and engineering to power the new industrial economy. The defense companies made remarkable technological advances to help deter Soviet aggression and win the space race. And before the link to lung cancer was discovered, tobacco was like coffee: it gave people a nice cognitive boost, as well as possibly helping them stay thin. reply joecool1029 1 hour agorootparent> And before the link to lung cancer was discovered, tobacco was like coffee: it gave people a nice cognitive boost Now you can just buy nicotine products for that boost with a substantially reduced risk profile (it's not a carcinogen). Some writeup on it: https://gwern.net/nicotine reply sangnoir 1 hour agorootparentprev> it gave people a nice cognitive boost, Does nicotine give a cognitive boost, or does it return one to their pre-addiction baseline? reply BriggyDwiggs42 1 hour agorootparentSame question applies perfectly to caffeine. I might just be justifying my addiction, but it’s possible this is still worthwhile as it allows one to manually select when they want to expend their mental energy, overriding the normally semirandom fluctuations brought on by exhaustion, mood, etc. reply cedilla 46 minutes agorootparentA big difference is that caffeine isn't addictive. It may seem like it, and you do get a few headaches when you quit cold turkey, but you just don't get the intense graving you get with addictions. reply chimeracoder 44 minutes agorootparent> A big difference is that caffeine isn't addictive. It may seem like it, and you do get a few headaches when you quit cold turkey, but you just don't get the intense graving you get with addictions. I'm curious what definition of \"addiction\" you're using to arrive at the conclusion that caffeine isn't addictive. reply lokar 7 minutes agorootparentIsn’t the standard definition that you continue to use the drug despite it causing serious negative problems in your life? xenospn 20 minutes agorootparentprevIt actually helps Alzheimer’s patients regain their faculties, if only temporarily. reply nradov 1 hour agorootparentprevThere is fairly good evidence that nicotine causes some mild cognitive benefits beyond the baseline. Of course there are also downsides even separate from the effects of smoking. https://peterattiamd.com/ama23/ reply AlbertCory 2 hours agorootparentprevThis is in no way to defend smoking. I've never smoked, and good riddance to it. However: Amidst all the talk about the causes of obesity, surely \"not smoking anymore\" is in there somewhere? reply TeaBrain 1 hour agorootparentThey aren't defending smoking. They're referring to the industry and its ethos when it originally grew. While smoking was established as being bad for health in the mid-20th century, this wasn't as clear in the late 19th and early 20th, when the tobacco industry boomed and brought many jobs to the southern US economy. reply paduc 2 hours agorootparentprevMeth addicts are often very thin as well. /s reply consteval 1 hour agorootparentPersonally I would argue that the addiction simply shifted for most Americans. It was cigarettes, now it is food, particularly ultra-processed food. There's a large subset of people, the majority in my opinion, who are somewhat prone to addiction. Most are just a wee bit prone. They certainly won't peruse the streets for drugs. But, if something is readily available and socially accepted, they'll do it. Before this was smoking, now its ultra-processed foods. Fast food, junk food, sweets type stuff. It's probably still a boon, I'd say. I mean, I think being obese is probably healthier than smoking. But we didn't really \"solve\" anything, we just moved the problem. reply cycomanic 18 minutes agorootparentThe connection is even closer. Tobacco company extensively invested in the highly processed food industry and brought their advertisement experts in. The obesity crisis (and addiction to sugary and fatty processed foods) is not an accident, it's the result of a sophisticated advertisement campaign directed by the brains behind tobacco and alcohol campaigns. reply willy_k 19 minutes agorootparentprevNot very surprising, considering it’s the same companies using the same playbook. I’m not so sure that being obese is healthier than smoking, being obese might not give you cancer but it will certainly mess with your heart, and I would not be surprised if someone who smoked for the same amount of time someone else was obese ends up having a better prognosis after quitting. reply kibwen 2 hours agoparentprev> I like how they started with different values compared to the old American companies (oil, cigarettes, defense). They started with the same values: make as much money as possible, even if it destroys the world. Any entreaties to the contrary were a smokescreen to deflect criticism until they could shore up their position. This is the natural end-state of incentivizing profit maximization. reply mrthrowaway999 2 hours agorootparentMoney makes bad. Get rid of money, no more bad. Simple as. reply willy_k 16 minutes agorootparentNo, power makes bad. Money drives bad because it makes power, but getting rid of money will not solve any problems, and historically it’s made them worse. reply drawkward 32 minutes agorootparentprevScale makes a difference. reply safety1st 2 hours agorootparentprevHow does your worldview explain the GPL? reply Matl 1 hour agorootparentRMS is not Mark Zuckerberg? reply tensor 1 hour agorootparentprevWhat does the GPL have to do with profit maximization? reply __MatrixMan__ 2 hours agoparentprevIf you let money drive, you end up where money wants to go. reply patcon 1 hour agorootparentI like this framing. Anyone care to suggest what money \"wants\" or where it wants to go? I know it's a flawed question, because money is in many ways inert, but we have similar caveats around genes and evolution. Non-sentient things can still create persistent structures that we can label with our human values. Or something :) reply __MatrixMan__ 1 hour agorootparentIts ancestors wanted to feed the Roman army, and to ensure that the recently conquered stayed conquered next year. I don't think it's much different today: provide legitimacy to cases where those with power want to coerce those without. The modern wrinkle is that our effects on the world are more significant. This design has side effects (re: the climate, to name just one). These are side effects which we may not be able to tolerate forever. I think that money, being a status-quo preserver, \"wants\" us to ignore them for as long as possible, but since they're cumulative, that's going to be tougher sell as time goes on. Also, It's an information technology. As the makers of such things I think it's up to us to figure out what the next version should look like. reply anonyfox 1 hour agorootparentprevI think money is simply like mass: it has some kind of gravitational force, so people are drawn to it (the more money the stronger the pull), and it attracts itself (money tends to accumulate like a black hole), if left unchecked. The only way to interfere with it is by using other unrelated forces, like kindness, just like we can move masses around with electromagnetism. reply Vegenoid 1 hour agorootparentprevMoney wants to go to where more money is, and accumulate in great pools that can pull in more and more money. Money seeks to erode barriers that would prevent this, so that the money pool can accumulate faster and faster. This force can overcome morals that are barriers to the process of money accumulation. reply stackskipton 1 hour agorootparentprevMoney tends to want more money. Thus, it's extremely amoral so we shouldn't be shocked when it does things people consider bad. reply usefulcat 47 minutes agorootparentprev\"Show me the incentive and I'll show you the outcome.\" --Charlie Munger reply scoofy 25 minutes agoparentprevTim Wu discussed this at length in The Master Switch back in 2010: https://www.goodreads.com/book/show/8201080-the-master-switc... reply flakiness 19 minutes agorootparentAlso \"The Curse of Bigness\" is as relevant as that book. https://www.amazon.com/Curse-Bigness-Antitrust-New-Gilded-eb... reply linguae 2 hours agoparentprevExactly. Silicon Valley used to be countercultural, an alternative to Corporate America. Yes, there was always a money-making emphasis, and there’s a long history of less-than-ethical practices, but this was countered by many positive aspects of the Valley, such as the paying-it-forward ethos and the feeling that technology matters. There’s a reason Xerox PARC was placed in Palo Alto and not in New England. There’s a reason why many of personal computing’s pioneers had a countercultural vibe. Today Silicon Valley is the establishment. It seems less like the nerd mecca it used to be (remember Weird Stuff?) and is now a place that is much more obsessed with money. When a basic 1960s suburban tract house within a reasonable commute from work costs more than $2 million, it’s hard not to be obsessed with money. There are still good people and awesome technologies in Silicon Valley, which is one of the main reasons I’ve decided to stay in the Bay Area, but it seems like some companies in Silicon Valley have gone absolutely mercenary, and they have eroded the goodwill the area had as recently as a decade ago. But I think this is part of the natural evolution of industry. The same could be said about the history of the railroad and car industries in America. Think of how essential telecommunications and electricity are to modern society, yet I’ve yet to meet a person who loves Comcast and PG&E. reply literallycancer 24 minutes agorootparentIt has nearly nothing to do with the tech industry or counterculture and everything to do with being shit at urban planning and selling land to speculators on the cheap, so you end up with nowhere for the sprawl to go, and the land owner voting block that sabotages any attempt at building high density settlements. reply cornercasechase 2 hours agorootparentprevI used to think this was the case as well, even though I suffered through the reality: computing was always a rich person's game. Even more so in the early days. Even a C64 was an upper middle class toy. Cosplaying counter cultural (really well) doesn't change the fact that if you cold afford to build the foundation of the computer industry, you were at least very aligned with capital. Now that alignment has been exposed, highlighted and scaled to global proportions. reply lovich 1 hour agorootparentMy eyes were opened when I started realizing the Wikipedia pages of luminaries in the software field had blue links to their famous parents and those parents also had blue links to _their_ famous parents Hell I’m guilty of some myself. I was surprised early in my career when I found out that most of my peers did not have a computer and internet access as children in the early 90s reply CPLX 1 hour agorootparentprev> Silicon Valley used to be countercultural, an alternative to Corporate America. Turns out this is bullshit. It's fine I bought into it too. But it's a pretty unbroken path from today back through the history of business in Silicon Valley. It's always been a smokescreen. I recommend this book, it's highly illuminating and specific: https://www.amazon.com/Palo-Alto-History-California-Capitali... reply mempko 2 hours agorootparentprevYou need to watch \"All Watched over by Machines of Loving Grace\" by Adam Curtis which outlines Silicon Valley's long Libertarian roots. Making money and greed has been deeply embedded from the start. reply Terr_ 35 minutes agorootparent> \"All Watched over by Machines of Loving Grace\" This reminds me of the tycoon's thunderously semi-religious speech from The Network [0], which has aged pretty well for a movie 50 years old. > The world is a business, Mr. Beale. It has been since man crawled out of the slime. And our children will live, Mr. Beale, to see that... perfect world... in which there's no war or famine, oppression or brutality. One vast and ecumenical holding company, for whom all men will work to serve a common profit, in which all men will hold a share of stock. All necessities provided, all anxieties tranquilized, all boredom amused. [0] https://www.youtube.com/watch?v=35DSdw7dHjs&t=0m43s reply marxisttemp 55 minutes agorootparentprevAbsolutely love Adam Curtis. I was first exposed to AWOBMOLG in the basement of the Palais de Tokyo as an idealistic undergrad, and bristled at his claims. Since then it has become one of my favorite pieces of filmmaking. Great soundtrack too. reply bee_rider 1 hour agorootparentprevIs it based on the poem or just named after it? I don’t remember anything particularly libertarian about the poem. reply marxisttemp 57 minutes agorootparentIt is just named after the poem, as a reference to the SV ideology of stepping back and letting the machines and the networks decide our future. reply azinman2 3 hours agoparentprevIn terms of numbers, there aren’t that many SV companies that exploit kids via insecurities and screen addiction. But I don’t doubt their impact. reply csomar 2 hours agorootparentThe ones that do are so huge that the number of companies doesn't matter. Facebook market cap is 1.5 Trillion and has $150bn of revenue every year. That's more than many countries around the world. (and I am comparing revenues to GDP). reply azinman2 2 hours agorootparent“I don’t doubt the impact” reply drawkward 3 hours agorootparentprevI am not sure what your subtext here is. Are you saying it doesnt matter as much because there are fewer SV companies? reply mjlawson 3 hours agorootparentMy take is that maybe we shouldn't paint all of SV with the same brush. Not every SV company is Philip Morris. reply linguae 2 hours agorootparentYou are right; there are still many good people and companies in Silicon Valley. Unfortunately the Philip Morrises of Silicon Valley have outsized influence since they control major social media platforms, Web infrastructure and standards, advertising networks, cloud computing platforms, and other essential tech infrastructure. Locally, they are also large employers in the Bay Area, and due to the housing crunch it is very difficult for non-“Philip Morris” workers, even in tech, to compete for housing (not every software engineer makes $200k+ in salary and gets RSU grants), which is pushing some of them out the Valley, helping make the Valley even more of a “Philip Morris” shop. reply mistercheph 33 minutes agorootparentprevBut all of them would kill for Philip Morris levels of product-market fit. reply drawkward 2 hours agorootparentprevRight, only the ones that touch all of humanity are Philip Morris! reply plussed_reader 2 hours agorootparentprevMaybe you shouldn't demonize all of SV, and it's the one working against your self interest? I admit that's a large ask for a consumer, to keep track of a larger organizations abusing you. reply azinman2 3 hours agorootparentprev“But they ended up exactly the same. Helping the worst regimes, all-in in military complex, taking advantage of screen addiction and teen insecurities.” This implies all. I then said I don’t doubt their impact. They’re not all like this is my point. reply drawkward 2 hours agorootparentThank you for clarifying. reply karmasimida 28 minutes agoparentprevMaybe because when the government looks to regulate them, that is why the lobbying effort takes up? Nothing ever has changed reply whoomp12342 53 minutes agoparentprevhoodies are the new white collar reply Rebuff5007 2 hours agoparentprev\"You either die a hero or you live long enough to become the villain\" reply A4ET8a8uTh0 3 hours agoparentprevMoney and power is clearly one hell of a drug. reply jajko 3 hours agorootparentAlso a collective mindset - if most people one knows are doing exactly the same, morality lines tend to conveniently blur, it requires very strong personality to keep the inner compass untainted for decades. And its a grey business so to say, you can see various justifications for it also here since many folks here work for them, its not some Zyklon-B factory. At the end, everybody under certain circumstances has a price, and kids tend to blur this even more for many. reply sitkack 3 hours agorootparentThis is one of the many reasons that tech wants to hire young. reply ChrisMarshallNY 2 hours agorootparentWhat really gets me, is very young software engineers (not managers or marketers), that emulate the values and behaviors of tech bro billionaires. It’s not surprising, but it is kind of disappointing. When I was young, I was convinced that all the bad in the world, was the fault of evil, greedy old men. If young folks ran things, we’d be empathetic, kind, ethical, and positive. As SV is basically completely run by young folks, of various races, genders, and backgrounds , I have to admit that it is actually worse, than the evil old men. reply fallingknife 2 hours agorootparentprevThere is only one reason that tech hires young and that's the growth rate in the number of software engineers. The profession looks young simple because there are probably 10x as many now as there were 30 years ago, so naturally there will be a whole lot less 50 year olds than 20 year olds. reply sangnoir 1 hour agorootparentThis is not new: read articles of transcripts of speeches from 10+ years ago by Thiel or pg that lionize young, inexperienced founders and the benefits of working at startups pitched to the youth. reply loeber 2 hours agoparentprevnext [4 more] [flagged] throw4847285 2 hours agorootparentThe thing about Pax Americana is that the brain trust behind sustaining it decides which regimes are good and which are evil based on their utility to the United States. This is classic empire building, and it's an effective strategy, but the moralizing about it is exhausting and hollow. Nobody with three brain cells thinks that one petrostate is more moral than another because it's backed by the US. I'm glad to live in the USA and benefit immensely from living at the heart of the modern Mongol or Ottoman Empire, but I have few illusions about the cost. reply giraffe_lady 2 hours agorootparentprevSV is doing both of those things. Palantir is to the palestinian genocide what IBM was to the holocaust. reply jasonlotito 2 hours agorootparentprev> Helping the worst regimes They aren't saying SV is firing the missiles. Rather, the suggestion is SV is helping those that do. So when you say > I don't see Silicon Valley firing missiles in the Red Sea You aren't countering the original claim. Instead, you'd have to argue that you don't see SV helping those firing missiles in the Red Sea. And, in that regard, I'm not so sure of that. Just look at crypto as a clear example of helping those regimes actually doing the firing. reply startupsfail 2 hours agoparentprevCrypto had never been particularly good. Main uses were criminal, from its start. reply edm0nd 1 hour agorootparentThis is in no way true. reply fragmede 1 hour agorootparentSilk Road definitely played a part in Bitcoin's rise though. reply architango 3 hours agoprevThe excellent financial podcaster/youtuber Patrick Boyle has a related entry, \"Crypto Has Bought The 2024 Election\": https://www.youtube.com/watch?v=kpZvC_5leDY reply sillyfluke 0 minutes agoparentCurious if there is a reference for his 86% win rate from 42 primaries for the crypro Super Pacs that another poster also mentioned? https://news.ycombinator.com/item?id=41720799 reply mistercheph 24 minutes agoparentprevAn unprecedented peek-behind-the-covers at the inside of mainstream media brain. Man with wide eyes and stock photos and nordvpn sponsorship on set with full makeup and steampunk costume rambles through various headlines, not forgetting to show you how to (mis)interpret facts, and how to manifest life as a rabid political fanatic, ready to ruin any dialog you come across with barking and screeching and red eyes. There is nothing in that video that even resembles thought or rationality, it is just classic mainstream media non-sequitur that adorns the regalia of knowledge and batters the viewer with meaningless \"facts\" and \"figures\" until they are in a state of ready-to-be-filled aporia. reply nickpinkston 1 hour agoparentprev+1 for Patrick Boyle's channel - very informative and his dry humor is fun too reply zelias 2 hours agoprevSemi on-topic: Why should the federal government even entertain a \"strongly\" pro-cryptocurrency agenda, when the real goal of cryptocurrencies is to displace/reduce the power of the U.S. dollar, from which the federal government (and the American people) derive many, many benefits? reply RestlessMind 9 minutes agoparentBecause blockchains can be a great weapon against dictatorships who cannot allow free expression. US has enough cultural and institutional flexibility to accommodate dissent. First amendment protects almost any speech against the government, including insulting the president. Compare that against China, who simply cannot tolerate citizens having any freedom of expression beyond what the party allows. See \"tanks of tiananmen\" smart contract[1] or \"winnie the pooh\" NFTs [2] - the only thing CCP can do at that point is to ban the entire blockchain ecosystem. But if blockchains are a big success in the rest of the world, CCP also has to cater to it just to keep pace with the West's tech progress. [1] https://bscscan.com/token/0xb79c9c73e8c7b4be7244e697e6bdb9f5... [2] https://opensea.io/collection/poohsnft reply snowwrestler 50 minutes agoparentprevThat’s not the real goal of cryptocurrencies. It started off that way—remember people buying pizzas and domain names with bitcoin? That is the sort of thing that people do with dollars. But it became quickly apparent that cryptocurrencies suck as currency because they are so strongly deflationary. However, that makes them work great as commodity financial instruments. So viewing crypto as a financial market (not a currency), it benefits the U.S. to have as much of the market transacting within the jurisdiction as possible. Then you get the tax benefits, and ancillary benefits (e.g. rich crypto owners buying things). reply drawkward 3 hours agoprevDismantle what SV has evolved into under venture capital. When tech was scrappy and didnt have billions at stake, its products seemed to benefit humanity. Now it is (the most powerful?) part of the oppresive regime. reply ericfr11 2 hours agoparentAt least the media and public are aware. The oil industry: not so much: they are destroying the whole planet, and their lobbying is so much more secret and politically motivated reply drawkward 2 hours agorootparentOne has to wonder if we are truly aware. How much is AI ruining the environment via its constant demands for electricity and water? I'd expect more harms will come to light way down the line, as was true with big oil, tobacco, etc. reply fragmede 1 hour agorootparentnot so far down the line. Deepfakes have obvious harms and Sora/similar is obviously going to get abused (as well as used). reply wslh 2 hours agorootparentprevThe media is also part of the corrupt system though. They also have an agenda, use clickbait, and focus on the topics that maximize conversions. reply lacy_tinpot 1 hour agoparentprevIf VCs disappear overnight most tech would not change ideologically. It hasn't changed too much ideologically for the past few decades. Most were libertarian and continue to be libertarian. The minor shift is from being left libertarian to more right libertarian, though that's mostly due to the political landscape where there isn't much room for contrarian leftists. reply sangnoir 19 minutes agorootparent> If VCs disappear overnight most tech would not change ideologically. It hasn't changed too much ideologically for the past few decades. What has changed is that the proportion of people who wind up in tech primarily due to passion has drastically dropped as it became more prestigious. I'll paraphrase something I read elsewhere that rang true to me. In the 80s and 90s, amoral,greed-is-good, get-rich-by-any-means people got into banking and finance because it paid well. Now they go into tech. reply startupsfail 2 hours agoparentprevSilicon Valley is not Crypto. Most likely Crypto influencers are Putin-proxies, as usual. There is a part of Silicon Valley that is dabbing in finance and banking, and somehow this part (crypto, X.com, etc) is somehow aligned with Putin’s agenda. reply adamrezich 1 hour agorootparent> Most likely Crypto influencers are Putin-proxies, as usual. What proof do you have for this assertion? I find cryptocurrency influencers to be incredibly distasteful and lame, but I am unaware of any evidence that supports this theory. reply mrguyorama 2 hours agorootparentprevMost of that is Peter Thiel and his \"friends\". The \"somehow\" is that he made a lot of money off of Paypal and has spent the past 25 years peddling influence and building a police state, because he earnestly believes in what Putin does. They want the USA to be Russia so they can be the Prigozhins, \"caterers\" who literally own a 60k strong army. Except the rest of SV doesn't have nearly as much of a problem with that as you seem to be implying. Zuck does not care whether the US is a democratic utopia or a Russian style oligarchical hellscape, he is rich and powerful either way. And as a backup, he's going to go live in one of his bunkers if anything goes bad. YC doesn't seem to care, and is happy to give these people money for dubious or outright scam projects because \"it invests in people and teams\", as if the team and people who put together an outright scam project are worth even talking to. Most of SV's big money know each other. Very very few have openly come out against any of this, and fewer still have put money behind preventing any of this bullshit. reply bee_rider 1 hour agorootparentIt seems like a bad move though, I mean these are nerdy investor guys, right? They do well in this meritocratic system. If I was a tech billionaire I’d be very worried that a guy who’s only skills were catering and gangster stuff would have had a lot more time to study the latter, than me. reply lovich 1 hour agorootparentThese tech moguls assume they will be in charge in the event of any sort of revolution or apocalypse. Look at some of what the Reddit founders have said for an example https://www.newyorker.com/magazine/2017/01/30/doomsday-prep-... reply bee_rider 1 hour agorootparentI don’t have a subscription, but does their plan involve preposterously loyal soldiers who have no motivation to follow their instructions for any reason other than suddenly valueless money? reply lovich 4 minutes agorootparentYes. It’s not unique to the wealthy in tech, most nouveau riche seem to forget at some point that the only reason most people attend to their needs or are in their orbit is to get money and if that becomes useless or turns off they will be dropped like a bad habit TeaBrain 1 hour agorootparentprev> he earnestly believes in what Putin does According to what? I've never heard him say anything positive about Putin. He also reportedly tipped off the FBI when the Kremlin invited him to meet with Putin. reply sangnoir 9 minutes agorootparentSince 1920, the vast increase in welfare beneficiaries and the extension of the franchise to women - two constituencies that are notoriously tough for libertarians - have rendered the notion of \"capitalist democracy\" into an oxymoron I no longer believe that freedom and democracy are compatible. Both quotes by Peter Thiel in a Cato Unbound[1] blog post, 2009. The second quote is - let's call it interesting - considering Thiel is the patron of a VP candidate who may be a heartbeat away from the presidency. 1. https://www.cato-unbound.org/2009/04/13/peter-thiel/educatio... reply lovich 1 hour agoprev“New” They’ve been this way for decades. The whole “don’t be evil” facade slipped away once the money started flowing reply mitchbob 4 hours agoprevhttps://archive.ph/94cDH reply mgh2 4 hours agoprevDirty brainwashing ad tricks sneak into Congress, no surprise: “If you are pro-crypto, we will help you, and if you are anti we will tear you apart.” reply startupsfail 3 hours agoparentCrypto = money from Putin, no? The regular channels for Putin to support disintegration of democracy in the West had dried up, so Crypto is now being used. reply edm0nd 1 hour agorootparentNo. Crypto = Crypto reply rlewkov 2 hours agoprevWhat a politician wants most is to win reelection. This explains everything reply farseer 1 hour agoprevInnocent question: what other major lobbying monsters are there in the US? reply karaterobot 1 hour agoparentSomething that surprised me recently was learning that the U.S. Chamber of Commerce is a private organization. I assumed it was some office funded by the government in a town, that worked with local businesses—or something along those lines. But no, they're a lobbying group, and they are the biggest lobbying group in the country. I'm not saying they are a monster (I clearly don't know anything about them) but at that size, how could you not be? https://www.opensecrets.org/federal-lobbying/top-spenders reply lovich 1 hour agorootparentId posit that they are simply from giving themselves that name and not disabusing anyone of the notion. They like to get puff pieces in the news with their analysis on legislation and most people have the same misconception as a result reply julianeon 24 minutes agoparentprevA lot of the action has moved upstream now that Citizens United is the law (January 21, 2010: \"this decision allowed corporations and unions to spend unlimited amounts of money on political campaigns\"). Ideally from the corp. perspective you don't wait until a crisis point. You spend early and shape the narrative to prevent a crisis from ever happening. The Koch brothers created a network of donors who operate on this strategic plane more than the tactical \"this specific regulation has to be changed\" level. See Mayer's \"Dark Money\" for a comprehensive history. reply mlinhares 1 hour agoparentprevCar dealers. Also the biggest financiers of the Republican party. reply AnarchismIsCool 26 minutes agorootparentSee also, MLMs and the supplement industry. They're contributing for the legal right to screw average people out of their money. reply auntienomen 1 hour agoparentprevOil/energy, real estate pros, health insurance, ... reply js2 1 hour agoparentprevYou can view it sliced various ways here: https://www.opensecrets.org/federal-lobbying reply it_citizen 58 minutes agoparentprevBoeing reply boh 4 hours agoprevI wouldn't say \"new\". This has been a thing for decades. While the companies who are the top spenders may have changed, Silicon Valley tech as a whole has been pretty aggressively represented for some time. reply MangoCoffee 3 hours agoparentMicrosoft intensified its lobbying efforts when it faced scrutiny for bundling IE with Windows. Similarly, Facebook, Google, Apple, and others have increased their lobbying over the years. As you pointed out, this is nothing new. When companies or entire industries feel threatened, they ramp up lobbying – even Huawei hired Tony Podesta to advocate for them. Hillary Clinton's call to repeal Section 230 for stricter social media regulation will likely prompt tech companies to boost their lobbying efforts. Sooner or later, companies in emerging industries will start lobbying to protect their interests or advance their agendas. https://www.politico.com/news/2021/07/23/huawei-hires-tony-p... reply marcosdumay 3 hours agoparentprevA bit more than a decade. A while ago software companies where taking every kind of abuse from the US government, coming from well represented entertainment, finance, and all kinds of other industries. This suddenly stopped when they started to lobby. reply Eumenes 3 hours agoparentprevMy thoughts exactly. This article calls out crypto but tech lobbying has a much longer history with Amazon, Google, and Meta. I view crypto as an extension of the financial services industry at this point (look at the boards and execs of these companies; they are more finance than tech), which obviously is the king of financing campaigns. reply alephnerd 3 hours agorootparentAnd the corporation that owns the The New Yorker (Condé Nast) is a notorious lobbyist in both the US and EU around AI Regulations and AdTech. Goes to show that most political op-eds are just entertainment. reply alephnerd 3 hours agoparentprevYep! For example, the EFF was notably supported by Google back when Google was taking on Microsoft, the ITIF has been active for decades, and the Obama admin was notorious for being overrepresented by Google leadership. Ain't no point adding morality to lobbying - it's just what it is. reply renewiltord 3 hours agoprevMakes sense. Lobbyists for legacy competitors tried to screw them so they picked up the game. In Canada and Australia they tried to screw them on the news / search linking debacle. The problem is that when you come at the king, you best not miss. reply resters 2 hours agoprevIf anyone needs to see what happen, read Marc Andreesen's cringy posts on X. The guy is a MAGA promoter and sounds more and more like an elderly \"the country is going to hell in a handbasket\" geriatric conservative every second. Too many years in a bubble of undeserved wealth where people suck up to him constantly. YC helped fund/promote Palantir, the dystopian surveillance company. The Iraq war defense contractor boom pumped tons of money into pro-war entities and this is just the result of that capital finding its way into new ventures. Ideology comes along for the ride. It's the very definition of dystopian, government propaganda and big-lie-driven malinvesment and (in hindsight) massive financial fraud on taxpayers. I'm personally shocked that PG touches any of this dirty money. When is enough enough? reply pj_mukh 1 hour agoparentYC does fund defense companies, esp tech innovative ones, a posture change they made due to the situation in Taiwan and Ukraine [1] But they didn’t fund Palantir or Anduril or any of the companies that blew up during the Border Panics of 2016-2020. Unless you’re playing guilty by association games in which case ..carry on. [1]: https://www.ycombinator.com/companies/ares-industries reply resters 1 hour agorootparentYC gave Palantir a massive platform at startup school to present the company and vision and recruit talent. Far more than the seed investment would have been worth. With respect to the ares industries, YC buys into the neocon view strongly enough that it wants to help create the next wave of military tech. The only reason anyone perceives these \"threats\" and perceives the US to be \"woefully unprepared\" because of the marketing budget the defense industry has after profiting handsomely from the Iraq wars, etc. The US is not in danger and does not need new and state of the art weapons systems. We are not losing in Ukraine because we don't have adequate weapons but because we are playing a very stupid and risky strategy. See the comments of Jeffrey Sachs on all of this. reply marxisttemp 1 hour agoparentprevPG is good buddies with Peter Thiel. Why do you think he’s not a part of this neo-monarchist SV milieu? reply beezlebroxxxxxx 3 hours agoprevThe Andreessen and Horowitz quotes in this are particularly hilarious and pathetic. The desperate attempts at linking their greed to the fate of the country itself is so nakedly self-serving (as are many of the quotes from these lobbyists and other VCs) that they should make them a laughing stock. The Citizens United decision will be a cancer in American politics and a boon for the ultra-wealthy for generations. reply coredog64 3 hours agoparentWhat specifically in Citizens United is bad? Are there any positive outcomes from CU? reply drawkward 2 hours agorootparentFor one thing, it equates money with speech, which destabilizes our democracy by giving more power to those who have money. Obviously, money does that in many ways outside of the electoral process, but one would imagine that any serious democracy would want to keep its democratic processes maximally equitable. reply IG_Semmelweiss 2 hours agorootparentI hear you , but that doesn't seem to pass the smell test. The vast majority (97% ?) of voters are low-information voters. 1 such vote is equal to 1 high information vote (3% ?), those who research, read bills, etc. So the process is already not maximally equitable - in fact, the system severely punishes high information voters via opportunity cost. At that point dollars positively affect your outcome because with paid mediums , low information voters can now get more information, despite being unwilling to inform themselves via hard work. So that's a benefit. That information flow is subject to competition, so the best ideas (and information) will win out. That's another plus. Money competes for eyeballs, and your dollars are just as good as mine. If you have more $$, i can pool my friends and beat you. There's nothing more equitable than that we have discovered so far. Now, enter someone with a ton of money, Musk. You could argue that he would tilt the game. But we then observe the opposite: Billionaires are also subject to the law of diminishing returns. 'Donations' will only buy so much favor, and then they become a losing proposition. So, do you want a billionaire burning his cash in the political process? The answer is absolutely: it signals a level of credibility and conviction, when business opportunity is already a loser. So in short, you want people to risk all their chips in something they believe, instead of limiting their bets and just get rich off the house, playing the long game . Perhaps we start from the other side of the table. Maybe money is not the problem. Perhaps it is the power concentrated in too few individuals ? How about cutting off power from those that are selling favors to the highest bidders ? reply zelias 1 hour agorootparentThe massive influx of money has empirically had a tendency to turn a substantial portion of your \"high information votes\" into \"high _dis_information votes\" reply piva00 1 hour agorootparentprev> At that point dollars positively affect your outcome because with paid mediums , low information voters can now get more information, despite being unwilling to inform themselves via hard work. So that's a benefit. That information flow is subject to competition, so the best ideas (and information) will win out. I don't believe in this rosy view of the \"marketplace of ideas\", fear wins out, populism wins out, not the best ideas. The best ideas have nuance, they demand to be understood in context, they aren't catchphrases nor explore scapegoats for societal problems. All of that demands a high-information voter to parse through and judge what's the \"best idea\" within their context, environment, community, etc. No, I do not believe that allowing ideas to fight it out inevitably leads to the best ideas raising to the top, people don't vote intellectually, they vote emotionally and the person who tries to fight strong emotions (such as fear) with ideas will lose out. reply drawkward 1 hour agorootparentprevAmerican democracy does not give special preference to information content of a voter, nor should it. Did you skip the entire Jim Crow era of history where the racially-presumed information content of a particular voter was used to disenfranchise those voters, even spuriously? Our constitution does not begin, \"We, the highly-informed people...\" Anything else, and you are no longer talking democracy. reply drawkward 1 hour agorootparentprev>That information flow is subject to competition, so the best ideas (and information) will win out. Did you miss all of COVID era on social media? reply IG_Semmelweiss 35 minutes agorootparentFor the best ideas to win, they must subject themselves to open conflict. I didn't make a claim on how long conflict would last, but money is not infinite. Noise is part of the system, and the system is not perfect, but that's a reflection of our imperfections, vs anything wrong with the final outcome. reply drawkward 24 minutes agorootparentYes, well we are discussing elections which very definitely have time frames. We can't wait for Edward Gibbons to come along 2 millennia later and tell us that some bullshit idea's poisoning of this particular election is going to wreck democracy. Add to that the fact that bullshit is cheaper to produce than truth. Sprinkle in the notion also that bullshit is likely more profitable too. Consider what might happen if the bullshitters control orders of magnitude more money in the first place, and then filter through Brandolini's Law: Add to that https://en.wikipedia.org/wiki/Brandolini%27s_law Democracy is too sacred to be left to the loudest. reply fallingknife 2 hours agoparentprevCitizens United was the correct decision. The alternative is our government deciding who gets to engage in politics. Without Citizens United politics would be even more restricted to a much smaller set of even wealthier people and established politicians who have access to the governemnt approved outlets of political speech. reply smaddox 54 minutes agorootparentFalse dichotomy. Prior to CU, individuals could contribute to any candidate of their choosing up to an annual contribution limit. Companies are compased of individuals. CU was twisted logic to reach a preconceived, corrupt outcome. reply Manuel_D 0 minutes agorootparentYou are misunderstanding what the Citizens United vs. FEC determined. Candidate contributions are still limited, even after CU. What's not limited is independent expenditures. p_j_w 2 hours agorootparentprevThis is a reasonable sounding hypothesis that has been shown to be completely false. reply drawkward 2 hours agorootparentprevThe correct decision is to set a limit on campaigning, and fund it with tax dollars, no outside spending. reply Manuel_D 0 minutes agorootparentDonations to political campaigns are still limited. Even after Citizens United. What's not limited is independent expenditures. You can, for example, organize rallies or put up billboards advocating more serious efforts to combat climate change without limit. fallingknife 2 hours agorootparentprevSo we should let the government choose who gets campaign funding? Sounds like an established politician's wet dream. reply throw4847285 2 hours agorootparentOh no! Established politicians! They know how the government works, and could possibly be effective. We can't let them get reelected! I joke, but the current system is so much more vastly corrupt than this imagined scenario where small potatoes money gets handed out equally to all candidates in a race, that I just can't really imagine what your actual nightmare scenario might be. reply snowwrestler 44 minutes agorootparentThe nightmare scenario is that the money is not handed out equally to all candidates. If corrupt incumbent politicians have complete control of the finances of an election, how could they be held accountable? Anyone wanting to run against them on this issue would simply be denied the money to do so. reply drawkward 35 minutes agorootparent>If corrupt incumbent politicians have complete control of the finances of an election, how could they be held accountable? The exact same way any relief is sought against the government: the court system. reply snowwrestler 25 minutes agorootparentOk, and who appoints the judges? And who is responsible for enforcing court decisions? reply drawkward 2 hours agorootparentprevAssuming that what you say is the outcome of a proposal like mine (which, btw, it isn't): I would rather have the government (which is beholden to the constitution) than the wealthy (who aren't beholden to the constitution) be the gatekeeper. In fact, this money would be available to anyone who met whatever threshold was legally enshrined, a far better alternative than having to kiss the ring of an oligarch, as you propose. reply piva00 1 hour agorootparentprevMany modern, developed democracies do exactly that, and are quite more democratic than 2 parties running the state. As well as not making elections a show might help democracy instead of the best showman. I will be waiting for the American exceptionalism arguments on why a thing that well functioning democracies/countries do wouldn't work in the USA. reply drawkward 1 hour agorootparentOh god not gun control! /s reply 9dev 1 hour agorootparentprevThe government should set sensible limits on campaign contributions that avoid a system of legalised bribery, aka. SuperPACs, such that actual people get to choose their representatives, not a handful of ultra-rich with the means to steer politics. The only candidate in American politics that lives up to your idealised, independent politician of the people, is Bernie Sanders. And guess what? He strongly opposes billionaires buying themselves a government. reply ikrenji 1 hour agorootparentprevcitizens united = sell the election to the highest bidder. no need to put lipstick on a pig reply samatman 17 minutes agorootparentprevIndeed. The First Amendment guarantees freedom of the press, which in clear language protects free speech for corporations. That includes the right to engage in political advocacy, which is what Citizens United v. FEC was about. A lot of people have a, let's say filtered understanding of the decision, which has become a sort of shibboleth rather than a direct reference to the Supreme Court's ruling. I encourage reading up on the ruling itself, at least the Wikipedia summary https://en.wikipedia.org/wiki/Citizens_United_v._FEC Or should you prefer primary sources (as ideally you should) just read the ruling https://supreme.justia.com/cases/federal/us/558/310/ The canonical source, just for completeness. Justia has the same text https://www.supremecourt.gov/opinions/boundvolumes/558bv.pdf reply pakyr 2 hours agorootparentprev> Without Citizens United politics would be even more restricted to a much smaller set of even wealthier people Really? Because there are many countries with far stricter regulations on campaign financing than the US had pre-Citizens United, that also have much more working class representation in their politics than the US.[0] [0]https://www.noamlupu.com/Carnes_Lupu_ARPS.pdf reply ModernMech 2 hours agoprevFrom \"Don't be evil\" to \"We're the baddies\". reply bbqfog 4 hours agoprev\"Political VC\" has been the worst and most damaging industry trend I've ever seen. From overt Zionism, to Gary Tan constantly whining about SF politics, to Reid Hoffman interfering with elections... Peter Thiel funding Trump, Vinod Khoslha fighting zoning laws... None of that has to do with technology or innovation, yet as innovators we are forced to navigate some of the most toxic politics known to man. I no longer seek venture capital because I don't want to work with any of these people. We need alternatives to fund innovation. reply geodel 2 hours agoparentOh \"we, the innovators\" need a new \"innovation fund\" what an awesome idea. I wonder what kind of innovations has been brought by these innovators. reply giraffe_lady 3 hours agoparentprevYou get a much more usable framework if you drop the idea that there is some \"pure\" tech or innovation that politics are a corruption of and instead realize that technology and innovation are always political. Politics were there from the beginning, but they may have been invisible to you in a previous era. Understanding how that could be and articulating what the politics were before is key to deciding what to do about the changes now. reply bbqfog 3 hours agorootparentI agree, so much of our tech was built out as part of the global war machine. It was easier to keep your head in the sand with \"libertarian-esque\" ideas (at least for me), but as those ideas came to fruition and social media exposed the thoughts of the people behind the money, the reality of the situation became easier to see. reply AlbertCory 2 hours agoparentprev> We need alternatives to fund innovation I know! A federal Department of Innovation! How about that? reply broken_clock 3 hours agoparentprev> None of that has to do with technology or innovation Liberalism was the greatest innovation we ever came up with as a species. VCs are paid to have strong opinions on technological innovations. Why wouldn't they have strong opinions on the institutions and systems of governance that make tech possible? reply candiddevmike 3 hours agorootparentBecause their strong opinions aren't enabling technological innovations, they're enabling commercialization and regulatory capture so they can make money. reply lazide 3 hours agorootparentOh my. And you think the prior attitudes were honest on their face, and not about undermining the prior incumbents so they could eat their lunch - to get the new folks here? The prior incumbents were the way they were because they were incumbents. all incumbents end up doing these things. Because they work. At least until someone figures out the chink in the armor, and it stops working. Welcome to the new set. Die young a hero, or live to be a villain. reply mrguyorama 2 hours agorootparentprev>Liberalism was the greatest innovation we ever came up with as a species Pottery, agriculture, control over the electromagnetic field, basket weaving, LANGUAGE, formalized society, written language, MATH, metalworking etc and you posit UNREGULATED MARKETS as the greatest innovation? NONE of the above monumental achievements of humanity are even remotely related to people being able to do business with slightly less regulation and slightly fewer taxes. Nearly all human innovation has occurred when being murdered by your king for petty reasons was the norm. Integrated circuits as a commodity mostly exists because the US air force wanted it to. None of that is about liberalism. The Navy trained up hundreds of thousands of people essentially to the level of electrical engineers, basic information sciences, and computers and computation and electric fields for WW2, to improve radar and targeting, including building and deploying a networked computer system for automated fleet defense and intercept tasking in the fifties, and then these people are mostly just let go, to spread their paid for knowledge to anyone in the commercial space, no non-competes. Wouldn't you know it, training up a bulk of people in a brand new but important field, with minimal cost to them, and then just freely giving that away reaped insane rewards for the next 50 years! reply bbqfog 3 hours agorootparentprevI don't agree at all that Liberalism was the greatest innovation. Neo-liberalism is quite the opposite actually. VCs are part of a rigged system of global capital. They get paid to be part of that system and continue its existence. It has nothing to do with innovation other than occasional random bets that pay off. reply loufe 2 hours agorootparentI'm not sure why you're getting voted down. The HN guidelines clearly state downvotes are not to be used to express disagreement. Your point was politely and concisely made, and stuck to the topic. Do better, voters. reply immibis 2 hours agorootparentWhat is supposed to happen and what actually happen are two different things. Example: Liberalism was supposed to make everyone prosperous, but it actually made everyone more prosperous according to metrics that don't matter (CPU cores per pocket) while taking away all the prosperity that does matter (stability of living space, nutritional quality of food, etc). reply fallingknife 2 hours agoparentprevIt has everything to do with tech an innovation. Nothing shifts the balance of power in a society like technological advancement. How could you possibly think a multi trillion dollar industry could avoid politics? You can't just bury your head in the sand. reply brodouevencode 1 hour agoprevThe comments in this thread are rather disheartening. Very few here were complaining four years ago when The Valley did all the bidding of politicians in the form of canceling those who questioned the Covid origins or the need for masks and vaccines, deplatformed/banned/mocked anyone talking about Hunter Biden's laptop, or disagreeing with oppression-movement du jour. Now that the tech bros have flipped sides it's a sudden problem. It's (nearly) completely one-sided on here. The honorable thing to do would be to acknowledge that this is an echo chamber if you're going to complain. Now, I fully expect to be downvoted into oblivion for this take, but don't let that stop you. reply John_Cena 23 minutes agoparentI was ostracized at a right-wing small military contractor during covid for being skeptical of the novel medical treatment being offered for free with zero liability for those profiting from it. I almost stopped trying to rationalize the world around me at that point in my life. I assume my experience was simply because the CTO had everyone under his thumb and he decided it was a good idea. reply mistercheph 5 minutes agoparentprevMoney loves power, silicon valley will continue to vanguard political causes that extend political, economic, psychological, and spiritual control over people, through useful idiots at the IC level that actually love the fart smells and don't care which direction the wind is blowing the farts towards, an HR machine that marches to the beat of a global borg-impulse, tirelessly filling any empty space or silence they can find with words and systems that attack the spirit, and a psychopathic management class that is unrepentantly self-serving. reply Sytten 1 hour agoprevThose articles kept reminding me that I am right to be very cynical. Is lobbying #foundermode? /s reply Der_Einzige 3 hours ago [flagged]prevnext [29 more] Imagine if Silicon Valley went back to its roots of backing cyber-libretarians like Ron Wyden or Bernie Sanders instead of its really bizarre right-wing turn all because they didn't like that a brown lady proposed taxing them a bit more. Folks that think that Trump and his friends like \"freedom of speech\" or any of the type of techno freedoms we take for granted are in for a deeply rude awakening. The reality is that democrats are so, SO much better on most cyber issues that you're actually a deplorable if you are voting for trump. Good luck having right to repair or net neutrality when republicans run the show, and losing that is the speck of rice sitting on the tip of the ice-berg of shit they will do if empowered to do so in the context of taking away your cyber freedoms. There's a reason why Stallman and his like are attracted to green party politics despite the green party being ineffectual and (more recently) likely a russian psyop - it's because they are the only party consistently taking a stand to defend open technology in all of its its instantiations. Even the \"libertarian\" party is extremely unreliable about this. The right-wing turn is absolutely disgusting and should be shattered as soon as possible with prejudice. reply game_the0ry 3 hours agoparentHillary Clinton went on TV and said we should get rid of Section 230 in order to moderate content. So is it really that surprising that SV made a right turn? reply AlexandrB 3 hours agorootparentThis argument doesn't hold water. Hillary Clinton is kind of irrelevant at this point. She doesn't hold any kind of elected office and remains pretty unpopular among lefties after losing in 2016. Meanwhile Trump openly tried to reduce the protections of Section 230 while he was president[1]. [1] https://www.forbes.com/sites/abrambrown/2020/05/28/what-is-s... reply jkaptur 3 hours agorootparentprevDonald Trump is strongly against Section 230. https://www.cfr.org/in-brief/trump-and-section-230-what-know reply ajuc 3 hours agorootparentprevSV made the choice before (by accepting money from dictatorships for political influence via social media). Attempts at regulation were a reaction to that. There's no way the situation can stay like that forever. There will be regulation like there's regulation of TV, radio, press, etc. Without regulation you don't get a libertarian utopia - you just get oligarchy. reply mlinhares 3 hours agorootparentWait, the libertarian utopia IS oligarchy. So it is all right by their standards. reply ajuc 2 hours agorootparentThere's orders of magnitude more libertarians than oligarchs. It's even in the name :) Most libertarians aren't rational egoists, they are just naive losers oligarchs take advantage of to get more power. reply mlinhares 1 hour agorootparentThey still firmly believe they'll be the oligarchs, just like lots of people think if they were in medieval Europe they would be nobles or knights. reply teaearlgraycold 3 hours agoparentprevThe worst are the seed stage founders complaining about the proposed paper gains tax (while not knowing enough about it to answer if it would apply to startup equity) because they’re definitely going to have enough value for the $100MM threshold to apply to them. reply dgs_sgd 3 hours agorootparentWhen the income tax was first passed it applied to less than 1% of citizens, yet look where we are now. Just because a new law doesn’t apply to you personally doesn’t mean you can’t be concerned about the potential downstream effects. reply kjkjadksj 2 hours agorootparentSlippery slope arguments with taxes make no sense. A new tax could appear any day on any income level. reply xienze 3 hours agorootparentprev> because they’re definitely going to have enough value for the $100MM threshold to apply to them. Tax cutoffs have this habit of drifting downward over time (the whole, \"eventually you run out of other people's money\" thing). See the original revenue act of 1913 (modern US income tax). At first it was 1% starting at today's equivalent of $93K and didn't go any higher until you made the equivalent of about $500K. And today, well, it's a little different. New taxes are always pitched this way -- \"oh it'll never affect you, just those rich people, what's your problem?\" reply Eumenes 3 hours agoparentprev> cyber-libretarians like Ron Wyden This dude is a certified ghoul. In May 2017, Wyden co-sponsored the Israel Anti-Boycott Act, Senate Bill 720, which made it a federal crime, punishable by a maximum sentence of 20 years imprisonment,[88] for Americans to encourage or participate in boycotts against Israel and Israeli settlements in the occupied Palestinian territories if protesting actions by the Israeli government. The bill would make it legal for U.S. states to refuse to do business with contractors that engage in boycotts against Israel.[89] https://en.wikipedia.org/wiki/Ron_Wyden#Israel reply Der_Einzige 3 hours ago [flagged]rootparentCongrats, you purity tested him and found he, like everyone else, has bad takes! Good thing Bernie Sanders thinks such a bill was dumb! https://www.sanders.senate.gov/press-releases/sanders-statem... Anti-BDS is not part of being a cyber-libertarian. Find a better argument please. reply dang 1 hour agorootparentCould you please stop posting in the flamewar style? Your account has been doing it a ton, it's not what this site is for, and destroys what it is for. I don't want to ban you because you clearly know a lot of things and have posted quite a few good things over the years. But your comments include so much name-calling, swipes, snark, and nastiness that you're not leaving much choice. Here's one of many examples, on an unrelated topic: https://news.ycombinator.com/item?id=41630774. That's a great comment, ruined by name-calling and personal attack. On the subject of cartoons of all things! There's no need to treat other people this way, or use the comment section to vent bile like that. If you wouldn't mind reviewing https://news.ycombinator.com/newsguidelines.html and taking the intended spirit of the site more to heart, we'd be grateful. reply Eumenes 3 hours agorootparentprevI don't know what a cyber-libertarian is but Bernie Sanders is completely irrelevant (literally). The 3 bills he sponsored that became law were renaming highways and post-offices. The guy is a meme at this point. Look how he rolled over for the DNC two cycles in a row. Here are his accomplishments: https://www.congress.gov/member/bernard-sanders/S000033?q=%7... If you look at co-sponsored, its more of the same fluff. I'd say he's pretty ineffective. reply geoka9 2 hours agorootparentFWIW, your post prompted me to take a bit of time to research. I found this interesting: \"According to The New York Times, \"Big legislation largely eludes Mr. Sanders because his ideas are usually far to the left of the majority of the Senate ... Mr. Sanders has largely found ways to press his agenda through appending small provisions to the larger bills of others.\"[146] During his time in the Senate, he had lower legislative effectiveness than the average senator, as measured by the number of sponsored bills that passed and successful amendments made.[147] Nevertheless, he has sponsored over 500 amendments to bills,[148] many of which became law.\" https://en.wikipedia.org/wiki/Bernie_Sanders reply Eumenes 2 hours agorootparentThats the conclusion I came to. It seems like he'd be more effective as a consultant/advisor or non-elected official. Taking up a valuable senate seat to make amendments seems like a waste of influence. reply geoka9 42 minutes agorootparentMy take is that he's still a successful lawmaker, it's just that he is more successful implementing his agenda with amendments rather than standalone bills. reply bbqfog 3 hours agorootparentprevnext [6 more] [flagged] AlexandrB 3 hours agorootparentI'm someone who has read up a lot on fascism and still finds it difficult to define clearly. Can you please explain to me how this kind of bill is \"literal fascism\". reply bbqfog 3 hours agorootparentIt's removing freedom of speech and freedom to boycott. It's using government force to oppress citizens. reply robertlagrant 3 hours agorootparentWhy is that fascism and not communism? It's just general authoritarianism. reply klabb3 2 hours agorootparentI don’t have an answer to this, I’m just here to say that Italian fascist economic policy was explicitly corporatist, promoting big, sector-wide industry conglomerates, criminalizing strikes etc. It's no coincidence that large business moguls (today in the form of corporations) have strong preference towards fascist leaning politics. reply Manuel_D 3 hours agorootparentprevMaking boycotting a crime seems a lot like proscribing a political view. I doubt the law has been enforced this way - or enforced at all - but if you went to the grocery store, saw a product sold by an Israeli company and decided not to buy it on account of that fact you have now committed a crime punishable by up to 20 years in prison. The reality is that the courts would never uphold the validity of this law, and it's not enforced because the legislators know it's BS. reply renewiltord 3 hours agorootparentprevKamala Harris and Donald Trump are both cyber libertarians so I don’t see the problem. reply Manuel_D 3 hours agoparentprev> Folks that think that Trump and his friends like \"freedom of speech\" or any of the type of techno freedoms we take for granted are in for a deeply rude awakening. The reality is that democrats are so, SO much better on most cyber issues that you're actually a deplorable if you are voting for trump. I'm curious as to why you conclude that Democrats are better than Republicans in terms of IT liberty. Trump ended operation choke point, a program of essentially harassing banks into dropping certain customers [1]. De-banking is, in my opinion, one of the most important threats to \"cyber liberties\" since it's absolutely crucial for any business to survive. Individuals who are de-banked also find it incredibly difficult to survive: Plenty of services, including apartments and HOAs, do not accept non-electronic payments. The censorship of \"misinformation\" (much of which later turned out to be true) also grew considerably under the 2020 administration, which did in fact reach out to social media companies to try and influence their decision making. One of the more salient examples was covered by HN here: https://news.ycombinator.com/item?id=41365868 To be clear, I'm not concerned about private companies autonomously deciding what to host. I take issue with the government coercing companies into making moderation decisions. This has huge potential for abuse, such as forcing the takedown of politically disadvantageous material. Both Republicans and Democrats are in favor of getting rid of Section 230, but for essentially totally different reasons: Republicans seem to want to return to the precedence set by Cubby Inc. vs. CompuServe [2]. Democrats seem to want to narrowly repeal Section 230 to empower the government to dictate the contents of social media. Both are negative in my opinion, but the patter seems more insidious and prone to long term harm. And to be clear, these issues are far, far from enough to get me to vote for Trump. I'm interested in hearing the counter-claim, that the Democrat party is better in terms of cyber liberties. 1. https://en.wikipedia.org/wiki/Operation_Choke_Point 2. https://en.wikipedia.org/wiki/Cubby,_Inc._v._CompuServe_Inc. reply robertlagrant 3 hours agoparentprevBernie Sanders is a cyber-libertarian? What does that mean? reply bpodgursky 3 hours agoparentprev> The right-wing turn is absolutely disgusting and should be shattered as soon as possible with prejudice. Do you understand how you are the problem here? Silicon Valley didn't care about politics until politicians started getting involved and attempting to impose censorship regimes, aggressive unrealized gains taxation, and killing crypto assets. Tech startups were find being ignored. But as you illustrate, you can't ignore DC if it chooses not to ignore you. If people like you want to use the power of the state to \"shatter\" anyone you politically disagree with, well, of course the tech companies will hit back. reply xhkkffbf 3 hours agoparentprevWhy are you being a Democrat shill? Didn't you see that recent video of Democratic party boss Hillary Clinton saying the quiet part outloud? If the social media isn't censored, \"we lose control.\" And by \"we\", she didn't mean you and me. I just don't see this as \"SO much better.\" reply AlbertCory 3 hours agoprev> Porter, who had initially polled well, lost decisively in the primary, coming in third, with just fifteen per cent of the vote. She was running against Adam Schiff, who had much broader name recognition. It also didn't help that she's overweight and \"Porker\" became her nasty nickname. This is a shoddy piece of reporting. Tech lobbying didn't start with Chris Lehane, Uber, Lyft, or crypto. Anytime government is capable of helping or hurting an industry, they form a lobbying group. The more money involved, the bigger the lobbying effort. reply JohnMakin 3 hours agoparentIt didn't help that the Schiff campaign (backed by the CA democratic party) donated to the Garvey campaign to force her out of the primary. Seems weird to not mention this. https://www.latimes.com/california/story/2024-03-06/californ... reply zooq_ai 3 hours agoprevLobbying is a necessary evil to counter populism, brainwashed people (from TikTok, Reddit). reply game_the0ry 3 hours agoparentNo, good policy that benefits a wide group of constitutes instead of a few key donors is how you counter populism. Lobbying makes it worse bc the response (populism) will have credible grievances, which is why you will see populist candidates all over the west. reply whimsicalism 3 hours agorootparentso-called “deliverism” as a cure to populism has empirically not worked very well. actual material changes do very little to defuse populist cultural moments in modern wealthy societies reply SpicyLemonZest 1 hour agorootparentI've seen this analysis before, but I'm really not sure what it's supposed to mean beyond \"modern populist movements want changes I don't consider to be material\". There's no big mystery here - Denmark successfully defused their populist movement by delivering what populists say are their biggest priorities, stricter immigration policies and an increased focus on small towns (https://www.project-syndicate.org/commentary/denmark-europea...). reply John_Cena 14 minutes agorootparentThere is no big mystery; I believe it is proposed as that because the politicians just want to act in their own self-interest or that of lobbyists and not those of their constituents. Even bringing up immigration policies leads to vitriol online, and all I can do is think \"to whose advantage is this really\".. reply game_the0ry 3 hours agorootparentprevThis comment comes of as over-intellectual incoherent internet babble. I do not even know what you are saying. reply Der_Einzige 3 hours agorootparentprevSource? Making your people first world is extremely good at defeating \"populist cultural moments\" like communism, unless of course you consider may 68 in France of evidence of the contrary. I'll even go further and claim that Occupy Wall-Street died because the US economy boomed from 2012 on (return to embarrassed millionaire) reply whimsicalism 2 hours agorootparentThe US is already first world. OWS was much smaller than the media might make you think - the US economy really only started booming 2017,2018,2019. https://democracyjournal.org/arguments/the-death-of-deliveri... reply kjkjadksj 2 hours agoparentprevThe people handing money to lobbyists in washington are the same people funding these misinformation campaigns that brainwash the populace. reply amelius 4 hours agoprevCan we use ChatGPT to neutralize them? reply 0_____0 4 hours agoparentTools of the master etc. etc. reply random3 4 hours agoparentprevWho’s “we”? reply prpl 3 hours agorootparentWho’s “them”? reply meiraleal 3 hours agorootparentprevThem reply mdgrech23 1 hour agoprev [–] YC was cool like 15 years ago when I first joined this forum now they kinda sux reply karaterobot 1 hour agoparent [–] What's a cooler place you could go instead? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Silicon Valley has emerged as a significant political force, utilizing super PACs like Fairshake to sway elections and pressure politicians into supporting tech-friendly policies.",
      "The tech industry, especially the cryptocurrency sector, is investing millions in political campaigns to safeguard its interests, exemplified by the targeting of California politician Katie Porter for her anti-crypto stance.",
      "Led by figures such as Chris Lehane, the tech industry's political engagement has positioned it as one of the largest corporate donors in American politics, using financial power to influence legislation and protect its interests."
    ],
    "commentSummary": [
      "Silicon Valley has evolved into a significant lobbying entity, comparable to established industries such as oil and defense.",
      "Initially perceived as countercultural, tech companies are now involved in controversial practices, including supporting authoritarian regimes and fostering screen addiction.",
      "This transformation reflects a broader trend where growing industries prioritize profit maximization and political influence, raising complex issues about tech's societal impact, politics, and regulation."
    ],
    "points": 213,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1728307314
  },
  {
    "id": 41766035,
    "title": "Python 3.13.0 Is Released",
    "originLink": "https://docs.python.org/3.13/whatsnew/3.13.html",
    "originBody": "dev (3.14) 3.13.0 3.12 3.11 3.10 3.9 3.8 3.7 3.6 3.5 3.4 3.3 3.2 3.1 3.0 2.7 2.6 English Spanish French Italian Japanese Korean Brazilian Portuguese Turkish Simplified Chinese Traditional Chinese Theme Auto Light Dark Table of Contents What’s New In Python 3.13 Summary – Release Highlights New Features A better interactive interpreter Improved error messages Free-threaded CPython An experimental just-in-time (JIT) compiler Defined mutation semantics for locals() Support for mobile platforms Other Language Changes New Modules Improved Modules argparse array ast asyncio base64 compileall concurrent.futures configparser copy ctypes dbm dis doctest email fractions glob importlib io ipaddress itertools marshal math mimetypes mmap multiprocessing os os.path pathlib pdb queue random re shutil site sqlite3 ssl statistics subprocess sys tempfile time tkinter traceback types typing unicodedata venv warnings xml zipimport Optimizations Removed Modules And APIs PEP 594: Remove “dead batteries” from the standard library 2to3 builtins configparser importlib.metadata locale opcode pathlib re tkinter.tix turtle typing unittest urllib webbrowser New Deprecations Pending Removal in Python 3.14 Pending Removal in Python 3.15 Pending Removal in Python 3.16 Pending Removal in Future Versions CPython Bytecode Changes C API Changes New Features Changed C APIs Limited C API Changes Removed C APIs Deprecated C APIs Pending Removal in Python 3.14 Pending Removal in Python 3.15 Pending Removal in Future Versions Build Changes Porting to Python 3.13 Changes in the Python API Changes in the C API Regression Test Changes Previous topic What’s New in Python Next topic What’s New In Python 3.12 This Page Report a Bug Show Source Navigation index modulesnextpreviousPython » English Spanish French Italian Japanese Korean Brazilian Portuguese Turkish Simplified Chinese Traditional Chinese dev (3.14) 3.13.0 3.12 3.11 3.10 3.9 3.8 3.7 3.6 3.5 3.4 3.3 3.2 3.1 3.0 2.7 2.6 3.13.0 Documentation » What’s New in Python » What’s New In Python 3.13Theme Auto Light DarkWhat’s New In Python 3.13¶ Editors: Adam Turner and Thomas Wouters This article explains the new features in Python 3.13, compared to 3.12. Python 3.13 was released on October 7, 2024. For full details, see the changelog. See also PEP 719 – Python 3.13 Release Schedule Summary – Release Highlights¶ Python 3.13 is the latest stable release of the Python programming language, with a mix of changes to the language, the implementation and the standard library. The biggest changes include a new interactive interpreter, experimental support for running in a free-threaded mode (PEP 703), and a Just-In-Time compiler (PEP 744). Error messages continue to improve, with tracebacks now highlighted in color by default. The locals() builtin now has defined semantics for changing the returned mapping, and type parameters now support default values. The library changes contain removal of deprecated APIs and modules, as well as the usual improvements in user-friendliness and correctness. Several legacy standard library modules have now been removed following their deprecation in Python 3.11 (PEP 594). This article doesn’t attempt to provide a complete specification of all new features, but instead gives a convenient overview. For full details refer to the documentation, such as the Library Reference and Language Reference. To understand the complete implementation and design rationale for a change, refer to the PEP for a particular new feature; but note that PEPs usually are not kept up-to-date once a feature has been fully implemented. See Porting to Python 3.13 for guidance on upgrading from earlier versions of Python. Interpreter improvements: A greatly improved interactive interpreter and improved error messages. PEP 667: The locals() builtin now has defined semantics when mutating the returned mapping. Python debuggers and similar tools may now more reliably update local variables in optimized scopes even during concurrent code execution. PEP 703: CPython 3.13 has experimental support for running with the global interpreter lock disabled. See Free-threaded CPython for more details. PEP 744: A basic JIT compiler was added. It is currently disabled by default (though we may turn it on later). Performance improvements are modest – we expect to improve this over the next few releases. Color support in the new interactive interpreter, as well as in tracebacks and doctest output. This can be disabled through the PYTHON_COLORS and NO_COLOR environment variables. Python data model improvements: __static_attributes__ stores the names of attributes accessed through self.X in any function in a class body. __firstlineno__ records the first line number of a class definition. Significant improvements in the standard library: Add a new PythonFinalizationError exception, raised when an operation is blocked during finalization. The argparse module now supports deprecating command-line options, positional arguments, and subcommands. The new functions base64.z85encode() and base64.z85decode() support encoding and decoding Z85 data. The copy module now has a copy.replace() function, with support for many builtin types and any class defining the __replace__() method. The new dbm.sqlite3 module is now the default dbm backend. The os module has a suite of new functions for working with Linux’s timer notification file descriptors. The random module now has a command-line interface. Security improvements: ssl.create_default_context() sets ssl.VERIFY_X509_PARTIAL_CHAIN and ssl.VERIFY_X509_STRICT as default flags. C API improvements: The Py_mod_gil slot is now used to indicate that an extension module supports running with the GIL disabled. The PyTime C API has been added, providing access to system clocks. PyMutex is a new lightweight mutex that occupies a single byte. There is a new suite of functions for generating PEP 669 monitoring events in the C API. New typing features: PEP 696: Type parameters (typing.TypeVar, typing.ParamSpec, and typing.TypeVarTuple) now support defaults. PEP 702: The new warnings.deprecated() decorator adds support for marking deprecations in the type system and at runtime. PEP 705: typing.ReadOnly can be used to mark an item of a typing.TypedDict as read-only for type checkers. PEP 742: typing.TypeIs provides more intuitive type narrowing behavior, as an alternative to typing.TypeGuard. Platform support: PEP 730: Apple’s iOS is now an officially supported platform, at tier 3. PEP 738: Android is now an officially supported platform, at tier 3. wasm32-wasi is now supported as a tier 2 platform. wasm32-emscripten is no longer an officially supported platform. Important removals: PEP 594: The remaining 19 “dead batteries” (legacy stdlib modules) have been removed from the standard library: aifc, audioop, cgi, cgitb, chunk, crypt, imghdr, mailcap, msilib, nis, nntplib, ossaudiodev, pipes, sndhdr, spwd, sunau, telnetlib, uu and xdrlib. Remove the 2to3 tool and lib2to3 module (deprecated in Python 3.11). Remove the tkinter.tix module (deprecated in Python 3.6). Remove the locale.resetlocale() function. Remove the typing.io and typing.re namespaces. Remove chained classmethod descriptors. Release schedule changes: PEP 602 (“Annual Release Cycle for Python”) has been updated to extend the full support (‘bugfix’) period for new releases to two years. This updated policy means that: Python 3.9–3.12 have one and a half years of full support, followed by three and a half years of security fixes. Python 3.13 and later have two years of full support, followed by three years of security fixes. New Features¶ A better interactive interpreter¶ Python now uses a new interactive shell by default, based on code from the PyPy project. When the user starts the REPL from an interactive terminal, the following new features are now supported: Multiline editing with history preservation. Direct support for REPL-specific commands like help, exit, and quit, without the need to call them as functions. Prompts and tracebacks with color enabled by default. Interactive help browsing using F1 with a separate command history. History browsing using F2 that skips output as well as the >>> and … prompts. “Paste mode” with F3 that makes pasting larger blocks of code easier (press F3 again to return to the regular prompt). To disable the new interactive shell, set the PYTHON_BASIC_REPL environment variable. For more on interactive mode, see Interactive Mode. (Contributed by Pablo Galindo Salgado, Łukasz Langa, and Lysandros Nikolaou in gh-111201 based on code from the PyPy project. Windows support contributed by Dino Viehland and Anthony Shaw.) Improved error messages¶ The interpreter now uses color by default when displaying tracebacks in the terminal. This feature can be controlled via the new PYTHON_COLORS environment variable as well as the canonical NO_COLOR and FORCE_COLOR environment variables. (Contributed by Pablo Galindo Salgado in gh-112730.) A common mistake is to write a script with the same name as a standard library module. When this results in errors, we now display a more helpful error message: $ python random.py Traceback (most recent call last): File \"/home/me/random.py\", line 1, inimport random File \"/home/me/random.py\", line 3, inprint(random.randint(5)) ^^^^^^^^^^^^^^ AttributeError: module 'random' has no attribute 'randint' (consider renaming '/home/me/random.py' since it has the same name as the standard library module named 'random' and the import system gives it precedence) Similarly, if a script has the same name as a third-party module that it attempts to import and this results in errors, we also display a more helpful error message: $ python numpy.py Traceback (most recent call last): File \"/home/me/numpy.py\", line 1, inimport numpy as np File \"/home/me/numpy.py\", line 3, innp.array([1, 2, 3]) ^^^^^^^^ AttributeError: module 'numpy' has no attribute 'array' (consider renaming '/home/me/numpy.py' if it has the same name as a third-party module you intended to import) (Contributed by Shantanu Jain in gh-95754.) The error message now tries to suggest the correct keyword argument when an incorrect keyword argument is passed to a function. >>> >>> \"Better error messages!\".split(max_split=1) Traceback (most recent call last): File \"\", line 1, in\"Better error messages!\".split(max_split=1) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^ TypeError: split() got an unexpected keyword argument 'max_split'. Did you mean 'maxsplit'? (Contributed by Pablo Galindo Salgado and Shantanu Jain in gh-107944.) Free-threaded CPython¶ CPython now has experimental support for running in a free-threaded mode, with the global interpreter lock (GIL) disabled. This is an experimental feature and therefore is not enabled by default. The free-threaded mode requires a different executable, usually called python3.13t or python3.13t.exe. Pre-built binaries marked as free-threaded can be installed as part of the official Windows and macOS installers, or CPython can be built from source with the --disable-gil option. Free-threaded execution allows for full utilization of the available processing power by running threads in parallel on available CPU cores. While not all software will benefit from this automatically, programs designed with threading in mind will run faster on multi-core hardware. The free-threaded mode is experimental and work is ongoing to improve it: expect some bugs and a substantial single-threaded performance hit. Free-threaded builds of CPython support optionally running with the GIL enabled at runtime using the environment variable PYTHON_GIL or the command-line option -X gil=1. To check if the current interpreter supports free-threading, python -VV and sys.version contain “experimental free-threading build”. The new sys._is_gil_enabled() function can be used to check whether the GIL is actually disabled in the running process. C-API extension modules need to be built specifically for the free-threaded build. Extensions that support running with the GIL disabled should use the Py_mod_gil slot. Extensions using single-phase init should use PyUnstable_Module_SetGIL() to indicate whether they support running with the GIL disabled. Importing C extensions that don’t use these mechanisms will cause the GIL to be enabled, unless the GIL was explicitly disabled with the PYTHON_GIL environment variable or the -X gil=0 option. pip 24.1 or newer is required to install packages with C extensions in the free-threaded build. This work was made possible thanks to many individuals and organizations, including the large community of contributors to Python and third-party projects to test and enable free-threading support. Notable contributors include: Sam Gross, Ken Jin, Donghee Na, Itamar Oren, Matt Page, Brett Simmers, Dino Viehland, Carl Meyer, Nathan Goldbaum, Ralf Gommers, Lysandros Nikolaou, and many others. Many of these contributors are employed by Meta, which has provided significant engineering resources to support this project. See also PEP 703 “Making the Global Interpreter Lock Optional in CPython” contains rationale and information surrounding this work. Porting Extension Modules to Support Free-Threading: A community-maintained porting guide for extension authors. An experimental just-in-time (JIT) compiler¶ When CPython is configured and built using the --enable-experimental-jit option, a just-in-time (JIT) compiler is added which may speed up some Python programs. On Windows, use PCbuild/build.bat --experimental-jit to enable the JIT or --experimental-jit-interpreter to enable the Tier 2 interpreter. Build requirements and further supporting information are contained at Tools/jit/README.md. The --enable-experimental-jit option takes these (optional) values, defaulting to yes if --enable-experimental-jit is present without the optional value. no: Disable the entire Tier 2 and JIT pipeline. yes: Enable the JIT. To disable the JIT at runtime, pass the environment variable PYTHON_JIT=0. yes-off: Build the JIT but disable it by default. To enable the JIT at runtime, pass the environment variable PYTHON_JIT=1. interpreter: Enable the Tier 2 interpreter but disable the JIT. The interpreter can be disabled by running with PYTHON_JIT=0. The internal architecture is roughly as follows: We start with specialized Tier 1 bytecode. See What’s new in 3.11 for details. When the Tier 1 bytecode gets hot enough, it gets translated to a new purely internal intermediate representation (IR), called the Tier 2 IR, and sometimes referred to as micro-ops (“uops”). The Tier 2 IR uses the same stack-based virtual machine as Tier 1, but the instruction format is better suited to translation to machine code. We have several optimization passes for Tier 2 IR, which are applied before it is interpreted or translated to machine code. There is a Tier 2 interpreter, but it is mostly intended for debugging the earlier stages of the optimization pipeline. The Tier 2 interpreter can be enabled by configuring Python with --enable-experimental-jit=interpreter. When the JIT is enabled, the optimized Tier 2 IR is translated to machine code, which is then executed. The machine code translation process uses a technique called copy-and-patch. It has no runtime dependencies, but there is a new build-time dependency on LLVM. See also PEP 744 (JIT by Brandt Bucher, inspired by a paper by Haoran Xu and Fredrik Kjolstad. Tier 2 IR by Mark Shannon and Guido van Rossum. Tier 2 optimizer by Ken Jin.) Defined mutation semantics for locals()¶ Historically, the expected result of mutating the return value of locals() has been left to individual Python implementations to define. Starting from Python 3.13, PEP 667 standardises the historical behavior of CPython for most code execution scopes, but changes optimized scopes (functions, generators, coroutines, comprehensions, and generator expressions) to explicitly return independent snapshots of the currently assigned local variables, including locally referenced nonlocal variables captured in closures. This change to the semantics of locals() in optimized scopes also affects the default behavior of code execution functions that implicitly target locals() if no explicit namespace is provided (such as exec() and eval()). In previous versions, whether or not changes could be accessed by calling locals() after calling the code execution function was implementation-dependent. In CPython specifically, such code would typically appear to work as desired, but could sometimes fail in optimized scopes based on other code (including debuggers and code execution tracing tools) potentially resetting the shared snapshot in that scope. Now, the code will always run against an independent snapshot of the local variables in optimized scopes, and hence the changes will never be visible in subsequent calls to locals(). To access the changes made in these cases, an explicit namespace reference must now be passed to the relevant function. Alternatively, it may make sense to update affected code to use a higher level code execution API that returns the resulting code execution namespace (e.g. runpy.run_path() when executing Python files from disk). To ensure debuggers and similar tools can reliably update local variables in scopes affected by this change, FrameType.f_locals now returns a write-through proxy to the frame’s local and locally referenced nonlocal variables in these scopes, rather than returning an inconsistently updated shared dict instance with undefined runtime semantics. See PEP 667 for more details, including related C API changes and deprecations. Porting notes are also provided below for the affected Python APIs and C APIs. (PEP and implementation contributed by Mark Shannon and Tian Gao in gh-74929. Documentation updates provided by Guido van Rossum and Alyssa Coghlan.) Support for mobile platforms¶ PEP 730: iOS is now a PEP 11 supported platform, with the arm64-apple-ios and arm64-apple-ios-simulator targets at tier 3 (iPhone and iPad devices released after 2013 and the Xcode iOS simulator running on Apple silicon hardware, respectively). x86_64-apple-ios-simulator (the Xcode iOS simulator running on older x86_64 hardware) is not a tier 3 supported platform, but will have best-effort support. (PEP written and implementation contributed by Russell Keith-Magee in gh-114099.) PEP 738: Android is now a PEP 11 supported platform, with the aarch64-linux-android and x86_64-linux-android targets at tier 3. The 32-bit targets arm-linux-androideabi and i686-linux-android are not tier 3 supported platforms, but will have best-effort support. (PEP written and implementation contributed by Malcolm Smith in gh-116622.) See also PEP 730, PEP 738 Other Language Changes¶ The compiler now strips common leading whitespace from every line in a docstring. This reduces the size of the bytecode cache (such as .pyc files), with reductions in file size of around 5%, for example in sqlalchemy.orm.session from SQLAlchemy 2.0. This change affects tools that use docstrings, such as doctest. >>> >>> def spam(): ... \"\"\" ... This is a docstring with ... leading whitespace. ... ... It even has multiple paragraphs! ... \"\"\" ... >>> spam.__doc__ 'This is a docstring with leading whitespace.It even has multiple paragraphs!' (Contributed by Inada Naoki in gh-81283.) Annotation scopes within class scopes can now contain lambdas and comprehensions. Comprehensions that are located within class scopes are not inlined into their parent scope. class C[T]: type Alias = lambda: T (Contributed by Jelle Zijlstra in gh-109118 and gh-118160.) Future statements are no longer triggered by relative imports of the __future__ module, meaning that statements of the form from .__future__ import ... are now simply standard relative imports, with no special features activated. (Contributed by Jeremiah Gabriel Pascual in gh-118216.) global declarations are now permitted in except blocks when that global is used in the else block. Previously this raised an erroneous SyntaxError. (Contributed by Irit Katriel in gh-111123.) Add PYTHON_FROZEN_MODULES, a new environment variable that determines whether frozen modules are ignored by the import machinery, equivalent to the -X frozen_modules command-line option. (Contributed by Yilei Yang in gh-111374.) Add support for the perf profiler working without frame pointers through the new environment variable PYTHON_PERF_JIT_SUPPORT and command-line option -X perf_jit. (Contributed by Pablo Galindo in gh-118518.) The location of a .python_history file can be changed via the new PYTHON_HISTORY environment variable. (Contributed by Levi Sabah, Zackery Spytz and Hugo van Kemenade in gh-73965.) Classes have a new __static_attributes__ attribute. This is populated by the compiler with a tuple of the class’s attribute names which are assigned through self. from any function in its body. (Contributed by Irit Katriel in gh-115775.) The compiler now creates a __firstlineno__ attribute on classes with the line number of the first line of the class definition. (Contributed by Serhiy Storchaka in gh-118465.) The exec() and eval() builtins now accept the globals and locals arguments as keywords. (Contributed by Raphael Gaschignard in gh-105879) The compile() builtin now accepts a new flag, ast.PyCF_OPTIMIZED_AST, which is similar to ast.PyCF_ONLY_AST except that the returned AST is optimized according to the value of the optimize argument. (Contributed by Irit Katriel in gh-108113). Add a __name__ attribute on property objects. (Contributed by Eugene Toder in gh-101860.) Add PythonFinalizationError, a new exception derived from RuntimeError and used to signal when operations are blocked during finalization. The following callables now raise PythonFinalizationError, instead of RuntimeError: _thread.start_new_thread() os.fork() os.forkpty() subprocess.Popen (Contributed by Victor Stinner in gh-114570.) Allow the count argument of str.replace() to be a keyword. (Contributed by Hugo van Kemenade in gh-106487.) Many functions now emit a warning if a boolean value is passed as a file descriptor argument. This can help catch some errors earlier. (Contributed by Serhiy Storchaka in gh-82626.) Added name and mode attributes for compressed and archived file-like objects in the bz2, lzma, tarfile, and zipfile modules. (Contributed by Serhiy Storchaka in gh-115961.) New Modules¶ dbm.sqlite3: An SQLite backend for dbm. (Contributed by Raymond Hettinger and Erlend E. Aasland in gh-100414.) Improved Modules¶ argparse¶ Add the deprecated parameter to the add_argument() and add_parser() methods, to enable deprecating command-line options, positional arguments, and subcommands. (Contributed by Serhiy Storchaka in gh-83648.) array¶ Add the 'w' type code (Py_UCS4) for Unicode characters. It should be used instead of the deprecated 'u' type code. (Contributed by Inada Naoki in gh-80480.) Register array.array as a MutableSequence by implementing the clear() method. (Contributed by Mike Zimin in gh-114894.) ast¶ The constructors of node types in the ast module are now stricter in the arguments they accept, with more intuitive behavior when arguments are omitted. If an optional field on an AST node is not included as an argument when constructing an instance, the field will now be set to None. Similarly, if a list field is omitted, that field will now be set to an empty list, and if an expr_context field is omitted, it defaults to Load(). (Previously, in all cases, the attribute would be missing on the newly constructed AST node instance.) In all other cases, where a required argument is omitted, the node constructor will emit a DeprecationWarning. This will raise an exception in Python 3.15. Similarly, passing a keyword argument to the constructor that does not map to a field on the AST node is now deprecated, and will raise an exception in Python 3.15. These changes do not apply to user-defined subclasses of ast.AST unless the class opts in to the new behavior by defining the AST._field_types mapping. (Contributed by Jelle Zijlstra in gh-105858, gh-117486, and gh-118851.) ast.parse() now accepts an optional argument optimize which is passed on to compile(). This makes it possible to obtain an optimized AST. (Contributed by Irit Katriel in gh-108113.) asyncio¶ asyncio.as_completed() now returns an object that is both an asynchronous iterator and a plain iterator of awaitables. The awaitables yielded by asynchronous iteration include original task or future objects that were passed in, making it easier to associate results with the tasks being completed. (Contributed by Justin Arthur in gh-77714.) asyncio.loop.create_unix_server() will now automatically remove the Unix socket when the server is closed. (Contributed by Pierre Ossman in gh-111246.) DatagramTransport.sendto() will now send zero-length datagrams if called with an empty bytes object. The transport flow control also now accounts for the datagram header when calculating the buffer size. (Contributed by Jamie Phan in gh-115199.) Add Queue.shutdown and QueueShutDown to manage queue termination. (Contributed by Laurie Opperman and Yves Duprat in gh-104228.) Add the Server.close_clients() and Server.abort_clients() methods, which more forcefully close an asyncio server. (Contributed by Pierre Ossman in gh-113538.) Accept a tuple of separators in StreamReader.readuntil(), stopping when any one of them is encountered. (Contributed by Bruce Merry in gh-81322.) Improve the behavior of TaskGroup when an external cancellation collides with an internal cancellation. For example, when two task groups are nested and both experience an exception in a child task simultaneously, it was possible that the outer task group would hang, because its internal cancellation was swallowed by the inner task group. In the case where a task group is cancelled externally and also must raise an ExceptionGroup, it will now call the parent task’s cancel() method. This ensures that a CancelledError will be raised at the next await, so the cancellation is not lost. An added benefit of these changes is that task groups now preserve the cancellation count (cancelling()). In order to handle some corner cases, uncancel() may now reset the undocumented _must_cancel flag when the cancellation count reaches zero. (Inspired by an issue reported by Arthur Tacca in gh-116720.) When TaskGroup.create_task() is called on an inactive TaskGroup, the given coroutine will be closed (which prevents a RuntimeWarning about the given coroutine being never awaited). (Contributed by Arthur Tacca and Jason Zhang in gh-115957.) base64¶ Add z85encode() and z85decode() functions for encoding bytes as Z85 data and decoding Z85-encoded data to bytes. (Contributed by Matan Perelman in gh-75299.) compileall¶ The default number of worker threads and processes is now selected using os.process_cpu_count() instead of os.cpu_count(). (Contributed by Victor Stinner in gh-109649.) concurrent.futures¶ The default number of worker threads and processes is now selected using os.process_cpu_count() instead of os.cpu_count(). (Contributed by Victor Stinner in gh-109649.) configparser¶ ConfigParser now has support for unnamed sections, which allows for top-level key-value pairs. This can be enabled with the new allow_unnamed_section parameter. (Contributed by Pedro Sousa Lacerda in gh-66449.) copy¶ The new replace() function and the replace protocol make creating modified copies of objects much simpler. This is especially useful when working with immutable objects. The following types support the replace() function and implement the replace protocol: collections.namedtuple() dataclasses.dataclass datetime.datetime, datetime.date, datetime.time inspect.Signature, inspect.Parameter types.SimpleNamespace code objects Any user-defined class can also support copy.replace() by defining the __replace__() method. (Contributed by Serhiy Storchaka in gh-108751.) ctypes¶ As a consequence of necessary internal refactoring, initialization of internal metaclasses now happens in __init__ rather than in __new__. This affects projects that subclass these internal metaclasses to provide custom initialization. Generally: Custom logic that was done in __new__ after calling super().__new__ should be moved to __init__. To create a class, call the metaclass, not only the metaclass’s __new__ method. See gh-124520 for discussion and links to changes in some affected projects. dbm¶ Add dbm.sqlite3, a new module which implements an SQLite backend, and make it the default dbm backend. (Contributed by Raymond Hettinger and Erlend E. Aasland in gh-100414.) Allow removing all items from the database through the new gdbm.clear() and ndbm.clear() methods. (Contributed by Donghee Na in gh-107122.) dis¶ Change the output of dis module functions to show logical labels for jump targets and exception handlers, rather than offsets. The offsets can be added with the new -O command-line option or the show_offsets argument. (Contributed by Irit Katriel in gh-112137.) get_instructions() no longer represents cache entries as separate instructions. Instead, it returns them as part of the Instruction, in the new cache_info field. The show_caches argument to get_instructions() is deprecated and no longer has any effect. (Contributed by Irit Katriel in gh-112962.) doctest¶ doctest output is now colored by default. This can be controlled via the new PYTHON_COLORS environment variable as well as the canonical NO_COLOR and FORCE_COLOR environment variables. See also Controlling color. (Contributed by Hugo van Kemenade in gh-117225.) The DocTestRunner.run() method now counts the number of skipped tests. Add the DocTestRunner.skips and TestResults.skipped attributes. (Contributed by Victor Stinner in gh-108794.) email¶ Headers with embedded newlines are now quoted on output. The generator will now refuse to serialize (write) headers that are improperly folded or delimited, such that they would be parsed as multiple headers or joined with adjacent data. If you need to turn this safety feature off, set verify_generated_headers. (Contributed by Bas Bloemsaat and Petr Viktorin in gh-121650.) getaddresses() and parseaddr() now return ('', '') pairs in more situations where invalid email addresses are encountered instead of potentially inaccurate values. The two functions have a new optional strict parameter (default True). To get the old behavior (accepting malformed input), use strict=False. getattr(email.utils, 'supports_strict_parsing', False) can be used to check if the strict parameter is available. (Contributed by Thomas Dwyer and Victor Stinner for gh-102988 to improve the CVE-2023-27043 fix.) fractions¶ Fraction objects now support the standard format specification mini-language rules for fill, alignment, sign handling, minimum width, and grouping. (Contributed by Mark Dickinson in gh-111320.) glob¶ Add translate(), a function to convert a path specification with shell-style wildcards to a regular expression. (Contributed by Barney Gale in gh-72904.) importlib¶ The following functions in importlib.resources now allow accessing a directory (or tree) of resources, using multiple positional arguments (the encoding and errors arguments in the text-reading functions are now keyword-only): is_resource() open_binary() open_text() path() read_binary() read_text() These functions are no longer deprecated and are not scheduled for removal. (Contributed by Petr Viktorin in gh-106532.) contents() remains deprecated in favor of the fully-featured Traversable API. However, there is now no plan to remove it. (Contributed by Petr Viktorin in gh-106532.) io¶ The IOBase finalizer now logs any errors raised by the close() method with sys.unraisablehook. Previously, errors were ignored silently by default, and only logged in Python Development Mode or when using a Python debug build. (Contributed by Victor Stinner in gh-62948.) ipaddress¶ Add the IPv4Address.ipv6_mapped property, which returns the IPv4-mapped IPv6 address. (Contributed by Charles Machalow in gh-109466.) Fix is_global and is_private behavior in IPv4Address, IPv6Address, IPv4Network, and IPv6Network. (Contributed by Jakub Stasiak in gh-113171.) itertools¶ batched() has a new strict parameter, which raises a ValueError if the final batch is shorter than the specified batch size. (Contributed by Raymond Hettinger in gh-113202.) marshal¶ Add the allow_code parameter in module functions. Passing allow_code=False prevents serialization and de-serialization of code objects which are incompatible between Python versions. (Contributed by Serhiy Storchaka in gh-113626.) math¶ The new function fma() performs fused multiply-add operations. This computes x * y + z with only a single round, and so avoids any intermediate loss of precision. It wraps the fma() function provided by C99, and follows the specification of the IEEE 754 “fusedMultiplyAdd” operation for special cases. (Contributed by Mark Dickinson and Victor Stinner in gh-73468.) mimetypes¶ Add the guess_file_type() function to guess a MIME type from a filesystem path. Using paths with guess_type() is now soft deprecated. (Contributed by Serhiy Storchaka in gh-66543.) mmap¶ mmap is now protected from crashing on Windows when the mapped memory is inaccessible due to file system errors or access violations. (Contributed by Jannis Weigend in gh-118209.) mmap has a new seekable() method that can be used when a seekable file-like object is required. The seek() method now returns the new absolute position. (Contributed by Donghee Na and Sylvie Liberman in gh-111835.) The new UNIX-only trackfd parameter for mmap controls file descriptor duplication; if false, the file descriptor specified by fileno will not be duplicated. (Contributed by Zackery Spytz and Petr Viktorin in gh-78502.) multiprocessing¶ The default number of worker threads and processes is now selected using os.process_cpu_count() instead of os.cpu_count(). (Contributed by Victor Stinner in gh-109649.) os¶ Add process_cpu_count() function to get the number of logical CPU cores usable by the calling thread of the current process. (Contributed by Victor Stinner in gh-109649.) cpu_count() and process_cpu_count() can be overridden through the new environment variable PYTHON_CPU_COUNT or the new command-line option -X cpu_count. This option is useful for users who need to limit CPU resources of a container system without having to modify application code or the container itself. (Contributed by Donghee Na in gh-109595.) Add a low level interface to Linux’s timer file descriptors via timerfd_create(), timerfd_settime(), timerfd_settime_ns(), timerfd_gettime(), timerfd_gettime_ns(), TFD_NONBLOCK, TFD_CLOEXEC, TFD_TIMER_ABSTIME, and TFD_TIMER_CANCEL_ON_SET (Contributed by Masaru Tsuchiyama in gh-108277.) lchmod() and the follow_symlinks argument of chmod() are both now available on Windows. Note that the default value of follow_symlinks in lchmod() is False on Windows. (Contributed by Serhiy Storchaka in gh-59616.) fchmod() and support for file descriptors in chmod() are both now available on Windows. (Contributed by Serhiy Storchaka in gh-113191.) On Windows, mkdir() and makedirs() now support passing a mode value of 0o700 to apply access control to the new directory. This implicitly affects tempfile.mkdtemp() and is a mitigation for CVE-2024-4030. Other values for mode continue to be ignored. (Contributed by Steve Dower in gh-118486.) posix_spawn() now accepts None for the env argument, which makes the newly spawned process use the current process environment. (Contributed by Jakub Kulik in gh-113119.) posix_spawn() can now use the POSIX_SPAWN_CLOSEFROM attribute in the file_actions parameter on platforms that support posix_spawn_file_actions_addclosefrom_np(). (Contributed by Jakub Kulik in gh-113117.) os.path¶ Add isreserved() to check if a path is reserved on the current system. This function is only available on Windows. (Contributed by Barney Gale in gh-88569.) On Windows, isabs() no longer considers paths starting with exactly one slash (\\ or /) to be absolute. (Contributed by Barney Gale and Jon Foster in gh-44626.) realpath() now resolves MS-DOS style file names even if the file is not accessible. (Contributed by Moonsik Park in gh-82367.) pathlib¶ Add UnsupportedOperation, which is raised instead of NotImplementedError when a path operation isn’t supported. (Contributed by Barney Gale in gh-89812.) Add a new constructor for creating Path objects from ‘file’ URIs (file:///), Path.from_uri(). (Contributed by Barney Gale in gh-107465.) Add PurePath.full_match() for matching paths with shell-style wildcards, including the recursive wildcard “**”. (Contributed by Barney Gale in gh-73435.) Add the PurePath.parser class attribute to store the implementation of os.path used for low-level path parsing and joining. This will be either posixpath or ntpath. Add recurse_symlinks keyword-only argument to Path.glob() and rglob(). (Contributed by Barney Gale in gh-77609.) Path.glob() and rglob() now return files and directories when given a pattern that ends with “**”. Previously, only directories were returned. (Contributed by Barney Gale in gh-70303.) Add the follow_symlinks keyword-only argument to Path.is_file, Path.is_dir, Path.owner(), and Path.group(). (Contributed by Barney Gale in gh-105793 and Kamil Turek in gh-107962.) pdb¶ breakpoint() and set_trace() now enter the debugger immediately rather than on the next line of code to be executed. This change prevents the debugger from breaking outside of the context when breakpoint() is positioned at the end of the context. (Contributed by Tian Gao in gh-118579.) sys.path[0] is no longer replaced by the directory of the script being debugged when sys.flags.safe_path is set. (Contributed by Tian Gao and Christian Walther in gh-111762.) zipapp is now supported as a debugging target. (Contributed by Tian Gao in gh-118501.) Add ability to move between chained exceptions during post-mortem debugging in pm() using the new exceptions [exc_number] command for Pdb. (Contributed by Matthias Bussonnier in gh-106676.) Expressions and statements whose prefix is a pdb command are now correctly identified and executed. (Contributed by Tian Gao in gh-108464.) queue¶ Add Queue.shutdown and ShutDown to manage queue termination. (Contributed by Laurie Opperman and Yves Duprat in gh-104750.) random¶ Add a command-line interface. (Contributed by Hugo van Kemenade in gh-118131.) re¶ Rename re.error to PatternError for improved clarity. re.error is kept for backward compatibility. shutil¶ Support the dir_fd and follow_symlinks keyword arguments in chown(). (Contributed by Berker Peksag and Tahia K in gh-62308) site¶ .pth files are now decoded using UTF-8 first, and then with the locale encoding if UTF-8 decoding fails. (Contributed by Inada Naoki in gh-117802.) sqlite3¶ A ResourceWarning is now emitted if a Connection object is not closed explicitly. (Contributed by Erlend E. Aasland in gh-105539.) Add the filter keyword-only parameter to Connection.iterdump() for filtering database objects to dump. (Contributed by Mariusz Felisiak in gh-91602.) ssl¶ The create_default_context() API now includes VERIFY_X509_PARTIAL_CHAIN and VERIFY_X509_STRICT in its default flags. Note VERIFY_X509_STRICT may reject pre-RFC 5280 or malformed certificates that the underlying OpenSSL implementation might otherwise accept. Whilst disabling this is not recommended, you can do so using: import ssl ctx = ssl.create_default_context() ctx.verify_flags &= ~ssl.VERIFY_X509_STRICT (Contributed by William Woodruff in gh-112389.) statistics¶ Add kde() for kernel density estimation. This makes it possible to estimate a continuous probability density function from a fixed number of discrete samples. (Contributed by Raymond Hettinger in gh-115863.) Add kde_random() for sampling from an estimated probability density function created by kde(). (Contributed by Raymond Hettinger in gh-115863.) subprocess¶ The subprocess module now uses the posix_spawn() function in more situations. Notably, when close_fds is True (the default), posix_spawn() will be used when the C library provides posix_spawn_file_actions_addclosefrom_np(), which includes recent versions of Linux, FreeBSD, and Solaris. On Linux, this should perform similarly to the existing Linux vfork() based code. A private control knob subprocess._USE_POSIX_SPAWN can be set to False if you need to force subprocess to never use posix_spawn(). Please report your reason and platform details in the issue tracker if you set this so that we can improve our API selection logic for everyone. (Contributed by Jakub Kulik in gh-113117.) sys¶ Add the _is_interned() function to test if a string was interned. This function is not guaranteed to exist in all implementations of Python. (Contributed by Serhiy Storchaka in gh-78573.) tempfile¶ On Windows, the default mode 0o700 used by tempfile.mkdtemp() now limits access to the new directory due to changes to os.mkdir(). This is a mitigation for CVE-2024-4030. (Contributed by Steve Dower in gh-118486.) time¶ On Windows, monotonic() now uses the QueryPerformanceCounter() clock for a resolution of 1 microsecond, instead of the GetTickCount64() clock which has a resolution of 15.6 milliseconds. (Contributed by Victor Stinner in gh-88494.) On Windows, time() now uses the GetSystemTimePreciseAsFileTime() clock for a resolution of 1 microsecond, instead of the GetSystemTimeAsFileTime() clock which has a resolution of 15.6 milliseconds. (Contributed by Victor Stinner in gh-63207.) tkinter¶ Add tkinter widget methods: tk_busy_hold(), tk_busy_configure(), tk_busy_cget(), tk_busy_forget(), tk_busy_current(), and tk_busy_status(). (Contributed by Miguel, klappnase and Serhiy Storchaka in gh-72684.) The tkinter widget method wm_attributes() now accepts the attribute name without the minus prefix to get window attributes, for example w.wm_attributes('alpha') and allows specifying attributes and values to set as keyword arguments, for example w.wm_attributes(alpha=0.5). (Contributed by Serhiy Storchaka in gh-43457.) wm_attributes() can now return attributes as a dict, by using the new optional keyword-only parameter return_python_dict. (Contributed by Serhiy Storchaka in gh-43457.) Text.count() can now return a simple int when the new optional keyword-only parameter return_ints is used. Otherwise, the single count is returned as a 1-tuple or None. (Contributed by Serhiy Storchaka in gh-97928.) Support the “vsapi” element type in the element_create() method of tkinter.ttk.Style. (Contributed by Serhiy Storchaka in gh-68166.) Add the after_info() method for Tkinter widgets. (Contributed by Cheryl Sabella in gh-77020.) Add a new copy_replace() method to PhotoImage to copy a region from one image to another, possibly with pixel zooming, subsampling, or both. (Contributed by Serhiy Storchaka in gh-118225.) Add from_coords parameter to the PhotoImage methods copy(), zoom() and subsample(). Add zoom and subsample parameters to the PhotoImage method copy(). (Contributed by Serhiy Storchaka in gh-118225.) Add the PhotoImage methods read() to read an image from a file and data() to get the image data. Add background and grayscale parameters to the write() method. (Contributed by Serhiy Storchaka in gh-118271.) traceback¶ Add the exc_type_str attribute to TracebackException, which holds a string display of the exc_type. Deprecate the exc_type attribute, which holds the type object itself. Add parameter save_exc_type (default True) to indicate whether exc_type should be saved. (Contributed by Irit Katriel in gh-112332.) Add a new show_group keyword-only parameter to TracebackException.format_exception_only() to (recursively) format the nested exceptions of a BaseExceptionGroup instance. (Contributed by Irit Katriel in gh-105292.) types¶ SimpleNamespace can now take a single positional argument to initialise the namespace’s arguments. This argument must either be a mapping or an iterable of key-value pairs. (Contributed by Serhiy Storchaka in gh-108191.) typing¶ PEP 705: Add ReadOnly, a special typing construct to mark a TypedDict item as read-only for type checkers. PEP 742: Add TypeIs, a typing construct that can be used to instruct a type checker how to narrow a type. Add NoDefault, a sentinel object used to represent the defaults of some parameters in the typing module. (Contributed by Jelle Zijlstra in gh-116126.) Add get_protocol_members() to return the set of members defining a typing.Protocol. (Contributed by Jelle Zijlstra in gh-104873.) Add is_protocol() to check whether a class is a Protocol. (Contributed by Jelle Zijlstra in gh-104873.) ClassVar can now be nested in Final, and vice versa. (Contributed by Mehdi Drissi in gh-89547.) unicodedata¶ Update the Unicode database to version 15.1.0. (Contributed by James Gerity in gh-109559.) venv¶ Add support for creating source control management (SCM) ignore files in a virtual environment’s directory. By default, Git is supported. This is implemented as opt-in via the API, which can be extended to support other SCMs (EnvBuilder and create()), and opt-out via the CLI, using --without-scm-ignore-files. (Contributed by Brett Cannon in gh-108125.) warnings¶ PEP 702: The new warnings.deprecated() decorator provides a way to communicate deprecations to a static type checker and to warn on usage of deprecated classes and functions. A DeprecationWarning may also be emitted when a decorated function or class is used at runtime. (Contributed by Jelle Zijlstra in gh-104003.) xml¶ Allow controlling Expat >=2.6.0 reparse deferral (CVE-2023-52425) by adding five new methods: xml.etree.ElementTree.XMLParser.flush() xml.etree.ElementTree.XMLPullParser.flush() xml.parsers.expat.xmlparser.GetReparseDeferralEnabled() xml.parsers.expat.xmlparser.SetReparseDeferralEnabled() xml.sax.expatreader.ExpatParser.flush() (Contributed by Sebastian Pipping in gh-115623.) Add the close() method for the iterator returned by iterparse() for explicit cleanup. (Contributed by Serhiy Storchaka in gh-69893.) zipimport¶ Add support for ZIP64 format files. Everybody loves huge data, right? (Contributed by Tim Hatch in gh-94146.) Optimizations¶ Several standard library modules have had their import times significantly improved. For example, the import time of the typing module has been reduced by around a third by removing dependencies on re and contextlib. Other modules to enjoy import-time speedups include email.utils, enum, functools, importlib.metadata, and threading. (Contributed by Alex Waygood, Shantanu Jain, Adam Turner, Daniel Hollas, and others in gh-109653.) textwrap.indent() is now around 30% faster than before for large input. (Contributed by Inada Naoki in gh-107369.) The subprocess module now uses the posix_spawn() function in more situations, including when close_fds is True (the default) on many modern platforms. This should provide a notable performance increase when launching processes on FreeBSD and Solaris. See the subprocess section above for details. (Contributed by Jakub Kulik in gh-113117.) Removed Modules And APIs¶ PEP 594: Remove “dead batteries” from the standard library¶ PEP 594 proposed removing 19 modules from the standard library, colloquially referred to as ‘dead batteries’ due to their historic, obsolete, or insecure status. All of the following modules were deprecated in Python 3.11, and are now removed: aifc audioop chunk cgi and cgitb cgi.FieldStorage can typically be replaced with urllib.parse.parse_qsl() for GET and HEAD requests, and the email.message module or the multipart library for POST and PUT requests. cgi.parse() can be replaced by calling urllib.parse.parse_qs() directly on the desired query string, unless the input is multipart/form-data, which should be replaced as described below for cgi.parse_multipart(). cgi.parse_header() can be replaced with the functionality in the email package, which implements the same MIME RFCs. For example, with email.message.EmailMessage: from email.message import EmailMessage msg = EmailMessage() msg['content-type'] = 'application/json; charset=\"utf8\"' main, params = msg.get_content_type(), msg['content-type'].params cgi.parse_multipart() can be replaced with the functionality in the email package, which implements the same MIME RFCs, or with the multipart library. For example, the email.message.EmailMessage and email.message.Message classes. crypt and the private _crypt extension. The hashlib module may be an appropriate replacement when simply hashing a value is required. Otherwise, various third-party libraries on PyPI are available: bcrypt: Modern password hashing for your software and your servers. passlib: Comprehensive password hashing framework supporting over 30 schemes. argon2-cffi: The secure Argon2 password hashing algorithm. legacycrypt: ctypes wrapper to the POSIX crypt library call and associated functionality. crypt_r: Fork of the crypt module, wrapper to the crypt_r(3) library call and associated functionality. imghdr: The filetype, puremagic, or python-magic libraries should be used as replacements. For example, the puremagic.what() function can be used to replace the imghdr.what() function for all file formats that were supported by imghdr. mailcap: Use the mimetypes module instead. msilib nis nntplib: Use the pynntp library from PyPI instead. ossaudiodev: For audio playback, use the pygame library from PyPI instead. pipes: Use the subprocess module instead. sndhdr: The filetype, puremagic, or python-magic libraries should be used as replacements. spwd: Use the python-pam library from PyPI instead. sunau telnetlib, Use the telnetlib3 or Exscript libraries from PyPI instead. uu: Use the base64 module instead, as a modern alternative. xdrlib (Contributed by Victor Stinner and Zachary Ware in gh-104773 and gh-104780.) 2to3¶ Remove the 2to3 program and the lib2to3 module, previously deprecated in Python 3.11. (Contributed by Victor Stinner in gh-104780.) builtins¶ Remove support for chained classmethod descriptors (introduced in gh-63272). These can no longer be used to wrap other descriptors, such as property. The core design of this feature was flawed and led to several problems. To “pass-through” a classmethod, consider using the __wrapped__ attribute that was added in Python 3.10. (Contributed by Raymond Hettinger in gh-89519.) Raise a RuntimeError when calling frame.clear() on a suspended frame (as has always been the case for an executing frame). (Contributed by Irit Katriel in gh-79932.) configparser¶ Remove the undocumented LegacyInterpolation class, deprecated in the docstring since Python 3.2, and at runtime since Python 3.11. (Contributed by Hugo van Kemenade in gh-104886.) importlib.metadata¶ Remove deprecated subscript (__getitem__()) access for EntryPoint objects. (Contributed by Jason R. Coombs in gh-113175.) locale¶ Remove the locale.resetlocale() function, deprecated in Python 3.11. Use locale.setlocale(locale.LC_ALL, \"\") instead. (Contributed by Victor Stinner in gh-104783.) opcode¶ Move opcode.ENABLE_SPECIALIZATION to _opcode.ENABLE_SPECIALIZATION. This field was added in 3.12, it was never documented, and is not intended for external use. (Contributed by Irit Katriel in gh-105481.) Remove opcode.is_pseudo(), opcode.MIN_PSEUDO_OPCODE, and opcode.MAX_PSEUDO_OPCODE, which were added in Python 3.12, but were neither documented nor exposed through dis, and were not intended to be used externally. (Contributed by Irit Katriel in gh-105481.) pathlib¶ Remove the ability to use Path objects as context managers. This functionality was deprecated and has had no effect since Python 3.9. (Contributed by Barney Gale in gh-83863.) re¶ Remove the undocumented, deprecated, and broken re.template() function and re.TEMPLATE / re.T flag. (Contributed by Serhiy Storchaka and Nikita Sobolev in gh-105687.) tkinter.tix¶ Remove the tkinter.tix module, deprecated in Python 3.6. The third-party Tix library which the module wrapped is unmaintained. (Contributed by Zachary Ware in gh-75552.) turtle¶ Remove the RawTurtle.settiltangle() method, deprecated in the documentation since Python 3.1 and at runtime since Python 3.11. (Contributed by Hugo van Kemenade in gh-104876.) typing¶ Remove the typing.io and typing.re namespaces, deprecated since Python 3.8. The items in those namespaces can be imported directly from the typing module. (Contributed by Sebastian Rittau in gh-92871.) Remove the keyword-argument method of creating TypedDict types, deprecated in Python 3.11. (Contributed by Tomas Roun in gh-104786.) unittest¶ Remove the following unittest functions, deprecated in Python 3.11: unittest.findTestCases() unittest.makeSuite() unittest.getTestCaseNames() Use TestLoader methods instead: loadTestsFromModule() loadTestsFromTestCase() getTestCaseNames() (Contributed by Hugo van Kemenade in gh-104835.) Remove the untested and undocumented TestProgram.usageExit() method, deprecated in Python 3.11. (Contributed by Hugo van Kemenade in gh-104992.) urllib¶ Remove the cafile, capath, and cadefault parameters of the urllib.request.urlopen() function, deprecated in Python 3.6. Use the context parameter instead with an SSLContext instance. The ssl.SSLContext.load_cert_chain() function can be used to load specific certificates, or let ssl.create_default_context() select the operating system’s trusted certificate authority (CA) certificates. (Contributed by Victor Stinner in gh-105382.) webbrowser¶ Remove the untested and undocumented MacOSX class, deprecated in Python 3.11. Use the MacOSXOSAScript class (introduced in Python 3.2) instead. (Contributed by Hugo van Kemenade in gh-104804.) Remove the deprecated MacOSXOSAScript._name attribute. Use the MacOSXOSAScript.name attribute instead. (Contributed by Nikita Sobolev in gh-105546.) New Deprecations¶ User-defined functions: Deprecate assignment to a function’s __code__ attribute, where the new code object’s type does not match the function’s type. The different types are: plain function, generator, async generator, and coroutine. (Contributed by Irit Katriel in gh-81137.) array: Deprecate the 'u' format code (wchar_t) at runtime. This format code has been deprecated in documentation since Python 3.3, and will be removed in Python 3.16. Use the 'w' format code (Py_UCS4) for Unicode characters instead. (Contributed by Hugo van Kemenade in gh-80480.) ctypes: Deprecate the undocumented SetPointerType() function, to be removed in Python 3.15. (Contributed by Victor Stinner in gh-105733.) Soft-deprecate the ARRAY() function in favour of type * length multiplication. (Contributed by Victor Stinner in gh-105733.) decimal: Deprecate the non-standard and undocumented Decimal format specifier 'N', which is only supported in the decimal module’s C implementation. (Contributed by Serhiy Storchaka in gh-89902.) dis: Deprecate the HAVE_ARGUMENT separator. Check membership in hasarg instead. (Contributed by Irit Katriel in gh-109319.) getopt and optparse: Both modules are now soft deprecated, with argparse preferred for new projects. This is a new soft-deprecation for the getopt module, whereas the optparse module was already de facto soft deprecated. (Contributed by Victor Stinner in gh-106535.) gettext: Deprecate non-integer numbers as arguments to functions and methods that consider plural forms in the gettext module, even if no translation was found. (Contributed by Serhiy Storchaka in gh-88434.) glob: Deprecate the undocumented glob0() and glob1() functions. Use glob() and pass a path-like object specifying the root directory to the root_dir parameter instead. (Contributed by Barney Gale in gh-117337.) http.server: Deprecate CGIHTTPRequestHandler, to be removed in Python 3.15. Process-based CGI HTTP servers have been out of favor for a very long time. This code was outdated, unmaintained, and rarely used. It has a high potential for both security and functionality bugs. (Contributed by Gregory P. Smith in gh-109096.) Deprecate the --cgi flag to the python -m http.server command-line interface, to be removed in Python 3.15. (Contributed by Gregory P. Smith in gh-109096.) mimetypes: Soft-deprecate file path arguments to guess_type(), use guess_file_type() instead. (Contributed by Serhiy Storchaka in gh-66543.) re: Deprecate passing the optional maxsplit, count, or flags arguments as positional arguments to the module-level split(), sub(), and subn() functions. These parameters will become keyword-only in a future version of Python. (Contributed by Serhiy Storchaka in gh-56166.) pathlib: Deprecate PurePath.is_reserved(), to be removed in Python 3.15. Use os.path.isreserved() to detect reserved paths on Windows. (Contributed by Barney Gale in gh-88569.) platform: Deprecate java_ver(), to be removed in Python 3.15. This function is only useful for Jython support, has a confusing API, and is largely untested. (Contributed by Nikita Sobolev in gh-116349.) pydoc: Deprecate the undocumented ispackage() function. (Contributed by Zackery Spytz in gh-64020.) sqlite3: Deprecate passing more than one positional argument to the connect() function and the Connection constructor. The remaining parameters will become keyword-only in Python 3.15. (Contributed by Erlend E. Aasland in gh-107948.) Deprecate passing name, number of arguments, and the callable as keyword arguments for Connection.create_function() and Connection.create_aggregate() These parameters will become positional-only in Python 3.15. (Contributed by Erlend E. Aasland in gh-108278.) Deprecate passing the callback callable by keyword for the set_authorizer(), set_progress_handler(), and set_trace_callback() Connection methods. The callback callables will become positional-only in Python 3.15. (Contributed by Erlend E. Aasland in gh-108278.) sys: Deprecate the _enablelegacywindowsfsencoding() function, to be removed in Python 3.16. Use the PYTHONLEGACYWINDOWSFSENCODING environment variable instead. (Contributed by Inada Naoki in gh-73427.) tarfile: Deprecate the undocumented and unused TarFile.tarfile attribute, to be removed in Python 3.16. (Contributed in gh-115256.) traceback: Deprecate the TracebackException.exc_type attribute. Use TracebackException.exc_type_str instead. (Contributed by Irit Katriel in gh-112332.) typing: Deprecate the undocumented keyword argument syntax for creating NamedTuple classes (e.g. Point = NamedTuple(\"Point\", x=int, y=int)), to be removed in Python 3.15. Use the class-based syntax or the functional syntax instead. (Contributed by Alex Waygood in gh-105566.) Deprecate omitting the fields parameter when creating a NamedTuple or typing.TypedDict class, and deprecate passing None to the fields parameter of both types. Python 3.15 will require a valid sequence for the fields parameter. To create a NamedTuple class with zero fields, use class NT(NamedTuple): pass or NT = NamedTuple(\"NT\", ()). To create a TypedDict class with zero fields, use class TD(TypedDict): pass or TD = TypedDict(\"TD\", {}). (Contributed by Alex Waygood in gh-105566 and gh-105570.) Deprecate the typing.no_type_check_decorator() decorator function, to be removed in in Python 3.15. After eight years in the typing module, it has yet to be supported by any major type checker. (Contributed by Alex Waygood in gh-106309.) Deprecate typing.AnyStr. In Python 3.16, it will be removed from typing.__all__, and a DeprecationWarning will be emitted at runtime when it is imported or accessed. It will be removed entirely in Python 3.18. Use the new type parameter syntax instead. (Contributed by Michael The in gh-107116.) wave: Deprecate the getmark(), setmark(), and getmarkers() methods of the Wave_read and Wave_write classes, to be removed in Python 3.15. (Contributed by Victor Stinner in gh-105096.) Pending Removal in Python 3.14¶ argparse: The type, choices, and metavar parameters of argparse.BooleanOptionalAction are deprecated and will be removed in 3.14. (Contributed by Nikita Sobolev in gh-92248.) ast: The following features have been deprecated in documentation since Python 3.8, now cause a DeprecationWarning to be emitted at runtime when they are accessed or used, and will be removed in Python 3.14: ast.Num ast.Str ast.Bytes ast.NameConstant ast.Ellipsis Use ast.Constant instead. (Contributed by Serhiy Storchaka in gh-90953.) asyncio: The child watcher classes MultiLoopChildWatcher, FastChildWatcher, AbstractChildWatcher and SafeChildWatcher are deprecated and will be removed in Python 3.14. (Contributed by Kumar Aditya in gh-94597.) asyncio.set_child_watcher(), asyncio.get_child_watcher(), asyncio.AbstractEventLoopPolicy.set_child_watcher() and asyncio.AbstractEventLoopPolicy.get_child_watcher() are deprecated and will be removed in Python 3.14. (Contributed by Kumar Aditya in gh-94597.) The get_event_loop() method of the default event loop policy now emits a DeprecationWarning if there is no current event loop set and it decides to create one. (Contributed by Serhiy Storchaka and Guido van Rossum in gh-100160.) collections.abc: Deprecated ByteString. Prefer Sequence or Buffer. For use in typing, prefer a union, like bytesbytearray, or collections.abc.Buffer. (Contributed by Shantanu Jain in gh-91896.) email: Deprecated the isdst parameter in email.utils.localtime(). (Contributed by Alan Williams in gh-72346.) importlib.abc deprecated classes: importlib.abc.ResourceReader importlib.abc.Traversable importlib.abc.TraversableResources Use importlib.resources.abc classes instead: importlib.resources.abc.Traversable importlib.resources.abc.TraversableResources (Contributed by Jason R. Coombs and Hugo van Kemenade in gh-93963.) itertools had undocumented, inefficient, historically buggy, and inconsistent support for copy, deepcopy, and pickle operations. This will be removed in 3.14 for a significant reduction in code volume and maintenance burden. (Contributed by Raymond Hettinger in gh-101588.) multiprocessing: The default start method will change to a safer one on Linux, BSDs, and other non-macOS POSIX platforms where 'fork' is currently the default (gh-84559). Adding a runtime warning about this was deemed too disruptive as the majority of code is not expected to care. Use the get_context() or set_start_method() APIs to explicitly specify when your code requires 'fork'. See Contexts and start methods. pathlib: is_relative_to() and relative_to(): passing additional arguments is deprecated. pkgutil: find_loader() and get_loader() now raise DeprecationWarning; use importlib.util.find_spec() instead. (Contributed by Nikita Sobolev in gh-97850.) pty: master_open(): use pty.openpty(). slave_open(): use pty.openpty(). sqlite3: version and version_info. execute() and executemany() if named placeholders are used and parameters is a sequence instead of a dict. date and datetime adapter, date and timestamp converter: see the sqlite3 documentation for suggested replacement recipes. types.CodeType: Accessing co_lnotab was deprecated in PEP 626 since 3.10 and was planned to be removed in 3.12, but it only got a proper DeprecationWarning in 3.12. May be removed in 3.14. (Contributed by Nikita Sobolev in gh-101866.) typing: ByteString, deprecated since Python 3.9, now causes a DeprecationWarning to be emitted when it is used. urllib: urllib.parse.Quoter is deprecated: it was not intended to be a public API. (Contributed by Gregory P. Smith in gh-88168.) Pending Removal in Python 3.15¶ ctypes: The undocumented ctypes.SetPointerType() function has been deprecated since Python 3.13. http.server: The obsolete and rarely used CGIHTTPRequestHandler has been deprecated since Python 3.13. No direct replacement exists. Anything is better than CGI to interface a web server with a request handler. The --cgi flag to the python -m http.server command-line interface has been deprecated since Python 3.13. importlib: __package__ and __cached__ will cease to be set or taken into consideration by the import system (gh-97879). locale: The getdefaultlocale() function has been deprecated since Python 3.11. Its removal was originally planned for Python 3.13 (gh-90817), but has been postponed to Python 3.15. Use getlocale(), setlocale(), and getencoding() instead. (Contributed by Hugo van Kemenade in gh-111187.) pathlib: PurePath.is_reserved() has been deprecated since Python 3.13. Use os.path.isreserved() to detect reserved paths on Windows. platform: java_ver() has been deprecated since Python 3.13. This function is only useful for Jython support, has a confusing API, and is largely untested. threading: RLock() will take no arguments in Python 3.15. Passing any arguments has been deprecated since Python 3.14, as the Python version does not permit any arguments, but the C version allows any number of positional or keyword arguments, ignoring every argument. typing: The undocumented keyword argument syntax for creating NamedTuple classes (e.g. Point = NamedTuple(\"Point\", x=int, y=int)) has been deprecated since Python 3.13. Use the class-based syntax or the functional syntax instead. The typing.no_type_check_decorator() decorator function has been deprecated since Python 3.13. After eight years in the typing module, it has yet to be supported by any major type checker. wave: The getmark(), setmark(), and getmarkers() methods of the Wave_read and Wave_write classes have been deprecated since Python 3.13. Pending Removal in Python 3.16¶ builtins: Bitwise inversion on boolean types, ~True or ~False has been deprecated since Python 3.12, as it produces surprising and unintuitive results (-2 and -1). Use not x instead for the logical negation of a Boolean. In the rare case that you need the bitwise inversion of the underlying integer, convert to int explicitly (~int(x)). array: The 'u' format code (wchar_t) has been deprecated in documentation since Python 3.3 and at runtime since Python 3.13. Use the 'w' format code (Py_UCS4) for Unicode characters instead. shutil: The ExecError exception has been deprecated since Python 3.14. It has not been used by any function in shutil since Python 3.4, and is now an alias of RuntimeError. symtable: The Class.get_methods method has been deprecated since Python 3.14. sys: The _enablelegacywindowsfsencoding() function has been deprecated since Python 3.13. Use the PYTHONLEGACYWINDOWSFSENCODING environment variable instead. tarfile: The undocumented and unused TarFile.tarfile attribute has been deprecated since Python 3.13. Pending Removal in Future Versions¶ The following APIs will be removed in the future, although there is currently no date scheduled for their removal. argparse: Nesting argument groups and nesting mutually exclusive groups are deprecated. array’s 'u' format code (gh-57281) builtins: bool(NotImplemented). Generators: throw(type, exc, tb) and athrow(type, exc, tb) signature is deprecated: use throw(exc) and athrow(exc) instead, the single argument signature. Currently Python accepts numeric literals immediately followed by keywords, for example 0in x, 1or x, 0if 1else 2. It allows confusing and ambiguous expressions like [0x1for x in y] (which can be interpreted as [0x1 for x in y] or [0x1f or x in y]). A syntax warning is raised if the numeric literal is immediately followed by one of keywords and, else, for, if, in, is and or. In a future release it will be changed to a syntax error. (gh-87999) Support for __index__() and __int__() method returning non-int type: these methods will be required to return an instance of a strict subclass of int. Support for __float__() method returning a strict subclass of float: these methods will be required to return an instance of float. Support for __complex__() method returning a strict subclass of complex: these methods will be required to return an instance of complex. Delegation of int() to __trunc__() method. Passing a complex number as the real or imag argument in the complex() constructor is now deprecated; it should only be passed as a single positional argument. (Contributed by Serhiy Storchaka in gh-109218.) calendar: calendar.January and calendar.February constants are deprecated and replaced by calendar.JANUARY and calendar.FEBRUARY. (Contributed by Prince Roshan in gh-103636.) codeobject.co_lnotab: use the codeobject.co_lines() method instead. datetime: utcnow(): use datetime.datetime.now(tz=datetime.UTC). utcfromtimestamp(): use datetime.datetime.fromtimestamp(timestamp, tz=datetime.UTC). gettext: Plural value must be an integer. importlib: load_module() method: use exec_module() instead. cache_from_source() debug_override parameter is deprecated: use the optimization parameter instead. importlib.metadata: EntryPoints tuple interface. Implicit None on return values. logging: the warn() method has been deprecated since Python 3.3, use warning() instead. mailbox: Use of StringIO input and text mode is deprecated, use BytesIO and binary mode instead. os: Calling os.register_at_fork() in multi-threaded process. pydoc.ErrorDuringImport: A tuple value for exc_info parameter is deprecated, use an exception instance. re: More strict rules are now applied for numerical group references and group names in regular expressions. Only sequence of ASCII digits is now accepted as a numerical reference. The group name in bytes patterns and replacement strings can now only contain ASCII letters and digits and underscore. (Contributed by Serhiy Storchaka in gh-91760.) sre_compile, sre_constants and sre_parse modules. shutil: rmtree()’s onerror parameter is deprecated in Python 3.12; use the onexc parameter instead. ssl options and protocols: ssl.SSLContext without protocol argument is deprecated. ssl.SSLContext: set_npn_protocols() and selected_npn_protocol() are deprecated: use ALPN instead. ssl.OP_NO_SSL* options ssl.OP_NO_TLS* options ssl.PROTOCOL_SSLv3 ssl.PROTOCOL_TLS ssl.PROTOCOL_TLSv1 ssl.PROTOCOL_TLSv1_1 ssl.PROTOCOL_TLSv1_2 ssl.TLSVersion.SSLv3 ssl.TLSVersion.TLSv1 ssl.TLSVersion.TLSv1_1 sysconfig.is_python_build() check_home parameter is deprecated and ignored. threading methods: threading.Condition.notifyAll(): use notify_all(). threading.Event.isSet(): use is_set(). threading.Thread.isDaemon(), threading.Thread.setDaemon(): use threading.Thread.daemon attribute. threading.Thread.getName(), threading.Thread.setName(): use threading.Thread.name attribute. threading.currentThread(): use threading.current_thread(). threading.activeCount(): use threading.active_count(). typing.Text (gh-92332). unittest.IsolatedAsyncioTestCase: it is deprecated to return a value that is not None from a test case. urllib.parse deprecated functions: urlparse() instead splitattr() splithost() splitnport() splitpasswd() splitport() splitquery() splittag() splittype() splituser() splitvalue() to_bytes() urllib.request: URLopener and FancyURLopener style of invoking requests is deprecated. Use newer urlopen() functions and methods. wsgiref: SimpleHandler.stdout.write() should not do partial writes. xml.etree.ElementTree: Testing the truth value of an Element is deprecated. In a future release it will always return True. Prefer explicit len(elem) or elem is not None tests instead. zipimport.zipimporter.load_module() is deprecated: use exec_module() instead. CPython Bytecode Changes¶ The oparg of YIELD_VALUE is now 1 if the yield is part of a yield-from or await, and 0 otherwise. The oparg of RESUME was changed to add a bit indicating if the except-depth is 1, which is needed to optimize closing of generators. (Contributed by Irit Katriel in gh-111354.) C API Changes¶ New Features¶ Add the PyMonitoring C API for generating PEP 669 monitoring events: PyMonitoringState PyMonitoring_FirePyStartEvent() PyMonitoring_FirePyResumeEvent() PyMonitoring_FirePyReturnEvent() PyMonitoring_FirePyYieldEvent() PyMonitoring_FireCallEvent() PyMonitoring_FireLineEvent() PyMonitoring_FireJumpEvent() PyMonitoring_FireBranchEvent() PyMonitoring_FireCReturnEvent() PyMonitoring_FirePyThrowEvent() PyMonitoring_FireRaiseEvent() PyMonitoring_FireCRaiseEvent() PyMonitoring_FireReraiseEvent() PyMonitoring_FireExceptionHandledEvent() PyMonitoring_FirePyUnwindEvent() PyMonitoring_FireStopIterationEvent() PyMonitoring_EnterScope() PyMonitoring_ExitScope() (Contributed by Irit Katriel in gh-111997). Add PyMutex, a lightweight mutex that occupies a single byte, and the new PyMutex_Lock() and PyMutex_Unlock() functions. PyMutex_Lock() will release the GIL (if currently held) if the operation needs to block. (Contributed by Sam Gross in gh-108724.) Add the PyTime C API to provide access to system clocks: PyTime_t. PyTime_MIN and PyTime_MAX. PyTime_AsSecondsDouble(). PyTime_Monotonic(). PyTime_MonotonicRaw(). PyTime_PerfCounter(). PyTime_PerfCounterRaw(). PyTime_Time(). PyTime_TimeRaw(). (Contributed by Victor Stinner and Petr Viktorin in gh-110850.) Add the PyDict_ContainsString() function with the same behavior as PyDict_Contains(), but key is specified as a const char* UTF-8 encoded bytes string, rather than a PyObject*. (Contributed by Victor Stinner in gh-108314.) Add the PyDict_GetItemRef() and PyDict_GetItemStringRef() functions, which behave similarly to PyDict_GetItemWithError(), but return a strong reference instead of a borrowed reference. Moreover, these functions return -1 on error, removing the need to check PyErr_Occurred(). (Contributed by Victor Stinner in gh-106004.) Add the PyDict_SetDefaultRef() function, which behaves similarly to PyDict_SetDefault(), but returns a strong reference instead of a borrowed reference. This function returns -1 on error, 0 on insertion, and 1 if the key was already present in the dictionary. (Contributed by Sam Gross in gh-112066.) Add the PyDict_Pop() and PyDict_PopString() functions to remove a key from a dictionary and optionally return the removed value. This is similar to dict.pop(), though there is no default value, and KeyError is not raised for missing keys. (Contributed by Stefan Behnel and Victor Stinner in gh-111262.) Add the PyMapping_GetOptionalItem() and PyMapping_GetOptionalItemString() functions as alternatives to PyObject_GetItem() and PyMapping_GetItemString() respectively. The new functions do not raise KeyError if the requested key is missing from the mapping. These variants are more convenient and faster if a missing key should not be treated as a failure. (Contributed by Serhiy Storchaka in gh-106307.) Add the PyObject_GetOptionalAttr() and PyObject_GetOptionalAttrString() functions as alternatives to PyObject_GetAttr() and PyObject_GetAttrString() respectively. The new functions do not raise AttributeError if the requested attribute is not found on the object. These variants are more convenient and faster if the missing attribute should not be treated as a failure. (Contributed by Serhiy Storchaka in gh-106521.) Add the PyErr_FormatUnraisable() function as an extension to PyErr_WriteUnraisable() that allows customizing the warning message. (Contributed by Serhiy Storchaka in gh-108082.) Add new functions that return a strong reference instead of a borrowed reference for frame locals, globals, and builtins, as part of PEP 667: PyEval_GetFrameBuiltins() replaces PyEval_GetBuiltins() PyEval_GetFrameGlobals() replaces PyEval_GetGlobals() PyEval_GetFrameLocals() replaces PyEval_GetLocals() (Contributed by Mark Shannon and Tian Gao in gh-74929.) Add the Py_GetConstant() and Py_GetConstantBorrowed() functions to get strong or borrowed references to constants. For example, Py_GetConstant(Py_CONSTANT_ZERO) returns a strong reference to the constant zero. (Contributed by Victor Stinner in gh-115754.) Add the PyImport_AddModuleRef() function as a replacement for PyImport_AddModule() that returns a strong reference instead of a borrowed reference. (Contributed by Victor Stinner in gh-105922.) Add the Py_IsFinalizing() function to check whether the main Python interpreter is shutting down. (Contributed by Victor Stinner in gh-108014.) Add the PyList_GetItemRef() function as a replacement for PyList_GetItem() that returns a strong reference instead of a borrowed reference. (Contributed by Sam Gross in gh-114329.) Add the PyList_Extend() and PyList_Clear() functions, mirroring the Python list.extend() and list.clear() methods. (Contributed by Victor Stinner in gh-111138.) Add the PyLong_AsInt() function. It behaves similarly to PyLong_AsLong(), but stores the result in a C int instead of a C long. (Contributed by Victor Stinner in gh-108014.) Add the PyLong_AsNativeBytes(), PyLong_FromNativeBytes(), and PyLong_FromUnsignedNativeBytes() functions to simplify converting between native integer types and Python int objects. (Contributed by Steve Dower in gh-111140.) Add PyModule_Add() function, which is similar to PyModule_AddObjectRef() and PyModule_AddObject(), but always steals a reference to the value. (Contributed by Serhiy Storchaka in gh-86493.) Add the PyObject_GenericHash() function that implements the default hashing function of a Python object. (Contributed by Serhiy Storchaka in gh-113024.) Add the Py_HashPointer() function to hash a raw pointer. (Contributed by Victor Stinner in gh-111545.) Add the PyObject_VisitManagedDict() and PyObject_ClearManagedDict() functions. which must be called by the traverse and clear functions of a type using the Py_TPFLAGS_MANAGED_DICT flag. The pythoncapi-compat project can be used to use these functions with Python 3.11 and 3.12. (Contributed by Victor Stinner in gh-107073.) Add the PyRefTracer_SetTracer() and PyRefTracer_GetTracer() functions, which enable tracking object creation and destruction in the same way that the tracemalloc module does. (Contributed by Pablo Galindo in gh-93502.) Add the PySys_AuditTuple() function as an alternative to PySys_Audit() that takes event arguments as a Python tuple object. (Contributed by Victor Stinner in gh-85283.) Add the PyThreadState_GetUnchecked() function as an alternative to PyThreadState_Get() that doesn’t kill the process with a fatal error if it is NULL. The caller is responsible for checking if the result is NULL. (Contributed by Victor Stinner in gh-108867.) Add the PyType_GetFullyQualifiedName() function to get the type’s fully qualified name. The module name is prepended if type.__module__ is a string and is not equal to either 'builtins' or '__main__'. (Contributed by Victor Stinner in gh-111696.) Add the PyType_GetModuleName() function to get the type’s module name. This is equivalent to getting the type.__module__ attribute. (Contributed by Eric Snow and Victor Stinner in gh-111696.) Add the PyUnicode_EqualToUTF8AndSize() and PyUnicode_EqualToUTF8() functions to compare a Unicode object with a const char* UTF-8 encoded string and 1 if they are equal or 0 otherwise. These functions do not raise exceptions. (Contributed by Serhiy Storchaka in gh-110289.) Add the PyWeakref_GetRef() function as an alternative to PyWeakref_GetObject() that returns a strong reference or NULL if the referent is no longer live. (Contributed by Victor Stinner in gh-105927.) Add fixed variants of functions which silently ignore errors: PyObject_HasAttrWithError() replaces PyObject_HasAttr(). PyObject_HasAttrStringWithError() replaces PyObject_HasAttrString(). PyMapping_HasKeyWithError() replaces PyMapping_HasKey(). PyMapping_HasKeyStringWithError() replaces PyMapping_HasKeyString(). The new functions return -1 for errors and the standard 1 for true and 0 for false. (Contributed by Serhiy Storchaka in gh-108511.) Changed C APIs¶ The keywords parameter of PyArg_ParseTupleAndKeywords() and PyArg_VaParseTupleAndKeywords() now has type char *const* in C and const char *const* in C++, instead of char**. In C++, this makes these functions compatible with arguments of type const char *const*, const char**, or char *const* without an explicit type cast. In C, the functions only support arguments of type char *const*. This can be overridden with the PY_CXX_CONST macro. (Contributed by Serhiy Storchaka in gh-65210.) PyArg_ParseTupleAndKeywords() now supports non-ASCII keyword parameter names. (Contributed by Serhiy Storchaka in gh-110815.) The PyCode_GetFirstFree() function is now unstable API and is now named PyUnstable_Code_GetFirstFree(). (Contributed by Bogdan Romanyuk in gh-115781.) The PyDict_GetItem(), PyDict_GetItemString(), PyMapping_HasKey(), PyMapping_HasKeyString(), PyObject_HasAttr(), PyObject_HasAttrString(), and PySys_GetObject() functions, each of which clears all errors which occurred when calling them now reports these errors using sys.unraisablehook(). You may replace them with other functions as recommended in the documentation. (Contributed by Serhiy Storchaka in gh-106672.) Add support for the %T, %#T, %N and %#N formats to PyUnicode_FromFormat(): %T: Get the fully qualified name of an object type %#T: As above, but use a colon as the separator %N: Get the fully qualified name of a type %#N: As above, but use a colon as the separator See PEP 737 for more information. (Contributed by Victor Stinner in gh-111696.) You no longer have to define the PY_SSIZE_T_CLEAN macro before including Python.h when using # formats in format codes. APIs accepting the format codes always use Py_ssize_t for # formats. (Contributed by Inada Naoki in gh-104922.) If Python is built in debug mode or with assertions, PyTuple_SET_ITEM() and PyList_SET_ITEM() now check the index argument with an assertion. (Contributed by Victor Stinner in gh-106168.) Limited C API Changes¶ The following functions are now included in the Limited C API: PyMem_RawMalloc() PyMem_RawCalloc() PyMem_RawRealloc() PyMem_RawFree() PySys_Audit() PySys_AuditTuple() PyType_GetModuleByDef() (Contributed by Victor Stinner in gh-85283, gh-85283, and gh-116936.) Python built with --with-trace-refs (tracing references) now supports the Limited API. (Contributed by Victor Stinner in gh-108634.) Removed C APIs¶ Remove several functions, macros, variables, etc with names prefixed by _Py or _PY (which are considered private). If your project is affected by one of these removals and you believe that the removed API should remain available, please open a new issue to request a public C API and add cc: @vstinner to the issue to notify Victor Stinner. (Contributed by Victor Stinner in gh-106320.) Remove old buffer protocols deprecated in Python 3.0. Use Buffer Protocol instead. PyObject_CheckReadBuffer(): Use PyObject_CheckBuffer() to test whether the object supports the buffer protocol. Note that PyObject_CheckBuffer() doesn’t guarantee that PyObject_GetBuffer() will succeed. To test if the object is actually readable, see the next example of PyObject_GetBuffer(). PyObject_AsCharBuffer(), PyObject_AsReadBuffer(): Use PyObject_GetBuffer() and PyBuffer_Release() instead: Py_buffer view; if (PyObject_GetBuffer(obj, &view, PyBUF_SIMPLE)standard header. It was included for the finite() function which is now provided by theheader. It should now be included explicitly if needed. Remove also the HAVE_IEEEFP_H macro. (Contributed by Victor Stinner in gh-108765.) Python.h no longer includes these standard header files: ,and . If needed, they should now be included explicitly. For example,provides the clock() and gmtime() functions,provides the select() function, andprovides the futimes(), gettimeofday() and setitimer() functions. (Contributed by Victor Stinner in gh-108765.) On Windows, Python.h no longer includes thestandard header file. If needed, it should now be included explicitly. For example, it provides offsetof() function, and size_t and ptrdiff_t types. Includingexplicitly was already needed by all other platforms, the HAVE_STDDEF_H macro is only defined on Windows. (Contributed by Victor Stinner in gh-108765.) If the Py_LIMITED_API macro is defined, Py_BUILD_CORE, Py_BUILD_CORE_BUILTIN and Py_BUILD_CORE_MODULE macros are now undefined by . (Contributed by Victor Stinner in gh-85283.) The old trashcan macros Py_TRASHCAN_SAFE_BEGIN and Py_TRASHCAN_SAFE_END were removed. They should be replaced by the new macros Py_TRASHCAN_BEGIN and Py_TRASHCAN_END. A tp_dealloc function that has the old macros, such as: static void mytype_dealloc(mytype *p) { PyObject_GC_UnTrack(p); Py_TRASHCAN_SAFE_BEGIN(p); ... Py_TRASHCAN_SAFE_END } should migrate to the new macros as follows: static void mytype_dealloc(mytype *p) { PyObject_GC_UnTrack(p); Py_TRASHCAN_BEGIN(p, mytype_dealloc) ... Py_TRASHCAN_END } Note that Py_TRASHCAN_BEGIN has a second argument which should be the deallocation function it is in. The new macros were added in Python 3.8 and the old macros were deprecated in Python 3.11. (Contributed by Irit Katriel in gh-105111.) PEP 667 introduces several changes to frame-related functions: The effects of mutating the dictionary returned from PyEval_GetLocals() in an optimized scope have changed. New dict entries added this way will now only be visible to subsequent PyEval_GetLocals() calls in that frame, as PyFrame_GetLocals(), locals(), and FrameType.f_locals no longer access the same underlying cached dictionary. Changes made to entries for actual variable names and names added via the write-through proxy interfaces will be overwritten on subsequent calls to PyEval_GetLocals() in that frame. The recommended code update depends on how the function was being used, so refer to the deprecation notice on the function for details. Calling PyFrame_GetLocals() in an optimized scope now returns a write-through proxy rather than a snapshot that gets updated at ill-specified times. If a snapshot is desired, it must be created explicitly (e.g. with PyDict_Copy()), or by calling the new PyEval_GetFrameLocals() API. PyFrame_FastToLocals() and PyFrame_FastToLocalsWithError() no longer have any effect. Calling these functions has been redundant since Python 3.11, when PyFrame_GetLocals() was first introduced. PyFrame_LocalsToFast() no longer has any effect. Calling this function is redundant now that PyFrame_GetLocals() returns a write-through proxy for optimized scopes. Regression Test Changes¶ Python built with configure --with-pydebug now supports a -X presite=package.module command-line option. If used, it specifies a module that should be imported early in the lifecycle of the interpreter, before site.py is executed. (Contributed by Łukasz Langa in gh-110769.) Table of Contents What’s New In Python 3.13 Summary – Release Highlights New Features A better interactive interpreter Improved error messages Free-threaded CPython An experimental just-in-time (JIT) compiler Defined mutation semantics for locals() Support for mobile platforms Other Language Changes New Modules Improved Modules argparse array ast asyncio base64 compileall concurrent.futures configparser copy ctypes dbm dis doctest email fractions glob importlib io ipaddress itertools marshal math mimetypes mmap multiprocessing os os.path pathlib pdb queue random re shutil site sqlite3 ssl statistics subprocess sys tempfile time tkinter traceback types typing unicodedata venv warnings xml zipimport Optimizations Removed Modules And APIs PEP 594: Remove “dead batteries” from the standard library 2to3 builtins configparser importlib.metadata locale opcode pathlib re tkinter.tix turtle typing unittest urllib webbrowser New Deprecations Pending Removal in Python 3.14 Pending Removal in Python 3.15 Pending Removal in Python 3.16 Pending Removal in Future Versions CPython Bytecode Changes C API Changes New Features Changed C APIs Limited C API Changes Removed C APIs Deprecated C APIs Pending Removal in Python 3.14 Pending Removal in Python 3.15 Pending Removal in Future Versions Build Changes Porting to Python 3.13 Changes in the Python API Changes in the C API Regression Test Changes Previous topic What’s New in Python Next topic What’s New In Python 3.12 This Page Report a Bug Show Source « Navigation index modulesnextpreviousPython » English Spanish French Italian Japanese Korean Brazilian Portuguese Turkish Simplified Chinese Traditional Chinese dev (3.14) 3.13.0 3.12 3.11 3.10 3.9 3.8 3.7 3.6 3.5 3.4 3.3 3.2 3.1 3.0 2.7 2.6 3.13.0 Documentation » What’s New in Python » What’s New In Python 3.13Theme Auto Light Dark© Copyright 2001-2024, Python Software Foundation. This page is licensed under the Python Software Foundation License Version 2. Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License. See History and License for more information. The Python Software Foundation is a non-profit corporation. Please donate. Last updated on Oct 07, 2024 (16:09 UTC). Found a bug? Created using Sphinx 8.0.2.",
    "commentLink": "https://news.ycombinator.com/item?id=41766035",
    "commentBody": "Python 3.13.0 Is Released (python.org)200 points by Siecje 5 hours agohidepastfavorite81 comments pansa2 4 hours agoPython versions 3.11, 3.12 and now 3.13 have contained far fewer additions to the language than earlier 3.x versions. Instead the newest releases have been focusing on implementation improvements - and in 3.13, the new REPL, experimental JIT & GIL-free options all sound great! The language itself is (more than) complex enough already - I hope this focus on implementation quality continues. reply underdeserver 3 hours agoparentFor those interested in the REPL improvements: \" Python now uses a new interactive shell by default, based on code from the PyPy project. When the user starts the REPL from an interactive terminal, the following new features are now supported: Multiline editing with history preservation. Direct support for REPL-specific commands like help, exit, and quit, without the need to call them as functions. Prompts and tracebacks with color enabled by default. Interactive help browsing using F1 with a separate command history. History browsing using F2 that skips output as well as the >>> and … prompts. “Paste mode” with F3 that makes pasting larger blocks of code easier (press F3 again to return to the regular prompt). \" Sounds cool. Definitely need the history feature, for the few times I can't run IPython. reply user070223 2 hours agorootparentThanks, I just found out about .pythonstartup and setup writing history to a file and pretty printing with pprint / rich. https://www.bitecode.dev/p/happiness-is-a-good-pythonstartup or search for a gist reply mixmastamyk 1 hour agorootparentprevI hope it doesn’t break ptpython, and/or is worse than it. I’ve been using it for quite a while. reply formerly_proven 2 hours agorootparentprevPresumably this also means readline (GPL) is no longer required to have any line editing beyond what a canonical-mode terminal does by itself. It seems like there is code to support libedit (BSD), but I've never managed to make Python's build system detect it. reply the__alchemist 3 hours agoparentprevI've love to see a revamp of the import system. It is a continuous source of pain points when I write Python. Circular imports all over unless I structure my program explicitly with this in mind. Using python path hacks with `sys` etc to go up a directory too. reply int_19h 1 hour agorootparentThe biggest problem with Python imports is that the resolution of non-relative module names always prioritizes local files, even when the import happens in stdlib. This means that, for any `foo` that is a module name in stdlib, having foo.py in your code can break arbitrary modules in stdlib. For example, this breaks: # bisect.py ... # main.py import random with: Traceback (most recent call last): File \".../foo.py\", line 1, inimport random File \"/usr/lib/python3.12/random.py\", line 62, infrom bisect import bisect as _bisect ImportError: cannot import name 'bisect' from 'bisect' This is very frustrating because Python stdlib is still very large and so many meaningful names are effectively reserved. People are aware of things like \"sys\" or \"json\", but e.g. did you know that \"wave\", \"cmd\", and \"grp\" are also standard modules? Worse yet is that these errors are not consistent. You might be inadvertently reusing an stdlib module name without even realizing it just because none of the stdlib (or third-party) modules that you import have it in their import graphs. Then you move on to a new version of Python or some of your dependencies, and suddenly it breaks because they have added an import somewhere. But even if you are careful about checking every single module name against the list of standard modules, a new Python version can still break you by introducing a new stdlib module that happens to clash with one of yours. For example, Python 3.9 added \"graphlib\", which is a fairly generic name. reply notatallshaw 1 hour agorootparentprevThere was an attempt to make imports lazy: https://peps.python.org/pep-0690/ It was ultimately rejected due to issues with how it would need to change the dict object. IMO all the rejection reasons could be overcome with a more focused approach and implementation, but I don't know if there is anyone wishing to give it another go. reply baq 2 hours agorootparentprevFor me 'from __future__ import no_cyclic_imports' would be good enough, if it's even possible reply derr1 2 hours agorootparentprevPathlib is your friend reply csdreamer7 3 hours agoparentprev> The language itself is (more than) complex enough already - I hope this focus on implementation quality continues. As do I. reply selimnairb 3 hours agorootparentAgreed. I still haven’t really started using the ‘match’ statement and structural pattern matching (which I would love to use) since I still have to support Python 3.8 and 3.9. I was getting tired of thinking, “gee this new feature will be nice to use in 4 years, if I remember to…” reply wdroz 2 hours agorootparentAfter this month, Python 3.8 will be end-of-life, so maybe you can push internally to upgrade python. reply formerly_proven 4 hours agoparentprevThe last couple years also saw a stringent approach to deprecations: If something is marked as deprecated, it WILL be removed in a minor release sooner than later. reply kstrauser 3 hours agorootparentYep. They’ve primarily (entirely?) involved removing ancient libraries from stdlib, usually with links to maintained 3rd party libraries. People who can’t/won’t upgrade to newer Pythons, perhaps because their old system that uses those old modules can’t run a newer one, aren’t affected. People using newer Pythons can replace those modules. There may be a person in the world panicking that they need to be on Python 3.13 and also need to parse Amiga IFF files, but it seems unlikely. reply tightbookkeeper 3 hours agoparentprevRemoving GIL only increases complexity. reply dagmx 3 hours agorootparentFor the runtime, but not the language reply bunderbunder 2 hours agorootparentprevBut they've worked very hard at shielding most users from that complexity. And the end result - making multithreading a truly viable alternative to multiprocessing for typical use cases - will open up many opportunities for Python users to simplify their software designs. I suppose only time will tell if that effort succeeds. But the intent is promising. reply dangom 12 minutes agorootparentDo you have any references or examples that describe how this simplification would come about? Would love to learn more about it. reply pkkm 2 hours agorootparentprevIt definitely does, but don't you think that it could be worth it if it makes multithreading usable for CPU-heavy tasks? reply tightbookkeeper 2 hours agorootparentNo. Python is orders of magnitude slower than even C# or Java. It’s doing hash table lookups per variable access. I would write a separate program to do the number crunching. Everyone must now pay the mental cost of multithreading for the chance that you might want to optimize something. reply zbentley 1 hour agorootparent> It’s doing hash table lookups per variable access. That hasn't been true for many variable accesses for a very long time. LOAD_FAST, LOAD_CONST, and (sometimes) LOAD_DEREF provide references to variables via pointer offset + chasing, often with caches in front to reduce struct instantiations as well. No hashing is performed. Those access mechanisms account for the vast majority (in my experience; feel free to check by \"dis\"ing code yourself) of Python code that isn't using locals()/globals()/eval()/exec() tricks. The remaining small minority I've seen is doing weird rebinding/shadowing stuff with e.g. closures and prebound exception captures. https://github.com/python/cpython/blob/10094a533a947b72d01ed... https://github.com/python/cpython/blob/10094a533a947b72d01ed... So too for object field accesses; slotted classes significantly improve field lookup cost, though unlike LOAD_FAST users have to explicitly opt into slotting. Don't get me wrong, there are some pretty regrettably ordinary behaviors that Python makes much slower than they need to be (per-binding method refcounting comes to mind, though I hear that's going to be improved). But the old saw of \"everything is a dict in python, even variable lookups use hashing!\" has been incorrect for years. reply tightbookkeeper 1 hour agorootparentThanks for the correction and technical detail. I’m not saying this is bad, it’s just the nature of this kind of dynamic language. Productivity over performance. reply int_19h 1 hour agorootparentprevPython doesn't do hash table lookups for local variable access. This only applies to globals and attributes of Python classes that don't use __slots__. The mental cost of multithreading is there regardless because GIL is usually at the wrong granularity for data consistency. That is, it ensures that e.g. adding or deleting a single element to a dict happens atomically, but more often than not, you have a sequence of operations like that which need to be locked. In practice, in any scenario where your data is shared across threads, the only sane thing is to use explicit locks already. reply kstrauser 1 hour agorootparentprev> No. Python is orders of magnitude slower than even C# or Java. That sounds like a fantastic reason to make it run faster on the multi-core CPUs we're commonly running it on today. reply tightbookkeeper 46 minutes agorootparentThe cost to write and debug multithreaded code is high and not limited to the area you use it. And for all that you get a 2-8x speed up. So if you care about performance why are you writing that part in python? > multi-core CPUs we're commonly running it on today. If you spawn processes to do work you get multi core for free. Think of the whole system, not just your program. reply wdroz 3 hours agoprevWith the 3.13 TypeIs[0] and the 3.10 TypeGuard[1], we can achieve some of Rust's power (such as the 'if let' pattern) without runtime guarantees. This is a win for the DX, but this is not yet widely used. For example, \"TypeGuard[\" appears in only 8k Python files on GitHub.[2] [0] -- https://docs.python.org/3.13/library/typing.html#typing.Type... [1] -- https://docs.python.org/3.13/library/typing.html#typing.Type... [2] -- https://github.com/search?q=%22TypeGuard%5B%22+path%3A*.py&t... reply Buttons840 1 hour agoparentWhat type checker do you recommend? reply boarush 2 hours agoprevPython version from 3.10 have had a very annoying bug with the SSLContext (something related only to glibc) where there are memory leaks when opening new connections to new hosts and eventually causes any service (dockerized in my case) to crash due to OOM. Can still see that the issues have not been resolved in this release which basically makes it very difficult to deploy any production grade service difficult. reply infocollector 2 hours agoparentCan you please link me to a raised issue for this? This sounds concerning. reply saurik 2 hours agorootparentWhile this bug has been around longer than merely since 3.10, I am betting this is the one, based on the description of the issue: https://github.com/python/cpython/issues/84904 (Don't let the associates with asyncio throw you: that was merely the code in which it was first found; later code transcends it.) reply orf 2 hours agoparentprevMaybe related to this segfault I found on MacOS? https://github.com/python/cpython/issues/114653 reply rkwz 5 hours agoprev> Free-threaded execution allows for full utilization of the available processing power by running threads in parallel on available CPU cores. While not all software will benefit from this automatically, programs designed with threading in mind will run faster on multi-core hardware. Would be nice to see performance improvements for libraries like FastAPI, NetworkX etc in future. reply v3ss0n 3 hours agoparentThey are not threaded at all. reply cr125rider 3 hours agorootparentCorrect. Async stuff in Python is based on libuv like event loops similar to how Nodejs and others operate, not full threads. reply CJefferson 4 hours agoprevGood to get advanced notice, if I read all the way down, that they will silently completely change the behavior of multiprocessing in 3.14 (only on Unix/Linux, in case other people wonder what’s going on), which is going to break a bunch of programs I work with. I really like using Python, but I can’t keep using it when they just keep breaking things like this. Most people don’t read all the release notes. reply icegreentea2 4 hours agoparentNot defending their specific course of action here, but you should probably try to wade into the linked discussion (https://github.com/python/cpython/issues/84559). Looks like the push to disable warnings (in 3.13) is mostly coming from one guy. reply CJefferson 4 hours agorootparentI think should have a dig. While it’s not perfect, I know a few other people people who do “set up lots of data structures, including in libraries, then make use of the fact multiprocessing uses fork to duplicate them”. While fork always has sharp edges, it’s also long been clearly documented that’s the behavior on Linux. reply int_19h 1 hour agorootparentI'm pretty sure that significantly more people were burned by fork being the default with no actual benefit to their code, whether because of the deadlocks etc that it triggers in multithreaded non-fork-aware code, or because their code wouldn't work correctly on other platform. Keeping it there as an option that one can explicitly enable for those few cases where it's actually useful and with full understanding of consequences is surely the better choice for something as high-level as Python. reply mixmastamyk 1 hour agoparentprevUnfortunately, they learned the wrong lesson from the 2->3 transition. Break things constantly instead of all at once. :p Still, this one doesn’t seem too bad. Add method=FORK now and forget about it. reply almostgotcaught 4 hours agoparentprev> I really like using Python, but I can’t keep using it when they just keep breaking things like this. So much perl clutching. Just curious, since I guess you've made up your mind, what's your plan to migrate away? Or are you hoping maintainers see your comment and reconsider the road-map? reply fmajid 2 hours agorootparentNot the person you are responding to, but my Python 3 migration plan was to move to Go for all new projects. reply almostgotcaught 37 minutes agorootparentI'm sure your departure from the community will be the tectonic shift that'll finally get the PSF to change their course. reply ajay-d 4 hours agoprevStill in prerelease (RC3), no? At least at time of writing reply milliams 4 hours agoparentIt looks like it releases today (see https://peps.python.org/pep-0719/), but the release is not yet made on https://www.python.org/downloads/ and the tag has not been made in Git yet. reply pidjan 1 hour agorootparenthttps://www.python.org/downloads/release/python-3130/ reply pansa2 4 hours agoparentprevYeah, the past tense on the What’s New page isn't (yet) accurate: > Python 3.13 was released on October 7, 2024 reply stevesimmons 3 hours agoprevAnd Azure Functions still doesn't support Python 3.12, released more than a year ago! reply cr125rider 3 hours agoparentWe have switched to exclusively using Docker Images in Lambda on AWS cause their runtime team constantly breaks things and is behind with a bunch of releases. reply mg 4 hours agoprevWhen I'm in a docker container using the Python 3 version that comes with Debian - is there an easy way to swap it out for this version so I can test how my software behaves under 3.13? reply ebb_earl_co 4 hours agoparentThis[0] is the Docker Python using Debian Bookworm, so as soon as 3.13.0 (not the release candidate I've linked to) is released, there will be an image. Otherwise, there's always the excellent `pyenv` to use, including this person's docker-pyenv project [1] [0] https://hub.docker.com/layers/library/python/3.13.0rc3-slim-... [1] https://github.com/tzenderman/docker-pyenv?tab=readme-ov-fil... reply mg 3 hours agorootparentHmm.. I think this is a misunderstanding. What I meant is: While I am already inside a container running Debian, can I ... 1: ./myscript.py 2: some_magic_command 3: ./myscript.py So 1 runs it under 3.11 (which came with Debian) and 2 runs it under 3.13. I don't need to preserve 3.11. some_magic_command can wrack havoc in the container as much as it wants. As soon as I exit it, it will be gone anyhow. The in a sense, the question is not related to Docker at all. I just mentioned that I would do it inside a container to emphasize that I don't need to preserve anything. reply maleldil 3 hours agorootparentYou can use pyenv to create multiple virtual environments with different Python versions, so you'd run your script with (eg) venv311/bin/python and venv313/bin/python reply kstrauser 3 hours agorootparentprevThe magic command in other settings would be pyenv. It lets you have as many different Python versions installed as you wish. Pro tip: outside Docker, don’t ever use the OS’s own Python if you can avoid it. reply maleldil 3 hours agorootparent> don’t ever use the OS’s own Python if you can avoid it. This includes Homebrew's Python installation, which will update by itself and break things. reply kstrauser 3 hours agorootparentYep. I only have Homebrew's Python installed because some other things in Homebrew depend on it. I use pyenv+virtualenv exclusively when developing my own code. (Technically, I use uv now, but to the same ends.) reply cuu508 2 hours agorootparentprev> Pro tip: outside Docker, don’t ever use the OS’s own Python if you can avoid it. Why not? reply kstrauser 1 hour agorootparentIt's unlikely that the OS's version of Python, and the Python packages available through the OS, are going to be the ones you'd install of your own volition. And on your workstation, it's likely you'll have multiple projects with different requirements. You almost always want to develop in a virtualenv so you can install the exact versions of things you need without conflicting with the ones the OS itself requires. If you're abstracting out the site-packages directory anyway, why not take one more step and abstract out Python, too? Things like pyenv and uv make that trivially easy. For instance, this creates a new project using Python 3.13. $ uv init -p python3.13 foo $ cd foo $ uv sync $ .venv/bin/python --version Python 3.13.0rc2 I did not have Python 3.13 installed before I ran those commands. Now I do. It's so trivially easy to have per-project versions that this is my default way of using Python. You can get 95% of the same functionality by installing pyenv and using it to install the various versions you might want. It's also an excellent tool. Python's own built-in venv module (https://docs.python.org/3/library/venv.html) makes it easy to create virtualenvs anytime you want to use them. I like using uv to combine that and more into one single tool, but that's just my preference. There are many tools that support this workflow and I highly recommend you find one you like and use it. (But not pipenv. Don't pick that one.) reply mixmastamyk 1 hour agorootparentThis is the conventional wisdom these days, and a real thing, but unless you are admin challenged, running your local scripts with the system Python is fine. Been doing it two decades plus now. Yes, make a venv for each work project. reply orf 2 hours agorootparentprevIgnore the talk below about pyenv, it’s not even slightly suitable for this task. You want precompiled Python binaries. Use “uv” for this, rather than hacking it together with pyenv. reply kstrauser 1 hour agorootparentUse either one. `pyenv install 3.x` is slower than `uv python install 3x`, but that's not the most common operation I use either of those tools for. Uv is also comparatively brand new, and while I like and use it, I'm sure plenty of shops aren't racing to switch to it. If you already have pyenv, use it. If you don't have pyenv or uv, install uv and use that. Either one is a huge upgrade over using the default Python from your OS. reply silveraxe93 3 hours agoparentprevInstall `uv` then run `uv run --python 3.13 my_script.py` reply BiteCode_dev 3 hours agoprevI still see Python 3.12.7 being the latest one, with 3.13 delayed because of the GC perf regression. The link, for me, points to the 3.13 RC. Am I seeing a cached version and you see 3.13 ? Cause I can't see it on the homage page download link either. reply fmajid 2 hours agoparentNo, they jumped the gun. reply causal 4 hours agoprevAny rule of thumb when it comes to adopting Python releases? Is it usually best to wait for the first patch version before using in production? reply oebs 4 hours agoparentWe follow this rule (about two dozen services with in total ~100k loc of Python): By default, use the version 1 release below the latest. I.e. we currently run 3.11 and will now schedule work to upgrade to 3.12, which is expected to be more or less trivial for most services. The rationale is that some of the (direct and transitive) dependencies will take a while to be compatible with the latest release. And waiting roughly a year is both fast enough to not get too much behind, and slow enough to expect that most dependencies have caught up with the latest release. reply jnwatson 1 hour agorootparentYeah some deprecated C API stuff just got removed, so it might take me, a package maintainer, to catch up. reply BiteCode_dev 3 hours agoparentprevI follow this: https://www.bitecode.dev/p/installing-python-the-bare-minimu... Which is mostly latest_major - 1, adjusted to production constraints, obviously. And play with latest for fun. I stopped using latest even for non serious projects, the ecosystem really needs time to catch up. reply user070223 4 hours agoparentprevWhen should you upgrade to Python 3.13? https://pythonspeed.com/articles/upgrade-python-3.13/ Python libraries support https://pyreadiness.org/3.13/ reply instig007 4 hours agoparentprevHave a rubust CI and tests, and deploy as early as you can. reply kstrauser 3 hours agorootparentYup. At my last gig, upgrading to a new version meant setting the Docker tag to the new one and running `make test`. If that passed, we were 99% certain it was safe for prod. The other 1% was covered by running in pre-prod for a couple days. reply zahlman 3 hours agoparentprev>Any rule of thumb when it comes to adopting Python releases? No, because it varies widely depending on your use case and your motivations. >Is it usually best to wait for the first patch version before using in production? This makes it sound like you're primarily worried about a situation where you host an application and you're worried about Python itself breaking. On the one hand, historically Python has been pretty good about this sort of thing. The bugfixes in patches are usually quite minor, throughout the life cycle of a minor version (despite how many of them there are these days - a lot of that is just because of how big the standard library is). 3.13 has already been through alpha, beta and multiple RCs - they know what they're doing by now. The much greater concern is your dependencies - they aren't likely to have tested on pre-release versions of 3.13, and if they have any non-Python components then either you or they will have to rebuild everything and pray for no major hiccups. And, of course, that applies transitively. On the other hand, unless you're on 3.8 (dropping out of support), you might not have any good reason to update at all yet. The new no-GIL stuff seems a lot more exciting for new development (since anyone for whom the GIL caused a bottleneck before, will have already developed an acceptable workaround), and I haven't heard a lot about other performance improvements - certainly that hasn't been talked up as much as it was for 3.11 and 3.12. There are a lot of quality-of-implementation improvements this time around, but (at least from what I've paid attention to so far, at least) they seem more oriented towards onboarding newer programmers. And again, it will be completely different if that isn't your situation. Hobbyists writing new code will have a completely different set of considerations; so will people who primarily maintain mature libraries (for whom \"using in production\" is someone else's problem); etc. reply zenonu 4 hours agoparentprevI'm constrained by libraries with guaranteed version compatibility. Unless you're operating in NIH universe, I bet you are as well. reply ilc 3 hours agoparentprevn-1 is the rule I follow. So if asked today I'd look at 3.12. reply HelloNurse 3 hours agoparentprevWait until they are actually released rather than RC3. What's the point of posting prematurely? reply yn6n767m76m 3 hours agoparentprevScream into a pillow because even PHP manages to have less breaking releases. Python is a dumpster fire that no one wants to admit or have an honest conversation about. If python 2 is still around my advice is don't upgrade unless you have a clear reason for the new features. reply SubiculumCode 3 hours agoprevWhat I've been surprised about is the number of python packages that require specific python versions(e.g., works on 3.10, but not 3.11. Package versioning is already touchy enough without the language itself causing it in minor upgrades. And will python 3.14 be named pi-thon 3.14. I will see myself out. reply bun_terminator 2 hours agoprevI appreciate the effort to leave out the \"And now for something completely different\" section (on https://www.python.org/downloads/release/python-3130/) after the previous drama. reply gjvc 4 hours agoprevlooking forward to the GraalVM version reply rhnamec 2 hours agoprev [3 more] [flagged] int_19h 1 hour agoparent [–] If you're making claims about defamation and libel, at least be specific. reply at_your_service 38 minutes agorootparent [–] Discussion of one of the victims of the Steering Council: https://news.ycombinator.com/item?id=41625044 https://news.ycombinator.com/item?id=41625688 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Python 3.13 introduces a better interactive interpreter, enhanced error messages, and experimental support for free-threaded CPython and a just-in-time (JIT) compiler.",
      "Key updates include defined mutation semantics for `locals()`, support for mobile platforms, and significant updates to the standard library.",
      "The release also features optimizations, changes to the C API, removal of deprecated modules and APIs, and new deprecations."
    ],
    "commentSummary": [
      "Python 3.13.0 has been released, emphasizing implementation improvements such as a new Read-Eval-Print Loop (REPL), experimental Just-In-Time (JIT) compilation, and options to run without the Global Interpreter Lock (GIL).",
      "The updated REPL now includes features like multiline editing and color prompts, enhancing user interaction.",
      "Discussions among users highlight concerns about potential breaking changes and library compatibility, alongside appreciation for the focus on performance and quality improvements."
    ],
    "points": 199,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1728309227
  },
  {
    "id": 41764635,
    "title": "Visualization of website accessibility tree",
    "originLink": "https://chromewebstore.google.com/detail/aria-devtools/dneemiigcbbgbdjlcdjjnianlikimpck",
    "originBody": "When COVID-19 started I needed something to get busy to not go crazy. I happened to work on our app WCAG compliance for a few months at the time and was frustrated by the state of of accessibility-related tools for developers.I&#x27;ve spend two months delivering a tool that is easy to understand and helps catching accessibility issues on my apps. A few years later it&#x27;s pretty popular despite being mostly abandoned.I will be happy to work on this further but honestly lost my enthusiasm some time ago. I&#x27;d love to get in touch with some company in the accessibility testing space and discuss how to improve it.",
    "commentLink": "https://news.ycombinator.com/item?id=41764635",
    "commentBody": "Visualization of website accessibility tree (chromewebstore.google.com)196 points by ziolko 8 hours agohidepastfavorite16 comments When COVID-19 started I needed something to get busy to not go crazy. I happened to work on our app WCAG compliance for a few months at the time and was frustrated by the state of of accessibility-related tools for developers. I've spend two months delivering a tool that is easy to understand and helps catching accessibility issues on my apps. A few years later it's pretty popular despite being mostly abandoned. I will be happy to work on this further but honestly lost my enthusiasm some time ago. I'd love to get in touch with some company in the accessibility testing space and discuss how to improve it. kilian 19 minutes agoVery cool. I recently implemented my own accessibility tree visualization [1] Yours is very interesting, getting away from the tree itself to a visualization more focused on grouping discrete units. My thinking was to show the entire structure and through that help people focus on a logical flow through the page. Flipping that around and thinking of the tree as a set of discrete blocks, where the cohesion inside each block is more important, is very interesting. Happy to chat if you want to compare notes! [1] https://polypane.app/blog/polypane-20-1-the-accessibility-tr... reply Evan-Purkhiser 6 minutes agoprevI’ve been using this for years now thank you so much for building this!! reply notjustanymike 5 hours agoprevA very handy tool for spot checking, but also training. My benefit from this visualization will be demonstrating to non-technical how to people to think about accessibility (specifically screen readers). Half the challenge with WCAG is getting our stakeholders to think beyond ticking boxes for compliance. reply yreg 5 hours agoprevThank you, this looks great! Could you please run it inside iframes as well? Being able to use this in the Storybook/Playroom would be awesome. --- Firefox link: https://addons.mozilla.org/en-US/firefox/addon/aria-devtools... reply ziolko 2 hours agoparentI am super glad you like it! The primary reason I haven't introduced support for iframes is that it requires much wider permissions (instead of just access to the current tab after clicking the \"ARIA DevTools\" icon you'd need to grant access to all data on every website you visit). I will research if things changed since I last checked, though. reply afloatboat 4 hours agoprevPretty clean, I like it. I tested it out on a page I'm developing that has some meta data on a TV show. One of the elements is a set of divs each containing span with an `aria-label` describing the contents. With MacOs' VO this gets called out correctly, and Chrome's Accessibility Tree also picks this up, but this tool doesn't show the `aria-label`, it just shows the separate values as strings one after another. It also picked up a `::before { content: \", \" / \"\"; }` as `, value`, but that's not supported very well in general. reply ziolko 3 hours agoparentIs there a chance to get a link to the page? I'd love to try it out and fix. reply afloatboat 1 hour agorootparentI can't link the page, but this is a cleaned up DOM output (the extra spans/divs are components): `IMDb 8.212+2007RomanceComedyCast:ABCDEFGHI` `section ul { margin: 0; padding: 0; list-style: none; li { display: inline; &:not(:first-child)::before { content: \", \" / \"\"; } } }` reply vladde 7 hours agoprevLooks neat, and way more clean than https://wave.webaim.org/ reply ziolko 6 hours agoparentThank you! I really think there's a lot of potential in ARIA DevTools. It's quite popular (at least for my standards) but I never had any connection with people that live and breathe web accessibility. And the devil is in the details here so to be fully fair, WAVE is most probably more accurate. reply ChrisMarshallNY 7 hours agoprevGood stuff! I'm big on accessibility support. Web sites aren't really my deal, anymore, but I always used to make sure that my sites were very accessible, when it was my deal. reply Someone 4 hours agoprevHow does this compare to Chrome’s built-in tooling (https://developer.chrome.com/docs/devtools/accessibility/ref...)? reply ziolko 4 hours agoparentWhen I designed the tool I've tried to mirror the experience of screen readers users instead of only displaying ARIA roles and properties. For example you have to navigate the page with your keyboard only. If a dropdown isn't accessible - it's instantly clear for the user. Another example are tables. They present only one cell + their headers at the time. I think it's super close to the actual experience of screen reader users. reply danng87 3 hours agoprevAwesome tool! I've been diving more into accessibility lately, especially trying to improve the experience for screen reader users. For those with more experience, has anyone tested this tool on more complex scenarios like extensive forms or dynamic tables? I'd love to hear how it compares to other accessibility tools in those cases. Any tips or insights would be appreciated! reply antriani_ 4 hours agoprevDoes it offer any additional features compared to the accessibility view in Chrome DevTools? reply Leimi 7 hours agoprev [–] Super useful, great job. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author developed an accessibility tool during the COVID-19 pandemic to address frustrations with existing tools and improve WCAG (Web Content Accessibility Guidelines) compliance.",
      "Despite its initial popularity, the tool has been mostly abandoned, and the author seeks collaboration with a company in the accessibility testing industry to enhance it.",
      "The post highlights the ongoing need for effective accessibility tools and the potential for innovation in this space."
    ],
    "commentSummary": [
      "A developer created a tool during COVID-19 to visualize website accessibility trees, enhancing Web Content Accessibility Guidelines (WCAG) compliance tools.",
      "The tool gained popularity for its unique focus on logical flow and discrete units, rather than solely on Accessible Rich Internet Applications (ARIA) roles.",
      "Users praise its simplicity and effectiveness, especially for screen reader demonstrations, and suggest improvements like iframe support, comparing it to Chrome's built-in tools."
    ],
    "points": 196,
    "commentCount": 16,
    "retryCount": 0,
    "time": 1728297516
  },
  {
    "id": 41765334,
    "title": "Photos of an e-waste dumping ground",
    "originLink": "https://www.npr.org/sections/goats-and-soda/2024/10/05/g-s1-6411/electronics-public-health-waste-ghana-phones-computers",
    "originBody": "Goats and Soda Stunning photos of a vast e-waste dumping ground — and those who make a living off it October 5, 20241:32 PM ET By Jonathan Lambert The now-demolished Agbogbloshie Scrapyard in Accra, Ghana, once received about 15,000 tons of electronic waste each year, making it one of the largest e-waste processing sites in Africa. Muntaka Chasant hide caption toggle caption Muntaka Chasant When he was just 18 years old, Emmanuel Akatire traveled about 500 miles from his home in Zorko, Ghana, to Accra, the nation’s capital, to find the only work he could — sifting through vast piles of discarded electronics to find valuable scrap metal. A week’s worth of painstaking, often dangerous work, earns him the equivalent of about 60 U.S. dollars. “I started doing scrap work around 2021 — after I lost both my parents — to provide for the rest of my family,” he told Muntaka Chasant, a photojournalist in Ghana. “There’s no electricity in my community, no development there,” he said. So he came to Accra, which has become a major dumping ground for the globe’s used electronics. Sponsor Message For years, a site called Agbogbloshie in Accra was one of the largest e-waste processing sites in Africa, getting 15,000 tons of discarded phones, computers and other used electronics each year. Many Western media outlets depicted the site as a public health and environmental tragedy, rife with toxic chemicals that leach into the water and poison the air. While that’s undoubtedly true, it’s not the full story, according to a new collaborative photojournalism project. Emmanuel Akatire, 20, roams around e-waste processing areas with a sack, sorting through discarded e-waste materials and separating them by material — copper, iron and aluminum — for sale. Muntaka Chasant for Fondation Carmignac hide caption toggle caption Muntaka Chasant for Fondation Carmignac The project, called E-Waste in Ghana: Tracing Transboundary Flows, which won this year’s Fondation Carmignac photojournalism award, aims to capture both the positive and negative aspects of e-waste. “The world cannot throw all its garbage here, it has truly negative consequences on the people,” says Anas Aremeyaw Anas, an investigative journalist in Ghana who co-led the project. “But there are positive aspects of sending us e-waste,” he says, as it’s sparked a dynamic, informal recycling economy in the country that, while often dangerous, can also help lift people like Emmanuel Akatire out of poverty. Globally, e-waste is an enormous problem. In 2022, humans discarded about 62 million tons of used electronics, enough to fill a line of trucks that spans the equator. But there’s opportunity too, as those trucks contain over $91 billion of valuable metals, the U.N. estimates, though people like Akatire who do the dangerous recycling work reap the smallest share of those potential profits. So-called “pickers” sift through the soil at e-waste dumps to find small pieces of precious metals, like these iron and copper fragments that likely came from a burned electrical cable. Muntaka Chasant for Fondation Carmignac hide caption toggle caption Muntaka Chasant for Fondation Carmignac E-waste falls into two broad buckets: functional and non-functional. The line between them can be fuzzy, as what’s still usable or repairable to one person may not be to another, but the distinction is important. International laws prohibit trafficking of non-functional e-waste containing toxic substances, but the United Nations sees trading functional e-waste as beneficial, as it can lengthen the lifespan of a product. Sponsor Message The project found that exporters often fail to separate functional from non-functional e-waste. “If you have a container full of TV screens, how on earth are you going to verify each and every one of them to make sure that they are functioning,” says photojournalist Bénédicte Kurzen, a co-author of the project. As a result, both kinds of e-waste get stuffed into container ships that make their way to low- and middle-income countries like Ghana. An X-ray image of a cargo truck going through customs in Accra, Ghana. A drive-through X-ray portal at the port can scan up to 120 vehicles an hour for illicit items, but the investigative team found that well-placed bribes can get some officials to allow e-waste through. Bénédicte Kurzen for Fondation Carmignac / NOOR hide caption toggle caption Bénédicte Kurzen for Fondation Carmignac / NOOR Formally, Ghana prohibits the import of many forms of hazardous e-waste material. But the team found that a well-placed bribe can get port officials to look the other way. As a result, informal e-waste sites are growing across Ghana’s coast. There, both functional and non-functional e-waste get dumped into vast piles that are encroaching on residential areas. Thousands of “pickers” come to these sites, picking through the rubbish to separate items that might be repaired from waste that could contain valuable minerals. An 18-year-old holding a hammer and a TV walks over pile of e-waste. Collecting e-waste to salvage can be dangerous work. Cuts, burns and other injuries are common, and e-waste sites are often blanketed in noxious fumes. Muntaka Chasant for Fondation Carmignac hide caption toggle caption Muntaka Chasant for Fondation Carmignac Many of the pickers are climate migrants from an area of Ghana known as the “Upper East,” says project co-author Chasant, where warming temperatures are upending traditional farming practices. “This area has the highest unemployment rate among young people,” he says. They come to e-waste sites to earn money during the dry season, Chasant says, which they bring back to the Upper East. “There’s a whole generation of young people that are building their society from e-waste work.” It’s fraught, precarious work. To separate valuable minerals, like copper wire or iron, from useless plastic, pickers often burn the trash, producing noxious fumes. Burns, cuts and other injuries are common. E-waste workers — many of whom are children, the team found — are at risk of exposure to over 1,000 harmful chemicals, according to the World Health Organization, including lead, mercury and brominated flame retardants, which are linked to higher rates of diseases like cancer and diabetes. Sponsor Message Bernard Akanwee Atubawuna, a 21-year-old picker from Upper East, died while doing scrap work, says Chasant. His cause of death is unknown, but “Akanwee was a sickle-cell carrier and should never have been anywhere near fires/picking. But his parents said it was the only way he supported them. He sent his sister to learn a trade. He was also helping the parents to build a house back in the Upper East — all from picking.” Simon Aniah, a 24-year-old from the Upper East region of Ghana, burns electrical cables to recover the copper wiring. In 2021, Upper East had the highest unemployment rate for young people in Ghana, forcing many to migrate to Accra to find work. Muntaka Chasant for Fondation Carmignac hide caption toggle caption Muntaka Chasant for Fondation Carmignac The harms extend beyond the dumps themselves. “There are many communities that have run to a halt as a result of the devastating effects of the poisonous gases that are inhaled,” says Anas. Heavy metals seep into the soil and water too, which can have profound health effects on local communities. “When people start burning, the poisonous fumes lead to people relocating.” A man in a canoe pulls toward the banks to recover more recyclable plastics. E-waste dumping takes a heavy environmental toll, often degrading the local water supply. Mutaka Chasant for Fondation Carmignac hide caption toggle caption Mutaka Chasant for Fondation Carmignac A burgeoning recycling and repair industry has risen up alongside those harms. The team documented informal marketplaces, where vendors sell scores of busted cell phones to buyers looking to repair circuit boards or extract their precious metals. On Zongo Lane in Accra, the reporters say, hundreds of small, independent shops sell used or repaired equipment, ranging from televisions to computers. “In Africa, people still have this thing of repairing is important. Don't throw it away. You can still do things about it,” says Kurzen. In Western countries, people view these objects as much more disposable, she says, which is helping fuel the growth of e-waste worldwide. The man in the bright green shirt is working in one of the hundreds of small shops that have popped up around e-waste sites, where discarded items are repaired and resold, or broken down for parts. Bénédicte Kurzen for Fondation Carmignac / NOOR hide caption toggle caption Bénédicte Kurzen for Fondation Carmignac / NOOR The most valuable minerals extracted from Ghana’s e-waste often don’t stay in Ghana. Many of the most valuable items get cherry picked and sent to more advanced smelters in Europe or Asia, the team found. “People are dismantling these items in toxic environments, and then the few piles that contain incredibly valuable minerals are going to be re-exported,” says Kurzen. Sponsor Message She and her colleagues hope that the project prompts people to re-examine their relationship to the electronics in their lives. The few years we spend with a new phone represents just a blip in the story of the materials that power it, Kurzen says. “Those materials are traveling worldwide,” she says, “These [devices] we hold in our hands come at the expense of somebody, somewhere in the world. Nothing comes for free.” Jonathan Lambert is a Washington, D.C.-based freelance journalist who covers science, health and policy. Correction Oct. 6, 2024 A previous version of this story incorrectly stated that the Agbogbloshie scrapyard in Ghana received 250,000 tons of e-waste each year. That number refers to a 2011 estimate of the amount of e-waste received by five African countries. A study published in 2022 estimates Agbogbloshie received about 15,000 tons of e-waste per year. e-waste Facebook Flipboard Email",
    "commentLink": "https://news.ycombinator.com/item?id=41765334",
    "commentBody": "Photos of an e-waste dumping ground (npr.org)187 points by andsoitis 6 hours agohidepastfavorite111 comments agentultra 5 hours agoThis is one reason I believe \"right to repair\" laws are so important. The environmental damage of producing the device is already done. Make it last as long as possible. Reduce, reuse... then recycle. Re-using devices helps us also reduce the number of new devices needed... which is what probably scares the corporate oligarchy. If we're not buying new phones every couple of years how will the stock prices keep going up? Never the less, the devices we make these days can last a long, long time. I've been repairing and maintaining iPhone 5's, 7's, and 8's that are no where near their end of life. The iPhone has a couple of small electrolytic capacitors which should have a useful life of at least 20 years. And can be replaced! The batteries and screens can replaced. These devices can last much longer than we give them credit for. But tech companies have been struggling to make it illegal or difficult to repair for a long time. I've been seeing photojournalist projects such as this since the late 90s at least (longer perhaps). In North America we had a culture that valued repairing and building things that lasted. It's as good a time as any to push for this to return! Support policy makers that are pushing for right-to-repair and environmental protection! And pick up a new hobby if you are able. Support your local tech geeks if you can! reply yndoendo 4 hours agoparentRefurbish and repairing viable electronics does not help keep Apple's, Google's or any manufacturer's stock high. Stock spikes high when the news organizations can talk about all the latest hardware and how sales doing well. Why would those companies CEOs want to hurt their golden package before exiting the industry? One way to start penetrating right-to-repair would be to force device unlocking after ownership, device payed off, and end-of-life classification by the manufacture. Next step would be for the manufacturers to require publishing open documents for 3rd party support without having to sign a NDA. Both of those require reverse engineering. With camera technology being so complex, this is the feature that limits alternative OS usage with continual security updates after the manufactures give up. Maybe rephrasing right-to-repair as \"consumer protection\" could help push it through better with less tech savvy consumers. reply ToucanLoucan 3 hours agorootparentConsumers aren't the issue. Consumer support for right to repair is broad. The issue is the government doesn't give a shit what consumers think the vast majority of the time, they're bought and paid for by corporate lobbyists. reply nickff 2 hours agorootparentConsumer support for right to repair is broad, so long as it comes at no cost to them. People don’t want to pay to fix things, and they don’t want to accept any reduction in performance either. reply t0bia_s 49 minutes agorootparentWhy would you pay same price for repairing a shoes when you can get a new one for similar price? reply hansvm 4 hours agoparentprevIt's a software problem too. To have the same capabilities my phone did when it was new a few years ago, I have to find 3rd party play store backups to get apps with the right SDK to install. The bootloader isn't unlockable. Samsung won't provide updates. Google is actively hostile to providing apps which work (both not hosting the working versions and abusing things like their power over the signing keys to quickly deprecate old Android SDKs). reply DowagerDave 20 minutes agorootparentyep - my old moto phone was fine, and I didn't add any new apps or desire new functionality, but performance got so bad over time to the point where it was unusable. There's really no attractive business model today in maintaining modest device usage over a long period of time. reply grishka 1 hour agorootparentprev> (both not hosting the working versions and abusing things like their power over the signing keys to quickly deprecate old Android SDKs) Android SDKs aren't getting deprecated. The SDK available on developer.android.com right now can still be used to build an app that runs on devices all the way down to Android 1.5. It's the developers who are dropping older Android versions by raising the minSDK in their apps. Google Play does allow the developer to keep older app versions available for older Android versions. Again, most developers don't do that. Google themselves support older Android versions for a very long time. Current versions of GSF and Google Play require Android 4.4, iirc. This came out more than 10 years ago. reply amelius 4 hours agoparentprevThis is also why general purpose computers should not be crippled by the manufacturer. Or at least there should be a way to uncripple them. reply agentultra 2 hours agorootparentSo many devices are general purpose computers that are treated like a specialized device. eg: modern games consoles. A Nintendo 3DS is an ARM11 board. You can run Linux on it. Most people don’t because it doesn’t look like a “computer.” And because they wouldn’t know how as it takes a very specific skill set to make it work. They do get reused a lot because gamers of that era tend to value them… but a device like that could have tons of useful applications to extend its life. A fold-up computer with built in wifi that runs on battery? Nice. With enough around you could run a low-power mesh network in an emergency to keep communication open between folks that are separated. But such repurposing is far outside of most people’s reach. Especially when we’re trained to think of these things as products. Phones are another one. An iPhone 5 could easily be repurposed into a firewall or other application to extend its usefulness and lifetime. It’s a general purpose computer crippled into being a product though. reply echelon_musk 4 hours agoparentprev> Re-using devices helps us also reduce the number of new devices needed... which is what probably scares the corporate oligarchy I agree with you. Reusing and repairing appliances flies in the face of current capitalism. We don't need new models of phones, laptops or cars every year. Sadly I'm not optimistic that we will be able to dial back greed any time soon. reply amelius 4 hours agorootparentWe need to reinvent capitalism. (Why does my phone need to be upgraded every year, while capitalism is kept at version 0.1beta?) reply Workaccount2 5 hours agoprevPeople should understand that proper clean electronic waste recycling does exist. This story isn't so much about \"we need to stop consuming new electronics\" as it is \"we need to ensure that electronic waste doesn't end up being dumped on random impoverished towns in Africa\". These guys are burning off the insulation from wires when there are simple cheap machines that automatically strip it all off. This is more a portrayal of extreme poverty than anything. reply HermanMartinus 4 hours agoparentI second this opinion. Here's an older article which is less dramatised and talks about attempted interventions such as trying to get the recyclers to use wire strippers instead of burning: https://www.worstpolluted.org/projects_reports/display/107 reply lnsru 4 hours agoparentprevAs an electrical engineer I am with you. There are machines to cut the cables and shred printed circuit boards to smallest pieces and recycle all the valuable materials. Even sort out plastic enclosure parts or glass by corresponding densities. But the world is run by greedy bastards who don’t care about anything else than their own pockets. That’s how plastic gets ditched in the ocean. That’s how electronics get shipped to this e-waste dumping ground. Or old ships end up in Bangladesh. I red probably too many science fiction books about future utopias, that the present makes me sad. Heck they can’t get the damn local commuter train line to run according the schedule in apparently wealthy part of Germany. Just shaking my head. reply fransje26 1 hour agorootparent> Heck they can’t get the damn local commuter train line to run according the schedule in apparently wealthy part of Germany. They manage in neighboring Switzerland though.. Less greed and more pride for a job well done maybe? reply pbronez 3 hours agorootparentprev> There are machines to cut the cables and shred printed circuit boards to smallest pieces and recycle all the valuable materials. Even sort out plastic enclosure parts or glass by corresponding densities. I'd love to learn more about this. What's the state of the art? How do the economics work out? For now, I take my end-of-life electronics to the local BestBuy. They have pretty good recycling standards, which include attempts to reuse & refurbish devices: https://www.bestbuy.com/site/recycling/recycle-guidelines/pc... reply Rinzler89 4 hours agoparentprev>People should understand that proper clean electronic waste recycling does exist. [..] This is more a portrayal of extreme poverty than anything. That like saying \"people should understand that eating cake is also an option, you don't have to eat dirt\". Because then answer me why most e-waste dumping gets shipped off to those impoverished countries instead of being processed locally using the \"cheap and clean\" ways you mention, directly in the rich western nations who are buying all those electronics in the first place. Throwing the blame back on the poor countries getting exploited by corporate interest of rich western countries doing greenwashing, feels like gaslighting. reply Workaccount2 4 hours agorootparentI cannot find any source that shows e-waste being primarily sent to third world countries. It looks like it mostly goes to India and China, if not processed locally. And at least in India it doesn't look like a burning hell hole of toxic waste.[1] [1]https://namoewaste.com/what-we-do/ reply devsda 3 hours agorootparent> Having long invaded Asia (Russia, India, China, etc.), e-waste from Europe and the United States is arriving in extensive quantities in the ports of West African countries such as Ghana, in violation of international treaties. This is from the first link[1] in the npr article. It doesn't say that it is the primary destination but does say that it is high. 1. https://www.fondationcarmignac.com/en/ANAS-AREMEYAW-ANAS-MUN... reply Workaccount2 2 hours agorootparent\"extensive quantities\" is a meaningless term. If we use the numbers from the article (250k tons) and from the site your provided (62 million tons), \"extensive quantities\" is 0.4% of e-waste. reply blitzar 2 hours agoparentprevThese guys are cheaper than the simple cheap machines that automatically strip insulation from wires. reply carlgreene 6 hours agoprevIt’s so easy to just mindlessly want and consume until you see pictures like these. They show that although my streets are pristine, with everyone having the latest “stuff”, it’s really only possible because we sweep all of the “bad stuff” under the proverbial rug reply bubaumba 5 hours agoparentThat's how it always was. People were eating only the best parts of the animal and dumping the rest. More over, the best is converted mostly to sh*t and dumped too. Fish do the same. reply paulcole 5 hours agoparentprev> It’s so easy to just mindlessly want and consume until you see pictures like these It’s still incredibly easy. If you could magically make every person in the United States look at these photos for 30 minutes, nothing would change about how we live and consume. All that matters is that my streets are pristine. reply maeil 5 hours agorootparentThis is of course untrue, given that we know that there are people who have changed their behaviour after learning about realities through articles such as this one. And a huge percentage of people still haven't learned about them. In one's highly-educated HN-reading savvy bubble, it might be easy to assume that surely by now everyone knows the realities, has seen all of them, and if they behave a certain way it's simply due to the degree to which they care. I've been prone to such biases myself, but the truth is very different. Billions of people, including hundreds of millions in wealthy regions, still simply do not know about this. They genuinely do not know that e.g. plastic recycling is a fantasy. They have not seen these images. Of course, many have and just don't care as long as their streets are pristine - the people you're talking about very much exist. But there's even more people who are simply unaware. It's very easy to take a nihilist view that nothing matters, as it completely absolves oneself of any potential responsibility whatsoever. But it doesn't reflect reality. reply paulcole 4 hours agorootparent> It's very easy to take a nihilist view that nothing matters, as it completely absolves oneself of any potential responsibility whatsoever No! I actually do take personal responsibility by: • Living in a small apartment in a dense city • Never having driven a car - I never even learned to drive • Never flying in an airplane – last flight was 10+ years ago and have no plans to fly again • Eating a plant-based diet • Not ever having kids But I will concede that none of those things matter. > This is of course untrue, given that we know that there are people who have changed their behaviour after learning about realities through articles such as this one. That is neat! People in general will not change their behavior. reply piva00 5 hours agoparentprevI can consume less, give new life to old electronics, etc. and seeing these pictures validates the feeling I have for it. At the same time it just makes me feel powerless, all the effort I go through to not make this problem bigger is all too small to have any effect, the powerlessness against the system is real. I can change my habits, advocate for others why I believe that's good but it all fall into deaf ears while the incentives are there to just consume, throw it away, rinse and repeat. It just makes me exhausted while not feeling I've helped to make the world any better, and in the end I still get flak from the mindless consumers if I bring this up as it's a damn boring subject to participate when one doesn't care about it. reply karles 5 hours agorootparentMorally, caring is the only option. To many people don't value values or morals, and only prioritize their own experiences. I find it hard to maintain relationships with people who only talk about their career, business and consumption, as it is hard to have any kind of discussion about \"we COULD, but SHOULD we\" in regards to purchasing the latest car, iphone, a new house etc. reply artursapek 5 hours agoparentprevThis is why the “MKBHD” etc type youtubers who worship “new tech” and do unboxing videos have always bothered me. reply y-curious 5 hours agorootparentWait til you find out about disposable vaporizers people buy in the states, use up in a week, and throw out. Those don't even get a chance to be recycled, and they're a relatively complex electronic. reply runjake 5 hours agorootparentI only know about these from seeing them on the ground, in the streets, and in bushes everywhere. reply artursapek 5 hours agorootparentprevI live in the US. Those are insane. I can’t believe how quickly they became normal. Not only will they definitely cause cancer, but they are a cancer on the planet too. And they’re marketed like toys. reply ulrikrasmussen 5 hours agorootparentFrom a health and harm reduction perspective they are probably saving lives by replacing cigarette smoke which is much more harmful. But I agree that from an environmental perspective they are much worse. Perfectly good Li-ion are thrown in the trash which is insane. I don't understand why people buy these. I don't vape, but it is my understanding that you can get reusable vapes with cartridges that are very easy to replace, and which are probably more enjoyable to use. reply kergonath 3 hours agorootparent> From a health and harm reduction perspective they are probably saving lives by replacing cigarette smoke which is much more harmful. There is some evidence that they are used by people who never smoked, though. The smoking reduction seems like a convenient excuse for these companies. In actual fact smokers or former-smokers is a limited and dwindling market. They want to expand aggressively, notably in younger populations. reply artursapek 5 hours agorootparentprevWe have no idea how their health effects compare to cigarettes, they're too new. People should just smoke pure tobacco (pipes, cigars) but those are too inconvenient. reply kortex 4 hours agorootparentThere's no world in which atomized nicotine, glycol/glycerin, and some flavors, is comparable, let alone worse, than inhaling smoldering tobacco leaves. Even the purest, organic tobacco made from the nicest leaves lovingly collected by happy family farmers, is still gonna give you cancer if you burn it and inhale the smoke. That's just chemistry. The only exception is if the vape juice contains something it \"shouldn't\", like the vitamin E acetate debacle, but if you put the same wrong things in tobacco, you get the same issue. This problem is avoided entirely with a verified source of ingredients. Partial combustion products will always contain, at minimum, polycyclic aromatic hydrocarbons, a verified carcinogen. The hotter the conditions, the more complex the precursors, and the more incomplete the reaction is, the more nasty junk you will create. Vaping might or might not be bad, but the chemistries of smoking includes the full set of chemistries of vaping, and then way way more, due to the incomplete combustion of the much larger molecules in plant matter. Bottom line: probably don't put the volatile reaction products of substances heated above 100°C into your lungs: tobacco smoke, vape, campfire smoke, car exhaust, brake dust, etc. But some reactants are far worse than others. reply userbinator 5 hours agorootparentprevA few videos have already been made about harvesting those for free rechargeable batteries. reply delfinom 5 hours agoparentprevTo be fair, part of the feel good for consume is the recycling centers in the west that are largely complete scams. Because they just aggregate the waste to ship to the third world for \"\"\"\"recycling\"\"\"\" reply tonyedgecombe 5 hours agorootparentThe West or America? I don't think we are in any way perfect in Europe but it does feel like we have a better handle on this stuff than the US. reply andsoitis 4 hours agorootparentSee https://en.wikipedia.org/wiki/Electronic_waste_in_Africa reply naming_the_user 5 hours agoprevCounterpoint to most of the posts here - I don’t see this and think “wow we should stop using things”, I see this and think “wow, we need to sort out governance / fix poverty”. A well run landfill looks nothing like this and these are in no way a foregone conclusion of someone throwing away an old iPhone 3 or whatever. There is no more correlation here than with, say, Newton has the apple fall and then we cut to scenes of firebombing. reply yunohn 4 hours agoparentThis not “well run” landfill literally exists because the companies/countries dumping their e-waste here do not want to pay for the “well run” ones. reply naming_the_user 2 hours agorootparentSure, so let's make them pay for it, job done. If I go to the loo and my water company decides it's cheaper to dump human faeces in the middle of the M1 motorway than to dispose of it properly, the solution isn't for me to stop going to the loo, it's to force my water company to stop doing that. reply superultra 5 hours agoprevI’m thankful I saw these pictures, if deeply unsettled. We can’t (just) take an individualized approach to a solution, which is an artifact of the 80s and 90s when corporations and governments shifted responsibility to the individual to recycle a water bottle, for example. It seems like the best solution is to impose a waste reduction fee that is built into price that pays for ewaste reduction. This could empower Ghanaians to build out this as a safer industry. How much would that fee be? And who would spend the political capital to enact such a tariff? That’s the part that feels impossible. reply wruza 5 hours agoprevScavenging e-waste for components feels so cyberpunk. Sometime someone designed an IC, lithographed it on a high tech factory, soldered it onto a PCB and now it lies under your feet like billions of other rusty sharp parts, as if they were potato skins or plastic bags. Just a few decades ago nations would start WW3 over this alien technology dump. Now they try find cheaper ways to sneak more waste into it. reply ta988 3 hours agoparentWe did war over energy, now we burn energy just to find out who can burn the most and give them a token (bitcoin) or get neighbors to fight each other on which can get the biggest SUV or sports car that guzzles like 2 or more optimized cars. reply 4ndrewl 5 hours agoprevYou can't throw things away. You can only move them somewhere else. reply riskable 5 hours agoparentImbesi's Law of the Conservation of Filth: In order for something to become clean, something else must become dirty. reply tomrod 5 hours agorootparentReminds me of entropy. reply schrectacular 5 hours agoparentprevCorrect. Some friends and I started saying \"throw it aways\" instead. I think it much better describes the actual situation. It didn't really catch on, though I wish it would. reply rrrix1 5 hours agoprevhttps://maps.app.goo.gl/LS4xWeuewBqwUNuN9?g_st=com.google.ma... That waterway is flowing directly into the ocean, and upstream from a fishing village. reply t0bia_s 51 minutes agoprevYet, we made and buy crappy devices like Niimbot printers, that are not working without proprietary app that collect your data and asks for paying for using different, then default font. What a wonderful e-waste. reply BrandoElFollito 1 hour agoprevI like to buy (some) used hardware when I have need to. Either the ones that people sent back because they thought that it would be simple and was not (my Cisco home switch), or older tech that is completely fine for my needs. My personal experience is that when electronics work for two weeks, they will work \"forever\" - I like someone else doing the test :) Of course it depends on the hardware. It will be different for a switch and a UPS, or an SSD, ... reply M95D 1 hour agoprevThe article mentions repairing some of the electronics. There's even a photo with something that looks like a repair shop. I would buy vintage electronics and PC parts, but these guys are not selling on ebay. So, where do they sell them after they fix them? reply rraaffff 5 hours agoprevThere's a BBC documentary by Reggie Yates \"A Week in a Toxic Waste Dump\" from 2017, it is about the same Agbogbloshie Scrapyard in Accra: https://www.bbc.co.uk/iplayer/episode/p05dmmns/the-insider-r... The BBC player only works in the UK, but you can easily find the episode on Youtube. reply imiric 5 hours agoprevThis is awful on so many levels. These images should be postered around the headquarters of all major electronics manufacturers. They should be used in courts as prosecution evidence to force these companies to comply with repairability regulations, and force governments to enact stricter regulations and higher fines. They can start by making planned obsolescence illegal. reply itishappy 5 hours agoparentWhat makes you assume planned obsolescence is at play here, and not just regular old obsolescence? I suspect the two-decade-old large-format CRTs on display in that shop aren't there due to a lack of replacement parts. reply imiric 5 hours agorootparentI'm sure that it's _part_ of the problem, no? What percentage of those tons of electronic waste do you think includes smartphones from the last 15 years? Do you think all of it is reused and recycled before it reaches these dump sites? EDIT: Somewhere around 5 billion phones in 2022 alone[1]. [1]: https://www.bbc.com/news/science-environment-63245150 reply itishappy 4 hours agorootparentIt sure sounds right to me too, but I'm looking at the photos and I don't see any phones. Actually, I don't see much I'd consider using, even if it were still working. That's a problem too, but it's different from what you're describing. reply lesuorac 5 hours agoparentprevidk, that looked like an awful lot of ewaste and 0 car tires. Probably just need to borrow the deposit system that car tires use where you pay a large fee (not the 5 cents that plastic bottles use) when buying tires unless you return an equivalent amount. reply mrguyorama 3 hours agorootparentHave you not seen the pictures of giant tire dumps? They exist. They also occasionally catch fire and blot out the sun. reply XorNot 5 hours agoparentprevYou have looked at a problem and proposed a bunch of completely irrelevant solutions. What happens when something is repaired? Components are replaced and discarded. What happens eventually when the device wears out? It is is discarded. If we did everything you listed, it wouldn't even appreciably change the volume of material discarded, since eventually all manufactured items wear out. And of course, what is missing in this little diatribe? Any solution to the question of what to do with discarded electronics. You aren't solving the core problem. reply dahart 4 hours agorootparentSo what’s the core problem, and what’s your proposal to solve it? Your logic seems questionable. The article mentions discarded components being recovered for their materials, e.g., copper & plastic. And when something is repaired, by definition some of the components are reused and not discarded. If it takes twice as long to wear out completely, then the replacement purchase rate drops to 50%. Why do you claim that’s not even partially addressing the core problem? reply imiric 5 hours agorootparentprev/sigh Typical pedantic contrarian HN response... Look, I'm not saying that this would solve all of these problems. I don't even claim to have the expertise to propose potential solutions. But speaking as a consumer, focusing on the source of what causes them might be a good place to start. But I'm sure that your expertise and infinite wisdom must be able to produce better ideas to fix this, which I'm eager to hear. reply kergonath 3 hours agorootparent> /sigh Typical pedantic contrarian HN response It is not pedantic or contrarian, though. The points they are making are real issues. The right to repair is important, but from an environmental point of view it is not that relevant. Besides, what the current demographic and economic trajectory of the world, huge populations are accessing the middle classes, with the associated increase in consumption. Even with perfect repairability (which does not solve the issue of discarded parts or plain broken devices, the amount of which is proportional to the number of devices in use), things physically cannot get better. The best lever we have right now is to reduce consumption. It’s about as credible as perfect repairability, but is much more effective. “Do we really need these 6 phones, 3 computers, 2 cars, and microprocessors in every light bulb” is a more pressing question than “can I fix my phone with a torx screwdriver”? Repairability is a good thing, but it is only part of the battle, and not the most critical. reply imiric 2 hours agorootparent> The right to repair is important, but from an environmental point of view it is not that relevant. The quote was \"completely irrelevant\". How is that not contrarianism? > The best lever we have right now is to reduce consumption. Ah, consumerism. And what magical lever do we have to reduce that? reply kergonath 11 minutes agorootparent> The quote was \"completely irrelevant\". How is that not contrarianism? That was a slight hyperbole. It is not “completely irrelevant”, merely irrelevant. Contrarianism implies bad faith and knee-jerk reactions. They provided arguments, which you are free to debate or question. > Ah, consumerism. And what magical lever do we have to reduce that? Well, realistically? None. Not before it gets significantly worse anyway. It’s still more realistic than getting out of this hole by repairing stuff. The orders of magnitude are just not there. Again, repairing devices is a good thing. But it’s not a panacea and won’t solve that specific problem. reply DrNosferatu 4 hours agoprevThe EU (and the US, and others for that matter) should increase the compulsory warranty from 2 years to 5 years. Not only it would reduce e-waste, but it would also disincentivize the lowest-margin, sweat shop production. reply userbinator 5 hours agoprevWorking conditions in mines have never been great. These are basically the mines of the future. reply jl6 4 hours agoprev> \"There’s a whole generation of young people that are building their society from e-waste work.\" This is hard, dangerous, indecent work by any first world standard, but it's still work, it's still opportunity, and it's still an industry for people who otherwise might not have one. I don't wish to see this kind of pollution and suffering exist, but I also don't wish to take away something that despite its awfulness is still someone's livelihood. Ladders need bottom rungs. When they closed sweatshops in Bangladesh, the children had to resort to prostitution. reply hcarvalhoalves 4 hours agoparentThis rhetoric is outdated by more than 200 years, when kids worked at coal mines in 18th century Britain. reply jl6 4 hours agorootparentAnd yet coal wealth was tremendously beneficial for those communities. Kids-in-mines was ended by better labor regulation, not by cutting off the source of the wealth. Ghana has an amazing opportunity here. The world is literally shipping gold to their doorstep. There has got to be a solution that improves standards without cutting them out of the loop. reply lr1970 3 hours agoprevUser swappable batteries will extend the life of mobile devices big time. I am old enough to remember that you could easily pop any phone's back cover and swap the battery. reply ErikAugust 3 hours agoprevI'm a software idiot, but why couldn't you do the Goodwill of Cloud Infrastructure? Build affordable cloud services out of \"junk\" electronics? reply crote 3 hours agoparentTotal cost of ownership. First you need to spend an absolute fortune on sysadmins to hack together functioning machines from heaps of mostly-broken parts. Then you need to deal with an admin nightmare as every machine will be different, so you need to manage them as individual machines rather than hundreds of identical clones who all behave exactly the same. Then you need to deal with tons of random hardware failures, none of which can be easily solved by hotswapping a standard fan or harddrive you've got lying on the shelf already. And to finish it off, you're also using 5x - 10x more power for the same compute. Whatever money you're saving on hardware purchase, you're spending many times more on all the other stuff. Free junk electronics are just too expensive. reply blitzar 2 hours agorootparentTotal pollution of ownership would likely be lower with new hardware vs old when you take into account the higher power usage vs lower performance. reply penguin_booze 3 hours agoprevDumping yards reminds me of a scene from the Office, where Dwight says (IIRC), \"humans are the only animals capable of this\". reply o-o- 5 hours agoprevI know a large retailer that sells electric screw drivers for €19 a piece. I also know from the chinese manufacturer's backwaters that it's deliberately designed to last for 12 minutes. That's roughly two years in the hand of an average non-professional, who will probably go back and buy another since it was so cheap. These tools don't have a second-hand market. The expensive built-to-last ones do. reply nolist_policy 5 hours agoparentI don't know, I recently saw electric drills for €29 at Aldi and to my surprise they used brushless motors! They will probably last an eternity for hobbyist (minus the batteries). reply BirAdam 5 hours agoprevYeah… In 2019, the world wasted more than 59.1 million tons of electronics. That's the equivalent of around 350 large cruise ships that are completely filled with e-waste. Most of it used to be due to slow and/or bloated software, but more of it is now batteries. There’s also a bit of just poor manufacturing/design where a device was never good, and therefore as soon as its owner could get better he/she did get better. Edit: and let’s not forget the deprecation of older standards like 2G and 3G cell networks, or the rise of USB-C. reply imiric 5 hours agoparentRegarding batteries specifically, it should be illegal to produce any device without user-replaceable batteries. The EU is at the forefront of these initiatives[1], as usual, so hopefully this trickles out to other governments. Batteries in EVs are also a growing problem, for both production and disposal. Hopefully we'll have similar regulations there as well. [1]: https://www.theverge.com/2023/6/24/23771064/european-union-b... reply FredPret 5 hours agoparentprev59 million tons is a cube 390m on a side, or a square pile 10m high and 2500m on a side. It’s a lot, but let’s not hyperventilate. reply phkahler 4 hours agorootparent>> 59 million tons is a cube 390m on a side, or a square pile 10m high and 2500m on a side. It's also 118 pounds for every person on earth. That seems really high for e-waste. reply Supermancho 4 hours agorootparentNot to nitpick since it's still in the same magnitude, but I think it's more like 25.5 lbs 118,000,000,000 / 4,640,000,000 = 25.4310344828 reply steviedotboston 5 hours agoprevI've wondered if it would be better for electronics to be just thrown out in regular trash. I know they have some hazardous materials in them, but when spread out in low levels across landfills maybe its better than concentrating them in places like this... reply adolph 3 hours agoprevI think one of the exciting byproducts of future long term space travel is how it will change people's expectations of the material world. Currently humans generate a significant amount of material which does not have a downstream constituency, and thus is stored, sometimes in less aesthetically acceptable ways like the pictured scrapyard. Since the topic of TFA is e-waste, many comments here promote \"right to repair\" legislation as a panacea. I don't think that \"right to repair\" addresses the root issue in a broad enough way to make a dent. It only addresses a subset of material, operates at hobby scale, and may mandate certain things, like socketed components, that make full-scale automated recycling more difficult. reply Mistletoe 6 hours agoprevThis is really heartbreaking to see and dystopian. reply worldsayshi 6 hours agoparentIt is indeed heartbreaking. But I don't see moral outrage solving the issue any time soon. People will rather forget about this reality than stop the consumption. If anyone wants to actually work towards solving the issue they should probably go there and try to invest in ways to clean up the practice. Better tools, better profitability. Education. Etc. reply greedylizard 5 hours agorootparent> go there and try to invest in ways to clean up the practice. You think the problem that needs to be solved is “there”? This sentence makes me question whose consumption you’re referring to in the previous sentence. reply worldsayshi 3 hours agorootparentSolve the problem at whatever junction that is exposed for a solution is my point. Being outraged about stuff seems to not magically solve problems. Rather it often has a similar effect as ruminating about problems when being depressed. It often enforces the idea that the problem is somehow unsolvable. Not saying outrage doesn't have a place. Just that other means might be more efficient. reply XorNot 5 hours agorootparentprevIt is literally only there. This problem exists because the governments of these places allow it to happen. The reason it doesn't happen here is because we have strong environmental regulations here. reply mrguyorama 2 hours agorootparent\"Strong environmental regulations\" would make it impossible to just ship to someone without. reply ta988 3 hours agorootparentprevFor now reply beepbooptheory 5 hours agorootparentprevWhat is this thing that is both heartbreaking but without any reason for outrage? Like getting rejected from the school dance? reply BirAdam 5 hours agorootparentprevA bit of a controversial take, but I think the reason that this won’t get solved is the AGW movement. Rather than addressing things like pollution, waste, strip mining, environmental toxicity, and so on the green movement was hijacked to care about a single aspect of environmentalism because rich people could get even richer trading carbon futures. reply worldsayshi 3 hours agorootparent> movement [X] was hijacked to care about a single aspect of [Y] I think this is a symptom of a larger issue that has nothing to do with environmentalism. Global cultural consciousness is getting more centralized. There isn't as much room on any particular agenda for multiple facets of any one issue when everything is being bottle necked through a much more centralized cultural sphere. reply gosub100 5 hours agoparentprevTrillion-dollar companies that produce this crap are sitting back in their skyscrapers saying \"not our problem, something-something the market \" reply 29athrowaway 6 hours agoprevI would rather call this: the receiving end of planned obsolescence. The other end is... you. reply exitb 5 hours agoparentA lot of the items on the pictures look like 15+ years old equipment. People don't use CRT TVs or cassette decks, but not because they broke down on schedule. Not saying that planned obsolescence is not an issue, but even if a piece of equipment serves you for decades, you still need a good plan on how it could be disposed of properly. reply tivert 4 hours agoparentprev> The other end is... you. Not really. The other end is the manufacturers. It's a pretty common pattern in capitalist democracies that powerful business interests attempt (often successfully) shunt responsibility away from themselves onto consumers, who just so happen to be in one of the weakest position to actually affect a change. It works because (in America at least) individualism is such a powerful force that all kinds of social problems can get re-contextualized into questions of individual morality, and people won't bat an eye. Also, from a PR standpoint, if someone does not want to solve a problem, it looks a lot better to acknowledge the problem but insist on an unworkable solution (e.g. all consumers must coordinate to change their preferences, so the manufactures never have to bother themselves with anything beyond market forces) than to straight-up insist the problem remain. reply farceSpherule 5 hours agoprevWho cares... Been happening for decades... reply blitzar 2 hours agoparentTrickle down economics at its finest. reply roenxi 5 hours agoprev [–] I'd like to see an arial photo of this site, because these images paint an awful picture without actually showing us how big this dump is. 15,000 tons/annum in one area shouldn't be all that much in the grand scheme of things but the photos manage to make it look like this is some sort of boundless hellscape. I'd hazard the actual problem in this picture is Ghana's GDP/capita being in 4 digit territory and not the badly disposed of waste dump. reply _visgean 4 hours agoparentThere is some drone footage here https://www.youtube.com/watch?v=BdPGO6sfc3c, also google maps has a good view https://maps.app.goo.gl/KwdiwCzF4sThGLVf9 reply deutschepost 3 hours agorootparentThe Maps view is insane. It literally has a garbage fire going on during photography. reply throwgfgfd25 5 hours agoparentprev> I'd hazard the actual problem in this picture is Ghana's GDP/capita being in 4 digit territory and not the badly disposed of waste dump. But if Ghana became a wealthy country and chose not to accept this waste, it will end up in the next one. The waste exists regardless, and the economic incentive for the original market \"export\" it, that is, hide the problem, and the receiving country to reluctantly accept it for some other consideration, whether it be money or state aid or tariff-free export of something else, will always exist while the waste does. Re: \"badly disposed of waste dump\", the difference between this and landfill anywhere in the west is largely just the soil on top. Staggering amounts of recyclable and dangerous stuff still gets thrown away in inappropriate ways right near where you live, I imagine. And if the global North exports waste to the global South, sooner or later the scale almost inevitably overwhelms the receiver. reply roenxi 5 hours agorootparentThere are a finite number of poor countries. At the rate wealth is being generated it is conceivable that they all get wealthy enough that the waste gets handled well. And this stuff all started out in heavy metals deposits, it is already present underground somewhere. The only real question is how serious the effects on humans are with any method of disposal. It isn't at all clear there is a problem as long as it is buried fairly deep and not leeching into the water table. reply phkahler 4 hours agorootparent>> There are a finite number of poor countries. At the rate wealth is being generated it is conceivable that they all get wealthy enough that the waste gets handled well. This waste was dumped. The fact that poor people moved to the dump to make a living scavenging is a secondary phenomenon. Without them it still would have been dumped. reply throwgfgfd25 4 hours agorootparentprev> There are a finite number of poor countries. This is a bit of an imaginary solution to the problem, is it not? And there will always be poor_er_ countries, which is the thrust of my point. The economic incentive does not go away. Not least because it is clearly already cheaper to float it away on a huge boat than bury it where it is used. One problem is land cost: it's extremely difficult to safely build new houses on top of landfill. But that doesn't explain everything, does it? After all the USA has plenty of room to bury all its consumer waste. Why is it exporting it? > And this stuff all started out in heavy metals deposits, it is already present underground somewhere. It does not start out all in one place, though. It starts out in small, dispersed concentrations of heavy metals, and ends up all in a few giant landfills in poorer countries. It's not clear what the risk is, but the lack of clarity doesn't mean there's no risk. reply carapace 4 hours agoparentprev [–] It doesn't matter how big the dump is, it shouldn't exist from first principles. Think about how incredibly worked out these devices are, how many brilliant people worked to design them, to figure out how to source the materials, how to combine them, etc... Miracles of engineering they are. Everything planned out carefully. And then you throw them away. That's the idea. It's not an accident. The lifecycle of these machines was designed. It's fucking insane. The best you can say about it is that it's not quite as insane as animal sacrifice. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Agbogbloshie Scrapyard in Accra, Ghana, was a major site for processing e-waste, handling 15,000 tons annually, and attracting workers like Emmanuel Akatire seeking economic opportunities despite health risks.",
      "A photojournalism project titled \"E-Waste in Ghana: Tracing Transboundary Flows\" sheds light on the dual nature of e-waste as both a danger and an economic opportunity, highlighting the informal recycling economies and the exposure to harmful chemicals faced by workers.",
      "The project aims to raise awareness about the global e-waste issue, emphasizing the impact on communities and the challenges posed by lax enforcement of international laws against non-functional e-waste trafficking."
    ],
    "commentSummary": [
      "Photos of an e-waste dumping ground underscore the significance of \"right to repair\" laws, which aim to extend the lifespan of devices and mitigate environmental harm.",
      "There is a debate over tech companies' resistance to repairability, as it may be a strategy to sustain sales, versus the need for improved recycling practices and governance to prevent e-waste dumping in impoverished regions.",
      "The conversation also involves consumer habits, corporate responsibility, and the potential for policy changes to tackle these environmental and ethical challenges."
    ],
    "points": 187,
    "commentCount": 112,
    "retryCount": 0,
    "time": 1728304207
  },
  {
    "id": 41766882,
    "title": "Building a single-page app with Htmx",
    "originLink": "https://jakelazaroff.com/words/building-a-single-page-app-with-htmx/",
    "originBody": "Building a Single-Page App with htmx October 7, 2024 #htmx #javascript Table of Contents Behind The Scenes Takeaways People talk about htmx as though it’s saving the web from single-page apps. React has mired developers in complexity (so the story goes) and htmx is offering a desperately-needed lifeline. htmx creator Carson Gross wryly explains the dynamic like this: no, this is a Hegelian dialectic: thesis: traditional MPAs antithesis: SPAs synthesis (higher form): hypermedia-driven applications w/ islands of intereactivity Well, I guess I missed the memo, because I used htmx to build a single-page app. It’s a simple proof of concept todo list. Once the page is loaded, there is no additional communication with a server. Everything happens locally on the client. How does that work, given that htmx is focused on managing hypermedia exchanges over the network? With one simple trick:1 the “server-side” code runs in a service worker. React developers hate him! ↩ Briefly, a service worker acts as a proxy between a webpage and the wider Internet. It intercepts network requests and allows you to manipulate them. You can alter requests, cache responses to be served offline or even create new responses out of whole cloth without ever sending the request beyond the browser. That last capability is what powers this single-page app. When htmx makes a network request, the service worker intercepts it. The service worker then runs the business logic and generates new HTML, which htmx then swaps into the DOM. There are a couple of advantages over a traditional single-page app built with something like React, too. Service workers must use IndexedDB for storage, which is stateful between page loads. If you close the page and then come back, the app retains your data — this happens “for free”, a pit of success consequence of choosing this architecture. The app also works offline, which doesn’t come for free but is pretty easy to add once the service worker is set up already. Of course, service workers have a bunch of pitfalls as well. One is the absolutely abysmal support in developer tools, which seem to intermittently swallow console.log and unreliably report when a service worker is installed. Another is the lack of support for ES modules in Firefox, which forced me to put all my code (including a vendored version of IDB Keyval, which I included because IndexedDB is similarly annoying) in a single file. This is not an exhaustive list! I would describe the general experience of working with service workers as “not fun”. But! In spite of all that, the htmx single-page app works. Let’s dive in! Behind The Scenes Let’s start with the HTML:htmx spa async function load() { try { const registration = await navigator.serviceWorker.register(\"./sw.js\"); if (registration.active) return; const worker = registration.installing || registration.waiting; if (!worker) throw new Error(\"No worker found\"); worker.addEventListener(\"statechange\", () => { if (registration.active) location.reload(); }); } catch (err) { console.error(`Registration failed with ${err}`); } } if (\"serviceWorker\" in navigator) load(); This should look familiar if you’ve ever built a single-page app: the empty husk of an HTML document, waiting to be filled in by JavaScript. That long inline tag just sets up the service worker and is mostly stolen from MDN. The interesting bit here is thetag, which uses htmx to set up the meat of the app: hx-boost=\"true\" tells htmx to use Ajax to swap in the responses of link clicks and form submissions without a full page navigation hx-push-url=\"false\" prevents htmx from updating the URL in response to said link clicks and form submissions hx-get=\"./ui\" tells htmx to load the page at /ui and swap it in hx-target=\"body\" tells htmx to swap the results into theelement hx-trigger=\"load\" tells htmx that it should do all this when the page loads So basically: /ui returns the actual markup for the app, at which point htmx takes over any links and forms to make it interactive. What’s at /ui? Enter the service worker! It uses a small home-brewed Express-like “library” to handle boilerplate around routing requests and returning responses. How that library actually works is beyond the scope of this post, but it’s used like this: spa.get(\"/ui\", async (_request, { query }) => { const { filter = \"all\" } = query; await setFilter(filter); const headers = {}; if (filter === \"all\") headers[\"hx-replace-url\"] = \"./\"; else headers[\"hx-replace-url\"] = \"./?filter=\" + filter; const html = App({ filter, todos: await listTodos() }); return new Response(html, { headers }); }); When a GET request is made to /ui, this code… grabs the query string for the filter saves the filter in IndexedDB tells htmx to update the URL accordingly renders the App “component” to HTML with the active filter and list of todos returns the rendered HTML to the browser setFilter and listTodos are pretty simple functions that wrap IDB Keyval: async function setFilter(filter) { await set(\"filter\", filter); } async function getFilter() { return get(\"filter\"); } async function listTodos() { const todos = (await get(\"todos\")) || []; const filter = await getFilter(); switch (filter) { case \"done\": return todos.filter(todo => todo.done); case \"left\": return todos.filter(todo => !todo.done); default: return todos; } } The App component looks like this: function App({ filter = \"all\", todos = [] } = {}) { return html` Todos AllActiveCompleted${todos.map(todo => Todo(todo))}`.trim(); } (As before, we’ll skip some of the utility functions like html, which just provides some small conveniences when interpolating values.) App can be broken down into roughly three sections: The filters form. This renders a radio button for each filter. When a radio button changes, it submits the form to /ui, which re-renders the app using the steps described above. Thehx-boost attribute from before intercepts the form submission and swaps the response back into thewithout refreshing the page. The todos list. This loops over all the todos matching the current filter, rendering each using the Todo component. The add todo form. This is a form with an input that submits the value to /todos/add.2 hx-target=\".todos\" tells htmx to replace an element on the page with class todos; hx-select=\".todos\" tells htmx that rather than using the entire response, it should just use an element with class todos. You might notice that the form method is GET rather than POST. That’s because service workers in Firefox don’t seem to support request bodies, which means we need to include any relevant data in the URL. ↩ Let’s take a look at that /todos/add route: async function addTodo(text) { const id = crypto.randomUUID(); await update(\"todos\", (todos = []) => [...todos, { id, text, done: false }]); } spa.get(\"/todos/add\", async (_request, { query }) => { if (query.text) await addTodo(query.text); const html = App({ filter: await getFilter(), todos: await listTodos() }); return new Response(html, {}); }); Pretty simple! It just saves the todo and returns a response with the re-rendered UI, which htmx thens swap into the DOM. Now, let’s look at that Todo component from before: function Icon({ name }) { return html``; } function Todo({ id, text, done, editable }) { return html` ${editable ? html`` : html` ${text} `} ${Icon({ name: \"ex\" })}`; } There are three main parts here: the checkbox, the delete button and the todo text. First, the checkbox. It triggers a GET request to /todos/${id}/update every time it’s checked or unchecked, with a query string done matching its current state; htmx swaps the full response into the . Here’s the code for that route: async function updateTodo(id, { text, done }) { await update(\"todos\", (todos = []) => todos.map(todo => { if (todo.id !== id) return todo; return { ...todo, text: text || todo.text, done: done ?? todo.done }; }) ); } spa.get(\"/todos/:id/update\", async (_request, { params, query }) => { const updates = {}; if (query.text) updates.text = query.text; if (query.done) updates.done = query.done === \"true\"; await updateTodo(params.id, updates); const html = App({ filter: await getFilter(), todos: await listTodos() }); return new Response(html); }); (Notice that the route also supports changing the todo text. We’ll get to that in a minute.) The delete button is even simpler: it makes a DELETE request to /todos/${id}. As with the checkbox, htmx swaps the full response into the . Here’s that route: async function deleteTodo(id) { await update(\"todos\", (todos = []) => todos.filter(todo => todo.id !== id)); } spa.delete(\"/todos/:id\", async (_request, { params }) => { await deleteTodo(params.id); const html = App({ filter: await getFilter(), todos: await listTodos() }); return new Response(html); }); The final part is the todo text, which is made more complicated by the support for editing the text. There are two possible states: “normal”, which just displays a simplewith the todo text (I’m sorry that this isn’t accessible!) and “editing”, which displays anthat allows the user to edit it. The Todo component uses the editing “prop” to determine which state to render. Unlike in a client-side framework like React, though, we can’t just toggle state somewhere and have it make the necessary DOM changes. htmx makes a network request for the new UI, and we need to return a hypermedia response that it can then swap into the DOM. Here’s the route: async function getTodo(id) { const todos = await listTodos(); return todos.find(todo => todo.id === id); } spa.get(\"/ui/todos/:id\", async (_request, { params, query }) => { const todo = await getTodo(params.id); if (!todo) return new Response(\"\", { status: 404 }); const editable = query.editable === \"true\"; const html = Todo({ ...todo, editable }); return new Response(html); }); At a high level, the coordination between webpage and service worker looks something like this: htmx listens for double-click events on todo text s htmx makes a request to /ui/todos/${id}?editable=true The service worker returns the HTML for the Todo component that includes therather than thehtmx swaps the current todo list item with the HTML from the response When the user changes the input, a similar process happens, calling the /todos/${id}/update endpoint instead and swapping the whole . If you’ve used htmx, this should be a pretty familiar pattern. That’s it! We now have a single-page app built with htmx (and service workers) that doesn’t rely on a remote web server. The code I omitted for brevity is available on GitHub. Takeaways So, this technically works. Is it a good idea? Is it the apotheosis of hypermedia-based applications? Should we abandon React and build apps like this? htmx works by adding indirection to the UI, loading new HTML from across a network boundary. That can make sense in a client-server app, because it reduces indirection with regard to the database by colocating it with rendering. On the other hand, the client-server story in a framework like React can be painful, requiring careful coordination between clients and servers via an awkward data exchange channel. When all interactions are local, though, the rendering and data are already colocated (in memory) and updating them in tandem with a framework like React is easy and synchronous. In this case, the indirection that htmx requires starts to feel more burdensome than liberatory.3 For fully local apps, I don’t think juice is worth the squeeze. htmx isn’t actually a required component of this architecture. You could, in theory, build a fully client-side single-page app with no JavaScript at all (outside of the service worker) by simply wrapping every button in atag and replacing the full page on every action. Since the responses all come from the service worker, it would still be lightning fast; you could probably even add in some slick animations using cross-document view transitions. ↩ Of course, most apps aren’t fully local — usually, there’s a mix of local interactions and network requests. My sense is that even in that case, islands of interactivity is a better pattern than splitting your “server-side” code between the service worker and the actual server. In any event, this was mostly an exercise to see what it might look like to build a fully local single-page app using hypermedia, rather than imperative or functional programming. Note that hypermedia is a technique rather than a specific tool. I chose htmx because it’s the hypermedia library framework du jour, and I wanted to stretch it as far as I could. There are other tools like Mavo that explicitly focus on this use case, and indeed you can see that the Mavo implementation of TodoMVC is far simpler than what I’ve built here. Better still would be some sort of HyperCard-esque app in which you could build the whole thing visually. All in all, my little single-page htmx todo app was fun to build. If nothing else, take this as a reminder that you can and should occasionally try using your tools in weird and unexpected ways! Like what you read? Subscribe to my RSS feed , or follow me on Mastodon or Twitter .",
    "commentLink": "https://news.ycombinator.com/item?id=41766882",
    "commentBody": "Building a single-page app with Htmx (jakelazaroff.com)165 points by veggieroll 3 hours agohidepastfavorite58 comments swyx 2 hours agoimo any experienced frontend framework person will be able to pick out the issues with this impl. OP is returning entire strings of HTML (App, Todo, Icon) to rerender on state changes. this works when 1) you dont care about keeping UI state on the parts that are replaced (incl any stateful children of the UI element that happen to be there in your DOM structure), and 2) when you dont have to update the app in any other places when your state changes. go ahead and build your whole app by replacing innerHtml, frontend frameworks will be right here when you get back from speedrunning the last 10 years. in other words, this todo app is just about the most complex of a frontend you can easily* build with htmx, i'm afraid. try to fix either 1 or 2 and you end up building components and your own little framework. as an exercise to demonstrate, try taking OPs code and adding a count of todos on each All/Active/Completed tab that should update every time u add/edit/delete the todos. see how much extra ui code that takes. compare with equivalent [framework of choice] impl (in most it will just involve 1 state update, thats it). this is htmx's explosion of complexity that makes it not [ optimized for change ] (https://overreacted.io/optimized-for-change/). code that is hard to change eventually calcifies and consumes code that is easy to change if you do not consistently garbage collect (nobody does) i bought the hype too until i tried building something nontrivial in htmx and im afraid the aforementioned islands of interactivity you can build are very very smol islands indeed. happy to revisit my opinion if there are componentlike design patterns in htmx i am not aware of. *emphasis on easily; with enough elbow grease u can do anything ofc. but then you fall out of htmx's very narrow [ pit of success ](obligatory codinghorror dot com link) reply evantbyrne 11 minutes agoparentIn my decades of experience building web applications, I have found it exceedingly rare for components to benefit from SPA-style state management. These frameworks build layers of abstractions to replace functionality that exists in the browser. For example, web forms should never require the level of complexity that frameworks like React steer developers towards: downloading JS components, listening to events to build a local state, crafting an AJAX request, rendering the return JSON as HTML. It almost seems as though there is an entire generation of frontend developers who never learned that there are way simpler alternatives that perform just as well in the real world. htmx might not end up being a SPA-killer, but progressive enhancement has always been a worthy contender. reply traverseda 1 hour agoparentprev> when you dont have to update the app in any other places when your state change You can replace elements outside of your direct tree if you want. The simplest case you replace the whole page and pick what elements you actually want to change. You're thinking about HTMX wrong. It's for progressive enhancement, the default is a full page reload and then you progressively enhance parts of the HTML. You should be using the same code paths and template to generate the islands of interactivity as you do for the whole page. You can then optionally send less HTML by just sending the parts that you expect to have actually changed, your \"islands\". reply swyx 1 hour agorootparentappreciate that. ive been given the progressive enhancement spiel a few times. its obviously a judgement call that will be the right call for some people. but i think many people underestimate how requirements grow over time because our UI standards have gone up over time, even for basic sites that you dont traditionally think of as SPAs. data drives everything, you want your UI to be a function of data. for the other stuff, the growing browser stdlib has basically replaced most of jquery's usecases. so (in the most non condescending or negative way possible) htmx occupies a very awkward sliver between \"The Platform\" and Frameworkland and after giving it some time I have yet to see the benefit from keeping any of it in my head reply LudwigNagasena 1 hour agorootparentprev> You can then optionally send less HTML by just sending the parts that you expect to have actually changed, your \"islands\". And then you end up with modern SSR frameworks that do the bookkeeping for you. reply Imustaskforhelp 1 hour agorootparentyes but mostly the island architecture is only dominant in the javascript world we can basically get the benefits of spa without having to learn js / use minimal js using htmx in some sense https://github.com/donseba/go-htmx check this out reply aaronbrethorst 2 hours agoparentprevI'm not super-stoked about the idea of building SPAs on top of htmx, but what I have found works incredibly well is to build a traditional MPA (SSR etc), embed islands of interactivity where needed, and where something fancier is really necessary, embed a React or Svelte app into just that one portion of a single page. reply swyx 2 hours agorootparentyea im not even talking about SPAs in my post, just the complexity explosion that comes with updating state in more than 1 place / preserving ui state in the place that gets rerendered. it blows up in your face quickly if you have even any requirement volatility (https://stackoverflow.blog/2020/02/20/requirements-volatilit...) reply jonathrg 48 minutes agorootparentI agree that it becomes complex if you have state on the frontend. htmx scales better if you keep all or most of your state on the backend. I've found that using the websockets extension really helps with automatically keeping the frontend in sync. reply tshaddox 39 minutes agoparentprev> frontend frameworks will be right here when you get back from speedrunning the last 10 years More like speedrunning the period between 20 years ago and 10 years ago! React is 11 years old, much older than jQuery was when React was initially released. reply jdiff 1 hour agoparentprev> 1) you dont care about keeping UI state on the parts that are replaced (incl any stateful children of the UI element that happen to be there in your DOM structure), and 2) when you dont have to update the app in any other places when your state changes Htmx does have tools for both of these cases. Out of the box, htmx throws out state, but there are plugins such as morphdom-swap for merging in the new DOM fragment into the old while keeping state. I have some client-only state that holds references to DOM elements in Javascript, and by default, yes, htmx breaks all those references as those elements no longer exist. Link in morphdom-swap, and my references live on across reloads, even across attribute and content changes. And for #2, htmx also allows you to swap in elements that are not the target element, just by specifying that that's what you want. IMO these are pretty basic tools of htmx. Like you said, without them about the most complex thing you can create is a to-do list, and sometimes not even that. reply swyx 1 hour agorootparent> morphdom-swap this https://github.com/bigskysoftware/htmx-extensions/tree/main/... with 159 stars is a basic tool of htmx? is this the community consensus? reply jdiff 1 hour agorootparentNo. Morph swaps[0] are the basic tool of htmx. Morphdom-swap is simply the one that works for my usecase. [0] https://htmx.org/docs/#morphing reply jerrygenser 1 hour agoparentprevI was just looking into htmx the other day. Came across the following library: https://github.com/iwanalabs/django-htmx-components It's an example using django-components. Does this satisfy your comment about component at all? reply mordechai9000 1 hour agorootparentIn that case, IIUC, state is managed on the server, and the client is only responsible for rendering views generated on the backend and returning user input via form or json. This is what htmx was really designed for, anyway. reply L3viathan 1 hour agoparentprevWhile I agree with this project being a bad idea, both 1 and 2 are addressed by HTMX itself, via hx-preserve and Out-Of-Bounds swaps. reply swyx 1 hour agorootparentthanks. yeah i will be the first to admit i've only spent like 2 days with htmx so i wont know everything (but still...) re: hx-preserve. what if i want to \"conditionally preserve\" - preserve this element when my state is one way, but not in other states? i dont see a way. re: hx-swap-oob. looking at https://htmx.org/attributes/hx-swap-oob/ i think it still does not address what i'm looking for. any master-detail list kind of UI will want updates in 2-3 places when 1 piece of state updates (aka ui consistency, ui as a function of state). i fail to see how attaching an attribute with 1 place for an ID solves that. perhaps theres another api for \"multiswap\"? even if it existed... idk if i'd be comfortable using it man (ofc, i am clearly biased/taught to \"think in components\" from 7 years of react exp) reply jdiff 1 hour agorootparentYou're misunderstanding hx-swap-oob. Each element with that attribute will go and replace the element with the matching ID, keeping them all in sync with one response from the server. reply mslip1 1 hour agoparentprevSo I’m building something with HTMX - combining it with alpine has been pretty nice for interactivity managed by client side state reply swyx 1 hour agorootparenti havent tried the combination - perhaps this is the thing ive been missing. any recommended intro/resource for alpine + htmx that we can browse? reply martinbaun 22 minutes agorootparentIt's pretty good combo. I use Alpinejs for the client side interactivty such as modals, but then use htmx for as much as I can that interacts with the backend. You could be using only htmx or only alpine but the combo is really nice reply dimfeld 1 hour agorootparentprevGoogle for AHA stack. (Astro, HTMX, Alpine) There was a great site by Flavio Copes that went into a lot of detail on using them together but it looks like it’s gone. reply flaviocopes 1 hour agorootparentDropped it a couple days ago to revisit it as a blog post, good timing I guess. Restored https://ahastack.dev/ reply stuckinhell 1 hour agoparentprevI agree with you. After seeing some internal prototypes at my job abusing htmx boost. I'm slowly going on the anti-htmx bandwagon. I'm still not pro-react or pro-vue. We need a stabler frontend js framework. reply wk_end 1 hour agoparentprevIt seems like it makes easy things easy and hard things hard, which seems like a pretty poor value proposition - at least relative to some of the hype around it. reply afavour 1 hour agorootparentI think the takeaway here is \"make the easy things easy, don't use it for the hard things\". One of the most exhausting things about any discussion about front end web dev is that it gets treated like a monolith. It _can_ be incredibly complicated. In many scenarios that complication is unwarranted. But in some it's justified. Htmx is a poor choice for a full single page app. That's fine. It excels in other areas. Right tool for the right job. reply wk_end 1 hour agorootparentBut easy things are also relatively easy when using other tools that scale up better. Even if HTMX makes the easy things slightly easier, is that worth investing time and energy into learning it in addition to those tools that scale up better? Is it worth building things with it if I know that months down the line I'm going to need to scrap it once my easy thing becomes slightly less easy? Like I said, this seems like a poor value proposition to me. Of course, if other people are happy with it more power to 'em. To me it feels like people are eager to latch onto this because it's different, and because there's a dopamine hit associated with seeing the easy thing become a little easier when taking a new path that's overoptimized for it - hence the hype - not because it's actually good engineering, in terms of the tradeoffs you're making. Of course, I feel the same way about Tailwind, so maybe I'm just old and grumpy. reply afavour 2 minutes agorootparent> Even if HTMX makes the easy things slightly easier, is that worth investing time and energy into learning it in addition to those tools that scale up better? Depends what your aim is. Do you want to become a full time front end engineer? Then no, probably focus on other frameworks. But do you, from time to time, want to put small pieces of interactivity on web pages when it isn’t the sole (or even major) focus of your job? Htmx might be ideal. Spivak 18 minutes agorootparentprevBecause it's a drop-in, no-dependency, no build-step library that can make your static MPA a little less static for the little bits of interactivity you need with not much effort. It's what you reach for to avoid \"write the whole website in JS.\" React -> \"The site exists in JS, HTML is just a render target.\" jQuery -> \"Poke at the HTML from JS.\" Which is brittle as hell. htmx -> \"The site exists in HTML, extend HTML to handle the common tasks you want to do with it.\" reply normanthreep 1 hour agoparentprev>smol small. thanks reply JodieBenitez 4 minutes agoprevIt's already bad enough that too many devs think only in SPA mode, now you're trying to use the wrong tool for the wrong solution ! Fun experiment though, in the true \"hacker\" spirit ;-) reply sgt 48 minutes agoprevI'm not sure about this approach but it does look interesting. I think it'll work fine. My method is however to use Django and templates to build a regular MPA, and then switch out link changes (between pages) with htmx functionality so there is no browser reload. At least then you'll have a webapp that acts mostly like an SPA. Next up, you can add more interactivity using htmx as much as you want (with some kind of Django components, ideally). You can even add VueJS to one of the pages if you want, but full blown SPA frameworks tend to eat into development time, so rather not unless absolutely needed. reply turtlebits 2 hours agoprevIME, Htmx is best for enriching server side rendered (non-SPA) apps. I'm having an great time with single file web apps using FastHTML (python). I've rewritten a bunch of my JS framework apps for simplicity and don't miss much. reply huuhee3 48 minutes agoparentI agree. I'm building some internal tools at work, and htmx is awesome when you just want to add a bit of interactivity to server side generated pages. With minimum amount of code I can just update the view by returning the relevant parts from the templating engine. It promotes code reuse and is faster to implement than any other framework based solution. reply extr 1 hour agoprevI am a backend/ML engineer and tried using HTMX to create a website with: * Search box * Typeahead * Instantly updating search results It was super instructive. In the end, I realized HTMX was probably not the best tool for that job, but it really helped me bridge the gap between \"I get in theory why we use JS on the FE\" and \"Ah, I can see why client side JS is the obvious choice for this\". reply berkes 1 hour agoparentI'm not sure if I follow this example, though. Doesn't a searchbox with typeahead and instantly updating results still require a backend? Isn't that backend not the most important ingredient of this use-case? Or did you send a large payload of objects to the frontend and have it indexed clientside with e.g. lunr.js? I've used fuse.js for this, but for a use-case where we knew we had less than a hundred documents to index and where the access control was simple and the content reasonably stable. I'd never use this for a search feature in e.g. an admin backend or a large, content-rich webapp. reply smallerfish 53 minutes agoparentprevWhat issues did you run into with this? I do exactly this in a couple of apps. You can use a `keyup changed delay:${delayMs}ms` event, and you target the area on your page where you want search results. reply davidedicillo 2 hours agoprevFWIW, as a hobbyist developer who never had a chance to learn React, I found HTMX really helpful to make my Flask projects more reactive without adding much complexity. reply rutierut 2 hours agoprevI see a couple of people here bashing on the practicality of this project. That's obviously not the point, it's an interesting weird use case that's more explorative/educational than practical. I thought it was an interesting and inspiring read! reply kccqzy 23 minutes agoprevAnyone who says React has mired developers in complexity needs to go back ten years and use jQuery to build a single-page app and remind themselves how much less complex React is. It is then that they will realize complexity comes from single-page apps. Pushing back against that complexity should result in not building SPAs, which is where Htmx comes in. reply hinkley 11 minutes agoparentI’ve noticed there’s some mental block where managers and product owners behave as if adding new pages to a multipage app is too expensive so they just keep cramming more functionality into the existing half-dozen, dozen, or twenty pages. After three or more years it feels like you have a handful of single page apps, and jquery and many other tools struggle there. reply v3ss0n 12 minutes agoprevThis is really bad abomination .. please keep SPA to SPA Frameworks and leave HTMX out of it.. reply koolala 2 hours agoprevI hope we build Single-page apps with iFrames one day like how the web originally used . Everyone only talks about iFrames as if their only purpose is cross-origin content andis forgotten. reply packetlost 2 hours agoparentIsn't there a bunch of complexity and security issues that come with using ? I'm not as familiar with . reply koolala 54 minutes agorootparentNo, thats the misconception. All the security is around cross-origin usage and optional if using same-origin html like a SPA. Each iframe gets its own window element but that can simplify component design a ton if embraced. Awas a system for loading same-origin html files from an Index. It was the original Single-page multi-page system and at the time seemed good enough for 90% of html 1.0 sites.was supposed to replace it but then everyone used it for Ads and closed-source widgets. reply altbdoor 1 hour agoprevI made a somewhat similar prototype by mocking XHR instead of service workers, in https://stackblitz.com/edit/typescript-h3wfwx?file=index.ts It was fun for a bit to quickly experiment with how htmx works, but I find it difficult to scale in terms of state. reply pier25 2 hours agoprevI'm thinking of starting a project with HTMX and islands of interactivity (probably web components). Anyone used this pattern in production? reply yawnxyz 2 hours agoparentI tried it and it works nicely for small sites where you look up some data / fill in some forms! Then I really wanted data reactivity, so I started using `nanostores`, but I kept wanting more, and eventually added `alpinejs` And then it got a bit too complicated for a one page microsite so I switched to Astro (for the interactive islands bit) reply BiteCode_dev 2 hours agoparentprevYes, first boosts, then a few gets then a few events that cascade. It's nice. reply BiteCode_dev 2 hours agoprevWhich means offline htmx is possible, althought I wouldn't love to maintain that. reply leephillips 1 hour agoprevI don’t know if the article describes the best use of HTMX, but when I wanted to experiment with building an interactive physics application on the web that used Julia on the backend, it was a perfect fit. I could design the whole page without any javascript (almost): https://lee-phillips.org/pluckit/ reply uhtred 1 hour agoprevI think 68% of SPAs don't need to be SPAs. The end user doesn't care if the browser page refreshes if it is quick. reply Bengalilol 2 hours agoprevI wanted to send a UX feedback : Multiline text shrinks the 'x' button (the more text, the more shrunk the button gets). reply synergy20 2 hours agoprevseems to me a stretch for what it's best for and designed for reply baggachipz 2 hours agoprev\"Your scientists were so preoccupied with whether they could, they didn't stop to think if they should\" At that point, maybe stop fighting with service workers and simply use a framework like Vue. It allows html templates to be swapped in, in much the same way. Except you can actually debug it and store in localStorage if you desire. reply jakelazaroff 2 hours agoparentBut if I’d used Vue it wouldn’t be an interesting article :) reply tzahifadida 1 hour agoprevWas wondering if any1 embeds react spa apps in golang binaries to build a lean SaaS? reply nsonha 2 hours agoprev [–] \"Simple\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "htmx is presented as a simpler alternative to complex single-page applications (SPAs) like those built with React, combining elements of traditional multi-page and single-page apps to create hypermedia-driven applications.",
      "The author built a todo list app using htmx and service workers, which allows for offline functionality and persistent data storage via IndexedDB, though it faces challenges like limited developer tool support and ES module compatibility issues in Firefox.",
      "While htmx facilitates dynamic content loading and interaction, it is not essential for fully client-side apps, where frameworks like React might offer more straightforward data and UI synchronization."
    ],
    "commentSummary": [
      "The use of Htmx for building single-page applications (SPAs) has sparked debate, with some developers arguing it is not ideal for maintaining UI state or updating multiple areas of an app.",
      "Critics suggest Htmx is suitable for simple tasks but becomes complex for advanced applications, leading to a preference for traditional frontend frameworks.",
      "The discussion emphasizes the importance of selecting the appropriate tool for the task, with some developers combining Htmx with other tools like Alpine.js to enhance interactivity in server-rendered apps."
    ],
    "points": 165,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1728314349
  },
  {
    "id": 41766610,
    "title": "AT&T, Verizon reportedly hacked to target US govt wiretapping platform",
    "originLink": "https://www.bleepingcomputer.com/news/security/atandt-verizon-reportedly-hacked-to-target-us-govt-wiretapping-platform/",
    "originBody": "AT&T, Verizon reportedly hacked to target US govt wiretapping platform{ \"@context\": \"https://schema.org\", \"@type\": \"NewsArticle\", \"url\": \"https://www.bleepingcomputer.com/news/security/atandt-verizon-reportedly-hacked-to-target-us-govt-wiretapping-platform/\", \"headline\": \"AT&T, Verizon reportedly hacked to target US govt wiretapping platform\", \"name\": \"AT&T, Verizon reportedly hacked to target US govt wiretapping platform\", \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"id\": \"https://www.bleepingcomputer.com/news/security/atandt-verizon-reportedly-hacked-to-target-us-govt-wiretapping-platform/\" }, \"description\": \"Multiple U.S. broadband providers, including Verizon, AT&T, and Lumen Technologies, have been breached by a Chinese hacking group tracked as Salt Typhoon, the Wall Street Journal reports.\", \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://www.bleepstatic.com/content/hl-images/2024/05/22/Chinese_hackers.png\", \"width\": 2504, \"height\": 1400 }, \"author\": { \"@type\": \"Person\", \"name\": \"Ionut Ilascu\", \"url\": \"https://www.bleepingcomputer.com/author/ionut-ilascu/\" }, \"keywords\": [\"AT&T\",\"China\",\"Salt Typhoon\",\"Verizon\",\"Wiretap\",\"Security\",\"InfoSec, Computer Security\"], \"datePublished\": \"2024-10-07T10:51:04-04:00\", \"dateModified\": \"2024-10-07T10:51:04-04:00\", \"publisher\": { \"@type\": \"Organization\", \"name\": \"BleepingComputer\", \"url\": \"https://www.bleepingcomputer.com/\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://www.bleepstatic.com/logos/bleepingcomputer-logo.png\", \"width\": 700, \"height\": 700 } } }!function(n){if(!window.cnx){window.cnx={},window.cnx.cmd=[];var t=n.createElement('iframe');t.src='javascript:false'; t.display='none',t.onload=function(){var n=t.contentWindow.document,c=n.createElement('script');c.src='//cd.connatix.com/connatix.player.js?cid=1ffdf4d6-eb53-11e9-b4d2-06948452ae1a',c.setAttribute('async','1'),c.setAttribute('type','text/javascript'),n.body.appendChild(c)},n.head.appendChild(t)}}(document); (new Image()).src = 'https://capi.connatix.com/tr/si?token=de820c7a-cd3f-49f4-9038-04e5790f8d5e&cid=1ffdf4d6-eb53-11e9-b4d2-06948452ae1a'; cnx.cmd.push(function() { cnx({ playerId: \"de820c7a-cd3f-49f4-9038-04e5790f8d5e\" }).render(\"0277831f664a4888be888d4b346dd76c\"); });var freestar = freestar || {}; freestar.queue = freestar.queue || []; freestar.config = freestar.config || {}; // Tag IDs set here, must match Tags served in the Body for proper setup freestar.config.enabled_slots = [];freestar.queue.push(function() { googletag.pubads().setTargeting('section', ['news','security']);}); freestar.initCallback = function () { (freestar.config.enabled_slots.length === 0) ? freestar.initCallbackCalled = false : freestar.newAdSlots(freestar.config.enabled_slots) } ;(function(o) { var w=window.top,a='apdAdmin',ft=w.document.getElementsByTagName('head')[0], l=w.location.href,d=w.document;w.apd_options=o; if(l.indexOf('disable_fi')!=-1) { console.error(\"disable_fi has been detected in URL. FI functionality is disabled for this page view.\"); return; } var fiab=d.createElement('script'); fiab.type = 'text/javascript'; fiab.src=o.scheme+'ecdn.analysis.fi/static/js/fab.js';fiab.id='fi+o.websiteId; ft.appendChild(fiab, ft);if(l.indexOf(a)!=-1) w.localStorage[a]=1; var aM = w.localStorage[a]==1, fi=d.createElement('script'); fi.type='text/javascript'; fi.async=true; if(aM) fi['data-cfasync']='false'; fi.src=o.scheme+(aM?'cdn':'ecdn') + '.firstimpression.io/' + (aM ? 'fi.js?id='+o.websiteId : 'fi_client.js'); ft.appendChild(fi); })({ 'websiteId': 5971, 'scheme': '//' });window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-GD465VRQLD'); NewsFeatured LatestMicrosoft warns of Windows 11 24H2 gaming performance issuesCritical Zimbra RCE flaw exploited to backdoor servers using emailsFake browser updates spread updated WarmCookie malwareMicrosoft Office 2024 now available for Windows and macOS usersQualcomm patches high-severity zero-day exploited in attacksAmerican Water shuts down online services after cyberattackAT&T, Verizon reportedly hacked to target US govt wiretapping platformHybrid Analysis Bolstered by Criminal IP’s Comprehensive Domain Intelligence TutorialsLatest PopularHow to access the Dark Web using the Tor BrowserHow to enable Kernel-mode Hardware-enforced Stack Protection in Windows 11How to use the Windows Registry EditorHow to backup and restore the Windows RegistryHow to start Windows in Safe ModeHow to remove a Trojan, Virus, Worm, or other MalwareHow to show hidden files in Windows 7How to see hidden files in Windows Virus Removal GuidesLatest Most Viewed RansomwareRemove the Theonlinesearch.com Search RedirectRemove the Smartwebfinder.com Search RedirectHow to remove the PBlock+ adware browser extensionRemove the Toksearches.xyz Search RedirectRemove Security Tool and SecurityTool (Uninstall Guide)How to Remove WinFixer / Virtumonde / Msevents / Trojan.vundoHow to remove Antivirus 2009 (Uninstall Instructions)How to remove Google Redirects or the TDSS, TDL3, or Alureon rootkit using TDSSKillerLocky Ransomware Information, Help Guide, and FAQCryptoLocker Ransomware Information Guide and FAQCryptorBit and HowDecrypt Information Guide and FAQCryptoDefense and How_Decrypt Ransomware Information Guide and FAQ DownloadsLatest Most DownloadedQualys BrowserCheckSTOPDecrypterAuroraDecrypterFilesLockerDecrypterAdwCleanerComboFixRKillJunkware Removal Tool DealsCategorieseLearningIT Certification CoursesGear + GadgetsSecurity VPNsPopularBest VPNsHow to change IP addressAccess the dark web safelyBest VPN for YouTube Forums MoreStartup Database Uninstall Database Glossary Chat on Discord Send us a Tip! Welcome Guide HomeNewsSecurityAT&T, Verizon reportedly hacked to target US govt wiretapping platformAT&T, Verizon reportedly hacked to target US govt wiretapping platform By Ionut Ilascu October 7, 2024 10:51 AM 3 Multiple U.S. broadband providers, including Verizon, AT&T, and Lumen Technologies, have been breached by a Chinese hacking group tracked as Salt Typhoon, the Wall Street Journal reports. The purpose of the attack appears to be for intelligence collection as the hackers might have had access to systems used by the U.S. federal government for court-authorized network wiretapping requests. It is unclear when the intrusion occurred, but WSJ cites people familiar with the matter, saying that \"for months or longer, the hackers might have held access to network infrastructure used to cooperate with lawful U.S. requests for communications data.\" Salt Typhoon is the name that Microsoft gave to this particular China-based threat actor. Other cybersecurity companies are tracking the adversary as Earth Estries (Trend Micro), FamousSparrow (ESET), Ghost Emperor (Kaspersky), and UNC2286 (Mandiant, now part of Google Cloud). Capturing sensitive traffic According to the WSJ, the attack was discovered in recent weeks and is being investigated by the U.S. government and security experts in the private sector. The impact of the attack - amount and type of observed and exfiltrated data - is still being assessed, people with information about the intrusion told WSJ.“The hackers appear to have engaged in a vast collection of internet traffic from internet service providers that count businesses large and small, and millions of Americans, as their customers” - Wall Street JournalApart from breaching service providers in the U.S. Salt Typhoon may have hacked similar entities in other countries, too. Salt Typhoon has been active since at least 2019 and is considered a sophisticated hacking group focusing on government entities and telecommunications companies typically in the Southeast Asia region. Security researchers also found that the threat actor attacked hotels, engineering companies, and law firms in Brazil, Burkina Faso, South Africa, Canada, Israel, France, Guatemala, Lithuania, Saudi Arabia, Taiwan, Thailand, and the United Kingdom. The hackers usually obtain initial access to the target network by exploiting vulnerabilities, such as the ProxyLogon vulnerabilities in Microsoft Exchange Server (CVE-2021-26855, CVE-2021-26857, CVE-2021-26858, and CVE-2021-27065). In previous attacks attributed to Salt Typhoon/Ghost Emperor, the threat actor used a custom backdoor called SparrowDoor, customized versions of the Mimikatz tool for extracting authentication data, and a Windows kernel-mode rootkit Demodex. Investigators are still looking for the initial access method for the recent attack. The WSJ says that one avenue being explored is gaining access to Cisco routers responsible for routing internet traffic. However, a Cisco spokesperson told WSJ that the company was looking into the matter but had received no indication that Cisco networking equipment was involved in the breach. BleepingComputer contacted AT&T about the alleged breach and was told they \"are not commenting on the WSJ report.\" Lumen also declined to comment. Verizon has not responded to our emails, and we will update the story if we receive a reply. Chinese APT hacking groups have been increasingly targeting U.S. and European networking devices and ISPs in cyberespionage attacks. In August, cybersecurity researchers at Lumen's Black Lotus Labs disclosed that the Chinese threat actors known as \"Volt Typhoon\" exploited a zero-day flaw in Versa Director to steal credentials and breach corporate networks. During these attacks, the threat actors breached multiple ISPs and MSPs in the U.S. and India, which is not believed to be related to the recent breaches. In September, Black Lotus Labs and law enforcement disrupted a massive Chinese botnet named \"Raptor Train\" that compromised over 260,000 SOHO routers, IP cameras with malware. This botnet was used by the \"Flax Typhoon\" threat actors for DDoS attacks and as a proxy to launch stealthy attacks on other organizations. While these attacks have been attributed to different Chinese hacking groups, they are believed to operate under the same umbrella, commonly sharing infrastructure and tools.Related Articles: Verizon outage: iPhones, Android devices stuck in SOS modeUS proposes ban on connected vehicle tech from China, RussiaChinese botnet infects 260,000 SOHO routers, IP cameras with malwareAT&T pays $13 million FCC settlement over 2023 data breachMicrosoft: Stealthy Flax Typhoon hackers use LOLBins to evade detection freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_728x90_320x50_InContent_1\", slotId: \"bleepingcomputer_728x90_320x50_InContent_1\" });AT&T China Salt Typhoon Verizon WiretapIonut IlascuIonut Ilascu is a technology writer with a focus on all things cybersecurity. The topics he writes about include malware, vulnerabilities, exploits and security defenses, as well as research and innovation in information security. His work has been published by Bitdefender, Netgear, The Security Ledger and Softpedia. Previous ArticleNext ArticleComments Wannabetech1- 3 hours ago So the government hackers got hacked; too funny!GT500- 3 hours ago The government shouldn't have a system to wiretap ISP's...DyingCrow- 1 hour ago This prompts for the opening of a big, nasty can of worms. Post a Comment Community RulesYou need to login in order to post a comment Not a member yet? Register NowYou may also like:(adsbygoogle = window.adsbygoogle || []).push({});freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_1\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_1\" });Popular Stories Google removes Kaspersky's antivirus software from Play StoreComcast and Truist Bank customers caught up in FBCS data breachHighline Public Schools confirms ransomware behind shutdownfreestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_2\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_2\" });Sponsor PostsDiscover how to build custom dictionaries in your AD password policyReduce cyber risk by securing passwords in your employee onboardingData Theft in Salesforce: Manipulating Public LinksHybrid Analysis Bolstered by Criminal IP’s Comprehensive Domain IntelligenceSee how you can manage shadow IT and reduce your attack surface freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_3\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_3\" }); freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_728x90_970x90_970x250_320x50_BTF\", slotId: \"bleepingcomputer_728x90_970x90_970x250_320x50_BTF\" });Follow us:Main SectionsNews VPN Buyer Guides SysAdmin Software Guides Downloads Virus Removal Guides Tutorials Startup Database Uninstall Database GlossaryCommunityForums Forum Rules ChatUseful ResourcesWelcome Guide SitemapCompanyAbout BleepingComputer Contact Us Send us a Tip! Advertising Write for BleepingComputer Social & Feeds Changelog Terms of Use -Privacy Policy - Ethics Statement - Affiliate Disclosure Copyright @ 2003 - 2024Bleeping Computer® LLC- All Rights Reserved Login UsernamePasswordRemember MeSign in anonymously Sign in with TwitterNot a member yet? Register Now$(document).ready(function(e) { $('.articleBody img').not('a>img').not('.contrib_but>img').click(function(e) { e.preventDefault(); $.fancybox({'href' : $(this).attr('src')}); }); }); $(document).ready(function(){ var content = $('.cz-main-left-section'); var sidebar = $('.bc_right_sidebar'); var count = 0; var myTimer; function setEqualContainer() { var getContentHeight = content.outerHeight(); var getSidebarHeight = sidebar.outerHeight(); if ( getContentHeight > getSidebarHeight ) { sidebar.css('min-height', getContentHeight); } if ( getSidebarHeight > getContentHeight ) { content.css('min-height', getSidebarHeight); } } // creating the timer which will run every 500 milliseconds // and will stop after the container will be loaded // ...or after 15 seconds to not eat a lot of memory myTimer = setInterval( function() { count++; if ( $('.testContainer').length == 0 ) { setEqualContainer(); } else { setEqualContainer(); clearInterval(myTimer); } if ( count == 15) { clearInterval(myTimer); } }, 500); $('#pinned').fixTo('.bc_right_sidebar', { bottom: 25, }); $('#more_dd').click(function (e) { e.preventDefault() }); $('.bc_goto_top a').click(function(){ $(\"html, body\").animate({ scrollTop: 0 }, 600); return false; }); jQuery('.bc_login_btn').on('click', function() { jQuery('.bc_popup').fadeIn(\"slow\"); }); jQuery('.bc_popup_close').on('click', function() { jQuery('.bc_popup').fadeOut(\"slow\"); }); });// validate comment box not empty function validate_comment_box_not_empty() {$('#frm_comment_box').submit(function(e) { if($('#comment_html_box').val().length==0) {alert(\"Please enter a comment before pressing submit\");return false; } else {return true; }}); } function cz_strip_tags(input, allowed) { allowed = (((allowed || '') + '') .toLowerCase() .match(//g) || []) .join(''); // making sure the allowed arg is a string containing only tags in lowercase () var tags = /]*>/gi, commentsAndPhpTags = /|/gi; return input.replace(commentsAndPhpTags, '') .replace(tags, function($0, $1) { return allowed.indexOf('') > -1 ? $0 : ''; }); } function cz_br2nl(str) {var regex = //gi; //var pure_str = str.replace(regex,\"\"); var pure_str = str.replace(regex,\"\"); return cz_strip_tags(pure_str,''); } $(document).ready(function(e) { // validate comment box not empty validate_comment_box_not_empty(); // report comment $('#comment-report-other-reason-wrap').css('display','none'); $('.cz-popup-close').click(function(e) { e.preventDefault(); $('.cz-popup').fadeOut(\"slow\"); }); $('.cz-comment-report-btn').click(function(e) { e.preventDefault(); $('.cz-popup').css('height',$( document ).height()+'px'); //var comment_box_report_top = $(this).offset().top; var comment_box_report_top = $(document).scrollTop(); $('.cz-popup-wrapp').css('top',(comment_box_report_top+100)+'px'); $('#comment-id-report').val($(this).attr('data-id')); $('.cz-popup').fadeIn(\"slow\"); }); $(\"input[type='radio'][name='comment-report-reason']\").click(function(e) { if($(this).val()=='Other') { $('#comment-report-other-reason-wrap').css('display','block'); } else { $('#comment-report-other-reason-wrap').css('display','none'); } }); $('.comment-report-submit-btn').click(function(e) { e.preventDefault(); var comment_report_reason = \"\"; var comment_report_reason = $(\"input[type='radio'][name='comment-report-reason']:checked\").val(); if (comment_report_reason=='Other') { comment_report_reason = $('#comment-report-other-reason').val(); } if(comment_report_reason=='') { alert('Please specify reason'); } else { $('.cz-popup-report-submiting').css('display','inline-block'); $.ajax({type: \"POST\", url: 'https://www.bleepingcomputer.com/report-comment/', data: { comment_id: $('#comment-id-report').val(), reason: comment_report_reason }, success: function(data) { $('.cz-popup-report-submiting').css('display','none'); $('.cz-popup').fadeOut(\"slow\"); }}); } }); // report comment $('.cz_comment_reply_btn').click(function(e) { e.preventDefault(); $('#parent_comment_id').val($(this).attr('data-id')); $('#comment_html_box').attr('placeholder','Replying to '+$(this).attr('data-name')); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); $('#comment_html_box').focus(); }); $('.cz_comment_quote_btn').click(function(e) { e.preventDefault(); var quote_comment_html =''; if($(this).attr('data-id')!=undefined && $(this).attr('data-id')!='') { $('#parent_comment_id').val($(this).attr('data-id')); quote_comment_html = $('#comment_html_'+$(this).attr('data-id')).html(); } quote_comment_html = cz_br2nl(quote_comment_html); $('#comment_html_box').val('\"'+quote_comment_html+'\"'); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); $('#comment_html_box').focus(); }); }); function editForm(cid) { $.ajax({ type: \"GET\", url: window.location.href+\"?sa=1\", data: { f: \"e\", cid: cid }, success: function(data) { $('.cz-post-comment-wrapp').html(data);validate_comment_box_not_empty(); } }); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); } $(document).on('click', '.cz-subscribe-button' , function(e) { e.preventDefault(); $.ajax({type: \"POST\", url: window.location.href, data: { a: 'sub' }, success: function(data) { if(data == '1')$( \"li.cz-subscribe-button\" ).replaceWith( ''); } }); }); $(document).on('click', '.cz-unsubscribe-button' , function(e) { e.preventDefault(); $.ajax({ type: \"POST\", url: window.location.href, data: { a: 'unsub' }, success: function(data) { if(data == '1')$( \"li.cz-unsubscribe-button\" ).replaceWith( ''); } }); });$('.cz-print-icon, .cz-lg-print-icon').click(function(e) { e.preventDefault(); var divToPrint = document.getElementById('.article_section'); var mywindow = window.open('','','left=0,top=0,width=950,height=600,toolbar=0,scrollbars=0,status=0,addressbar=0'); var is_chrome = Boolean(mywindow.chrome); mywindow.document.write($( \".article_section\" ).html()); mywindow.document.close(); // necessary for IE >= 10 and necessary before onload for chrome if (is_chrome) { mywindow.onload = function() { // wait until all resources loaded mywindow.focus(); // necessary for IE >= 10 mywindow.print(); // change window to mywindow mywindow.close();// change window to mywindow }; } else { mywindow.document.close(); // necessary for IE >= 10 mywindow.focus(); // necessary for IE >= 10 mywindow.print(); mywindow.close(); } return true; });var loginhash = '880ea6a14ea49e853634fbdc5015a024'; var main_nav_hide_flag = true; var scrollTop =0; var main_nav_hide_timer = ''; function call_main_nav_hide() { if(main_nav_hide_flag && scrollTop >=100) { $('header').addClass(\"nav-up\"); } } var cz_header_pos = $('header').offset().top; $(window).scroll(function() {$('header').each(function(){var cz_top_of_window = $(window).scrollTop()-100; if (cz_top_of_window > cz_header_pos) { $('.bc_goto_top').fadeIn(\"slow\"); } else {$('.bc_goto_top').fadeOut(\"slow\");}}); }); var prevScrollTop = 0; $(window).scroll(function(event){ scrollTop = $(this).scrollTop(); if ( scrollTop$('body').height() - $(window).height() ) { scrollTop = $('body').height() - $(window).height(); } if (scrollTop >= prevScrollTop && scrollTop) { $('header').addClass(\"nav-up\"); } else {if (scrollTop >=100){ $('header').removeClass(\"nav-up\"); main_nav_hide_timer = setTimeout(\"call_main_nav_hide()\",5000);}else{ $('header').removeClass(\"nav-up\"); clearInterval(main_nav_hide_timer);} } prevScrollTop = scrollTop; }); $(document).ready(function(){var bLazy = new Blazy(); $(\".bc_dropdown a\").mouseenter(function(e) { $(this).parent('.bc_dropdown').delay(250).queue(function(){ $(this).addClass('show_menu').dequeue(); bLazy.revalidate(); }); main_nav_hide_flag = false; }); $(\".bc_dropdown\").mouseleave(function(e) { $(\".bc_dropdown\").clearQueue().stop().removeClass('show_menu'); main_nav_hide_flag = true; if (scrollTop >=100) { main_nav_hide_timer = setTimeout(\"call_main_nav_hide()\",5000); } }); $('.bc_dropdown a').each(function(){ if($(this).is(\":hover\")) { $(this).mouseenter(); } }); $('#bc_drop_tab a').hover(function (e) { e.preventDefault() $(this).tab('show') bLazy.revalidate(); });$('#more_dd').click(function (e) { e.preventDefault()});$('.bc_goto_top a').click(function(){$(\"html, body\").animate({ scrollTop: 0 }, 600);return false;});jQuery('.bc_login_btn').on('click', function() { jQuery('.bc_popup').fadeIn(\"slow\"); $('#ips_username').focus(); });jQuery('.bc_popup_close').on('click', function() { jQuery('.bc_popup').fadeOut(\"slow\"); }); }); $(document).mouseup(function (e) { var container = $(\".bc_login_form\"); if (!container.is(e.target) // if the target of the click isn't the container... && container.has(e.target).length === 0 && $('.bc_popup').css('display') =='block') // ... nor a descendant of the container { jQuery('.bc_popup').fadeOut(\"slow\"); } }); if($(window).width()ReporterHelp us understand the problem. What is going on with this comment? Spam Abusive or Harmful Inappropriate content Strong language OtherRead our posting guidelinese to learn what content is prohibited.Submitting... SUBMITvar loadDeferredStyles = function() { var addStylesNode = document.getElementById(\"deferred-styles\"); var replacement = document.createElement(\"div\"); replacement.innerHTML = addStylesNode.textContent; document.body.appendChild(replacement) addStylesNode.parentElement.removeChild(addStylesNode); }; var raf = requestAnimationFrame || mozRequestAnimationFrame || webkitRequestAnimationFrame || msRequestAnimationFrame; if (raf) raf(function() { window.setTimeout(loadDeferredStyles, 0); }); else window.addEventListener('load', loadDeferredStyles);",
    "commentLink": "https://news.ycombinator.com/item?id=41766610",
    "commentBody": "AT&T, Verizon reportedly hacked to target US govt wiretapping platform (bleepingcomputer.com)145 points by el_duderino 4 hours agohidepastfavorite23 comments NelsonMinar 2 hours agoIt used to be the US government worked to secure American communications. But between these backdoors and the NSA losing control of exploits thanks to the Shadow Brokers, they do more now to undermine American security than protect it. reply hypeatei 2 hours agoparentNo, Intel agencies have always been too powerful and Truman saw it when disbanding the OSS (Office of strategic services) after WWII. Then, he begrudgingly created the CIA to compete in the cold war. They've always undermined American security so they could have more information and power. reply diggan 2 hours agoparentprev> It used to be the US government worked to secure American communications. When was this? As far as I remember (but I'm not that old to be honest), it seems to mostly been about the US government making sure the government has secure communications, while the rest get to fend for themselves. reply ffujdefvjg 1 hour agorootparentFend for themselves, and if they don't cooperate with the wishes of the TLAs they get legal trouble nobody could possibly afford. And if you end up in the secret FISA courts, you basically can't get legal representation because it's secret, or ever really talk about it. Also there's no real oversight for this stuff because it's that secret. reply ben_w 34 minutes agoparentprevBoth statements are simultaneously true. The goal is to protect the physical and institutional USA (and equivalent for other countries' intel agencies); this requires making sure there's no successful conspiracies, from within or without, to destroy it; this requires all the things we here all agree are bad for digital security, including the security necessary to running e.g. electronic banking ledgers or votes. I don't have any actual solutions here, that's just a description of the problem space as I understand it to be. There's a bunch of US agencies sponsoring Tor, presumably to undermine hostile governments, even though there's also US agencies trying to subvert it. reply formerly_proven 1 hour agoparentprev^ this guy is about to learn about the crypto wars. reply immibis 2 hours agoparentprevYou misunderstood \"American security\" to mean \"security of Americans\" instead of the intended meaning \"security of the American regime\" reply mistrial9 25 minutes agorootparentyou are not wrong, but this has always been the case, from the earliest times. Similar problems with the institution of the military. It calls for moving past the initial indignation, and engaging somehow.. otherwise you get the government you deserved, as they say. reply olliej 4 minutes agoprevHappily, this kind of attack would not compromise secure communication with government mandated \"secure intercept\" technology, because of magic fairy dust reasons :-/ reply TriangleEdge 3 hours agoprevThe article didn't say but I'm guessing the target could of been JSI Telecom. I knew some people that worked for JSI ~10 years ago and the US govt used their platform in a handful of organizations. reply r721 3 hours agoprevOriginal WSJ story (unpaywalled): https://archive.is/RqwMQ reply phendrenad2 1 hour agoprevTech imitates life. Hyenas specialized in chasing lions away from their prey. reply Hizonner 3 hours agoprevWell, that's what happens when you deliberately compromise your own infrastructure with \"lawful intercept\" back doors. reply Neonlicht 3 hours agoparentThe CCP doesn't need the backdoor. The US intelligence agencies have to do the whole masquerade of freedom and liberty. reply photochemsyn 2 hours agoprevSounds like another government-approved leak to a compliant corporate media outlet by 'anonymous sources'. I don't know why the relevant government agencies don't just issue a press release unless they're unusually embarrassed by this apparent security failure. The other possibility is the story is no more true than all those 'anonymous source' leaks about Iraq's (nonexistent) chemical, biological and nuclear weapons programs from two decades ago. If we're not going to accept Seymour Hersch's anonymously-sourced claim that the US Navy was involved in the destruction of the Nordstream pipelines, why accept this claim at face value either? For an example of reporting of a major hacking incident not reliant on anonymous government sources, see the OPM hack: https://www.nytimes.com/2015/06/05/us/breach-in-a-federal-co... Notably, the WSJ source report doesn't include any mention of reporters attempting to get official statements from the relevant US government agencies and being rebuffed. That smells like plausible deniability of the kind involved in the bogus Iraq WMD leaks. reply jeroenhd 3 hours agoprev [–] Targeting wiretapping infrastructure may be a viable attack, but with how few details are available to the public, it's hard to estimate the impact. Just because a wiretapping platform was hacked doesn't mean any data was gathered, and if it really was, we don't know what kind of data. Thanks to mobile networks, information can be anything from live internet traffic to live location information of cars and phones. However, I suspect if someone did a hack that juicy, carrier SOCs would've noticed immediately. This type of infrastructure isn't exactly hooked up to a public IP address somewhere. reply ziddoap 2 hours agoparent>However, I suspect if someone did a hack that juicy, carrier SOCs would've noticed immediately. We're talking real deal nation-state actors targeting an industry where for the last few decades the only downside of being breached is having to say \"oh oops, sorry\" and maybe providing a year of credit monitoring. Security is something taken just seriously enough to avoid a ruling of negligence, but no more. It is very optimistic to assume that carriers would immediately notice a breach by threat actors this sophisticated. reply runjake 3 hours agoparentprev> This type of infrastructure isn't exactly hooked up to a public IP address somewhere. Without going into details, consider that sometimes they are, even with very large providers that you think should know better. Law enforcement’s got to get to them somehow. And much of the documentation for these systems is publicly available. Search for your favorite enterprise company and for “lawful intercept”. reply AmericanChopper 2 hours agorootparentI was going to comment basically this. Everything you could possibly want to know about how LI systems work is documented by the vendors online. It’s really just network interfaces that forward intercepted traffic to aggregators. The thing about CSPs is their core business is edge routing. A majority of their core assets are going to be internet connected routers, and you’d actually be able to collect more data by owning some of those. The additional information you can get from LI (and the reason you often need a clearance to work on LI systems) is information about who law enforcement are running intercepts on. Also, LI is just a regulatory cost centre for CSPs. It’s hilarious (or scary, depending on your perspective) how poorly those systems are maintained, and how often the break. reply throwway120385 42 minutes agoparentprevI would just assume there's a cloud provider that handles all of the wiretapping services for both or all carriers. There's a single-point-of-failure for everything else nowadays anyway. Look at what happened with Crowdstrike, or Solarwinds, or any number of other big single-source providers. Nobody wants to maintain it in house, with predictable results. reply hansvm 3 hours agoparentprev> This type of infrastructure isn't exactly hooked up to a public IP address somewhere. It's getting from point A to point B, and probably not via sneakernet. The details will make it more or less secure, but I'd be shocked if it's going through anything other than public internet pathways. reply Hizonner 3 hours agoparentprev> This type of infrastructure isn't exactly hooked up to a public IP address somewhere. Snort. reply A4ET8a8uTh0 3 hours agoparentprev [–] << few details are available to the public, it's hard to estimate the impact. Would it not be a good indicator that it may not be a great idea to begin with? << carrier SOCs would've noticed immediately. I want to believe that. I do. But the longer I live in corporate, the more I think that we are experiencing a serious competency problem across the board. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Chinese hacking group named Salt Typhoon reportedly breached AT&T, Verizon, and Lumen Technologies, targeting a U.S. government wiretapping platform for intelligence collection.- The breach potentially allowed access to systems used for court-authorized wiretapping, with the intrusion possibly lasting for months.- Salt Typhoon, active since 2019, exploits vulnerabilities in government and telecom sectors, including those in Microsoft Exchange Server, prompting investigations by the U.S. government and private security experts."
    ],
    "commentSummary": [
      "AT&T and Verizon were reportedly hacked, targeting a US government wiretapping platform, raising concerns about the security of American communications.",
      "The incident has sparked debates about whether government backdoors and intelligence agencies compromise security more than they enhance it.",
      "Critics point to potential vulnerabilities in infrastructure due to insufficient security measures and reliance on public internet pathways, emphasizing ongoing discussions about government surveillance and security."
    ],
    "points": 145,
    "commentCount": 23,
    "retryCount": 0,
    "time": 1728312813
  }
]
