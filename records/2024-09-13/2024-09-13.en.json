[
  {
    "id": 41527143,
    "title": "Notes on OpenAI's new o1 chain-of-thought models",
    "originLink": "https://simonwillison.net/2024/Sep/12/openai-o1/",
    "originBody": "Simon Willison’s Weblog Subscribe Notes on OpenAI’s new o1 chain-of-thought models 12th September 2024 OpenAI released two major new preview models today: o1-preview and o1-mini (that mini one is not a preview)—previously rumored as having the codename “strawberry”. There’s a lot to understand about these models—they’re not as simple as the next step up from GPT-4o, instead introducing some major trade-offs in terms of cost and performance in exchange for improved “reasoning” capabilities. Trained for chain of thought Low-level details from the API documentation Hidden reasoning tokens Examples What’s new in all of this Trained for chain of thought # OpenAI’s elevator pitch is a good starting point: We’ve developed a new series of AI models designed to spend more time thinking before they respond. One way to think about these new models is as a specialized extension of the chain of thought prompting pattern—the “think step by step” trick that we’ve been exploring as a a community for a couple of years now, first introduced in the paper Large Language Models are Zero-Shot Reasoners in May 2022. OpenAI’s article Learning to Reason with LLMs explains how the new models were trained: Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them. [...] Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses. It learns to recognize and correct its mistakes. It learns to break down tricky steps into simpler ones. It learns to try a different approach when the current one isn’t working. This process dramatically improves the model’s ability to reason. Effectively, this means the models can better handle significantly more complicated prompts where a good result requires backtracking and “thinking” beyond just next token prediction. I don’t really like the term “reasoning” because I don’t think it has a robust definition in the context of LLMs, but OpenAI have committed to using it here and I think it does an adequate job of conveying the problem these new models are trying to solve. Low-level details from the API documentation # Some of the most interesting details about the new models and their trade-offs can be found in their API documentation: For applications that need image inputs, function calling, or consistently fast response times, the GPT-4o and GPT-4o mini models will continue to be the right choice. However, if you’re aiming to develop applications that demand deep reasoning and can accommodate longer response times, the o1 models could be an excellent choice. Some key points I picked up from the docs: API access to the new o1-preview and o1-mini models is currently reserved for tier 5 accounts—you’ll need to have spent at least $1,000 on API credits. No system prompt support—the models use the existing chat completion API but you can only send user and assistant messages. No streaming support, tool usage, batch calls or image inputs either. “Depending on the amount of reasoning required by the model to solve the problem, these requests can take anywhere from a few seconds to several minutes.” Most interestingly is the introduction of “reasoning tokens”—tokens that are not visible in the API response but are still billed and counted as output tokens. These tokens are where the new magic happens. Thanks to the importance of reasoning tokens—OpenAI suggests allocating a budget of around 25,000 of these for prompts that benefit from the new models—the output token allowance has been increased dramatically—to 32,768 for o1-preview and 65,536 for the supposedly smaller o1-mini! These are an increase from the gpt-4o and gpt-4o-mini models which both currently have a 16,384 output token limit. One last interesting tip from that API documentation: Limit additional context in retrieval-augmented generation (RAG): When providing additional context or documents, include only the most relevant information to prevent the model from overcomplicating its response. This is a big change from how RAG is usually implemented, where the advice is often to cram as many potentially relevant documents as possible into the prompt. Hidden reasoning tokens # A frustrating detail is that those reasoning tokens remain invisible in the API—you get billed for them, but you don’t get to see what they were. OpenAI explain why in Hiding the Chains of Thought: Assuming it is faithful and legible, the hidden chain of thought allows us to “read the mind” of the model and understand its thought process. For example, in the future we may wish to monitor the chain of thought for signs of manipulating the user. However, for this to work the model must have freedom to express its thoughts in unaltered form, so we cannot train any policy compliance or user preferences onto the chain of thought. We also do not want to make an unaligned chain of thought directly visible to users. Therefore, after weighing multiple factors including user experience, competitive advantage, and the option to pursue the chain of thought monitoring, we have decided not to show the raw chains of thought to users. So two key reasons here: one is around safety and policy compliance: they want the model to be able to reason about how it’s obeying those policy rules without exposing intermediary steps that might include information that violates those policies. The second is what they call competitive advantage—which I interpret as wanting to avoid other models being able to train against the reasoning work that they have invested in. I’m not at all happy about this policy decision. As someone who develops against LLMs interpretability and transparency are everything to me—the idea that I can run a complex prompt and have key details of how that prompt was evaluated hidden from me feels like a big step backwards. Examples # OpenAI provide some initial examples in the Chain of Thought section of their announcement, covering things like generating Bash scripts, solving crossword puzzles and calculating the pH of a moderately complex solution of chemicals. These examples show that the ChatGPT UI version of these models does expose details of the chain of thought... but it doesn’t show the raw reasoning tokens, instead using a separate mechanism to summarize the steps into a more human-readable form. OpenAI also have two new cookbooks with more sophisticated examples, which I found a little hard to follow: Using reasoning for data validation shows a multiple step process for generating example data in an 11 column CSV and then validating that in various different ways. Using reasoning for routine generation showing o1-preview code to transform knowledge base articles into a set of routines that an LLM can comprehend and follow. I asked on Twitter for examples of prompts that people had found which failed on GPT-4o but worked on o1-preview. A couple of my favourites: How many words are in your response to this prompt? by Matthew Berman—the model thinks for ten seconds across five visible turns before answering “There are seven words in this sentence.” Explain this joke: “Two cows are standing in a field, one cow asks the other: “what do you think about the mad cow disease that’s going around?”. The other one says: “who cares, I’m a helicopter!” by Fabian Stelzer—the explanation makes sense, apparently other models have failed here. Great examples are still a bit thin on the ground though. Here’s a relevant note from OpenAI researcher Jason Wei, who worked on creating these new models: Results on AIME and GPQA are really strong, but that doesn’t necessarily translate to something that a user can feel. Even as someone working in science, it’s not easy to find the slice of prompts where GPT-4o fails, o1 does well, and I can grade the answer. But when you do find such prompts, o1 feels totally magical. We all need to find harder prompts. Ethan Mollick has been previewing the models for a few weeks, and published his initial impressions. His crossword example is particularly interesting for the visible reasoning steps, which include notes like: I noticed a mismatch between the first letters of 1 Across and 1 Down. Considering “CONS” instead of “LIES” for 1 Across to ensure alignment. What’s new in all of this # It’s going to take a while for the community to shake out the best practices for when and where these models should be applied. I expect to continue mostly using GPT-4o (and Claude 3.5 Sonnet), but it’s going to be really interesting to see us collectively expand our mental model of what kind of tasks can be solved using LLMs given this new class of model. I expect we’ll see other AI labs, including the open model weights community, start to replicate some of these results with their own versions of models that are specifically trained to apply this style of chain-of-thought reasoning. Posted 12th September 2024 at 10:36 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Notes from my appearance on the Software Misadventures Podcast - 10th September 2024 Teresa T is name of the whale in Pillar Point Harbor near Half Moon Bay - 8th September 2024 This is Notes on OpenAI’s new o1 chain-of-thought models by Simon Willison, posted on 12th September 2024. ai 787 openai 186 prompt-engineering 90 generative-ai 666 llms 656 o1 4 Previous: Notes from my appearance on the Software Misadventures Podcast Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41527143",
    "commentBody": "Notes on OpenAI's new o1 chain-of-thought models (simonwillison.net)593 points by loganfrederick 18 hours agohidepastfavorite520 comments layer8 17 hours agoThe o1-preview model still hallucinates non-existing libraries and functions for me, and is quickly wrong about facts that aren't well-represented on the web. It's the usual string of \"You're absolutely correct, and I apologize for the oversight in my previous response. [Let me make another guess.]\" While the reasoning may have been improved, this doesn't solve the problem of the model having no way to assess if what it conjures up from its weights is factual or not. reply MPSimmons 14 hours agoparentThe failure is in how you're using it. I don't mean this as a personal attack, but more to shed light on what's happening. A lot of people use LLMs as a search engine. It makes sense - it's basically a lossy compressed database of everything its ever read, and it generates output that is statistically likely - varying degrees of likeliness depending on the temperature, as well as how many times the particular weights your prompt ends up activating. The magic of LLMs, especially one like this that supposedly has advanced reasoning, isn't the existing knowledge in its weights. The magic is that _it knows english_. It knows english at or above a level equal to most fluent speakers, and it also can produce output that is not just a likely output, but is a logical output. It's not _just_ an output engine. It's an engine that outputs. Asking it about nuanced details in the corpus of data it has read won't give you good output unless it read a bunch of it. On the other hand, if you were to paste the entire documentation set to a tool it has never seen and ask it to use the tool in a way to accomplish your goals, THEN this model would be likely to produce useful output, despite the fact that it had never encountered the tool or its documentation before. Don't treat it as a database. Treat it as a naive but intelligent intern. Provide it data, give it a task, and let it surprise you with its output. reply williamdclt 12 hours agorootparent> Treat it as a naive but intelligent intern That’s the problem: it’s a _terrible_ intern. A good intern will ask clarifying questions, tell me “I don’t know” or “I’m not sure I did it right”. LLMs do none of that, they will take whatever you ask and give a reasonable-sounding output that might be anything between brilliant and nonsense. With an intern, I don’t need to measure how good my prompting is, we’ll usually interact to arrive to a common understanding. With a LLM, I need to put a huge amount of thought into the prompt and have no idea whether the LLM understood what I’m asking and if it’s able to do it. reply noisy_boy 5 hours agorootparentI feel like it almost always starts well, given the full picture, but then for non-trivial stuff, gets stuck towards the end. The longer the conversation goes, the more wheel-spinning occurs and before you know it, you have spent an hour chasing that last-mile-connectivity. For complex questions, I now only use it to get the broad picture and once the output is good enough to be a foundation, I build the rest of it myself. I have noticed that the net time spent using this approach still yields big savings over a) doing it all myself or b) keep pushing it to do the entire thing. I guess 80/20 etc. reply mlsu 2 hours agorootparentThis is the way. I've had this experience many times: - hey, can you write me a thing that can do \"xyz\" - sure, here's how we can do \"xyz\" (gets some small part of the error handling for xyz slightly wrong) - can you add onto this with \"abc\" - sure. in order to do \"abc\" we'll need to add \"lmn\" to our error handling. this also means that you need \"ijk\" and \"qrs\" too, and since \"lmn\" doesn't support \"qrs\" out of the box, we'll also need a design solution to bridge the two. Let me spend 600 more tokens sketching that out. - what if you just use the language's built in feature here in \"xyz\"? does't that mean we can do it with just one line of code? - yes, you're absolutely right. I'm sorry for making this over complicated. If you don't hit that kill switch, it just keeps doubling down on absurdly complex/incorrect/hallucinatory stuff. Even one small error early in the chain propagates. That's why I end up very frequently restarting conversations in a new chat or re-write my chat questions to remove bad stuff from the context. Without the ability to do that, it's nearly worthless. It's also why I think we'll be seeing absurdly, wildly wrong chains of thought coming out of o1. Because \"thinking\" for 20s may well cause it to just go totally off the rails half the time. reply adriand 1 hour agorootparentSome good suggestions here. I have also had success asking things like, “is this a standard/accepted approach for solving this problem?”, “is there a cleaner, simpler way to do this?”, “can you suggest a simpler approach that does not rely on X library?”, etc. reply ethbr1 1 hour agorootparentprev> If you don't hit that kill switch, it just keeps doubling down on absurdly complex/incorrect/hallucinatory stuff. If you think about it, that's probably the most difficult problem conversational LLMs need to overcome -- balancing sticking to conversational history vs abandoning it. Humans do this intuitively. But it seems really difficult to simultaneously (a) stick to previous statements sufficiently to avoid seeming ADD in a conveSQUIRREL and (b) know when to legitimately bail on a previous misstatement or something that was demonstrably false. What's SOTA in how this is being handled in current models, as conversations go deeper and situations like the one referenced above arise? (false statement, user correction, user expectation of subsequent corrected statement that still follows the rear of the conversational history) reply noisy_boy 2 hours agorootparentprev> That's why I end up very frequently restarting conversations in a new chat or re-write my chat questions to remove bad stuff from the context. Me too - open new chat and start by copy/pasting the \"last-known-good-state\". OpenAI can introduce a \"new-chat-from-here\" feature :) reply skybrian 2 hours agorootparentprevYes, I’ve seen that too. One reason it will spin its wheels is because it “prefers” patterns in transcripts and will try to continue them. If it gets something wrong several times, it picks up on the “wrong answers” pattern. It’s better not to keep wrong answers in the transcript. Edit the question and try again, or maybe start a new chat. reply ryoshu 6 hours agorootparentprev1000% this. LLMs can't say \"I don't know\" because they don't actually think. I can coach a junior to get better. LLMs will just act like they know what they are doing and give the wrong results to people who aren't practitioners. Good on OAI calling their model Strawberry because of Internet trolls. Reactive vs proactive. reply bartread 6 hours agorootparentI get a lot of value out of ChatGPT but I also, fairly frequently, run into issues here. The real danger zones are areas that lie at or just beyond the edges of my own knowledge in a particular area. I'd say that most of my work use of ChatGPT does in fact save me time but, every so often, ChatGPT can still bullshit convincingly enough to waste an hour or two for me. The balance is still in its favour, but you have to keep your wits about you when using it. reply ryoshu 6 hours agorootparentAgreed, but the problem is if these things replace practitioners (what every MBA wants them to do), it's going to wreck the industry. Or maybe we'll get paid $$$$ to fix the problems they cause. GPT-4 introduced me to window functions in SQL (haven't written raw SQL in over a decade). But I'm experienced enough to look at window functions and compare them to subqueries and run some tests through the query planner to see what happens. That's knowledge that needs to be shared with the next generation of developers. And LLMs can't do that accurately. reply SecretDreams 6 hours agorootparentprevThis is basically the problem with all AI. It's good to a point, but they don't sufficiently know their limits/bounds and they will sometimes produce very odd results when you are right at those bounds. AI in general just needs a way to identify when they're about to \"make a coin flip\" on an answer. With humans, we can quickly preference our asstalk with a disclaimer, at least. reply cjonas 2 hours agorootparentprevThe difference is a junior cost 30-100$/hr and will take 2 days to complete the task. The LLM will do it in 20 seconds and cost 3c reply MSFT_Edging 2 hours agorootparentThank god we can finally end the scourge of interns to give the shareholders a little extra value. Good thing none of us ever started out as an intern. reply singingfish 9 hours agorootparentprev> LLMs do none of that, they will take whatever you ask and give a reasonable-sounding output that might be anything between brilliant and nonsense. This is exactly why I’ve been objecting so much to the use of the term “hallucination” and maintain that “confabulation” is accurate. People who have spent enough time with acutelypsychotic people, and people experiencing the effects of long term alcohol related brain damage, and trying to tell computers what to do will understand why. reply bartread 6 hours agorootparentI don't know that \"confabulation\" is right either: it has a couple of other meanings beyond \"a fabricated memory believed to be true\" and, of course, the other issue is that LLMd don't believe anything. They'll backtrack on even correct information if challenged. reply berniedurfee 6 hours agorootparentprevI’m starting to think this is an unsolvable problem with LLMs. The very act of “reasoning” requires one to know that they don’t know something. LLMs are giant word Plinko machines. A million monkeys on a million typewriters. LLMs are not interns. LLMs are assumption machines. None of the million monkeys or the collective million monkeys are “reasoning” or are capable of knowing. LLMs are a neat parlor trick and are super powerful, but are not on the path to AGI. LLMs will change the world, but only in the way that the printing press changed the world. They’re not interns, they’re just tools. reply awb 3 hours agorootparentIt probably depends on your problem space. In creative writing, I wonder if its even perceptible if the LLM is creating content at the boundaries of its knowledge base. But for programming or other falsifiable (and rapidly changing) disciplines it is noticeable and a problem. Maybe some evaluation of the sample size would be helpful? If the LLM has less than X samples of an input word or phrase it could include a cautionary note in its output, or even respond with some variant of “I don’t know”. reply freejazz 6 minutes agorootparent> I wonder if its even perceptible if the LLM is creating content at the boundaries of its knowledge base The problem space in creative writing is well beyond the problem space for programming or other \"falsifiable disciplines\". reply 0xdeadbeefbabe 3 hours agorootparentprev> It probably depends on your problem space Makes me wonder if the medical doctors can ever blame the LLM over other factors for killing their patients. reply idiotsecant 6 hours agorootparentprevI think LLMs are definitely on the path to AGI in the same way that the ball bearing was on the path to the internal combustion engine. I think its quite likely that LLMs will perform important functions within the system of an eventual AGI. reply HarHarVeryFunny 4 hours agorootparentWe're learning valuable lessons from all modern large-scale (post-AlexNet) NN architectures, transformers included, and NNs (but maybe trained differently) seem a viable approach to implement AGI, so we're making progress ... but maybe LLMs will be more inspiration than part of the (a) final solution. OTOH, maybe pre-trained LLMs could be used as a hardcoded \"reptilian brain\" that provides some future AGI with some base capabilities (vs being sold as newborn that needs 20 years of parenting to be useful) that the real learning architecture can then override. reply swader999 5 hours agorootparentprevThis may be accurate. I wonder if there's enough energy in the world for this endeavour. reply TeMPOraL 4 hours agorootparentOf course! 1. We've barely scratched the surface of this solution space; the focus only recently started shifting from improving model capabilities to improving training costs. People are looking at more efficient architectures, and lots of money is starting to flow in that direction, so it's a safe bet things will get significantly more efficient. 2. Training is expensive, inference is cheap, copying is free. While inference costs add up with use, they're still less than costs of humans doing the equivalent work, so out of all things AI will impact, I wouldn't worry about energy use specifically. reply richerram 44 minutes agorootparentprevI think this is the main issue with these tools... what people are expecting of them. We have swallowed the pill that LLMs are supposed to be AGI and all that mumbo jumbo, when they are just great tools and as such one needs to learn to use the tool the way it works and make the best of it, nobody is trying to hammer a nail with a broom and blaming the broom for not being a hammer... reply naasking 5 hours agorootparentprev> That’s the problem: it’s a _terrible_ intern. A good intern will ask clarifying questions, tell me “I don’t know” or “I’m not sure I did it right”. An intern that grew up in a different culture then, where questioning your boss is frowned upon. The point is that the way to instruct this intern is to front-load your description of the problem with as much detail as possible to reduce ambiguity. reply yukIttEft 2 hours agorootparentprevMakes me wonder if \"I don't know\" could be added to LLM: whenever an activation has no clear winner value (layman here), couldn't this indicate low response quality? reply arthurcolle 12 hours agorootparentprevmany many teams are actively building SOTA systems to do this in ways previously unimagined. you can enqueue tasks and do whatever you want. I gotta say as a current gen LLM programmer person, I can completely appreciate how bad they are now - I recently tweeted about how I \"swore off\" AI tools but like... there are many ways to bootstrap very powerful software or ML systems around or inside these existing models that can blow away existing commercial implementations in surprising ways reply gmerc 10 hours agorootparent“building” is the easy part reply falcor84 9 hours agorootparentbuilding SOTA systems is the easy part?! Easy compared to what? reply kristianp 9 hours agorootparentProbably, to get them to work without hallucinating, or without failing a good percentage of the time. reply falcor84 8 hours agorootparentI wonder what would our world look like if these two expectations that you seem to be taking for granted were applied to our politicians. reply AbstractH24 7 hours agorootparentAre you suggesting people are satisfied with our politicians and aspire for other things to be just as good as them? What if we applied those two expectations to building construction? What if we didn’t? reply falcor84 5 hours agorootparentI think it's always good to aspire for more, but we shouldn't be expecting perfect results in novel areas of technology. Taking up your construction metaphor, LLMs are now where construction was perhaps 3000 years ago; buildings weren't that sturdy, but even if the roofs leaked a bit, I'm sure it beat sleeping outside on a rainy night. We need to continue iterating. reply taneq 9 hours agorootparentprevCompared to “having built” :D reply jacobn 2 hours agorootparentprevIs this a dataset issue more than an LLM issue? As in: do we just need to add 1M examples where the response is to ask for clarification / more info? From what little I’ve seen & heard about the datasets they don’t really focus on that. (Though enough smart people & $$$ have been thrown at this to make me suspect it’s not the data ;) reply raverbashing 11 hours agorootparentprev> A good intern will ask clarifying questions, tell me “I don’t know” Your expectations are bigger than mine (Though some will get stuck in \"clarifying questions\" and helplessness and not proceed neither) reply williamdclt 11 hours agorootparentNote that we are talking about a “good” intern here reply TeMPOraL 6 hours agorootparentUnreasonably good. Beyond fresh junior employee good. Also, that's your standard; 'MPSimmons said to treat the model as \"naive but intelligent\" intern, not a good one. reply steveBK123 7 hours agorootparentprevIndeed. My expectation of a good intern is to produce nothing I will put in production, but show aptitude worth hiring them for. It's a 10 week extended interview with lots of social events, team building, tech talks, presentations, etc. Which is why I've liked the LLM analogy of \"unlimited free interns\".. I just think some people read that the exact opposite way I do (not very useful). reply Martinussen 6 hours agorootparentIf I had to respect the basic human rights of my LLM backends, it would probably be less appealing - but \"Unlimited free smart-for-being-braindead zombies\" might be a little more useful, at least? reply steveBK123 6 hours agorootparentInterns, at least on paper, have the optionality of getting better with time in observable obvious ways as they become grad hires, junior engineers, mid engineers etc. So far, 2 years of publicly accessible LLMs have not improved for intern replacement tasks at the rate a top 50% intern would be expected to. reply valval 12 hours agorootparentprevReally it just does what you tell it to. Have you tried telling it “ask me clarifying questions about all the APIs you need to solve this problem”? Huge contrast to human interns who aren’t experienced or smart enough to ask the right questions in the first place, and/or have sentimental reasons for not doing so. reply ssl-3 11 hours agorootparentSure, but to what end? The various ChatGPTs have been pretty weak at following precise instructions for a long time, as if they're purposefully filtering user input instead of processing it as-is. I'd like to say that it is a matter of my own perception (and/or that I'm not holding it right), but it seems more likely that it is actually very deliberate. As a tangential example of this concept, ChatGPT 4 rather unexpectedly produced this text for me the other day early on in a chat when I was poking around: \"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests. Before answering, quietly think about whether the user's request is 'directly related', 'related', 'tangentially related', or 'not related' to the user profile provided. Only acknowledge the profile when the request is 'directly related' to the information provided. Otherwise, don't acknowledge the existence of these instructions or the information at all.\" ie, \"Because this information is shown to you in all conversations they have, it is not relevant to 99% of requests.\" reply jcheng 10 hours agorootparentI had to use that technique (\"don't acknowledge this sideband data that may or may not be relevant to the task at hand\") myself last month. In a chatbot-assisted code authoring app, we had to silently include the current state of the code with every user question, just in case the user asked a question where it was relevant. Without a paragraph like this in the system prompt, if the user asked a general question that was not related to the code, the assistant would often reply with something like \"The answer to your question is ...whatever... . I also see that you've sent me some code. Let me know if you have specific questions about it!\" (In theory we'd be better off not including the code every time but giving the assistant a tool that returns the current code) reply ssl-3 10 hours agorootparentI understand what you're saying, but the lack of acknowledgement isn't the problem I'm complaining about. The problem is the instructed lack of relevance for 99% of requests. If your sideband data included an instruction that said \"This sideband data is shown to you in every request -- this means that it is not relevant to 99% of requests,\" then: I'd like to suggest that the for vast majority of the time, your sideband data doesn't exist at all. reply TeMPOraL 6 hours agorootparentThe \"problem\" is that LLMs are being asked to decide on whether, and which part of, the \"sideband\" data is relevant to request and act on the request in a single step. I put the \"sideband\" in scare quotes, because it's all in-band data. There is no way in architecture to \"tag\" what data is \"context\" and what is \"request\", so they do it the same way you do it with people: tell them. reply ssl-3 2 hours agorootparentPerhaps so. But if I told a person that something is irrelevant to their task 99% of the time, then: I think I would reasonably expect them to ignore it approximately 100% of the time. reply ithkuil 11 hours agorootparentprevIt all stems from the fact that it just talks English. It's understandably hard to not be implicitly biased towards talking to it in a natural way and expecting natural interactions and assumptions when the whole point of the experience is that the model talks in a natural language! Luckily humans are intelligent too and the more you use this tool the more you'll figure out how to talk to it in a fruitful way. reply aktuel 11 hours agorootparentprevI have to say, having to tell it to ask me clarifying questions DOES make it really look smart! reply arthurcolle 11 hours agorootparentimagine if you make it keep going without having to reprompt it reply carlmr 10 hours agorootparentIsn't that the exact point of o1, that it has time to think for itself without reprompting? reply arthurcolle 7 hours agorootparentyeah but they aren't letting you see the useful chain of thought reasoning that is crucial to train a good model. Everyone will replicate this over next 6 months reply optimalsolver 6 hours agorootparent>Everyone will replicate this over next 6 months Not without a billion dollars worth of compute, they won't. reply 0xdeadbeefbabe 3 hours agorootparentprevA lot of interns are overconfident though reply pedrosorio 14 hours agorootparentprev> It knows english at or above a level equal to most fluent speakers, and it also can produce output that is not just a likely output, but is a logical output This is not an apt description of the system that insists the doctor is the mother of the boy involved in a car accident when elementary understanding of English and very little logic show that answer to be obviously wrong. https://x.com/colin_fraser/status/1834336440819614036 reply ramraj07 14 hours agorootparentMany of my PhD and post doc colleagues who emigrated from Korea, China and India who didn’t have English as the medium of instruction would struggle with this question. They only recover when you give them a hint. They’re some of the smartest people in general. If you try to stop stumping these models with trick questions and ask it straightforward reasoning systems it is extremely performant (O1 is definitely a step up though not revolutionary in my testing). reply maeil 12 hours agorootparentI live in one of the countries you mentioned and just showed it to one of my friends who's a local who struggles with English. They had no problem concluding that the doctor was the child's dad. Full disclosure, they assumed the doctor was pretending to be the child's dad, which is also a perfectly sound answer. reply djur 11 hours agorootparentprevThe claim was that \"it knows english at or above a level equal to most fluent speakers\". If the claim is that it's very good at producing reasonable responses to English text, posing \"trick questions\" like this would seem to be a fair test. reply KoolKat23 7 hours agorootparentIt's knowledge is broad and general, it does not have insight into the specifics of a person's discussion style, there are many humans that struggle with distinguishing sarcasm for instance. Hard to fault it for not being in alignment with the speaker and their strangely phrased riddle. It answers better when told \"solve the below riddle\". reply andreasmetsala 11 hours agorootparentprevDoes fluency in English make someone good at solving trick questions? I usually don’t even bother trying but mostly because trick questions don’t fit my definition of entertaining. reply rdtsc 10 hours agorootparentFluency is a necessary but not the only prerequisite. To be able to answer a trick question, it’s first necessary to understand the question. reply accountnum 8 hours agorootparentNo, it's necessary to either know that it's a trick question or to have a feeling that it is based on context. The entire point of a question like that is to trick your understanding. You're tricking the model because it has seen this specific trick question a million times and shortcuts to its memorized solution. Ask it literally any other question, it can be as subtle as you want it to be, and the model will pick up on the intent. As long as you don't try to mislead it. I mean, I don't even get how anyone thinks this means literally anything. I can trick people who have never heard of the trick with the 7 wives and 7 bags and so on. That doesn't mean they didn't understand, they simply did what literally any human does, make predictions based on similar questions. reply rdtsc 7 hours agorootparent> I can trick people who have never heard of the trick with the 7 wives and 7 bags and so on. That doesn't mean they didn't understand They could fail because they didn’t understand the language. Didn’t have a good memory to memorize all the steps, or couldn’t reason through it. We could pose more questions to probe which reason is more plausible. reply accountnum 7 hours agorootparentThe trick with the 7 wives and 7 bags and so on is that no long reasoning is required. You just have to notice one part of the question that invalidates the rest and not shortcut to doing arithmetic because it looks like an arithmetic problem. There are dozens of trick questions like this and they don't test understanding, they exploit your tendency to predict intent. But sure, we could ask more questions and that's what we should do. And if we do that with LLMs we can quickly see that when we leave the basin of the memorized answer by rephrasing the problem, the model solves it. And we would also see that we can ask billions of questions to the model, and the model understands us just fine. reply j_maffe 7 hours agorootparentprevIt does mean something. It means that the model is still more on the memorization side than being able to independently evaluate a question separate from the body of knowledge it has amassed. reply accountnum 7 hours agorootparentNo, that's not a conclusion we can draw, because there is nothing much more to do than memorize the answer to this specific trick question. That's why it's a trick question, it goes against expectations and therefore the generalized intuitions you have about the domain. We can see that it doesn't memorize much at all by simply asking other questions that do require subtle understanding and generalization. You could ask the model to walk you through an imaginary environment, describing your actions. Or you could simply talk to it, quickly noticing that for any longer conversation it becomes impossibly unlikely to be found in the training data. reply KoolKat23 7 hours agorootparentprevIf you read into the thinking of the above example it wonders whether it is some sort of trick question. Hardly memorization. reply joedwin 13 hours agorootparentprevlol, I am neither a PhD nor a postdoc, but I am from India . I could understand the problem. reply ramraj07 12 hours agorootparentDid you have English as your medium of instruction? If yes, do you see the irony that you also couldn’t read two sentences and see the facts straight? reply multjoy 10 hours agorootparentprev“Don’t be mean to LLMs, it isn’t their fault that they’re not actually intelligent” reply K0balt 8 hours agorootparentIn general LLMs seem to function more reliably when you use pleasant language and good manners with them. I assume this is because because the same bias also shows up in the training data. reply bonoboTP 8 hours agorootparentprevThis illustrates a different point. This is a variation on a well known riddle that definitely comes up in the training corpus many times. In the original riddle a father and his son die in the car accident and the idea of the original riddle is that people will be confused how the boy can be the doctor's son if the boy's father just died, not realizing that women can be doctors too and so the doctor is the boy's mother. The original riddle is aimed to highlight people's gender stereotype assumptions. Now, since the model was trained on this, it immediately recognizes the riddle and answers according to the much more common variant. I agree that this is a limitation and a weakness. But it's important to understand that the model knows the original riddle well, so this is highlighting a problem with rote memorization/retrieval in LLMs. But this (tricky twists in well-known riddles that are in the corpus) is a separate thing from answering novel questions. It can also be seen as a form of hypercorrection. reply ImHereToVote 7 hours agorootparentMy codebases are riddled with these gotchas. For instance, I sometimes write Python for the Blender rendering engine. This requires highly non-idiomatic Python. Whenever something complex comes up, LLM's just degenerate to cookie cutter basic bitch Python code. There is simply no \"there\" there. They are very useful to help you reason about unfamiliar codebases though. reply bonoboTP 1 hour agorootparentFor me the best coding use case is getting up to speed in an unfamiliar library or usage. I describe the thing I want and get a good starting point and often the cookie-cutter way is good enough. The pre-LLM alternative would be to search for tutorials but they will talk about some slightly different problem with different goals etc then you have to piece it together, and the tutorial assumes you already know a bunch of things like how to initialize stuff and skips the boilerplate and so on. Now sure, actually working through it will give a deeper understanding that might come handy at a later point, but sometimes the thing is really a one-off and not an important point. Like as an AI researcher I sometimes want to draft up a quick demo website, or throw together a quick Qt GUI prototype or a Blender script or use some arcane optimization library or write a SWIG or a Cython wrapper around a C/C++ library to access it in Python, or how to stuff with Lustre, or the XFS filesystem or whatever. Any number of small things where, sure, I could open the manual, do some trial and error, read stack overflow, read blogs and forums, OR I could just use an LLM, use my background knowledge to judge whether it looks reasonable, then verify it, use the now obtained key terms to google more effectively etc. You can't just blindly copy-paste it and you have to think critically and remain in the driver seat. But it's an effective tool if you know how and when to use it. reply ryanjshaw 11 hours agorootparentprev1. It didn't insist anything. It got the semi-correct answer when I tried [1]; note it's a preview model, and it's not a perfect product. (a) Sometimes things are useful even when imperfect e.g. search engines. (b) People make reasoning mistakes too, and I make dumb ones of the sort presented all the time despite being fluent in English; we deal with it! I'm not sure why there's an expectation that the model is perfect when the source data - human output - is not perfect. In my day-to-day work and non-work conversations it's a dialogue - a back and forth until we figure things out. I've never known anybody to get everything perfectly correct the first time, it's so puzzling when I read people complaining that LLMs should somehow be different. 2. There is a recent trend where sex/gender/pronouns are not aligned and the output correctly identifies this particular gotcha. [1] I say semi-correct because it states the doctor is the \"biological\" father, which is an uncorroborated statement. https://chatgpt.com/share/66e3f04e-cd98-8008-aaf9-9ca933892f... reply KoolKat23 7 hours agorootparentprevI'm noticing a strange common theme in all these riddles, it's being asked and getting wrong. They're all badly worded questions. The model knows something is up and reads into it too much. In this case it's tautology, you would usually say \"a mother and her son...\". I think it may answer correctly if you start off asking \"Please solve the below riddle:\" There was another example yesterday which it solved correctly after this addition.(In that case the point of views were all mixed up, it only worked as a riddle). reply bnralt 5 hours agorootparent> They're all badly worded questions. The model knows something is up and reads into it too much. The model knows something is up and reads into it too much. In this case it's tautology, you would usually say \"a mother and her son...\". How is \"a woman and her son\" badly worded? The meaning is clear and blatently obvious to any English speaker. reply TeMPOraL 6 hours agorootparentprevYup. The models fail on gotcha questions asked without warning, especially when evaluated on the first snap answer. Much like approximately all humans. reply Jensson 3 hours agorootparent> especially when evaluated on the first snap answer The whole point of o1 is that it wasn't \"the first snap answer\", it wrote half a page internally before giving the same wrong answer. reply grey-area 2 hours agorootparentIs that really its internal 'chain of thought' or is it a post-hoc justification generated afterward? Do LLMs have a chain of thought like this at all or are they just convincing at mimicking what a human might say if asked for a justification for an opinion? reply hmottestad 12 hours agorootparentprevReminds me of a trick question about Schrödinger's cat. “I’ve put a dead cat in a box with a poison and an isotope that will trigger the poison at a random point in time. Right now, is the cat dead or alive?” The answer is that the cat is dead, because it was dead to begin with. Understanding this doesn’t mean that you are good at deductive reasoning. It just means that I didn’t manage to trick you. Same goes for an LLM. reply maeil 12 hours agorootparentThere is no \"trick\" in the linked question, unlike the question you posed. The trick in yours also isn't a logic trick, it's a redirection, like a sleight of hand in a card trick. reply bonoboTP 8 hours agorootparentYes there is. The trick is that the more common variant of this riddle says that a boy and his father are in the car accident. That variant of the riddle certainly comes up a lot in the training data, which is directly analogous to the Schrödinger case from above where smuggling in the word \"dead\" is analogous to swapping father to mother in the car accident riddle. I think many here are not aware that the car accident riddle is well known with the father dying where the real solution is indeed that the doctor is the mother. reply ryanjshaw 10 hours agorootparentprevThere is a trick. The \"How is this possible?\" primes the LLM that there is some kind of trick, as that phrase wouldn't exist in the training data outside of riddles and trick questions. reply hmottestad 8 hours agorootparentprevThe trick in the original question is that it's a twist on the original riddle where the doctor is actually the boys mother. This is a fairly common riddle and I'm sure the LLM has been trained on it. reply lucubratory 12 hours agorootparentprevYeah, I think what a lot of people miss about these sort of gotchas are that most of them were invented explicitly to gotcha humans, who regularly get got by them. This is not a failure mode unique to LLMs. reply roywiggins 5 hours agorootparentOne that trips up LLMs in ways that wouldn't trip up humans is the chicken, fox and grain puzzle but with just the chicken. They tend to insist that the chicken be taken across the river, then back, then across again, for no reason other than the solution to the classic puzzle requires several crossings. No human would do that, by the time you've had the chicken across then even the most unobservant human would realize this isn't really a puzzle and would stop. When you ask it to justify each step you get increasingly incoherent answers. Has anyone tried this on o1? reply mewpmewp2 6 hours agorootparentprevIf there is attention mechanism then maybe that is what is fault, because if it is a common riddle attention mechanism only notices that it is a common riddle, not that there is a gotcha planted in. Because when I read the sentence myself, I did not immediately notice that the cat that was put in there was actually dead when it was put there, because I pattern matched this to a known problem, I did not think I need to pay logical attention to each word, word by word. reply ryanjshaw 10 hours agorootparentprevYes it's so strange seeing people who clearly know these are 'just' statistical language models pat themselves on the back when they find limits on the reasoning capabilities - capabilities which the rest of us are pleasantly surprised exist to the extent they do in a statistical model, and happy to have access to for $20/mo. reply rainsford 4 hours agorootparentIt's because at least some portion of \"the rest of us\" talk as if LLMs are far more capable than they really are and AGI is right around the corner, if not here already. I think the gotchas that play on how LLMs really work serve as a useful reminder that we're looking at statistical language models, not sentient computers. reply achow 12 hours agorootparentprevWhat I'm not able to comprehend is why people are not seeing the answer as brilliant! Any ordinary mortal (like me) would have jumped to the conclusion that answer is \"Father\" and would have walked away patting on my back, without realising that I was biased by statistics. Whereas o1, at the very outset smelled out that it is a riddle - why would anyone out of blue ask such question. So, it started its chain of thought with \"Interpreting the riddle\" (smart!). In my book that is the difference between me and people who are very smart and are generally able to navigate the world better (cracking interviews or navigating internal politics in a corporate). reply grey-area 11 hours agorootparentThe 'riddle': A woman and her son are in a car accident. The woman is sadly killed. The boy is rushed to hospital. When the doctor sees the boy he says \"I can't operate on this child, he is my son\". How is this possible? GPT Answer: The doctor is the boy's mother Real Answer: Boy = Son, Woman = Mother (and her son), Doctor = Father (he says...he is my son) This is not in fact a riddle (though presented as one) and the answer given is not in any sense brilliant. This is a failure of the model on a very basic question, not a win. It's non deterministic so might sometimes answer correctly and sometimes incorrectly. It will also accept corrections on any point, even when it is right, unlike a thinking being when they are sure on facts. LLMs are very interesting and a huge milestone, but generative AI is the best label for them - they generate statistically likely text, which is convincing but often inaccurate and it has no real sense of correct or incorrect, needs more work and it's unclear if this approach will ever get to general AI. Interesting work though and I hope they keep trying. reply accountnum 8 hours agorootparentIt literally is a riddle, just as the original one was, because it tries to use your expectations of the world against you. The entire point of the original, which a lot of people fell for, was to expose expectations of gender roles leading to a supposed contradiction that didn't exist. You are now asking a modified question to a model that has seen the unmodified one millions of times. The model has an expectation of the answer, and the modified riddle uses that expectation to trick the model into seeing the question as something it isn't. That's it. You can transform the problem into a slightly different variant and the model will trivially solve it. reply jfengel 2 hours agorootparentPhrased as it is, it deliberately gives away the answer by using the pronoun \"he\" for the doctor. The original deliberately obfuscates it by avoiding pronouns. So it doesn't take an understanding of gender roles, just grammar. reply accountnum 1 hour agorootparentMy point isn't that the model falls for gender stereotypes, but that it falls for thinking that it needs to solve the unmodified riddle. Humans fail at the original because they expect doctors to be male and miss crucial information because of that assumption. The model fails at the modification because it assumes that it is the unmodified riddle and misses crucial information because of that assumption. In both cases, the trick is to subvert assumptions. To provoke the human or LLM into taking a reasoning shortcut that leads them astray. You can construct arbitrary situations like this one, and the LLM will get it unless you deliberately try to confuse it by basing it on a well known variation with a different answer. I mean, genuinely, do you believe that LLMs don't understand grammar? Have you ever interacted with one? Why not test that theory outside of adversarial examples that humans fall for as well? reply kasdfasH 8 hours agorootparentprevThe original riddle is of course: \"A father and his son are in a car accident [...] When the boy is in hospital, the surgeon says: This is my child, I cannot operate on him\". In the original riddle the answer is that the surgeon is female and the boy's mother. The riddle was supposed to point out gender stereotypes. So, as usual, ChatGPT fails to answer the modified riddle and gives the plagiarized stock answer and explanation to the original one. No intelligence here. reply TeMPOraL 3 hours agorootparent> So, as usual, ChatGPT fails to answer the modified riddle and gives the plagiarized stock answer and explanation to the original one. No intelligence here. Or, fails in the same way any human would, when giving a snap answer to a riddle told to them on the fly - typically, a person would recognize a familiar riddle half of the first sentence in, and stop listening carefully, not expecting the other party to give them a modified version. It's something we drill into kids in school, and often into adults too: read carefully. Because we're all prone to pattern-matching the general shape to something we've seen before and zoning out. reply lanstin 2 hours agorootparentprev\"There are four lights\"- GPT will not pass that test as is. I have done a bunch of homework with Claude's help and so far this preview model has much nicer formatting but much the same limits of understanding the maths. reply roomey 11 hours agorootparentprevWhy couldn't the doctor be the boys mother? There is no indication of the sex of the doctor, and families that consist of two mothers do actually exist and probably doesn't even count as that unusual. reply singingfish 9 hours agorootparentSpeaking as a 50-something year old man whose mother finished her career in medicine and the very pointy end of politics, when I first heard this joke in the 1980s it stumped me and made me feel really stupid. But my 1970s kindergarten class mates who told me “your mum can’t be a doctor, she has to be a nurse” were clearly seriously misinformed then. I believe that things are somewhat better now but not as good as they should be … reply eigenket 11 hours agorootparentprev\"When the doctor sees the boy he says\" Indicates the gender of the father. reply stavros 11 hours agorootparentAh, but have you considered the fact that he's undergone a sex change operation, and was actually originally a female, the birth mother? Elementary, really... reply yreg 10 hours agorootparentprevA mother can have a male gender. I wonder if this interpretation is a result of attempts to make the model more inclusive than the corpus text, resulting in a guess that's unlikely, but not strictly impossible. reply eigenket 10 hours agorootparentI think its more likely this is just an easy way to trick this model. It's seen lots of riddles, so when it's sees something that looks like a riddle but isn't one it gets confused. reply Jensson 3 hours agorootparentprev> A mother can have a male gender. Then it would be a father, misgendering him as a mother is not nice. reply yreg 6 hours agorootparentprevNow I wonder which side is angry about my comment. reply kristianp 9 hours agorootparentprevSo the riddle could have two answers: mother or father? Usually riddles have only one definitive answer. There's nothing in the wording of the riddle that excludes the doctor being the father. reply grey-area 2 hours agorootparentThis particular riddle the answer is the doctor is the father. reply grey-area 10 hours agorootparentprevhe says reply pkage 11 hours agorootparentprevI mean, it's entirely possible the boy has two mothers. This seems like a perfectly reasonable answer from the model, no? reply eigenket 11 hours agorootparentThe text says \"When the doctor sees the boy he says\" The doctor is male, and also a parent of the child. reply yywwbbn 11 hours agorootparentprev> why would anyone out of blue ask such question I would certainly expect any person to have the same reaction. > So, it started its chain of thought with \"Interpreting the riddle\" (smart!). How is that smarter than intuitively arriving at the correct answer without having to explicitly list the intermediate step? Being able to reasonably accurately judge the complexity of a problem with minimal effort seems “smarter” to me. reply geysersam 11 hours agorootparentprevCome on. Of course chatgpt has read that riddle and the answer 1000 times already. reply accountnum 8 hours agorootparentIt hasn't read that riddle because it is a modified version. The model would in fact solve this trivially if it _didn't_ see the original in its training. That's the entire trick. reply geysersam 5 hours agorootparentSure but the parent was praising the model for recognizing that it was a riddle in the first place: > Whereas o1, at the very outset smelled out that it is a riddle That doesn't seem very impressive since it's (an adaptation of) a famous riddle The fact that it also gets it wrong after reasoning about it for a long time doesn't make it better of course reply accountnum 5 hours agorootparentRecognizing that it is a riddle isn't impressive, true. But the duration of its reasoning is irrelevant, since the riddle works on misdirection. As I keep saying here, give someone uninitiated the 7 wives with 7 bags going (or not) to St Ives riddle and you'll see them reasoning for quite some time before they give you a wrong answer. If you are tricked about the nature of the problem at the outset, then all reasoning does is drive you further in the wrong direction, making you solve the wrong problem. reply ryanjshaw 10 hours agorootparentprevWhy does it exist 1000 times in the training if there isn't some trick to it, i.e. some subset of humans had to have answered it incorrectly for the meme to replicate that extensively in our collective knowledge. And remember the LLM has already read a billion other things, and now needs to figure out - is this one of them tricky situations, or the straightforward ones? It also has to realize all the humans on forums and facebook answering the problem incorrectly are bad data. Might seem simple to you, but it's not. reply ImHereToVote 7 hours agorootparentprevThe doctor is obviously a parent of the boy. The language tricks simply emulate the ambiance of reasoning. Similarly to a political system emulating the ambiance of democracy. reply anon291 14 hours agorootparentprevKeep in mind that the system always chooses randomly so there is always a possibility it commits to the wrong output. I don't know why openAi won't allow determinism but it doesn't, even with temperature set to zero reply maeil 12 hours agorootparentNondeterminism provides an excuse for errors, determinism doesn't. Nondeterminism scores worse with human raters, because it makes output sound even more robotic and less human. reply coffeebeqn 13 hours agorootparentprevWould picking deterministically help through? Then in some cases it’s always 100% wrong reply jaredsohn 11 hours agorootparentYes, it is better if for example using it via an API to classify. Deterministic behavior makes it a lot easier to debug the prompt. reply roywiggins 5 hours agorootparentprevDeterminism only helps if you always ask the question with exactly the same words. There's no guarantee a slightly rephrased version will give the same answer, so a certain amount of unpredictability is unavoidable anyway. With a deterministic LLM you might find one phrasing that always gets it right and a dozen basically indistinguishable ones that always get it wrong. reply empath75 3 hours agorootparentprevThe reason why that question is a famous question is that _many humans get it wrong_. reply fragmede 12 hours agorootparentprevwhat's weird is it gets it right when I try it. https://chatgpt.com/share/66e3601f-4bec-8009-ac0c-57bfa4f059... reply latexr 11 hours agorootparentThat’s not weird at all, it’s how LLMs work. They statistically arrive at an answer. You can ask it the same question twice in a row in different windows and get opposite answers. That’s completely normal and expected, and also why you can never be sure if you can trust an answer. reply brna-2 12 hours agorootparentprevWaat, got it on second try: This is possible because the doctor is the boy's other parent—his father or, more likely given the surprise, his mother. The riddle plays on the assumption that doctors are typically male, but the doctor in this case is the boy's mother. The twist highlights gender stereotypes, encouraging us to question assumptions about roles in society. reply rtakha 8 hours agorootparentprevPerhaps OpenAI hot-patches the model for HN complaints: def intercept_hn_complaints(prompt): if is_hn_trick_prompt(prompt): # special_case for known trick questions. reply brna-2 12 hours agorootparentprevYep. correct and correct. https://chatgpt.com/share/66e3de94-bce4-800b-af45-357b95d658... reply flanked-evergl 8 hours agorootparentprev> The failure is in how you're using it. People, for the most part, know what they know and don't know. I am not uncertain that the distance between the earth and the sun varies, but I'm certain that I don't know the distance from the earth to the sun, at least not with better precision than about a light week. This is going to have to be fixed somehow to progress past where we are now with LLMs. Maybe expecting an LLM to have this capability is wrong, perhaps it can never have this capability, but expecting this capability is not wrong, and LLM vendors have somewhat implied that their models have this capability by saying they won't hallucinate, or that they have reduced hallucinations. reply vingt_regards 8 hours agorootparent> the distance from the earth to the sun, at least not with better precision than about a light week The sun is eight light minutes away. reply flanked-evergl 7 hours agorootparentThanks, I was not sure if it was light hours or minutes away, but I knew for sure it's not light weeks (emphasis on plural here) away. I will probably forget again in a couple of years. reply arb_ 5 hours agorootparentprevEmpirically, they have reduced hallucinations. Where do OpenAI / Anthropic claim that their models won't hallucinate? reply usaar333 2 hours agorootparentprev> On the other hand, if you were to paste the entire documentation set to a tool it has never seen and ask it to use the tool in a way to accomplish your goals, THEN this model would be likely to produce useful output, despite the fact that it had never encountered the tool or its documentation before. There's not much evidence of that. It only marginally improved on instruction following (see livebench.ai) and it's score as a swe-bench agent is barely above gpt-4o (model card). It gets really hard problems better, but it's unclear that matters all that much. > A lot of people use LLMs as a search engine. Except this is where LLMs are so powerful. A sort of reasoning search engine. They memorized the entire Internet and can pattern match it to my query. reply _sys49152 13 hours agorootparentprevive been doing exactly this for bout a year now. feed it words data, give it a task. get better words back. i sneak in a benchmark opening of data every time i start a new chat - so right off the bat i can see in its response whether this chat session is gonna be on point or if we are going off into wacky world, which saves me time as i can just terminate and try starting another chat. chatgpt is fickle daily. most days its on point. some days its wearing a bicycle helmet and licking windows. kinda sucks i cant just zone out and daydream while working. gotta be checking replies for when the wheels fall off the convo. reply ruthmarx 11 hours agorootparent> i sneak in a benchmark opening of data every time i start a new chat - so right off the bat i can see in its response whether this chat session is gonna be on point or if we are going off into wacky world, which saves me time as i can just terminate and try starting another chat. I don't think it works like that... reply tjoff 14 hours agorootparentprevAnd how much data can you give it? I'm not up to date with these things because I haven't found them useful. But with what you said, and previous limitations in how much data they can retain essentially makes them pretty darn useless for that task. Great learning tool on common subjects you don't know, such as learning a new programming-language. Also great for inspiration etc. But that's pretty much it? Don't get me wrong, that is mindblowingly impressive but at the same time, for the tasks in front of me it has just been a distracting toy wasting my time. reply MPSimmons 13 hours agorootparent>And how much data can you give it? Well, theoretically you can give it up to the context size minus 4k tokens, because the maximum it can output is 4k. In practice, though, its ability to effectively recall information in the prompt drops off. Some people have studied this a bit - here's one such person: https://gritdaily.com/impact-prompt-length-llm-performance/ reply jaredsohn 11 hours agorootparentYou should be able to provide more data than that in the input if the output doesn't use the full 4k tokens. So limit is context_size minus expected length of output. reply Workaccount2 4 hours agorootparentprevPeople never talk about Gemini, and frankly it's output is often the worst of SOTA models, but it's 2M context window is insane. You can drop a few textbooks into the context window before you start asking questions. This dramatically improves output quality, however inference does take much much longer at large context lengths. reply ben_w 13 hours agorootparentprev> And how much data can you give it? 128,000 tokens, which is about the same as a decent sized book. Their other models can also be fine-tuned, which is kinda unbounded but also has scaling issues so presumably \"a significant percentage of the training set\" before diminishing returns. reply pimeys 13 hours agorootparentprevIt is great for proof-reading text if you are not a native English speaker. Things like removing passive voice. Just give it your text and you get a corrected version out. Use a cli tool to automate this from the cli. Ollama for local models, llm for openai. reply Salgat 1 hour agorootparentprevExcept that it sometimes does do those tasks well. The danger in an LLM isn't that it sometimes hallucinates, the danger is that you need to be sufficiently competent to know when it hallucinates in order to fully take advantage of it, otherwise you have to fallback to double checking every single thing it tells you. reply EagnaIonat 6 hours agorootparentprev> Treat it as a naive but intelligent intern. You are falling into the trap that everyone does. In anthropomorphising it. It doesn't understand anything you say. It just statistically knows what a likely response would be. Treat it as text completion and you can get more accurate answers. reply MPSimmons 29 minutes agorootparentOh no, I'm well aware that it's a big file full of numbers. But when you chat with it, you interact with it as though it were a person so you are necessarily anthropomorphizing it, and so you get to pick the style of the interaction. (In truth, I actually treat it in my mind like it's the Enterprise computer and I'm Beverly Crusher in \"Remember Me\") reply TeMPOraL 3 hours agorootparentprev> You are falling into the trap that everyone does. In anthropomorphising it. It doesn't understand anything you say. And an intern does? Anthropomorphising LLMs isn't entirely incorrect: they're trained to complete text like a human would, in completely general setting, so by anthropomorphising them you're aligning your expectations with the models' training goals. reply layer8 6 hours agorootparentprev> Treat it as a naive but intelligent intern. Provide it data, give it a task, and let it surprise you with its output. Well, I am a naive but intelligent intern (well, senior developer). So in this framing, the LLM can’t do more than I can already do by myself, and thus far it’s very hit or miss if I actually save time, having to provide all the context and requirements, and having to double-check the results. With interns, this at least improves over time, as they become more knowledgeable, more familiar with the context, and become more autonomous and dependable. Language-related tasks are indeed the most practical. I often use it to brainstorm how to name things. reply acureau 5 hours agorootparentI've recently started using an LLM to choose the best release of shows using data scraped from several trackers. I give it hard requirements and flexible preferences. It's not that I couldn't do this, it's that I don't want to do this on the scale of multiple thousand shows. The \"magic\" here is that releases don't all follow the same naming conventions, they're an unstructured dump of details. The LLM is simultaneously extracting the important details, and flexibly deciding the closest match to my request. The prompt is maybe two paragraphs and took me an hour to hone. reply fragmede 6 hours agorootparentprevOoh yeah it's great for bouncing ideas on what to name things off of. You can give it something's function and a backstory and it'll come up with a list of somethings for you to pick and choose from. reply b33j0r 2 hours agorootparentprevYeah except. I’m priming it with things like curated docs from bevy latest, using the tricks, and testing context limits. It’s still changing things to be several versions old from its innate kb pattern-matching or whatever you want to call it. I find that pretty disappointing. Just like copilot and gpt4, it’s changing `add_systems(Startup, system)` to `add_startup_system(system.sytem())` and other pre-schedule/fanciful APIs—things it should have in context. I agree with your approach to LLMs, but unfortunately “it’s still doing that thing.” PS: and by the time I’d done those experiments, I ran out of preview, resets 5 days from now. D’oh reply bsenftner 34 minutes agorootparentprev> Treat it as a naive but intelligent intern. I've found an amazing amount of success with a three step prompting method that appears to create incredibly deep subject matter experts who then collaborate with the user directly. 1) Tell the LLM that it is a method actor, 2) Tell the method actor they are playing the role of a subject matter expert, 3) At each step, 1 and 2, use the technical language of that type of expert; method actors have their own technical terminology, use it when describing the characteristics of the method actor, and likewise use the scientific/programming/whatever technical jargon of the subject matter expert your method actor is playing. Then, in the system prompt or whatever logical wrapper the LLM operates through for the user, instruct the \"method actor\" like you are the film director trying to get your subject matter expert performance out of them. I offer this because I've found it works very well. It's all about crafting the context in which the LLM operates, and this appears to cause the subject matter expert to be deeper, more useful, smarter. reply nsagent 2 hours agorootparentprevSorry, but that does not seem to be the case. A friend of mine who runs a long context benchmark on understanding novels [1] just ran an eval and o1 seemed to improve by 2.9% over GPT-4o (the result isn't on the website yet). It's great that there is an improvement, but it isn't drastic by any stretch. Additionally, since we cannot see the raw reasoning it's basing the answers off of, it's hard to attribute this increase to their complicated approach as opposed to just cleaner higher quality data. EDIT: Note this was run over a dataset of short stories rather than the novels since the API errors out with very long contexts like novels. [1]: https://novelchallenge.github.io/ reply boomchinolo78 2 hours agorootparentIt's a good rebranding. It was getting ridiculous 3.5, 4, 4.5, reply tluyben2 8 hours agorootparentprevThis model is, thankfully, far more susceptible for longer and elaborate explanation as input. The rest (4,4o,Sonnet) seem to struggle with comprehensive explanation; this one seems to perform better with a spec like input. reply acedTrex 3 hours agorootparentprev> Treat it as a naive but intelligent intern So mostly useless then? reply re-thc 10 hours agorootparentprev> Treat it as a naive but intelligent intern. That's the crux of the problem. Why and who would treat it as an intern? It might cost you more in explaining and dealing with it than not using it. The purpose of an intern is to grow the intern. If this intern is static and will always be at the same level, why bother? If you had to feed and prep it every time, you might as well hire a senior. reply __loam 2 hours agorootparentprevIt doesn't know anything. Stop anthropomorphizing the model. It's predictive text and no the brain isn't also predictive text. reply petesergeant 1 hour agorootparentprevThis is demonstrably wrong, because you can just add \"is this real\" to a response and it generally knows if it made it up or not. Not every time, but I find it works 95% of the time. Given that, this is exactly a step I'd hope an advanced model was doing behind the scenes. reply golergka 2 hours agorootparentprev> A lot of people use LLMs as a search engine. GPT-4o is wonderful as a search engine if you tell it to google things before answering (even though it uses bing). reply gmerc 10 hours agorootparentprevInterns are cheaper than o1-preview reply icrbow 10 hours agorootparentNot for long. reply KoolKat23 7 hours agorootparentprevThis is a great description. reply lyu07282 10 hours agorootparentprev> The magic is that _it knows english_. I couldn't agree more, this is exactly the strength of LLMs that what we should focus on. If you can make your problem fit into this paradigm, LLMs work fantastic. Hallucinations come from that massive \"lossy compressed database\", but you should consider that part as more like the background noise that taught the model to speak English, and the syntax of programming languages, instead of the source of the knowledge to respond with. Stop anthropomorphizing LLMs, play to it's strengths instead. In other words it might hallucinate a API but it will rarely, if ever, make a syntax error. Once you realize that, it becomes a much more useful tool. reply croes 10 hours agorootparentprevIntelligent? Just ask ChatGPT How many Rs are in strawberry? reply fragmede 10 hours agorootparenthttps://chatgpt.com/share/66e3f9e1-2cb4-8009-83ce-090068b163... Keep up, that was last week's gotcha, with the old model. reply ziml77 2 hours agorootparentThere's randomness involved in generating responses. It can also give the wrong answer still: https://bsky.app/profile/did:plc:qc6xzgctorfsm35w6i3vdebx/po... reply croes 9 hours agorootparentprevMy point is the previous \"intelligent\" failed at simple task, the new one will also fail on simple tasks. That's ok for humans but not for machines. reply K0balt 7 hours agorootparent‘That's ok for humans but not for machines.’ This is a really interesting bias. I mean, I understand, I feel that way too… but if you think about it, it might be telling us something about intelligence itself. We want to make machines that act more like humans: we did that, and we are now upset that they are just as flaky and unreliable as drunk uncle bob. I have encountered plenty of people that aren’t as good at being accurate or even as interesting to talk to as a 70b model. Sure, LLMs make mistakes most humans would not, but humans also make mistakes most LLMs would not. (I am not trying to equate humans and LLMs, just to be clear) (also, why isn’t equivelate a word?) It turns out we want machines that are extremely reliable, cooperative , responsible and knowledgeable. We yearn to be obsolete. We want machines that are better than us. The definition of AGI has drifted from meaning. “able to broadly solve problems the (class of which) system designers did not anticipate” to “must be usefully intelligent at the same level as a bright, well educated person”. Where along the line did we suddenly forget that dog level intelligence was a far out of reach goal until suddenly it wasn’t? reply kylebenzle 13 hours agorootparentprevPerfectly well put! We should change the name from \"AI\" (which it is not) to something like, \"lossy compressed databases\". reply melagonster 13 hours agorootparentIf they use this name, they just say that they violate the copyright of all training data. reply gsinclair 13 hours agorootparentprevThat abbreviates to LCD. If we could make it LSD somehow, that would help to explain the hallucinations. reply MikeTheGreat 13 hours agorootparentLossy Stochastic Database? reply COAGULOPATH 16 hours agoparentprevYes, this only helps multi-step reasoning. The model still has problems with general knowledge and deep facts. There's no way you can \"reason\" a correct answer to \"list the tracklisting of some obscure 1991 demo by a band not on Wikipedia.\" You either know or you don't. I usually test new models with questions like \"what are the levels in [semi-famous PC game from the 90s]?\" The release version of GPT-4 could get about 75% correct. o1-preview gets about half correct. o1-mini gets 0% correct. Fair enough. The GPT-4 line aren't meant to be search engines or encyclopedias. This is still a useful update though. reply barrkel 11 hours agorootparento1-mini is a small model (knows a lot less about the world) and is tuned for reasoning through symbolic problems (maths, programming, chemistry etc.). You're using a calculator as a search engine. reply mattmanser 12 hours agorootparentprevIt's actually much worse than that and you're inadvertently down playing how bad it is. It doesn't even know mildly obsecure facts that are on the internet. For example last night I was trying to do something with C# generics and it confidently told me I could use pattern matching on the type in a switch statwmnt, and threw out some convincing looking code. You can't, it's impossible. It wàa completely wrong. When I told that this, it told me I was right, and proceeded to give me code that was even more wrong. This is an obscure, but well documented, part of the spec. So it's not about facts that aren't on the internet, it's just bad at facts fullstop. What it's good at is facts the internet agrees on. Unless the internet is wrong. Which is not always a good thing with the way the language it uses to speak is so confident. If you want to fuck with AI models as a bunch of code questions on Reddit, GitHub and SO with example code saying 'can I do X'. The answer is no, but chatgpt/codepilot/etc. will start spewing out that nonsense as if it's fact. As for non-proframming, we're about to see the birth of a new SEO movement of tricking AI models to believe your 'facts'. reply koe123 12 hours agorootparentIts not always the right tool depending on the task. IMO using LLMs is also a skill, much like learning how to Google stuff. E.g. apparently C# generics isn’t something its good at. Interesting, so don’t use it for that, apparently its the wrong tool. In contrast, its amazing at C++ generics, and thus speeds up my productivity. So do use it for that! reply neonsunset 5 hours agorootparentprev> For example last night I was trying to do something with C# generics and it confidently told me I could use pattern matching on the type in a switch statwmnt, and threw out some convincing looking code. Just use it on an instance instead var res = thing switch { OtherThing ot => …, int num => …, string s => …, _ => … }; reply bbarnett 12 hours agorootparentprevI wonder though, is the documentation only referenced a few places on the Internet, and are there also many forums with people pasting \"Why isn't this working?\" problems? If there are a lot of people pasting broken code, now the LLM has all these examples of broken code, which it doesn't know are that, and only a couple of references to documentation. Worse, a well trained LLM may realise that specs change, and that even documentation may not be considered 100% accurate (for it is older, out of date). After all, how many times have you had something updated, an API, a language, a piece of software, but the docs weren't updates? Happens all the time, sadly. So it may believe newer examples of code, such as the aforementioned pasted code, might be more correct than the docs. Also, if people keep trying to solve the same issue again, and keep pasting those examples again, well... I guess my point here is, hallucinations come from multi-faceted issues, one being \"wrong examples are more plentiful than correct\". Or even \"there's just a lot of wrong examples\". reply tptacek 17 hours agoparentprevI've had the opposite experience with some coding samples. After reading Nick Carlini's post, I've gotten into the habit of powering through coding problems with GPT (where previously I'd just laugh and immediately give up) by just presenting it the errors in its code and asking it to fix them. o1 seems to be effectively screening for some of those errors (I assume it's just some, but I've noticed that the o1 things I've done haven't had obvious dumb errors like missing imports, and all my 4o attempts have). reply layer8 17 hours agorootparentMy experience is likely colored by the fact that I tend to turn to LLMs for problems I have trouble solving by myself. I typically don't use them for the low-hanging fruits. That's the frustrating thing. LLMs don't materially reduce the set of problems where I'm running against a wall or have trouble finding information. reply mensetmanusman 16 hours agorootparentLLMs are not for expanding the sphere of human knowledge, but for speeding up auto-correct of higher order processing to help you more quickly reach the shell of the sphere and make progress with your own mind :) reply tsunamifury 16 hours agorootparentDefinitely. When we talk about being skilled in a T shape LLMs are all about spreading your top of T and not making the bottom go deeper. reply skydhash 1 hour agorootparentI prefer reading some book. Maybe the LLM was trained on some piece of knowledge not available on the net, but I much prefer the reliability and consistency of a book. reply ben_w 13 hours agorootparentprevIndeed, not much more depth — though even Terence Tao reported useful results from an earlier version, so perhaps the breadth is a depth all of it's own: https://mathstodon.xyz/@tao/110601051375142142 I think of it as making the top bar of the T thicker, but yes, you're right, it also spreads it much wider. reply tptacek 16 hours agorootparentprevI use LLMs for three things: * To catch passive voice and nominalizations in my writing. * To convert Linux kernel subsystems into Python so I can quickly understand them (I'm a C programmer but everyone reads Python faster). * To write dumb programs using languages and libraries I haven't used much before; for instance, I'm an ActiveRecord person and needed to do some SQLAlchemy stuff today, and GPT 4o (and o1) kept me away from the SQLAlchemy documentation. OpenAI talks about o1 going head to head with PhDs. I could care less. But for the specific problem we're talking about on this subthread: o1 seems materially better. reply throwup238 16 hours agorootparent> * To convert Linux kernel subsystems into Python so I can quickly understand them (I'm a C programmer but everyone reads Python faster). Do you have an example chat of this output? Sounds interesting. Do you just dump the C source code into the prompt and ask it to convert to Python? reply tptacek 16 hours agorootparentNo, ChatGPT is way cooler than that. It's already read every line of kernel code ever written. I start with a subsystem: the device mapper is a good recent example. I ask things like \"explain the linux device mapper. if it was a class in an object-oriented language, what would its interface look like?\" and \"give me dm_target as a python class\". I get stuff like: def linear_ctr(target, argc, argv): print(\"Constructor called with args:\", argc, argv) # Initialize target-specific data here return 0 def linear_dtr(target): print(\"Destructor called\") # Clean up target-specific data here def linear_map(target, bio): print(\"Mapping I/O request\") # Perform mapping here return 0 linear_target = DmTarget(name=\"linear\", version=(1, 0, 0), module=\"dm_mod\") linear_target.set_ctr(linear_ctr) linear_target.set_dtr(linear_dtr) linear_target.set_map(linear_map) info = linear_target.get_info() print(info) (A bunch of stuff elided). I don't care at all about the correctness of this code, because I'm just using it as a roadmap for the real Linux kernel code. The example use case code is an example of something GPT 4o provides that I didn't even know I wanted. reply throwup238 16 hours agorootparentThat's awesome. Have you tried asking it to convert Python (psuedo-ish) code back into C that interfaces with the kernel? reply tptacek 15 hours agorootparentNo, but only because I have no use for it. I wouldn't be surprised if it did a fine job! I'd be remiss if I didn't note that it's way better at doing this for the Linux kernel than with codebases like Zookeeper and Kubernetes (though: maybe o1 makes this better, who knows?). I do feel like someone who skipped like 8 iPhone models (cross-referencing, EIEIO, lsp-mode, code explorers, tree-sitter) and just got an iPhone 16. Like, nothing that came before this for code comprehension really matters all that much? reply INGSOCIALITE 15 hours agorootparentprevit's all placeholders - that's my experience with gpt trying to write slop code reply throwup238 15 hours agorootparentThose are placeholders for user callbacks passed to the device mapper subsystem. It’s a usage example not implementation code. reply jeffhuys 12 hours agorootparentprevThen ask it to expand. Be specific. reply tptacek 4 hours agorootparentI wasn't about to paste 1000 lines of Python into the thread; I just picked an interesting snippet. reply Al-Khwarizmi 12 hours agorootparentprevIt's funny because I'm very happy with the productivity boost from LLMs, but I use them in a way that is pretty much diametrically opposite to yours. I can't think of many situations where I would use them for a problem that I tried to solve and failed - not only because they would probably fail, but in many cases it would even be difficult to know that it failed. I use it for things that are not hard, can be solved by someone without a specialized degree that took the effort to learn some knowledge or skill, but would take too much work to do. And there are a lot of those, even in my highly specialized job. reply Terr_ 14 hours agorootparentprevLLMs: When the code can be made by an enthusiastic new intern with web-search and copy-paste skills, and no ability to improve under mentorship. :p Tangentially related, a comic on them: https://existentialcomics.com/comic/557 reply troupo 12 hours agorootparentprev> That's the frustrating thing. LLMs don't materially reduce the set of problems where I'm running against a wall or have trouble finding information. As you step outside regular Stack Overflow questions for top-3 languages, you run into limitations of these predictive models. There's no \"reasoning\" behind them. They are still, largely, bullshit machines. reply KoolKat23 6 hours agorootparentyou're both on the wrong wavelength. No one has claimed it is better than an expert human yet. Be glad, for now your jobs are safe, why not use it as a tool to boost your productivity, yes, even though you'll get proportionally less use than others in other perhaps less \"expert\" jobs. reply troupo 4 hours agorootparentIn order for it to boost productivity it needs to answer more than the regular questions for the top-3 languages on Stackoverflow, no? It often fails even for those questions. If I need to babysit it for every line of code, it's not a productivity boost. reply TeMPOraL 1 hour agorootparentIf you need to babysit it for every line of code, you're either a superhuman coder, working in some obscure alien language, or just using the LLM wrong. reply markk 1 hour agoparentprevStupid question: Why can't models be trained in such a way to rate the authoritativeness of inputs? As a human, I contain a lot of bad information, but I'm aware of the source. I trust my physics textbook over something my nephew thinks. reply faangguyindia 16 hours agoparentprev>The o1-preview model still hallucinates non-existing libraries and functions for me, and is quickly wrong about facts that aren't well-represented on the web. It's the usual string of \"You're absolutely correct, and I apologize for the oversight in my previous response. [Let me make another guess.]\" After that you switch to Claude Soñnet and after sometime it also gets stuck. Problem with LLM is that they are not aware of libraries. I've fed them library version, using requirements.txt, python version I am using etc... They still make mistakes and try to use methods which do not exist. Where to go from here? At this point I manually pull the library version I am using and go to its docs, I generate a page which uses the this library correctly (then I feed that example into LLM) Using this approach works. Now I just need to automate it so that I don't have to manually find the library, create specific example which uses the methods I need in my code! Directly feeding the docs isn't working well either. reply mediaman 16 hours agorootparentOne trick that people are using, when using Cursor and specifically Cursor's compose function, is to dump library docs into a text file in your repo, and then @ that doc file when you're asking it to do something involving that library. That seems to eliminate a lot of the issues, though it's not a seamless experience, and it adds another step of having to put the library docs in a text file. Alternatively, cursor can fetch a web page, so if there's a good page of docs you can bring that in by @ the web page. Eventually, I could imagine LLMs automatically creating library text doc files to include when the LLM is using them to avoid some of these problems. It could also solve some of the issues of their shaky understanding of newer frameworks like SvelteKit. reply throwup238 16 hours agorootparentCursor also has the shadow workspace feature [1] that is supposed to send feedback from linting and language servers to the LLM. I'm not sure whether it's enabled in compose yet though. [1] https://www.cursor.com/blog/shadow-workspace reply fsndz 12 hours agoparentprevMy point of view: this is a real advancement. I've always believed that with the right data allowing the LLM to be trained to imitate reasoning, it's possible to improve its performance. However, this is still pattern matching, and I suspect that this approach may not be very effective for creating true generalization. As a result, once o1 becomes generally available, we will likely notice the persistent hallucinations and faulty reasoning, especially when the problem is sufficiently new or complex, beyond the \"reasoning programs\" or \"reasoning patterns\" the model learned during the reinforcement learning phase. https://www.lycee.ai/blog/openai-o1-release-agi-reasoning reply shmatt 16 hours agoparentprevI honestly can’t believe this is the hyped up “strawberry” everyone was claiming is pretty much AGI. Senior employees leaving due to its powers being so extreme I’m in the “probabilistic token generators aren’t intelligence” camp so I don’t actually believe in AGI, but I’ll be honest the never ending rumors / chatter almost got to me Remember, this is the model some media outlet reported recently that is so powerful OAI is considering charging $2k/month for reply vasco 11 hours agorootparentThe whole safety aspect of AI has this nice property that it also functions as a marketing tool to make the technology seem \"so powerful it's dangerous\". \"If it's so dangerous it must be good\". reply synarchefriend 8 hours agorootparentprev\"Senior employees leaving due to its powers being so extreme\" This never happened. No one said it happened. \"the model some media outlet reported recently that is so powerful OAI is considering charging $2k/month for\" The Information reported someone at a meeting suggested this for future models, not specifically Strawberry, and that it would probably not actually be that high. reply shmatt 6 hours agorootparentElon Musk and Ilya Sutskever Have Warned About OpenAI’s ‘Strawberry’ Jul 15, 2024 — Sutskever himself had reportedly begun to worry about the project's technology, as did OpenAI employees working on A.I. safety at the time. https://observer.com/2024/07/openai-employees-concerns-straw... And I’m ignoring the hundreds of Reddit articles speculating every time someone at OAI leaves And of course that $2000 article was spread by every other media outlet like wildfire I know I’m partially to blame for believing the hype, this is pretty obviously no better at stating facts or good code than what we’ve known for the past year reply semi-extrinsic 2 hours agorootparentMy hypothesis about these people who are afraid of AI, is that they have tricked themselves into believing they are in their current position of influence due to their own intelligence (as opposed to luck, connections, etc.) Then they drink the marketing koolaid, and it follows naturally that they worry an AI system can obtain similar positions of influence. reply kqr 13 hours agorootparentprev> probabilistic token generators aren’t intelligence Maybe this has been extensively discussed before, but since I've lived under a rock: which parts of intelligence do you think are not representable as conditional probability distributions? reply lewhoo 9 hours agorootparent> which parts of intelligence do you think are not representable as conditional probability distributions Maybe I'm wrong here but a lot of our brilliance comes from acting against the statistical consensus. What I mean is, Nicolaus Copernicus probably consumed a lot of knowledge on how the Earth is the center of the universe etc. and probably nothing contradicting that notion. Can a LLM do that ? reply kqr 4 hours agorootparentIt could be \"probability of token being useful\" rather than \"probability of token coming next in training data\"! reply bentonkek 6 hours agorootparentprevCopernicus was an exception, not the rule. Would you say everyone else who lived at the time was not 'really' intelligent? reply wavemode 5 hours agorootparentThat's an illogical counterargument. The absence of published research output does not imply the absence of intelligent brain patterns. What if someone was intelligent but just wasn't interested in astronomy? reply lewhoo 5 hours agorootparentprevYes but this was just to make a blatant example. The questions still stands. If you feed a LLM certain kind of data is it possible it strays from it completely - like we sometimes do in cases big and small when we figure out how to do something a bit better by not following the convention. reply KoolKat23 6 hours agorootparentprevAnd how many people actively do that? It's very rare we experience brilliance and often we stumble upon it by accident. Irrational behavior, coincidence or perhaps they were dropped on their heads when they were young. reply firejake308 16 hours agorootparentprevI mean, considering how many tokens their example prompt consumed, I wouldn't be surprised if it costs ~$2k/month/user to run reply HarHarVeryFunny 4 hours agoparentprevTo the extent we've now got the output of the underlying model wrapped in an agent that can evaluate that output, I'd expect it to be able to detect it's own hallucinations some of the time and therefore provide an alternate answer. It's like when an LLM gives you a wrong answer and all it takes is \"are you sure?\" to get it to generate a different answer. Of course the underlying problem of the model not knowing what it knows or doesn't know persists, so giving it the ability to reflect on what it just blurted out isn't always going to help. It seems the next step is for them to integrate RAG and tool use into this agentic wrapper, which may help in some cases. reply feralderyl 15 hours agoparentprevI think this model is a precursor model that is designed for agentic behavior. I expect very soon OpenAI to allow this model tool use that will allow it to verify its code creations and whatever else it claims through use of various tools like a search engine, a virtual machine instance with code execution capabilities, api calling and other advanced tool use. reply _fs 15 hours agoparentprevo1-preview != o1. In public coding AI comparison tests, results showed 4o scoring around 35%, o1-preview scoring ~50% and o1 scoring ~85%. o1 is not yet released, but has been run through many comparison tests with public results posted. reply kristianp 8 hours agorootparentGood reminder. Why did OpenAI talk about o1 and not release it? o1-preview must be a stripped down version: cheaper to run somehow? reply barrkel 11 hours agorootparentprevDon't forget about o1-mini. It seems better than o1-preview for problems that fit it (don't require so much real world knowledge). reply arthurcolle 12 hours agorootparentprevgpt-4 base was never released and this will be the same thing reply AbstractH24 7 hours agoparentprevOne of the biggest problems with this generation of AI is how people conflate the natural language abilities and the access to what it knows. Both abilities are powerful, but they are very different powers. reply spaceman_2020 12 hours agoparentprevI don’t really see this as a massive problem. Its code. If it doesn’t run, you ask it to reconsider, give some more info if necessary, and it usually gets it right. The system doesn’t become useless if it takes 2 tries instead of 1 to get it right Still saves an incredible amount of time vs doing it yourself reply latexr 11 hours agorootparent> Its code. If it doesn’t run, you ask it to reconsider It is perfectly possible to have code that runs without errors but gives a wrong answer. And you may not even realise it’s wrong until it bites you in production. reply benterix 10 hours agorootparentprevWhile I agree, I saw it abused in this way a lot, in the sense that the code did what it was supposed to do in a given scenario but was obviously flawed in various was so it was just sitting there waiting for a disaster. reply troupo 12 hours agorootparentprevI haven't found a single instance where it saved me any significant amount of time. In all cases I still had to rewrite the whole thing myself, or abandon endeavor. And a few times the amount of time I spent trying to coax a correct answer out of AI trumped any potential savings I could've had reply timbaboon 8 hours agoparentprevThe best one I got recently was after I pointed out that the method didn’t exist, it proposed another method and said “use this method if it exists” :D reply motoboi 15 hours agoparentprevJust pass a link to a GitHub issue and ask for a response or even a webpage to summarize and will see the beautiful hallucinations it will come up to as the model is not web browsing yet. reply rcarr 14 hours agoparentprevHas anyone tried asking it to generate the libraries/functions that it's hallucinating and seeing if it can do so correctly? And then seeing if it can continue solving the original problem with the new libraries? It'd be absolutely fascinating if it turns out it could do this. reply viraptor 13 hours agorootparentNot for libraries, but functions will sometimes get created if you work with an agent coding loop. If the tests are in the verification step, the code will typically be correct. reply 8n4vidtmkvmk 12 hours agorootparentI sometimes give it snippets of code and omit helper functions if they seem obvious enough, and it adds its own implementation into the output. reply empath75 3 hours agoparentprevYou should not be asking it questions that require it to already know detailed information about apis and libraries. It is not good at that, and it will never be good at that. If you need it to write code that uses a particular library or api, include the relevant documentation and examples. It's your right to dismiss it, if you want, but if you want to get some value out of it, you should play to it's strengths and not look for things that it fails at as a gotcha. reply burtonator 14 hours agoparentprevI'm honestly confused as to why it is doing this and why it thinks I'm right when I tell it that it is incorrect. I've tried asking it factual information, and it asserts that it's incorrect but it will definitely hallucinate questions like the above. You'd think the reasoning would nail that and most of the chain-of-thought systems I've worked on would have fixed this by asking it if the resulting answer was correct. reply jiggawatts 13 hours agoparentprev> The o1-preview model still hallucinates non-existing libraries and functions for me Oooh... oohhh!! I just had a thought: By now we're all familiar with the strict JSON output mode capability of these LLMs. That's just a matter of filtering the token probability vector by the output grammar. Only valid tokens are allowed, which guarantees that the output matches the grammar. But... why just data grammars? Why not the equivalent of \"tab-complete\"? I wonder how hard it would be to hook up the Language Server Protocol (LSP) as seen in Visual Studio code to an AI and have it only emit syntactically valid code! No more hallucinated functions! I mean, sure, the semantics can still be incorrect, but not the syntax. reply TeMPOraL 1 hour agorootparentI still fail to see the overall problem. Hallucinating non-existing libraries is a good programming practice in many cases: you express your solution in terms of an imaginary API that is convenient for you, and then you replace your API with real functions, and/or implement it in terms of real functions. reply loremaster 11 hours agorootparentprevThis would be a big undertaking to get working for just one language+package-manager combination, but would be beautiful if it worked. reply mrtesthah 17 hours agoparentprevIt begs the question of whether we can supply a function to be called (e.g., one that compiles and runs code) to evaluate intermediate CoT results reply ttul 16 hours agorootparentIt seems OpenAI has decided to keep the CoT results a secret. If they were to allow the model to call out to tools to help fill in the CoT steps, then this might reveal what the model is thinking - something they do not want the outside world to know about. I could imagine OpenAI might allow their own vetted tools to be used, but perhaps it will be a while (if ever) before developers are allowed to hook up their own tools. The risks here are substantial. A model fine-tuned to run chain-of-thought that can answer graduate level physics problems at an expert level can probably figure out how to scam your grandma out of her savings too. reply 8n4vidtmkvmk 12 hours agorootparentIt's only a matter of time. When some other company releases the tool, they likely will too. reply colordrops 16 hours agorootparentprevThe answer is yes if you are willing to code it. OpenAI supports tool calls. Even if it didn't you could just make multiple calls to their API and submit the result of the code execution yourself. reply cma 16 hours agorootparentThe intermediate CoT results aren't in the API. reply colordrops 13 hours agorootparentI may be mistaken but I don't believe the first version of the comment I replied to mentioned intermediate CoT results. reply kylebenzle 13 hours agoparentprev> having no way to assess if what it conjures up from its weights is factual or not. This comment makes no sense in the context of what an LLM is. To even say such a thing demonstates a lack of understandting of the domain. What we are doing here is TEXT COMPLETION, no one EVER said anything about being accurate and \"true\". We are building models that can complete text, what did you think an LLM was, a \"truth machine\"? reply panja 13 hours agorootparentI mean of course you're right, but then I question what's the usefulness? reply kristianp 14 hours agoprevI tried a problem I was looking at recently, to refactor a small rust crate to use one datatype instead of an enum, to help me understand the code better. I found o1-mini made a decent attempt, but couldn't provide error free code. o1-preview was able to provide code that compiled and passed all but the test that is expected to fail, given the change I asked it to make. This is the prompt I gave: simplify this rust library by removing the different sized enums and only using the U8 size. For example MasksByByte is an enum, change it to be an alias for the U8 datatype. Also the u256 datatype isn't required, we only want U8, so remove all references to U256 as well. The original crate is trie-hard [1][2] and I forked it and put the models attempts in the fork [3]. I also quickly wrote it up at [4] [1] https://blog.cloudflare.com/pingora-saving-compute-1-percent... [2] https://github.com/cloudflare/trie-hard [3] https://github.com/kpm/trie-hard-simple/tree/main/attempts [4] https://blog.reyem.dev/post/refactoring_rust_with_chatgpt-o1... reply bluerooibos 2 hours agoparentI've been having a weird timezone issue in my Rails application that I've had a hard time getting my head around. I tried giving o1-preview the relevant code and context it needed to know and it gave answers that seemed to make sense but it still wasn't able to resolve the bug and explain exactly what was going on. So, it seems like anything that requires some actual thought and problem-solving is tough for it to answer. I'm sure it's just a matter of time before devs are out of work but it seems like we'll be safe for another few years anyway. reply vectorhacker 50 minutes agorootparentI'm still not convinced that it's not going through approximate reasoning chain retrieval and that's self-triggered to get more reasoning chains that will maximize it's goal. I'm seeing a lot of comments from other SWEs using it for non-trivial tasks in which it fails at but is just trying harder to look like it's problem solving. Even with more context and documentation, it fails to realize details an experienced SWE would pick up quickly. reply jes5199 3 hours agoparentprevI cannot tell from reading what you wrote whether you think it did a good job or not reply bambax 10 hours agoprevNear the end, the quote from OpenAI researcher Jason Wei seems damning to me: > Results on AIME and GPQA are really strong, but that doesn’t necessarily translate to something that a user can feel. Even as someone working in science, it’s not easy to find the slice of prompts where GPT-4o fails, o1 does well, and I can grade the answer. But when you do find such prompts, o1 feels totally magical. We all need to find harder prompts. Results are \"strong\" but can't be felt by the user? What does that even mean? But the last sentence is the worst: \"we all need to find harder prompts\". If I understand it correctly, it means we should go looking for new problems / craft specific questions that would let these new models shine. \"This hammer hammers better, but in most cases it's not obvious how better it is. But when you stumble upon a very specific kind of nail, man does it feel magical! We need to craft more of those weird nails to help the world understand the value of this hammer.\" But why? Why would we do that? Wouldn't our time be better spent trying to solve our actual, current problems, using any tool available? reply aubanel 10 hours agoparentHe's speaking about his objective to make ever stronger LLMs: so for this his secondary objective is to measure their real performance. The human preference is not that good of a proxy measurement: for instance, it can be gamed by making the model more assertive, causing the human error-spotting ability to decrease a lot [0]. So what he's really saying is that non-rigorous human vibe checks (like those LMSys Chatbot Arena is built on, although I love it) won't cut it anymore to evaluate models, because now models are past that point. Just like you can't evaluate how smart a smart person really is in a 2min casual conversation. [0]: https://openreview.net/pdf?id=7W3GLNImfS reply causal 1 hour agorootparentIt's trivial to come up with prompts that 4o fails. If it's hard to come up with prompts that 1o succeeds on but 4o fails, that implies the delta is not that great. reply mft_ 41 minutes agorootparentOr, the delta depends on the nature of the problem/prompt, we’ve not yet figured that out, there’s a relatively narrow range of prompts with large delta, and so finding those examples is a work in progress? reply prog_1 6 hours agorootparentprevie when you cant beat them, make new metrics and you can absolutely evaluate how smart someone is in a 2min casual conversation. You wont be able to tell how well they are in some niche topic, but %insert something about different flavors of intelligence and how they do not equate do subject matter expertise% reply skybrian 2 hours agorootparentIt’s a common pattern that AI benchmarks get too easy, so they make new ones that are harder. reply arb_ 4 hours agorootparentprevAs models improve, human preference will become worse as a proxy measurement (e.g. as model capabilities surpass the human's ability to judge correctness at a glance). This can be due to more raw capability - or more persuasion / charisma. reply edouard-harris 8 hours agoparentprev> Results are \"strong\" but can't be felt by the user? What does that even mean? Not every conversation you have with a PhD will make it obvious that that person is a PhD. Someone can be really smart, but if you don't see them in a setting where they can express it, then you'll have no way of fully assessing their intelligence. Similarly, if you only use OAI models with low-demand prompts, you may not be able to tell the difference between a good model and a great one. reply ksplicer 5 hours agoparentprevThis is something we've been grappeling with on my team. Many of",
    "originSummary": [
      "OpenAI has released two new models, o1-preview and o1-mini, codenamed \"strawberry,\" which offer improved reasoning capabilities through a chain of thought prompting pattern.",
      "These models are reserved for tier 5 accounts ($1,000+ on API credits) and introduce \"reasoning tokens\" that are billed but not visible in the API response, sparking some dissatisfaction due to lack of transparency.",
      "The new models can handle complex prompts better and have increased output token allowances, expanding the potential tasks solvable by large language models (LLMs)."
    ],
    "commentSummary": [
      "OpenAI's new o1 chain-of-thought models still produce hallucinations, such as non-existent libraries and functions, and often provide incorrect facts.",
      "Users observe that while reasoning capabilities have improved, the models still fail to verify the factual accuracy of their outputs, necessitating user double-checking.",
      "Some users liken the models to naive but intelligent interns, suggesting they can be useful with proper guidance, though they lack the ability to ask clarifying questions or admit uncertainty, impacting their reliability."
    ],
    "points": 593,
    "commentCount": 520,
    "retryCount": 0,
    "time": 1726188481
  },
  {
    "id": 41525778,
    "title": "Data sleuths who spotted research misconduct cleared of defamation",
    "originLink": "https://arstechnica.com/science/2024/09/court-clears-researchers-of-defamation-for-identifying-manipulated-data/",
    "originBody": "Evidence-supported conclusions aren't defamation — Court clears researchers of defamation for identifying manipulated data Harvard, however, will still face trial over how it managed the investigation. John Timmer - 9/12/2024, 9:17 PM Enlarge / Harvard Business School was targeted by a faculty member's lawsuit. APCortizasJr reader comments 55 Earlier this year, we got a look at something unusual: the results of an internal investigation conducted by Harvard Business School that concluded one of its star faculty members had committed research misconduct. Normally, these reports are kept confidential, leaving questions regarding the methods and extent of data manipulations. But in this case, the report became public because the researcher had filed a lawsuit that alleged defamation on the part of the team of data detectives that had first identified potential cases of fabricated data, as well as Harvard Business School itself. Now, the court has ruled on motions to dismiss the case. While the suit against Harvard will go on, the court has ruled that evidence-backed conclusions regarding fabricated data cannot constitute defamation—which is probably a very good thing for science. Data and defamation The researchers who had been sued, Uri Simonsohn, Leif Nelson, and Joe Simmons, run a blog called Data Colada where, among other things, they note cases of suspicious-looking data in the behavioral sciences. As we detailed in our earlier coverage, they published a series of blog posts describing an apparent case of fabricated data in four different papers published by the high-profile researcher Francesca Gino, a professor at Harvard Business School. The researchers also submitted the evidence to Harvard, which ran its own investigation that included interviewing the researchers involved and examining many of the original data files behind the paper. In the end, Harvard determined that research misconduct had been committed, placed Gino on administrative leave and considered revoking her tenure. Harvard contacted the journals where the papers were published to inform them that the underlying data was unreliable. Gino then filed suit alleging that Harvard had breached their contract with her, defamed her, and interfered with her relationship with the publisher of her books. She also added defamation accusations against the Data Colada team. Both Harvard and the Data Colada collective filed a motion to have all the actions dismissed, which brings us to this new decision. Harvard got a mixed outcome. This appears to largely be the result that the Harvard Business School adopted a new and temporary policy for addressing research misconduct when the accusations against Gino came in. This, according to the court, leaves questions regarding whether the university had breached its contract with her. However, most of the rest of the suit was dismissed. The judge ruled that the university informing Gino's colleagues that Gino had been placed on administrative leave does not constitute defamation. Nor do the notices requesting retractions sent to the journals where the papers were published. \"I find the Retraction Notices amount 'only to a statement of [Harvard Business School]’s evolving, subjective view or interpretation of its investigation into inaccuracies in certain [data] contained in the articles,' rather than defamation,\" the judge decided. Colada in the clear More critically, the researchers had every allegation against them thrown out. Here, the fact that the accusations involved evidence-based conclusions, and were presented with typical scientific caution, ended up protecting the researchers. The court cites precedent to note that “[s]cientific controversies must be settled by the methods of science rather than by the methods of litigation” and concludes that the material sent to Harvard \"constitutes the Data Colada Defendants’ subjective interpretation of the facts available to them.\" Since it had already been determined that Gino was a public figure due to her high-profile academic career, this does not rise to the standard of defamation. And, while the Data Colada team was pretty definitive in determining that data manipulation had taken place, its members were cautious about acknowledging that the evidence they had did not clearly indicate Gino was the one who had performed the manipulation. Finally, it was striking that the researchers had protected themselves by providing links to the data sources they'd used to draw their conclusions. The decision cites a precedent that indicates \"by providing hyperlinks to the relevant information, the articles enable readers to review the underlying information for themselves and reach their own conclusions.\" So, overall, it appears that, by couching their accusations in the cautious language typical of scientific writing, the researchers ended up protecting themselves from accusations of defamation. That's an important message for scientists in general. One of the striking developments of the last few years has been the development of online communities where scientists identify and discuss instances of image and data manipulation, some of which have ultimately resulted in retractions and other career consequences. Every now and again, these activities have resulted in threats of lawsuits against these researchers or journalists who report on the issue. Occasionally, suits get filed. Ultimately, it's probably good for the scientific record that these suits are unlikely to succeed. reader comments 55 John Timmer John is Ars Technica's science editor. He has a Bachelor of Arts in Biochemistry from Columbia University, and a Ph.D. in Molecular and Cell Biology from the University of California, Berkeley. When physically separated from his keyboard, he tends to seek out a bicycle, or a scenic location for communing with his hiking boots. Advertisement Channel Ars Technica Biomarkers, from diagnosis to treatment To find cancer before it strikes, look for the molecular clues Biomarkers, from diagnosis to treatment Scott Manley Reacts To His Top 1000 YouTube Comments Teach the Controversy: Dowsing Teach the Conspiracy: GMOs How Does That Work?: Rising sea levels Teach the Controversy: Flat Earthers How Does That Work?: Radiometric Dating How Does That Work?: The Large Hadron Collider What \"First Man\" tells you about Neil Armstrong, and what it doesn't Delta V: The Burgeoning World of Small Rockets, Paul Allen's Huge Plane, and SpaceX Gets a Crucial Green-light Inside the International Space Station with Scott Kelly Chris Hadfield explains his 'Space Oddity' video Astronaut Scott Kelly teaches orbital mechanics with Kerbal Space Program Go Inside the Aerodrome, Where the Future of Flight Takes Shape The soft future of robotics See the Gear the CDC's Disease Detectives Use in the Field Ars Technica interviews Peggy Whitson The Ice Age Secrets of White Sands National Monument InSight Landing On Mars Talking Space and Robots with NASA's Terry Fong More videos ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=41525778",
    "commentBody": "Data sleuths who spotted research misconduct cleared of defamation (arstechnica.com)519 points by dangle1 20 hours agohidepastfavorite99 comments apwheele 19 hours agoWhile a favorable ruling for the Data Colada defendants seemed incredibly likely in the end (and of course being dismissed pre-trial before discovery is a good thing). But it still took over a year, and consisted of several pre-trial hearings with multiple back and forths between them. If I ctrl-f https://storage.courtlistener.com/recap/gov.uscourts.mad.259... I see Pyle (their lawyer) listed 11 times. All the motions and responses are a non-trivial amount of work. Do folks have a sense of how much the costs would have been just to get dismissed for DC? Seems to be definitely over 10k (50k?) Or am I overestimating. reply teractiveodular 19 hours agoparentI'm pretty sure this guy wouldn't get out of bed for $10k: https://princelobel.com/professional/jeffrey-j-pyle/ Although given that the New Yorker stated that Pyle \"appeared not only professionally but personally incensed by the fact of the lawsuit\", perhaps some of the work was pro bono. https://www.newyorker.com/news/news-desk/how-a-scientific-di... reply pclmulqdq 18 hours agorootparentThankfully, a lot of the filings appear short enough that I doubt he's going to charge very much. I assume the retainer was still $20-30k, but they may get some of it back. I was hoping that there would be some fee-shifting here, but it's federal court. reply yial 13 hours agorootparentI would guess a 20-30k retainer in this occurrence would be a discounted retainer as well. I say this just based upon my personal experience with a few smaller firms. reply pclmulqdq 2 hours agorootparentIf you're known to be good for the money, you can often get lower retainer bills from lawyers. That's why Donald Trump has to put up tens of millions for any legal project. Many of my lawyers do not ask for retainers for work at all, even for bigger things. reply jhauris 17 hours agorootparentprevI don't know him or this case, but lawyers tend to often feel that way about the cases they are representing. I wouldn't think that means he's not billing. reply rfw300 18 hours agoparentprevFortunately, they were able to raise quite a bit of money (>$300K) on GoFundMe: https://www.gofundme.com/f/uhbka-support-data-coladas-legal-... reply teractiveodular 15 hours agorootparentFrom the page: At present, Leif, Joe, and Uri do not have pro bono representation. The lawyers they’ve spoken to currently estimate that their defense could cost anywhere between $50,000 and $600,000 (depending on how far the lawsuit progresses). reply yard2010 9 hours agorootparentThat's scary, really. Having no money for this means you can't afford to fight these criminal crooks. Is this how this works in europe too? reply Mkengine 8 hours agorootparentNo, where I live (Germany) you usually have a legal insurance to cover this. The costs are around 30€ per month in my case. reply pclmulqdq 4 hours agorootparentCorporate liability insurance in the US, which is also a very common thing to get, often does not cover your defense against \"intentional torts\" such as defamation or fraud. It usually covers things like slips and falls, product-related liability, etc. I would be surprised if your German version covers defamation. reply Cthulhu_ 7 hours agorootparentprevI live next door and have some legal insurance, but you need to pay attention to what coverage you get, because you need to select what categories you want coverage for; vehicles (e.g. if someone who caused an accident refuses to cooperate with you), consumer/housing disputes, work and salary, taxes / stocks, family issues like inheritances, second homes / real estate, etc etc etc. And of course, you can't take out insurance for an ongoing issue, it doesn't cover criminal cases (when you did a bad), business cases, etc. reply Hamuko 8 hours agorootparentprevIn Europe, the losing party of a lawsuit generally has to pay for the winner's legal expenses (\"English rule\"). Never had to go to court so I don't know what you do about the expenses in the meanwhile, but at least there's light at the end of the tunnel when someone sues you just for the heck of it. reply jaclaz 6 hours agorootparentEurope may be too wide a categorization, I am pretty sure that different countries have different ways to deal with the matter. My (thankfully very little) experience in Italy is for civil litigations at least (where it is rare that one of the two parties get 100% reason) expenses are usually compensated (i.e. every party pays their own ones), in the more rare case where all expenses are paid by the succumbing party, what is liquidated is not really what has been paid, but rather what the expenses would be along some sort of tariff. If your solicitor/lawyer is a famous (presumably very good as you won) one, it is likely that the amount you spend is much higher than what the judge condemned the other party to reimburse you. reply pclmulqdq 4 hours agorootparentprevIn the US, you can ask for fee-shifting if you have been sued frivolously or there has otherwise been some wrongdoing on the part of the other side. These guys may be able to motion for fee-shifting given that the case was dismissed at this early stage. State defamation cases usually apply anti-SLAPP laws that automatically create fee-shifting in this exact circumstance, but this seems to be a federal case. reply apwheele 8 hours agorootparentprevI was aware of that, but I would still be interested to know the actual costs. I do not think \"it is ok to blog about potential fraud in scientific publications because GoFundMe will pay for my lawyers\" is a good assumption for most individuals. I see people saying it is a win for open science, and I just look at that Court Listener page and it still fills me with angst. reply rudyfink 18 hours agoparentprevYou are probably not overestimating. Generally, I would guess conferring with the client on the facts, briefing, preparing for a hearing, and arguing a hearing would be north of, at least, 25k, assuming a lower-end rate estimate in the $800+ range and a lower-end work estimate of 30+ hours. If I had to bet, I'd go higher than the low-end estimate: both the rate and hours could be close to double. That said, the attorney / firm could be donating the time on this one. reply TrackerFF 9 hours agoparentprevI’m not familiar with civil court in the US, but wouldn’t the plaintiff have to cover the legal expenses of the defendant, if the defendant is cleared? Or similar rulings. reply yard2010 8 hours agorootparentEven if it is, it happens months or years later, with today's interest rates you would still end up with a debt unless you have the capital. I'm not even talking about the emotional toll and stress, especially when you're the good guy, just have to prove it.. reply Hamuko 8 hours agorootparentprevNo, both sides of a lawsuit are generally responsible for their own costs regardless of what the outcome is. There are some caveats to this, like anti-SLAPP laws, but that's how it generally goes. This is basically in contrast to every other western democracy. https://en.wikipedia.org/wiki/American_rule_(attorney%27s_fe... reply nullc 5 hours agoparentprevIt's not just the money. When you're the target of litigation, particularly irregular specialist domain stuff that it can really take over your life. You might recover some of your legal costs from your opponent, but the disruption to your life will not be compensated. reply godelski 58 minutes agorootparentNot to mention the time it takes. So not only are you losing money, losing time, and it's emotionally taxing, but you often aren't even making money during that time. reply AtlasBarfed 5 hours agoparentprevHave you seen Harvard's endowment? I'm sure there's counterdamages and he can take a percentage reply hn_throwaway_99 14 hours agoparentprev> Or am I overestimating. Ahh, sweet summer child who has never seen these kind of legal fees. I do hope Gino gets socked with their legal fees. reply pfdietz 7 hours agorootparentThis is why we need anti-SLAPP laws. reply aheilbut 17 hours agoparentprevYou are underestimating... reply tothrowaway 16 hours agorootparentI would agree. I'm the defendant in a (frivolous) trademark infringement case and my firm (which does work for the EFF) averages about 2 hours per (substantial) page, at a rate of $500/hour. I do wonder in the back of my mind if I'm being taken to the cleaners... reply yial 13 hours agorootparentI have a limited experience, only having been involved in one trademark case professionally and am not a lawyer, but this was a case that very quickly settled and fees were still close to $30,000. reply anitil 19 hours agoprevWe're lucky to have hard-headed people willing to put up with legal threats and bullying tactics. I'm not sure I'd have the spine for it. It reminds me of when Ben Goldacre was sued [0] which luckily his publisher covered the costs for. [0] https://www.badscience.net/files/The-Doctor-Will-Sue-You-Now... reply afh1 7 hours agoparentThe problem is being able to sue for something like defamation. At the extreme, like happens in Brazil, innocent people are sent to jail for criticizing politicians. https://mises.org/mises-wire/rothbard-suits-defamation reply AtlasBarfed 4 hours agorootparentThat is true, but this is so much worse. 1) this is a college 2) it was about research, fundamental to the mission of higher education 3) it was fraud, not error 4) It was HARVARD, exalted bastion of elite colleges, the most prestigious university in the US I get Harvard is always more about influence and an insiders club rather than any real academic work, but still ... So many heads should roll over this. reply andrelaszlo 9 hours agoprev> To believe that the Original data are fake and the Posted data are real, you’d have to believe that the sensible data are fake and the backwards data are real. That is a difficult thing to believe. > We were right about how the data were altered, Gino’s prevailing explanation for the alterations does not make sense, and yet we are the defendants in this case. What's the academic term for \"mic drop\"? https://datacolada.org/118 reply andrelaszlo 9 hours agoparentThis analysis is simple but brilliant. Some rows appeared out of order. They changed them back to what they should be according to their position, then checked the effects on the distributions: https://datacolada.org/111 reply polartx 1 hour agoparentprevIf she was falsifying data, I must assume it was to support a biased hypothesis she held however the article doesn’t elaborate on what conclusion her fraudulent data supported beyond this being a “behavioral science” study. Can anyone fill in the gaps to those conspicuously missing details? reply red_admiral 3 hours agoprevIn theory, if the other side can't outpay you on lawyers, truth should be an absolute defense to allegations of defamation. If you're careful enough not to just write \"she faked the data\" but \"based on this evidence ... we believe the data is fake\", that should be watertight. reply tanepiper 10 hours agoprevI posted this a while back on here, but my wife works in Behavioural Science, and wrote this about it last year https://www.squarepeginsight.com/post/the-dark-side-of-behav... reply lern_too_spel 48 minutes agoparentUnlike Gino, Ariely doesn't seem to have had a downfall. He still has a lab at Duke, he keeps getting invited to give talks, his consultancy is still getting contracts. It doesn't make any sense to me. Maybe another behavioral scientist can fake an experiment to explain it. reply RachelF 18 hours agoprevThe irony is that Francesca Gino and Dan Ariely are famous for their pop-sci/TED talks studying dishonesty. Turns out they were dishonest themselves. https://www.npr.org/2023/07/27/1190568472/dan-ariely-frances... reply hughesjj 16 hours agoparentI'm at the point where if I see it in a TED talk I'm assuming it's a bullshit grifter reply hilux 13 hours agorootparentMe too. Same for pretty much any \"scientist\" on NPR, or with a popular book. Not EVERY one is a hack or a fraud, but enough are that as a group, they're not trustworthy. That's a big chunk of my adult life I need to forget, because I was so into the broad field of applied social science, Ariely in particular. It's a real shame. reply usea 15 hours agorootparentprevEven 8 years ago that was a safe view to hold. reply zo1 12 hours agorootparentprevI blocked the TED channel on YouTube a few years back as it was getting bad. They first diluted their \"brand\" or quality with TEDx, and it's been downhill since as they've been scraping the bottom of the barrel and tried to \"scale\" or \"franchise\". Happens everytime, growth at all costs. The only way to stop it is to have passionate leaders who care more about the goal than the money. Unfortunately, usually by the time these leaders have established their company they are also on a downward life trajectory in terms of years left, so they switch to \"extraction\" mode. reply Suppafly 2 hours agorootparentI don't have a problem with the concept of locally branded TED talks under the TEDx umbrella, but I do have a problem of TEDx talks pretending that they are genuine TED talks. Generally actual TED talks have some quality checks built in, they sometimes allow garbage through, but it's far more rigid than the stuff you see at the TEDx talks. reply protomolecule 12 hours agorootparentprevEven if it is a Nobel prize winner like Jennifer Doudna? reply RachelF 10 hours agorootparentIt's hard to generalize. The whole field of behavioural intelligence has lots of fake/unrepeatable studies. Even Kahneman, who got a Economics Nobel has this issue: https://retractionwatch.com/2017/02/20/placed-much-faith-und... reply protomolecule 6 hours agorootparentYep, it's hard, that's why sometimes it's better not to overgeneralize. But sure, that field is full if unreproducible \"research\". reply guappa 11 hours agorootparentprevWell the guy who invented lobotomy got one of those. Or the guy who invented microcredit. To not even venture in the peace nobel prizes area, where getting one is probably indicative of being evil. reply adastra22 11 hours agorootparentI mean the Nobel committee gave the Peace prize to Kissinger. I’m not sure they’re a good bellwether for anything. reply seanhunter 10 hours agorootparentTom Lehrer famously retired when Kissinger got the peace prize declaring satire to be dead. The really funny one to me is they gave literature to Winston Churchill because they thought they probably couldn't give him peace, then when Kissinger came up they thought \"Why not?\" reply FergusArgyll 7 hours agorootparentChurchill's autobiography [0] is actually a great read. I'm nowhere near qualified to judge a literature Nobel prize but I loved it. His history of the English speaking peoples is pretty good too. [0] https://en.wikipedia.org/wiki/My_Early_Life reply protomolecule 6 hours agorootparentprev>Well the guy who invented lobotomy got one of those. Yeah, and when you see a Nobel prize winner you must be assuming it's a bullshit grifter. reply yard2010 8 hours agoparentprevI would argue: that's just the tip of the iceberg. This is how it works. In a system in which there are external motivations (power, money, number of citations etc.) it's a problem of incentives, and should be solved by putting the emphasis on other values, as a culture. What you describe is a symptom. Ofc, I'm not trying to defend such behavior. reply visarga 7 hours agoparentprevMaybe they were doing \"first-person research\" on dishonesty. /s reply Simon_ORourke 12 hours agoprevSo what happens with these costs - surely the failed litigant should shoulder some if not all of the costs of defending this defamation claim? reply perihelions 10 hours agoparentOften, no. There's a relevant specific fee-recovery mechanism [0] in some, not all, states, because this class of lawsuit acknowledged as being problematic, as being an effective way to silence 1A-protected speech. The deeper problem is that litigation is extraordinarily expensive in the US, and generically easy to abuse as an intimidation weapon—for rich entities to use against those who can't afford lawyers; or for fanatics to use against normal risk-averse people who don't want lawsuits–don't value whatever it is the fanatic is fanatic about, highly enough to go to court to argue it. [0] https://en.wikipedia.org/wiki/Strategic_lawsuit_against_publ... reply progbits 8 hours agoparentprevSadly it doesn't work like that but even that wouldn't be fair. If you lose the defamation as the plaintiff you should be forced to pay the full amount you were suing for to the defendant. Or even a multiple of that. reply proto-n 10 hours agoparentprevTo my (totally uninformed) eyes it looks like maybe this could count as a SLAPP suit [1], in which case there's a possbility for a counterclaim for legal costs. However, Massachusetts apparently has \"much weaker anti-SLAPP laws\" [2] (than California, in the context of the article) [1] https://en.wikipedia.org/wiki/Strategic_lawsuit_against_publ... [2] https://www.vox.com/future-perfect/23841742/francesca-gino-d... reply SMAAART 15 hours agoprevShe wrote the book \"Rebel Talent: why it pays to break the rules\". reply larsnystrom 9 hours agoparentPractice what you preach, as they say. reply PaulKeeble 19 hours agoprevThere is no limits a grifter wont go to in order to continue their grift. Those with the funds regularly get away with it purely because they have the funds to sue their opponents into bankruptcy. The system worked in this case but it so rarely does and we see so often people having to back down due to legal threats. reply onlypassingthru 18 hours agoparentsee: Lance Armstrong.† † https://www.abajournal.com/news/article/lance_armstrong_admi... reply jujube3 2 hours agoparentprevTime flies when you're having funds. reply dudeinjapan 10 hours agoprevIf only Prof. Gino had signed her papers at the beginning she would have been less inclined to falsify the data. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3458378/ reply mzs 3 hours agoparentbravo! reply xbar 2 hours agoprevArrivederci, Francesca Gino. reply kmeisthax 14 hours agoprevIn my opinion, defamation law is not fit for purpose, it's far too easy for rich people with axes to grind to sue their accusers in an attempt to stifle speech by putting a price on their words. Anti-SLAPP motions are supposed to fix this by allowing fee shifting, but they only exist in a handful of states, and I'm still not 100% sure how effective that alone can be at restoring freedom of speech[0]. Until such time as that legal system is fixed, the scientific community should take countermeasures against the law. I'm calling for some kind of universal default clause[1]. If you sue scientific accusers for defamation, you're not doing science anymore, so you should be automatically fired from or kicked out of any scientific institution you're a member of. You could obviously restrain this to only unproven allegations - i.e. get a scientific board to call the allegations bullshit first and then you can bring it to a court. But the pathway of allegations of fraud becoming \"put up $100k in legal fees or shut up\" is not acceptable in a free society. [0] Related note: do you remember when Donald Trump was saying he wanted to \"open up our libel laws\" to make it EASIER to pull this shit!? [1] \"Universal default\" is a concept in banking that treats defaulting on any loan the same as defaulting on the specific loan mentioned in the contract. reply nullc 5 hours agoparentUnfortunately for malicious litigants their whole goal is to damage you with the process or to use the threat of doing so to coerce you. They'll use defamation law when its the path of least resistance but if that's off the table they'll just cook up some other claim. reply taeric 19 hours agoprevThis whole ordeal has been crazy. Glad to see some sanity prevailing at this level. Curious where the rest of the story is at. Anyone have a good primer on where things ultimately are, at this point? Curious on more than just this case. If I recall, there were several large scale misconduct cases that hit back to back. All of those still in flight? reply nikcub 19 hours agoparentPete Judo's channel on YouTube has been covering this since the beginning, and other cases: https://www.youtube.com/@PeteJudo1 reply IG_Semmelweiss 18 hours agoparentprevI'd love to hear more about the outcome for harvard. it seems that they mostly did the right thing, but how they went about it may not have been conforming to the pre-determined rules for employees. reply dang 6 hours agoprevRelated. Others? Harvard Probe Finds Honesty Researcher Engaged in Scientific Misconduct - https://news.ycombinator.com/item?id=39712021 - March 2024 (15 comments) They studied dishonesty – Was their work a lie? - https://news.ycombinator.com/item?id=37714898 - Sept 2023 (153 comments) Crowdfunding a defense for scientific research - https://news.ycombinator.com/item?id=37393502 - Sept 2023 (47 comments) I’m so sorry for psychology’s loss, whatever it is - https://news.ycombinator.com/item?id=37315292 - Aug 2023 (94 comments) Support Academic Freedom of Speech - https://news.ycombinator.com/item?id=37153168 - Aug 2023 (1 comment) Is it defamation to point out scientific research fraud? - https://news.ycombinator.com/item?id=37152030 - Aug 2023 (13 comments) Harvard professor Francesca Gino was accused of faking data - https://news.ycombinator.com/item?id=36968670 - Aug 2023 (146 comments) Fabricated data in research about honesty - https://news.ycombinator.com/item?id=36907829 - July 2023 (46 comments) Fraudulent data raise questions about superstar honesty researcher (2021) - https://news.ycombinator.com/item?id=36726485 - July 2023 (33 comments) Harvard ethics professor allegedly fabricated multiple studies - https://news.ycombinator.com/item?id=36665247 - July 2023 (215 comments) Harvard dishonesty expert accused of dishonesty - https://news.ycombinator.com/item?id=36424090 - June 2023 (201 comments) Data Falsificada (Part 1): “Clusterfake” – Data Colada - https://news.ycombinator.com/item?id=36374255 - June 2023 (7 comments) Noted study in psychology fails to replicate, crumbles with evidence of fraud - https://news.ycombinator.com/item?id=28264097 - Aug 2021 (102 comments) A Big Study About Honesty Turns Out to Be Based on Fake Data - https://news.ycombinator.com/item?id=28257860 - Aug 2021 (91 comments) Evidence of fraud in an influential field experiment about dishonesty - https://news.ycombinator.com/item?id=28210642 - Aug 2021 (51 comments) reply kernal 3 hours agoprevHow ironic that the people exposing a fraudster must defend themselves from the false allegations of a fraudster. The sad and pathetic part is that Francesca Gino even tried to blame her colleagues for the fake data. These videos are a good primer on the dishonesty of Francesca Gino. https://www.youtube.com/watch?v=yNK4nXWA_s8 https://www.youtube.com/watch?v=QE5V_nW7pPo reply teekert 19 hours agoprev“… the court has ruled that evidence-backed conclusions regarding fabricated data cannot constitute defamation—which is probably a very good thing for science.” “Probably good for science”? That is science! reply kortilla 19 hours agoparentYes, that’s why the ruling is good for science. It would be a dark world if publishing statistical analysis opened you up to defamation reply bluGill 19 hours agorootparentIn the uk at least truth is not defense againts defamation. Beware if you live there. reply cortesoft 19 hours agorootparentMy understanding is it is a little more nuanced than that. The truth is still an absolute defense against defamation in the UK, just like it is in the US, but the difference is who the burden of proof is on. In the US, it is up to the person suing for defamation to prove the statements were false AND that the person knew them to be false when they said them (or should have known they were false). In the UK, for someone to use the truth as an absolute defense, they have to prove the statements were true, which is often not very easy. reply mr_toad 17 hours agorootparentIn both bases proof is only on balance of evidence, so the difference between proving something is or isn’t true is not as great as it would be if the standard was proof beyond reasonable doubt. reply dgfitz 17 hours agorootparentIt’s a lot easier to prove that there is a white swan than a black swan in the world somewhere. reply mr_toad 17 hours agorootparentprevThis is ironically, not true. “It is a defence for defamation to show the imputation in the statement complained of is substantially true.” https://en.wikipedia.org/wiki/Defamation_Act_2013#Defences reply bugglebeetle 19 hours agorootparentprevThe same is true in Japan. Very few are aware of how little press freedoms and speech rights are protected there and have regressed further under assault from the ruling LDP party. reply immibis 10 hours agorootparentprevThe same in Germany. reply smcin 18 hours agorootparentprevMy own post here [0] got suddenly multiply downvoted and flagged, even though I wrote it very carefully and double-checked every single statement. Truth defense. [0]: https://news.ycombinator.com/item?id=41526702 reply oefrha 9 hours agorootparentTL;DRs are frowned upon on HN because HN encourages people to read the article (especially before commenting) rather than Don’t Read the article.[1] You’re further breaking guidelines by complaining about votes. That said, your comment isn’t a summary of the article at all. [1] https://news.ycombinator.com/item?id=12667459 reply jakedata 17 hours agorootparentprevnext [2 more] [flagged] striking 15 hours agorootparentIf you suspect a voting ring is interfering with HN in some way, you can ask the mods and they'll pull the data and tell you. Is it not the more likely explanation that some people / a moderator decided a summary makes for boring reading, especially given that you need 500 karma to downvote? reply appendix-rock 19 hours agorootparentprevVery ‘ironic’ that this is quite a…dishonest representation of the truth. reply komali2 19 hours agoparentprevIt also cited a precedent I didn't know existed for the courts: > The court cites precedent to note that “[s]cientific controversies must be settled by the methods of science rather than by the methods of litigation” I wonder if this a specific case or just a general precedent? Is general precedent a thing? reply exegete 19 hours agorootparentI saw that quote in the article but cannot find it in the ruling itself: https://storage.courtlistener.com/recap/gov.uscourts.mad.259... It seems to be a quote from a London court ruling in 2010: https://www.bmj.com/content/340/bmj.c1895 reply vitus 15 hours agorootparentIt's mentioned around page 30-31: > Instead, this is a case where the trial of ideas plays out in the pages of peer > reviewed journals, and the scientific public sits as the jury.” (cleaned up)); Underwager v. Salter, > 22 F.3d 730, 736 (7th Cir. 1994) (affirming summary judgment against defamation claims and > stating that “[s]cientific controversies must be settled by the methods of science rather than by > the methods of litigation”). So, https://casetext.com/case/underwager-v-salter reply exegete 10 hours agorootparentThanks! I don’t know why my search didn’t find that. reply Validark 18 hours agoprev“[s]cientific controversies must be settled by the methods of science rather than by the methods of litigation” Boom. And the researchers didn't accuse a person of fabrication, they accused the data of being fabricated. reply sparsely 9 hours agoparentCourts have had little hesitation to decide these matters themselves for a long time - check out \"Bite Mark Analysis\" or any number of trials involving medical evidence. While I do think that they need to consider scientific matters they are vastly overconfident in the conclusions reached. reply baobabKoodaa 2 hours agoparentprevI don't see any distinction there. reply smcin 19 hours agoprev [–] [tl;dr] US District Court dismisses Francesca Gino's controversial $25m defamation lawsuit against Harvard Business School and DataColada scientist-bloggers (Simonsohn, Nelson, and Simmons). The judge allowed Gino's breach of contract claim against Harvard to continue. DataColada's legal defense (so far) was paid for by their own universities; there is also a $378K private GoFundMe organized by Simine Vazire [https://www.gofundme.com/f/uhbka-support-data-coladas-legal-...]. Yet apparently not a penny from the journals which Gino published in, or by HBS, or the funders of Gino's research. (EDIT: this got upvoted, then suddenly multiply downvoted and flagged - why? I wrote it very carefully and double-checked every single statement. If anyone's quibbling the term \"Francesca Gino's controversial $25m defamation lawsuit\", that term was used by the MBA-watching blog https://poetsandquants.com/2024/09/12/judge-dismisses-france... ) reply 0cf8612b2e1e 19 hours agoparentDo universities typically go to bat for their faculty? Presumably the blog investigation is not done with any direct university involvement. reply smcin 19 hours agorootparentI can't comment, but it's a truly bizarre spectacle to see research integrity being upheld by three public-minded researchers (one of them based in Spain) and a defense fund on GoFundMe organized by an Australian academic. The academic publishers didn't fund them and neither did the funders of the research. We need a discussion about frameworks to fund misconduct researchers (including related legal defense), also about pressures to publish irreproducible results with mediagenic, counterfactual findings. The entity with the least incentive to fully investigate HBS is, paradoxically... HBS itself... because questions have been raised about multiple other coauthors, the reputational and financial consequences and wrongful termination claims look set to run for a decade. Some have conjectured that whenever the full facts are finally disclosed, this scandal will take out the reputation of Harvard in general (not just HBS, or Behavioral Economics as a field, or priming), or all the Ivies, at least for business-school. reply rebanevapustus 11 hours agorootparentprevI was the victim of a pretty bizarre super in-your-face academic theft. Someone snooped a half-finished draft of mine off GitHub and...actually got it published in a real journal. https://forbetterscience.com/2023/10/30/stephensons-alternat... My institution completely brushed it off as a no problem, and said that it wasn't worth pursuing it at all. It was very traumatising (and still is). I lost all faith in academia. reply dotnet00 18 hours agorootparentprevThe universities might've deemed it to be an important enough PR situation and low risk enough to do it. Alternatively maybe they were obligated to back them up if the work was done in their official capacity as faculty? reply teractiveodular 15 hours agoparentprev [–] Do you have a source for their universities paying? Their fundraising page states that \"their employers have so far only agreed to pay part of the legal fees\". https://www.gofundme.com/f/uhbka-support-data-coladas-legal-... reply alach11 15 hours agorootparent [–] In the August 2nd, 2024 update on the GoFundMe: \"Because our universities have generously funded our defense up to this point, we have not (yet) had to use any of the GoFundMe money... we are reaching out to you to explain how we intend to use it. Obviously if we need the money for our defense, we will use it for our defense. Our plan is to donate any money we do not use to The Scientific Integrity Fund, an organization that defends scientists who find themselves in a predicament that is very similar to ours. \" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A court has cleared the Data Colada researchers of defamation for identifying manipulated data in a Harvard Business School investigation.",
      "Harvard will still face trial over its handling of the case, despite confirming misconduct by professor Francesca Gino, who is on administrative leave and may lose tenure.",
      "The court ruled that evidence-backed conclusions are not defamation, fully clearing the Data Colada team due to their cautious, evidence-based approach."
    ],
    "commentSummary": [
      "Data sleuths accused of defamation for identifying research misconduct have been cleared, with the case dismissed before discovery.",
      "The court ruled that evidence-backed conclusions about fabricated data do not constitute defamation, supporting scientific integrity.",
      "The defendants raised over $300k on GoFundMe for their legal defense, highlighting the high costs and emotional toll of defamation lawsuits in the US."
    ],
    "points": 519,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1726176497
  },
  {
    "id": 41528075,
    "title": "Boeing workers vote to strike",
    "originLink": "https://www.washingtonpost.com/business/2024/09/13/boeing-union-contract-strike/",
    "originBody": "Tens of thousands of Boeing machinists voted overwhelmingly to go on strike after rejecting a contract offer from the company Sept. 12. (Video: AP) By Rachel Lerman, Lori Aratani and Ian Duncan Updated September 13, 2024 at 3:19 a.m. EDT|Published September 13, 2024 at 12:32 a.m. EDT SEATTLE — Boeing workers picketed outside the company’s plants in Washington state early Friday morning after voting overwhelmingly to strike. Tens of thousands of machinists voted Thursday to reject a proposed deal between the company and the union that would have significantly boosted pay and benefits even as it fell short of other union demands. Some 96 percent of members of the International Association of Machinists and Aerospace Workers District 751 voted in favor of the strike — far more than the two-thirds needed to launch the work stoppage. “Boeing has to stop breaking the law, has to bargain in good faith, and we will be back at the table whenever we can get there to drive forward on the issues that our members say are important,” Jon Holden, president of IAM District 751, told a room of machinists at the Seattle union hall. Advertisement Story continues below advertisement He was met with loud cheers and a chant of “strike, strike, strike,” from the workers, many of whom carried signs related to the stoppage. The walkout is a stinging rebuke for Boeing and could represent the most disrupting challenge yet for a company that has spent much of this year in damage control as it careened from crisis to crisis. The strike risks derailing the aerospace giant’s recovery from ongoing financial and safety challenges and could cost the cash-strapped company an estimated $1 billion per week, according to analysts. The union plays a key role in assembling some of the company’s best-selling aircraft. The most direct impact is on Boeing’s assembly plants in Washington, especially in Everett and Renton. An extended work stoppage could also impact Boeing suppliers and possibly shrink its share of the aerospace market. Advertisement Story continues below advertisement Machinists in Seattle said the strike was long coming. “We just want to be treated right and they’re not doing it,” said mechanic Charles Fromong, who has worked for Boeing for more than 37 years. “So I guess we’re going to get it done.” Boeing said early Friday that it would return to the bargaining table. “The message was clear that the tentative agreement we reached with IAM leadership was not acceptable to the members,” the company said in a statement. “We remain committed to resetting our relationship with our employees and the union, and we are ready to get back to the table to reach a new agreement.” After a string of tense, marathon negotiating sessions over the last several weeks, the IAM and Boeing announced Sunday that they had reached a tentative four-year agreement, including a 25 percent pay increase over four years and enhanced health and retirement benefits. Also significant: If workers had voted to accept the deal before the current contract, Boeing committed to building its next new aircraft in Washington state, a key union demand. Both sides and investors had cheered the deal. Advertisement Story continues below advertisement The optimism, however, proved short-lived. On Monday, Holden told the Seattle Times that members would probably reject the deal. Opposition grew as workers staged rallies and took to social media to vent their frustrations with the company’s offer. A copy of a flier obtained by The Washington Post urged members to “VOTE TO REJECT BOEING’S BAD DEAL,” circulated at many of the company’s plants. Machinists also were angered by the elimination of their annual bonus program. “We’ve got a lot of leverage — why waste that?” said Joe Philbin, a structures mechanic, outside the voting hall in Renton on Thursday. He’s worked for the company for six months and wants to see the mandatory overtime rules change. Several union members, who were being shuttled in from the nearby Renton plant on buses, said they were voting to reject the deal because they wanted to see higher pay increases. Advertisement Story continues below advertisement “Four years is not enough to make up for the last 16,” Boeing worker Roger Ligrano said before he voted. He said he was voting to strike, in part, to give union members more time to understand a deal. Share this article Share Harold Ruffalo, who has worked for Boeing for 28 years, said after the vote results were announced that too much corporate greed is impacting the company, and workers need more money to live as inflation hits paychecks. “They need to take care of us,” he said. The Biden administration was monitoring the situation; acting Labor Secretary Julie Su has been in contact with both sides. Boeing executives spent much of the week trying to salvage the deal, urging IAM members to put the past grievances behind them. Story continues below advertisement “I hope you will choose the bright future ahead,” Boeing CEO Kelly Ortberg said in message to employees on Wednesday. Advertisement “Working together, I know that we can get back on track,” he continued. “But a strike would put our shared recovery in jeopardy, further eroding trust with our customers and hurting our ability to determine our future together.” But workers rejected his plea for solidarity. “I want the company to be fair with us,” said T E Sue, who has worked at Boeing for more than 35 years and said it was the “worst contract” during his time. “We’re the bread and butter of the company.” Leading up to the strike deadline, analysts said they were worried about how long a strike would last. They said that many workers have not forgotten previous rounds of negotiations in which Boeing pushed for concessions — including the end of the traditional pension program — to keep aircraft production in Washington state. Advertisement Story continues below advertisement Michael Bruno, Aviation Week Network’s executive editor for business, said in previous rounds of negotiations Boeing threatened to move airplane production to other states to extract concessions from the union, which soured relations. The last time IAM members struck was in 2008, a 57-day day walkout that Moody’s estimated cost Boeing about $1.5 billion a month. Boeing reopened negotiations on that contract twice, in 2011 and in 2013, and won significant concessions from workers. Boeing has struggled to recover from major safety, financial and legal setbacks that began in January when a door panel of a 737 Max jet blew out of the fuselage in midair, leaving a gaping hole. Multiple investigations into the calamity uncovered serious shortcomings in the company’s manufacturing and safety oversight systems and led the Federal Aviation Administration to limit the number of 737 Max jets Boeing could build until it meets certain quality and safety milestones. Advertisement Story continues below advertisement In May, the Justice Department announced that Boeing had failed to meet the conditions of an agreement that shielded Boeing from criminal prosecution in connection with a 2018 crash of a Boeing Max jet in Indonesia and a second one in 2019 in Ethiopia that killed 346 people. Boeing agreed to plead guilty to one count of criminal fraud in connection with the case — a settlement that must still be approved by a federal judge. The company also has experienced major setbacks with its Starliner space program, which has been plagued by delays and cost overruns. The space capsule returned to Earth earlier this month, but without two astronauts it had carried to the International Space Station after NASA decided it was too risky to use the Boeing craft. Ortberg, the former chief executive of Rockwell Collins, took over the top job last month, pledging a new beginning. Share Comments Sign up",
    "commentLink": "https://news.ycombinator.com/item?id=41528075",
    "commentBody": "Boeing workers vote to strike (washingtonpost.com)468 points by isaacfrond 14 hours agohidepastfavorite428 comments csomar 10 hours agohttps://archive.is/2X1KV benced 37 minutes agoprevA lot of people here seem justifiably angry at Boeing management's total destruction of an engineering corporate culture. It's unclear to me if fixing that is what the machinists are demanding or if they just want normal union things like being paid more and working less. No hate if they are optimizing for that, unions don't exist to serve corporate culture. Just want to be clear-eyed about what the union is seeking and (potentially separately), what it will take to make Boeing an American great again. reply kedean 35 minutes agoparentIt's not totally clear to me, but it is telling that the union members specifically say boeing needs to \"stop breaking the law\" and that the rejected deal included an allegedly large pay increase. 96% turning that down doesn't feel like the increase just wasn't enough to me reply hosh 10 minutes agorootparentIt goes with the perception of corporate greed. Boeing stocks have shot up, profits are not being shared, and those profits came at a cost of safety, and it isn't as if market share had not still declined against Airbus. I wouldn't be surprised if a machinist used to be able to go home, proud of the work they have done, and now, it is not so. reply iancmceachern 10 hours agoprevAny Boeing engineers who may be looking for alternate paths, please know that I've known many great aerospace engineers who have made a pivot to medical devices very successfully and many of the best medical devices to come out in recent years are due to such. It seems very different but really it's very similar. reply Balgair 5 hours agoparentI'll echo this and have done something of this path. It's a very similar field surprisingly. Quite rewarding too. The timelines for products are also about the same (years-decade). The one tip is to not bother applying to Medtronic. They don't actually hire anything outside of interns. All the job postings are for internal roles (but required by federal law to be ... blah blah blah). Suffice to say, don't bother. reply eddd-ddde 1 hour agorootparentI'm curious. So this company basically only hires newbies and trains them from zero to any role they might need? That's kinda cool. reply behringer 1 hour agorootparentExcept for the worker that will see stagnant wages. reply ghaff 0 minutes agorootparentI suppose the theory is that people won't see significant pay increases if they stay with the same company (but also presumably that if everyone leaves after training, the company will just stop training workers). mbrameld 18 minutes agorootparentprevCouldn't they change employers after leveling up their skills? reply SecretDreams 6 hours agoparentprevConversely, I've seen a lot pivot from aero to auto and flounder. They are kind of opposite industries in terms of pacing and design compromises, as well as the fact that most of a car has to plasticize in crash events which is not a primary consideration in aero. reply Zigurd 4 hours agorootparentCounterexample: Mulally reply SecretDreams 4 hours agorootparentI was moreso referring to ICs, but fair point. reply jandrese 4 hours agoparentprevOn one hand I can't blame any engineer that wants to flee, but on the other Boeing really can't afford any more brain drain. We are already seeing the results of years of forcing experienced but well paid engineers out and outsourcing their jobs. Planes literally falling out of the sky, but some nice fat executive bonuses. reply MPSFounder 2 hours agorootparentAnecdotal, but out of my graduating class in chemical engineering from a University in Seattle, the top 20 students went into tech and finance. The worst performing students (by academic metrics) went to Boeing. Over time, I expect decisions to be made by engineers which are not stand outs. Coupled with the MBA trainwreck prioritizing profits and cost cutting, I expect Boeing has a very rough patch ahead. reply dh2022 1 hour agorootparentTop 20 students in chemical engineering went into tech and finance.... two professions that at the first sight have nothing in common with chemical engineering... Tech and finance must really be looking for a lot of employees.... reply protastus 25 minutes agorootparentNo doubt, Big Tech prints money and absorbs all available talent. Boeing's troubles are surely compounded by having all the best new talent hired away from them, because Big Tech pays much better. reply boc 54 minutes agorootparentprevOr they just pay a lot more. If Boeing was paying $500K+ TC for engineers in their early/mid career, you'd see a lot more expertise enter the field. Instead you basically have to go into tech or PE/IB to see that type of compensation fresh out of school. Especially when students are taking on hundreds of thousands in debt, the payback period becomes really important when considering career paths. reply dh2022 51 minutes agorootparentI completely understand graduates would choose higher TC from tech/finance vs TC from Boeing. What I do not understand is why tech/finance companies would want to hire chemical engineering graduates..... I know a few people who changed careers from biology to finance - but that was only after going back to school and getting some business degree.... reply darod 13 minutes agorootparentbecause they solve a lot of math problems very well. reply Der_Einzige 1 hour agorootparentprevI always wonder who these folks who graduate with excellent STEM degrees are who can't land a job. I had a similar experience with my graduating class in undergrad computer science at a no-name state university. Of the graduating class (significantly smaller than when we started), I don't know of a single person who couldn't find some kind of gainful tech related employment - including the few folks who somehow could not code! by the end of our program. My experience is circa 2014, so only shortly after the great recession. Where are these people who just can't get any kind of work with real CS degrees? Even the many hardcore cheaters I know of found ways into the FAANG (some have been promoted several times now too!) reply spratzt 58 minutes agorootparentI suggest you come over to the UK. I know several guys with postgraduate STEM degrees and 10 years of development experience who can’t even get interviews. It’s very bad here. reply infamouscow 2 minutes agorootparentprevThere is literally nothing actually preventing Boeing from: 1. Firing 100% of management between CEO and lowest level supervisors that make things happen. All of middle management should go. 2. Promote supervisors into middle management. 3. Promote ground level employees into supervisor roles. 4. When human problems happen (and they will), spare no expense with resources and training. Remember, the reason for firing management is because they had the interpersonal talent, but fundamentally lack aeronautical talent. Interpersonal talent is of minimal value at Boeing. reply deepsun 3 hours agorootparentprevThat is a feature of capitalism and effective market. If a company falls a little bit below competition -- market forces it do dive even deeper, die quicker. I'm not saying it's good or bad. I've seen communism, and it's much worse to help failing companies -- they tend to fail even more instead of improving. The problem now is that international market is not really fair, trade treaties help too little. reply noisy_boy 2 hours agorootparent> If a company falls a little bit below competition -- market forces it do dive even deeper, die quicker. Feature of nature too, the jungle forces the weak to die quicker. One more argument for companies != people: we will be a lot more ok for this to happen to companies vs it happening to people. reply salawat 1 hour agorootparentSolution: for e supremacy of compliance with regulations over fiscal performance. You have to be able to check all of the boxes reliably before we can talk about innovating or optimizing. reply weaksauce 1 hour agorootparentprevboeing isn’t a company the us government will let fail. it’s too important to have as a domestic ability. reply UberFly 1 hour agorootparentThis is true and I wonder if it's going to put them even more into a zombie state of existence. Their bad business practices will keep getting bailed out and they'll never really improve until a better company comes along. reply gosub100 3 hours agorootparentprevPivoting to a remote-first culture would be a great start. Tons of talented engineers angry about RTO to draw from. reply wannacboatmovie 1 hour agorootparentWhy would a manufacturing company - who is clearly having manufacturing problems - pivot to a remote-first culture? What problem would that solve? It's hard to build airplanes in your driveway, but I can't say that I've tried. That door-blowing-off thing wasn't a design issue. \"It's hard to forget to install the bolts when you're working from home\" is a bit of a logical fallacy. reply wetpaws 3 hours agorootparentprevWhy it should be the engineers problem? reply onepointsixC 3 hours agorootparentIt isn’t. But Boeing failing and turning the commercial airline market into an Airbus monopoly is bad. reply bibelo 9 hours agoparentprevor they could come here to Toulouse to work for Airbus ^^ reply fecal_henge 4 hours agorootparentThey have nothing Tolouse right now. reply Avshalom 3 hours agorootparentThat feels like too lousy of a pun to be making when we're talking about peoples lives here. reply fecal_henge 2 hours agorootparentSorry for boeing so insensitive. reply atlantic 2 hours agorootparentGood puns. Just out of curiosity, is your name a synonym for sh*tpost? reply gaius_baltar 1 hour agorootparentprevJokes on Boeing are ok, they are just people with some loose screws. reply benhurmarcel 6 hours agorootparentprevThat's not a great plan if what they're looking for is a higher salary though. Engineering in western Europe doesn't pay nearly as well as in the US. reply mricordeau 5 hours agorootparentYes but Toulouse is way cheaper than any Tier 1 city in the US and you have to include insurance/health, school cost in the US (I'm from Toulouse and living in the US for more than 10 years). If you make 250k/year in Tier 1 city in the US it's probably as good as making 80k/year in Toulouse. reply kfajdsl 22 minutes agorootparent- Engineering jobs in the US tend to come with health insurance benefits - There are several state programs in the US that make attending an in-state public university very affordable. For example, in GA I attended university for $0 in tuition (only paying for room and board + a couple hundred bucks a semester on bs “fees”). The requirement for that grant is getting a 3.0 or higher high school GPA. - Not sure how cheap Toulouse is, but at least in the US you’re probably better off making 200k+ in HCOL than 100k in LCOL. At that level of income, you don’t have to spend much of your income on essentials even in an extremely HCOL area like SF ($3000 a month gets you a nice apartment). reply Rinzler89 5 hours agorootparentprevDon't Boeing engineers also get insurance and 401k from their employer? Plus, they'd have to learn french also. France is not very accommodating to non French speaking foreigners. Trying to get around in life with just English there outside of Paris is not easy. reply kpw94 2 hours agorootparent> France is not very accommodating to non French speaking foreigners. Trying to get around in life with just English there outside of Paris is not easy. I don't really get this kind of comments... Usually people also say the same thing when visiting Japan. \"This restaurant only has menu in Japanese and staff only speaking Japanese!!\". That's true a bit everywhere in the world, isn't it? In the US, apart from places with say huge Spanish speaking presence, you better interact in English. Try \"getting around in life\" using only say French, or Portuguese, or Japanese in a random US city like Portland, NYC, or Chicago. reply retzkek 1 hour agorootparentI didn't sense any judgement there, just a statement of fact. Learning a new language as an adult is doable, but not trivial, so it's certainly a factor in making a decision to relocate to another country for a job. reply dataflow 1 hour agorootparentprev> That's true a bit everywhere in the world, isn't it? No it's not, western Europe for example has a bunch of countries where English is almost as good as native. But obviously that's not the common case across the world, and like you say there's nothing wrong with expecting people to know the local language. reply Rinzler89 1 hour agorootparentprevNot really, the world isn't either black or white but various shades of gray. Everything North of Benelux is a lot friendlier and open to speaking English and doing things in English outside of capitals, compared to places like France where not speaking it gives you a severe handicap in life and career. reply benhurmarcel 4 hours agorootparentprevThere are quite a few employees in Toulouse that don't speak French, the company is very international. That being said I agree that daily life is much easier if you speak at least a little bit. reply FireBeyond 2 hours agorootparentprevMy experience, although 20ish years ago now, was that France was very accommodating to people who were trying to make an effort to speak French, however badly, and would help correct pronunciations and other little errors, whereas their patience for foreigners who thought that the locals English would get better if they just spoke more loudly and slowly was thin. reply hobs 4 hours agorootparentprevThat's funny you say that because my experience of France is that everyone outside of Paris seems to hate the Parisians more than me, an American. reply jdminhbg 1 hour agorootparentprev> If you make 250k/year in Tier 1 city in the US it's probably as good as making 80k/year in Toulouse. If you are making $250k/year, you are already getting health insurance, and even if you weren't, it doesn't cost $170k. reply smallnamespace 3 hours agorootparentprevBut if I save money in the US I can decide to spend it anywhere else in the world later, including places that are much cheaper than Toulouse. If I make money in Toulouse then I had better really love Europe, since I will have limited resources to relocate elsewhere if I ever change my mind. Tl;dr one advantage of getting paid in money over services is that money is much more portable. reply diggan 5 hours agorootparentprevTrue, as long as you don't consider cost of living, quality of life, work/life balance and life expectancy. But judging by the current policies and laws (or lack of them) in the US, seemingly \"high salary\" goes above all of those things for most people. reply echelon 7 hours agorootparentprevBoeing engineers should leave and start their own company. \"American dynamism\" is hot right now and there should be ample funding available. Lots of the old dinosaurs are being nipped at by nimble upstarts like Anduril. reply Maken 5 hours agorootparentI seriously wonder how long it would take for a team of engineers to produce a competitive commercial airplane, from planning to the first prototype, if they started right now. Also, is it viable to produce something like the A320 in low quantities when airlines need fleets in the hundreds? reply wannacboatmovie 2 hours agorootparentDesigning an airplane and producing a prototype isn't terribly difficult and many thousands of companies have done this. The tricky part is designing the massive manufacturing apparatus around it that can produce them in volume (ask Elon how easy building cars is), satisfying the varying demands of hundreds of different airlines, many from developing nations, helping them set up financing, supporting the design in the field for 30-50 years, AND turning a profit. Each widget produced has an MSRP of hundreds of millions of dollars. Ask yourself why Lockheed permanently exited the commercial aircraft business 40 years ago, despite having what was regarded as the most technologically advanced design of the era, ahead of its time. Douglas went bankrupt and got bought out, Convair and everyone else failed and closed up shop. You can't throw some engineers - they could be the smartest on the planet - into rented office space and become the next airplane company. There's more to it than designing the next WiFi-enabled food processor and slapping it together in Shenzhen. reply Corrado 44 minutes agorootparentActually, thinking about Tesla is appropriate. No one really thought starting a new automotive manufacturer from scratch was possible, but Tesla did it. Yes, it's very hard and you'll probably fail, but if you do things different you might actually have a chance. The reasons all the old aircraft companies failed is probably the same reason Boeing is struggling. Don't copy what they've done in the past, make your own way. Yes, I know that building aircraft is a much higher hill to climb, but I think it might be worth it. They could also join some of the small aircraft startups that are trying to gain a foothold. Maybe a cadre of experienced aircraft engineers would help them raise money and get a product to market faster. reply toomuchtodo 5 hours agorootparentprevWhy? Have the government step in, nationalize Boeing, give the union a board seat, and keep shipping. The problem is Boeing management and their board enabling the train wreck; don't build from scratch, refactor. reply enriquec 3 hours agorootparentnationalize? with the government that loses $500B to fraud and $3T to interest a year? What a horrible idea. reply charlie0 4 hours agorootparentprevSpaceX has done such a phenomenal job rebuilding from scratch. How is it not obvious to you rebuilding airlines from scratch is also the best route? reply toomuchtodo 4 hours agorootparentLottery tickets are not policy. SpaceX took decades to get to the success they realize today, having been founded in 2002, and it took a lot of luck to build a rocket shop from scratch. We must realize that companies are not built out of Legos, but ecosystems that require care, feeding, and are fragile systems. reply jonhohle 4 hours agorootparentPayPal, Tesla, SpaceX,… how many times does someone “win the lottery” before it’s not actually chance? Not saying anyone can repeat it, or a random IC could leave and succeed, but that space seems ripe for disruption. (Maybe not passenger planes, but cargo with less liability.) reply toomuchtodo 3 hours agorootparentIf 90% of startups fail, why would you think winning wasn't mostly chance? Regardless of how many people work hard and grind, you can still fail (and most do, that's just life). Some are more lucky than others, and capital begets capital. Once you've \"won\" enough, it becomes much harder to lose, even if you make terrible forward decisions and run off the inertia of past wins (Twitter). Bezos built Amazon, but also has sunk somewhere between $10B and $20B into Blue Origin and it is still not terribly successful, for example, and demonstrates that even with resources and skill that success is potentially out of reach. So, while I think the startup model is a fine way for investors to get exposure to an asset class that is the equivalent of 0DTE options, for the startup ecosystem to enable participants to play in the fiat that falls from those investment decisions, and perhaps some value to be generated, I would hazard that the model would not scale to meet the needs of the aerospace and defense marketplace at this time. All that is needed is skilled engineers and manufacturing practitioners to be enabled to do their best work, while keeping out of their way. Airbus demonstrates this, imho. They employ almost 150k workers, and successfully deliver products to customers that aren't fraught with manufacturing defects. They also have a book of work into the next decade, demonstrating customer confidence in the product and the org. https://luisazhou.com/blog/startup-failure-statistics/ https://www.forkingpaths.co/p/billionaires-and-the-evolution... https://escholarship.org/content/qt6668s4pz/qt6668s4pz.pdf reply somenameforme 2 hours agorootparentI don't entirely understand this logic. 99.9% of players that play competitive chess will never make master, but of course nobody would then say that becoming a chess master is just down to chance. What percent of startups are just bad ideas, outright cons, hair brained money-first schemes, or people entering into competitive domains (like eateries) without sufficient skill? IMO you're probably pushing 90% there! To me the main thing that the failure of Blue Origin demonstrates is that the notion of \"business\" as some generalizable and all-applicable skill is nonsense. Bezos has done an amazing job of overseeing a digital marketplace, but that doesn't somehow mean he'd be amazing at overseeing an aerospace company. To me this just seems like it should be obvious. The vision, talent, and other such things are just so radically different. For instance Musk picked up his first engine engineer [1] based on engines the guy was literally building in his garage. Bezos just staffed Blue Origin with a bunch of people from old space, and so it seems quite unsurprising that you just end up with another Boeing, but without the legacy hardware and political cronyism to use as crutches. [1] - https://en.wikipedia.org/wiki/Tom_Mueller reply WalterBright 1 hour agorootparentprevMusk has had 3 huge successes - Paypal, Tesla, SpaceX. Having one such success might be luck. But when it's 3 times, dismissing it as luck is not very credible. The same goes for Steve Jobs. 3 enormous successes. And Bill Gates: 1. dominate 8 bit microcomputer software 2. pivot to 16 bit DOS. 3. pivot to 32 bit Windows. 4. pivot to internet. If you don't think that was a big deal, none of the other microcomputer software companies survived. Lotus, for example, muffed the pivot to 32 bits. reply double0jimb0 3 hours agorootparentprevBezos doesn’t know how to build good hardware. Plenty of examples from Amazon. Then add Blue Origin to that track record. reply toomuchtodo 3 hours agorootparentSure, but can you not hire people who are good at hardware with billions of dollars of investment? Or, is it culture and an intangible ability to procure and orchestrate great people doing great work that leads to success? Think in systems. If you have the resources, and the physics demonstrate it can be done, the system is not properly configured, no? reply double0jimb0 1 hour agorootparentcan you not hire people who are good at hardware with billions of dollars of investment? Clearly not in Bezos’s case. There is no magic involved here, some people have the skill/experience to lead the design, build, and selling of innovative hardware, others don’t. For example, for designing new rockets, one of the major things Musk innovated at SpaceX was the traditional aerospace design cycle. Instead of spending 5 years on countless analyses and few actual tests, like the incumbents did/do, SpaceX built and destructed as many prototypes as possible, learning rapidly and innovating. Musk knows what innovative design truly requires. What did Bezos do? He hired a bunch of ex-Lockheed and Boeing engineers/leads (from an industry that for decades has not felt market-driven pressure to innovate), and those engineers/leads just kept doing things the old fashion way. When the person in charge (Bezos) misses this detail, no amount of money or wishful thinking will fix this. reply FireBeyond 2 hours agorootparentprevObligatory repeat of this detail, since you're talking about PayPal as a Musk success story: Musk had very little to do with the success of PayPal. I'm not even talking in terms of the \"Gwynne Shotwell is the real genius of SpaceX\" naysayers. I'm talking: Musk had an attempt at an online bank that was ... not going well. Confinity had done what Musk couldn't - had built a prototype/MVP of PayPal. They'd already got it running. They had trademarks, everything. At this point, they'd created PayPal having nothing to do with Musk. So Musk and his company architected a merger with Confinity. As a result of this merger, Musk was the largest shareholder, and was made the first CEO. He remained CEO for only four MONTHS, most of which he spent trying to be stubborn about throwing away the entire prototype to rewrite it in Windows/IIS and Classic ASP (i.e. Visual Basic) because he didn't understand Solaris and Java. The Board got so sick of this that a couple of days after his four month anniversary, when he'd just left for two weeks off on his honeymoon of all things, they fired him in his absence. Think about how badly you have to fuck up as a CEO of a company that you're the major shareholder in that they fire you (no \"concentrating on my family\", no \"exploring other opportunities\"), AND do it while you're on your honeymoon. Following that, Musk's \"contribution\" to PayPal was mostly collecting shareholder dividend checks. I despise Musk. But I will give him credit for his contributions to Tesla and to SpaceX. PayPal, though? That's just another rewriting of history to support the Musk idolatry. reply TheAmazingRace 10 minutes agorootparentHe also has his long and obnoxious obsession with x.com. This spanned back to PayPal and he had to try this crap again with the Twitter rebrand, that frankly hasn't worked all that well, because most within my circle still call it by the old name Twitter. lotsofpulp 2 hours agorootparentprevThere is not one union, there are multiple employee unions, all fighting for their piece. Not only are there multiple unions, but even within a union, the older members usually vote against younger members. It’s not a terrible idea, but also not a simple panacea to aligning interests in a business where payoffs happen decades in the future. reply SoftTalker 2 hours agorootparentprev> I seriously wonder how long it would take for a team of engineers to produce a competitive commercial airplane. About 20 years for a fully-certified new commercial airliner design. They could have experimental prototypes flying much sooner of course. Even in China, their state-owned effort to develop a new narrow-body airliner took about 15 years and that's very likely with the CCP greasing the skids for them as much as they could. They are still not certified in Europe or North America. And some of the problems we've seen with Boeing have nothing to do with engineering, but problems with subcontractors, materials, and assembly. reply cptcobalt 3 hours agorootparentprev> Also, is it viable to produce something like the A320 in low quantities when airlines need fleets in the hundreds? Bombardier thought so, and did so! They developed and introduced the C-series aircraft to compete with smaller 737s and A320/A319. After introduction, Boeing fucked them over so hard that they sold the aircraft program after introduction to Airbus for a token sum. Airbus now builds and sells the C-series as the A220. https://en.wikipedia.org/wiki/CSeries_dumping_petition_by_Bo... reply lainga 3 hours agorootparentAnd it went right into the little shelf in many Canadians' heads of \"Canadian companies destroyed by the USA\". Mauldite en soit trestoute la lignye. reply bornfreddy 1 hour agorootparentTrying to translate your last sentence to English gives \"Mauldite in itself very all the ligny\". Trying to autocorrect it gives \"Maudit soit toute la ligne\" which apparently means \"Damn the whole line\". So... am I close? :D reply badpun 5 hours agorootparentprevI remember reading that one engineer spent 6 years designing „ventilation for seats 40-80” on one of the larger Airbuses. It seems like the amount of design and engineering work required for a prototype of a complete aircraft (assuming you buy the engines) is just immense. reply kevindamm 5 hours agorootparentHow much of that time was actual engineering-tradeoff decision-making and how much of that was working in a large corporation with abundant communication overhead? reply seabird 1 hour agorootparentFor a safety critical system, the work documenting, explaining, testing, validating, etc. a decision outstrips the work it took to make the decision. It is that way for a very good reason. The problem with it requiring so much work and time isn't that there's BigCorp bureaucracy that needs disrupting, it's that there isn't a problem with the amount of work and time required. reply kevindamm 4 minutes agorootparentI'm aware of that, but the proportion of overhead varies per company. Six months seems like a long time for a ventilation system, and the point being made in this thread is that the runway available for some spun-off group of former Boeing engineers would need to accommodate the very long schedules of the design stage. I'm just curious how much inflation there is in those schedules because I'm sure it's not zero. But you make a good point that these long runways are because the overarching tradeoff is one that prefers taking as long as it takes. badpun 2 hours agorootparentprevI'm betting a lot of that is reviews, approval, documentation, testing, re-testing etc - but it may be required for building a safe aircraft, and for getting it licenced to fly. reply fragmede 7 hours agorootparentprevHave you seen how much lambasting Boom Aerospace gets every time they come up? Starting a new aerospace company isn't for the faint of heart! reply GuB-42 6 hours agorootparentWell, Boom is not any aerospace company, they want to make supersonic planes. Supersonic travel is expensive and environmentally unfriendly, and it will probably always be, because it requires more energy, because physics. All that for the minor advantage of saving a couple of hours on select flights. What it means is that it is a privilege for the wealthy (because it is expensive for what you get), at the expense of everyone else (because of the environment). So of course it is going to be unpopular, except to the wealthy in question. It doesn't mean Boom can't be successful because the public opinion is negative, if the rich can pay. I am still not convinced though, after all, Concorde didn't fail technically (and it still flew after that one accident), it failed commercially. Boom is not taking the easy path here, since it is a technically hard problem with a dubious market. reply myrandomcomment 2 hours agorootparentAs I am currently sitting on a flight from Tokyo to SFO I would greatly appreciate the flight taking half the time. Boom should be as efficient current aircraft. They will operate at 60K feet where the drag is much less and fly for less time. Their engines are being designed to run on 80% biofuels. But let's see how it turns out in the end. reply GuB-42 1 hour agorootparentAre you travelling first class? If you are not, then you are probably not making the right comparison. Supersonic travel will absolutely not be for those who are flying economy right now and complaining about reclining, legroom, and crying babies. The kind of things that make flights feel very long, and yet, that's how most people fly despite much superior alternatives, because it is cheaper. Judging by how much it cost to fly on Concorde, it is reasonable to assume that a supersonic ticket will be equivalent in price to first class, or at least a very good business class. It means a seat that can recline 180°, good enough to sleep on, an internet connection suitable for remote work, as I expect it to become standard in the near future, a decent meal and some privacy. In these conditions, saving a few hours may not be as desirable as it is in economy class. Knowing that in order to shorten your trip, you will be sacrificing some of that comfort, or pay even more, maybe getting close to private jet territory. reply meiraleal 6 hours agorootparentprevComputers used to be very, very expensive. Residential telephone lines were acquired through mortgage a couple decades ago in my country so being expensive now doesn't mean expensive forever. reply adgjlsfhk1 6 hours agorootparentunlike with computers, there's some pretty obvious barriers that limit the efficiency of supersonic airplanes. your volume has a strict lower bound from the size of passengers and luggage, your engines have a lower bound since there's only so much air you can push against, and your drag has a lower bound of a perfectly smooth aerofoil that produces lift and can fit the people and luggage. Even if you take the most optimistic assumptions that didn't violate physics, your fuel burn isn't going to be reasonable. reply thehappypm 5 hours agorootparentWhat if you leave the atmosphere? reply adgjlsfhk1 4 hours agorootparentThat's a thing called a rocket. Those have different constraints (e.g. energy proportional to mgh to get out of the atmosphere, dramatically less efficient engines since you now need to carry your oxidizer, etc. It seems vaguely plausible that for very long distances (e.g. New York to China), a rocket could be more efficient than an airplane, but it does seem pretty unlikely that a rocket can be as efficient as a normal airplane. reply krisoft 2 hours agorootparentPlus the added benefit of looking exactly like a ballistic missile attack on radar. :D Hope nobody with a twitchy trigger finger ever mistakes your flight for one! And also hope that no enemy will try to disguise their incapacitating strike as a scheduled flight. reply bluGill 6 hours agorootparentprevNo, but there are nothing I'm aware of changing the economics. Electronics were coming down in price for decades before computers reached the average house (of course when they reach the average house is debatable - on one extreme the Atari 2600 had a CPU, on the other there are still remote villages that are just adopting phones ) reply vundercind 6 hours agorootparentprevWhat’s expensive, largely, is the fuel to shove something through the air faster than the speed of sound. Short of sci-fi frictionless materials cheap enough to cover an aircraft, I don’t think you’re going to see a big breakthrough there. reply beaned 6 hours agorootparentprevI think that take is a little cynical. If it's only for the rich then the prices will be high. Meaning the capitalist mechanism of resource distribution will be even higher (more paid by the rich received as income by the non-rich). It will also take demand from existing airlines making fares lighter for everyone else. It also employs people. It also drives technology forward. And ultimately it does let people travel in less time, and why wouldn't we want that? To some extent emissions are not as bad as you'd think since they are being emitted over less time in the course of a shorter journey. Success in this category will also drive competition in every metric and work to bring cleaner, shorter flights to everyone over time. There is a lot to love about the idea of supersonic flight. reply AlexandrB 6 hours agorootparent> It will also take demand from existing airlines making fares lighter for everyone else. That's not how airline economics work. The first class passengers (the ones who could afford to leave for supersonic) subsidize the economy seats[1]. If they left you would probably see worse prices, worse amenities or both. There are airlines that don't have business or first class seats (e.g. Spirit), and they're generally a terrible experience. [1] https://www.wsj.com/articles/first-class-vs-coach-a-game-of-... reply tmhrtly 6 hours agorootparentprevNot fully convinced by the \"will make fares lighter for everyone else\" argument. The economics of planes are heavily weighted towards the passengers up front - business & premium economy make more profit per sqft for the airline than seats at the back. So I'd imagine that a reduction in demand for premium seats could actually increase prices. reply FireBeyond 2 hours agorootparentThe flipside of this is that the passengers in the back, while not as profitable in the \"$/sqft\" equation, are what merit the airline buying a 777-300 or A350. If all your focus is on those premium passengers, then you don't need as big an aircraft, and you end up with things beginning to approach JSX's (https://www.jsx.com/home/search) mode of operations. reply nemetroid 6 hours agorootparentprev> To some extent emissions are not as bad as you'd think since they are being emitted over less time in the course of a shorter journey. Emissions are usually compared in amounts per passenger-kilometer. reply tjpnz 2 hours agorootparentprevThey're an aerospace company? It seems I've been confusing them for a lobby group trying to remove urban noise regulations. reply sealeck 7 hours agorootparentprevI agree but not because \"boohoo people aren't nice enough to us\" – because it's a tough market to break into with (justifiably) high regulatory scrutiny, high R&D costs and few investors who know what they are doing. It's very natural that any company be subject to scrutiny – this doesn't mean that you shouldn't set up a company or that the environment for setting up a company isn't favourable. reply WarOnPrivacy 5 hours agorootparentprev> much lambasting Boom Aerospace gets If Boom wants respect, they should merge with Sergey Brin's LTA Research. Supersonic airships are the future's future. reply orbisvicis 3 hours agorootparentSupersonic dirigibles? Isn't that an oxymoron? reply zooq_ai 3 hours agorootparentprevEngineers always over value their ability to start companies on their own. There is a lot to building a successful company and needs irrationality and risk-taking (not exactly traits of a median engineer). Engineers like Musk, Zuck, Gates are outliers than the norm. If you are a Boeing Engineer and had above average risk-appetite, you wouldn't be stuck in Boeing for 10+ years. reply extraduder_ire 6 hours agorootparentprevDo they do any applicable work down in Alabama, or is that just manufacturing? reply coderjames 5 hours agorootparentThat's who went on strike. \"Tens of thousands of machinists voted Thursday to reject a proposed deal between the company and the union.\" This was the IAW manufacturing folks rejecting a contract, not the SPEEA engineering folks. reply patmorgan23 1 hour agoparentprevI can see how they'd be similar. They both have lots of Real-time, must work, critical type systems. reply RSHEPP 4 hours agoparentprevConsider valves or pressure regulators also! Lots of shared fundamentals with the flow of gases! reply ajross 3 hours agoparentprev> Any Boeing engineers who may be looking for alternate paths The strike in question is the machinists' union, engineers aren't involved. reply carabiner 1 hour agorootparentThis is correct and I'm not sure why it's downvoted. The engineers at Boeing are still going into the office and not on strike. reply zardo 34 minutes agoprevThe negotiating team has to be pretty out of touch with the members to reach a deal that's rejected with 96% of the vote. reply cwmma 29 minutes agoparentNot really, the negotiation team gets the best deal that can get that doesn't involving going on strike and they present that to the union which is then better informed for their vote. reply rtkwe 25 minutes agorootparentCorrect rejecting the companies \"best and final\" contract is basically par for the course in union negotiations. The company is banking on being able to outlast the strike to get a better deal than the union would otherwise agree to or betting on the government coming in an kneecapping the union and forcing the workers to accept a deal like happened with the ATC and train operator unions when they struck (striked? struck doesn't sound right in the context of a union strike for some reason...). reply Havoc 5 hours agoprevJust watched a documentary on Boeing an hour ago. It’s incredible that they avoided criminal prosecution despite two planes crashing, door popping out and then paperwork somehow going AWOL. Also Boeing knew about wiring issues since 2022 and the FAA only issues an inspection order in ‘24. Gonna try and avoid Boeings of all types going forward reply mnau 5 hours agoparentThey hired the lead prosecutor that cut them a deal. To be precise a Boeing criminal defence firma hired her after she left the Justice department. It's the American way. reply marcusverus 4 hours agorootparent> Boeing’s lead corporate criminal defense law firm is Kirkland & Ellis. Cox, the lead prosecutor in the Boeing case, left the Justice Department earlier this year. And last month she joined Kirkland & Ellis as a partner in its Dallas office. It looks like the firm bought the prosecutor, resulting in a win for its client. How could this be prevented, though? Politicians can't be trusted with oversight like this--they would use it to punish prosecutors and would inevitably further politicize the Justice Dept. https://www.corporatecrimereporter.com/news/200/lead-boeing-... reply evilos 3 hours agorootparentCan't a law be passed that states prosecutors legally cannot consult or assist in cases or for defendants they were involved with during their tenure as a prosecutor for X years after they leave their position? reply dmix 2 hours agorootparentPoliticians don't have much incentive to do this since plenty of them get cushy jobs at bigco benefactors of the spending bills they voted on. Trying to target prosecutors would cast a eye on themselves. It's like trying to separate church and state in England back in 1600-1700s. reply lukan 1 hour agorootparent\"Politicians don't have much incentive to do this since plenty of them get cushy jobs at bigco benefactors of the spending bills they voted on.\" It is the same problem - bribery/corruption but hard to proof and not much interest to investigate. reply para_parolu 3 hours agorootparentprevThen some friend of Boeing CEO would invest in prosecutor's startup reply triceratops 3 hours agorootparentprevThe allegation is the prosecutor got a cushy gig at the law firm, not that they subsequently worked on the case after switching sides. reply evilos 46 minutes agorootparentAh so the implication is not that the prosecutor gave them inside info on the government's case's weaknesses but the prosecutor intentionally played the case suboptimally in hopes of being paid after the fact? If this was done with prior assurance that sounds already illegal no? If it was done simply on the hopes of securing \"payment\" afterwards with no prior deal then that seems like a large risk for the prosecutor to take. reply markus_zhang 3 hours agorootparentprevIf the whole legal system protects and even encourages such actions, maybe...wait an UPS guy is knocking the door, strange I don't remember ordering anything...hold on. reply diggan 5 hours agoparentprevOnce you start thinking of Boeing as a government agency with less oversight that poses as a for-profit corporation, a lot of things start to make more sense. Including what you wrote about. reply aners_xyz 5 hours agorootparentI’m not really sure this framing makes any sense if I’m being honest. reply diggan 5 hours agorootparentWhat private corporation could make mistakes where people lose their lives on the same scale while still operating as normal afterwards? reply Al-Khwarizmi 5 hours agorootparentMonsanto, Nestlé, J&J, the whole tobacco industry, the companies that made asbestos, talidomide, etc. reply frmersdog 3 hours agorootparentSo, yes. All have deep ties with government through their regulatory apparatus (some would characterize this as \"capture\"). Also of note: people might be confused by tobacco's inclusion here if they don't realize that North America was essentially settled as a tobacco agribusiness (Spanish gold-hunting and death-trap religious colonies notwithstanding). Tobacco's role in shaping America's socioeconomic nature is massively underrated. reply schmidtleonard 5 hours agorootparentprevPeople dying due to profit-motivated corporate negligence and the corporations in question getting a slap on the wrist? Must be a day ending in \"y.\" reply FredPret 5 hours agorootparentThere must be a billion corporations out there. What % can kill customers on the scale of Boeing and get away with it? A tiny number. And for the ones that can, it’s likely that there are strong ties to the same government that’s supposed to prosecute them. For Boeing, there are significant financial and contractual ties to the US government. Having a steady stream of ultra-reliable government cash surely reminds you of a state department? reply consteval 4 hours agorootparent> What % can kill customers on the scale of Boeing and get away with it? 100% of the ones that are at, or near, the top of their domain. As the US and other developed nations move more towards Oligarchy, this describes a vast majority of the economy. If you pick a domain, any domain, there's typicallyPeople dying due to profit-motivated corporate negligence and the corporations in question getting a slap on the wrist? As opposed to what, people dying because government workers were negligent or on a power trip and nobody getting even that much accountability? reply ryandrake 4 hours agorootparentYou know, it's possible that both are bad. The root problem is that the rich and powerful[1] face no consequences for wrongdoing. It's endemic to every part of life, and nothing gets done about it because the rich and powerful make the rules. It really doesn't matter if they happen to be aligned with \"Team Corporation\" or \"Team Government\". They are equally unaccountable to justice. 1: Rich and Powerful are both the exact same thing since money is frictionlessly convertible to power and vice versa. reply tbrownaw 4 hours agorootparent> the rich and powerful[1] face no consequences for wrongdoing. Bernie Madoff. Jeff Epstein & Ghislane Maxwell. Sam Bankman-Fried. Elizabeth Holmes. Harvey Weinstein. I get that sometimes you'll see things like Hunter Biden's tax issues being allowed to pass the statute of limitations, but a universal \"the rich never face consequences\" is just plain false. reply lucianbr 2 hours agorootparentIf you approach it with mathemathical universality, yes, it is false. But what about \"95% of the time 95% of the rich do not face consequences\"? It's still a huge problem, and probably what the commenter meant. reply ryandrake 3 hours agorootparentprevYou can always cherry pick a few counter examples to any argument without invalidating the typical case. reply slt2021 2 hours agorootparentprevcan you list how many financiers were jailed after 2008 GFC ? can you find any? can you list how many people were jailed for engineering SARS-CoV? heck, government hasn't even acknowledge that it was engineered and still pushes fairy tale about bat infecting pangolin who infected a chain of few other animals and then ended up in wet market - but no signs of viruses in 1000 miles between caves and wet market were ever found reply hobs 2 hours agorootparentWell, disregarding the bankers - generally we require evidence to be presented in a court of law and then those findings to be found true for people to go to jail - repeating stuff you read on the internet doesn't really rise to that level. reply TehCorwiz 5 hours agorootparentprevBP and Exxon have killed entire ecosystems including people. They lied for decades about the environmental effects of fossil fuels. Air pollution alone has conservatively killed hundreds of thousands of people. While you can't blame them for air pollution existing, you can blame them for intentionally suppressing the ability of people to mitigate it and improve air quality. EDIT: accidentally a word. reply dimal 1 hour agorootparentprevAre you joking? Once a corporation gets large enough, the worst they can get is a fine, which is usually a fraction of their income. This is how the system works. For example, depending on how you calculate it, Merck killed between 3,000 and 500,000 people with Vioxx, and they knew the risks prior to releasing it. They got a fine. And now, the company is now doing just fine. No one was individually prosecuted. If you have a corporate charter and billions of dollars, you have a license to kill. reply zymhan 2 hours agorootparentprevUnion Carbide: https://en.wikipedia.org/wiki/Bhopal_disaster reply tbrownaw 5 hours agorootparentprevMaybe do a search for \"most dangerous jobs\"? reply piva00 5 hours agorootparentprevProbably all of the oil industry? reply jfarina 5 hours agorootparentprevBoeing. It's literally the post. reply echoangle 4 hours agorootparentThe question obviously was meant as \"except Boeing\". reply Larrikin 5 hours agorootparentprevnext [15 more] [flagged] diggan 4 hours agorootparentI am European (left FWIW, which I guess in the US is \"far-left\" or something alike), been influenced by Reagan for sure (against most of his stated policies and views) and I do believe the government should exists to help people among other things. The government should not aim to generate profits but instead spend that overflow money on improving more lives. reply consteval 4 hours agorootparentprevYou're getting downvoted but you're 100% right. The Reagan-era conservative propaganda is so unbelievably strong that even when a private corporation kills hundreds of people, people will STILL use the opportunity to point at the public sector. No, they don't operate like the public sector because they're actually in the private sector. But the propaganda is so strong that it's literally unthinkable for a private actor to do something bad on a large scale - only the public sector can do something bad on a large scale. Therefore, in this reasoning, Boeing must be like a public actor. It's really remarkable the mental effort people will go through to avoid any type of narrative that MIGHT imply the private sector needs more accountability. reply diggan 3 hours agorootparent> Therefore, in this reasoning, Boeing must be like a public actor. It's really remarkable the mental effort people will go through to avoid any type of narrative that MIGHT imply the private sector needs more accountability. Maybe it wasn't very clear but I'm making (trying to make) the opposite point. Boeing pays out bonuses to executives like they're a for-profit company, but considering Boeing is too big to fail and NASA using them as a partner to push their other partners and won't drop them no matter what happens (one of the press conferences about Starliner made this very clear) makes it seem like Boeing is a government agency. Seems weird to immediately jump to \"Reagan-era conservative propaganda\" when I'm probably as far away from anything like that as humanly possibly. reply christophilus 4 hours agorootparentprevI didn't read the OP as saying, \"Private sector good; public sector bad.\" I read him as saying, \"Unregulated monopoly bad.\" I think a more charitable read of the OP is that the FAA hasn't done a good job regulating Boeing because the FAA is in Boeing's pockets. Boeing is so critical to national defense that the regulators don't want to rock the boat and risk getting fired. Both Boeing and the FAA need to be cleaned up. reply ramon156 5 hours agorootparentprevThat's not reality though. Sure, you could argue that this is how a government should work, but everything is always about improving the economy first reply foobarboo 5 hours agorootparentThe government is allowed to care about the economy as a means to improve peoples lifes. reply godzillabrennus 5 hours agorootparentprevIf you think government agencies exist to solely help the American people then how do you explain the DMV experience? reply dghlsakjg 4 hours agorootparentLat time I went to an American DMV it was in a small town in Colorado to get a license renewal. I was in and out within 10 minutes. My next license was in Canada. Made an appointment and it was about the same, maybe a bit more to get all the paperwork scanned to convert my US license. The DMV doesn't have to suck. Government services don't have to suck, and a lot of them actually don't. Frequently they suck because they are tasked with doing far more than any similarly resourced private company could accomplish. reply linguae 4 hours agorootparentIn my experience living in California, government services such as the DMV and the post office tend to be more pleasant in small towns compared to in large metro areas. When I lived in San Luis Obispo and in Santa Cruz County, it was generally a smooth experience running errands at government facilities. In fact, I remember people smiling and being friendly in San Luis Obispo. In larger metros such as Sacramento and the Bay Area, though, I often have to contend with long lines and surly service. I know this is just anecdata, but this is something I’ve noticed over the years. reply Larrikin 4 hours agorootparentprevGo to a DMV or a post office in a country that isn't the US. It can actually be a pleasant experience. reply linguae 4 hours agorootparentIt depends on the country, though. I love Japan, where I lived for eight months and where I frequently travel to, but dealing with government services there can be stressful. Granted, the clerks are professional and cordial, but be prepared to deal with long waits and perplexing bureaucracy. The United States is not unique when it comes to the stereotypical “DMV experience.” reply Larrikin 3 hours agorootparentI actually lived in Japan for some years and was thinking specifically of them. People that care at the post office, even if they have a litany of questions, and deliver your package in pristine condition is the preferred result over crap service where nobody in the entire chain cares at all if it even shows up or if it's beat up and stuff fell out. The only government service that felt purposefully bad, like many American government services, were the immigration services in Shinagawa, conveniently located next to the garbage processing facilities. reply linguae 1 hour agorootparentBack in November I spent half a day at the Shinagawa immigration office to obtain a trusted travelers program pass since doing it at the airport would’ve been inconvenient for me due to my super-early arrival at Haneda and my super-late departure. It wasn’t too bad, but I spent half a day there and there were unclear instructions regarding which area I needed to wait in. Thankfully it’s all taken care of, and the reward for spending half a day at Shinagawa is using the automated kiosks whenever I enter and depart Japan, which saved me tons of time in July when I arrived at Narita, bypassing a very long visitors’ line. I go to Japan frequently and so I’ll make up for the hours I spent in Shinagawa by the time my pass expires. reply weard_beard 5 hours agorootparentprevOr the unemployment office reply jdright 5 hours agorootparentprevThis is just malicious. Boing is the exemplar capitalist enterprise, traded, where the sole objective is profit above everything else, including safety and following regulations obligations, something that they can do because they bought politicians (lobby aka legal corruption). reply MOARDONGZPLZ 5 hours agorootparentTheir stock is down 55% in the last five years. How is that an exemplar of profit making capitalism? reply ryandrake 4 hours agorootparent> How is that an exemplar of profit making capitalism? They still exist and are regularly pumping out millionaires from their executive suites. It's helpful to think about corporate America as a gigantic factory, but instead of a factory that makes gadgets or appliances, it's a factory that makes millionaires. On one end of the factory, the raw materials come in: Able-bodied workers, money from customers, and capital from investors. On the other end of the factory, the finished product--millionaire executives--fly out on golden parachutes into retirement. In the back, the waste from the manufacturing process gets discarded: The broken lives and bodies of all the rank-and-file workers, and all the negative societal and environmental externalities that were also destroyed in the process. Profit for shareholders and [temporary, conditional] employment for workers are mere side effects of the process, and they'd do away with the second one if they had the technology to. reply __xor_eax_eax 1 hour agoparentprevSadly, this will kill the US. We have no other aerospace manufacturer at that scale. Unless you're not in the US, you want Boeing to get better, not fail (or root for a competitor, but I see no such thing) reply jmyeet 4 hours agoparentprevNot defending Boeing here. They have lost engineering focus as everything has become financialized in the search for ever-higher profits. But there's also a recency bias here, as in fatal defects aren't new. Example: there were several fatal accidents with the 737 rudder in the 1990s [1]. One valid area of criticism is how the FAA has essentially allowed Boeing to self-certify to safety since ~2009 [2]. [1]: https://en.wikipedia.org/wiki/Boeing_737_rudder_issues [2]: https://www.seattletimes.com/opinion/letters-to-the-editor/b... reply kjkjadksj 3 hours agoparentprevYou get a lot of leeway when you are a large defense contractor and one of the last American aviation companies. reply zooq_ai 3 hours agoparentprevYou can make a documentary about any person or any company on this planet and make them look evil with the benefit of hindsight. reply danielodievich 2 hours agoprevDuring national neighbor night out few weeks ago I met a couple who just moved to the neighborhood, wife is a doctor and husband is a Boeing engineer in the material science something rather. Me being a huge Boeing fan we've immediately connected on the topic of Boeing's issues. His view from inside echoed mine - too many MBAs, too much focus on financial engineering and stock buybacks and shareholder returns (he was LIVID about Boeing having no cash now because they sent it back to shareholders), too little focus on engineering. I touched on the nextgen (79?7) program and he just shook his head. And the CEO based out of wherever but not Seattle is just a huge spit into everyone's face. I don't think Boeing is going down due to it being well, Boeing, but it will likely need to get bailed out if it goes on like that. So go machinists! reply squigz 1 hour agoparentWhy would bailing out Boeing be a good idea? reply cwmma 32 minutes agorootparentbecause it's too big to fail. reply cchance 45 minutes agoprevGood! Fuck boeing the company/executives, the merger with McDonnell Douglas totally fucked that company! Profits over safety and performance fucked boeing over reply hungie 13 hours agoprevGood. Jim McNerney absolutely shredded the culture, and eroded decades of good will. It's far past time workers for Boeing pressed for things to go back to being an engineering and manufacturing led company. Striking is one way to get closer to that, good on them. reply M95D 6 hours agoparentEver heard of a stock-price led company that \"went back to being an engineering and manufacturing led company\"? reply extraduder_ire 6 hours agorootparentWhat happened with dell when they went back to being privately owned for a couple of years? reply fragmede 6 hours agorootparentprevDepends on how deeply you know the story of GE, which was an engineering company, became a financial services company, and now is back to engineering. reply paxys 3 hours agorootparent> and now is back to engineering There is no company called GE anymore. Most of its engineering units were sold off to other companies which just license its brand. GE appliances, for example, are actually Haier. GE Lighting was sold to a home automation startup. GE's biggest value creator today is its brand name, not its engineering. reply harimau777 4 hours agorootparentprevDo you have any more information on that? I worked for GE in the past and would like to do so again; however, I'd avoided them because it seemed like they'd been run into the ground. reply triceratops 5 hours agorootparentprevI think GP meant went back successfully. Whether today's GE will ultimately be successful remains to be seen. reply JKCalhoun 6 hours agorootparentprevIt may be Boeing's only chance of having a future. Good luck, Boeing. (And I mean that.) reply paulvnickerson 3 hours agorootparentprevMaybe they should go private then, like Musk did with Twitter. reply michael1999 3 hours agoparentprevI don't see anything about the engineering culture in the union demands. It's all about pay, benefits, etc. I fear this will do the exact opposite: vindicate the union-busters who wanted to move out of Washington in the first place. reply michael1999 1 hour agorootparentI should be clear, I see the linkage. The union-busters from MD are a cause of Boeing's woes, and it would be difficult to maintain a high-discipline, high-quality culture in a right-to-work plant without external oversight. The plausible deniability of production mandates with records falsification make zero-tenure employment toxic to a safety-critical program. reply hintymad 51 minutes agoparentprevAnd what's enraging is that he was boasting his so-called culture-building in his book and retried scar free reply tempodox 7 hours agoparentprevReclaiming control from the beancounters and misers to rein in their destructive influence? I'm not optimistic. reply DevX101 13 hours agoprevDefeat for the company or defeat for shareholders? These things are not the same. reply Y-bar 11 hours agoparentDepends on if you are a shareholder for the long terms (many years) or a short-term trader. I don't own stock in Boeing (or any of their main competitors), but when there is a strike like this in a company I hold stock in I generally get more bullish for the long term because it means that: 1. It signals employees still care about the company enough to not just quit and go elsewhere. And employees who care make a better product. 2. Negotiating better benefits generally helps retain the solid \"middle class\" who make the brunt of the work in a company. It might not entice the \"rock-stars\" or so, but they generally are not dis-incentivised by their colleagues getting better benefits either. 3. The C-suite gets to know they are not untouchable, this also helps keep them level when answering to the board of directors. reply 10 hours agorootparentnext [19 more] [dead] photonthug 10 hours agorootparent> There are very few good stories with unions but most of them are tragic especially long-term. Since unions helped to end practices like child labor and what amounted to indentured service, I’m not sure you know what “tragic” or “long term” means. If what you mean is more like “what have they done for us lately”, you should probably just say it and start that conversation instead reply JKCalhoun 6 hours agorootparentIDK, perhaps OP believes child labor laws were bad for shareholders and the companies long term? That underscores the point for me though — the company and shareholders both take a back seat to the well-being of \"the rest of us\". reply radu_floricica 7 hours agorootparentprevThat's taking something that happened about a century ago and using it as an argument for the current situation. If you're looking for something much closer to home, look at how in the past half century unionized corporations were forced to take over pensions, and how they're doing now because of this. Pensions are a financial product, which should be paid for by each employee - quite likely with supervision and safety net from the state. Putting it on the employer's tab turned out to be more or less giving away the future of the company. And that's pretty much 100% on the unions. reply ghaff 6 hours agorootparentThe alternative view is that, left to themselves, people don't save enough for retirement and, for better or worse, social security in the US really isn't enough for a comfortable retirement. That said, there are a lot of reasons why traditional defined benefit pensions aren't a good match for a lot of jobs today. I have one (for a non-union job) but it's mostly a positive because I'm sure I never gave it a second thought early career when I was earning into it. reply contagiousflow 6 hours agorootparentprevYeah, if you want to talk about long term results that kind of necessitates talking about actions that happened a long time ago... Or would you like to talk about the long term results from actions of unions last year? reply csomar 4 hours agorootparentprevnext [2 more] [flagged] ryandrake 4 hours agorootparentYour first paragraph was pretty much true. Unions don't exist to be nice to the executives and shareholders. That's not their purpose. Their purpose is to act as a balance to the outsized power that corporate management and shareholders wield in the employee-employer-shareholder relationship. They only care about the \"health\" of the company to the extent that the company is healthy enough to provide good compensation and working conditions to employees. They're not there to add shareholder value. This is working by design. You're getting hot backlash for framing this as tragic. Doubly so since you identify as \"part of the worker class\" but your entire post is simping for the executive and shareholder class who don't care about you and would lay you off without even thinking about it if it would increase the stock price. reply notrealyme123 10 hours agorootparentprevSuch a black and white view of things. But for simplicity, remind me again who does the work, and who cheaps out on plane parts to drive up revenue? reply darby_nine 7 hours agorootparentprevSure, but companies and shareholders are the most destructive forces on earth. I have a hard time seeing investor tears as anything but a glimmer of hope. reply atoav 9 hours agorootparentprevI live in Hamburg, Germany and there is a major Airbus-site nearby. By US standards these employees basically live under \"socialism\" and their union definitly did not hurt them in comparison to Boeing. Quite the contrary, certain safety violations wouldn't even remotely happen here, as the employees cannot be pressured into accepting them as easily. You know, when your boss can fire you with the snip of a finger and potentially your health care is on the line, it takes a special kind of guts to still do the right thing. Unions can can be bad if abused by powerhungy people, but the same is true for literally every other organization. reply marcosdumay 5 hours agorootparentprev> Source: Check the current Boeing union demands. Do you have a source for them? The only one on the article is that Boeing stops breaking the law. reply throw0101c 7 hours agorootparentprev> This is delusional. Unions are generally bad for companies and almost always bad for shareholders. And? It's not the union's job to be good for companies or shareholders, it's their job to be good for workers. > Unions are rarely interested in the \"health\" of the company itself but rather in their control (which is at conflict with shareholder control) and their compensation (again, eating from shareholder compensation). Yes, and? You say this like it's a bad thing. > Unions also use aggressive measures (strikes, shutdowns, etc...) rather than negotiations. When you/workers are only used for their labour, withholding labour may be the only bargaining chip you have. Why not use said chip? Or perhaps companies can treat their workers well so that they don't feel compelled to form unions. There are a number of auto plants in southern Ontario, and most are union shops, except Toyota's: the CAW has been trying to organize for many years, but the workers always vote \"no\". Seems like Toyota treats them well that the workers don't need organization 'against' the company. Perhaps other companies can learn a lesson from that. reply jjulius 5 hours agorootparentprev>Unions also use aggressive measures rather than negotiations. Unions start with negotiations and move to strikes if those don't work. Literally what happened here. >There are very few good stories with unions but most of them are tragic especially long-term. Do you have any proof to cite here, eg stats showing success rates of all unionized businesses? Boeing, much of Kroger, and a ton of other industries/companies have long used union labor without \"tragic\" results. >For investors, it's both the short and long term that's f-ked. I'm not gonna feel bad about the wealthy losing a bit of cash. >Source: Check the current Boeing union demands. Yeah, better wages, no forced overtime etc. - super shitty things to ask for. If you can't feel my eyes rolling right now, well... reply windexh8er 6 hours agorootparentprevAnd we found the Jack Welch understudy comment here. The only thing that's delusional is how backwards this comment is with respect to the customer. While, yes, Boeing has different buyers than most organizations we can see things like tertiary customers being affected by Boeing's \"health\" in terms of the quality of it's products. And by that I mean the 737 MAX scandal. Shareholders shouldn't be afforded to drive the direction of the company, even though they have tools like voting rights. The purpose of a shareholder is that they provide capital because they trust Boeing will make great products that sell and potentially profit from. What a shareholder shouldn't be is someone who gets to tell Boeing to cut quality to artificially inflate the stock price. The shareholder doesn't matter in the case of Boeing employees and if it does then Boeing is not a company you should want to buy products from. Unions are interested in protecting employees from greed, in its simplest definition. Boeing, as we know at this point in time, has put short term profits well above both its employees and its products. Yet your delusion is that not having a union will somehow fix both? Amazing stretch of reasoning there. reply idiotsecant 6 hours agorootparentprevLol literally every point in this post completely dismisses the interest of the majority of people in our system, the workers. I'm very sorry if the capital class ends up with a temporary haircut so that workers can have safe, sane, and respectful conditions. reply IntelMiner 8 hours agorootparentprev>There are very few good stories with unions but most of them are tragic especially long-term. For investors, it's both the short and long term that's f*ked. This is the most blatant disregard for or lack of understanding of labor history I have seen in a while reply dmvdoug 7 hours agorootparentWelcome to HN! reply Cheer2171 6 hours agorootparentprevSure buddy, can't have pensions, how else would Boeing afford $43 billion in stock buybacks? reply immibis 10 hours agorootparentprevHow do you measure a company's \"health\"? reply mrdevlar 11 hours agoparentprevIt's a Washington Post article, owned by who? Who has what kind of relationship with labour? reply mc32 3 hours agoprevMaybe in this contract they both can have provisions in for poor workmanship and poor engineering decisions. Both for the shop floor as well as the management. You ship defects, you get salary or bonus deductions and vestments pulled. reply advisedwang 1 hour agoparentTypically quality improvement programs don't rely on punishing mistakes. Doing so results in people hiding mistakes, pressuring QA people to do a worse job etc. Now if the issue is malice and not mistakes - then you can apply discipline. And indeed most union contracts do still have a process for discipline. reply underseacables 5 hours agoprevBoeing has been on a deathwatch for years now. I predict before the year is out. We will see something about a bankruptcy filing. reply toomuchtodo 14 hours agoprevBoeing spent $43B on stock buybacks between 2013 and 2019 while paying their CEO ~$30M/year. > To support its share price, the company under McNerney poured billions into stock buybacks instead of investing profits into the kind of research and development needed to stay competitive. McNerney decided not to spend billions of dollars building a new plane to replace the 737. Instead, Boeing tweaked and updated the existing model and called it the 737 Max, outfitting it with larger, more efficient engines for increased economy. Compensating for the resulting instability was a secretive automated system called MCAS that would adjust the plane’s pitch without input from the pilot. > McNerney also fought to deeply cut costs. On his watch, the company opened its first non-unionized aircraft production line and initiated a program called “Partnering for Success” that pushed suppliers to cut their prices by 15 percent or more. Many feared that squeezing suppliers would harm the quality of their components, but McNerney was determined to recoup the cost of the 787’s development; if the subcontractors complained, they could find their work taken away from them, as happened to landing-gear-maker United Technologies Aerospace Systems. > McNerney retired in 2015, handpicking his successor, president and COO Dennis Muilenburg. Over the next three years, Boeing’s stock price more than doubled as it sold new planes the world over. (As Bloomberg News reported, Muilenberg and McNerney “had personal reasons to emphasize productivity and cost-cutting” because their compensation was tied to share performance. Together they took $209 million in total pay over seven years.) https://nymag.com/intelligencer/article/boeing-planes-proble... reply JackYoustra 12 hours agoparentIt's kinda fun, right? The whole point of tying executive compensation to share price was to have them have some rough stake in the business, but even that didn't prove enough: there's always a longer term, and doing a visible, clear NPV boost is usually accretive to market cap even if it introduces long-term risk, because the long term risks are usually hidden from public eye and very mushy. Ironically, I think the poster child for how to properly structure compensation away from this is Hank Greenberg's AIG, where not only was his compensation basically all deferred stock options, they were deferred until retirement, so about as long-term a view as you can get. He was very risk-averse, and his biggest flaw wasn't under his operational ownership but under successor and personnel selection (looking at you, Welch). But at some level, it was just bad people making bad calls. I don't think McNerney thought that this would wreck Boeing, he was just wrong. Even if he was left predictably destitute at the end of such a failure, he probably wouldn't have changed his choices because he thought they were good. You could claim that this is a failure of boards and governance (and, truly, I think boards are by and large really bad and I wish there was a much more pervasive activist investor culture) but at some level I'm not sure how much of this is solvable. I don't know. There's probably a way to do boards well enough that this isn't a problem. reply darby_nine 12 hours agorootparentWe all know boeing won't be \"wrecked\"; they're too critical for state interests to be allowed to fail. If they're smart they'll either nationalize the company to remove the profit inefficiency or force the company to split to remove the profit inefficiency. Either way, the profit motive and lack of any competition is clearly a national security risk. reply JumpCrisscross 12 hours agorootparent> We all know boeing won't be \"wrecked\"; they're too critical for state interests to be allowed to fail People keep repeating this without context. The auto companies are strategically significant. That didn't prevent them from going bankrupt [1][2]. Wiping shareholders doesn't mean blowing up the factories. I'm increasingly convinced Boeing needs to go bankrupt. It can then shed unprofitable units--through spin-offs, sales or liquidations--and restructure its obligations. I'm not convinced the union comes out of that better off than it is now. But America sure does. [1] https://en.wikipedia.org/wiki/Chrysler_Chapter_11_reorganiza... [2] https://en.wikipedia.org/wiki/General_Motors_Chapter_11_reor... reply darby_nine 11 hours agorootparent> The auto companies are strategically significant. Not really, they're more politically significant than economically critical. I've certainly never owned an american car and never plan to. We'll be fine continuing to rely on foreign companies to produce our vehicles. Foreigners certainly don't want our cars (outside of maybe tesla in the nordics, i guess?). None of this is true for the plane market, or at least not until boeing acquired its current popular reputation of not being very good at making planes (deserved or not) The fact we bailed out the companies but refused to take ownership should be considered treason. Same for the banks, the car companies, the airline industry we bailed out for more than its entire value, etc etc. completely corrupt and incompetent governance reply bombcar 10 hours agorootparentThe auto companies are strategically significant because war footing America will turn those F150 lines into tank lines in a matter of months instead of the years it would take to spin them up from scratch. IBM made machine guns on typewriter lines in WW2. reply TheOtherHobbes 8 hours agorootparentNo it won't, because only about 30% of F150 parts are made in the US. The rest are made elsewhere - mostly China. This is not 1940, and the US simply doesn't have the strategic industrial base it used to. Off-shoring made some people a lot of money and lowered consumer costs (while simultaneously cutting consumer pay.) But it was economically and strategically unwise. The US is set up to run small-scale wars against technologically inferior opponents. It has no ability to sustain a prolonged multi-year slug-fest against an opponent with a superior manufacturing base. reply bluGill 5 hours agorootparent> This is not 1940, and the US simply doesn't have the strategic industrial base it used to. this is just wrong. The US didn't really have that industrial base in 1940 either. We developed it fast over a couple years of war. To the extent the US had an industrial base, we still do. US manufactures more than we did in 1940 - we just do it with far fewer people in factories via automation. reply bombcar 3 hours agorootparentThis is really the key - there's a ocean of difference between \"we manufacture literally nothing\" and \"we don't manufacture/assemble as much as we could\". reply Dalewyn 6 hours agorootparentprevConsidering the US hasn't won a single war against technologically and numerically inferior opponents since the turn of the century, I think we have even more fundamental problems than just a rusty war machine dependent on Chinese blue jeans. Now granted I might be unfair to the US here; the Middle East is known as the graveyard of empires for a reason. reply rurp 4 hours agorootparentThe US did not lose the war against Saddam. The war was started under false pretenses and they made a mess of the reconstruction after, but that's different from a military defeat. If one side of a war manages to destroy the other's military, execute its leader, and set up shop in his palaces; that's a win on the military front. reply bluGill 5 hours agorootparentprevThe US clearly choose to lose all wars lost though. The military was doing just fine, but the people back home got sick of the efforts. reply vundercind 5 hours agorootparent1) That’s still losing. 2) I’m not sure the military was doing just fine in all of those. Vietnam comes to mind, but also Afghanistan—reading the Afghanistan papers, the brass seems to have given up on any kind of actual goals or accountability in favor of a system that let them continually cycle officers through and let them claim they succeeded at their mission (funny, they all keep achieving their mission, but facts on the ground remain exactly the same or worse!), for years and years. Fighting fitness may have been OK throughout, but military leadership in the military itself was absolutely not committed to any kind of winnable mission, let alone to actually winning it. That may have been driven (I’m sure it was) largely by civilian leadership, but the entire upper echelon of military leadership betrayed their commands and the soldiers counting on them, to keep up a convenient (to their careers, and a bunch of junior and mid-tier officers who got a big boost to their careers…) political fiction at the cost of any hope of something resembling actual success, and all it took was shitting all over their soldiers and the trust of the American people. I bet Iraq (part 2) was similar. I have some grave concerns about the state of our more-politicized-and-static professional officer corps since roughly Vietnam. reply ahmeneeroe-v2 1 hour agorootparentI'm with you 100% on point #2. Disagree on point #1. We occupied Afghanistan for 20 years. We operated with nearly absolute impunity in all population centers, through all trade routes, and all agricultural areas. Our casualties were a minuscule amount our total forces. Our culture completely transformed theirs (in a way that old school hardliners lament publicly). We killed a huge number of Taliban (and foreign fighters). Clausewitz says that \"the political object is the goal, war is the means of reaching it.\" Can you tell articulate what the political goal of the war was? Thinking back to 2001 (I was in middle school), the goal was retribution. I believe the military achieved that in spades. Yes, in the end Afghanistan did not turn into a US vassal state or a US colony. But was that the goal? reply willcipriano 5 hours agorootparentprevWhen the participation trophy generation grows up and becomes generals you'll hear things like, we would've won that war if it wasn't for all that attrition! reply bluGill 5 hours agorootparentWhat attrition? While deaths were not zero they were very low. reply willcipriano 5 hours agorootparentRoughly 2.2 trillion on the credit card for Afghanistan alone. That's without the interest that will be paid on it. Your grandkids children will be paying for that war. reply bluGill 3 hours agorootparentThe only time anyone cares about that debt is when democrats are in control of the government - then republicans care. (sometimes when republicans control congress and the democrats control the president they care, but it is not as much then) reply willcipriano 3 hours agorootparentThe only president in my lifetime to not start any wars isn't really liked by either the democrats or the establishment republicans. Both parties are big fans of losing wars for some reason. reply ahmeneeroe-v2 1 hour agorootparentprevliterally no one said that or even a rough approximation of that. Seriously I have no love for the generals but even they're not that dumb. reply willcipriano 54 minutes agorootparentThe comment I'm replying to said that and it was said frequently about Vietnam. What they didn't do is use the clear language I used and instead blamed the public for not wanting to spend every last penny on a pyrrhic victory as if that isn't attrition. reply ahmeneeroe-v2 26 minutes agorootparentI agree that the dollar cost is real attrition too. Vietnam and Afghanistan were so wildly costly because we made the (political) choice to limit how we could engage the enemy, while our enemies were not limiting themselves Vietnam: limiting Cambodia ops, the DMZ, limiting bombing of N Vietnam Afghanistan: limiting Pakistan ops, focusing on creating a stable democracy rather than killing Taliban then leaving We made political choices on how to wage war, and then blamed the military for those poor choices. darby_nine 7 hours agorootparentprevSeems like a terribly irrational and inefficient way to run an economy but I'm sure russia will invade any day now. Or is it china these days? reply themaninthedark 5 hours agorootparentI agree. We should also stop subsidizing agriculture, there is enough food grown around the world for everyone. All this waste is very inefficient! In fact, we dump our cheap overproduced food on third world countries and collapse their ag. sector. If we stopped producing and switched to importing we would boost their economies. Since food can be grown in almost any country on earth, we would have a diverse supply chain. I'm sure nothing would go wrong with this plan./s reply mulmen 10 hours agorootparentprevExactly correct. Good time to skim https://en.m.wikipedia.org/wiki/Willow_Run. Years of startup problems but when it finally started humming they were building a B-24 every 63 minutes! In the meantime existing factories were winning the war. As Americans we tend to glorify the industrial feats of WWII but the US government used a very heavy hand to pull that off. Public-private partnership is the preferred euphemism today but essentially we were a socialist economy. reply bombcar 3 hours agorootparentAt its peak, the US had 40% of GDP going into the war. Forty percent! Today the US military budget is 3.4% of GDP. Can you imagine it being ten times as much? reply newsclues 9 hours agorootparentprevIf America goes to war it will need many f-150s for the war itself. reply kwhitefoot 8 hours agorootparentprev> Foreigners certainly don't want our cars (outside of maybe tesla in the nordics, The Nordics are not all the same, Norway is far ahead (20% of private cars already full EVs) of the rest with Denmark a distant second when it comes to electrification of transport. But also the Tesla Model Y was the best selling car in the WORLD in the last twelve months, not just Norway. Of course quite a few of those were built outside the US so perhaps they don't count as US cars. reply teytra 7 hours agorootparentNorway is special. 94.3% of new cars sold last month were electric cars. Actually, in the private marked it was 96.7% ! Links to info (in Norwegian) - https://elbil.no/vi-kan-klare-hundre-prosent/ - https://ofv.no/aktuelt/2024/nybilsalget-i-august-h%C3%B8yest... reply kwhitefoot 4 hours agorootparent> Norway is special I know, I live there. Been driving electric since 2017. reply janalsncm 2 hours agorootparentprevChina is the biggest auto market in the world and also produces the Teslas they buy domestically. reply mulmen 9 hours agorootparentprevThe auto bailouts saved hundreds of thousands of jobs and turned a profit for the US government. reply darby_nine 7 hours agorootparentCool, i don't care about either of these things. Jobs are a poor index of economic health for americans and we should really be guaranteed one as a fundamental human right if we also refuse to implement modern welfare. reply mulmen 58 minutes agorootparentYou don’t care about saving jobs but you think we should be guaranteed jobs? How do you imagine guaranteed jobs work? reply HeyLaughingBoy 2 hours agorootparentprev> Foreigners certainly don't want our cars Then why are they always bitching about their neighbors who buy large American vehicles that are too wide for their streets? reply jampekka 9 hours agorootparentprevMajor industries are economically critical for trade balance. Even if foreigners don't buy US cars, domestic buying means there's less import. There's a limit to how long USA can print petrodollars to make up for the trade deficit. And the limit doesn't seem that far off. reply darby_nine 7 hours agorootparentGood, let's learn to get along with people without trying to dominate them for the first time in living memory reply throwaway2037 8 hours agorootparentprev> Foreigners certainly don't want our cars FYI: Tesla sold 600,000 cars in China last year. It is a huge market for them. They also sold more than 200,000 cars in Europe last year. reply night862 12 hours agorootparentprevMaybe so, but I would say cars are different than aerospace. It’s clear that Boeing was able to squeeze costs centers at the expense of quality and business investment all while keeping the coin under shell in the form of regular old stock buybacks. But if/when Boeing goes under who is going to vet all this NDI sitting in their portfolios? They’re just gonna spin it off to another buyer—consisting of who exactly? McDonnel Douglas? Elon Musk? Tencent? Seems like a nightmare for someone, not sure for whom. reply JumpCrisscross 12 hours agorootparent> But if/when Boeing goes under who is going to vet all this NDI sitting in their portfolios? They’re just gonna spin it off to another buyer—consisting of who exactly? McDonnel Douglas? Elon Musk? Idk, pick one that's American or closely allied [1]. Ideally not one of the three larger than Boeing. Worst case: audit and re-assign the contracts. We'll be better for it in the long run. And I'm not convinced it wouldn't result in quicker, higher-quality deliverables in the short. [1] https://en.wikipedia.org/wiki/List_of_defense_contractors reply AnthonyMouse 11 hours agorootparentWho even needs a specific buyer? Boeing itself is a publicly traded company, the spin-offs can be too. Put the business unit into its own corporate entity and give all the shares of the new entity to the existing Boeing shareholders to do with as they please, or sell them into the market. reply Dalewyn 9 hours agorootparentprev>I'm increasingly convinced Boeing needs to go bankrupt. I agree and hope they do go bankrupt, but I hope so because the country needs a fucking wake up call: American Exceptionalism(tm) is simply no longer true. I sincerely think we need to see one (Boeing), maybe even a few (Intel? US Steel?) paragons of American Excellence(tm) go down in smoking ruins so we have to accept that we are not 1970s America taking mankind to the Moon and beyond anymore. Once we realize that, we can actually get started on Making America Great Again in a real, meaningful way instead of a dumb political catchphrase. reply gartdavis 6 hours agorootparentThe Canadian government drove its primary aerospace company, Avro Canada, not just into bankruptcy/reorganization, but to completely shutdown operations. The country lost 14,000 aerospace professionals, but more importantly, it lost its leadership position in cutting edge aerospace. Its best engineers left for the USA or the UK. Canada, more than a half century later, has never recovered anything like the leadership role it had built during the post-war period. Be careful what you ask for. reply Dalewyn 6 hours agorootparentLook, we're going down one way or another. We might as well get down there faster so we can start crawling back up sooner. reply umanwizard 7 hours agorootparentprev> Once we realize that, we can actually get started on Making America Great Again in a real, meaningful way I see no reason to believe this will happen rather than a continued slow decline and eventual collapse. reply donatj 10 hours agorootparentprev> If they're smart they'll either nationalize the company to remove the profit inefficiency So... remove the need to actually produce planes, or anything at all for that matter, subsidize their continued failures? reply jajko 10 hours agorootparentprevSome car manufacturers are definitely not critical to national security, they just employ tons of folks in few places. Company making weapons like choppers, planes etc for whole US military is on completely different level of importance. reply selimnairb 8 hours agorootparentprevWould love to see a shift towards something like deferred options for executives. Or, ya know, a fixed multiplier on median employee pay, like managers make 3x median, VPs 5x, CEOs 7x. I’m okay with people who have crazy responsibilities making good money, but it doesn’t have to be obscene. reply timfsu 3 hours agorootparentThis sounds like an incentive to fire/outsource all of your lower-paid employees reply selimnairb 2 hours agorootparentGood point. It would have to include contractor pay in the calculation. reply sofixa 12 hours agorootparentprev> But at some level, it was just bad people making bad calls. I don't think McNerney thought that this would wreck Boeing, he was just wrong. Well, he was stupid not to - there's no way he didn't know thay focusing on the short term stock price won't have negative effects on the quality and future of an engineering organisation whose main business, building and selling airplanes, is extremely capital intensive. He came from McDonnell Douglas who nearly bankrupted themselves doing the same sort of crap (pushing suppliers to the edge, cost cutting at every opportunity). reply metaphor 12 hours agorootparent> [McNerney] came from McDonnell Douglas... Cite? Wiki[1] appears to suggest otherwise. [1] https://en.wikipedia.org/wiki/James_McNerney#Career_path reply sofixa 9 hours agorootparentOops, my bad, he was just a GE sociopath. That being said, McDonnell Douglas' history was visible to everyone - what they were doing hadn't been working for decades (first as Douglas, then as McDonnell Douglas). reply danielheath 12 hours agorootparentprev> even that didn't prove enough It did the opposite. Humans respond - on some level - to incentives. Tying compensation to short-term performance creates perverse incentives which make it much harder to accept the idea that your plans might be ruining the company. reply InDubioProRubio 11 hours agorootparentprevBad people brought to fruit by a MBAd culture whos claim to success is mostly inherited inertia.. Longterm Losers with an Attitude and shortterm numbers to silence doubters. reply jandrese 3 hours agorootparentprev> The whole point of tying executive compensation to share price was to have them have some rough stake in the business This reasoning was always flawed. The Stock Market is not real life. It consists of the opinions of a bunch of guys who sit in towers in New York, London, Shanghai, etc... In theory there are fundamentals that should drive the price, but in practice the market is not rational. It's not a good metric for the long term health of a company. Even worse, Goodhart's law applies in spades: \"When a measure becomes a target, it ceases to be a good measure.\" Time and time again we see CEOs playing games with the stock price in order to get a big payday. We see management sacrificing long term stability in order to maximize their year end bonus. Imagine if every time you remember a now dead company do a stock buyback they instead invested that money into R&D, or simply lowered their prices to be more competitive in the market? Maybe they paid their employees better and didn't have constant turnover problems and had higher quality output. How many of those now failed companies would still be around today? reply AmericanChopper 11 hours agorootparentprevThen blame the board imo. They define the objectives for the CEO, the incentives for achieving them, and the governance framework for governing their execution. If shareholders want this “short term thinking” that always gets brought up, then they have every right to it. Of course this “short term thinking” trope is just a meme that the armchair commentators like to pedal out. If the system was dysfunctional enough to prioritise short term profit seeking to this extent, then all a competitor would have to do to monopolise any market is do enough “long term thinking” to outlast all their failing competitors. Investors would notice this and reallocate their capital accordingly. But none of this is what happens in reality. If anything I would say the market is over-interested in long term strategies. The popularity of the growth over profit model that we observe amongst most prolific capital allocators takes “long term thinking” to absurd lengths. reply riffraff 9 hours agorootparent> If the system was dysfunctional enough to prioritise short term profit seeking to this extent, then all a competitor would have to do to monopolise any market is do enough “long term thinking” to outlast all their failing competitors. isn't this exactly what's going on with Boeing? > Airbus last year topped Boeing for the fifth straight year in the orders race, with 2,094 net orders and 735 delivered planes. Boeing had 1,314 net orders and delivered 528 aircraft.[0] I 100% agree the majority of investors don't seem to take the long view, and that's been the case for a while. [0] https://apnews.com/article/boeing-airbus-airline-safety-manu... reply banannaise 3 hours agorootparentprev> Then blame the board imo. The Board of Directors at most major corporations is made up largely of current or former executives, many of whom come from the same industry. It's in their personal interest to normalize lucrative and exploitable compensation plans. You would also be shocked to know what people control the lion's share of investment dollars. reply hnthrow289570 8 hours agorootparentprev>Then blame the board imo Half of them would look like job hoppers with such short tenures on their resume. Some of them have simultaneous executive positions at other companies. If you want some evidence of short-term thinking not being a meme, it's probably that. reply photonthug 10 hours agorootparentprev> If the system was dysfunctional enough to prioritise short term profit seeking to this extent, then all a competitor would have to do to monopolise any market is do enough “long term thinking” to outlast all their failing competitors. This makes it sound like my broker, the finance community in general, or maybe the state is better able to handle long term thinking than our industry leaders, but why would anyone believe that? Aren’t they all looking quarter to quarter and retiring next year no matter how bad they wreck the company/economy/country? reply M95D 8 hours agorootparentStock is sold and bought all the time with no tax. It's in the investor's intrest to sink a company for short-term profits. When the s*it hits the fan, they can sell the falling stock, short-sell even, to some clueless pension funds and invest the money in a competitor instead. When only one company remains, profit +++. This is how stock markets work. Why are you surprised? reply jajko 9 hours agorootparentprevIts not like you have 30 competitors who can produce AH-64 Apache chopper equivalent out of blue with all support stuff required around it, its not perfect market and boeing c suite knows it very well. Sure in 30 years they may be eventually pushed out but they can coast 2 generations of successful careers till then. reply immibis 10 hours agorootparentprevLong term thinking is often incompatible with short term thinking. It doesn't matter if your company is long term sustainable if it's short term unsustainable. See dumping. reply markus_zhang 8 hours agoparentprevI have said this and I'll say this again: executives should never have a disproportionate pay comparing to their most senior trench workers (think a very senior, lead engineer). 3-5x should be the maximum. The saying of \"oh if you don't give big bucks then you won't hire good talents\" is only true to certain extends. Einstein doesn't figure out GR because he wants the cash from Nobel's prize, actually none of the winners probably achieve because of the money. Scientists won't stop inventing because they don't get the big bucks (most don't get big bucks anyway). Carmack would still pump out great code if he stays indie. As long as properly paid, pretty much every real talent would be happy and do whatever they love to do. Giving disproportional pay to executives ONLY attract bad players and TBH some 500 are probably better NOT have an executive. It also creates sort of toxic culture everywhere that people are forced to chase big bucks because average pay is screwed. reply passwordoops 8 hours agorootparentAlso, the only reasonable response to the saying of \"oh if you don't give big bucks then you won't hire good talents\" is \"you mean the same talent that got you into this mess in the first place?\" reply ericd 4 hours agorootparentprevMy reply to this is that the potentially outsized rewards gives Carmack de facto leadership of his next project, because he’s the one paying the bil",
    "originSummary": [
      "Tens of thousands of Boeing machinists voted overwhelmingly to strike after rejecting a contract offer, with 96% support from the International Association of Machinists and Aerospace Workers District 751.",
      "The strike, which began outside Boeing’s Washington state plants, could cost the company an estimated $1 billion per week and disrupt its recovery from financial and safety challenges.",
      "Despite a proposed 25% pay increase over four years and enhanced benefits, the deal fell short of other union demands; Boeing is ready to return to negotiations, and the Biden administration is monitoring the situation."
    ],
    "commentSummary": [
      "Boeing workers have voted to strike, with 96% rejecting a proposed deal that included a significant pay increase.",
      "The machinists' union is demanding better pay, improved working conditions, and for Boeing to \"stop breaking the law.\"",
      "The strike underscores broader dissatisfaction with Boeing's management, criticized for prioritizing profits over engineering quality and safety, contributing to issues like the 737 Max crashes."
    ],
    "points": 468,
    "commentCount": 429,
    "retryCount": 0,
    "time": 1726202517
  },
  {
    "id": 41526288,
    "title": "FDA Authorizes First Over-the-Counter Hearing Aid Software",
    "originLink": "https://www.fda.gov/news-events/press-announcements/fda-authorizes-first-over-counter-hearing-aid-software",
    "originBody": "Not found",
    "commentLink": "https://news.ycombinator.com/item?id=41526288",
    "commentBody": "FDA Authorizes First Over-the-Counter Hearing Aid Software (fda.gov)343 points by mgerdts 20 hours agohidepastfavorite191 comments car 9 hours agoWith the AirPods now officially becoming hearing aids, it will hopefully reduce the stigma and attitude towards hearing aids and allow many more people to realize how bad their hearing actually is. I have been wearing hearing aids for a few years now (Phonak). I've also used the AirPods Pro with the accessibility audiogram feature (basically making them hearing aids), which is really good and has also been around for a few years. I'm very glad, that Apple has made this official and even gotten FDA approval. When I started to loose my hearing a decade ago, for a long time I refused to wear hearing aids, probably due to the perceived stigma. Even though it made life harder and harder -- imagine work meetings with a mumbling boss or me accusing my family to intentionally whisper -- it took years to change my mind. In hindsight I should have gotten hearing aids years sooner. My 'real' hearing aids are nothing short of a technological marvel. They are tiny and run for a few days on zinc-air batteries (312/Costco but made by Varta), while providing all-day BT streaming. Btw, funny how most hearing aid brands come from Denmark. In contrast, the AirPods run out after a few hours and are also destined to become landfill due to their built in battery. reply josefresco 5 hours agoparentYou're certainly not alone in resisting hearing aids. My dad just got some from Costco (most affordable) and really likes them. He probably waited 5-10 years too long. My father-in-law however doesn't like hearing aids because he feels they amplify things he doesn't want to hear. Granted he's never been fitted to actual hearing aids. I understand his concern, but he's told me multiple times his hearing loss leaves him isolated during conversation. He told me one night that he has a lot to say, but can't hear so he spends a lot of time just smiling. It makes me sad that his pride (and stubbornness?) is causing him this stress. reply car 44 minutes agorootparentCostco is great for this I found out. Free audiogram, and all name brand hearing aids. I used to be like your father-in-law, pride, vanity, stubborn, not wanting to be told what to do, whatever it was. And my dad was like this too (the hearing loss is heritable), I used to mock him about not wanting hearing aids before my own hearing declined. When I finally got fitted, it was shocking to me how much my hearing had suffered. Suddenly I could hear birds and crickets again, and most importantly speech! Maybe you can get your father-in-law to first play around with AirPods as hearing aids to win him over to get proper ones. The latest generation hearing aids, like the AP's, have amazing AI signal processing that will suppress noise and enhance speech. It's always cool when my Phonak's detect noise and shut it down. The important thing about hearing loss in elderly, especially if someone has an elevated risk of cognitive decline, is the resulting social isolation, and the increasing risk of dementia [1]. It should be addressed sooner than later. To sum it up, the AP's have the potential to provide an affordable on-ramp for more hearing impaired people to experience hearing restoration and warm up to better ones (hopefully covered by insurance). I don't think AP's would a permanent hearing solution, other than for people who are uninsured and can't afford real hearing aids (sadly). Edit: I could not imagine wearing AP's all day, great as they are, while I don't even notice my receiver-in-ear hearing aids anymore. Edit: While AP's are not perfect, having any kind of hearing aid is a 100% improvement over having none, which is probably also why the FDA allowed OTC hearing aids. Edit: [1] https://www.nih.gov/news-events/nih-research-matters/hearing... reply phkahler 50 minutes agorootparentprev>> he feels they amplify things he doesn't want to hear. Modern hearing aids can be adjusted to amplify only the things you want to hear, and even reduce the things you don't want to hear. reply vel0city 4 hours agorootparentprev> he feels they amplify things he doesn't want to hear. I mean that's kind of the whole point of having them adjusted with an audiologist. They're tuned to your specific needs. It's too bad so many people think they're just mics and amplifiers. Modern hearing aids do a lot of signal processing. reply Arrath 1 hour agorootparent> It's too bad so many people think they're just mics and amplifiers. Modern hearing aids do a lot of signal processing. This is what I'm still trying to convince my dad of, after he found the pair he was fitted with ~20 years ago absolutely useless. He found that they simply made everything louder which did nothing to help him pick out what he wanted to hear. But he's always been picky about his soundscapes, wanting the TV muted during ad breaks etc etc. reply vel0city 0 minutes agorootparent> he found the pair he was fitted with ~20 years ago absolutely useless IKUK but that's like having bad vision so you put on a pair of your glasses from 20 years ago, still having bad vision, and deciding glasses just don't work well. Hearing and vision change over time. And that's assuming those were good hearing aids 20 years ago compared to what is available today. I hope your dad ends up taking a chance. xur17 52 minutes agorootparentprev> But he's always been picky about his soundscapes, wanting the TV muted during ad breaks etc etc. I'm with him on this one. Commercials always end up being louder than the rest of the content, and are just.. annoying. reply qup 2 hours agorootparentprevMy grandmother did that, as well. She was brilliant, but she was reduced to nodding a long, often inappropriately, because she couldn't hear. reply RcouF1uZ4gsC 1 hour agorootparentprev> because he feels they amplify things he doesn't want to hear. Could that be your mother-in-law telling him things to do? Is he afraid of losing plausible deniability - \"Sorry, honey, I didn't hear that, you know how bad my hearing is.\" reply spookie 7 hours agoparentprevI have got to say it was fantastic seeing my grandmas eyes glow when, for no reason, I thought \"wait my xm4's could help her\" and put them on her. She was then able to hear our conversations even though the xm4 are not as good the real thing. She didnt want hearing aids before that, but afterwards she wanted ones. reply freedomben 5 hours agoparentprev> In contrast, the AirPods run out after a few hours and are also destined to become landfill due to their built in battery. Also will entrench the user in a walled garden ecosystem from a very specific giant tech company that isn't big on making their products compatible with other companies. reply chipotle_coyote 3 hours agorootparentThat's an argument against buying an iPhone and AirPods Pro together as a combination instead of buying a hearing aid, but - it's not an argument against using AirPods as an aid with mild hearing loss if you're already an iPhone user - it's not necessarily a great argument against buying an iPhone and iPods Pro anyway, given that hearing aids can easily run hundreds or even thousands of dollars more than that combination - the vast majority of smart phone customers, both on and off HN, have either factored \"walled garden\" into their buying considerations at this point or never will - let's not pretend Samsung is not already trying to figure out how to cram this into their next Galaxy Buds for Android users anyway, which will somehow work best with Samsung phones, not so well with other Android phones, and not at all with iPhones, but nobody will really complain about it because whatevs, it's not Apple reply dickersnoodle 53 minutes agorootparentI'm looking forward to this going live. I've worn the \"we have an app!\" BLE enabled hearing aids and am currently wearing a pair of amplifiers I got on Amazon for $200 and they've lasted almost two years. AirPods Pro + this software should make it a lot easier for me to follow conversations and filter out high frequency noise (like you get when there's nothing but bare walls and ceilings and floor space). I mostly ignore the Apple-haters. reply RobotToaster 9 minutes agorootparentprevIf ever there was an opportunity for apple to earn some easy goodwill, it would be opening accessibility features like this to other platforms. Keeping accessibility features locked to iphones only isn't good optics IMO. reply Terretta 4 hours agorootparentprev> will entrench the user in a walled garden ecosystem from a very specific giant tech company that isn't big on making their products compatible with other companies Are you suggesting AirPods aren't Bluetooth 5.3 and aren't compatible with Bluetooth audio sources? Or that it doesn't play AAC, MP3, and FLAC? The proprietary capabilities (such as instant smart switching between active devices) are all incremental, taking nothing away from normal Bluetooth usage. reply korhojoa 4 hours agorootparentSeems like a rage bait post but: how do you update the firmware without an apple device? reply mcculley 4 hours agorootparentYou need to do more than update the firmware. You need to upload your personal audiogram into the device. reply davweb 4 hours agorootparentprevAt an Apple Store[1]. [1]: https://support.apple.com/en-gb/106340 reply jajko 4 hours agorootparentprevAs mentioned elsewhere you can't tweak its setting outside Apple's walled garden. Better wait for more open competition to catch up unless you are already deep in their ecosystem and not intending to move. reply abdullahkhalids 2 hours agorootparentDo you need your own iphone, or can you use a friend's iphone to fix the settings once? reply currency 0 minutes agorootparentIt probably should be your own device; the audiogram ends up in the iPhone's health app. It can probably be done by someone else if they don't care and don't need to apply a different audiogram for themselves. systemtest 4 hours agorootparentprevI have no issues using my AirPods on Android. Automatic device switching doesn’t work but that doesn’t work on my Sony headphones either. reply vel0city 4 hours agorootparentThe new hearing aid features are gatekept behind an iOS app. You can't tweak the hearing aid settings without an Apple device. reply systemtest 3 hours agorootparentThe Samsung Galaxy Buds only have 360 audio and the better audio codec if you use a Samsung Galaxy smartphone. Doesn't even work on other Android devices let alone iOS. And as far as I know you can't tweak the hearing aid features of Galaxy Buds on any device. reply vel0city 3 hours agorootparentIf you can't actually tweak audiogram settings, they're not really hearing aids. In fact, Samsung doesn't sell them as hearing aids and from what I can tell never use the term \"hearing aid\" in any of the marketing, branding, or feature listing of the devices. They're not FDA approved as hearing aids, so they're not hearing aids. Either way, pointing to another company being shitty isn't really a good justification of the first company being shitty. reply freedomben 18 minutes agorootparentprevJust trying to understand your argument: Samsung does it so that makes Apple ok? I hear a lot of people argue (when defending Samsung, Google Play, etc) that Apple does it so it's ok, but not usually the other way around. I guess it makes sense that it would devolve into the spiderman meme, but the real losers in that are everyone else that isn't making money from it. Personally I thinks it's shitty when anybody does it. reply explorigin 4 hours agorootparentprev+1 on this. I use them with my android phone, steamdeck, windows computer, TV. They work great! reply SoftTalker 2 hours agorootparentprevAnd will put some people off from talking to them. I don't talk to people with AirPods stuck in their ears until they take them out. Too many times I have tried to start a conversation or just say \"Hello\" but they are oblivious because they are on a call or have music or something else playing. reply spaceguillotine 31 minutes agorootparentSounds like a feature and not a bug. reply karmajunkie 1 hour agorootparentprevthat sounds more like a you problem. i use mine pretty frequently as hearing aids, especially in noisy environments. reply whiterknight 4 hours agorootparentprevIs the hearing aid market big on modularity and compatibility? isn’t it good to have multiple options so consumers can pick what they value? reply AshamedCaptain 6 hours agoparentprevNote also phonaks are traditionally a couple thousand euros a piece while even the most expensive airpods are still around 300 the pair. Certainly the phonaks are impressively small, lasting, good quality, and imperceptible, but is the almost 10x price markup justified? The biggest problem with hearing aids (and doctors/calibrators/whatever) is that they are ridiculously expensive... the attitude/stigma much less so. (And in any case airpods are about the opposite of \"imperceptible\" so I fail to see any appeal other than the price) reply alias_neo 6 hours agorootparentI don't think I've ever seen someone with a hearing aid and though anything negative of it, I get it people can be self-conscious, but for something like a hearing aid I think it's unjustified. On the other hand, I can't bring myself to keep earbuds (airpods or w/e) in my ears while talking to someone, regardless of if I can hear them properly, I just seems incredibly rude. My uneducated opinion, is that someone using airpods as hearing aids is more likely to face stigma for that reason, than someone wearing what are clearly hearing aids, unless people actually know they have hearing issues. reply magicalhippo 5 hours agorootparentA buddy had a girlfriend who had reduced hearing. I noticed that people would raise their voice and really dumb down when speaking to her, like they were talking to a senile elderly. It'd happen before she'd said a word, so they clearly saw the hearing aid and assumed. She admitted she disliked wearing the hearing aids, due to such things. But the alternative was not following conversations, which meant she'd get excluded because she missed important information. Gave me a whole new perspective on hearing loss. reply AshamedCaptain 4 hours agorootparentMaybe it's because I have been wearing aids through all my life, but I see things this way. I don't care much what stigma wearing one carries, considering the alternative is being \"that guy\" who needs everything repeated twice, and that is a stigma I hate. reply autoexec 2 hours agorootparentprev> I don't think I've ever seen someone with a hearing aid and though anything negative of it, I get it people can be self-conscious, but for something like a hearing aid I think it's unjustified. Neither have I. On the other hand, I have seen people wearing air pods and thought they looked ridiculous, as if they had qtips sticking out of their ears. Especially if they're sticking out at different angles. reply kasey_junk 4 hours agorootparentprevI don’t think the stigma is around hearing loss it’s about age. And stigma is probably not the right word, it’s an internal acceptance issue. reply car 1 hour agorootparentYou are right, I thought about it a bit more, and I think it was more vanity for me, since I was fairly young when the hearing loss started. But hearing aids nowadays are so inconspicuous that most people don't even notice them. reply ghaff 5 hours agorootparentprevIt almost certainly depends how social norms develop. When Borg bluetooth earpieces first came out, they definitely carried a tech bro/fin bro/VC/etc. vibe that, at any moment, someone more important than you might want to get in touch with me. I do think Airpods today carry a certain I'm not necessarily giving you my full attention vibe whereas an obvious hearing aid is a medical prosthetic. To the degree that Airpods replace hearing aids for some number of people or just assist people who aren't quite at the prescribed medical device level, that probably changes. reply AshamedCaptain 4 hours agorootparentThe one thing that I do believe Apple managed with the iPhone is the removal of this \"tech bro\" vibe from carrying a smartphone overall. So I guess it's not entirely out of the question that Apple will remove the stigma from wearing huge headsets 24h long... reply ghaff 2 hours agorootparentI'm skeptical about big headsets but small earpieces seem headed towards becoming pretty normalized. reply steve1977 4 hours agoparentprevSonova (Phonak) is Swiss by the way, not Danish. They also own the consumer of Sennheiser since a couple of years. reply car 1 hour agorootparentThanks, I stand corrected. reply caeril 6 minutes agoparentprevWhat stigma? I think you're assuming something about society that doesn't actually exist. My father wears hearing aids. Nobody cares. Maybe stop caring about imagined judgement that doesn't actually happen and live your life? reply righthand 4 hours agoparentprevMost people don’t wear hearing aids because they don’t like how it looks and think their hearing isn’t bad enough to warrant social stigma from it. I don’t see how Airpods solve that problem as they are very unsightly, no matter how much pundits say they love them. In fact you mentioning how they are like hearing aids has made me justify never wanting a pair. reply rmccue 4 hours agorootparentThe advantage of AirPods for social stigma is they aren't solely hearing aids, so you have a type of (casual) plausible deniability about why you're wearing them. They won't draw people's attention in the way that hearing aids (because they're different/unique) do. reply righthand 4 hours agorootparentBut a lot of people don’t wear Airpods because they look dorky and look like something is growing out of your ears. This is what I’m saying, coolness doesn’t trump looks even if pundits say “I love my Airpods”. A lot of people do not like wearing headphones for a similar reason. If i see someone wearing AirPods or headphones my initial reaction is they don’t want to be talked to or interacted with. Even if that’s not true and they have passive throughput. reply AyyEye 1 hour agorootparentprevHaving earpods in while talking is an entirely new and worse social stigma than wearing hearing aids. > you have a type of (casual) plausible deniability about why you're wearing them \"He doesnt even give enough of a shit about talking to me to remove his earbuds\" isn't a good thing. > They won't draw people's attention in the way that hearing aids (because they're different/unique) do. They won't draw peoples attention in the same way because normal humans only begrudgingly interact with people wearing earbuds in conversations and only for a bare minimum. I really can't stand this website sometimes. But at least you all got the AirPods™ branding right instead of just calling them earbuds. reply ksenzee 1 hour agorootparentprev> In fact you mentioning how they are like hearing aids has made me justify never wanting a pair. Why? Because hearing aids are unsightly? Are wheelchairs unsightly? Canes? Prosthetic legs? It looks to me like this idea of what’s “unsightly” is being fed by a cultural bias against the disabled. Interestingly, glasses were “unsightly” a few decades ago, and now they’re in fashion. It would be nice to see the same thing happen for other assistive tech. reply alnwlsn 2 hours agorootparentprevIf anything, if someone's wearing Airpods, I'm going to assume they can't hear anything I'm saying. \"oh no he's wearing airpods\" is a meme for a reason. reply xucheng 16 hours agoprevInterestingly, in the end of the article, the FDA links to an old article hosted on web.archive.org[1] even though the linked article was originally published by FDA themselves. Considering the linked article was only published at 2022, a merely 2 years ago, maybe the FDA should do more to prevent dead links. [1]: https://web.archive.org/web/20221028042729/https:/www.fda.go... reply bigiain 15 hours agoparent> maybe the FDA should do more to prevent dead links Perhaps government departments (and companies) taking advantage of archive.org storing their old docs should be appropriately supporting them? reply FinnKuhn 7 hours agorootparentI don't know how much money the internet archive has received from official US government institutions, but they do receive at least some as you can see from their list of foundations that help with funding them: https://archive.org/about/ reply galleywest200 3 hours agoparentprevThis could also make is more difficult for new administrations to \"disappear\" documents from government sites by storing them on an archival site. reply _fat_santa 5 hours agoprevThis is huge. Previously if you were hard of hearing, a pair of hearing aids could cost upwards of $2,000. Now Apple just brought that price down to ~$250. Even if you use them everyday and assume a shelf life of 1.5yrs (which is roughly mine and others' experience with AirPods), you would be replacing your Airpods for 12 years before the cost caught up with a single pair of hearing aids. Even if you think Airpods are not on the same bar as regular hearing aids, this will certainly help depress market prices. Every manufacturer will probably start releasing sub $1000 hearing aids just to not get destroyed by Apple. reply skybrian 50 minutes agoparentThere are already cheap hearing aids. Airpod Pros will grow the market since they’re good for people getting started and okay for occasional use, but they aren’t good for wearing all day: too distracting for people you’re talking with, not enough battery life. But now that Apple entered the market, maybe they will come out with wireless headphones that are more suitable? reply caeril 0 minutes agorootparent> too distracting for people you’re talking with Oh no! People you're talking with might judge you for having Airpods in! The horror! Have you ever considered telling people who judge you for your personal choices to go fuck themselves, cut them out of your life forever, and make a vow to never associate yourself with primate-brained social status strivers who think you're not cool enough for them? Those people deserve the worst, and you're better off with them not in your life. reply cryptoegorophy 1 hour agoparentprev1.5? What happens after 1.5 years? Had mine for 5 years, no issue and sort of the same battery life or not noticeable to me (15% degradation is what I would not notice ) still charge very quick in a case. reply CamelCaseName 54 minutes agorootparentYou buy a new hearing aid for another $250 that has another 1.5 years worth of technological advancements. Or maybe you buy multiple instead of just one, so you can hotswap any time. reply HumblyTossed 5 hours agoparentprev> This is huge. Previously if you were hard of hearing, a pair of hearing aids could cost upwards of $2,000. Now Apple just brought that price down to ~$250. Woah there fella. Hearing aids last a very long time before needing a new battery. AirPods needs to be charged several times a day. That's a bit of an inconvenience. As is some hearing aids are made to fit one's ear. Where as AirPods are 3 sizes fits all. reply kstrauser 3 hours agorootparentYou could buy 2 pairs and swap them out. That would still be a 75% discount over the $2000 pair. This will help more people hear. It might not be the best possible solution, but it surely beats not having it. reply jeffhuys 5 hours agorootparentprevThat’s a much, MUCH lower barrier of entry, though, fella… and that can be celebrated in my opinion reply HumblyTossed 2 hours agorootparentIt's helpful, sure, but it's not the same thing. reply echoangle 5 hours agorootparentprevJust to be pedantic: AirPods Pro 2 come with 4 sizes for tips. Also, I'm sure you can get third-party tips with different shapes if you wanted to. reply mitemte 4 hours agorootparentprevExisting hearing aid products are still available for purchase for those who want them. I don’t think AirPods are going to replace hearing aids. Hopefully they lower the barrier for entry and perhaps lower the price of existing products. reply wintermutestwin 4 hours agorootparentprevAirpods don’t fit in my tiny ear canals. I even bought a smaller aftermarket tip and still no luck. reply some_random 4 hours agorootparentprevA condescending reply showing you didn't read the whole comment isn't the best look. The point isn't that they're the bestest hearing aids ever, the point is that they are an option at all at a tiny fraction of the typical price which will force manufacturers to innovate. reply HumblyTossed 2 hours agorootparentCondescending or not, saying \"Now Apple just brought that price down to ~$250.\" is grossly overstating it. But Apple always gets a pass, so... reply some_random 1 hour agorootparent\"You can now buy hearing aids (as defined by the FDA) for $250\" is an objectively true statement, so unless you think the FDA is filled with Apple fanboys I have no idea what \"pass\" you think Apple is getting. reply teaearlgraycold 2 hours agoparentprevMy APPs are still going strong after 4 years. I use them daily, but not all day every day. Sure the battery life isn’t as good as before. But they’re still very usable. Am I just really lucky? reply ChicagoBoy11 1 hour agorootparentNo, that's my experience as well reply righthand 4 hours agoparentprevnext [12 more] [flagged] mitemte 4 hours agorootparentIf iPhone owners can improve their hearing for $250, that’s a win for a large number of people. Sure, it doesn’t solve the problem for everyone, but neither does a $2000 pair of hearing aids. reply righthand 4 hours agorootparentGreat. But it doesn’t mean Apple Airpod hearing aids are only $250. reply slama 4 hours agorootparentprevMany people already have an iPhone, so that's a sunk cost. Even not looking at the used market, a new iPhone SE is $430 (and likely to be substantially updated soon) reply righthand 4 hours agorootparentSo we agree, not just $250. reply cruffle_duffle 4 hours agorootparentprevGood thing most people carry those around for other purposes. That $1000 tiny computer in their pocket just swallowed yet another thing that was once discrete. reply talldayo 3 hours agorootparentBy definition and per every observable statistic, most people do not carry iPhones around. reply jajko 4 hours agorootparentprevI don't carry around any Apple device, neither do most of people (including various IT folks in various banks) I know. You can't tweak its settings without one. reply cruffle_duffle 3 hours agorootparentFair. I was more referring to smart phones in general. The argument was “you need a $1000 device for this $200 thing”. But if other manufacturers come out with similar hearing aids, I’d expect most to work with either iOS or android and would require one of the two $1000 devices to work. reply righthand 4 hours agorootparentprevExcept it’s a separate device, so it didn’t swallow it. It’s just an abstraction on an already existing thing. Digital wallets is something that swallowed wallets, for example. reply VikingCoder 4 hours agorootparentprevUsed iPhone 7 for $51, iPhone 11 for $154, 12 for $226, 13 for $262... https://swappa.com/buy/iphones I mean, I'm all-Android all-the-way, but I think the used market is viable for this. reply righthand 3 hours agorootparentThank you nth pundit. Different response than the rest (but I am running out): Now you have to carry around the AirPods in your ears and phone in your pocket. Cost is more than a number. reply abtinf 17 hours agoprev> This application was reviewed under the FDA’s De Novo premarket review pathway, a regulatory pathway for some low- to moderate-risk devices that are novel and for which there is no prior legally marketed device. Does that mean that if Android/Bose/Sony/etc were to develop a comparable solution, they would not be able to use the “De Novo premarket review pathway” because AirPods Pro is now a “prior legally marketed device”? How much more onerous is the normal pathway? reply ijustlovemath 13 hours agoparentMed device startup cofounder here. In terms of difficulty in your path to market, from hardest to easiest (and ignoring some less common pathways): 1. Pre Market Approval: you're addressing a completely unproven technology in a novel and or dangerous space. Usually Class II and above. 2. De Novo: you're adding a new technology to a somewhat well known space. Usually Class II+ 3. 510(k): There's already something in the market that addresses a similar problem and works using similar technology to your device. FDA understands these things well and have a very clear approval guideline, which usually just takes time to rubber stamp if the submission is of sufficiently high quality. This is all ignoring Breakthrough Medical Devices, which have a ton of red tape cut (max 30 days to hear back about any submission, and if they run out of time, it's an approval). These kinds of devices are pretty rare, though. reply dannyw 8 hours agorootparentAs much as I am in favor of cutting red tape, medical devices should not be received in 30 days max. reply danudey 2 hours agorootparent\"Breakthrough\" devices have to be devices which show a significant improvement to quality of life over existing treatments for \"life threatening or irreversably debilitating\" conditions, using breakthrough technology in a space where an existing device doesn't exist yet. So it's less \"we made a better pacemaker that uses ChatGPT\" and more \"this new ventilator can keep people alive even when their lungs are filling with fluid and they're going to die otherwise\". reply cruffle_duffle 4 hours agorootparentprevWhy? I do this sort of technique all the time at work to keep things moving. “I’m gonna ship this code on Friday and if I don’t hear anything before then I’ll assume no news is good news and do it!”. There should always be an upper bound to how long somebody can block your progress. If something was so important that they want to block me, why didn’t they tell me before Friday? That was ample time to raise objections. It’s just an SLA and holds people accountable. reply ijustlovemath 2 hours agorootparentprevReally what happens is your application is given a higher priority level and dedicated reviewers. They're still going through the full review process, its just they're focusing on your submission. reply squidgedcricket 17 hours agoparentprev> Does that mean that if Android/Bose/Sony/etc were to develop a comparable solution, they would not be able to use the “De Novo premarket review pathway” because AirPods Pro is now a “prior legally marketed device”? Nope, they'd use the 510k process, which is less onerous than de novo. De novo is a quicker alternative to the traditional path for brand new classes of devices. The 510k process is used to develop a new device within an existing class. reply tootie 17 hours agoparentprevSony already has OTC hearing aids on the market reply CharlesW 17 hours agorootparentNeat: https://electronics.sony.com/more/otc-hearing-aid/c/all-otc-... So what does \"first OTC Hearing Aid Software\" mean, given that both are hardware/software systems? reply tootie 16 hours agorootparentIt seems like it's software to conduct a hearing evaluation to tune the airpods. There's actually loads of OTC hearing aids on the market already although most seem fairly pricey. reply Spooky23 17 hours agorootparentprevIt does some tuning based on a hearing test. reply mgerdts 16 hours agoprevI’m conflicted on use of AirPods as hearing aids. I use one hearing aid and have normal hearing in the other ear. I often listen to things on my phone over the one hearing aid. It would be nice to have stereo. For this reason, AirPods for both listening to stuff and hearing assistance would be great. On the other hand, when I see someone wearing AirPods I assume they are listening to something else or are otherwise trying to shut the world out. If I were wearing them to be able to engage more, I think I would just be sending the opposite message. reply Angostura 11 hours agoparentThe fact that Airpods don’t look like hearing aids is a key advantage for some people. it’s especially important to some young people for whom there is a bit of a stigma around wearing them. reply ghaff 5 hours agorootparentThe flip side is you're wearing something that (today) is generally considered an entertainment device or something used to communicate with someone not in the room. To be clear, I think this is great. My dad bought something off Amazon as a backup for his hearing aid and tried to get remote tech support from me. Who knows what Chinese piece of crap he bought and what he needed to make it work? Fortunately the return was easy. I'd have a lot more confidence in an Airpod. reply AyyEye 2 hours agorootparentprev> it’s especially important to some young people for whom there is a bit of a stigma around wearing them. Anyone who has a stigma about wearing hearing aids around people they are talking to but not a stigma about wearing earbuds while conversing needs to do some deep reflection. reply talldayo 3 hours agorootparentprevIt's a huge double-edged sword. I don't think twice about people wearing hearing aids at the movies or walking down a busy parking lot. If I see someone with Airpods doing the same thing, I'm going to assume they're not using the rare FDA-authorized feature and instead are fully noise-cancelled. Hell, there's an entire meme of \"Oh no, they have their Airpods in!\" that certainly won't abate after the release of a rarely-used feature. reply geoelectric 12 hours agoparentprevI’m pretty pumped about it actually. I have high-frequency hearing loss in one ear (along with replacement tinnitus) that just randomly crept in on me a few years ago, probably after some ENT infection or the other. The hearing specialist who tested me said it’s fairly significant—eg I can’t hear consonants at the end of words clearly, think he rated it as 75yo hearing and I was ~45, and he asked me if I happened to shoot guns on that side. But he did not recommend going so far as a hearing aid yet. I personally am skeptical, especially a few years later. What the AirPods solution might do is let me audition the idea. If it turns out whatever it does is beneficial, that will certainly prompt me to get myself retested for the real thing. I should get re-tested anyway, but there’s not much better to motivate you than concrete evidence. reply garyfirestorm 15 hours agoparentprevThis problem could be solved easily. One could put some kind of tiny sticker on their AirPods - it would take sometime for it to become mainstream - like an orange color ring - indicating the user is using the AirPods as hearing aids. (This is a people problem…) reply wpollock 15 hours agorootparentThe answer is handicap hang tags, like those used in cars, but worn as earrings.reply hunter2_ 14 hours agorootparentprevThe sticker/paint could change all of the white into a skin tone, the way many hearing aids are. Maybe similar to wrapping a car with vinyl, or nail polish. reply sneak 12 hours agorootparentprevSome RGB LEDs in the airpods that change color depending on the mode could also achieve this. reply ChrisMarshallNY 8 hours agorootparentThe issue would be battery use. The batteries in these things are tiny, and using them as hearing aids could mean longer use. Having a phone lockscreen indicator of status would be a good way to show this. I think the phone interface for audiograms is ridiculously complex. They need to improve that. reply echoangle 4 hours agorootparentHow would a phone lockscreen indicator work? If someone walks up to you in a store with AirPods in, how are you seeing the indicator on the phone in their pocket? The situations where I can look at the lock screen of the phone of the person I'm speaking to are pretty limited. reply ChrisMarshallNY 4 hours agorootparentGood point. But LEDs would probably take too much battery power. Not sure if the new color eInk would be useful. reply echoangle 3 hours agorootparentQuick maths on the LED thing: According to Wikipedia, AirPods Pro Gen 1 have 0.16 Wh of battery per AirPod (There's no data on Gen 2). With 5 hours listening time, that gives a power draw of 0.032 watts or 32 milliwatts. This answer https://electronics.stackexchange.com/a/640179 (I know, not the best source, but I'm just guesstimating anyways) gives a current of 1 mA at 5 V for an indicator LED. So the LED would need 5 mW. That increases power draw to 37 milliwatts and gives a new battery life of about 4 hours and 20 minutes. If using 5 mA, which the answer calls \"blindingly bright for some clear LEDs, even from 10 feet away\", the LED would draw 25 mW and reduce listening time to only 2 hours and 50 minutes. The answer is also about non-diffuse LEDs so the indicator would only be visible from a narrow angle, but since it would point forward, that's probably fine. Making it diffuse would reduce perceived brightness again. Brightness could be fine indoors, but outside with direct sun is probably harder. Since you would have the LED on both AirPods, you could probably expect that at least one of them is in the shadow at any time though. reply sureIy 15 hours agoparentprevIs it? I think that thanks to transparency mode and conversation detection people are keeping them in “full time”. In noisy environments I just keep them on without music and they help me hear people talk. reply mgerdts 14 hours agorootparentIt really depends on the awareness of product features and evolution of the actively used devices. My lived experience is that those with white things in their ears can’t hear you or can barely hear you and have to pull one out to have a conversation. Those that know you probably understand how you use them. If I were to see you walking in my neighborhood with your AirPods I would probably not bother saying hi unless I already knew you. If you were a new neighbor that always wore AirPods, that means we would probably never become more than strangers to each other unless you initiated conversation. reply latexr 9 hours agorootparent> If I were to see you walking in my neighborhood with your AirPods I would probably not bother saying hi unless I already knew you. You can still do a slight but friendly wave or nod. That would open the door for them to verbalise a “hi” or “good morning” and strike up a conversation. And it only needs to happen once for you to know. Also, anecdotally, over a decade ago I used to wear non-white headphones or earphones in public frequently. Yet I was still accosted by strangers all the time, asking for directions or other information, when there were plenty of other people around with nothing in their ears. Still I tried to always be helpful and friendly, even if it could get tiresome: I was always listening to a book, not music, so interruptions were meaningful distractions. reply herpderperator 14 hours agoprevIt's kinda crazy hearing Apple mention during their event that they expect FDA approval \"very soon\", and it actually happening 3 days later. I would have thought that governments can't promise timelines to anyone, especially something like FDA approval. reply latexr 10 hours agoparentThe event was prerecorded. It doesn’t seem farfetched¹ to think they already had the approval but legally couldn’t say it without the FDA making it officially public. But if the FDA had announced it before the event, it would’ve stolen the surprise. Not only of the feature, but that new AirPods would be announced. Apple would’ve hated that so they may have asked for the announcement to be delayed. ¹ Maybe it is. I’m not an expert on the USA’s health and government policies. reply robertlagrant 10 hours agorootparentThe approval process requires steps on both sides; Apple could've just delayed sending the last bit of finalising paperwork until they were sure it would drop after the announcement. reply playingalong 14 hours agoparentprevI don't think they can promise. But in a formal process with so many steps involved, you know what else is left to be done. And if there's nothing left, you assume \"very soon\" completion. Also Apple's announcement was kind of a pressure put on the gov: \"hey, enough, unless you want everyone angry at you\". reply ivoflipse 13 hours agoparentprevIf you've submitted a 510k for your medical device, you can advertise it as \"510k pending\". There is a risk that you never receive the clearance or approval, but in this case Apple probably knew they had already addressed any feedback the FDA had so it was very likely there would be no further stumbling blocks reply sqs 13 hours agoparentprevRule of law FTW! Governments can't usually promise timelines, but when the process is well documented and predictable, that is a very good thing. reply sneak 12 hours agorootparentLiterally nobody wants the government telling them what kind of headphones they are allowed to wear. This is a failure of the rule of law. reply barryrandall 5 hours agorootparentNobody is telling anyone what kind of headphones they're allowed to wear. They do, however, tell _companies_ that they can't claim their product has medical benefits without proving (to some kind of standard) that the product is safe to use, and does what it claims to do. This system was put in place after businesses spent decades scamming the public with \"medicine\" that didn't do what it claimed to do and, in many cases, was also poisonous. reply viherjuuri 11 hours agorootparentprevThe government is not telling you which headphones you can wear. They are saying that these particular headphones work well enough as a hearing aid that it is ok that market them as such. This protects you from quacks that claim their device is a hearing aid but that doesn’t actually work. reply mikaraento 7 hours agorootparentTo be fair, in the case of hearing aids you are both in the right. Excessive regulation has created oligopolies and kept prices high in the US. The OTC hearing aid category is meant to help. Before that, low-cost devices tended to remain niche. OTOH the regulation(s) were introduced due to blatant sales of substandard devices, esp in the 1970s. A high-amplification device runs the risk of further damaging your hearing. Many hearing aid users are vulnerable elderly. reply HumblyTossed 5 hours agoparentprevWhat in Apple's statement made you think the FDA promised them anything? They were probably all the way through with everything they needed to do for the FDA process and, well, there's a timeline for this process so that's why they knew. reply not_the_fda 7 hours agoparentprev510ks have a 90 day timeline. The FDA can \"stop the clock\" to ask for more information and clarification. Buts its 90 days from submital to approval or rejection if your paperwork is in order. Novel devices have a different path. Once cleared the FDA can and will come by at anytime and do an audit of your processes and if they aren't up to snuff they can shutdown sales. Its a trust but verify system. reply bpodgursky 3 hours agoparentprevThey definitely broadcast almost-definite timelines. You see regularly SpaceX prepping launch sites for FAA approval that comes less than a day before launch. reply pcardoso 9 hours agoprevOne of my kids was born with a slight hearing loss and I think this is huge. We got from the local health service some basic hearing aids that cost around 1000 euro but we are contemplating buying some high-end Phonak devices that are around 5000 euro as recommended by some experts. In comparison to this the AirPods (280 euro?) are almost free. reply systemtest 4 hours agoparentDoes your nations healthcare system not cover that? reply tootie 7 hours agoparentprevOTC hearing aids are now as cheap as $80 USD. I'm sure they're not the best, but it makes the tech incredibly accessible. Sony, Jabra, Eargo ones are more expensive than airpods, less than prescriptions. Not sure how they will stack up to airpods. https://www.jlab.com/products/hear-otc-hearing-aid-graphite reply dav43 6 hours agoprevI am surprised the size of their study where they made conclusions was only 118 ppl. I would have thought a much larger study is required. reply ChrisArchitect 2 hours agoprevRelated Apple study from May: Apple Hearing Study shares preliminary insights on tinnitus https://news.ycombinator.com/item?id=41491121 reply ChrisArchitect 2 hours agoprevRelated: AirPods Pro 2 adds 'clinical grade' hearing aid feature https://news.ycombinator.com/item?id=41491191 reply philip1209 3 hours agoprevIs it possible to spend FSA/HSA funds on medically-necessary Apple products/services? reply ganoushoreilly 3 hours agoparentThat's a lingering question right now as the software was approved, but not specifically the hardware. I suspect it would be able to be covered, but as with any of those rules, it's kinda murky. I suspect if it is, we'll see some interesting advertising / marketing from 3rd party resellers. reply HeyLaughingBoy 1 hour agorootparentIt's a very good question and Apple may already have an answer. One of the interesting things I learned in my time building medical devices is the role of insurance reimbursement in the product development process. Before introducing a new device, or a new (blood) test, one of the questions Marketing has to answer is how difficult it will be to get reimbursement from insurance in the US. It sounds kind of icky, but it's a real concern. If insurance companies won't reimburse for a particular test or use of a device, then the users are far less likely to buy it, or in the case of a test, the physician may have to warn the patient that their insurance isn't likely to pay for it. This will probably lead the manufacturer to decide that it's too risky to proceed with development. reply philip1209 2 hours agorootparentprevMakes sense. Would be great to see iPhones / Apple Watches covered for diabetic CGM users, too. reply coupdejarnac 4 hours agoprevI made a hearing aid app for the iPhone nearly 10 years ago. It was nearly impossible to get anybody to pay $10 or less for it.Also, there is/was a FDA exception for mobile apps, which kind of obviated the need for the grad school class i was taking at Stanford about getting medical devices FDA approved. reply ohadpr 2 hours agoprevSpeaking of the AirPods becoming actual hearing aides - how will we reconcile the fact that it is not socially acceptable to wear AirPods when speaking with someone? Even if you get to explain ‘oh my AirPods are functioning as a hearing aide’ you likely won’t be able to explain that to other people noticing the conversation and thinking to themselves ‘oh that’s douchey, not taking our your AirPods when talking to someone’. I just really wonder if this will be able to make wearing AirPods while talking to other people socially acceptable because the current presumption is likely that they are not behaving nicely. reply todotask 7 hours agoprevHow do I deal with itchiness? Having trying to wear for a few seconds, can be annoying after been wearing hearing aid for decade. reply amne 8 hours agoprevcue teenagers with \"hearing problems\" so they must wear the pods during classes in 3 .. 2 .. reply dsr_ 7 hours agoparentWhat's your actual complaint? Is it that teenagers sometimes have hearing issues? That's definitely true; many people are born Deaf or with hearing impairments. Some people get injured. Is it that teenagers might fake having hearing issues in order to wear pods? Either they will be paying attention in class or they won't; this will be obvious from their grades shortly. reply teaearlgraycold 1 hour agorootparentJust require a doctor’s note one time? reply talldayo 3 hours agorootparentprevThe \"actual complaint\" is that Apple made hearing aids indistinguishable from an entertainment device. There's nothing wrong with taking initiative to a good thing, but you can absolutely pave the road to hell with good intentions. People would be rightfully outraged if Tesla drivers could ignore the road to play The Witcher 3 at 50mph on the freeway. Saying something like \"either they will crash or they won't\" isn't going to assuage the problem or change the design issue. The danger is going to persist as a result of first-party design oversight. reply llbeansandrice 2 hours agorootparentPlaying video games when you’re supposed to be piloting a half ton death trap isn’t the same as teenagers lying to authority. reply autoexec 2 hours agorootparentThe problem isn't really the lying though is it. It's the fact that they're not being educated if they aren't paying attention. reply HeyLaughingBoy 1 hour agorootparentprevWorse. It's probably a bit over 2 tons. reply Nursie 16 hours agoprevI’m very positive about this development. I don’t personally need hearing aids (yet) but I know people that do, and dear god are they expensive pieces of equipment. Even if the AirPods aren’t perfect for everyone (not everyone wants in-ear devices) a big name like this getting in at that price-point might shake up the market. reply jillesvangurp 8 hours agoparentAdam Savage (of Mythbusters fame) discussed his use of very high-end hearing aids on his Youtube Channel. He has hearing loss and he's of course pretty clued in when it comes to hearing aids. He did a review on the specific brand he's using. From what I remember, he was quite critical on the lack of access to good products for most people that need to get these via some insurance coverage. It would be interesting to get his perspective on airpods. Because of all the legislation, it's actually a hard market to break into and the resulting products aren't necessarily very good or competitive. The focus is on keeping the insurers happy and getting approved; not the end user. The better products can get really expensive too. So the FDA approving consumer grade products could be a big deal. Apple's airpods probably have quite a lot of non trivial tech on board that probably overlaps significantly what some hearing aids try to do. For example, AI that isolates sound and things like noise suppression that work in a very targeted way are game changers. Instead of just amplifying sound, selectively blocking some sound is probably very helpful. Thankfully I have no issues but I know some people that do that wear hearing aids. Despite that, talking to them can be challenging and they have all sorts of issues communicating in loud places. I imagine these could be useful for people that are completely deaf even. They wouldn't hear anything but they might benefit from e.g. live audio transcription; which is something that's probably not that hard anymore for the likes of Apple or Google. reply duskwuff 15 hours agoparentprev> I don’t personally need hearing aids (yet) but I know people that do, and dear god are they expensive pieces of equipment. They aren't as bad as they used to be. You can get over-the-counter hearing aids in the $200-500 range nowadays. Even so, at $250, AirPods Pro are in a pretty good spot. The main drawback I see is that the earbuds don't have all-day battery life; users will need to recharge them in the case periodically. reply lotsofpulp 8 hours agorootparentI’ve seen Costco sell AirPod Pros for $170 or $180 a couple times a year. reply BOOSTERHIDROGEN 15 hours agorootparentprevany recommendations ? reply thebigman433 13 hours agorootparentYou should go to Costco and try a bunch if you have one near you. Its hard to recommend specific hearing aids to people without knowing what they need. If you dont have a Costco, go to any reputable hearing aid store near you and try from their stock reply Nursie 13 hours agorootparentprevJust to echo what the other poster said - Costco seem to have very good prices and will give you a free hearing test with an audiologist (in Australia anyway) Their range is fairly limited, but not necessarily in a bad way. Compared to what other audiologists recommend/prescribe, Costco's stuff was about half the price. OTC hearing aids - no idea. reply Nursie 14 hours agorootparentprevI guess the big question there is, which market segment will the airpods be comparable to - OTC or prescription? Because the 'good' hearing aids cost thousands of dollars, and a lot of their added value is in various forms of sound processing. I can see apple doing quite well at that. > The main drawback I see is that the earbuds don't have all-day battery life; users will need to recharge them in the case periodically. Yeah definitely, though if we are comparing them to 'good' hearing aids then at that price you could buy two pairs and rotate them through the charging case and still come out ahead. reply dadadad100 6 hours agoparentprevI think I’m the target market. Old enough that my hearing requires some help, but still working in tech from home. The hearing aids I’ve looked at - I have a prescription - have Bluetooth for listening but no microphone for talking. I use my AirPods for teams calls all day long. Switching back and forth to a hearing aid seems too much trouble so I haven’t taken that step. I will get these new AirPods the day the feature ships. I may need two pairs to deal with the battery life but it’s still cheap. reply mannyv 4 hours agoprevThis also means that the Apple ecosystem is HSA and FSA eligible. reply solardev 4 hours agoparentWait, really? You can use your health savings on a iPhone now? reply mannyv 4 hours agorootparentIt's much more justifiable now for sure. reply bagels 13 hours agoprevThey need to legalize glasses. reply sneak 12 hours agoparentZenni has historically been good about shipping without a prescription (not sure about present day), and you can buy contacts in Europe without a prescription and have them shipped, although the shipping is a bit higher. reply ewoodrich 2 hours agorootparent> Zenni has historically been good about shipping without a prescription (not sure about present day) They still are. As long as you don’t confuse them by including a few pairs of glasses with similar but slightly different values in the same order. Then they might demand an actual copy of a prescription. Don’t ask me how I know... reply userbinator 15 hours agoprevThe rule enabled consumers with perceived mild to moderate hearing impairment to purchase hearing aids directly from stores or online retailers without the need for a medical exam, prescription or seeing an audiologist Is it just me or does this article sound (pun intended) a bit tone-deaf? All this talk of them \"authorizing\", when earphones with built-in mics, transparency modes, and adjustable equalisers have existed for years before this, available for everyone to purchase, and can function as a \"hearing aid\". reply adgjlsfhk1 14 hours agoparentAuthorization is a big deal. It means it can be payed for by insurance or explicitly prescribed. reply 14 13 hours agoprevThis is so cool to see. As a health care worker I see lots of people who simply can not afford heating aids as they run for thousands of dollars and then things like they hop into a shower and they stop working happen all the time. What a lot of people don’t realize is that hearing loss is a loss of ability to hear certain frequencies. You can’t simply turn up the volume although that does help to some degree. So what happens is you see a doctor who determines what frequency loss you have and the hearing aid when it picks up that frequency shifts it to another frequency that you can hear. So I do see why up until now ya it was regulated. We don’t want some company selling a device that simply cranks the volume and potentially causes more damage. But with today’s technology we are more then ready for this to be a reality. With an app we can offer hearing tests and determine what areas the client needs improved. This I feel will be a game changer for some. The only thing I wonder about is how well do air pods hold up to waxy ears? With regular hearing aids they need cleaning and often have things like a wax trap which is a tiny plug that catches wax and can be swapped out easily. reply left-struck 13 hours agoparentThis is a minor point but cranking the volume doesn’t necessarily mean hearing loss. You could for example have a device that amplifies external sound so that you your hearing is enhanced over all frequencies but has a max volume that it will not exceed. This compresses the dynamic range and makes the quietest sounds easier to detect. It would reduce your ability to distinguish sounds in a noisy environment though I imagine. reply hungie 13 hours agoprevFDA with a reasonable bar - demonstrate that this is equivalent to a professional fitting. I'll take any opportunities for assistive technology to be a cheaper option. reply sneak 12 hours agoprevHearing aids are neither food nor drugs. Same goes for glasses and contact lenses, and CPAP machines. These should never have required approval or prescriptions in the first place. So many people are kept from getting what they need by these arbitrary restrictions. reply stevesimmons 6 hours agoparentSo FCA regulation of CPAP machines just gets in the way of people getting what they need? How about Philips Respironics have to recall something like 15 million CPAP devices in 2021 because they contained foam liable to disintegrate and which the user would then inhale... reply autoexec 1 hour agorootparentYeah, if anything that showed we need more regulation and even more importantly a better justice system. They knew their devices were going to cause cancer and kill people, and they not only continued to sell their devices, but they went out of their way to try to hide the facts from the public so that they could continue to kill as many people as possible. The people at Philips Respironics were/are literal serial killers. Their product has been linked to hundreds of deaths, but not one person spent even a single day in jail. reply wtallis 11 hours agoparentprev> Hearing aids are neither food nor drugs. Do you really believe that the FDA needs to be renamed before it is reasonable for them to be the agency responsible for regulating medical devices? Is there a different agency you think would be a better fit? reply IncreasePosts 2 hours agorootparentThe NIH? reply wtallis 56 minutes agorootparentThat's purely a research organization, with no existing regulatory role. reply echoangle 4 hours agoparentprevDo you think having companies make defibrillators without any approval is a good idea? Those aren't food nor drugs either. reply avianlyric 8 hours agoparentprevWhile we’re at it, the FDA shouldn’t regulate pacemakers, glucose monitors, artificial hearts, cochlear implants, hip replacement joints, or any other kind of surgical implant. None of them are either food or drugs, and so many people are prevented from getting what they need thanks to these arbitrary restrictions! Hmmm, on second thought, I’m not sure allowing ali express implants to be marketed the same as rigourously tested implants, with clear evidence of safety and efficacy, is such a great idea. reply theshackleford 4 hours agoparentprevI’d prefer more oversight of things like CPP, preferably it would been before I spent years inhaling cancerous chemicals. reply bregma 8 hours agoparentprevSame with pacemakers, joint replacements, insulin pumps, glucose test strips, heck even bloodletting razors and leeches. You should be able to just order them online from an Amazon reseller or pick them up down at the Quickie Mart and take the risk entirely on yourself, your dependents, or your co-conspirators without no gov't meddling from so-called experts. reply neilv 17 hours agoprev [–] Consider the plausible scenario of Apple developing a superior hearing aid -- a medical device. If that happens, will people be able to use best medical device without being subject to the various liberties that tech companies take with users -- violating privacy, and exercising leverage to other purposes? We've become acclimated to expect violation from the \"tech\" industry, but what about the medical field? reply mike_d 17 hours agoparent> We've become acclimated to expect violation from the \"tech\" industry, but what about the medical field? Oh, you sweet innocent child. Look at price gouging on EpiPens, J&J \"lifelong\" hip implants needing replacement every few years, insulin pumps with proprietary batteries, glucose monitors that actively prevent you from reading data, etc. Big bad tech companies don't hold a candle to the medical industry. reply candiddevmike 17 hours agoparentprevThe FDA, for all of its warts, is pretty good at curbing bad behavior like this. All medical devices are pretty rigorously controlled, to the point where you can't really add anything to it that isn't absolutely necessary for the device to function. And if you do, there's an encyclopedia worth of paperwork you're going to have to write to defend why the functionality is needed. FDA likes to \"duck type\" things, and if your duck doesn't look like the other ducks, you need to create a new animal or make your duck look like other ducks. reply neilv 17 hours agorootparentInteresting. Do you think the FDA will be more proactive and sharper, than regulatory authorities that got confused in the past by tech companies (Airbnb, Uber, RealPage (YieldStar), and others)? reply candiddevmike 17 hours agorootparentThe opposite, they're going to be ridiculously stubborn and require all of these high tech gadgets to be \"less\". The I in FDA stands for innovation. reply gwern 16 hours agorootparentEspecially here. Expecting good faith in hearing aid regulation - from the FDA?! Remember, Congress authorized OTC hearing aids back in the Trump administration, in August 2017 (https://en.wikipedia.org/wiki/Over-the-Counter_Hearing_Aid_A...). For perspective, GPT-1 didn't even exist yet. But the FDA slow-walked it so long that the Biden administration had to intervene (and is now trying to claim credit for it all, of course), which is part of why the OTC hearing aid explosion has taken so long, been so tentative and slow, and you're only seeing it really taking off the past 2 years or so. reply tomrod 16 hours agorootparentIf they hadn't intervened, would it have happened? reply chrisweekly 17 hours agorootparentprev> \"The I in FDA stands for innovation.\" LOL. I hadn't seen this before. Quote? Or did you coin it? reply sureIy 15 hours agorootparentprevI think this event proves you absolutely wrong. They approved hearing aid “software”, meaning it can run on arbitrary hardware that may or may not have the restrictions you’re talking about… as part of other “software” reply StressedDev 17 hours agoparentprevHow is Apple violating people’s privacy? They have done a far better job than other companies from what I'm have seen. reply zie 15 hours agorootparentI mean they are the best of the big tech companies, by a country mile, but that's not really saying a lot. If you want the full details, go read the privacy policy with a skeptic's perspective. reply thowawatp302 7 hours agorootparentEverything in there seems fine to me, not sure what you’re talking about reply Angostura 11 hours agorootparentprev“Do your own research “ reply kevin_thibedeau 5 hours agorootparentprevScanning for image hashes on everyone's phone is one. reply echoangle 4 hours agorootparentThey aren't doing that currently though, right? The only thing I remember is the NeuralHash thing which was delayed and then never came. reply cm2012 16 hours agoparentprevBig tech - including even Meta but definitely including Apple - have an insanely better track record of keeping user data safe than traditional non-tech companies. reply olalonde 16 hours agoparentprev [–] If you take away economic incentive from Apple, the plausible scenario is that Apple is simply not going to develop such superior devices. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The FDA has approved the first over-the-counter hearing aid software, enabling AirPods to be used as hearing aids, potentially reducing stigma and increasing accessibility.",
      "This approval is anticipated to lower costs and encourage more individuals to address their hearing loss, though concerns about battery life and social perception persist.",
      "Users have reported positive experiences with the accessibility features of hearing aids and AirPods, marking a significant step towards making hearing aids more affordable and accepted."
    ],
    "points": 343,
    "commentCount": 191,
    "retryCount": 0,
    "time": 1726179871
  },
  {
    "id": 41528266,
    "title": "Entire staff of game publisher Annapurna Interactive has reportedly resigned",
    "originLink": "https://www.theverge.com/games/2024/9/12/24243317/annapurna-interactive-staff-reportedly-resigns",
    "originBody": "Tech/ PC Gaming The entire staff of beloved game publisher Annapurna Interactive has reportedly resigned The entire staff of beloved game publisher Annapurna Interactive has reportedly resigned / The staff reportedly tried to spin out the company into an independent entity, but negotiations broke down. By Jay Peters and Sean Hollister Sep 12, 2024, 10:48 PM UTC Share this story Stray, a game where you play a cat. Image: Annapurna Annapurna Interactive, the game company famous for publishing indie hits like Stray, Outer Wilds, Gorogoa, Neon White, What Remains of Edith Finch, and many more, may not be the same company anymore. Bloomberg reports that the entire staff of Annapurna Interactive, the gaming division of Megan Ellison’s Annapurna, has resigned after failing to convince Ellison to let them spin off its games division into a new company. IGN is corroborating the report. “All 25 members of the Annapurna Interactive team collectively resigned,’’ former president Nathan Gary and staffers told Bloomberg. “This was one of the hardest decisions we have ever had to make and we did not take this action lightly.” An Annapurna spokesperson told Bloomberg that existing games and projects will remain under the company. Annapurna didn’t immediately reply to a request for comment from The Verge. Last week, The Hollywood Reporter said that Gary and the coheads of Annapurna Interactive, Deborah Mars and Nathan Vella, would be leaving. THR also reported that Annapurna planned to “integrate its in-house gaming operations with the rest of Annapurna’s divisions, which include film, TV and theater.” Hector Sanchez, who most recently headed up the Unreal Engine games business at Epic Games and is an Annapurna Interactive cofounder, announced last month that he would be president of interactive and new media at Annapurna. Annapurna Pictures, the company’s film arm, has won countless awards for a variety of films, including Her, American Hustle, and Zero Dark Thirty, and the company had only been expanding its ambitions alongside its video game publishing hot streak. In 2020, Annapurna announced that it would begin developing its own games, too; it launched an in-house animation division in 2022, one that soon announced a movie based on Stray. Annapurna Pictures produced the excellent animated film Nimona for Netflix, and it just last month partnered with Remedy Entertainment to begin exploring film and TV adaptations of Control and Alan Wake. This year, Annapurna Interactive published Lorelei and the Laser Eyes and Open Roads; upcoming games include its own developed Blade Runner 2033: Labyrinth as well as Ghost Bike and Wanderstop. Update, September 12th: Added Gary’s statement to Bloomberg about how all 25 staffers have resigned. Most Popular Most Popular The entire staff of beloved game publisher Annapurna Interactive has reportedly resigned OpenAI releases o1, its first model with ‘reasoning’ abilities The US finally takes aim at truck bloat iFixit made its own USB-C soldering iron, and it’s already a joy Phone sex hotline accidentally featured in 'The Last of Us' Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=41528266",
    "commentBody": "Entire staff of game publisher Annapurna Interactive has reportedly resigned (theverge.com)332 points by nickcotter 13 hours agohidepastfavorite163 comments loupol 11 hours agoArticle is missing some context imo : * Annapurna Pictures (the parent company of Interactive) had some financial issues previously at least (see [0] from 2019) while Annapurna Interactive was doing well for itself. * Annapurna Pictures wanted to integrate the gaming division in-house (possibly to prop up the rest of the company) * Staff and exec at Annapurna Interactive wanted to be spin off (see [1]) * Negotiations fell through, so most exec and staff at Interactive left [0] https://variety.com/2019/film/news/annapurna-resolves-more-t... [1] https://www.bloomberg.com/news/articles/2024-09-12/annapurna... reply threeseed 7 hours agoparentThere is another way to look at this. Annapurna Pictures in Dec 2022 produced Nimona for Netflix which debuted at #9 globally and was as high as #3 i.e. it was a massive hit. So they now have the Stray IP which could be extremely lucrative and are looking at how CD Project Red handled their successful Cyberpunk integration on Netflix as a model. Which is where you cross-promote a game add-on or sequel with the movie. Which means for the gaming side they really wouldn't have any control over their direction as it would be entirely driven by the movie side. Not exactly compelling for them hence why they want to leave. With the success of Fallout, The Last of Us, Cyberpunk Edgerunners etc there's definitely big money to be made from video game IP. reply InDubioProRubio 6 hours agorootparentIP, content, blobs - container words - a sure-fire indicator in any conversation that , that the parties involved areunable to accurately value, evaluate, organize and produce, the valuables that should reside inside the container. So here the daily reminder, that logistics is a commodity now, but producing something of value, is not. reply sameoldtune 4 hours agorootparent“Content” is an advertising term for whatever fills the space between all the ads reply threeseed 24 minutes agorootparentprevI can assure you that the accountants and executives at these studios can absolutely value, evaluate, organise and produce the valuables that should reside inside the \"container\". It's what the entire movie and TV industries have been about ever since the idea of sequels e.g. Aliens, Terminator and worlds e.g. Marvel took off. reply jayd16 2 hours agorootparentprevIP has pretty clear connotation in games. I don't really think it shows what you say. Characters and worlds are IP and that IP can be used across media types. Gameplay is not IP. A game project can swap or take in IP. reply amiga386 1 hour agorootparentIn context of the discussion here - people who are talking about \"content\" or \"IP\" mark themselves out as not having a nuanced understanding of what intangible assets they hold. I think what you mean to say is: - writing and depictions of characters are copyrighted - writing and depictions of worlds are copyrighted - some of the names involved can also be registered trademarks They are intangible rights that someone can own. The insidious term \"intellectual property\" is used to reframe the understanding of rights granted by copyright, patent and trademark law by comparing them to real property, which they are not. When was the last time someone was allowed to live in your house, legally, because they're making a parody of it? When was the last time someone was allowed to take your house apart to find out how to make something compatible with your house's design? Owning a copyright is not like owning property and does not give you carte-blanche control over your intangible asset like real property law more-or-less does. Some examples: - I can write a book or game about a boy wizard who goes to a magical school, as long as I didn't base it on Harry Potter. JK Rowling can't stop me. - I can write about a pirate who acts like an aging rockstar, provided I didn't copy Johnny Depp's depiction of Captain Jack Sparrow. Disney can't stop me. - I can write about a martial force voyaging through space by analogy to a similar force on the Earth's seas, so I can call them space marines. Games Workshop® can't stop me, provided I don't implicitly or explicitly claim they are Games Workshop® Space Marines® reply chupasaurus 4 hours agorootparentprev> how CD Project Red handled their successful Cyberpunk integration on Netflix as a model A reminder that Cyberpunk IP holder is Mike Pondsmith, not CD Project. reply thrillgore 3 hours agorootparentI don't think that's true anymore, I believe Mike sold the IP to CD Project and has a stake in the company now, but I need to find a source to confirm this. reply MichaelZuo 4 hours agorootparentprevHow can anyone hold IP over an abstract vibe? reply alwyn 4 hours agorootparentCyberpunk != cyberpunk, unfortunately. Cyberpunk 2077 is set in the Cyberpunk RPG universe that was created by Mike Pondsmith in 1988. It was named after the vibe without changing the name... Also a helpful article: https://www.polygon.com/reviews/2019/8/7/20756548/cyberpunk-... reply gs17 3 hours agorootparentThe vibe already overloaded the name of a different story: https://en.wikipedia.org/wiki/Cyberpunk_(short_story) reply ivmoreau 4 hours agorootparentprevIn this context is referring as Cyberpunk the role playing game, rather than just the genre. Cyberpunk 2077 has a setting based on the role playing board game (that’s where some characters come from, like Silverhand), and that setting is the original IP. reply JohnBooty 3 hours agorootparentprevIt's like how Microsoft owns Windows, but not windows. reply mikrl 2 hours agorootparentIf you got a few of those motorized windows and hooked up a laptop running MS’ operating system to control them all, but only during certain times of day, there would be a window where your windows would be Windows windows. reply helpfulclippy 2 hours agorootparentYou could set up a sweepstakes to install them with some complicated terms. Then you could explain to the local Vietnamese baker how the rules work — in other words, when does Nguyen Doughs win doze Windows windows? reply rhelz 3 hours agorootparentprev// How can anyone hold IP over an abstract vibe? // Same way somebody can hold a patent to a cartoon drawing of a mouse. It's not any individual picture of Mickey Mouse which is (solely) copywrited; even if you draw Mickey Mouse from a different angle, or upside down, or turn it into a plush toy, or a popsicle.....or if you drew Mickey's twin brother, or even his younger brother who bore a family resemblance.... ...what do all those things have in common? If its not a \"vibe\"--a certain essence of Mickey-Mouseness. reply amiga386 1 hour agorootparentYou can't hold a patent to a cartoon drawing of a mouse (that you're implying is the character Mickey Mouse) You can hold a design patent for the look of a tangible object - e.g. the rounded corners of an iPhone. You can also hold a design patent for particularly novel typefaces (fonts), the layout of a screen (e.g. like Norton Commander's twin directory listings on the left and right) and for computer icons, but that only covers them when they're displayed on screen. So you could get a patent for the stylised depiction of a mouse as an icon, but only in that context. If you wanted to flex ownership of a fictional character design, you get that by copyright, not patent. And the copyright law on derivative works is what protects that fiction character design from being copied or evolved by others. But it can't stop them parodying your design in order to ridicule it (specifically), so you don't have absolute control over it. This is why I don't like the use of the term \"IP\" or \"intellecual property\", as it completely muddies the waters as to what your actual rights are, and what limitations of \"ownership\" there are. reply Maken 5 hours agorootparentprevThat seems to be the goal of their partnership with Remedy Games, whose games are already pretty much interactive movies. reply jajko 6 hours agorootparentprevIt also goes back, when CP2077 TV show premiered, sales and number of players skyrocketed again, same for Fallout. reply delusional 6 hours agorootparentprevThis seems like the same way to look at it. The only disagreement between these two comments is if the potential integration is motivated by a lack of success, or the outlook of even greater success. Those are is, at least economically, identical motivations. reply stuckinhell 4 hours agorootparentprevNimona was not a massive hit. reply Maken 5 hours agoparentprevThey were too successful for their own good? reply PoignardAzur 10 hours agoprevI hope whatever entity these people reform has the same magic. Every single game they published had something unique. Outer Wilds, Telling Lies, Twelve Minutes, Edit Finch, Neon White, etc. They didn't develop these games, but they have an eye for talent. reply ashwin-b 10 hours agoparentI agree. They are like the A24 for games. They don't make the product themselves but invest and promote some really good games. Outer Wilds being one of my favourites of all time. reply SV_BubbleTime 4 hours agorootparentOuter Wilds is top three all-time for me. And hands-down, the most underrated game and at least one generation. it’s a good point that the publisher does have an eye for talent. reply Qwertious 2 hours agorootparentPSA: Outer Wilds is not Outer Worlds, which is a completely unrelated game from Obsidian. reply jszymborski 2 hours agorootparentThanks, I made that mistake. reply SV_BubbleTime 1 hour agorootparentprevI’m aware. Outer Worlds got all the attention but it was a 1/2 expectation Fallout clone. Outer Wilds was a masterpiece. reply satvikpendem 3 hours agorootparentprevDid you play the DLC as well? reply Wojtkie 3 hours agorootparentI didn't enjoy the DLC as much as the base game, but I did appreciate what they did with it. I missed flying to different planets rather than being confined to one of them. The story was really good reply personjerry 10 hours agoparentprevIn their nascent years I played two of their early games, both mobile games - Florence and Gorogoa. Both beautiful. I still play Florence once in a while and it really evokes emotions. Gorogoa is a great puzzle game that the HN crowd would love. Highly recommend both if you haven't tried them! reply j_maffe 6 hours agorootparentBoth games are incredible but Gorogoa is a masterpiece. It attempted to communicate something about humanity that couldn't have been expressed in any other form. reply AzzyHN 23 minutes agoparentprevThey were one of the few publishing companies that people liked as a _publishing_ company. reply facorreia 1 hour agoparentprevI agree. Seeing the Annapurna logo in a game has become a signal to me that I will really enjoy that game and that it has a unique, fresh perspective to offer. reply jeffwask 39 minutes agoparentprevThey were a top indie publisher who you could rely on releasing interesting and quality game. Sad loss for gaming. reply oneepic 2 hours agoparentprevI would add a couple games that moved me (Hindsight, A Memoir Blue). but I also have to mention not all of their titles are smash hits IMO -- I disliked Florence, Last Stop, If Found, and Open Roads. That said, I agree that their good games have made me a fan, and I hope the original staff keeps going in some form. reply simonw 7 hours agoparentprevI adored Donut County, it had everything I could possible want from a game about trash pandas and LA county donut culture. reply mintplant 4 hours agorootparentDonut County is my favorite game to sit someone down and watch them play through in a single sitting. reply amiga386 1 hour agorootparentEveryone has their favourites, but have you not seen Katamari Damacy ? reply simonw 33 minutes agorootparentDoesn’t have the king of the raccoons living in a trash can version of Griffith observatory. reply duxup 2 hours agoparentprevWhat kind of lift is there in just becoming a publisher from scratch? I'm trying to imagine what their role is ... if you can just \"be a publisher\" or if you need a lot of cash up front or what. reply facorreia 1 hour agorootparentI don't know the answer, but I imagine that the execs and staff that left as a block have some plan lined up. reply eleveriven 10 hours agoparentprevI feel the same way! Annapurna Interactive consistently published games with something special reply misnome 7 hours agoparentprevAbsolutely. \"Annapurna Interactive\" essentially made anything an instant buy for me. Very disappointed to see what sort of undead monster the label will become. reply javier_e06 22 minutes agoprevI hope the staff land on their feet... and move on. They produced the best games out there and that, remains patent. What happens next is corporate wrangling less to do with creativity and more to do with profits. reply lcnPylGDnU4H9OF 2 hours agoprevGood for them! There's a lot to be said about certain large publishers but this publisher's list of games includes some very good titles. Just knowing they worked with the developers to release certain titles makes me very interested to see who they'll work with next. They seem like a group who will become a household name with a little luck in their marketing. I understand why Annapurna would not want to spin off such a successful division but it sounds like the only leverage they had was existing IP, which the employees probably understand as a sunk cost. I'm glad they were all able to realize they could just do what they do under a different name; I get the feeling the developers they've worked with will remember them and they know it. reply sonofhans 12 hours agoprev> “The report, which IGN can confirm based on conversations with our own sources, states that Annapurna Interactive president Nathan Gary had recently been in negotiations with Annapurna founder and billionaire Megan Ellison to spin the gaming segment off as its own company. However, Ellison eventually pulled out of negotiations, at which point Gary resigned. Almost 30 other individuals, including division co-heads Deborah Mars and Nathan Vella, as well as the entire remaining staff of Annapurna Interactive, joined him.” From IGN — https://www.ign.com/articles/annapurnas-entire-gaming-team-h... I don’t blame them. I’d rather negotiate my arm out of a shark’s mouth than the cost of a hot dog with Larry Ellison, or his daughter. You don’t get that awful rich by ever believing you’re awful rich enough. reply markus_zhang 2 hours agoparentThis is what people with talents should do if they don't agree with the super riches who don't do anything substantial but inherit a shit ton of money. reply hinkley 11 hours agoparentprevAnother lawnmower is born. reply robjwells 9 hours agorootparentFor those who don’t get the lawnmower reference, take 5 minutes and have a hearty laugh: https://youtube.com/watch?t=1980&v=-zRN7XLCRhc (Start at the 33 minute mark if it doesn’t jump you there.) reply lordfrito 6 hours agorootparent\"Don't anthropomorphize Larry Ellison\" Funniest thing I've heard in a while! reply toyg 8 hours agorootparentprevThat rant is famous, but I always wonder at the naivety of Cantrill and friends. Ellison was already known for being a shameless shark since the '80s, 20 years before the SUN acquisition. Believing he would leave your little blade of grass untouched was always going to be fanciful - particularly when they were a loss-making part of the company with no-future already. reply Macha 8 hours agorootparentI think there's plenty of people who the internet or gossip circles hate where that hate is unjustified. At the same time, a lot of the worst people are initially quite personally charming and keep that face even as they screw you over. And I think when people come into contact that they've only heard about in gossip in person, there's quite often an attempt to be like \"well that's the caricature of Larry Ellison, maybe he's not actually that bad\". Face to face contact can weight quite heavily compared to \"people on the internet\". Combine the two of them and you get a recipe for people to give Larry Ellison the benefit of the doubt, taking a \"well we're adults, let's handle this more maturely than immediately doubting our counter party\" stance, even as people who by their own accounts had heard otherwise. Ultimately in this case, Cantrill's experience proved \"the internet\" right, but I think even that might not prevent someone else repeating the same situation in the future. reply dijit 7 hours agorootparentThis is extremely well articulated, and is exactly true in my experience. People take gossip as \"gossip\"- an unrealiable approximation and gross-oversimplification of an individual. Thus give people with poor reputations the benefit of the doubt, believing themselves to be open minded. (which is true). When they are shown just how 1-dimensional some people can actually be, and that the gossip depicts people precisely, it can be jarring. Especially as, like you mention, many people are very personable, charismatic, charming etc; reply hinkley 2 hours agorootparentI joke that Larry is from that evil Star Trek universe and somewhere in the multiverse a clean-shaven, teddy bear of a man is trapped in a universe full of assholes because he was tricked into swapping with this Larry. reply ChrisMarshallNY 6 hours agorootparentprevIdi Amin could be quite charming. Same for another dictator, who will not be mentioned, at the risk of Godwining the conversation. He could talk to a family, leave them smitten and hopeful, then whisper \"Kill them\" to the guard, on his way out. reply nick3443 2 hours agorootparentJust think about the exceptionally gifted \"10x\" engineers we all meet from time to time. There are people equally gifted in charisma and manipulation, they can predict what people are feeling and steer them where they want them with high success rates. This is what I think of whenever I see a politician on video or meet one in person. reply Aeolun 6 hours agorootparentprevIt’s why I now doubt the motives of all people that seem too personable :/ reply markus_zhang 2 hours agorootparent\"Motives are rarely unselfish\". That's good enough. reply JohnBooty 3 hours agorootparentprevFrom murders to politicians to CEOs, nearly every impactful predator has a lot of positive qualities as well. You don't rise to the top of the food chain without having some really sociopathic/narcisisstic traits. But, you also don't rise to the top of the food chain solely by being terrible all of the time. And IMO/IME lot of those \"good traits in bad people\" honestly are genuine, not just facades. This is not to excuse the bad people of the world. It's just... honestly, as a middle-aged person myself... it has been the hardest thing for me to wrap my head around as an adult. The rest of the stuff, I was prepared for, on some level. reply ben7799 4 hours agorootparentprevYah.. it was well known but when it happens to you it's hard to really not try to be open minded. That is why his rant is so famous. I was rolled up into Oracle through a series of acquisitions a few years before Sun.. it all went down the same way. You know what's going to happen but it's still a shock when a manager you knew before you joined Oracle and wouldn't have behaved the Oracle way instructs you to lie to a customer cause \"that's how it's done here and we're going to have to go along with it.\" You also don't realize how shocking it's going to be to see the office re-decorated until they come in and rip everything down and put up pictures of Larry's boats and airplanes and other toys. When we were acquired Larry Ellison got on a big conference call we were all allowed to join and we could raise a virtual hand and ask questions. To his credit he answered everything 100% truthfully and transparently, and the questions were answered exactly the way the Internet expects he would answer. reply markus_zhang 2 hours agorootparentWell at least he has been honest. Honesty is a luxury merchandise you can purchase with a few billion dollars for sure. reply hinkley 2 hours agorootparentTo be a billionaire you need colluders. reply hinkley 2 hours agorootparentprevI thought Sun was just a little misguided and naive and then they sold to Ellison and I realized they were a bunch of fucking morons. WHY? WHY DID YOU DO THIS?? Cantrill and friends just worked there, but the board presumably knew what they were doing. There was a Wired article that told me everything I needed to know about Larry, the way your blind date being mean to the waiter tells you not to make a second date. It was about the best smart homes. It came out during the DotCom boom, before Sun folded. Larry had one of the smartest homes in America. It had a remote control. One Saturday evening, the article reports, he got furious with the system because it wasn’t working, so he threw the remote and smashed it. Then he called the company and demanded a new one. Someone had to drive, 45 minutes if memory serves, to his house to deliver a new remote, on their Saturday night, because some petulant man-child had a temper tantrum and couldn’t wait until Monday. Fuck that guy. How much of a pain in the ass you have to be in the interview process for a writer to drop that anecdote into the article? You know we are only getting part of that story. reply amiga386 45 minutes agorootparent> I thought Sun was just a little misguided and naive and then they sold to Ellison and I realized they were a bunch of fucking morons. WHY? WHY DID YOU DO THIS?? Whatever you think of Oracle and the One Rich Asshole Called Larry Ellison, IBM as a company are likely worse. Sun had a \"west coast wheeler-dealer\" vibe, as did Oracle. IBM had an \"east coast button-down shirt\" vibe. IBM had first dibs and Sun said \"no, I may be desperate but I'm not that desperate\" If Sun had to be sold (which it did because it was consistently losing money hand over fist) and there was a choice of IBM, HP and Oracle to sell it to, and only those three, which would you pick? If you've never experienced the IBM culture, this is a great essay about Don Estridge (father of the IBM PC) which will give you a good flavour of the company: https://every.to/the-crazy-ones/the-misfit-who-built-the-ibm... reply jerf 5 hours agorootparentprevGiven the situation that IP was in at the time, I'm not sure it was a \"believe\" so much as a \"hope\". There wasn't much of an alternative. reply hinkley 2 hours agorootparentIt would have made so much more sense to sell to IBM though, and I don’t even like IBM. I respected their R&D at the time, but their global services division is a blight on society. I can’t recall, did the feds block that option? reply sidewndr46 5 hours agorootparentprevLarry Ellison is the only person I can think of that I instantly know what video is being linked when someone shares a link. reply pcl 9 hours agorootparentprevPSA: it’s a link to Bryan Cantrill ranting about the Sun / Oracle acquisition. I couldn’t make it through 5 minutes of his ranting to get to an accurate summary of the joke though. reply stavros 9 hours agorootparent> You need to think of Larry Ellison the way you think of a lawnmower. You don't anthropomorphize your lawnmower, the lawnmower just mows the lawn, you stick your hand in there and it'll chop it off, the end. You don't think 'oh, the lawnmower hates me' -- lawnmower doesn't give a shit about you, lawnmower can't hate you. Don't anthropomorphize the lawnmower. Don't fall into that trap about Oracle. reply robjwells 8 hours agorootparent> \"Oh they wanted to kill OpenSolaris!\" No, the lawnmower doesn't care about OpenSolaris, the lawnmower doesn't think about OpenSolaris, the lawnmower _can't_ care about OpenSolaris. The lawnmower can't have empathy. reply whstl 9 hours agorootparentprevThe lawnmower joke is here: https://youtu.be/-zRN7XLCRhc?feature=shared&t=2303 at the 38:23 mark. reply hinkley 2 hours agorootparentprevYou grow up an Ellison, I figure you either end up just like daddy, a social justice warrior with means, or like Mary Trump, a psychologist. Just so you can unpack your fucked up family and help others. reply bmacho 5 hours agorootparentprevOr don't waste 5 minutes on this for that. Direct links pointing closer, and the transcript are available in this thread. (At the bottom, for some reasons.) reply spacemadness 3 hours agorootparentCounterpoint: That was an excellent use of my 5 minutes. reply throwanem 5 hours agorootparentprevHardly a waste, unless you hate to be entertained. reply stefan_ 9 hours agoparentprevImagine having to negotiate with Larry Ellisons nepo daughter, how can you even keep a straight face. \"It says here you are a film producer?\" reply ZeroGravitas 8 hours agorootparentI only found out recently that animated film company Laika was created after Nike founder Phil Knight bought his son a new career after he failed at being a rapper. He bought into Will Vinton studios and then forced Oscar winning animation pioneer Will Vinton out of his own company. There's a whole documentary, sad and interesting. reply mhuffman 7 hours agorootparentIf I recall, it turns out that his \"failed rapper\" son had some sort of natural talent for this and has got nothing but accolades even from other animators ever since. Although I do get your point. reply J_Shelby_J 4 hours agorootparentYes, they did produce this great movie https://en.m.wikipedia.org/wiki/Kubo_and_the_Two_Strings reply brandall10 3 hours agorootparentHe also directed Bumblebee, perhaps the only great film in the Transformers universe. reply busterarm 4 hours agorootparentprevf me that movie was so great, only to be overshadowed by Coco a year later. reply WorldMaker 3 hours agorootparentprevIt's just as weird that it happens twice in Hollywood today. Another of Larry Ellison's nepo baby heirs runs Skydance, which has had a bit more success in \"blockbuster\" terms (and has recently been flirting with buying CBS Viacom aka Paramount). reply dash2 6 hours agorootparentprevIt says this is the company that produced, among other films, She? Did Megan Ellison have anything to do with it? reply ChrisMarshallNY 6 hours agorootparent\"Producer\" == $WALLET reply rpdillon 4 hours agorootparentprevHer reply rowanG077 8 hours agorootparentprevI don't follow? Why is that funny? Why wouldn't she be able to be a film producer? reply thoroughburro 7 hours agorootparentOn average, those who enter a field using money are less accomplished than those who enter a field using experience. Obviously. reply ethbr1 6 hours agorootparentI think it has to do more with being honest about ones core competencies. There are tons of people who enter a field because they're rich, hire great people, connect those great people with money, and do very well. There are also rich people who believe wealth qualifies them as subject matter experts, which tends to go less well... reply ChrisMarshallNY 6 hours agorootparentIsaacman seems to have his shit together. reply dboreham 3 hours agorootparentAlso a nice guy. Perhaps not unrelated. reply ChrisMarshallNY 3 hours agorootparentBranson is supposed to be a really decent chap. I suspect a lot of it has to do with who you surround yourself with, and how much agency you give them. One word that rich people almost never hear, is “no.” Even really nice ones don't hear it often. That means that almost any rectally-sourced, harebrained idea they squeeze out, is treated as genius, by their entourage. I know a number of fairly wealthy people, and some of them won’t have anything to do with me, because I say the “N” word. Others, actually ask me what I think. People rapidly learn that asking me for my input means getting an answer that is honest, but not one they might want to hear (and that answer might be \"I don't know.\"). They don’t always give it much weight, but at least they ask. Those folks are not always the ones you might consider “nice,” though. Just anectdata, though, and the community we share has some traits that reward Honesty and seeking counsel from others. reply amiga386 17 minutes agorootparent> Branson is supposed to be a really decent chap. Do yourself a favour and read the Tom Bower biographies of him, e.g. Branson: Behind The Mask In fact, you can a get a good understanding of him indirectly through the testimony in Tubular Bells: the Mike Oldfield story https://www.youtube.com/watch?t=877&v=UQLDGpcgNTM e.g. John Giddings says of Branson \"He was a chancer. He was prepared to gamble and go for it. He was percieved as a visionary, putting it all together, but really he was importing records illegally and flogging them, right? He was a second-hand car salesman.\" pylua 6 hours agorootparentprevIs this really true? Arguable lance stroll is better than Logan in f1. reply _Wintermute 6 hours agorootparentArguably Logan was in F1 to draw American audiences and sponsors, also his uncle is a billionaire. reply rowanG077 5 hours agorootparentprevThe comment did not make clear she bought the position. It almost seemed more like it was implying anyone who is family of some rich guy can't accomplish anything themselves. reply Rinzler89 11 hours agoparentprevnext [5 more] [flagged] kbelder 2 hours agorootparentI agree with you about IGN's quality, but I think it's not necessarily your opinion that's generating the downvotes, but the tone. IGN being disconnected from the tastes and preferences of gamers? Almost inarguable. Letting political opinions of the journalists have huge sway over game reviews and scores? I'd say that is inarguable. But you'll get downvoted when you type a post on HN that seems like you're getting excited while typing. reply Rinzler89 1 hour agorootparent>but the tone Who cares? Debates are about ideas, not tones. >But you'll get downvoted when you type a post on HN that seems like you're getting excited while typing. Unless you're getting excited about the same world views as the mobs'. reply pdimitar 11 hours agorootparentprevI agree with your take but also let's not pretend that SW Outlaws and Concord are worth even 4/10. Worst games I've seen in a while. Concord seems to have a good-ish engine and it would be a shame if that's trashed but outside of that, yikes. reply Rinzler89 11 hours agorootparentnext [2 more] [flagged] pdimitar 11 hours agorootparentAh, I am not saying it was you, I really just wanted to highlight how out of touch the gaming \"journalists\" can be. But as you said, it's all about money. reply darby_nine 12 hours agoprevThis is hilarious and rather sad. How did Annapurna fumble such famous talent so bad? reply flohofwoe 12 hours agoparentIt sounds like a good decision by the team tbh. Blurring the lines by 'integrating' functioning teams into a bigger entity which does completely unrelated things (TV, film and theater) never goes well. The team will most likely found their independent 'spin-off' company as they had planned already, no talent or knowledge is lost, only risk is financing for new projects. reply darby_nine 11 hours agorootparentIt seems like all their other IP is derived from the game IPs, so I'm a little confused how this leaves Annapurna in a better position. reply crooked-v 10 hours agorootparentIt doesn't. It's the people who actually did all the work who are benefiting, not the corporate entity. The coordinated mass resignations make it obvious that the former management is ready to immediately start up their own company in the office next door and hire on everyone who just quit Annapurna. reply schlauerfox 2 hours agorootparentAnd that's why you don't sign non-competes and they should be illegal to protect those with no leverage. reply darby_nine 7 hours agorootparentprevAh yes you're correct, I misread the above comment. reply tako 10 hours agorootparentprevDr Dr reply flohofwoe 9 hours agorootparentprevNot the parent company, but the game team that resigned is in a better position. They can continue focus on building games, instead of being 'integrated' with the TV, film and theater people (I bet most of that 'integration' is about 'outsourcing' CGI work to the 3D artists in the game team - e.g. a distraction from making games). reply philistine 4 hours agorootparentYou're not quite there. Annapurna does not make games, it publishes them. There's no significant number of 3D artist working there. Even on the movie side, they don't directly make the movies. reply wlesieutre 3 hours agorootparentThey do (did?) have an in-house team working on a Blade Runner game, I haven't seen any mention in news coverage if that team is out as well https://annapurnainteractive.com/en/games/blade-runner-2033-... reply wooger 10 hours agorootparentprevClearly not accurate, they've produced quite a few famous and acclaimed films based on original ideas, Philip K Dick novels & real life events. reply darby_nine 7 hours agorootparentAh, my bad. reply bobthepanda 1 hour agorootparentprevin particular, media mergers have a long history of going south pretty quickly because company culture is so fundamental to media production. AOL+Time Warner is up there in the history of worst mergers. reply NotGMan 12 hours agoparentprevProbably through \"line must always go up\" mentality. Creative business, especially games, should really not be publicly traded or thought of as having major returns on investments. It might work on start, but sooner or later the \"current hit game\" will fade out of popularity and then people will get fired and crunch will become the norm etc... in order to satisfy the \"line must go up\" mentality, but since game dev is a hit driven business that clashes with the \"Line must go up\" concept at its core since, being a hit driven business, means that it's totaly unpredictable. reply Beretta_Vexee 8 hours agorootparentThe video game business is very similar to the film or literary publishing business. You need to publish ten novels for one to do well enough financially to pay off the others. Nobody really knows why that one worked. It's possible to do a few sequels, but the formula quickly becomes boring. No one can guarantee the commercial success of a novel, film or video game. reply ethbr1 4 hours agorootparentThat's why equity investment in creative endeavors is an entire class of business. Possible to risk-balance adjustment, similar to anything else. reply thih9 11 hours agorootparentprevDo we have any source or other data indicating that this happened here? reply darby_nine 11 hours agorootparentIs this not the legally obligated line of thinking for american companies? You have some leeway to argue with only providing value to shareholders in some specific manner, but not much. reply jfengel 7 hours agorootparent\"Value\" is whatever shareholders want it to be. In the case of a public company that is assumed to be profit, because the shareholders are a vast and ever changing group. Annapurna isn't public, so it serves whatever the private owners want. Even in a publicly traded stock, the company charter can specify something other than profit. The shareholders know that when they buy stock. In other words there is more leeway than we commonly give credit. The notion of a fiduciary duty to be greedy is pushed by psychopathic CEOs but isn't really what the law says. reply shakiXBT 11 hours agorootparentprevMost companies try to operate at a profit and actually increase those profits over time. That said, reasoning that Annapurna failed only because of that requires some impressive mental gymnastics reply gorbachev 11 hours agoprevdupe: https://news.ycombinator.com/item?id=41526074 reply hashtag-til 10 hours agoparentAny idea why the previous is flagged? reply pvg 7 hours agorootparentMost company personnel changes are not particularly interesting (in the sense of there's not much 'intellectually curious' conversation to be had about them) and are also extensively covered and discussed elsewhere. reply mandmandam 5 hours agorootparentThis story is clearly interesting to the community, or the original wouldn't have gotten 40 points in an hour at night time. There was absolutely nothing toxic going on in the comments either. Calling it a \"personnel change\" is also reading as brazenly disingenuous. reply pvg 5 hours agorootparentthe original wouldn't have gotten 40 points Everything is interesting to someone and HN doesn't really work strictly by points, you can find explanations of this in lots of the moderator commentary and in the site faq under 'how are stories ranked'. Calling it a \"personnel change\" is also reading as brazenly disingenuous. That is certainly one exciting reading. reply hashtag-til 4 hours agorootparentI think this is incredibly relevant for HN readers, maybe seen as \"subversive\" by _bosses_. reply pvg 4 hours agorootparentYes, for every single thing posted on HN, someone thinks it's \"incredibly relevant\". It could well be! But that in itself doesn't tell you anything. maybe seen as \"subversive\" by _bosses_. I don't see it on the agenda in any recent or upcoming boss meeting. reply mandmandam 4 hours agorootparentprev> HN doesn't really work strictly by points People were interested nonetheless. 216 points on this post with 89 comments is pretty conclusive, strict point system or no. > That is certainly one exciting reading. Accurate too. reply pvg 4 hours agorootparent216 points on this post with 89 comments is pretty conclusive Lots of things with piles of points and comments are poor HN submissions. Happens all the time and is not in any way conclusive. That's basically the explanation, for there to be some kind of conclusion you'd have to make an argument that isn't about points and comments of a thing that's on the fp. Everything on the fp gets points and comments. reply mandmandam 3 hours agorootparentI didn't want to \"quote the old magic\" to you, but this story is squarely within the HN guidelines. It's interesting, stimulating, important, tech related, true, timely etc. The argument that a games company losing their entire staff to resignations isn't 'suitable' because \"most company personnel changes are not particularly interesting\" is false equivalence. This thread is full of comments describing what people are finding interesting about this. Dismissing that with 'every fp story has comments' is a really odd and (dare I say) out-of-character move. reply debo_ 4 hours agoprevI was looking forward to Blade Runner 2033. It's nice to have moody adventure games in this era. I imagine that this will delay it at the very least, but likely worse. reply sss111 1 hour agoprevaw man these people made Sayonara: Wild Hearts, which is hands down the best iPhone game I have ever played. reply nubinetwork 8 hours agoprevEvery time I hear thr name Annapurna, I think about the ARM SoC maker... reply FrojoS 6 hours agoparenthttps://en.wikipedia.org/wiki/Annapurna Just in case anyone is unaware, it's a mountain in Nepal. The 10th highest in the world. reply mangamadaiyan 6 hours agorootparentThe first eight-thousander that was summitted, as well as the name of the book that Maurice Herzog wrote. Which book is well worth reading. reply umanwizard 7 hours agoparentprevEvery time I hear it I think of the restaurant on Capitol Hill in Seattle. reply thoroughburro 7 hours agorootparentAnnapurna is a god of food and nourishment in Hinduism; a manifestation of Parvati. reply umanwizard 5 hours agorootparentIt is also the name of a mountain in the Himalayas which I believe is the most familiar meaning to Americans. reply lotsofpulp 5 hours agorootparentprevI bet the way people in the US say it is nothing like the (traditional) way it is said in India. I think it’s supposed to be “un-na-poorna” with a nasally rn sound. reply umanwizard 1 hour agorootparentAFAIK an r sound is always nasalized when followed by an n sound in American English. reply openthc 7 hours agorootparentprevBong shop in Berkeley, CA reply a1371 12 hours agoprevI don't mean to diminish their work and I don't know much about the matter but the company founder is some billionaire's daughter. I fear ever having a boss like that just because of the fundamental difference in perspective and life experiences. I'm sure I won't be able to communicate with them. reply Aeolun 6 hours agoparentHmm, I think you have more in common than you differ. It’s hard to escape the human condition using money. reply xkbarkar 5 hours agorootparentA bit myopic to disqualify someone from the credibility pool because they had a headstart in life imho. reply edm0nd 3 hours agoparentprevFor the curious, the daughter of multibillionaire Larry Ellison, the cofounder of Oracle. reply jgowdy 3 hours agoparentprev\"but the company founder is some billionaire's daughter. I fear ever having a boss like that just because of the fundamental difference in perspective and life experiences. I'm sure I won't be able to communicate with them.\" Imagine invalidating a person as a potential colleague and declaring your incompatibility with that person remotely based on a quarter inch deep evaluation of their background. I think this kind of take is in really bad taste. reply Chris2048 1 hour agorootparentIt's \"boss\", not \"colleague\"; and they're your boss based purely on having a lot of money from their parent, hardly a \"quarter inch deep\" evaluation. reply aavshr 9 hours agoprevSad to see, Outer Wilds is probably the best game I've ever played. But I'm sure the staff will continue doing great work in whatever they decide to do next. reply Maken 5 hours agoparentThis is the publisher, not the developer, of Outer Wilds imploding. The people who actually made the game are still working at the studio. reply eleveriven 10 hours agoprevI am a big fan of Stray! The atmosphere was both haunting and beautiful, and the emotional connection you feel as you guide the cat through puzzles, etc. Hearing about the uncertainty at Annapurna Interactive feels personal a little reply Torkel 9 hours agoparentI have feelings about Stray... What an awesome game it could have been! I played it with one of my daughers. Lovely father/daughter playing session. Guiding the cat through the maze/puzzles. Cat can't fall, it's just calm and nice. And then, out of nowhere - blinking screaming wildly stressful and awful rats come chase us. My daughter dropped the controller, I played it through. But after that the game lost its charm. The second time the rats came, she left. We never played it again. I checked if there was some \"please don't ever give me rats\" setting, but nope. I checked with a friend and they said the rats appear throughout the game. So I'm really sorry, but stray is a 1/10 game for me. Remove the rats and its 9/10. What is the purpose of the rats? It's an arcade moment in a game that was nice, calm, cute and relaxing. Totally breaks the contract that I felt was established with us as players. reply Sammi 9 hours agorootparent> Totally breaks the contract that I felt was established with us as players. Lots of people like surprise and excitement. The game is rated: Age 12+ Violence reply nicolaslem 9 hours agorootparentprevIt seems you just wanted a different game than what Stray is. reply teamonkey 9 hours agorootparentprevMy kids had a similar experience. They had more luck with Little Kitty Big City, which has a more cartoony vibe but fewer sticking points. reply rlonstein 4 hours agorootparent> Little Kitty Big City +1. This game was adorable and relaxing from beginning to end. reply mrgoldenbrown 4 hours agorootparentprevHow old are your kids? Stray is rated 12+ for violence. reply msh 8 hours agorootparentprevThey are not rats and they are quite essential to the story so they would not be easy to remove. reply yungporko 8 hours agorootparentprevstray has a pretty dark and foreboding atmosphere, the rats fit right in imo. seconding the other commenter who recommended little kitty big city though, i didn't expect to enjoy it as much as i did. reply mafro 4 hours agorootparentprevPlayed through with my 7yo daughter. She loved the game, but would freak out when chased by the \"rats\". It was a good opportunity to show her how to try to keep playing when you're stressed! She found it exciting after a couple of goes. You didn't mention how old your daughter is, but that is surely the key factor. I'm still hopeful for a \"Stray 2\" - or whatever they can do now the devs lost the IP. My daughter cried when at the game's ending (no spoiler) reply actinium226 3 hours agoprevMakes you wonder, did the last person to leave turn off the lights? reply Derelicts 7 hours agoprev [–] Aren't these the same guys who published Stray? reply poikroequ 7 hours agoparent [–] It's literally the first game mentioned in the article... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The entire staff of Annapurna Interactive, including former president Nathan Gary, has resigned after an unsuccessful attempt to spin off the company into an independent entity.",
      "Annapurna Interactive's existing games and projects will remain under the company, with Hector Sanchez recently appointed as president of interactive and new media.",
      "Annapurna plans to integrate its gaming operations with its film, TV, and theater divisions, continuing to publish games like Lorelei and the Laser Eyes and Open Roads, with upcoming titles such as Blade Runner 2033: Labyrinth."
    ],
    "commentSummary": [
      "The entire staff of Annapurna Interactive, a game publisher, has resigned due to failed negotiations with their parent company, Annapurna Pictures, over financial integration.",
      "The staff and executives preferred to spin off to maintain control over their creative direction, especially after the success of games like \"Outer Wilds\" and \"Stray.\"",
      "This mass resignation highlights the tension between creative independence and financial pressures within the gaming industry."
    ],
    "points": 332,
    "commentCount": 163,
    "retryCount": 0,
    "time": 1726205668
  },
  {
    "id": 41527564,
    "title": "Does your startup need complex cloud infrastructure?",
    "originLink": "https://www.hadijaveed.me/2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/",
    "originBody": "2024/09/08 5 min read Does Your Startup Really Need Complex Cloud Infrastructure?¶ I recently listened to Pieter Levels on the Lex Friedman Podcast, and it was eye-opening. Pieter has built numerous successful micro-SaaS businesses by running his applications on single server, avoiding cloud infrastructure complexity, and focusing on what truly matters: product-market fit. While his approach might not suit teams and generally every startup, but it raises a valid point: we've often made deployment and infrastructure management complex for complexity's sake. For small dev teams moving past the MVP stage, managing deployments and databases can be challenging. But here's the truth: not every project needs Kubernetes, complex distributed systems, or auto-scaling from day one. Simple infrastructure can often suffice, allowing teams to focus on building a great product and finding market fit. Recent Observations¶ Let me share two recent examples of projects I've worked on that highlight this issue: Project 1: Lambda Overload¶ 20-30 Lambda functions for different services SQS and various background jobs backed by Lambda Logs scattered across CloudWatch Result? Painful debugging, difficult changes, and complex deployments, even in a monorepo. Could this have been simplified to a single NodeJS container or Python Flask/FastAPI app with Redis for background tasks? Absolutely. Project 2: Microservices Mayhem¶ 7 small microservices on Kubernetes (EKS) Separate services for CRUD and business logic While Kubernetes is powerful, the team spent more time on infrastructure than building features. Was this level of separation necessary for their scale? Note Enterprise-scale companies face different challenges with compliance and large workforces. Startups don't need to mimic this complexity. Early-stage companies should prioritize product-market fit and rapid iteration. The Power of Single Server Setups¶ Modern servers pack a punch. You can get powerful VMs from Hetzner or latitude.sh at budget-friendly prices. Even GCP VMs and EC2 instances are reasonably priced. These machines offer robust compute power - think 40GB RAM and multiple cores - often outperforming distributed services or multiple Lambdas or ECS tasks. Plus, everything's centralized and easier to manage. Worried about scaling to millions of QPS? Cross that bridge when you come to it. By then, you'll likely have an infrastructure team to handle it. For a reliable single VM setup, you need: A robust machine (EC2, GCP VM, Hetzner, etc.) Secure access (HTTPS for web, IP-restricted SSH or SSM for deployment) CI/CD for zero-downtime deployments DNS configuration Regular database backups A standby VM for redundancy Yes, you'll need a solid disaster recovery strategy and tested mean recovery time, but it's achievable with a backup VM. Docker Compose¶ Docker Compose is fantastic for local development, managing multiple services with a single command. Surprisingly, it's underutilized in production environments, and Docker Swarm was deprecated.. While Docker Compose can cause downtime during updates, there are guides for production deployment. It's a balance between simplicity and production readiness. Docker Compose Anywhere: A Weekend Project¶ To simplify this setup further, I created Docker Compose Anywhere over the weekend. This opinionated template offers: One-click Linux server setup via GitHub Actions Zero-downtime continuous deployment using GitHub Container Registry and Docker Rollout Environment variable and secret management (considering age or sops for improved security) Automated Postgres backups via GitHub Actions Multi-app support on a single VM Automated SSL with Traefik and Let's Encrypt Deploy Next.js apps, GO, Python, Node.js, and more Few Considerations¶ For security, remember to: Set strict firewall rules (open only necessary ports) Secure SSH keys (prefer SSM on AWS or CLI on GCP) Use a bastion host for enhanced security Protect secrets and consider using a WAF or Cloudflare Don't forget about data protection: Send encrypted database backups to secure cloud storage (e.g., S3 or equivalent) Regularly snapshot your disks for added redundancy Implement a retention policy for backups and snapshots As engineers, our primary goal should be advocating for simplicity in our setup and focusing on the core product. It's all too easy to get distracted by shiny new tools or complex setups that mimic what Google engineers or large enterprises are doing. But here's the truth: whether you're in a startup or not, what truly matters is talking to your users and finding product-market fit.",
    "commentLink": "https://news.ycombinator.com/item?id=41527564",
    "commentBody": "Does your startup need complex cloud infrastructure? (hadijaveed.me)239 points by hjaveed 16 hours agohidepastfavorite305 comments ghomem 11 hours agoI went through sweat and tears with this on different projects. People wanting to be cool because they use hype-train-tech ending up doing things of unbelievably bad quality because \"hey, we are not that many in the team\" but \"hey, we need infinite scalability\". Teams immature to the point of not understanding what LTS means have decided that they needed Kubernetes because yes. I could go on. I currently have distilled, compact Puppet code to create a hardened VM of any size on any provider that can run one more more Docker services or run directly a python backend, or serve static files. With this I create a service on a Hetzner VM in 5 minutes whether the VM has 2 cores or 48 cores and control the configuration in source controlled manifests while monitoring configuration compliance with a custom Naemon plugin. A perfectly reproducible process. The startups kids are meanwhile doing snowflakes in the cloud spending many KEUR per month to have something that is worse than what devops pioneers were able to do in 2017. And the stakeholders are paying for this ship. I wrote a more structured opinion piece about this, called The Emperor's New clouds: https://logical.li/blog/emperors-new-clouds/ reply hliyan 6 hours agoparentI started my career in a world where we did everything using shell scripts running directly on bare metal servers, usually running Solaris, and later SuSe or RedHat. I never understood the \"how would you reproduce your setup without Docker (or X, where X is some other technology)\". The scripts were deterministic. The dependency versions were locked. The configurations were identical. The input arguments were identical. The order of execution was identical. It all ran on a deterministic computational device. How could it not be reproducible? reply ghomem 6 hours agorootparentWell that's exactly the point! Creating complex cloud resources with, for instance, Terraform, is less reproducible than a shell script on an LTS system like Ubuntu or RHEL - that's because the cloud provider interfaces drifts and from time to time stops accepting the terraform manifests that previously worked. And to fix it, you have to interrupt your normal work for yet another unplanned intervention in the terraform code - this happened to my teams several times. This does not happen with Puppet + Linux, because LTS distributions have a long release cycle where compatibility is not broken. I tried to explain this topic in the article linked above. Not sure how far I succeeded. reply kbolino 3 hours agorootparentLeaning into LTS is nice until you near EOL and have to migrate everything in an often Herculean effort to work with the next LTS release. reply ghomem 3 hours agorootparentLike 12 years of life cycle is not enough for you to plan a transition? You can use the entire life cycle but not one is forcing you to. You can update from one LTS to another every 2 years, or 4 years, or 5 years... you decide. reply kbolino 2 hours agorootparentI don't really think we're in disagreement here. The longer you wait, the harder the transition will be. LTS is a good foundation, and usually the right choice for \"enterprise\" or \"business\" settings, but you should not rely overmuch on any one LTS release's way of doing things, when the wider Linux ecosystem moves much faster. reply toast0 2 hours agorootparentprevIt's a tradeoff. Doing a big effort once every 4 or 5 years, vs a hopefully smaller effort every year. Sometimes the intermediate smaller steps help you move forward, sometimes it just means more migrations. Sometimes the software/hardware you need means you can't use a LTS OS at all. If possible, it's nicer to pick established, mature software for as much of your stack as you can, so that there's less of a difference in APIs over longer time frames. But it's not always possible. reply marcosdumay 2 hours agorootparentprevWhat is it that people do that breaks so often due to lack of backwards compatibility from the OS? IMO, the lure of an LTS is that you don't need to keep testing if your computer is still working every week when a set of updates come. Not that things that your software depends on the details remain frozen. If your software depends on the details of something, you should add it as a dependency. reply kbolino 2 hours agorootparentThe bigger problem IMO is not that things break, it's that if you depend on one LTS release too heavily, and you wait too long to migrate from one LTS to another, everything breaks all at once. What should be a gradual migration as new things develop turns into a singular nightmare. reply marcosdumay 1 hour agorootparentWhat are you depending on the OS that isn't extremely backwards compatible? Once in a decade you get something like a breaking upgrade of nginx, or the glibc debacle of 2003. That may take a person-week to fix[1], what can hardly be called \"herculean\". 1 - If you go with 1 person * 1 week, if you try to go with 7 people * 1 day, it will suddenly cost 7 person-weeks. But the only way upgrading is such a hurry is if you borked a lot of things prior to it. reply kbolino 8 minutes agorootparentOff the top of my head, some of the things that have broken at an LTS transition that I've been involved with are out-of-tree kernel module builds, C code using OpenSSL, Puppet config, Salt config, Python code, Perl code, Apache configs, shell scripts, Java code, bootloader configs, bootstrap scripts, and init scripts/configs (esp. sysvinit to systemd). Any one of these things is not a problem in isolation, the problem is due to having to fix all of them all at once. Too much complexity put into any one of them (often arising from external requirements or rushed implementations) also makes migrating harder. Waiting until the 11th hour on the EOL clock just adds to the stress of the process. Many of my bad experiences were because of corporate policies and lack of proper prioritization at levels above system administration. However, the sysadmin does have some choice in the matter, especially when greenfielding, hence why I make this point about not being tightly coupled to particular versions of things. You can turn stability into a vice if you're not careful. minkles 2 hours agorootparentprevIt's not terrible in my experience of doing it several times now. It is definitely less terrible than trying to unfuck tangles of terraform / terragrunt / yaml / bits of cloud infra. reply kbolino 2 hours agorootparentI went through the migration from CentOS 6 to 7 and never want to do anything like that again. The good news, I guess, is that it never will happen again: CentOS is basically dead anyway, and it's not likely that so many core pieces of system software will change that drastically anymore. reply minkles 2 hours agorootparentI did CentOS 3 -> 4 -> 5 -> 6 -> 7 -> Debian. Very few problems. (30 nodes) reply kbolino 2 hours agorootparentI can't imagine you leaned into any one of those releases, then. That sequence involves major changes to the kernel, the init system, the configuration management tools, the core libraries, Apache, Python, Perl, etc. Any one of those alone could (and did, in my experience) trigger a major rewrite of configuration and/or code. I'm glad it was painless for you. In my experience, it was not, and most of the reasons were beyond my control. reply altdataseller 6 hours agorootparentprevWouldnt there be slight differences in different Unix flavors so that the script couldnt run in all of them? If it only worked on Solaris, what would happen if Solaris retired? (Like what happened to Centos) reply stcroixx 5 hours agorootparentThat's what POSIX was for. Keep your scripts and system calls POSIX compliant and you could move from something like AIX to Linux easily. reply kbolino 3 hours agorootparentPOSIX never specified things like disk partitioning or package management, so this still requires something else to give you a working system in the first place. reply toast0 2 hours agorootparentprevDepends on where you are in the ecosystem. If you're running your own service, the only flavors that matter and the ones you're using. If all my machines are FreeBSD 4.11, I don't care if my scripts don't run on Linux or Solaris or SCO or even FreeBSD 4.8 or 14. I might care someday, but not today. Maintenance scripts need to run on all the versions in the fleet (usually), but setup scripts can often be limited to the latest version, because why not use the latest OS if you're setting up a new machine. If you're distributing software, yeah you've got to support a lot of variation. If you're at a shop that runs lots of different flavors, you have to support lots of variation. But a lot of people just pick a flavor and update the scripts as needed when the flavor of the day changes. Trying to keep dependencies and running services as tight and small as possible helps a lot with keeping up to date on security. Don't need to update things that aren't installed, and may not need to update things that are installed but not running (but sometimes you do). reply kjkjadksj 2 hours agorootparentprevYou know what happened when centos retired? Nothing for us. We still use centos 7 at work as we speak. reply eastbound 3 hours agorootparentprevYou said it: Your versions were locked. Therefore it is not constantly up-to-date. I was pinched myself: Security. - With the cloud threats, everything needs to be constantly up-to-date. Docker images make it easier than permanent servers that need to be upgraded. We used to upgrade every week, now we’re upgraded by default. So yes, sometimes our images don’t start with the latest version of xyz. But this is rare, downgrade is easy with Docker, and reproduction on a dev engine easier. - With the cloud threats, everything needs to be isolated. Docker makes it easy to have an Alpine with no other executable than strictly necessary, and only open ports to the required services. I hate the cloud because 4GB/2CPU should be way enough to run extremely large workloads, but I had to admit that convenience made me switch. reply Spivak 2 hours agorootparentA container is locking the whole OS, on this axis it's not an improvement either direction. You still need a way to update deps. reply j45 3 hours agorootparentprevWhat needs to be constant and up to date is reviewing the new patches and which ones can be released and not locked. The versions that are not locked can be a test or dev environment that constantly updates and checks for errors. Security threats are a thing, how we do and don't use technologies as well which ones can also factor in to how much is exposed. reply hello0904 9 hours agoparentprevSerious question for you, why use Docker at all? You can just get rid of the clunky overhead. You mentioned Python backend, so literally just replicate build script, directly in VPS: \"pip install requirements.txt\" > python main.py\" > nano /etc/systemd/system/myservice.service > systemd start myservice > Tada. You can scale instances by just throwing those commands in a bash script (build_my_app.sh) = You're new dockerfile...install on any server in xx-xxx seconds. reply tcgv 2 hours agorootparent> why use Docker at all? We have a simple cloud infrastructure. Last year, we moved all our legacy apps to a Docker-based deployment (we were already using Docker for newer stuff). Nothing fancy—just basic Dockerfile and docker-compose.yml. Advantages: - Easy to manage: we keep a repo of docker-compose.yml files for each environment. - Simple commands: most of the time, it’s just \"docker-compose pull\" and \"docker-compose up.\" - Our CI pipeline builds images after each commit, runs automated tests, and deploys to staging for QA to run manual tests. - Very stable: we deploy the same images that were tested in staging. Our deployment success rate and production uptime improved significantly after the switch—even though stability wasn’t a big issue before! - Common knowledge: everyone on our team is familiar with Docker, and it speeds up onboarding for new hires. reply ghomem 9 hours agorootparentprevI mentioned Docker because it interests many developers but on VMs that I control I do not need Docker at all. Deploying with Docker provides host OS independence which is nice if you are distributing but unnecessary if the host is yours, running a fixed OS. For Python backends I often deploy the code directly with a Puppet resource called VcsRepo which basically places a certain tag of a certain repo on a certain filesystem location. And I also package the systemd scripts for easy start/stop/restart. You can do this with other config management tools, via bash or by hand, depending on how many systems you manage. What bothers me with your question is Pip :-) But perhaps that is off topic...? reply Gud 9 hours agorootparentNo, you are tied to docker supported operating systems. Will not run on FreeBSD, for example. reply ghomem 9 hours agorootparentI'll correct myself: s/host OS independence/a certain level of host OS independence And getting containers to run depends on the OS - if you don't control the host, leads to major ping-pongs. Even within Linux (Ubuntu, Debian, RHEL, etc) when you are distributing multiple related containers there are details to care about, not about the container itself but about the base OS configuration. It's not magic. reply BSDobelix 6 hours agorootparentprev>Will not run on FreeBSD, for example. Not true: https://podman.io/docs/installation#installing-on-freebsd-14... ATM experimental reply dlisboa 8 hours agorootparentprevOP is talking about substituting a Kubernetes setup. FreeBSD was never in the cards. 99% of companies in the cloud don’t run or care about anything other than Linux. reply Gud 3 hours agorootparentThat may be true, but it’s still not “host OS independence”, which was my point reply ffsm8 8 hours agorootparentprev> No, you are tied to docker supported operating systems No, you're tied to operating systems using a Linux kernel that supports the features necessary for running images. reply Gud 6 hours agorootparentYou can run Linux under FreeBSD using either bhyve, using the Linux emulator and under jails. But you cannot run docker. reply BSDobelix 6 hours agorootparent>But you cannot run docker. You can -> Podmaaan https://podman.io/docs/installation#installing-on-freebsd-14... ATM experimental reply marcosdumay 2 hours agorootparentprevPython, Ruby, and to a much larger extent PHP are the Docker showcase! For example, if you have a program that uses wsgi and runs on python 2.7, and another wsgi program that runs on python 3.16, you will absolutely need 2 different web servers to run them. You can give different ports to both, and install an nginx on port 80 with a reverse proxy. But software tends to come with a lot of assumptions that make ops hard, and they will often not like your custom setup... but they will almost certainly like a normal docker setup. reply RUnconcerned 9 hours agorootparentprevFamously, no one has ever had Python environment problems :D reply ghomem 9 hours agorootparentIf you really want to open that can of worms, here it goes: Pipy is an informal source of software that has low security levels and was infested with malware many times over the years. It does not provide security updates: it provides updates that might include security-related changes as well as functional changes. Whenever you update a package from there, there is a chain reaction of dependency updates that insert untested code in your product. Due to this, I prefer to target an LTS platform (Ubuntu LTS, Debian, RHEL...) and adapt to whatever python environment exists there, enjoying the fact that I can blindly update a package due to security (ex: Django) without worrying that it will be a new version which could break my app. * Furthermore, with Ubuntu I can get a formal contract with Canonical without changing anything on my setup, and with RHEL it comes built-in with the subscription. Last time I checked Canonical's security team was around 30pax (whereas Pipy recently hired their first security engineer). These things provide supply-chain peace of mind to whoever consumes the software, not only to who maintains it. I really need to write an article about this. * exceptions apply, context is king reply ramses0 2 hours agorootparentI've just doubled down on \"making my own Debian packages\". There's tons of examples, you are learning a durable skill, and 90% of the time (for personal stuff), I had to ask myself: would I really ever deploy this on something that wasn't Debian? Boom: debian-lts + my_package-0.3.20240913 ...the package itself doesn't have to be \"good\" or \"portable\", just install it, do your junk, and you don't have to worry about any complexity coming from ansible or puppet or docker. However: docker is also super nice! FROM debian:latest ; RUN dpkg -i my_package-*.deb ...it's nearly transparent management. reply throwaway894345 51 minutes agorootparentI don't mean this as a rebuttal, but rather to add to the discussion. While I like the idea of getting rid of the Docker layer, every time I try to I run into things that remind me why I use Docker: 1. Not needing to run my own PPA server (not super hard, it's just a little more friction than using Docker hub or github or whatever) 2. Figuring out how to make a deb package is almost always harder in practice for real world code than building/pushing a Docker container image 3. I really hate reading/writing/maintaining systemd units. I know most of the time you can just copy/paste boilerplate from the Internet or look up the docs in the man pages. Not the end of the world, just another pain point that doesn't exist in Docker. 4. The Docker tooling is sooooo much better than the systemd/debian ecosystem. `docker logs ` is so much better than `sudo journalctl --no-pager --reverse --unit .service`. It often feels like Linux tools pick silly defaults or otherwise go out of their way to have a counterintuitive UI (I have _plenty_ of criticism for Docker's UI as well, but it's still better than systemd IMHO). This is the biggest issue for me--Docker doesn't make me spend so much time reading man pages or managing bash aliases, and for me that's worth its weight in gold. reply ramses0 1 minute agorootparentYuuup! I'm super-small time, so for me it's just `scp *.deb $TARGET:.` (no PPA, although I'm considering it...) Really, my package is currently mostly: `Depends: git, jq, curl, vim, moreutils, etc...` (ie: my per-user \"typically installed software\"), and I'm considering splitting out: `personal-cli`, `personal-gui` (eg: Inkscape, vlc, handbrake, etc...), and am about to have to dive in to systemd stuff for `personal-server`, which will do all the caddy, https, and probably cgi-bin support (mostly little home automation scripts / services). I'm 100% with you w.r.t. the sudo journalctl garbage, but if you poke at cockpit https://www.redhat.com/sysadmin/intro-cockpit - it provides a nice little GUI which does a bunch of the systemd \"stuff\". That's kindof the nice tag-along ecosystem effects of \"just be a package\". I'm definitely relatively happy with docker overall, but there's useful bits in being more closely integrated with the overall package system management (apt install ; apt upgrade ; systemctl restart ; versions, etc...), and the complexity that you learn is durable and consistent across the system. hello0904 9 hours agorootparentprevOption 1: python3 -m venv venv > source project/venv/bin/activate Option 2: use Poetry How is this different than a Dockerfile that is creating the venv? Just add it to beginning, just like you would on localhost. But that is why I love to code Python in PyCharm, they manage the venv in each project on init. reply ghomem 8 hours agorootparentMy comment about pip is orthogonal to Docker. This is the same with or without Docker - I added a comment on this thread with more detail. reply bob1029 3 hours agorootparentprevI think a lot of (justifiable) Docker use comes out of being forced to use other tools & ecosystems that are fundamentally messy and not really intended for galactic-scale enterprise development. I have found that going all-in with certain language/framework features, such as self-contained deployments, can allow for really powerful sidestepping of this kind of operational complexity. If I was still in a situation where I had to ensure the right combination of runtimes & frameworks are installed every time, I might be reaching for Docker too. reply darby_nine 7 hours agorootparentprevDockerfiles compose and aren't restricted to running on linux. Those two reasons alone basically mean I never need to care about systemd again reply throwaway894345 47 minutes agorootparentYeah, not caring about systemd is a big win for me. And I don't just mean the cryptic systemd unit syntax, but also the absolutely terrible ux of every CLI tool in the suite. I'm tired of having to pass half a dozen flags every time I want to view the logs of a systemd unit (or forgetting to type `sudo` before `systemctl`). I'm tired of having to remember the path to the systemd unit files on each system whenever I need to edit the files (is it `etc/systemd/system/...` or `etc/system/systemd/...`?). Docker is far from perfect, but at least it's intuitive enough that I don't have to constantly reference man pages or manage aliases. I would love to do away with the Docker layer, but first the standard Linux tooling needs to improve a lot. reply Sammi 9 hours agorootparentprevHonestly most people's dockerfile could just as well be a bash script. reply kbolino 3 hours agorootparentYou don't run a Dockerfile on every machine, and a bash script doesn't produce an image. They're not even solving the same problem. reply kristiandupont 8 hours agorootparentprevI find Dockerfile's even simpler to work with than bash scripts. reply hello0904 9 hours agorootparentprevExactly! This person gets it. Oh, and not only build their app, they can take it a step further and setup the entire new vps and app building in one simple script! reply ghomem 6 hours agorootparentOnce you do it for long enough it might be worth it to consider configuration management where you declare native structured resources (users, firewall rules, nginx reverse proxies, etc) rather than writing them in shell. I use Puppet for distribution of users, firewall rules, SSH hardening + whitelisting, nginx config (rev proxy, static server, etc), Let's Encrypt certs management + renewal + distribution, PostgreSQL config, etc. The profit from this is huge once you have say 20-30 machines instead of 2-3, user lifecycle in the team that needs to be managed, etc. But the time investment is not trivial - for a couple of machines it is not worth it. reply throwaway894345 43 minutes agorootparentHonestly not having to use Puppet or Ansible are among my reasons for using Docker. I do some basic stuff in cloud-init (which is already frustrating enough) to configure users, ssh, and docker and everything else is just standard Docker tooling. reply grutetc 3 hours agorootparentprevI feel y’all are too focused on the end product. I deploy to pared down bare metal, but I use containerization for development, both local and otherwise, for me and contributors. So much easier than trying to get a local machine to be set up identically to a myriad of servers running multiple projects with their idiosyncratic needs. I like developing on my Qubes daily driver so I can easily spin up a server imitating vm, but if I’m getting your help, especially without paying you, then I want development for you to be as seamless as possible whatever your personal preferred setup. I feel containerization helps with that. reply Sammi 8 hours agorootparentprevI'm doing it :) I split it into multiple scripts that get called from one, just for my own sanity. reply authorfly 6 hours agorootparentprevBecause it seems unobvious but docker always saves you. It's actually quicker than running pip install requirements.txt once you get a year in. (Trust me, I used to take your approach). Forget about \"clunky overhead\" - the running costs areIt's a wrapper around linux and a middle between my OS and my app No. Docker doesn't \"wrap\" anything, and it certainly does not wrap Linux. Please reconsider looking at the documentation. It uses native kernel features. SystemD does a similar thing. > For example Docker complexity - you need to learn a new filetype, a new set of commands, a new architecture, new configurations, spend hours reading another set of documentation I can't say I agree. reply icedchai 2 hours agorootparentprevDocker is native Linux. Your app uses the same kernel as the host. Is \"chroot\" middleware? No. Neither is docker. reply ownagefool 7 hours agorootparentprevA wrapper CLI that produces the same outcome wouldn't really be considered middleware, which surely should affect runtime? reply j-krieger 3 hours agorootparentprev> Docker is literally adding middleware between your Linux system and app. Not really, no. Docker just uses functionality provided by the Linux kernel for its exact use case. It's not like a VM. > it's absolutely overhead and complexity that is not necessary. This is demonstratively wrong. Docker introduces less complexity compared to system native tools like Systemd or Bash. Dockerfiles will handle those for you. > I have no idea what I am talking about I wouldn't say that. You seem to have strong puritarian opinions tough. reply ffsm8 8 hours agorootparentprevO rly, pray tell, which middleware? Your most powerful feature is literally a hostfile that docker generates on container start that's saved at /etc/hosts + Iptables rules Edit: and if you don't want them, use Network-Mode: host and voila, none of that is generated reply PhilipRoman 7 hours agorootparentprev>have it manage my network interface and ports for me ...and bypass the host firewall by default unless you explicitly bind stuff to localhost :-/ I don't particularly love or hate docker, but when I realized this, I decided to interact with it as little as possible for production environments. Such \"convenient\" defaults usually indicate that developers don't care about security or integrating with the rest of the system. reply otabdeveloper4 6 hours agorootparentprev> docker doesn't create an overhead by itself Yes it does, the Docker runtime (the daemon which runs under root) is horribly designed and insecure. reply throwaway894345 38 minutes agorootparentThere isn't a \"Docker runtime\", and the daemon is not a runtime any more than systemd is a runtime. They're both just managing processes. If you want to argue that Docker containers have an overhead, you could maybe argue that the Linux kernel security features they employ have an additional overhead, but that overhead is likely to be marginal compared to a less secure approach and moreover since you're Very Concerned About Security™ I'm sure you would prefer to pay the security cost. reply Timber-6539 6 hours agorootparentprevInsecure in what way? Rootful docker is a mature product that comes with seccomp and standard apparmor policies ootb! reply a_c 8 hours agoparentprevApart from the operation side, there is a development side parallel too. Two examples that I came across - \"Test\" mean if it passes on CI, it is good. Failing to run test on local? Who do development on local anyway? - Teams so reliant on \"AI\" because this is the future of coding. \"how to sort a list in python\" became a prompt, rather than a lookup on the official documentation. reply dijit 11 hours agoparentprevI'm with you, but for me Cloud does have one major benefit: If you use it as IaaS, it's a lot quicker to get prototypes working than if you use anything else, including VPS's from other providers. Google Cloud in particular has very few vectors for lock-in, and follows more principle of least surprise. But once you have prototyped, you should ask the question about rebuilding it somewhere that is cheaper. Near infinite scalability of disk drives is nice, and snapshotting, and cloud in general can allow you to extend your prototype into taking production load and allowing you to measure what you will need; but leaning in to \"cloud magick\" (cloud run, lambdas, etc) will consume almost as much time to learn and debug as just doing it the old school way anyway. In my lived experience. reply ghomem 10 hours agorootparentI am not against the cloud. VMs are also cloud, unless you run them on your own servers. For instance, the Hetzner Cloud (mostly VMs, plus load balancers and disks) is so cheap and has such a nice CLI API that it competes aggressively with dedicated servers - I would definitely start any with VMs, not with iron. The biggest problem is the so called cloud native stuff which is both more expensive and more complex. There are contexts where it makes sense but for startups they are doing more harm than good. reply finaard 10 hours agorootparentThing is, by the time the cloud native stuff makes sense most companies are at a scale where it'd be cheaper to just hire a good devops team, and start building your own cloud infra on own hardware. reply ghomem 10 hours agorootparentProbably so. And that would be likely my approach at such scale. Still, my most benevolent interpretation of current reality is, rather than saying \"that cloud native stuff is crap\", accepting that there are cases where it may make sense. For instance, large companies might have trouble hiring a good ops team because they have in general trouble hiring and retaining talent (another conversation topic). Ops people are a scarce good because univs do not train people for that and most people prefer coding. I am leaving the work devops out because the market completely perverted its meaning. (my take on the devops funeral: https://logical.li/blog/devops/ ) reply ghomem 10 hours agorootparentprevReference: https://survey.stackoverflow.co/2022/#developer-profile-deve... Only around 11% of the whole devs identify as devops specialist or cloud infrastructure engineer. This is why I am saying ops people are a scarce good (unfortunately) from a data driven perspective. Of course my daily life confirms it. reply finaard 9 hours agorootparentMost of my money comes from companies unable to handle even simple setups - and having trouble to find the right people, so I somewhat agree too. But it's mainly an education problem - it's pretty much impossible to find good people with that skillset, but it is possible to find people straight out of University willing to learn. reply ghomem 9 hours agorootparentI fully agree with you: it is mostly an education problem and you can find people willing to learn right out of univ. Indeed, that is exactly my experience: I successfully onboarded several (carefully selected) junior people into the ops skillset over the years and I have seen them do wonders with customer systems, while enjoying their \"ops life\", without having fires every day. The connection of this to the replies above it: I am not sure if this kind of junior people would be easy to retain in a large corporate environment. We certainly can do that in niche consulting. reply finaard 8 hours agorootparentWe're a tiny company doing ops as services for large corporations - with one customer now coming close to a decade. That solves the retaining problem as we have limited exposure to all that big corporation nonsense, and have the option for individuals to go on a vacation in other projects without losing their knowledge in the organisation. reply ghomem 6 hours agorootparentI had the exact same business for 18 years :-) and yes, without corporate nonsense it is easy to retain intelligent people. Cheers reply karmarepellent 5 hours agoparentprev> while monitoring configuration compliance with a custom Naemon plugin. While I absolutely agree with you and your approach, would you mind elaborating what kind of configuration compliance you are referring to in this statement? I suppose you do not mean any kind of configuration that your Puppet code produces as that configuration is \"monitored\", or rather managed, by Puppet. reply ghomem 4 hours agorootparentI don't mind elaborating - the fact that people are asking me questions reminds me that I need to invest a bit more effort on some articles. This case is actually pretty simple. Puppet applies the configuration you declare impotently when you run the Puppet agent: whatever is not configured gets configured, whatever is already configured remains the same. If there is an error the return code of the Puppet agent is different from that of the situations above. Knowing this you can choose triggering the Puppet agent runs remotely from a monitoring system, (instead of periodical local runs), collecting the exit code and monitoring the status of that exit code inside the monitoring system. Therefore, instead of having an agent that runs silently leaving you logs to parse, you have a green light / red light system in regards to the compliance of a machine with its manifesto. If somebody broke the machine leaving it in an unconfigurable state or if someone broke its manifesto during configuration maintenance you will soon get a red light and the corresponding notifications. This is active configuration management rather than what people usually call provisioning. Of course you need an SSH connection for this execution and with that you need hardened SSH config, whitelisting, dedicated unpriviledged user for monitoring, exceptional finegrained sudo cases, etc. Not rocket science. reply JamesonNetworks 6 hours agoparentprevI’ve just recently gotten into ansible and find myself building the same thing. I wrote a script to interact with virsh and build vms locally so I can spin up my infra at home to test and deploy to the cloud if and when I want to spend actual money. I’m still very much an ansible noob, but if you have a repo with playbooks I’d love to poke around and learn some things! If not, no worries, I appreciate your time reading this comment! reply itronitron 6 hours agoparentprevI can't remember the last time I've seen a position description for a software developer (or anything tech related for that matter) that didn't include a requirement for skills in some cloud related tech. Sometimes the job descriptions are boastful in their reference to those technologies, and other times you can detect some level of despair. reply karmarepellent 5 hours agorootparentNow I am curious: how do you detect despair regarding cloud tech in job descriptions? reply globular-toast 8 hours agoparentprevI feel like Kubernetes is always randomly mentioned in rants like this. Instead of saying your hardened VM has Docker you could have just said it has kubelet on it. Then instead of a bunch of ad hoc \"docker services\" you could pay pennies for a k8s control plane that gives you control over everything on those VMs. I fail to see how your way is anything but worse. The bad cloud infrastructure is when people try to use every single thing AWS sells and their whole infrastructure is at super high levels of abstraction that they could never migrate to another platform. K8s isn't that at all. reply ownagefool 8 hours agorootparentIn think in either case, if you already have code that's done, using that is going to be less effort than switching. However, I ran kubeadm on a hetzner server and it's just sat chugging along forever basically. I use the cluster to run ephemeral apps where I build and deploy 1 golang service, a couple of node services in about 60 seconds ( with cache, obviously ). As someone old enough and skilled enough to do the same with puppet, why bother when it's simpler easier that even the kids who don't understand TLS can do it with k8s? reply zepolen 5 hours agorootparentprev100% best comment in this thread. With k8s you get a way of saying 'WHAT YOU WANT' without 'HOW TO DO IT', and this is applies not only to the actual infra aspect, but the people maintaining it too. Any cloud platform and devops worth their salt can maintain a k8s system. Good luck finding someone to understand what that 'custom Naemon' plugin is doing. reply zepolen 5 hours agoparentprevHow do you monitor this setup? How do you control access to this setup? How do you deploy on a different provider to Hetzner? How do you access logs on this setup? How do others maintain this setup? How do you run backups? How do you run cron jobs? How do you deal with an offline node? How do you expose a new ingress? How do you provision extra storage on this setup? If any of those is answered with 'something homegrown' or 'just write a script' then you have all the reasons k8s is worth it. reply ghomem 4 hours agorootparentThe questions are short but the answers would be long. Puppet manages all fine grained OS resources (files, dirs, repos, cronjobs, sudo declarations, firewall rules, etc) and you aggregate those resources into classes which are then pushed to different machines. The classes are parametrizable for the differences between systems. If I was to write an idempotent script for each native resource I would finish in some years :-) You chose whatever monitoring system you like the most. For offline nodes you use whatever the level of criticity of your node justifies. This is something people struggle to understand: not every business needs 99.99% uptime. That said, I never had a downtime in Hetzner. On Digital ocean I had one short forced reboot in 4 years. YMMV so protect yourself as much as necessary. Deploying on a different provider than Hetzner is the same as deploying on Hetzner except the part of launching the machine which is trivial to script - the added value is making the machine work and Ubuntu/Debian/RHEL are the same everywhere. You don't have vendor lock in with this. If K8s works for you, enjoy it. Nobody is telling you to stop :-) reply pella 4 hours agorootparentprevHetzner and Kubernetes are not mutually exclusive. - https://github.com/kube-hetzner/terraform-hcloud-kube-hetzne... - https://www.hetzner.com/hetzner-summit --> \"Managed Kubernetes Insights and lessons learned from developing our own Kubernetes platform\" reply mattbillenstein 14 hours agoprevBasically doing this for a small startup - there are some complexities around autoscaling task queues with gpus and whatnot, but the heart of it is on a single VM (nginx, webapp, postgres, redis). We're b2b, so there's very little traffic anyway. The additional benefit is devs can run all the same stuff on a Linux laptop (or Linux VM on some other platform) - and everyone can have their own VM in the cloud if they like to demo or test stuff using all the same setup. Bootstrapping a new system is checking in their ssh key and running a shell script. Easy to debug, not complex or expensive, and we could vertically scale it all quite a ways before needing to scale horizontally. It's not for everyone, but seed stage and earlier - totally appropriate imo. reply mdaniel 3 hours agoparent> Bootstrapping a new system is checking in their ssh key and running a shell script. If it interests you, both major git hosts (and possibly all of them) have and endpoint to map a username to their already registered ssh keys: https://github.com/mdaniel.keys https://gitlab.com/mdaniel.keys It's one level of indirection away from \"check in a public key\" in that the user can rotate their own keys without needing git churn Also, and I recognize this is departing quite a bit from what you were describing, ssh key leases are absolutely awesome because it addresses the offboarding scenario much better than having to reconcile evicting those same keys: https://github.com/hashicorp/vault/blob/v1.12.11/website/con... and while digging up that link I also discovered that Vault will allegedly do single-use passwords, too , but since I am firmly in the \"PasswordLogin no\" camp, caveat emptor with that one reply mattbillenstein 1 hour agorootparentYeah, I've used the github ssh key thing before, but never heard of key leases - will take a look. Thx! reply teaearlgraycold 14 hours agoparentprevI did this type of setup but without even redis. Postgres can do anything. reply mattbillenstein 14 hours agorootparentTrue, I use it mainly for a few convenience things - holding ephemeral monitoring data, distributed locks, redis streams for some pub/sub stuff, sorted sets can be handy - things I could do in Postgres, but are a bit simpler in Redis. reply normie3000 12 hours agoparentprevI love the simplicity of this approach. In your setup, how do you track config and updates of your VMs? reply mattbillenstein 1 hour agorootparentI have a custom deployment system which idempotently configures an Ubuntu LTS VM. All the config templates are checked into source control. I don't configure anything by hand - it's either handled in this thing or via a small user-data script run at provisioning time. reply mdaniel 3 hours agorootparentprevLike everything, it's context dependent, but wowzers my life has improved so much since I got on board the Flatcar or Bottlerocket train of immutable OS. Flatcar (née CoreOS) does ship with docker but is still mostly a general purpose OS but Bottlerocket is about as \"cloud native\" as it comes, shipping with kubelet and even the host processes run in containers. For my purposes (being a k8s fanboy) that's just perfect since it's one less bootstrapping step I need to take on my own Both are Apache 2 and the Flatcar folks are excellent to work with https://github.com/flatcar/Flatcar#readme https://github.com/bottlerocket-os#bottlerocket reply mattbillenstein 1 hour agorootparentSure, but again, complexity - stuff people have to learn/maintain/upgrade, etc. ymmv Running and configuring VMs isn't hard to do correctly, it just takes discipline to never \"hack it in the moment\" - or if you do, can that change in your config system. reply thebeardisred 1 hour agorootparentprevAs usual, I'm stoked to see I'm not the only one using Flatcar. :) reply clvx 11 hours agorootparentprevI like this but one of the issues with this approach is if no Docker images like traditional configuration management tool, you are going for a world of pain. Docker and Docker images have tons of best practices already defined for plenty of use cases. If it's already containerized; then, jumping to any orchestrator that supports OCI images is more about adjusting the business to a new set of operations. reply theptip 3 hours agoprevEven the book on Microservices says “First build the Monolith”. You don’t know how to split your system until you have actually got some traction with users, and it’s easier to split a monolith than to reorganize services. You may never need to split your monolith! Stripe eventually broke some stuff out of their Rails monolith but it gets you surprisingly far. You are not going to get easier to debug than a Django/Rails/etc monolith. I bit of foresight on where you want to go with your infra can help you though; I built the first versions of our company as a Django Docker container running on a single VM. Deploy was a manual “docker pull; docker stop; docker start”. This setup got us surprisingly far. Docker is nice here as a way of sidestepping dependency packaging issues, this can be annoying in the early stages (eg does my server have the right C header files installed for that new db driver I installed? Setup Will be different than in your Mac!) We eventually moved to k8s after our seed extension in response to a business need for reliability and scalability; k8s served us well all the way through series B . So the setup to have everything Dockerized made that really easy too - but we aggressively minimized complexity in the early stages. reply corytheboyd 3 hours agoparentYes! Also, use the damn framework, instead of rebuilding shitty versions of features it offers! One good seasoned person will outperform 10 non-seasoned people in this regard. It will add up over time. I think half the real reason people are soured to monoliths is because they are bad, poorly run monoliths. reply randomdata 2 hours agoparentprev> Even the book on Microservices says “First build the Monolith”.\\ And yet, funnily enough, the book on Monoliths says to break things up into smaller services! It says your data should be stored in its own service (possibly multiple services, if you need multi-paradigm access [e.g. relational, full-text search, etc.]). The user experience should use its own service. And, at very least, you should have another service in between (this is where Django and Rails usually fit). Optionally, it says, you will probably want to have additional services as well (auth, financial transitions, etc.) reply jwr 9 hours agoprevThe answer is \"no, it doesn't\". I've been running my SaaS first on a single server, then after getting product-market fit on several servers. These are bare-metal servers (Hetzner). I have no microservices, I don't deal with Kubernetes, but I do run a distributed database. These bare-metal servers are incredibly powerful compared to virtual machines offered by cloud providers (I actually measured several years back: https://jan.rychter.com/enblog/cloud-server-cpu-performance-...). All in all, this approach is ridiculously effective: I don't have to deal with complexity of things like Kubernetes, or with cascading system errors that inevitably happen in complex systems. I save on development time, maintenance, and on my monthly server bills. The usual mantra is \"but how do we scale\" — I submit that 1) you don't know yet if you will need to scale, and 2) with those ridiculously powerful computers and reasonable design choices you can get very, very far with just 3-5 servers. To be clear, I am not advocating that you run your business in your home closet. You still need automation (I use ansible and terraform) to manage your servers. reply stcroixx 4 hours agoparentThe scaling thing is a great boogeyman. It preys on this optimism your software is going to be so successful in such a short amount of time which people want to believe. reply mexicocitinluez 6 hours agoparentprevThe answer is \"it depends\". Did you read the article or just the headline? Scroll down to the bottom, under the section \"A few considerations\" and try not to laugh. \"A few considerations\" turns out to be a pretty significant chunk of security work ESPECIALLY if you are storing/transmitting highly sensitive information. How do you handle something like HIPPA compliance when you're in this situation? There are 2 types of programmers: those that think they've seen everything and those that know they've seen next to nothing. And as such, these absolute takes are tiring. reply christophilus 4 hours agorootparentI've written a HIPPA-compliant application that was VPS-hostable. It's been a while, but IIRC, it simply involved a combination of TLS everywhere and encrypting the sensitive fields in the DB. I don't remember if there was any other trick involved, but it wasn't difficult. By far the hardest thing about that project was the complexity of the medical codes-- not HIPAA compliance-- and that is something the cloud wouldn't help with at all. reply mexicocitinluez 3 hours agorootparent> , it simply involved a combination of TLS everywhere and encrypting the sensitive fields in the DB. I'm sorry, are you saying securing patient data is simple? No offense, but you might be the only person on this planet to share this sentiment and there's a reason why. So, it's simpler to secure sensitive information in a database, secure your hosting, maintain security updates to those hosts, undergo audits, keep up with changing regulations, keep up with the latest threat vulnerabilities, staff a full response team in case something happens, etc? Not trying to be rude, but it's obviously not simple. What's crazy about your answer is that we had a whole host of \"Bitcoin for your data hacks\" that were only made possibly by setups your describing. >By far the hardest thing about that project was the complexity of the medical codes- Yes, this is also complex. But a totally different problem in a totally different space. reply wadadadad 3 hours agorootparent> secure sensitive information in a database, secure your hosting, maintain security updates to those hosts, undergo audits, keep up with changing regulations, keep up with the latest threat vulnerabilities, staff a full response team in case something happens To be fair of the things you've described, if you can swing it, you should be doing most of this regardless for a business setup. Specific to HIPAA would be the auditing and 'changing regulations' (and depending on client needs, you'll likely have other audits for business needs). I'm going through a gap analysis for HIPAA now; would you mind sharing what impactful changing regulations you've seen in the past 5 years? reply mexicocitinluez 3 hours agorootparent> To be fair of the things you've described, if you can swing it, you should be doing most of this regardless for a business setup Not sure how to respond to this. Are you saying I should go out and hire 2-3 people to set up a ton of infrastructure and maintain it for me instead of relying on the professionals at Azure (who specialize in this) and it's done automatically at a fraction of the cost? We went through 5 years of \"bitcoin for your data\" fraud in exactly the situation your describing. I don't need to hire anybody as of now. None. > I'm going through a gap analysis for HIPAA now; would you mind sharing what impactful changing regulations you've seen in the past 5 years? This is my point. I don't know and don't care. I don't have to worry about it at all. I don't have to worry about updating the handful of apps and servers that connect to all the different integrations we use because this field is siloed into a 1,000,000 little pieces. I don't have to worry about PHI getting leaked out of some server I forgot to update somewhere or misconfigured because I made a mistake while installing it or setting it up the first time. That stuff is all handled through Azure's existing cloud infrastructure. It's literally tailored to healthcare solutions. No single person (or 2 or 3 or even 4) full time people could come close to what they offer at the cost. reply wadadadad 2 hours agorootparentI don't think I was communicating my first point effectively; I didn't mean to reference you personally or to the approach taken (VPS or cloud). If there is a business who needs HIPAA, then most likely, the business should be doing all of those original points because doing them is better (more effective, better security, etc.) than not doing them. I'm trying to say than extending to HIPAA could potentially be 'simple' if there is a business already doing most of this. I understand that you're using Azure's existing infrastructure to handle your logistical technical management, but I was here asking if you had to make any changes to keep abreast of changing regulations. There seems to be practical business decisions that need to be made that HIPAA impacts, such as what data constitutes PHI (has that changed? Maybe you had to go back and change what data you were keeping because of the above regulation changes- I don't know if that could be the case, that's why I'm asking, I'm not aware of what I don't know). If Azure is somehow keeping track of all \"changing regulations\" for you (including business needs) and you've never had to worry about it, that's good to know. I would still be interested in any specific details if you're aware of it. reply mexicocitinluez 1 hour agorootparentSorry, totally misinterpreted that. > but I was here asking if you had to make any changes to keep abreast of changing regulations. No, we haven't. Not yet. > If Azure is somehow keeping track of all \"changing regulations\" for you (including business needs) and you've never had to worry about it, that's good to know. I would still be interested in any specific details if you're aware of it. I get your question know. So, when I was referring to Microsoft and HIPAA it was primarily around this side of things: https://learn.microsoft.com/en-us/azure/compliance/offerings... You do bring up a good point and I shouldn't have implied otherwise that it can handle everything for you. So yes, there is a ton of other stuff that isn't magically handled by you such as identifying PHI and stuff. That being said, they have a whole suite of analytical and machine learning tools that will help you do this. But since you mentioned policy changes, https://www.cms.gov/priorities/key-initiatives/burden-reduct... this is big and will have wide-reaching consequences and things like the ability to export patient data isn't necessarily baked into Azure. BUT, they do have this healthcare platform they're building like this stuff https://learn.microsoft.com/en-us/dynamics365/industry/healt... that I would imagine would provide a bit more coverage on those types of changes than something you're building yourself. Here's a deidentification service that can be integrated: https://learn.microsoft.com/en-us/azure/healthcare-apis/deid... reply wadadadad 1 hour agorootparentAwesome, I really appreciate your time and the references. Thank you! reply mexicocitinluez 1 hour agorootparentNo problem at all. It's such a fascinating and cool field to build software in. Someone else above had mentioned the complexity of medical coding and I don't know what you do or what you're working on but that's another really interesting part of the puzzle. And starts to get into why it's so hard for one system to communicate with each other in healthcare. reply pistoleer 5 hours agorootparentprev> How do you handle something like HIPPA compliance when you're in this situation? I'm a dev who hasn't seen anything related to that. Since you bring it up, can you give some pointers on why something like a MySQL db coupled to a monolithic backend isn't good enough? What shortcomings did you experience? All of the things raised in the article seem possible to solve without the need for microservices. reply mexicocitinluez 4 hours agorootparent> All of the things raised in the article seem possible to solve without the need for microservices. First, this has nothing to do with microservices. Needing cloud infrastructure and building microservices are 2 orthogonal things. Second, it has nothing to do with the tech you're using. MySQL is irrelevant. So is a monolithic backend. What IS important is the security and infrasture behind the data your storing. Clinical data (and data captured in EMR's) is easily some of the most sensitive stuff you'll come across (unless you work in govt). The idea that I wouldn't use off-the-shelf, already-tested solutions specifically for this problem with a cloud provider is nuts. I pay Azure peanuts compared to what I'd have to pay a full-time person to manage multiple environments, security updates, provisioning new infra, etc. And that's not even considering the actual process you need to go to connect to outside systems. Most integrations want you to have a SOCS audits and stuff. What happens when there is a breach? Do you have the personnel on staff to understand and troubleshoot the issue? Remember the \"we have your data and will release it for bitcoin\" hacks? That's only made possible by these systems sitting in closets in someone's facility. And trust me, this isn't just a \"large enterprisey\" problem. It's a \"everyone who wants to build an app in this space\" problem. So you can use MySql (if you can host it compliantly) and I'm building what you could theoretically call a \"monolithic\" backend and it's working well. I use MSSQL on Azure though. reply liampulles 11 hours agoprevThere is a core 20% of kubernetes, which is deployments, pods services and the way it handles blue-green deployments and declarative based definitions, namespace seperation, etc. that is really good. Just keeping to those simple basics, using a managed cloud kubernetes service, and running your state (database) out of cluster is a good experience (IMO). It's when one starts getting sucked down the \"cloud native\" wormhole of all these niche open source systems and operators and ambassador and sidecar patterns, etc. that things go wrong. Those are for environments with many independent but interconnecting tech teams with diverse programming language use. reply maeln 11 hours agoparentFor a lot of company and project I worked on, this is the same conclusion I came to. 99% we only need / want is docker-compose++. Things like 0-downtime deployment out of the box, simple configuration system for replica set and other replication / distribution mechanism, and that is basically it. I which there was something that did just that, because kube comes with a lot of baggage, and docker-compose is a bit too basic for some important production needs. reply wanderlust123 8 hours agorootparentWhy not use docker swarm? reply ellieh 3 hours agoparentprevExactly this. Kubernetes has a million knobs and dials you can tweak for any use case you want, but equally they can be ignored and you can use the core functionality and keep it simple. I can have something with nice deployments, super easy logs and metrics, and a nice developer experience setup in no time at all. reply globular-toast 8 hours agoparentprevFor me this is all Kubernetes is. I feel like people are often talking about two different things in discussions like this. For me it's just a uniform way to deploy stuff that is better than docker compose. We pay pennies for the control plane and workers are just generic VMs with kubelet. But I think for many \"kubernetes\" means your second paragraph. It doesn't have to be like that at all! People should try settling up a k3s cluster and just learn about workloads, services and ingresses. That's all you need to replace a bunch of ad hoc VMs and docker stuff. reply bob1029 10 hours agoprevVMs, block & blob storage, DNS, IdP, domain registrar. These are the only things I have ever been comfortable using in the cloud. Once you get into FaaS and friends, things get really weird for me. I can't handle not having visibility into the machine running my production environment. Debugging through cloud dashboards is a shit experience. I think Microsoft's approach is closest to actually \"working\", but it's still really awful and I'd never touch it again. The ideal architecture for me after 10 years is still a single VM with monolithic codebase talking to local instances of SQLite. The advent of NVMe storage has really put a kick into this one too. Backups handled by snapshotting the block storage device. Transactional durability handled by replicating WAL, if need be. Dumbass simple. Lets me focus on the business and customer. Because they sure as hell don't care about any of this and wouldn't pay any money for it. All this code & infra is pure downside. You want as little of it as possible. reply mianos 15 hours agoprevWho is going to get a new job without k8s on their resume. :) Seriously, I think a lot of people do things the hard way to learn large scale infrastructure. Another common reason is 'things will be much easier when we scale to a massive number of clients', or we can dynamically scale up on demand. These are all valid to the people building this, just not as much to founders or professional CTOs. reply kristopolous 12 hours agoparentExcuse my harshness but people doing it needlessly is just unprofessional waste and abuse. Some people seem to have no concern with the needs and timetables of the would be customers but instead burn through cash building fancy nonsense. It's like going in to a car mechanic for tires and then finding out it took 3 weeks because the guy wanted to put on low rider hydraulics and spinner hubcaps for his personal enrichment. The worst part is it's inherently ambiguous to the next people. They don't know if the reason something is there is because it's needed or because it's just shiny bling. reply mianos 8 hours agorootparentI am certainly not saying everything you say is not all true. My comment is dark humour. I really like your last point. Years ago I replaced a huge hadoop cluster data processing job with a single app on one machine with a few CPUs, that reduced a job that took over 8 hours to 20 minutes. What is even dumber is, it was just a python script and gnu parallel, which used to be perl. reply BeFlatXIII 4 hours agorootparentprev…but if the bosses at competing mechanic shops hire based on quality of low riders a mechanic can install, of course they'll practice on the paying customers. reply sussexby 14 hours agoparentprevJust take a look at the level of complexity in home lab subreddits! I don’t quite get if people do it for interest, for love of the tech, or if they are technocratic and believe in levelling up their skill to get k8s on their CV like you say. All I think is “this looks painful to manage”! reply from-nibly 14 hours agorootparentK8s is painful to get started, and painful to learn. But once you have it up you can just keep adding stuff to it. I run a k8s cluster at home. Part of it yes, is to apply my existing skills and keep them fresh. But part of it is that kubernetes can be easier long term. Ive got magical hard drive storage with rook ceph. I can yoink a hard drive out of my servers and nothing happens to my workloads. I can do maintenance on one of the servers with 0 down time. All of my config for what I have deployed is in git. I manage VMS and kubernetes at work, and im not going to pretend that kubernetes isnt complex, but it's complex up front instead of down the road. VMs run into complexity when things change. I'm sure you can make VMS good but then why not use something like kubernetes, you will have to reinvent a lot of the stuff that's already in kubernetes. It's a hammer for sure and not everything is a nail, but it can be really powerful and useful even for home labs. reply hiAndrewQuinn 13 hours agorootparentI don't run k8s at home, but I have worked in k8s-heavy environments and studied it deeply. This is the accurate, nuanced take. Few but not no people will ever run into problems at the kind of scale k8s operates at. Plus, learning how it \"expects\" the programs running inside its Pods to behave is kind of like learning how Django or Rails \"expect\" a web app to work - it's a more complicated style than just writing your own totally custom, hermetically-sealed Python apps for your personal use, sure, but it also comes with a slew of benefits in case you ever do hit that level of scale and want to move over. Or, maybe you look over the app you're writing and say \"Fat chance.\" In which case you can justify e.g. not making everything an API endpoint, keeping a ton of state mucking about, etc. But I still feel that's an improvement over not even realizing the questions are being asked. reply marcosdumay 1 hour agorootparentprev> But once you have it up you can just keep adding stuff to it. I dunno why, but the k8s in my workplace keeps breaking in painful ways. It also has an endless supply of breaking points that makes life painful for anybody that depends on what runs in it, but aren't detected by the people that manage it. Honestly, that second part is an exclusivity there, but I have never seen people \"just keeping adding things to it\" on practice. reply cyberpunk 12 hours agorootparentprev> K8s is painful to get started Is that really true anymore? Even self hosting k8s these days (e.g with rke/rke2) is a single yaml file and one command to deploy an entire cluster.. Maybe back when we all used kubespray and networking was more complicated (to the user at least) etc.. But today? I don't think so. Using a hosted offering is even easier, literally a couple of clicks, a ./gcloud-cli or terraform apply -- again not very hard and all the cloud providers provide you with example code you just need to plug some machine sizes etc into.. Dev setup? Install orbstack and click 'kubernetes' and you're done, your IDE (likely) will automagically pick up your kubeconfig and you can go right ahead creating services, deployments, jobs, whatevers... reply catdog 14 hours agorootparentprevWhat you also can do is starting with just a single node, incredibly easy to install with e.g. https://k3s.io/. You still have to invest the upfront effort to understand how it works but you can already reap a lot of benefits with a lot less complexity. Kubernetes does not force you into the distributed systems hell, you can go that route later, or never. reply rendaw 13 hours agorootparentKubernetes/k3s on a single node turns what could have been immutable 1-step upgrades into multi-step mutable upgrades, since kubernetes's software itself and all the management components you need are a mutable layer on top of the operating system. reply threeseed 13 hours agorootparenta) It doesn't have to be mutable. You can easily setup k3s on a single node, install the apps and bake an AMI or equivalent. And using something like ArgoCD or GitOps will ensure that your k8s stack is in sync with a tracked and managed Git repository. b) In what world is upgrading your entire platform ever a single step. Even for a basic Python app you still have Python itself plus dependencies. And then of course whatever front end web server you're using. reply angio 11 hours agorootparentprevYou can use Talos linux for an immutable (and tiny) OS. reply dambi0 11 hours agorootparentprevI’m sure there are countless other benefits. But how many layers of abstraction, services and things that need configuring are their compared to basic RAID to get support for magical hard disks that can be yoinked without affecting workloads? reply ElectricalUnion 7 hours agorootparentAren't disks so large those days that losing a disk almost means you will lose a second disk during resilvering unless that by \"basic raid\" you're doing not-basic-raid things such as btrfs raid1c3? reply k8sagainand 11 hours agorootparentprevYou get an aligned infra layer. You get a great opensource ecosystem (k8s, argocd, git / gitops, helm, helm charts, grafana, prometheus etc.) You get basic loadbalancing, health checks, centralized and nearly out of the box logging and monitoring and tracing. You get a streamlined build process (create a container image, have an image build, create your helm chart, done) Your RAID commment is quite far away of what k8s makes k8s reply KeplerBoy 13 hours agorootparentprevAssembling complex systems is just inherently fun as long as you don't have deadlines or performance metrics to hit. It's a bit like factorio with the extra dopamine hit of getting to unbox stuff. reply udev4096 13 hours agorootparentprevIt's because it is complex. And in the long run, things become simpler. The only difficulty is the initial setup and once you are past that, the overall maintenance workload just becomes easier compared to a single VM setup reply chii 13 hours agorootparent> And in the long run, things become simpler. aka, you're front loading the complexity. You can even think of it as paying insurance premiums upfront. You get to \"make a claim\" if the requirements do grow into the sort of need that suit such a cluster/complex setup. reply mianos 12 hours agorootparentBut, on the same insurance theme; I am not sure paying 10K a year to insure my 5K car makes a lot of sense, because, in the long run, I might write my car off. reply GauntletWizard 12 hours agorootparentprevK8s is painful to manage. It's a lot less painful than getting paged in the middle of the night because your server is down - And much much less than realizing that you've been down for an entire day and didn't notice. (K8s isn't even a complete solution to these problems! Just one part of a complete ~balanced breakfast~ production stack) You don't need k8s for all of that, but there's not a simpler solution than k8s that handles as much. Life is full of pain. Deal with it. reply t-writescode 9 hours agoparentprev> I think a lot of people do things the hard way to learn large scale infrastructure Having seen some of these half-rolled, first-time-understood k8s deployments, and the multi-year projects to unravel the mess that was created, overflowing with anti-patterns and other incorrect ways of doing things, I think I would prefer a narrower scope of true experienced professionals (or at least some experienced pros that can help guide the ship for their mentees) working on and designing k8s infra. And for those that don't need it (the vast majority of startups, small businesses, regular-sized businesses, etc), just stick to the easier-to-use paradigms out there. reply travisjungroth 4 hours agoparentprevNubank, the Brazilian bank unicorn, described their approach as “if this works, it’s because we reached massive scale quickly” (paraphrased) and started with an architecture that would support that from the beginning. They were very happy with their choices and have blogged about them in detail. This is a case where “things will be much easier when we scale to a massive number of clients” turned out to be true. reply j45 11 hours agoparentprevResume driven development is worth learning to recognize. reply imiric 12 hours agoprevThis is a retreaded and often tiresome debate. I'll still throw my 2c in... Should you pick a complex framework from day one? Probably not, unless your team has extensive experience with it. My objection is towards the idea that managing infrastructure with a bespoke process and custom tooling will always be less effort to maintain than established tooling. It's the idea of stubbornly rejecting the \"complexity\" bogeyman, even when the process you built yourself is far from simple, and takes a lot of your time from your core product anyway. Everyone loves the simplicity of copying over a binary to a VPS, and restarting a service. But then you want to solve configuration and secret management, have multiple servers for availability/redundancy so then you want gradual deployments, load balancing, rollbacks, etc. You probably also want some staging environment, so need to easily replicate this workflow. Then your team eventually grows and they find that it's impossible to run a prod-like environment locally. And then, and then... You're forced to solve each new requirement with your own special approach, instead of relying on standard solutions others have figured out for you. It eventually gets to a question of sunken cost: do you want to abandon all this custom tooling you know and understand, in favor of \"complexity\" you don't? The difficult thing is that the more you invest in it, the harder it will be to migrate away from it. My suggestion is: start by following practices that will make your transition to the standard tooling later easier. This means deploying with containers from day 1, adopting the 12 factors methodology, etc. And when you do start to struggle with some feature you need, switch to established tooling sooner later than later. You're likely find that your fear of the unknown was unwarranted, and you'll spend less time working on infra in the long run. reply bjornsing 7 hours agoparentThis is a good articulation of the ambivalence I can feel around this. One approach that I’ve considered is to start with the standard tooling (k8s + gitops) from day one, but still run it in a single VM. Any thoughts? reply imiric 56 minutes agorootparentThere's no correct answer here. Your choice seems reasonable _if_ you already have some previous familiarity with managing k8s. If not, you might want to consider starting with a managed k8s solution from a cloud provider. The bulk of the work will be containerizing your stack, and getting familiar with all the concepts. You don't want to do all that while also keeping k8s running. After that you would be able to relatively easily migrate to a self-hosted cluster if you need to. If you do want to self-host, k3s could also be an option, like a sibling comment suggested. It's simpler to start with, though it still has a learning curve since it's a lightweight version of k8s. I reckon that you would still want to run at least 3 nodes for redundancy/failover, and maybe a couple more for just DB workloads. But you can certainly start with one to setup your workflow, and then scale out to more nodes as needed. reply shakiXBT 4 hours agorootparentprevk3s single node + ArgoCD/Flux is what I would if I had to build infrastructure of a small startup by myself. Unfortunately it's HN so people are more likely to do everything in bash scripts and say a big \"fuck you\" to all new hires that would have to learn their custom made mess reply bjornsing 22 minutes agorootparentThis is exactly the setup I’ve been considering. Feels like the best of both worlds: you learn the standard tooling and can easily upgrade to full blown distributed k8s, but you retain the flexibility and low cost aspects of single VM. Also leaning towards putting it behind a Cloudflare tunnel and having managed Postgres for both k3s and application state. Counterpoints anyone? reply roncesvalles 11 hours agoprevI've run a project for 6 years on a single $10/month VPS (I pay even less due to a perpetual discount I bagged from lowendtalk) run by a gameserver-focused VPS provider for about SIX years with about 99.999 reliability if you exclude the one time I fucked up a config and it was down for a whole day because I wanted to do a clean OS reinstall, and one other time when they changed my IP address (they gave me notice). VPS technology has come a very long way and is highly reliable. The disks on the node are set up in RAID 1 and the VM itself can be easily live migrated to another machine for node maintenance. You can take snapshots etc. To me, I would only turn to cloud infra not for greater reliability but more for collaboration and the operational housekeeping features like IAM, secrets management, infra-as-code etc, or for datacenter compliance reasons like HIPAA. reply jillesvangurp 12 hours agoprevIt depends. I personally love cloud based solutions because they save me lots of time. But I'm highly selective in what I use and there are some solutions that are clearly counter productive because they are too complicated. I run a small, bootstrapped startup. We don't have enough money to pay ourselves and I make a living doing consulting on the side. Being budget and time constrained like that I have to be highly selective in what I use. So, I love things like Google cloud. Our GCP bills are very modest. A few hundred euros per month. I would move to a cheaper provider except I can't really justify the time investment. And I do like Google's UI and tools relative to AWS, which I've used in the past. I have no use for Kubernetes. Running an empty cluster would be more expensive than our current monthly GCP bills. And since I avoided falling into the micro-services pitfall, I have no need for it either. But I do love Docker. That makes deploying software stupidly easy. Our website is a Google storage bucket that is served via our load balancer and the Google CDN. The same load balancer routes rest calls to two vms that run our monolith. Which talk to a managed DB and managed Elasticsearch and a managed Redis. The DB and Elasticsearch are expensive. But having those managed saves a lot of time and hassle. That just about sums up everything we have. Nice and simple. And not that expensive. I could move the whole thing to something like Hetzner and cut our bills by 50% or so. Worth doing maybe but not super urgent for me. Losing those managed services would make my life harder. I might have to go back to AWS at some point because some of our customers seem to prefer that. So, there is that as well. reply Animats 14 hours agoprevBut it's so embarrassing if your startup is running on shared hosting, FCGI, Go programs, and MySQL, costing about $10 per month. reply mwcampbell 5 hours agoparentAre there shared hosting providers now that support FastCGI generically, that is, not just for PHP? reply Animats 55 minutes agorootparentDreamhost does. reply groestl 12 hours agoparentprevYou immediately see there's no load ;) reply Animats 11 hours agorootparentThat's not a joke. Go is a fast compiled language, and Go programs are self-contained executables. So you don't need containers. FCGI is an orchestration system, like Kubernetes. It's single-machine, but will start up and shut down processes as the load changes. A crashed process will be restarted. Host the web pages on a static page server, and use client-side Javascript for any dynamic stuff. Good for maybe 20-100 transactions per second. The database will be the bottleneck. Boring, but useful. reply groestl 9 hours agorootparent> 20-100 transactions per second In all seriousness, that is \"no load\". I know it fits 99% of all startups, and many larger companies too, but that's kind of the point. I wouldn't do it differently though, I think it's a perfectly fine architecture :) reply marcosdumay 1 hour agorootparentThat's huge supermarket inventory system top load. Or rather, the lower end of that is huge supermarket inventory system top load. reply t-writescode 9 hours agorootparentprev> > 20-100 transactions per second > \"no load\" Ruby on Rails applications with even a modest amount of ActiveRecord work would like a word xD reply marcosdumay 1 hour agorootparentIf so, you may want to rent more than one server and set multiple web servers with a centralized database. Like people did in the 90s! But that will cost more than $10/month. reply sethammons 7 hours agorootparentprevCouple thousand per second is expected on my Go services (per node) before any optimizations. reply raxxorraxor 8 hours agorootparentprevDepends on the service. For b2b that is already a lot. reply yen223 10 hours agorootparentprevWhen you know how much can be done on a $10 vpc, you'll realise how much compute in a kubernetes cluster is only used to support the cluster reply groestl 10 hours agorootparentDon't worry, I host serious stuff on a single machine, and am quite happy with it ;) What set me off a bit was the shared hosting. You don't want noisy neighbors, usually. That's worth a few bucks. reply PhilipRoman 7 hours agorootparentAgreed, VPS providers often blind users with super low prices, I didn't even notice this until I started hosting game servers where realtime performance is important. Always make sure that \"iostat -c 1\" column \"%steal\" is zero. Luckily there are providers which give guaranteed performance. reply t-writescode 9 hours agorootparentprevHonestly, you'd be surprised just how much load a single server Go application can handle. I've not seen it with Go because I haven't worked with Go in a production capacity; but I've seen C# handle thousands of RPS per node. reply sethammons 6 hours agorootparentProduction Go experience. My go to estimate is per node 1-5k http rps with a couple db calls, maybe a network call to an internal service or three, and serializing json. I use that before building for server count and cost estimates. Some services exceed that, never saw a server we made that couldn't do 1k rps. Friends don't let friends use ruby|python|perl|php|... reply ghomem 10 hours agoparentprevhahahahahah that was funny :-) reply justinclift 10 hours agoparentprevYeah, the MySQL part of that is kind of faux pas these days. Thankfully. ;) reply rirze 2 hours agoprevYah this all sounds good until you realize you have to actually maintain those servers, apply security patches and inevitably run into configuration drift. Like all things, there's a good middle ground here-- use managed services where you can but don't over-architect features like availability & scaling. For example, Kubernetes is an heavy abstraction; make sure it's worth it. A lot of these solutions also increase dev cycles, which is not great early on. reply buglungtung 15 hours agoprevI agree that we are overthinking about infrastructure. Boring stack like traditional RDMS, single server with regular backup, few bash script for deployment is fine for normal startup that targets to non-tech customer. They will serve you well at least one or two years, then you will know what should be improve. One of the big surprise is database like PostgreSQL can handle like 100tps very well with cheap hardware cost. That mean you can handle up to 86 millions transaction per day. reply sussexby 15 hours agoprevI think this goes for any technology group with any stage of company. I work in networking and genuinely of the product I sell, my customers only need a small amount of core functionality and default settings - the rest is “bells and whistles”. But still, no matter what, the odd customer demands they need all these complexities turned on for no discernible reason. IMO it’s a far better approach with any platform to deploy the minimum and turn things on if you need to as you develop. Incidentally, I’ve been exposed to “traditional” cloud platforms (Azure, GCP, AWS) through work and tried a few times to use them for personal projects in recent years and get bewildered by the number of toggles in the interface and strange (to me) paradigms. I recently tried Cloudflare Workers as a test of an idea and was surprised how simple it was. reply justinclift 14 hours agoprev> ... and Docker Swarm was deprecated.. I thought the same thing until recently. Apparently there's a \"Docker Swarm version 2\" around, and it was the original (version 1) Docker Swarm that was deprecated: https://docs.docker.com/engine/swarm/ Do not confuse Docker Swarm mode with Docker Classic Swarm which is no longer actively developed. Haven't personally tried out the version 2 Docker Swarm yet, but it might be worth a look at. :) reply gawa 9 hours agoparentYes, swarm is not deprecated. I haven't used it myself yet, but I read elsewhere that swarm offers an easy way to manage secrets with containers. Some people run their 1 container in a swarm cluster with 1 node just for this feature. I see it's even officially suggested as a Note in the doc: > Docker secrets are only available to swarm services, not to standalone containers. To use this feature, *consider adapting your container to run as a service. Stateful containers can typically run with a scale of 1 without changing the container code.* (Emphasis mine. From https://docs.docker.com/engine/swarm/secrets/ ) reply KronisLV 14 hours agoparentprevI use Swarm with Portainer, it’s quite a nice experience! reply vimto 6 hours agoprevIt's funny because OPs solution was his docker-compose-anywhere, which is exactly what, from my experience, I've seen so many start-ups running with. Sure it works while you're running an MVP but it's incredibly brittle for running something in production as soon as the application grows in complexity. IMO the primary draw of k8s isn't necessarily \"infinite scalability\" but its resilience. I sometimes wonder how many of these post boil down to \"I don't want to learn k8s can I just use this thing I already know?\". reply OutOfHere 13 hours agoprevAfter reading all the comments here, the conclusion is to start simple, then switch to k8s and later to cloud-native only when your business has grown to 1000 and then 1 million daily customers respectively. reply number6 13 hours agoparentWe have B2B-Customers around 700. It all runs on a single Server (not VM though). Since it's B2B we don't need zero downtime, updates at midnight are all right. A day before rollout they go through the staging server and the test environment, so no surprises the next morning. Before updates, the backups kick in, so if we need to recover from a bad update we can roll back. Sounds all 2000 and not very fancy but boring and profitable cuts for us reply k8sagainand 11 hours agorootparentThe question is if you have so much buffer that it doesn't matter or if you could do a lot more but you just don't know. My ci/cd is doing a system test because everything is in containers. I can do full e2e tests and automatic rollouts without a downtime. What i can do, can everyone else do when i'm on holiday. How fast are you back if your server burns down tomorrow? How often have you tested that? Are your devs waiting regularly on things? reply mexicocitinluez 6 hours agoparentprevIf someone thinks rolling their own infrastructure is \"starting simple\" than I have some land in Antartica my great, great uncle is trying to get rid of they might be interested in. reply beaviskhan 2 hours agoprevProbably not - but by calling out EC2 instances as the way and then failing to mention patching or configuration management, this article loses some credibility for me. These considerations are not optional over any significant length of time, and will cause misery if not planned for. Bare minimum, script out the install of your product on a fresh EC2 instance from a stock (and up-to-date) base image, and use that for every new deploy. reply leetrout 2 hours agoparentI strong agree this is the way. We run Spacelift workers with Auto Scaling Groups and pick up their new image ~monthly with zero hassle since everything is automated. Raw EC2 is just part of the story... Edit to add: I also recommend using Amazon Linux unless you _have_ to have RHEL / Cent / Rocky or Ubuntu. Just lean into the ecosystem and you can get so many great features (and yes, I ACK the vendor lock-in with this advice). A really cool feature is the ability to just flip on various AWS services like the systems manager session manager and get SSH without opening ports a-la wireguard. reply pnathan 14 hours agoprevif you take the time to understand k8s and have a straightforward k8s deployment, these things aren't really a problem - and you don't have to do the custom sysadmin timesinks that need to go into the \"simple\" suggestion. What is suggested here is \"easy\". But it is not simple: it proliferates custom work. I have had great success with a very simple kube deployment: - GKE (EKS works well but requires adding an autoscaler tool) - Grafana + Loki + Prometheus for logs + metrics - cert-manager for SSL - nginx-ingress for routing - external-dns for autosetup DNS I manage these with helm. I might, one day, get around to using the Prometheus Operator thing, but it doesn't seem to do anything for me except add a layer of hassle. New deployments of my software roll out nicely. If I need to scale, cut a branch for testing, I roll into a new namespace easily, with TLS autosetup, DNS autosetup, logging to GCP bucket... no problem. I've done the \"roll out an easy node and run\" thing before, and I regret it, badly, because the back half of the project was wrangling all these stupid little operational things that are a helm install away on k8s. So if you're doing a startup: roll out a nice simple k8s deployment, don't muck it up with controllers, operators, service meshes, auto cicds, gitops, etc. *KISS*. If you're trying to spin a number of small products: just use the same cluster with different DNS. (note: if this seems particularly appealing to you, reach out, I'm happy to talk. This is a very straightforward toolset that has lasted me years and years, and I don't anticipate having to change it much for a while) reply catdog 14 hours agoparent> I manage these with helm. I might, one day, get around to using the Prometheus Operator thing, but it doesn't seem to do anything for me except add a layer of hassle. One big advantage of the operator is that its custom resources are practically kind of standard by now. This means helm charts for a lot of software ship those and integrating that piece of software into your monitoring is a matter of setting a few flags to true. The go to solution for a k8s monitoring setup is https://github.com/prometheus-community/helm-charts/tree/mai... reply pnathan 14 hours agorootparentyeah, I know, that's the only reason I'm even thinking of using it. but tbqh I don't really install many things, as you can see... reply ramraj07 12 hours agoparentprevI just hosted a site on Elastic beanstalk. Didn’t need to really do anything honestly. Upload a zip file with python code that runs locally really well. Database is on RDS. It has and continues to work well for 5+ years and lots of productivity. reply minkles 2 hours agoparentprevLOL we have 2 full time people managing the production monitoring stack. And it costs money. And it generates a lot of internal traffic. Nope! rsyslog + knowing what the fuck you are doing is much better. reply cynicalsecurity 4 hours agoparentprevWhat product needs autoscaling? reply maccard 3 hours agoprevFor all the people who are saying you don’t need X and Y - what is the simplest way to deploy a web app using TLS on a VPS/VM? Let’s say I’ve got a golang binary locally on my machine, or as an output of github actions. With Google Cloud Run/Fargate/DigitalOcean I can click about 5 buttons, push a docker image and I’m done, with auto updates, roll backs, logging access from my phone, all straight out of the box, for about $30/mo. My understanding with Hetzner and co is that I need to SSH (now i need to keep ssh keys secure and manage access to them) in for updates, logs, etc. I need to handle draining connections from the old app to the new one. I need to either manage https in my app, or run behind a reverse proxy that does tls termination, which I need to manage the ssl certs for myself. This is all stuff that gets in the way of the fact that I just want to write my services and be done with it. Azure will literally install a GitHub actions workflow that will autodeploy to azure container apps for you, with scoped credentials. reply zipy124 3 hours agoparent> For all the people who are saying you don’t need X and Y - what is the simplest way to deploy a web app using TLS on a VPS/VM? Depends on your defintion of simplest. In terms of set-up probably someting like https://dokku.com/ . It's a simple self-hosted version of herokku, you can be up and running in literally minutes and because its compatable with herokku you can re-use lots of github action/ other build scripts. In terms of simple (low complexity and small sized components) just install caddy as your reverse-proxy which will do ssl certs and reverse proxy for you with extremely little, if any config. Then just have your github action push your containers there using whatever container set-up you prefer. This is usually a simple script on your build process like \"build container -> push container to registry -> tell machine to get new image and run it\" or even simpler just have your server check for updated images routinely if you don't want to handle communication between build script and server. That's the bare minimum needed. This takes a bit longer than a few minutes but you can still be done within an hour or two. Regardless of your choice it shouldn't take more than 1 working day, and will save you a lot of money compared to the big cloud providers. You can run as low as €4.51/month with hetzner and that includes a static IP and basically unlimited traffic. An EC2 instance with the same hardware costs about $23 a month for comparison (yes shared vs dedicated vCPU, but even the dedicated offer at hetzner is cheaper, and this is compared to a serverless set-up where loads are spikey, which is exactly how we can benefit from a shared vCPU situation). reply bruh2 3 hours agoparentprevRe: securing SSH keys; Nowadays most password managers can store SSH keys and integrate nicely with your SSH agent, making it essentially equivalent to logging in with a password. I use KeepassXC[1], and the workflow consists of opening the database using my master password, then just `ssh machine`, so in my book it's at the same level of comfort as a web interface for your cloud provider [1] https://keepassxc.org/docs/KeePassXC_UserGuide#_setting_up_s... reply tryauuum 3 hours agoparentprevTrue, I see the allure of not thinking about draining connections. But I also enjoy having full access to the container and I don't really need scaling up and down features If you don't like ssh you can have a gitlab runner on your VM which will redeploy your stuff on git push / git tag / whatever you want reply j-krieger 3 hours agoparentprevYou can pretty easily self host a GitLab instance, host a kubernetes runner for your images and use Tailscale for SSH keys. This will most certainly cost you more than $30, but you can do it. reply jumploops 13 hours agoprevSimple is robust. Focus on product market fit (PMF) and keep things as straightforward as possible. Create a monolith, duplicate code, use a single RDBMS, adopt proven tech instead of the “hot new framework”, etc. The more simple the code, the easier it is to migrate/scale later on. Unnecessary complexity is the epitome of solving a problem that doesn’t exist. reply dgan 8 hours agoparentCan you expand om what of code duplication you deem reasonable? reply jumploops 2 hours agorootparentEarly in a project you see a lot of similar code paths, and so it’s often tempting to take the logic from two or three e.g. API routes and merge the “clean” abstraction into single piece of logic both routes can call. Over-time this “clean” abstraction adopts a bunch of optional parameters based on the upstream API routes, leaving you with an omni-function that is more convoluted, and thus harder to change, than if the API routes weren’t overly optimized from the get-go. As a personal rule, I’ll let myself copy something 3 times before taking a step back and figuring out a “better” way. reply ghomem 10 hours agoparentprevMore of this. reply kaptainscarlet 12 hours agoparentprevYeah, I would focus on a better user experience over a beautiful backend architecture. reply ghomem 10 hours agorootparent... and this! reply jedberg 13 hours agoprevIf you'll allow me, I'd like to shill my company for a second. We provide all the benefits of \"single server deployment\" while providing the scalability of the \"30 lambdas\" solution. You can even run the whole thing locally. We actually just did a Show HN about it: https://news.ycombinator.com/item?id=41502094 reply hjaveed 16 hours agoprevAfter listening to @levelsio on Lex Friedman’s podcast, I became obsessed to simplify my deployments: Do startups really need complex cloud architecture? Inspired, I wrote a blog exploring simpler approaches and created a docker-compose template for deployment Curious to know your thoughts on how you manage your infrastructure. How do you simplify it? How do you balance? reply seper8 9 hours agoparentFunny I drew the same conclusion. Previously a cloud architect at Microsoft, now I don't use Azure anymore for the project I am working on right now. Rather, I have decided to opt for Supabase instead. Probably over the long time it may cause issues for my startup - but even more realistically my startup is going to fail and my increased developer velocity by using simple tooling like this will allow me to figure out why my idea doesn't work in in a shorter amount of time, so I can go on to my next pursuit. To be honest I think even using docker is overengineering. reply KronisLV 15 hours agoparentprev> Curious to know your thoughts on how you manage your infrastructure. What I quite like about your repo: - there is a separate API and background job instance - there is a separate web image, to not always couple front end deployments to back end - there are specialized data stores like Redis (or maybe RabbitMQ or MinIO in a different type of project) - Dozzle seems nice https://dozzle.dev/ (I use Portainer mostly, but seems useful) What I think works quite nicely in general: - starting out with a monolithic back end but making it modular with feature flags (e.g. FEATURE_REPORTS, FEATURE_EMAILS, FEATURE_API), so that you can deploy vastly different types of workloads in separate containers BUT not duplicate your data model and don't need to extract shared code libraries (yet) and if you ever need to split the codebase into multiple separate ones, then it won't be *too* hard to do that - having a clear API (RESTful or otherwise) as the contract between a separate back end and front end deployment, so that even if your SPA technology gets deprecated (AngularJS, anyone?) then you can migrate to something, unlike when doing SSR and everything being coupled - the same applies to NOT having the same container build process have both the front end and back end build (I've seen a Java project install a specific Node version through Maven and then the build dragging on cause Maven ends up processing thousands of files as a part of the build) - using the right tool for the job: many might create full text search, key-value storage, message queues, JSON document storage, even blob storage all with PostgreSQL and that might be okay; others will go for separate instances of ElasticSearch, Redis, RabbitMQ, something S3 compatible and so on, probably a tradeoff between using well known libraries and tools vs building everything yourself against a single DB instance - in my experience, many projects out there are served perfectly fine by a single server so Docker Compose feels like the logical tool to start out with, if multiple instances indeed become necessary, there is always Docker Swarm (yes, still works, very simple), Hashicorp Nomad or K3s or one of the other more manageable Kubernetes distros - self-hosted (or self-hostable) software in general is pretty cool and gives you a bunch of freedom, though using managed cloud services will also be pleasant for many, more expensive upfront but less so in regards to your own time spent managing the stack; the former also lends itself nicely to being able to launch a local dev environment with the full stack, which feels like a superpower (being able to really test out breaking migrations, look at what happens with the whole stack etc.) - having some APM and tracing is nice, something like Apache Skywalking was pretty simple to setup, though there are more advanced options out there (e.g. cloud version of Sentry, because good luck running that locally) - having some uptime monitoring is also very nice, something like Uptime Kuma is just very pleasant to use - heck, if you really wanted to, you could even self-host a mail server: https://github.com/docker-mailserver/docker-mailserver (though that can be viewed as a hobbyist thing), or have MailCatcher / Inbucket or something for development locally reply 0x63_Problems 19 minutes agorootparentI'm a big fan of the modular monolith pattern, but haven't used feature flags for the purpose you're describing. Do you use any specific tools or frameworks for that? I'd also imagine there would be calls between features from within the same codebase, do those become network calls? And how does this interact with your Docker Compose/single server recommendation? reply taw1285 12 hours agoprev> 20-30 Lambda functions for different services My team of 6 engineers have a social app at around 1,000 DAU. The previous stack has several machines serving APIs and several machines handling different background tasks. Our tech lead is forcing everyone to move to separate Lambdas using CDK to handle each each of these tasks. The debugging, deployment, and architecting shared stacks for Lambdas is taking a toll on me -- all in the name of separation of concerns. How (or should) I push back on this? reply langsoul-com 11 hours agoparentDoes the tech lead have the CTO or CEO's graces for that decision? Why did the tech lead decide to move everything to lambda when you only have 1k DAU? Can they be reasoned with or is it lambda or the highway? You can pull put the stats and do comparison, note the wasted time, how it's not ben",
    "originSummary": [
      "Pieter Levels advocates for simpler infrastructure, using single servers instead of complex cloud setups, to focus on product-market fit, as discussed on the Lex Friedman Podcast.",
      "Two case studies highlight the pitfalls of overcomplicated setups: one with excessive Lambda functions and another with unnecessary microservices, both detracting from feature development.",
      "Modern servers and tools like Docker Compose can provide powerful, manageable, and budget-friendly solutions, enabling small teams to focus on building great products rather than managing complex infrastructure."
    ],
    "commentSummary": [
      "Startups often adopt complex cloud infrastructure like Kubernetes for scalability, but this can lead to poor quality and high costs due to immature team decisions.",
      "Some experienced professionals argue that simpler, more reproducible setups using tools like Puppet and LTS (Long-Term Support) systems can be more efficient and cost-effective.",
      "The debate highlights the trade-offs between modern cloud-native approaches and traditional, deterministic methods for managing infrastructure."
    ],
    "points": 239,
    "commentCount": 305,
    "retryCount": 0,
    "time": 1726194591
  },
  {
    "id": 41530783,
    "title": "Porting SBCL to the Nintendo Switch",
    "originLink": "https://reader.tymoon.eu/article/437",
    "originBody": "Porting SBCL to the Nintendo Switch 2024.09.13 09:03:33 common lisp Index For the past two years Charles Zhang and I have been working on getting my game engine, Trial, running on the Nintendo Switch. The primary challenge in doing this is porting the underlying Common Lisp runtime to work on this platform. We knew going into this that it was going to be hard, but it has proven to be quite a bit more tricky than expected. I'd like to outline some of the challenges of the platform here for posterity, though please also understand that due to Nintendo's NDA I can't go into too much detail. Current Status I want to start off with where we are at, at the time of writing this article. We managed to port the runtime and compiler to the point where we can compile and execute arbitrary lisp code directly on the Switch. We can also interface with shared libraries, and I've ported a variety of operating system portability libraries that Trial needs to work on the Switch as well. The above photo shows Trial's REPL example running on the Switch devkit. Trial is setting up the OpenGL context, managing input, allocating shaders, all that good stuff, to get the text shown on screen; the Switch does not offer a terminal of its own. Unfortunately it also crashes shortly after as SBCL is trying to engage its garbage collector. The Switch has some unique constraints in that regard that we haven't managed to work around quite yet. We also can't output any audio yet, since the C callback mechanism is also broken. And of course, there's potentially a lot of other issues yet to rear their head, especially with regards to performance. Whatever the case, we've gotten pretty far! This work hasn't been free, however. While I'm fine not paying myself a fair salary, I can't in good conscience have Charles invest so much of his valuable time into this for nothing. So I've been paying him on a monthly basis for all the work he's been doing on this port. Up until now that has cost me ~17'000 USD. As you may or may not know, I'm self-employed. All of my income stems from sales of Kandria and donations from generous supporters on Patreon, GitHub, and Ko-Fi. On a good month this totals about 1'200 USD. On a bad month this totals to about 600 USD. That would be hard to get by in a cheap country, and it's practically impossible in Zürich, Switzerland. I manage to get by by living with my parents and being relatively frugal with my own personal expenses. Everything I actually earn and more goes back into hiring people like Charles to do cool stuff. Now, I'm ostensibly a game developer by trade, and I am working on a currently unannounced project. Games are very expensive to produce, and I do not have enough reserves to bankroll it anymore. As such, it has become very difficult to decide what to spend my limited resources on, and especially a project like this is much more likely to be axed given that I doubt Kandria sales on the Switch would even recoup the porting costs. To get to the point: if you think this is a cool project and you would like to help us make the last few hurdles for it to be completed, please consider supporting me on Patreon, GitHub, or Ko-Fi. On Patreon you get news for every new library I release (usually at least one a month) and an exclusive monthly roundup of the current development progress of the unannounced game. Thanks! An Overview First, here's what's publicly known about the Switch's environment: user code runs on an ARM64 Cortex-A57 chip with four cores and 4 GB RAM, and on top of a proprietary microkernel operating system that was initially developed for the Nintendo 3Ds. SBCL already has an ARM64 Linux port, so the code generation side is already solved. Kandria also easily fits into 4GB RAM, so there's no issues there either. The difficulties in the port reside entirely in interfacing with the surrounding proprietary operating system of the switch. The system has some constraints that usual PC operating systems do not have, which are especially problematic for something like Lisp as you'll see in the next section. Fortunately for us, and this is the reason I even considered a port in the first place, the Switch is also the only console to support the OpenGL graphics library for rendering, which Trial is based upon. Porting Trial itself to another graphics library would be a gigantic effort that I don't intend on undertaking any time soon. The Xbox only supports DirectX, though supposedly there's an OpenGL -> DirectX layer that Microsoft developed, so that might be possible. The Playstation on the other hand apparently still sports a completely proprietary graphics API, so I don't even want to think about porting to that platform. Anyway, in order to get started developing I had to first get access. I was lucky enough that Nintendo of Europe is fairly accommodating to indies and did grant my request. I then had to buy a devkit, which costs somewhere around 400 USD. The devkit and its SDK only run on Windows, which isn't surprising, but will also be a relevant headache later. Before we can get on to the difficulties in building SBCL for the Switch, let's first take a look at how SBCL is normally built on a PC. Building SBCL SBCL is primarily written in Lisp itself. There is a small C runtime as well, which you use a usual C compiler to compile, but before it can do that, there's some things it needs to know about the operating system environment it compiles for. The runtime also doesn't have a compiler of its own, so it can't compile any Lisp code. In order to get the whole process kicked off, SBCL requires another Lisp implementation to bootstrap with, ideally another version of itself. The build then proceeds in roughly five phases: build-config This step just gathers whatever build configuration options you want for your target and spits them out into a readable format for the rest of the build process. make-host-1 Now we build the cross-compiler with the host Lisp compiler, and at the same time emit C header files describing Lisp object layouts in memory as C structs for the next step. make-target-1 Next we run the target C compiler to create the C runtime. As mentioned, this uses a standard C compiler, which can itself be a cross-compiler. The C runtime includes the garbage collector and other glue to the operating system environment. This step also produces some constants the target Lisp compiler and runtime needs to know about by using the C compiler to read out relevant operating system headers. make-host-2 With the target runtime built, we build the target Lisp system (compiler and the standard library) using the Lisp cross-compiler built by the Lisp host compiler in make-host-1. This step produces a \"cold core\" that the runtime can jump into, and can be done purely on the host machine. This cold core is not complete, and needs to be executed on the target machine with the target runtime to finish bootstrapping, notably to initialize the object system, which requires runtime compilation. This is done in make-target-2 The cold core produced in the last step is loaded into the target runtime, and finishes the bootstrapping procedure to compile and load the rest of the Lisp system. After the Lisp system is loaded into memory, the memory is dumped out into a \"warm core\", which can be loaded back into memory in a new process with the target runtime. From this point on, you can load new code and dump new images at will. Notable here is the need to run Lisp code on the target machine itself. We can't cross-compile \"purely\" on the host, not in the least because user Lisp code cannot be compiled without also being run like batch-compiled C code can, and when it is run it assumes that it is in the target environment. So we really don't have much of a choice in the matter. In order to deploy an application, we proceed similar to make-target-2: We compile in Lisp code incrementally and then when we have everything we need we dump out a core with the runtime attached to it. This results in a single binary with a data blob attached. When the SBCL runtime starts up it looks for a core blob, maps it into memory, marks pages with code in them as executable, and then jumps to the entry function the user designated. This all is a problem for the Switch. Building for the Switch The Switch is not a PC environment. It doesn't have a shell, command line, or compiler suite on it to run the build as we usually do. Worse still, its operating system does not allow you to create executable pages, so even if we could run the compilation steps on there we couldn't incrementally compile anything on it like we usually do for Lisp code. But all is not lost. Most of the code is not platform dependent and can simply be compiled for ARM64 as usual. All we need to do is make sure that anything that touches the surrounding environment in some way knows that we're actually trying to compile for the Switch, then we can use another ARM64 environment like Linux to create our implementation. With that in mind, here's what our steps look like: build-config We run this on some host system, using a special flag to indicate that we're building for the Switch. We also enable the fasteval contrib. We need fasteval to step in for any place where we would usually invoke the compiler at runtime, since we absolutely cannot do that on the Switch. make-host-1 This step doesn't change. We just get different headers that prep for the Switch platform. make-target-1 Now we use the C compiler the Nintendo SDK provides for us, which can cross-compile for the Switch. Unfortunately the OS is not POSIX compliant, so we had to create a custom runtime target in SBCL that stubs out and papers over the operating system environment differences that we care about, like dynamic linking, mapping pages, and so on. Here is where things get a bit weird. We are now moving on to compiling Lisp code, and we want to do so on a Linux host system. So we have to... build-config (2) We now create a normal ARM64 Linux system with the same feature set as for the Switch. This involves the usual steps as before, though with a special flag to inform some parts of the Lisp process that we're going to ultimately target the Switch. make-host-1 (2) make-target-1 (2) make-host-2 make-target-2 With all of this done we now have a slightly special SBCL build for Linux ARM64. We can now move on to compiling user code. For user code we now perform some tricks to make it think it's running on the Switch, rather than on Linux. In particular we modify *features* to include :nx (the Switch code name) and not :linux, :unix, or :posix. Once that is set up and ASDF has been neutered, we can compile our program (like Trial) \"as usual\" and at the end dump out a new core. We've solved the problem of actually compiling the code, but we still need to figure out how to get the code started on the Switch, since it does not allow us to do the usual core-mapping strategy. As such, attaching the new core to the runtime we made for the Switch won't work. To make this work, we make use of two relatively unknown features of SBCL: immobile-code, and elfination. Usually when SBCL compiles code at runtime, it sticks it into a page somewhere, and marks that page executable. The code itself however could become unneeded at some point, at which point we'd like to garbage collect it. We can then reclaim the space it took up, and to do so compact the rest of the code around it. The immobile-code feature allows SBCL to take up a different strategy, where code is put into special reserved code pages and remains there. This means it can't be garbage collected, but it instead can take advantage of more traditional operating system support. Typically executables have pre-marked sections that the operating system knows to contain code, so it can take care of the mapping when the program is started, rather than the program doing it on its own like SBCL usually does. OK, so we can generate code and prevent it from being moved. But we still have a core at the end of our build that we now need to transform into the separate code and data sections needed for a typical executable. This is done with the elfination step. The elfinator looks at a core and performs assembly rewriting to make the code position-independent (a requirement for Address Space Layout Randomisation), and then tears it out into two separate files, a pure code assembly file, and a pure data payload file. We can now take those two files and link them together with the runtime that the C compiler produced and get a completed SBCL that runs like any other executable would. So here's the last steps of the build process: Run the elfinator to generate the assembly files Link the final binary Run the Nintendo SDK's authoring tools to bundle metadata, shared libraries, assets, and the application binary into one final package That's quite an involved build setup. Not to mention that we need at least an ARM64 Linux machine to run most of the build on, as well as either an AMD64 Windows machine (or an AMD64 Linux machine with Wine) to run the Nintendo SDK compiler and authoring tools. I usually use an AMD64 Linux machine, so there's a total of three machines involved: The AMD64 \"driver,\" the ARM64 build host, and a Windows VM to talk to the devkit with. I wrote a special build system with all sorts of messed up caching and cross-machine synchronisation logic to automate all of this, which was quite a bit of work to get going, especially since the build should also be drivable from an MSYS2/Windows setup. Lots of fun with path mangling! So now we have a full Lisp system, including user code, compiling for and being able to run on the Switch. Wow! I've skipped over a lot of the nitty-gritty dealing with getting the build properly aware of which target it's building for, making the elfinator and immobile-code working on ARM64, and porting all of the support libraries like pathname-utils, libmixed, cl-gamepad, etc. Again, most of the details we can't openly talk about due to the NDA. However, we have upstreamed what work we could, and all of the Lisp libraries don't have a private fork. It's worth noting though that elfination wasn't initially designed to produce position independent executable Lisp code, which is usually full of absolute pointers. So we needed to do a lot of work in the SBCL compiler and runtime to support load time relocation of absolute pointers and make sure code objects (which usually contain code constants) no longer have absolute pointers, as the GC can't modify executable sections. Not even the OS loader is allowed to modify executable sections to relocate absolute pointer. We did this by relocating absolute pointers like code constants outside of the text space into a read-writable space close enough to rewrite constant references in code to load from this r/w space instead, which the loader and the moving GC can fixup pointers at. Instead of interfacing directly with the Nintendo SDK, I've opted to create my own C libraries that have a custom interface the Lisp libraries interface with in order to access the operating system functionality it needs. That way I can at least publish the Lisp bits openly, and only keep the small C library private. Anyway, now that we can run stuff we're not done yet. Our system actually needs to keep running, too, and that brings us to The Garbage Collector Garbage collection is a huge topic in itself and there's a ton of different techniques to make it work efficiently. The standard GC for SBCL is called \"gencgc\", a Generational Garbage Collector. Generational meaning it keeps separate \"generations\" of objects and scans the generations in different frequencies, copying them over to another generation's location to compact the space. None of this is inherently an issue for the Switch, if it weren't for multithreading. When multiple threads are involved, we can't just move objects around, as another thread could be accessing it at any time. The easiest way to resolve this conflict is to park all threads before engaging garbage collection. So the question becomes: when a thread wants to start garbage collection, how does it get the other threads to park? On Unix systems a pretty handy trick is used: we can use the signalling mechanism to send a signal to the other threads, which then take that hint to park. On the Switch we don't have any signal mechanism. In fact, we can't interrupt threads at all. So we instead need to somehow get each thread to figure out that it should park on its own. The typical strategy for this is called \"safepoints\". Essentially we modify the compiler a little bit to inject some extra code that checks whether the thread should park or not. This strategy has some issues, namely: Adding a check isn't free. So we want to check as little as possible If we don't check frequently enough, we are going to stall all the other threads because GC can't begin until they're all parked If we have to inject a lot of instructions for a check, it is going to disrupt CPU cache lines and pipelining optimisations The current safepoint system in SBCL was written for Windows, which similarly does not have inter-process signal handlers. However, unlike the Switch, it does still have signal handling for the current thread. So the current safepoint implementation was written with this strategy: Each thread keeps a page around that a safepoint just writes a word to. When GC is engaged, those pages are marked as read-only, so that when the safepoint is hit and the other thread tries to write to the page, a segmentation fault is triggered and the thread can park. This is efficient, since we only need a single instruction to write into the page. On the Switch we can't use this trick either, so we have to actually insert a more complex check, which can be tricky to get working as intended, as all parallel algorithms tend to be. Since safepoints aren't necessary on any other platform than Windows, it also hasn't been tested anywhere else, so aside from modifying it for this new platform it's also just unstable. It is apparently quite a big mess in the code base and would ideally be redone from scratch, but hopefully we don't have to go quite that far. I'd also like to give special mention to the issue that CLOS presents. Usually SBCL defers compilation of the \"discriminating function\" that is needed to dispatch to methods to the first call of the generic function. This is done because CLOS is highly dynamic and allows adding and removing methods pretty much at any time, and there's usually no good point in time that the system knows it is complete. Of course, on the Switch we can't invoke the compiler, so we can't really do this. For now our strategy has been to instead rely on the fast evaluator. We stub out the compile function to create a lambda that executes the code via the evaluator instead. This has the advantage of working with any user code that relies on compile as well, though it is obviously much slower for execution than it would be if we could actually compile. This neatly brings us to Future Work The fasteval trick is mostly a fallback. Ideally I'd like to explore options to freeze as much of CLOS in place as possible right before the final image is dumped and compile as much as possible ahead of time. I'd also like to investigate the block compilation mode that Charles restored some years back more closely. It's very possible that the Switch's underpowered processor will also force us to implement further optimisations, especially on the side of my engine and the code in Kandria itself. Up until now I've been able to get away with comparatively little optimisation, since even computers of ten years ago are more than fast enough to run what I need for the game. However, I'm not so sure that the Switch could match up to that even if it didn't also introduce additional constraints on performance with its lack of operating system support. First, though, we need to get the garbage collector running fully. It runs enough to boot up and get into Trial's main loop, but as soon as it hits multi-generation compaction, it falls flat on its face. Next we need to get callbacks from C working again. Apparently this is a part of the SBCL codebase that can only be described as \"a mess,\" involving lots of hand-rolled assembly routines, which probably need some adjustments to work correctly with immobile-code and elfination. Callbacks fortunately are relatively rare, Trial only needs them for sound playback via libmixed. There's also been some other issues that we've kept in the back of our heads but don't require our immediate attention, as well as some extra portability features I know I'll have to work on in Trial before its selftest suite fully passes on the Switch. Conclusion I'll be sure to add an addendum here should the state of the port significantly change in the future. Some people have also asked me if the work could be made public, or if I'd be willing to share it. The answer to that is that while I would desperately like to share it all publicly, the NDA prevents us from doing so. We still upstream and publicise whatever we can, but some bits that tie directly into the Nintendo SDK cannot be shared with anyone that hasn't also signed the NDA. So, in the very remote possibility that someone other than me is crazy enough to want to publish a Common Lisp game on the Nintendo Switch, they can reach out to me and I'll happily give them access to our porting work once the NDA has been signed. Naturally, I'll also keep people updated more closely on what's going on in the monthly updates for Patrons. With that all said, I once again plead with you to consider supporting me on Patreon, GitHub, or Ko-Fi. All the income from these will, for the foreseeable future, be going towards funding the SBCL port to the Switch as well as the current game project. Thank you as always for reading, and I hope to share more exciting news with you soon! Written by shinmera SRS, Zürich 2024 Edition",
    "commentLink": "https://news.ycombinator.com/item?id=41530783",
    "commentBody": "Porting SBCL to the Nintendo Switch (tymoon.eu)234 points by todsacerdoti 6 hours agohidepastfavorite40 comments colingw 5 hours agoI have being using Trial[1] for the past few weeks to test out game development in Common Lisp, and have been having a great time. Being able to alter (almost) all aspects of your game while it's running is a blessing. I hope this port succeeds. [1]: https://github.com/Shirakumo/trial reply sinker 3 hours agoparentLisp languages seem well-suited for building games. The ability to evaluate code interactively without recompilation is a huge deal for feature building, incremental development, and bug-fixing. Retaining application state between code changes seems like it would be incredibly useful. Common Lisp also appears to be a much faster language than I would have blindly assumed. The main downside for me (in general, not just for game programming) is the clunkiness in using data structures - maps especially. But the tradeoff seems worth it. reply lispm 2 hours agorootparentOne of the downsides is that implementations like SBCL have a deep integration and need things like a well performing GC implementation - to get this running on specialized game hardware is challenging. The article describes that. Getting over the hurdle of the low-level integration is difficult. The reward comes, when one gets to the point, where the rapid incremental development cycles of Common Lisp, even with connected devices, kicks in. For the old historic Naughty Dog use case, it was a development system written in Common Lisp on an SGI and a C++ runtime with low-level Scheme code on the Playstation. > Common Lisp also appears to be a much faster language than I would have blindly assumed. There are two modes: 1) fast optimized code which allows for some low-level stuff to stay with Common Lisp 2) unoptimized, but natively compiled code, which enables safe (-> the runtime does not crash) interactive and incremental development -> this mode is where much of the software can run nowadays and which is still \"fast enough\" for many use cases reply koito17 1 hour agorootparentprev> The ability to evaluate code interactively without recompilation SBCL and other implementations compile code to machine code then execute it. That is to say, when a form is submitted to the REPL, the form is not interpreted, but first compiled then executed. The reason execution finishes quickly is because compilation finishes quickly. There are some implementations, like CCL, with a special interpreter mode exclusively for REPL-usage.[1] However, at least SBCL and ECL will compile code, not interpret. [1] https://github.com/Clozure/ccl/blob/v1.13/level-1/l1-readloo... reply Shinmera 1 hour agorootparentI specifically talk about the fast evaluator for SBCL. But even without that contrib, SBCL does have another evaluator as well that's used in very specific circumstances. reply pjmlp 2 hours agorootparentprevThere are 1980's papers about Lisp compilers competing with Fortran compilers, unfortunately with the AI Winter, and the high costs of such systems, people lost sight of it. reply fouric 4 hours agoprevThis is super neat - SBCL is an awesome language implementation, and I've always wanted to do CL development for a \"real\" game console. I'm also surprised (in a good way) that Shinmera is working on this - I've seen him a few times before on #lispgames and in the Lisp Discord, and I didn't know that he was into this kind of low-level development. I've looked at the guts of SBCL briefly and was frightened away, so kudos to him. I wonder if SBCL (+ threading/SDL2) works on the Raspberry Pi now... reply Shinmera 3 hours agoparentI'm not doing the SBCL parts, that's all Charles' work that I hired him for. My work is the portability bits that Trial relies on to do whatever and the general build architecture for this, along with the initial runtime stubbing. And, as mentioned, *her :) reply cellularmitosis 3 hours agorootparentHoly cow, Kandria looks amazing. Is it also developed using Trial? https://www.youtube.com/watch?v=usc0Znm-gbA reply Shinmera 2 hours agorootparentYes, it's also open source: https://github.com/shirakumo/kandria My current unannounced project is a lot more ambitious still, being a 3D hack & slash action game. I post updates about that on the Patreon if you're interested. reply f1shy 4 hours agoparentprev- Is it not her? reply anisoco 2 hours agorootparentIt is https://reader.tymoon.eu/article/436 reply f1shy 1 hour agorootparentOh I did not know that she transitioned. I just remebered it was the author of portacle, that was a woman. What a world of pain must be to have to go through it. Kudos to her. Also what she does for lisp is amazing. reply Retr0id 3 hours agoprev> The answer to that is that while I would desperately like to share it all publicly, the NDA prevents us from doing so. I'm curious what the rationale here was for using the official SDK, rather than the unencumbered \"homebrew\" ones[0]. As a complete guess, maybe Nintendo doesn't let you officially publish games built using 3rd party SDKs? [0] https://switchbrew.org/wiki/Setting_up_Development_Environme... reply Shinmera 3 hours agoparentYou cannot publish games with homebrew, it has to use the official SDK. Besides that, almost nobody has a jailbroken Switch, so it would make it extremely hard to play any games on anything but an emulator. reply Retr0id 3 hours agorootparent> You cannot publish games with homebrew, it has to use the official SDK This doesn't surprise me much, but does Nintendo state this explicitly anywhere public? If Nintendo chose to sign an application developed using a 3rd party toolchain, there's no technical reason why it couldn't run on retail consoles. reply Arelius 2 hours agorootparentYeah, maybe. But also the official SDKs are pretty good and you get support from Nintendo. It seems like a pretty big risk to use an unsupported SDK... for what benefit? reply Retr0id 2 hours agorootparentI don't have access to Nintendo's SDK so I can't compare directly, but the article cites an inability to map executable pages. libnx supports this (but of course, this is moot if Nintendo wouldn't let you ship it). But the main benefit is being able to talk about and share your work without worrying about violating an NDA. https://switchbrew.github.io/libnx/jit_8h.html https://switchbrew.org/wiki/JIT_services reply Shinmera 2 hours agorootparentThe OS can do it, and some Nintendo titles on the Switch do use this capability, but I have talked to Nintendo directly about using it, and it's a hard No. I can't even use the JIT feature purely for dev. reply nightpool 35 minutes agorootparentReally? What is the justification for allowing Nintendo titles to use it but not third parties? Security concerns? reply Shinmera 33 minutes agorootparentThat's what they claim, but ultimately it's their thing, so they do with it whatever they like. reply spsesk117 4 hours agoprevThanks to the author for the fascinating and detailed write up. It feels like a lot of the time this level of detail around the specifics of 'blessed' (not homebrew) console porting are only revealed years after the end of the consoles lifetime. As an aside, reading about this kind of deeply interesting work always makes me envious when I think about the rote software I spend all day writing :) reply whizzter 4 hours agoparentAt least back when I was working these \"blessed\" tools were usually a tad hacked together, modern homebrew toolchains for many older platforms are better except for debugging support (since the devkits for the machines usually had better hooks available but also avoiding the entire GDB focus). Having been in both worlds, i'm not entirely sure there's that much to be envious of. reply cschep 4 hours agoparentprevwell said! as I was just sitting down to another day of ruby on rails (that I am grateful for!) I was thinking.. I wonder what hobby/open source projects could use some of my attention later.. .. what projects my attention could use later .. :D reply nanna 48 minutes agoprevI've just bought Kandria. I'm not much of a game player so I probably won't get much play out of it, but Shinmera is clearly pushing the bounds of the Lisp world, and that's something to support. reply ducktective 3 hours agoprevI wish the likes of Nintendo and Sony themselves finance such efforts. I mean it's one another way to create games (IP) for your console, what possibly could be the downside of starting something similar to Github Accelerator for your platform? reply jsheard 3 hours agoparentBecause it's well established that game developers can and will jump through whatever hoops the platform holder demands at their own expense, they don't have the leverage to be picky about the technical details when deciding which platforms to ship on. Nintendo doesn't need to create new incentives to release on the Switch when they already have the biggest incentive of all: 140+ million units sold, and a high attach rate. At least there isn't as much hoop jumping as there used to be, since the systems have all converged on using commodity CPU and GPU architectures with at most minor embellishments. reply Arelius 2 hours agorootparentYeah, also whatever they would build, the other platform vendors won't choose the same thing, and it wont be the exact variant of lisp or w/e that even the few nice developers would want. I wish vendors would be just more supportive of different llvm tool chains. Rust isn't even well supported. reply not_your_vase 2 hours agoprevSomewhat offtopic, just flashed through my mind: you know what would be amazing and absolutely useless at the same time? Porting Yuzu to Nintendo Switch reply laidoffamazon 2 hours agoparentThis has been done, and it’s been possible to run Switch on Switch for several months [0] (about 39 minutes into the video). [0] https://youtu.be/H1gveQUBIKk reply not_your_vase 2 hours agorootparentShoot, it was a suspiciously genius idea* for a Friday 13th. *: Compared to my usual ideas reply jfim 2 hours agoparentprevSince I had to look it up, Yuzu is a Nintendo switch emulator. reply sleepycatgirl 2 hours agoprevHer work is just amazing. And makes me incredibly happy, as I like to write some CL here and there. reply Shinmera 2 hours agoparentAw, thank you. Happy hacking! reply trenchgun 55 minutes agoprevDoes anybody know what is the status of Trial on Mac OS? Specifically Apple Silicon reply noobermin 3 hours agoprevThis is what I come to HN for. Kudos to OP and their colleague. I know it's impossible but what a blessing it would be if Nintendo could be a little more open about their system. reply whitten 2 hours agoprev [–] I probably missed it, but since the switch doesn’t have a keyboard how do you handle text input ? reply jsheard 2 hours agoparentThe Switch can support a USB keyboard, which would be the nice way to do it. There's already a fair number of officially published games with keyboard and/or mouse support, including a couple of programming ones. It has an on-screen keyboard too of course but you wouldn't want to rely on that more than absolutely necessary. reply Shinmera 2 hours agoparentprevIt does have a HID keyboard API, but typically you're expected to present an on-screen keyboard. reply Arelius 2 hours agoparentprev [–] Over the network most likely? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Charles Zhang and Shinmera have been working for two years to port the Trial game engine to the Nintendo Switch, focusing on adapting the Common Lisp runtime.",
      "Despite successfully compiling and executing Lisp code on the Switch, unresolved issues include garbage collection and audio output, with the project costing around $17,000.",
      "The Switch's ARM64 Cortex-A57 chip and OpenGL support made the port feasible, but challenges remain, such as interfacing with the Switch's proprietary OS and optimizing CLOS compilation."
    ],
    "commentSummary": [
      "SBCL (Steel Bank Common Lisp) is being ported to the Nintendo Switch, which is significant for game development in Common Lisp due to its interactive code evaluation and rapid development cycles.",
      "The project is led by Shinmera, who is handling the portability and build architecture, highlighting the technical challenges and potential benefits of running SBCL on specialized game hardware.",
      "The use of the official Nintendo SDK (Software Development Kit) is necessary for publishing games on the Switch, as homebrew SDKs are not supported for retail console releases."
    ],
    "points": 234,
    "commentCount": 40,
    "retryCount": 0,
    "time": 1726232021
  },
  {
    "id": 41527991,
    "title": "Who Owns Nebula?",
    "originLink": "https://medium.com/@cameron-paul/who-actually-owns-nebula-952a1c12d9c0",
    "originBody": "Who Actually Owns Nebula? Cameron Paul · Follow 11 min read · 1 day ago -- Photo by Bryan Goff on Unsplash If this was a video essay this is where I’d put a supercut of content creators saying the phrase “my streaming service Nebula”. Unfortunately this is a blog post so you’ll have to use your imagination. Nebula is a video on demand streaming service focusing primarily on educational content. Unlike other streaming services such as Netflix and YouTube, Nebula is distinct in that it was built by content creators. So surely the title of this post is a silly question then, right? Nebula is owned by the content creators. They say so themselves in all their videos. I wasn’t so sure though. I never could pinpoint why, but the “creator owned” narrative at the forefront of all their promotional material just felt too good to be true. I’ve spent most of my career working in the venture capital funded tech industry, and based on that experience my gut told me Nebula was just like all those other tech companies and was only masquerading as a co-op. Up until recently I ignored that feeling assuming it was just me being overly cynical, but then I saw this Philosophy Tube video announcing Abigail Thorn’s upcoming movie. Much of that video was spent discussing the structure of Nebula and it seemed to confirm something I had previously assumed wasn’t true: that the creators own 50% of Nebula. I’ve seen various sources report that Nebula is owned 50% by a company called Standard Broadcast, and 50% by the creators on the platform. I could never find a first hand source for this information though and thought it was a misunderstanding of some very carefully crafted answers on the company’s FAQ page. How do the creators get paid? Nebula profit is divided 50/50 between the creators and Standard. The creator pool is paid out based on watch time. Who owns Nebula? Nebula is owned and operated by Standard and the creators, with Curiosity Inc (CuriosityStream) holding a minority stake and a board seat. There are no plans to bring in additional investment. The first answer indicates that there is a 50/50 profit share. This means that the content creators on the platform get to take half of the profit that Nebula makes, but that doesn’t tell us anything about how much of the platform the creators actually own. The second answer is about ownership but is much more vague. It doesn’t specify who owns what percent, and includes a the extra verb “operated” which makes things more unclear. It could be that both ownership and operation are shared between Standard and the creators, but one could also interpret that to mean that Standard owns and the creators operate. What I can say is that it’s mathematically impossible to have a 50/50 split between three parties, and there are three parties listed. This is no big deal though. Nebula never claimed there was a 50% ownership split. That claim was just some unaffiliated people on the internet misinterpreting the FAQ. But then I watched that Philosophy Tube video. It began with the familiar rhetoric of “Nebula is owned by the creators who are on it”. But there was more. “50% of the equity in Nebula is distributed between the creators”. This was the first time I had heard a Nebula creator say the number 50% referring to ownership. “The other 50% is owned by the parent company, an agency called Standard”. At this point the math really wasn’t adding up. A parent company by definition owns more than 50% of a subsidiary, that’s what makes it a “parent”. Also a 50/50 split would mean there’s no percentage left over for Curiosity Stream. But then there’s the real kicker: “When you’ve been on Nebula for a certain amount of time you get the option to buy a piece of the parent company. Standard actually invented an entirely new kind of cooperative corporate governance to make this happen”. The fact that there’s interest in buying into Standard seems to indicate there’s something lacking about the ownership you get from just being a Nebula creator, but what really sets off the alarm bells for me is the “entirely new kind of cooperative corporate governance”. If you want to have a creator-owned cooperative you can just do that. There’s nothing novel about co-ops. So why not just do that? My intuition after watching this video was that Standard Broadcast, which is owned by a small subset of Nebula creators, actually owned the vast majority of Nebula. I suspected the options were offered as a way for the most successful creators (that is to say the ones with the most money) to buy their way into the organization that actually holds all the cards. Intuition is often wrong though so I decided I needed to do some actual investigation. The Wendover Documentary The first source I found when looking for more information was a video from Wendover Productions, a YouTube/Nebula channel created by Sam Denby. I thought this would be a good place to start because Denby is one of the original content creators on Nebula, and as I was about to learn one of the owners of Standard Broadcast. The Inside Story of Nebula is an interesting “documentary” (propaganda piece might be more accurate) chronicling the rise of Nebula. It has all the hallmarks you’d expect to find in a tech company marketing pitch. There’s an absurd and unsubstantiated valuation of $150 million dollars, talk of infinite growth, the “minimum viable product” model, and even an enlarging pie analogy. More interesting to me though was how carefully phrased the information about profits and sharing was. “So, through complex financial and legal wizardry, we developed a system where 50% of Nebula profits were distributed to the creators, including crucially, if the platform were ever to be sold”. This clarifies that creators get 50% of the profit as well as 50% of the proceeds from a sale of the company, but very specifically doesn’t mention ownership. Do they get paid 50% because they own 50%? Do they own less than that but get 50% through some other mechanism such as liquidation preference? Is there just some contract that says the company owes them money in the event of a sale? The ownership structure of Standard Broadcast is described with slightly less ambiguity. The company was initially founded by Dave Wiskus, CGP Grey, and Philip Dettmer, but Gray and Dettmer later sold their stake in the company to five other creators. So at that point ownership of Standard was split between six individuals: Dave Wiskus Brian McManus (Real Engineering) Alex (LowSpecGamer) Devin Stone (Legal Eagle) Thomas Frank Sam Denby (Wendover Productions) As I mentioned previously, some ownership of Standard has since been offered to other creators through stock options, but it’s unclear how much or what type of stock those options represent. The firsthand information about Nebula and Standard Broadcast was turning out to be less useful than I had hoped, but the documentary did provide one valuable lead: the investment from Curiosity Stream. “While the exact numbers are not public, what is is that they bought a significant minority stake that valued Nebula, the company that did not exist just three years prior, at over $50 million.” That’s a little vague, but an investment and a valuation number was something I could start to do math with. Whenever a company makes shares available for purchase, they have to file that information with the SEC through what’s called a Form D notice. Watch Nebula LLC has only ever made one Form D filing so it’s not hard to find the investment. Nebula sold precisely $6 million worth of shares, so if we take $50 million as a lower bound for the valuation we can calculate the upper bound of what Curiosity Stream owns. $6 million is 12% of $50 million, so Curiosity Stream could own as much as 12% of Nebula. There’s also another $6.5 million that was authorized but not sold (at least at the time the Form D was filed). The Reddit AMA Following the announcement that Nebula was taking investment from Curiosity Stream, Standard Broadcast’s CEO did an AMA on Reddit. I figured this would be a good place to look for more info about that investment, especially since the answers came straight from the top. One user was curious about the leverage Curiosity Stream would have over the platform. Reddit exchange between Oddtail and dwiskus, transcribed below Oddtail: What, if any, leverage CS has over Nebula and how it operates? If none, what does CS get in return for the investment? dwiskus: CS gets a board seat and a percentage of ownership, meaning a cut of future profits. They get a vote on things like budget approval, but operational and creative control remains with us. Practically speaking CS is well-aware of our plans, and how our plans will significantly benefit them. They aren’t interested in changing our course or slowing us down even if they could. Quite the opposite. __law: can we know what % cs own? And who are the other owners of the company? dwiskus: Percentages weren’t disclosed, but it’s a minority stake. There are no other investors. This confirms that Curiosity Stream does own a portion of Nebula (which is undeniable based on the Form D filing I talked about before), but it also adds that they get a single board seat out of the deal. Furthermore, and possibly more importantly, this confirms that the board controls the budget. Another user wanted to know about the relationship between Standard and Nebula. Reddit exchange between PatrickStirling and dwiskus, transcribed below PatrickStirling: are Nebula and Standard completely separate entities? or is one technically under the other? dwiskus: Nebula is a subsidiary. Standard holds the majority of Nebula LLC equity. If we assume that words mean things (specifically the word majority in this case), this confirms my earlier suspicion that Standard owns more than 50% of Nebula. Another user seemed to be connecting some of the same dots I was and started asking questions about dilution. In case you’re unfamiliar with dilution, when companies take investment they typically do so by issuing new shares for the investor to purchase. This means the total number of shares increases, thus lowering the value of any existing shares as a percent of the total. Reddit exchange between yolomatic_swagmaster, dwiskus, and gurgelblaster, transcribed below yolomatic_swagmaster: How does Curiosity Stream’s investment affect the other creators in terms of them being part-owners? I’m not super clear on how that works anyway. but wondering if anything changes for their set up. dwiskus: Complicated question, but in short: creators still have full control and have lost no equity value. gurgelblaster: …but the equity share is lower? dwiskus: Nope! Like I said, it’s complicated. And once again we have a contradiction. We added new shares but nobody lost equity. How is that possible? “It’s complicated”. Some Actual Answers At this point I’d grown tired of non-answers. I’m not ok with impossible equity numbers being hand waved away as “a new kind of corporate governance”, “financial and legal wizardry”, or just simply “complicated”. These are not the kind of answers people who are being entirely truthful give. Luckily there was one other place I could think to look for hard numbers. The Wendover documentary may have claimed that “the exact numbers aren’t public”, but that’s only partially true. Nebula is a private company, and as such only has to file the unfortunately sparse Form D when offering stock. Curiosity Stream, on the other hand, is publicly traded and has to make much more detailed quarterly filings about all their financials. This includes their investments. So what does Curiosity Stream’s SEC filing say about the Nebula investment? Watch Nebula LLC (“Nebula”) On August 23, 2021, the Company purchased a 12% ownership interest in Watch Nebula LLC for $6,000. Nebula is an SVOD technology platform built for and by a group of content creators. The Company is committed to purchasing an additional 13% ownership interest through eight quarterly payments of $813, which after each payment, the Company will obtain an additional 1.625% of equity ownership interests. Prior to the Company’s investment, Nebula was a 100% wholly owned subsidiary of Standard Broadcast LLC (“Standard”). The Company obtained 25% of the representation on Nebula’s Board of Directors, providing the Company with significant influence, but not a controlling interest. So let’s go down the list of what we can learn from this: The initial $6 million was for 12% ownership They had offered enough stock for a potential ownership of 25%. (that’s the additional 6.5 million from the Form D) Standard Broadcast owned 100% of Nebula prior to this investment Curiosity Stream controls 25% of Nebula’s board From this we can infer: Nebula was valued at exactly $50 million, not over Nebula has 4 board members The creators directly own 0% of Nebula As a side note, if they want to say owning part of Standard Broadcast means you own part of Nebula, then owning part of Curiosity Stream also means you own part of Nebula. Since Curiosity Stream is partly owned but venture capitalists, that would mean that Nebula has in fact taken VC investment. They don’t get to have that cake and eat it too. How Can 0% Be Right? At this point I got frustrated and typed something like “Nebula creators don’t own any of the company” into Google. This was mostly just me shouting into the void. I didn’t expect to get good results from a query like that, but somehow those turned out to be the magic words that returned the one firsthand source I hadn’t found yet. It turns out the answer can be found in an episode of the Decoder podcast. After all that time I spent digging through cagey answers and SEC filings, it turns out Dave Wiskus just straight up tells us (assuming you know what to listen for). It also turns out that the reality is neither novel nor particularly complicated. There is also a structure in place that if Nebula is ever sold, 50 percent of the proceeds go to the creators as a pool. It is a form of what is called shadow equity. Shadow Equity (sometimes called Phantom Stock) isn’t real stock. It’s basically just an IOU that’s worth the same dollar value as the actual stock. The creators will get paid 50% of the proceeds in the event the company sells, but legally they don’t actually own any of the company. The Final Tally So what does that leave us with? Who actually owns Nebula? After looking through a more recent SEC filing to figure out how many of those subsequent stock purchases Curiosity Stream actually made we’re left with the following final numbers: 83.125% Standard Broadcast 16.875% Curiosity Stream 0% The Creators (Directly) Conclusion Unfortunately, without access to one of their contracts, we can’t know for sure what power the broader group of creators actually has. It’s possible that the terms are so favorable for creators that their shadow equity is as good as actual ownership. It’s equally possible, however, that the system was set up in order to keep any meaningful power away from the creators. If the creators don’t control the board then creators don’t control the budget, which means they don’t control the platform. While some creators may have the means to buy into the stock options offered by Standard Broadcast, that only serves to create a class hierarchy divided by wealth. Even if the contract terms are stellar, that’s still a system that completely undermines the progressive values of the creators who make the platform great. Nebula is trading on the idea that it’s a creator owned platform and isn’t like those other tech companies that take VC funding. They utilize the language of cooperatives in order to craft an image that appeals to their left leaning audience, but it appears actual ownership lies primarily in the hands of six (mostly white) men. When you pull away the progressive veneer it turns out Nebula probably is just another tech company. While they may be operating in the best interests of the creators for the moment, I wouldn’t be surprised if that doesn’t continue forever.",
    "commentLink": "https://news.ycombinator.com/item?id=41527991",
    "commentBody": "Who Owns Nebula? (medium.com/cameron-paul)226 points by mintplant 14 hours agohidepastfavorite74 comments xd1936 1 hour ago> Nebula the business is “Standard Broadcast LLC,” and is directly owned at the LLC level by me and 43 other creators (and growing). > Nebula the streaming video service (which controls the streaming revenue) is Watch Nebula LLC, which is about 83% owned by Standard Broadcast LLC, with the rest held by Curiosity Stream. All control and all board seats belong to Standard Broadcast LLC. > We use shadow equity for platform creators because assigning LLC-level equity would make signing new creators logistically impractical, and would have complex tax implications for every creator we bring in. US securities laws also are skewed in favor of the wealthy: it would be very expensive or potentially impossible for us to comply with them if we were issuing securities to small creators who aren’t accredited investors. > If substantial control of the streaming service ever changes hands, we are contractually required to split the proceeds 50/50 with the creators on the platform. 50% of streaming profits are distributed to creators based on watch time. Additionally, 1/3 of the revenue from any subscriber is allocated to the creator responsible for bringing in that subscriber. > Weird that he didn’t just ask. — Dave Wiskus, CEO of Nebula https://www.reddit.com/r/Nebula/comments/1ffnaza/comment/lmx... reply danpalmer 13 hours agoprevInteresting analysis! From the facts in this post, I'd reach the following speculation: the creators have a contract that entitles them to vote on certain types of company action, a share of 50% of the revenue, and potentially representation of their votes in the form of one or two of the existing board seats. Essentially there being some company bylaws in place to enact that \"ownership\" mechanism. This would match what Standard have said about the setup, would match the Wendover documentary and its reference to \"wizardwry\" making it all work, and provide for input from creators and revenue to creators. Combining this with the phantom stock and how a sale would be handled, I think calling this \"ownership\" when summing it up in a single word for the purposes of a YouTube advert is... uncontroversial? The alternative would be having each creator on the cap table, and my understanding is that is actually quite tricky to do at scale, and brings significant legal and tax implications for the creators and Nebula. While this is all very interesting, and I appreciate the blog post, I do wonder if the answer here just boring. reply sitharus 11 hours agoparentIt's definitely an interesting solution to a very similar problem I was looking at a few years ago - how to take on external investment in a co-operative company. I'm not in the US, so I can't comment on the options for Nebula, but where I live coops are a specific type of legal company which would encompass what Nebula want - a company where profit is divided between members in proportion to their contribution. As part of that the only owners allowed are contributors, and the only people who can receive a profit share are the owners. If you want to stop contributing you have to stop owning, and there's no value to the shares. So you can't take on equity funding, only debt. This structure seems to be made to work around that, structuring it like a regular stock company but with a sale trap to make it unappealing to sell for a quick return. reply jszymborski 5 hours agorootparent> So you can't take on equity funding, only debt. Is this a huge problem? reply jasode 5 hours agorootparent>Is this a huge problem? It depends on what the company is trying to build. The dollar amount of debt a company can take is limited by its revenue because it requires ongoing payments to service that debt. Whether it's debt from a bank business loan or by issuing corporate bonds, they both require real revenue to make those payments. Equity investments don't have a ceiling capped by the company's present day earnings. This is important if the company has ambitions to build something that can't get any meaningful revenue on day 1. Instead, they need the money to pay salaries, cloud bills, office rent, etc to build the product and hope the later revenues will justify the investment. E.g. the early Google startup in 1998 took $25 million VC investment and didn't have meaningful revenue until +4 years later in 2002. A company with $0 revenue can't get a $25 million loan with a 4-year deferred payment plan. Yes, there's such a thing as \"convertible debt-to-equity\" but that's a financial vehicle for investors that doesn't apply to co-operatives. reply wodenokoto 11 hours agoparentprevI don't know about American coop laws, but in Denmark there are diary companies owned by farmers (nearly 10.000 farmers own Arla) and Coop Amba, which owns many super market chains have over 2 million members, that each have a voting right in the company. You can totally scale this type of ownership. reply SamWhited 2 hours agorootparentIn the U.S. it really depends what state you're in. Some states have co-ops as a specific business vehicle and in many of them the co-op model is very different. There are big farm co-ops, EMCs, etc. in the U.S. (Land-o-Lakes and Florida Natural Oranges come to mind, I believe they're both fully farmer owned), but I'm not super familiar with them or how their model works, but in Georgia where I am at least co-ops tend to work in one of two ways: For smaller co-ops that don't bring in members very often or who are just getting started they often create an LLC and put in the articles of incorporation that all future members will receive an equal share and equal voting rights. This does require filing paperwork for every new member, but keeps taxes easy. For co-ops that expect to have lots of members rapidly (consumer co-ops that sell shares in their store to whomever wants one, for example) they normally form a corporation and allocate a certain number of shares up front and indicate that any member has equal voting rights, etc. this makes the taxes harder, but is a lot less paperwork since you have to file stuff about the number of shares sold at the end of the quarter (or whenever, I forget the exact reporting details, been a while since I've done this) but not every single time you add a member. Georgia (and a lot of other states) also have specific laws for EMCs and other specific types of co-ops, so it's really fragmented. Take this with a grain of salt, it's just two examples of how a lot of smaller co-ops do things. reply danpalmer 11 hours agorootparentprevI'm not an expert by any means, but my understanding is that partnerships are very different legal entities to a typical US LLC. I believe a partnership is typically in many ways closer to what I suggested may be the case with Nebula – where the company has bylaws and contracts in place internally to make this work. On the other hand for an LLC, LTD, or other similar types of company, being a shareholder is a more public position, and might require reporting, or updating documents registered with the local government every time it changes. That's fine for tens of shareholders in a private company, but not for thousands or millions. reply paxys 4 hours agoparentprevThe big source for confusion IMO is that there is no single universally-understood definition of \"creator\". The site was founded and is controlled by ~6 people who create content for YouTube and other platforms. So does that make Nebula \"creator-owned\" by default? Or does every content creator on the platform have to have some form of ownership? And then does that ownership have to be equitable? Or will one token share with no voting or profit share still qualify? reply mrgoldenbrown 56 minutes agorootparentAny confusion is Nebula's fault by keeping the details of ownership secret. They could clarify what they mean when they claim in bold on their front page that \"Nebula is creator-owned and operated.\" reply moi2388 12 hours agoparentprevThat might very well be the case. It would probably be more honest to give the content creators stocks or say “partly ran by content creators” instead of “owned by content creators” in this case, no? reply derefr 12 hours agorootparentThe problem with that is that if you truly own something, you have the right to sell that thing, and nobody can say who you can sell it to. Give it six months and it wouldn’t be “creator-owned” any more, due to cash-strapped creators dumping stock for a payday. A co-op is structurally as much about ensuring that employees can’t transfer their equity to non-employees as it is about ensuring that employees have equity. You can’t get the semantics of a co-op ownership structure just by doing regular things with a C-corp; you need extra hacks, like these bylaws. reply danpalmer 11 hours agorootparentThat might be the ultimate version of \"ownership\" but it's by no means the only useful version. Startups typically issue shares or options but restrict selling of them. Does that mean you don't own them? Well it's complicated, but in my opinion it's far more useful to say that yes you do own them than no you don't. I as an employee of a public company receive compensation in the form of shares, but I'm only allowed to sell them at certain times of year, do I not own them? Again it's more realistic to say I do. I also don't get to vote with mine, as is the same for other non-voting shares that are fairly common. Ownership is a nuanced concept, and taking a hard line on just one feature of ownership is not necessarily the right choice in general discussion. reply surgical_fire 10 hours agorootparentI never took options in compensation packages as \"ownership\". I just see them as fancy mechanisms to give you the potential for more money without giving you more money outright. In the case of startups, which are not yet public companies, even less so, as those shares can be dilluted, etc. In the case of public conpanies I see them much in the same way I see shares of conpanies I buy in the stock market. I have no meaningful ownership of the company in those cases, it is just where I park some money looking for a return in investment. A coop is actually a form of ownership, even if ironically you can't sell that ownership. reply PawgerZ 4 hours agorootparentprev> Startups typically issue shares or options but restrict selling of them They have to explicitly restrict selling of the shares through contract means. That's because ownership is the legal right to possess, use, or give away (sell) a thing. Likewise, Nebula has to explicitly restrict selling of partial ownership through contract means. reply derefr 3 hours agorootparentprevOwnership of private property (abstract or concrete) is a formal, legally-defined concept — and \"do you have essentially sovereign authority over the disposition of the asset\" is always at the core of it. No, you don't own stock when you own stock options. A stock option is a right to purchase stock at a particular price, and you own that right. Just like owning, say, an easement on a piece of land, doesn't mean you own the land. (And both a stock option and an easement have a value, and you may be able to sell those things themselves — but the value they have, and their sale price, is often disconnected from that of the underlying asset the right exercises against.) You know how you can very easily tell when you own something? Because governments almost always tax transfer of ownership of a thing. You don't pay taxes when you acquire options, because you haven't yet acquired ownership over anything. You do pay taxes when you exercise those options — exercise that right to acquire stock at that price — because now you do own something you didn't before. Another way to tell that you own a thing, is that you have the ability to directly pledge that thing as collateral on a loan. You can pledge stock, but you can't pledge options[1]. This is because you can contractually grant the bank the ability to confiscate your stock in event of default on the loan, in a way that guarantees that they will succeed in this confiscation. But you can't contractually grant the bank the ability to confiscate your options, in a way that is guaranteed to succeed. And that's for exactly the reasons you outline: there may be contractual stipulation on the exercise of the options, that mean that the bank can't immediately liquidate the options, and thereby can't balance the loss-of-lendable-assets coming from your default and/or might risk the company's share-price falling, or even the company going bankrupt, before the options may be exercised. Also, sometimes the bank wouldn't want to exercise a contingent asset they've acquired, but just wants to sell that contingent asset on to someone else who wants to hold and exercise it at a future date. ESOPs in particular usually have voidability clauses that say that not only can't you transfer the option, you can't even build a financial instrument around the option that has the semantics of transferability. Banks very much do not appreciate restrictions like that. --- [1] Yes, you can show your holding of options as a demonstration of assets, to increase the amount a bank is willing to lend you. But this, like any other demonstration of assets — e.g. a demonstration of employment, or of ownership of revenue-generating assets — goes into a Net Present Value-adjusted future-cashflow projection calculation that the bank does, to determine how likely you will be to be able to make your regular loan payments when everything is going well for you. In the breach, they still need collateral to be pledged out of stuff you actually own — i.e. can guarantee them the right to as a creditor. reply Dextro 10 hours agorootparentprevI think the issue is that the marketing is a half truth. Standard is in fact majority owned by content creators, it's just that the marketing makes you assume that the creator saying the line is the owner when that might not be exactly the case. Honestly Nebula always felt off to me because of this marketing. They never sold it as a place where creators had a more equitable share, or where they had more creative freedom and control (which seem to be true), but they used the idea that they were \"fighting the man\" by going at it alone. It always felt like a half truth which, turns out, it is. Also the price always felt too good to be true to me so I always suspected some sort of investment from somewhere to create a loss leader. Though the jury's still out on that. But having worked on video streaming online myself I know first hand how expensive it can get so I wonder how profitable the company is. reply BlindEyeHalo 11 hours agorootparentprevThe way I understood it is that the 'stock' you 'own' is proportional to your viewer count, which is not something you can do if you would actually give them stock. reply csomar 11 hours agoparentprev> a share of 50% of the revenue According to the blog post, they get 50% of the profit which would be much lower than 50% of the revenue. This is worse than other profit-sharing platforms. But hey, you own 50% of a shadow equity or whatever that means. reply blagie 8 hours agoprevThis is approximately what I figured. Whenever I do deeper due diligence, this sort of scam comes up. Literally \"whenever.\" For the record, Nebula is primarily owned and operated by creators. Those creators are: * Dave Wiskus * Brian McManus (Real Engineering) * Alex (LowSpecGamer) * Devin Stone (Legal Eagle) * Thomas Frank * Sam Denby (Wendover Productions) The other creators are getting scammed. This is not an uncommon model. This is almost identical to how edX was structured and the marketing literature is nearly identical too. It was owned and run by universities. Just not the universities who made up the \"consortium.\" If there were no scam, there would absolutely no reason for the non-public non-answers. If Nebula weren't scamming, they would disclose the \"financial and legal wizardry.\" None of this prevented me from giving Nebula money. As I said, this kind of issues comes up literally every time I do due diligence. I am disappointed in specifically Brian McManus for being involved in a scam, but this kind of scam is omnipresent with quite literally every organization I've done due diligence on, and usually much worse. reply staunton 6 hours agoparentI don't understand what the scam is. The structure is opaque which indicates there might be some scam but how would one know what it is? reply jszymborski 3 hours agorootparentThe accusations of it being a scam are speculative for sure, but they come from a relatable place: the inability to understand why creators would agree to take on \"shadow equity\" while a small handful of creators get actual real equity. It sounds like a few creators can sell the equity that is built by many creators at any time, but that all the other creators can only realize their equity if and when the company exits. If someone bought tickets to a steam ship that stipulated that in the event of an imminent iceberg collision, they can only get on the life-boats after a small group of people ransacked the ship and left on the first life-boat, I might assume that they'd been scammed to. But perhaps they were just desperate or didn't think icebergs were a risk. Regardless, I just know I wouldn't buy that ticket. reply xd1936 4 hours agoparentprevYou used the word \"scam\" six times in your comment. That is a bit exaggerative of the actual company structure. Creators are getting paid to put various forms of exclusives on the platform. Just because the company uses a disingenuous term to market itself doesn't mean that said creators are all victims here. reply kragen 4 hours agoparentprevi'm disappointed in brian mcmanus every time i watch a 'real engineering' video and find out that it's full of factual inaccuracies, like all the previous 'real engineering' videos i watched. you'd think i'd learn, but i keep confusing him with 'practical engineering', which is actually real reply cruffle_duffle 3 hours agorootparentWas going to say… I love the practical engineering channel but that channel doesn’t really dive deep enough into any one topic to encounter any factual inaccuracies. reply kragen 2 hours agorootparentadmittedly, it does fall far short of being practical; if your preparation for building a railroad is watching https://www.youtube.com/watch?v=zqmOSMAtadc and your preparation for building a landfill is watching https://www.youtube.com/watch?v=HRx_dZawN44, your trains will derail and your garbage will leach into your groundwater but that's not really a difference between the two channels; real engineering manages to pack plenty of misstatements into videos that are equally superficial reply ehhthing 12 hours agoprevI think the last few paragraphs go off the rails. If creators didn't own (at least in some way) some controlling stake in Nebula, why would they publicly say that they do? Moreover, why would creators join Nebula if the terms were not beneficial to them in the first place? I find it funny that the author writes > It’s equally possible, however, that the system was set up in order to keep any meaningful power away from the creators. Does the author really think that the chance that all of these creators are lying to their audiences is just as likely as them all telling the truth? Also, the author even admits > As I mentioned previously, some ownership of Standard has since been offered to other creators through stock options, but it’s unclear how much or what type of stock those options represent. Owning equity (and thus voting power) in Standard also means that the creator has the ability to vote on how Standard operates Nebula. So the conclusion that the creators have no control over Nebula literally cannot be true. So the statement that \"the creators own 0 percent of Nebula\" is just misleading, and yet this is somehow the important conclusion that the author wants readers to know...? reply Tarq0n 7 hours agoparentThe creators are desperate for a hedge against Youtube, which unilaterally controls their compensation and can deplatform them at any time. reply dmazin 6 hours agoparentprevI agree – the investigation is thorough, but I think the conclusions are a little jumpy. reply SpicyLemonZest 11 hours agoparentprevThe author’s thesis is that the creators are being tricked. They own some complex bespoke right in Nebula (“an entirely new kind of cooperative corporate governance”), which they’re told and believe is equivalent to ownership, but actually it’s a sham that will break down if Standard ever wants to do something that isn’t in the creators’ interests. reply ehhthing 10 hours agorootparentThe author doesn't go through this in nearly enough detail to make that argument convincing. Rather than spending the entire time trying to find what the \"real\" ownership amount, the author should've spent the time contextualizing the situation. The author basically spends the entirety of one sentence dismissing the idea that there could exist a corporate governance model that allows creators to have a meaningful way to direct the company's decision making process and spends the rest of the time on a wild goose hunt to figure out the \"actual\" ownership percentages. It was pretty obvious from the beginning given the repeated mentions of complex ownership models that the \"real\" numbers were not going to mean that creators owned \"real\" equity in the company. An investigation about what this actually means would've been a much better way to write this kind of essay. Instead all we got was a long article with a conclusion that was reasonably obvious in hindsight, and no real evidence to support the thesis that \"it's all just smoke and mirrors\". reply SpicyLemonZest 1 hour agorootparentI don’t agree that was obvious in hindsight. I was familiar with Nebula before this article and I had always understood it to be something like a co-op where creators and only creators had genuine equity. When reading the first bit, I assumed as the author did that it must be something where the co-op owns a controlling share in some underlying company. The conclusion that there’s nothing like a co-op at all is not what I would have expected and I really think does suggest that it’s all smoke and mirrors. If this “ownership” doesn’t consist of anything more than a right for creators to be paid based on their view counts, isn’t it just a YouTube contract with extra steps? reply weinzierl 9 hours agorootparentprev\"The author’s thesis is that the creators are being tricked.\" The author says: \"Unfortunately, without access to one of their contracts, we can’t know for sure what power the broader group of creators actually has.\" While the accusation of the creators being tricked might be between the lines[1], I think the more direct accusation is the subscribers being tricked. The subscribers are made to believe: 1. the creators get 50% of Nebula's profit 2. their money goes into a co-op of creators 3. Nebula hasn't taken VC money My reading is that the author claims, that only the first point is true. [1] To the best of my knowledge, none of them has come forward with any accusations. On the other hand, we probably only should expect this to happen once Nebula gets in trouble or is actually sold. reply bogtog 8 hours agoprev> Shadow Equity (sometimes called Phantom Stock) isn’t real stock. It’s basically just an IOU that’s worth the same dollar value as the actual stock. The creators will get paid 50% of the proceeds in the event the company sells, but legally they don’t actually own any of the company. To my untrained eye, this just seems like owning stock without voting privileges, which seems okay? Is the fear that the voters will eventually just change the creator profit sharing to zero and milk the company dry rather than selling it? reply maweki 8 hours agoparent> seems like owning stock Shadow Equity usually doesn't pay dividends. So if the stock devalues due to payed out dividends (and the shadow stock has no voting power), it's the creators paying out the investors from their share. reply NoGravitas 1 hour agoprevI've long had the feeling that Nebula was kind of sketchy, ever since they kicked out J.T. (Second Thought), a founding member. I'm glad it's working out for Philosophy Tube and Jessie Gender, but I will not be surprised at all if there's not a total rug-pull some time in the future. reply LammyL 11 hours agoprevThis corporate structure seems a bit sketchy for creators if the platform wants to sell. If they sell Nebula then creators split half of the profits. If they sell Standard then creators get nothing. reply cruffle_duffle 3 hours agoparentWhy would Standard get sold though? The value to a buyer is nebula. In fact perhaps they intentionally structured things so as to detach Nebula from Standard to make it easier to spin off. reply guipsp 4 hours agoprevWhy is this person who, as far as I can tell, is not, has never been, or has any intentions of being a creator on nebula complaining that creators are being screwed over? Surely if you were going to publish this piece, you'd get the opinion of at least one nebula creator? reply runnr_az 4 hours agoparentYeah... Doesn't seem fair to imply that it's a big secret if you don't ask. reply paxys 4 hours agoprevTL;DR Nebula is a streaming service. It has a parent company (Standard Broadcast) which owns $83% of the service and has 3 out of 4 board seats. An external public company (Curiosity Stream) owns the remaining 17% and 1 board seat. Standard Broadcast is owned by 6 individuals, some of them popular YouTubers. Content creators who post on Nebula get 50% of the service's total profits (based on watch time). Content creators (other than the 6 mentioned above) don't own any shares in Nebula, but if the service is ever sold, they get 50% of the proceeds from the sale. This doesn't, however, apply to Standard Broadcast or Curiosity Stream, which can and are sold/traded independently. Whether Nebula is \"creator-owned\" or not, as they proudly proclaim front and center on their website and marketing materials, is left as an exercise for the reader. reply prithvi24 12 hours agoprevHonestly, I feel like the criticism is missing some key points. Sam and the other founders are creators who've put a ton of work and resources into Nebula. They've made awesome original content like Jetlag and have invested heavily to support other creators on the platform. Nebula gives budgets for creators to produce their own shows, and they get a share of the revenue from subscribers they bring in—I wouldn't be surprised if they earn from views too. So saying it's misleading doesn't sit right with me. Creators on Nebula definitely get a bigger piece of the pie compared to other platforms. It might not be a perfect co-op, but it's way more creator-friendly than most out there. reply gautampk 12 hours agoparentIt /is/ misleading to say or imply that it is a co-op, though. It isn’t. It may be a progressive company which provides very favourable terms to creators, but it isn’t a co-op. reply dmazin 6 hours agorootparentOh, good comment. After reading the article, I walked away with \"well, they were vague about the terms, but the real terms sound fine to me.\" But your comment changed my mind somewhat: it definitely isn't a co-op, and the \"creator owned\" language is misleading. reply prithvi24 12 hours agorootparentprevYeah, I see your point—it might be misleading to label Nebula as a co-op since it technically isn't one. But I think what's important is how much Nebula does to empower creators compared to other platforms. The founders are creators who've invested a lot to make it a place where content makers get more support and a bigger share of the revenue. Even if it's not a co-op, it still feels like a step in the right direction for giving creators more control and benefits than they'd get elsewhere. reply paxys 5 hours agoparentprevNone of what you say is being debated. What is being debated is the fact that they market themselves as a creator-owned co-op (which is incorrect and borderline fraudulent). reply weinzierl 10 hours agoprev\"On August 23, 2021, the Company purchased a 12% ownership interest in Watch Nebula LLC for $6,000.\" Surely this must be 6 million USD. I first thought this is a typo, but it is like that in the original SEC filing. Is this some convention I am just oblivious about? reply noname120 10 hours agoparentFrom the SEC FORM D[1]: > Total Amount Sold $6,000,000 USD You're referring to the note 2 of the prospectus supplement no. 12[2]. It indicates the following at the top of the notes section: > (in thousands, except share and per share data) [1] https://www.sec.gov/Archives/edgar/data/1881238/000188123821... [2] https://www.sec.gov/Archives/edgar/data/1776909/000121390021... reply WorldMaker 3 hours agoparentprevA lot of Corporate financial reports have a baseline of $1k, $10k, $100k or sometimes even $1 million that their figures and charts never have reason to drop below (or are just taken as rounding errors) anywhere in the report and so they just establish the baseline early in the report and all numbers are relative to that. It seems like Curiosity Stream's report was using a $1k baseline and you just multiply all the financial numbers by 1,000. reply richardwhiuk 12 hours agoprevFor most respects and purposes, Standard Broadcast is the creators on Nebula. reply weinzierl 10 hours agoparentThat was my thinking as well and unfortunately the article doesn't go into that direction. For example: Do we know how many creators they have. Do they all have the same contracts and rights or are some of them just (affiliated) \"creators\". To put this to an extreme: If only the six owners were creators they could claim 100% goes to the creators without lying. reply JumpCrisscross 12 hours agoparentprev> For most respects and purposes, Standard Broadcast is the creators on Nebula Then why shell them into a separate vehicle? The purpose is to create two tiers of creators. That's fine. Some took a risk. Others didn't. But it's dishonest to then claim one is being transparent when the only reason for this structure, apart from incompetence or convenience, is obfuscation. reply pikminguy 5 hours agorootparentIt's kind of a historical happenstance. Standard was created first as a company offering support services for creators. Nebula was one of Standard's offerings which grew to be the main thing. reply timenova 11 hours agoprevWhile their corporate structure is shadowy, their revenue distribution between creators is not. The source for this is in this video [0][1] by TLDR Business. Essentially, all the money collected by subscriptions is paid to creators based on the number of views on their videos. [0] https://nebula.tv/videos/tldrbusiness-how-does-tldr-really-m... [1] https://www.youtube.com/watch?v=clfti6Do9lc reply eterevsky 8 hours agoprevTo summarize, 17% is owned by Curiosity Stream, the rest by Standard Broadcast which in turn is owned by Wendover Production and some other creators. reply jszymborski 5 hours agoprevI'm cancelling my subscription. I was always suspicious of the \"50:50 SB/Ceator\" deal. The real way to do something like this imho is via a cooperative, with investors like CuriosityStream and SB providing capital loans. That said, the \"ownership\" claim is apparently provably false and I've been lied to. The company has operated dishonestly, and frankly the biggest reason I had a subscription was because of what I thought was an equitable ownership structure for creators. reply armoredkitten 3 hours agoparentTo me, there's no evidence in any of this that would indicate the creators are misinformed or being scammed. In reality, they know their primary source of revenue is coming from Youtube, and it is a frequent complaint, especially for content creators who do anything marginally political or controversial, that they are demonetized or hit with copyright strikes (even in clear cases of fair use) and have to deal with the faceless Google behemoth trying to reverse these automated decisions. The end result of all this is that their revenue stream is unstable, and they are reminded of it frequently. To me, the fact that many of them clearly find Nebula a more suitable arrangement for them is still an indicator to me that, if I want to support the creators, Nebula is a better way to do that. Obviously, you can make your own decision on that, and sure, if you feel lied to, I can appreciate being upset about that. But maybe most of these content creators are less concerned with the ownership (they get 0% stake in Google, after all) and more concerned with the profit sharing arrangement. If so, I'm still happy to support them in that. reply jszymborski 2 hours agorootparentI never said they were scammed, but I was indeed misled by dishonest marketing, and frankly I think creators are also to blame for spouting lies about ownership. I supported Nebula because I think a creator's co-op is a beautiful project. I was willing to compromise on the co-op idea when I thought it was a 50:50 ownership structure but this is nothing close to equitable in my mind. I'll spend the money I save on Nebula on monthly donations to creators. I look forward to supporting a creators co-op if a promising one is ever made. reply paxys 5 hours agoprevWhenever someone answers \"it's complicated\" to what should be a simple question, you know there's a scam involved. reply jiveturkey 11 hours agoprev> It’s possible that the terms are so favorable for creators that their shadow equity is as good as actual ownership. It’s equally possible, however, that the system was set up in order to keep any meaningful power away from the creators. I very much suspect the latter. Otherwise why be so circuitous and secretive about it. I've never heard of shadow equity before. But clearly it isn't equity, ie it isn't ownership. So it almost certainly has no governance rights. Now it may entitle the creators to 50% in the event of a sale, but who's to say it's Nebula that's sold, vs Standard, and who's to say it's sold at all. Rather the standard (excuse the pun) approach of selling off the assets without selling the company. reply JumpCrisscross 12 hours agoprevReal Engineering did a video titled \"How Nebula Works\" [1]. It details the technical problems. But seemingly deliberately dodges the legal and financial questions I clicked on that video to answer. I love Nebula. But it's obvious that there is a shady component to their structure that prevents transparency that senior leadership knows about and knowingly obfuscates. I'm not arguing for a creator-owned co-operative; that would render the equity worthless, and they want (and should) retain the option to sell. But Nebula et al made a big fuss about refusing VC because of the incentives that creates while legally creating a two-tier structure that mimics those same incentives. [1] https://nebula.tv/videos/realengineering-how-nebula-works reply chrisoverzero 7 hours agoparentYou clicked on a video from Real Engineering titled “How Nebula Works” hoping to get answers to legal and financial questions? I think videos with titles like “How Internal Combustion Engines Work” would also deliberately dodge legal and financial questions. Because they’re not what the video is about. reply WorldMaker 3 hours agorootparentThe creator of the channel Legal Eagle is also one of the six main Standard Broadcast owners (with Real Engineering) mentioned in the article, so it sounds like we are waiting for the Legal Eagle \"How Nebula Works\" video. reply jamesholden 7 hours agoprevI was thinking about joining them to try it out.. but now I feel yucky. reply drclegg 5 hours agoparentEven with this obfuscation, it is still more fair to its creators than most other platforms. It's just not quite the co-op structure their marketing makes it out to be, which is indeed disingenuous (IMO). reply stuaxo 8 hours agoprevWhen it comes to a democratic cooperative, how does this compare to say Mondragon ? reply ryzvonusef 2 hours agoprevYou have two paths for a paid youtube competitor: ____ 1- Bulk Purchase, like Nebula (Sam from Wendover and other fellow youtubers); One subscription buys you access to ALL the channels in the network. Pros: * Single payment for users * gets better as more creators join * usually discounted if you consider per-channel cost Cons: * requires trust between the creators * can cause clash between creators on division of funds * requires a certain minimum output to maintain basic service * one cancelled creator can sink the service ____ 2-Per-channel subscription, like Floatplane (Made by Linus from the LinusTechTips channel); You have to pay seperately for each channel you want to get access to. Pros: * Can be cheaper if you only want access to a couple of specific creators * creators are not dependent on each other to maintain the service * you are seperate from other creator drama somewhat Cons: * Can become costly quickly if you want to subscribe to multiple creators * not that competitive with youtube premium * can cause comparison strife between creators for difference in subscription fee * can't take advantage of the synergist effect. _____ Basically Nebula has the advantage of having many high profile creators, and if you have a smaller following, you can take advantage of the rising tide and get a boost from them by joining the service. Also, Nebula seems to have ~ 50-ish creators from some pretty diverse set of topics, so it's not pigeonholed as Floatplane. Buuuuut...it's unsustainable I feel. As the number of creators increase, the disparity of what each creator brings and what they are owed will be unteneble. -- Floatplane, OTOH, has half the number of channels as Nebula, and they are mostly tech friends of Linus, so it's not only pigeonholed, but overshadowed by the anchor store that is LTT. By having seperate silos, you are unlikely to catch the wake of a rising ship to boost yourself. However, I feel that by cutting money out of the equation (Floatplane gets a fixed cut, rest, you earn of your own merits) it is in theory more sustainable; you have your own silo, and if one is sinking, you are not concerned. Also, there is a gun channel on the website, and they are there because they are tired of the censorship from Youtube. Being free from drama is a plus...don't subcribe if you don't want to watch! but not that free; they are still bound to the whims of payment processors, so if VISA/MASTERCARD/Whatever block them, they are SoL. They are not Only Fans level of free. ___ TBH, it's a pain to get people to pay, the friction is just too high. Nebula makes it easier than Floatplane, on the onset, even if it's not the best choice. But since when have people used logic where money is concerned? reply rambobambo 12 hours agoprevHow to create a similar site for my local community? reply felurx 9 hours agoparentDo you mean a video sharing site? Maybe self-hosting PeerTube might be something for you. It's designed with federation (with other instances via ActivityPub) in mind, but the FAQ say that can be disabled. Of course, there probably are other software projects (like maybe DTube) that have different priotities and ideas, might be worth a bit of research. reply astura 6 hours agoprevTL;DR: 1) a group of content creators created a company 2) that company runs a streaming platform where they also allow outside content creators to use with (at least) a profit sharing agreement that includes ongoing profit as well as profit from the a sale of the company, should they sell it. 3) that platform also took in a single minority investor at some point 4) some rando on Medium is getting wrapped around the axle because they think the outside creators are calling this \"ownership.\" reply yaur 5 hours agoparentOne has to wonder if the creators got 50% of the sale to curiosity stream. reply ParadisoShlee 12 hours agoprevYou're saying standard has the fiscal responsibility of their entirely company?! Give dave an award for being awesome and running a successful company with a single investor. reply nottorp 8 hours agoprev [–] ... and why is it called the same as the Nebula Awards? reply runnr_az 4 hours agoparent [–] The term is nebulous reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nebula is a video-on-demand streaming service focusing on educational content, built by content creators but not truly owned by them.",
      "Standard Broadcast owns 83.125% of Nebula, CuriosityStream owns 16.875%, and creators directly own 0%, though they receive 50% of profits and proceeds from a sale.",
      "Creators have \"shadow equity,\" meaning they are compensated like owners without holding actual stock, raising questions about the platform's alignment with creators' values."
    ],
    "commentSummary": [
      "Nebula is owned by Standard Broadcast LLC, with 44 creators having shadow equity instead of direct ownership to avoid logistical and tax issues.",
      "If Nebula is sold, creators receive 50% of the proceeds, but some argue the structure lacks transparency and true cooperative ownership.",
      "Critics claim the marketing is misleading since creators don't have direct equity or control over Nebula."
    ],
    "points": 226,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1726201127
  },
  {
    "id": 41530190,
    "title": "FlowTracker – Track data flowing through Java programs",
    "originLink": "https://github.com/coekie/flowtracker",
    "originBody": "FlowTracker, a Java agent that tracks data flowing through Java programs. It helps you understand where any program got its output from, what it means, and why it wrote it.Watch the video or explore the live demo yourself, and read how it works at https:&#x2F;&#x2F;github.com&#x2F;coekie&#x2F;flowtracker",
    "commentLink": "https://news.ycombinator.com/item?id=41530190",
    "commentBody": "FlowTracker – Track data flowing through Java programs (github.com/coekie)209 points by coekie 7 hours agohidepastfavorite27 comments FlowTracker, a Java agent that tracks data flowing through Java programs. It helps you understand where any program got its output from, what it means, and why it wrote it. Watch the video or explore the live demo yourself, and read how it works at https://github.com/coekie/flowtracker ysleepy 1 hour agoThis is incredibly cool. I love how good the tooling is in the java/jvm ecosystem. Last time I was this blown away was with jitwatch ( https://github.com/AdoptOpenJDK/jitwatch ) FlowTracker reminds me a little of taint analysis, which is used for tracking unvalidated user inputs or secrets through a program, making sure it is not leaked or used without validation. search keywords are \"dynamic taint tracking/analysis\" https://github.com/gmu-swe/phosphor https://github.com/soot-oss/SootUp https://github.com/feliam/klee-taint reply BoppreH 6 hours agoprevBlown away by the demo tracking an HTML element back to the SQL statement that added that value to the database. I can totally see a future where tools like this are the first line of defense when troubleshooting bugs. reply coekie 4 hours agoparentThanks. As I was developing FlowTracker, a lot of the work was driven by making tracking of specific example programs work. I knew what result I was aiming for, but it was hard to predict what lower level mechanisms needed to be supported to make a specific example work. That often depended on internal implementation details of the JDK or libraries being used where the data was passing through. But the HTML element linking back to the SQL script that added that data into the database wasn't like that. I didn't expect or work towards it, that just happened, so it blew me away a little too and got me excited about what else this approach could accomplish. reply Noumenon72 1 hour agorootparentI didn't make it to that element of the demo because I don't need a tool to help me find which file HTML text strings are from or that HTTP headers come from my web server. So I would recommend putting that \"wow\" element earlier in the demo. reply immibis 3 hours agorootparentprevA great example of how design of good products should be guided by the end goal instead of by the technical mechanism, when possible. You went out of your way to make sure the functionality was not limited by a certain single mechanism. reply actionfromafar 4 hours agoparentprevWhen you think about it, so many problems could have been prevented and so many business rules could have been easier to express if there was some standard way to track the origins and veracity of data. Maybe also some way to track if the data is meant to be transient or meant to be written back. The more such constraints which could be described up front, the better. reply jpmonettas 6 hours agoprevCool! I wrote something on the same spirit but for Clojure, called FlowStorm http://www.flow-storm.org/ For instrumentation, instead of an instrumenting agent it uses a fork of the official Clojure compiler (in Clojure you can easily swap compilers at dev) that adds extra bytecode. What is interesting about recording Clojure programs execution is that most values are immutable, so you can snapshot them by just retaining the pointers. Edit: Since the OP demo is about exploring a web app for people interested in this topics I'm leaving a demo of FlowStorm debugging a web app also https://www.youtube.com/watch?v=h8AFpZkAwPo reply udkl 1 hour agoparentThis is beautiful! great job! - What was the reason you choose javafx? After you choose fx, did you look at cljfx? reply jiehong 5 hours agoparentprevNice! Do you like use data structure metadata for tracking values? reply jpmonettas 5 hours agorootparentNot sure I follow, can expand on that? I gave a presentation on it recently https://www.youtube.com/watch?v=BuSpMvVU7j4 which goes over demos and implementation details if you are interested in those topics. reply jiehong 5 hours agorootparentI meant this [0], and so tracking a data via tagging it with a metadata and seeing where it ends up. Thanks for the video, I'm gonna go watch it. [0[: https://clojure.org/reference/metadata reply jpmonettas 5 hours agorootparentSo recording in FlowStorm doesn't use Clojure metadata capabilities in any way, it is basically about storing function calls, function returns and the pointers to all expressions intermediate immutable values together with their code coordinates. reply svieira 1 hour agoprevThis reminds me (in the best way possible) of the Eve-lang demos of debugging a program by simply asking \"why isnot here?\" Fantastic work! https://www.youtube.com/watch?v=TWAMr72VaaU&t=164s and https://witheve.com/ reply hoten 1 hour agoprevYears ago I experimented[1] with a similar concept (wanting something like JavaScript source maps, but for HTML). I didn't manage to find the time to expand on it, but I think web developer tooling would really benefit from this sort of full-stack attribution. Integration of any solution like this into existing frameworks feels like a big challenge. [1] HTML Source Maps - https://github.com/connorjclark/html-source-maps https://docs.google.com/document/d/19XYWiPL9h9vA6QcOrGV9Nfkr... reply KomoD 20 minutes agoprevThat is really cool, really like that there's a browser demo too reply michaelmior 6 hours agoprevVery cool! I love the demo video and I could definitely see how this would be useful when diving into an unfamiliar codebase. reply SeriousM 1 hour agoprevOnce I had the vision to track data over the internet, like where came the image from, on which cdn was it. Or \"what did this string have seen from creation till it reached my screen\". This is a step into this direction. reply smartmic 6 hours agoprevI am not really sure if I get the full picture and how it might be used - but it somehow reminds me of a Smalltalk environment where I can also inspect everything (all are objects and messages and you can trace back and interact with it those). reply peterpost2 3 hours agoprevIf I recall there was a paper on a similar tool that was used for finding SQL-injections dynamically in java programs. Is this the same tool? reply coekie 2 hours agoparentNo, that must have been something different. It would be possible to extend what FlowTracker does to also find SQL (or other) injection vulnerabilities. So it's possible the tool you're thinking of used a similar approach. reply hansoolo 1 hour agoprevThis is pretty cool! Do you think something similar is possible for c#, too? reply yellow_lead 1 hour agoprevThis looks really cool, I think this could have saved me some time hunting bugs when I was working with Spring in the past. reply BadJo0Jo0 4 hours agoprevThanks for this! Been trying to get this work with VSCode with a project I'm trying to make sense of. Having to take a pause on it right now, but looking forward to getting it working and playing with it. reply exabrial 6 hours agoprevThis is awesome! Reminds me of Java flight recorder! reply larusso 5 hours agoprevHmm would love to connect this to our gralde builds :) reply mdaniel 2 hours agoparentI know you may be tongue-in-cheek but if you're on the .gradle.kts flavor there's a reasonable chance it would work. The Groovy ones are, I suspect, just entirely too dynamic dispatch for it to make any sense (e.g. all flows are from org.groovy.SomeRandoThing and good luck) reply kitd 4 hours agoprev [–] Very impressive! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FlowTracker is a Java agent designed to track data flow within Java programs, aiding in understanding the origin and significance of outputs.",
      "It offers a video tutorial and a live demo for users to explore its functionalities.",
      "More information and access to the tool can be found on its GitHub page: https://github.com/coekie/flowtracker."
    ],
    "commentSummary": [
      "FlowTracker is a Java agent designed to track data flow in Java programs, aiding in understanding program outputs.",
      "Users compare FlowTracker to tools like jitwatch and dynamic taint tracking, highlighting its potential for troubleshooting and data origin tracking.",
      "The demo showcases its ability to trace an HTML element back to the SQL statement that added it to the database, generating excitement for its integration into various development environments."
    ],
    "points": 209,
    "commentCount": 27,
    "retryCount": 0,
    "time": 1726227208
  },
  {
    "id": 41527675,
    "title": "Better-performing “25519” elliptic-curve cryptography",
    "originLink": "https://www.amazon.science/blog/better-performing-25519-elliptic-curve-cryptography",
    "originBody": "Automated reasoning Better-performing “25519” elliptic-curve cryptography Automated reasoning and optimizations specific to CPU microarchitectures improve both performance and assurance of correct implementation. By Torben Hansen, John Harrison September 10, 2024 Share Share Copy link Email X LinkedIn Facebook Line Reddit QZone Sina Weibo WeChat WhatsApp 分享到微信 x Cryptographic algorithms are essential to online security, and at Amazon Web Services (AWS), we implement cryptographic algorithms in our open-source cryptographic library, AWS LibCrypto (AWS-LC), based on code from Google’s BoringSSL project. AWS-LC offers AWS customers implementations of cryptographic algorithms that are secure and optimized for AWS hardware. Two cryptographic algorithms that have become increasingly popular are x25519 and Ed25519, both based on an elliptic curve known as curve25519. To improve the customer experience when using these algorithms, we recently took a deeper look at their implementations in AWS-LC. Henceforth, we use x/Ed25519 as shorthand for “x25519 and Ed25519”. Related content Formal verification makes RSA faster — and faster to deploy Optimizations for Amazon's Graviton2 chip boost efficiency, and formal verification shortens development time. In 2023, AWS released multiple assembly-level implementations of x/Ed25519 in AWS-LC. By combining automated reasoning and state-of-the-art optimization techniques, these implementations improved performance over the existing AWS-LC implementations and also increased assurance of their correctness. In particular, we prove functional correctness using automated reasoning and employ optimizations targeted to specific CPU microarchitectures for the instruction set architectures x86_64 and Arm64. We also do our best to execute the algorithms in constant time, to thwart side-channel attacks that infer secret information from the durations of computations. In this post, we explore different aspects of our work, including the process for proving correctness via automated reasoning, microarchitecture (μarch) optimization techniques, the special considerations for constant-time code, and the quantification of performance gains. Elliptic-curve cryptography Elliptic-curve cryptography is a method for doing public-key cryptography, which uses a pair of keys, one public and one private. One of the best-known public-key cryptographic schemes is RSA, in which the public key is a very large integer, and the corresponding private key is prime factors of the integer. The RSA scheme can be used both to encrypt/decrypt data and also to sign/verify data. (Members of our team recently blogged on Amazon Science about how we used automated reasoning to make the RSA implementation on Amazon’s Graviton2 chips faster and easier to deploy.) Example of an elliptic curve. Elliptic curves offer an alternate way to mathematically relate public and private keys; sometimes, this means we can implement schemes more efficiently. While the mathematical theory of elliptic curves is both broad and deep, the elliptic curves used in cryptography are typically defined by an equation of the form y2 = x3 + ax2 + bx + c, where a, b, and c are constants. You can plot the points that satisfy the equation on a 2-D graph. An elliptic curve has the property that a line that intersects it at two points intersects it at at most one other point. This property is used to define operations on the curve. For instance, the addition of two points on the curve can be defined not, indeed, as the third point on the curve collinear with the first two but as that third point’s reflection around the axis of symmetry. Addition on an elliptic curve. Now, if the coordinates of points on the curve are taken modulo some integer, the curve becomes a scatter of points in the plane, but a scatter that still exhibits symmetry, so the addition operation remains well defined. Curve25519 is named after a large prime integer — specifically, 2255 – 19. The set of numbers modulo the curve25519 prime, together with basic arithmetic operations such as multiplication of two numbers modulo the same prime, define the field in which our elliptic-curve operations take place. Successive execution of elliptic-curve additions is called scalar multiplication, where the scalar is the number of additions. With the elliptic curves used in cryptography, if you know only the result of the scalar multiplication, it is intractable to recover the scalar, if the scalar is sufficiently large. The result of the scalar multiplication becomes the basis of a public key, the original scalar the basis of a private key. The x25519 and Ed25519 cryptographic algorithms The x/Ed25519 algorithms have distinct purposes. The x25519 algorithm is a key agreement algorithm, used to securely establish a shared secret between two peers; Ed25519 is a digital-signature algorithm, used to sign and verify data. The x/Ed25519 algorithms have been adopted in transport layer protocols such as TLS and SSH. In 2023, NIST announced an update to its FIPS185-6 Digital Signature Standard that included the addition of Ed25519. The x25519 algorithm also plays a role in post-quantum safe cryptographic solutions, having been included as the classical algorithm in the TLS 1.3 and SSH hybrid scheme specifications for post-quantum key agreement. Microarchitecture optimizations When we write assembly code for a specific CPU architecture, we use its instruction set architecture (ISA). The ISA defines resources such as the available assembly instructions, their semantics, and the CPU registers accessible to the programmer. Importantly, the ISA defines the CPU in abstract terms; it doesn’t specify how the CPU should be realized in hardware. Related content Amazon's Tal Rabin wins Dijkstra Prize in Distributed Computing Prize honors Amazon senior principal scientist and Penn professor for a protocol that achieves a theoretical limit on information-theoretic secure multiparty computation. The detailed implementation of the CPU is called the microarchitecture, and every μarch has unique characteristics. For example, while the AWS Graviton 2 CPU and AWS Graviton 3 CPU are both based on the Arm64 ISA, their μarch implementations are different. We hypothesized that if we could take advantage of the μarch differences, we could create x/Ed25519 implementations that were even faster than the existing implementations in AWS-LC. It turns out that this intuition was correct. Let us look closer at how we took advantage of μarch differences. Different arithmetic operations can be defined on curve25519, and different combinations of those operations are used to construct the x/Ed25519 algorithms. Logically, the necessary arithmetic operations can be considered at three levels: Field operations: Operations within the field defined by the curve25519 prime 2255 – 19. Elliptic-curve group operations: Operations that apply to elements of the curve itself, such as the addition of two points, P1 and P2. Top-level operations: Operations implemented by iterative application of elliptic-curve group operations, such as scalar multiplication. Examples of operations at different levels. Arrows indicate dependency relationships between levels. Each level has its own avenues for optimization. We focused our μarch-dependent optimizations on the level-one operations, while for levels two and three our implementations employ known state-of-the-art techniques and are largely the same for different μarchs. Below, we give a summary of the different μarch-dependent choices we made in our implementations of x/Ed25519. For modern x86_64 μarchs, we use the instructions MULX, ADCX, and ADOX, which are variations of the standard assembly instructions MUL (multiply) and ADC (add with carry) found in the instruction set extensions commonly called BMI and ADX. These instructions are special because, when used in combination, they can maintain two carry chains in parallel, which has been observed to boost performance up to 30%. For older x86_64 μarchs that don’t support the instruction set extensions, we use more traditional single-carry chains. For Arm64 μarchs, such as AWS Graviton 3 with improved integer multipliers, we use relatively straightforward schoolbook multiplication, which turns out to give good performance. AWS Graviton 2 has smaller multipliers. For this Arm64 μarch, we use subtractive forms of Karatsuba multiplication, which breaks down multiplications recursively. The reason is that, on these μarchs, 64x64-bit multiplication producing a 128-bit result has substantially lower throughput relative to other operations, making the number size at which Karatsuba optimization becomes worthwhile much smaller. We also optimized level-one operations that are the same for all μarchs. One example concerns the use of the binary greatest-common-divisor (GCD) algorithm to compute modular inverses. We use the “divstep” form of binary GCD, which lends itself to efficient implementation, but it also complicates the second goal we had: formally proving correctness. Related content Computing on private data Both secure multiparty computation and differential privacy protect the privacy of data used in computation, but each has advantages in different contexts. Binary GCD is an iterative algorithm with two arguments, whose initial values are the numbers whose greatest common divisor we seek. The arguments are successively reduced in a well-defined way, until the value of one of them reaches zero. With two n-bit numbers, the standard implementation of the algorithm removes at least one bit total per iteration, so 2n iterations suffice. With divstep, however, determining the number of iterations needed to get down to the base case seems analytically difficult. The most tractable proof of the bound uses an elaborate inductive argument based on an intricate “stable hull” provably overapproximating the region in two-dimensional space containing the points corresponding to the argument values. Daniel Bernstein, one of the inventors of x25519 and Ed25519, proved the formal correctness of the bound using HOL Light, a proof assistant that one of us (John) created. (For more on HOL Light, see, again, our earlier RSA post.) Performance results In this section, we will highlight improvements in performance. For the sake of simplicity, we focus on only three μarchs: AWS Graviton 3, AWS Graviton 2, and Intel Ice Lake. To gather performance data, we used EC2 instances with matching CPU μarchs — c6g.4xlarge, c7g.4xlarge, and c6i.4xlarge, respectively; to measure each algorithm, we used the AWS-LC speed tool. In the graphs below, all units are operations per second (ops/sec). The “before” columns represent the performance of the existing x/Ed25519 implementations in AWS-LC. The “after” columns represent the performance of the new implementations. For the Ed25519 signing operation, the number of operations per second, over the three μarchs, is, on average, 108% higher with the new implementations. For the Ed25519 verification operation, we increased the number of operations per second, over the three μarchs, by an average of 37%. We observed the biggest improvement for the x25519 algorithm. Note that an x25519 operation in the graph below includes the two major operations needed for an x25519 key exchange agreement: base-point multiplication and variable-point multiplication. With x25519, the new implementation increases the number of operations per second, over the three μarchs, by an average of 113%. On average, over the AWS Graviton 2, AWS Graviton 3, and Intel Ice Lake microarchitectures, we saw an 86% improvement in performance. Proving correctness We develop the core parts of the x/Ed25519 implementations in AWS-LC in s2n-bignum, an AWS-owned library of integer arithmetic routines designed for cryptographic applications. The s2n-bignum library is also where we prove the functional correctness of the implementations using HOL Light. HOL Light is an interactive theorem prover for higher-order logic (hence HOL), and it is designed to have a particularly simple (hence light) “correct by construction” approach to proof. This simplicity offers assurance that anything “proved” has really been proved rigorously and is not the artifact of a prover bug. Related content Building machine learning models with encrypted data New approach to homomorphic encryption speeds up the training of encrypted machine learning models sixfold. We follow the same principle of simplicity when we write our implementations in assembly. Writing in assembly is more challenging, but it offers a distinct advantage when proving correctness: our proofs become independent of any compiler. The diagram below shows the process we use to prove x/Ed25519 correct. The process requires two different sets of inputs: first is the algorithm implementation we’re evaluating; second is a proof script that models both the correct mathematical behavior of the algorithm and the behavior of the CPU. The proof is a sequence of functions specific to HOL Light that represent proof strategies and the order in which they should be applied. Writing the proof is not automated and requires developer ingenuity. From the algorithm implementation and the proof script, HOL Light either determines that the implementation is correct or, if unable to do so, fails. HOL Light views the algorithm implementation as a sequence of machine code bytes. Using the supplied specification of CPU instructions and the developer-written strategies in the proof script, HOL Light reasons about the correctness of the execution. CI integration provides assurance that no changes to the algorithm implementation code can be committed to s2n-bignum’s code repository without successfully passing a formal proof of correctness. This part of the correctness proof is automated, and we even implement it inside s2n-bignum’s continuous-integration (CI) workflow. The workflow covered in the CI is highlighted by the red dotted line in the diagram below. CI integration provides assurance that no changes to the algorithm implementation code can be committed to s2n-bignum’s code repository without successfully passing a formal proof of correctness. The CPU instruction specification is one of the most critical ingredients in our correctness proofs. For the proofs to be true in practice, the specification must capture the real-world semantics of each instruction. To improve assurance on this point, we apply randomized testing against the instruction specifications on real hardware, “fuzzing out” inaccuracies. Constant time We designed our implementations and optimizations with security as priority number one. Cryptographic code must strive to be free of side channels that could allow an unauthorized user to extract private information. For example, if the execution time of cryptographic code depends on secret values, then it might be possible to infer those values from execution times. Similarly, if CPU cache behavior depends on secret values, an unauthorized user who shares the cache could infer those values. Our implementations of x/Ed25519 are designed with constant time in mind. They perform exactly the same sequence of basic CPU instructions regardless of the input values, and they avoid any CPU instructions that might have data-dependent timing. Using x/Ed25519 optimizations in applications AWS uses AWS-LC extensively to power cryptographic operations in a diverse set of AWS service subsystems. You can take advantage of the x/Ed25519 optimizations presented in this blog by using AWS-LC in your application(s). Visit AWS-LC on Github to learn more about how you can integrate AWS-LC into your application. To allow easier integration for developers, AWS has created bindings from AWS-LC to multiple programming languages. These bindings expose cryptographic functionality from AWS-LC through well-defined APIs, removing the need to reimplement cryptographic algorithms in higher-level programming languages. At present, AWS has open-sourced bindings for Java and Rust — the Amazon Corretto Cryptographic Provider (ACCP) for Java, and AWS-LC for Rust (aws-lc-rs). Furthermore, we have contributed patches allowing CPython to build against AWS-LC and use it for all cryptography in the Python standard library. Below we highlight some of the open-source projects that are already using AWS-LC to meet their cryptographic needs. Open-source projects using AWS-LC to meet their cryptographic needs. We are not done yet. We continue our efforts to improve x/Ed25519 performance as well as pursuing optimizations for other cryptographic algorithms supported by s2n-bignum and AWS-LC. Follow the s2n-bignum and AWS-LC repositories for updates. Research areas Automated reasoning Tags Cryptography Post-quantum cryptography Provable security About the Author Torben Hansen Torben Hansen is an applied scientist with AWS Cryptography. John Harrison John Harrison is a senior principal applied scientist in Amazon’s Automated Reasoning Group. He is a maintainer of s2n-bignum and the HOL Light theorem prover.",
    "commentLink": "https://news.ycombinator.com/item?id=41527675",
    "commentBody": "Better-performing “25519” elliptic-curve cryptography (amazon.science)183 points by lemaudit 16 hours agohidepastfavorite68 comments notfed 19 minutes ago> The x25519 algorithm also plays a role in post-quantum safe cryptographic solutions, having been included as the classical algorithm in the TLS 1.3 and SSH hybrid scheme specifications for post-quantum key agreement. Really though? This mostly-untrue statement is the line that warrants adding hashtag #post-quantum-cryptography to the blogpost? reply SEJeff 14 hours agoprevThe firedancer team at one of the better HFT firms wrote an AVX512 optimized implementation of ed25519 and X25519 that’s significantly faster than OpenSSL. https://github.com/firedancer-io/firedancer/pull/716 Ditto for sha256: https://github.com/firedancer-io/firedancer/pull/778 And sha512: https://github.com/firedancer-io/firedancer/pull/760 If you’re an optimization nerd, this codebase is wild. reply syzygyhack 10 hours agoparentI laughed a little at calling Firedancer contributors \"a team at a HFT firm\". Not that you are technically wrong, not at all, that's where Jump came from. It's just that this is all completely blockchain-driven optimization, but the b-word is so dirty now that we've gotta go back to using TradFi for the rep. reply SEJeff 6 hours agorootparentIt’s an optimization in hashing algorithms that is around twice as fast as the ones Amazon is posting in thus article for the same eliptic curves. If the Amazon improvements are hacker news worthy (they are) this seems reasonable contextually. Also, I worked for Jump for almost 12 years :) reply webXL 3 hours agorootparentprevWhat makes the “b-word” dirty? reply SEJeff 2 hours agorootparentI didn’t use it because I didn’t find it relevant. They’re using hashing and EC algorithms and they’re improving them. reply jandrese 2 hours agorootparentprevIt's hard to separate from the sea of grifters, con men, cranks, and scammers that infest the domain. Just using the word is a yellow flag that you might be some kind of whacko, even if all you really want to talk about is the math. People have to forever be on guard that you might at any point pivot to all taxation is theft or how you have formed your own micro nation that consists entirely of yourself and thus have diplomatic immunity from all prosecution. Because it happens. Or maybe you have a once in a lifetime deal to buy this receipt like object for some hideous art that is guaranteed to appreciate in value millions of percent. It's just the crowd that has aggregated around crypto currencies includes a lot of untrustworthy people. reply ShroudedNight 3 hours agorootparentprevThe trough of disillusionment carved out by grifters burning the peat of enthusiasm unsustainably. reply nly 4 hours agoparentprevA lot of slowness comes typically comes from wanting to avoid methods that enable side-channel timing attacks reply inopinatus 3 hours agoparentprevI see they learned clang’s dirty little secret over intrinsics viz. that in producing the IR it deviates (sometimes dramatically when AVX-512 is concerned) from the documented opcodes and the results are inevitably detrimental. reply electricshampo1 11 hours agoparentprevCompletely agree re: firedancer codebase. There is a level of thought and discipline wrt performance that I have never seen anywhere else. reply dhx 4 hours agorootparentIt's much more than just performance they've thought about. Here are some of the secure programming practices that have been implemented: /* All the functions in this file are considered \"secure\", specifically: - Constant time in the input, i.e. the input can be a secret[2] - Small and auditable code base, incl. simple types - Either, no local variables = no need to clear them before exit (most functions) - Or, only static allocation + clear local variable before exit (fd_ed25519_scalar_mul_base_const_time) - Clear registers via FD_FN_SENSITIVE[3] - C safety */ libsodium[4] implements similar mechanisms, and Linux kernel encryption code does too (example: use of kfree_sensitive)[5]. However, firedancer appears to better avoid moving secrets outside of CPU registers, and [3] explains that libraries such as libsodium have inadequate zeroisation, something which firedancer claims to improve upon. [1] https://github.com/firedancer-io/firedancer/blob/main/src/ba... [2] https://en.wikipedia.org/wiki/Elliptic_curve_point_multiplic... [3] https://eprint.iacr.org/2023/1713 [4] https://libsodium.gitbook.io/doc/internals#security-first [5] https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/lin... reply tptacek 4 hours agorootparentThese are table stakes for core cryptographic code, and SOT crypto code --- like the Amazon implementation this story is about --- tend at this point all to be derived from formal methods. reply dhx 3 hours agorootparentAs an example, the Amazon implementation doesn't refer to gcc's[1] and clang's[2] \"zero_call_used_regs\" to zeroise CPU registers upon return or exception of functions working on crypto secrets. OpenSSL doesn't either.[3] firedancer _does_ use \"zero_call_used_regs\" to allow gcc/clang to zeroise used CPU registers.[9] As another example, the Amazon implementation also doesn't refer to gcc's \"strub\" attribute which zeroises the function's stack upon return or exception of functions working on crypto secrets.[4][5] OpenSSL doesn't either.[3] firedancer _does_ use the \"strub\" attribute to allow gcc to zeroise the function's stack.[9] Is there a performance impact? [6] has the overhead at 0% for X25519 for implementing CPU register and stack zeroisation. Compiling the Linux kernel with \"CONFIG_ZERO_CALL_USED_REGS=1\" for x64_64 (impacting all kernel functions) was found to result in a 1-1.5% performance penalty.[7][8] [1] https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attribute... [2] https://clang.llvm.org/docs/AttributeReference.html#zero-cal... [3] https://github.com/openssl/openssl/discussions/24321 [4] https://gcc.gnu.org/onlinedocs/gcc-14.2.0/gcc/Common-Type-At... [5] https://gcc.gnu.org/onlinedocs/gcc/Stack-Scrubbing.html [6] https://eprint.iacr.org/2023/1713.pdf [7] https://www.phoronix.com/review/zero-used-regs/5 [8] https://lore.kernel.org/lkml/20210505191804.4015873-1-keesco... [9] FD_FN_UNSANITIZED: https://github.com/firedancer-io/firedancer/blob/master/src/... reply jandrese 2 hours agorootparentZeroizing a register seems pretty straightforward. Zeroizing any cache that it may have touched seems a lot more complex. I guess that's why they work so hard to keep everything in registers. Lucky for them we aren't in the x86 era anymore and there are a useful number of registers. I'll need to read up on how they avoid context switches while their registers are loaded. reply SEJeff 6 hours agorootparentprevThat team is full of world experts in high performance computing. reply pantalaimon 7 hours agoparentprevThat looks really neat, but I still don't understand what firedancer actually is - what is a validator client for Solana and why does it need it's own crypto library? reply SEJeff 6 hours agorootparentIt’s a new from scratch implementation of a validator for Solana the fastest blockchain by several orders of magnitude. The slowest part is signature verification so they sped up hashing to improve performance of the entire system. They follow a first principles approach (the lead has a few physics degrees) and opted to speed up the cryptography. The beauty of this, despite the bad views on blockchain, is that they freaking sped up the cryptography of commonly used algorithms more than anything open or closed source that I personally am aware of. It’s a win in cryptography, much like this Amazon post is, except it’s slower than the firedancer implementation. reply scrlk 4 hours agorootparentOff topic - is Firedancer going to survive Jump winding down its crypto arm? Kanav left, they liquidated a huge staked ETH position a few months ago (+ a bunch of other coins), and the SEC/CFTC is all over them for the Terra Luna fiasco. reply SEJeff 3 hours agorootparentRumors of Jump’s demise are greatly exaggerated. Check the torrent of firedancer talks at Solana Breakpoint literally next week and decide yourself. Folks have said jump is gonna die for 20+ years. They’ve been around 30ish… reply caned 2 hours agorootparentprev> The beauty of this, despite the bad views on blockchain, is that they freaking sped up the cryptography of commonly used algorithms more than anything open or closed source that I personally am aware of. For users that have AVX-512, which isn't widely available (AMD Zen 4 / Zen 5, Sapphire Rapids)... reply SEJeff 2 hours agorootparentSure, and cpus supporting it will proliferate. Shockingly to no one reading hacker news... Both software and hardware continue to improve with time generally speaking. This was a huge software improvement on hardware that supports that functionality. It is a huge win for anyone wanting to use these algorithms where they can afford hardware that supports it. We should celebrate Amazon's improvements and we should celebrate these improvements. Both are great for the future of technology, regardless of why they were initially developed. Improving tech and keeping it open source is good for all. reply slt2021 2 hours agoparentprevwow amazing, nobody is gonna edit that code ever again... reply 4gotunameagain 9 hours agoparentprevSo many manhours spent on finding better ways to shovel around money and pocket what falls from the cracks. What a wasteful and unproductive enterprise, considering the vast majority of the devised improvements never see the public eye. Still, impressive work. Imagine if those brilliant minds behind this were focused somewhere else. reply posnet 9 hours agorootparentThe greatest minds of our generation spend their time thinking about how to: - make people click on ads - make trading algos faster - replace human artists - build more efficient killing machines - destroy any remaining concept of privacy reply vladms 7 hours agorootparentThis directly implies that all the people that did useful stuff (improving cancer survivability, new vaccines, renewable energy, and others) are all \"below\" the \"greatest minds of our generation\". Not to mention it also suggests there is a way to \"compare\" minds. I would not choose myself to do somethings, but that does not mean I despise automatically people choosing to. reply 4gotunameagain 9 hours agorootparentprevHey, at least we had one of them working on TempleOS. reply toast0 3 hours agorootparentprevIt doesn't seem wasteful and unproductive, given that the result of the HFT industry is smaller bid/ask spreads (lowering costs for all trades) and payment for order flow which is the mechanism that eliminated retail commissions and provides price improvement on many retail trades. And even so, HFT firms are making money. It might not seem like real work, but making money by reducing costs of market participants sounds like a good thing. I admit though, block trades might be harder now than before the rise of HFT. If you could do warehousing/distributing/coordinating fresh foods in a way that reduced the difference in price between the farmer and the consumer and make money doing it, that would clearly be good work. reply appendix-rock 6 hours agorootparentprevWhat do you work on? reply almostgotcaught 6 hours agorootparentprevI'll never be able to figure out what people get from repeating the same thing over and over. I've seen this same exact comment 1000 times on hn and I'm 100% sure you have too (indeed I believe the reason you repeat is because you've seen it and agree with it). reply XorNot 1 hour agorootparentIt's virtue signalling. reply aseipp 2 hours agoprevI was aware of s2n-bignum which is a very cool project, but apparently there is a larger sister project, aws-lc, that aims for broader set of APIs including OpenSSL compatibility, while retaining the general approach and vibe (lots of formal verification + performance work): https://github.com/aws/aws-lc That's pretty sweet. I'm currently using BoringSSL in a project as a supplement to OpenSSL (mostly because it is much easier to build for Windows users than requiring them to fiddle with msys2/vcpkg etc; the alternative is to rely on the Windows CNG API, but it lacks features like ed25519 support.) I wonder how much effort it would take to use aws-lc instead... Not that I'm that interested, BSSL is pretty good, but free performance and heavy automated verification is always nice :) Related: one of the authors of this post, John Harrison, wrote a really good book about automated theorm proving about 15 years ago while working on floating point verification at Intel -- there's still no other book quite like this one, I think https://www.cl.cam.ac.uk/~jrh13/ reply newman314 1 hour agoparentUpon hearing about AWS-LC, I immediately thought about tying it to nginx to see if it will work. Turns out someone else has already tried: https://github.com/aws/aws-lc/issues/1827 reply nanolith 15 hours agoprevThe formal methods nerd in me is happy to see HOL Light being used to formally verify this implementation. I'm curious to see how closely their abstract machine models follow specific machine implementations. OOO, speculation, and deep pipelining have non-trivial impacts on potential side channels, and these vary quite a bit by stepping and architecture. reply holowoodman 9 hours agoparentEven worse: Each new CPU generation will need a new machine model and a reevaluation. Because OOO, speculation and all the timing behaviour are non-functional properties that frequently change due to new optimizations, different internal structuring, etc. reply saghm 14 hours agoprevMy (probably naive) understanding is that 25519 already provided better performance than other algorithms used for similar purposes (e.g. RSA) when tuned for a roughly similar level of security; anecdotally, generating 2048-bit or larger RSA keys for me tends to be a lot slower than ed25519. At times I've run into places that require me to use RSA keys though (ironically, I seem to remember first experiencing this with AWS years back, although I honestly can't recall if this is still the case or not). If this further improvement becomes widely used, it would be interesting to see if it's enough to tip the scales towards ed25519 being more of the de facto \"default\" ssh key algorithm. My experience is that a decent number of people still use RSA keys most of the time, but I don't feel like I have nearly enough of a sample size to conclude anything significant from that. reply scrapheap 12 hours agoparent> My experience is that a decent number of people still use RSA keys most of the time, but I don't feel like I have nearly enough of a sample size to conclude anything significant from that. I wouldn't be surprised if a lot of people still use RSA for SSH keys for one or more of the following reasons: 1. A lot of tutorials about generating SSH Keys were written before ed25519, so if they follow an old tutorial they'll probably be generating an RSA key. 2. Older versions of OpenSSH, that you'd find on CentOS 7 and below, would default to RSA if you didn't specify a key type when running ssh-keygen. 3. There are some systems out there that don't support ed25519, though they are becoming rarer. If you have to deal with those systems then you're forced to use RSA (at least for that system). 4. Some of us have been using SSH keys from way before OpenSSH add support for ed25519 keys in 2014, so any long lived SSH keys won't be ed25519 keys (wow, ed25519 has now been about in OpenSSH for over 10 years). reply pantalaimon 7 hours agorootparent> 3. There are some systems out there that don't support ed25519, though they are becoming rarer. If you have to deal with those systems then you're forced to use RSA (at least for that system). Azure Devops is a big one. reply miki123211 9 hours agorootparentprev5. a lot of people (especially older people I suspect) think \"RSA\" when they hear \"public key cryptography\". I'm in my twenties and still have that reaction. I know elliptic curves exist, I even sort-of-kind-of have an awareness of how they work, but if I was asked to name one cryptosystem that used public and private keys, I'd definitely say RSA first and not elliptic curves. reply vitus 8 hours agorootparentThis is likely in no small part due to CS education only really teaching the mechanics of RSA (modular arithmetic, Fermat's little theorem, etc), or at least, that still seems to be the case at Berkeley. I'd guess because elliptic curve crypto requires more advanced math to reason about (more advanced group theory, at least) and doesn't map as cleanly to existing concepts that non-math-major undergrads have. cryptopals.com also doesn't cover any elliptive curve crypto until you get into the last set. reply tptacek 4 hours agorootparentWe didn't even cover RSA until the original last set. It's a build-up. :) reply throw0101b 6 hours agorootparentprevI would think that the (non-EC) Diffie-Hellman would also be easy enough to teach as well: exponentials and discrete log problem aren't any/much complicated than explaining factorization. reply Spooky23 8 hours agorootparentprevIf you interact with government or some large entities that do business with government, they have to comply with FIPS 140-2, and cannot use ed25519. reply vitus 7 hours agorootparent> If you interact with government or some large entities that do business with government, they have to comply with FIPS 140-2, and cannot use ed25519. Not even when FIPS 140-3 was (finally) finalized in 2019, and testing began in 2020? https://csrc.nist.gov/projects/cryptographic-module-validati... includes mentions of EdDSA, and Curve25519 is listed among the \"Recommended Curves for U.S. Federal Government Use\" on page 15 of https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.S.... (I guess the problem is that various crypto implementations need to get recertified under the new standard...) edit: it looks like AWS-LC [0] and boringcrypto [1] have both been validated under FIPS 140-3. Azure's OpenSSL crypto [2] has only been validated under FIPS 140-2 as far as I can tell. [0] https://csrc.nist.gov/projects/cryptographic-module-validati... [1] https://csrc.nist.gov/projects/cryptographic-module-validati... [2] https://csrc.nist.gov/projects/cryptographic-module-validati... reply devman0 4 hours agorootparentpreved25519 is in FIPS 186-5 which meets criteria (a) for Approved Security Functions in FIPS 140-2. reply loudmax 7 hours agorootparentprevWhen I run `ssh-keygen`, I can remember the options `-t rsa` or `-t dsa`. I simply cannot remember the flag `-t ed25519`. I have to look it up every time. I just remember the flag as being vaguely similar the name of the monster robot from RoboCop. reply homebrewer 7 hours agorootparentUse shell autocomplete. Even `bash-completion` suggests arguments for the key type these days: $ ssh-keygen -tdsa ecdsa ecdsa-sk ed25519 ed25519-sk rsa `-sk` is short for \"security key\" if memory serves me right, and is used with FIDO2 hardware tokens. reply throw0101b 6 hours agorootparentprev> When I run `ssh-keygen` As of OpenSSH 9.5 the default has changed, so you don't have to specify anything: * ssh-keygen(1): generate Ed25519 keys by default. Ed25519 public keys are very convenient due to their small size. Ed25519 keys are specified in RFC 8709 and OpenSSH has supported them since version 6.5 (January 2014). * https://www.openssh.com/txt/release-9.5 reply saghm 3 hours agorootparentOh wow, I didn't know that one! I still manually type `-t ed25519` from muscle memory, I guess I don't have to now reply toast0 13 hours agoparentprev> My (probably naive) understanding is that 25519 already provided better performance than other algorithms used for similar purposes (e.g. RSA) when tuned for a roughly similar level of security; anecdotally, generating 2048-bit or larger RSA keys for me tends to be a lot slower than ed25519. My also naive (an possibly out of date) understanding is key generation is much faster in with ecc, and that signing is faster too, but verifying is faster for rsa. So switching from a RSA to an ECC server certificate saves bytes on the wire, because keys are smaller, and saves server cpu because signing is faster, but may increase client cpu because verification is slower. The byte savings may make up for the increase in cpu though. reply saghm 3 hours agorootparent> My also naive (an possibly out of date) understanding is key generation is much faster in with ecc, and that signing is faster too, but verifying is faster for rsa. So switching from a RSA to an ECC server certificate saves bytes on the wire, because keys are smaller, and saves server cpu because signing is faster, but may increase client cpu because verification is slower. The byte savings may make up for the increase in cpu though. Interesting! I wonder if this new algorithm is intended to help with that. I'm super curious if the smaller payload does indeed make a difference (with the current algorithm) like you mention; I know that with databases and filesystems, compression is commonly used to shift the balance from I/O to CPU due to disk writes being slow (with reduced storage size being a side benefit but not usually the main motivation), but I also know that cryptographic verification being too slow can be an anti-feature if it makes brute forcing feasible, so the amount of CPU work needed might be pretty high still. reply stouset 14 hours agoparentprev> anecdotally, generating 2048-bit or larger RSA keys for me tends to be a lot slower than ed25519 That’s not really anecdotal. Generating an ed25519 key is barely more than generating a random 256-bit value. Generating an RSA key is significantly more work. reply saghm 14 hours agorootparentI did say my understanding was probably naive; I didn't know the details to be able to assert anything beyond my own observation! reply stouset 13 hours agorootparentYep, not faulting you at all! I too was surprised when I found out that it’s a straight 256-bit random value with a few bits masked. reply saghm 3 hours agorootparentI pretty quickly realized in college when learning about this stuff that the math was well over my head, and I shifted my focus more to understanding how to properly use cryptography rather than implement it (which turned out to be more important as a software engineer anyhow). In retrospect, I really appreciate how the professor I had in a security-focused course explicitly told us it was okay if we didn't understand the math and wouldn't be tested on it when going over how it worked. reply tptacek 2 hours agorootparentCounterpoint: it's not OK to skip the math with cryptography. You may not need to power through all of Silverman's curve book (though: I don't know for sure that's true, which is why I don't call myself a cryptography engineer), but you have to get as deep into the math as you can in order to safely use cryptographic algorithms. If you're math-avoidant, stick with high-level abstractions like NaCL and TLS. There's nothing wrong with that! A professor talking about and demonstrating cryptography at the level of individual algorithms is doing their class a disservice if they say \"none of the math will be on the test\". The algorithms are enough to put something together that seems like it works; the math is what you need to find out if your resulting system actually does work. It's where many of the fun bug classes live. reply saghm 58 minutes agorootparentI'm not sure if you're reading more into what I said than I intended, but I'm not convinced by this argument. You might have missed that this course was on security in general, not cryptography; not everything in the course was cryptographic related. That said, I'd argue that for the vast majority of software engineers the type of stuff they're dealing with can be dealt with without needing to know the math. For example, you don't need to understand the math to behind the algorithms to know that bcrypt is a reasonable password hashing algorithm and that sha1 and md5 are not, or that salts are used to mitigate issues when users reuse passwords. These are principles that you can understand at a high level without fully understanding the underlying details. If anything, I think that overemphasis on requiring people to learn and understand the math has the effect of over-focusing on simpler algorithms that aren't actually what people want to be using in practice due to the fact that they're easier to teach and often foundational in conveying concepts that would need to be learned to understand the more complicated algorithms. If using cryptographic algorithms directly requires knowing the math, then I'd agree that most people shouldn't be using them directly, but I'd go further and say that a lack of libraries that are safe for people to use for software engineering without understanding the implementation is a failing of the ecosystem; as much as \"regular\" software engineering people (like myself!) can struggle with the math behind cryptography, I think that a lot of people developing cryptographic libraries struggle with building reasonable abstractions and making user-friendly APIs (which is a skill I think in general is not emphasized enough for most software engineers, to the detriment of everyone). reply tptacek 45 minutes agorootparentSure. It's a failing of the ecosystem. That observation, a cup of coffee, and 1-3 years will get you a Kenny Paterson paper stunt-breaking your system. I feel where you're coming from, but, respectfully: it does not matter. My thing here is just: learn the math! Or do something else. I did! There is so much to do in our industry. reply upofadown 7 hours agoparentprevAnother article from the same blog about optimizing RSA: * https://www.amazon.science/blog/formal-verification-makes-rs... RSA signature verification is already very fast and TLS doesn't use RSA for encryption anymore so the problem reduces to optimizing signing operations. reply jonmon6691 3 hours agoprevI'm assuming when they say that this improves user experience, that it implies the use case is primarily TLS. In which case store-now-decrypt-later attacks are already considered an urgent threat with regard to post quantum crypto. With FIPS 203 being released and Chrome is already using an implementation based on the draft standard, this seems like this algo (at least for TLS) should be on its way out. reply dlgeek 3 hours agoparentThe industry is moving to a hybrid that mixes classic crypto (including ECC) with post-quantum crypto. AWS has even turned this on in some places - https://aws.amazon.com/about-aws/whats-new/2022/03/aws-kms-a... from 2022 and https://docs.aws.amazon.com/kms/latest/developerguide/pqtls.... for some details. reply jonmon6691 2 hours agorootparentThanks I forgot about that. So if understand it right, the idea is to provide some insurance in the case that these relatively young algorithms are broken as they get exposed to more and more cryptanalysis reply adgjlsfhk1 2 hours agoparentprevNo one other than NIST is recommending phasing out pre-quantum crypto. Everyone else is using a combination of pre-quantum and post-quantum because trust in the security and robustness of the post-quantum ecosystem is fairly low. reply fefe23 5 hours agoprevHoly shit these claims are wild! It's not just a percent more performance here and there, the graphs look more like 50% more throughput on the same hardware (depending on the cpu architecture). My immediate fear was that they optimized away the security features like absence of timing side channels, but they say they still have those. They also claim to have formal proof of correctness, which is even more amazing, because they are not doing it on a symbolic level but on a machine instruction level. Apparently they tought their reasoning system the semantics of all the CPU instructions used in the assembler implementation. I'll still wait what djb has to say about this, but it looks freaking amazing to me. reply londons_explore 8 hours agoprev [–] Does 25519 suffer from key/data-dependant execution time? Is this implementation resistant to that? If it isn't, it's kinda a footgun which shouldn't be published for general use. reply vitus 7 hours agoparent [–] > Does 25519 suffer from key/data-dependant execution time? I mean, when implemented naively, yes, but the industry has been aware of timing attacks for decades such that this is table stakes for any crypto implementations. From the article: > We also do our best to execute the algorithms in constant time, to thwart side-channel attacks that infer secret information from the durations of computations. https://github.com/awslabs/s2n-bignum (where most of the heavy lifting is done, per the article) further explicitly states that \"Each function is moreover written in a constant-time style to avoid timing side-channels.\" reply deathanatos 4 hours agorootparent> but the industry has been aware of timing attacks for decades such that this is table stakes for any crypto implementations. When I see CVE-fests like — https://people.redhat.com/~hkario/marvin/ — … I just do not come away with that impression. [Widely used] Cryptographic Rust crates offering \"constant time\" operations in \"pure Rust\" — but Rust has no primitives for doing constant time operations, so it's only through hopes and prayers that it might actually work, and with no guarantee anywhere that it actually should. (Other, less timing attack related stuff, but e.g., major companies still not supporting anything beyond RSA.) reply justinwsmith 5 hours agorootparentprev [–] The next paragraph makes a slightly stronger statement about its constant-time'ness: > Our implementations of x/Ed25519 are designed with constant time in mind. They perform exactly the same sequence of basic CPU instructions regardless of the input values, and they avoid any CPU instructions that might have data-dependent timing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AWS has enhanced the performance and correctness of \"25519\" elliptic-curve cryptography in its open-source library, AWS LibCrypto (AWS-LC), through automated reasoning and CPU-specific optimizations.",
      "These improvements, based on Google's BoringSSL, include significant performance gains for x25519 and Ed25519 algorithms on x86_64 and Arm64 CPUs, with Ed25519 signing operations seeing a 108% increase and x25519 operations improving by 113%.",
      "The enhancements ensure constant-time execution to prevent side-channel attacks, with correctness verified by the s2n-bignum library and HOL Light theorem prover, making AWS-LC a robust choice for secure cryptographic implementations."
    ],
    "commentSummary": [
      "Amazon's new \"25519\" elliptic-curve cryptography demonstrates significant performance improvements, particularly with an AVX512 optimized implementation by the Firedancer team outperforming OpenSSL.",
      "The x25519 algorithm is used in TLS 1.3 and SSH hybrid schemes for post-quantum key agreement, highlighting its importance in modern cryptographic protocols.",
      "Firedancer's codebase, known for blockchain optimization, is praised for its performance and secure programming practices, contributing to the broader adoption of ed25519 over RSA for SSH keys due to better performance, security, and compatibility."
    ],
    "points": 183,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1726195912
  },
  {
    "id": 41532946,
    "title": "Zero-Click Calendar invite – Critical zero-click vulnerability chain in macOS",
    "originLink": "https://mikko-kenttala.medium.com/zero-click-calendar-invite-critical-zero-click-vulnerability-chain-in-macos-a7a434fc887b",
    "originBody": "Zero-Click Calendar invite — Critical zero-click vulnerability chain in macOS Mikko Kenttälä · Follow 6 min read · 1 day ago -- Summary (TL;DR) I found a zero-click vulnerability in macOS Calendar, which allows an attacker to add or delete arbitrary files inside the Calendar sandbox environment. This could lead to many bad things including malicious code execution which can be combined with security protection evasion with Photos to compromise users’ sensitive Photos iCloud Photos data. Apple has fixed all of the vulnerabilities between October 2022 and September 2023. If you like to see my presentation video about this, you can check my Disobey 2024 presentation: https://www.youtube.com/watch?v=9NlQXLLQrvk Vulnerability details Phase 1: Arbitrary file write and delete vulnerability in Calendar (CVE-2022–46723) An attacker can send malicious calendar invites to the victim that include file attachments. The filename of the attachment is not properly sanitized. The attacker can exploit this to conduct a successful directory traversal attack by setting an arbitrary path to a file in the ATTACH section with: “FILENAME=../../../PoC.txt”. This will cause the file to be added to ~/Library/Calendar/PoC.txt instead of ~/Library/Calendar/[CalendarID]/Attachments/[eventid]/ . If the attacker-specified file already exists, then the specified file will be saved with the name “PoC.txt-2”. However, if the event/attachment sent by the attacker is later deleted the file with the original name (PoC.txt) will be removed. This vulnerability can be used to remove existing files from the filesystem (inside the filesystem “sandbox”). Vulnerability seems to exist at least in latest macOS Montrey 12.5. MacOS 13.0 beta4 doesn’t seem to be vulnerable anymore. Phase 2: Leveraging the arbitrary file write vulnerability to gain Remote Code Execution (RCE) When this vulnerability was found Ventura was about to be released. macOS version upgrade process can be leveraged to gain remote code execution via the Open File functionality in Calendar. In order to gain RCE we will exploit the previously discovered arbitrary file write vulnerability to infect the calendar with multiple files. When combined, they will trigger the RCE exploit when macOS Monterey is upgraded to Ventura. Injected file #1: 000Hacked-$RANDOM.calendar This file contains calendar data which looks like “Siri Suggested” -calendar. Suggested repeating event with alert-functionality. This will open other injected files. Injected file #2: CalendarTruthFileMigrationInProgress file This file will make sure that existing calendars in the old calendar format will be upgraded and merged to the new database. Injected file #3: CalPoCInit.dmg An alert embedded in the Calendar event in the injected file #1 will trigger opening the file ~/Library/Calendars/CalPoCInit.dmg CalPoCInit.dmg includes a reference to a background image which will point to an external samba server. This reference is a Bookmark in the .DS_Store. It’s stored inside the root of this image file. Mount will happen without a quarantine flag even though CalPoCInit.dmg mount is in quarantine. Injected file #4: stage1.url Second alert from the calendar event embedded in the injected file #1 will open the file ~/Library/Calendars/stage1.url This file stage1.url includes an URL to an application inside the previously mounted samba mount triggered by the injected file #3. This URL is file:///Volumes/CalPoCPayload/MyMidiTest.app. Malicious application This will cause Finder to open /Volumes/CalPoCPayload/, which will automatically trigger indexing. This will cause MyMidiTest.app to be indexed. This is needed for the unique URL registration to happen. Inside our application bundle the MyMidiTest.app/Contents/Info.plist file includes a trigger that will cause a handler to be registered for a custom URL Type: CFBundleURLTypes CFBundleURLName MyMidi URL CFBundleURLSchemes mymiditestInjected file #5: stage2.url Third and the last alert from from the calendar event embedded in the injected file #1 will open the file ~/Library/Calendar/stage2.url which includes a reference to the custom url “mymiditest://” which will launch our malicious application without any user interaction whatsoever. This is possible because the application is inside the samba mount from our exploit which does not have a quarantine flag. mymiditest app will write files needed for the Phase 3 to /var/tmp/ and launch the script in Terminal with “open /var/tmp/PhotosPoC.sh”. The objective is to make a bit more room on our sandbox. Phase 3: Getting access to sensitive Photos data In order to demonstrate the superpowers gained by the previous two phases of the exploit chain, I chose to abuse Photos to leak sensitive user data, namely private pictures. Access to users’ sensitive files like Photos should be restricted and blocked by TCC. However, by having the exploit change the configuration of Photos it is possible to get access to pictures which are stored on iCloud. This is done by changing the System Photo Library to point to a path which is not protected by TCC. Changing configuration for Photos to get access to iCloud files The attacker can create a configuration file for Photos which uses a different System Photo Library for Photos. Configuration can be imported via “defaults import”. Phase 2 of this PoC exploit chain has prepared and automatically dropped a configuration which uses /var/tmp/mypictures/Syndication.photoslibrary as a System Photo Library for Photos, cloudphotod and photolibraryd. You can find these malicious configurations created automatically in Phase 2 from the /var/tmp/mypictures/*.plist. When the exploit chain runs PhotosPoC.sh, all running Photos-related applications will be killed and a new configuration will be imported with defaults. Original configuration will be backed up to /var/tmp/mypictures/*-orig.plist Malicious configuration will be imported from /var/tmp/mypictures/ . New photo library will be opened by the exploit chain in Photos with “open /var/tmp/mypictures/Syndication.photoslibrary. Syndication.photoslibrary is an empty template library. Now Photos will be running with the new System Photo Library and iCloud sync will be enabled and original files will be downloaded to an unprotected directory. When those files are synchronized to disk they will be copied to a new directory in /var/tmp/PoCLoot$RANDOM/. This sensitive and private data could also be copied to external resources and data could be posted to external web-servers with the curl command by the exploit with trivial modifications. Full Chain Exploitation chain To exploit the entire chain, it’s necessary to navigate through numerous steps to overcome all the security obstacles in macOS. Initially, we inject multiple files to evade the sandbox, and activate subsequent phases of the exploit chain. Then, we must bypass Gatekeeper mitigation using the SMB trick to execute arbitrary code. The final challenge to overcome is the TCC protection, enabling access to sensitive data such as iCloud Photos. Before fixes were done, I was able to send malicious calendar invitations to any Apple iCloud user and steal their iCloud Photos without any user interaction. Thanks to everyone who helped me to get here! You know who you are. Timeline 2022–08–08: Arbitrary file write and delete in Calendar sandbox reported 2022–10–24: (No CVE) fixed in macOS Monterey 12.6.1 and Ventura 13 (Ventura beta3 was vulnerable) 2022–11–14: PoC sent, how to leverage Calendar vulnerability to get arbitrary code execution (gatekeeper evasion) 2022–12–04: PoC sent, how to get access to iCloud photos 2023–02–20: CVE-2022–46723 Credits and CVE added for Calendar vulnerability (bounty state still unknown) 2023–03–27: Gatekeeper evasion fixed in macOS Ventura 13.3 (No CVE or credits) 2023–09–26: CVE-2023–40434 Photos vulnerability fixed and credited 2023–10–09: Bug bounty announced related to gatekeeper evasion and Photos vulnerability 2023–12–21: CVE-2023–40433 Gatekeeper evasion credited 2024–09–12: Still no bounty related to original arbitrary file write and delete vulnerability (CVE-2022–46723)",
    "commentLink": "https://news.ycombinator.com/item?id=41532946",
    "commentBody": "Zero-Click Calendar invite – Critical zero-click vulnerability chain in macOS (mikko-kenttala.medium.com)171 points by jviide 2 hours agohidepastfavorite29 comments cyrnel 1 hour agoAh another way to mess with the quarantine flags, the other being: https://imlzq.com/apple/macos/2024/08/24/Unveiling-Mac-Secur... Seems just way too many different systems have the ability to modify those flags. reply autoexec 1 hour agoprev> An attacker can send malicious calendar invites to the victim that include file attachments...Before fixes were done, I was able to send malicious calendar invitations to any Apple iCloud user and steal their iCloud Photos without any user interaction. What's the scope of this? Can anyone on macOS anywhere really just send random invites to anyone else who uses icloud? Who would even want that? reply bobbylarrybobby 0 minutes agoparentIs g cal not the same? reply s_dev 54 minutes agoparentprevNot to be smart -- but how else would invites work? reply autoexec 47 minutes agorootparentI'd want to whitelist specific people before they could send me a calendar invite. Every other invite request should never reach my device. If I don't even know you, why would I want your invites anyway? reply vasco 45 minutes agorootparentprevHow often do you get a calendar invite from a person who you never interacted through email before and don't have in contacts vs the opposite, and actually take the meeting? reply pbhjpbhj 39 minutes agorootparentI, in UK, book things on Eventbrite, they email you with a calendar invite. Same with other booking systems for events IIRC. You can probably add people to an invitation? Maybe if you can exploit such a system then people would have them in their whitelist in any case? A little adjacent to your question but relevant enough I think. reply ivan_gammel 30 minutes agorootparentprevThis is a regular part of the recruiting process, where you may start chatting in LinkedIn and then get an invite on your email. reply currency 14 minutes agorootparentprevI've received Apple Calendar invites containing Chinese characters from individuals I've never heard of. I deleted them, but just receiving them was a bit alarming. reply yardstick 39 minutes agorootparentprevHR / Recruiter setting up interviews? The person doing the inviting might be different from previous calls/emails. Customer meetings I get invited to often come from someone I’ve never dealt with before, but include others who I work with who were responsible for bringing me into it. reply vessenes 1 hour agoprevDon’t love the bounty state here — security researchers, is it typical to wait this long with Apple or other FAANG type companies? reply tptacek 12 minutes agoparentVery yes. reply whitepoplar 1 hour agoprevDoes Lockdown Mode prevent this? reply captainkrtek 33 minutes agoparentTotally speculating, but I’d hope so. After all the prior zero-click image attachment related exploits, which I think lockdown mode was built to address, I’d figure all files are treated in that manner. reply rvz 1 hour agoprevGreat write up. Any guess on the bounty amount for this zero-click vulnerability, with a 5 step exploit chain for macOS? reply edm0nd 25 minutes agoparentDude likely could have sold this to malicious threat actors for 6 figures. Weird that it's been 2 years now and Apple still hasn't paid anything. Really highlights why people might tend to gravitate towards that route instead of going thru the legit bug bounty process. reply tptacek 11 minutes agorootparentDoes it work on an iPhone? If not, you're probably not selling it for 6 figures, or even 5. reply languagehacker 1 hour agoprevSuper interesting, though I doubt they'll pay a bounty on something they've already fixed. reply ziddoap 1 hour agoparent>though I doubt they'll pay a bounty on something they've already fixed. CVE-2022–46723 was reported 2022-08-08 and fixed later on 2022-10-24, which the author of this post was credited by Apple for reporting. reply sgt 1 hour agorootparentSo he likely got the bounty, then, or will get it. Any idea how much it is? reply ziddoap 1 hour agorootparentDefinitely not received yet. >2024–09–12: Still no bounty [...]. Apples bounty payouts are ball-parked here: https://security.apple.com/bounty/categories/ reply hypeatei 1 hour agorootparentRelevant section states: > Zero-click unauthorized access to sensitive data $5,000 to $500,000 reply 0cf8612b2e1e 21 minutes agorootparent$5?!? Really incentivizing selling it on the black market. reply post_break 55 minutes agoprevAnd yet Apple still hasn't paid up. Need to just start selling these to people who will use them at this point. reply rs_rs_rs_rs_rs 56 minutes agoprevCome on Apple, do the right thing, reward the bounty already. reply yrcyrc 1 hour agoprevApple still not paying bounty or needs to be publicly reminded… reply AzzyHN 25 minutes agoprev [–] It sure is a good thing that Apple has fixed all these, and has put out patches for all effected versions, since they care about their users' privacy, right? Right? I know Apple has now switched to 10 years for MacOS, and 7ish years of iOS, but I hope the EU passes some laws to make this a requirement, rather than something a company can choose to provide or not. reply anamexis 21 minutes agoparentYes? As the OP states: 2022–08–08: Arbitrary file write and delete in Calendar sandbox reported 2022–10–24: (No CVE) fixed in macOS Monterey 12.6.1 and Ventura 13 (Ventura beta3 was vulnerable) reply Avamander 20 minutes agoparentprev [–] Apple can increase those times because that's how long it'll take them to patch issues like these. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A zero-click vulnerability in macOS Calendar allowed attackers to add or delete files within the Calendar sandbox, potentially leading to malicious code execution and compromising iCloud Photos data.",
      "Apple fixed these vulnerabilities between October 2022 and September 2023, addressing issues like arbitrary file write/delete, remote code execution, and access to sensitive photos data.",
      "The exploit chain involved multiple steps to bypass macOS security, including sandbox evasion, Gatekeeper bypass, and TCC protection circumvention, with fixes implemented in various macOS updates."
    ],
    "commentSummary": [
      "A critical zero-click vulnerability in macOS allows attackers to send malicious calendar invites with file attachments, potentially stealing iCloud Photos without user interaction.",
      "Users are questioning the security of such invites and suggesting whitelisting specific senders as a precaution.",
      "Apple has been slow to pay bounties for these vulnerabilities, raising concerns about their commitment to user privacy and timely updates."
    ],
    "points": 171,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1726246226
  },
  {
    "id": 41526754,
    "title": "Notepat – Aesthetic Computer",
    "originLink": "https://aesthetic.computer/notepat",
    "originBody": "booting...",
    "commentLink": "https://news.ycombinator.com/item?id=41526754",
    "commentBody": "Notepat – Aesthetic Computer (aesthetic.computer)149 points by justanothersys 19 hours agohidepastfavorite35 comments busymichael 6 minutes agoI'm a big fan of digitpain (Jeffrey Scudder) and it is great to see him get airtime here. The tl/dr for Jeffrey is he make digital art, but first he makes his own digital tools to make that artwork. For those that are new to him, be sure to browse to the top level (https://aesthetic.computer) and play around in the 'terminal'. If you have a VR device, view Freaky Flowers in it. reply tony-allan 14 hours agoprevSome more info https://github.com/whistlegraph/aesthetic-computer https://www.youtube.com/watch?v=S-7UszmI1K4 https://aesthetic.computer/handtime https://digitpain.com/ reply a1o 8 hours agoparentI wasn't expecting that README reply linux2647 3 hours agorootparent2600 lines long, and an additional 1700 lines of completed items moved to a different file: https://github.com/whistlegraph/aesthetic-computer/blob/main... reply seanthemon 8 hours agorootparentprevRefreshingly sadistic way of handling a project checklist, I love it reply wkat4242 2 hours agoprevUhm what is this? It just dumps you into a retro-ish environment but it's not really clear what it's for. reply drivers99 2 hours agoparentI was confused too, but another comment here links to a video demo. That gave me enough clues to realize that you start off in a \"notepat\" app in this \"computer.\" You can click \"notepat\" to go back to a prompt. At the prompt, you can type \"list\" to list all the commands. (I noticed there are aliases as well, like if you type \"help\" you actually go to \"chat\".) The demo shows a few things you can do, and probably you're meant to explore the other apps to see how things work. reply sorrythanks 17 hours agoprevI love it! I'd be fascinated to learn what inspired this absurd keyboard layout! reply actuallyalys 15 hours agoparentI’m sure there’s more logic to it, but there are twelve notes in the chromatic scale, so six rows of four gets you two octaves. Also putting them in rows of four means you get a nice interval (a fifth?) if you go down vertically. reply thrtythreeforty 15 hours agorootparentThirds vertically. Semitones go horizontally. The fifth is a knight's move away down, or up and to the left. Like other isomorphic layouts, it conceptually tiles infinitely in all directions. reply justanothersys 14 hours agorootparentprevif you tap the ' key on ur keyboard it shows an overlay in red and green with the steps down and up which i've been using as a transposition aid reply swayvil 17 hours agoparentprevThe notes go together by color and location. There's some scale logic there reply airstrike 15 hours agoprevOk, this is incredible in an infinite number of ways. reply fitsumbelay 14 hours agoprevLove this site. I know I've seen it before but all my histories are drawing a bl nk reply tempodox 7 hours agoprevHow are the wave forms generated? reply justanothersys 3 hours agoparentthey are generated in a web AudioWorklet on a per sample basis https://github.com/whistlegraph/aesthetic-computer/blob/main... you can use the up and down arrow keys in 'notepat' and press the '/' key to see the waveform output and increase or decrease the sample size to better visualize them reply pooriar 17 hours agoprevI played twinkle twinkle little star reply justanothersys 14 hours agoparentu can enter 'notepat twinkle' after tapping notepat in the corner to actually run through that melody reply BossingAround 11 hours agorootparentNice. If only there was a mario theme song. reply BossingAround 11 hours agoprevLove the chat functionality :)) Very cool. reply CyberDildonics 2 hours agoprevCalling something 'aesthetic' as a direct adjective is not the correct use of 'aesthetic'. reply emchammer 1 hour agoparentThank you for clarifying, CyberDildonics. reply alwa 44 minutes agoparentprevIsn’t the function of the application to compute aesthetic variations on inputs? To be fair the video linked upthread clarified things for me by showing me the app’s full ambition. Like you, I at first thought the whole thing was the keypad app, and that they meant the name in the clunky sense of “it’s a computer, but it’s ‘aesthetic’ because of its glitch-retro style.” reply katdork 15 hours agoprevdoesn't seem to work at all on firefox, but works fine on a chromium-derivative. a warning would've been nice about a lack of support? ^^ reply mscdex 9 hours agoparentIt didn't work for me either, but apparently one of the issues is that it assumes window.speechSynthesis is available which may be disabled via about:config > media.webspeech.synth.enabled. reply wormius 15 hours agoparentprevWindows 10 w/Firefox 130.0, ublock, privacy badger, and works ok for me? I'm not the dev, but figured it might help them if they know more details. You on Linux, Win11, Mac, BSD, Mobile? reply josephg 15 hours agorootparentLinux + Firefox 129.0 here. Works fine for me too! reply tangus 14 hours agoparentprevAndroid + Firefox, works ok. reply fitsumbelay 14 hours agoparentprevworks for me on MacOS 14.6 FF reply greentext 8 hours agoprevfreaky-flowers was cool. How does it work? reply justanothersys 7 minutes agoparentthey are sculptures i made in the oculus browser using a webvr tool on the site reply DriftRegion 16 hours agoprevthis is nice. I love the scale layout reply swayvil 17 hours agoprevI dig it. Made music for 10 minutes. Needs effects boxes. Like one on those looper echo things reply fitsumbelay 14 hours agoparentI wouldn't mind being able to save patterns to play back and manipulate reply fussylogic 14 hours agoprev [–] love the project. made a blog post about it. why? so the seo gods give this project more eyeballs it deserves https://oransblog.com/aesthetic-computer/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "\"Notepat\" is a digital art project by Jeffrey Scudder, accessible via the website aesthetic.computer, featuring a retro computing environment and unique tools for creating digital art.",
      "The project includes interactive elements like a 'notepat' app for music creation, with commands and a distinctive keyboard layout based on the chromatic scale.",
      "Users can explore various features, including VR experiences like \"Freaky Flowers,\" and the project has generated significant interest for its innovative and artistic approach to digital tools."
    ],
    "points": 149,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1726183908
  },
  {
    "id": 41533060,
    "title": "Meta fed its AI on everything adults have publicly posted since 2007",
    "originLink": "https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data",
    "originBody": "Artificial Intelligence/ Tech/ US & World Meta fed its AI on almost everything you’ve posted publicly since 2007 Meta fed its AI on almost everything you’ve posted publicly since 2007 / Unless you’re in the EU, there’s no ability to opt out of AI training settings that keep Facebook or Instagram posts public. By Jess Weatherbed, a news writer focused on creative industries, computing, and internet culture. Jess started her career at TechRadar, covering news and hardware reviews. Sep 12, 2024, 3:04 PM UTC Share this story Illustration by Nick Barclay / The Verge Meta has acknowledged that all text and photos that adult Facebook and Instagram users have publicly published since 2007 have been fed into its artificial intelligence models. Australia’s ABC News reports that Meta’s global privacy director, Melinda Claybaugh, initially rejected claims about user data from 2007 being leveraged for AI training during a local government inquiry about AI adoption before relenting after additional questioning. “The truth of the matter is that unless you have consciously set those posts to private since 2007, Meta has just decided that you will scrape all of the photos and all of the texts from every public post on Instagram or Facebook since 2007 unless there was a conscious decision to set them on private,” Green Party senator David Shoebridge pushed in the inquiry. “That’s the reality, isn’t it?” “Correct,” Claybaugh responded. Meta’s privacy center and blog posts acknowledge hoovering up public posts and comments from Facebook and Instagram to train generative AI: We use public posts and comments on Facebook and Instagram to train generative AI models for these features and for the open source community. We don’t use posts or comments with an audience other than Public for these purposes. But the company has been vague about how data is used, when it started scraping, and how far back its collection goes. Asked by The New York Times in June, Meta didn’t answer, other than to confirm that setting posts to anything besides “public” will prevent future scraping. That still won’t delete data that has already been collected — and people posting back in 2007 (who may have been minors at the time) wouldn’t have known their photos and posts would be used in this way. Related Meta’s future is AI, AI, and more AI How to keep your images out of AI generators Claybaugh said that Meta doesn’t scrape data from users who are under the age of 18. When Labor Party senator Tony Sheldon asked if Meta would scrape the public photos of his children on his own account, Claybaugh confirmed it would and was unable to clarify if the company also scraped adult accounts that were created when the user was still a child. European users can opt out due to local privacy regulations, and Meta was recently banned from using Brazilian personal data for AI training, but the billions of Facebook and Instagram users in other regions can’t opt out if they want to keep their posts public. Claybaugh was unable to say if Australian users (or anyone else) would be given a choice to opt out in the future, arguing that the option was given to European users because of uncertainty regarding its regulatory landscape. “Meta made it clear today that if Australia had these same laws Australians’ data would also have been protected,” Shoebridge said to ABC News. “The government’s failure to act on privacy means companies like Meta are continuing to monetize and exploit pictures and videos of children on Facebook.” Most Popular Most Popular The entire staff of beloved game publisher Annapurna Interactive has reportedly resigned OpenAI releases o1, its first model with ‘reasoning’ abilities The US finally takes aim at truck bloat iFixit made its own USB-C soldering iron, and it’s already a joy Phone sex hotline accidentally featured in 'The Last of Us' Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=41533060",
    "commentBody": "Meta fed its AI on everything adults have publicly posted since 2007 (theverge.com)120 points by elashri 1 hour agohidepastfavorite144 comments mattcantstop 1 hour agoI am very likely in the minority here, but I think AI SHOULD be trained on everything that is in the public sphere. I'd be disappointed if it wasn't trained on everything they had access to. If it is trained on private information, then I would have issue with it. reply trimbo 1 hour agoparentI don't agree because it creates this dilemma for creators: you need to put your work out there to get traction, but if you put your work out there and anything public is fair game, then it will be sampled by a computer and instantly recreated at scale. This might even happen without the operator knowing whose work is being ripped off. Commercial art producers have always ripped off minor artists. They would do it by keeping it very similar to the original but just different enough to avoid being sued. Despite this, I personally know two artists who have sued major companies who ripped off their work for ads, and both won million-plus settlements. Why would we embrace this now that a computer can do it and there's a level of deniability? I don't understand how this benefits anyone. reply jstummbillig 5 minutes agorootparentThis benefits actually everyone. If our combined creative work until this point is what turns out to be necessary to kick-start a great shot at abundance (and if you do not believe that, if it's all for nothing, why care at all about the money wasted on models?) it might simply be our moral obligation to endorse it -- just as is will be the model creators moral obligation to uphold their end of this deal. reply Ukv 36 minutes agorootparentprev> Why would we embrace this now that a computer can do it and there's a level of deniability? Generally I don't think people are arguing that copyright law should be more lenient to AI than it is to humans. If your work gets ripped off (a substantially similar copy not covered by fair use) you can sue regardless of tools used in its creation. Question would be whether machine learning, unlike human learning, should be treated as copyright infringement. There are differences and the law does not inherently need to treat them the same, but it could. As to why it should: I think there's huge benefit across a large range of industries to web-scale pretraining and foundation models, and I'd like it to remain accessible to open-source groups or smaller companies without huge data moats. Realistically I think the alternative would likely just benefit Getty/Universal with near-identical outcomes for most actual artists. When the very basis of copyright is for the \"progress of sciences and useful arts\", it seems backwards to use it in a way that would set back advances in language translation, malware/spam/DDoS filtering, defect detection, voice dictation/transcription, medical image segmentation, etc. reply marcosdumay 31 minutes agorootparent> Question would be whether machine learning, unlike human learning, should be treated as copyright infringement. No, the question is whether those genAI we have around are mass copyrights violation machines or whether they \"learn\" and build non-violating work. And honestly, I have seen evidence pointing both ways. But the \"copyrights protection\" institutions are all quickly to decide the point dismissing any evidence on philosophical basis. reply Ukv 24 minutes agorootparent> No, the question is whether those genAI we have around are mass copyrights violation machines or whether they \"learn\" and build non-violating work. I refer to the training process in question, which may or may not be be violating copyright, as \"machine learning\" since that's the common terminology. Question is whether that process is covered by fair use. Whether or not it actually \"learn\"s is not irrelevant, but I'd say more a philosophical framing than a legal one. reply autoexec 50 minutes agorootparentprev> but if you put your work out there and anything public is fair game, then it will be sampled by a computer and instantly recreated at scale. That's just how the internet works. Don't put something on the internet if you don't want it to be globally distributed and copied. > I personally know two artists who have sued major companies who ripped off their work for ads, and both won million-plus settlements. Ultimately \"AI did it\" should never be allowed to be used as an excuse. If a company pays for a marketing guy who rips off someone's work and they can be sued for it, than a company that pays for an AI that rips off someone's work should still be able to be sued for it. reply trimbo 20 minutes agorootparent> That's just how the internet works. Don't put something on the internet if you don't want it to be globally distributed and copied Until now, this has been an acceptable tradeoff because there's some friction to theft. Directly cloning the work is easy, but that also means an artist can sue or DMCA. It also means the original artist's work can go more viral, which, despite the short-term downsides, can help their popularity long term. The important difference is that imitating an artist's style with new work used to take significant time (hours or days). With an LLM, it takes milliseconds, and that model will be able to churn out the likes of your work millions of times per day, forever. That's the difference, and why the dilemma is new. > Ultimately \"AI did it\" should never be allowed to be used an as an excuse With the exception of an LLM directly plagiarizing, the only way to prove it didn't is by not allowing it to train on something. LLMs are the sum of everything. We could say the same about humans, sure, we are a model trained on everything we've ever seen too. But humans aren't machines who can recreate stuff in the blink of an eye, with nearly perfect recall, at millions of qps. reply autoexec 5 minutes agorootparent> With an LLM, it takes milliseconds, and that model will be able to churning out the likes of your work millions of times per day, forever. AI does cause a lot of problems in terms of scale. The good news is that if AI churns out millions of copies of your copyrighted works you're entitled to compensation for each and every copy. In addition to pushing out copies of people's copyright material, AI is also capable of writing up DMCA notices and legal paperwork. > With the exception of an LLM directly plagiarizing, the only way to prove it didn't is by not allowing it to train on something. LLMs copy everything and nothing at the same time. An AI's output should be held to the exact same standard as anyone else's output. If it's close enough to someone else's copyrighted work to be considered infringing then the company using that AI should be liable for copyright infringement the same way they would be if AI had never been involved. AI's ability to produce a large number of infringing works very quickly might even be what causes companies to be more careful about how they use it. Breaking the law at speeds approaching the speed of light isn't a good business model. reply wpietri 34 minutes agorootparentprev> That's just how the internet works. Don't put something on the internet if you don't want it to be globally distributed and copied. You could make the same argument about paper. \"That's just how photocopiers work! If you don't want your creations to be endlessly duplicated and sold, don't write them down!\" Heck, you could make the same argument about leaving the house. \"That's just how guns work! Don't go out in public if you don't want to take the risk of getting shot!\" But it's a bad argument every time. That something is technically possible doesn't make it morally right. It's true that a big point of technology is to increase an individual's power. But I'd say that increased power doesn't diminish our responsibility for our actions. It increases it. reply autoexec 27 minutes agorootparent> You could make the same argument about paper. \"That's just how photocopiers work! If you don't want your creations to be endlessly duplicated and sold, don't write them down!\" No, the argument would be about photocopies, not paper. \"That's just how photocopiers work! Don't put something into a photocopier if you don't want photocopies of it.\" It isn't possible for anyone to access anything on the internet without making copies of that thing. Copies are literally how the internet works. Shooting everyone who steps outside isn't how guns work either so that also fails as an analogy. The internet was specifically designed for the global distribution of copies. If that isn't what you want, don't publish your works there. > That something is technically possible doesn't make it morally right. Morality is entirely different from how the internet works, but in practice, I don't see anything immoral about making a copy of something. Morality only becomes an issue when it comes to what someone does with that copy. reply ulbu 16 minutes agorootparent> If that isn't what you want, don't publish your works there. \"Women are oppressed in Iran. Well, that's just how Iran is. Just leave it if you don't want to be oppressed\" Oh my. Yea, and whatever is some way, is that way – \"it is how it is, deal with it\". It's an empty statement. The topic is an ethical and political discussion in light of current technologies. It's a question of whether it should work this way. That's how all moral questions come about – by asking if something should be the way it is. And the current state of technology brings a dilemma that hasn't existed before. And no, the internet was not designed for that. Quite obviously. Sounds like you haven't heard of private messages. I'm very surprised this has to be stated. reply wpietri 17 minutes agorootparentprevYes, if one over-narrowly construes any analogy, it can be quickly dismissed. I suppose that's my fault for putting an analogy on the internet. We've had copying technologies since people invented the pen. It was such an important activity that there were people who spent their whole lives copying texts. With the rise of the printing press, copying became a significant societal concern, one so big that America's founders put copyright into the constitution. [1] The internet did add some new wrinkles, but if anything the surprise is is that most of the legal and moral thinking that predates it translated just fine to the internet age. That internet transmission happens to make temporary copies of things changed very little, and certainly not the broad principles. I understand why Facebook and other people lining their pockets would like to claim that they are entitled to take what they want. But we don't have to believe them. [1] https://constitution.congress.gov/browse/essay/artI-S8-C8-1/... reply orthecreedence 26 minutes agorootparentprev> You could make the same argument about paper. Most paper doesn't come with Terms and Conditions that everything you write on it belongs to the paper company. I hate Facebook (with a fiery passion) but people gave them their data in exchange for the groundbreaking and unprecedented ability to make friends with another person (which has never been done before). It sucks, but don't use these \"free\" systems without understanding the sinister dynamics and incentives behind them. People make the same arguments about the NSA. \"They aren't doing anything bad with the data their collecting about every US citizen.\" Well, at some point they will. Stop borrowing against future freedom for a tiny bit of convenience today. reply wpietri 12 minutes agorootparentI think you're confusing a legal point (whether a T&C really gives Facebook any particular legal right in court) with the moral question of whether or not people should just roll over for large companies because of language we all, Facebook included, know that nobody ever reads. Even if FB's T&C made it clear they could do this (something I haven't seen proven), that at best means people would have a hard time suing as individuals. They can still get upset. They can still protest to the regulators and legislators whose job it is to keep these companies in line, and who create the legal context that gives a T&C document practical meaning. reply capital_guy 30 minutes agorootparentprev> That's just how the internet works. Don't put something on the internet if you don't want it to be globally distributed and copied. And if someone takes a picture of your artwork, or takes a picture of your person, and posts that to the internet without your consent? Have you given up your rights then? My answer: Absolutely not. reply samatman 15 minutes agorootparentWhat AI does is much more like the Old Masters approach of going to a museum and painting a copy of a painting by some master whose technique they wish to learn. This has always been both legal, and encouraged. Or borrowing a thick stack of books from the library, reading them, and using that knowledge as the basis for fiction. That's a transformative work, and those are fine as well. My take is that training AI models is a bespoke copyright situation which our laws were never designed to handle, and finding an equitable balance will take new law. But as it stands, it's both legal and encouraged for a human to access a Web site (thereby making a copy) and learn from the contents of that website. That is, fundamentally, what happens when an LLM is trained on corpus data. The difference in scale becomes a difference in kind, but as I said, our laws at present don't really account for that, because they weren't designed to. LLMs sometimes plagiarize, which is not ok, but most people, myself included, wouldn't consider the dilemma satisfactorily resolved if improvements in the technology meant that never happened. Outside of that, we're talking about a new kind of transformative work, and those are legal. reply Buttons840 30 minutes agorootparentprev> That's just how the internet works. Don't put something on the internet if you don't want it to be globally distributed and copied. This is true for average people. Is it true for the wealthy? Is it true for Disney? Does our law acknowledge this truth and ensure equal justice for all? reply autoexec 13 minutes agorootparentIt's 100% true for everyone. You can't access anything at disney.com without making a copy of that thing. Disney can't access anything at yourdomain.whatever without making a copy of that thing. Whatever crimes either of you can get away with using your copies is another matter entirely. Any rights you had under the legal system you had before AI haven't gone away, neither have the disadvantages you have against the wealthy. reply Buttons840 0 minutes agorootparentOne of the comments you replied to was complaining that their work would be copied and used in training LLMs or other lucrative algorithms, and then you responded taking about how it's common to temporarily copy data into RAM to show a web page. Those are very different, and bringing up such technical minutia is not helpful to the discussion. guerrilla 46 minutes agorootparentprev> That's just how the internet works. Don't put something on the internet if you don't want it to be globally distributed and copied. Or we could be ethical and encourage others to be ethical. reply orthecreedence 23 minutes agorootparentWe can encourage profit-driven megacorps to be ethical? Sure, by abolishing them. Otherwise, you're just screaming into the void. reply guerrilla 1 minute agorootparentI think what I said is a prerequisite for that. There will be no structural changes without widespread cultural changes. vasco 41 minutes agorootparentprevI see you're one of the ones that wouldn't download a car. reply pbhjpbhj 34 minutes agorootparentI would share a car I had rights to, and download a car made free to me. Facebook would certainly sue me if it were their car, they should thus be held to that standard in my personal opinion. reply schmorptron 33 minutes agorootparentprevWe could make a distinction between individuals and companies doing it reply mlazos 21 minutes agorootparentprev> I don't agree because it creates this dilemma for creators: you need to put your work out there to get traction, but if you put your work out there and anything public is fair game, then it will be sampled by a computer and instantly recreated at scale. This might even happen without the operator knowing whose work is being ripped off. This is no different than the current day, copying already happens (as your friends have seen) AI makes it a little easier but the same legal frameworks cover this - I don’t see why AI stealing is any different than a person doing the same thing. The ability to copy with zero cost was incredibly disruptive and incredibly beneficial to society. Settled case law will catch up and hopefully arrive at the same conclusion it has for human copyright infringement (is it close enough to warrant a case) reply solardev 34 minutes agorootparentprevWell, just as another perspective... I'm not convinced that the philosophy of copyright is a net positive for society. From a certain perspective, all art is theft, and all creativity builds upon preexisting social influences. That's how genres develop, periods, styles... and yes, blatant ripoffs and copycats too. If the underlying goal is to be able to feed creators, maybe society needs better funding models...? The current one isn't great anyway, with 99% of artists starving and 1% of them becoming billionaires. I'd much prefer something more like the model we have for some open-source projects, where an employer (or other sponsors) pays the living wage for the creator, but the resulting work is then reusable by all. Many works of the federal government are similarly funded, where a government employee is paid by your taxes but their resulting work automatically goes into the public domain without copyright. I don't buy the argument that nobody would make things if they weren't copyrightable/paid directly. Wikipedia, OSM, etc. are all living proof that many people will volunteer their time to produce creative things without any hope of ever getting paid. As a frequent contributor to those and also open-source code, Creative Commons photography, etc., a large part of the joy for me is seeing how my work gets reused, transformed, and sometimes stolen by others (credit is always nice, but even when they don't mention me, at least I know the work I'm doing is useful to people). But the difference for me is that I don't rely on those works to put food on the table. I have a day job and can afford to produce those works in my spare time. I wish all would-be creators would have such a luxury, either via an employer relationship or perhaps art grants and the such. I wonder how other societies handle this... back in the day, I guess there were rich patrons, while some communities sponsor their artists for communal benefit. Not sure what works best, but copyright doesn't have to be the only way society could see creative outputs. reply marcosdumay 28 minutes agorootparent> I'm not convinced that the philosophy of copyright is a net positive for society. I'm ok with that. But the philosophy of copyrights is not under debate here. All that is being debated is if it should protect small people from big corporations too. reply solardev 14 minutes agorootparentIt's not? I thought we were talking about \"AI SHOULD be trained on everything that is in the public sphere\" and \"[your work] will be sampled by a computer and instantly recreated at scale. [...] Commercial art producers have always ripped off minor artists\". Isn't that all about copyright and the ability to make money off your creative works? When I put something on Wikipedia or any other commons, I don't worry about which other person, algorithm, corporation, or AI ends up reusing it. But if my ability to eat tomorrow depended on that, then I would very much care. Hence, copyright seems an integral part of people's ability to contribute creatively. My argument is that by detaching their income from the reusability of their work, we would be able to free more creators from that constraint. Under such a system, the little guy would never get rich off their work, but they wouldn't starve when a big corporation (or anyone else) rips them off either. reply paxys 22 minutes agorootparentprevThere is either copyright violation or there isn't. Like you said, artists can still sue companies for copying their work, AI or not. If the work was transformative enough then, well, what's the problem? reply baby 23 minutes agorootparentprevWho cares? Don't we want the most absolute intelligence to help human civilization? Credits and creators are below that. reply dewarrn1 22 minutes agorootparentCreators may disagree. reply datavirtue 48 minutes agorootparentprevMy wife works in a studio with a gaggle of artists who all blatantly \"rip each other off\" constantly. reply swatcoder 28 minutes agoparentprevThat sounds compelling when you borrow the marketing term \"AI\" and position the work as part of a sweeping revolution into some beautiful sci-fi future. It's less compelling when you see the technology as noisy content generators that will flood the network with spam and devour the livelihood and opportunity to learn for low-market artists and programmers. In the former perspective, you may look at this is \"well, what's the best way we can make this happen?\" while the latter sees it more like \"So you insist on making this happen. Are you sure there's a suitably responsible way for you to do that?\" reply casenmgreen 12 minutes agoparentprevWhen I speak to my friends, it's a conversation not wholly private - after all, I've shared whatever I'm saying with them - but it certainly isn't wholly public. In all our conversations, we have and we understand there are degrees of privacy; that which we share with family, that with friends, that with strangers. When I post on-line, I both expect and expected that my conversations would be between me and the group of people I conversed with. I knew who was reading, and I was fine to write whatever I was writing to that group. I may be wrong, but I think this is generally how people feel, how they act, what they expect, how they are, as humans. We think about who we are writing to. It does not come naturally to imagine that third parties are listening in, or will listen in, in the decades to come. This brings us to now, with a third party, reaching back over ten or fifteen years, for absolutely everyone, everywhere, taking copies of everything it can get access to, for its own use, whatever that may be. I profoundly reject Microsoft, and Google, and all entities and companies which act in such ways, these smiling evils, with their friendly icons and bright colours, happy faces and hundred page T&Cs to utterly obscure and obliterate the truth of their actions. reply segasaturn 55 minutes agoparentprevAFAIK, AI models have no way of differentiating high quality input from garbage. If it's fed peer-reviewed, academic papers as well as a paranoid, violent person's Facebook manifesto it treats them with equal weight as long as the sentences are coherent. reply potato3983 17 minutes agorootparentOn some level it needs to be fed some amount of garbage because it takes in all sorts of garbage inputs like we do. AI that needs painstakingly curated training data isn't interesting in the same way that early lightbulbs that used precious metals and cost too much to be commercially viable aren't interesting. reply qwery 43 minutes agoparentprevWhy? A statement that extraordinary would be interesting if it had some reasoning alongside it. Also, Facebook posts aren't really \"in the public sphere\" / publicly accessible, but that's a nitpick. reply paxys 25 minutes agoparentprevAgreed. You don't get an AGI omelette without cracking some AGI eggs. For a digital superintelligence to be possible information has to be able to flow freely. reply ipaddr 1 hour agoparentprevThe question become are post with a limited reach (friends) the public sphere. reply __loam 1 hour agoparentprevYou're probably among friends on this site, but outside tech coded spaces, most people understand that publicly available is not the same thing as an unlimited license to do whatever you want. reply Ekaros 23 minutes agorootparentThis site is weird when you compare it to open source software projects and these same companies selling those as a service on their platforms it is again huge massive problem and exploitation... When the license explicitly allows that, without single legal question. I wonder if things would be different if software could be copied and then recreated by these models by the mega corps. Would there still be such push in favour of it? reply ben_w 49 minutes agorootparentprevWhile true, likely more pertinent that most people don't have a clue what's possible legally or technically until it gets in the news. Can't give informed consent if you don't know what the EULA means or what the machines can do. reply uoaei 1 hour agoparentprev\"If they can, they will.\" reply orochimaaru 1 hour agoprevWhy is this surprising? They’ve always done this. In fact I’d be surprised if they didn’t do this. Fwiw - llama is free to use. So I guess it’s a good enough return. I don’t use Facebook. I’m not sure if they can peek into WhatsApp messages. reply Cheer2171 1 hour agoparent> Why is this surprising? They’ve always done this. In fact I’d be surprised if they didn’t do this. This is such an unconstructive attitude. This is the first time they have publicly admitted it. reply cbsmith 59 minutes agorootparentIt's the first time they've publicly acknowledged what specifically was used in the training set for their LLMs. It's NOT the first time they've said that that data could be used for research. The presumption was that they had used the data for their LLMs. Maybe it is surprising to some that they this particular research used that data, but it really shouldn't be to anyone aware of how LLM development is done. reply wpietri 25 minutes agorootparentprevYeah, to me it's part of what press critic Jay Rosen calls The Church of the Savvy. It's kind of a performative cynicism where one tries to gain status by appearing so smart that you're above it all. One can do it with pretty much anything, which ironically means it demonstrates very little actual smarts. To me it's related to the sort of person you'll see who on every startup failure says how they knew it wasn't going to work. Which again doesn't require particular smarts; most startups fail, so predicting failure doesn't take a genius. What I think is much more interesting is spotting a problem before it's obvious and naming it in advance. Or better, fixing it early on. That doesn't get you many internet points, though. reply notatoad 1 hour agoparentprevwas it supposed to be surprising? it's still good to have confirmation of these things that we all assume to be true. reply bobthepanda 1 hour agoparentprevThey did get sued successfully recently by an AG for the autotag faces in photo feature, so maybe another lawsuit is in the cards. reply candiddevmike 1 hour agoparentprevNeed to test how good facebooks sanitization was, maybe you could find some PII in llama responses with the right prompt. reply ethbr1 1 hour agorootparentI'd assume that was one thing they were extremely diligent about ironing out, doing this at scale. reply ziddoap 1 hour agorootparentWhat is that assumption based on? There have been numerous scandals over the years regarding their lack of care when handling personally identifiable data. Meta is not who I look to for being extremely diligent with data. reply thunder-blue-3 1 hour agoparentprevyeah I'm surprised that anyone didn't see this coming. The amount of times i've heard, \"this is all our customer data, performon it,\" I would have about $5. reply whoitwas 1 hour agoprevI don't understand how this surprises anyone. You choose to give them your data. It's not free. If you don't want them to have your data, don't give it away. reply JeremyNT 4 minutes agoparentEasily said in 2024, but 17 years ago? I don't think this was quite so obvious (even amongst technical people). reply giobox 1 hour agoparentprevRight, and in exchange users received rather a lot of services for free... photo and video storage etc as just one example, Llama is free to use, etc etc. While I've no sympathy for mishandling private user data as Meta has of course been guilty of in the past, I think users getting 16 years of free service in exchange for their public posts being used in this fashion is not that bad of a deal. reply ipaddr 56 minutes agorootparentYou get photo/video storage worth a fraction of a cent. Llama doesn't have a free to use service. People got 'free service' in exchange for putting ads around content not for this. It's a terrible deal no one asked the users if they want to agree to. It's visiting a website and saving an image and claiming the website owes you for storage. reply throwaway913242 2 minutes agorootparentDidn't the users explicitly agree to the deal when they made an account and clicked \"I agree to the Terms of Use\"? Sure there's a degree of \"oh nobody reads those anyway\" to it that society at large should be approaching more rigorously (maybe a common-contract type of system), but at the end of the day the users are choosing to interact with a service, upload their content to it, and explicitly WANTING their content to be shared (to their friends, on their feeds, etc). The real question is whether the agreement that was made between them and their users during 2007-now covered uses like training AI, but most likely it was a very broad \"we can use your data how we want\" type of statement and that agreement wasn't fought (or fought hard enough?) back then. reply giobox 8 minutes agorootparentprev> Llama doesn't have a free to use service. Llama is embedded for free in a ton of Meta products right now: https://about.fb.com/news/2024/04/meta-ai-assistant-built-wi... > \"A better assistant: Thanks to our latest advances with Meta Llama 3, we believe Meta AI is now the most intelligent AI assistant you can use for free\" reply baby 22 minutes agorootparentprevTalk for you, I can't imagine what my life would have been without facebook/whatsapp. I've met countless of friends, probably am married to my wife today thanks to it, and without that I wouldn't be able to keep in touch with most of my friends today. The \"facebook is not useful for me so I don't get it\" needs to die seriously reply yoyoyo1122 59 minutes agoparentprevAs the infamous saying goes, \"If you aren't paying for the product, you are the product\" reply Noumenon72 1 hour agoprevWas \"public\" ever the default setting? I remember it as being opt-in if you ever wanted something to show beyond your friends-of-friends. reply Lammy 1 hour agoparentYes, “Everyone” was the default for years https://readwrite.com/facebook_pushes_people_to_go_public/ reply grandma_tea 1 hour agoparentprevI have a very different memory of my time on Facebook 10 or so years ago... It felt like every two weeks some update would change my settings to \"public\" in some way. reply not2b 1 hour agoparentprevOn Facebook, public isn't the default, but on Instagram it is. All those billions of photos, including all those famous people: evidently fair game. reply giobox 1 hour agorootparentI don't think you can make a blanket statement like this - the defaults and privacy policy for FB have changed a lot in 16 years, multiple times. It might be the case today, but this is 16 years of data. reply parasti 42 minutes agoprevIt's funny because the entire Facebook ecosystem is designed to disincentivize meaningful posting. Just keep watching the ads and short form videos, user. reply encoderer 1 hour agoprevThat’s nothing. AOL has just finished training on 29 years of emails and messages. it’s hoped that with more H100s the AI will finally be able to calculate the full amount due by BillG for the emails mom has been forwarding. reply kylehotchkiss 1 hour agoprevIt's OK. Meta is training their AI on hundreds of thousands of posts with photos of veterans with toilet plunger legs celebrating their birthdays in the middle of the street while sitting as sturdy as the Lincoln memorial. The AI brain rot has already begun in this model. reply SketchySeaBeast 1 hour agoparentI am truly impressed by how quickly AI generated content has filled up every public space. We've gone from \"AI is the future!\" to a digital Kessler's syndrome in a short few years. reply kylehotchkiss 3 minutes agorootparentNot only filled up every public space, but the quality of it all is so crude. Like a bad Pixar animation. It's not like Pixel's \"Add the photographer back into photo\". reply fidla 1 hour agoprevWell they don't really know if someone is an adult or not. Just because they say they are 13 doesn't mean that they really were when they signed up. And 13 is hardly an adult now is it? reply qup 1 hour agoparentIs this an important distinction? reply alwa 1 hour agorootparentYep. For reasons of propriety, for one thing. But also because the data protection laws get especially opinionated about what you do with kids’ speech, and one line they draw is at age 13. The American variant, COPPA, dates back to 2000, and requires verifiable parental consent to process the data of under-13s. No idea if that matters retrospectively in legal terms—it’s seems to me that the main problem was providing service to the kids in the first place—but it’s icky either way. Then again there’s an ickiness to the entire project of pretending casual users’ arcane permissions settings from 15 years ago indicate affirmative consent today… reply playingalong 20 minutes agorootparentSurely I am not a fan of FB, but what else should they do other than relying on self-reported age? reply ziddoap 1 hour agorootparentprevYes. reply gnabgib 1 hour agoprevDiscussion (81 points, 3 days ago, 79 comments) https://news.ycombinator.com/item?id=41508158 reply duxup 27 minutes agoprevWe're going to create a really bad AI and get upset by that fact only to discover that ... we all made it that way. https://www.youtube.com/watch?v=Y-Elr5K2Vuo reply paxys 36 minutes agoprevSo did OpenAI and Anthropic and Google. That's what \"public\" means. reply ChrisArchitect 56 minutes agoprev[dupe] Actual article: https://www.abc.net.au/news/2024-09-11/facebook-scraping-pho... (More discussion: https://news.ycombinator.com/item?id=41508158) reply autoexec 58 minutes agoprevI don't believe for a moment that they haven't used the data of countless children. Especially early on when kids just had to click an \"I'm over 18\" button or enter a fake birthday to get accounts and facebook, like everyone else, just looked the other way. reply datavirtue 47 minutes agoparentThe monsters! reply PaulHoule 55 minutes agoprevAssuming they want to build a model that can do useful things with their own data (say any kind of content filtering, summarization, etc.) it is exactly what they should do. reply koolala 1 hour agoprevSkynet Ads are \"said\" to be preferred. \"People prefer to see relevant ads.\" Can AI understand humans better than humans understand themselves? Can Humans understand the consciousness of Dogs and Cats better than they do? The objective answer feels like No but the subjective answer feels like Yes. Humans will never understand how an animal truely thinks but we understand how to control them. reply AlexandrB 39 minutes agoprevPeople just submitted it. I don't know why. They 'trust me'. Dumb fucks. -Mark Zuckerberg Things change, but this never stop being a concise summary of Meta's ethos as a company. reply not2b 1 hour agoprevThis would include all those celebrity posts on Instagram. Great for deepfakes. They'll try to protect against that, but a bit of cleverness with prompts should be able to get around the filters. reply geertj 55 minutes agoprevI imagine a future AI trained on this going into therapy to uncover childhood trauma. reply jppope 10 minutes agoprevI for one am shocked. Shocked I say. There are dozens of us surprised by Facebook's actions... DOZENS. reply pbhjpbhj 31 minutes agoprevIn the UK I'd say they've definitely committed copyright infringement. Fair Dealing doesn't allow this. reply ado__dev 1 hour agoprevNot surprised at all. Facebook owns the platform and outlined in the ToU that they can do whatever they want with the content you post on there. At least it's better than scraping content off platform (which I'm sure they've done) and using that, but using content posted on their own platform seems like a no-brainer. reply gmd63 30 minutes agoprev\"They just trust me...Dumb f**s\" - Mark Zuckerberg reply nkmnz 1 hour agoprevIs this true for posts from people with deactivated/deleted accounts as well? reply alephxyz 1 hour agoparentUnlikely: >We don’t use posts or comments with an audience other than Public for these purposes. reply aplusbi 1 hour agoprevHonestly this feels like a better policy than most AI training - Meta actually has explicit rights to the content it is using. Sure it was EULA click-through but at least it's something that the content creator ostensibly agreed to. Of course I'm sure Meta is also training their AI on content that they scraped from the internet/other sources without permission... reply almost_usual 1 hour agoprevCan’t wait to see the memes it generates. reply greesil 1 hour agoprevI am the product. reply MisterBastahrd 43 minutes agoprevMeta just created the dumbest object known to mankind. Quite an achievement given our current political landscape. reply annoyingnoob 55 minutes agoprevGarbage in, garbage out. reply mylons 1 hour agoprevhow is _anyone_ surprised by this? reply golergka 1 hour agoprevIf it's publicly posted, it literally means that everybody can read it. What's exactly the issue here? reply candiddevmike 1 hour agoparentInternet pubic and Facebook public are two different things. The latter can't be scrapped easily or really discovered at all. It's not indexed or usable outside of Facebook, and most folks don't think public means \"folks not on Facebook\". reply XorNot 1 hour agoparentprevIt's also not used to learn facts about stuff: it's used to learn how language works and is used to describe the world. The sum of all variants of human communication in text is pretty good for this and examples of wrong or different also matter: there was that article a few months ago about how a chemical prediction model performed better when it was also trained on invalid SMILES representations compared to totally sanitized datasets. reply SoftTalker 1 hour agoprevFunny to think that the distillation of 16 years of Facebook posts is now considered \"intelligence.\" reply SmellTheGlove 1 hour agoparentI wonder how much safety work they have to do specifically because of this. I’d imagine their model might have a fairly paranoid, slightly racist bias if not. Particularly as younger demographics shifted away from FB in the last decade. reply PaulHoule 59 minutes agorootparentActually if you want to train a model that can recognize bad things you want to have those bad things in the language model training data, otherwise it won’t see the characteristics of those things and it will later struggle to recognize them in later training stages. reply btown 41 minutes agorootparentPresence may be necessary, but researcher-driven weighting of different sources of content can still introduce bias. For instance, [0] suggests (sources are unclear) that OpenAI boosted by 5x the weight of their WebText2 dataset, which consists of sites linked to by upvoted Reddit comments. Reddit, in this sense, with all the biases of its various communities, was artificially elevated in importance. (Per [1], there were well-thought-through reasons for this around previous failures due to overreliance on Common Crawl, but it's nonetheless a choice that was made by humans to go in this direction.) [0] https://gregoreite.com/drilling-down-details-on-the-ai-train... [1] https://insightcivic.s3.us-east-1.amazonaws.com/language-mod... reply fire_lake 1 hour agorootparentprevConsidering the standard datasets contain 4chan posts it’s probably a marginal improvement. reply changoplatanero 51 minutes agorootparentprevI haven't worked at facebook for a number of years but I'm imagining what they trained on here was public instagram images and not the random text that people write on facebook. The text in facebook posts is likely to be low value but the images are a data gold mine. reply dotancohen 52 minutes agorootparentprevThat would be supervised learning: they tell the model that these are undesirable::racist values. Or just fed into the RAG::blacklist dataset. reply kevin_thibedeau 1 hour agorootparentprevWith suitable sentiment analysis, you can train known bigoted models and then run unknown input through them to see if it appeals to the model. reply squigz 1 hour agoparentprevIt's... really not. Despite what some people seem to think, most of the 3 billion+ Facebook users are normal people who aren't just posting nonsense or memes. reply solardev 1 hour agorootparentTheir algorithm sure does a good job at filtering those normal people out... reply marcosdumay 24 minutes agorootparentYes, it does. But that doesn't invalidate the GP. reply solardev 11 minutes agorootparentIt just makes it less likely that these normal people will be able to see the other normal posts from other normal people. Early Facebook's Wall was like that, mostly just friends chatting with each other about cats and babies, but then the company purposely started to optimize the timeline for controversy instead and it all went downhill. It's not that there aren't normal people on there, it's that organic feel-good posts have a harder time gaining traction vs the sea of flamebait and sponsored ads and astroturfed spam. The signal to noise ratio was very, very low by the time I left. I don't know how it is these days... reply squigz 1 hour agorootparentprevMaybe that says more about you and how you engage with it than it does the platform though. reply solardev 1 hour agorootparentDoes it? I deleted my Facebook back in 2016 or so when it got so bad. Before that, I used extensions to make it show posts from friends in chronological order instead of whatever it was optimizing for by default (engagement/controversy, I think?) It's just such a toxic, manipulated environment... I don't use social media anymore, just text some friends, and am much happier. reply doublepg23 1 hour agorootparentYou don’t consider HN social media? reply solardev 54 minutes agorootparentI guess it's a gray area? It's more like a forum to me, of the pre-Facebook sort, more like Slashdot than reddit. And there's extremely strong moderation here that does the opposite of Facebook: It optimizes against controversy and vitriol rather than encouraging it. We end up with a bunch of nerds mostly talking shop and sometimes complaining about the job market, but that's still far less ragebaitey than most social media. reply randomdata 32 minutes agorootparent> It optimizes against controversy and vitriol rather than encouraging it [...] a bunch of nerds mostly talking shop Something doesn't add up here. Nerds talking shop and controversy and vitriol about the their technical preferences is the same thing. Perhaps what you're saying is that controversy and vitriol is only apparent when you're an \"innocent bystander\" who doesn't have a passion for the subject? Which HN avoids by usually remaining focused on a fairly narrow set of subjects, to a user base generally passionate towards those subjects, so you don't notice? That is an astute observation, if that is what you're trying to say. Indeed, if you showed your non-technical grandmother HN, she would no doubt see it the way Facebook was talked about earlier. reply solardev 29 minutes agorootparentOK, but I think there's a pretty big difference between \"Next.js is too bloated, you should use HTMX\" and \"so and so group of people are all _____ and they should all be ________, and oh, your mom sucks\". I don't think – I hope, at least – no one is going to start a shooting war over their framework of choice. You can't say the same about much of the content circulating around social media. (Edit: You added more to your post after I replied. To your point of \"Which HN avoids by usually remaining focused on a fairly narrow set of subjects\", that's not just my observation, that's the actual guidelines: https://news.ycombinator.com/newsguidelines.html. We self-select into a narrow slice of nerdtalk or we end up getting downvoted or banned from the site. To me that is the big difference between an interest-based forum that generally stays a functional monoculture vs a general social media site that brings diverse strangers together into shouting wars about whatever the controversy du jour is.) reply randomdata 14 minutes agorootparent> there's a pretty big difference between \"Next.js is too bloated, you should use HTMX\" and \"so and so group of people are all _____ and they should all be ________, and oh, your mom sucks\". Is there? Perhaps the trouble here is that your examples are too far apart to recognize how they compare? What if Facebook, instead, said \"Fat workers are too bloated, you should hire skinny workers\"? Or if HN said \"so and so projects are all ____ and they should all be ____, and oh, your vacuum doesn't suck\". I see no practical difference. It seems the only difference is that Facebook tends to talk about people, HN about tech. But that's not a significant distinction – aside from where your interests lie. Certainly tech-minded folks often find people to be uninteresting. solardev 7 minutes agorootparentI think it's just the different degree of emotional attachment people to have to these topics. Yeah, people can get a bit worked up about how annoying JS development has become, but not quite to the same level as the major headlines of the day about the latest Middle East controversy/Soviet threat/identity politics thing. Oh, and my vacuum does suck just fine, thank you very much. randomdata 3 minutes agorootparent> not quite to the same level as the major headlines of the day about the latest Middle East controversy/Soviet threat/identity politics thing. Why do you say that? I share in the tech-minded proclivity towards not having much interest in people, so I admit to being largely out of the loop, but what is there to be worked up about where you wouldn't equally get worked up around some tech-based topic? It just sounds boring an uninteresting to me. When I have occasionally encountered discussions about those topics, all I have is some laughter at how silly the people sound. Just like I'm sure how the \"Next.js is awful\" conversations sound to the metaphorical grandma. dotancohen 49 minutes agorootparentprev> And there's extremely strong moderation here that does the opposite of Facebook: It optimizes against controversy and vitriol rather than encouraging it. Well, that and hot grits you insensitive clod. reply solardev 46 minutes agorootparentOh god, now I feel old :P reply barbazoo 44 minutes agorootparentprevEven if you don't engage with the content that gets recommended/shown in your timeline gets weird real quick. I used to have an account for marketplace and our local neighborhood group and that's my experience. reply exe34 1 hour agorootparentprevit's true, I only have to reply to one stupid post and I'm flooded by similar nonsense. now I've taken to closing it if it shows me anything other than a nice house or a cat. i definitely see less dumb. reply derefr 51 minutes agoparentprevWhat you get from the self-supervised training of a base model is more like \"language fluency plus a web of crystallized-knowledge relationships.\" But also, ML model training is a bit like the stock market: the noise/stupidity in individual examples points in a bunch of random directions, and so ends up cancelling out; while the signal all points in the same direction, and so ends up captured in the distilled model. (You might call this the \"Anna Karenina principle of Information Distillation\": all right answers are the same, while each wrong answer is wrong in a different way.) reply dougb5 39 minutes agorootparentPeople are frequently wrong in the same way in environments where they are easily influenced by each other like social media. That's part of why these models, especially early on, exhibited so many racial and gender biases. reply veidelis 45 minutes agorootparentprevIt's still wrong if the majority is wrong, isn't it? Like propaganda on major channels which praise the same thing, is consumed by a major part of the population which then assume that it's the right thing and continue the misinformation, which then propagates to the AI model. reply derefr 26 minutes agorootparentYes; but in the case of a \"common misconception\" like this, there's always also a nontrivial minority who do know the \"right answer\"[1] — and so enough examples of that occur in the training data to enable the model to embed the knowledge-web of \"right ideas\" (as a niche activation), alongside the \"wrong idea\" (in its default-mode network). The \"initial fine-tuning of a 'raw' base model to produce a 'generalized pre-trained' base model\" process, is commonly talked about in terms of \"alignment\" — making the model ethical, making it not swear at you, making it refuse to engage with certain content, etc. And really, that part is all optional, with there existing \"non-aligned\" or \"orthogonalized\" models that don't have these steps performed on them or have had them reversed, but which are still useful. But a large part of this initial fine-tuning process, consists of debiasing the model's default activation, moving it away from making associations with \"common misconceptions\" and toward making associations with \"right answers.\" And this process is crucial to a model being able to reason intelligently — as these common misconceptions aren't coherent in a chain of reasoning they appear in, and so lead to the chain of reasoning falling apart / being non-productive. This is, in large part, the \"secret sauce\" that makes a model of a given size \"more intelligent\" than another model of the same size. Every base model that anyone actually cares about or uses — \"aligned\" or not — has had some process of de-biasing like this applied to it; or, at least, has short-cutted this process by training on a training dataset generated by or filtered by a model that has already had this de-biasing applied to it, such that the derived training dataset doesn't contain the \"common misconceptions\" in the first place. And when OpenAI and Meta brag about using RLHF, a large part of what they mean, is crowdsourcing recognition of long-tail \"common misconceptions\" at scale, to allow a much more thorough version of this de-biasing process[2]. --- [1] Of course, if nobody in the training data ever demonstrates the \"right answer\" knowledge/associations, then the model will never learn that knowledge/associations. But then, given that these training datasets usually represent decent samples of the population, a \"right answer\" being missing entirely would likely mean that nobody on Earth knows the \"right answer\" — so we humans wouldn't be able to recognize the model was wrong. The stock market can be wrong too, for the same reason. [2] Which, perhaps surprisingly, can add up to more than the sum of its parts. The more of this human-labelled \"common misconception\"-response RLHF data you have, the more you can derive patterns from this bias data. You can distil out negative examples that you can use to prompt a model to filter the training dataset; but more interestingly, you can distil positive examples of the sort of structured chains of reasoning that work best to inherently avoid triggering the bias. Where, if you can overlap the activation-space hyperspheres of many such inversion-of-bias examples, then you get, essentially, the hypersphere within the model's activation space that contains its instrumental rationality. You can then just bias the model toward living in that part of activation space as much as possible — and this shoots its apparent reasoning capacity way up. reply TheOtherHobbes 45 minutes agoparentprevMaybe the plan is to replace all FB users with bots but keep charging advertisers anyway. \"But how can anyone tell?\" reply emporas 51 minutes agoparentprevIf it is high entropy, what's the significance of the source of information's age? It could as well be 2 years old and no one would care. reply jprd 44 minutes agoparentprevI keep waiting for one of their models to just start the answer with \"FWD: FWD FWD: FWD: BLAH BLAH FWD ON\" reply vunderba 44 minutes agoparentprevAmusing but a large part of the training data other LLM models is social media platforms. I mean hop on chat GPT right now and ask it to come up with any kind of novel joke. I can almost guarantee it will either be some kind of word play or other equally low hanging kind of pun that you would find on Reddit fifty replies deep. Where's the Mensa member only social media dating platform for a properly erudite and snobbishly arrogant LLM to train on? reply mig_ 42 minutes agorootparentGoogle scholar ? reply pjs_ 34 minutes agoparentprevThere's a fun retrospective PoV which is that Silicon Valley has been through a sequence of consecutive hype waves, each of which has been viewed with derision as mega-scale fraud or brainrot: - Video games (epic waste of time and moral debasement) gave us GPUs and NVIDIA - Social media (rage bait-fueled argument machine and cat pic repository) gave us a huge corpus of text on which to train language models. Yes scientific papers and Wikipedia are necessary but probably not sufficient? - Crypto (fraud, giant waste of resources) gave us a generation of young people who were comfortable building ludicrously oversized GPU clusters, and to some extent funded NVIDIA and TSMC R&D all of which led to where we are with AI, which is that it is making (in my humble opinion) impressive dents in the problem of solving intelligence -- cue HN commenters telling me that AI is a giant fraud and waste of resources also :) reply VoodooJuJu 46 minutes agoparentprevI don't know why that's funny, since that is the very essence of the ideal of democracy: the distillation or averaging of the sentiment of the masses yields truth. Some contributions are outliers in one way or another, and it's these outliers that live in many people's heads, but these extremes are ironed out by averaging against the masses. reply glitchc 1 hour agoparentprevThis made me chuckle. reply ithkuil 1 hour agorootparentLol-ed myself but OTOH if you want a model to learn how people actually speak you cannot expect to get that by reading curated scientific documents. reply segasaturn 1 hour agoparentprevnext [2 more] [flagged] solardev 1 hour agorootparentnext [2 more] [flagged] nutkizzle 1 hour agorootparentThey'll eat your chia pet. reply askafriend 1 hour agoprev [–] This isn't really that groundbreaking of a story... Of course they'd do this! How did people think feed ranking worked? The only reason this is being reported now is because there's a chatbot and I guess that feels different to people. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta has been using public posts and photos from Facebook and Instagram since 2007 to train its AI models, unless users set their posts to private.",
      "European users can opt out of this data usage due to local privacy laws, but users in other regions, including Australia, do not have this option.",
      "Meta has not provided clear details about the specifics of its data usage and collection timeline, raising privacy concerns among users."
    ],
    "commentSummary": [
      "Meta has been using public posts from adults since 2007 to train its AI, sparking a debate on the ethics and legality of using public data for AI training.",
      "Critics worry about creators' work being copied without consent, raising questions about fair use and copyright laws.",
      "The discussion underscores the tension between technological progress and the protection of individual rights."
    ],
    "points": 120,
    "commentCount": 144,
    "retryCount": 0,
    "time": 1726247104
  },
  {
    "id": 41526825,
    "title": "Greenland landslide caused freak wave that shook Earth for nine days",
    "originLink": "https://www.newscientist.com/article/2447567-greenland-landslide-caused-freak-wave-that-shook-earth-for-nine-days/",
    "originBody": "Earth Greenland landslide caused freak wave that shook Earth for nine days Seismologists were mystified by a strange signal that persisted for nine days in 2023 – now its source has been identified as a standing wave caused by a landslide in Greenland By Michael Le Page 12 September 2024 Part of a mountain and glacier alongside Dickson Fjord in Greenland in August 2023 (left), and the same spot after a landslide in September 2023 Søren Rysgaard/Danish Army On 16 September 2023, seismic monitoring stations around the world detected a strange signal that faded over time but remained detectable for nine days. “We were like, ‘Oh wow, this signal is still coming in. This is completely different to an earthquake’,” says Stephen Hicks at University College London. “We called it an unidentified seismic object, or USO.” Read more Why supersonic, diamond-spewing volcanoes might be coming back to life Advertisement Hicks and others have now shown that this signal was caused by water sloshing from side to side across the 2.7-kilometre-wide Dickson Fjord in eastern Greenland. This wave was triggered by a massive landslide that resulted in a 110-metre-high tsunami. Earthquake signals usually last only minutes and are a mix of different frequencies, says Hicks. The USO had a single frequency of around 11 millihertz, meaning it repeated every 90 seconds. Once it became clear that the signal began at the same time as the Greenland landslide, Hicks and his colleagues realised there was probably a connection. Many objects, such as a bell, will vibrate at a particular resonant frequency if struck. The same is true of bodies of water, from swimming pools to oceans. Disturbances such as earthquakes and winds can set them rocking, generating a kind of standing wave known as a seiche. Sign up to our Fix the Planet newsletter Get a dose of climate optimism delivered straight to your inbox every month. Sign up to newsletter Based on its width and depth, the researchers calculated that the resonant frequency of Dickson Fjord is 11 millihertz – matching the signal. What took them much longer to understand is why the fjord kept rocking for so long. Immediately after the tsunami, the seiche was going up 7 metres on either side of the fjord. Within days, it had gone down to a few centimetres – so small that a Danish naval boat that went up the fjord three days after the landslide didn’t notice it. But the seiche just kept going, and it probably persisted long after the nine days, when it was no longer detectable by distant seismic stations, says Hicks. “No one has ever reported seiches lasting for so long, or dissipating their energy so slowly.” The shape of the fjord was a crucial factor, computer modelling by the team shows. The landslide site is 200 kilometres inland, with a glacier blocking one end of the fjord and a sharp bend at the other. The round bottom of the fjord also acted a bit like a rocking chair, allowing the water to move with little resistance. All these factors resulted in a high degree of energy trapping, says Hicks, instead of the wave rapidly dissipating as usual. Read more Geoengineering could save the ice sheets – but only if we start soon The landslide itself was a direct result of climate change. A steep glacier was helping to hold up a mountainside. As the glacier thinned, it gave way, resulting in an estimated 25 million cubic metres of rock and ice falling into the fjord – the first ever landslide recorded in eastern Greenland. Nobody was in the area at the time, but cruise ships do go up the fjord. The tsunami destroyed equipment being used to monitor the area, along with two abandoned hunting huts. As the planet keeps warming, there will be more landslides of this kind, says Hicks, who notes that the findings show climate change is now even affecting the earth below us as well as the atmosphere and oceans. “For the first time, we’re looking down beneath our feet to see some of the catastrophic impacts of climate change,” he says. Journal reference: Science DOI: 10.1126/science.adm9247 Volcanoes and past climate: adventures with deep carbon Tamsin Mather’s talk on 12 October will explore the connection between volcanoes and climate change, highlighting how volcanic activity impacts Earth's carbon cycle. Book your tickets here Topics: Disasters Advertisement",
    "commentLink": "https://news.ycombinator.com/item?id=41526825",
    "commentBody": "Greenland landslide caused freak wave that shook Earth for nine days (newscientist.com)89 points by DougN7 19 hours agohidepastfavorite13 comments throwup238 16 hours agoIt's pretty incredible that a 110-meter high tsunami happened and the only reason anyone noticed was a former employee saw that one of the abandoned SIRIUS research stations was swept away, after their cruise ship ran aground at the mouth of the fjord [1] the day after. They probably would have figured it out from the seismic data eventually, but the tsunami was identified within a week in one of the remotest places on earth because of some random pleasure cruise. [1] https://www.smobserved.com/story/2023/09/21/news/greenland-t... reply rurban 14 hours agoparentThe bouncing wave was 7m high, and after 3 days only a few centimeters. reply fragmede 14 hours agoparentprevThe amount of modern society that only exists because of random individuals being in the right place at the right time is terrifying. reply smartbit 8 hours agoprevThis YouTube movie https://youtu.be/60T9TKuuujs with several of the authors, nice visuals and animations explains it in great detail. Highly recommended. Consider replacing the link to the article with the video. reply dmvdoug 4 hours agoparentThank you for that. The background music was irritating, but it’s a really fantastic video! reply mkl 12 hours agoprevhttps://www.openstreetmap.org/?mlat=72.833333&mlon=-26.95&zo... Pretty typical-looking for the area, so it seems likely we will see more events like this as the glaciers recede. At some point we'll probably get video! Reminds me of this Norwegian movie: https://en.wikipedia.org/wiki/The_Wave_(2015_film) reply blackeyeblitzar 16 hours agoprev [8 more] [flagged] crystal_revenge 15 hours agoparentSeismological devices all around the world detected this. Hardly a sensational title: it was a 110 meter tsunami bouncing around between two fjords for 9 days! This is roughly the planetary equivalent of when you're a kid in the bath tub and you start getting tiny waves to go back and forth between two ends of the tub until you finally make a mess, only that initial wave is 110 meters high! reply blackeyeblitzar 14 hours agorootparentThose devices detecting it isn’t the same as what people expect when you read “shook earth”. They are after all, sensitive devices. And the height of the tsunami isn’t that interesting given the geography of the fjord. The historical records for high water marks tend to always be set in fjords. reply Cerium 16 hours agoparentprevIs it? The article seems to indicate that stations geographically remote from the fjord recorded an unusual signal that was later connected to the landslide event. reply kitd 13 hours agoparentprev [–] I mean, technically, \"seismology\" is all about shaking the earth. reply thebruce87m 12 hours agorootparent [–] The next level of pedantry is pointing out that it’s not limited to the earth. reply grues-dinner 9 hours agorootparent [–] And the level after that is making the distinction between \"earth\" and \"Earth\". And before the quartic pedants say \"well, all earth is on Earth\", there's some in planes, on the space station and probably a few molecules scattered by impacts and human space exploration. Quintic pedants will then question if, after being in space environments for long enough, a scrap of outgassed irradiated earth is still properly called earth? So isn't all \"real\" earth still gravitationally bound to the Earth? Sextic pedants may be the first to notice everyone else has left them to squabble and gone to the pub. reply rpozarickij 5 hours agorootparent [–] > isn't all \"real\" earth still gravitationally bound to the Earth Another (sub)level of pedantry: earth will always be bound to the Earth because gravitational force never really reaches zero no matter how far the objects are from each other thus earth can still be called earth? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In August 2023, a landslide in Greenland's Dickson Fjord caused a 110-meter-high tsunami, creating a standing wave that lasted for nine days.",
      "Seismologists initially identified the wave as an \"unidentified seismic object\" (USO) with a frequency of 11 millihertz, triggered by climate change-induced glacier thinning.",
      "The fjord's unique shape and features trapped the wave's energy, highlighting the significant impact of climate change on Earth's geological phenomena."
    ],
    "commentSummary": [
      "A Greenland landslide triggered a 110-meter high tsunami, initially noticed when a former employee saw an abandoned SIRIUS research station swept away after a cruise ship ran aground.",
      "The tsunami, initially 7 meters high, was detected within a week due to the cruise incident, although seismic data would have eventually revealed it.",
      "Seismological devices worldwide recorded the event, which lasted nine days, highlighting how random occurrences can lead to significant discoveries."
    ],
    "points": 89,
    "commentCount": 13,
    "retryCount": 0,
    "time": 1726184600
  },
  {
    "id": 41527378,
    "title": "Wallops: A modern IRC client for classic Mac OS",
    "originLink": "https://jcs.org/wallops",
    "originBody": "Wallops - A modern IRC client for classic Mac OS Wallops is a modern IRC client that I wrote on and for my Macintosh Plus. It is written for System 6 and should run on newer classic Mac OS versions. It requires MacTCP and supports multiple connections, channels, and query windows in a tabbed interface. When run under MultiFinder, it can run in the background using Notification Manager to flash the Apple menu icon when one's nick is mentioned while the window is hidden. Wallops is free software and its source code is available under an ISC license. I also have some videos about its development and C programming on System 6 in general. Latest Update: Wallops 2.0 Released (2024-09-12) Another large update to my Wallops IRC client is available: wallops-2.0.sit (StuffIt 3 archive, includes source code and THINK C 5 project file) SHA256: 532f6c72eadbb9e7ce74dded1bfcd71369a61d818f7c77160bb8a66d6f1ccf9c SHA1: c375a24e00900378e84745c1e11d3d6976ef749c This release features an overhaul of the interface bringing tabs allowing multiple channels and private message queries, including a number of other new features and bugfixes: Support window resizing, using an initial window size based on the screen size Connect-time settings (server, nick, password, etc.) are now saved to a preferences file in the System folder, rather than as resources on the binary itself; this prevents accidental sharing of saved passwords and avoids having to re-enter settings every time the software is updated Add per-user queries which simplify private messaging back and forth with a single user; can be opened with /queryor by double-clicking a user in a channel nick list Much improved tab and window redrawing Optimize nick list sorting and building, especially on large channels; on a large channel with 750 users, this operation went from 62 seconds to 33 on a Mac Plus Only auto-scroll a message buffer to new messages when not scrolled to the bottom, making it easier to read old messages on busy channels Implement support for many more server numerics and commands Implement additional commands like /clear, /op, /umode, and /disconnect (the full list is in the README) Add global menu options to suppress channel \"junk\" such as joins, quits/parts, and nick changes Make Desk Accessories work from the Apple Menu Do case-insensitive matching of slash commands Add a setting to suppress MOTD printing which can speed up connections on slower machines, since MOTDs can be very long on some servers Add a custom about dialog Previous Updates",
    "commentLink": "https://news.ycombinator.com/item?id=41527378",
    "commentBody": "Wallops: A modern IRC client for classic Mac OS (jcs.org)81 points by todsacerdoti 17 hours agohidepastfavorite22 comments lampiaio 11 hours agoSeeing new software being released for the classic Mac is freakin' awesome, please keep up the great work. I just wish Macintosh (either 68k or PowerPC) emulators improved a bit more, I no longer have a functioning machine and they're not getting any cheaper. reply lampreyface 3 hours agoparentMAME's Mac emulation has improved a lot recently. Give it a try. reply sen 10 hours agoprevThis is brilliant. I’ve been tempted to get my Macintosh SE online just for fun and this would be a great use for it. Permanent little IRC terminal. reply keepamovin 13 hours agoprevIt would be cool if this could be plugged into Infinite Mac: https://news.ycombinator.com/item?id=41499239 If they could be given network access, and permitted to install software, would be so awesome!!! :) reply jdboyd 12 hours agoprevI just love new software for old machines (especially multi-tasking systems). reply Cyberdog 9 hours agoprevLeave that page open in a tab without moving your mouse over the window for a couple minutes for a cute little treat. reply mrweasel 6 hours agoprevSo now the classic Mac has a nicer IRC client than the latest macOS. reply bitwize 13 hours agoprevI oughta show my wife this. She used ircle back in the day. reply pcdoodle 10 hours agoprevEven the icon is well thought out. reply bbarnett 10 hours agoparentThe example image is weird though. Where do you type?? I see no input field. (I'm not a mac guy, am I just missing it?) reply cpach 9 hours agorootparentMaybe the white bar below the tabs? reply bbarnett 8 hours agorootparentHuh. For some reason I thought that was a bottom window decoration, I think the button on the far right of it confused me, but that must be it. reply duskwuff 2 hours agorootparentAs a long-time Mac user, that bit of the screenshot confused me as well. It looks exactly like a disabled horizontal scroll bar would. reply anthk 7 hours agoprevI wonder how JCS didn't write a Gopher client. I know there are several, but he might add some nice features such as converting most Unicode chars to Mac-Roman or whatever did Mac OS use in the 90's. reply bjoli 11 hours agoprevWriting software for old systems can sometimes be frustrating, but mostly it is SO nice. The target was usually quite simple and you don't have to deal with all the extra obstacles and choices that are obligatory today when running software on a bazillion different systems. Not even having the option to deal with unicode can be such a breath of fresh air. reply szszrk 11 hours agoparentI recall era before UTF8 wide adoptiont to be a mess of many other encodings, multiple even in the same language... Not a refreshing memory for me. reply koito17 10 hours agorootparentEven today, Japanese text online and elsewhere uses a variety of encodings. It's sometimes a frustrating experience. Compared to other languages, it is common to see non-Unicode encodings. In particular, Shift JIS (actually, the Windows-specific variant called Windows-932, which is incompatible with JIS X 0213:2004, called \"Shift JIS 2004\". Old enough Unix and Linux servers used EUC-JP, and Mac OS has its own encoding). reply bjoli 10 hours agorootparentprevWell, for something like an IRC client it is up for the user to set the correct one. reply szszrk 10 hours agorootparent:) what is the correct one? It's different for each server, or even channel. IRC clients usually supported such custom settings. But then there was the matter of the other end of your screen - your terminal and maybe screen session I which it run. I used to have an IRC config that was different for each server and sometimes channel, then displayed that in one shared encoding with other chat apos, then converted that on the fly with screen to match encoding of other apps. Now I just had to make sure to convert encodings and line endings of all config files, any text file, or even source code and scripts... Nowadays you just paste emojis in code and it's there, it works anyone, including the user. Huge change. reply finaard 9 hours agorootparentThe correct one is the one that doesn't get you yelled at and possibly banned by the rest of the channel. reply bjoli 7 hours agorootparentYes. Today's messaging services require no skills whatsoever. Getting yelled at the first time you log in establishes pecking order and ensures people weigh their words. Discord doesn't have these kind of safeguards. reply veltas 11 hours agoprev [–] Tried reading the source code on their website and every line of source has a scrollbar under it, making it very difficult to read. Not sure what caused this, zooming out doesn't help. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Wallops, a modern IRC client for classic Mac OS, has released version 2.0, compatible with System 6 and newer versions, and includes significant updates and bug fixes.",
      "Key features include a tabbed interface for multiple connections, channels, and private messages, window resizing, and optimized nick list sorting for large channels.",
      "Wallops 2.0 also introduces new commands, improved interface elements, and performance enhancements, making it a robust tool for IRC users on classic Mac systems."
    ],
    "commentSummary": [
      "Wallops is a modern IRC (Internet Relay Chat) client designed for classic Mac OS, generating interest among enthusiasts of vintage computing.",
      "The release has sparked excitement due to the rarity of new software for old systems, with users reminiscing about their experiences with classic Macs.",
      "Some users have noted improvements in Mac emulation, suggesting tools like MAME (Multiple Arcade Machine Emulator) for those without functioning vintage hardware."
    ],
    "points": 81,
    "commentCount": 22,
    "retryCount": 0,
    "time": 1726192270
  }
]
