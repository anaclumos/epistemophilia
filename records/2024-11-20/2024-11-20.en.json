[
  {
    "id": 42191228,
    "title": "Let's Encrypt is 10 years old now",
    "originLink": "https://letsencrypt.org/2014/11/18/announcing-lets-encrypt/",
    "originBody": "Vital personal and business information flows over the Internet more frequently than ever, and we don’t always know when it’s happening. It’s clear at this point that encrypting is something all of us should be doing. Then why don’t we use TLS (the successor to SSL) everywhere? Every browser in every device supports it. Every server in every data center supports it. Why don’t we just flip the switch? The challenge is server certificates. The anchor for any TLS-protected communication is a public-key certificate which demonstrates that the server you’re actually talking to is the server you intended to talk to. For many server operators, getting even a basic server certificate is just too much of a hassle. The application process can be confusing. It usually costs money. It’s tricky to install correctly. It’s a pain to update. Let’s Encrypt is a new free certificate authority, built on a foundation of cooperation and openness, that lets everyone be up and running with basic server certificates for their domains through a simple one-click process. Mozilla Corporation, Cisco Systems, Inc., Akamai Technologies, Electronic Frontier Foundation, IdenTrust, Inc., and researchers at the University of Michigan are working through the Internet Security Research Group (“ISRG”), a California public benefit corporation, to deliver this much-needed infrastructure in Q2 2015. The ISRG welcomes other organizations dedicated to the same ideal of ubiquitous, open Internet security. The key principles behind Let’s Encrypt are: Free: Anyone who owns a domain can get a certificate validated for that domain at zero cost. Automatic: The entire enrollment process for certificates occurs painlessly during the server’s native installation or configuration process, while renewal occurs automatically in the background. Secure: Let’s Encrypt will serve as a platform for implementing modern security techniques and best practices. Transparent: All records of certificate issuance and revocation will be available to anyone who wishes to inspect them. Open: The automated issuance and renewal protocol will be an open standard and as much of the software as possible will be open source. Cooperative: Much like the underlying Internet protocols themselves, Let’s Encrypt is a joint effort to benefit the entire community, beyond the control of any one organization. If you want to help these organizations in making TLS Everywhere a reality, here’s how you can get involved: Sponsor ISRG Help Us Build Let’s Encrypt To learn more about the ISRG and our partners, check out our About page.",
    "commentLink": "https://news.ycombinator.com/item?id=42191228",
    "commentBody": "Let's Encrypt is 10 years old now (letsencrypt.org)448 points by gslin 12 hours agohidepastfavorite166 comments mrtksn 11 hours agoHands down one of the greatest services out there, stopped a racket and made the internet secure. I remember a time when having an HTTPS connection was for \"serious\" projects only because the cost of the certificate was much higher than the domain. You go commando and if it sticks then you purchase a certificate for a 100 bucks or something. reply dachris 11 hours agoparentThere's still enough people out there who don't know better, manually (or auto-renew) purchasing new a certificate every year from their hosting provider like it's 2013. reply karel-3d 10 hours agorootparentI have dealt with banking environment when they required SSL with at least 1-year validity on the callback API URL. Which excluded Let's Encrypt. We were looking for a SSL provider that had > 1 year old certs AND supported ACME... for some reason we ended up with SSL.com that did support ACME for longer lasting certs; however, there was some minor incompatibilities in how kubernetes cert-manager implemented ACME and how SSL.com implemented ACME; we ended up debugging SSL.com ACME protocol implementation. Fun. We should have just clicked once per 3 years, better than debugging third parties APIs. No, I don't remember the details and they are all lost in my old work emails. (Nowadays I think zerossl.com also supports ACME for >1 year certs? but they did not back then. edit: no they still don't, it's just SSL.com I think) reply lol768 9 hours agorootparent> I have dealt with banking environment when they required SSL with at least 1-year validity on the callback API URL Why are (some) banks always completely clueless about these things? Validating ownership of the domain more often (and with an entirely automated provisioning set-up that has no human weak links) can only be a good thing. Perhaps the banking sector will finally enter the 21st century in another ten years? reply karel-3d 8 hours agorootparentThe banking sector usually goes with \"checkbox security\". They have these really, really long lists what all needs to be secured and how. Some of it is reasonable, some of it is bonkers, there is way too much of that stuff, and it overall increases the price of any solution 10x at least. But OTOH I can hardly blame them, failures can be catastrophic there, as they deal with real money directly and can be held liable for failures. So they don't really care about security, and more about covering their asses. reply dspillett 7 hours agorootparent> some of it is bonkers Some of it is truly bonkers and never was good practise, but much of the irritating stuff is simply out-of-date advice. The banks tend to be very slow to change unless something happens that affects (or directly threatens to affect) the bottom line, or puts them in the news unfavourably. Of course some of it is bonkers, like HSBC and FirstDirect changing the auth for my personal accounts from “up to 9 case-sensitive alpha-numeric characters” (already considered bad practise for some years) to “6 digits”, and assuring me that this is just as secure as before… reply dizhn 7 hours agorootparentThat sounds like \"we were truncating your id and pass before anyway\". reply dspillett 6 hours agorootparentI don't think so, because it would also imply they were also throwing away anything non-numeric, and I really hope nothing that stupid was going on. When the change happened everyone had to establish a new password. I read it as “we have been asked to integrate an ancient system that we can't update (or more honestly in many cases: can't get the higher-ups to agree to pay to update), so are bringing out other systems down to the lowest common denominator”. That sort of thing happens too often when two organisations (or departments within one) that have different procedures, merge or otherwise start sharing resources they didn't previously. reply Veen 8 hours agorootparentprevThe problem is more likely one of regulation than technical knowledge. Banks hire very smart people who know that a lot of what they do is bullshit, but they're paid to comply with banking and security regulations that lag a long way behind technical advances. Banks are also inherently conservative in their technical choices, and for good reason. reply JoshTriplett 9 hours agorootparentprev> I have dealt with banking environment when they required SSL with at least 1-year validity on the callback API URL. Which excluded Let's Encrypt. I wonder if this would be an opportunity for revenue for Let's Encrypt? \"We do 90-day automated-renewal certificates for free for everyone. If you're in an unusual environment where you need certificates with longer validity, we offer paid services you can use.\" reply karel-3d 8 hours agorootparentIf they want to do something commercial, they should go for the code signing certificates, that stuff is still a racket. reply account42 9 hours agorootparentprevProbably better to keep LE / ISRG completely non-profit. Adding a profit motive has too big of a chance to end with actually security-relevant features being gated behind payment eventually. reply JoshTriplett 8 hours agorootparentIt's less about the profit motive, and more about removing the remaining incentives to stay outside the ACME ecosystem. The funding would be to provide additional infrastructure (e.g. revocation servers for longer-lasting certificates), and to fund new such efforts. reply account42 8 hours agorootparentBut once there is an income stream from issuing certificates there is an incentive to increase it which will quickly find itself at odds with the primary missions of providing secure connections to as many people as possible. Making infrastructure depend on that income stream only increases that incentive. Perhaps you trust the ISRG to resist the temptaton but as far as I know they are run by humans. reply JoshTriplett 7 hours agorootparentThere are many, many opportunities in both the business and non-profit world to make more money by screwing your customers/users, and despite that, it does not always happen. Businesses and non-profits are built on the trust of users (or built in spite of the utter lack of it, e.g. Comcast). I don't think they should be afraid to provide things users need. It is, in fact, possible to choose and keep choosing to maintain the trust of your users. I think there's still incentive alignment here. Getting people moved from the \"purchase 1 year certificate\" world (which is apparently still required in some financial contexts) into the ACME-based world provides a path for making a regulatory argument that it'd be easy for such entities to switch over to shorter-lived certificates because the ACME infrastructure is right there. reply Nekit1234007 6 hours agorootparentprevI'm pretty sure ISRG doesn't want to deal with payments any more than they do now (i.e. outside of donations and sponsorships) reply mrtksn 10 hours agorootparentprevAFAIK there's things like Extended Validation Certificate Verification that used to make the browser address bar look more trustworthy by making it green but I don't know if its still a thing. At least in Safari, I don't see a green padlock anywhere. reply mrweasel 10 hours agorootparentI remember our boss really wanted that green bar, so we got an extend validation certificate. What we had failed to realise is that they would only be issued to the actual legal name of your company, but not any other names you may be operating under. We had a B2C webshop, where we wanted the ev-cert, but because the B2C side of the business wasn't it's own legal entity, the cert we go issued was for our B2B name, which none of our customer customers knew and it looked like a scam. The only good thing dealing with certificate resellers at the time was that they where really flexible in a lot of ways. We got our EV cert refunded, or \"store credit\" and used the money to buy normal certificates. reply Propelloni 10 hours agorootparentprevThey are still there, but most browsers don't do anything with it anymore since 2019, when Firefox and Chrome stopped caring. There are some scenarios where you still have to employ EV certificates, e.g. code signing. reply bux93 10 hours agorootparentprevChrome 77 removed the prominent green EV badge. \"A series of academic research in the 2000s studied the EV UI in lab and survey settings, and found that the EV UI was not protecting against phishing attacks as intended. The Chrome Security UX team recently published a study that updated these findings with a large-scale field experiment, as well as a series of survey experiments.\" [1] Extended Validation can still play a role in a corporate's IT control framework; the extended validation is essentially a check-of-paperwork that then doesn't need to be performed by your own auditor. Some EV certificates also come with some (probably completely useless) liability insurance. [1] https://chromium.googlesource.com/chromium/src/%2B/HEAD/docs... reply duskwuff 10 hours agorootparent> Some EV certificates also come with some (probably completely useless) liability insurance. Warranties / insurance on SSL certificates typically only pay out if a certificate is issued improperly, often in conjunction with other conditions like a financial loss directly resulting from the misissuance. Realistically, any screwup serious enough to result in that warranty paying out would also result in the CA being abruptly removed from browser root certificate programs. reply Systemmanic 10 hours agorootparentprevChrome and Firefox removed the extra UI stuff for EV certs in 2019: https://groups.google.com/a/chromium.org/g/security-dev/c/h1... https://groups.google.com/g/firefox-dev/c/6wAg_PpnlY4 reply sureIy 10 hours agorootparentprevYeah that also stopped being a thing. I'm really happy how Chrome and then other browsers gradually shifted the blame to insecure websites rather than highlighting \"secure\" ones. You'll still find people online clamoring EV certificates are worth anything more than $0 but you can ignore them just as well. reply tannhaeuser 9 hours agorootparentprevHuh? EV certificates are actually certifying you're the (juristical) person you're claiming to be based on ID and trade register checks, unlike Let's Encrypt certificates which only certify you're in possession of a domain. Isn't using EV certificates legally required for e-commerce web sites at least in parts of the world, and also obligatory for rolling out as MasterCard/Visa merchant by their anti-fraud requirements along with vulnerability checks and CI/site update processes being in place? reply khuey 8 hours agorootparent> Isn't using EV certificates legally required for e-commerce web sites at least in parts of the world Not in any jurisdiction I'm aware of, though it's a big world so it wouldn't shock me if some small corner of it has bad laws. > and also obligatory for rolling out as MasterCard/Visa merchant by their anti-fraud requirements PCI DSS does not require EV certificates. reply _betty_ 9 hours agorootparentprevthey were also pretty bad for performance due to the extra lookup (and reduction in caching) reply account42 9 hours agorootparentWhat extra lookup. AFAIU they are just like normal certificates but with a \"customer paid extra\" flag. reply _betty_ 9 hours agorootparenthttps://simonhearne.com/2020/drop-ev-certs/ reply _betty_ 9 hours agorootparentprevthey normally require a revocation lookup on the spot, and iirc there was differences in if they could or how stapling worked. reply account42 8 hours agorootparentInteresting. Sounds like a cost that is entirely reasonable for use cases like online banking though. reply irjustin 10 hours agorootparentprevRelated point - we interface with Singapore gov services (MYINFO). They don't recognize LE nor AWS's certs. Only the big paid ones. Such an annoying process too - to pay, to obtain and update the certs. reply tialaramex 10 hours agorootparentI guess the good thing there is that it's absolutely transparent that this is just a way to make you pay somebody else. Like the Jones Act (Merchant Marine Act, but everybody just calls it the Jones Act). The US government doesn't get a slice if you want to buy ships to move stuff from one part of the US to another, but it does require that you buy the ships from an American shipyard, and so those yards needn't be internationally competitive because the US government has their back. Nobody is like \"Oh, the Jones Act ensures high quality ships\" because it doesn't, the Jones Act just ensures that you're going to use those US shipyards, no matter what. reply vmit 9 hours agorootparentprevMyinfo did away with certificate requirement altogether! Yay! (hello from Singapore) reply ta1243 10 hours agorootparentprevOur company bans the use of letsencrypt because of the legal terms. Nobody at the CxO level will sign off on it, so we end up paying whatever to globalsign. reply seszett 10 hours agorootparentWhat legal terms do they find objectionable? What about ZeroSSL, which is basically interchangeable with Let's Encrypt? reply wink 8 hours agorootparentprevDoesn't necessarily have anything to do with knowing, some environments are just not worth automating or support it so badly that even paying twice or more would still be nothing compared to the annoyance. It's been getting better over the years though. reply yread 10 hours agorootparentprevUnfortunately, the code signing certificates work pretty much the same way reply technion 10 hours agorootparentprevI deal with multiple enterprise applications where idea of scripting a renewal involves playing with scripting headless Chrome. I'm really not a fan of it but I'm happier paying for a one year cert than doing that reply yurishimo 10 hours agorootparentSorry if this is a dumb question, but why? If I'm not mistaken, Let's Encrypt supports validation via DNS now so you don't even need to have a working webserver to issue a certificate. Automating a script to perform a renewal should be much simpler than headless Chrome! If your DNS provider doesn't have an API, that seems like a separate issue but one that is well worth your organization's time if you're working in the enterprise! reply patrakov 10 hours agorootparentI guess it is not about renewal but about certificate deployment. reply blipvert 9 hours agorootparentprevYou can set up the _acme-challenge (or whatever it is)as a CNAME to point to a domain which does support an API for automating the renewal (looking in to setting this up for a bunch of domains at work) reply technion 8 hours agorootparentprevObtaining a certificate via dns doesn't help you install it via a Web interface that takes 20+ clicks and a 15 minute reboot to apply . reply pastage 7 hours agorootparentAnd open a ticket on a suppliers website, click through four pages with free text input, then send certificate via email. Lets not talk about key delivery. We will get back the admin cost and of all that in a year if we tunnel them through one of our LBs. reply christophilus 6 hours agoparentprevI had a lazily configured proxy which would request a cert for any domain you threw at it. An attacker figured this out and started peppering it with http requests with randomly generated subdomains prefixed. When I discovered it, my first thought wasn’t, “Oh, I hope I didn’t get flagged by Let’s Encrypt.” It was, “Oh, man. I feel really bad that my laziness caused undue load on Let’s Encrypt.” Let’s Encrypt is the best thing to happen to the web in at least a decade. reply dewey 7 hours agoparentprevAt least there were some services where you could get a single domain \"real\" certificate for free before (But a complicated and annoying process) but then if you wanted a wildcard certificate to cover a bunch of subdomains for personal projects it became really expensive. Glad this problem just got completely resolved. reply BiteCode_dev 7 hours agoparentprevMozilla is getting a lot of criticism, but just for letsencrypt alone they are making the world a better place. Before them I never used SSL for anything, because the cost/benefit ratio was just not there for my services. Since then, I never not use it. reply qskousen 6 hours agorootparentTwo Mozilla employees were involved in starting Let's Encrypt, and the Mozilla foundation is one of the sponsors, but as far as I can tell the foundation was not directly involved in creating it. reply dtquad 8 hours agoparentprevGoogle/Chrome and Firefox also deserve credit for making a free and open CA viable. reply jaas 7 hours agoprevWe consider our ten year anniversary to be in 2025 but I appreciate the kind words here! Today is roughly the ten year anniversary of when we publicly announced our intention to launch Let's Encrypt, but next year is the ten year anniversary of when Let's Encrypt actually issued its first certificate: https://letsencrypt.org/2015/09/14/our-first-cert/ In December of 2015 (~9 years ago today) is was made available to everyone, no invitation needed: https://letsencrypt.org/2015/12/03/entering-public-beta/ reply nadermx 3 hours agoparentIt's actually serendipitous that it happened exactly on December 2015. That's when I had only enough for a domain, but not for a ssl, and my site needed an ssl. Thanks to let's encrypt free ssl, the project hit. reply pests 11 hours agoprevIt feels like just yesterday I was paying for certs, or worst, just running without. Can't believe its been ten years. reply ozim 11 hours agoparentCan’t believe there are still anti TLS weirdos. reply Pannoniae 10 hours agorootparentTLS is not panacea and it's not universally positive. Here are some arguments against it for balance. TLS is fairly computationally intensive - sure, not a big deal now because everyone is using superfast devices but try browsing the internet with a Pentium 4 or something. You won't be able to because there is no AES instruction set support accelerating the keyshake so it's hilariously slow. It also encourages memoryholing old websites which aren't maintained - priceless knowledge is often lost because websites go down because no one is maintaining them. On my hard drive, I have a fair amount of stuff which I'm reasonably confident doesn't exist anywhere on the Internet anymore.... if my drives fail, that knowledge will be lost forever. It is also a very centralised model - if I want to host a website, why do third parties need to issue a certificate for it just so people can connect to it? It also discourages naive experimentation - sure, if you know how, you can MitM your own connection but for the not very technical but curious user, that's probably an insurmountable roadblock. reply ozim 8 hours agorootparent*It also discourages naive experimentation* that's the point where if you put on silly website no one can easily MitM it when its data is sent across the globe and use 0-day in browser on \"fluffy kittens page\". Biggest problem that Edward Snowden uncovered was - this stuff was happening and was happening en-mass FULLY AUTOMATED - it wasn't some kid in basement getting MitM on your WiFi after hours of tinkering. It was also happening fully automated as shitty ISPs were injecting their ads into your traffic, so your fluffy kittens page was used to serve ads by bad people. There is no \"balance\" if you understand bad people are going to swap your \"fluffy kittens page\" into \"hardcore porn\" only if they get hands on it. Bad people will include 0-day malware to target anyone and everyone just in case they can earn money on it. You also have to understand don't have any control through which network your \"fluffy kitten page\" data will pass through - malicious groups were doing multiple times BGP hijacking. So saying \"well it is just fluffy kitten page my neighbors are checking for the photos I post\" seems like there is a lot of explaining on how Internet is working to be done. reply account42 8 hours agorootparent> It also discourages naive experimentation that's the point where if you put on silly website no one can easily MitM it when its data is sent across the globe and use 0-day in browser on \"fluffy kittens page\". Transport security doesn't make 0-days any less of a concern. > It was also happening fully automated as shitty ISPs were injecting their ads into your traffic, so your fluffy kittens page was used to serve ads by bad people. That's a societal/legal problem. Trying to solve those with technological means is generally not a good idea. > There is no \"balance\" if you understand bad people are going to swap your \"fluffy kittens page\" into \"hardcore porn\" only if they get hands on it. Bad people will include 0-day malware to target anyone and everyone just in case they can earn money on it. The only people who can realistically MITM your connection are network operators and governments. These can and should be held accountable for their interference. You have no more security that your food wansn't tampered with during transport but somehow you live with that. Similarly security of physical mail is 100% legislative construct. > You also have to understand don't have any control through which network your \"fluffy kitten page\" data will pass through - malicious groups were doing multiple times BGP hijacking. I don't but my ISP does. Solutions for malicious actors interfering with routing are needed irrespective of transport security. > So saying \"well it is just fluffy kitten page my neighbors are checking for the photos I post\" seems like there is a lot of explaining on how Internet is working to be done. Not at all - unless you are also epecting them to have their fluffy kitten postcards checked for Anthrax. In general, it is security people who often need to touch grass because the security model they are working with is entirely divorced from reality. reply ozim 7 hours agorootparentAll I got from your explanation is: I am going to cross the street in front of that speeding car because driver will be held liable when I get hit and die. If there is not even a possibility to hijack the traffic whole range of things just won’t happen. And holding someone liable is not the solution. reply account42 7 hours agorootparentThe situation is more akin to demanding that pedestrians should be prevented from crossing the road at all cost because a malicious driver could ignore all red lights. And of course banning pedestrias ins't enough. After all, motorcyles are also pretty unsafe so we ban those too. But you see someone could also be pointing a bazooka at the road so then we require all cars to have sufficient armor plating in order to be allowed on the road. That is, before realizing that portable nukes exists and you never know who has one. We don't do that. Instead we develop specific solutions (e.g. an over/underpass for high risk intersections, walls for highways) where they are actually needed without loosing sight of the unreasonable cost (not just monetary) that demanding zero risk would impose. reply ozim 4 hours agorootparentFor me TLS is an overpass - yeah it costs more to build it, pedestrians have to climb the stairs to get on the other side but it is worth it. Then hopefully we have Let's Encrypt that can be an elevator/lift so pedestrians don't have to climb the stairs. But that analogy of course runs dry rather quick because you can look both ways when crossing street - on the internet as I mentioned you cannot control where data flows and bad actors already proven that they are doing so. This is why it is not like overpass that you can build where the need is - because for internet traffic the need is everywhere. reply wizzwizz4 7 hours agorootparentprevTechnological measures don't make things impossible: they make them harder. And they rarely solve all the consequences of a problem: only the ones that have been explicitly identified. reply hehehheh 6 hours agorootparentprevCounterpounts: > Transport security doesn't make 0-days any less of a concern. It does. Each layer of security doesn't eliminate the problem but does make the attack harder. Mail and food are different in that there are not limitless scalable attacks that can originate anywhere around the globe. reply OkayPhysicist 5 hours agorootparentprev> transport security doesn't make 0-days any less of a concern. It does make the actual execution of said attacks significantly harder. To actually hit someone's browser, they need to receive your payload. In the naive case, you can stick it on a webserver you control, but how many people are going to randomly visit your website? Most people visit only a handful of domains on a regular visit, and you've got tops a couple days before your exploit is going to be patched. So you need to get your payload into the responses from those few domains people are actually making requests from. If you can pwn one of them, fantastic. Serve up your 0-day. But those websites are big, and are constantly under attack. That means you're not going to find any low-hanging fruit vulnerability-wise. Your best bet is trying to get one of them to willing serve your payload, maybe in the guise of an ad or something. Tricky, but not impossible. But before universal https, you have another option: target the delivery chain. If they connect to a network you control? Pwned. If they use a router with bad security defaults that you find a vulnerability in? Pwned. If they use a small municipal ISP that turns out to have skimped on security? Pwned. Hell, you open up a whole attack vector via controlling an intermediate router at the ISP level. That's not to mention targeting DNS servers. HTTPS dramatically shrinks the attack surface for the mass distribution unwanted payloads down to basically the high-traffic domains and the CA chain. That's a massive reduction. > The only people who can realistically MITM your connection are network operators and governments. Literally anyone can be a network operator. It takes minimal hardware. Coffee shop with wifi? Network operator. Dude popping up a wifi hotspot off his phone? Network operator. Sketchy dude in a black hoodie with a raspberry pi bridging the \"Starbucks_guest\" as \"Starbucks Complimentary Wifi\"? Network operator. Putting the security of every packet of web traffic onto \"network operators\" means drastically reducing internet access. > You have no more security that your food wasn't tampered with during transport but somehow you live with that. I've yet to hear of a case where some dude in a basement poisoned a CISCO truck without having to even put on pants. Routers get hacked plenty. HTTPS is an easy, trivial-cost solution that completely eliminates multiple types of threats, several of which are either have major damage to their target or risk mass exposure, or both. Universal HTTPS is like your car beeping at you when you start moving without your seat belt on: kinda annoying when you're doing a small thing in tightly controlled environments, but has an outstanding risk reduction, and can be ignored with a little headache if you really want to. reply dspillett 6 hours agorootparentprev> It is also a very centralised model I can see why the centralisation is suboptimal (or even actively bad if I'm feeling paranoid!), but other schemes (web of trust, etc.) tend to end up far more complicated for the end user (or their UA). So far no one has come up with a practical alternative without some other disadvantage that would block its general adoption. > if I want to host a website, why do third parties need to issue a certificate for it just so people can connect to it? Because if we don't trust those few 3rd parties, we end up having to effectively trust every host on the Internet, which means trusting people and trusting all the people is a bad idea. Some argue that needing a trusted certificate for just a personal page is extreme, but this one of those cases where the greater good has to win out. For instance: if we train people that self-signed certs are fine to trust in some circumstances, they'll end up clicking OK to trust them in circumstances where they really shouldn't. This can seem a bit nanny-ish, but people are often dumb, or just lazy to the point where it is sometimes indistinguishable from dumb (I'm counting myself here!) so need a bit of nannying. And anyway, if your site doesn't take any input then no browser will (yet) complain about plain HTTP. > It also discourages naive experimentation When something could affect security, discouraging naive experimentation on the public network is a good thing IMO. Do those experiments more locally, or at least on hosts you don't expect the public to access. reply chaxor 4 hours agorootparentI agree that centralization is bad, and one of the worst parts of HTTPS (the other being that things like ed22519 systems, chacha20, poly1305, sntrup are generally viewed as better modern alternatives to AES, so postquantum system like rosenpass https://github.com/rosenpass/rosenpass are more preferable). However, I think there is no reason at all that a system that is decentralized is not far _far_ simpler to instantiate for a user (not to mention far more secure and private). Crypto gets a lot of hate on HN, but it seems that it is mostly due to people's dislike of anything dealing with 'currency' systems or financial that touch it. This is a despised opinion here, but I am still actually excited for crypto systems that solve real world problems like TLS certs, DNS, et al. Iroh seems like a _fantastic_, phenomenal system to showcase this idea. It allows for a very fast decentralized web experience on modern cryptography such as Blake3, QUIC, and so on but doesn't really touch any financial stuff at all. Its simply a good system. I hope we can slowly move to a system that uses the decntralized consensus algorithms created in the crypto space to remove the trust in (typically big, corporate, and likely backdoored) centralized entities that our system today _requires_ without any alternative. reply account42 8 hours agorootparentprevI find the lack of backwards compatibility also concerning - and that is not something can be fixed as deprecation of old SSL/TLS versions and ciphers is intentional. Beyond that, TLS is also adds additional points of failure. For one, it preventing users from accessing websites that are still operational but have an outdated cert or some other configuration issue. And HSTS even requires browsers to deprive users of the agency to override default policies and access the site anyway. TLS is also a complex protocol with complex implementations that are prone to can bring their own security issues, e.g. heartbleed. There are also many cases where there are holes in the security. E.g. old HTTP links, even if they redirect to HTTP, provide an opportunity for interception. Similarly entering domain names without a scheme requires Browsers to either allow downgrade to HTTP or break older sites. The solutions to this (mainly HSTS and HSTS preload) don't scale and bring many new issues (policy lifetimes outlive domain ownership, taking away user agency). In my ideal world a) There would be no separate HTTPS URL scheme for secure connections. Cool URIs don't change and the transport security doesn't change the resource you are addressing. A separate protocol doesn't prevent downgrade attacks in all cases anyway (old HTTP URLS, entering domains in the address bar, no indication of TLS version and supported ciphers in the scheme). b) Trust should be provided in a hierarchical manner, just like domains themselves - e.g. via DNSSEC+DANE. c) This mechanism would also securely inform browsers about what protocols and ciphers the server supports to allow for backwards compatiblity with older clients (where desired) while preventing downgrade attacks on modern clients. d) Network operators that interfere with the transmitted data are dealth with legal means (loss of common carrier status at the very least, but ideally the practice should be outright illegal). Unecrypted connections shouldn't allow service providers to get away with scamming you. reply Sesse__ 9 hours agorootparentprevThe handshake doesn't primarily depend on AES; it is typically a Diffie-Hellman variant (which doesn't have any acceleration) that takes time. Anyway, you're hopefully using TLS 1.3 by now, where you can use ChaCha20 instead of AES :-) reply ratorx 9 hours agorootparentprev> if I want to host a website … The fundamental problem is a question of trust. There’s three ways: * Well known validation authority (the public TLS model) * TOFU (the default SSH model) * Pre-distribute your public keys (the self-signed certificate model) Are there any alternatives? If your requirement is that you don’t want to trust a third party, then don’t. You can use self-signed certificates and become your own root of trust. But I think expecting the average user to manually curate their roots of trust is a clearly terrible security UX. reply xorcist 9 hours agorootparent> Are there any alternatives? The obvious alternative would be a model where domain validated certificates are issued by the registrar and the registrar only. Certificates should reflect domain ownership as that is the way they are used (mostly). There is a risk that Let's Encrypt and other \"good enough\" solutions takes us further from that. There are also many actors with economic interest in the established model, both in the PKI business and consultants where law enforcement are important customers. reply ratorx 9 hours agorootparentHow would you validate whether a certificate was signed by a registrar or not? If the answer is to walk down the DNS tree, then you have basically arrived at DNSSEC/DANE. However I don’t know enough about it to say why it is not more widely used. reply xorcist 8 hours agorootparentHow do you validate any certificate? You'd have to trust the registrar, presumably like you trust any one CA today. The web browsers do a decent job keeping up to date with this and new top domains aren't added on a daily basis anyway. Utilizing DNS, whois, or a purpose built protocol directly would alleviate the problem altogether but should probably be done by way of an updated TLS specification. Any realistic migration should probably exist alongside the public CA model for a very long time. reply rocqua 9 hours agorootparentprevThere is web of trust, where you trust people that are trusted by your friends. There's issues with it, but it is an alternative model, and I could see it being made to work. reply ratorx 9 hours agorootparentAh, I forgot about that and never really considered it because GPG is so annoying to use, but it is fairly reasonable. I don’t see how it has too many advantages (for the internet) over creating your own CA. If you have a mutually trusted group of people, then they can all share the private key and sign whatever they trust. I think the main problem is that it doesn’t scale. If party A and party B who have never communicated before want to communicate securely (let’s say from completely different countries), there’s no way they would be able to without a bridge. With central TLS, despite the downsides, that is seamless. reply account42 8 hours agorootparentprevProviding initial trust via hyperlinks could be interesting. reply MrGreenTea 10 hours agorootparentprevRegarding the stuff you safe guard: what are your reasons for not sharing them somehow to prevent that loss when (not if) your drive fails? reply Pannoniae 10 hours agorootparentI mean, I do! The music I have I put on Soulseek, although the more obscure stuff hasn't been downloaded yet. I also have fairly old video game mods - I don't even know where to share them or if anyone would be interested at all. reply account42 8 hours agorootparentYou could try to upload them to modding sites (preferrably not onces with a longin requirement for downloading) if you don't want to host them yourself. That can be either general modding archives or game-specific community sites - the latter are smaller but more likely to be interested in older mods. Make sure that whatever host you use can be crawled by the internet archive. Interest is probably going to be low but not zero - I often play games long after they have been released and sometimes intentionally using older versions that are no longer supported by current mods. reply Pannoniae 6 hours agorootparentYou are entirely right - although I'd have to be careful with uploading it and where because on Steam Workshop, there's assholes who threaten to DMCA you without basis and there are similar problems on other sites too. But I'll look around :) reply tomalbrc 9 hours agorootparentprevThe Internet Archive? reply michaelt 7 hours agorootparentprevI am 99% in favour of widespread use of TLS - but the reality is it means the web only works at the whim of the CA/Browser Forum. And some members of the forum are very eager to flex their authority. If I do everything perfectly, but the CA I used makes some trivial error which, in the case of my certificate, has no real-world security impact? They can send me an e-mail at 6:40 PM telling me they're revoking my certificate at 2:30 PM the next day. Just what you want to find in your inbox when you get in the next day. I hope you weren't into testing, or staged rollouts, or agreeing deployment windows with your users - you'd better YOLO that change into production without any of that. Even though it wasn't your mistake, and there's no suggestion you shouldn't have the certificate you have. As far as the CA/B Forum is concerned, safety-critical systems that can't YOLO changes straight into production with minimal testing and only a few hours of notice don't belong on their PKI infrastructure. You'd better jump to it and fix their mistake right now. reply account42 7 hours agorootparentI'm probably more critical of TLS in general than you are, but to be fair to LE one of their biggest contributions has been to change certificate updates from a deployment to something that should happen automatically during normal operations. If you have things setup the recommended way your daily certbot/etc run will simply pick up a new certificate and loat it into whatever servers that need it without you having to lift a finger. Of course in practice it doesn't always work out that way. reply michaelt 5 hours agorootparentA daily certbot run won't protect you if the CA discovers the problem at 2pm (starting the 24 hour revocation timer) but they only have a fix rolled out by 6pm. Anyone whose certbot run was between 2pm and 6pm would get their cert revoked the next day at 2pm anyway - even if it was only issued 18 hours ago. There's also a higher level question: Is this the web we want to be building? One where every site and service has to apply for permission to continue existing every 24 hours? Do we want a web where the barrier to entry for hosting is a round-the-clock ops team, complete with holiday cover? And if you don't have that, you should be using Facebook or Twitter instead? reply hehehheh 6 hours agorootparentprevHopefully you terminate TLS far away from your app code so rolling that out to prod is a non issue. But I get your point! reply dijit 11 hours agorootparentprevThe digital equivalent of a local kebab shop menu does not need encryption. The lack of understanding from us as technologists for people who would have had a working site and are now forced into either: an oligopoly of site hosting companies, or, for their site to break consistently as TLS standards rotate is one thing that brings me shame about our community. You can come up with all kinds of reasons to gatekeep website hosting, “they have to update anyway” even when updating means reinstallion of an OS, “its not that hard to rotate” say people with deep knowledge of computers, “just get someone else to do it” say people who have a financial interest in it being that way. Framing people with legitimate issues as weirdo’s is not as charming as you think it is. reply johannes1234321 10 hours agorootparentTLS doesn't just hide the information transmitted, but also ensures the integrity. Thus nobody on the network tinkered with the prices on the menu. Also the Kebap Shop probably has a form for reservation or ordering, which takes personal information. True, they are all low risk things, but getting TLS is trivial (since many Webservers etc can do letsencrypt rotation fully automatically) and secure defaults are a good thing. reply dijit 10 hours agorootparentThere are plenty of websites that were just static pages used for conveying information. Most people who set them up lacked the ability to turn them into forms that connected to anything. They’ve nearly all been lost to time now though, if a shop has a web-presence it will be through a provider such as “bokabord”, doordash, ubereats (as mentioned), some of whom charge up to 30% of anything booked/ordered via the web. But, I guess no MITM can manipulate prices… except, by charging… reply matrss 10 hours agorootparent> There are plenty of websites that were just static pages used for conveying information. If you care about the integrity of the conveyed information you need TLS. If you don't, you wouldn't have published a website in the first place. A while back I've seen a wordpress site for a podcast without https where people also argued it doesn't need it. They had banking information for donations on that site. Sometimes I wish every party involved in transporting packets on the internet would just mangle all unencrypted http that they see, if only to make a point... reply account42 7 hours agorootparentWhat ensures the integrity of conveyed information for physical mail? For flyers? For telephone conversations? The cryptography community would have you believe that the only solution to getting scammed is encryption. It isn't. reply matrss 6 hours agorootparent> What ensures the integrity of conveyed information for physical mail? For flyers? For telephone conversations? Nothing, really. But for physical mail the attacks against it don't scale nearly as well: you would need to insert yourself physically into the transportation chain and do physical work to mess with the content. Messing with mail is also taken much more seriously as an offense in many places, while laws are not as strict for network traffic generally. For telephone conversations, at least until somewhat recently, the fact that synthesizing convincing speech in real time was not really feasible (especially not if you tried to imitate someones speech) ensured some integrity of the conversation. That has changed, though. reply eesmith 7 hours agorootparentprevThere is a specific class of websites that will always support non-TLS connections, like http://home.mcom.com/ and http://textfiles.com/ . Like, \"telnet textfiles.com 80\" then \"GET / HTTP/1.0\", , \"Location: textfile.com\"and you have the page. What would be the point of making these unencrypted sites disappear? reply matrss 6 hours agorootparenttextfiles.com says: \"TEXTFILES.COM has been online for nearly 25 years with no ads or clickthroughs.\" I'd argue that that is a most likely objectively false statement and that the domain owner is in no position to authoritatively answer the question if it has ever served ads in that time. As it is served without TLS any party involved in the transportation of the data can mess with its content and e.g. insert ads. There are a number of reports of ISPs having done exactly that in the past, and some might still do it today. Therefore it is very likely that textfiles.com as shown in someones browser has indeed had ads at some point in time, even if the one controlling the domain didn't insert them. Textfiles also contains donation links for PayPal and Venmo. That is an attractive target to replace with something else. And that is precisely the point: without TLS you do not have any authority over what anyone sees when visiting your website. If you don't care about that then fine, my comment about mangling all http traffic was a bit of a hyperbole. But don't be surprised when it happens anyway and donations meant for you go to someone else instead. reply eesmith 5 hours agorootparentThere is a big difference between \"served ads\" and \"ads inserted downstream.\" If you browse through your smart TV, and the smart TV overlays an ad over the browser window, or to the side, is that the same as saying the original server is serving those ads? I hope you agree it is not. If you use a web browser from a phone vendor who has a special Chromium build which inserts ads client-side in the browser, do you say that the server is serving those ads? Do you know that absolutely no browser vendors, including for low-cost phones, do this? If your ISP requires you configure your browser to use their proxy service, and that proxy service can insert ads, do you say that the server is serving those ads? Are you absolutely sure no ISPs have this requirement? If you use a service where you can email it a URL and it emails you the PDF of the web site, with some advertising at the bottom of each page, do you say the original server is really the one serving those ads? If you read my web site though archive.org, and archive.org has its \"please donate to us\" ad, do you really say that my site is serving those ads? Is there any web site which you can guarantee it's impossible for any possible user, no matter the hardware or connection, to see ads which did not come from the original server as long as the server has TLS? I find that impossible to believe. I therefore conclude that your interpretation is meaningless. > \"as shown in someones browser\" Which is different than being served by the server, as I believe I have sufficiently demonstrated. > But don't be surprised when it happens anyway Jason Scott, who runs that site, will not be surprised. reply matrss 5 hours agorootparent> If you browse through your smart TV, and the smart TV overlays an ad over the browser window, or to the side, is that the same as saying the original server is serving those ads? I hope you agree it is not. I agree it is not. That is why I didn't say that the original server served ads, but that the _domain_ served ads. Without TLS you don't have authority over what your domain serves, with TLS you do (well, in the absence of rogue CAs, against which we have a somewhat good system in place). > If you use a web browser from a phone vendor who has a special Chromium build which inserts ads client-side in the browser, do you say that the server is serving those ads? Do you know that absolutely no browser vendors, including for low-cost phones, do this? This is simply a compromised device. > If your ISP requires you configure your browser to use their proxy service, and that proxy service can insert ads, do you say that the server is serving those ads? Are you absolutely sure no ISPs have this requirement? This is an ISP giving you instructions to compromise your device. > If you use a service where you can email it a URL and it emails you the PDF of the web site, with some advertising at the bottom of each page, do you say the original server is really the one serving those ads? No, in this case I am clearly no longer looking at the website, but asking a third-party to convey it to me with whatever changes it makes to it. > If you read my web site though archive.org, and archive.org has its \"please donate to us\" ad, do you really say that my site is serving those ads? No, archive.org is then serving an ad on their own domain, while simultaneously showing an archived version of your website, the correctness of which I have to trust archive.org for. > Is there any web site which you can guarantee it's impossible for any possible user, no matter the hardware or connection, to see ads which did not come from the original server as long as the server has TLS? I find that impossible to believe. Fair point. I should have said that I additionally expect the client device to be uncompromised, otherwise all odds are off anyway as your examples show. The implicit scenario I was talking about includes an end-user using an uncompromised device and putting your domain into their browsers URL bar or making a direct http connection to your domain in some other way. reply eesmith 1 hour agorootparentWhile both those domains have a specific goal of letting people browse the web as it if were the 1990s, including using 1990s-era web browsers. They want the historical integrity, which includes the lack of data integrity that you want. reply johannes1234321 4 hours agorootparentprevInstead of using telnet, switch over to an TLS client. openssl s_client -connect news.ycombinator.com:443 and you can do the same. A simple wrapper, alias or something makes it as nice as telnet. reply eesmith 1 hour agorootparentMy goal was to demonstrate that it supported http, and did not require TLS. reply burnished 10 hours agorootparentprevHuh. Never thought about it that way; replacing hypothetical MITM attacks with genuine middlemen. reply account42 8 hours agorootparentprevThe Kebab Shop also takes orders over the phone, which is not any more encrypted. And prices are more likely to be simply outdated than modified by a malicious entity. Your concerns are not based in reality. reply philistine 5 hours agorootparentThe fact that content on http websites hasn’t been maliciously switched does not mean that https didn’t work. It’s like a vaccine. We vaccinated most of the web against a very bad problem, and that has stopped the problem from happening in the first place. If 90% were still on http, way more ISPs would insert ads. reply megous 10 hours agorootparentprevYou can get integrity at higher levels in the stack (or lower). reply pests 10 hours agorootparentprevYou say that until some foreign national gets their kabab order MITM to deliver them some malicious virus that ends up getting him killed. reply account42 7 hours agorootparentWhich is of course a real concern for the average joe. reply gotodengo 10 hours agorootparentprevTheir site will break consistently in any case. Running a site in 2024 comes with a responsibility to update regularly for a good reason. There are more than enough forgotten kebab shop restaurant pages that are now serving malware because they never updated WordPress that an out of date certificate warning is a very good \"heads up, this site hasn't been maintained in 6 years\" If we're talking hosting even a static HTML file without using a site hosting company, that already requires so much technical knowledge (Domain purchasing, DNS, purchasing a static IP from your ISP, server software which again requires vuln updates) that said person will be able to update a TLS cert without any issue. reply account42 7 hours agorootparent> There are more than enough forgotten kebab shop restaurant pages that are now serving malware [citation needed] There are plenty of organizations that actively scan the web for \"malware\" (aka anything that the almighty machine learning algorithms don't like) and are more than happy to harass the website owner and hosting company until their demands are met. Security is ultimately a social issue. Technical means are only one way to improve it and can never solve it 100%. You must never loose sight of the cost imposed by tecnological security solutions versus what improvement they actually offer. reply serbuvlad 10 hours agorootparentprevI'm really curious as to what you see as the disadvantages of TLS. Sure, the advantages are minor for some services and critical for other services. However, if you already have bought a domain name, the cost of setting up TLS is basically 0. You just run certbot and give it the domains you want to license. It will set up auto-renew and even edit your Apache/NGINX configs to enable TLS. Sure, TLS standards rotate. But that just means you have to update Apache/NGINX every like 5 years. Hardly a barrier for most people imo. reply dijit 10 hours agorootparentIts better than it was, but TLS has a lot more knobs to fail than even a basic http server does; theres a whole host of handoff thats happening and running multiple sites is fraught with minor issues. certbot is a python program, better hope it keeps working- it’s definitely not kept working for me and I’m a seasoned sysadmin. a combination of my python environment becoming outdated (making updates impossible) and a deprecation of a critical API needed for it to work. The #1 cause of issues with a hobby website: darkscience.net is that it refuses to negotiate on Chrome because the TLS suites are considered too old, yet in 2020 I was scoring A+ on Qualys SSL report. Its just time, time and effort and its wasted mostly. The letsencrypt tools are really wonderful, just pray they don’t break, and be ready to reinstall everything from scratch at some point. reply homebrewer 2 hours agorootparentModern http servers (like caddy) do not make it any more difficult than setting up plain http (it's actually the opposite — you have to specify the schema — http:// — in front of the domain name if you do not want https; otherwise you get https + 301 from http). reply ndsipa_pomu 7 hours agorootparentprev> certbot is a python program, better hope it keeps working- it’s definitely not kept working for me and I’m a seasoned sysadmin. a combination of my python environment becoming outdated (making updates impossible) and a deprecation of a critical API needed for it to work. You could try out acme.sh that's written purely in shell. It's extremely capable and supports DNS challenge and multiple providers https://github.com/acmesh-official/acme.sh reply usr1106 9 hours agorootparentprev> certbot is a python program, better hope it keeps working There is also https://github.com/srvrco/getssl which is a bash script. I have lightly audited it years ago and it did not seem to upload your private keys anywhere... I've used it occasionally, but I don't let it run as root, so I need to copy the retrieved certs into the the server config manually. reply dijit 9 hours agorootparentTheres a bunch of alternative clients and I’ve tried many. Larger point is regarding the fact that its required for what amounts to a poster on a wall: yes, someone can come along with a pen an alter the poster- but its not worth the effort to secure for most people and will degrade rapidly with such security too. So, instead they turn to middlemen, or don’t bother. Theres a myriad of other issues, but, its not as easy as we claim. reply JoshTriplett 9 hours agorootparentprev> the cost of setting up TLS is basically 0. You just run certbot certbot is not even close to the pinnacle of easy TLS setup. Using an HTTP server that fully integrates ACME and tls-alpn-01 is much nicer: tell your server what domain you use, and it automatically obtains a certificate. reply taneliv 10 hours agorootparentprevI'm always reminded about this by being on the other side of the equation with my car. There is regulation, like mandatory yearly inspections and anyone is only allowed to sell road worthy vehicles. These rules are rather strict, likewise for the driver's license. They aren't impossible to know or understand, but there's a lot of details. However, when I take it to the shop, whether for that yearly inspection, regular maintenance, or because there's something apparently wrong with it, I never know what to expect in terms of time and money. Oh, it needs a new thingamajig? I start to mildly sweat, fearing it to cost six hundred like the flux capacitor that had to be replaced last week/month/year and took two weeks to get shipped from another country. \"Ninety cents, and we put it in place for no charge, it literally takes ten seconds\", like, I love to hear the news, could have saved me from the anguish by giving a hint when I asked about the price! But need a new key? Starting from three hundred fifty, plus one hundred seventy for a backup copy. Like, where do these prices come from? Actually, don't tell me, I'm a software engineer. I know, I know. I'll just wait until you want your car shop web pages up. Oh, for that you'll need PCI DSS and we can't do that other things because of GDPR. Sorry, my hands are tied here. That'll be four thousand plus tax, mister auto mechanic shop owner. reply dns_snek 10 hours agorootparentI don't think that's a good analogy, you're comparing a mass produced product to an individualized B2B service that's going to generate profits for your customer. reply taneliv 8 hours agorootparentIt's not an analogy. It's asymmetric warfare. reply sureIy 10 hours agorootparentprevIrrelevant. Safe transfer should be the default. Your argument is akin to \"I don't have anything to hide.\" You just do it and don't think about it. Modern servers and services make this completely transparent. The kebab guy doesn't need to worry about this as long as they're not fooled into buying from mala fide hosting companies who tries to upsell you on something that should be the baseline. reply clan 9 hours agorootparentNah ah. Not. While we might be able to find common ground in the statement that \"safe transfer should be the default\", we will differ on the definition of \"safe\". Unfortunately these discussions often end up in techno-babble. Especially here on HN were we tend to enjoy rather binary viewpoints without too many shades of gray. Try being your own devils advocate: \"What if I have something to hide?\". Then deal with that. Legitimately. Reasonably. Unless you are an anarkist I assume that we can agree that we need authoraties. A legal framework. Policing. So I 100% support Let's Encrypt and what they have done to destroy the certificate racket. That is a force of good! But I do not think it was a healthy thing that the browsers (and Google search results) \"forced\" the world defacto to TLS only. Why? Look at the list of Trusted Root Certificates in the big OS and browsers. You are telling me only good guys are listed? None here are or can be influenced by state actors? But that is the good kind of MITM? This then hinges on your definition of \"safe transport\". Only the anarkist can win against the government. I am not. It might sound like I am in the \"I do not have anything to hide\" camp. I am not that naive. But I am firmly in the \"I prefer more scrutiny when I have something to hide\". Because the measures the authorities needs to employ today are too draconian for my liking. I preferred the risk of MITM on an ISP level to what the authoraties need to do now to stay in control. We have not eliminated MITM. Just made it harder. And we forgot to discuss legitimate reasons for MITM because \"bad\". This is not a \"technical\" discussion on the fine details of TLS or not. But should be a discussion about the societal changes this causes. We need locks to keep the creeps out but still wants the police to gain access. The current system does not enable that in a healthy way but rather erodes trust. Us binary people can define clear simple technical solutions. But the rest of the world is quite messy. And us bit twiddlers tend to shy away from that and then ignore the push-back to our actions. We cannot have a sober conversation unless we depart from the \"encrypt everything\" is technically good and then that is set in stone. But here we are: Writing off arguments as irrelevant. reply dspillett 7 hours agorootparentprevOr worse: people who still go on and on about how self-signed certificates should be accepted by browsers, and can't be convinced that blind-trust-no-first-use is lousy security. They usually counter with “but SSH uses TOFU” because they don't see, and can't be convinced of, the problem of not verifying the server key signature⁰. I can be fairly sure that I'm talking to the daemon that I've just setup myself without explicitly checking the signature¹, but that particular side-channel assurance doesn't apply to, for example, a client connecting to our SFTP endpoint for the first time² to send us sensitive data. -- [0] Basically, they get away with doing SSH wrong, and want to get away with doing HTTPS wrong the same way. [1] Though I still should, really, and actually do in DayJob. [2] Surprisingly few banks' tech teams bother to verify SSH server signatures on first connection, I know because the ones in our documentation were wrong for a time and no one queried the matter before I noticed it when reviewing that documentation while adding further details. I doubt they'd even notice the signature changing unexpectedly even though that could mean something very serious is going on. reply guappa 5 hours agorootparentprevMy letsencrypt cert, despite all my attempts, works fine with browsers but WILL NOT work with wget/curl/python/whatever. Plus setting up letsencrypt isn't really really easy. Last time it was failing because I had disabled HTTP on port 80 entirely on my server… but letsencrypt uses that to verify that my website has the magic file. So I had to make a script to turn it on for 5 minutes around the time when the certificate gets renewed. -_ None of this is easy or quick, and people have other stuff to do than to worry about completely hypothetical attacks on their blog. reply mmsc 5 hours agorootparent>letsencrypt uses that to verify that my website has the magic file. So, instead, use the other authentication methods. For example, DNS. reply guappa 4 hours agorootparentIs that easier to configure? (no it isn't) reply mmsc 2 hours agorootparentSetting a single DNS record which doesn't need to be change is more difficult than setting a crontab to open port 80 \"around the time you expect the ACME challenge\"? How's that? reply account42 8 hours agorootparentprevIn general if you need to resort to ad hominens like calling your detractors weirdos then maybe your position isn't as justified as you want to believe. reply wannacboatmovie 10 hours agorootparentprevCan't believe the HTTPS everywhere cargo cult still can't get it through their skulls there is still a place and use cases for plaintext HTTP. In some cases, CRLs for example, they shall not be served over HTTPS. reply account42 9 hours agoprevI'm kinda mixed on LE. It's nice that you can now get free TLS certs without having to resort to shady outfits like StartSSL. This allows any website to easily move to HTTPS, which has basically elimated sensitive data (including logins) from being sent over unencrypted connections. On the otherhand, this reinforces the inherently proken trust model of TLS certificates where any certificate authority (and a lot of them are controlled by outright hostile entities) has the ability to issue certificates for your domain without your involvement. Yes there are tons of kludges to try and mitigate this design flaw (CAA records, certificate transparency) but they don't 100% solve the issue. If not for LE perhaps there would have been more motivation to implement support for a saner trust mechanism by now that limmits certificate issuance to those entities who actually have any authority to decide over domain ownership, like with DNSSEC+DANE. I'm also concerned with the (intentional) lack of backwards compatibility with moving sites to TLS, which is not just a one time TLS on/off issue but a continual deprecation of protocols and ciphers. This is warranted for things that need to be secure like banking or email but shouldn't really be needed to view a recipe or other similar static and non-critical information. Concerns about network operators inserting ads or other shit are better solved with regulation. reply rocqua 9 hours agoparent> If not for LE perhaps there would have been more motivation to implement support for a saner trust mechanism by now I would argue that LE has only highlighted these problems, and now actually causes people with power to worry about them. There is a chance we would have gotten something better than TLS if the lack of LE kept certificates a pain. But that seems unlikely to me. Because the fundamental problem remains hard. reply selectnull 10 hours agoprevWhat I'm most thankful is the ACME protocol. Does anyone remember how we renewed certificates before LE? Yeah, private keys were being sent via email as zip attachments. That was a security charade. And as far as I know, it was a norm among CAs (I remember working with several). Thank you Let's Encrypt. reply ta1243 10 hours agoparentJust handholding a renewal with globalsign I generate the new key on the server as part of the csr creation process. I run it on the server itself so the key never leaves the server's internal storage. CSR gets sent off to globalsign (via a third party because #largeCompany), then a couple of days later I get the certificate back and apply to the server Would love to use ACME instead, and store the key in memory (ramdrive etc), but these are the downsides of working for a company less agile than an oil-tanker reply chrismorgan 10 hours agoparentprevWhat of Certificate Signing Requests? The whole purpose was that you wouldn’t send private keys around. (I was only slightly involved with a couple of TLS certificates before then, and certainly they enforced the CSR approach, but maybe such terrible practice was more common in the real world that I knew.) reply selectnull 10 hours agorootparentMy memory of the whole process is kinda fuzzy, you're probably right about CSRs. Hopefully the private keys were not sent around via unencrypted email. But the point still stands: the whole process was a nightmare, no automation, error prone, renewal easily forgetable... The large companies could have had a staff to manage all that. I was just a solo developer managing my own projects, and it was a hassle. reply jillesvangurp 9 hours agoparentprevI still have to go through that bs with some of my setups. Load balancers in cloud environments don't tend to integrate easily with external ACME providers like letsencrypt and the internal ones require moving your domain to them which doesn't always work. And not all cloud providers even have this. Most of them seem to treat ACME as an afterthought. You can sort of do some hacks with scripting this together via things like terraform, cron jobs, or whatever. But it gets ugly and the failure modes are that your site stops working if for whatever reason the certificates fail to renew (I've had this happen), which courtesy of really short life times for certificates is of course often. So, I paid the wildcard certificate tax a few days ago so I don't have to break my brain over this. A couple of hundred. Makes me feel dirty but it really isn't worth days of my time to dodge this for the cost of effectivelyYeah, private keys were being sent via email as zip attachments. Internally, perhaps. And also on a small scale maybe with CA \"resellers\" who were often shady outfits which were in it for a quick buck and didn't much care about the rules. But as a formal issuance mechanism I very much doubt it. The public CAs are prohibited from knowing the private key for a certificate they issue. Indeed there's a fun incident some years back where a reseller (who have been squirrelling away such private keys) just sends them all to the issuing CA, apparently thinking this is some sort of trump card - and so the issuing CA just... revokes all those certificates immediately because they're prohibited from knowing these private keys. The correct thing to do, and indeed the thing ACME is doing, although not the interesting part of the protocol, is to produce a Certificate Signing Request. This data structure goes roughly as follows: Dear Certificate Authority, I am Some Internet Name [and maybe more than one], and here is some other facts you may be entitled to certify about me. You will observe that this document is signed, proving I know a Private Key P. Please issue me a certificate, with my name and other details, showing that you associate those details with this key P which you don't know. Signed, P. This actually means (with ACME or without) that you can successfully air gap the certificate issuance process, with the machine that knows the private key actually never talking to a Certificate Authority at all and the private key never leaving that machine. That's not how most people do it because they aren't paranoid, but it's been eminently possible for decades. reply JoshTriplett 9 hours agorootparent> Indeed there's a fun incident some years back where a reseller (who have been squirrelling away such private keys) just sends them all to the issuing CA, apparently thinking this is some sort of trump card - and so the issuing CA just... revokes all those certificates immediately because they're prohibited from knowing these private keys. That sounds like a fun story. I'd love to read the post-mortem if it's public. reply FateOfNations 9 hours agorootparentLooks like it was this: https://www.theregister.com/2018/03/01/trustico_digicert_sym... reply tialaramex 7 hours agorootparentYup. Trustico. As usual my preference is to avoid caring whether people are malevolent or simply incompetent, by judging on the results of their actions not guessing their unknowable mental state, so hey, maybe Trustico incompetently believed it was a good idea to know private keys (it is not) and incompetently acted in a way they thought was in their customers' best interests (it was not) and so they're in the doghouse for that reason. [Edited: I originally said Trustico was out of business, but astoundingly the company is still trading. I have no Earthly idea why you would pay incompetent people to do something that's actually zero cost at point of use, but er... OK] reply account42 6 hours agorootparentprevAccording to that article Trustico wanted the certs revoked and intentionally send the keys to DigiCert in order to get them to act. While they still shouldn't have had those keys in the first place it sounds like the \"trump card\" worked here. reply tialaramex 4 hours agorootparentAt the time my guess was that Trustico thought if the certificates have to be revoked they get their money back, and I can't imagine DigiCert's contracts are bad enough that a customer can get their money back if the customer screws up, but I have not read the contract. The claims from Trustico are very silly. They want their customers to believe everything is fine, and yet the only possible way for this event to even occur is that Trustico are at best incompetent. To me this seems like one of those Gerald Ratner things where you make it clear that your product is garbage and so, usually the result is that your customers won't buy it because if they believe you it's garbage and if they don't believe you they won't want your product anyway - but whereas Ratner more or less destroyed a successful business, Trustico is still going. reply computergert 8 hours agoprevCoincidentally I just got an email from a potential client, Dutch governmental institution, that they don’t want me to use Letsencrypt. They prefer paying for a certificate themselves. Not sure why, apparently they don’t trust it. reply gloosx 10 hours agoprevI really wish something like this comes up for the desktop certification world as well. Microsoft just went full insane mode with their current requirements, and their certificate plugs are making more money than ever without lifting a finger. So funny that all of their security, vetting and endless verifications are standing on a single passport photo sent over an email to this day. reply lambdaone 9 hours agoprevLet's Encrypt is a massive achievement, and is now essential infrastructure. Basing it on an open protocol, so it doesn't become a single point of failure, was a clever idea that allows the idea to survive the demise of any single organization. May there be many more such anniversaries. reply brchr 9 hours agoprevPeter Eckersley (1978-2022) was posthumously inducted into the Internet Hall of Fame for his founding work on Let’s Encrypt. The Internet is a better place because of Peter (and his many collaborators and colleagues). reply usr1106 9 hours agoparenthttps://www.internethalloffame.org/inductees/ None of them I have ever heard of. Whatever that may mean. Edit: On the whole list https://www.internethalloffame.org/inductees/all/ I spotted maybe seven names. Still a single digit percentage. reply brchr 8 hours agorootparentVint Cerf & Bob Kahn (TCP/IP), Paul Baran (packet switching), Tim Berners-Lee (WWW), Marc Andreesen (Netscape), Brewster Kahle (Internet Archive), Douglas Engelbart (hypertext), Aaron Swartz (RSS, Creative Commons), Richard Stallman (GNU, free software movement), Van Jacobson (TCP/IP congestion control), Jimmy Wales (Wikipedia), Mitchell Baker (Mozilla), Linus Torvalds (Linux)... ...but you’re missing the point of my comment, which is simply to acknowledge and honor (my late dear friend) Peter. reply usr1106 8 hours agorootparentAh, I missed Linus Torvalds and you might have missed Bob Metcalfe (Ethernet) and Jon Postel (RFC work). My point was not do criticize the achievements of the work of any of those people. 1. I was not actively aware that this hall exists 2. I am mostly critical to such awards in general. I have noted that several companies receiving the \"Export company of the year\" here in this country (doesn't matter which one) have went bust a couple of years later. I received the \"hacker of the year\" award at my workplace some years ago. It was supposed to hang with all previous awards in the cafeteria. I did not like that and \"forgot\" it at home. I quit the company a year later anyway. Edit: Forgot that I worked for the \"software product of the year\" twice in my life. One needed heavy, painful architectural rework 3 years later. The other was Series 60. People old enough know how that went, killed a global market leader. reply bigtex 2 hours agoprevLet's Encrypt helped reduce our OUTRAGEOUS Entrust bill(legacy vendor, I didn't pick them, they had insane security protocols for a small company who just needed SSL certs). We had a 4 yr/$14k contract for about 11 certs. Now our SSL is near 0, except for a cert for SSRS that is hard to automate with LE. reply INTPenis 9 hours agoprevConfig management took me many years to adopt, containers took me about 6 years to warm up to. But LE was something I jumped on immediately. I had worked in web hosting for 10 years already when it came out so I remember faxing your driver's license in order to validate a TLS cert. It just felt like such a scam for so long that these CAs were over charging for something that is just a key signing. But I guess automation and standards had to catch up in order for LE to securely setup their CA. reply pplonski86 7 hours agoprevLet's encrypt saved me :) I love to use it with certbot in docker-compose :) deploying really can be simple reply CarpaDorada 11 hours agoprevA lot of people are not aware that HTTPS certificates do not necessarily guard you from certain types of attacks like DNS injection. You can seefor one example where an attack campaign called DNSPionage obtained valid certificates for their attacks. To explain the issue with HTTPS certificates simply, issuance is automated and rests on the security of DNS, which is achieved via DNSSEC and most do not implement. reply ta1243 10 hours agoparentTechnically it's an attack against the certificate issuing authority, bypassing their authorisation checks (is this person really authorised to issue a certificate for the domain). Trouble is even CAA entries won't help here (if you're spoofing A records, you can spoof CAA records too). DNSSEC might help against this, I don't know enough about DNS though. Another type of attack is an IP hijack, which allows you to pass things like http authentication (the normal ACME method), but won't bypass CAA records. Can't use letsencrypt to issue a cert - even if you own the IP address my A or AAAA records point to - if my CAA doesn't have letsenctypt as an approved issuer. reply CarpaDorada 9 hours agorootparentWith DNSSEC you can be certain that the response you got was issued by the nameserver that is claimed (well, by someone who owns the private key). The domain owner, and registrar can both be at fault, the CA is the last entity to blame because they are performing an automated check of domain ownership. For maximum security you'd want to buy your own TLD as my YT video talks about, to circumvent any other registries, registry wholesalers, and registrars' security models, but an adequate protection for most is to use registry/registrar lock and implement DNSSEC correctly. IP hijack will then not work when all of the above is done correctly. Another option is manual certificate issuance with a CA whose security model is better than yours, but not implementing DNSSEC leaves you open to other attacks. reply KronisLV 10 hours agoprevHere’s to 10 more years! With web servers like Caddy, software like certbot and even something like Apache2 getting mod_md, I’d say we’re in a pretty good spot! That said, I’m wondering why there aren’t 10 or so popular alternatives to LE, since that seems to be the landscape for domain registrars, for example. reply _0xdd 5 hours agoprevSuch an awesome service (and protocol!) reply aurareturn 10 hours agoprevPeople talk about paying for certificates but one major pain point solved by PaaS companies over the last 5 years is automatically adding certificates and renewing them for your app deployments. It saves a huge amount of headache. In 2024, if your PaaS does not have automated encryption for deploys, I will never use it. reply kome 11 hours agoprevthank you Edward Snowden reply vaylian 8 hours agoparentI wanted to post that exact comment. reply stephenr 7 hours agoprevI really wish they would finally branch out and offer S/MIME certificates. Good email clients support them out of the box, it's just a PITA to get them if you don't want to order 100 at a time or something equally ridiculous for SME/individuals. reply account42 6 hours agoparentWould frequent rotation be reasonable for S/MIME certs though? reply Tepix 4 hours agorootparentOnce per year or less. Remember to decrypt messages, you need to keep your old certificates/keys around. You can request a new certificate with the same key but i'm not sure that's a good safety practice. reply stephenr 5 hours agorootparentprevThere's nothing specifically that says S/MIME certs would need to have the same 90-day expiration date, but even if they did, I'm making a basic assumption that if there were a standardised, free API to issue S/MIME certs, major email clients would build-in a client to request a certificate - heck it might even prompt major email providers to offer their own solutions for certs, to compete with alternatives that supported using LE certs. reply lakomen 9 hours agoprevTime flies when you're having fun. Congratulations reply Havoc 8 hours agoprevReminder that they donation dependent reply wannacboatmovie 11 hours agoprev [–] Nothing makes me trust a site with my payment info more than seeing a LE or domain-validated certificate with no ownership details in the DN. reply sunaookami 9 hours agoparentHTTPS does not validate the trustworthiness of a site. Never has and never will. It only validates that the site has not been tampered with during transfer. Phishing sites can also have HTTPS, that doesn't make them trustworthy. reply jonathantf2 7 hours agoparentprevGoogle.com (and my bank) use a DN certificate, if it's good enough for them it's good enough for anyone. reply aaomidi 10 hours agoparentprev [–] The rate of misissuance of EV and OV is much higher than DV. reply wannacboatmovie 10 hours agorootparent [–] Source? I'm not questioning it, I'd like to know more. DV always seemed vulnerable to DNS tampering. reply ta1243 10 hours agorootparent [–] And EV is vulnerable to a fancy looking fax (remember them?) Do you really check your site has an EV every single time? Especially now browsers treat them the same? If not, how do you know someone hasn't got a DV certificate for this specific visit? Scott Helme has a thorough takedown of them, and that was 7 years ago when they were still a thing. https://scotthelme.co.uk/are-ev-certificates-worth-the-paper... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Let’s Encrypt is a free certificate authority that simplifies obtaining and managing server certificates, addressing the challenges of cost and complexity.",
      "Supported by major organizations like Mozilla, Cisco, and the Electronic Frontier Foundation (EFF), it aims to make Internet security accessible and open to all.",
      "Key principles of Let’s Encrypt include being free, automatic, secure, transparent, open, and cooperative, with opportunities for sponsorship or contribution to support the initiative."
    ],
    "commentSummary": [
      "Let's Encrypt, celebrating its 10th anniversary, has transformed internet security by offering free HTTPS certificates, democratizing secure connections.",
      "Despite its widespread adoption, some sectors, such as banking, require longer certificate validity, posing challenges for Let's Encrypt's use.",
      "The service has improved web security through its open protocol and automation, though it remains reliant on donations, underscoring its non-profit mission."
    ],
    "points": 448,
    "commentCount": 166,
    "retryCount": 0,
    "time": 1732083220
  },
  {
    "id": 42190541,
    "title": "Epic Allows Internet Archive to Distribute Unreal and Unreal Tournament Forever",
    "originLink": "https://www.techdirt.com/2024/11/18/epic-allows-internet-archive-to-distribute-for-free-unreal-unreal-tournament-forever/",
    "originBody": "Anonymous Coward says: November 18, 2024 at 8:00 pm If HR 9495 passes the house and senate, they could just kill the archive by declaring it a terrorist-organization, with no requirement for explanation, provided the internet archive is a non-profit. ..But then again the internet archive may not exactly be a high priority for them in that regard. Collapse replies (19) Reply View in chronology Make this comment the first word Make this comment the last word",
    "commentLink": "https://news.ycombinator.com/item?id=42190541",
    "commentBody": "Epic Allows Internet Archive to Distribute Unreal and Unreal Tournament Forever (techdirt.com)445 points by chocmake 15 hours agohidepastfavorite104 comments modeless 13 hours agoI wish they would GPL it like Quake. Even if it was missing some proprietary dependencies I bet the community could replace them. I recently helped port ioquake3 to the web, complete with UDP multiplayer, and set up an online demo using the internet archive's copy of the Quake III demo: https://thelongestyard.link It would be cool to be able to do the same with Unreal Tournament. reply Ralfp 7 hours agoparentThe state of things here is that Unreal Engine 1 may be open sourced one day but would need cleaning up first. And they just never got around doing it: https://forums.unrealengine.com/t/unreal-engine-1/14084/6 reply tempaccount420 3 hours agorootparentWhy clean up first? Could be devs pushing back because they're ashamed (unnecessarily) of their code? reply pxc 8 hours agoparentprevWhen I was a kid, I had so much fun playing open-source games based on Quake engines. Tremulous stands out as especially delightful at the little LAN parties we threw at my house on birthdays and New Years' Eve. reply kaoD 7 hours agoparentprevI can't find a link to your port's code. I think you have to include it as per GPL's license (and also because it's awesome and I want to see the code!) reply modeless 5 hours agorootparentIt is here: https://github.com/jdarpinian/ioq3 reply looshch 7 hours agoparentprev> to the web you mean like to browser? If yes, how did you achieve UDP connections? To my best knowledge, browsers allow TCP only reply desdenova 7 hours agorootparentWebSockets are TCP-only. WebRTC can run over UDP, but it's not raw UDP. I think that's what they meant. reply modeless 6 hours agorootparentYes, it's WebRTC. It uses real unreliable and unordered UDP packets, peer-to-peer. But WebRTC requires a connection establishment step to happen first, so it can't send traffic to arbitrary UDP services, only cooperating WebRTC peers. Which is fine for multiplayer games. reply kaoD 7 hours agorootparentprevI don't have time to look into the code right now, but it might also be WebTransport: https://developer.mozilla.org/en-US/docs/Web/API/WebTranspor... EDIT: actually I can't find the code, shouldn't it be linked as per GPL's license? reply junon 6 hours agorootparentprevWebSockets also initiate a normal Http request first, further complicating things anyway. reply dmead 13 hours agoparentprevIoquake was helpful for the open Jedi project get an open source version of jk3 working. https://github.com/OpenJediProject/OJP I'm sure there is some small group of UT people that would gladly keep it alive in a similar fashion. reply deafpolygon 12 hours agoparentprevIm willing to bet the code for UT is being used for Fortnite. reply Melonotromo 6 hours agorootparentFortnite uses Unreal Engine 5, the most advanced Game Engine in existens which sourcecode is freely available for a few years now. Open sourcing old stuff is a lot of effort. You need to find all people involved or know the legal status of the copyright. You need to go through all the code in case you had some properitary stuff in there which you might have paid for. And you need to do all of this next to what you normally do without a direct benefit. reply numpad0 11 hours agorootparentprevIt doesn't make a lot of intuitive sense, but code can be released under multiple different licenses. A 1:1 relationship between code and its terms is not required. reply pests 12 hours agorootparentprevMy first introduction to Unreal was via the \"Unreal Tournament: Game of the Year Edition\" right around the year 2000. I fondly remember some of those maps (Facing Worlds for one) but to think that lineage ends up at Fornite is crazy to me. reply iakov 11 hours agorootparentOh man, I loved that game as a kid. I did not have internet back then, but I didn’t care - I spent hours playing against bots. I still remember the voice taunts and callouts (“I slaughtered that guy!”) and creative maps like ctf_face. Then there were the built in gameplay mods like insta-gib and low gravity, fun guns like Redeemer - kept me glued to the screen for hours. I did not understand why people are playing CS at the computer clubs - UT was a much better and bigger game! Thanks for a trip down the memory lane. Good, simple times. reply zelos 10 hours agorootparent\"I did not understand why people are playing CS at the computer clubs...\" I was the same. UT was so much fun - bouncing flak cannon shrapnel round corners! But my LAN group all wanted to play CS or that UT mod that was similar (Tactical Ops?). reply pests 10 hours agorootparentprevI don't think I knew too much about computers or gaming at that age to figure all that out. I just played the base mode. My introduction to programming was thru a different game (Subspace VIE, a MMO on dialup back in the 90s later community-remade as Continuum) when squad had a login page at a domain. I really wondered how example.com/?login worked and that led me to where I am today. reply ZaoLahma 9 hours agorootparentprevThe base game was fantastic. Never could say no to a game on dm-morpheus. But what really made it great for me were the total conversion mods. I spent probably thousands of hours playing Infiltration back in the day. I can't imagine how much time and energy must have been poured into completely re-doing a game like that. reply rounce 9 hours agorootparentThat’s a name that brings back memories. Infiltration was so far ahead of its time, and it’s interesting to look at how many of its features are now standard on even run-and-gun FPS these days. reply ZaoLahma 9 hours agorootparentIndeed. It's a shame that Sentry Studios never transitioned into a \"proper\" game development studio. They were at least 10 years ahead of the curve. But it's good that the games industry caught up so the infiltration-itch can be scratched in other ways. reply bloqs 6 hours agorootparentprevFacing worlds liquid drum and bass soundtrack (and the rest of the games) was the basis for my music interest! reply mikepurvis 12 hours agorootparentprevThat shouldn’t have any impact on its ability to be relicensed and open sourced though— any secret sauce in Fortnite is elsewhere, and arguably none of it is code, but rather in asset-level stuff like game/map/character design and monetization/progression strategy. reply jsiepkes 11 hours agorootparentprevIt's a 25 year old game engine. There is not going to be anything in it which they consider a trade secret today. reply jsheard 9 hours agorootparentEven the current Unreal Engine doesn't have much in the way of trade secrets, its entire source code is available on GitHub for anyone to look at (aside from the console-specific parts bound by NDAs). reply magicalhippo 14 hours agoprevUnreal Tournament multiplayer with mutators[1] was awesome fun, like the one that made the player avatar grow bigger when the player scored a kill, and vice versa. It's a shame later multiplayer games didn't pick up on the mutator concept. Being able to easily tweak the gameplay mixed it up and added extra challenge or fun. [1]: https://unreal.fandom.com/wiki/Mutator reply hyperman1 8 hours agoparentIn a LAN party, we hacked a mutator together from code scraps of other mutators. It replaced all weapons with the green one, modified to replace the bullet stream of little plasma balls with a stream of redeemer rockets. Turns out, if a redeemer rocket flys over, sound frequency goes slightly lower. If a few 100s of these fly over, sound frequency goes rock bottom, making the announcer say stuff like: Monsterkiiiiiiiuuuuuuuuuuuuuuuooorrgglllll. And then the whole level explodes because hundreds of redeemers tend to set each other off in a chain reaction Fun times. reply duskwuff 13 hours agoparentprev> It's a shame later multiplayer games didn't pick up on the mutator concept. The terminology didn't catch on, but the idea is out there. Compare \"game modes\" in Overwatch, for example: https://overwatch.blizzard.com/en-us/news/22938941/introduci... reply gombosg 8 hours agorootparentLet's just call mutators as 'mixins' :) reply mylons 12 hours agorootparentprevoverwatch is essentially the anti-UT. there’s no simplicity, just complexity and toxicity. reply pests 12 hours agorootparentJust hide quickchat and don't join voice, its a fine game. The workshop is really cool though, tons of cool modes. Only downside is you can't do any map modifications or anything like that. reply duskwuff 10 hours agorootparentprevNot saying it's a good game (or otherwise), just saying the idea's out there. reply moepstar 13 hours agoparentprevOne game i recall using that mechanic (edit: changing size of body parts) is \"Morphies Law\" on Nintendo Switch: https://www.nintendo.com/us/store/products/morphies-law-swit... reply klysm 3 hours agoparentprevRocket league has mutators reply raffraffraff 11 hours agoparentprevWas there a bighead mutator? I seem to recall players with huge heads. reply nanna 9 hours agorootparenthttps://unreal.fandom.com/wiki/BigHead reply Gabrys1 11 hours agorootparentprevYes there was reply RajT88 1 hour agoprevI remember my freshman year of college playing Unreal. I had worked all summer to be able to buy myself a computer for college (and made sure it had a decent video card). I recall, some weeks into my Freshman year, one Saturday night getting a call from a friend of mine who lived down the hall, \"Hey I'm at this party, and my friend (Jenny or some other common lady name) wants to talk to you\". So he puts this girl on the phone. Her: \"What are you doing?\" Me: \"Playing Unreal.\" Her: \"So, you're going to be doing that all night?\" Me: \"Yes.\" Her: \"OK... I guess I'll talk to you later\" All these years later, I still think I made the right decision. reply notamy 14 hours agoprevFingers crossed UT2004 will end up in the Archive eventually. I still boot it up to play occasionally because nothing else has scratched the arena shooter itch that well. reply snarfy 13 hours agoparentUT2004 is on archive.org already, fully patched with some modern additions. I'm very hopeful this will get Epic's blessing too. What is strange is that they approve of the archive.org download, instead of say, OldUnreal hosting it themselves. The archive.org uploader could update it with malware. It would be nice if they allow OldUnreal to host. reply bayindirh 13 hours agorootparentUT2004 shipped with its Linux binaries in the 3rd disc. I remember playing it a ton until migrating to 64bit system. Wish they GPL that one, too, so we can build a 64bit version of that. reply all2 14 hours agoparentprevWith the vehicles? That game was nuts. reply pandemic_region 12 hours agorootparentPancake ! reply elbear 11 hours agoparentprevAnother UT2004 fan signing in. I haven't played it that much recently, but I still have it installed. reply hackernewds 13 hours agoparentprevwhy not just buy it? reply goosedragons 13 hours agorootparentYou can't anymore besides used physical copies. Epic delisted all the Unreal games. reply jamesfinlayson 11 hours agorootparentYes, I recently found that out. I bought Unreal on GOG a few years back and was wanting to get Unreal Tournament 2004 recently (I played a bit of the demo back in 2005 or 2006) but currently no digital options. reply flakes 13 hours agorootparentprevWhy do you assume they didn't? > I still boot it up to play occasionally reply deknos 11 hours agorootparentprevi would buy instantly if it works on linux on 64 bit machines. all of them. reply simonw 14 hours agoprevI spent so much of the early year 2000 playing Unreal Tournament instagib mode on Facing Worlds: https://en.wikipedia.org/wiki/Facing_Worlds reply pests 12 hours agoparentFacing Worlds is still one of those maps that I wish every game had. The concept is so simple. Two towers with a bridge between them. Have fun. It would be fun in any game. GTA, Overwatch, Halo, anything. Funny seeing it have its own wiki page. I didn't know my childhood would be archived like this. :D I used to get so many MONSTER KILLS on that map it was nuts. reply Arrath 11 hours agorootparentIts a crime that Halo 1 had Boarding Action and it never resurfaced (outside of Forge recreations) in later titles such as Reach with movement options like jetpacks. reply pests 11 hours agorootparentI'll be honest I was never a huge Halo buff at least until Multiplayer. Maybe you can help me. My earlies memories of Halo2 multiplayer involved a map with two bases on opposite sides in a canyon. I bring it up only to talk about it, I know I could look up the name... It was the first online pvp game I had played. Having 1v1s with people with voice chat was so addicting. I remember it used to use an ELO calculation for rank so I would calculate my buddies matches when I was at his house telling him how much he would gain or lose in the current match. Good times. reply ashdnazg 9 hours agorootparentI think you mean Coagulation[1]. I remember playing this map with a bunch of friends and two split screen TVs. Hearing somebody swear at you from the neighbouring room is a wonderful experience. [1] https://halo.fandom.com/wiki/Coagulation reply skykooler 7 hours agorootparentprevI used to play it all the time on CTF mode in Sauerbraten. I remember it being very difficult to score since you could be sniped from basically anywhere on the map. reply navbaker 6 hours agoparentprevInstagib was a fantastic mode to work on aim! Mine improved dramatically across FPS games after playing for a while with instagib turned on. reply kombookcha 10 hours agoparentprevMan. Facing Worlds, low gravity, instagib CTF - might be my favourite map/mode combination in any shooter ever. That was so much fun. reply disillusioned 12 hours agoparentprevI never really got into UT, but from the screenshot, it's giving me The Longest Yard Q3 Demo vibes... instagib railgun... those were the days... reply bayindirh 8 hours agorootparentI can play The Longest Yard nonstop for days. Yes, I’m more of a Quake fan more than Unreal. reply bigstrat2003 11 hours agorootparentprevI would agree that the two maps have a very similar feel. And both are great maps to practice sniping versus bots. reply pajeetz 14 hours agoparentprevthose were truly the golden years of online gaming do you remember the monster hunter mod ? huge random monsters would spawn and it would take multiple people to take them down reply jonny_eh 14 hours agoprevNo link to the actual downloads? Update, found it: https://www.oldunreal.com/downloads/unreal/full-game-install... reply Crosseye_Jack 6 hours agoparenthttps://archive.org/details/ut-goty This is where the installer you linked to downloads the iso's from. The iso's have been on IA for quite a while already. reply calmbonsai 14 hours agoprevUT2004 in Onslaught mode on well-administrated private servers is still the best team-based arena FPS I've ever played. Here's hoping it can make a revival someday akin to City of Heroes https://www.polygon.com/gaming/471719/city-of-heroes-homecom... . reply Physkal 15 hours agoprevMan this game changed my life. I always preferred it to quake 3 mainly because of the brighter color palette and the assault mode. Also facing worlds was the most amazing maps I had ever seen. reply amatecha 12 hours agoparentheck yeah. UT was one of those absolutely-essential LAN games, to be sure. I remember taking my computer and heavy CRT monitor to my buddy's place to play -- good memories :D reply malux85 13 hours agoparentprevMe too, there was a real “wow” moment for me when I went into the computer store at like age 14 and saw it running, it really started to show what computers were capable of to me reply nanna 9 hours agoprevI made a UT level (DM-Charge) and it got onto the PC Gamer CD as part of a competition. An UT designer reviewed it but was pretty critical which as a teenager made me feel like I didn't have what it took to go further. Shame. These days I look back and think I did pretty damn well to teach myself without any help at all what it took to build a level, one that was good enough to get onto PC Gamer's CD and that I would occasionally see people playing online, even if it wasn't perfect, and wish i had had the confidence to take the criticism on the chin. Something to bear in mind - a lot of talented kids and people out there just need a bit of encouragement and may take criticism harder than you intend it to be. reply anotherhue 13 hours agoprevA recent engine recreation : https://github.com/dpjudas/SurrealEngine Quick start instructions also pull UT from the archive https://github.com/NixOS/nixpkgs/pull/337069 reply tosmatos 10 hours agoprevI will never forgive Epic for not trying to finish their alpha of the latest Unreal Tournament. I had so much fun on that alpha. Unfortunately the dev team got put to fortnite when that started printing money. Logical, but still sad. reply paulryanrogers 15 hours agoprevI wish they'd go further and offer maintained versions on their store, ideally free or very low cost as GOG.com and Steam have done. That said, it's appreciated. reply chocmake 14 hours agoparentFrom what I gather Epic delisted various of their Unreal Tournament games across all stores a couple years ago due to them not supporting their modern online services (including chat). This was within days of being fined $500m for, among other things, allowing on-by-default text chat in one of their other games that is available for children/teens to play, so some believe they'd rather delist than update some of their older games. reply 8f2ab37a-ed6c 12 hours agoprevWould love it if Epic made the source to UE actually public so that LLMs could be trained on it and provide better help when learning the engine. The source code is semi-public already, you can see it if you register an account, seems like a small step to have it be much more accessible as a learning resource through LLMs. reply gadtfly 14 hours agoprevhttps://www.youtube.com/watch?v=i_0G6WPuss4 reply bittwiddle 14 hours agoparentDid you know the soundtrack was composed in a module tracker? Someone recently recorded the full soundtrack playing back in Milkytracker. Pretty neat since you can see how the composers wrote the songs. https://www.youtube.com/watch?v=iONsjiiqeKg reply bobim 12 hours agorootparentAnd you could use a utility called umr (unreal media reaper) to extract the songs. It was mind blowing to then understand how the game switched patterns to suit the pace, a very clever use of this technology. reply drproteus 14 hours agoparentprevI listen to Foregone Destruction so much that when I play the actual map that uses it I always think I left my music player running in the background. reply imiric 11 hours agoparentprevAh, another jungle music example.[1] [1]: https://pikuma.com/blog/jungle-music-video-game-drum-bass reply gadtfly 9 hours agorootparentI don't really understand where the dividing line is between Jungle and Drum and Bass, but I'm pretty sure Foregone Destruction is on the DnB side of it. reply saithir 11 hours agoprevOr, \"How To Get Some Public Appreciation With Minimal Effort: An Attempt\". If they actually cared they'd host (and more importantly, supported since they probably don't run on modern systems without some fiddling) those games themselves. Not like they don't have a store with games or anything. reply kergonath 11 hours agoparentI have not much love for Epic and they can always do it better. Still, it’s a step in the right direction and I wish other game developers would do that. Also, another argument for proper funding of the Internet Archive. reply kristopolous 11 hours agorootparentYou mean like government/international body level funding? That's not unreasonable. reply stavros 11 hours agoparentprevShitting on people when they do something good for not going further is a surefire way of stopping them from doing anything else. reply sebstefan 11 hours agoparentprev? Internet Archive is sure to outlast Epic. Putting all your eggs in the same basket is worse than allowing the archive to do what they were created for reply Mindwipe 9 hours agorootparentGiven the continued mismanagement of the IA I would put decent money on that not happening actually. reply rounce 9 hours agorootparentCan you elaborate or link to more on the mismanagement aspect? I’m not up to speed on IA drama. reply pests 12 hours agoprevI just want to make a top level comment noting how many people (myself included) have mentioned the Facing Worlds map. I think everyone here agrees it's a fond memory. reply danpalmer 8 hours agoprevEpic also retracted my purchase of UT2004 on the Epic games store so I can no longer download it. reply deknos 11 hours agoprevIs there a working 64 bit linux version for Unreal/ut 2004? :( We should make a petition that they opensource the code at least for these two ones. i still have the CDRoms. i even would buy it again, if that would make this more likely. That being said, xonotic is a bit like it (and opensource) and there are maps like Facing Worlds available, but sadly no good npc / npc-way-mapping for it. reply pavelstoev 14 hours agoprevLove both games spending countless hours, Unreal single player story was great (for its time). Are there any servers still online for UT? reply astlouis44 14 hours agoprevSlightly related thread, Unreal Engine 5 runs in WebGPU now: https://news.ycombinator.com/item?id=42190897 reply bathtub365 13 hours agoparentDoes it? It’s not an officially supported platform and this link (which was posted by you) has no information. reply astlouis44 13 hours agorootparentYes, it does. We're a third party that built our own WebGPU backend. Feel free to try this demo here: https://play.spacelancers.com/ reply DerSaidin 13 hours agoprevImagine if they added something like this to UE5 licensing: If your game has not been updated in N years... 1) Internet Archive can distribute it for free 2) Let people distribute modified versions that does not need license key or whatever copy protection. Harder but extra cool: To get a UE royalty discount, put source code in escrow set to release it if game not updated in N years. reply johnmaguire 12 hours agoparent> If your game has not been updated in N years... This would impact indie developers and small publishers more than large ones. reply olliej 9 hours agoprevThat's nice of them (esp given their track record of shittiness). It's always seemed absurd to me how companies refuse to just make old games that they no longer sell available for free, or just leave them available for cheap purchase. I just don't understand \"we refuse to make this available, even for money\" just because something is old. I can kind of understand the behavior in the case of non-game software, e.g if a company makes a tool to do X, and someone wants to do X, you want them to buy the new profitable version not the old one for cheap/free. But I just don't think that applies to games - even a \"remake\" that is literally just a graphics update (no gameplay, UI, or anything changes, just increased asset resolution) people prefer the updated graphics so will generally buy that when it becomes available, but in the absence of such an update the old game is not competing for new ones. reply PittleyDunkin 14 hours agoprevWhy not just release to the public domain? reply renewiltord 13 hours agoparentAnd why not pay for a few devs to keep it up to date? reply rounce 8 hours agorootparentThe nostalgia is so strong I think the community is would happily take stewardship of it. reply blackeyeblitzar 14 hours agoprevThe Golden era of PC gaming .., reply Andrew_nenakhov 11 hours agoprevForever? reply yakshaving_jgt 12 hours agoprev [–] Does it work on MacOS? Or do we still need to do some Wine thing? reply RulerOf 9 hours agoparent [–] It does. Check OldUnreal on github. There's even a Metal renderer. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "There is a suggestion that if HR 9495, a proposed bill, passes, it could potentially classify the Internet Archive as a terrorist organization without clear justification.",
      "The concern is raised by an anonymous source, indicating that this classification might not be a primary focus or priority of the bill.",
      "The implication of such a classification could have significant consequences for the Internet Archive, a digital library offering free access to a vast collection of digital content."
    ],
    "commentSummary": [
      "Epic Games has permitted the Internet Archive to distribute Unreal and Unreal Tournament indefinitely, sparking hopes for potential open-source releases similar to Quake.",
      "The Unreal Tournament community remains vibrant, with discussions about preserving and updating classic games, especially after Epic's decision to delist older titles from stores.",
      "Open-sourcing Unreal Engine 1 could eventually occur, but it requires significant cleanup, and this move could benefit learning, development, and the preservation of gaming history."
    ],
    "points": 445,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1732072550
  },
  {
    "id": 42190065,
    "title": "Tiny Glade 'built' its way to >600k sold in a month",
    "originLink": "https://newsletter.gamediscover.co/p/how-tiny-glade-built-its-way-to-600k",
    "originBody": "Share this post How Tiny Glade 'built' its way to >600k sold in a month! newsletter.gamediscover.co Copy link Facebook Email Note Other Discover more from The GameDiscoverCo newsletter Analysis, data and insight about how people find & buy video games in the 2020s. Over 31,000 subscribers Subscribe Continue reading Sign in How Tiny Glade 'built' its way to >600k sold in a month! Also: a whole heap of game discovery & platform news, & some bonus radical politics. Simon Carless Oct 29, 2024 26 Share this post How Tiny Glade 'built' its way to >600k sold in a month! newsletter.gamediscover.co Copy link Facebook Email Note Other 4 2 Share [The GameDiscoverCo game discovery newsletter is written by ‘how people find your game’ expert & company founder Simon Carless, and is a regular look at how people discover and buy video games in the 2020s.] We’re back for a new week, and thanks for the feedback on our ‘news up front, party main feature in the back’ newsletter structure, which seems to have gone down well. (We spend a lot of time picking the right news - not all the news - for that section.) Before we start, we’re going to ask you an important question - should you be sinking your 401k into vTuber stocks? Dungeon Investing is trying to answer that by a deep financial dive into Hololive parent company Cover Corp, whom you might know from hit free Steam fangame HoloCure and that L.A. Dodgers baseball collab. Huh! [HEADS UP: you can support GameDiscoverCo by subscribing to GDCo Plus right now. You get base access to a super-detailed Steam back -end for unreleased & released games, full access to a second weekly newsletter, Discord access, eight game discovery eBooks & lots more.] Game discovery news: watch out for Wuchang… We’re starting out with the latest game platform & discovery news, as is now our rule. And here’s what we’d like to point out: GameDiscoverCo’s latest ‘trending unreleased Steam games’ chart - sorted by new followers in the last 7 days - shows that Chinese interest in Wuchang: Fallen Feathers, post-Black Myth, is keeping it at #1. (And Monster Hunter Wilds - #2 - leapfrogged the newly announced Subnautica 2 at #3.) Elsewhere on that chart - viewable daily & sortable by our Plus members - leftover Steam Next Fest interest (Delta Force, #4, The Precinct, #10) and pre-launch surges (Dragon Age: The Veilguard at #5) also feature strongly. And Towers of Aghasba (#12) is an interesting-looking survival crafter debuting in November. Late-breaking news from PlayStation: Sony has decided to close first-party Concord dev Firewalk Studios, as well as German-based mobile outfit Neon Koi (formerly Savage Game Studios), noting “we will work to find placement for some of those impacted within our global community of studios where possible.” Veteran journoconsultant Brian Crecente notes that “the gaming console isn't dying; it's just being redefined”, as handheld PCs like Steam Deck get mainstreamed over time. A similar change snuck up for PC laptops - per IDC: “of all PCs shipped in 2023, desktops made up about 30 percent [&] tower computers… were roughly half of that.” The crew at Stream Hatchet looked at the top Steam Next Fest games in terms of live Twitch streams, noting: “Multiplayer shooter games saw by far the most attention, with hero shooter/MOBA Supervive and tactical shooter Delta Force being the only titles to exceed 1M hours watched.” Fast Food Simulator also saw 506K hours watched.. Now here’s wild UGC crossover for a hit game: \"Adobe Express is.. available in Team Builder for [EA’s] College Football 25”, allowing players to customize their team logos & jerseys with royalty-free designs. There’s Adobe upsell, too: “With the free Adobe Express plan, you can create up to 25 custom images for your teams per month.\" Here’s a complex article on ‘Steam, aggregators & the game industry’, which laments: “Why publish games from experienced teams with a pedigree when they can’t consistently predict the market through the lens of their expertise, and end up having the same or lower hit rate as any random rookie developer whose meme hobby demo went viral?” (Too much sneering at ‘low quality’ games here for us, tho - the market decides.) Talking of EA Sports College Football 25, it’s the best-selling American football game ever by U.S. dollar sales, per Circana, and also “the overall #2 best-selling sports game in US history in terms of US dollar sales, bettered only by NBA 2K21.” (GDCo estimates the PS5 version as 93.5% U.S. players, btw - wow, that’s high.) Roblox things the company’s upcoming creator road map includes in-experience commerce via Shopify, music discovery & in-world video, among other features; Roblox is providing new parental controls for younger kids on the platform, likely planned pre-Hindenburg Research, but good to roll out now. Just Steam platform things: did you know you can see how much money you’ve spent directly on your Steam account via this link for ‘External Spend’? And did you know you can look up the ‘market value’ of any public Steam account via this SteamDB link? (Be scared.) Microlinks: Fortnite is allowing you to ‘cross-advance’ your Battle Passes for Fortnite, LEGO Fortnite and Fortnite Festival by using the same XP for all ; Apple “appears to be scaling back production of Vision Pro, and may even halt manufacturing entirely by the end of the year, The Information reports”; short-episode live action ad/content is spreading from the Quibi-like Reelshort to mobile games. Tiny Glade: a month, >600k copies sold on Steam! We’re guessing you might have seen Pounce Light’s glorious “relaxing building game” Tiny Glade ($15), which Ana Opara and Tomasz Stachowiak launched on Steam on Sept. 23rd, after a two-year dev period and a Top 10 appearance in June’s Next Fest. We expected the game to do well - but at 10,000+ CCU on launch, and >1,000 CCU even now, it’s doing amazing for a micro-indie. Why? It appealed to the cozy demo, like The Sims streamers (above), wider ‘city builder’ influencers - and has UGC galore, since players are building Helm’s Deep from The Lord Of The Rings in the game. So we had to contact the devs for a Q&A. They were kind enough to be transparent with their numbers - as of a few days ago - including this Steam back-end overview: A few things stand out there as impressive or different to the norm: the 616,000 copies sold in less than a month, pretty much guaranteeing Tiny Glade is selling a million or two over time. (Blimey.) the big DAU (daily active) number compared to CCU (concurrents) - ~30x, versus 8-10x for stickier titles. (But the game is still 97% Positive in user reviews.) the median time played of just 1 hour 4 minutes - relatively low, though we know some outliers build for hundreds of hours. Just flagging: we don’t really see low play time as a negative here. Tiny Glade is, at its heart, a gorgeous software toy. It doesn’t have in-game goals - it’s a sandbox. The people who bought it love it, and want to support it, and don’t have any regrets. Neat! The Tiny Glade team also passed along the country-based stats for Steam buyers, which are intriguing: United States (32%), Germany (9%), France (7%), UK (7%), China (7%), Canada (4%), Russian Federation (4%), Australia (3%), Netherlands (2%) and Japan (2%). So - less Asia-centric than a number of other recent PC hits… Switching to GameDiscoverCo data: here’s our Steam Deep Dive ‘Affinity’ data, showing medium-sized (or above) games which have a high player overlap with Tiny Glade, and are >10x more likely than a ‘normal’ Steam player to own that game: (‘Affinity multiplier’ is how much more likely you are to own this game than the average Steam player, if you also own Tiny Glade.) This gives a really good flavor of the kinds of players who pick up Tiny Glade. They’re: Interested in freeform town/city builders, like Townscaper and Dorfromantik. Delighted by cozy games with more abstract goals, like Unpacking. Fans of medieval city-builders, like Banished and Manor Lords. But… why did people buy Tiny Glade? The answer is - in our view - that every single video (or demo) that the game has ever put out, from early viral Tweets to the Future Games Show 2023 trailer and beyond, screams ‘this’ll be so fun to build things in, play me!’ With the devs being so good at putting out new WIP work and trailers, the game was rarely not viral. It launched with a mindblowing 1,375,441 Steam wishlists - the team notes that “the big spike at ~20k [daily additions] around May 2024 is Steam Next Fest”: Due to the sheer amount of players, streamers and influencers recommending the game at launch - hence that Overwhelmingly Positive review score - and a Steam ‘takeover’ feature - Tiny Glade also had a visibly good post-launch ‘long tail’: The game hasn’t had any discounts yet, either - it should shoot back up then. Listen, we know that ‘incredibly well-made game sells’ is self-evident, and perhaps not news. But the kind of game this is, goals-wise - and the fact the devs could charge $15 for it, despite being so freeform - is super interesting. So don’t dismiss it out of hand. To finish up, here’s a brief Q&A we had with Ana & Tom. We don’t generally reprint these in full. But the answers they had were so fascinating, we felt we had to. Ta da: Q: There's a trend recently for games that really don't have strong failure states or put any pressure on the player. Sometimes 'game designers' don't want to design 'games' like that. Can you explain why you decided to make Tiny Glade like that? I think it depends on what kind of experience you're trying to achieve… We wanted to craft a serene space that you can escape to, the childhood feeling that you have all the time in the world. Sometimes you want a high intensity game - but sometimes you just want to kick back and see where your imagination takes you. Q: How much did you iterate with alpha/beta testers pre-release to polish the game, or did you end up doing a lot of the UI/UX iteration yourself? Oh, we iterated a lot. Some tools went through 6 or 7 fully fleshed out prototypes before we settled on what you can see in the game today. We first do a simple version that we can test on ourselves. Sometimes that stage alone can take multiple attempts. If it does pass our internal evaluation, we polish it up a bit, and then we run a playtest. If we're lucky, then that version works and then it's about smoothing the rough edges, doing micro iterations, so to say. But often things don't work like you'd expect, and you need to go back to the drawing board and try again. Sometimes you can only tell if something works when the rest of the pieces are at a certain level of completion. It's a very, very iterative process, where you work on all the pieces together, fleshing them all out little by little. Before we shipped, we had 5 external playtests in the two year development period. Q: Do you have two or three rules of 'game feel' that you think you did great in Tiny Glade? It's clear that 'game feel' is a big part of its success! Yes! We actually outlined design pillars in the very beginning of the development. They were \"a lot from little effort\", \"no wrong answers\", \"it's alive\" (the latter referring to the world reacting to what you've built, such as ivy, birds, sheep, etc). For the 'game feel', I think \"a lot from little effort\" is probably the biggest one. Whenever you draw a wall, change roof shape, drag out fences, a lot of stuff is being generated right here and now, just on your whim. Each brick, pebble and plank is carefully placed by the game. With anything that's generated, we aim for it to feel hand-crafted and perfectly imperfect, as if someone manually constructed all these things just for you. You can hear it from the sound design too. We wanted it to be very tactile, and have an association with real materials, as if you're building a diorama in real life. Q: Tech-wise, I was blown away that [often high-end focused game tech eggheads] Digital Foundry gave you a rave video review! Congrats on that - the tech is standout. Do you have any tech inspirations, and do you think procedural elements are still under-used in games? Thank you :D From a rendering perspective, the biggest inspirations were Limbo & Inside. There, you don't need to tweak a million settings in options to get a beautiful experience from the start. You launch the game, and you’re immediately in it. We strived for the full experience to be the same across all machines, so that you could experience beautiful lighting even on low-end PCs. When it comes to lighting technologies, after many iterations, Tiny Glade actually ended up being similar to Metro Exodus :D I think we’re used to seeing procedural techniques used for generating huge worlds, or ‘infinite’ content. So one could say that procedural is used to a narrow extent. But that might be just a matter of semantics, because one could also draw parallels between procedural generation and systemic gameplay. Many games amplify your input via multiple layered systems; they might just not be labeled as \"procedural generation\". You could even say that the wand design system in Noita is procedural generation. We happen to use it to make the act of creation satisfying and responsive instead. Finally: the digital Steam fiefdom revolution, wen? Too cynical? This classic Matt Bors cartoon came to mind - go buy his stuff. It’s true that a dominant platform-led status quo can be smothering. But for practical reasons, grumbling about it often gets suppressed. So it’s fascinating to see a spinoff of Disco Elysium studio ZA/UM - a game very much forged in radical politics - go straight for the jugular about the workers vs. the ruling (platform) parties. Banger quote #1, from Summer Eternal’s Aleksandar Gavrilović? “I am still eagerly awaiting a second crisis [beyond the current layoffs], one which would spotlight the largest structural issue in game development… one third of all PC revenue from all developers (from indies to AAA) is syphoned to digital fiefdoms, of which Valve is the most egregious example. I can imagine a near future with more worker power, but I lack the imagination to envision the replacement of Valve with a community owned alternative. That 'winter castle' will not fall as easily, but we should at least start openly discussing alternatives.\" Banger quote #2, from the company’s Dora Klindžić? \"It’s true, Summer Eternal will not fix the games industry, although as a byproduct of our operation we might generate a panacea for agriculture, astronomy, inaccurate bus timetables, those hoax messages that target your mom, local elections, and syphilis. I think this industry is finished. But fortunately for everyone, video games are not.” Now that’s a soundbite…. [We’re GameDiscoverCo, an agency based around one simple issue: how do players find, buy and enjoy your PC or console game? We run the newsletter you’re reading, and provide consulting services for publishers, funds, and other smart game industry folks.] Subscribe to The GameDiscoverCo newsletter By Simon Carless · Thousands of paid subscribers Analysis, data and insight about how people find & buy video games in the 2020s. Subscribe Error 26 Share this post How Tiny Glade 'built' its way to >600k sold in a month! newsletter.gamediscover.co Copy link Facebook Email Note Other 4 2 Share",
    "commentLink": "https://news.ycombinator.com/item?id=42190065",
    "commentBody": "Tiny Glade 'built' its way to >600k sold in a month (gamediscover.co)391 points by TaurenHunter 17 hours agohidepastfavorite95 comments ykl 13 hours agoOne of the many fun/cool things about Tiny Glade is that it quietly has one of the most advanced realtime global illumination lighting engines shipping in any game today, and it’s all completely custom for just this game. One of the devs, Tomasz Stachowiak, is a big deal in the realtime rendering world and previously was at DICE and Embark; at Embark he led the Kajiya experimental realtime GI project [1] that made quite a splash in the rendering world a while back. The other dev, Anastasia Opara, is a big name in the procedural graphics world. She gave one of my favorite presentations on the topic a few years ago [2]. Anyhow if you can’t tell I’m a big fan of both the game and the devs. :) [1] https://github.com/EmbarkStudios/kajiya [2] https://youtu.be/dpYwLny0P8M reply jb_briant 11 hours agoparentI’m creating my own builder-focused game with procedural parts, and I’m truly humbled by the expertise and execution behind Tiny Glade. As you mentioned, the GI is stunning, and I’d add that the entire game feels incredibly polished. Achieving that level of technical and artistic refinement is such a huge factor in its success. Watching it in action, everything seems so intuitive and effortless—a quality I’m still striving for in my own work. My creative mode leans towards a mix of Valheim and procedural meshes, but I’m still figuring out the best way to make it feel both intuitive and powerfull. reply stavros 10 hours agorootparent> Achieving that level of technical and artistic refinement is such a huge factor in its success. Yep, the article talks about how they built their way to success, but the actual method they used is \"have a team of two superheroes make the game\". reply prox 10 hours agorootparentprevDo you have a Steam page? I love Valheim and looking for more games that scratch that explorer/builder itch. reply jb_briant 9 hours agorootparentYes I do! Ardaria on Steam or https://steam.ardaria.com The survival mode is in development. What I have now: - Procedural Voxel planet with 1 biome, oceans and mountains - A highly scalable and powerfull build system which empowers builders - Light parkour: climbing, vaulting reply nkrisc 7 hours agorootparentWatching the video without sound is delightfully funny. Left click! Right click! Click and hold! It has all the clicks! But seriously it looks very cool. reply jb_briant 7 hours agorootparent:)))) And you forgot the middle click! It will copy both scale and rotation of the element you are aiming it! reply prox 7 hours agorootparentprevGreat! Wishlisted. Looks good too! reply jb_briant 7 hours agorootparentThanks! I forgot to mention that you can play right now to the creative mode, which will stay free in this form reply m0llusk 6 hours agorootparentprevHave you tried Enshrouded? Many Valheim players have found it to be similar but improved in various ways. reply jb_briant 6 hours agorootparentYeah unshrouded is from the same niche: survival creative Incredibly well executed game, solid team of 40+ people behind and they are working on voxel games for a decade with Portal Knights. TBH I freaked out when Unshrouded trailer got released because it's a direct competitor and how cool their game is ! reply IshKebab 11 hours agoparentprevYeah it looks amazing. I just wish it was more of an actual game. I guess lots of people like digital lego, but for me it seems like a ridiculously well polished toy or tech demo. I don't know why I'd play with it for more than an hour... reply jb_briant 9 hours agorootparentI feel I made that mistake to dev the creative mode instead of the survival mode for my game. reply lopis 10 hours agorootparentprevThey did introduce daily challenges/themes, which give you a sort of soft goal each day. But in the end, it's a zen art game more than anything. reply robertlagrant 9 hours agorootparentprevAccording to the article most people play it for around an hour. reply deadbabe 8 hours agorootparentprevAgreed, these kind of games feel lazy imo. Like a dev just wanted to toil endlessly on making a game engine but then never add actual gameplay elements. I played around with Tiny Glade for a little bit and then never really touched it again. Neat, but no depth. reply delta_p_delta_x 6 hours agorootparent> feel lazy Game studios which release games with a good 'survival' or 'story mode' have dedicated 'art and story' departments (story- and script-writers, world-builders, art and music directors) who collaborate with the engineering department (engine and gameplay programmers) to produce a cohesive game. The smallest 'indie' games that I've played with a semblance of a story—anything by Amanita Design[1], for instance, or The Long Dark by Hinterland Games[2]—still have teams of about 10-20 people working on different aspects. Tiny Glade was developed by two people—both extremely talented engineers. But story writers they may not necessarily be. Even though they're 'just engineers' they still managed to produce a beautiful art style backed by serious engineering effort, including real-time non-raytraced GI. There's nothing 'lazy' here. Game developers are frequently just that—developers. They may be creative, but to write an engaging narrative is an entirely different ball-game. Give them a chance to reap the profits of Tiny Glade, grow their two-person studio, and hopefully hire more staff and put out expansions, sequels, and other titles that have what you wish for. [1]: https://amanita-design.net/about.html [2]: https://hinterlandgames.com/ reply jb_briant 7 hours agorootparentprevThe \"lazy\" adjective to speak about game dev is provocative because it sounds ignorant. You can't say \"lazy\" regarding a piece of software which requires that amount of effort, it's just a trolling non sense. reply famahar 7 hours agorootparentprevThe depth comes from the players own creativity. Not everything needs goals and gameplay loops. Nothing about this is lazy. It's sophisticated piece of software that does exactly what it sought out to do. reply jb_briant 7 hours agorootparentAnd we can argue that using your creativity to tranform thoughts into pixels is a goal in itself reply Eliah_Lakhin 16 hours agoprevThis is likely the first gamedev project written entirely in Rust and Vulkan to achieve significant financial success on Steam. I'm genuinely proud of the authors — they've set an inspiring example and given us hope for a bright future where the Rust ecosystem serves as a foundation for unique and creative game development projects. reply smitec 16 hours agoparentThe Gnorp Apologue (mentioned in another thread here) was also notably written in Rust. https://store.steampowered.com/news/app/1473350?emclan=10358... reply j_bum 15 hours agorootparentLovely game, would recommend. I play it in the background when chatting with friends on weekly game nights! reply ramon156 9 hours agorootparentprevBeat me to it. There's some more title, but honestly they're not all that memorable :'). Gnorp is worth the money though! reply jsheard 10 hours agoparentprevIt'll be interesting to see if its success leads to it being ported to consoles. To my knowledge Rust has yet to ship in a console game, obviously there's a number of roadblocks to making that happen but the biggest one is that Sony apparently has a strict approval process for new languages/compilers to be used on their platforms. reply Animats 16 hours agoparentprevYes. It's a good success story. And a cute little game. reply modernerd 10 hours agoprevTiny Glade is a beautiful piece of technical art. It's worth playing even if the genre is not normally your thing. Procedural art is fascinating and worthy of a deep dive too since it combines art, modelling, rendering, programming and maths (constraint solvers, discrete optimization, more). Anastasia Opara (“procedural art nerd”) has a Gumroad with procedural art tutorials that show how to create environments procedurally with Houdini: https://anopara.gumroad.com/ (the first one is free) Tomasz Stachowiak (“technical debt generator”) gave a talk at GPC 2024 earlier this month called “rendering tiny glades with entirely too much ray marching” (no recording I can find yet but worth keeping an eye on): https://www.graphicsprogrammingconference.nl/#tiny-glade Outside of the Tiny Glade team, Oskar Stålberg (Townscaper) is worth following for his frequent insights into proc-gen: https://x.com/OskSta reply MrMcCall 5 hours agoparentI find \"technical debt generator\" to be hilarious, if a little too close to home ;-) reply GenericCanadian 16 hours agoprevI wrote https://taintedcoders.com/ for anyone looking for an introduction to Rust game development with Bevy. Bevy is still early, but the sweet spot right now is simulations. It's particularly weak in its UI, but that's the coming focus for getting the editor built. If anyone needs ideas, making [boids](https://slsdo.github.io/steering-behaviors/) in Bevy is a great weekend project. reply kookamamie 12 hours agoparentWhile Bevy might be the hottest thing in the Rust gamedev scene, Tiny Glade didn't use it for rendering purposes - AFAIK only the ECS was used from Bevy. reply prox 10 hours agoparentprevCan I also use Rust to build UI / window / data heavy apps? I am webdeveloper mostly so looking into other avenues right now. reply trissi1996 9 hours agorootparentSure, there's \"native\" UI with stuff like slint(QT-like) or iced(system76 is actually building the COSMIC linux desktop environment with this). And you can also get an electron-like stack going, that is actually much less bloated than \"normal\" electron, by using tauri-webview, which uses the OS-provided webview and combining it with one of the many cool rust WASM-based reactive web-ui frameworks, like leptos or dioxus. This gets you compiled sizes of ~10s of MB compared to electrons 100s of MBs. There's also bindings to a lot of traditional ui libs, like GTK, QT & Tk. I'm currently going for the 2nd option (with leptos for the web part) as I'm used to the web-stack and am very productive with this approach, but native UI also seems very tempting to dig into further. Some related links: - https://www.arewewebyet.org/topics/frameworks/ - https://areweguiyet.com/#ecosystem reply Tmpod 9 hours agorootparentI can personally vouch for Slint. The DSL is very pleasant to work with, it's still evolving (and the devs are very active and responsive), tooling is also getting better and better each release and it supports translations and accessibility already (through AccessKit). I've been (veeeery) slowly working on a Matrix client with it and I've quite enjoyed it so far. reply rubymamis 7 hours agorootparentI'm also developing a chat client (tho for LLMs) using QML. Here's a little video showcasing it: https://custom-downloads.s3.us-west-2.amazonaws.com/chat_cli... Still, very much in its infancy. The backend is currently C++ but I'm considering using other languages since there are so many bindings (Mojo is on my radar). reply Tmpod 4 hours agorootparentLooking really good so far, keep it up! Is the source available somewhere I could take a peek? o.o reply rubymamis 1 hour agorootparentThanks! Unfortunately not, it's been too hard for me to monetize FOSS projects, so it's going to be closed source. reply prox 7 hours agorootparentprevHey that is really cool thank you. I have Sublime as my IDE, and it looks like I can use that as well, and using a webview might leverage my understanding of webtech. reply 01HNNWZ0MV43FF 3 hours agorootparentprevTauri 2 is quite good. It even builds for Android now reply Kiro 11 hours agoparentprev> but the sweet spot right now is simulations What kind of games are you referring to? reply pdpi 10 hours agorootparentNot so much games proper, but more like scientific computing visualisations, CAD, stuff like that. This interview talks about it in a bit of length: https://youtu.be/PND2Wpy6U-E reply npinsker 17 hours agoprevThis is a great result -- and the game looks wonderful! -- but it's not that unexpected, since it launched with over 1 million wishlists and was, just before release, the top wishlisted game on Steam. The interesting part, the \"building\", was actually already done before the game launched. More interesting to me are games like Palworld and (the) Gnorp Apologue (both of which are covered here: https://newsletter.gamediscover.co/p/how-this-solo-dev-incre...) These sold one or two orders of magnitude over their wishlist total! Steam's recommendation algorithm must be so powerful nowadays. reply Iulioh 12 hours agoparentTiny glade was in big short form content (tiktok, shorts and reels). I feel like this was a great part of it, the dev created a community before the release reply mutagen 14 hours agoprevMarketing for this was on point or I fell into the exact set of channels they were using because it seemed like I was coming across an update every few months that kept it at a base level of consciousness yet not overwhelming. I guess I fall into perfect demographic, aging gamer developer with interest in Rust, casual games, and generative content. Congrats to the team for such a great job and great success! reply the_duke 16 hours agoprevDigital Foundry did a review of the game. It is quite unusual for that channel to feature such a small indie: https://www.youtube.com/watch?v=cvswAg5Lrtw reply swiftcoder 11 hours agoparentThere's a decent chance they did so pretty much on the strength of Tomasz Stachowiak' name. The guy knows his rendering, and Tiny Glade has an extremely impressive global illumination stack. reply paulcapewell 4 hours agoparentprevThat's referenced and linked to in the article. reply rco8786 16 hours agoprevI was randomly following this build. Following indie game builds is not something I normally do, but I remember always enjoying seeing their demos of what they'd been working on. Eventually I unsubscribed because it felt like the updates started feeling the same, but I'm happy they're doing so well! reply pico303 14 hours agoprevIt’s a truly amazing, beautiful, stunning piece of tech. The ability to seamlessly blend the pieces of your build together in such an elegant way can’t be appreciated enough. What these devs have accomplished is truly stunning. I always think I’m a pretty decent engineer. Then I see someone create something like this and I just feel like I should hang up my spurs and pick different career. reply k1w1 15 hours agoprevThe seamless integration between one type of object and another is really impressive. The way that the blocks in the roofline perfectly work regardless of the height of the roof is a great example. How is this possible? Is it some kind of procedural geometry that fills in the available space? reply PythagoRascal 11 hours agoparentAs far as I know they are using a customised variant of the \"wave-function collapse\" technique, used and popularised by Oskar Stalberg in his games \"Bad North\" and \"Townscaper\". The technique boils down to hand-crafting tons of tiles with adjacency rules about which tiles can slot together. When the user adds/removes a tile the algorithm iteratively tries to find fitting tiles and, if needed, changes neighbouring tiles for ones with the best transitions. He gave a talk where he goes into detail about this[1]. You can also find more if you google his name and \"wave-function collapse\". [1]: https://youtu.be/0bcZb-SsnrA reply PcChip 15 hours agoparentprevIt is surely procedural, maybe wave function collapse? I’d love to read a writeup from the author, in the same style of Factorio’s reply p1necone 14 hours agorootparentI can't remember where exactly but I think I did read somewhere it was based on wave function collapse. I think wave function collapse is still super underexplored in game dev. This recent paper is pretty interesting: https://dl.acm.org/doi/10.1145/3582437.3587209 reply chefandy 14 hours agorootparentprevThe texture is certainly procedural for brick patterns, etc. but I'll bet smoothing intersecting polys happens at render time. reply jaimex2 14 hours agorootparentJust a clever shader reply m463 16 hours agoprevsigh. https://www.gog.com/wishlist/games/tiny_glade reply deely3 12 hours agoparentNot sure why downvotes, but I will be happy to buy it again from GOG. For me Tiny Glade is a spiritual sequel to Townscaper : https://store.steampowered.com/app/1291340/Townscaper/ https://www.gog.com/en/game/townscaper reply Jzush 14 hours agoprevI've been having a lot of fun with this \"game\", it strikes the same zen cord in my mind as Minecraft use too before Minecraft got too into the weeds with game mechanics. I know I can still throw it into creative mode but Tiny Glade just nails it. That said I do have a few gripes that I hope get addressed. I need to be able to copy/paste structures, I'd love to be able to group structures and I wish the maps were bigger. That said, if you want to turn your brain off I do highly recommend it, the developers knocked it out of the park. reply deely3 4 hours agoparentI recommed checking Townscaper for similar vibes. reply LarsDu88 16 hours agoprevReally impressive. I wonder if the fact that it's one of the only graphically cohesive games built with Rust/Bevy has anything to do with its relative success. I'm jealous. My indie VR game Rogue Stargun has sold abysmally on Steam. reply sbarre 15 hours agoparentWhat kind of marketing or outreach have you done for your game? Edit: I ask because the devs on Tiny Glade are ex-DICE (I think?) and definitely ex-Embark Studios, two very well-known AAA studios, and they've been very active both in the Rust community (Discord, Meetups, etc), the Houdini community (conferences and meetups) and elsewhere to promote this game during development.. So they already had a personal brand and an audience built-in, which I'm sure helped.. Plus it's a great game. If you want to give the marketing side a real try for your game, this is a decent resource if you're not already familiar with it: https://howtomarketagame.com/ (No affiliation but I know a few indie devs who were not well-known and had no pre-existing audience, and had success with the approach). reply gauge_field 12 hours agorootparentFrom my observation, their youtube views have been really high. Whenever I see a video review on youtube, it was on average ~100k views with lots of engagement. reply LarsDu88 11 hours agorootparentprevQuite honestly, virtually no marketing and no more than $200 in paid ads. I actually have a full time job so spending time on making the game decent and marketing and raising a kid is currently impossible. As long as my game is out in the world creating a bit of entertainment, I'm content reply sbarre 5 hours agorootparentThat's great! I mean you shipped a game, that's more that the majority of people who do gamedev as a side hustle or hobby, so congrats! Sounds like you've got the right attitude.. :-) reply ferfumarma 10 hours agorootparentprev> As long as my game is out in the world creating a bit of entertainment, I'm content Wow; great attitude! If that's the case, then why not make it free for a couple weeks and try to build up some review volume? reply xingped 14 hours agorootparentprevYeah, I was going to suggest something similar. It seems many successful indie titles these days are born from people working social media to create some sort of following before or during development so you already have an audience aware of the game and possibly excited for it on launch. Just standard marketing probably isn't the way to go these days. reply xingped 16 hours agoparentprevSorry to hear that! I just checked out the game and I don't see anything immediately off-putting on the store page that could explain it. It may just be a matter of visibility? Can you elaborate on how you attempted to market or get the word out about your game? reply alumic 15 hours agorootparentI have to imagine much of this comes down to the fact that VR games, which have additional hardware requirements, necessarily cater to a much smaller segment of the market. Selling an indie game is already hard enough! reply LarsDu88 11 hours agorootparentprevQuite honestly I have no time to market the game nor add significant features since becoming a father. My day job as a machine learning engineer pays more than I would expect to ever make from this game reply ferfumarma 10 hours agoparentprevHave you given copies away to streamers? Have you considered giving Fanatical 200 licenses to put in a bundle? VR is a niche segment, so it's harder than a general purpose game. reply waltbosz 14 hours agoprevThat build tool UI looked really slick. I don't have much experience with 3D modeling tools or building games. Is this UI uniquely innovative? reply modernerd 10 hours agoprevAny recommendations for learning interactive procedural art? Or advice for a path to be able to build things like Tiny Glade? (Other than \"start 20 years ago\".) reply jmiskovic 3 hours agoparentStart following the relevant communities like old.reddit.com/r/proceduralgeneration/ and #procgen tag on social networks, read up on techniques others use. Commit to jams and community events such as PROCJAM and GENUARY. Learn about perlin/simplex noise, Poisson disk sampling, packing algorithms, WFC, boids, inverse kinematics, SDFs. Choose a style (2D, 3D, animation...) and from that start learning the relevant tool (Blender nodes, Houdini, Processing or something else). This is a decent start and it branches too much from here on, depending on your tastes. reply polytely 2 hours agoparentprevThe coding train YouTube channel is also a great place to start, its based on processing.js but honestly most procgen techniques can be applied in any language reply andystanton 6 hours agoparentprevThe Book of Shaders [1] perhaps? It's about fragment shaders rather than geometry but it felt to me like a good introduction to generating things programmatically. [1]: https://thebookofshaders.com/ reply myrmidon 9 hours agoprevLooks really cool, might need to get this! What actually suprised me though was the almost 10% return rate on steam sales that the article did not even comment on. Is this typical? Are there a lot of people that pretty much demo and return games? Don't think its necessarily a bad thing, just the numbers are unexpectedly high... reply Cthulhu_ 9 hours agoparentI'd say 10% is pretty good; for a long time, one of the biggest criticisms of a lot of games and publishers nowadays was the lack of a demo or a return policy for digital purchases, meaning people were more hesitant to buy games on a whim or some people got scammed with an unfinished or broken game. reply mst 5 hours agoparentprevI think, basically, it being a game where you build stuff for the sake of building stuff, a bunch of people will have bought it out of curiosity, spent half an hour or an hour playing it, got disillusioned by the lack of a 'point' and returned it. Of course, the lack of a 'point' as such is absolutely intentional and I'd presume the people who do keep paying it like it *because* of that rather than *in spite* of it. Thinking about it that way (which I did have to work through in my head first, this is Not My Area Of Expertise at all), the numbers don't seem particularly surprising, and I'd guess the article didn't comment on it because the people involved in the article knew what they were talking about sufficiently that they didn't find it surprising either. ... that or steam return rates are just higher than I thought in general; it would be difficult to overestimate my level of ignorance here. reply kleiba 9 hours agoprevIf people find it harder and harder to tell AI voices from real humans, it must be in parts because of people like that youtuber. If you gave me a recording of her voice and had me guess, I would have said \"AI\" for sure. reply domlebo70 14 hours agoprevDoes anyone know how it works? Is it some sort of wave function collapse? reply iKlsR 14 hours agoparentThe developer has a lot of threads and wip on their twitter, I remember when I saw it first, it was a simple demo showing the wall drag and build functionality. reply jaimex2 14 hours agoparentprevyup, WFC is exactly how it works reply bch 12 hours agorootparent> yup, WFC is exactly how it works Not according to the 1 of the 2 that are the studio[0]. [0] https://x.com/h3r2tic/status/1695073463294120219 reply goldenshale 13 hours agoprevI was just looking at this thinking it would be fun to play with my wife, and then realized its windows only. When does the mac version come out?! reply tbillington 12 hours agoparentThe devs mentioned they're not currently looking into it. The game uses Vulkan which isn't supported by MacOS, so they'd have to write a whole second renderer just for Mac. It is also available on linux. reply wffurr 12 hours agorootparentMoltenVK should make it possible to port to Mac. Curious that it uses Vulkan and not wgpu. reply ferbivore 10 hours agorootparentThe renderer is too complex to run on MoltenVK correctly. See https://steamcommunity.com/app/2198150/discussions/0/4425436... reply satvikpendem 12 hours agoparentprevYou can use Whisky [0] which is a free tool that uses WINE to run Windows applications on macOS, including games. There are also paid tools like CrossOver and Parallels but Whisky works well enough for most use cases. [0] https://github.com/Whisky-App/Whisky reply kQq9oHeAz6wLLS 16 hours agoprevLove Tiny Glade. You can very casually create amazing places. I fire it up on boring corporate calls just to keep my hands busy. reply PittleyDunkin 15 hours agoprevCongratulations!! The post could use some screenshots of the game. reply pgib 15 hours agoprevI've been considering getting a Steam Deck just to play this game. reply mbStavola 12 hours agoparentMy wife has played a few hours of Tiny Glade on her Steam Deck and it runs well with functional controller support. Perfect fit for a cozy Sunday play session! reply UltraSane 14 hours agoprevI remember reading a blog post where the author didn't think Rust was suitable for game development because it made making large changes to the code base too hard. reply d0mine 12 hours agoparentif something is possible, it does not mean it is the best option. reply lofaszvanitt 12 hours agoprevSubstack is full of these sites where in good cases 2 sentence of useful into is buried into 15 pages of text. But most of the time there isn't any useful info - like here - and almost 99% of that is useless filler bullshit laden crap. reply m0llusk 12 hours agoprev [–] got vines growing on my stuff reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tiny Glade, a relaxing building game by Pounce Light, sold over 600,000 copies within a month of its release on Steam, indicating significant commercial success.",
      "The game gained popularity through viral videos and participation in Steam's Next Fest, resulting in over 1.3 million wishlists, showcasing effective marketing strategies.",
      "Its success is attributed to its serene, goal-free sandbox experience and engaging procedural generation technology, appealing to fans of cozy and city-building games."
    ],
    "commentSummary": [
      "Tiny Glade sold over 600,000 copies in a month, attributed to its advanced custom lighting engine and the expertise of developers Tomasz Stachowiak and Anastasia Opara.",
      "The game, developed using Rust and Vulkan, became popular through effective marketing and community engagement, showcasing Rust's potential in game development.",
      "Despite some criticism for lacking depth, Tiny Glade offers a zen-like building experience, emphasizing the importance of community building before a game's launch."
    ],
    "points": 391,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1732067113
  },
  {
    "id": 42191709,
    "title": "AAA – Analytical Anti-Aliasing",
    "originLink": "https://blog.frost.kiwi/analytical-anti-aliasing/",
    "originBody": "Today’s journey is Anti-Aliasing and the destination is Analytical Anti-Aliasing. Getting rid of rasterization jaggies is an art-form with decades upon decades of maths, creative techniques and non-stop innovation. With so many years of research and development, there are many flavors. From the simple but resource intensive SSAA, over theory dense SMAA, to using machine learning with DLAA. Same goal - vastly different approaches. We’ll take a look at how they work, before introducing a new way to look a the problem - the ✨analytical🌟 way. The perfect Anti-Aliasing exists and is simpler than you think. Having implemented it multiple times over the years, I'll also share some juicy secrets I have never read anywhere before. The Setup To understand the Anti-Aliasing algorithms, we will implement them along the way! Following WebGL canvases draw a moving circle. Anti-Aliasing cannot be fully understood with just images, movement is essential. The red box has 4x zoom. Rendering is done at native resolution of your device, important to judge sharpness. Please pixel-peep to judge sharpness and aliasing closely. Resolution of your screen too high to see aliasing? Lower the resolution with the following buttons, which will integer-scale the rendering. Native Resolution ½ Resolution ¼ Resolution ⅛ Resolution Draw Circle Show Quad Screenshot, in case WebGL doesn't work WebGL Vertex Shader circle.vs WebGL Fragment Shader circle.fs WebGL Javascript circleSimple.js Let’s start out simple. Using GLSL Shaders we tell the GPU of your device to draw a circle in the most simple and naive way possible, as seen in circle.fs above: If the length() from the middle point is bigger than 1.0, we discard the pixel. The circle is blocky, especially at smaller resolutions. More painfully, there is strong “pixel crawling”, an artifact that’s very obvious when there is any kind of movement. As the circle moves, rows of pixels pop in and out of existence and the stair steps of the pixelation move along the side of the circle like beads of different speeds. The low ¼ and ⅛ resolutions aren't just there for extreme pixel-peeping, but also to represent small elements or ones at large distance in 3D. At lower resolutions these artifacts come together to destroy the circular form. The combination of slow movement and low resolution causes one side’s pixels to come into existence, before the other side’s pixels disappear, causing a wobble. Axis-alignment with the pixel grid causes “plateaus” of pixels at every 90° and 45° position. Technical breakdown Understanding the GPU code is not necessary to follow this article, but will help to grasp whats happening when we get to the analytical bits. 4 vertices making up a quad are sent to the GPU in the vertex shader circle.vs, where they are received as attribute vec2 vtx. The coordinates are of a “unit quad”, meaning the coordinates look like the following image. With one famous exception, all GPUs use triangles, so the quad is actually made up of two triangles. Schematic make-up of the unit quad The vertices here are given to the fragment shader circle.fs via varying vec2 uv. The fragment shader is called per fragment (here fragments are pixel-sized) and the varying is interpolated linearly with perspective corrected, barycentric coordinates, giving us a uv coordinate per pixel from -1 to +1 with zero at the center. By performing the check if (length(uv) < 1.0) we draw our color for fragments inside the circle and reject fragments outside of it. What we are doing is known as “Alpha testing”. Without diving too deeply and just to hint at what’s to come, what we have created with length(uv) is the signed distance field of a point. Just to clarify, the circle isn't \"drawn with geometry\", which would have finite resolution of the shape, depending on how many vertices we use. It's \"drawn by the shader\". SSAA SSAA stands for Super Sampling Anti-Aliasing. Render it bigger, downsample to be smaller. The idea is as old as 3D rendering itself. In fact, the first movies with CGI all relied on this with the most naive of implementations. One example is the 1986 movie “Flight of the Navigator”, as covered by Captain Disillusion in the video below. SSAA as used in \"Flight of the Navigator\" (1986) Excerpt from \"Flight of the NavigatorVFXcool\" YouTube Video by Captain Disillusion 1986 did it, so can we. Implemented in mere seconds. Easy, right? Native Resolution ½ Resolution ¼ Resolution ⅛ Resolution Screenshot, in case WebGL doesn't work SSAA buffer Fragment Shader post.fs WebGL Javascript circleSSAA.js circleSSAA.js draws at twice the resolution to a texture, which fragment shader post.fs reads from at standard resolution with GL_LINEAR to perform SSAA. So we have four input pixels for every one output pixel we draw to the screen. But it’s somewhat strange: There is definitely Anti-Aliasing happening, but less than expected. There should be 4 steps of transparency, but we only get two! Especially at lower resolutions, we can see the circle does actually have 4 steps of transparency, but mainly at the 45° “diagonals” of the circle. A circle has of course no sides, but at the axis-aligned “bottom” there are only 2 steps of transparency: Fully Opaque and 50% transparent, the 25% and 75% transparency steps are missing. Conceptually simple, actually hard We aren’t sampling against the circle shape at twice the resolution, we are sampling against the quantized result of the circle shape. Twice the resolution, but discrete pixels nonetheless. The combination of pixelation and sample placement doesn’t hold enough information where we need it the most: at the axis-aligned “flat parts”. Four times the memory and four times the calculation requirement, but only a half-assed result. Implementing SSAA properly is a minute craft. Here we are drawing to a 2x resolution texture and down-sampling it with linear interpolation. So actually, this implementation needs 5x the amount of VRAM. A proper implementation samples the scene multiple times and combines the result without an intermediary buffer. With our implementation, we can't even do more than 2xSSAA with one texture read, as linear interpolation happens only with 2x2 samples. To combat axis-alignment artifacts like with our circle above, we need to place our SSAA samples better. There are multiple ways to do so, all with pros and cons. To implement SSAA properly, we need deep integration with the rendering pipeline. For 3D primitives, this happens below API or engine, in the realm of vendors and drivers. SSAA sample patterns. Source In fact, some of the best implementations were discovered by vendors on accident, like SGSSAA. There are also ways in which SSAA can make your scene look worse. Depending on implementation, SSAA messes with mip-map calculations. As a result the mip-map lod-bias may need adjustment, as explained in the article above. WebXR UI package three-mesh-ui, a package mature enough to be used by Meta, uses shader-based rotated grid super sampling to achieve sharp text rendering in VR, as seen in the code. MSAA MSAA is super sampling, but only at the silhouette of models, overlapping geometry, and texture edges if “Alpha to Coverage” is enabled. MSAA is implemented by the graphics card in-hardware by the graphics vendors and what is supported depends on hardware. In the select box below you can choose different MSAA levels for our circle. There is up to MSAA x64, but what is available is implementation defined. WebGL 1 has no support, which is why the next canvas initializes a WebGL 2 context. In WebGL, NVIDIA limits MSAA to 8x on Windows, even if more is supported, whilst on Linux no such limit is in place. On smartphones you will only get exactly 4x, as discussed below. No MSAA MSAA - 2x MSAA - 4x MSAA - 8x MSAA - 16x MSAA - 32x MSAA - 64x Native Resolution ½ Resolution ¼ Resolution ⅛ Resolution edge smoothing edge smoothing MSAA 4x Screenshot, in case WebGL 2 doesn't work WebGL Javascript circleMSAA.js What is edge smoothing and how does MSAA even know what to sample against? For now we skip the shader code and implementation. First let's take a look at MSAA's pros and cons in general. Implementation specific headaches We rely on hardware to do the Anti-Aliasing, which obviously leads to the problem that user hardware may not support what we need. The sampling patterns MSAA uses may also do things we don’t expect. Depending on what your hardware does, you may see the circle’s edge transparency steps appearing “in the wrong order”. Sample pattern and circle shape clash: pixels are seemingly \"checkerboxed\" When MSAA became required with OpenGL 3 & DirectX 10 era of hardware, support was especially hit & miss. Even latest Intel GMA iGPUs expose the OpenGL extension EXT_framebuffer_multisample, but don’t in-fact support MSAA, which led to confusion. But also in more recent smartphones, support just wasn’t that clear-cut. Double edges - iOS 2xMSAA, created by iOS rounding transparency of 4xMSAA Mobile chips support exactly MSAAx4 and things are weird. Android will let you pick 2x, but the driver will force 4x anyways. iPhones & iPads do something rather stupid: Choosing 2x will make it 4x, but transparency will be rounded to nearest 50% multiple, leading to double edges in our example. There is hardware specific reason: Performance cost: (maybe) Zero Looking at modern video games, one might believe that MSAA is of the past. It usually brings a hefty performance penalty after all. Surprisingly, it’s still the king under certain circumstances and in very specific situations, even performance free. As a gamer, this goes against instinct... Video: MSAA 4x is performance free in certain contexts Excerpt from \"Developing High Performance Games for Different Mobile VR Platforms\" GDC 2017 talk by Rahul Prasad Rahul Prasad: Use MSAA […] It’s actually not as expensive on mobile as it is on desktop, it’s one of the nice things you get on mobile. […] On some (mobile) GPUs 4x (MSAA) is free, so use it when you have it. As explained by Rahul Prasad in the above talk, in VR 4xMSAA is a must and may come free on certain mobile GPUs. The specific reason would derail the blog post, but in case you want to go down that particular rabbit hole, here is Epic Games’ Niklas Smedberg giving a run-down. Video: Tiled based rendering GPU architecture Excerpt from \"Next-Generation AAA Mobile Rendering\" GDC 2014 talk by Niklas Smedberg and Timothy Lottes In short, this is possible under the condition of forward rendering with geometry that is not too dense and the GPU having tiled-based rendering architecture, which allows the GPU to perform MSAA calculations without heavy memory access and thus latency hiding the cost of the calculation. Here’s deep dive, if you are interested. A complex toolbox MSAA gives you access to the samples, making custom MSAA filtering curves a possibility. It also allows you to merge both standard mesh-based and signed-distance-field rendering via alpha to coverage. This complex features set made possible the most out-of-the-box thinking I ever witnessed in graphics programming: Assassin’s Creed Unity used MSAA to render at half resolution and reconstruct only some buffers to full-res from MSAA samples, as described on page 48 of the talk “GPU-Driven Rendering Pipelines” by Ulrich Haar and Sebastian Aaltonen. Kinda like variable rate shading, but implemented with duct-tape and without vendor support. The brain-melting lengths to which graphics programmers go to utilize hardware acceleration to the last drop has me sometimes in awe. Post-Process Anti-Aliasing In 2009 a paper by Alexander Reshetov struck the graphics programming world like a ton of bricks: take the blocky, aliased result of the rendered image, find edges and classify the pixels into tetris-like shapes with per-shape filtering rules and remove the blocky edge. Anti-Aliasing based on the morphology of pixels - MLAA was born. Computationally cheap, easy to implement. Later it was refined with more emphasis on removing sub-pixel artifacts to become SMAA. It became a fan favorite, with an injector being developed early on to put SMAA into games that didn’t support it. Some considered these too blurry, the saying “vaseline on the screen” was coined. It was the future, a sign of things to come. No more shaky hardware support. Like Fixed-Function pipelines died in favor of programmable shaders Anti-Aliasing too became \"shader based\". FXAA We’ll take a close look at an algorithm that was inspired by MLAA, developed by Timothy Lottes. “Fast approximate anti-aliasing”, FXAA. In fact, when it came into wide circulation, it received some incredible press. Among others, Jeff Atwood pulled neither bold fonts nor punches in his 2011 blog post, later republished by Kotaku. Jeff Atwood: The FXAA method is so good, in fact, it makes all other forms of full-screen anti-aliasing pretty much obsolete overnight. If you have an FXAA option in your game, you should enable it immediately and ignore any other AA options. Let’s see what the hype was about. The final version publicly released was FXAA 3.11 on August 12th 2011 and the following demos are based on this. First, let’s take a look at our circle with FXAA doing the Anti-Aliasing at default settings. Native Resolution ½ Resolution ¼ Resolution ⅛ Resolution Screenshot, in case WebGL doesn't work WebGL FXAA Shader post-FXAA.fs WebGL Javascript circleFXAA.js A bit of a weird result. It looks good if the circle wouldn’t move. Perfectly smooth edges. But the circle distorts as it moves. The axis-aligned top and bottom especially have a little nub that appears and disappears. And switching to lower resolutions, the circle even loses its round shape, wobbling like Play Station 1 graphics. Per-pixel, FXAA considers only the 3x3 neighborhood, so it can’t possibly know that this area is part of a big shape. But it also doesn’t just “blur edges”, as often said. As explained in the official whitepaper, it finds the edge’s direction and shifts the pixel’s coordinates to let the performance free linear interpolation do the blending. For our demo here, wrong tool for the job. Really, we didn’t do FXAA justice with our example. FXAA was created for another use case and has many settings and presets. It was created to anti-alias more complex scenes. Let’s give it a fair shot! FXAA full demo A scene from my favorite piece of software in existence: NeoTokyo°. I created a bright area light in an NT° map and moved a bench to create an area of strong aliasing. The following demo uses the aliased output from NeoTokyo°, calculates the required luminance channel and applies FXAA. All FXAA presets and settings at your finger tips. This has fixed resolution and will only be at you device's native resolution, if your device has no dpi scaling and the browser is at 100% zoom. Enable FXAA Enable Red Box Play / Pause Loading... 0% Show Luma Green as Luma ? FXAA_QUALITY_PRESET10 (fastest) 11 12 (default) 13 14 15 (highest quality) 20 (fastest) 21 22 23 24 25 26 27 28 29 (highest quality) 39 (EXTREME QUALITY)? fxaaQualitySubpix0.75 ? fxaaQualityEdgeThreshold0.166 ? fxaaQualityEdgeThresholdMin0.0833 ? Screenshot, in case WebGL doesn't work WebGL Vertex Shader FXAA-interactive.vs WebGL Fragment Shader FXAA-interactive.fs WebGL Javascript FXAA-interactive.js Just looking at the full FXAA 3.11 source, you can see the passion in every line. Portable across OpenGL and DirectX, a PC version, a XBOX 360 version, two finely optimized PS3 version fighting for every GPU cycle, including shader disassambly. Such level of professionalism and dedication, shared with the world in plain text. The sharing and openness is why I'm in love with graphics programming. It may be performance cheap, but only if you already have post-processing in place or do deferred shading. Especially in mobile graphics, memory access is expensive, so saving the framebuffer to perform post processing is not always a given. If you need to setup render-to-texture in order to have FXAA, then the “F” in FXAA evaporates. In this article we won’t jump into modern temporal anti-aliasing, but before FXAA was even developed, TAA was already experimented with. In fact, FXAA was supposed to get a new version 4 and incorporate temporal anti aliasing in addition to the standard spatial one, but instead it evolved further and rebranded into TXAA. Analytical Anti Aliasing Now we get to the good stuff. Analytical Anti-Aliasing approaches the problem backwards - it knows the shape you need and draws the pixel already Anti-Aliased to the screen. Whilst drawing the 2D or 3D shape you need, it fades the shape’s border by exactly one pixel. Native Resolution ½ Resolution ¼ Resolution ⅛ Resolution edge smoothing edge smoothing Screenshot, in case WebGL doesn't work WebGL Vertex Shader circle-analytical.vs WebGL Fragment Shader circle-analytical.fs WebGL Javascript circleAnalytical.js Always smooth without artifacts and you can adjust the amount of filtering. Preserves shape even at low resolutions. No extra buffers or extra hardware requirements. Even runs on basic WebGL 1.0 or OpenGLES 2.0, without any extensions. With the above buttons, you can set the smoothing to be equal to one pixel. This gives a sharp result, but comes with the caveat that axis-aligned 90° sides may still be perseved as “flat” in specific combinations of screen resolution, size and circle position. Filtering based on the diagonal pixel size of √2 px = 1.4142..., ensures the “tip” of the circle in axis-aligned pixel rows and columns is always non-opaque. This removes the perception of flatness, but makes it shape ever so slightly more blurry. Or in other words: as soon as the border has an opaque pixel, there is already a transparent pixel \"in front\" of it. This style of Anti-Aliasing is usually implemented with 3 ingredients: Enabled Screen Space Derivative extension or having a modern graphics context Pixel-size calculated via length+dFdx+dFdy or approximated with fwidth Blending with smoothstep But if you look at the code box above, you will find circle-analytical.fs having none of those. And this is the secret sauce we will look at. Before we dive into the implementation, let’s clear the elephants in the room… What even is “Analytical”? In graphics programming, Analytical refers to effects created by knowing the make-up of the intended shape beforehand and performing calculations against the rigid mathematical definition of said shape. This term is used very loosely across computer graphics, similar to super sampling referring to multiple things, depending on context. A picture is worth a thousand words... Character soft-shadow from stretched spheres in The Last Of Us. Lighting Technology of \"The Last Of Us\", Siggraph 2013 talk by Michał Iwanicki Very soft soft-shadows which include contact-hardening, implemented by algorithms like percentage-closer soft shadows are very computationally intense and require both high resolution shadow maps and/or very aggressive filtering to not produce shimmering during movement. This is why Naughty Dog’s The Last of Us relied on getting soft-shadows on the main character by calculating the shadow from a rigidly defined formula of a stretched sphere, multiple of which were arranged in the shape of the main character, shown in red. An improved implementation with shader code can be seen in this Shadertoy demo by romainguy, with the more modern capsule, as opposed a stretched sphere. This is now an integral part of modern game engines, like Unreal. As opposed to standard shadow mapping, we don’t render the scene from the perspective of the light with finite resolution. We evaluate the shadow per-pixel against the mathematical equation of the stretched sphere or capsule. This makes capsule shadows analytical. A video is worth a thousand words, 30 times a second. Capsule representation of characters in The Last of Us Part II YouTube Video by \"Max Lebled's 2nd channel\" Staying with the Last of Us, The Last of Us Part II uses the same logic for blurry real-time reflections of the main character, where Screen Space Reflections aren’t defined. Other options like raytracing against the scene, or using a real-time cubemap like in GTA V are either noisy and low resolution or high resolution, but low performance. Here the reflection calculation is part of the material shader, rendering against the rigidly defined mathematical shape of the capsule per-pixel, multiple of which are arranged in the shape of the main character. This makes capsule reflections analytical. An online demo with is worth at least a million... ...yeah the joke is getting old. Shadertoy demo for Analytical Ambient Occlusion by Inigo Quilez Ambient Occlusion is essential in modern rendering, bringing contact shadows and approximating global illumination. Another topic as deep as the ocean, with so many implementations. Usually implemented by some form of “raytrace a bunch of rays and blur the result”. In this Shadertoy demo, the floor is evaluated per-pixel against the rigidly defined mathematical description of the sphere to get a soft, non-noisy, non-flickering occlusion contribution from the hovering ball. This implementation is analytical. Not just spheres, there are analytical approaches also for complex geometry. By extension, Unreal Engine has distance field approaches for Soft Shadows and Ambient Occlusion, though one may argue, that this type of signed distance field rendering doesn’t fit the description of analytical, considering the distance field is precalculated into a 3D texture. Implementation Let’s dive into the sauce. We work with signed distance fields, where for every point that we sample, we know the distance to the desired shape. This information may be baked into a texture as done for SDF text rendering or maybe be derived per-pixel from a mathematical formula for simpler shapes like bezier curves or hearts. Based on that distance we fade out the border of the shape. If we fade by the size of one pixel, we get perfectly smooth edges, without any strange side effects. The secret sauce is in the implementation and under the sauce is where the magic is. How does the shader know the size of pixel? How do we blend based on distance? This approach gives motion-stable pixel-perfection, but doesn't work with traditional rasterization. The full shape requires a signed distance field. Native Resolution ½ Resolution ¼ Resolution ⅛ Resolution Pixel size methodPre-calculate pixel size length + dFdx + dFdy fwidth? Blend methodSimple division Linear step Linear step, No Clamp Smooth step? Smoothing1.0px ? Radius adjust 0.0px ? Screenshot, in case WebGL doesn't work WebGL Vertex Shader circle-analytical.vs WebGL Fragment Shader circle-analyticalCompare.fs WebGL Javascript circleAnalyticalComparison.js How big is a pixel? Specifically, by how much do we fade the border? If we hardcode a static value, eg. fade at 95% of the circle’s radius, we may get a pleasing result for that circle size at that screen resolution, but too much smoothing when the circle is bigger or closer to the camera and aliasing if the circle becomes small. Too much edge fading relative to this circle size We need to know the size of a pixel. This is in part what Screen Space derivatives were created for. Shader functions like dFdx, dFdy and fwidth allow you to get the size of a screen pixel relative to some vector. In the above circle-analyticalCompare.fs we determine by how much the distance changes via two methods: pixelSize = fwidth(dist); /* or */ pixelSize = length(vec2(dFdx(dist), dFdy(dist))); Relying on Screen Space derivatives has the benefit, that we get the pixel size delivered to us by the graphics pipeline. It properly respects any transformations we might throw at it, including 3D perspective. The down side is that it is not supported by the WebGL 1 standard and has to be pulled in via the extension GL_OES_standard_derivatives or requires the jump to WebGL 2. Luckily I have never witnessed any device that supported WebGL 1, but not the Screen Space derivatives. Even the GMA based Thinkpad X200 & T500 I hardware modded do. Possibly painful Generally, there are some nasty pitfalls when using Screen Space derivatives: how the calculation happens is up to the implementation. This led to the split into dFdxFine() and dFdxCoarse() in later OpenGL revisions. The default case can be set via GL_FRAGMENT_SHADER_DERIVATIVE_HINT, but the standard hates you: OpenGL Docs: The implementation may choose which calculation to perform based upon factors such as performance or the value of the API GL_FRAGMENT_SHADER_DERIVATIVE_HINT hint. Why do we have standards again? As a graphics programmer, anything with hint has me traumatized. Luckily, neither case concerns us, as the difference doesn’t show itself in the context of Anti-Aliasing. Performance technically dFdx and dFdy are free (or rather, their cost is already part of the rendering pipeline), though the pixel size calculation using length() or fwidth() is not. It is performed per-pixel. dFdx + dFdy + length() vs fwidth() This is why there exist two ways of doing this: getting the length() of the vector that dFdx and dFdy make up, a step involving the historically performance expensive sqrt() function or using fwidth(), which is the approximation abs(dFdx()) + abs(dFdy()) of the above. It depends on context, but on semi-modern hardware a call to length() should be performance trivial though, even per-pixel. To showcase the difference, the above Radius adjust slider works off of the Pixel size method and adjusts the SDF distance. If you go with fwidth() and a strong radius shrink, you’ll see something weird. Rhombous warping at small shape sizes due to use of fwidth() The diagonals shrink more than they should, as the approximation using addition scales too much diagonally. We’ll talk about professional implementations further below in a moment, but using fwidth() for AAA is what Unity extension “Shapes” by Freya Holmér calls “Fast Local Anti-Aliasing” with the following text: Fast LAA has a slight bias in the diagonal directions, making circular shapes appear ever so slightly rhombous and have a slightly sharper curvature in the orthogonal directions, especially when small. Sometimes the edges in the diagonals are slightly fuzzy as well. This effects our fading, which will fade more on diagonals. Luckily, we fade by the amount of one pixel and thus the difference is really only visible when flicking between the methods. What to choose depends on what you care more about: Performance or Accuracy? But what if I told you can have your cake and eat it too… DIY …Calculate it yourself! For the 2D case, this is trivial and easily abstracted away. We know the size our context is rendering at and how big our quad is that we draw on. Calculating the size of the pixel is thus done per-object, not per-pixel. This is what happens in the above circleAnalyticalComparison.js. /* Calculate pixel size based on height. Simple case: Assumes Square pixels and a square quad. */ gl.uniform1f(pixelSizeCircle, (2.0 / (canvas.height / resDiv))); No WebGL 2, no extensions, works on ancient hardware. The results are identical to the dFdx + dFdy + length() case, with the benefit of fully skipping the per-pixel calculation. This does become more involved, once the quad is stretched and performance-painful when perspective is involved. How do we blend? Ok, now we have the amount we want to blend by. The next step is to perform the adjustment of opacity. If we are doing 2D, then Alpha blending is the way to go. Straight forward, will never betray you. Another option is using MSAA + Alpha to Coverage, as is done in the MSAA demo above. There are pit falls with the latter, as discussed previously and more headaches to follow below. The reason you would need this is for depth-buffer writes for correct blending in 3D scenes. For the MSAA and AAA demos above, merely an API level switch. In both cases, the shaders are 100% identical! Still the alpha itself has to be faded based on distance. Here is where a “step” function comes in. We can input a start, an end point and the function will fade between them. Usually, this is where the graphics programmer’s favorite smoothstep() comes in and where this blog post’s hot take begins: Don’t use smoothstep() Its use is often associated with implementing anti-aliasing in GLSL, but its use doesn’t make sense in this context. It performs a hermite interpolation, but we are dealing with a function applied across 2 pixels or just inside 1. There is no curve to be witnessed here. To be precise, both sampling and blending witness the smoothstep curve in the sub-pixel make-up of the edge, but the difference is tiny and can be corrected using an adjusted smoothing amount. Smoothstep and linear comparison Even though the slight performance difference doesn’t particularly matter on modern graphics cards, wasting cycles on performing the hermite interpolation doesn’t make sense to me. Let’s DIY it! The implementation of smoothstep() is up to the vendor, but for the float case it’s essentially just : float smoothstep(float edge0, float edge1, float x) { float t = clamp((x - edge0) / (edge1 - edge0), 0.0, 1.0);return t * t * (3.0 - 2.0 * t); } ... float alpha = smoothstep(1.0, 1.0 - pixelSize * smoothingAmount, dist); We can rip out the hermite interpolation and stick to the simple linear one. If you flick between the two in the above demo, you’ll see only a slight change, with pixel sized smoothing. At pixel size, the difference can easily be counter acted with an adjustment to the smoothing factor if you like one method over the other. /* Step function with Linear Interpolation instead of the Hermite Interpolation */ float linearstep(float edge0, float edge1, float x) { return clamp((x - edge0) / (edge1 - edge0), 0.0, 1.0); } ... float alpha = linearstep(1.0, 1.0 - pixelSize * smoothingAmount, dist); But why even clamp? Alpha values below 0.0 or above 1.0 will be taken care of by the rendering pipeline during the blending step and thus no clamping is required. It is required when having multiple shapes on one quad, something I’ll go into below. But in the one shape per quad case, we can delete it. /* Step function with Linear Interpolation, but no clamping */ float linearstepNoclamp(float edge0, float edge1, float x) { return (x - edge0) / (edge1 - edge0); } ... float alpha = linearstepNoclamp(1.0, 1.0 - pixelSize * smoothingAmount, dist); But wait a moment… When doing Anti-Aliasing we wish to affect the border of the shape, specifically distance 1.0, so most of this function cancels out! In fact, we don’t need a step function. The blending can be performed by a simple division. float alpha = (1.0 - dist) / (pixelSize * smoothingAmount); I have been using this simplified term in different places for years. Performance wise, the most expensive thing still remains: the per-pixel division. Modern cards should also have no issues optimizing the hermite interpolation’s multiplication and addition down to a few Fused Multiply-Add instructions. Still, I prefer the simplicity. What’s with the shrinking? There is an ellusive implementation interaction with MSAA and the rasterizer. Only when using this with MSAA + Alpha to Coverage (regardless of sample count), there will be exactly one side of the quad with a missing 0.5 pixels, on some hardware. This is why there is this weird 0.5 px breathing room being added. Hard edge bug with MSAA on select hardware Our circle is drawn to the very edge of the quad, which works, but only as long the graphics card doesn’t surprise us with edge cases. Specifically modern NVIDIA cards seems to eat one side of the quad too soon, though I have never seen this occur with alpha blending. To combat this, we give our SDF 0.5px of breathing room: /* We add half a pixel of breathing room. This is only required for the MSAA case. Depending on Hardware implementation, rasterization, MSAA sample count and placement, one row pixels may or may not disappear too soon, when the circle's edge is right up against the unit quad's border */ dist += pixelSizeAdjusted * 0.5; An edge case. Drawing multiple? You can draw multiple shapes in one Quad and both will be Anti-Aliased, though blending will start to get more involved. In that case both shapes will need to be evaluated per-pixel and their results will need to be clamped, weighted and summed, otherwise there won’t be Anti-Aliasing when they intersect. Aliasing free blending of multiple visualizations From 🔮 Mathematical Magic Mirrorball Here is what blending looks like in my WebApp 🔮 Mathematical Magic Mirrorball, a WebApp which pulls 360° panoramic projections from photos, videos and live-streams of mirror balls. There I have multiple visualizations and color overlays explaining resolution distribution of the projection. The code to keep all this anti-aliased is: float factorGreen = area_toggle * clamp((area_f - lenCircle) * pxsize_rcp, 0.0, 1.0); float factorRed = area_toggle * clamp((lenCircle - area_b) * pxsize_rcp, 0.0, 1.0) * smoothedAlpha; float factorBlack = mask_toggle * (1.0 - smoothedAlpha); vec3 finalColor = baseColor * (1.0 - factorGreen - factorRed - factorBlack) + greenColor * factorGreen + redColor * factorRed + blackColor * factorBlack; All this additional stuff … why not draw color overlays in an additional pass? The cost of drawing across that area again is an order of magnitude higher than just coloring the output in the shape we need as we go. Tinting in an Anti-Aliased fashion in one draw-call is the cleanest way to do this I think. 3D Everything we talked about extends to the 3D case as well. We won’t dig into 3D shapes themselves and will stick to a 2D rounded square in 3D perspective with a moving camera. I use this a lot when graphics programming to create a scene with a “ground floor” where my objects live on. Native Resolution ½ Resolution ¼ Resolution ⅛ Resolution Draw Rounded Square Show Quad Screenshot, in case WebGL doesn't work WebGL Vertex Shader 3DAnalytical.vs WebGL Fragment Shader 3DAnalytical.fs WebGL Javascript 3DAnalytical.js With the 3D camera and resulting perspective matrix multiplication, we use the reliable screen space derivatives again to get the pixel size. But in reality, we can still do without! This would require us to multiply of the inverse perspective matrix with the fragment coordinates per pixel. Performance-painful, yet possible. Unmentioned challenges There is something I have not explained yet, a persistent misunderstanding I held until Yakov Galka explained the deetz to me on stackoverflow. Depending on how we setup the blending math, to perform the smoothing we may remove pixel alpha on the inside of the shape, add it to the outside or center it. Adding or subtracting would mess with the shape every so slightly, especially at small sizes or under strong perspective. So centering is the way to go. Unfortunately, centering the fade on the border can put the edge outside our quad and lead to hard edges or clipping. Clipping of the border. (Overdone for emphasis) In 3D this is especially painful, as there is no amount of safety margin that would solve this, with the camera at oblique angles. Nvidia introduced the vendor specific extension NV_conservative_raster_dilate to always give you an extra pixel at the border. Unfortunately it’s not available in WebGL and specific to NVIDIA hardware. Border pixels not rasterized due to fading overshooting the quad Source: Explanation on Stack overflow by Yakov Galka So we are forced to shrink the border in all cases. This leads to smooth edges even under strong perspective, but technically influences the shape. This is absolutely not visible in isolation, but may lead to mismatches or unexpected behavior, as even perspective has now an influence on the shape. Border pixels rasterized with shrunken border Source: Explanation on Stack overflow by Yakov Galka For the 2D case, we could implement a kind of NV_conservative_raster_dilate ourselves, by growing the quad in the vertex shader by one pixel and shrink the signed distance field by one pixel in the fragment shader. And this is exactly what’s happening in the 2D demos on this page! This is really pedantic and just here for correctness. In most cases, you don't need to be so precise. That is the reason the red box always lines up with the border, at all resolution switches and with all 2D demos on this page. Specifically in the vertex shader, the line responsible for this is: /* Grow the Quad and thus the \"canvas\", that the circle is drawn on. The pixelSize is added for two reasons: 0.5px to get the original circle size again, as the AAA fading is set to fade the edge on the circle inside, preventing hard edges due to unrasterized pixels. And another 0.5px is to correct the \"breathing room\" added in the fragment shader, specifically for the MSAA sampling case, as hardware specific issues around MSAA sampling may or may not result in transparent pixels disappearing too soon. */ vertex *= size + pixelSize; Not messing up gamma and multiplied vs premultiplied alpha are important for all forms of AA, but are very context dependant. This blog post is about AAA specifically, thus we ignore these. What are the big boys doing? This rendering approach has found its way into many professional products. Let’s finish by looking at some of them. “Shapes” for Unity Feature-wise the most complete implementation of this approach is in Unity extension Shapes by Freya Holmér. There the SDFs are either anti-aliased by MSAA or are blended like in this blog post, though it’s referred to as “Fast Local Anti-Aliasing” for the fwidth() case and “Corrected Local Anti-Aliasing” for the length() case. Trailer for \"Shapes\" by Freya Holmér With motion-blur, shape-respecting color gradients and lines below 1px being opacity faded to prevent further aliasing, this is signed-distance field rendering and AAA by extension, implemented to its logical conclusion. Valve Software’s implementation Hud elements in Team Fortress 2 Valve introduced extensive use of signed distance field rendering to the Source engine during the development of the Orange Box. Most prominently in Team Fortress 2, where it was used to create smooth yet sharp UI elements on the HUD. It even received its own Developer Commentary entry. Alden Kroll: Two-dimensional HUD elements present a particular art problem, because they have to look good and sharp no matter what resolution the user is running their game at. Given today’s availability of high resolution wide-screen displays, this can require a lot of texture memory and a lot of work anticipating different display resolutions. The problem for Team Fortress 2 was even more daunting because of our desire to include a lot of smooth curved elements in our HUD. We developed a new shader system for drawing ‘line art’ images. The system allows us to create images at a fixed resolution that produced smooth silhouettes even when scaled up to a very high resolution. This shader system also handles outlining and drop-shadows, and can be applied in the 3D space to world elements such as signs. 64x64 Texture: Alpha blended, Alpha Tested and SDF rendering Paper: Improved Alpha-Tested Magnification for Vector Textures and Special Effect They also released a paper describing the specific implementation, including a showcase for use in the 3D game world, though I have never seen it used in the game world itself in Valve titles. Added as a mere footnote to the paper, was a way to improve rendering with sharp corners… The future of all things font? If you save a signed distance field into a texture and sample it with linear interpolation, you will get perfectly sharp characters at any size, but the limited resolution will result in clipped or rounded corners, depending on implementation math. Picking up on that foot note and bringing the technique to its logical conclusion was the most thorough and well composed Master Thesis I ever read: “Shape Decomposition for Multi-channel Distance Fields” by Viktor Chlumský, which included code for the font-file to SDF conversion and a full font atlas generator. Basically, use RGB and a median term to get perfectly sharp text at any size, including an Alpha channel with the classical SDF for effects like glows and drop shadows, all done on the GPU with no run-time baking or intense processing. If you dig around in video games, you will find SDF based font rendering from time to time! Multi-Channel SDF demo from msdf-atlas-gen From experience I can tell you, that there are more implementation headaches. Chinese, Japanese, Korean characters require bigger textures to resolve their minute details. Bigger textures means you’ll often minimize during rendering, but minimizing may introduce artifacts on its own… But considering the current state of browser font baking + rendering and the pure insanity of edge-cases covered, including synthetic fallbacks for missing italic or bold variants and baking 4 variants with 0.25px offsets to account for minute sampling issues, I think SDF text rendering has not been given enough serious consideration. \"Text rendering hates you\" is a recommended read if you want to see how crushingly complex this topic gets. You may be wondering, if we can get the analytical solution for a bezier curve, why bake into textures instead? We may know the solution for one segment, but to get the full shape we need to sum up all the contributions from all segments. This works, but performance tanks hard, as we solve every bezier curve segment per pixel. Clarity should not be a luxury Modern video games often use TAA in combination with dynamic resolution scaling, a concoction guaranteed to result in blurriness. These AA algorithms come with post-process sharpening built-in to combat this, as is done in FSR or TAA. Fixing blurring by sharpening, I find this a bit of graphics programming sin. TAA Sharpening in Warframe Whole communities rally around fixing this, like the reddit communities “r/MotionClarity” or the lovingly titled “r/FuckTAA”, all with the understanding, that Anti-Aliasing should not come at the cost of clarity. FXAA creator Timothy Lottes mentioned, that this is solvable to some degree with adjustments to filtering, though even the most modern titles suffer from this. What we have not talked about are the newer machine learning approaches as done for instance with NVIDIA’s DLAA, as that is really outside the scope of this post. Suffice to say Timothy Lottes is not a fan. As for AAA, it’s lovely being able to draw smooth yet sharp, motion-stable shapes of any size at native resolutions. Please feel free to use these techniques in your projects.",
    "commentLink": "https://news.ycombinator.com/item?id=42191709",
    "commentBody": "AAA – Analytical Anti-Aliasing (frost.kiwi)321 points by todsacerdoti 6 hours agohidepastfavorite37 comments FrostKiwi 5 hours agoThanks for sharing! Author here, happy to answer any questions. reply vanderZwan 31 minutes agoparentOne small bit of tecnical feedback for the website itself: it would be nice if the links in the article open in a new tab by default, because reloading the webpage via the back button is a little broken on my mobile browsers. I suspect it has something to do with trying to restore the state of the page while also having WebGL contexts. reply amitp 3 hours agoparentprevFantastic article! I've been trying to figure out antialiasing for MSDF fonts, and have run across some claims: 1. antialiasing should be done in linear rgb space instead of srgb space [1] [2] 2. because of the lack of (1) for decades, fonts have been tweaked to compensate, so sometimes srgb is better [3] [4] Do you have advice on linear vs srgb space antialiasing? [1] https://www.puredevsoftware.com/blog/2019/01/22/sub-pixel-ga... [2] http://hikogui.org/2022/10/24/the-trouble-with-anti-aliasing... [3] https://news.ycombinator.com/item?id=12023985 [4] http://hikogui.org/2022/10/24/the-trouble-with-anti-aliasing... reply flohofwoe 5 hours agoparentprevGreat post! Minor nitpick: WebGL does support MSAA since WebGL1, but in WebGL1 only on the canvas, and you don't have any control over the number of samples (can only select antialiasing on/off) - not that it matters much anymore :) What WebGL2 is still missing is MSAA texture objects (it only supports MSAA render buffers), which makes it impossible to directly load individual samples in a shader (useful for custom-resolve render passes). That's only possible in WebGPU. reply tobr 1 hour agoparentprevGreat write up, excellent explorables. I skimmed some parts so forgive me if this was covered, but I wonder what happens with overlapping shapes in this approach. For example, a white background with a black disc and then a white disc of the exact same size and position would probably leave a fuzzy gray hairline circle? With regular antialiasing it should be all white. reply Lerc 1 hour agoparentprevHow long did this take to write? I have done a few live visualization based blog posts, and they take me ages to do. I kind of think that's the right idea though. There is so much content out there, taking longer to produce less content at a higher quality benefits everyone. reply enbugger 2 hours agoparentprevAs a non-gamedev person but just gamer, I should expect that this will replace TAA anytime soon? Should it replace TAA? reply ferbivore 5 hours agoparentprev> Mobile chips support exactly MSAAx4 [...] the driver will force 4x anyways On what GPUs and through what APIs did you see this? This seems fairly weird. I especially wouldn't expect Apple to have problems. reply ferbivore 4 hours agoparentprevNot a question but some unsolicited (sorry) feedback. The intro seems designed to set people up for disappointment. You start off by talking about AA methods used for 3D scenes, and you've picked a very cool way to present them... but the article is actually about antialiased drawing of SDFs, which is not exactly a hard problem and not applicable to 3D scenes. Unless your scene is made up of SDF shapes, but I don't think the method you're presenting would be fast enough on a nontrivial scene as you would need to rely on alpha-blending across seams. (I think Alex Evans' talk on Dreams mentions they tried something similar to get fuzzy shapes but dropped it due to perf and sorting issues.) In any case, it would have been nice for the article's intro to more clearly say what it's about and what the technique is useful for. reply Moosturm 5 hours agoparentprevWhat an absolutely fantastic read. reply amjoshuamichael 4 hours agoprevGraphics programming analysis done using examples written in WebGL–genius. Hypertext that takes full advantage of the medium. This reminds me of something I'd see on https://pudding.cool/, but it goes far more in depth than anything there. Absolutely fantastic article. I've been using MSAAx4 in my rendering engine for some time and only recently have considered switching to a FXAA / TAA implementation. I'm actually not sure I'm going to go through with that now. I definitely learned a lot here, and will probably use the analytical approach for UI items, I hadn't heard about that anywhere. Not often you see graphics-programming stuff on HN. For anyone interested in more graphics write-ups, this list of frame breakdowns is one of my favorite resources: https://www.adriancourreges.com/blog/ reply TimTheTinker 1 hour agoparentSteve Wittens also does a lot of these kinds of articles (math with WebGL-infused illustrations, etc.) at https://acko.net/ One of my favorites: https://acko.net/blog/how-to-fold-a-julia-fractal/. This helped me understand the relationship between trigonometric functions and complex numbers like nothing else I've ever seen. reply art0rz 2 hours agoparentprevI really dislike TAA, especially on lower framerates. There's too much ghosting. I often switch it to a slower algorithm just so I don't get ghosting. reply kridsdale1 1 hour agorootparentIt’s very strange. I had a vivid dream, only about 5 hours ago, where I was debating the drawbacks of TAA with some scientists in a lab (likely because Half Life has been in the news this week). I think I dream about rendering algorithms once every several years. And now today there’s this post and your comment here in the front page. reply vanderZwan 1 hour agoprevTangent: my biggest problem with AA is something adjacent to it, which is that almost none of my games bother explain what the differences are between the different abbreviations available in the settings, half of which are completely unknown to me. Like, sure, I can look them up but a little bit of user-friendliness would be appreciated. This article will probably help for future reference though! reply ndileas 1 hour agoparentGames/graphics are one of those domains with a lot of jargon for sure. If you don't want to be a wizard you can just mess with it and see what happens. I like how dolphin approaches this with extensive tooltips in the settings, but there's always going to be some implicit knowledge. On a meta level - I feel like I've seen anti-acronym sentiment a lot recently. I feel like it's never been easier to look these things up. There's definitely levels of acronyms which are anti-learning or a kind of protectionism, but to my mind there are appropriate levels of it to use because you have to label concepts at a useful level to accomplish things, and graphics settings of a game definitely are on the reasonable side. reply GuB-42 45 minutes agorootparent> just mess with it and see what happens And even if you know every detail, that's still the best course of action, I think. Which kind of antialiasing you prefer, and how it trades with performance and resolution is highly subjective, and it can be \"none\". There are 3 components to rescaling/rendering pixels: aliasing, sharpness and locality. Aliasing is, well, aliasing, sharpness is the opposite of blurriness, and locality is about these \"ringing\" artefacts you often see in highly compressed images and videos. You can't be perfect on all three. Disabling antialiasing gives you the sharpest image with no ringing artefacts, but you get these ugly staircase effects. Typical antialiasing trades this for blurriness, in fact, FXAA is literally a (selective) blur, that's why some people don't like it. More advanced algorithms can give you both antialiasing and sharpness, but you will get these ringing artefacts. The best of course is to increase resolution until none of these effects become noticeable, but you need the hardware. The best algorithms attempt to find a good looking balance between all these factors and performance, but \"good looking\" is subjective, that's why your best bet is to try for yourself. Or just keep the defaults, as it is likely to be set to what the majority of the people prefer. reply kridsdale1 1 hour agorootparentprevThe PS4 Pro introduced the gaming world to the simplification of settings from dozens of acronyms that were common to PC Gamers, down to “Performance” and “Quality”. I wouldn’t be surprised if there’s now a market demand for that to spread back to PC land. reply armada651 27 minutes agorootparentPC games have had Low, Medium, High presets for graphics settings for decades. I don't think reducing that from 3 choices to 2 is going to be a big win for user friendliness. And I certainly think it's user-hostile if it means taking the customization away and only letting users choose between two presets. PlayStation does have shining examples of user-friendly settings UI though, namely in their PC ports. Look at this UI in Ratchet and Clank: https://x.com/digitalfoundry/status/1684221473819447298 Extensive tooltips for each option and any time you change a setting it is applied immediately to the paused game while you're still in the settings menu allowing you to immediately compare the effects. reply ferbivore 0 minutes agorootparentNixxes are very enthusiastic about every PC port having proper settings and exclusive options. I hope the C-levels at Sony continue to not notice. whywhywhywhy 6 minutes agorootparentprev> I don't think reducing that from 3 choices to 2 is going to be a big win for user friendliness It’s nowhere near as simple as 3 settings now there are different antialiasing techniques, path tracing lighting or reflections, upscaling (multiple algorithms) etc. Nothing is all just fully positive and each has tradeoffs reply rootext 5 hours agoprevAwesome article. SDF(or mSDF) isn't the future. It's already \"good enough\" classic. > This works, but performance tanks hard, as we solve every > bezier curve segment per pixel This is \"the future\" or even present as used in Slug and DirectWrite with great performance https://sluglibrary.com/ https://learn.microsoft.com/en-us/windows/win32/directwrite/... reply Lichtso 4 hours agoparentDon't forget about implicit curve rendering [0]. The patent will expire soon [1]. [0]: https://www.microsoft.com/en-us/research/wp-content/uploads/... [1]: https://patents.google.com/patent/US20070097123A1/en reply Asooka 4 hours agoparentprevI may be remembering totally wrong, but isn't the algorithm used in Slug patented? reply qingcharles 57 minutes agorootparenthttps://news.ycombinator.com/item?id=42194175 reply SirMaster 1 hour agoprevI really miss MSAA. I still dislike DLSS personally. I realize many people seem to like it, but it just does not look that good to me. Or as good as things used to look or I believe could look. Sure it's better than TAA, but come on, this can't be the ultimate end for gaming graphics... At least I hope it isn't. reply mavamaarten 1 hour agoparentSame here. I can't put my finger on what exactly it is, but it just never feels and looks as good as lower quality at full resolution. reply serf 3 hours agoprevscrolling thru the post the NeoTokyo screenshot struck me instantly, I ran through the hallway thousands of times - I ran a server for that mod for some years and had great fun with a small community of good/capable people. reply apexalpha 3 hours agoprevThose frames with the circle and zoomed bit are a fantastic way to convey this message, well done the whole article reads great. reply Lichtso 5 hours agoprevGreat write-up! Though a little caveat from my side, as I have written both 2D and 3D rendering engines. Let me tell you, they could not be more different. It is not just another dimension but completely different goals, use-cases and expectations. So instead of: > Everything we talked about extends to the 3D case as well. I would say the entire post is mostly about 3D, not 2D rendering. If you are curious about this topic being approached for 2D rendering here is a nice write-up I found about that: https://ciechanow.ski/alpha-compositing/ One particular criteria for AA techniques that no one cares about in 3D but is very relevant in 2D is correctness and bias. AAA for example is heavily biased and thus incorrect. Drawing the exact same shape multiple times in the same place will make it more opaque / darker. The same thing does not happen with MSAA which has a bounded error and is unbiased. reply lagrange77 4 hours agoparentHey, i'm brainstorming for a 3d vector renderer in WebGPU on JS/TS and stumbled on your project [0] yesterday. (Thick) line drawing is especially interesting to me, since it's hard [1]. I also stumbled upon this [2] recently and then wondered if i could use that technique for every shape, by converting it to quadratic bezier curve segments. Do you think that's a path to follow? [0] https://github.com/Lichtso/contrast_renderer [1] https://mattdesl.svbtle.com/drawing-lines-is-hard [2] https://scribe.rip/@evanwallace/easy-scalable-text-rendering... reply Lichtso 3 hours agorootparentMy implementation does: - Implicit Curve Rendering (Loop-Blinn) and stencil geometry (tessellation-less) for filling - Polygonization (with tangent space parameter distribution) of offset curves for stroking > by converting it to quadratic bezier curve segments Mathematically, the offset curve of a bezier curve is not a bezier curve of the same degree in the general case (exceptions are trivial cases like straight lines, circles and ellipses). Instead you get terrible high degree polynomials. You will have to approximate the offset curve anyway. I choose to use polygons (straight line segments), but you could also use splines (bezier segments), it is just overtly complex for little to no benefit IMO. BTW, distance fields and offset curves are very similar. In fact the distance field is the set of all possible offsets of offset curves and the offset curves are the isolines on the distance field. Here is a good summary of all the edge cases to think about in 2D rendering: https://www.slideshare.net/slideshow/22pathrender/12494534 About subpixel AA: Don't bother, LCDs are on the down trend. reply lagrange77 2 hours agorootparentThank you very much for the hints! This rabbit hole gets deeper every day. :D reply polytely 1 hour agoprevamazing blog, both in content and presentation. Love it when articles give you controls to play with. Gives me hope for the future of the web. the NeoTokyo mention reveals great taste. reply theandrewbailey 4 hours agoprevI've been so used to screen space techniques that I initially read SSAA as \"screen space antialiasing\", not \"super sampled antialiasing\". reply rahkiin 4 hours agoparentMy favorite is stil SSSSS, or screen space sub surface scattering. reply hoseja 5 hours agoprev [–] What's the catch? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Analytical Anti-Aliasing is a technique designed to remove jagged edges (jaggies) in rasterized images by pre-calculating the fade of shape borders, ensuring smooth edges without artifacts.- This method is efficient as it does not require additional buffers or hardware and is compatible with basic WebGL 1.0, making it accessible for various applications.- It is highlighted for its professional use in platforms like Unity and Valve Software, offering clarity and sharpness without the blurriness common in other anti-aliasing methods, such as SSAA (Super-Sample Anti-Aliasing), SMAA (Subpixel Morphological Anti-Aliasing), and DLAA (Deep Learning Anti-Aliasing)."
    ],
    "commentSummary": [
      "The article \"AAA – Analytical Anti-Aliasing\" delves into anti-aliasing techniques, emphasizing analytical methods and their application in graphics programming.",
      "It discusses the debate between linear and sRGB color spaces, limitations of WebGL, and challenges with overlapping shapes, providing a comprehensive exploration of these topics.",
      "The article is noted for its depth, interactive elements, and feedback on its presentation, highlighting the complexity of graphics settings in games."
    ],
    "points": 321,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1732089800
  },
  {
    "id": 42193771,
    "title": "What is the origin of the lake tank image that has become a meme? (2021)",
    "originLink": "https://history.stackexchange.com/questions/57033/what-is-the-origin-of-the-lake-tank-image-that-has-become-a-meme",
    "originBody": "Join History By clicking “Sign up”, you agree to our terms of service and acknowledge you have read our privacy policy. Sign up with Google OR Email Password Sign up Already have an account? Log in X Skip to main content Stack Exchange Network Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange Loading… Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the workings and policies of this site About Us Learn more about Stack Overflow the company, and our products current community History help chat History Meta your communities Sign up or log in to customize your list. more stack exchange communities company blog Log in Sign up History Home Questions Tags Users Unanswered Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Try Teams for free Explore Teams Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Explore Teams Teams Q&A for work Connect and share knowledge within a single location that is structured and easy to search. Learn more about Teams What is the origin of the lake tank image that has become a meme? Ask Question Asked 4 years, 8 months ago Modified 3 years, 8 months ago Viewed 66k times This question shows research effort; it is useful and clear 43 Save this question. Show activity on this post. This image, the template for the \"Panzer of the Lake\" meme, depicts a soldier from an unknown nation looking at what appears to me to definitely be a Panzer. However, no-one seems to know the origins of the image. So, where and when was this image taken, and where was the image originally published? sources tanks photography Share Improve this question Follow Follow this question to receive notifications asked Mar 4, 2020 at 2:34 qazwsxqazwsx 90711 gold badge88 silver badges1313 bronze badges 7 1 The tank turret is definitely a Pzkfw-III. The soldier's uniform looks Soviet, but I'm no expert on that. – Jur Commented Mar 4, 2020 at 9:15 5 FWIW, the first apparition of this photo online referenced in tineye is in 2017 on this Russian website : feldgrau.info/2010-09-02-14-48-28/16731-podborka-foto-252 (almost at the bottom of the page). It doesn't seem to be sourced, but someone with a better command of Russian than me might find a lead there. – Evargalo Commented Mar 4, 2020 at 9:44 4 The turret is an early PzKpfw IV, definitely with a 7.5cm KwK 37 and I'd guess a model A to D, because from E onwards the cupola was modified and had an obvious \"Gepäckkasten\" on the rear of the turret that would be visible here, but then, it might just have been lost in whatever accident happened here. – nvoigt Commented Mar 4, 2020 at 17:27 2 @Evargalo Good find. Probably the only way to find out is to contact the site administrator. – Lars Bosteen Commented Mar 5, 2020 at 1:15 1 I think we're finally about to get an answer. ConeOfArc is the same person who found the tonk. – Schwern Commented Feb 26, 2021 at 3:23Show 2 more comments 5 Answers Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) This answer is useful 42 Save this answer. Show activity on this post. It's a Panzer IVD of the 31st Panzer Regiment assigned to the 5th Panzer Div. commanded by Lt. Heinz Zobel lost on May 13th, 1940. The \"lake\" is the Meuse River. The man is a German pioneer. All credit to finding the Panzer of the Lake goes to ConeOfArc for coordinating the search, and miller786 and their team for finding the Panzer. Full sources and details are in Panzer Of The Lake - Meuse River Theory The Panzer and the \"Lake\" The photo was taken about coordinates 50.29092467073664, 4.893099128823844 near modern Wallonia, Belgium on the Meuse River. The tank was not recovered until much later in 1941. The man is an unnamed German pioneer likely at the time of recovery. Comparison of an alternative original photo and the most recent image available of the location (July 2020, Google Street View) On May 12th, 1940 the 31st Panzer Regiment, assigned to the 5th Panzer Division, attempted to capture a bridge over the Meuse River at Yvoir. The bridge was demolished by 1st Lieutenant De Wispelaere of the Belgian Engineers. Werner Advance Detachment (under Oberst Paul Hermann Werner, commander, 31st Panzer Regiment), which belonged to the 5th Panzer Division, under Rommel’s command... Werner received a message from close support air reconnaissance in the afternoon that the bridge at Yvoir (seven kilometers north of Dinant) was still intact. He (Werner) immediately ordered Leutnant [Heinz] Zobel’s armored assault team of two armored scout cars and one Panzer platoon to head to the bridge at top speed... Belgian engineers under the command of 1st Lieutenant de Wispelaere had prepared the bridge for demolition while a platoon of Ardennes Light Infantry and elements of a French infantry battalion screened the bridge... Although the last soldiers had already passed the bridge, de Wispelaere delayed the demolition because civilian refugees were still approaching... two German armored scout cars charged toward the bridge while the following three Panzers opened fire. De Wispelaere immediately pushed the electrical ignition, but there was no explosion... Wispelaere now left his shelter and worked the manual ignition device. Trying to get back to his bunker, he was hit by a burst from a German machine gun and fell to the ground, mortally wounded. At the same time, the explosive charge went off. After the gigantic smoke cloud had drifted away, only the remnants of the pillars could be seen. A few kilometers south at Houx, the Germans used a portion of a pontoon bridge (Bruckengerat B) rated to carry 16 tons to ferry their 25 ton tanks across. By noon on May 13, Pioniere completed an eight-ton ferry and crossed twenty anti-tank guns to the west bank, however to maintain the tempo of his divisions advance, he needed armor and motorized units across the river. Rommel personally ordered the ferry converted to a heavier sixteen-ton variant to facilitate the crossing of the light Panzers and armored cars. Simultaneously, the Pioniere began construction on a bridge capable of crossing the division’s heavier Panzers and motorized units. Major Erich Schnee in “The German Pionier: Case Study of the Combat Engineer’s Employment During Sustained Ground Combat” On the evening of the 13th, Lt. Zobel's tank is crossing. Approaching the shore, the ferry lifts, the load shifts, and the tank falls into the river. The panzer IV of Lieutenant Zabel [sic] of the 31. Panzer Regiment of the 5. Panzer-Division, on May 13, 1940, in Houx, as good as underwater except for the vehicle commander’s cupola. Close to the west bank, at the pontoon crossing site and later site of 5. Panzer Division bridge, a 16 tonne ferry (Bruckengerat B) gave way to the approaching shoreline, likely due to the rotating movement of the panzer, which turned right when disembarking (the only possible direction to quickly leave the Meuse’s shore due to the wall created by the rail line). The tank would be fished out in 1941 during the reconstruction of the bridge. The Man Sometime later the photograph was taken of a German pioneer infantryman looking at the tank. Later the tank was recovered and its ultimate fate is unknown. Available evidence suggests the soldier in the photo is a Pioneer/Tank recovery crew, holding a Kar98k and wearing an EM/NCO'S Drill & Work uniform, more commonly known as “Drillich”. His role is proven by the presence of pontoon ferries on the Meuse river, used by the 5th Panzer Division. That is also proven by his uniform, which, as evidence suggests, was used during work to prevent damage to their standard woolen uniform. German pioneers prepare Panzer IIs for ferry crossing An early version of the Drillich My own speculation and research prior to the discovery. While I can't identify the photo, I can narrow down the tank. I believe it is a Panzer IV D. It has the short barrelled 7.5 cm KwK 37 narrowing it down to a Panzer IV Ausf. A through F1 or a Panzer III N. Both had very similar turrets, but the Panzer III N has a wider gun mantlet, a more angular shroud, and lacked (or covered) the distinctive angular view ports (I believe they're view ports) on either side of the turret face. Panzer III N in Italy 1944. source This leaves the Panzer IV. The distinctive cupola was added in model B. The external gun mantlet was added in model D. Panzer IV model C lacking the external gun mantlet. source. Panzer IV model D in France 1940 with the external gun mantlet and periscope. source Note the front half of the turret top is smooth. There is a protrusion to the front left of the cupola (I believe it's a periscope sight) and another circular opening to the front right. Finally, note the large ventilation hatch just in front of the cupola. Model E would eliminate the ventilation hatch and replace it with a fan. The periscope was replaced with a hatch for signal flags. Panzer IV model E lacking the periscope and ventilation hatch. source. Panzer IV model D entered mass production in October 1939 which means it would be too late for Poland, but could have seen service in France, Norway, or the Soviet Union. As for the soldier... The rifle has a turned down bolt handle, a bayonet lug (missing from late rifles), a distinctive disassembly disc on the side of the stock (also missing from late rifles), no front site hood (indicative of an early rifle), and you can just about make out extra detail in the nose cap (also early). This is likely an early Karabiner 98k which is missing its cleaning rod. See Forgotten Weapons: Evolution of the Karabiner 98k, From Prewar to Kriegsmodell. UPDATE ConeOfArc posted a video The Search for Panzer of the Lake. He broke down what he could identify about the solder, probably German. German winter style lower. German wool tunic. M34 Army Standard cap. Kar98 rifle missing the cleaning rod. For the tank he confirms it's a Panzer IV D using similar criteria I used and he found two additional photos of what appear to be the same tank claiming to be from the Western front in 1940. He then found a Russian source claiming it was found in Romania at the onset of Barbarossa in 1941. Unfortunately that's all for now. ConeOfArc has put a bounty of $100 US for definitive proof of the tank's location. More detail can be had on ConeOfArc's Discord. Share Improve this answer Follow Follow this answer to receive notifications edited Mar 19, 2021 at 23:12 answered Mar 5, 2020 at 2:12 SchwernSchwern 55.8k1010 gold badges174174 silver badges209209 bronze badges 3 If the gun was indeed a Karabiner 98k, that would hint that the soldier might be German ? – Evargalo Commented Mar 5, 2020 at 10:20 3 @Evargalo Could be German. Could be someone who obtained one like a soldier of a client state or partisan. – Schwern Commented Mar 5, 2020 at 19:05 Wow I never thought it would be found, great work everybody! – qazwsx Commented Mar 19, 2021 at 20:58 Add a commentThis answer is useful 9 Save this answer. Show activity on this post. Tank gun itself is short barrelled 7.5cm KwK 37, as mentioned in nvoigt comment. This means that the tank is either early PzKpfw IV (D to F1 versions employed in USSR) , or less likely PzKpfw III Ausf. N . However, PzKpfw III Ausf. N usually had armored skirts around turret and hull, plus camo pattern painted on. If we assume tank to be Pzkw IV, then it must be either from 1941 or early 1942, because after that long barrelled version of Pzkw IV appeared, and short barrelled versions were either withdrawn or destroyed. Solider on the picture is even more interesting. Rifle in his hand does not appear to be usual Soviet Mosin-Nagnat which has distinctive magazine protrusion near trigger guard. It looks more like German Kar98k, especially shoulder stock. On the other hand, uniform is rather peculiar, if we could call this attire uniform at all. Side hat could be either Soviet or German since both sides used them. However, Germans in the field usually had helmets, especially in 1941-42 period, so it is a bit more probable that this is Soviet solider rather then German. Uniform confirms this, it doesn't look German at all. Top part could be Soviet Telogreika, but it doesn't appear to be padded or quilted (I could be wrong on this ) and it appears more like summer then winter Soviet tunic. Pants look like a part of winter uniform, and both Soviets and Germans had similar. However, it should be noted that Germans in the winter of 1941/42 had problems with their supply trains, and often had to resort in using parts of Soviet uniform to keep themselves worm. Overall, as a probable (not definite) conclusion, I would guess this: Picture is taken in the spring of 1942. Panzer in question tried to go over ice, but the ice was weakened and broke, few days latter warm temperatures melted it completely. Solider in question could be German : he is using German service rifle, but still wears parts of Soviet military uniform acquired last winter in order to survive. His units has seen a lot, as a consequence military discipline is relaxed, soldiers do what they have to do and officers & NCO let them because only few of them remain in the field. Other option, he is a Soviet solider, maybe partisan, and this explains German rifle. In case of partisans, he is wearing various part of either Soviet or German uniforms that he could find. There is a possibility that he is a regular Soviet solider using trophy rifle - again in 1941/42 strict Soviet discipline was relaxed and officer often turned blind eye to violations if the man in question was good fighter. Share Improve this answer Follow Follow this answer to receive notifications answered Mar 5, 2020 at 3:14 rs.29rs.29 11.3k2121 silver badges4545 bronze badges 5 1 Rather than ice, perhaps a pontoon bridge that was hit and sunk. There's something else sticking out of the water behind the tank. – Spencer Commented Dec 1, 2020 at 0:31 Soldier's side cap doesn't look like a soviet Pilotka, which should be more rounded. – AlexD Commented Dec 3, 2020 at 14:05 @AlexD It is really unclear from the picture, and both soldier and the cap have seen a lot. Personally, I'm more inclined to German solider version, with some ragtag uniform, but there is also possibility of him being Soviet soldier or partisan. There is simply no proof to make definite conclusion. – rs.29 Commented Dec 4, 2020 at 8:45 Is it possible he's not a soldier at all but a citizen who just found the tank? Or do elements of his confusing clothing strongly suggest a uniform? – Luke Sawczak Commented Dec 27, 2020 at 14:41 A partisan seems most likely for that mashup. – Trish Commented Feb 27, 2021 at 8:56 Add a commentThis answer is useful 1 Save this answer. Show activity on this post. The rifle is almost certainly a K98 Mauser - one giveaway is the silver grommet on the stock halfway between the trigger and butt plate. A rod would slide through that grommet in a rack of K98's to lock them in the rack when not in use. Most K98's have that grommet, while that particular locking method isn't used much outside of the Mauser. It is possible, though not guaranteed, that the solder is from Finland, during the Continuation War, when Finland sought to reclaim territory it had lost in 1940 from the Soviet invasion. In the few photos of Finn soldiers during that conflict, they tend to wear caps instead of helmets. In the wiki article, a Finn officer is seen with a Soviet officer after an armistice was reached in 1944, and the cap in the photo looks a lot like the cap that Finn officer is wearing. Germany did supply some arms to Finland during this second conflict to take some of the pressure off of their own efforts, which would account for the Mauser rifle and even the Panzer 4... by 1942, Germany changed over to the longer barreled 75mm cannon to better combat the T34, and the few tanks they did send to Finland would likely have been the now obsolete short barreled 75mm equipped tanks. Share Improve this answer Follow Follow this answer to receive notifications answered Dec 27, 2020 at 6:21 tj1000tj1000 3,4311414 silver badges1616 bronze badges 2 I think you're right about it being a K98, but I've never heard of using the grommet to store the rifle, do you have a source? The silver grommet is a \"disassembly disk\" to help disassembling the bolt. The bolt is under spring pressure and it helps to have a hard surface to press against. The hole is there to accommodate a protruding firing pin. Here's a demonstration at about 2:30. As for the tanks, I believe the Finns only received 15 PzIV Js in 1944. Could be a Finn looking at a German tank in the Lapland War. – Schwern Commented Dec 27, 2020 at 9:52 The metal disk is to disassemble the bolt. In racks, a metal bar was used that pressed the rifle into the notch around the front wood. – Trish Commented Feb 27, 2021 at 9:02 Add a commentThis answer is useful -2 Save this answer. Show activity on this post. Want to improve this answer? Add details and include citations to explain why this answer is correct. Answers without enough detail may be edited or deleted. I just wish to add a few things to this discussion 1: The tank is definitely a short-barreled Panzer IV, meaning that the soldier is definitely German or a very very lost Soviet 2: The Germans wore white trousers and a white shirt during training or leisure. He maybe a soldier who, due to unfortunate circumstances, or material shortages is wearing a cobbled together uniform. Share Improve this answer Follow Follow this answer to receive notifications answered Nov 26, 2020 at 21:33 Wisemankugel MemicusWisemankugel Memicus 11 Add a commentThis answer is useful -3 Save this answer. Show activity on this post. The rifle looks to be a Finnish Sako m39, so this could be in Finland, as the Germans did send some tanks and such to the Finns when they fought the soviets. Share Improve this answer Follow Follow this answer to receive notifications answered Dec 26, 2020 at 18:46 Brandon RebarBrandon Rebar 1 2 1 We expect researched answers presented with evidence on this site, especially when contradicting other, extremely well evidenced, responses. – Pieter Geerkens Commented Dec 26, 2020 at 18:49 The stock is right, but it lacks the distinctive magazine in front of the trigger guard and straight bolt handle of the M39 / Mosin-Nagant. Good try. – Schwern Commented Dec 27, 2020 at 6:20 Add a commentHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity. Not the answer you're looking for? Browse other questions tagged sources tanks photography or ask your own question. Featured on Meta We’re (finally!) going to the cloud! More network sites to see advertising test Related 3 Missing link in the German tank development? 48 Is the photograph \"England’s Revenge in India\" real, staged, or fake? 21 When did Josef Wissarionowitsch become \"sexy\" Stalin? 7 What is the approximate date of this photo? 19 Is that armistice wagon photograph authentic? 29 When was this photo of Mission Dolores *actually* taken? 19 Is this a Sherman, and if so what model? 8 When was this picture of Ostend taken? 2 Who claimed Alexander had a hairy heart? Hot Network Questions Area denial organism for pre-industrial human beings on Earth Noun+なの, when の is the indefinite pronoun? 64-bit Linux and x86_64-v1 micro-architecture Is there any correspondence between physical objects' shape and activity occurring in a neural network? Movie with a girl watching a magical horse or unicorn Fibrations of categories with terminal objects What is the proper interpretation of the output 'HF' energy of a ground-state Gaussian 16 calculation using the opt keyword (B3LYP)? What's the translation of \"stand-up meeting\" in French? Counts repetitions within a list Is there any incentive for the 100 answerers to get the questions right? Why did Tunisian Jews not celebrate Purim for a long time? What this 1917 WW1 Shell Casing? Why are there no monadic operations in std::expected Follow up after PhD Position Interview Could Dukkha be interpeted as anxiety? Generate A Point Inside An Arbitrary Concave Polygon Why didn't Galileo take pictures as it descended into Jupiter's clouds, really? How would an ability that changes immunity to resistance work with damage absorption? Examples of countries that decided whether to change their voting rule? Multiple names in a point A place somewhere in Europe What was the first mobile game on a portable device? In a (math) PhD personal statement/statement of purpose, should I use mathematical notation, or english, if math is likely clearer? Can I in Coq define a recursive function with a notation such that the recursive calls can use it? more hot questions Question feed Subscribe to RSS Question feed To subscribe to this RSS feed, copy and paste this URL into your RSS reader. History Tour Help Chat Contact Feedback Company Stack Overflow Teams Advertising Talent About Press Legal Privacy Policy Terms of Service Cookie Settings Cookie Policy Stack Exchange Network Technology Culture & recreation Life & arts Science Professional Business API Data Blog Facebook Twitter LinkedIn Instagram Site design / logo © 2024 Stack Exchange Inc; user contributions licensed under CC BY-SA . rev 2024.11.20.18986",
    "commentLink": "https://news.ycombinator.com/item?id=42193771",
    "commentBody": "What is the origin of the lake tank image that has become a meme? (2021) (history.stackexchange.com)306 points by napolux 4 hours agohidepastfavorite58 comments mxfh 2 hours agoSince Know Your Meme doesn't give the reference for why it's a lake, maybe not everybody is familiar with british lore: The mythical Lady of the Lake: Probably best known via Monthy Python: Strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony. In short: She teaches Lancelot arts and writing, infusing him with wisdom and courage, and overseeing his training to become an unsurpassed warrior. https://en.wikipedia.org/wiki/Lady_of_the_Lake https://tvtropes.org/pmwiki/pmwiki.php/Main/EnigmaticEmpower... reply mrandish 1 hour agoparentThis reminds me that Monty Python and the Holy Grail contributed actual historical knowledge about Arthurian legends to my knowledge base while growing up. Other examples of Python unintentional education include knowing the names of a myriad of obscure cheeses (the cheese shop skit), a shocking number of anachronistic synonyms for death (the parrot skit) and notable contributions of the Roman Empire (Life of Brian 'What have the Romans ever done for us?' skit). While it didn't contribute to my GPA at the time, I'm sure I could name more notable philosophers than any other 8th grader in my school (philosopher's song skit). However, in high school it did spark the interest to look up and read about each of the philosophers in the song. reply initramfs 1 hour agoparentprevThere's also Father Thames, the River God of London https://www.nationaltrustcollections.org.uk/object/1140390 reply legutierr 4 hours agoprevThis feels like a ghost of the internet of the 1990s. This writeup deserves its own website, something with minimal CSS, where you'll discover a bunch of family snapshots and party photos if you click around. reply ndileas 2 hours agoparentThat's an aesthetic / scene preference (that I happen to agree with). The content is the most important part -- you can find this kind of curiosity and knowledge seeking all over the place. It'll probably even stay readable on stackexchange longer than the average handmade site from the 90s. reply verisimi 1 hour agoparentprev> where you'll discover a bunch of family snapshots and party photos if you click around. Yes, lovely. The sort of site where private moments might be kindly shared by an individual. To be distinguished from the forcible asset stripping and loss of ownership (theft, really) that form the terms and conditions of a large corporate's ToS today. reply flir 55 minutes agorootparentI still think wikipedia hit those \"this is my passion\" sites harder than social media did. What's the point of building a site about widgets, when 90% of people are just going to hit the Widget page on wikipedia? reply shahzaibmushtaq 3 hours agoprevI haven't seen the lake tank image being used as a meme anywhere, except now or maybe I have to explore the world of memes some more. Hats off to all who helped each other find this once lost story from history. reply napolux 3 hours agoparenthttps://knowyourmeme.com/memes/panzer-of-the-lake reply shahzaibmushtaq 1 hour agorootparentWent there for the first time and found out it's banned in my country. reply sss111 4 minutes agorootparentwhich country? reply Iulioh 3 hours agorootparentprevAnother point is the fact that i saw more \"anime version\" of this meme than the original foto reply napolux 2 hours agorootparenti'm aware of the \"senpai of the pool\" version, but probably i'm biased... I'm a huge WW2 nerd. reply rcarmo 23 minutes agoprevThe fact that this extraordinarily obscure question had such a thoroughly researched and intricately detailed answer almost restores my faith in Internet forums. reply oxguy3 3 hours agoprev> The photo was taken about coordinates 50.29092467073664, 4.893099128823844 near modern Wallonia, Belgium on the Meuse River. Great writeup, but I did have a little chuckle reading \"it was taken about near here\", followed by coordinates precise enough to identify a single atom. https://xkcd.com/2170/ reply vardump 3 hours agoparentGoing to be a different atom once you walk near. Or temperature changes, the wind blows, and so on. We’ll need to give each atom a unique ID. That would solve the problem. reply dylan604 2 hours agorootparentIPv8 is accepting RFCs reply wongarsu 1 hour agorootparentEarth has about 2^170 atoms. If we ignore the core and mantle, focusing on the crust, surface and atmosphere, we should be able to cram it into IPv6. Even if we add a couple planets and moons in the future. At least if we stop giving each person 18 quintillion IPs just because we once thought encoding MAC addresses in the lower 64 bits was a good idea. reply Aerroon 1 hour agorootparentprevThere are 10^80 atoms in the universe, therefore 266 bits are enough to give each a unique identifier. Due to how computers work maybe we can do two numbers: a 32-bit type or area code and a 256-bit counter. Or perhaps we just combine them into a single 272 or 288 or 320-bit number. reply stackghost 1 hour agorootparentTime for Intel to climb out of the pit by introducing x86_266 reply hermitcrab 3 hours agoprevGermans pioneers wore white uniforms? That sounds like the worst possible colour for digging ditches, recovering tanks or camouflage (if it isn't snowing). Why would they do that? Did Hugo Boss do the design? reply icegreentea2 1 hour agoparentFrom the link, the white pants are part of the \"Drillich\" work uniform. From searching around, these were intended as work uniforms / overalls. You were intended to wear these (there were both pants and jackets) over your actual uniform, and these would take the abuse. It seems like the early war patterns were simply undyed. Mid-war versions were apparently dyed darker. Here's a forum with a bunch of pictures of examples: https://www.militariacollectors.network/forums/topic/4042-th... reply hermitcrab 46 minutes agorootparentUndyed coveralls makes sense, thanks. reply Retric 2 hours agoparentprevEdit: ops, that joke wasn’t clear. A prisoner’s uniform needs to be cheap, distinctive, and easy to spot it doesn’t need to be clean. reply jabl 1 hour agorootparentIf the person were a prisoner he wouldn't be carrying a rifle.. reply Retric 36 minutes agorootparentThus the joke… reply motoboi 2 hours agoprevCannot wait for the day that question will be a ChatGPT prompt and the answer will be its response. A very different ChatGPT of course, but what a dream would that be. reply layer8 17 minutes agoparentI’m so scarred by LLMs that I wonder if I’d ever be able to trust it. reply triyambakam 1 hour agoparentprevHow would that be very different? I sent that prompt now and got a similar response, not as detailed, but the summary is correct and the same. reply Rebelgecko 1 hour agoparentprevAre they training on StackOverflow? reply gojiramothra 2 hours agoprev> It's a Panzer IVD of the 31st Panzer Regiment assigned to the 5th Panzer Div. commanded by Lt. Heinz Zobel lost on May 13th, 1940. The \"lake\" is the Meuse River. The man is a German pioneer. Interesting uniform reply arnaudsm 3 hours agoprevNerd sniping is my favorite kind of content on the internet https://xkcd.com/356/ reply kedarkhand 3 hours agoparentOk, now I need the answer to that question, what will the resistance... reply jackwilsdon 2 hours agorootparentAnswered on explain xkcd ((4/π − 1/2) ohms, or roughly 0.773 ohms): https://www.explainxkcd.com/wiki/index.php/356:_Nerd_Sniping reply brcmthrowaway 1 hour agoprevCan tanks work underwater? reply wongarsu 1 hour agoparentWith a snorkel attached. The engine needs oxygen and dislikes water. Both sides of the war invented that capability in the early 40s, though obviously not every tank had the capability. It's also a great example of the doctrines and tradeoffs of different armies. For example Russian tanks usually have space-efficient thin snorkels, while modern Western tanks have wide snorkels that double as a way for the crew to escape if they get stuck while driving submerged reply marssaxman 1 hour agoparentprevSome of them can: https://www.youtube.com/watch?v=R9itXfVSMj0 reply INTPenis 1 hour agoparentprevOnly if they get paid overtime. reply Deprogrammer9 31 minutes agoprevnew to me, kinda lame meme lol reply dist-epoch 4 hours agoprevModern remix: https://www.google.com/search?q=tank+in+river+ukraine reply lqet 4 hours agoprevWhy on earth doesn't the top answer have more upvotes. Impressive research, with full background, alternative pictures and an original picture of the panzer falling into the river. reply pbrowne011 4 hours agoparentWhile this didn't get much attention on History Stack Exchange, see ConeOfArc's YouTube video (which has 963k views as of today): https://www.youtube.com/watch?v=HaRO_dTqO1E See also ConeOfArc's video from a month later, https://www.youtube.com/watch?v=RO58B6LcTfM (1M views). The video above is about the initial search and problem, this video is after many Internet strangers worked together to solve it. reply bbqfog 3 hours agoprev [–] That's a meme? I've never seen that photo before in my life and I'm pretty aware of most memes. reply layer8 6 minutes agoparentMost of them? Are you sure? https://www.reddit.com/r/MemeEconomy/comments/egxfws/12880_m... reply CoopaTroopa 3 hours agoparentprevJust google 'tank of the lake, what is your wisdom' and you can catch up on a new meme genre reply SoftTalker 1 hour agorootparentI think a very small number of people today are aware of Arthurian legend. I think I have heard the phrase \"lady of the lake\" before but never really knew any context around it until I just now searched the term. I would have guessed it was the name of a ship or something. reply cedilla 3 hours agoparentprevI highly doubt that - most memes are short-lived, community specific or barely identifiable to outsiders. But you are, of course, unaware of memes you are not aware of. reply tailspin2019 3 hours agorootparentSpeak for yourself. I’m not aware of any memes that I am unaware of. reply ranger207 1 hour agoparentprevIt's most popular in military enthusiast circles, especially around the video games World of Tanks and War Thunder, which tend to be somewhat insular reply napolux 3 hours agoparentprevhttps://knowyourmeme.com/memes/panzer-of-the-lake reply bityard 1 hour agoparentprevThere are more memes than one person can know. reply mholt 3 hours agoparentprevThis begs the question, is it a meme if it is not seen? reply acheron 3 hours agorootparentSounds like a question you should ask the panzer of the lake. reply napolux 2 hours agorootparentwinniethepoohrecursion.gif reply jt2190 3 hours agoparentprevYeah and I seriously question what feels like “I couldn’t find anything about this in Google therefore nobody knows anything about this”. [1] I worked in a specialized reference library for a while and it was very eye-opening to see university students fail to find, say, 90% of our materials. [1] Quoting: > However, no-one seems to know the origins of the image reply swores 1 hour agorootparentDo you have a go-to bit of advice you give to students who you've spotted are lacking research (and just plain search) skills? (i.e. Something to kickstart them in the right direction, not just a way of saying \"learn how to search better!\") reply plagiarist 3 hours agoparentprev [–] I think people prefer the similar (derivative?) \"senpai of the pool\" for receiving wisdom from a non-native occupant of a body of water. reply actionfromafar 2 hours agorootparent [–] As long as you remember that supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The \"lake tank\" meme features a submerged Panzer IV tank in the Meuse River, Belgium, from WWII, and is popular among military enthusiasts and gamers.",
      "It draws humor from the concept of receiving wisdom from unexpected sources, akin to the \"Lady of the Lake\" from Arthurian legend and the \"senpai of the pool\" meme.",
      "The meme's niche appeal is partly due to its connection to historical and gaming contexts, resonating with specific online communities."
    ],
    "points": 306,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1732109403
  },
  {
    "id": 42191394,
    "title": "Yi Peng 3 crossed both cables C-Lion 1 and BSC at times matching when they broke",
    "originLink": "https://bsky.app/profile/auonsson.bsky.social/post/3lbc5va7f722p",
    "originBody": "@auonsson.bsky.social on Bluesky /** * Minimum styles required to render splash. * * ALL OTHER STYLES BELONG IN `src/style.css` * * THIS NEEDS TO BE DUPLICATED IN `bskyweb/templates/base.html` */ @font-face { font-family: 'InterVariable'; src: url(\"/static/media/InterVariable.c9f788f6e7ebaec75d7c.ttf\") format('truetype'); font-weight: 300 1000; font-style: normal; font-display: swap; } @font-face { font-family: 'InterVariableItalic'; src: url(\"/static/media/InterVariable-Italic.55d6a3f35e9b605ba6f4.ttf\") format('truetype'); font-weight: 300 1000; font-style: italic; font-display: swap; } html { background-color: white; scrollbar-gutter: stable both-edges; } @media (prefers-color-scheme: dark) { html { background-color: black; } } html, body { margin: 0px; padding: 0px; font-family: InterVariable, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Liberation Sans', Helvetica, Arial, sans-serif; text-rendering: optimizeLegibility; /* Platform-specific reset */ -webkit-overflow-scrolling: touch; -webkit-text-size-adjust: 100%; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; -ms-overflow-style: scrollbar; font-synthesis-weight: none; } html, body, #root { display: flex; flex: 1 0 auto; min-height: 100%; width: 100%; } #splash { position: fixed; width: 100px; left: 50%; top: 50%; transform: translateX(-50%) translateY(-50%) translateY(-50px); } /* We need this style to prevent web dropdowns from shifting the display when opening */ body { width: 100%; } JavaScript Required This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. Learn more about Bluesky at bsky.social and atproto.com.Post auonsson auonsson.bsky.social did:plc:d56aevbrhhknlxhjisuouofm Chinese-flagged cargo ship Yi Peng 3 crossed both submarine cables C-Lion 1 and BSC at times matching when they broke. She was shadowed by Danish navy for a while during night and is now in Danish Straits leaving Baltics. No signs of boarding. AIS-caveats apply. 2024-11-19T09:50:25.253Z",
    "commentLink": "https://news.ycombinator.com/item?id=42191394",
    "commentBody": "Yi Peng 3 crossed both cables C-Lion 1 and BSC at times matching when they broke (bsky.app)289 points by perihelions 12 hours agohidepastfavorite356 comments nabla9 10 hours agoOctober 2023 there was similar incident where Chinese cargo ship cut Balticonnector cable and EE-S1 cable. Chip named 'Newnew Polar Bear' under Chinese flag and Chinese company Hainan Xin Xin Yang Shipping Co, Ltd. (aka Torgmoll) with CEO named Yelena V. Maksimova, drags anchor in the seabed cutting cables. Chinese investigation claims storm was the reason, but there was no storm, just normal windy autumn weather. The ship just lowered one anchor and dragged it with engines running long time across the seabed until the anchor broke. These things happen sometimes, ship anchors sometimes damage cables, but not this often and without serious problems in the ship. Russians are attempting plausible deniability. reply cabirum 9 hours agoparentAfter the Nordstream pipeline attacked and destroyed, its reasonable to expect shortened lifetimes for undersea cables and sattelites. reply nradov 4 hours agorootparentYes, this is why having a prompt satellite launch capability to replace attrition losses is now a strategic imperative. We need to be able to put up new ones in a matter of hours, not months. reply PaulDavisThe1st 15 minutes agorootparent\"we\" are not doing anything AFAICT. Various privately owned corporations might be, and that's very different. Yes, I know the undersea cables are privately owned too. reply Gud 3 hours agorootparentprevIf someone starts blowing up satellites it’s pretty much game over for space based communications. https://en.m.wikipedia.org/wiki/Kessler_syndrome reply tialaramex 1 hour agorootparentKessler is often overplayed. Kessler trashes a low orbit and you wouldn't want to launch more birds into the trashed orbit. But, loads of com sats live in MEO or GEO, which is far too high for the numbers to work. They're all fine. You will even see Kessler cited as some sort of barrier to leaving, which is nonsense. Imagine there's a 1x1m spot where on average once per week, entirely at random and without warning a giant boulder falls from the sky and if you're there you will be crushed under the boulder. Clearly living on that spot is a terrible idea, you'd die. But merely running through it is basically fine, there's a tiny chance the boulder hits you by coincidentally arriving as you do, but we live with risks that big all the time. If you're an American commuter for example that's the sort of risk you shrug off. Likewise, Kessler isn't a barrier to leaving, humans won't be leaving because there's nowhere to go. The only habitable planet is this one, and we're already here. reply jgalt212 50 minutes agorootparentThe latency on GEO orbits exclude them from many use cases. reply nradov 3 hours agorootparentprevThe military is shifting toward LEO constellations for communications such as SpaceX Starshield. Kessler syndrome isn't a serious concern for those because the orbits decay fairly quickly anyway. reply yencabulator 1 hour agorootparentThat \"quickly\" is on the order of years (as opposed to decades, centuries, etc). If the Starlink constellation goes boom, you can't start launching new ones for several years -- and then the build-up would take years, from there. reply varispeed 53 minutes agorootparentprevCould they place a giant electromagnet in space to collect debris? reply kube-system 16 minutes agorootparentSpace is too big, and the field of even the world's strongest electromagnets are too small for this to be practical. And even if it did work, you'd only collect ferromagnetic material. reply littlecranky67 51 minutes agorootparentprevWhy is that? Undersea cables makes way more sense - the issue is we have maritime law that allows any nation state to freely roam over important cables. During wartimes this is a complete different story - ships won't be allowed near the lines, and if they do get close they will be destoryed without prior warning. No more anchoring \"accidents\". reply dylan604 3 hours agorootparentprevYou can have the ability to launch 100 satellites in 10 days, but that doesn't really help if you don't have 100 satellites reply nradov 3 hours agorootparentWell obviously you need to have a supply of replacements in stock. From a military perspective, think of satellites as rounds of ammunition that will be expended during a conflict. reply dylan604 1 hour agorootparentI think it'd be more apropos to compare them to fighter jets/tanks vs bullets reply trhway 8 hours agorootparentprevit sounds like you've probably never seen this - tanker Minerva Julie (belonging to Putin's friends) traveling through the Baltic Sea suddenly decided to hang around for a week right at the same place where couple weeks later Nord Stream exploded: https://i.dailymail.co.uk/1s/2023/03/16/23/68797949-11868975... reply Lichtso 8 hours agorootparenthttps://www.nytimes.com/2024/08/14/world/europe/nord-stream-... reply PittleyDunkin 3 hours agorootparenthttps://english.almayadeen.net/news/politics/us-navy-was-at-... reply mcphage 3 hours agorootparentI think it's pretty clear that the NordStream explosion was a joint US-Russia-Ukraine operation. reply JacobThreeThree 16 minutes agorootparentClearly NordStream was destroyed in a drunken escapade on a rented yacht. https://www.wsj.com/world/europe/nord-stream-pipeline-explos... reply mmooss 41 minutes agorootparentprevWhere is evidence that the US and Russia were involved? reply oneshtein 23 minutes agorootparent> it sounds like you've probably never seen this - tanker Minerva Julie (belonging to Putin's friends) traveling through the Baltic Sea suddenly decided to hang around for a week right at the same place where couple weeks later Nord Stream exploded: https://i.dailymail.co.uk/1s/2023/03/16/23/68797949-11868975... reply PittleyDunkin 2 hours agorootparentprevHell, throw sweden in there too: https://omni.se/marinen-pa-plats-dagarna-fore-explosionerna/... reply secondcoming 1 hour agorootparentNope, it was definitely Poland trying to maximise on their gas connection to Norway. https://en.m.wikipedia.org/wiki/Baltic_Pipe reply yett 10 hours agoparentprevYeah and this time they won't let them get away. According to Finnish Minister of Defence: \"The authorities in the Baltic Sea region have learned from the mistakes of the Baltic Connector investigation and are prepared, if necessary, to stop a ship in the Baltic Sea if it is suspected of being involved in damaging communications cables.\"[1] And it looks like according to marinetraffic.com that the Yi Peng 3 is indeed at full stop surrounded by at least 3 Danish navy vessels. 1. article in Finnish https://www.hs.fi/politiikka/art-2000010845324.html reply dingdingdang 8 hours agorootparentBoarded according to: https://x.com/visegrad24/status/1859132263746744367 reply lukan 1 hour agorootparentNot confirmed by any mainstream newspaper. The danish forces only confirm, that they are there, but nothing more. reply bananapub 7 hours agorootparentprevworth noting that twitter account is not the most trustworthy or independent. reply hersko 3 hours agorootparentWhat have they posted that was wrong? reply ceejayoz 3 hours agorootparenthttps://en.wikipedia.org/wiki/Visegr%C3%A1d_24#Content details a number of cases. reply spongebobstoes 10 hours agoparentprevWhat are some concrete reasons why someone would want to damage these cables? Who benefits? reply flohofwoe 10 hours agorootparentAssuming it was intentional, just trying the waters. Testing what the response is, who actually responds versus who's willing to sweep the incident under the carpet, how hard any response is and how quickly it happens, how much of the internet infrastructure is affected for how long, etc... etc... that's a lot of useful information as preparation for an actual attack. reply eric-hu 7 hours agorootparentThis is really interesting how you’ve explained it. In many professional fights the competitors start matches with light, quick jabs to probe their opponents defense. This feels just like that now that you put it this way. I never connected those dots though. reply diggan 7 hours agorootparentMaybe it's because I'm Swedish and we've experienced Russia's \"probing defenses\" tactic for a very long time (mainly \"breaking\" into Swedish airspace with airplanes, and discovering submarines at the Swedish shores), but I always thought this was common knowledge, always interesting to learn it isn't for everyone :) reply eric-hu 23 minutes agorootparentI lived in Taiwan for a while and China does this to Taiwan often. Flying planes into Taiwan’s air defense identification zone, sailing warships through the strait. It’s portrayed in (US, TW) media as war preparations, but some locals assume it’s all bark with no bite. How are those Russian actions portrayed in Swedish media? reply Gud 3 hours agorootparentprevNot just Russian. Even NATO aircraft were rejected frequently, though not anymore for obvious reasons. https://youtu.be/Z_EnkvE6LZA reply lifestyleguru 7 hours agorootparentprevThe situation escalated beyond probing, this is tit for tat response for Ukraine getting and launching US tactical missiles. Russia seems to be now aggressively monitoring and raiding the submarine pipes and cables. Blowing up of Nord Stream made Russia go ballistic. reply diggan 6 hours agorootparent> The situation escalated beyond probing Not sure we understand \"probing\" differently. Russian currently is at the edges, testing the responses from things like cutting cables and otherwise interfering with the infrastructure. This is what \"probing\" means for me. \"Beyond probing\" would be actually launching attacks one way or another, which we haven't seen yet (except of course, for the Ukraine invasion). reply onlypassingthru 1 hour agorootparent> actually launching attacks one way or another, which we haven't seen yet On the contrary. The attacks have been ongoing for years now. You're looking for the tanks and missiles when the attack is actually happening right under your feet. Rot and corruption are more powerful than any bullets or missiles. reply euroderf 1 hour agorootparentprevA next step for them might be to disable/poison something like an entire urban water distribution system. But come to think of it, the US et al. might be able to do the same back to Russia. Because, you see, there is a whole 'nother ladder of escalation to explore. A submarine cable is an attractive target for Russia because Russia doesn't have cables of their own exposed: Russia is a continental power, not a maritime alliance. A cable attack is an asymmetric attack, difficult to respond to appropriately. reply mongol 4 minutes agorootparentI recently saw a cable from St Petersburg to Kaliningrad at one of these maps. fsckboy 3 hours agorootparentprev>\"Beyond probing\" would be actually launching attacks one way or another, which we haven't seen yet he's saying \"this was not a probe, this was an actually launched attack\" reply drtgh 2 hours agorootparentprev> Blowing up of Nord Stream made Russia go ballistic Russia started invading Ukraine six months before Nord Stream blow up. Previously Russia invaded Crimea in 2014. The next invaded country, will be also an escalation? All of this is about a few psychopaths filling their pockets with the money that generates the corpses of their criminal business, some encouraging the production of war, others encouraging the waging of war. Why are these psychopaths and their \"business\" not prosecuted? reply Numerlor 2 hours agorootparentBecause their prosecution means going to war. I don't know about you but as someone living less than 30 minutes from Ukraine I don't want my country to go to war. reply wbl 1 hour agorootparentSi vis pacem, para bellum. reply dgfitz 59 minutes agorootparent> If you want peace, prepare for war reply mediaman 1 hour agorootparentprevWho are you referring to? Putin and Russian oligarchs? If so, how would you imagine the mechanics of prosecuting them to work? reply lifestyleguru 1 hour agorootparentGerman political and industrial elite with their former chancellor are within the reach of Western jurisdiction. They were smirking at Trump when he was exactly pointing out their dependency on Russian gas so.... who knows... reply llamaimperative 6 hours agorootparentprevNo, decades of rampant kleptocracy and alcoholism made Russia go ballistic reply WalterSear 1 hour agorootparentDecades? I suppose that 40 decades is still decades. https://www.youtube.com/watch?v=vK7l55ZOVIc reply snapcaster 3 hours agorootparentprevso i guess you've got russia all figured out. what's your excuse for the staggering amounts of violence and invasions from americans? reply llamaimperative 3 hours agorootparentA savior complex that's sometimes misled, sometimes absolutely warranted. reply viraptor 7 hours agorootparentprevThat's very similar to how the \"accidental\" flights over neighbouring territory works as far as I understand. This happens regularly between many countries. Just far enough to get some response, but not enough to get shot down immediately. reply diggan 6 hours agorootparent> This happens regularly between many countries. I cannot find any lists (either in English or Swedish) but I remember Russia has been accidentally breaking into Swedish airspace like once a year for as long as I can remember. Submarines also sometimes \"accidentally\" end up close to Swedish shores. It'd be interesting to see some total numbers, and compare other countries with how often it happens between Sweden/Russia. reply pantalaimon 6 hours agorootparentprev> but not enough to get shot down Doesn't always work https://en.wikipedia.org/wiki/2015_Russian_Sukhoi_Su-24_shoo... reply nabla9 5 hours agorootparentprevRussia wants to end NATO without going to war with NATO. NATO's political unity and ability to respond is tested with these attacks. Russia does them one after another gradually escalating. Russia maintains plausible deniability or does so small operations that they can always walk them back. Eventually, some country invokes Article 4 or 5 consultations. Russia hopes that US, Hungary, or Germany waters down NATO response. The conflict continues, but between individual countries not under NATO. NATOs as a organization may continue, but raison d'être is gone. reply dylan604 3 hours agorootparentRussia and these NATO countries being probed are like the two siblings in the back seat. Mom, he's touching me. Stop touching your brother. Mom, he's holding his finger right next to me. Dad eventually says, don't make me pull this car over and start a global thermonuclear war reply wbl 14 minutes agorootparentExcept its always Russia instigating. We never sent someone to look at the spire of Saint Basil (the pathetic excuse offered for explaining the presence of GRU officers in Salisbury carrying out chemical warfare), or really struck at their weak points. reply callc 35 minutes agorootparentprevHumorous yet concerning that our governments act like children. reply Salgat 32 minutes agorootparentprevThis is strange to me because this is basically forcing drills that better prepare their enemy. reply michaelt 13 minutes agorootparentSound the fire alarm over a birthday cake candle once, and you've got a drill making people get better at evacuating. Sound the fire alarm over a birthday cake candle several times a week, and people learn the alarm means there's no fire, no need to rush, they've got time to finish that e-mail and grab their coat. reply kube-system 12 minutes agorootparentprevIf you never go to war with your enemy, your enemy's continued preparations are wasted money and resources (both political and economic), aren't they? reply mmooss 19 minutes agorootparentprevLook up 'Grey Zone Conflict': Destroying another country's assets is generally an act of war, but obviously this incident falls short of causing a war. That is the 'grey zone', a prominent feature of current international relations and a major focus of the defense of the democratic world and international order, including in the US military. The international order is often called the 'US-led rules-based interntional order'. Russia, China, and some others dislike the first element, of course. The second element refers to the legal, rules-based structure (rather than power-based anarchy, which led to the centuries or millennia of war before the 'order' was created post-WWII). Aggressive international warfare is outlawed, for example; if France and Germany have a dispute, there is no question of violence - they use a legal structure to resolve it, which wasn't always true! Grey zone activities accomplish illegal things without reprocussions. And therefore they also serve the goal of undermining the international order by demonstrating its powerlessness in these situations. In some ways, it's like trolling. Russia uses grey zone tactics heavily - for example, they used them to capture Crimea (which was before the clear act of war, their 2022 invasion). They use them to run destabilizing 'grey zone' campaigns throughout the world, including directly interfering in elections. The tactics suit Russia in particular because they cannot compete miltarily with the democratic world. China uses them too, for example using their 'coast guard' and 'civilian' 'fishing boats' to attack (up to a point) and intimidate ships from other countries in the South China Sea. If China used their navy, it would possibly be acts of war. A Chinese coast guard ship shooting water cannon at a fishing boat, though illegal in international waters, isn't going to start a war. 'Civilian' 'fishing' boats from China blockading access to a reef won't either. Edit: Before you look at Russia and China and other Grey Zone actors as miscreants, understand that it's just the normal behavior of 'revisionist' powers - those who want to change the current rules. The current rules serve the interests of the 'status quo' powers, who get all self-righteous about 'illegal' activities. In a more common situation on HN, think of IP outsiders, who break the 'rules' made by major IP holders, such as DMCA or those extending copyright for decades or restricting access to scientific knowledge - the IP holders want the status quo and call violations 'theft' and the outsiders 'criminals', etc. If the US wasn't a status quo power, they'd be doing grey zone things. (That doesn't at all justify Russia and China's goals of stealing land, oppressing people's freedoms, and solving problems through violence.) reply krisbolton 8 hours agorootparentprevWhile not directly addressing undersea cable sabotage this is a comprehensive open access article with case studies on 'hybrid warfare' which provides context to these types of actions. 'Shadows of power beneath the threshold: where covert action, organized crime and irregular warfare converge' - https://www.tandfonline.com/doi/full/10.1080/02684527.2024.2... reply benterix 7 hours agorootparentprevThe ship was sailing from Russia and the captain is Russian. Using a Chinese ship is a good trick from Putin. As for the core of your question: there is no benefit, it's just his mentality. \"The West\" supports Ukraine so let's just do some harm, retaliate in some way. Burn some buildings here and there, plants some inflammable materials on airplanes etc. Pointless for you and me, meaningful for that guy. reply viraptor 6 hours agorootparentDoes \"Chinese ship\" really mean anything here? As far as I understand the ship official registration is a very vague concept https://en.wikipedia.org/wiki/Flag_of_convenience reply emmelaich 6 hours agorootparentand according this tweet https://x.com/erikkannike/status/1858883945607094541/history \"So - according to Russian federal port records, the Chinese ship suspected of cutting the communications cables in the Baltic Sea was captained by a Russian citizen (one Stechentsev A.E.). Interestingly Yui Peng 3 was only transferred to its current owner in China earlier this month. The ship is carrying goods/oil from Ust-Luga in Russia, to Port Said in Egypt. Same captain also comandeered URSUS ARCTOS also carrying goods from Ust-Luga to Egypt. Mapped using @SensusQ . \" reply bluGill 3 hours agorootparentprevHard to say. They will claim this is only Flag of convenience as they are caught. However China still has the opportunity to say that this is something for their law enforcement to take care of not international, and then give the captain \"a slap on the wrist\". What we don't know is if China knew they were going to try this beforehand or not. Flag of Convenience is common enough that we can't be sure. This could have been planned on the high level from China and we would never know - something conspiracy theorists will run with! If China knew they would probably give the crew a sever punishment, but unofficially it is for getting caught and not doing the act. Most likely though China didn't know before hand. reply threeseed 10 hours agorootparentprevWhen Trump becomes President next year he is expected to demand that Ukraine settle the war with Russia or risk losing US aid and military support. It is why Russia is throwing everything at re-taking Kursk and US is now allowing long range strikes. If the EU decides to join the US the war is over and Russia will keep the occupied lands. If the EU decides to support Ukraine then because of the devastating sanctions there is a strong chance Russia loses. So it's in Russia's interest to make life as difficult as possible for Europe over the coming months in order to convince them that ending the war is in their best interest. reply diggan 7 hours agorootparent> If the EU decides to join the US the war is over and Russia will keep the occupied lands. As a European, I'd say there is just about 0 chance of the EU unilaterally supporting Russian taken any occupied areas to themselves and Ukraine surrendering. Not only would it signal to Russia that they can take European land without consequences, but public opinion is very much against any sort of cessation of defenses. In my ~30 years I've never seen as strong NATO support from the common man in countries like Sweden and Spain as there is today. reply thaklea 5 hours agorootparentPublic opinion is against further weapons shipments to Ukraine: https://de.statista.com/statistik/daten/studie/1454716/umfra... 51% are against further shipments, 38% are in favor of further shipments, 11% are undecided. In Ukraine, 52% want to negotiate as soon as possible. Interestingly, also 38% want to continue: https://news.gallup.com/poll/653495/half-ukrainians-quick-ne... reply honzabe 3 hours agorootparent> Public opinion is against further weapons shipments to Ukraine The linked article is about the opinion of Germans about shipments of German weapons. When you don't specify that in the context of this thread, which is about Europe, not Germany, people might mistakenly interpret that as data about Europe. reply diggan 4 hours agorootparentprevI know there are some countries where support is less than in other places (Germany being one, as you highlighted). I still stand by my original statement that unilateral decision in EU of stop supporting Ukraine and letting Russia keep the occupied territories. reply rksbank 3 hours agorootparentSupport in Germany is the same as support in Ukraine itself (according to the above Gallup poll at least). Support in other European countries does not differ much from Germany: https://www.theguardian.com/world/2024/feb/21/barely-10-per-... reply diggan 3 hours agorootparent> Support in other European countries does not differ much from Germany: Yes, it seems to differ in at least one place by a lot: https://news.ycombinator.com/item?id=42193290 I'm sure that's true for other places too. reply bananapub 7 hours agorootparentprev> As a European, I'd say there is just about 0 chance of the EU unilaterally supporting Russian taken any occupied areas to themselves I agree, but it's not about accepting or saying it's a good idea, it's about whether European countries can replace the US support enough that Ukraine can reasonably keep defending themselves. reply diggan 7 hours agorootparentI don't know if EU would be able to match the current support the US gives to Ukraine (maybe it already does? Or maybe it exceeds? I don't know either way) but what I'm sure off is that Europe won't stop trying even if it wouldn't be enough. reply adriand 6 hours agorootparentIf you add up all the aid from the US and compare it to aid from the EU plus European nations, I think the share of contributions is roughly equal. But if that’s right (and I did the math in my head while scrolling a huge spreadsheet on my phone), then the loss of support from the US is significant. The US ability to produce armaments is also unparalleled in the West, so a loss of that supply is also a huge issue. Then you have the loss of the US as a military backer which may free Putin to be more aggressive - dirty bombs, tactical nukes, blowing up a nuclear reactor, assassinating Ukrainian leadership, who knows what. It’s a huge problem for Ukraine if they lose the US. But will they? It’s hard to know for certain. reply bluGill 3 hours agorootparentEurope is great at producing armaments as well - but there are a lot of useful armaments that are only produced in the US. If you had to choose either EU or US support, the US is the better option as they can give you things that the EU cannot even though the EU has more people than the US and a good economy. The Patriot system is one the of best examples. EU doesn't really have anything in this space, but Ukraine needs more of it yesterday. reply diggan 3 hours agorootparent> The Patriot system is one the of best examples. EU doesn't really have anything in this space, but Ukraine needs more of it yesterday. Are you talking about SAM capabilities or something else? Because there are plenty of SAMs produced by European countries; https://en.wikipedia.org/wiki/List_of_surface-to-air_missile... reply bluGill 1 hour agorootparentThe full setup for missile defense. This includes radar, computers and so on. reply diggan 6 hours agorootparentprevThanks a lot for doing that, even thought kind of ad-hoc :) Some data for guesses is better than none! I'm guessing that if US pulls their support, EU will try to add as much to cover up for it as humanly possible, as most compatriots see Ukraine as the frontline of something that can grow much, much bigger which because of remembering history, we'd obviously like to avoid. reply sabbaticaldev 6 hours agorootparentprevhow sure are you? I think the economic struggles + losing US support would make every incumbent leader lose their jobs until UE is full of Trump supporters reply diggan 6 hours agorootparentFairly confident, at least for the countries I frequent and have friends in. As an example, public opinion of NATO in Sweden was really negative up until ~2013 (Crimea occupation) where it kind of was equally positive/negative and then fast forward to today where it's at 64% positive. https://www.gu.se/en/news/opinion-on-nato-record-shift-betwe... Being a Swede myself, and knowing how apathetic Swedish people are about basically anything, something having that large of support is pretty uncommon and signal a strong will to make NATO and EU defenses stronger, if anything. Even people I know who been historically anti-\"anything military\" in the country have quickly turned into \"We need to defend our Nordic brothers and sisters against the Russians\" which kind of took me by surprise. > UE is full of Trump supporters That won't ever happen. Even right-wingers (Europe right, not US right) are laughing at Trump and the Republicans. reply ssijak 7 hours agorootparentprev\"If the EU decides to support Ukraine then because of the devastating sanctions there is a strong chance Russia loses.\" How did that not work then yet? reply justin66 7 hours agorootparentThey question you're really asking is \"why is the war taking so long?\" Because it's a war. reply misja111 11 minutes agorootparentI think he is asking how well the devastating sanctions have been working so far. Which is a retorical question of course, because obviously they haven't harmed Russia all that much. Actually, they are hurting the EU as well because of the risen energy prices. reply sabbaticaldev 6 hours agorootparentprevlook, if someone looks like they are losing a war in the beginning, middle and the end act of it, I wouldn’t have much faith that extending it is the best solution to finally win. reply llamaimperative 6 hours agorootparentTautological The Nazis were mopping the floor with Europe until they weren’t. The Japanese were conquering Asia until they weren’t. reply lukan 6 hours agorootparentBut obligatory reminder, that back then there were no nukes. So it is not exactly the same situation. reply llamaimperative 4 hours agorootparentEh, MAD brings us back to equilibrium. It's a significantly more dangerous equilibrium, for sure, but we should be much more afraid of a nuclear accident (not reactor meltdowns but accidental weapon launch) than of purposeful use of a nuclear weapon. reply lukan 2 hours agorootparentWell, the result is the same, no? If one rocket flies, chances are, they will all fly. reply actionfromafar 5 hours agorootparentprevNeither is now the situation exactly that having nukes, means you can tell everyone to back down and do exactly as you say or else. reply meiraleal 5 hours agorootparentprevThe nazis won many wars even tho they lost the big one. Will NATO win against Russia? Who knows. But in the showdown NATO/Ukraine vs Russia, they lost. reply llamaimperative 4 hours agorootparent“NATO/Ukraine”? I am literally giggling at the absurdity :D Get a grip. Russia is getting bombed every day and doesn’t even hold all of its initial territory. It is not clear who will win this. It is extremely obvious that Russia would be crushed within days by a confrontation with NATO (but this conflict almost certainly wouldn't materialize due to nuclear weapons). reply justin66 2 hours agorootparent> It is extremely obvious that Russia would be crushed within days by a confrontation with NATO (but this conflict almost certainly wouldn't materialize due to nuclear weapons). It's interesting the extent to which people haven't internalized this. Russia's industry has really ramped up on military production in the past two years, and their military will eventually get to the point where it can cause tremendous damage against a poorly-equipped Ukraine, through attrition. But the invasion revealed how far behind they are technologically, and a combined NATO force would turn off their entire military's command and control on day one of a real conflict. It's an inversion of the situation forty or fifty years ago, when Europe had to rely on the the nuclear threat because the Russian conventional forces were considered to be overwhelming. reply chinathrow 8 hours agorootparentprevIt would be so nice to not be dragged into this war by the aggressor. Russia is playing a very stupid game here. reply mschuster91 8 hours agorootparent> Russia is playing a very stupid game here. They are not, if you take the larger context into account - and that is China and their saber rattling not just against Taiwan but also against everyone else in what China thinks is \"their\" influence sphere such as the Philippines. Russia's warmongering (not just in Ukraine, but also via Syria, Iran and Yemen!) is breaking apart both the US and EU internally - recent elections have shown that both populations are pretty much fed up with the wars and their consequences, and once enough countries either fall to Putin's 5th column outright or their governments pull a Chamberlain, China can be relatively certain no one will intervene too much when they decide that now is the best time to annex other countries. reply justin66 6 hours agorootparentI wonder if anyone thinks this seems likely: American Secretary of Defense: \"Mr. President, the Chinese just destroyed our Naval base in the Philippines, killing hundreds of US servicemen. As part of a plan to annex the country or something.\" American president: \"Let's not intervene too much.\" reply mschuster91 6 hours agorootparentI don't think the Chinese will attack US infrastructure or vessels directly, they are not that stupid - but they did attack Philippine ships in what is widely recognized Philippine territory [1] or fish illegally in Philippine territory [2]. The only response the entire West was able to give in years of Chinese transgressions were strong words, about as effective as \"thoughts and prayers\". China is a bully that escalates continuously (similar to Russia's behavior in Syria with the countless \"red lines\" that were crossed, eventually including chemical weapons) and needs to be brought to its knees before they one day trigger WW3 by accident. [1] https://www.reuters.com/world/asia-pacific/chinese-coast-gua... [2] https://maritime-executive.com/article/philippine-official-a... reply bdndndndbve 3 hours agorootparentprevPutin and Xi's big advantage over the US is that American presidents get elected every 4 years. If they gradually encroach on their neighbors and make intervention unpopular in the US via propaganda they don't need to attack a US base. reply mindslight 30 minutes agorootparentThe other big issue is US adventurism in Iraq (and to a lesser extent Afghanistan) has made US citizens wary of any international actions, no matter the details. It's especially galling how many of the same people who were cheering on the direct military conquering of Iraq are now against supporting Ukraine at an arms length. \"Can't get fooled again\", indeed. reply throwawaymaths 7 hours agorootparentprevWell the result of China's 5d chess has been to install a leader in the US that is likely to escalate a trade war with china when with an impending demographic crisis they most need someone to stop the trade war. Sheer genius! reply mschuster91 7 hours agorootparentThe problem with dictators of all kinds is that their personal concerns (say, appearing before the local populace as \"the one who re-unified China\") can and will trump over what makes sense for the country long-term. Of course that can and does also happen in democracies, but at least most reasonable democracies have some sort of \"checks and balances\" that at least prevents open war from breaking out. reply llamaimperative 6 hours agorootparentprevThe world will be looking to China as a stable partner while the US voluntarily dismantles its economy and very possibly its political system. So yeah, the US absolutely got outplayed here. reply throwawaymaths 4 hours agorootparentThe us is currently one of the most stable economies, so there's a long way to go. I think it's unlikely that the world will pick an economic partner that: - builds 90% of the new coal fired plants while the rest of the world (including the US) is decarbonizing - has 280+% debt to GDP ratio - has capital controls on its currency (the real exchange rate could change suddenly at the drop of a hat) reply llamaimperative 3 hours agorootparentWell... that stuff will be easier to overlook when the US deploys its military to deport millions of people operating the most foundational portions of its economy like agriculture and construction. reply throwawaymaths 3 hours agorootparentOK this is some sort of \"America bad\" fever dream. Listen America isn't perfect or anything, but you're basically looking down the barrel of crazy if you ignore the steel advantages that the US has, and the history and pattern of US recovery from crises reply llamaimperative 2 hours agorootparentIt isn't \"America bad\" at all! I believe America is the greatest country in the world, its economy is clearly second to none, and it's clearly the best trading partner for the vast majorities of nations. I also believe America will almost certainly recover from whatever dark period it's (probably) about to endure. But I'm also well aware of the fact the US has gone through extremely dark periods and its past success is not a promise of future success. At the end of the day a country very possibly plunged into Great Depression II and almost certainly with trade policy changing by the day is not a good trading partner. There is a very real possibility that we deport our way into a famine. The US economy cannot possibly sustain the type of deportations that have been promised and are already being put into motion by the incoming administration. reply mschuster91 2 hours agorootparentprev> and the history and pattern of US recovery from crises Well at least in prior crises, the US had sensible leadership on both sides that was willing to put country before party. The 47th however? Not just the man himself but especially the cabinet picks are an utter joke. None of the currently known picks are known for any kind of competence or even experience in their respective fields, and there are ideas floating to have the Senate go into recess so the 47th can appoint them without the usual review process - astonishing in itself given that the Republicans control the full Congress, they shouldn't have to fear any of their candidates not getting past the Senate. What politics they want to follow is just as dangerous - Musk and DOGE slashing 2 trillion $ from government expenditure for example, large parts of the government will literally be unable to do their job (which is, among others, to handle crises). reply tzs 1 hour agorootparentprevChina is building new coal plants but the their utilization rate is going down and is expected to continue to go down because of all the solar, hydro, and nuclear plants they are building. As far as stability goes, the comment above you talked about a stable trading partner, not a stable economy. China is probably more stable as a trading partner than the US is. The US changes trade policy too often. reply chinathrow 7 hours agorootparentprevSure, but I am commenting from a non-military, non-geopolitics, non-strategy related background: It's a stupid game. Stupid in the sense of: I don't like it, I don't want to play it, thus it's stupid. reply jacknews 7 hours agorootparentprevPlausible. But alternatively, it is the outgoing Biden administration that do not want a freeze, and are escalating their involvement in the war, by giving permission to use their long-range missiles to attack inside Russia, in order to derail any potential 'agreement'. And they are now sewing the press with 'hybrid war' mania. I see news sites are now plastered with fearmongering stories about embassies being closed in Kyiv, that Ukraine front might collapse without aid, and so on and on. Note that none of it is actual Russian attacks or any actual events, just fear of them. It looks very much like a media campaign to me. edit: oh dear, a few people on HN really do not like this take, without offering any take-down, which just makes me think there's probably something to it. reply ethbr1 7 hours agorootparentRussia has been striking civilian targets throughout Ukraine with ballistic missiles since the beginning of the war. How is allowing Ukraine to use ATACMS on military targets in Russia an escalation? reply jacknews 7 hours agorootparentThat's beside the point. It is a very clear escalation in US/European involvement. Ukraine were prohibited from using long-range western weapons to attack targets inside Russia up until now. I'm not saying if it's right or wrong. But it's a very clear escalation in western 'participation'. Russia have for a long time been saying that such action would be tantamount to a NATO attack, and so everyone involved surely understands that this is an escalation in the NATO-Russia face-off. reply ceejayoz 7 hours agorootparent> Russia have for a long time been saying that such action would be tantamount to a NATO attack They say this every time. When Obama sent non-lethal aid, they used the same line. reply jacknews 6 hours agorootparentnone-the-less, it is a clear escalation ON THE INVOLVEMENT OF EUROPE AND THE US in the war. It is not that Ukraine are escalating the war by using long-range missiles. Of course Russia have been using them all along. But it is a clear escalation in western 'participation' in the war. reply soco 6 hours agorootparentSo \"finally replying to constant attacks\" gets redefined by putin as escalation, no surprise here. Or is there any other argument I'm missing? reply mapt 6 hours agorootparentprevThat is a very particular use of the term 'escalation' which is bound to mislead people. Normally, if we show up at the flagpole at noon to confront each other, and you throw a punch, you have escalated things to a fistfight, and then my return punch is not an escalation. If I pull a knife, I have escalated things to a knife fight. We escalate from fist to knife to gun. Reciprocation - self defense - does not count. The only way to torture the term into contextual use is to suggest that Russia is not firing rockets at NATO because Ukraine is not NATO, but NATO is firing rockets at Russia because all these missile systems are not Ukrainian, but NATO. This is Putin's framing, and it incorporates the idea that the missile systems are actually being manned but US & EU soldiers. If you are not adopting that frame, \"escalation\" only really works if you explicitly define the context as a Great Powers proxy war with a potential nuclear endpoint, where Ukraine is stipulated for the sake of argument to have no agency. reply honzabe 2 hours agorootparent> That is a very particular use of the term 'escalation' which is bound to mislead people. I am not the OP, but I think your interpretation is not as obvious as you make it to be. This often leads to misunderstandings. AFAIK military analysts use the term escalation as a morally neutral term. Escalation is anything that goes up on the 'scala' (= \"ladder\", the Latin root of the word). In this interpretation, D-Day would be an e_scala_tion (climbing up the ladder) simply because opening a new front means number_of_fronts_today > number_of_fronts_yesterday. In this interpretation, self-defense and escalation are not mutually exclusive. Apparently, the term changed meaning. Many people now treat it the way you do (if I understand you correctly) as something associated with aggression. Therefore, they assume that when someone labels something like an escalation, they mean it is an act of aggression, unjustified, something you should not be allowed to do, and not morally neutral. I am not saying you are wrong. I am just pointing out that when people talk about escalation, it is worth checking whether they mean the same escalation. reply sabbaticaldev 6 hours agorootparentprevRight. URSS putting nuclear missiles in Cuba was not an escalation then. reply throwaway2037 5 hours agorootparentI only learned about this a few years ago. Before the Cuban Missile Crisis (where Russia installed nuclear missiles in Cuba), the US installed nukes in Italy and Turkey. This made USSR very upset. Plus, the US was heavily meddling in Cuban domestic affairs. The first two paragraphs are very instructive here: https://en.wikipedia.org/wiki/Cuban_Missile_Crisis My point: I think USSR (and Cuba) had a good reason to install those missiles. It wasn't an unprovoked action. reply tmnvix 19 minutes agorootparentAnd as I understand it, part of the solution to the Cuban Missile Crisis involved the US quietly agreeing to abandon the placement of nukes in Turkey. There is some analogy here for the Ukraine NATO situation. jacknews 4 hours agorootparentprevUkraine is very clearly a proxy war between NATO and Russia, merely framed as a plucky country defending it's sovereignty, though it is that too, of course. With all the backlash here, I feel like some kind of radical, but here is a BBC article from 2 DAYS AGO that basically says what I'm saying: https://www.bbc.com/news/articles/cx2nrlq1840o Although they miss out the bit about a media campaign, and so on, of course. This is the BBC, pretty much the mouthpiece of the UK government. And although they frame recent actions as trying to give Ukraine an advantage in any Trump negotiations with Russia, the truth is that these missiles will probably not advance Ukraine's military position, but will certainly change Europe and America's standing, possibly to the point of derailing any possibility of negotiation. reply ethbr1 2 hours agorootparent> though [Ukraine] is [a plucky country defending it's sovereignty] too, of course No \"too\" It is only that. If Russia retreated behind its internationally recognized borders and returned Crimea today, Ukraine would stop attacking it today. That tells you everything you need to know about who the aggressor and escalator is in this conflict. Anything else is a Russian talking point in service to their trying to lose fewer troops while invading a neighboring country. reply jacknews 1 hour agorootparentnext [3 more] [flagged] ethbr1 1 hour agorootparent> yeah, fook off, you have nothing to say. Oh, sorry, I was under the impression you wanted a discussion. > edit: oh dear, a few people on HN really do not like this take, without offering any take-down If you just wanted to complain, but not have anyone challenge your opinions, you should have phrased the above differently. reply jacknews 1 hour agorootparentIndeed, you are right, and I apologize. I took your comment to be a dismissive 'Russia should just retreat' directive. Ain't gonna happen. And The problem is, Ukraine really is not just a simple country that got invaded. It really matters, for the whole world, if we let Russia get away with aggression. It matters if we push too hard and in the chaos Russia unleashes nuclear weapons. It matters how the west conducts supposed peace-keeping operations, etc. It's reallt is not just about Ukraine, and the very fact that you (probably not Ukrainian or Russian) are commenting is evidence. reply ceejayoz 3 hours agorootparentprevYour link backs up what people here are trying to get across to you: > Russia has set out “red lines” before. Some, including providing modern battle tanks and fighter jets to Ukraine, have since been crossed without triggering a direct war between Russia and Nato. This is the latest of a long list of small, slow, racheting-up responses to unilateral Russian aggression. https://en.wikipedia.org/wiki/Red_lines_in_the_Russo-Ukraini... reply jacknews 3 hours agorootparentno. And no-one has been 'getting anything across to me', inferring that I'm 'not getting it'. They've been throwing incomplete or irrational arguments, like yours, or simply downvoting. Sure there have been 'red lines' by Russia, and the US has continuously pushed across them. But this one was also a US 'red line'. Consistent with keeping a proxy-war in-theater. Why have they crossed it, now? What do they hope it will achieve? Most likely very little militarily. But maybe quite a lot in shaping or constraining future US policy. reply ceejayoz 2 hours agorootparent> But this one was also a US 'red line'. Consistent with keeping a proxy-war in-theater. Why have they crossed it, now? For the same reason they crossed all the others - continued Russian aggression. Each expansion of US aid or reduction in restrictions on how that aid is utilized has followed logically from Russian actions. Obama started with non-lethal aid; we've initially balked at every single step since that before eventually going \"ok, now it's warranted\". It's very clear the US is keeping responses small and incremental to take the wind out of Russian bluster about nuclear holocaust if they do this one more little thing to piss Putin off. It's also very clear the Russian \"no don't send Javelins/HIMARS/Patriots/Abrams/MiGs/F-16s/ATACMS, we'll be very mad\" has lost a lot of its potency. reply jacknews 2 hours agorootparentSo what, would you say, triggered the US to cross their own red line, and a rather obvious principle of proxy warfare? And, backtracking, how aware have you been about the situation in Ukraine, or baltic sea infrastructure, in the past few months (even year), compared to the last week? Just a marginal increment, no doubt. reply ceejayoz 2 hours agorootparent> So what, would you say, triggered the US to cross their own red line... I'd first reject the use of the term \"red line\" entirely for the ATACMS situation. \"No, not ever\" is a red line. The Russians love issuing these for other people, but it's embarassing when they're crossed without significant consequence. \"No, not now\" is not a red line. The US tends to shy away from issuing them - one of Obama's biggest mistakes was proclaiming one in Syria and then looking a bit feckless when they violated it. (https://www.pbs.org/wgbh/frontline/article/the-president-bli...) Letting Ukraine hit Russian territory with ATACMS is like the fourth or fifth expansion of how they're permitted to use that weapons system so far, as was giving them ATACMS in the first place after HIMARS (which saw a similar set of gradually reduced limitations; https://www.defensenews.com/land/2022/07/08/us-to-send-more-...). > And, backtracking, how aware have you been about the situation in Ukraine, or baltic sea infrastructure, in the past few months (even year), compared to the last week? Just a marginal increment, no doubt. I've closely followed the situation in Ukraine since Euromaidan. reply aguaviva 1 hour agorootparentprevHere is a BBC article from 2 DAYS AGO that basically says what I'm saying Which says nothing at all about the conflict being \"a proxy war\". reply jacknews 55 minutes agorootparentnitpick. It exactly states that Biden might be stirring things up in anticipation of Trump sueing for a freeze. reply aguaviva 43 minutes agorootparentWhich still says nothing about the conflict being fundamentally a proxy war. reply Symbiote 7 hours agorootparentprevThe USA, UK and France approving the use of the long-range missiles was described as a response to Russia using North Korean soldiers. reply no_exit 1 hour agorootparentNorth Korean soldiers that mysteriously have yet to materialize in a fashion that isn't blatant propaganda. reply jacknews 4 hours agorootparentprevA fair point, but described by who? And was this just a post-hoc justification, or had the western powers declared that they would retaliate if Russia involved other armies? In any case, surely the 'punishment' should be directed at North Korea? reply ethbr1 2 hours agorootparentWhy should it be directed at North Korea? North Korean troops are helping Russia invade Ukraine (by freeing up Russian garrison troops to participate in their offensive). Ergo, redress is something that helps Ukraine resist the military advantage North Korean involvement gives Russia -- e.g. being able to target Russian military targets supporting the invasion, in Russia. reply dragonwriter 2 hours agorootparentprev> In any case, surely the 'punishment' should be directed at North Korea? The problem is at least as much Russia inviting NK as North Korea positively responding, aiding Ukraine works against all the belligerents aligned against it, NK as well as Russia, and the North Koreans in Russia are not protected by the Armistice the way North Koreans on the Korean peninsula are. reply preisschild 7 hours agorootparentprev> are escalating the war (they started, with the long-range missiles), Wrong. Using long range missiles is not an escalation. Russia has been using them against Ukrainian lands for years now. Why shouldn't Ukraine be allowed to use them against Russian land? reply jacknews 6 hours agorootparentNo, you are wrong. Russia are at war with Ukraine, so they are bombing them. Ukraine have every right to reply with their own long range weapons too, and that would indeed not be an escalation in the fighting itself. But, the west clearly prohibited the use of their donated long range weapons in direct attacks on Russia, in order to limit their liability, responsibility, 'participation' or whatever, until now. Russia have been very clear that such permission would constitute an escalation OF WESTERN 'PARTICIPATION' in the war, and even be tantamount to a direct NATO attack, and so it is at least an escalation. Whether it is right or wrong is not the point, it is a clear change in the depth of western involvement. reply close04 4 hours agorootparent> right to reply with their own This seems like an arbitrary line [0] drawn exactly where it suits your argument. How does having North Korean soldiers fighting for Russia stay on the right side of that line? What about any components that originated outside of Russia but are employed in Russian weaponry or equipment (for example chips)? The information war is a part of \"the war\", is an \"official\" non-Russian hacker or troll crossing the line? Or a non-Russian boat or crew employed for acts of sabotage. [0] It can be fair to draw an arbitrary line, at least you know it's straight and will intersect whatever is unfortunate to be in its way regardless of the side you prefer. But you're trying to draw tiny arbitrary circles around whatever you don't like and that's feeble. reply jacknews 4 hours agorootparentThe line is clear, that western, or US-supplied long-range missiles should not be used to attack targets inside Russia, and it was drawn both by the US (fearing that they'd be 'drawn into the war') and by Russia who clearly stated this as a 'red line'. You can argue about the arbitrariness, but it was clearly understood on both sides. Ukraine is quite obviously not just a plucky country defending it's sovereignty (though it is that too), but the theater of a great-power proxy war. The rules of that game are that you keep the conflict within the theater, or risk a world war. That was already breached by Ukrainian incursions into Russia, armed to some extent with western weapons, but this is much more direct, and a clear escalation of US participation in the conflict. reply zdp7 3 minutes agorootparentThe line isn't clear, because there is no line. These lines you keep bringing up are just gamesmanship. Nothing changes because any of them are crossed. The war was fully escalated when they invaded. Ukraine has every right to attack targets in Russia. Russia and everyone else is just posturing to hopefully extract advantages. Everybody is trying to figure out what they can get away with that doesn't negatively impact them. When Trump won the situation changed for the current administration. Do you believe Russia wouldn't use nukes if it would strengthen Russia? Do you believe Europe and the US wouldn't have immediately shut down the invasion if Russia wasn't a nuclear power. ceejayoz 2 hours agorootparentprev> The rules of that game are that you keep the conflict within the theater, or risk a world war. That was already breached by Ukrainian incursions into Russia... In what insane alternate Marvel universe is Russia not part of the Russo-Ukrainian War theater? reply thaklea 5 hours agorootparentprevPublic opinion is being manipulated hard, the U.S. just closed down its embassy in Kyiv: https://www.newsweek.com/russia-ukraine-war-latest-us-shuts-... The current U.S. administration wants to make the most out of the remaining 60 days. Perhaps they have a little help: https://www.wired.com/story/inside-the-77th-brigade-britains... reply paganel 10 hours agorootparentprevRussia will not stop taking its land in Kursk back because the Americans tell them to do so, this is just Western delusion, and, as I've said before on this forum, a complete misunderstanding coming from the Westerners on how Russia operates. > devastating sanctions Devastating for Europe, you mean. reply suraci 7 hours agorootparentI'm very curious, can any European here, or perhaps a German for specificity, tell me whether they believe these sanctions have harmed Russia more than Europe? Also it would be better if any Russians here could answer a similar question reply rksbank 3 hours agorootparentAs a European, I can say that the sanctions did harm European economies, which is reflected in various political Eu government crises. It is hard to know how much Russia has been harmed, because both sides probably exaggerate the figures. I wonder whether \"more harm\" is the right question. The question should be whether the sanctions have any impact on Russia's war economy, which they do not. If anything, they make Russia more independent and strengthen Russian ties with China and India. This is all to the detriment of the EU, the only one here who profits is the U.S. by making the EU more dependent. reply brazzy 6 hours agorootparentprevGerman here. Yes, it seems pretty obvious these sanctions have harmed Russia more than Europe. Russia: inflation around 8-9%. EU: inflation around 2%. reply EVa5I7bHFq9mnYK 6 hours agorootparentThat's not a result of sanction, simply Russia spends 40% of its budget on the war, and Europe spends nothing. reply suraci 6 hours agorootparentprevThank you for the information. I believe that only those who are there can truly describe the situation there, beyond what I read in the media Recently, a professor I know wrote an article about his impressions of Russia and Germany when he attended meetings in both countries. Can you help to check what he said? > Macroeconomic data indicates that the European economy is not doing well, but the economic conditions I experienced during my days in Berlin could be described as depression. What surprised me the most was that there were not many people or cars on the streets of Berlin during the daytime on weekdays. Berlin in early October is not yet cold, but the desolate feeling on the streets does not match the image of the capital of Europe's largest economy. Europe's inflation, which started later than in the United States, has also clearly hurt the lives of the people, which was my perception from conversations with taxi drivers during my rides. reply throwaway2037 5 hours agorootparent> the European economy Any time you see \"European\" used in an argument... run away. Europe is a continent. It is huge and varied. There are 27 countries in the EU and further 23 more countries in the European continent. It is very, very hard to generalise about \"Europe\". Albania and Norway are both in Europe, and, yet, they could not be further apart in terms of human and economic development. reply brazzy 5 hours agorootparentprevYes, inflation was pretty high in 22 and 23, that hurt a lot of people. But his claim of a \"desolate feeling on the streets\" being an indication of \"economic conditions ... could be described as depression\" read like badly written propaganda. There's nothing to be checked there, just some vague feelings. Berlin isn't as crowded as he expected, so the only explanation is that nobody can afford a car and half the population is sitting at home wallowing in misery due to economic depression? Really? reply suraci 6 hours agorootparentprevAlso, here's the sections about Russia, hope any locals can help to check this > (In Vladivostok) War typically leads to a rise in prices. Several Russian sources have reported that compared to two and a half years ago, current prices have roughly doubled, and housing prices have also increased significantly. However, it is somewhat comforting that the wages of most people have also increased proportionally, so people's lives have not been greatly affected so far. The supply of goods on the market is still quite abundant. Due to financial sanctions from the US and Europe, as well as multinational corporations, many brands' products and services are no longer available in the Russian market. Nevertheless, this does not prevent Russian citizens from drinking cola or eating American fast food. It is said that these brands have localized, but the products remain essentially unchanged: for example, the taste of Russian cola is not significantly different from Coca-Cola, as they can purchase the concentrate from third countries and mix it themselves. > The official unemployment rate published by Russia is only 2%, and I believe this data is likely accurate. The reasons are not only because the war itself requires the hiring of a large number of young people, but also due to the wealth redistribution, increased consumption, and robust production that the war has brought about. Russia is a country with severe wealth disparity, where the lower classes traditionally lack money for consumption. This war has provided an opportunity for lower-income families to obtain cash flow: by sending their sons or husbands to the battlefield, families can receive a one-time subsidy of nearly 500,000 yuan. Even prisoners in jail can receive this benefit. This sum of money, equivalent to targeted transfer payments and proactive fiscal policies aimed at the poor, has given lower- and middle-income families a chance to gamble their lives for money. This has led to cases where some people join the military to escape punishment and receive subsidies, serve for a year, return home, and then reoffend and go to jail again, relying on a second enlistment to escape punishment and receive another subsidy. > The increased cash flow among the lower-income population has led to a surge in consumer demand, and the robust production of military goods has also stimulated employment, income, and consumption. While the products of military industry are indeed consumed on the battlefield, for the macroeconomy, what matters is the flow rather than the stock; production and consumption are meaningful in themselves. As for whether the produced goods are expended as shells and missiles on the battlefield or become paper wealth on the other side of the ocean as export commodities, there is no fundamental difference for the current macroeconomic operation. There are rumors circulating on Chinese self-media about how much the ruble has depreciated on the black market in Russia. I specifically went to restaurants and other consumer venues in Vladivostok to test for any significant difference between the official and black market exchange rates by using US dollars and Chinese yuan for payment. However, neither Russian-run nor Chinese-run restaurants offered discounts for payment in US dollars or Chinese yuan cash. This phenomenon is usually sufficient to debunk rumors about the Russian ruble black market. The current social mood in Russia is relatively stable, which may be due not only to a decent economic foundation but also to strict control over public opinion. According to our research feedback, even in private settings, if colleagues or neighbors make remarks against Putin or the war, and are reported, those who oppose the war or Putin may face legal troubles. reply actionfromafar 3 hours agorootparentDid the source also mention that the low unemployment is in no small part due to the would-be workforce going to the frontlines, and also a huge initial wave of emigration to other countries among those privileged enough to own a passport. reply rksbank 3 hours agorootparentprevThe professor is correct. reply EVa5I7bHFq9mnYK 7 hours agorootparentprevThese consumer side sanctions are idiotic. When a Russian buys a European beer, he spends money which goes from Russia to Europe, and in addition he damages his health. On the other side, Europe buys billions of dollars of oil and gas from Russia. That money goes in the opposite direction, from Europe to Russia, and is used toward soldier salaries, Iran drones and North Korean mercenaries. reply viraptor 6 hours agorootparentHas this been a recent change? In 2023 NL announced they're not dependent on Russian energy anymore https://nltimes.nl/2023/02/10/netherlands-longer-dependent-r... reply EVa5I7bHFq9mnYK 6 hours agorootparent$1.7b in August 2024 + $2.3B exports to Turkey, much of which is just transshipment to Europe. https://energyandcleanair.org/august-2024-monthly-analysis-o... reply EVa5I7bHFq9mnYK 6 hours agorootparentprevMaybe true for the Netherlands, I apologize, I meant all of EU. reply raverbashing 8 hours agorootparentprevNeither will Ukraine try to take their territory back as much as sycophants and dictator-appeasers think Ukraine have no agency reply huijzer 4 hours agorootparentprevProf. Stephen Kotkin — an historian who wrote multiple extensive biographies on Stalin — calls the Russian regime a \"gangster regime\".* Once you see them as gangsters, it's not difficult to see why they would do this. *A full link with exact timestamp of Kotkin saying this is [1]. Here he talks about why Merkel kept making oil deals with Putin even though in hindsight this was probably not the best idea. Kotkin argues that, yes, according to econ 101 trade is good for both parties, but not when the opposite party is a gangster. Merkel thought that Putin was thinking like her, but he wasn't. [1]: https://www.youtube.com/live/jJSDdCPpbto?t=4410 reply euroderf 1 hour agorootparentOne theme of cyberpunk is that Russia remains a gangster regime in the future. William Gibson's \"Kombinat\". reply mopsi 4 hours agorootparentprevIt should be noted that Putin was personally an enforcer for St Petersburg's mayor Anatoly Sobchak[1] in the early 1990s, and his \"circle of friends\" from that time now mans key positions of the entire government. For example, Viktor Zolotov[2], Sobchak's bodyguard and Putin's judo partner, is now in charge of National Guard, despite not having qualifications for the job. Russia is literally run buy thugs who ran protection rackets not so long ago. So there's much more to this than just a fitting figure of speech. Someone from the worst parts of LA would be better equipped to understand and deal with such people than those who spent their teens and early adulthood playing Model UN at a foreign relations club. [1] https://en.wikipedia.org/wiki/Anatoly_Sobchak [2] https://en.wikipedia.org/wiki/Viktor_Zolotov reply Mistletoe 10 hours agorootparentprevIt doesn’t even really stop anything right? Communications just have to route around it and use other cables and satellites. It just seems like Russia wants to be annoying. reply Hamuko 10 hours agorootparentDestroying the gas pipeline between Estonia and Finland did take it out for like six months. I think it may have had some negative impact on Estonian electricity prices during that time. reply lifestyleguru 7 hours agorootparentprevThis is basically Russian retaliation for US providing Ukraine with ATACMS and first Ukrainian attack using ATACMS. reply tauntz 52 minutes agorootparentThe \"retaliation\" against US is to disrupt communications between.. Finland and Germany? Applying the same logic, Ukraine should retaliate against Russia for bombing their hospitals with an attack on.. Iranian civilian infrastructure? Did I get that right? reply aguaviva 6 hours agorootparentprevTit-for-tat response to the NS2 bombing. Assuming it bears out that the Russian state is the perpetrator. reply brazzy 10 hours agoparentprevSo according to the Bluesky thread, the ship was captained by a Russian citizen. One has to wonder whether this was done with the approval of the Chinese government, or whether the ship was just chosen by opportunity (which seems possible given that China is the second most common merchant flag). Or whether implicating China was even an explicit goal. reply netsharc 9 hours agorootparentFor an analogy, it seems like a scrappy preteen throwing around his big brother's name, knowing that if he gets into trouble, big brother will intervene... (i.e. the European countries might be more wary about boarding a Chinese ship compared to a Russian ship, because escalating against China is scarier...). reply _djo_ 9 hours agorootparentIndeed. The best way to understand Russia's approach to foreign policy is that it's an extension of its mafia state-derived domestic policy, where there are no true allies and anyone brought into the circle is tainted through compromising actions to ensure they stay loyal to you. It's not dissimilar to the way criminal gangs will ensure that they have dirt on anyone joining or intentionally implicate others in order to ensure compliance. reply graemep 8 hours agorootparentprevI think China stands to gain from escalation of the war so its possible they approved. It makes Russia weaker and more dependant on them, distracts the US from the Pacific, and weakens Europe in many ways. Similar to both Russia and China gaining from war and disruption in the Middle East. There are many possibilities here. reply whizzter 9 hours agorootparentprevRussian captain, how does the ownership history of the ship look? Could be some sanction evading ship that was owned by Russian interests anyhow. reply pantalaimon 6 hours agorootparentIt was a Russian ship until a month ago reply jeroenhd 3 hours agorootparentDo you have a source for that? According to https://www.vesselfinder.com/vessels/details/9224984 it's been registered as Chinese since 2016. Doesn't mean its current Russian captain is serving Chinese interests, of course, but at least it seems to be Chinese owned. reply lukan 1 hour agorootparentprevI doubt China will be happy, if Russia staged chinese support. But rumors have it, that the North Korean troop support for the war in Ukraine also came out of the blue for China, so Putin might make a risky gamble here, but I doubt he dares it. If China would seriously drop support for Russia, they would be srewed. reply mytailorisrich 8 hours agorootparentprevChina did not want the war in Ukraine, which has created serious problems for them including for Belt and Road. So behing closed doors China must be passed off but Russia is important to them and they can't let them collapse. Of course Putin knows this hence him somewhat taking the p. reply aurareturn 10 hours agoparentprevGiven that ships often cut undersea internet cables and China has the biggest export economy, doesn't it make sense that the most likely country to accidentally cut an internet cable would be a Chinese trade ship? On average, it seems like undersea internet cables break 200+ times per year. For example, Vietnam's internet cables break on average 10 times per year. What would be the motivation for a Chinese trade ship to deliberately cut an internet cable? It has next to no impact on internet communication and only serves to annoy a small amount of people for a short period of time. In addition, China and Europe are trying to have a better relationship in general so it doesn't make sense for the Chinese government to order this. reply brazzy 6 hours agorootparentI could believe that cutting one cable was an accident. But two, by the same ship, 60 miles apart? Absolutely no way this wasn't intentional. reply rixrax 9 hours agorootparentprevAt the Baltic Sea the cables and such break mostly because of one reason only: russia. [0] [0] https://www.csce.gov/briefings/russias-genocide-in-ukraine/ reply Hamuko 10 hours agorootparentprev>What would be the motivation for a Chinese trade ship to deliberately cut an internet cable? Money. Russia is reportedly bribing people into doing sabotage in western nations. There's also reports that Yi Peng 3 is captained by a Russian national, which would also be another reason for a Chinese trade ship to conduct sabotage operations beneficial to Russia. reply raverbashing 10 hours agorootparentprev> What would be the motivation for a Chinese trade ship to deliberately cut an internet cable? The most charitable reason is that they don't give a fluck. Same reason why their rocket boosters just fall wherever they fall, population center or not Edit: https://x.com/Tendar/status/1859147985424196010 > The skipper of the Chinese ship is a Russian national and the route leads from Ust-Luga (Russia) to Port Said (Egypt). reply aurareturn 10 hours agorootparentIs there any data on which country's ships cut the most internet cables? I think we need a total ships sailing for country / cuts. reply miningape 8 hours agorootparentThis would be an interesting project for someone to work on, I wonder if there's a place where all the internet cable outages + reasons are available? reply bobbob1921 17 minutes agoprevWhat I don’t understand - if the yi peng was intentionally trying to damage the FO cables, why would they not spoof or disable their AIS data/broadcast (ship tracking transponder which is the source of this positioning data we see). Anyone have some insight on that? reply thecodemonkey 7 hours agoprevThe Danish defense forces now confirms their presence but they are not providing any other information right now: https://x.com/forsvaretdk/status/1859195509866381402 (This is also a rare English-language tweet from an account that usually only tweets in Danish) reply adverbly 27 minutes agoprevShould be very easy to verify if this was the cause. All you have to do at this point is go look at the cable near the crossings. If there is evidence of an anchor hitting the cables in both of these locations then you've got pretty clear proof. Someone should obviously be checking into this right now. No point speculating until it's confirmed really. I guess you might still want to board just to find out weather there is any evidence of intent rather than negligence in the case that this is confirmed to be the cause... reply ActionHank 20 minutes agoparentAt best fall guy captain will claim ignorance, malfunction, or negligence. Retire or move to some cushy job. No one will want to implicate China in something that would support Russia's war and would all be afraid of the economic fallout. reply threeseed 10 hours agoprevAnd 4 days ago a Russian spy ship was escorted out of Irish waters: https://www.theguardian.com/world/2024/nov/16/russian-spy-sh... So definitely seems like a coordinated attempt to destabilise Europe ahead of anticipated peace talks early next year. reply pcardoso 9 hours agoparentSame in Portugal https://www.theportugalnews.com/news/2024-11-18/russian-ship... reply INTPenis 1 hour agoparentprevWhy would destabilising europe before peace talks be beneficial? Seems like they would lose a lot of leverage. reply soco 6 hours agoparentprev\"Russian mission installs more ‘spy’ antennas in Geneva, Swiss TV report claims\" https://www.swissinfo.ch/eng/foreign-affairs/russian-mission... reply jacknews 7 hours agoparentprevSo how long ago were US long-range missiles used to attack Russia? Because that's what seems to be claimed here, that Russia are retaliating for that. How long does it take a ship to travel to a 'suspicious' site like this? versus, how long does it take to intercept the nearest Russian ship, and escort it away as a spy ship and 'potential saboteur'? reply A_D_E_P_T 7 hours agoprevMuch more information here: https://gcaptain.com/details-of-baltic-sea-cable-incident-re... reply emmelaich 6 hours agoparentand some here https://www.newsweek.com/baltic-cable-sabotage-nato-1988689 including > Social media reports said that the vessel had a Russian captain, although this has not been independently confirmed. reply 01100011 11 hours agoprevTitle could be a lot more descriptive. Your average reader might scroll on by because that title makes no sense without context. reply usr1106 8 hours agoprevLooks suspicious, but there were 4 vessels in the area whose transponder signal was lost by public trackers during that night. It has also been pointed out that this is a location with lively traffic. So if it turns out that is was an anchor (as in the New New Polar Bear case) that's extra suspicious because anchoring in such location is not normal. On the other hand if it were explosives like in the Nord Stream case, they could have been applied also weeks before. reply fjfaase 2 hours agoprevIt looks like that the pilot ship Styrbjoern [1] came along side the Yi Peng 3 today. It traveled from the harbor of Grenaa to the ship and back. It possible that they took some people in for questioning or put a pilot and/or guards on the ship. [1] https://www.vesselfinder.com/?mmsi=219003826 reply TinkersW 7 hours agoprevThis is the 2nd time China did this in that Baltic isn't it? Both times look intentional.. maybe don't allow Chinese ships in the Baltic? reply Arnt 4 hours agoparentNo it isn't. Both of the two Chinese registries are open, pretty much anyone can register ships there. It's a bit like the .tv domain — if you see something.tv you can't assume that it's a company in the country Tuvalu. Look at the nationality of the captain and the beneficial owner instead. reply tossandthrow 6 hours agoparentprevThat would not swing. Denmark controls the waters of the seaway to Sct. Petersburg and Kaliningrad that are some of the strategically most important ports of Russia. Blocking of traffic to these would be a severe escalation. Regularly Russian subs pass through Danish waters - controlled and allowed. reply mihaaly 6 hours agorootparentI'd consider the serious escalation of offensive (cowardly) acts were carried out by Russia many many years ago repeatedly, increasingly, throughout Europe (elsewhere too), with mild consequences. Got seriously unabashed escalating further. Being cautious with the nazi Germany blew into the face of the World, will definitely not work with the imperialist Russia either. China acts on behalf of Russia here - Russia being coward for open confrontation with anyone (believed by them) able hitting back hard. China has secondary benefits for self as well. reply rightbyte 6 hours agorootparentUnilateral rightoussness is a liability. We are living on the graces that the American, Russian and Chinese elites are somewhat on the same page. And pretending otherwise is not very helpful. Hitler had no such leverage. reply euroderf 36 minutes agorootparentprev> Regularly Russian subs pass through Danish waters - controlled and allowed. I've always wondered how subs handle tidal flows there, and how challenging the tidal flows are. reply Tade0 6 hours agorootparentprevDamaging infrastructure is already a severe escalation. Should not have done that. reply tossandthrow 6 hours agorootparentThese are times to chill - unless we want a full on nuclear war. (I do realize that in particular US citizens have very high confidence in their own military capacity and might be overly bullish on situations like these) reply Tade0 6 hours agorootparentNot American - I'm Polish. I've got friends who got drafted already (if only for training) so it's entirely possible I'll join them eventually. My take is that Russia's plan is to continue sabotaging and a weak (or lack of) response to that only emboldens them. Also nuclear war with what? Their recent Satan II ICBM test demonstrated that they don't necessarily have the technical chops to launch anything sufficiently capable and it must have come as a surprise to them as well. reply tallanvor 6 hours agorootparentprevNuclear war is not a realistic concern, luckily. If it was, it would have happened after the first \"red line\" Russia claimed the west had crossed. reply tokai 6 hours agorootparentprevOn the contrary, chilling would endanger everyone living the in the free world even further. reply malermeister 6 hours agorootparentprevHow severe an escalation would it be? As severe as... say starting the largest war in Europe since WW2 right at our doorstep? Or as damaging our critical infrastructure? Or manipulating our democratic processes? It's time the West pulls its head out of its ass. We're already at war, whether we want it or not. reply rightbyte 6 hours agorootparent> We're already at war, whether we want it or not. I think you should complain about 'appeasement' abit longer before switching gear to 'to late YOLO'. That would help your cause better. reply malermeister 6 hours agorootparentI don't think I have a cause. I'd like to not be constantly attacked by foreign adversaries, is that a cause? But if attacks happen, we can't just ignore them because hitting back might make the abuser more mad. reply rightbyte 6 hours agorootparentSeems quite easy for an 'adversary' to manipulate two other 'adversaries' into an extended suicide with that mindset. reply talldayo 1 hour agorootparentprevAm I missing something, or do you post peacenik appeasement demands under every HN submission? It's such a radically stupid position that I'm legitimately starting to think you're a Russian propagandist. Why would any rational country appease a madman? Because people like you write internet comments about pissing your pants? If we reach the \"to [sic] late YOLO\" stage it won't matter what options we picked. That's why appeasement is a fundamentally pointless idea that the US has refused for decades. If you even once play the \"give a mouse a cookie\" game you will end up surrendering everything to a power that can threaten you with nuclear terrorism. Only a moron would appease Russia in this scenario. reply Gualdrapo 6 hours agoprevGoing from fishing illegally in south american waters to damaging internet cables in Europe. reply a1o 10 hours agoprevC-Lion -> Sea Lion, but not the IDE from JetBrains. reply trhway 9 hours agoprev>Last ports: Murmansk - Port Said - Luga Bay (never docked, Ust-Luga, Russia) All the way to Luga and decided to not dock. Large cargo ship pleasure wandering the sea like a yacht. reply Hamuko 11 hours agoprevYi Peng 3 has been stopped in the Kattegat with Danish navy ships around it for about 11 hours now. Currently HDMS Søløven is anchored right next to it. HDMS Hvidbjørnen was also not too far away before its signal went dark. reply nickpp 6 hours agoprevAlso, Russia is sabotaging European satellites: https://nltimes.nl/2024/11/15/dutch-childrens-channel-outage... reply geor9e 35 minutes agoparentTo be less ambiguous in word choice, they jammed a satellite from the ground. Russians used a ground based dish to spoof a TV station signal to a repeater satellite, causing TV stations near Ukraine to go down and show an interference error. I'm just clarifying because \"sabotage\" could mean any number of more costly and damaging things, like a spy loosening a bolt before launch or something. https://nos.nl/nieuwsuur/artikel/2544558-verantwoording-en-b... reply nik_alberta 10 hours agoprevYI PENG 3 (IMO: 9224984) is a Bulk Carrier and is sailing under the flag of China. Her length overall (LOA) is 225 meters and her width is 32.3 meters. Source: https://www.marinetraffic.com/en/ais/details/ships/shipid:21... reply selimnairb 6 hours agoprevI guess WWIV has been on a slow burn for going on three years now. reply VyseofArcadia 3 hours agoparentDid I miss WWIII? reply queuebert 3 hours agoparentprevMore like since Deng Xiaoping initiated the modern Chinese economic strategy in the '80s to control the West through trade. reply RevEng 8 minutes agorootparentThe West did a fine job of this themselves. Outsourcing to poorer countries is what has made the West so wealthy for so long - goods whose price is subsidized by cheap labor. Now that China and other countries have caught up, the West doesn't get the same discount, but they also don't have their own manufacturing because they all outsourced. We did this to ourselves. reply mitjam 7 hours agoprevIt was crossing right on time for the interruptions, a Russian officer was on board, it slowed down while crossing, no other ships were slowing down in that area during that time (rulingnout headwinds) - it cannot get much clearer. China is now participating in hybrid warfare against Europe (unless they present stronger evidence against this assumption) reply netsharc 6 hours agoparent> China is now participating in hybrid warfare against Europe Geez, I'm glad you're not war minister. It's a Chinese registered ship with a Russian captain. If a terrorist crashes a truck with Portuguese plates into the US embassy in Berlin, would that mean Portugal's declared war against the USA? reply Arnt 4 hours ago [flagged]rootparentnext [15 more] Relax. A lot of people don't understand ship registration. https://en.wikipedia.org/wiki/Flag_of_convenience explains it well. Or you can just think of it like a domain. Registering your .com domain with Gandi doesn't make you French, it just means you can be sued there and a few other details. Registering your ship with the Chinese registry doesn't make the ship Chinese in reality, you do need to pay taxes there but you don't have to locate your office in China. reply holowoodman 3 hours agorootparentWell, yes, Flag of Convenience is a thing. But there is a \"but\", which is that in the articles of war, the flag of a ship does have quite a few implications. E.g. when two nations are at war, stopping ships flagged as belonging to the opposition gives certain rights of stopping and searching them, blockading their passage, seizing the vessels and cargo, etc. And the relevant characteristic in that case is the flag, not the captain's nationality: > Art. 51. Enemy character. The enemy or neutral character of a vessel is determined by the flag which it is entitled to fly. http://hrlibrary.umn.edu/instree/1913a.htm reply Arnt 2 hours agorootparentIf you want to be formal about it, none of the countries with Baltic coastlines are formally at war. reply holowoodman 2 hours agorootparentYes, but there is the huge other \"but\" that in modern use, a formal declaration of war is no longer necessary, committing acts of war is sufficient for a state of war to exist. (However, committing acts of war without a preceding declaration is of course a war crime.) Of course this isn't really automatic and triggered by the smallest thing, both sides kind of have to \"agree\" to be at war, e.g. by a counter-attack, a declaration following the attack or something like that. And nobody really wants to take that bait, due to the huge consequences involved. Yet, it is China playing with fire here, we all can be happy that none of the affected nations took them up on their \"offer\" of war. reply escape_goat 2 hours agorootparentJust to clarify again, this is a dry bulk / Panamax vessel. It is part of the shipping industry. At scale, it is analogous to a railroad car. In 2015 it was operating as the Avra under the flag of Greece. The foreknowledge of the Chinese government that a Russian officer would conduct hybrid operations from the vessel cannot be inferred from the circumstance. It is like thinking that someone with an American passport is an American spy. reply holowoodman 2 hours agorootparentIt is quite the opposite from what you are arguing. China is responsible for the conduct of the vessels they allowed to fly their flag. They can later claim that the crew and captain acted on their own will, without orders from the Chinese leadership. They can duly punish the captain and crew or disavow the vessel and declare them renegade, disallow them to fly their flag. But without such a declaration, a nation such as China is responsible for the conduct of their fleets, be they civilian or military. And any vessel they allow to fly their flag is part of their (in this case civilian) fleet. reply WitCanStain 1 hour agorootparentIs the US responsible for any crime committed by members of ships that fly the star-spangled banner? reply aldous 3 hours agorootparentprevYes, good points. It's not a wild stretch of the imagination that Mr P and gang are actively trying to drag China into the Ukraine conflict and I'd imagine Beijing is pretty pissed off today about being (ostensibly) implicated in this sabotage. So the usual underhand scheming from the Kremlin imho, don't fall for it. China and Russia's relationship is very complicated of course and there's many a convincingly analysis out there that predicts conflict between them in the near future (an example flashpoint being Siberia). reply netsharc 4 hours agorootparentprevYes, this is what I'm saying, but with less words. But look around (even in these comments) and look at how many people are thinking \"Chinese act of war!!!11!!\" reply jstummbillig 3 hours agorootparent> Yes, this is what I'm saying, but with less words. That's really not all you are saying, and the difference is important. Maybe not to you, though. reply netsharc 3 hours agorootparentThen, elaborate please, Jochen, what's the important difference? reply PhasmaFelis 2 hours agorootparentprevAs far as I can tell, you're both saying the same thing: that registering a ship in China does not mean China is responsible for that ship's actions. If you've got a different point to make, please make it clear. reply Arnt 4 hours agorootparentprevYes… A lot of them really need have it spelt out, twice, in large clear type. reply scrps 2 hours agorootparentprevSo the Russians who are at this point highly dependant on Daddy Xi to keep their economy and military afloat are gonna false flag the West to suck China into a quagmire of a war a few months before the most unpredictable and venomously anti-china president (who has thin skin, a hair trigger, and no qualms about conducting airstrikes on high-ranking Iranian generals unilaterally on a whim) in modern US history is about to take office at the head of a country with the largest functioning stockpile of nuclear weapons and a massive military? You think Chinese intelligence is asleep at the wheel and wouldn't notice given the stakes and absurd levels of geopolitical risk the entire planet is at? China may back Russia to try to shift perception of the west's military might/will or to drain resources or just to buy Russia by making them dependant to get those juicy Russian natural resources but they aren't going to start world war iii to help Putin with his fetishistic \"yet another European dictator\" fantasy. The Chinese know how to play the game same as the Russians and the US. All these little games are just calibrated psyops, why destroy, very publicly, comms lines when tapping it would be far more beneficial to a war effort and much quieter? Maybe to make the West look weak and unable to defend their borders which affects consequences domestically like say channeling political support to isolationist politicians who want to retreat from supporting Ukraine? Cause those politicians didn't make gains in the last European elections or nothing. reply xbar 4 hours agorootparentprevWell said. reply mitjam 14 minutes agorootparentprevTrue but China can support or not support investigations and prosecution. After all they are the ones who can exercise their sovereign rights on ships sailing under their flag. I‘m really curious and open minded how this plays out but sadly would be surprised if China would support the EU in this case. reply bungle 2 hours agorootparentprevIt was the second Chinese registered ship with Russian crew within a short period of time. A year ago this https://en.wikipedia.org/wiki/Newnew_Polar_Bear cut the gas pipe and another communications line. I am sure if the cowardly Russians ever did this to USA, it would cause a much bigger drama and retaliation wave, and China would take the hit as well. reply Larrikin 6 hours ago [flagged]rootparentprevnext [3 more] Russia gets no benefits of the doubt. reply Octoth0rpe 4 hours agorootparentThey're not asking for Russia to get the benefit of the doubt, they're asking (reasonably IMO) for China. reply jajko 2 hours agorootparentprevA well-earned result of decades of their hard work, although this is about china-registered vessel reply mschuster91 3 hours agorootparentprev> If a terrorist crashes a truck with Portuguese plates into the US embassy in Berlin, would that mean Portugal's declared war against the USA? At the very least, the cooperation of Portugal's authorities would be expected to determine how the truck ended up being used for the attack, and if anyone knew about how the vehicle was to be used. I expect the same amount of cooperation from China as the flag state. reply drewcoo 6 hours agorootparentprev> war minister Due to an earlier generation's newspeak, that's \"defense,\" not \"war.\" reply Arnt 4 hours agorootparentAre you sure about that? I happened to notice that at least in some cases, the change of terminology happened roughly when it became clear that offensive war was a losing proposition in terms of money and resources. I suspect that as invading the neighbours became financially irrational, the cool heads that tend to survive in management shifted their stand from mixed offense/defense to just defense. reply mitjam 7 minutes agorootparentYes Mr Pistorius is „Verteidigungsminister“ as in defence, and it‘s called that way since 1955. Not that hard to find out. reply giraffe_lady 4 hours agoparentprevWhy did they leave AIS on? reply diggan 4 hours agorootparentHaving AIS on is mandatory. I'm sure turning it off would raise even higher warning flags than just leaving it on while doing your shady stuff. Regardless, there are satellites covering the area, so you wouldn't get rid of being tracked anyways, would just be a bit slower. reply jeroenhd 3 hours agorootparentHaving AIS on is mandatory, but in practice a lot of ships turn it off regardless. From shadow oil fleets laundering sanctioned oil to fishermen, fake or disabled AIS systems are hardly an exception. I don't think Russia is trying to hide their sabotage, though. Even with AIS disabled, there's no way European intelligence agencies didn't know what ships were floating above these cables at the time they went down. This was a warning, not a secret operation. reply bergie 3 hours agorootparentprevHaving AIS on is mandatory, and in many places taken quite seriously. Last night we sailed from Fuerteventura to Gran Canaria. There was a cargo ship with broken AIS in the area, and the VTS broadcasted their position over VHF every half hour (with DSC all ships alarms and everything) reply giraffe_lady 3 hours agorootparentprevEvery recreational sailor knows that AIS is \"mandatory.\" It's completely routine to see commercial ships running without it. reply WinstonSmith84 3 hours agorootparentWith \"commercial\", I guess you imply fishing vessels doing this to go fishing outside their delimited area. That's different from a massive bulk carrier in the middle of the Baltic reply giraffe_lady 55 minutes agorootparentNo I meant what I said. I've never seen a like supertanker without AIS but I've seen smaller cargo ships, ferries, and specifically in northern europe energy company tenders running without it. reply diggan 3 hours agorootparentprev> It's completely routine to see commercial ships running without it I think this depends a lot on the location, as different areas seems to make it different levels of \"mandatory\". Are you speaking about the Baltic Sea specifically based on experience? reply giraffe_lady 3 hours agorootparentYes. I spent a pandemic summer sailing the north sea, denmark, sweden with a friend. We sailed much less in the baltic and I admittedly kind of mix the north & baltic in my memory but they are very similar regulatory environments re boats so it would surprise me if it was common in one but not the other. reply diggan 3 hours agorootparent> they are very similar regulatory environments re boats so it would surprise me if it was common in one but not the other. One has Russia and their ports, while the other doesn't. So preparedness and military presence certainly is different between the two at least. reply mistrial9 4 hours agorootparentprevrecent statistic : Global Fishing Watch’s study published in Science Advances on November 2, 2022, revealed that: Over 55,000 suspected intentional disabling events of AIS signals were identified between 2017 and 2019, obscuring nearly 5 million hours of fishing vessel activity. This phenomenon accounts for up to 6% of global fishing vessel activity. reply greener_grass 6 hours agoparentprevSo if Trump is against China, and China aligns with Russia, will Trump then support Ukraine? Interesting (and choppy) times ahead. reply n4r9 6 hours agorootparentEven if China doesn't explicitly align with Russia, I believe there are strategic reasons why the US would want a favourable outcome for Ukraine. I outlined a few points in a post a couple of weeks ago: https://news.ycombinator.com/item?id=42059787 I'm no international relations hawk though, so I'm keen to hear opposing viewpoints. reply pclmulqdq 1 hour agorootparentI agree with what you have said here, but I don't know if the US is in a position to turn the war around in 2024 without a huge escalation. It remains to be seen if there is any possible way to do that without \"boots on the ground\" (formally starting WW III) or the use of nuclear weapons (again, formally starting WW III). There were plenty of options to pressure Ukraine into preventing Russia from having a causus belli in early 2022 (too bad the Biden admin didn't do any of those), but those are gone now and Russia currently controls much of the territory they had as military objectives. reply aguaviva 1 hour agorootparentDoesn't need to be a huge escalation. Just enough to send the tide of attrition turning slowly the other way for a while. After which HN will instantly fill up with comments about \"how badly Russia is losing\", \"it's clear Ukraine has already lost\", and so forth. There were plenty of options to pressure Ukraine into preventing Russia from having a causus belli in early 2022 Russia never had casus belli in this conflict, and no one did anything to present it with such. reply pclmulqdq 17 minutes agorootparentI'm not sure Ukraine wins a war of attrition in any meaningful way. Russia is also shockingly good at wars of attrition, and the entire Russian economy has been built around war with the West. Ukraine is a small state in comparison, and they are running out of men, money, and munitions so fast that even tipping the scales by 10x will sink Ukraine before Russia retreats from the territory they now own. In 2022, the goal would be to make it costly to acquire territory so ideas about attrition would have worked a lot better, but it's 2024 and Russia has already grabbed the land. Someone needs to go take it back. Here's a memo for you on Russia's causus belli. You can claim that they didn't have a legitimate one (I don't think they did), but they had one that got them enough local and international support to work in both 2014 and 2022: https://www.ponarseurasia.org/vladimir-putins-casus-belli-fo... reply n4r9 5 minutes agorootparentIn your opinion what could Ukraine have done to avoid the causus Belli in 2022? Dalewyn 2 hours agorootparentprevI used to support Ukraine winning the war at any cost (them losing and that result being recognized implies that warmongering is acceptable). However, that war is now in its third year with no end in sight. Our (the west's) response to warmongering has been to trickle just enough resources and monies to keep Ukraine from losing but not so much that they win. The \"donated\" resources of course need to be replenished, the military industrial complex is quite literally making a killing. At this point the question of declaring a firm stand against warmongering is lost. It's three years and going, warmongering as it turns out is fine. I hate that. My tax dollars are going towards endlessly and needlessly extending human suffering for the benefit of the military industrial complex. I hate that. So I say, enough of this bullshit. Unless we suddenly send in so much support that Ukraine decisively wins very quickly, I don't want to see a single cent more of my tax dollars going towards this. My taxes are not blood money and the military industrial complex can go fuck themselves. reply myrmidon 1 hour agorootparentI think classifying western aid to Ukraine as tax transfer to the military industrial complex is just incorrect. Because a lot of it does/did NOT need to be directly replenished for the donors-- instead the donations was more like getting rid",
    "originSummary": [],
    "commentSummary": [
      "A Chinese cargo ship, Yi Peng 3, allegedly severed undersea cables in the Baltic Sea, leading to suspicions of deliberate sabotage.",
      "The ship's movements, captained by a Russian national, coincided with the timing of the cable breakage, raising geopolitical concerns.",
      "This incident, following a similar one in October 2023, has led to increased scrutiny and ongoing investigations by the Danish navy."
    ],
    "points": 289,
    "commentCount": 356,
    "retryCount": 0,
    "time": 1732085807
  },
  {
    "id": 42187506,
    "title": "When did estimates turn into deadlines?",
    "originLink": "https://domainanalysis.io/p/architecture-modernization-execution",
    "originBody": "Share this post Architecture Modernization Execution: When did estimates turn into deadlines? domainanalysis.io Copy link Facebook Email Note Other Discover more from DomainAnalysis.io Indu's take on analyzing the complexity of any given domain using structured approaches, such as Systems Thinking, Domain-Driven Design, and other methods that stem from the Service Design world, like Service Blueprints. Subscribe Continue reading Sign in Architecture Modernization Execution: When did estimates turn into deadlines? Tip: In complex software modernization projects treat estimates as guidelines, not deadlines. Indu Alagarsamy Nov 19, 2024 7 Share this post Architecture Modernization Execution: When did estimates turn into deadlines? domainanalysis.io Copy link Facebook Email Note Other 1 1 Share Can I please return to my vacation days in early October? After my unforgettable and fun vacation in Seoul and Sokcho, my original plan was to write about Systems Thinking and the book I read, \"Zen and the Art of Motorcycle Maintenance\" by Robert Pirsig. Thanks for reading DomainAnalysis.io! Subscribe for free to receive new posts and support my work. Subscribe The Great Buddha at Seoraksan National Park, South Korea But all it takes is a moment, which has lasted more than two weeks, and I feel my life has turned upside down. The last two weeks have been quite literally a car wreck. First, the Sunday before the US elections, while pulling over for a paramedic, I got rear-ended by a monster pickup truck. I am fine, thankfully, but my car still is not. Second, I really believed that the company I work for, the New York Times, would avoid the ULP strike by the tech workers by agreeing to a fair contract before the US elections. That did not happen. And I don't want to discuss the US election results because that hurt will not go away anytime soon. Meanwhile, my colleagues and I went on an 8-day ULP strike. Since the company refused to bargain with us while we were on strike, we decided to end the strike and return to work earlier this week. We would like to get the contract we deserve soon. I am trying very hard to return back to functioning, I must be succeeding because here I am, able to write some words! Estimates - Is it an Art or Science? My car has been in the auto body repair shop for the past two weeks, and I am still trying to get an estimate of its damages. I am learning that it's an interesting domain: First, the insurance adjuster submits what they think is the damage to the car to the repair shop. Let's say this is $15000 Next, the repair shop submits what they think is the damage back to the insurance company. Let's say it's $18000, and the shop estimates it would take 30 days to fix it. Eventually, the insurance adjuster and the repair shop expert meet to assess the damage together. Let's say the insurance adjuster approves this new estimate from the repair shop. The repair shop then starts with the repair process. At least this is my understanding so far. Unforeseen Issues But what if there is damage that can't be seen yet? Let's say the repair shop is in the process of doing the repairs they notice additional damage. Or they put the car on the frame machine to see if there is damage to the frame. In a unibody frame, it's not just the point of impact of the frame; some of the effects can also transfer to the back of the frame. So they'll have to inspect it to assess the full damage. They might start unassembling the car and notice more damage that was not originally on the estimate. Now what? Emergent Solutions Let's say, based on the new findings, it's going to cost an additional $20,000. They now need the approval of the insurance company to proceed. So they write up the new findings with the new cost and send a supplemental repair for approval. The insurance company will determine whether, at this point, it still makes sense to proceed with the repair or call it a total loss. And, of course, there are guidelines and rules to determine that. Let's assume it's not a total loss. The insurance company then approves the additional cost of repair. Real Talk or Crazy Talk? Can you imagine if the insurance company started arguing with the repair shop, asking them—no—telling them that they would only pay the $18,000 and not the additional $20,000 because that was the original estimate? Does that sound ridiculous to you? It does to me, too. Thank heavens, reality does not operate like this. Isn't Complex Software Architecture Modernization the same? When you are in the process of modernizing legacy software, you are in the realm of complex software. When you look at it from the outside, you see some obvious things, and based on experience and expertise, you come up with an initial estimate. This is the repair shop's original estimate of $18,000 As you are in the process, you start uncovering additional complexity. This complexity could be similar to some hidden damages the repair shop discovered after un-assembling the parts, or this could be significant frame damage that the repair shop discovered. What do you do? You need additional approvals to proceed at this point. Good Leaders Ask the Right Questions The right questions are asked if you are in a healthy software development environment when you bubble these problems up. How complex is this problem? What are the different ways of solving this? What are the tradeoffs? Are there workarounds or alternate solutions to this? Suppose the questions start leading to how you did not see this complexity, how you missed this, why it is taking so long, and you are beholden to the original estimated dates? May the force be with you and your team to give you the strength to see this through. Hopefully, your modernization journey will be a short and successful one. To Proceed or call it a Total Loss? I have been involved in both types of modernization projects. Cases where the supplemental cost gets approved, and work proceeds until the next step, rinse and repeat until the project has come to completion. I've also seen total loss, i.e. the project got canned because it was too expensive to proceed, and it would cost more than it was worth. It's not easy deciding this, and there are frameworks and decision workshops to try and choose the direction. Is this a complex context or a complicated context? I urge you to read the excellent article from David J Snowden and Mary E Boone on, “A Leaders Framework for Decision Making”. Using the Cynefin framework, motorcycle maintenance or auto repair might fall under the complicated context. In order to fix the car, the expert technician, after having listened to how the collision happened, also has to analyze and test multiple factors, such as unseen damage, frame damage, etc., to determine the best course of action for the car. However, in a complex context, you can only decide whether or not it is right or wrong in hindsight, i.e. after you've tried the thing. Does this jive more and more with complex legacy modernization software projects? I.e. you try something and learn that the integration you thought would work doesn’t work or you uncover a completely new behavior you never knew existed and you now have to account for it? There is no fixed path when modernizing a complex legacy system. There is no rulebook to follow. You try, experiment, discover, solve, and move on to the next piece until you're done. Denial - Anger - Bargaining - Depression - Acceptance?? When modernizing applications, if we understand that this effort falls somewhere between Complex and Complicated Contexts, we can implement the right sort of gauges to see how to proceed and determine success. Applying the process of estimates for a complex context is wrong. It’s like trying to use a hammer as a solution for every screw or a lug nut! Sometimes you need a screwdriver and sometimes you need a wrench! Curveballs in modernization projects are just a reality. You can't foresee every outcome ahead of time. No amount of upfront analysis would lead you to a perfect data model. There will be new learnings almost every step of the way. Data Models will have to change based on the complexity you have uncovered. So it's a matter of when, not if, for curveballs. When a curveball hits and the estimate changes, take a step forward instead of getting angry or blaming folks or trying to do a what-if analysis on how best to bring the schedule in. When you’re dealing with curveballs, which Ron Westrum’s defined model of culture does your organization adopt? Power-Oriented where messengers who inform about the curveballs are shot and failure leads to scapegoated, Rule-Oriented where the messengers who inform about the curveballs are ignored and failure leads to justice or Performance- Oriented where the messengers who inform about the curveballs are trained and failure leads to enquiry? As long as the problem you're trying to solve is still relevant and still meets the business needs, take it one step at a time. Ultimately the software you are trying to modernize is for users. Tip for Leaders Leading Modernization Initiatives To quote from the same article, \"Leaders who don't recognize that a complex domain requires a more experimental mode of management may become impatient when they don't seem to be achieving the results they were aiming for. They may also find it difficult to tolerate failure, an essential aspect of experimental understanding. If they try to overcontrol the organization, they will preempt the opportunity for informative patterns to emerge. Leaders who try to impose order in a complex context will fail. Still, those who set the stage, step back a bit, allow patterns to emerge, and determine which ones are desirable will succeed\". A New Hope Either way I will be ok, whether the repair company is able to fix my car or the insurance company deems it a total loss. The system works. I wish I could say the same for modernization projects and the awful industry practice about how much ceremony we attribute to estimates, completely forgetting that an estimate means an approximate of the actual. I hope more and more software companies and leadership use the proper framework for measuring success. I hope this leaves you with some ideas on implementing changes or at least questioning the practices if they seem wrong. \"I've wondered why it took us so long to catch on. We saw it, and yet we didn't see it. Or rather we were trained not to see it. Conned perhaps into thinking that the real action was metropolitan and all this was just boring hinterland. It was a puzzling thing. The truth knocks on the door and you say, \"Go away. I'm looking for the truth.\" And so it goes away. Puzzling.\" ― Robert Pirsig Thanks for reading DomainAnalysis.io! Subscribe for free to receive new posts and support my work. Subscribe 7 Share this post Architecture Modernization Execution: When did estimates turn into deadlines? domainanalysis.io Copy link Facebook Email Note Other 1 1 Share",
    "commentLink": "https://news.ycombinator.com/item?id=42187506",
    "commentBody": "When did estimates turn into deadlines? (domainanalysis.io)282 points by alexzeitler 22 hours agohidepastfavorite207 comments redleggedfrog 20 hours agoI've gone through times when management would treat estimates as deadlines, and were deaf to any sort of reason about why it could be otherwise, like the usual thing of them changing the specification repeatedly. So when those times have occurred I've (we've more accurately) adopted what I refer to the \"deer in the headlights\" response to just about anything non-trivial. \"Hoo boy, that could be doozy. I think someone on the team needs to take an hour or so and figure out what this is really going to take.\" Then you'll get asked to \"ballpark it\" because that's what managers do, and they get a number that makes them rise up in their chair, and yes, that is the number they remember. And then you do your hour of due diligence, and try your best not to actually give any other number than the ballpark at any time, and then you get it done \"ahead of time\" and look good. Now, I've had good managers who totally didn't need this strategy, and I loved 'em to death. But for the other numbnuts who can't be bothered to learn their career skills, they get the whites of my eyes. Also, just made meetings a lot more fun. reply bigiain 19 hours agoparent> I've gone through times when management would treat estimates as deadlines, and were deaf to any sort of reason about why it could be otherwise, like the usual thing of them changing the specification repeatedly. I worked at a place where this management insanity was endemic, which lead to everyone padding all estimates with enough contingency to account for that. Which lad to the design team, and the front-end team, and the backend team, and the QA team, all padding out their estimates by 150 or 200% - to avoid the blame storms they'd seen for missing \"deadlines\". Then the Project managers added those all ups and added 150 - 200%. Then the account managers and sales teams added 150 - 200% to the estimated costs before adding margins and setting prices. Which ended up in literally around 1 million dollars a month to maintain a website which could _easily_ have been handled by a full time team of 8 or 10 decent web and full stack devs. Hell, apart from the 24x7 support requirement, I reckon I know a few great Rails or Django devs who could have done all the work on their own, perhaps with a part time of contracted graphic designer. That all lasted a handful of years, until the client worked out what was going on, and my company management flew the whole thing into the mountain, with ~100 people losing their jobs and their owed entitlements (I was out about $26K that day.) reply ethbr1 18 hours agorootparentThis is literally the endgame. And the only cure is instead building a company that's tolerant of mistakes while still aspiring to excellence. The one I've worked at which got the closest had a corporate culture that failures were atrributable to processes, while successes were atrributable to individuals/teams. Of course that had its own negative side effects, but on the whole it made the company a lot more honest with itself. And consequently got better work out of everyone. reply veunes 8 hours agorootparentTo shift the focus from blame to improvement is critical for fostering innovation reply likium 17 hours agorootparentprevJust curious if the processes got tuned/adjusted as a result? And what were the negative side effects? reply ethbr1 16 hours agorootparentAbsolutely! That was one of the common productive outcomes: this policy / approach is screwed up, and we could do it better. Negative side effects were about what you'd imagine. Some low performers unjustly shielded themselves. Safeguards were overbuilt as proof \"something\" was changed to prevent a failure repeat. Executive promotion criteria could get squirrelly. Etc. But on the whole, I think the individual/team productivity boost and agility created by honesty was a huge net win. reply heelix 5 hours agorootparentThe first time I'd seen a blameless post mortem, I thought it was a load of bs, as another organization had just caused the first significant production outage our app had ever had. Convenient... no blame. We went along with the process and it did not take very long to understand how this changed the culture. If someone horked a step on a manual deploy, the real question is why is this not automated. People stopped hiding mistakes - so the old snipe hunts where information to trouble shoot might 'disappear' faded and made it easier to debug and then figure out what could be done better. It helped the business understand that 'running in production' did not mean done. Ryan, if you are out there reading this - ty. reply ethbr1 2 hours agorootparent100%. It was an eye opening experience. Felt somewhat akin to running an RCA on \"Why do people hide mistakes?\" Well, because it's in their self interest to do so! reply MichaelZuo 2 hours agorootparentprevWouldn’t a hybrid system make more sense? To only assign blame to people/teams when they’ve guaranteed in writing that it would be so and so, avoiding the downsides. And blaming the process when there were no such guarantees? reply ethbr1 2 hours agorootparentThe issue with any hybrid system is you have to play the incentives out at scale. E.g. if blame is assigned when there's a written guarantee, why would anyone ever make a written guarantee? And not trying to be obtuse, but I've only ever seen blameless cultures work in absolute. Compromises let back in all the nasty mal-incentives you see driving unproductive CYA behaviors. reply okeuro49 4 hours agorootparentprev> padding out their estimates by 150 or 200% - to avoid the blame storms they'd seen for missing \"deadlines\". This is good advice, as devs tend to underestimate. reply TaurenHunter 15 hours agorootparentprevThat is probably analogous to what happens in the American healthcare sector with physicians/hospitals/insurance carriers/pharma/etc. Each one padding their bills making it horrendously expensive for everyone at the end of the chain. reply timy2shoes 12 hours agorootparentThe padding in healthcare is part of the system. One part is to have high prices so insurance can negotiate them down. And for hospitals in particular, prices are padded to subsidize emergency care for the indigent (which they have to provide without regard to ability to pay; thanks Reagan). reply oefnak 12 hours agorootparentHow can you not be grateful for that? You don't have money, so you should die? Is that really what you mean? reply jprete 6 hours agorootparentThat's a hyperbolic misstatement of the situation on the ground. Poor people use free emergency rooms as primary care instead of paying for primary care physicians. That's a cost disaster no matter what you think should be done about health care. We'd be much better off with actually free primary care for the poor, and it would at least make sense to prevent the emergency room misuse since it's so wasteful. But it's politically untenable in the US to fix a broken system in any direction people don't like, even when it's Pareto optimal. reply skeeter2020 4 hours agorootparentThis is the story in Canada as well, but way more than the very poor, because there are not enough primary care physicians where needed, and not enough people pursue family medicine. Why would you? What med student looks at the prospect of administering a dinky small business on top of actually practicing medicine, pay well but not great, and have zero equity when they retire? So we land in a similar position because the change might be publicly funded group practices instead of pay per service which has better optics. reply MichaelZuo 2 hours agorootparentYeah it seems partial privatization is inevitable or at least the default outcome, at least in Ontario. No other way out that’s also politically viable to enact. reply rickydroll 1 hour agorootparentprevSee \"We've got you covered\" for an analysis of reallocating current US healthcare spending into a general healthcare program that aligns with your thinking. https://www.penguinrandomhouse.com/books/690632/weve-got-you... reply sobkas 8 hours agorootparentprevHow about government is paying for treatment of people too poor to pay themselves and everyone is paying their share to finance that spending? And as a bonus everyone else will also get their medical treatment financed this way? reply euroderf 9 hours agorootparentprev> One part is to have high prices so insurance can negotiate them down. One basic truism in business is that \"Everybody wants a discount\". reply ykonstant 9 hours agorootparentprevDid... did you just chide Reagan because his healthcare policy was not sociopathic enough? I'll admit, that's new. Impressive. reply takemetoearth 12 hours agorootparentprevYeah, it would certainly be cheaper if those uppity poors just died instead. reply genghisjahn 18 hours agoparentprevWhat helped me was to track Sprint Volatility in addition to Sprint Velocity. We had our over all capacity, let's say 40 points and that would go up or down some based on people leaving the team, joining the team, etc. It's just an average of how much a team can get done in a given sprint. Velocity was gauged as points per person per day. Volatility is how much the sprint changes. Sure you can pull one 5 pt ticket out and add in a 3 point and 2 point, but if you do that 12 times in a two week sprint, we will not finish the sprint even if total capacity stays under 40 points. I would snapshot the sprint each day, so each day I could see how many tickets got removed/added. The end result being I could show my manager, look, when volatility is low, we almost always finish the sprint. When the volatility is high, we don't, it doesn't matter if we are over/under velocity because we don't have the time to properly plan and get clarity on asks. Have our product team think more than two weeks out and we'll deliver. That worked to a degree. reply Izkata 15 hours agorootparent> Volatility is how much the sprint changes. Sure you can pull one 5 pt ticket out and add in a 3 point and 2 point, but if you do that 12 times in a two week sprint, we will not finish the sprint even if total capacity stays under 40 points. Isn't the entire point of a sprint that, once planning at the start of the sprint is over, you can't change what's in it by reprioritizing? All of product's reprioritizing should be in the backlog, not the sprint, and only affect what the next sprint is going to be, not the current one. reply skeeter2020 4 hours agorootparent...which is one of my major (of many) complaints about all \"scaled agile\" frameworks: the promise is that you put all this planning effort in we won't blow up your plan for the next 4/6/n sprints; you get stability and predictability, and can actually shave a yak or boil the ocean, things that are hard to progress with just a 2 week window. Who has ever seen an org with the discipline to NOT do this? Executives are rewarded for the \"action imperative\" and quick course corrections. They want big, bankable delivery dates AND agile responsiveness to fundamental goal changes. Good luck. reply wolpoli 11 hours agorootparentprevIn official scrum, the development team could choose to accept substitution. It looks like the GP's case, they are obligated to accept substitution. reply genghisjahn 7 hours agorootparentAfter I presented the volatility findings, that changed. Sprints largely had to stay as they were and the team decided what we’d take on after the sprint started. It got better. But the whole sprint/scrum/agile thing is still weird. It can be helpful but in a large org it’s never easy. I hope for further improvement in the area of scheduling/estimation in software development. reply chipdart 12 hours agorootparentprev> What helped me was to track Sprint Volatility in addition to Sprint Velocity. I dread to imagine the number of bike shedding meetings and planning poker it takes to change a 5 to a 3 or 7. reply nunez 17 hours agorootparentprevJitter in agile; I love it. reply aoeusnth1 19 hours agoparentprevIn my experience, super large estimates don’t make you look good in the long run, they make you look incompetent. The engineers who are most likely to be under-performers are also those who give super inflated estimates for simple tasks. Maybe this is a good strategy for dealing with people who aren’t going to judge you for delivering slowly, or for managers who don’t know what the fuck is going on. For managers who do, they will see right through this. reply eminent101 19 hours agorootparentSo many bold claims in this comment and little to no justification. For what it's worth I've seen pretty much the opposite. I don't know about competent vs. incompetent engineers. But when it comes to experience, I've seen the inexperienced ones giving super low estimates and the experienced people giving larger estimates. reply koyote 17 hours agorootparent> I've seen the inexperienced ones giving super low estimates and the experienced people giving larger estimates I have the same anecdotal experience with a possible explanation: Inexperienced engineers often don't see the greater picture or the kind of edge cases that will probably need to be handled ahead of time. I've often had the following type of conversation: Engineer: \"I think that would be a day's work\" Me: \"This will need to interact with team X's package. Have you accounted for time spent interacting with them?\" Engineer: \"Oh no, I guess two days then\" Me: \"Will this account for edge case 1 and 2?\" Engineer: \"Ah yes, I guess it would be three days then\" Me: \"Testing?\" Engineer: \"Maybe let's say a week?\" On the other hand experienced devs might have their judgement clouded by past events: \"Last time we did something with X it blew out by 3 months\" - Ignoring the fact that X is now a solved issue reply roenxi 15 hours agorootparent> \"Last time we did something with X it blew out by 3 months\" - Ignoring the fact that X is now a solved issue This is software though, if X has actually been done before then it doesn't need to be done again. It is already done. Task X clearly had the potential to blow out by 3 months, and they are now working on task Y that is similar to X. It is a reasonable position to assume that there are other as-yet-unknown issues that might cause it to blow out by 3 months until someone has demonstrated that all the unknown unknowns are also resolved by doing it quickly. That is just basic evidence based planning. reply lesuorac 14 hours agorootparentI've always found that finding a similar scope problem and how long it took is the best predictor of how long the new problem is going to take. reply c0balt 18 hours agorootparentprevIme, as a junior dev/ops person, there is almost always scope creep and adding padding grants you room to account for the new idea your supervisor/ user thought of when being midway into development. As far as I can tell, my supervisor also assumes my estimates should be padded more because sometimes you might need wait on human i/o for longer than planned (holidays/ sick leave/...). reply skeeter2020 3 hours agorootparentone thing that I like, that can help, is to add explicit things in the spec that it will NOT do. If you keep this \"types\" of functionality you can shut down a lot of scope creep: \"we need to send an email alert after the job is done.\" gets answered \"we can do that in a future iteration because this says the feature will not include any alerting or notifications, just log to a file and finish\". reply taurath 18 hours agorootparentprevA big problem when you're a more experienced engineer is when you have your hands in a lot of things and know the relative priority of stuff and how likely it is that something else of importance will pop up. So you anticipate things getting sidetracked over time, and try to make a bit of a longer estimate, usually to give yourself the slack to do other important things without looking like you're falling behind in JIRA. Giving an \"if I had nothing else going on\" estimate can be a big trap to fall into - they will only see the number and judge your performance based on that. This dovetails into the problem that untracked but still important work being thankless in low trust environments - not all work can ever be tracked, or else the time to track that work would take as long as doing the work. Examples: literally any emotional labor, time to monitor, time to train, time to document when its not explicitly required, time to solve little problems. In the environment where none of this counts because its not quantifiable, everyone with knowledge makes themselves into a silo in order to protect perceptions of their performance, and everyone else suffers. I'll go even a little further to say that companies that attempt to have no untracked work are by nature far more sociopathic - thus far there's basically no consequences for sociopathic organizations but I hope one day there will be. reply bdangubic 18 hours agorootparentprevI think general problem on HN is that you can't say something \"bold\" without people going \"nuts\" - especially when it comes to estimating work. In my experience (been hacking since the '90's before it was cool) great developers are great at estimating things. And these are not outliers, all except 1 great developer I've had pleasure of working with over these years has never been \"off\" on estimates by any statistically significant margin. but you say anything like that here on HN and it is heresy. My general opinion is that developers LOVE making everyone believe that software development is somehow \"special\" from many other \"industries\" for the lack of a better word and that we simply cannot give you accurate estimates that you can use to make \"deadlines\" (or better said project plans). and yet most developers (including ones raising hell here and downvoting any comment contrary to \"popular belief\") are basically doing sht that's been done million times before, CRUD here, form there, notification there, event there etc... It is not like we are all going \"oh sht, I wonder how long it'll take to create a sign-up form.\" I think we have (so far) been successful at running this \"scam\" whereby \"we just can't accurately estimate it\" because of course it is super advantageous to us. and WFH has made this even worse in my opinion - given that we can't provide \"accurate estimates\" now we can simply pad them (who dare to question this, it is just an estimate, right? can't hold me to the estimate...) and then chill with our wifes and dogs when sh*t is done 6 weeks earlier :) reply mewpmewp2 18 hours agorootparentIt really depends on what you are working on. When I did agency type of work, building things you have built before, from scratch, it's easy to estimate. E.g. some sort of e-commerce website from scratch. On the other hand, working in a large corp, with massive legacy systems, unknown domain knowledge and dependency on other teams, it becomes near impossible. What might have been a 2 hour task in agency, might either be completely impossible during our lifetime unless the whole company was built from scratch or take 2 years. You might need 6 other teams to agree to make some sort of change, and you first might have to convince those teams, and then 2 teams will initially agree to it, then pull out in the last moment, or realize they can't do it. reply bdangubic 16 hours agorootparentIn your 2nd example of a large corp I can see myself joining and not knowing anything about all these potential landmines etc… but if I was at that corp for say 2-3 years those would all be known things, no? And hence estimates can come with number of assumptions based on 6 other teams doing whatever it is that they need to do etc…? But I totally agree with your point and would not be disappointed if someone missed an estimate in that sort of chaos. My comment was not really geared towards chaotic organizations but general sense (especially in myriad threads on this forum) that SWEs think that somehow our profession is so “special” that we cannot possible know how long something will take. I simply do not accept that and personally believe that more likely vast majority of SWEs simply suck at their job. there are roughly 4.5 million of us, how many are really good at their jobs (big part of which is estimation)? probably like 0.4% :) reply WOTERMEON 9 hours agorootparentI agree with the general sentiment (estimation is part of the work, ppl are not good at estimates, ppl are not good at their job) I also think that in high churn rate companies, or where team get created and disbanded every two quarters, it’s quite difficult to have a mental model on other teams ability to deliver a dependency for your team. And this situation I find quite common tbh reply redleggedfrog 16 hours agorootparentprevYou can't make accurate estimates that can be used for deadlines for non-trivial work. You can make educated guesses on how long specific things will take, and it might be a pretty good guess if you've been keeping metrics on your past work, including things like vacation days and other similar disruptions as well, and keeping a team together long enough to have solid institutional knowledge on your code-base. And you can respectfully lay these numbers out to a manager in the form of, \"This will probably take 1 to 3 days,\" or for bigger stuff, \"2 to 3 weeks\", and so on. And the manager can take the sum of all this and say, \"The soonest it can probably be done is 3 months, but most likely it'll be 4, with a small chance of a bit longer\", or whatever, you get the idea. And then the mangers can set the deadline as they see fit. Now, for some reason, many managers just look at the first number and are done - that's the due date. And then after that they get deer in the headlights treatment, so the worst case becomes the best case. That's on them. If they don't understand that software estimation isn't an exact science they're in the wrong field. As for software development being special, I really hope that what I've described above is like other engineering disciplines, and we're not special. I don't want to be special, I want to be an engineer, like those who work with aircraft or bridges and what not. I feel like in those fields the concept of estimation is a little more respected. But I'm probably wrong. :^) I'll mention I've been a professional software developer since the early 90's, not that experience equals veracity. But I've had good success using the system above, and even though the bad managers to good managers was pretty even during that time, the company experienced outstanding success during my tenure (20 years!). In the end, bad managers never last. Good managers, who take reasonable estimates to their superiors, succeeded, where managers who brought \"It'll be done July 1st\" got doubted because their superiors know it really doesn't work that way. reply bdangubic 13 hours agorootparentyour comment is exactly what I am talking about. you actually CAN make accurate estimates for non-trivial works. try to envision this - I hand you a non-trivial assignment to estimate with a condition that if you meet your estimate -/+ 5% you get 7-figure bonus. alternatively if you do not you get fired. after working 30 years in the biz you tell me which of the two is happening for you? I worked at two places that gave huge bonuses when deadlines were met (based on “estimates”) and wouldn’t you know it sh*t always got done on time and people got paid. reply leetcrew 12 hours agorootparentin the short term, people will work crazy hours to hit a date if it's the difference between a 7 figure bonus and getting fired. if the estimate is based on devs working reasonable hours, that's a lot of slack built in. I'm sure they hit the date more often than not in your scenario, provided they control most of the dependencies, but it's not a sustainable approach for delivering features. \"provided they control most of the dependencies\" is a pretty important caveat by the way. many times I've seen people get the rug pulled out from under them by partner teams at the last second. it doesn't matter how clever you are or how hard you work. if you depend on something owned by a team far away in the org chart, they can always blow up your project with little consequence. reply bdangubic 8 hours agorootparentI honestly do not think it is about working crazy hours. perhaps my example can be misunderstood in a sense that if you give someone 7-figure bonus they will inevitably work 20hrs/day if necessary to get there which of course would not mean that they estimated correctly but were off by 12hrs/day :) as things stand what is my incentive to provide an accurate estimate? if no one can question my estimate and hold me to it (well perhaps they can question it but we as industry have successfully been able to convince everyone that these are just estimates, nothing else...) what is my incentive to be accurate? If like one of the commenters above can say \"it'll be 2 to 3 weeks\" there is an INSANE difference between 2 and 3 weeks, 33% difference. it's like coming to buy a house and agent says \"this house is $200k or $300k but you sign here on the bunch of dotted lines and we'll tell you all about it eventually before you have to cut a check.\" It is good to be in this industry (and especially if you WFH) - say 2 to 3 weeks, finish in 2 and get a week of working on your wellness (or another job :) ) reply yetihehe 5 hours agorootparent> it's like coming to buy a house and agent says \"this house is $200k or $300k but you sign here on the bunch of dotted lines and we'll tell you all about it eventually before you have to cut a check. Not really with price, but when I've had my house built, the date was overshoot by about 30% too, because of various reasons, like having to stop for winter because some supplies were late by a week several times or my builders had to help teams at other places from time to time (because other teams were late too), not doing anything at my house sometimes for days. So even when building homes (something they do again and again) you can't really put exact estimates. reply redleggedfrog 10 minutes agorootparentprevThat's an interesting scenario you're proposing. To answer it personally, which of the two, 7 figure bonus or being fired, it'd be I'd quit. If someone is structuring the development of software based on this premise, then they are going to need a different kind of person than me. But I admit I'm probably an outlier here. I don't really work for the money, and my salary is enough, and I don't like undo pressure. For arguments sake let's say the 7 figures is $1,000,000. To offer that kind of bonus the project is likely going to be a larger one. And I'm assuming my estimate is determining the deadline, so of course I'm making sure it's something I think I can achieve. But then there are other significant problems with this structure and the likelihood of meeting the deadline, and, more importantly, generating good code and user experience. - +5% (in ignoring the -5% as no one cares if you're early unless it creates some sort of QA burden) implies a narrow window. On a 6 month project that is ~6 days. Enough that personnel changes or other uncontrollable factors could lead to a missed deadline. One person getting fed up and leaving would be a huge problem. - The specification would have to be really clear and agreed upon, since there is much at stake. - Any changes, scope creep, customer requests, could change the development time, and you'd have to have some sort negotiating buffer built in since there is now so much at stake. Otherwise you're going to get literally everything rejected by the developer as they drive towards the deadline (maybe that's what you want, though). - Is the result worth having? A focus on a deadline, in my experience, tends to shortchange quality. But maybe the deadline is more important than quality. - And lastly, if that deadline is missed, or worse, something changes the scope of the project and the bonus is not awarded because that led to the deadline being missed, you're going to have some super pissed developers that will not trust such an arrangement in the future. I suspect you're talking about situations beyond my pay-grade. I've been a meat and potatoes programmer working in e-commerce and integrations mostly, and we don't see 7 figure bonuses. We certainly have had can't miss deadlines that we mostly didn't miss, but mostly those deadlines were due to external factors (API deprecation mostly), or financial considerations, or lastly, arbitrary deadlines set by management. On the latter, those mostly got missed. But that was to be expected as they were not tied to reality. And I'd second leetcrews comment below of, \"...but it's not a sustainable approach for delivering features\". Maybe this scenario works once or twice, but it seems like a terrible way to develop software. But still, an interesting thought experiment. reply gjvc 19 hours agorootparentprevBut when it comes to experience, I've seen the inexperienced ones giving super low estimates and the experienced people giving larger estimates is the essence of this thread reply tivert 19 hours agorootparentprev> Maybe this is a good strategy for dealing with people who aren’t going to judge you for delivering slowly, or for managers who don’t know what the fuck is going on. For managers who do, they will see right through this. I think a manager who doesn't know the difference between and estimate and a deadline is one who \"[doesn't] know what the fuck is going on,\" and that's the kind of manager the GP uses this strategy with. reply jjk166 1 hour agorootparentThe big issue is when a manager knows the difficulty of the task but not the context it's being done in. A project may be perfectly reasonable to complete in 4 weeks if it's given the priority it deserves, but I know that I'm almost certainly going to get pulled off to do something else so it's going to wind up taking 12 weeks, and then with a very moderate 33% padding giving an overall length of time of 16 weeks, the manager (who has no visibility to the thing which will pull me away) thinks I'm adding 300% padding. Then they say \"surely you can do it in less time if we just don't let you get pulled away\" and of course you say \"well I've been pulled away from all of the past 27 projects over the last 5 years\" and they say \"don't worry I'll make sure this time is different.\" It's not a lack of technical competence, it's a lack of introspection and managerial soft skills. reply tonyedgecombe 5 hours agorootparentprevI think a lot of them know the difference, they just don't care. The estimate is a tool to beat you with. reply DennisP 4 hours agorootparentprevI wouldn't advocate \"super-inflated\" estimates but within reason, there are long-term benefits if you go about it right. Where I mostly worked, managers cared about deadlines they could tell to external clients, which they really hated to miss. Early on, I didn't realize that, and gave my best guess. If I guessed the correct median, I was missing it 50% of the time, and managers kept getting mad at me. So I switched to estimates I could meet 90% of the time, and on the slow 10% I worked extra hours to meet my estimate anyway. Managers were happy. If I told them it would be done by Tuesday, it would be done by Tuesday. But it had enormous benefits beyond that. In almost 90% of cases, I had free time. Sometimes I'd admit to finishing early, but I also used that time to clean up technical debt, automate the tedious parts of my job, or advance my skills. After a while, I could give estimates as short as my old 50% estimates, and still beat them 90% of the time because I'd made my tasks so much easier. Less technical debt also meant the resulting code was less likely to have bugs. After a while, it seemed to me that all the other devs were overworked and I had it easy. But management gave me raises, and when they got in a jam, I was the guy they called on to bail them out. reply mewpmewp2 18 hours agorootparentprevI've been considered high performer everywhere I went, only when I was beginning I usually gave very low and naive estimates, experience has taught me otherwise. Of course it will also depend on who and why I'm giving those estimations to. Usually there are just too many unknowns that higher estimate is justified to avoid having to explain why you didn't make it by certain deadline. The estimates I give are not median or average that I expected the task to complete, they are so that I can be 95% sure it's possible to do it and then some. reply rightbyte 8 hours agorootparentYe and this is the problem with management using estimates as deadline. When I was naive and believed that Agile was not a sinister micromanagement toolkit to mess with programmers, I tried to explain to people that about half of our estimates should overshoot and half undershoot or they are biased and that there should be more overshoots since there is no upper bound on how much time a task can take if the estimate is wrong. Ye. No. The burndown chart shouls be as straight as possible. reply mewpmewp2 5 hours agorootparentYeah, and even if it is not being done as of moment, there is always a possibility of someone clueless from leadership deciding it is a good idea to check how many story points you have completed by some rough statistical analysis, in which case people who put higher estimates and completed those tickets will look better. reply rightbyte 4 hours agorootparentYe. The manager need to be a programmer and involved in the project to be able to evaluate the participants. I guess 'estimation poker' is a way to counteract the obvious strategy to coast and look competent. In poker you can also look good by underbidding your peers and then snatch the easy ones to look good while the scapegoats look bad. The strategy need some social status or incubent code knowledge relative to the team though, to get the good tasks. reply mewpmewp2 40 minutes agorootparentI have thought about how it would be fun to have something where people will either openly or blindly estimate and bid. In practice I might be concerned about few things like introducing too much of a competitive culture within the team. Or it could lead to a place where people get too specialized and knowledge doesn't spread around, since everyone will bid on things they have experience with, and so they will be the only one with that experience, which might hurt in the long run. I couldn't actually imagine doing it in my current team. I think people are diligent anyway, and already work more hours than usually would be required. I find it better to just try to protect each other within the team, to drive everyone making higher estimates. Also doing bidding for those estimates in addition could mean that there might be strong incentives for a lot of corner cutting for certain tasks, etc. People will value short term gains over long term gains when there's such pressure. reply Lanolderen 4 hours agorootparentprevI'm a junior and practically refuse to give estimates currently because the projects I currently get have no real requirements. \"We'd like to replace an excel table for some calculations with a dialog. Here's the template, how long do you need?\" which sounds simple enough turns into: 1- Decypher what the example excel template developed by someone over 10 years even does. 2- Oh, there are actually 10 templates and manual actions that give the end result. 2.5- Oh, btw, we asked an external company about doing this for us a while back and they wanted 1kk euros, crazy right? 3- Oh, we also need to generate, send and track offers via the app with the ability to add comments and upload files related to the offer. We also want the user to be queried about what data he has on hand so that calculations he cannot complete are not offered/he's notified as to what else he needs to proceed. 4- Oh, we also need change tracking/audit logs for everything. 5- Oh, we also need to get data from this place, find a free API and also a way to get data out of this software here. In comparison to that at my previous job the tasks were way smaller and clearer so I'd essentially give myself deadlines when talking to my manager by saying X and Y should be done by Z, A by B. The only thing I can think of in this situation is to essentially make internal pseudo contracts regarding requirements but then I'm making a pseudo contract with someone 3 levels in the hierarchy above me who's also the person who can terminate me. It's not like that pseudo contract will be read by anyone besides us so it seems better to display lots of uncertainty. At least if you're senior you have more authority in discussion and don't really have to give a fuck since everyone's looking to hire senior devs + your downgrade is a normal dev position. From junior the downgrade seems to be testing or McDonalds and you get to redo junior. reply Moru 19 hours agorootparentprevIt is however a logical follow up of the managers behaviour. If they try to hold the estimate as deadline, the estimate will be larger next time. If manager doesn't see this coming, the manager needs to work on some people skills. reply ebiester 19 hours agorootparentprevThe managers are often just as trained in this by the organization. If I'm in a \"commitment\" organization, I'm sure as hell telling them all to pad their estimates. The punishment for a commitment culture is inflated estimates. reply switchbak 18 hours agorootparentprevIt's exactly this kind of prideful ego centric attitude that these managers rely on to get folks to commit to unrealistic estimates, then work nights and weekends (and cut corners) to fulfil. reply qaq 17 hours agorootparentprevThis can only hold water in reasonably small orgs. In large orgs you often have to coordinate with large number of teams to get something delivered. Those teams have changing priorities that can impact when they complete their tasks. Your teams priorities can be shifted too to fight some fire. So this small estimate has no value because it has 0 correlation to the overall delivery date higher ups can commit to for the overall project. reply watwut 10 hours agorootparentprev> The engineers who are most likely to be under-performers are also those who give super inflated estimates for simple tasks. Definitely did not seen this. Under performers are underestimating or just do wild random guesses. Under performance is most likely to be in the form of \"making small estimate, try to make it technically, but then it has about millions of problems\". Big estimates require courage and confidence - under performers usually do not have either. They are too scared to estimate high. reply Aeolun 18 hours agorootparentprev> In my experience, super large estimates don’t make you look good in the long run, they make you look incompetent. The manager need to know how to make the estimate to know is bullshit. If the manager knows how to make the estimate and what it represents, there is no need to inflate it for them. reply disambiguation 12 hours agorootparentprevYeah but missing estimates makes you look super duper incompetent by comparison. reply Etheryte 18 hours agoparentprevThe hallmark of a bad manager who doesn't know they're a bad manager: \"Why can't you just give me a number?\" Inexperienced managers or people backfilling for someone else I can completely understand, they're not comfortable with the uncertainty they're dealing with. However in any other circumstance I think it's inexcusable. reply trashtester 10 hours agorootparentHere is an approach for estimation that works pretty well (from the point of view of a manager). 1. Ask the dev team to provide an optimistic estimate, and to then multiply by 2 to make it \"realistic\". 2. On top of that, add another x2 (which can be recalibrated as you learn how accurate this tech team is over time with estimates). Don't tell the developers about this, but make sure your higher-ups understand that this is what they need to be prepared for in terms of budgeting and time limits. The reason you don't ask the developers to multiply by 4 directly, is to keep them motivated to aim for the x2, and avoid slacking or over engineering while feeling overly comfortable early on. But by having the extra x2 in reserve, your back is covered, and you can afford to be cheritable with the dev team as they (as usually happens) go a bit over the x2 estimate. This buys some early goodwill that can later be traded back in if you need them to up their game later on. The alternative to the above is to exclusively find managers (at all levels) that can combine manager skills with high level engineering skills. Such managers often have the ability to expose unneccery delays directly, which includes the ability to tell apart delays caused by devs slacking from incompetence, scope creep or unexpected but valid causes. Such people are really hard to find, though, for most companies. But companies that manage to build such high level top to bottom tech lead cultures may certainly be able to go from the 4x back down to 2x or even 1x compared to companies with non-technical managers. reply ignoramous 5 hours agorootparent> Ask the dev team to provide an optimistic estimate, and to then multiply by 2 to make it \"realistic\". On top of that, add another x2 ... Reminds me of: Always Multiply Your Estimates by π (2021), https://news.ycombinator.com/item?id=28667174 > ... scope creep ... If you want to know what Tesla does right and most of us do wrong, it's this: they ship something small, as fast as they can. Then they listen. Then they make a decision. Then they stick to it. And repeat. They don't make decisions any better than we do. That's key. It's not the quality of the decisions that matters. Well, I mean, all else being equal, higher quality decisions are better. But even if your decisions aren't optimal, sticking to them, unless you're completely wrong, usually works better than changing them. An epic treatise on scheduling, bug tracking, and triage (2017), https://apenwarr.ca/log/20171213 reply jimmydddd 18 hours agorootparentprevBut you have to remember that the manager is going to be asked for an estimate by his boss. He can't just say some time between \"1 day and 10 years.\" In the real world, you have to be able to give some sort of estimate and help the poor guy do his job. reply xedrac 18 hours agorootparentAnd thus we get to the root of the problem. As as business executive, why not simply track how long your big projects tend to take, rather than try and dictate how long they should take? reply mewpmewp2 18 hours agorootparentHow can you tell what is worth doing if you don't know how long it might take? reply zelphirkalt 18 hours agorootparentYou make projections instead of estimates. You split the work that needs to be done into many tasks and project from past experiences. You cannot rely 100% on any estimates either, and all you are doing by demanding estimates is creating stress and making people less productive. The meta work imposed by that in itself will make a project take more time, as everyone will be padding their estimates. reply mewpmewp2 18 hours agorootparentWhat is the difference between a projection and an estimate? reply zelphirkalt 6 hours agorootparentFor more info see: https://www.youtube.com/watch?v=QVBlnCTu9Ms reply Tostino 17 hours agorootparentprevWho is doing it IMO. reply tchalla 10 hours agorootparentprevYou work backwards. You decide how much time you’re willing to spend to get the worth. Then, take steps towards it with checkpoints. reply veunes 8 hours agorootparentprevManagers are often caught in the middle reply veunes 8 hours agorootparentprevThe irony is that managers who demand certainty often undermine the very trust and collaboration they need to get reliable insights reply skeeter2020 4 hours agoparentprevif you need to deal with this, you must present estimates as ranges or distributions. Management needs a value for both concrete, legit purposes like budgets and also for (still legitimate) psychological reasons like building comfort that they know what's going on and they are in control. As you mention, people will anchor on a number and that in a nutshell is how an estimate becomes a deadline. Planning and execution will refine the value right up to the point you ship with a very accurate estimate of \"how long do you think this will take?\". reply andai 18 hours agoparentprevReminds me of Hofstadter's Law: It always takes longer than you think, even when you take into account Hofstadter's Law. We could say, always say it will take longer than you think? Though by this principle, it seems that \"overestimates\" are likely to be actually accurate? Joel Spolsky wrote about his time estimation software which recorded the actual time required for completion, and then calculated for each person a factor by which their estimates were off, and this factor was consistent enough that it could be reliably used as a multiplier to improve estimation accuracy. > Most estimators get the scale wrong but the relative estimates right. Everything takes longer than expected, because the estimate didn’t account for bug fixing, committee meetings, coffee breaks, and that crazy boss who interrupts all the time. This common estimator has very consistent velocities, but they’re below 1.0. For example, {0.6, 0.5, 0.6, 0.6, 0.5, 0.6, 0.7, 0.6} https://www.joelonsoftware.com/2007/10/26/evidence-based-sch... reply ethbr1 17 hours agorootparentDoesn't the article say that for experienced developers, the scaling factor tended to be converge on an average for each individual, even if variable for any particular task? And Joel sidesteps the unknown-unknowns problem in that piece, by discussing boiling down tasks to It's beyond me why we, as a supposed engineering profession, are unable to talk about risk, probabilities, and confidence intervals. And that isn't just on managers. Because management quits listening after hearing \"3 months,\" and bad management heard \"3 months minus three weeks\" and goes \"okay 2 months it is\". reply mock-possum 19 hours agoparentprevSome of the best career advice I got was very early on at my first gig - I had a designer tell me, over a cup of sake, that I should just inflate all my estimates by 60%. 30% to cover the stuff I hadn’t thought of, 30% to cover what they hadn’t thought of. That sounded insane to me… nearly two decades later, with plenty of remote freelance and full time onsite team experience under my belt… and I fully agree. It’s always going to take significantly longer, and if you pretend it’s not, it’s going to come down on your head, like it or not. Always better to underpromise and overdeliver than the other way around. reply LeifCarrotson 20 hours agoprevThe article is about modernization projects, which have soft deadlines because ostensibly the legacy software is still running while you're developing the replacement. There's always budget pressure, and promises may have been made, and users may be hoping for new features and eliminated frustrations... but if the replacement is a day late, it really wouldn't matter much. Conversely, if you're trying to launch a space probe and the planets are no longer in the right positions for the required gravity assist, your spacecraft will not get where it needs to go. Or if you're a little $100M/yr toolmaker, and Ford asks you for a die for the 2026 F150 production line, to be delivered by March, and the contract states you owe a $20,000 per MINUTE penalty if you're late...you don't wait until February to say something surprising happened and it's not going to be ready. You don't sign on that dotted line unless you know for certain that you can do it. Ford or NASA won't bat an eye when you tell them that a quote is going to cost $XX,XXX. They won't be surprised when they give you an ECO and you say that it's going to take 3 weeks and $8,000 to deliver a part that everyone knows you can probably make by hand in 30 minutes, they know that you're hedging against those deadlines, and pricing in the acceptance phase and inspection phase and contingency plans and everything else that makes their deadline-heavy industry function. But if you tell someone at OP's modernization group that due to incomplete information you think that the 30-minute task to change the text of that button will take \"no more than 3 weeks and $8,000\" they'll laugh you out the door. Optimistic estimates get rewarded, pessimistic estimates get discouraged, accurate estimates are irrelevant, and in the end you're constantly behind schedule and no one's really surprised. reply analog31 19 hours agoparentOne technique is to run the modernization project, but use maintenance of the legacy software to keep the business going. Such maintenance could be for keeping up with hardware changes, OS upgrades, new features, and so forth. I've seen projects run in parallel like this for 10+ years. reply Tostino 17 hours agorootparentI just got done doing exactly that with my old SaaS I built / worked on for a decade. Went through a roughly 3 year rewrite process while utilizing maintenance mode on the framework I had originally decided on back in 2014 and which sadly had an \"upgrade path\" of \"you look like you could really use a full rewrite for your entire frontend\" to get on the very next major version in like 2016. I'd say the main \"use\" for utilizing their maintenance support, was the fact that they would still fix issues with browser incompatibility, security issues, etc. Like the fact that back in the day Chrome changed background tabs to no-longer respond to push notifications unless they are the active tab (after some delay)...it broke things in our app. But luckily we able to lean on the vendor for those types of issues, because there was very little my team could do to make a rewrite of a massive webapp any faster than it was already going. Glad it's done, and I am out. reply snakeyjake 20 hours agoprevIn 1505 Michelangelo gave an estimate of five years to finish the tomb for Pope Julius II. Due to small side projects like the painting of the Sistine Chapel ceiling it took around 40. Failure to meet the deadline informed by the estimate meant that the scale of the project was massively reduced because: Pope Julius II had died prior to completion, there were changes requested by the customer (both Julius and his heirs), supply chain issues, contract renegotiations, labor disputes, shortages of qualified workers, and money running out due to the long duration of the project. So, since 1505 at least? The funny thing is that the pope isn't even interred there. reply baxtr 21 hours agoprevI learned something important early in my career: the first number you put out will be remembered. Unfortunately it’s often true. People keep saying: \"but didn’t you initially say X?\" \"Sure I did, but I have new knowledge\" won't always work. A nasty side-effect is that people who are aware of this shy away from giving you numbers. reply floren 21 hours agoparent> Kirk: Mr. Scott. Have you always multiplied your repair estimates by a factor of four? > Scotty: Certainly, sir. How else can I keep my reputation as a miracle worker? reply dhosek 20 hours agorootparentThere’s a whole generation of developers who have internalized this. reply ikiris 20 hours agorootparentThere’s a whole generation of management who have caused this due to their own behavior. reply dtgriscom 15 hours agorootparentprevMy rule: list all the tasks, estimate times for each task, add up all the estimates, and multiply the results by π. If you're using unknown technology, use π^2. reply vander_elst 12 hours agoparentprevMy method is take an educated guess multiply by 2 add 1 as extra buffer and then change it to the next unit, e.g. day->week, week->month, months->quarter. So for something that it should take 1 day I'd say 3 weeks. It seems a lot but at the end there's usually so much red tape, burocracy and and technical debt that it usually ends in the latter ballpark. reply baxtr 10 hours agorootparentNice algorithm! reply darekkay 21 hours agoparentprev> the first number you put out will be remembered This is called anchoring effect, a psychological bias. reply robocat 19 hours agorootparentI think you meant \"anchoring bias\": https://www.scribbr.com/research-bias/anchoring-bias/ Anchoring effect is something related but subltley different: https://en.m.wikipedia.org/wiki/Anchoring_effect reply kgwgk 17 hours agorootparentIs it? If you visit https://www.wikipedia.org/wiki/List_of_cognitive_biases#Anch... and follow the first link labeled Main article: Anchoring (cognitive bias) you may be surprised. Actually, you may also read the first link you sent and be equally surprised: Anchoring bias (also known as anchoring heuristic or anchoring effect) https://www.scribbr.com/research-bias/anchoring-bias/#what reply tyingq 20 hours agoprev\"Can you imagine if the insurance company started arguing with the repair shop, asking them—no—telling them that they would only pay the $18,000 and not the additional $20,000 because that was the original estimate? Does that sound ridiculous to you? It does to me, too. Thank heavens, reality does not operate like this.\" That happens all the time with insurance. I'm surprised at the confident tone in \"reality does not operate like this\". Not just car/home insurance either...health insurance also. They do often negotiate to a reasonable place, but not always. reply SoftTalker 17 hours agoparentIt's also why they will write off a car if the estimate is much more than 70% of the value of the car. Yes the write-off may cost them more, but it's a known upper bound and it closes the claim. Insurance companies don't like open claims. reply brookst 20 hours agoparentprevIt depends on whether we're talking about estimates or negotiated rates. In the latter (also called \"preferred rates\" for auto insurance) there's a blanket agreement that all work of a certain type is to be billed at a fixed, negotiated rate. That is VERY different from a binding estimate, which typically means a one-off estimate for a specific job where the estimator takes the risk and promises to complete the work at the rate, even if it's much more complicated than they bargained for. reply tyingq 19 hours agorootparentThere's also whether they will pay at all for some things. They have some discretion to claim some damage was existing, or unrelated to the claim, caused by the repair shop, not normally covered, etc. Or where they won't pay for OEM parts if some substitute is available. There's lots of ways they can shuffle around. reply bargainbot3k 19 hours agoparentprevAs much as I want to take a fair and balanced perspective to the article, the author works for NYTimes. They’re overpaid for what they do and their worldview is warped and pampered. It taints the article for me to approach it with both sides being biased, and the subject matter tone being, for the lack of a better word, insufferable. reply avidiax 20 hours agoprevOne trick, if you can get away with it, is to ensure that you are always estimating for a fixed scope exclusive of unknown unknowns. You should not provide an estimate for \"feature X implemented\", but rather for \"feature X engine\". If you discover additional work to be done, then you need to add \"existing code refactor\", \"feature X+Y integration\", etc. as discovered milestones. Unfortunately, you need that nomenclature and understanding to go up the chain for this to work. If someone turns your \"feature X engine\" milestone into \"feature X complete\" with the same estimate, you are screwed. ------ There is a related problem that I've seen in my career: leadership thinks that deadlines are \"motivating\". These are the same people that want to heat their home to a temperature of 72F, but set the thermostat to 80F \"so it will do it faster\". I was once in a leadership meeting, where the other participants forgot that I, lowly engineer, was invited to this meeting. Someone asked if we should accept that deadline X was very unlikely to be met, and substitute a more realistic deadline. To which the senior PM responded that \"we never move deadlines! Engineering will just take any time given to them!\" Engineering, in that case, gave the time back when I left that team. reply trashtester 10 hours agoparentSetting the thermostat to 80F WILL bring the room to 72F faster than if you set it to 72F on most ovens/AC devices, unless the thermostat is located far away from the device. Also, many engineering teams WILL take any time given to them. But instead of making estimates and plans into hard deadlines (when facing the engineers), managers can make sure the organization is ready for overruns. And as the estimated completion time approaches, they can remain reasonable understanding as long as the devs can explain what parts took longer than estimated, and why. Part of this is for the manager to make sure customers, sales and/or higher level managers also do not treat the planned completion time as a deadline. And if promises have to be made, customer facing deadlines must be significantly later than the estimated completion time. reply avidiax 1 hour agorootparent> Setting the thermostat to 80F WILL bring the room to 72F faster than if you set it to 72F on most ovens/AC devices, unless the thermostat is located far away from the device. The thermostat is meant to be far away. This isn't a valid analogy if the thermostat is measuring the temperature of the heater rather than the room. > Also, many engineering teams WILL take any time given to them. Agree, engineering teams are not single-stage heaters. They can make more progress toward the goal by working harder (in the short term), or reducing quality, or reducing scope. But holding hours/week, quality and scope equal, engineering teams aren't going to implement faster because the deadline is sooner. If there is actual slack in the schedule, they will tend to increase scope (i.e. address tech debt, quality of life improvements, plan better). It might seem that engineers take all the time given to them because most engineering orgs tend to oversubscribe engineering (which makes business sense, since engineering is expensive). reply rightbyte 7 hours agoparentprev> These are the same people that want to heat their home to a temperature of 72F, but set the thermostat to 80F \"so it will do it faster\". This usually works though in water based heating systems where the flow in the radiators is proportional to the error in temperature. In practice it might work for electrical radiators too, as the radiator wont cut off when just the air close to it is warm. reply AdieuToLogic 18 hours agoprevAfter too many iterations of providing \"some wild-ass guess\" estimates and them turning into hard deadlines, I now try to champion a No Estimates[0][1] approach with stakeholders. There is often understandable resistance to this at first. To address concerns, I find it helpful to share with stakeholders that a reasonably accurate estimate, one which could be legitimately used in planning concerns, is really only possible in one of two situations: A) the outstanding work is a carbon-copy of a previous effort, such as the second time provisioning a data center for the same system. B) the remaining new functional work is determined by the team to be in the last quartile and is well-defined, including remaining risks to successful completion. EDIT: Micro-estimates are the enabler of micro-management. A healthy team identifies the highest priority tasks to perform and does so in descending order, where priority is defined as risk to project success. 0 - https://www.youtube.com/watch?v=MhbT7EvYN0c 1 - https://www.goodreads.com/book/show/30650836-noestimates reply j1elo 18 hours agoprevThis is a fun formula that caught my eye a while ago in HN, it looks flashy and very cool. Of course, just like others do their estimations, this one is just a made up formula and without any formal validity, apart from supposedly personal experience: https://news.ycombinator.com/item?id=37965582 My estimate math: R = t × [1.1^ln(n+p) + 1.3^X] R - time it really takes. t - shortest possible time it would take without need to communicate. n - number of people working and involved during the process, both customers and developing organization. p - longest communication distance network involved in the project (typically from the lowest level developer to the end user) X - number of new tools, libraries, techniques, used in the process. Example. Project involving one developing writing code. Project would take 2 weeks (t=2), but it has 5 people (n=5) involved total, only 1 new tool (X=1) and longest communication distance is 4. 2×(1.1^ln(5+4) + 1.3^1) = 4.5 weeks. reply mjevans 17 hours agoparentThe X factor is likely correct, but needs some additional notes. Include additional X quantity for unknown unknowns, not just the known unknowns. reply softwaredoug 17 hours agoprevThe best estimation system I've seen is to actually BET on the completion date, closest gets a free lunch paid for by other members of the group. Those dates were mostly informed guesses of what would actually happen or go wrong. Importantly this was between friends. Needless to say, they turned out very accurate. reply ristos 12 hours agoparentI could see that working well for managers, if they incentivize estimates as biweekly or monthly bonuses going to the most accurate one, and each person gives a detailed estimate of ballpark plus probability of what sorts of things can go wrong and how that impacts the estimate. reply resters 18 hours agoprevOf course businesses want to be able to de-risk by having highly accurate predictions of the future. Too bad those don't exist in any domain of business planning. More often, the focus on estimating comes from management layers where incentives are not structured to reward anyone for accurate estimates, merely to punish them for missed deadlines. Time to finished is only one dimension of estimation. With any unit of engineering work there may be code debt added or removed, complexity increased or decreased, morale increased or decreased, etc. Focusing only on time, especially in a punitive way surely negatively impacts the others. reply tartoran 20 hours agoprevNot only that, in the name of efficiency where I work estimates are actively being pushed down and no spillovers are allowed. I've been in crunch like mode for over a year with no recuperation at all. I kept on hoping it will get better but it seems it's getting worse. On top of it we're all rotated to different areas of the product all the time with no recourse. Though I'm still running along I feel absolutely spent mentally... reply yoelhacks 18 hours agoprevI often see takes on this topic from the engineering side. \"It's hard!\". \"Managers just don't understand\". It feels like as a community, it would be useful to get more articles seeing things from the other side and exploring functional approaches beyond provide-a-worst-case-scenario-estimate. There's a reason this dynamic is so pervasive. In order for everyone in an organization to do their job well, people do often need a realistic set of estimates. Can sales promise the prospect this integration? Can marketing plan a launch for the new feature? Can the CPO make a reasonable bet on a new line of work? In my experience, the nuance here is more about handling the mis-estimates. How do we discuss the risks up front? How much work should we put into contingency planning? How do we handle the weeks / months before a deadline when it is clear that we need to limit scope? reply cplat 17 hours agoparentThis is it. I'm a hardcore engineer at heart who has a lot of these sales, marketing, and product folks as friends, and can attest to the fact that they also have constraints. The whole world runs on deadlines and timelines. Even a president is elected for a specific duration. If you're in a B2B setting, the customer demands (sometimes even contractually binding) at least the Quarter when something will be delivered. Time is the only common denominator by which different activities can be coordinated. Without some heed to time, there will be no coherence. reply takemetoearth 12 hours agorootparentPresidents are technically timeboxed, at least in the US. reply spjt 11 hours agoprevMy estimation technique is to completely ignore the nature of the task, and instead just try to figure out the highest number the person asking will accept. reply bberrry 11 hours agoparentFunny, their technique is to completely ignore the nature of the task and ask for the smallest possible number you will accept. reply jph 19 hours agoprevEstimates are tricky because different manager roles and different personalities bias toward totally different/incompatible concepts of what an estimate actually means. The author's article is conflating realistic and pessimistic estimates: - Realistic e.g. tech managers and people who favors agile/lean/XP/etc. - Optimistic e.g. sales managers and people who want to promote. - Pessimistic e.g. risk managers and people who need firm deadlines. - Equilabristic e.g. project managers and people doing critical chain The abbreviation is ROPE, and it turns out to work really well in practice to cover all four bases. My notes are below. Constructive criticism welcome. https://github.com/SixArm/project-management-rope-estimate reply the8472 19 hours agoparentSounds like they need curves (probability distributions), not point estimates. reply senkora 18 hours agorootparent+1. I wrote myself a script that does this with distributions based on my personal time tracking data for doing certain tasks. More concretely, I sample with replacement N times from the empirical distributions of each step, then sum the steps to get N “samples” from the total distribution. This is called bootstrapping: https://en.m.wikipedia.org/wiki/Bootstrapping_(statistics) It isn’t too hard to do, and I can confirm that it works reasonably well. reply skirmish 16 hours agorootparentprevI once tried to give interval range estimates, the manager said \"I cannot work with that, give me a single number\". (And when I gave a single number, he said \"that is much too long\" and tried to negotiate it down to 10 times shorter). Happy to be out of there. reply wglb 19 hours agoprevIn my direct personal experience, for me it was 1969 during my first major gig. I remember one project, called the March 1 system, that slowly came into being sometime that October, if my memory serves me correctly. There was tension, of course. This was exacerbated by having to work nights to get access to the system to continue development. I eventually learned, when asked for a \"quick\" estimate, I would give something drastically longer than I knew would be accepted. I said \"But I can give you a better estimate if you give me a few days to do a better plan.\" This always got me the extra time to provide an estimate. reply pseudosavant 18 hours agoprevI would say that the trend against actual agile and towards waterfall (PRDs are totally en vogue) suggests that deadlines are how most company management looks at this. Definitely not suggesting it is right, but \"really aggressive 'estimates' (that are accurate beyond human ability) you can plan on\", aka deadlines, are expected of any product/engineering org today. Tell me a story I want to believe, even if it isn't true. Then you can make it the teams' fault because they said they would and didn't. reply rogerbinns 20 hours agoprevMy technique is to give an estimate with error bars. Something like 6 weeks plus or minus 2. That then leads into discussion as to what is unknown/undefined leading to the uncertainty. Sometimes the error bars are larger than the estimate because I know there will be endless feature creep and UI revisions for something like \"analytics\". And sometimes the number could be negative like 6 weeks plus or minus 8. That is when the functionality already exists (eg the data is already there - you can already load it into Excel and a pivot table) and could be sufficient. reply dhosek 20 hours agoparentAt my very first job out of college,¹ the VP who led the division that I was part of invited me into his office (I think there was someone else from the team, but this is 34 years ago so who knows) and talked about a little game he had encountered about estimates. He gave us a list of 20 things to estimate with 95% confidence (the only one I remember was the weight of a 747), all given as a range. If we did it right, we would get 19 of the 20 correct. The point was to set the range large enough that you could be confident about the answer. I kept that as a practice when I did freelancing work, always giving my estimates as a range rather than a single number. It would be nice if agile(ish) practices incorporated this in the estimation process. ⸻ 1. Technically, my second job since I had a short temp job doing some cataloging work at a local used bookstore between leaving school and starting at that company. reply OutOfHere 20 hours agoprevWhen I give an estimate, I always reiterate that it's just an estimate that is based on limited and incomplete information, and that the real number can be more or less. If they don't like it, they're free to put someone else on the project. reply dspillett 20 hours agoprev> When did estimates turn into deadlines? In my personal experience: the first time I gave what could be construed as an official estimate. reply readthenotes1 19 hours agoparentThe deadline portion come in when someone expects to pay for the work or to pay for the opportunity cost for the work not being completed... reply dspillett 9 hours agorootparentAnd that someone usually needs to get some information to me by a certain time for my estimate to be reliable, and often doesn't. Or they need to not change the plan half way through and expect the same delivery time. Or needs to understand that when I say two tasks will take about a day each, no I can have both done by tomorrow. And do on, and so forth. reply takemetoearth 11 hours agorootparentprevAnd yet Deloitte continues to exist and overcharge and overrun deadlines, and the economy, I'm told, is still doing quite well. Maybe this is all kayfabe and it doesn't actually matter. reply disambiguation 12 hours agoprevIt's really simple. To succeed you need to look good in person and on paper. Under estimate and it can blow up in your face. Over estimate and you start to look incompetent. Fail to walk the tight rope and you'll soon be laid off. Succeed and you'll survive long enough to get laid off anyway when the next recession hits. reply magicalhippo 16 hours agoprevIn my experience, there are two different reasons for why I get asked for estimates. There's the cases where it's used to roughly schedule work, or to prioritize features. My boss wants to know roughly how much is on our plates, so he can plan for known upcoming work. Then there's the cases where it's more of a XY situation, where the boss is asking for estimates because in reality they've got a customer on the hook but they won't sign unless we can implement some functionality before go-live, or something along those lines. Typically that'll be a hard deadline, as customer will either have to switch to us or pay another year of licensing, and the boss wants to know if I can deliver. I try to suss out if it's the latter, and if I'm unsure I will simply ask why they want the estimate. If that's the case and it'll be a struggle to make the deadline, I'll try to help figure out if we can perhaps solve the core issue some other way. Perhaps a temporary solution that the client can live with for a week or two extra while we finish the proper solution, or perhaps we just simplify our proposed solution, enabling us to leverage existing infrastructure, and that turns out to be good enough for the customer. reply perrygeo 2 hours agoprevThe problem of estimates as they exist in a \"Agile\" process - they force decisions to be made when the least amount of empirical data is available. Then once work starts and information starts flowing in, you can't change your estimate. The scientific method is explicitly banned! This is often by design; data-driven decisions are incompatible with management-vibe-driven decisions. At the very least, you need to do a bit of legwork to gather data prior to giving an estimate. Call it design, call it architecture, call it research, call it proof-of-concept, I don't care. Just stop insisting that decisions be made in a vacuum of data. Real results from running code trumps everything. To be clear, you can produce software without using the scientific method. You can build anything without a data-driven process. But you get what you pay for. The head-in-the-sand approach ignores valuable information and yields poor quality as a result - it doesn't fit the definition of engineering. reply kanisae 20 hours agoprevI normally only commit to \"We will give an update in X (minutes/hours) to make sure we understand the problem first. Then start giving estimated timelines in ranges with specific call outs for updates and possible changes to the timeline. I've found that most management just want to be involved in the process and have definite times set for updates and can handle timeline changes as long as information is coming at regular intervals. reply neom 19 hours agoprevThat's cool that they went to Sokcho!! Not many people go to Sokcho but imo it's the best city in Korea, the vibes there are on point, nobody really speaks English and it's pretty dead for the most part, but I really love Sokcho for the vibes, it's maybe the place i've felt the most at peace in my life. If you ever get the chance, I recommend Sokcho. :) reply opdahl 17 hours agoparentI visited Sokcho a few months ago! I agree with you, it just has such a nice and chill vibe that makes you be at peace, and lot's of interesting history in relations to North Korea. It was also really nice to go for a bike ride around the big lake that is there. reply ahallock 20 hours agoprevWorking in smaller steps is how you should build software. Constantly get feedback and re-evaluate what you're working on with other members of the team. Instead of giving an estimate, use t-shirt size. With constant feedback, the whole team is participating in the emergent complexity, instead of being passive and just annoying you with \"is it done yet\"? reply takemetoearth 11 hours agoparentI don't need constant feedback, I mostly need to be left alone to do the actual work. Problem is, the Cult of Agile gets nervous by the third daily standup where you just say you're still working on the same thing, because everyone knows no programming activity ever takes more than a large t-shirt's worth of days, however many that is. reply dijit 20 hours agoparentprevbut what if I’m working on something meaningful? I can’t MVP my way to a simulation physics engine, when each feature or partial feature requires weeks of planning, testing, iterating and tweaking- privately, before anything can be delivered to be used. Feedback, implies a working widget. reply jeltz 18 hours agorootparentFeedback does not need to be on a MVP, it can be given before that from your fellow engineers. That said there are tasks which really take a lot of research before even fellow engineers can give feedback. reply goalieca 19 hours agoparentprevAgile pretty much throws out estimating anything bigger than a sprint. Even then, points don’t mean time and velocity can be wild for a mature team. reply mydriasis 13 hours agoprevEven worse -- giving a thorough estimate, having the other party decline and reply with a different, significantly shorter estimate, and then turning their new estimate into your new deadline. Woof! reply ristos 12 hours agoprevNobody is going to fix the problem without fixing the culture, which isn't easy to do. The issue isn't around tasks that are predictable in nature and therefore easy to estimate with a small margin of error, it's around complexity in software, unforeseen things, bugs, etc, which can compound for larger long term projects. If engineers give estimates close to what it would be if everything goes right, then they risk overpromising and underdelivering if something goes wrong (hofstader's law). They might've just wanted to do the right thing by saving the company money and time, but in the end they footgunned themselves. Or engineers intentionally over-estimate in order to manage the complexity, but then you end up with a lot of padding and parkinson's law. Because as soon as the engineer starts underpromising and overdelivering consistently, management will pressure them to lower their estimates because they have a track record of doing that, so instead they're incentivized to pad and then fill up the entire time they estimated even if it took less time. Sprints were probably invented in order to deal with some of these issues, so that people just work with a bunch of smaller tickets that are much easier to estimate, with the more complex long term estimates going to management, which are incentivized to get it right because they're shareholders. That often leads to micromanagement and burnout, and it doesn't fix the padding/overestimation issue either, it might even amplify it in a lot of cases. People here mention giving ranges or probability distributions, and have also equally mentioned that they don't work because management wants a single number, or management just assumes the best-case or middle-case of the range as the actual estimate, and then they still get in trouble for giving ranges and it didn't solve anything. It also doesn't solve the problem of unanticipated setbacks, the whole you don't know what you don't know thing, which can only really be solved culturally in some way. While there are certainly bad managers that want to squeeze their workers, a lot of the time management is probably also pressured to give estimates and that's why they want and need that accuracy, because they're pressured by investors and clients that want to know how much time and money something will cost. Overall the entire problem is a system cultural issue around managing complexity. reply MrMcCall 14 hours agoprevWhen managers refused to accept that we just can't predict the future of the creative work that is software design and implementation. And that's because their entire existence is based upon money, not results. I've only ever had one good manager, and that was because he knew what he didn't know and accepted that we do and are trying our best. reply takemetoearth 11 hours agoparentI was asked to estimate a major project after a month of onboarding and such, and despite being lied to that the deadline was flexible, management decided the estimate was a suicide pact. Best part is: we all got laid off right as we finished up most of the work. So yeah. Predicting the future is hard. reply throwaway106382 19 hours agoprevI learned how to manage client expectations from Scotty in Star Trek: https://youtu.be/L3jXhmr_o9A reply djbusby 19 hours agoprevThis is especially problematic in early business when the team is small and manager act with same policy/process as when at BigCo. In this case don't estimate the time to build a poorly defined $something - invert the problem to estimate the value of $somethig. It's amazing how many of those managers asking for estimates push back when they have to put one out. With all the same reasoning that engineers have when estimating. A good manager should start with the Value first and allocate time-budget that makes that Value payoff. reply sublinear 11 hours agoprevWhen incompetent management who scam their sorry asses to the top for a lick of the brass ring before getting their asses kicked to the curb became the norm. This is a problem people and we're not impressed. reply nextworddev 20 hours agoprevMultiply your original estimate by 3, works most of the time reply dredmorbius 20 hours agoparentHeuristics I'd learned was \"double the time and bump the unit\". So: 2 hours -> 4 days, 1 week -> 2 months, etc. (I'm not sure where this turned up, but it's a long time ago, going on three decades.) The other option is to carefully track tasks, relevant dimensions, estimates, and actual performance, and see if there's any prediction modelling which can be derived from that. Problem is that this a classic instance of modelling in which the model itself affects the domain (as with economics and sociology, contrasted with astronomy and meterology where this isn't the case), such that estimates incorporating past performance data will incorporate the fact that the prediction model is being used. reply natebc 19 hours agorootparent> (I'm not sure where this turned up, but it's a long time ago, going on three decades.) That means it was 1.5 years (wall clock) ago right? I like this method. reply makz 18 hours agorootparentprevSo 1 year -> 2 decades? reply dredmorbius 17 hours agorootparentYes. Note that this also comes out about the same if you're using months (12 months -> 24 years (2.4 decades)) and (at least within a factor of two-ish) weeks (52 weeks -> 104 months (0.867 decades). I'm not claiming this is accurate, I'm stating that it's a heuristic I'm familiar with. This may have been in Arthur Bloch's Murphy's Law and Other Reasons Things Go Wrong, though I'm not finding it in comparable collections. Possibly from project planning literature I was reading in the 1990s (DeMarco & Lister, Brooks, Boehm, McConnell, etc.). reply dsego 20 hours agoparentprevThen you get the dreaded can you explain why it takes so long, or I asked engineer xyz and they gave a different estimate, where is the complexity etc. reply Moru 18 hours agorootparentIf you already asked someone, I'm not needed here so I'm going back to work on project Y that already is late enough. reply mandevil 19 hours agoparentprevI prefer pi myself. For the sort of false precision that managers love. reply theamk 19 hours agoparentprevI've heard \"multiply by 2, use next time unit\" rule. So 1 hour -> 2 days, 2 weeks -> 4 months, etc.. reply loloquwowndueo 20 hours agoparentprevMontgomery Scott recommends 4 instead. reply ben_w 20 hours agorootparentA friend suggests doubling the number and increasing to the next unit — hours become days, days become weeks, etc. I've certainly seen some environments — plural — where a task that should take 1 hour actually takes 2 days, and one that should take 2 days takes 4 weeks. reply ikiris 19 hours agorootparentprevBy the book admiral, hours could seem like days. reply rectang 20 hours agoparentprevMultiply by π — it's more accurate. reply Moru 18 hours agorootparentAccurate and precise is different things :) reply rectang 17 hours agorootparentOf course you’re right, however: 1. If we’re talking about bamboozling people who are either ignorant or willfully obtuse, excess precision is a stupid but useful tool for getting a realistic multiple of 3x-4x past them. 2. If the target audience understands excess precision, the excess precision is a nice little in-joke to flatter them and acknowledge their cleverness. 3. The absurdity of the precision illustrates tha very imprecision of the estimate. reply stevage 20 hours agoprevI definitely relate to the basic issue of estimates with my one-person consulting projects. The bit they didn't mention is: who pays for the cost of the inspections and analysis? In software, it can take a long time to analyse the requirements and the solution, in order to come up with an estimate (or fixed quote), and I find it awkward trying to get paid for that time. reply kareemm 19 hours agoparentDo a roadmap. Fixed scope small project that you get paid for where you define requirements. Deliverable is a functional requirements doc. It derisks the project for them, and gives you more confidence when quoting a fixed scope fixed price project. You just don’t guarantee timeline. I’ve been doing it for over a decade - it works wonders. reply stevage 19 hours agorootparentWhat is the total cost of these projects and how much are you typically charging for the requirements doc? I'm skeptical that most of the clients I work with would go for something like this. Generally my projects are in the $7k-20k USD range (though with some clients the total spend is much greater with additional phases of work...) Also, what do you mean \"you just don't guarantee timeline\"? Why not? reply intelVISA 18 hours agorootparentprevBest way but hard to find clients open to this as it requires more trust especially if it starts to drag on. reply albert_e 19 hours agoprevwhen an external / vendor team picks a project... estimates = contract amount = project budget ... it is hard for a project team to negotiate later often the estimates need to be competitive or bottom most for you to win the contract no one acknowledges the unknown and risks at the beginning of the project writing down more than 4-5 risks along with the bid amount is taken as a sign of a team that will not be easy to work with and fight over every bullet point whether something is in scope or a change request and often the one who estimates and wins the project may not be on the actual project team with delivery accountability reply russellbeattie 19 hours agoprevSadly, estimates are a negotiation. Whoever provides a number first generally loses because of \"The Wince\". \"What's your estimate?\" \"I'm not sure.\" \"Just ballpark it.\" \"Well, when would you want it by?\" This is the trap that new managers will fall into every time. If they give you an answer? Bingo. You give them \"The Wince\": You suck air between your teeth, and with a frown say, \"Oh, that's completely unrealistic. Where in the world did you get that number from??\" Then you provide a number that's many multiples higher, or offer a reduced amount of work: \"Oh, gosh. We'd only be able to get X feature done in that amount of time, and only if we got lucky and cut feature Y from the other project.\" Regardless, whatever you do, don't be the first to provide a number. It takes a little bit to get the feel for it, like poker or buying a car. Time is money, even in a big corporation. Treat it as such: It's a zero sum game. reply kelseyfrog 20 hours agoprev> Can you imagine if the insurance company started arguing with the repair shop, asking them—no—telling them that they would only pay the $18,000 and not the additional $20,000 because that was the original estimate? Well yeah, because there's not an inherent power imbalance like there is in employment. Part of this imbalance results in the ability for managers to employ Taylorization upon their directs. The majority of the time Taylorization hinders workers but management loves it because they can have more control in outcomes. What ends up happening though, is that an shadow work plan ends up getting established that management has less control over unless they want to drive out top talent by employing technocratic solutions to social problems. reply bigstrat2003 19 hours agoparentAlso, insurance does do stuff like that. reply kelseyfrog 18 hours agorootparentThey do, I considered adding that, but felt like it detracted from the point. Any passing glance at the healthcare system immediately reveals that doctors don't give patients cost estimates, inflate costs for insurers, and then settle at negotiated rates. Often they tack on procedures that aren't necessarily part of the initial plan due to unforeseen circumstances while the patient is unconscious and unable to consent, and you can go into a hospital that is ostensibly covered only to find out that a particular provider isn't. It's like if your treads were discovered to be bare, they decided to replace them and then the guy who does wheels sends you his own bill a month later. reply Aeolun 18 hours agoprevEstimates turned into deadlines around the time oneone came up with the concept of an estimate. reply NikkiA 21 hours agoprevSometime around 1956 at a guess. reply jappgar 19 hours agoprevA lot of ink has been spilled on this topic. The solution is simple: get better at estimating. Software engineers act as if they're the only ones in the world who are asked to estimate and then held accountable. It's a skill issue. reply turbojet1321 18 hours agoparentYou're correct to a degree, but IME it's a discipline issue, not a skill issue as such. What makes software hard to estimate is not so much the work, but how much requirements, priorities and resourcing change on the fly. I've worked in places that did quite strict scrum: once the sprint was planned and agreed to, we pushed back hard on our business area if they wanted to change anything. For practical reasons we didn't outright ban it, but for example, if they wanted something new done \"right now\" they had to choose something of roughly the same scope to be removed. If they couldn't do that, we'd cancel and re-plan the sprint. They hated having to make that choice, so most things just ended up at the top of the backlog for the next sprint. The other part was our own management wanting to take people out of the team mid-sprint to work on something else. We never said no, but did say \"this means we have to cancel and re-plan the sprint (and tell our business area)\". Basically, we made the people making the changes take responsibility for the impact on deadlines. Our estimates in that scenario were quite good (within the bounds of the sprint, at least). Of course, the longer the estimation window the less well this works. The only way to estimate accurately is to constrain scope and resources, which is not actually what management/business want. reply milesvp 18 hours agoparentprev> It's a skill issue. Maybe. Sort of. Skill hints at the problem, but it's more of an experience issue. More experienced developers can give pretty reliable estimates for work they've already done. The catch is, how often do you ask an engineer to do something they've already done? The beauty of software, is that you don't solve the same problem over and over, because if you did, you'd automate it. So where does that leave us? Well, developers who've seen a lot of different problems recognize when a new problem they see rhymes with a problem they've solved before. That can allow for a very good estimate. But even then, engineers tend to think in terms of best case, or maybe even typical case. I saw a study on this a few years ago, and it showed how often engineers tended to cluster around the median time for solutions. But, since really long tasks, with big error bars have a tendency to really skew timelines, the solution averages was much farther from these median times. Believe it or not, lawyers have this problem too. They are taught in law school to come up with an estimate based on expected effort, then they apply a formula similar to: double the estimate, then increase the time units. So a 5 hour task becomes 10days. A 2 week task becomes 4 months. Mind you, this isn't the amount of billable hours they're predicting, it's the amount of calendar time that a given task will take until it's complete. This ends up taking into account a lot of variables that lawyers have no control over. Things like court scheduling, or communication delay. I suspect it also takes into account blind spots we humans have around time estimates in isolation. Like, 1 task, if you can focus on it can be done in so many hours. But if you have to do 5 similarly sized tasks, it takes more than 5 times as long, simply because it's easy to expend resources like brain power, and the estimate ignores the refractory periods necessary once the task is completed. (BTW this was one of the problems with the very first stop watch study in the rail yard, where the rail workers didn't work at their sustainable pace, but worked at their depletion pace). reply jappgar 17 hours agorootparentYes I think you're right about it being based on experience. It's still a \"skill issue\" but it's more about knowing how long it will take you or your team. If you don't have the relevant experience with the tech or with the team, you can't really gauge how long something will take. Before I estimate larger projects I always ask who will be on my team. The estimate is very different depending on the answer. reply halfcat 16 hours agorootparentprev> how often do you ask an engineer to do something they've already done? The beauty of software, is that you don't solve the same problem over and over, because if you did, you'd automate it I heard someone summarize this as saying, a surgeon may perform the same surgery over and over for decades, while if a programmer does something more than a few times, it becomes an app (or library, etc). In a sense, unless we are building the same thing we've built before, we are, by definition, always operating at the limit of our abilities. reply jappgar 15 hours agorootparentThat's a narrow view of what surgery entails and also a grand interpretation of what professional programming actually looks like. reply HelloMcFly 19 hours agoparentprevIt's literally impossible to know what you don't know. Sometimes you discover things in the process of your work that couldn't have reasonably been accounted for on first estimate, and it is not good practice to always give an estimate that assumes a 5x difficulty multiplier for some unforeseen reason. Yes, if this is an \"every time experience\" then there could be a skill issue or possibly some other undiagnosed or unrecognized systemic factor reply jappgar 17 hours agorootparentWhen I cut open my ceiling to fix a plumbing issue I didn't know what I was going to find exactly, but I had a pretty good idea of the possibilities. My estimate for how long the fix would take was pretty close. Software is the same thing. There are unknowns but there aren't unlimited possibilities. reply HelloMcFly 4 hours agorootparentIt's a great analogy, but I think it's the kind of analogy that works on the surface and not in the details most of the time due to a) much more codification of practice in construction and b) more \"fixed\" knowledge of what's likely based on location and build era of the home, whereas software is much more dynamic and often subject to individual whims of developers or management. reply gopher_space 18 hours agoparentprevIf you're hiring architects and engineers to design and build your home, you might already have a pretty good idea of the home you want. The people you've hired provide estimates on cost and timing based on solidly known quantities. They've put in a basement like that before. They've worked with your weird materials. Their vendors report on material status daily. Software development is not surrounded by this sort of infrastructure or codification. My discovery process establishes these lines of communication, and I have no idea when I'll uncover a black box or when one will be imposed upon me. reply jappgar 17 hours agorootparentI guess I just don't buy that software is full of unknowns whereas construction is not. Contractors have to plan for surprises as well. The thing is they've done enough similar work to understand the risks and account for them in budget and timelines. I think a lot of software engineers, possibly because of the classical world they inhabit, are reluctant to look at things as probabilistic. Your estimate can take into account unknowns you just need to estimate the likeliness of encountering certain snags and the penalty they will impose. reply halfcat 12 hours agorootparentYou're framing the problem in terms of mathematical expected value (probability and penalty/reward), but the business environment in which software operates is fundamentally complex (see below). There is a spectrum of complexity: simple, complicated, complex. These can be framed in terms of ergodicity and you can search for Barry O'Reilly residuality theory if you want to go down this rabbit hole. In a simple system we can easily predict the future states based on knowledge of past states. In a complicated system, we can also predict future states based on past, but it requires expert knowledge, though it's still fundamentally able to be understood (e.g. SpaceX rockets). These are both ergodic systems. Complex systems are non-ergodic. Construction is a complicated system that exists within a complicated environment. Software is a complicated system that exists within a complex environment. Complex environments can be wrangled along three dimensions: constraining the environment until we can treat it as \"only complicated\", evolutionary survivorship via random stress and remediation, and avoiding commoditization. Construction benefits from all three. Environments are constrained to enable the things we build to operate (cars work mostly on paved roads). There is a history of evolutionary survivorship spanning millennia. And construction is less easily commoditized (people typically do the same thing they did yesterday, and repeat for decades). All of this contributes to the ability to a better ballpark estimate. Software primarily only attempts to constrain the computing environment in which the software runs: If you build your app to run in a container that can tolerate getting yanked and reincarnated elsewhere, you're golden, i.e. cloud computing is an example of constraining a complex environment until it can be treated as \"only complicated\". But the business environment remains complex and largely unconstrained. We attempt to constrain it via \"give us your requirements\", but that's more of an anchoring or negotiating technique than actually addressing the complex business environment. Software doesn't have millennia of evolutionary battle testing. And software is more easily commoditized, meaning if you do the same thing you did yesterday on repeat, that turns into a library or an app, so fundamentally you're always being pushed into novel territory, which therefore is less battle tested by evolutionary survivorship. All of this contributes to less clear estimates in software. reply jeltz 19 hours agoparentprevNo, it is a funding issue. Nobody wants to pay software engineers for actually doing prestudies to learn enough to do a proper estimate. They want a good estimate but do not want to pay for it. reply Atheros 18 hours agoparentprevNonsense. Code is copy-pasteable; other things are not. One can give very accurate estimates of how long it takes to build a brick wall because building brick walls takes time and labor. You can make highly accurate estimates of how long it takes based on how long it has taken to do the exact same task in the past. But suppose I laid one brick and then could copy-paste it, then copy-paste that into four bricks, then eight, until I have a wall. Then I can copy-paste the entire wall. Once I have a building I can copy-paste the entire building in five seconds. The ability to copy and paste an entire building is very valuable but how long does it take to create that copyable template? No one knows because it has never been done before. reply jeltz 18 hours agorootparentBuilding brick walls is easy to estimate not because it is physical labor but because the company has done it many times before. Ask a construction company to estimate a unique one-off job and they will most likely fail. And that is despite them getting a lot of resources for investigating and planning which software engineers almost never get. reply jappgar 17 hours agorootparentYes exactly this. Perha",
    "originSummary": [
      "In software modernization projects, estimates should be viewed as flexible guidelines rather than strict deadlines due to the unpredictable nature of such projects.",
      "Indu Alagarsamy likens architecture modernization to car repair, highlighting the need for adaptability and additional approvals when unforeseen issues arise.",
      "Effective leadership in these projects involves asking insightful questions, embracing experimentation, and utilizing suitable frameworks to navigate complexities and achieve success."
    ],
    "commentSummary": [
      "The post discusses the common issue of management treating project estimates as strict deadlines, often ignoring factors like changing specifications that can cause delays.",
      "This practice leads to inflated estimates as a defensive measure to avoid blame for missing so-called \"deadlines,\" which can stifle innovation and honest work.",
      "The suggested solution is to cultivate a company culture that tolerates mistakes and focuses on continuous improvement rather than assigning blame, promoting a more innovative and productive environment."
    ],
    "points": 282,
    "commentCount": 207,
    "retryCount": 0,
    "time": 1732046590
  },
  {
    "id": 42188687,
    "title": "SpaceX Super Heavy splashes down in the gulf, canceling chopsticks landing",
    "originLink": "https://twitter.com/spacex/status/1858995009384837380",
    "originBody": "Super Heavy is proceeding to a landing burn and splashdown in the Gulf of Mexico— SpaceX (@SpaceX) November 19, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42188687",
    "commentBody": "SpaceX Super Heavy splashes down in the gulf, canceling chopsticks landing (twitter.com/spacex)260 points by alach11 20 hours agohidepastfavorite390 comments starspangled 17 hours agoThey demonstrated the engines re-lighting in space, which is significant. There had been some questions about this because the engine is of a design that is said to be very tricky to start, and the tank pressurization system of the rocket has the risk of water and CO2 ice forming in the methane tanks, which had caused several failures in past tests flights. So this is a pretty good milestone. So we might start seeing test flights actually entering orbit soon. Possibly even carrying some real payloads soon. reply icpmacdo 16 hours agoparentWhy don't they already carry payloads? Is there anything worth taking up with the current expected value of it exploding ect? reply gpm 16 hours agorootparentSpeculation: Putting a starship in low earth orbit right now would be a bit reckless, because if the engines fail to relight it's going to come down at some point in a completely random spot along the trajectory, and it's quite a large piece of debris that is designed to not burn up. By contrast this test (which involved the first 0g relight of a raptor) was designed so that if that failed the ship would still come down in a designated keep out zone in the ocean. Even ignoring the safety risk, the value SpaceX gets from this flight is largely testing the re-entry (heatshield, flaps, etc) of starship. Putting it in an orbital trajectory risks a failed engine relight making it impossible to test that, because the ship will be dead (out of power) by the time it comes back down. Whatever low-cost payload you could put up there for \"free\" (the cost of risking the payload being destroyed if the test flight goes wrong too early) might not be sufficient to pay for the risk stopping in an orbital trajectory imposes on the test objectives. Now that they've successfully lit a raptor in 0g, I imagine they're a fair bit more likely to make the subsequent flights orbital. reply pfundstein 8 hours agorootparentThat's what the flight termination system is for, and thus it'll be exploded remotely instead. reply philipwhiuk 8 hours agorootparentAn FTS does not prevent 100t of debris of which a significant amount is designed to survive re-entry from impacting the surface. If you blow up 100t you still have 100t of debris, just in lots of bits (and honestly, still big chunks - an FTS does not atomise or even close - it punches a hole and then the vehicle collapses structurally) All it really does is remove the explosive potential of the fuel. So no, it's not designed to do this. Once in orbit the FTS system is usually deactivated (safed). reply mapt 6 hours agorootparentIt's designed to survive re-entry in a very particular, and repeatedly adjusted, orientation. It's unclear how much would survive re-entry in an uncontrolled tumble. But better safe than sorry. reply chippiewill 7 hours agorootparentprevYeah, you do not want the FTS activated on orbit. It's not something you want going off where it can leave 100t of hazardous debris in orbit. reply mapt 6 hours agorootparentDebris in orbits that nearly intersect the atmosphere are practically harmless. The issues arise when you have an orbit that will intersect other orbits a billion times before decay. While the reality is very complicated, you can make a rough handwave model for orbital debris in a circular orbit: At 100km it lasts 1 orbit (90 minutes) at most, and every ~100km you add after that increases orbital lifespan by ~10x. reply simonh 6 hours agorootparentprevYou want to turn a single dead 100t object in orbit into 100t of randomly blasted debris in orbit? reply PittleyDunkin 14 hours agorootparentprev> Putting a starship in low earth orbit right now would be a bit reckless, because if the engines fail to relight it's going to come down at some point in a completely random spot along the trajectory, and it's quite a large piece of debris that is designed to not burn up. More relevantly—it's harder to make a marketing stunt out of a fallible mission. reply MarkusQ 13 hours agorootparentWhat exactly do you think they're marketing? And don't say launch services, 'cause you're using \"it's a marketing stunt\" to explain why they aren't taking payloads. \"It's R&D\" makes a heck of a lot more sense. reply davidguetta 9 hours agorootparentprevAs if Space X needed more marketing than being the cheapest and more consistent launch provider for the past 10 years reply rogerrogerr 14 hours agorootparentprevIs it your position that flight 6, or the Starship program, or SpaceX is a marketing stunt? reply GuB-42 8 hours agorootparentprev> More relevantly—it's harder to make a marketing stunt out of a fallible mission. SpaceX is well known for making a marketing stunt out of their failures (see \"How Not to Land an Orbital Rocket Booster\"). In fact, Elon Musk is known to make marketing stunts out of anything. But that's not it, they could have launched a dummy payload if they wanted to, they have already done it for other rockets, and yeah, they made it a marketing stunt. Famously, they launched Elon Musk's Tesla Roadster in the direction of Mars. But here, launching a payload at this stage of development is just not the best thing to do. reply odirf 55 minutes agorootparentIn fact, IFT-6 had some payload. They put a banana into the payload bay. reply modeless 11 hours agorootparentprevIt's exactly because they hadn't demonstrated engine relight in space yet. That means they couldn't guarantee they would be able to deorbit Starship in a controlled way. An uncontrolled deorbit would be bad because it could come down literally anywhere and large chunks would hit the ground. Because they couldn't guarantee precise deorbiting, they never put it in orbit to begin with. Starship was on a ballistic trajectory that falls back to Earth. Any payloads they deployed would fall back too, unless they carried their own rockets to reach orbit themselves. reply chippiewill 7 hours agorootparent> Starship was on a ballistic trajectory that falls back to Earth Actually Starship on this flight wasn't quite on a ballistic trajectory, the periapsis was actually above the ground so it counts as an orbital trajectory. Without the atmosphere it would keep going. reply nomilk 16 hours agorootparentprevThe utility of test flights doesn't come from delivering a payload; but from collecting data. For example today's test involved intentionally weakening starship's heat shield to see if previous estimates of required shielding had been too conservative. reply soheil 13 hours agorootparentWell looks like you imposed a qualification on these types of flights and now by definition they cannot deliver payload. The flights could deliver cheap payload for example. reply cwillu 8 hours agorootparentYou can't deliver a payload to orbit if you're not planning on going to orbit. They have a launch licence for a particular mission plan. The license for Falcon 9 and Falcon Heavy specifically says “Authorization: SpaceX is authorized to conduct flights of launch vehicles: […] (c) Transporting Dragon 2 to low Earth orbit or a payload to orbit;” The license for Starship/Super Heavy Launch Vehicle does not include such an authorization. https://www.faa.gov/media/69476 https://www.faa.gov/media/75501 reply AlessandroF6587 12 hours agorootparentprev\"The payload is data.\" Elon The data they get from the test and they focus on getting the most and most valuable data. This is an explicit choice. A physical payload at this stage would reduce the overall value that they are able to get from a flight. reply adastra22 8 hours agorootparentprevLots of technical speculation in the sibling comments. I suspect the real answer is that any real payload would require at least FCC approval, and possibly modification of their FAA flight plan. The income from the payload would not be worth the delay. reply Treegarden 6 hours agorootparentprevFrom what I followed, its because: They already send payload to space with the falcon rockets (reliable cash cow) and the only point of starship system is proof of concept of full reusability. There is no point to the whole project if they dont achieve this. They are constrained in launches, either through regulatory (FCC) or engineering timeline and so far, every launch counted. They achieved a substantial improvement. If they dont catch the starship (upper stage), the whole project is pointless so its important keep things simple (details like payload door, cargo dont matter) until they achieved the whole loop (land & catch starship) They already have a cheap way of launching. Cheap for current market rate. Whole point of this project is to improve cost by / 1000. They achieved substantial progress with every flight. 1. It flies, 2. hot staging 3. reach orbit 4. land both & heat shield, 5. catch booster, 6. extreme conditions & improve heat shield. reply baq 12 hours agorootparentprevTheir payload door situation is complicated. reply busssard 8 hours agorootparentcan you elaborate? https://ringwatchers.com/article/ship-pez-dispenser Seems like their door is made exactly for the purpose of dispensing starlink satelites, with minimal impact to the hull integrity.. reply antoniuschan99 16 hours agorootparentprevthey carried a banana this time :) reply Keppl8R 15 hours agorootparentAlso the structure behind it is the Starlink pez dispenser https://ringwatchers.com/article/ship-pez-dispenser reply fooker 13 hours agorootparentHuh nice, missed that detail! reply ethbr1 16 hours agorootparentprevIs the Starship upper stage boosting itself up to full orbit yet? reply mgsouth 16 hours agorootparentNo. This and all previous flights have intentionlly been barely sub-orbital, with less than one orbit. Launch in Texas, re-entry over Indian Ocean. A full orbit at that altitude takes about 90 minutes; this was over in a little over an hour. The reason was safety. If it was orbital, then controlling the re-entry would require a sucessful relight of the engines. If that failed then the re-entry point would depend upon the vagaries of orbital decay from residual atmospheric drag. That's no doubt why today's relight was so brief; they didn't want to significantly alter the reentry point. reply mohaine 13 hours agorootparentI don't believe this is quite correct. The last few trips are actually orbital, just not of the correct elliptical shape to do more than a half orbit as the perigee is less than the radius of earth. If earth was a point mass, it would have orbited. This means you don't have to do anything to deorbit while proving you could have made a full orbit if you wanted to. reply gpm 13 hours agorootparentIf the earth suddenly became a point mass except for all the humans, we'd all already be in orbit. At the \"top\" of a highly elliptical orbit that passes relatively close to that point mass, yes, but an orbit. Everything that was within the Earth's sphere of influence, not moving so fast to be on an escape trajectory, and not with 0 horizontal velocity relative to the point mass would be in an orbit. Orbital means \"on a trajectory that doesn't intersect (or escape from) the body you are orbiting\", otherwise the word is meaningless. reply kbelder 2 hours agorootparentA person standing at the north pole wouldn't be in orbit. They would fall directly down. (Wouldn't they?) reply Toutouxc 12 hours agorootparentprev> the perigee is less than the radius of earth Which is what we call \"sub-orbital\" :) reply lutorm 9 hours agorootparentprevEvery time you throw a rock, it ends up on a trajectory that \"would have orbited if the earth was a point mass\". That's just not a very useful definition of \"being in orbit\". reply foodevl 12 hours agorootparentprevIf the earth were a point mass than almost any trajectory at all would be orbital. reply slow_typist 12 hours agorootparentCan’t hit a point mass on any trajectory. reply echoangle 11 hours agorootparentWhy not? If your starting point is completely static compared to the point mass and your aspect area isn’t zero, you’re going to fall directly down towards the point mass and are going to hit it. reply ben_w 8 hours agorootparentIf it was a point mass, and you had exactly zero horizontal motion relative to it, you'd go right through and out the other side. Well, except for relativity turning it into a black hole with a Schwarzschild radius of 8.87 mm so it won't be \"point-like\". But most of the disintegrate sheen of plasma that used to be your body would have had some horizontal motion compared to it, even if only due to you starting off as an extended body. reply echoangle 7 hours agorootparentIf the point mass is inside my body for some time, I would describe that as „hitting the point mass“ reply ben_w 6 hours agorootparentIf you like. But would you say you hit neutrinos that pass through you without interaction? If pointlike was possible, it can be similar: nothing beyond the spaghetification that happens well before you reach the event horizon. reply AlessandroF6587 11 hours agorootparentprevThat's an orbital-velocity ballistic trajectory. And that's sub-orbital. Barely reply slow_typist 12 hours agorootparentprevI believe that is called a ballistic trajectory. reply 16 hours agorootparentprevnext [4 more] [deleted] fastball 16 hours agorootparentOrbit is not about distance, it is about speed (perpendicular to gravity). Starship deliberately did not reach orbital velocity on this flight, so although they are fairly certain it is capable of doing so (esp after testing engine relight on this flight), they have not done it yet. Not being at orbital velocity is one reason why they are not delivering payloads. The reason for choosing not to go orbital at this point is indeed as another commenter said: safety. A craft which has not reached orbital velocity is therefore traveling on a parabola, which has a predictable flight path. If they lose control of all systems, they can still be confident that it will crash where they want it to (in this case, the Indian Ocean). If instead you boost enough to get orbital and then lose control, it is much harder (almost impossible) to know where the craft will end up. SpaceX (and the FAA) wants to make sure everything works properly by doing sub-orbital flights, then when they are fairly confident nothing will completely break on them they can start testing in an orbital regime, by powering on Starship's engines long enough to achieve the requisite speed. reply fixprix 14 hours agorootparentThey don't need to relight to reach orbit, they just performed SECO a bit early. For reference at 190 km up which Starship reached, it needs to go 28,000 km/h to stay in orbit. Today it was flying at 26,000 km/h. reply hnaccount_rng 12 hours agorootparentBut then they’d need to relight in order to break orbit. So technically correct ;) reply hoschicz 7 hours agorootparentprevthey showed a banana in the payload bay, that was the testing payload:) they pushed the rocket to the limits in this flight reply sebzim4500 7 hours agoparentprevI would argue that IFT-6 was already orbital since the perigee was above the ground (albeit inside the atmosphere) reply chippiewill 7 hours agorootparentAlthough this is getting very nitpicky, while I'd agree a perigee above ground would count as an orbital _trajectory_, an orbital flight is usually considered as one that completes at least one full orbit around the planet. reply sebzim4500 7 hours agorootparentBy that logic Gagarin did not reach orbit, so I don't think that's a common definition. reply m4rtink 5 hours agorootparentThere was also another issue - Gagarin did not land with the capsule, but ejected during the landing as expected & both the and the capsule landed under (separate) parachutes. This was because the capsule landing speed was higher than what could be achieved if only the kosmonaut parachuted down, making the landing safer. But the FAI guidelines for for space records at the time were based on normal aircraft flights & expected the crew to board the craft & only disembark on landing. So soviets just lied about Gagarin landing with the capsule up until 1971: https://en.wikipedia.org/wiki/Vostok_1#World_records Also the orbital booster used (R7) was also still an active ICBM (even sometimes holding combat readiness with a live nuke on top on the same launch pad) so that was IIRC secret as well. :) reply tsukikage 7 hours agorootparentprevDo you have a source for that? My understanding was that he completed a full orbit. reply sebzim4500 6 hours agorootparentThe insertion burn finished at 06:18 UTC and they started the deorbit burn at 07:25 UTC, while the period of the target orbit was 89 minutes. reply tsimionescu 11 hours agoparentprevI'm curious if they will have enough fuel to carry (significant) payloads. Have they commented anything on the amount of fuel that they are taking today vs the capacity? Are they not flying with full tanks at the moment? reply ortusdux 19 hours agoprevI remember there was a phase of Falcon design where it looked like they had perfected barge landing, and then they had a rash of failures. Later on they admitted to intentionally crashing older boosters so they could find the limits of the hardware. They were iterating at such a pace that the data was worth more than a recovered booster. I wonder if that was the case today? reply mjamesaustin 18 hours agoparentTo give an example of this, the Starship they've been flying on the last few flights is already obsolete. There's a newer V2 but they wanted to burn through the rest of the V1s they had already built and get more data before flying V2. reply fastball 16 hours agorootparentThey also apparently already have a V3 in the works. reply cyphertruck 13 hours agorootparentIt's been announced, and is surely being designed, but they are only building hardware for V2 right now. Most likely V2 will continue as long as they need to get dialed in what they want V3 to be, then they will switch. V2 seems to be based on the learnings from V1, and I think V3 is the real design they want to fly-- but V3 depends on Raptor 3 and probably other advancements, and Raptor 3 is still at the testing stage in MacGregor. For instance the V2 design seems to use Raptor 2.5 which is a Raptor 2 variant with Raptor 3 style interface with the ship. So they are testing the ship design to support Raptor 3 before they have Raptor 3. The engines are really heart of these things and drive the development cadence. reply polishdude20 14 hours agorootparentprevThat makes so much sense. Once you find the correct parameters for an optimal flight, use the rest of the rockets to map the state space of configurations to see how much they can deviate reply rajnathani 8 hours agorootparentprevInteresting, so if they do not intend to recover the rocket from the water here, then this was an effective waste-disposal method for their obsolete V1 design? reply ricardobeat 8 hours agorootparentI was surprised to learn that, besides the environmental impact of the leftover fuel - though methane is the least toxic of them, and decomposing electronics, the metal rocket body itself can be a boost for marine life and corals by serving as shelter. Guess the outcome depends a lot on the design and materials used. reply iknowstuff 18 hours agoparentprevDuring the livestream they did keep saying that they’re pushing it past its expected limits. reply whaaaaat 17 hours agorootparentThat was primarily said for the second stage. The primary stage I didn't hear them call out as often (if at all?) reply bensandcastle 14 hours agorootparentwas a stated objective before the launch: \"faster/harder booster catch\" https://x.com/elonmusk/status/1858867695233425734 reply rksiitd 17 hours agoparentprevI guess you are right! https://x.com/elonmusk/status/1859036912348262787 reply jryan49 17 hours agorootparentThat is recovery of the top part he’s talking about not the bottom reply sib301 15 hours agorootparentprevYou believe him? reply pbreit 14 hours agorootparentYes. reply wil421 18 hours agoparentprevWhy would they plan to catch it and then divert mid flight if they didn’t want to reuse it? reply afro88 17 hours agorootparentIt's possible to plan for multiple eventualities. They may have pushed it to its limits (or beyond) and decided the best destination based on how it handled it reply ammojamo 17 hours agorootparentprevI believe they diverted to save the tower from being potentially damaged/destroyed by a failed landing. reply qingcharles 17 hours agorootparentOne of the commentators said (roughly) \"they can make another rocket real quick, but if they blow their one pad up then they are hosed for a long time.\" reply zamadatix 17 hours agorootparentprevWhich would be in direct conflict with the reason given originally above. reply eru 17 hours agorootparentThey might be happy to push hard enough to risk the booster, but be much less willing to risk the tower? Seems perfectly consistent to me. reply zamadatix 17 hours agorootparentI like SpaceX as much as the next nerd but that's not \"intentionally crashing the booster\" it's \"doing the only other type of landing you can when you abort the first plan of landing it successfully\". I'm sure they got useful data out of it (it's better than \"booster blows up in mid air\") but this is squarely in \"2nd attempt to land with chopsticks wasn't as ready as they hoped\" bucket, not \"132nd attempt to land the booster was intentionally destroying it to see how much farther they could be pushing it\" as was originally implied with the wording and prior example. reply eru 16 hours agorootparentOh, sure, I didn't want to make any comment on what they were actually doing or trying to accomplish. Only that the hypothetical we were talking about would have been consistent. reply samatman 15 hours agorootparentprevThere's a lot of middle ground here. I suspect what's most accurate is \"let's push the booster out of envelope a bit, if we get really nice numbers we'll go for the chopsticks landing, otherwise it's into the drink\". In other words, they were optimistic enough to think that another upright landing was within the realm of possibility, while also deliberately doing things which made that outcome less likely, to get the data they need. If that's true, I wouldn't characterize it as a second attempt at a chopstick land, that would just be a stretch goal. Who knows if it is, but it's consistent with how SpaceX operates. reply lesuorac 14 hours agoparentprevDo you have to pay to scrap old rockets? I guess that could be an interesting cost savings measure, just lose/destroy your old inventory instead of paying disposal. reply gpm 12 hours agorootparentYou have to pay to get the rocket back anywhere near to a barge at all. If you don't want it back you just let it fall into the ocean without spending fuel on slowing it down (and more fuel bringing that fuel along so that it is there to slow it down). You don't pay for a barge. You don't pay for the engineering work associated with bringing it back. Dropping rockets into the ocean was what everyone* did before SpaceX came along. If you're already paying the cost to bring it back though, it's very hard to imagine that you would have to pay to scrap it. The thing is primarily a large metal tank, and some engines made up of a bunch of metal - people pay money for the privilege of scrapping metal, not the other way around. * Technically some other countries dropped them on deserted land, or not so deserted land in the case of China. reply keeperofdakeys 6 hours agorootparentprevDepending on what's in the rocket, just leaving it to decay in the environment would likely lead to contamination. Especially if you have any Hypergolic fuels lying around (explode on contact, and extremely harmful to humans / the environment). So you either pay to scrap it, or pay to clean up the site in X years. reply echoangle 11 hours agorootparentprevThey could probably get museums to pay them for their scrap. They would have some work to remove any confidential components before handing it over though. reply simondotau 14 hours agorootparentprevI have no idea, but my guess is that the cost of fuel would be an order of magnitude greater than any possible disposal costs. Also, people pay good money for high-quality metals. The scrap value is probably greater than the logistics cost of getting it to a scrapyard. You wouldn’t even need to scrap it. Cut it up and turn it into some mega silos for agriculture or materials suppliers. Or just set it aside and donated to the Smithsonian in a few decades… reply hoseja 11 hours agorootparentIt's running on natural gas and air. The fuel is actually really cheap. reply simondotau 14 hours agorootparentprevAnd, I should add, but if anyone discovered that the fuel was being wasted for no benefit, it would be a PR nightmare, especially as there’s no end of people eager to find fault with the company’s founder and largest shareholder. reply whycome 13 hours agorootparentWhy do you think that would be a PR nightmare? reply kfrzcode 13 hours agorootparentprevThis seems like the tail wagging the dog reply kfrzcode 13 hours agorootparentprevProbably not as much as if you steer it toward the graveyard https://en.wikipedia.org/wiki/Spacecraft_cemetery reply ericcumbee 19 hours agoparentprevYes Musk said before one of the first flights, that they are making changes and building new hardware at a much faster rate than they could ever hope to fly it right now. pretty much something to the effect that by the time they get to fly hardware a lot of it is obsolete. reply ryandvm 13 minutes agoprevFascinating, the more distractions Musk has, the more impressive SpaceX does. Just imagine what he could do if he were the CEO of a hundred companies! reply simonw 19 hours agoprevWhat's the advantage of the chopsticks landing over splashing the thing down in the ocean? Does an ocean landing cause significant damage that's not present with an on-land chopsticks landing? Presumably there are pretty big advantages considering how much it must have cost to develop the chopsticks approach. reply modeless 18 hours agoparentThe booster is destroyed when it tips over after \"landing\" vertically on the water. It's like a 20 story building falling over. If you're asking why they don't land it on a floating barge like Falcon 9, there are two reasons. One is that landing back on the launchpad lets them refuel and relaunch immediately. The other is that landing legs are big and heavy and significantly reduce the payload capacity. If you're already landing on the launchpad you might as well add arms to catch it. The mass of the arms is free because they're on the tower instead of the rocket, and the rocket only needs tiny nubs to catch on the arms instead of giant legs. Also the arms double as a crane to lift and stack the rocket on the launchpad. reply raldi 15 hours agorootparentLike a falling chimney (google it), it breaks because the top can't fall fast enough. reply chrisco255 14 hours agorootparentprevAre they going to need legs when they land the Starship on the moon or Mars? reply murkt 12 hours agorootparentYes, Starship needs legs to land on the Moon or Mars. They will be lighter than legs for Earth landings, as Moon/Mars have lower gravity. Not sure about the amount of fuel needed to land, as there is much less atmosphere on Mars, and none on the Moon. I would still guess that it needs less fuel there as well. reply liminvorous 8 hours agorootparentAn atmosphere reduces the amount of fuel needed to land, because you can use aerobraking to slow down rather than carry fuel to do it. See the Apollo return capsule, which landed without any rockets, only parachutes and a heat shield. reply __m 10 hours agorootparentprevDon't they also need fuel to lift off again? reply lutorm 9 hours agorootparentThat will have to happen through in-situ resource utilization. aka making the fuel on Mars. reply perryizgr8 10 hours agorootparentprevI think Elon has said that it's going to be a one way trip to Mars. reply jmercouris 14 hours agorootparentprevthe booster does not need legs, only starship does reply modeless 13 hours agorootparentYes, also the lunar variant won't need a heat shield so that will compensate for the extra leg mass. Not sure what their plans are for a heat shield for Mars. reply cwillu 8 hours agorootparentprevThey're planning on catching starship as well however, as the in-orbit refueling will require a lot of starship launches that aren't going anywhere except orbit and back. reply preisschild 6 hours agorootparentprevYes. Starship HLS will have them. But the other Starships don't and adding them would make a big difference. Since you need a huge fuel tank in orbit that needs to be refilled by multiple starships to refuel Starship HLS to actually land on the moon adding legs on those refueling Starships would decrease payload capacity, and thus you'd need even more Starships to refuel the tank for a single lunar landing. reply double0jimb0 17 hours agorootparentprevAlso, a rocket that hangs doesn’t need near the structural beef as one that is designed to withstand landing on its feet. reply askvictor 15 hours agorootparentNot convinced about this argument; wouldn't the forces during the slow-down burn be applied much the same as the from the feet? reply HPsquared 7 hours agorootparentThe chopsticks on the tower can absorb a lot of the impact, I'd guess. reply ars 15 hours agorootparentprevActually no - the forces travel inside the \"tank\" of the rocket, and push against the top of it. The force doesn't push against the nozzle of the engine. reply krisoft 7 hours agorootparent> The force doesn't push against the nozzle of the engine. I’m not sure what you are saying here. The force pushes against the nozzle, (and of course the walls of the combustion chamber.) That is the purpose of the rocket engine, to push the rocket forward. They are not just there to provide mood-lights. reply ars 38 minutes agorootparentI said \"nozzle\", as in the side walls of the engine. The force doesn't transfer via those walls, rather it goes through the full tank of the rocket and pushes against the top. reply slow_typist 11 hours agorootparentprevSame pressure in the tank as in the nozzle? That doesn’t sound right. It would burst. reply ben_w 8 hours agorootparentSame force, not same pressure. The nozzles are relatively small. I wouldn't want to try to intuit what the force distribution would be and how much is carried through each component of the structure, though — that's what simulations are for. reply panick21_ 10 hours agorootparentprevAlso, a ship big enough to carry Starship booster would be huge. And then what port infrastructure do you use to bring it to land and transport it to your launch pad? You would basically need your own port right next to the launch site. reply bmitc 16 hours agorootparentprev> One is that landing back on the launchpad lets them refuel and relaunch immediately. How do they structurally, electrically, etc. checkout the rocket after landing? reply modeless 15 hours agorootparentThey haven't done it yet and nobody outside of SpaceX knows their specific plans. Of course we can speculate that they will design sensors and cameras to replace any previously required manual inspections. reply mewc 15 hours agorootparentprevAn ops problem for later. At least now its possible! reply bmitc 4 hours agorootparentIt seems like a pretty expensive assumption that landing the rocket is enough. Relaunching the rocket, which requires inspection and validation procedures and technology, is just as important as landing it. reply redmajor12 17 hours agorootparentprevSo is it cheaper to dump the booster in the ocean then land it back at the tower and then have to dispose of it? reply modeless 17 hours agorootparentThe booster always follows a trajectory for an ocean landing until it passes a bunch of safety checks, and then it diverts to the launchpad. This way a failure early on can't cause it to crash on land. What probably happened is it failed a safety check (e.g. a sensor read out of range) and so it didn't divert to the launchpad. It's cheaper to dump the booster in the ocean than to build a new launch tower if it's destroyed in a failed landing. They have an assembly line for boosters, but only one fully complete launch tower at the moment. reply sneak 17 hours agorootparentprevThe launch tower takes multiple months to build. The orbital launch mount and launch tower and quick disconnect hardware is all custom and expensive and huge. They call it “Stage 0”. Losing that is an order of magnitude more of a setback than losing a Stage 1. reply avmich 17 hours agorootparentI guess they didn't optimize the towers to be fast-buildable :) because there are several obvious ideas. reply martyvis 17 hours agorootparentThe 2nd tower at Starbase is much more modular than the first and is being built at quite a rapid rate. reply sneak 17 hours agorootparentThey’re building one at the Cape, too, but they’re still a multi-month process. reply mgiampapa 16 hours agorootparentprevConstruct additional pylons? reply avmich 16 hours agorootparentWhy not to use the Mechazilla for adding the next section of tower on top of existing one? So we transport parts to the tower, assemble relatively thin sections of tower nearby, the lifter moving along the tower brings the sections on top of existing ones, and we have a few places for optimizations here. The proponents of The Boring Company may see some parallels here. reply jojobas 18 hours agorootparentprev>immediately So far this hasn't been shown even on much simpler Falcons. The barge gives quite some energy advantage for not having to boost back. Ideally they'd build a capesize kinda barge with chopsticks to catch it in the ocean, then perhaps service what they have to while it's steaming back. reply modeless 17 hours agorootparentFalcon 9 wasn't designed for immediate reuse. Immediate reuse is just a theory right now, but SpaceX has an excellent track record of turning their theories into reality. SpaceX did purchase some oil rigs with the intention to turn them into launch platforms, but later abandoned the idea. It's probably something they will return to later once Starship is flying regularly. You're right that avoiding the boostback burn is a big advantage. But maybe they don't need to bring the booster back after it lands on a platform, it can just launch again from there. Maybe they could have a bucket brigade of launch platforms ringing the Earth! reply ethbr1 16 hours agorootparentThe oil rig idea is brilliant, especially for cargo flights. I.e. the bulk of near / intermediate term launches Far fewer people care if you \"oops\" an unmanned rig in the ocean. reply CableNinja 3 hours agorootparentThe issue is logistics, and in this case isnt an easy solve. You have to get fuel, and the rocket, out to a pad in the ocean, and have to deal with a rocket lift on varying conditions. If you dont want to do most of that, then the only option is putting your manufacturing on the rig too, which negates lifting a rocket, but instead makes the rig huge, and requires having a train of ships in and out 24/7 to keep it supplied. There was even a company in the late 90s that tried oil rig launch platforms and ultimately abandoned it. reply ethbr1 2 hours agorootparentMost heavy lift is transported by barge over water anyway. Since the 2 stages of Starship aren't intended to be road mobile, due to size, there's no transportation benefit to being land-accessible. So really the main concern is piping propellant... but afaik some rigs off the shallow coast of Texas are directly piped to land? The main benefit you get is terrifying the FAA et al. less, as the consequences of a missed catch are now out in the ocean. reply jojobas 13 hours agorootparentprevMusk spoke of 24-hour Falcon turnaround as early as 2011 and as late as 2019. reply modeless 13 hours agorootparentImmediate reuse means significantly less than 24 hours. Falcon 9's current cadence is fast enough to meet their current launch demand and doesn't need to improve, especially with Starship on the horizon. On the other hand, Starship will need to launch repeatedly in a short amount of time for orbital refueling to work. reply jojobas 13 hours agorootparentSo it was declared as a target, was possible but wasn't done because no demand? Knowing SpaceX they'd do it just for bragging. The fact that it's required for Artemis to work, and the amount (nobody knows exactly but lower bound is like 15) of Starships required to launch in quick succession just highlights how risky, to the point of unsoundness, the project is. reply valine 8 hours agorootparentThey don’t need rapid reuse for Artemis. It would certainly help but Starship can just hang out in orbit for a few weeks while they do what they need to do to launch all their rockets. reply HPsquared 6 hours agorootparentI wonder what the boil-off rate is on the cryogenic propellants. reply m4rtink 5 hours agorootparentIt really depends - you can do direct injection to GEO now with cryogenic stages. And IIRC Soviets did some tests with kerolox stage (that should eventually launch on the ill fated N1) around the Moon, meaning multi-day flight times with liquid oxygen on board. If you do the thermal design right, possibly use a sun shade (space is a large thermos bottle after all) or even use active cooling to remove the little heat that gets through, then it should work just fine. :) reply HPsquared 5 hours agorootparentThe heat shield probably provides some degree of insulation from the solar radiation as well, and it's only needed on one side if you're far from Earth. reply m4rtink 1 hour agorootparentYeah, I was thinking about the orientation during the coast phase & how thermal management could explain the Starship orientation at that point. :) reply literalAardvark 18 hours agorootparentprevHasn't been shown because they're not meant to be immediately reusable. Spaceship and/or super heavy are (I don't remember the details). reply jojobas 17 hours agorootparentI'll believe it when I see it. reply avmich 17 hours agorootparentIt would be good because there are many people who, roughly, refuse to believe their own eyes and keep moving goalposts, misrepresenting what they said or meant earlier, inventing additional conditions, changing their mind etc. If you would honestly believe that Starship can fly frequently when you see it flying frequently, you're already ahead of some. reply jiggawatts 17 hours agorootparentI remember when people doubted that the full-flow staged combustion methalox engines could work... until SpaceX showed over a hundred of them working now. Then the ULA CEO Tory Bruno claiming that the SpaceX photos of the first Raptor 3 were “partially assembled”, to which Gwynne Shotwell replied with this: https://x.com/Gwynne_Shotwell/status/1821674726885924923?t=v... reply modeless 17 hours agorootparentAlso catching a booster, also landing boosters at all, also achieving cost reduction via landings, also cheap enough phased arrays to make Starlink viable, also also also... SpaceX has a long history of proving doubters wrong. reply sroussey 15 hours agorootparentI remember talking about landing booster when I was in college in the 1980s. Super awesome I got to see it in my lifetime! And yeah, Starlink was a great purchase by SpaceX. reply modeless 14 hours agorootparentWhat part of Starlink was purchased? reply rogerrogerr 14 hours agorootparentIt’s fake news, SpaceX built Starlink internally. Even Wikipedia says so: https://en.m.wikipedia.org/wiki/Starlink reply cwillu 8 hours agorootparentNot every piece of misinformation floating around is “fake news” reply avmich 17 hours agorootparentprevI'd consider that cheating on the Gwynne side. It could be argued if chamber tilting mechanism is part of the engine - after all, it was added separately to NK-33 - but surely for Raptors it's considered an integral part, the fire test was then a test of chamber, not the engine :) . Though Gwynne had to answer positively, having little good choice. reply timschmidt 16 hours agorootparentThe outer ring of engines on Superheavy do not tilt. They're rigidly mounted and exclude even the engine start hardware which is contained in the launch mount, further reducing weight. And they're only used during liftoff, not for boostback or landing. reply avmich 16 hours agorootparentRaptor is non-functional without tilting. That is, it's possible to build rocket where all Raptors are fixed, but that defeats some ideas of Raptors. And I haven't seen tilt mechanisms considered parts of the rocket. So, while engines without tilting have been used - e.g. NK-15 - Raptors aren't from that category. reply modeless 14 hours agorootparentRaptor is used in a non-tilting configuration on both the booster and ship. You could certainly design a rocket to get to space with only non-tilting raptors, if you wanted. It would be silly to consider it incomplete without tilt, even if Starship does have some tilting ones too. Consider it two variants if you like, both complete on their own. reply timschmidt 11 hours agorootparentprevDifferential thrust has been used to steer rockets for decades, and raptor has the sort of deep throttling capability required for it. Tilting is absolutely not required for anything but landing. reply m4rtink 5 hours agorootparentYeah - you would still need roll-control, but given there are huge potato smashers already bolted on, it should not be an issue while in enough atmosphere. ;-) reply timschmidt 4 hours agorootparentYup. In addition to the grid fins, there is the warm gas RCS which could perform roll control. reply Davidzheng 16 hours agorootparentprevhave they tried a giant net? lol reply Titan2189 16 hours agorootparentYes: https://youtu.be/oTH3mq7SsK4?t=14 reply Aaargh20318 19 hours agoparentprevThere are several advantages: - Turnaround time is a major one for SpaceX. They want to stack a new Starship on top of it and launch the booster again in hours, not weeks. By catching the booster they can simply lower it back onto the launch mount, refuel and relaunch. - No need for landing legs. The legs add significant weight, especially on something as large as Super Heavy. Leaving these out means more usable payload to orbit. reply carabiner 18 hours agorootparentIf the chopsticks are catching the rocket by the grid fins, doesn't that mean the grid fins (and associated structure) have to be strengthened (weight added) to support the entire weight of the vehicle? That would negate some of the weight savings of removing the legs. Does this end up being more efficient because more of the loading is in tension instead of compression? reply rockemsockem 18 hours agorootparentThe chopsticks actually don't catch the booster by the grid fins, there are little struts that stick out from the rocket that don't stick out nearly as far. I thought the same thing before the first catch, if you go look at the catch footage you can see the booster resting those on the chopsticks. reply eichin 17 hours agorootparentOne of the things that came up in one of the livestreams was that some of the changes to the starship heat shielding were to test a couple of different spots for those struts - because the booster doesn't do orbital-speed reentry, starship itself does, so to catch that, you need to avoid burning off the struts... reply carabiner 18 hours agorootparentprevSo the struts (plus supporting structure) are lighter than the legs? Why is that? reply addaon 18 hours agorootparent> So the struts (plus supporting structure) are lighter than the legs? Why is that? Besides the other answers you've received, the lugs hold the booster from (near) the top. This means that the body of the booster is in tension during and after landing. Legs, on the other hand, support the landing load and weight after loading in compression. The booster is basically a thin-shelled tube, which is limited in compression strength (for a given wall thickness) by buckling; in tension, the strength approaches the strength of the material, so less additional reinforcement is needed in the structure to support landing loads. reply lutorm 8 hours agorootparentThe booster is already strong enough to support itself in compression, because that's what it does during ascent and the landing burn. The entire bottom structure of a rocket (the \"octaweb\" for F9) is basically made to transfer the thrust compression loads of the engines into the tanks. reply avmich 17 hours agorootparentprevThe tanks can surely be pressurized at landing, which greatly helps to avoid buckling. reply IX-103 17 hours agorootparentPressurized with what? They've already used their fuel for the landing. They can't put anything else in the tanks without worrying about contamination for the next flight. reply agrajag 13 hours agorootparentThey autogenously pressurize the tanks - they heat up the cryogenic propellants with the engines and use some of the gas to pressurize the tanks. In Starship’s case it’s methane and oxygen. reply r2_pilot 15 hours agorootparentprevHelium is a common pressure/purge gas in the fuel/oxidizer tanks. reply avmich 16 hours agorootparentprevPressurization gases? The fuel goes from tanks to engines (engine pumps) because tanks are under pressure, right? Even if the liquids are spent - they are rarely spent in full - the gases remain. reply robocat 15 hours agorootparentprev> surely Rewrite: \"Why doesn't [huge successful project] do [simple thing]?\" At least link to some details of the design? Here's the best diagram of the tank design I could find: https://www.elonx.net/wp-content/uploads/SpaceX-BFR-spaceshi... Which doesn't show the design constraints but who wants those - edit and it's not an image of the booster? Elon mentions a design feature missing from the diagram: https://x.com/elonmusk/status/1093643894917492736 I would personally guess you'd need to be very careful with your implied load bearing connections between the tanks at x Kelvin and the skin at redhot reentry temperatures... Good luck on buying spaceY.com and competing against those engineering fools at SpaceX ;) I am mocking unreasonably, and I know I would find similar comments in my own internet history. I am hoping you will learn to be a little less thoughtless in your armchair. We all assume other rocket-science engineers must not know what they are doing but usually that just shows our own ignorance. reply avmich 15 hours agorootparentNah, misses. We discuss technical possibilities, not flame on forums. The previous post was an answer itself. Or you're implying that tank pressurization isn't a standard practice and not a simple thing?.. reply simmonmt 16 hours agorootparentprevBut why, if you don't need to reply avmich 16 hours agorootparentAghm, sorry, I meant, the tanks actually are pressurized at landing and not at risk of buckling. Why depressurize? reply fastball 16 hours agorootparentThe pressure is enough to help push out liquid fuel but I don't think that means the pressure can be fully relied upon to provide structural support. reply avmich 15 hours agorootparentThat's literally how original Atlas rockets and modern Centaur stages work. reply fastball 15 hours agorootparentStarship is literally not an Atlas rocket or Centaur stage. Starship uses autogenous pressurization, which is not what Atlas/Centaur used. reply m4rtink 5 hours agorootparentSure, but I am quite sure pressure is expected to be in place & provides the necessary strength for all the maneuvers. reply LgWoodenBadger 17 hours agorootparentprevWhat is the booster’s body in during launch/flight? reply addaon 16 hours agorootparentTension from internal pressure -- these aren't really balloon tanks, but they absolutely benefit from internal pressure. reply askvictor 15 hours agorootparentWhat about during the slow-down burn? reply panick21_ 10 hours agorootparentThe same, its just a much higher proportion of gas rather then liquid. Basically on the pad its mostly full with liquid, as it launches, it pumps back part of the gas created in the engine back into the tank. That called 'Autogenous pressurization'. So they don't need an extra gas like helium, as for example Falcon 9 needs. The big issue during landing is that you need to make sure that the engine doesn't suck in gas. That causes bubbles and can destroy the engine. This was actually the failure that caused some of the earlier SN flights to explode or not produce enough power from the engine. You need to either have header tanks, like the booster. Or some kind of method to push the liquids into the right place. If you want to deep dive into the whole problem, 'CSI Starbase' on youtube has a brilliant series on all the engineering problems with all of this. Its a very complex problem. reply carabiner 15 hours agorootparentprevOk, so I was right: Does this end up being more efficient because more of the loading is in tension instead of compression? This bugged me because everyone was saying the deletion of legs was key, but to me the struts are basically legs mounted up high. It's taking advantage of tensile loading that promotes the weight reduction. reply adgjlsfhk1 12 hours agorootparentthe other big difference is that legs need to extend below the engine which means they need up move, which makes them much bigger and more complicated than the catch pins reply vvillena 7 hours agorootparentprevAlso, the struts are much smaller than the legs, and there's no need for moving parts or hydraulic mechanisms. reply krisoft 18 hours agorootparentprevThe legs would need to be much longer (because you can’t push the engine nozzles all the way into the ground and still hope for good things.) Longer structure means more mass, but also larger torque which need to be handled with the support of the structure. Legs need to move to deploy. The struts are just there, static things are much simpler. Simpler things weigh less. Legs need to contain shock absorbers. With the struct solution the shock absorber is in the chopsticks. It doesn’t matter how much the shock absorber weighs when you don’t need to carry it up with you. reply avmich 17 hours agorootparentIt would be good to see the numeric analysis of variants here. Legged landings are surely possible - say, with longer legs (twice as long as struts?), possibly static (legs are always deployed, even when launching), with shock absorbers which aren't that heavy... Would be good to see good and bad qualities next to each other. reply rockemsockem 16 hours agorootparentI think it all just comes down to weight. If you can trim mass on the vehicle then you should do it. reply adolph 17 hours agorootparentprevIt was pretty neat how Bezos explained to Everyday Astronaut how they were using 6 legs so the legs weren’t as long. On the whole, pad catch is the way to go for non-expeditionary vehicles. For orbital uses each booster basically becomes a pyrotechnic elevator. reply mhio 18 hours agorootparentprevThe pins/struts are a 2 point system that double as the booster lift points in general operations. The booster mostly hangs in tension which the existing tank structure can support. I would guess they share some of the structure beefiness with the grid fins. Legs require at least 4 points, probably more. Shock absorption hardware, ability to unfurl to an acceptable width. Require reinforcement (cross bracing) near the base of the tanks to handle the loads pushing inwards toward the center of the tanks. reply fooker 18 hours agorootparentLegs require at least 3, not 4 points. reply avmich 17 hours agorootparentYou can technically imagine two legs with really wide feet, allowing some perpendicular stability. I wonder if one-leg lander could be imagined. 3-legged landing scheme was used in Surveyors, first American automatic Moon landers, and was surely considered for Appolo LEMs, but rejected. So there could be additional, secondary reasons when choosing the number of legs. reply timschmidt 16 hours agorootparentThe cost of one additional leg is pretty inexpensive for the redundancy it provides for the other three. reply chipsa 14 hours agorootparent4 legs have no additional redundancy over 3. One leg failing will still result in the booster tipping over. They do push the maximum angle of tipping before your CG is no longer supported out farther though. reply kevin_thibedeau 13 hours agorootparentUseful for when you land on the rim of a crater like Apollo 12. reply allannienhuis 17 hours agorootparentprevI think you are both using different meanings for the word 'least'. reply willy_k 13 hours agorootparentNitpick, I think the issue is different meanings for the word ‘require’. Practical/realistic minimum vs absolute/literal minimum. reply bunabhucan 18 hours agorootparentprevThe \"struts\" are needed anyway to lift/move the rocket using cranes, they aren't optional. reply ceejayoz 18 hours agorootparentprevThey’re much smaller and don’t have to move/deploy. reply coderjames 18 hours agorootparentprev> If the chopsticks are catching the rocket by the grid fins The chopsticks don't catch the rocket by the grid fins. There are dedicated supports (pins) sticking out the sides of Super Heavy that support the load. It does negate some of the savings from removing the legs, but by returning not only near the launch site like Falcon 9 but literally to the launch tower itself, they can save a whole bunch of time on transporting the stage back to where it will be launched again. They want to launch these things at such a rapid pace that every hour they can save in the refurb / repair / refuel part after landing matters. reply jcims 18 hours agorootparentprevThis is a pic of the top of the booster. You can see the landing lugs between the grid fins. https://x.com/TrainOfError/status/1846030879602209054 reply pseudosavant 18 hours agorootparentprevI thought it was the grid fins initially too until I saw that there are actually just two pins that the entire thing rests on. Check this clip around 19:30. https://youtu.be/dpxB1S-ohEU?si=yozlCWmDCNeEFO4B&t=1169 reply Teever 18 hours agorootparentprevSort of. They need to lift the rocket to put it in place on the launch structure so that is already built into the design. reply whaaaaat 17 hours agorootparentprevI've never understood this, because the economics of hourly launches just don't make sense. There's not nearly enough demand, even assuming they drive prices down and induce demand. Today, there are about ~1,100 metric tons of satellites launched into orbit annually. Starship is aiming for $100/kilogram cost per kilo to orbit. Let's get absolutely wild and assume that Starship takes over the entire world's launches. It would earn what, 1,100 tons * 1000 kilos/ton * $100/kilo = $110,000,000. $110M is... not a tremendous amount of money. It's definitely not enough to be building a fleet of rockets up. Only about 20% of satellite costs are due to launch (and that was found in a pre-SpaceX era), so it's not likely satellite builders are going to optimize solely on cost. It's not an order of magnitude cost savings for builders. So SpaceX will have to find other means to compete -- reliability, capability, etc. The US puts up 10 Starships to launch, and it's better to have them closer together so that you don't have fuel in space being heated by the sun for days. It remains to be seen if they will actually reuse a booster on the same day, but there is a use case for it. reply HPsquared 6 hours agorootparentI wonder how much improvement a \"sunshade\" type thing could make in reducing boil-off. EDIT: or even just orienting the heat shield itself towards the sun, it probably has a fair amount of insulation ability at normal temperatures too. reply avmich 15 hours agorootparentprev> it's better to have them closer together so that you don't have fuel in space being heated by the sun for days. A hydrogen stage for Soviet N-1 rocket was designed so that it would be used near the Moon. The shelf life was going to be about 11 days (I think astronautix.com has this datapoint). Starship is bigger, and methane/LOX is hotter than liquid hydrogen. Will it be storable for a month?.. reply wffurr 16 hours agorootparentprevThe 80% cost of satellites is in large part optimizing them for infrequent high cost launches. Bringing the cost of launch down means we can launch a lot more stuff and that stuff doesn't have to built to the same quality as e.g. the James Webb or even an Intelsat GEO satellite. This kind of launch capacity is going to change the entire economics of building stuff for space. https://caseyhandmer.wordpress.com/2021/10/28/starship-is-st... has some interesting writing on this. reply dark_star 16 hours agorootparentprevIt's not nonsense. To refuel Starship to land on the Moon like the NASA HLS program proposes [1], it will take 16 Starship Tanker flights [2][3]. So 16 launch, transfer propellant, land, refuel and refill propellant on the ground, and repeat. For Mars launches, which is what Starship is mainly designed for, it's also 8-16 Tanker flights to fully fuel a Mars Starship. But SpaceX anticipates sending fleets of ships each synodic period (2 years), when Earth and Mars are closest. For a fleet of 10 Starships, that would be 10 launches of the Mars Starships, then 160 launches of Tanker Starships to fuel them. You might debate whether Mars colonization is possible or desirable, but Starship and the high launch rate is designed for refueling Moon and Mars landing vehicles. [1] https://en.wikipedia.org/wiki/Human_Landing_System [2] https://en.wikipedia.org/wiki/SpaceX_Starship#Planned_launch... [3] https://en.wikipedia.org/wiki/Starship_HLS reply whaaaaat 16 hours agorootparentI think you are being taken for a ride, but hey, if SpaceX does this in the next 15 years come find me. reply mulmen 16 hours agorootparentprevStarship is intended to be human-rated. It’s possible to get anywhere on Earth in an hour. One possible use of Starship is to compete with long-haul aircraft routes. Rapid reusability becomes very important in that situation. For Mars and Moon missions multiple Starships have to launch to refuel the Starship that will actually take the trip. Like, a dozen or more. Again, rapid reusability of the booster is appealing in this situation. reply dlisboa 16 hours agorootparentThe Starship Earth-to-Earth idea is a complete non-starter. Landing what amounts to an ICBM anywhere near a populated area is not something that’ll be allowed for multiple lifetimes, if ever. Maybe in the US because Musk is the government now. Just risk aversion will inhibit it, plus the economics for it will never make sense either. Just because Musk says some things doesn’t mean they should (or will) exist. His predictions are mostly marketing. reply LeroyRaz 15 hours agorootparentHow is a starship in anyway like an ICBM? ICBMs are weapons, and are dangerous because you target them at specific targets and they explode. How is starship coming down significantly more dangerous than a plane? If they can demonstrate similar levels or reliability (a huge ask), then I don't see a problem. reply lazide 15 hours agorootparentICBM’s take a payload (warhead) and deliver it from the launch point to somewhere on the earth very far away (on another continent). If you don’t see how earth to earth starship use couldn’t be construed as a type of ICBM, I suspect you’re thinking branding means a lot more than capability. reply mulmen 14 hours agorootparentThe B-2 famously bombed Afghanistan from Missouri. That doesn’t make the 747 a weapon. reply lazide 13 hours agorootparentIf you don’t think a 747 should be considered a potential weapon, then…. 9/11. Literally. Same as trucks/cars and carbombs. For the same reason, anything like we’re discussing will also be considered a potential weapon by any country paying attention at all. And counter measures and restrictions will be installed. reply mulmen 11 hours agorootparent> If you don’t think a 747 should be considered a potential weapon, then…. 9/11. Literally. Yet we still have airplanes, boats, cars, sports equipment, lawn tools, and kitchen utensils. Nothing about Starship makes it more likely to be weaponized than anything else we already account for in our daily lives. > For the same reason, anything like we’re discussing will also be considered a potential weapon by any country paying attention at all. And counter measures and restrictions will be installed. Is “potential weapon” really the way countries view vehicles crossing borders? I have never gotten that impression. Border crossings maintain some healthy skepticism but not because a Camry is similar to an M1 Abrams if you squint really hard. reply lazide 11 hours agorootparentI’m honestly not sure what relation your comment has to what I wrote or this part of the thread. ICBMs and M1 Abrams also exist? They also are used carefully and heavily regulated. Heavy aircraft are also heavily regulated, and their presence near occupied areas is heavily controlled - including with fighter jets and AA installations on standby in many areas. Car bombs are a huge issue in many parts of the world, and approaching some facilities in a car in those places without going through exactly the right procedures will get you shot before you can get too close. I’m not saying it shouldn’t be built, rather that if you expect it to be able to be allowed to go anywhere and do anything without significant security measures and/or even bans, that isn’t how this works. Because it wouldn’t be hard for it to be defacto a ICBM, just like it wasn’t hard to turn those planes on 9/11 into massive cruise missiles. You can’t really turn a car into an ICBM the same way, correct? reply mulmen 10 hours agorootparent> I’m not saying it shouldn’t be built, rather that if you expect it to be able to be allowed to go anywhere and do anything without significant security measures and/or even bans, that isn’t how this works. Why would I expect this? Did I say something to make you think I believe this? Clearly rocket travel would be regulated, is that not obvious? ICBMs are scary because of their payloads. A weaponized Starship wouldn’t do anywhere near the damage of an ICBM’s nuclear payload. reply lazide 10 hours agorootparentPer your comment above. “The B-2 famously bombed Afghanistan from Missouri. That doesn’t make the 747 a weapon.” Then later I pointed out that 747’s literally had already been used as weapons to commit one of the most notorious crimes in modern history. Then later you said “Nothing about Starship makes it more likely to be weaponized than anything else we already account for in our daily lives”. Except it does - because it literally can be trivially turned into an ICBM way easier that anything in our normal daily lives. Just like an airliner being hijacked can give a terrorist a huge cruise missile they otherwise would not. And ICBMs are not just dangerous because of nukes. But would also allow a non-state actor who somehow gets ahold of a nuke, or dirty bomb, or anthrax, or whatever to potentially deliver it in an ICBM way. But they could also be targeted at someone with actual nukes to force them into a response which could potentially kick off an actual nuclear war, yes? None of which is feasible with what anyone normally experiences in their daily lives. reply mulmen 9 hours agorootparent> Then later I pointed out that 747’s literally had already been used as weapons to commit one of the most notorious crimes in modern history. I see, you aren’t differentiating between something created as a weapon and weaponizing otherwise peaceful objects. > Except it does - because it literally can be trivially turned into an ICBM way easier that anything in our normal daily lives. Just like an airliner being hijacked can give a terrorist a huge cruise missile they otherwise would not. I don’t think a Starship could be turned into an ICBM at all. Anyone who tried to replicate that trajectory in a Starship would be turned into jelly by G forces shortly before being incinerated by atmospheric drag. > And ICBMs are not just dangerous because of nukes. But would also allow a non-state actor who somehow gets ahold of a nuke, or dirty bomb, or anthrax, or whatever to potentially deliver it in an ICBM way. This is already possible with existing rockets. Is your concern that a terrorist would sneak a WMD onto a rocket? Because if they can do that they can also sneak it onto an airliner and do the same damage. > But they could also be targeted at someone with actual nukes to force them into a response which could potentially kick off an actual nuclear war, yes? How does Starship uniquely make this a possibility? Like someone hijacks a Starship in Texas and then suicide bombs Beijing in some kind of false flag operation? Starship is clearly not an ICBM. It doesn’t have the same flight characteristics and doesn’t originate at an ICBM site. China can see that. Given the proposed capabilities of Starship I don’t see a novel threat. Our existing defense mechanisms remain effective. reply lazide 9 hours agorootparentwow. reply samatman 14 hours agorootparentprevTwo paratroopers and some electronics can turn a passenger jet into a bomb carrying many kilotons of conventional explosive. Trucks have blown up buildings. Anything is a weapon if you arm it. Conversely... reply mulmen 10 hours agorootparent> Two paratroopers and some electronics can turn a passenger jet into a bomb carrying many kilotons of conventional explosive. A fully fueled 747 only carries about 190 tons of fuel and 140 tons of cargo. How do electronics turn that into kilotons of explosives? reply samatman 4 hours agorootparentCentitons of explosive then. Much like a Starship. reply mulmen 1 hour agorootparentI don’t think jet fuel is as explosive as TNT so these two paratroopers would have to fill the cargo hold with tens of tons of explosives. reply lazide 48 minutes agorootparentJet fuel has significantly more total energy per unit of mass than TNT. 46 MJ/kg for Kerosene [https://en.m.wikipedia.org/wiki/Heat_of_combustion], and 14.5 MJ/KG for TNT [https://en.m.wikipedia.org/wiki/TNT]. Kerosene can be used to make a thermobaric bomb in the right conditions. It’s just just trickier to actually do than detonating TNT. Notably, TNT can certainly help accomplish making a thermobaric bomb. Either way, the cargo capacity by weight for a 747 is still the same. reply Aaargh20318 9 hours agorootparentprev> The Starship Earth-to-Earth idea is a complete non-starter. Landing what amounts to an ICBM anywhere near a populated area is not something that’ll be allowed for multiple lifetimes The US military actually has a contract with SpaceX to develop this to enable cargo drops, and in a later stadium even personnel, in 1 hour anywhere on the planet. I suspect that if you're at the point where the US military intents to drop cargo or soldiers into your country within an hour, they're not going to be too concerned with asking for permission. reply JacobThreeThree 16 hours agorootparentprevJust setup oil-rig style landing terminals in the various oceans. He's already landing Starships in oceans. People today pay $15,000+ USD per seat now for 1st class, and it still takes them 18+ hours. reply tsimionescu 14 hours agorootparent1st class is 18h of extreme comfort. Starship would be a few hours of extreme discomfort. It's very likely much of the target audience wouldn't even survive the accelerations if they were allowed to attempt it. reply hparadiz 10 hours agorootparentIt's 60 minutes to any point on the planet. 100 people fitting comfortably with a lot more room than a current airliner. The G forces are meant for humans. Very different considerations. No need to bring food or have bathrooms when the flight is that quick. reply tsimionescu 10 hours agorootparentThe G forces are meant for astronauts, not for regular people rich enough to buy this flight. And the whole point of first class is that you pay for luxury. The duration of the flight barely matters, the luxury is the point, and a rocket just can't offer that. There are very few situations where rich people would be willing to put up with the discomfort for a shorter trip. Especially given that the total trip time will likely be much longer than the flight itself. Consider that you can't take off or land Starship anywhere near a densely populated area, it has to be at least a few hours away by car from anywhere that people actually live. So you can take a chauffeur to the airport, go trough priority and special luxuries as a first class passenger until your flight for say 1h total, board your 15h flight spent in luxury, and then a limo waits to take you to your destination 30m away from the landing airport. Or, you can get driven for 3 hours out to the Starship launch site, board the rocket, probably in a special life support suit, wait some hours on the ship for it to be filled (humans are never allowed to approach an already full rocket), fly for one hour in an extremely bare bones flight that literally feels like a roller-coaster (so forget any kind of phone access, you'll be lucky not to puke while just holding on). Then you'll arrive at your destination landing area, ready for some limo to take you on another three hour trip back to civilization. So you've saved maybe 8-10 hours, being extremely generous and only for the longest haul flights possible, but got none of the luxuries you'd expect. And you get to pay much more for the whole deal. Remember that the Concorde halved or less the Paris-New York trip, and gave all the luxury you could want, and still went out of business. reply hparadiz 8 hours agorootparentHave you actually bothered to look it up? I have. https://preview.redd.it/b9f14da6y0ac1.png?width=1105&format=... You only experience anything over 1.5 Gs for 180 seconds. Please stop making stuff up and look at the data. reply tsimionescu 7 hours agorootparentAccording to SpaceX themselves [0], the axial acceleration can reach up to 6g, though they do say it can be throttled, so what can be achieved in practice remains to be seen. Some graph on reddit with no other context is hard for me to trust. [0] https://www.spacex.com/media/starship_users_guide_v1.pdf reply hparadiz 12 minutes agorootparentThe link I provided sources the data from an actual Starship flight. Just cause it's hosted on Reddit doesn't change the data. The link you provided was put together before flights even started from simulations. mulmen 14 hours agorootparentprevStarship is to an ICBM what a 747 is to an F/A 18. Noise is a major concern for sure. But when the competition takes 18 hours you can put the launch and landing sites in very remote places where that’s less of a concern then feed them with planes or trains. Regardless of how practical you think this is it is the reason SpaceX is pushing rapid reusability. reply whaaaaat 13 hours agorootparentprev> One possible use of Starship is to compete with long-haul aircraft routes. The per pax price here would be astronomical. Starship launches are in the tens of millions of dollars per launch, and human rated spacecraft vehicles cost even more. Even if you are putting a thousand people onto the spacecraft (which is a stretch), you are looking at 10s to 100s of thousands of dollars per ticket. Then you'd need the infrastructure to actually operate the rockets. That includes refurbishment, grounds crews, basically a whole Kennedy Space Center operating to launch these things. And on top of that, you'd need an urban area willing to deal with constant sonic booms. Even one launch/landing cycle from these rockets is multiple sonic booms. The noise would be unbelievable. No urban center is going to allow regular starship launches out of it, so you'd have to go a loooong ways out. Which then means either a long boat ride or a short flight back to the city center. Which entails baggage transfer and potentially significant delays. On top of that, space flight is not easy on the body. You can't just put grandma on a rocket and trust that it'd be a comfortable experience. Both the exit, zero-g, and re-entry portions of spaceflight are significant w.r.t. the forces they exert on the body. It's a neat idea, but like all the neat ideas in the thread mentioned so far it's all marketing. Run the numbers yourself, think through the externalities. It's not like air transport at all. reply mulmen 11 hours agorootparent> The per pax price here would be astronomical. Starship launches are in the tens of millions of dollars per launch, and human rated spacecraft vehicles cost even more. Even if you are putting a thousand people onto the spacecraft (which is a stretch), you are looking at 10s to 100s of thousands of dollars per ticket. That’s the case today but they’re essentially all disposable so far. If it meets expectations the cost will be much lower, approaching the cost of fuel. According to Quora (yuck, I know) fully fueling a Starship snd Super Heavy costs about $1m [1] and a 747 is about $200k [2]. If Starship can carry 1,000 people that’s $1,000 per passenger in fuel. A 747-8 can carry up to about 600 people for $333.00 per passenger. 3x the price in fuel is something but Starship can get to orbit on that fuel load which means anywhere on earth. The 747-8 can “only” go about a third of the way around the earth on a full tank. So it’s within the realm of economic possibility especially considering the enormous time savings. If all we cared about was fuel efficiency we’d use trains and boats for long distance travel. Time is money. > It's a neat idea, but like all the neat ideas in the thread mentioned so far it's all marketing. Run the numbers yourself, think through the externalities. It's not like air transport at all. Correct. The difference is more like an airplane vs an ocean liner or train. I agree it is impractical but it is a reason for rapid reusability. A smaller version of something like Starship could be more practical for earth-to-earth service. It’s already the case that some people can’t fly for health reasons. Space travel won’t be for everyone but the fact is availability will continue to expand. [1]: https://www.quora.com/What-does-it-cost-to-fully-fuel-a-Spac... [2]: https://www.quora.com/How-much-does-it-cost-to-fill-a-747-je... reply imtringued 11 hours agorootparentprevGreat, now we can subject millions of people to hearing loss for the benefit of billionaires being too petty for international flights. reply mulmen 10 hours agorootparentThe ticket price would be more like first class airfare so not limited to billionaires. I don’t think anyone is suggesting operating Starship anywhere near populated areas so hearing loss also isn’t a concern. reply verzali 4 hours agorootparentIf you don't operate it near populated areas then that negates the faster transport. People don't pay a lot of money to fly to the middle of nowhere, they pay a lot of money to go between populated areas. If Starship has to land 200km from its destination city, then you need to plan for several hours of onward travel. reply mulmen 1 hour agorootparentIt doesn’t negate the faster transport because it’s 18x faster. That leaves time to take a short flight on both sides. reply wasmitnetzen 7 hours agorootparentprevStarbase is like 10km/6mi from Port Isabel (5 000 people) and 30km/18mi from Brownsville/Matamoros (700 000 people). That's not that far. reply fastball 16 hours agorootparentprev$100/kg is the cost, not what they are charging. The only missions that will be launched at cost are SpaceX's own payloads (Starlink satellites / Mars colony shenanigans). reply steveoscaro 13 hours agorootparentprevThey’ll need something like 8 or 10 tanker launches to fully refuel an orbiting starship that will then depart Earth orbit. That’s the initial use case for quick turnaround at the launch pad. reply martyvis 17 hours agorootparentprevI think they have a \"build it and they will come\" attitude. While their own Mars goals will need 100s if not 1000s of launches they also see new customers that would want launch and even recover much larger payloads than what are feasible today reply whaaaaat 17 hours agorootparentI think that's a reasonable attitude to a point, but like, it doesn't scale infinitely. Build it and it will come to 50-100x today's launch capacity? And Mars is still a laughable pipedream. Doing 100s of launches will cost SpaceX so much more than they are making selling launches to the rest of the world, it simply makes no sense. And like, I'm a space enthusiast. I think we should be out mining asteroids and setting up space living quarters. I just... hourly starship launches don't make any sort of logical sense. What they do make sense as is a marketing gimmick for Elon to get on stage to appeal to emotions of investors and nerds online. It's a gorgeous dream! I want it to be! But it's just a clever emotional appeal to get you to not think too hard or too critically. reply avmich 16 hours agorootparent> Build it and it will come to 50-100x today's launch capacity Yes, because Starship promises 50-100 times cheaper delivery of kg to LEO. Read https://caseyhandmer.wordpress.com/2021/10/28/starship-is-st... on the subject. reply whaaaaat 13 hours agorootparentLet's take the pessimistic estimates of Falcon 9 Heavy, which are about $3000/kg to LEO. (The optimistic estimates put it closer to $1500.) You are suggesting that pessimistically, Starship is aiming at $30-60/kg to LEO. (Or, using the optimistic estimates, $15-$30/kg to LEO). I don't think in even Elon Musk's wildly optimistic press conferences he pushed a number below ~$100/kg to LEO. I don't know where you get the idea that launch costs are going to come down 50-100x. reply mulmen 16 hours agorootparentprevThe asteroid belt is even further than Mars so you need rapid reusability for that too. reply mgiampapa 16 hours agorootparentThe asteroid belt doesn't have nearly the gravity well to send payloads back from, but it seems much harder to make propellant there in situ. reply whaaaaat 13 hours agorootparentSome asteroids are water rich, some asteroids are mineral rich. Many mineral rich asteroids appear to be 'hydrated', meaning that among the rocks they contain ice. Solar power will be more effective on an asteroid in NEO than on the surface of Mars, but gravity will be lower. I don't know that we, as a species, really know which will be harder. They'll require different technologies, but the raw materials exist in both places sufficient to manufacture fuel. reply whaaaaat 13 hours agorootparentprevYou don't need to go to the asteroid belt to get to meaningful asteroids, and in fact many fantastic candidate asteroids come much, much closer than Mars. reply mulmen 11 hours agorootparentThey don’t come as close as LEO though so you still need rapid reusability. reply dark_star 16 hours agorootparentprevIt depends on what you are thinking critically about - what is your frame of mind. You don't see a viable business here. But SpaceX does see several possibilities. One is supplying a US Moon base and US space stations. Since Starship/Superheavy rockets are so inexpensive to build (about 100M in expendable configuration [1] even doing something like that would be profitable for SpaceX. For Mars colonization, Elon Musk has said his target for Starship to Mars cost per flight was USD 10M. If it can take 100 people, and they each pay USD 200K per person, that's USD 20M, a 10M profit for SpaceX. It might also be that a nation state might want to fund something like that to establish a base there. Again, you may see a viable business in Mars colonization. But SpaceX does. So do other people. It was conventional wisdom that Starlink would not work, but it is now quite profitable. [2] [1] https://en.wikipedia.org/wiki/SpaceX_Starship [2] https://www.nasdaq.com/articles/billionaire-elon-musks-new-s... reply imtringued 7 hours agorootparentYou seemingly ignored the cost of the return ticket, which even with your fantasy numbers would cost many millions of dollars. Cramming one hundred people into a starship is eerily reminiscent of overloaded slave ships. The assumption is that you will die on the journey or at your destination. reply verzali 4 hours agorootparentThey don't really plan for people coming back from Mars, for most of them its a one way ticket... reply tsimionescu 14 hours agorootparentprev> For Mars colonization, Elon Musk has said his target for Starship to Mars cost per flight was USD 10M. If it can take 100 people, and they each pay USD 200K per person, that's USD 20M, a 10M profit for SpaceX. Musk has lied many times about many things. This one in particular makes less than 0 sense - Starship has nowhere near the capacity to take 100 humans to Mars even just including the provisions needed for the trip, unless you assume that those people will essentially sit in their own little cell for 2 months. reply whaaaaat 13 hours agorootparentThank you. I was timed out, but I was going to say there's 0% chance of Starship taking 100 people. Do you have any concept of how much water, oxygen, C02 scrubbing, food, shielding, medicine, and infrastructure you need to support 100 people?! Imagine the device they have on the ISS, multiply it by 20, and then pack all the water it uses in a year in advance onto the ship. Then pack 100 warm bodies in there too? It's simply not possible in the ship they've designed. reply avmich 16 hours agorootparentprevBill Gates once complained that independent developers, other software companies aren't keen about building software for Microsoft's modern graphical environment, Windows, so he had to put his own engineers to work on that. Legend then goes that it's how Office application suit was born :) . With SpaceX Musk surely understands he's aiming way higher than the capacity of the modern space launch market. Your, whaaaaat, reasoning was - and unfortunately even now sometimes is - the standard among the industry professionals. That's why rather early on Starlink - the project which was going to employ Falcon's capacities - was born. With Starship we see some obvious uses for launches - orbital tankers - because Starship doesn't really fly anywhere from LEO without refueling, Solar system probes - we probably going to see many, space telescopes, unmanned satellites of many kinds, manned orbital stations. I hope a Moon base - or several - would be another customer of Starship launches. Elon was talking about picking some slice of the world market of cargo and passenger transportation. Maybe we'll see some other uses which we don't see today. The point, roughly is that, yes, here we have \"build and they'll come\", and SpaceX will help them to come in all possible ways. So I disagree that it's total nonsense, it might be actually a very good idea. reply Narishma 12 hours agorootparent> Legend then goes that it's how Office application suit was born :) . Legend indeed. All the main Office applications either started on the Mac and/or were bought from third-party developers. reply HPsquared 6 hours agorootparentprevVertical integration has always been a key aspect of SpaceX (and Tesla for that matter). Starlink is perfectly in line with that strategy. reply JacobThreeThree 16 hours agorootparentprev>I've never understood this, because the economics of hourly launches just don't make sense. There's not nearly enough demand, even assuming they drive prices down and induce demand. There's no demand for travel that would take you to the other side of the earth in 1-2 hours? reply mullingitover 15 hours agorootparentprev> The US puts upI don't think Musk cares much about winning wars or these Starwars DoD projects. He wants to get to Mars. If DoD pays SpaceX to build something, he might do it, but that's about it. If he's involved in a neo-SDI program I would not expect any of his public statements about his motivations to mean anything at all. He most assuredly has a TS-SCI clearance and probably handlers who are watching his every word and ready to haul him to jail for running his mouth. If I were in that position, I too would be a good soldier and frequently monologue about the agreed-upon cover story of settling Mars. reply panick21_ 1 hour agorootparentHe is involved because DoD asked for people to bid on projects and DoD pays well. SpaceX is part of a contract for missile defense, this is public information. SpaceX delivers the sat bus, as far as I know. But this isn't all that big of a contract. I would have to look up the details again, but this is public, you can go find it. Why would he since 20+ years talk about Mars? He went to Mars Society conventions long before he wasn't even remotely famous. If he cared about SDI, why not talk about it, its not that controversial. If it was an interest of his, nothing stops him from talking about it. You think he openly talks about Ukraine, trans issues, Israel and almost everything else that's controversial. But mentioning SDI is somehow to controversial? What? > probably handlers who are watching his every word You are disagreeing with every journalist who has interacted with Musk. And tons of other people who have interacted with him. In fact its the opposite, its a whole thing that Musk can't shut the fuck up even if he should by any reasonable definition. Have you done literally any research on this topic? > If I were in that position, I too would be a good soldier and frequently monologue about the agreed-upon cover story of settling Mars. What the fuck are you even talking about? 'Agreed up on' with who? People from the Starwars days are very open about what they want and thinking that its a good idea to continue that. They talk openly about it. Musk talking and pushing these ideas publicly that would be a good thing for them. Because the people that need to be convinced are the decision makers at DoD and the congress. If Musk used his lobbying power to push these ideas, people like Griffin would welcome that. But Musk doesn't, because he doesn't really care. And he rather lobbies for Mars. Of course if he is part of an ongoing DoD project then he would be under NDA for that project and couldn't talk about it. That's not a conspiracy, that just how DoD contracting works. But SpaceX has not started bidding on such contracts until recently. You just creating a conspiracy where non exists. The whole conspiracy doesn't even make sense. You don't need Mars as a smoke-screen, you can just say 'we build rockets in order to support DoD and NASA and gain commercial contracts as to make money', that is what other rocket companies do. Talking about Mars in 2002 made Musk look like a delusional idiot. reply askvictor 15 hours agorootparentprevAs if I needed more things to worry about... reply jojobas 18 hours agorootparentprevnext [12 more] [flagged] recursive 18 hours agorootparentIf you think of a way to make that happen, I guess you should do it then. reply talldayo 17 hours agorootparentApparently a good start is promising to build $25,000 EVs, raising capital and tax subsidies stock value, and then bailing on the promise entirely. You might even be able to afford two or three submarines with that kind of money. reply jaredhansen 17 hours agorootparentprevI don't understand this take. Are you saying it's in-principle impossible for them to accomplish these goals? reply jojobas 13 hours agorootparentNot in principle, but it's by no means a small feat. Raptor 3 is supposed to be way more heat-stressed than the RS-25 and hasn't reached the design pressure yet. Promising rapid reuse at this stage of development is very ambitious. They haven't reached anywhere near the turnaround with the much less stressed Falcon's Merlin engines. Rapid reuse of the Starship itself is even more ambitious. reply notfish 18 hours agorootparentprevIf you had spacex’s budget you could make all of these things happen reply jojobas 17 hours agorootparentDidn't they already burn through their Starship development budget? reply malfist 17 hours agorootparentThey're a private company funded personally by the richest man in the world. What makes you think there is such a thing as a \"starship development budget\" and that it's limited? reply jojobas 13 hours agorootparentMusk's interplanetary delirium aside, HLS is the customer for Super Heavy/Starship. They've reported spending all of the NASA's 3 billion and delivered very little of contracted capability so far. SpaceX filings show $3.8B of funding from various mostly undisclosed investors, Musk's own stake can't exceed that can it? The richest man wouldn't be so rich if he didn't watch his money. reply fallingknife 17 hours agorootparentprevThat's what they said about landing a booster reply jojobas 13 hours agorootparentThe technology of rocket landing on their tails is decades old. Turbopumped rocket engines working more than a few cycles of few minutes is radically more ambitious. reply fallingknife 13 hours agorootparentThey have already flown the same f9 booster more than 20 times reply AgentK20 19 hours agoparentprevKeep in mind, splashing the booster down in the ocean almost always results in rapid unscheduled disassembly (explosion) of the booster. It lands upright but has no ability to stay upright on its own, so usually makes a soft water landing and then immediately tips over. The shock of the side of the booster hitting the water usually ruptures one or more pressure vessels, resulting in a nice fireball and destruction of the vehicle. Even the Falcon 9 when it lands \"at sea\" is actually landing on a barge that is able to keep it upright (usually) and out of the water, but any booster vehicle that SpaceX (or anyone) launches that does a soft water landing is a write-off. The only real exception to this is return capsules with astronauts in them which are explicitly designed to land in the water and deploy buoys to keep themselves afloat while they wait for the Coast Guard to come pick them, and the capsule (which is a one-time-use component) up. reply skissane 18 hours agorootparent> The only real exception to this is return capsules with astronauts in them which are explicitly designed to land in the water and deploy buoys to keep themselves afloat while they wait for the Coast Guard to come pick them, and the capsule (which is a one-time-use component) up. With SpaceX Dragon (both crew and cargo variants), the capsule is designed to be reusable, so it is no longer a “one-time-use” component. The same is true of Boeing Starliner and NASA/LockheedMartin Orion. “One-time-use” was true of previous ocean-landing capsules, such as Apollo’s Command Module, with the sole exception of the Jan 1965 Gemini 2 uncrewed testflight’s capsule, which was reused for another uncrewed testflight the next year, as part of USAF’s Manned Orbital Laboratory program (which was cancelled in 1969) Of course, reuse after a spaceflight and ocean landing requires significant refurbishment. Also, both Starliner and Orion are only partially reusable, since both (like Apollo) have a service module designed to burn up on re-entry. Dragon likewise has a trunk, but Dragon’s trunk contains fewer spacecraft systems than Starliner or Orion’s service modules, making it more reusable overall. reply Gee101 17 hours agorootparentprevRocketlab seems to be able to recover their boosters intact after a sea landing. reply ranger207 14 hours agorootparentThe space shuttle solid rocket boosters were also recovered after a sea landing and reused, but solid rockets are more like simple big metal tubes than liquid fueled rockets are reply martyvis 17 hours agorootparentprevBut they haven't reused one yet AFAIK. They have reused an engine. reply usehand 18 hours agorootparentprevtiny correction, but I believe the capsules are also re-used reply davidguetta 19 hours agoparentprev- salt water is horrific for anything - time to transport, move back into initial position. reply fixprix 13 hours agorootparentThis is the answer. Salt water destroys the engines. reply simonw 19 hours agorootparentprevThat makes sense - sending out barges / recovery vehicles into the ocean can't be cheap. The environmental impact of a chopsticks landing is likely a whole lot less damaging too. reply singleshot_ 19 hours agorootparentprevWhat do you figure is worse for chemical changes to materials, being engulfed in a big call of plasma, or being dunked in the ocean? I don’t know any chemistry but there seem to be a lot of smart people round here. What say you? reply lukeschlather 19 hours agorootparentSeagoing ships are good at being in salt water but don't deal as well with being engulfed in plasma. Starship is pretty good at being engulfed in plasma but doesn't handle being engulfed in salt water very well. Making a single ship that can handle both is a very tall order, even just doing one or the other is hard. reply shiroiushi 18 hours agorootparentSeagoing ships are pretty good at floating on top of salt water for a while, but they're not very good at being \"dunked\" in it (i.e. sunken). It is possible to raise sunken ships and rehabilitate them to regular service, and this has been before, but it generally destroys everything inside, and requires a ton of work. Salt water is one of the most corrosive substances we know of. reply wbl 18 hours agorootparentprevDunked in the ocean. There just isn't a ton of plasma and it's not very good at transferring heat to things it isn't being pressed up against. So heat shields and the like function very well. Seawater by contrast gets everywhere, and if it does even a little damage, keeps doing it for a long time. reply chroma 16 hours agorootparentprevA lot of the damage happens as soon as the spacecraft enters the water, because it's extremely hot, causing more chemical reactions than you might think. Even jet engines have trouble with corrosion from ingesting small amounts of aerosolized salt water.[1] 1. https://ntrs.nasa.gov/api/citations/19690007944/downloads/19... reply iterance 18 hours agorootparentprevOcean is worse. (For rockets.) - used to be an engineer in aerospace reply jojobas 18 hours agorootparentprevBeing dunked in seawater while still being red/white hot from the plasma. reply piombisallow 18 hours agorootparentprevDesigning it for both is probably worse than either one separately reply dantheman252 18 hours agoparentprevI don't think they reuse the boosters that they do a splashdown with. So reuse vs trash. reply adolph 17 hours agorootparentRocketlab is working in it. They have sent the first one or two to the production line for refurb. https://www.rocketlabusa.com/updates/rocket-lab-announces-re... reply pfannkuchen 19 hours agoparentprevHard to have a spaceport if your vehicle needs to land in the ocean. reply db48x 16 hours agoparentprevWhy do people ask the same questions every time? Can’t you search? reply diebeforei485 14 hours agoparentprevAbility to re-launch the same day. reply kfrzcode 13 hours agoparentprevEfficiency. If you don't have to recover the booster, load it up and transfer it back to the recovery & restoration bay, you save time and money. Not to mention if you can land the booster directly on the tower, clean it, fuel it up, and send it right back... imagine 4 Starships launching per day! reply odirf 42 minutes agoprevMany people have lost money on Polymarket. The chance of the sticks catching the Starship was around 75% and of course immediately dropped to 0% after the announcement. reply rkagerer 11 hours agoprevAny technical specifics available yet on the reason the catch was aborted? Eg. Which parameters were out of range. reply fernandotakai 9 hours agoparent> Following a nominal ascent and stage separation, the booster successfully transitioned to its boostback burn to begin the return to launch site. During this phase, automated health checks of critical hardware on the launch and catch tower triggered an abort of the catch attempt. The booster then executed a pre-planned divert maneuver, performing a landing burn and soft splashdown in the Gulf of Mexico. from here https://www.spacex.com/launches/mission/?missionId=starship-... reply rob74 10 hours agoparentprevSpaceX hasn't yet released anything AFAIK. My wild speculation: not wanting to risk anything with both Trump and Musk being present at the launch site?! reply ragebol 8 hours agorootparentBut normally they do risk any other people? No, not at all. People are miles from where the robot could potentially go even after it is remotely aborted/blown up. reply RobinUS2 7 hours agorootparentprevSounds a bit farfetched, not like they wouldn't hear it later? reply umeshunni 13 hours agoprevIs there a publicly documented (or guesstimated) timeline for Starship? What comes next? reply Culonavirus 11 hours agoparentAll actual future launch dates and profiles are all TBD. This program is still in its experimental phase. Also all guesstimates are technical only and optimistic - the FAA and/or any kind of serious anomaly can slow things down by months. What has been guesstimated by the observers and/or stated by Gwynne/Elon: - next flight some time in jan/feb - next flight will be a V1 booster with a V2 ship (still Raptor 2s) - next flight profile will be similar to this one, if they land the ship in the ocean precisely again, the they will try to catch the next one afer that (note this is not currently possible as no existing ship has catch hardware installed) - stated flight test cadence they would like to reach in 2025 is 25, observers think 10-12 are more likely, in both cases they need to move pretty fast - both the star factory and the second launch pad should come online fully within the first half of 2025, launch cadence should improve a lot after that - for Artemis 3 they need to demonstrate fuel transfer in 2025, otherwise the timeline of that program will for sure have to be extended (probably even beyond the already expected delay to 2027) - elon wants to send a few starships to mars in the 2026 window, and that may actually happen if everything goes smooth as butter until then, but the 2028 window with humans on board is just a wish, very much \"elon time\", most observers do not think starship will be human rated for such long flights before 2030s reply Seanambers 10 hours agoparentprevMars in two years if that goes well - Humans 2 years later, so humans on Mars before end of decade seems like a bet. They are not that far behind the aspirational goals Elon laid out at IAC in 2016. Then it was called ITS not the starship and steel was not yet thought about. https://youtu.be/WVacRKN1tAo?si=s0MBP8ejQt3zv-sF&t=3309 As you can see from the chart Mars flights from late 2022. But there was a pandemic amongst other things that came along etc. reply yread 6 hours agorootparentSeems they are about 4 years late on a thing 4 years out so you have to double everything? reply Veserv 12 hours agoparentprevYes. Flight around the moon with passengers scheduled for 2023 [1][2]. No point looking to the public statements of a person who is legally recognized [3] as making such fanciful claims that no reasonable person could possibly believe them. [1] https://www.cnbc.com/2021/03/02/yusaku-maezawa-opens-up-publ... [2] https://en.wikipedia.org/wiki/DearMoon_project [3] https://www.theverge.com/2024/10/1/24259588/tesla-lawsuit-au... reply imtringued 7 hours agorootparentI have been promised a fast iterative development cycle. All I'm seeing are very very small incremental gains with each launch. It looks like it will take as long as any space project. It will be delayed and take almost a decade from start to completion. reply da",
    "originSummary": [],
    "commentSummary": [
      "SpaceX's Super Heavy booster performed a soft splashdown in the Gulf of Mexico as a safety measure after automated health checks aborted the landing attempt on the \"chopsticks\" at the launch site.- The test flight successfully demonstrated engine relighting in space, marking a significant milestone for future orbital flights and SpaceX's goal of rapid reusability with Starship.- While SpaceX aims for frequent launches and missions like Mars colonization, the timeline for achieving these goals is uncertain due to potential technical and regulatory challenges."
    ],
    "points": 260,
    "commentCount": 390,
    "retryCount": 0,
    "time": 1732054534
  },
  {
    "id": 42187761,
    "title": "Using Erlang hot code updates",
    "originLink": "https://underjord.io/how-i-use-erlang-hot-code-updates.html",
    "originBody": "How I use Erlang Hot Code Updates 2024-11-19 Underjord is a tiny, wholesome team doing Elixir consulting and contract work. If you like the writing you should really try the code. See our services for more information. One of the Erlang ecosystem’s spiciest nerd snipes are hot code updates. Because it can do it. In ways that almost no other runtime can. I use Elixir which builds on Erlang and has the same capabilities. The standard way of doing Elixir releases via mix release does not support Erlang hot code updates. As in, it will not generate the necessary files for you. And if you do want to do it there are several blog posts you need to stitch together or you need to use the Erlang docs in great detail. Learn You Some Erlang has documented much of it of course. AppSignal also has a neat guide on hot code reloading in Elixir. Bryan Hunter and Chris Keathley are both people I listen when they speak because it tends to be interesting, challenging and something I just might not have heard before. Both have described hot code updates as something that people should learn and use. I imagine Whatsapp’s initial engineering crew would agree. They did pretty well. Of course we do use it. But mostly for trivial things. Anyone using IEx r MyModule or recompile are doing hot code updates. But reloading a module in development on you local machine doesn’t feel all that spicy. It is cool and useful but feels like an incremental compile or watcher/builder type of thing. I use it constantly with Nerves. When developing on an embedded Elixir device and needing to tune some numbers, or reworking a module, pasting into the IEx is faster than uploading new firmware and waiting for the reboot. I stop and start parts of the applications or just terminate the relevant GenServer if I need a state reset. I’ve also used it for remote devices over NervesHub’s built in web console that offers an IEx prompt. When debugging a misbehaving Real Time Clock it was pretty nice to just paste chunks of utility functions and relevant I2C-calls over to the device to get clear answers about what it was doing. I’d love to see more tooling for the full-blown delivery of hot code updates on top of Elixir’s mix release tooling. Or in the vein of the predecessor distillery. I don’t think there are any shortcuts for doing a correct hot code update. Much like database migrations they require care. And you’ll need to know how you dependencies react to hot code updates and many other interesting topics. This was just a quick one in case people are curious how Erlang’s hot code updates are actually used, day-to-day, in Elixir. If you have comments or questions you can find me on the fediverse via @lawik or email me at lars@underjord.io. Underjord is a 4 people team doing Elixir consulting and contract work. If you like the writing you should really try the code. See our services for more information. Note: Or try the videos on the YouTube channel.",
    "commentLink": "https://news.ycombinator.com/item?id=42187761",
    "commentBody": "Using Erlang hot code updates (underjord.io)254 points by lawik 22 hours agohidepastfavorite47 comments jhgg 19 hours agoWhen I worked at Discord, we used BEAM hot code loading pretty extensively, built a bunch of tooling around it to apply and track hot-patches to nodes (which in turn could update the code on >100M processes in the system.) It allowed us to deploy hot-fixes in minutes (full tilt deploy could complete in a matter of seconds) to our stateful real-time system, rather than the usual ~hour long deploy cycle. We generally only used it for \"emergency\" updates though. The tooling would let us patch multiple modules at a time, which basically wrapped `:rpc.call/4` and `Code.eval_string/1` to propagate the update across the cluster, which is to say, the hot-patch was entirely deployed over erlang's built-in distribution. reply davisp 18 hours agoparentThis matches my experience. I spent a decade operating Erlang clusters and using hot code upgrades is a superpower for debugging a whole class of hard to track bugs. Although, without the tracking for cluster state it can be its own footgun when a hotpatch gets unpatched during a code deploy. As for relups, I once tried starting a project to make them easier but eventually decided that the number of bazookas pointed at each and every toe made them basically a non-starter for anything that isn’t trivial. And if its trivial it was already covered by the nl (network load, send a local module to all nodes in the cluster and hot load it) style tooling. reply scotty79 7 hours agorootparent> Although, without the tracking for cluster state it can be its own footgun when a hotpatch gets unpatched during a code deploy. This and everything else said sounds so much like PHP+FTP workflow. It's so good. reply stouset 11 hours agoparentprevCan someone explain how this is not genuinely terrifying from a security perspective? reply nelsonic 11 hours agorootparentWhere is the security problem? All code commits and builds can still be signed. All of this is just a more efficient way of deploying changes without dropping existing connections. Are you suggesting that hot code replacement is somehow a attack vector? Ericsson has been using this method for decades on critical infrastructure to patch switches without dropping live calls/connections it works. No need to fear Erlang/BEAM. reply stouset 11 hours agorootparentMy interpretation of the GP was that a code change in one node can be automagically propagated out to a cluster of participating Erlang nodes. As a security person, this seems inherently dangerous. I asked why it is safe, because I presumed I’m missing something due to the lack of ever hearing about exploitation in the wild. reply badpenny 9 hours agorootparentWhy is it any more dangerous than a conventional update, which also needs to be propagated? reply stouset 2 hours agorootparentA conventional update takes place out of band. If someone were to exploit a running Erlang process, the description of this feature sounds to me like they would have access to code paths that allow pushing new code to other Erlang processes on cooperating nodes. reply ramchip 7 hours agorootparentprevErlang distribution shouldn't be used between nodes that aren't in the same security boundary, it promises and provides no isolation whatsoever. It's kind of inherent to what it does: it makes a bunch of nodes behave as part of a single large system, so compromising one node compromises the system as a whole. In a use case like clustering together identical web servers, or message broker nodes like RabbitMQ, I don't think it's all that scary. It gives an attacker easier lateral movement, but that doesn't gain them a whole lot if all the nodes have the same permissions, operate on the same data, etc. Depending on risk appetite and latency requirements you can also isolate clusters at the deployment / datacenter level. RabbitMQ for instance uses Erlang clustering within a deployment (nodes physically close together, in the same or nearly the same configuration) and a separate federation protocol between clusters. This acts as a bulkhead to isolate problems and attackers. reply aunderscored 11 hours agorootparentprevIt's the same amount of terrifying as a regular deploy, you need to ensure that you limit access as needed reply elcritch 15 hours agoprevCode reloading on embedded Nerves devices is fantastic. If you have non-trivial hardware or state you can just hot load new code to test a fix live. Great for integration testing. I literally used hot code reloading a few weeks back to fix a 4-20 mA circuit on a new beta firmware while a client was watching in remote Colorado. Told them I was “fixing a config”. Tested it on our device and then they checked it out over a satellite PLC system. Then I made an update Nerves FW, uploaded it. Made the client happy! Note that I’ve found that using scp to copy the files to /tmp and then use Code.compile to work better than copy and paste in IEx. The error messages get proper line numbers. It’s also very simple to write a helper function to compile all the code in /tmp and then delete it. I’ve got a similar one in my project that scp’s any changed elixir files in my project over. It’s pretty nice. reply rozap 20 hours agoprevI used to work on a pretty big elixir project that had many clients with long lived connections that ran jobs that weren't easily resumable. Our company had a language agnostic deployment strategy based on docker, etc which meant we couldn't do hot code updates even though they would have saved our customers some headache. Honestly I wish we had had the ability to do both. Sometimes a change is so tricky that the argument that \"hot code updates are complicated and it'll cause more issues than it will solve\" is very true, and maybe a deploy that forces everyone to reconnect is best for that sort of change. But often times we'd deploy some mundane thing where you don't have to worry about upgrading state in a running gen server or whatever, and it'd be nice to have minimal impact. Obviously that's even more complexity piled onto the system, but every time I pushed some minor change and caused a retry that (in a perfect world at least...) didn't need to retry, I winced a bit. reply ElevenLathe 19 hours agoparentI work in gaming and have experienced the opposite side of this: many of our services have more than one \"kind\" of update, each with its own caveats and gotchas, so that it takes an expert in the whole system (meaning really almost ALL of our systems) to determine which would be the least impactful possible one if nothing goes wrong. Not only is there a lot of complexity and lost productivity in managing this process (\"Are we sure this change is zero downtime-able?\" \"Does it need a schema reload?\" etc) but we often get it wrong. The result is that, in practice, anything even remotely questionable gets done during a full downtime where we kick players out. It's sometimes helpful to have the option to just restart one little corner of the full system, to minimize impact, but it is helpful to customer experience (if we don't screw it up) and very much the opposite for developer experience (it's crippling to velocity to need to discuss each change with multiple experts and determine the appropriate type of release). reply rozap 19 hours agorootparentNo doubt that traditional deployments are much better for dev experience at (sometimes) the cost of customer experience. reply toast0 18 hours agorootparentI disagree. Hot loading means I can have a very short cycle on an issue, and move onto something else. Having to think about the implications of hot loading is worth it for the rapid cycle time and not having to hold as many changes in my mind at once. reply ElevenLathe 19 hours agorootparentprevOne thing that would help both is deployment automation that could examine the desired changes and work out the best way to deploy them without human input. For distributed systems, this would require rock-solid contracts between individual services for all relevant scenarios, and would also require each update to be specified completely in code (or at least something machine readable), ideally in one commit. This is a level of maturity that seems elusive in gaming. reply hauxir 19 hours agoprevWe use hot code upgrades on kosmi.io with great success. It's absolute magic and allows for very rapid development and ease of deploying fixes and updates. We do use have to use distillery though and have had to resort to a bunch of custom glue bash scripts which I wish was more standardized because it's such a killer feature. Due to Elixirs efficiency, everything is running on a single node despite thousands of concurrents so haven't really experienced how it handles multiple nodes. reply edude03 17 hours agoprevNerves and hot code reloading got me into erlang after I watched a demo of patching code on a flying drone ~8 years ago. While I can't imagine hot reloading is super practicle in production, it does highlight that erlang/beam/otp has great primitives for building reliable production systems. reply atonse 15 hours agoparentI have told so many people about that video over the years. It was one of the most amazing demonstrations of a programming language/ecosystem that I've ever seen. Yet I've never been able to find it again. reply trq01758 12 hours agoparentprevProbably not the same video, but the one on this topic on youtube is https://youtu.be/XQS9SECCp1I reply opnitro 14 hours agoparentprevDo you have a link? reply throwaway81523 19 hours agoprevYou have to be very very very careful when preparing relups. The alternative on Linux is to launch an entire new server on the same machine, then transfer the session data and the open sockets to it through IPC. I once asked Joe Armstrong whether this was as good as relups and why Erlang went the relup route. I don't remember the exact words and don't want to misquote him, but he basically said it was fine, and Erlang went with relups and hot patching because transferring connections (I guess they would have been hardware interfaces rather than sockets) wasn't possible when they designed the hot patch system. Hot patching is a bit unsatisfying because you are still running the same VM afterwards. WIth socket migration you can launch a new VM if you want to upgrade your Erlang version. I don't know of a way to do it with existing software, but in principle using something like HAProxy with suitable extensions, it should be possible to even migrate connections across machines. reply toast0 18 hours agoparentState migration is possible, and yeah, if you want to upgrade BEAM, state migration would be effective, whereas hot loading is not. If your VM gets pretty big, you might need to be careful about memory usage though, the donor VM is likely not going to shrink as fast as the heir VM grows. If you were so inclined, C does allow for hot loading too, but I think it'd be pretty hard to bend BEAM into something that you could hot load to upgrade. Migrating socket state across machines is possible too, but I don't think it's anywhere close to mainstream. HAProxy is a lovely tool, but I'm pretty sure I saw something in its documentation that explicitly states that sort of thing is out of scope; they want to deal with user level sockets. Linux has a TCP Repair feature which can be used as part of socket migration; but you'll also need to do something to forward packets to the new destination. Could be arping for the address from a new machine, or something fancier that can switch proportionally or ??? there's lots of options, depending on your network. As much as I'd love to have a use case for TCP migration, it's a little bit too esoteric for me ... reconnecting is best avoided when possible, but I'm counting TCP migration as non-possible for purposes of the rule of thumb. reply throwaway81523 18 hours agorootparentTCP migration on the same machine is real and it's not that big a deal, if that's what you meant by TCP migration. Doing it across machines is at best a theoretical possibility, I would agree. I have been wanting to look into CRIU more carefully, but I believe it uses TCP Repair that you mentioned. I'm unfamiliar with it though. The saying in the Erlang crowd is that a non-distributed system can't be really reliable, since the power cord is a single point of failure. So a non-painful way to migrate across machines would be great. It just hasn't been important enough (I guess) for make anyone willing to deal with the technical obstacles. I wonder whether other OS's have supported anything like that. I worked on a phone switch (programmed in C) a long time ago that let you do both software and hardware upgrades (swap CPU boards etc.) while keeping connections intact, but the hardware was specially designed for that. reply toast0 18 hours agorootparent> I wonder whether other OS's have supported anything like that. I don't think I've seen it, but I don't see everything, and it'd be pretty esoteric. From my memory of working with the FreeBSD tcp stack, I suspect it wouldn't be too hard to make something like this work there, too; other than the security aspects, but could probably do something like ok to 'repair' a connection that matches a listen socket you also pass or something. But you'd really need the use case to make the hassle worth it, and I don't think most regular server applications are enough to warrant it. reply robocat 19 hours agoprevBackground to the article: https://underjord.io/unpacking-elixir-iot-embedded-nerves.ht... Seems like they deploy Elixir on embedded Linux. The embedded Linux distro is Nerves which replaces systemd and boots to the BEAM VM instead as process 1, putting Elixir as close to the metal as they can. I know nothing about any of the above (assumption is I'm fool enough to try and simplify) plus I know I've misused the concepts I wrote but that's my point so read the article. All simplifications are salads reply rkangel 8 hours agoparentDon't worry - this is an accurate and concise summary of what Nerves is. What I would add is: you do not have bash (or sh, or any other \"conventional\" shell). The BEAM is your userspace. You can SSH in, but what you end up with is an IEx prompt (the Elixir REPL). This is surprisingly fine once you get used to it (and once you've built a few helpers for your usecase). reply whorleater 20 hours agoprevWhatsApp very long ago used to hot reload across all nodes with a ssh script to incrementally deploy during the day reply aeturnum 19 hours agoprevIMO Hot Code Updates are a tantalizing tool that can be useful at times but are extremely easy to foot-gun and have little support. I suspect that the reason why no one has built a nice, formal framework for organizing and fanning out hot code changes to erlang nodes is that it's very hard to do well, involves making some educated guesses about the halting problem, and generally doesn't help you much unless you're already in a real bind. Most of the benefits of hot code updates (with better understanding of the boundaries of changes) can be found through judicious rolling restarts that things like k8s make easier these days. Any time you have the capacity to hot patch code on a node, you probably have the capacity to hot patch the node's setup as well. That said I think that someone could use the code reloading abilities of erlang to make a genuinely unparalleled production problem diagnostic toolkit - where you can take apart a problem as it is happening in real time. The same kinds of people who are excited about time traveling debugging should be excited about this imo. reply toast0 21 hours agoprev> Both have described hot code updates as something that people should learn and use. I imagine Whatsapp’s initial engineering crew would agree. They did pretty well. Yeah. Hot loading is clearly better than anything else when you've got a million clients connected and you want to make a code change. Of course, we didn't have any of these fancy 'release' tools, we just used GNU Make to rsync the code to prod and run erlc. Then you can grab a debug shell and l(module). (we did write utilities to see what code was modified, and to provide the right incantations so we wouldn't load if it would kill processes) reply rybosome 20 hours agoparent> Hot loading is clearly better than anything else when you've got a million clients connected and you want to make a code change. In the contexts in which I’ve worked, this was solved by issuing a command to the server to enter a lame-duck mode and stop accepting new connections, then restarting the process with updated code after all existing connections ended. This worked in our case because connections had a TTL with a “reasonable” time, couldn’t have been more than an hour. We could always wait it out. I suppose hot reloading is more necessary when you have connections without a set TTL. reply toast0 19 hours agorootparentThat way works, but it means you're spending that much more time on a deploy. For a small change, you can hot load your change and be done in minutes. This means you can push several small changes in an hour. Which helps get things done rapidly. It's also really nice to be able to see that the change works as expected (or not) at full load right away. If you've got to wait for connections to accumulate on the new server, that takes longer without hot load too. Some changes can't be effectively hot loaded[1], and for those you do need to do something to kick out users and let them reconnect elsewhere, and you could do all your updates that way, but it means a lot more client time spent reconnecting. On the one hour TTL. Sometimes that's reasonable, but sometimes it's really not. Someone downloading a large file on a slow connection is better served by letting the download continue to trickle for hours than forcing them to reconnect and resume. A real time call is better served by letting it run until the participants are done. For someone on a low end phone, staying connected for as long as they can is probably better than forcing a reconnect where they'll need to generate new ephemeral keys and do a key exchange exercise. [1] At the very least, BEAM updates and kernel changes are much more easily done by restarting. But not all userspace Erlang changes are easy to make hot loadable, either. reply arnon 21 hours agoprevA few years ago, the biggest problem with Erlang's hot code updates was getting the files updated on all of the nodes. Has this been solved or improved in any way? reply comboy 20 hours agoparentI don't think updating files is the problem. The biggest issue with hot code updates seems to be that they can create states that cannot be replicated in either release on its own. reply ketralnis 20 hours agorootparentThis is my experience. About 25% of the time I'd encounter a bug that's impossible to reproduce without both versions of the code in memory, and end up restarting the node anyway dropping requests in the process. Whereas if I'd have architected around not having hot code updates I could built it in a way that never has to drop requests reply faizshah 19 hours agorootparentprevIn general, you can save your team a lot of ops trouble just by periodically restarting your long running services from scratch instead of trying to keep alive a process or container for a long time. I’m still new to the erlang/elixir community and I haven’t run it in prod yet but this is my experience coming from Java, Node, and Python. reply toast0 19 hours agoparentprevThere's about a thousand different ways to update files on servers? You can build os packages, and push those however you like. You can use rsync. You could push the files over dist, if you want. You could probably do something cool with bittorrent (maybe that trend is over?) If you write Makefiles to push, you can use make -j X to get low effort parallelization, which works ok if your node count isn't too big, and you don't need as instant as possible updates. Erlang source and beam files don't tend to get very large. And most people's dist clusters aren't very large either; I don't think I've seen anyone posting large cluster numbers lately, but I'd be surprised if anyone was pushing to 10,000 nodes at once. Assuming they're well connected, pushing to 10,000 nodes takes some prep, but not that much; if you're driving it from your laptop, you probably want an intermediate pusher node in your datacenter, so you can push once from home/office internet to the pusher node, and then fork a bunch of pushers in the datacenter to push to the other hosts. If you've got multiple locations and you're feeling fancy, have a pusher node at each location, push to the pusher node nearest you; that pushes to the node at each location and from there to individual nodes. Other issues are more pressing; like making sure you write your code so it's hotload friendly, and maybe trying to test that to confirm you won't use the immense power of hotloading to very rapidly crash all your server processes. reply samgranieri 18 hours agorootparentI think Twitter once cobbled together a BitTorrent based deployment strategy for Capistrano called murder, that was a cool read from their eng blog back in the day. I wish I had used a pusher node to deploy things when a colleague was using almost all the upstream bandwidth in the office making a video call when my bosses were giving demo and the fix I coded for an issue discovered during the demo could not deploy via Capistrano reply samgranieri 18 hours agoprevI tried to do this back in 2017 as an elixir newbie with distillery, but for some reason just went with standard deploys with distillery. Now it’s just using mix release to build elixir apps in a docker image deployed to k8s. reply Thaxll 20 hours agoprev [–] Hot code update is one of those thing I don't understand, just use a rolling deployment, problem solved. You have a new version of the code without loosing any connection. It's one of those thing that sound nice on paper but a actually couple your runtime with ci/cd, if you have anything else beside Erlang what do you do? You now need a second solution to deploy code. reply AlphaWeaver 19 hours agoparentI'm not sure that rolling deployments guarantee you won't lose connections, depending on the type of connection. Imagine your customer is downloading a large file over a single TCP connection, and you want to upgrade the application mid-download. With rolling deployments, your only choice is to wait until that connection drains by completing or failing the download. If that doesn't fit your use case, you're out of options. If your application is an Erlang app, you could hot code reload an unaffected part of the application while the download finishes. Or, if the part of the application handling the download is an OTP pattern that supports hot code reloading (like a gen_server) you could even make changes to that module and release e.g. speed improvements mid download stream. This is why Erlang shines in applications like telephony, which it was originally designed for. reply fsckboy 15 hours agorootparent>With rolling deployments, your only choice is to wait until that connection drains by completing or failing the download. If that doesn't fit your use case, you're out of options. one of the cool things about unix is (and perhaps windows can do this in the right modes, idk), the running copy of a program is a link to the code on the disk (a link is a reference to the file, without the file name). You can delete a running program from the disk and replace it with a new program, but the running copy will continue and not be aware that you've done that. You don't need to wait till the program finishes anything. on an every day basis, this is what happens when you run software updates while you are still using your machine, even if your currently active programs get updated. you'll sometimes notice this in a program like Firefox, it will lose its ability to open new tabs; that's because they go out of their way to do that, they wouldn't have to if they wanted to avoid it, just fork existing processes. reply toast0 14 hours agorootparent> one of the cool things about unix is (and perhaps windows can do this in the right modes, idk), the running copy of a program is a link to the code on the disk (a link is a reference to the file, without the file name). You can delete a running program from the disk and replace it with a new program, but the running copy will continue and not be aware that you've done that. You don't need to wait till the program finishes anything. An even cooler thing is the running code is just mmaped into memory. One of the nifty things about mmaped files is if you change the backing file, you change it everywhere. Not my recommended way to hot load code, but it might work in a pinch. unlink, replace, start a new one, have the old one stop listening does work for many things. Some OSes have/had issues with dropping a couple pending connections sometimes; or you have to learn the secret handshake to do it right. A bigger problem is if your daemon is sized to fit your memory, you might not be able to run a draining and a filling daemon at once. It also doesn't really solve the issue of changing code for existing connections, it's a point in time migration. Easier to reason about, for sure, but not always as effective. reply AlphaWeaver 15 hours agorootparentprevRight, but in this example, to \"pick up\" the code after you have updated it, you still have to trigger a restart of the program somehow. Controlling that handoff can prove challenging if you're just swapping out the underlying binary. reply rozap 19 hours agoparentprevBut what if you have long lived stateful connections? And you don't want a deploy to take forever? Ofc you can say \"don't do that\" but sometimes it's just the way it is... But I agree, 99% of the time a rolling update is easier and works fine. reply fiddlerwoaroof 19 hours agoparentprevThe other perspective on this is that, at some level of your system you are always doing a hot code reload: terraform, kubernetes, etc. are taking a new deployment description in and reconciling it with the existing state of the world. Wouldn’t it be nice if this process was just more code in your preferred programming language rather than YAML soup? BEAM encourages you to structure your program as isolated interacting processes and so it’s not that far from a container runtime in itself. reply nthh 19 hours agoparentprev [–] It could be useful if you have an embedded device that you don't want to miss data from, but for most deployments I would agree. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Underjord, an Elixir consulting team, investigates Erlang's hot code updates, a distinctive feature allowing code changes without stopping the system.",
      "Although Elixir, based on Erlang, supports hot code updates, the standard mix release lacks the necessary files, prompting experts to recommend learning this skill.",
      "Hot code updates are particularly useful in development with Nerves for embedded devices, and more tooling in Elixir could enhance this process."
    ],
    "commentSummary": [
      "Erlang's hot code updates enable rapid deployment of fixes without disconnecting users, which is particularly advantageous for systems with long-lived connections, such as telephony.",
      "Although these updates can be complex and risky, potentially leading to non-replicable states, they are essential for maintaining continuous connections in real-time systems.",
      "While some advocate for simpler rolling deployments, hot code updates provide unique benefits in scenarios where uninterrupted service is critical, despite the challenges they present."
    ],
    "points": 254,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1732048143
  },
  {
    "id": 42189581,
    "title": "Apple Confirms Zero-Day Attacks Hitting macOS Systems",
    "originLink": "https://www.securityweek.com/apple-confirms-zero-day-attacks-hitting-intel-based-macs/",
    "originBody": "www.securityweek.com Verifying you are human. This may take a few seconds. www.securityweek.com 8e5ab17849052898",
    "commentLink": "https://news.ycombinator.com/item?id=42189581",
    "commentBody": "Apple Confirms Zero-Day Attacks Hitting macOS Systems (securityweek.com)251 points by fortran77 18 hours agohidepastfavorite92 comments TekMol 11 hours agoThe article sounds like it also applies to iOS The company urged users across the Apple ecosystem to apply the urgent iOS 18.1.1, macOS Sequoia 15.1.1 and the older iOS 17.7.2. And that it is web based maliciously crafted web content may lead to arbitrary code execution Has this happened before? That iPhones had a security hole that could be exploited over the web? reply e28eta 11 hours agoparentAbsolutely. I don’t follow the scene, but early in the iphone’s product life I distinctly remember a web-based jailbreak, where you loaded a page and then you could ‘slide to jailbreak’. I don’t know if user action was strictly required, or if it was a UX thing. reply TekMol 9 hours agorootparentShouldn't that lead to a massive amount of iPhones being broken into? If not, why? If so, what happened to all those phones? I never hear stories like \"My iPhone was broken into and this happened: ...\" reply phillypham 8 hours agorootparentIt used to be possible to break into iPhones by sending just a text message without the target clicking on anything. The only thing that kept this under control was there was an agreement to not target US-based numbers and the exploit was expensive. Reference: The Battle for the World’s Most Powerful Cyberweapon https://www.nytimes.com/2022/01/28/magazine/nso-group-israel... and https://en.wikipedia.org/wiki/Pegasus_(spyware) reply pwagland 7 hours agorootparentNot quite, from the Wikipedia: > Pegasus' iOS exploitation was identified in August 2016. Emirati human rights defender Ahmed Mansoor received a text message promising \"secrets\" about torture happening in prisons in the United Arab Emirates by following a link. Mansoor sent the link to Citizen Lab of the University of Toronto, which investigated, with the collaboration of Lookout, finding that if Mansoor had followed the link it would have jailbroken his phone and implanted the spyware into it, in a form of social engineering. So the link was sent via text message, but you had to click on it. Receiving the text message did nothing in and of itself. reply phillypham 7 hours agorootparentInitial versions were one-click. The attack became more sophisticated and became zero-click. See https://en.wikipedia.org/wiki/Pegasus_(spyware)#Development_... for timeline. See https://en.wikipedia.org/wiki/Pegasus_(spyware)#Saudi_Arabia for the iMessage version. reply TheDong 9 hours agorootparentprevWhy would it? Do you regularly visit \"hot-iphone-porn-apps.info\" and other untrusted sites? Do you expect sites you do visit, like \"google.com\" or such, are going to serve up malware? Do you expect hackers who build these very labor-intensive exploit chains will want to try and hit as many low-value targets as possible, leading to apple patching the exploit quickly, or to try and hit high-value targets only so it's not noticed by apple as quickly and can be used against more high-value targets to make more money in total than doing a \"spray and pray\" with it? What thought process do you think would lead to using the exploit against as many people as possible vs selling it to zerodium.com or a similar company for more money than you can get from spraying, and then zerodium reselling it to israel to hack into the iphones of a few key palestinians? reply ceejayoz 6 hours agorootparent> Do you expect sites you do visit, like \"google.com\" or such, are going to serve up malware? Absolutely. One of the main reasons to run an adblocker. Malicious ads slip through regularly onto entirely reputable sites. reply TekMol 8 hours agorootparentprevYou are implying that the web based exploits in the history of iOS were not publicly known but only available to very few. If that holds true, that would be an importent addition to the discussion. The comment I replied to was about a public website that could jailbreak an iPhone though. reply acdha 5 hours agorootparentprev> Do you expect sites you do visit, like \"google.com\" or such, are going to serve up malware? With absolute certainty. Google ads has triggered downloads of Windows executables on NYtimes.com for me before and I am confident attackers will keep trying. The idea that advertisers get to run JavaScript on clients makes that problem effectively unwinnable even though they spend considerable amounts trying to make it hard to slip dodgy code into ads. reply throwaway290 8 hours agorootparentprevHow are you sure your phone was not broken into? Do you think some big alert magically appears? reply pwagland 7 hours agorootparentprevBecause most people apply the software updates at some point, and this was fixed many years ago. Everything sold in the last years comes with a version of iOS that isn't vulnerable anymore. reply iknowstuff 11 hours agoparentprevhttps://www.jailbreakme.com/ reply hyder_m29 11 hours agorootparentI'm assuming we don't click that link if we're on iPhones? reply imp0cat 11 hours agorootparentThis site is very old by now and does not support recent firmware, but you can still use it. JailbreakMe is the easiest way to free your device. Experience iOS as it could be, fully customizable, themeable, and with every tweak you could possibly imagine. Safe and completely reversible (just restore in iTunes), jailbreaking gives you control over the device you own. It only takes a minute or two, and as always, it's completely free. Please make an iTunes backup before jailbreaking. reply kafrofrite 11 hours agoparentprev> Has this happened before? That iPhones had a security hole that could be exploited over the web? Yes, there were exploits in the past that could be exploited remotely, including some that were used for jailbreaking. reply hutattedonmyarm 11 hours agoparentprev> Has this happened before? That iPhones had a security hole that could be exploited over the web? IIRC yes. Back around maybe iOS 4-6ish a web-based jailbreak existed, don't remember exactly when reply ThePowerOfFuet 7 hours agoparentprevSounds like those who don't use Safari on macOS are less exposed. reply acdha 18 hours agoprevInteresting that they’re mentioned as only being exploited on Intel. Has anyone seen whether that’s because the attacker only targeted that platform or is it actually stopped by something like pointer protection? reply justinclift 17 hours agoparentDoesn't seem to completely line up that they're rushing out iOS updates (ie for phones, etc) for something they're saying they've only confirmed on Intel cpus. Unless they're assuming it's exploitable on Apple Silicon as well, or are being extra careful just in case. reply bigiain 16 hours agorootparent> Apple is aware of a report that this issue may have been actively exploited on Intel-based Mac systems. Is kinda weasel-wordy, if you read it with sufficient cynicism. Its doesn't rule out them also being aware of reports (or actual instances) of it being exploited on iOS or Apple silicon Macs. It _might_ actually mean \"Apple could not deny in a lawsuit that it's been sent a report of this being exploited on Intel Macs.\" reply duxup 15 hours agorootparentOr they’re just not able to confirm it everywhere but feel the code change is necessary regardless? I’ve certainly addressed a potential issue with code that I thought might have occurred even when I couldn’t confirm it with 100% certainty. A detailed analysis / testing and confirmation that provides certainty may take longer than addressing code. reply kafrofrite 11 hours agorootparentprevMost probably what Apple means is that since their codebase is shared, the vulnerability exists across devices. This does not mean that the vulnerability is actively exploited in iOS nor that it will not be actively exploited as part of some other campaign. reply brookst 13 hours agorootparentprevIf you read it with enough cynicism, it doesn’t rule out Apple having actual knowledge that it was exploited to steal every last bit of information from every Mac, iPhone, iPad, iPod, Apple TV, and Apple II ever produced. reply saagarjha 14 hours agorootparentprevThis just means the bug is in WebKit and they shipped the fix to every platform. reply initplus 14 hours agorootparentprevWill be an underlying safety issue in some system library, but they have only seen \"in the wild\" exploits targeting Intel. \"Defence in depth\" - better to push the bugfix to all than to scrutinize ARM security features to understand if an exploit is possible there as well. reply oddevan 17 hours agorootparentprev> Unless they're... being extra careful just in case. That's where my money is. reply ajross 17 hours agorootparentOr they just don't know. Full analysis of an exploit usually takes days to weeks. It's possible it's only exploitable on x86, but equally possible that only the x86 version of the payload was discovered in the wild. reply 486sx33 15 hours agorootparentRosetta2 runs an x86 exploit? Doesn’t explain iOS but still sounds interesting! reply ruthmarx 16 hours agorootparentprevWhy? Putting a lot of stock in Apple's various protections? reply tedunangst 16 hours agorootparentprevIt's not unheard of for exploits to target two or more bugs. reply SoftTalker 14 hours agorootparentprevSometimes problems manifest differently on different architectures. It's one of the advantages of building for more than just one: it shakes out bugs. Doesn't mean you don't fix the root issue in all builds. Apple for the most part has one codebase that they build for their different architectures. They've been doing this since the NeXT days when they supported Motorola, Intel, Sparc, and maybe a couple of other architectures. reply 2muchcoffeeman 17 hours agorootparentprevThere must be millions of Intel Macs still around. Why wouldn’t they update it? reply shepherdjerred 17 hours agorootparentThe parent comment said that Apple is rushing iOS updates. iOS is the operating system for iPhones which use Apple Silicon rather than Intel processors. reply wannacboatmovie 17 hours agorootparentprevWell for starters, they stopped providing any updates for many perfectly functional Intel Macs years ago for no other reason than planned obsolescence. A side effect of the \"they make both the hardware and software that's why it's better\" paradigm. Things like OpenCore Legacy Patcher prove it's possible; they just don't want to. I don't think anyone feels entitled to new features in perpetuity. Security updates only would be fine thank you. Don't tell me the richest company in the world can't pay for a couple of developers who just want to rest and vest to take care of and test the legacy platforms. A cushy job and you keep the customers happy. Ironically the best way to stay safe on these computers is to install Windows or Linux. reply StressedDev 17 hours agorootparentSoftware needs longer support life cycles in general. I find it frustrating that organizations do not support operating systems, hardware, and applications for at least 10 years. Note Apple is one of the better organizations on this. Consumer router companies are notorious for shipping unpatched software. Here is what I would like to see: 1. All hardware and software should come with a highly visible end of support date. 2. All hardware and software should notify people when it is no longer receiving security patches. It should also explain to users why running unpatched software or hardware is dangerous. reply pjmlp 11 hours agorootparentWhich is why having cybersecurity laws and liability in computing is so relevant. reply wannacboatmovie 16 hours agorootparentprevTo my knowledge Apple has never published EOL or support dates in the future. Someone correct me if something has changed in the last few years. reply wtallis 15 hours agorootparenthttps://support.apple.com/en-us/102772 outlines \"vintage\" and \"obsolete\" status for hardware products, with a few exceptions. I'm not aware of a similarly straightforward criteria or comprehensive list for software support periods. reply danieldk 11 hours agorootparentSamsung nowadays tells you ahead of time how long a phone will get major updates and security updates. I think it's the same with Google Pixel. And they have a list of models and their release schedules: https://security.samsungmobile.com/workScope.smsb My qualm with them is though that not all devices are updated at the same time (like iOS/iPadOS/macOS). One phone may get an update the 10th of the month, while another only gets it the 30th. As a result, there is often quite a large window where vulnerabilities are known, but not yet patched (it's even worse with the cheap models that only get quarterly updates). reply philistine 15 hours agorootparentprevThat list relates strictly to hardware repairs. Vintage macs have often been fully supported software-wise. reply wtallis 15 hours agorootparentYes, I'm fully aware that the support article I linked to is specifically about hardware support—that's why I mentioned that there isn't a similar list for software support. reply wannacboatmovie 15 hours agorootparentprevThe issue with passing off a list of vintage products as some kind of past tense support schedule is by definition products become vintage when they are added to the list at some arbitrary date. My expectation is a table of OS versions and EOL dates published in advance. Like nearly every other responsible OS vendor in existence. Apple continuing to get a pass on this in 2024 is abhorrent. reply wtallis 15 hours agorootparent> The issue with passing off a list of vintage products as some kind of past tense support schedule is by definition products become vintage when they are added to the list at some arbitrary date. If you read some of the text above the product list, you'll see that Apple does publish guidelines about when products can be expected to be added to the list: > Products are considered vintage when Apple stopped distributing them for sale more than 5 and less than 7 years ago. > Products are considered obsolete when Apple stopped distributing them for sale more than 7 years ago. Monster-branded Beats products are considered obsolete regardless of when they were purchased. > Apple discontinues all hardware service for obsolete products, and service providers cannot order parts for obsolete products. Mac laptops may be eligible for an extended battery-only repair period for up to 10 years from when the product was last distributed for sale, subject to parts availability. So as you can see, it's not arbitrary or unpredictable when a product is going to show up on the vintage product list. The only unpredictable or obscure part of this process is finding out how long an outdated product was still being sold after its successor launched. reply wannacboatmovie 14 hours agorootparentOk, but this is an Apples vs oranges comparison. (Carlos!) We are talking about software support here. The vintage products list is specifically targeting hardware support; e.g. how long Apple will keep spare parts in stock. After a set number of years they purge stock and you are SOL going to Chinese third party vendors and places like iFixit for batteries etc. reply vetinari 7 hours agorootparentNot really; vintage macs turning obsolete are being dropped from the macOS support very reliably. I.e. the 2015 mbp was dropped from 2022 macos release like on the clock. reply threeseed 17 hours agorootparentprevSequoia is supported on most Intel Macs going back to 2018. And it's far more than just a \"couple of developers\" to support older operating systems. reply brian_cunnie 15 hours agorootparentAgreed. It takes more than a few developers to support older operating systems. At my old job we supported only two versions of our software product, Tanzu Operations Manager versions 2.10.x and 3.0.y), and we cut new patch releases every few weeks (similar to Apple's cadence). Bumping dependencies was a pain. Well, usually it went fine, but sometimes you'd hit a gnarly incompatibility and you'd either pin a Ruby package to a known version or try to modify the code just enough to make it work without making a major change. If I had to put a number to it, I'd say it cost us 2 developers to keep our older product line consistently patched, and our product was a modest Ruby app, much less complicated than an entire OS. reply justinclift 10 hours agorootparent> new patch releases every few weeks (similar to Apple's cadence) Is Apply really releasing new patched OS updates every few weeks? reply fouc 16 hours agorootparentprevmy favorite Intel MacBook is from 2015 reply wannacboatmovie 16 hours agorootparentprevYou act as if we should be thankful for 6 years of support when the hardware and sane support cycles easily exceed 10 years. And those aren't 6 years of security updates; they are 6 years of forced yearly feature upgrades and breaking things along the way. reply fn-mote 15 hours agorootparentWhat software are you talking about? Who is forcing you to upgrade? For that matter, what hardware? I run an old Intel Mac and it’s perfectly reasonable for casual work. I’m not paying for stuff like Adobe leases though. reply vetinari 7 hours agorootparentWhat exactly is an old Intel mac and what is a casual work? For example, I have 2015 macbook pro. The last macos release for it is Monterey. Even brew has problems with that, erroring out when installing packages like libpng and complaining, that I should upgrade xcode cli tools. Which I can't. reply chris_wot 17 hours agorootparentprevNot on Macbook Airs that are only 3-5 years old though. We have a number that we plan on replacing after EOY, but we are still using for now. Can't get Sequoia. reply phony-account 15 hours agorootparent> Macbook Airs that are only 3-5 years old MacBook Airs from 2020 support Sequoia - so just the very upper limit of your range is relevant. reply wannacboatmovie 11 hours agorootparentAbsolutely not. Apple was still selling non-Retina Intel MacBook Airs until 2019. Those are now completely unsupported with no security updates having topped out at Monterrey. 5 years of updates on a new laptop is borderline criminal. reply password4321 17 hours agorootparentprevNot really suitable for a corporate environment but in case you weren't aware: https://github.com/dortania/OpenCore-Legacy-Patcher macOS Big Sur and newer on machines as old as 2007 macOS Big Sur, Monterey, Ventura, Sonoma and Sequoia reply chris_wot 16 hours agorootparentNice. Yeah, never going to fly here :( pity reply dataflow 18 hours agoparentprevIt might just be that the only machines they have information about are Intel? It doesn't say how many data points they have, but if it's only a handful then it's not at all surprising. reply acdha 16 hours agorootparentThat’s what I was wondering: if this is like some of those Citizen Lab reports it might simply be that they have a small number of targeted individuals who noticed something and reported it. reply ngneer 17 hours agoparentprevI came to ask this very question. Could be an Intel hardware bug, but then we would see an advisory from Intel. So, it must be a different issue. Apple probably did not realize that their advisory implicates Intel components rather than Apple components. reply alphabetting 16 hours agoprev>The vulnerabilities, credited to Google’s TAG (Threat Analysis Group) Do they find these by monitoring the brokers of zero days or analyzing devices of people who are being targeted? reply ledoge 16 hours agoparentThere's actually a very recent talk about this! https://www.youtube.com/watch?v=2zrcemxCg4Y reply tonfa 6 hours agorootparentI also found a recent article: https://www.nzz.ch/english/googles-spyware-hunters-track-sta... reply wutwutwat 16 hours agoparentprevlittle of column a, little of column b they also have insane peering and backbone network infra, run one of the largest cloud providers, host basically everyone's email, documents, and file storage, chat, app store, and have a native browser installed I'm sure they have many different signals they can look at to see compromised type behavior differing from the profile they have on you reply xorcist 8 hours agoparentprevBoth, but also a lot of original research. They are public about this. reply myHNAccount123 16 hours agoparentprevI suspect firebase crashlytics is the source of many reply grupthink 16 hours agoprevI have a perfectly functional iPad 5 that no longer receives software updates. It'd be cool if Apple would at least give it security updates, or allow alternative browser engines that don't have this vulnerability. If my iPad gets pwned, my day is going to suck. reply Jtsummers 16 hours agoparentiPadOS 16 was at least updated 3 months ago (August) so there's a chance you could still get a security update if it's applicable to that version. iPadOS 15 was updated in July. https://en.wikipedia.org/wiki/IPadOS_version_history reply zarzavat 6 hours agoparentprevApple really cares about saving the planet so they will graciously allow you to recycle your working iPad and buy a new one. reply wannacboatmovie 16 hours agoparentprevnext [4 more] [flagged] stephen_g 15 hours agorootparentIt's probably because the first bit isn't really true (iPads have a longer supported life than many Android tablets, for example I updated my 2018 iPad Pro with the most recent iPadOS 18.1.1 including this security fix earlier today). And given that the silly joke falls flat. The iPad 5 in question from the OP supports iPadOS 16 and that last got a security update in August of this year. So if it hasn't got an update today then possibly the vulnerability was only introduced in iOS / iPadOS 17. reply saagarjha 14 hours agorootparentprevI can assure you that the toilet paper is much more comfortable than dollar bills. reply brookst 13 hours agorootparentprevMeta meta: comments making vague complaints about other comments being downvoted, typically get downvoted. reply timeon 17 hours agoprevDoes this affect Firefox/Chrome on macOS? reply bigiain 16 hours agoparentI'd guess yes, since they're both (due to Apple rules) basically wrappers around webkit and JavaScriptCore. (Modulo the possible hint that this is just an Intel-based Mac systems problem) reply Jtsummers 16 hours agorootparent> I'd guess yes, since they're both (due to Apple rules) basically wrappers around webkit and JavaScriptCore. That is not true for macOS which is what GP asked about. reply mdavidn 16 hours agorootparentprevWhile that is true on iOS, macOS has no such restriction on Firefox or Chrome. reply pram 16 hours agorootparentprevmacOS != iOS, Firefox uses Gecko. reply rswail 12 hours agoprevInteresting that I had a security update for iOS (18.1.1) and a Safari update for MacOS (still running Sonoma) reply hrvstr 5 hours agoparentHmm, may I ask what version you are running now? I am on Sonoma too and don't see any updates. The apple support page lists macOS Sonoma 14.7.1 as the most recent version released on 28 Oct 2024. https://support.apple.com/en-us/100100 reply consumerx 12 hours agoprevnext [2 more] [flagged] tgv 11 hours agoparentAnd what's your advice when there's a threat for Linux? reply pjmlp 11 hours agoprevnext [2 more] [flagged] tim333 8 hours agoparentgosh, 18 years ago https://youtu.be/sdF5IsyOxU4 reply consumerx 12 hours agoprev [–] that's why you turn on Lockdown Mode or swap to Linux completely :) reply danieldk 11 hours agoparentI love Linux, but this is really a cheap shot. Out of the box, desktop security is much better on the Mac. Slim boot ROM in place of UEFI (which can be backdoored), no always-running Intel ME/AMD PSP, fully verified boot chain, sealed system volumes, heavy use of a secure enclave to protect secrets, mandatory sandboxing for App Store apps, malware checks through XProtect, limited access of apps to key folders (Desktop, Documents, iCloud Drive), limited access to privacy-sensitive devices (camera, mic), etc. Linux will get there, but currently macOS is much more secure as a desktop. reply adrian_b 5 hours agorootparentWhile in general you are right, you should not forget that almost one year ago it has been revealed that the \"Apple Silicon\" CPUs had a hardware backdoor that had been exploited for years by malicious entities (i.e. some unprotected test registers that allowed the attacker to bypass the memory protection and gain complete control remotely, through the sending of an invisible message, without any chance of being detected by the owner; the complete exploit had used a chain of bugs in the Apple system libraries, together with the hardware backdoor). Such a hardware backdoor is rather more severe than most of what has ever been discovered on non-Apple devices. As long as the main protection of the Apple devices consists mostly in their lack of detailed technical documentation, one can never know whether other such hardware backdoors exist. reply acdha 5 hours agorootparentDo you have a reference for that? It doesn’t sound like GoFetch, which is the closest on timing. reply vetinari 7 hours agorootparentprevHalf of the stuff you names is security from you, not security for you. reply danieldk 3 hours agorootparentYou can turn pretty much all of it off, disable SIP, boot Linux, whatever you like. Good security is layered. For example, even with a sandbox escape, and app could not read your full Documents directory, modify the OS, or install a firmware-level rootkit. reply leoh 12 hours agoparentprevI have got to believe that there are some nasty zero days for linux reply proxynoproxy 12 hours agorootparentThe advantage of everyone running the same software and hardware platform is that you can concentrate on hardening that one system. The disadvantage is that vulnerability is universal. The advantage of everyone running a disparate environment of many of different libraries and binaries is that vulnerability is likely unique. The disadvantage is there are many more opportunities for the researcher to find vulnerability in the mess. Choose your poison, the only secure system is powered down. reply notactuallyben 8 hours agorootparentVulnerabilities in the Linux kernel would have a similar impact to a macOS kernel bug. It’s a myth that “more eyes means more secure” for OSS ;-) - it can be true, but often that’s not the reason reply fsflover 8 hours agoparentprev [–] You probably mean to Qubes OS. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Apple has acknowledged zero-day attacks on macOS systems and advised users to update to the latest versions: iOS 18.1.1, macOS Sequoia 15.1.1, and iOS 17.7.2.",
      "The vulnerabilities involve malicious web content that could allow arbitrary code execution, highlighting the critical need for timely software updates.",
      "This incident is part of a recurring issue with iPhones, which have previously experienced web-exploitable security vulnerabilities, including web-based jailbreaks."
    ],
    "points": 251,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1732062390
  },
  {
    "id": 42190650,
    "title": "Understanding the BM25 full text search algorithm",
    "originLink": "https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/",
    "originBody": "Understanding the BM25 full text search algorithm Nov 19, 2024 BM25, or Best Match 25, is a widely used algorithm for full text search. It is the default in Lucene/Elasticsearch and SQLite, among others. Recently, it has become common to combine full text search and vector similarity search into \"hybrid search\". I wanted to understand how full text search works, and specifically BM25, so here is my attempt at understanding by re-explaining. Motivation: can BM25 scores be compared across queries? Ranking documents probabilistically Components of BM25 Behold, Math! Query terms Inverse Document Frequency (IDF) Term frequency in the document Document length normalization Putting it all together Cleverness of BM25 and its precursors Ranking by probability without calculating probability Assuming most documents are irrelevant Conclusion: BM25 scores can be compared within the same collection Further reading Motivation: can BM25 scores be compared across queries? For a quick bit of context on why I'm thinking about search algorithms, I'm building a personalized content feed that scours noisy sources for content related to your interests. I started off using vector similarity search and wanted to also include full-text search to improve the handling of exact keywords (for example, a friend has \"Solid.js\" as an interest and using vector similarity search alone, that turns up more content related to React than Solid). The question that motivated this deep dive into BM25 was: can I compare the BM25 scores of documents across multiple queries to determine which query the document best matches? Initially, both ChatGPT and Claude told me no — though annoyingly, after doing this deep dive and formulating a more precise question, they both said yes 🤦♂. Anyway, let's get into the details of BM25 and then I'll share my conclusions about this question. Ranking documents probabilistically At the most basic level, the goal of a full text search algorithm is to take a query and find the most relevant documents from a set of possibilities. However, we don't really know which documents are \"relevant\", so the best we can do is guess. Specifically, we can rank documents based on the probability that they are relevant to the query. (This is called The Probability Ranking Principle.) How do we calculate the probability that a document is relevant? For full text or lexical search, we are only going to use qualities of the search query and each of the documents in our collection. (In contrast, vector similarity search might use an embedding model trained on an external corpus of text to represent the meaning or semantics of the query and document.) Components of BM25 BM25 uses a couple of different components of the query and the set of documents: Query terms: if a search query is made up of multiple terms, BM25 will calculate a separate score for each term and then sum them up. Inverse Document Frequency (IDF): how rare is a given search term across the entire document collection? We assume that common words (such as \"the\" or \"and\") are less informative than rare words. Therefore, we want to boost the importance of rare words. Term frequency in the document: how many times does a search term appear in a given document? We assume that more repetition of a query term in a given document increases the likelihood that that document is related to the term. However, BM25 also adjusts this so that there are diminishing returns each time a term is repeated. Document length: how long is the given document compared to others? Long documents might repeat the search term more, just by virtue of being longer. We don't want to unfairly boost long documents, so BM25 applies some normalization based on how the document's length compares to the average. These four components are what make up BM25. Now, let's look at exactly how they're used. Behold, math! The BM25 algorithm might look scary to non-mathematicians (my eyes glazed over the first time I saw it), but I promise, it's not too hard to understand! Here is the full equation: score ( 𝐷 , 𝑄 ) = ∑ 𝑖 = 1 𝑛 ln ( 𝑁 − 𝑛 ( 𝑞 𝑖 ) + 0.5 𝑛 ( 𝑞 𝑖 ) + 0.5 + 1 ) · 𝑓 ( 𝑞 𝑖 , 𝐷 ) · ( 𝑘 1 + 1 ) 𝑓 ( 𝑞 𝑖 , 𝐷 ) + 𝑘 1 · ( 1 − 𝑏 + 𝑏 ·𝐷avgdl ) Now, let's go through it piece-by-piece. Query terms score ( 𝐷 , 𝑄 ) = ∑ 𝑖 = 1 𝑛 . . . 𝐷 is a given document 𝑄 is the full query, potentially composed of multiple query terms 𝑛 is the number of query terms 𝑞 𝑖 is each of the query terms This part of the equation says: given a document and a query, sum up the scores for each of the query terms. Now, let's dig into how we calculate the score for each of the query terms. Inverse Document Frequency (IDF) The first component of the score calculates how rare the query term is within the whole collection of documents using the Inverse Document Frequency (IDF). ln ( 𝑁 − 𝑛 ( 𝑞 𝑖 ) + 0.5 𝑛 ( 𝑞 𝑖 ) + 0.5 + 1 ) The key elements to focus on in this equation are: 𝑁 is the total number of documents in our collection 𝑛 ( 𝑞 𝑖 ) is the number of documents that contain the query term 𝑁 − 𝑛 ( 𝑞 𝑖 ) therefore is the number of documents that do not contain the query term In simple language, this part boils down to the following: common terms will appear in many documents. If the term appears in many documents, we will have a small number ( 𝑁 − 𝑛 ( 𝑞 𝑖 ) , or the number of documents that do not have the term) divided by 𝑁 . As a result, common terms will have a small effect on the score. In contrast, rare terms will appear in few documents so 𝑛 ( 𝑞 𝑖 ) will be small and 𝑁 − 𝑛 ( 𝑞 𝑖 ) will be large. Therefore, rare terms will have a greater impact on the score. The constants 0.5 and 1 are there to smooth out the equation and ensure that we don't end up with wildly varying results if the term is either very rare or very common. Term frequency in the document In the previous step, we looked at how rare the term is across the whole set of documents. Now, let's look at how frequent the given query is in the given document. 𝑓 ( 𝑞 𝑖 , 𝐷 ) 𝑓 ( 𝑞 𝑖 , 𝐷 ) + 𝑘 1 The terms in this equation are: 𝑞 𝑖 is a given query 𝐷 is a given document 𝑓 ( 𝑞 𝑖 , 𝐷 ) is the frequency of the given query in the given document 𝑘 1 is a tuning parameter that is generally set between 1.2 and 2 This equation takes the term frequency within the document into effect, but ensures that term repetition has diminishing returns. The intuition here is that, at some point, the document is probably related to the query term and we don't want an infinite amount of repetition to be weighted too heavily in the score. The 𝑘 1 parameter controls how quickly the returns to term repetition diminish. You can see how the slope changes based on this setting: From The Probabilistic Relevance Framework: BM25 and Beyond Document length normalization The last thing we need is to compare the length of the given document to the lengths of the other documents in the collection. ( 1 − 𝑏 + 𝑏 ·𝐷avgdl ) From right to left this time, the parameters are:𝐷is the length of the given document 𝑎 𝑣 𝑔 𝑑 𝑙 is the average document length in our collection 𝑏 is another tuning parameter that controls how much we normalize by the document length Long documents are likely to contain the search term more frequently, just by virtue of being longer. Since we don't want to unfairly boost long documents, this whole term is going to go in the denominator of our final equation. That is, a document that is longer than average (𝐷𝑎 𝑣 𝑔 𝑑 𝑙 > 1 ) will be penalized by this adjustment. 𝑏 can be adjusted by the user. Setting 𝑏 = 0 turns off document length normalization, while setting 𝑏 = 1 applies it fully. It is normally set to 0.75 . Putting it all together If we take all of the components we've just discussed and put them together, we arrive back at the full BM25 equation: score ( 𝐷 , 𝑄 ) = ∑ 𝑖 = 1 𝑛 ⏟ Summing each query term's score ln ( 𝑁 − 𝑛 ( 𝑞 𝑖 ) + 0.5 𝑛 ( 𝑞 𝑖 ) + 0.5 + 1 ) ⏟ Inverse Document Frequency · 𝑓 ( 𝑞 𝑖 , 𝐷 ) · ( 𝑘 1 + 1 ) 𝑓 ( 𝑞 𝑖 , 𝐷 ) + 𝑘 1 · ( 1 − 𝑏 + 𝑏 ·𝐷avgdl ) ⏟ Document length normalization ⏞ Term frequency in the document Reading from left to right, you can see that we are summing up the scores for each query term. For each, we are taking the Inverse Document Frequency, multiplying it by the term frequency in the document (with diminishing returns), and then normalizing by the document length. Cleverness of BM25 and its precursors We've just gone through the components of the BM25 equation, but I think it's worth pausing to emphasize two of its most ingenious aspects. Ranking by probability without calculating probability As mentioned earlier, BM25 is based on an idea called the Probability Ranking Principle. In short, it says: If retrieved documents are ordered by decreasing probability of relevance on the data available, then the system’s effectiveness is the best that can be obtained for the data. The Probabilistic Relevance Framework: BM25 and Beyond Unfortunately, calculating the \"true\" probability that a document is relevant to a query is nearly impossible. However, we really care about the order of the documents more than we care about the exact probability. Because of this, researchers realized that you could simplify the equations and make it practicable. Specifically, you could drop terms from the equation that would be required to calculate the full probability but where leaving them out would not affect the order. Even though we are using the Probability Ranking Principle, we are actually calculating a \"weight\" instead of a probability. 𝑊 ( 𝑑 ) = ∑ 𝑡 ∈ 𝑞 , 𝑓 𝑡 , 𝑑 > 0 log 𝑃 ( 𝐹 = 𝑓 𝑡 , 𝑑𝑅 = 1 ) 𝑃 ( 𝐹 = 0𝑅 = 0 ) 𝑃 ( 𝐹 = 𝑓 𝑡 , 𝑑𝑅 = 0 ) 𝑃 ( 𝐹 = 0𝑅 = 1 ) This equation calculates the weight using term frequencies. Specifically: 𝑊 ( 𝑑 ) is the weight for a given document 𝑃 ( 𝐹 = 𝑓 𝑡 , 𝑑𝑅 = 1 ) is the probability that the query term would appear in the document with a given frequency ( 𝑓 𝑡 , 𝑑 ) if the document is relevant ( 𝑅 = 1 ) The various terms boil down to the probability that we would see a certain query term frequency within the document if the document is relevant or not relevant, and the probabilities that the term would not appear at all if the document is relevant or not. The Robertson/Sparck Jones Weight is a way of estimating these probabilities but only using the counts of different sets of documents: 𝑤 𝑅 𝑆 𝐽 = log ( 𝑟 + 0.5 ) ( 𝑁 − 𝑅 − 𝑛 + 𝑟 + 0.5 ) ( 𝑛 − 𝑟 + 0.5 ) ( 𝑅 − 𝑟 + 0.5 ) The terms here are: 𝑟 is the number of relevant documents that contain the query term 𝑁 is the total number of documents in the collection 𝑅 is the number of relevant documents in the collection 𝑛 is the number of documents that contain the query term The big, glaring problem with this equation is that you first need to know which documents are relevant to the query. How are we going to get those? Assuming most documents are irrelevant The question about how to make use of the Robertson/Sparck Joes weight apparently stumped the entire research field for about 15 years. The equation was built up from a solid theoretical foundation, but relying on already having relevance information made it nearly impossible to put to use. The BM25 developers made a very clever assumption to get to the next step. For any given query, we can assume that most documents are not going to be relevant. If we assume that the number of relevant documents is so small as to be negligible, we can just set those numbers to zero! 𝑅 = 𝑟 = 0 If we substitute this into the Robertson/Sparck Jones Weight equation, we get nearly the IDF term used in BM25: log ( 0 + 0.5 ) ( 𝑁 − 0 − 𝑛 + 0 + 0.5 ) ( 𝑛 − 0 + 0.5 ) ( 0 − 0 + 0.5 ) = log 0.5 ( 𝑁 − 𝑛 + 0.5 ) ( 𝑛 + 0.5 ) 0.5 = log ( 𝑁 − 𝑛 + 0.5 ) ( 𝑛 + 0.5 ) Not relying on relevance information made BM25 much more useful, while keeping the same theoretical underpinnings. Victor Lavrenko described this as a \"very impressive leap of faith\", and I think this is quite a neat bit of BM25's backstory. Conclusion: BM25 scores can be compared within the same collection As I mentioned at the start, my motivating question was whether I could compare BM25 scores for a document across queries to understand which query the document best matches. In general, BM25 scores cannot be directly compared (and this is what ChatGPT and Claude stressed to me in response to my initial inquiries 🙂↔). The algorithm does not produce a score from 0 to 1 that is easy to compare across systems, and it doesn't even try to estimate the probability that a document is relevant. It only focuses on ranking documents within a certain collection in an order that approximates the probability of their relevance to the query. A higher BM25 score means the document is likely to be more relevant, but it isn't the actual probability that it is relevant. As far as I understand now, it is possible to compare the BM25 scores across queries for the same document within the same collection of documents. My hint that this was the case was the fact that BM25 sums the scores of each query term. There should not be a semantic difference between comparing the scores for two query term and two whole queries. The important caveat to stress, however, is the same document within the same collection. BM25 uses the IDF or rarity of terms as well as the average document length within the collection. Therefore, you cannot necessarily compare scores across time because any modifications to the overall collection could change the scores. For my purposes, though, this is useful enough. It means that I can do a full text search for each of a user's interests in my collection of content and compare the BM25 scores to help determine which pieces best match their interests. I'll write more about ranking algorithms and how I'm using the relevance scores in future posts, but in the meantime I hope you've found this background on BM25 useful or interesting! Thanks to Alex Kesling and Natan Last for feedback on drafts of this post. Further reading If you are interested in diving further into the theory and history of BM25, I would highly recommend watching Elastic engineer Britta Weber's 2016 talk Improved Text Scoring with BM25 and reading The Probabilistic Relevance Framework: BM25 and Beyond by Stephen Robertson and Hugo Zaragoza. Also, I had initially included comparisons between BM25 and some other algorithms in this post. But, as you know, it was already a bit long 😅. So, you can now find those in this other post: Comparing full text search algorithms: BM25, TF-IDF, and Postgres. Discuss on Lobsters and Hacker News. #scour #search #understanding 12",
    "commentLink": "https://news.ycombinator.com/item?id=42190650",
    "commentBody": "Understanding the BM25 full text search algorithm (emschwartz.me)236 points by rrampage 15 hours agohidepastfavorite45 comments DavidPP 4 hours agoWe use https://typesense.org/ for regular search, but it now has support for doing hybrid search, curious if anyone has tried it yet? reply kkielhofner 2 hours agoparentI've used it for hybrid search and it works quite well. Overall I'm really happy to see Typesense mentioned here. A lot of the smaller scale RAG projects, etc you see around would be well served by Typesense but it seems to be relatively unknown for whatever reasons. It's probably one of the easiest solutions to deploy, has reasonable defaults, good docs, easy clustering, etc while still be very capable, performant, and powerful if you need to dig in further. reply hubraumhugo 7 hours agoprevGiven the recent advances in vector-based semantic search, what's the SOTA search stack that people are using for hybrid keyword + semantic search these days? reply noduerme 5 hours agoparentA generic search strategy is so different from something you want to target. The task should probably determine the tool. So I don't know the answer, but I was recently handed about 3 million surveys with 10 free-form writing fields each, and tasked with surfacing the ones that might require action on the part of the company. I chose to use a couple of different small classifier models, manually strip out some common words based on obvious noise in the first 10k results, and then weight the model responses. It turned out to be almost flawless. I would NOT call this sort of thing \"programming\", it's more just tweaking the black-box output of various different tools until you have a set of results that looks good for your test cases. (And your client ;) All stitching together small Hugging Face models running on a tiny server in nodejs, btw. reply emschwartz 6 hours agoparentprevMost of the commercial and open source offerings for hybrid search seem to be using BM25 + vector similarity search based on embeddings. The results are combined using Reciprocal Rank Fusion (RRF). The RRF paper is impressive in how incredibly simple it is (the paper is only 2 pages): https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf reply softwaredoug 2 hours agorootparentA warning that RRF is often not Enough, as it can just drag a good solution down towards the worse solution :) https://softwaredoug.com/blog/2024/11/03/rrf-is-not-enough reply emschwartz 1 hour agorootparentAh, that's great! Thanks for sharing that. I had actually implemented full text search + vector search using RRF but I kept it disabled by default because it wasn't meaningfully improving my results. This seems like a good hypothesis as to why. reply softwaredoug 4 hours agoparentprevMy opinion is people need to not focus on one stack. But be prepared to use tools best for each job. Elasticsearch for BM25 type things. Turbopuffer for simple and fast vector retrieval. Even redis to precompute results for certain queries. Or certain extremely dynamic attributes that change frequently like price. Combine all these in a scatter/gather approach. I say that because almost always you have a layer outside the search stack(s) that ideally can just be a straightforward inference service for reranking that looks most like other ML infra. You also almost always route queries to different backends based on an understanding of the users query. Routing “lookup by ID” to a different system than “fuzzy semantic search”. These are very different data structures. And search almost always covers very broad/different use cases. I think it’s an anti pattern to just push all work to one system. Each system is ideal for different workloads. And their inference capabilities won’t ever keep pace with the general ML tooling that your ML engineers are used to. (I tried with Elasticsearch Learning to Rank and its a hopeless task.) (That said, Vespa is probably the best 'single stack' that tries to solve a broad range of use-cases.) reply d4rkp4ttern 5 hours agoparentprevIn the Langroid[1] LLM library we have a clean, extensible RAG implementation in the DocChatAgent[2] -- it uses several retrieval techniques, including lexical (bm25, fuzzy search) and semantic (embeddings), and re-ranking (using cross-encoder, reciprocal-rank-fusion) and also re-ranking for diversity and lost-in-the-middle mitigation: [1] Langroid - a multi-agent LLM framework from CMU/UW-Madison researchers https://github.com/langroid/langroid [2] DocChatAgent Implementation - https://github.com/langroid/langroid/blob/main/langroid/agen... Start with the answer_from_docs method and follow the trail. Incidentally I see you're the founder of Kadoa -- Kadoa-snack is one of favorite daily tools to find LLM-related HN discussions! reply khaki54 5 hours agoparentprevWe're doing something like BM25 with a semantic ontology enhanced query (naive example: search for truck hits on Ford F-150, even if truck never appears in the doc) then vector based reranking. In testing, we always get the best result in the top 3. reply dmezzetti 5 hours agoparentprevExcellent article on BM25! Author of txtai [1] here. txtai implements a performant BM25 index in Python [2] via the arrays package and storing the term frequency vectors in SQLite. With txtai, the hybrid index approach [3] supports both convex combination when BM25 scores are normalized and reciprocal rank fusion (RRF) when they aren't [4]. [1] https://github.com/neuml/txtai [2] https://neuml.hashnode.dev/building-an-efficient-sparse-keyw... [3] https://neuml.hashnode.dev/benefits-of-hybrid-search [4] https://github.com/neuml/txtai/blob/master/src/python/txtai/... reply treprinum 5 hours agoparentprevtext-embedding-3-large + SPLADE + RRF reply jankovicsandras 12 hours agoprevShameless plug: https://github.com/jankovicsandras/plpgsql_bm25 https://github.com/jankovicsandras/bm25opt reply softwaredoug 4 hours agoparentIf we're shameless plugging passion projects, SearchArray is a pandas extension for fulltext (BM25) search for dorking around with things in google colab https://github.com/softwaredoug/searcharray I'll also plug Xing Han Lu's BM25S which is very popular with similar goals: https://github.com/xhluca/bm25s reply mark_l_watson 7 hours agoparentprevThanks, yesterday I was thinking of adding BM25 to a little side project, so a well timed plug! Do you know of any pure Python wrapper projects for managing large numbers of text and PDF documents? I thought of using Solr or ElasticSearch but that seems too heavy weight for what I am doing. I am considering using SQLite with pysqlite3 and PyPDF2 since SQLite uses BM25. Sorry to be off topic, but I imagine many people are looking at tools for building hybrid BM25 / vector store / LLM applications. reply jll29 11 hours agoprevNice write-up. A few more details/background that are harder to find: \"BM25\" stands for \"Best Matching 25\", \"best matching\" becaue it is a formula for ranking and term weighting (the matching refers to the term in the query versus the document), and the number 25 simply indicates a running number (there were 24 earlier formula variants and some later ones, but #25 turned out to work best, so it was the one that was published). It was conceived by Stephen Robertson and Karen Spärck Jones (the latter of IDF fame) and first implemented in the former's OKAPI information retrieval (research) system. The OKAPI system was benchmarked at the annual US NIST TREC (Text Retrieval Conference) for a number of years, the international \"World Champtionship\" of search engine methods (although the event is not about winning, but about compariing notes and learning from each other, a highly recommended annual event held every November in Gaithersburg, Maryland, attended by global academic and industry teams that conduct research on improving search - see trec.nist.gov). Besides the \"bag of words\" Vector Space Model (sparse vectors of terms), the Probabilistic Modles (that BM25 belongs to), there are suprising and still growing number of other theoretical frameworks how to rank a set of documents, given a query (\"Divergence from Randomness\", \"Statistical Language Modeling, \"Learning to Rank\", \"Quantum Information Retrieval\", \"Neural Ranking\" etc.). Conferences like ICTIR and SIGIR still publish occasionaly entirely new paradigms for search. Note that the \"Statistical Language Modeling\" paradigm is not about Large Language Models that are on vogue now (that's covered under the \"Neural Retrieval\" umbrella), and that \"Quantum IR\" is not going to get you to a tutorial about Quantum Information Retrieval but to methods of infrared spectroscopy or a company with the same name that produces cement; such are the intricacies of search technology, even in the 21st century. If you want to play with BM25 and compare it with some of the alternatives, I recommend the research platform Terrier, and open-source search engine developed at the University of Glasgow (today, perhaps the epicenter of search research). BM25 is over a quarter century old, but has proven to be a hard baseline to beat (it is still often used as a reference point for comparing new nethods against), and a more recent variant, BM24F, can deal with multiple fields and hypertext (e.g. title, body of documents, hyperlinks). The recommended paper to read is: Spärck Jones, K.; Walker, S.; Robertson, S. E. (2000). \"A probabilistic model of information retrieval: Development and comparative experiments: Part 1\". Information Processing & Management 36(6): 779–808, and its successor, Part 2. (Sadly they are not open access.) reply CSSer 1 hour agoparentCoincidentally, US NIST TREC is happening right now! It started on the 18th and will conclude on the 22nd. Link for more details: https://trec.nist.gov/ reply marcyb5st 10 hours agoparentprevThanks for sharing! Do you have more information about BM24F? Googling (also Google scholar) didn't yield anything related. Thanks in advance! reply bradleyjkemp 9 hours agorootparentA typo I think, should be BM25F. From Wikipedia: > BM25F (or the BM25 model with Extension to Multiple Weighted Fields) is a modification of BM25 in which the document is considered to be composed from several fields (such as headlines, main text, anchor text) https://en.wikipedia.org/wiki/Okapi_BM25 Some papers are linked in the references reply marcyb5st 6 hours agorootparentThanks, really appreciate it! reply MPSimmons 2 hours agoprevDoes anyone know if the average document length mentioned in the document length normalization is median? It seems like it would need to be to properly deweight excessively long documents, otherwise the excessively long documents would unfairly weight the average, right? reply softwaredoug 2 hours agoparentIt’s the mean. At least in Lucene. Using median would be an interesting experiment. Do you know of a search dataset with very large document length differences? MSMarco for example is pretty consistent in length. reply sidcool 10 hours agoprevGood article. I am genuinely interested to learn about how to think of problems in such a mathematical form. And how to test it. Any resources? reply tselvaraj 4 hours agoprevHybrid search solves the long-standing challenge of relevance with search results. We can use ranking fusion between keyword and vector to create a hybrid search that works in most scenarios. reply RA_Fisher 8 hours agoprev [–] BM25 is an ancient algo developed in the 1970s. It’s basically a crappy statistical model and statisticians can do far better today. Search is strictly dominated by learning (that yes, can use search as an input). Not many folks realize that yet, and / or are incentivized to keep the old tech going as long as possible, but market pressures will change that. reply mrbungie 7 hours agoparentAre those the same market pressures that made Google discard or repurpose a lot of working old search tech for new shiny ML-based search tech? The same tech that makes you add \"+reddit\" in every search so you can evade the adversarial SEO war? PS: Ancient != bad. I don't know what weird technologist take worries about the age of an invention/discovery of a technique rather than its usefulness. reply RA_Fisher 6 hours agorootparentGoogle’s come a long way since PageRank + terms. Ancient doesn’t mean bad, but usually it means outdated and that’s the case here. Search algos are subsumed by learning models, our species can do better now. reply mbreese 5 hours agorootparentSo, I’m not entirely sure if I follow you here… How would one use a language model to find a document out of a corpus of existing documents? As opposed to finding an answer to a question, trained on documents, which I can see. I mean answering a query like “find the report containing X”? I see search as encompassing at least two separate, but related, domains: information gathering/seeking (answering a question) and information retrieval (find the best matching document). I’m curious how LLMs can help with the later. reply ordersofmag 3 hours agorootparentThat's the 'vector search' people are talking about in this discussion. Use the LLM to generate an embedding vector that represents the 'meaning' of your query. Do the same for all the documents (or better with chunks of all the documents). Find the document vector that's closest to your query vector and you have a document that has a 'meaning' similar to your query. Obviously that's just a starting point. And lots of folks are doing hybrid where they combine bm25 search with some sort of vector search (e.g. run them in parallel and combine results, or do a bm25 and then use vector search to rerank the top results). reply simplecto 7 hours agoparentprevThose are some really spicy opinions. It would seem that many search experts might not agree. David Tippet (formerly opensearch and now at Github) A great podcast with David Tippet and Nicolay Gerold entitled: \"BM25 is the workhorse of search; vectors are its visionary cousin\" https://www.youtube.com/watch?v=ENFW1uHsrLM reply RA_Fisher 6 hours agorootparentI’m sure Search experts would disagree, because it’d be their technology they’d be admitting is inferior to another. BM25 is the workhorse, no doubt— but it’s also not the best anymore. Vectors are a step toward learning models, but only a small mid-range step vs. an explicit model. Search is a useful approach for computing learning models, but there’s a difference between the computational means and the model. For example, MIPS is a very useful search algo for computing learning models (but first the learning model has to be formulated). reply softwaredoug 4 hours agorootparentI don't know a lot of search practitioners who don't want to use the \"new sexy\" thing. Most of us do a fair amount of \"resume driven development\" so can claim to be \"AI Engineers\" :) reply RA_Fisher 2 hours agorootparentI don’t think it’s realistic to think that software engineers can pick up advanced statistical modeling on the job, unless they’re pairing with statisticians. There’s just too much background involved. reply binarymax 1 hour agorootparentYour overall condescending attitude in this thread is really disgusting. reply RA_Fisher 1 hour agorootparentStatisticians are famously disliked, especially by engineers (there are open-minded folks, of course! maybe they’d taken some econometrics or statistics, are exceptionally humble, etc). There are some interesting motives and incentives around that. Sometimes I think in part it’s because many people would prefer their existing beliefs be upheld as opposed to challenged, even if they’re not well-supported (and likely to lead to bad decisions and outcomes). Sticking with outdated technology is one example. reply simplecto 5 hours agorootparentprevIt seems that the current mode (eg fashion) is a hybrid approach, with vector results on one side, BM25 on the other, and then a re-reank algo to smooth things out. I'm out of my depth here but genuinely interested and curious to see over the horizon. reply RA_Fisher 44 minutes agorootparentBest is to use one statistical model and encode the underlying aspects of the context that relate to goal outcomes. reply authorfly 5 hours agorootparentprevOut of interest how come you use the word \"mode\" here? reply simplecto 4 hours agorootparentbecause the space moves fast, and from my learning this is the current thing. Like fashion -- it changes from season to season reply dumb1224 7 hours agorootparentprevAgreed. In the 2000s it was all about BM25 in the NLP community. I hardly see any paper that did not mention it in my opinion. reply authorfly 5 hours agorootparentAnd dependency chaining. But yes, lots of BM25. The 2000s and even 2010s was a wonderful and fairly theoretical time for linguistics and NLP. A time when NLP seemed to harbor real anonymized general information to make the right decisions with, without impinging on privacy. Oh to go back. reply RA_Fisher 6 hours agorootparentprevFor sure, it’s very popular, just not the best anymore (and actually far from it). reply netdur 7 hours agoparentprevWhile BM25 did emerge from earlier work in the 1970s and 1980s (specifically building on the probabilistic ranking principle), I'm curious about your perspective on a few things: What specific modern statistical approaches are you seeing as superior replacements for BM25 in practical applications? I'm particularly interested in how they handle edge cases like rare terms and document length normalization that BM25 was explicitly designed to address. While I agree learning-based approaches have shown impressive results, could you elaborate on what you mean by search being \"strictly dominated\" by learning methods? Are you referring to specific benchmarks or real-world applications? reply RA_Fisher 6 hours agorootparentBM25 can be used as a starting point for a statistical learning model and more readily built on. A key advantage is that one gains a systematic way to reduce edge cases, instead of handling a couple, bc they’re so large as to be noticeable. reply softwaredoug 4 hours agoparentprev [–] I think there are also incentives to \"sell new things\". That's always been the case in search which has had a bazillion trends and \"AI related things\" as long as I've worked in it. We have massively VC funded vector search companies with armies of tech evangelists pushing a specific point of view right now. Meanwhile, the amount of manual curation, basic, boring hand-curated taxonomies that actually drive things like \"semantic search\" at places like Google are simply staggering. Just nobody talks about them much at conferences because they're not very sexy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "BM25, or Best Match 25, is a widely used algorithm for full-text search, implemented in systems like Lucene/Elasticsearch and SQLite, and often paired with vector similarity search in hybrid search systems.",
      "It ranks documents based on relevance to a query using factors such as query terms, Inverse Document Frequency (IDF), term frequency, and document length normalization, following the Probability Ranking Principle.",
      "BM25 scores are context-specific, meaning they are comparable within the same document collection but not across different collections or over time, due to potential changes in the collection affecting scores."
    ],
    "commentSummary": [
      "The discussion focuses on the BM25 search algorithm and its integration with modern search technologies, such as hybrid systems like Typesense that combine BM25 with vector-based semantic search.- Participants debate the effectiveness of BM25 compared to newer learning models, with some advocating for its continued use and others suggesting more advanced statistical models.- The conversation also explores tools and methods like Reciprocal Rank Fusion (RRF) and the role of machine learning in enhancing search capabilities."
    ],
    "points": 236,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1732074190
  },
  {
    "id": 42190863,
    "title": "Blender 4.3",
    "originLink": "https://www.blender.org/download/releases/4-3/",
    "originBody": "Splash artwork by Blender Studio Released November 19th, 2024 Video Recap Round-up of what’s new, in detail. Blender 4.3 new features overview by Jonathan Lampel from CGCookie, Harry Blends, Christopher 3D, and Aidy Burrows from Creative Shrimp. EEVEE It’s (real-)time for LIGHT LINKING The real-time renderer EEVEE now supports Light Linking and Shadow Linking—features previously available only in Cycles. More control than eeveer before. With light linking, lights can be set to affect only specific objects in the scene. Shadow linking additionally gives control over which objects acts as shadow blockers for a light. This is now feature parity with Cycles. Read More Simply create a Light Linking collection and drag in the objects you want to affect. Toggle the checkbox to invert the effect. SHADERS METAL TO THE PEDAL A new Metallic BSDF node has been added to the shader editor. Metallic BSDF This new node exposes existing, but hard to access metallic material configurations in a small node. Read More Metallic BSDF recreating Nickel and Copper materials. Metallic BSDF node. F82 Tint The F82 Tint approximation is artist friendly, taking colors as inputs. This is currently in use by the Principled BSDF. Conductor Fresnel This method is more complex to use, requiring IOR and Extinction coefficients per color channel as inputs. However, it can produce subtly more accurate results for real-world metals, previously achievable only through custom OSL scripts. EEVEE does not yet support the Conductor Fresnel type, so it internally remaps the IOR and Extinction coefficients to colors and uses them with the F82 Tint approximation. TEXTURES Make Some (Gabor) Noise A new texture node was added that can create procedural Gabor noise for random interleaved bands with controllable direction and width. New texture node Gabor Noise example in Blender 4.3 See the Manual COMPOSITOR YOU SHALL PASS Forget F12. EEVEE passes are now available for interactive compositing. Multi-pass compositing lets you create complex NPR setups and effects directly in the 3D Viewport. MORE COMPOSITOR Color Balance: New White Point conversion mode File Output: New global Save As Render option GPU Compositor: Support for meta-data necessary for exchanging Cryptomatte through EXRs using the File Output node Compositor Auto Render has been removed BLENDER EXTENDED The Blender Extensions platform continues to grow, offering hundreds of free add-ons and themes that enable endless customization of workflows. You can also share your own add-ons and themes! Blender Extensions Platform Share Your Extensions UV EDITING SLIM UVs A new method makes unwrapping organic shapes a breeze. Minimum Stretch, Minimal Effort A new iterative unwrapping method “Minimum Stretch” has been added which can deliver results with less distortion by iteratively refining the result. Ideal for organic shapes. This uses the SLIM algorithm (Scalable Local Injective Mappings) internally, see the research publication. It can be used from the Unwrap menu as well as “Live Unwrap”. New UV Unwrap menu Minimum Stretch (SLIM) Conformal Angle Based (default) Read Manual See Research Paper PLUS Added support for manual seams in UV Sphere/Cylinder Projection Two new Select Similar options: Winding, Object STUDIO FRIENDLY Thanks to portable installation, new environment variables, custom bundling of extensions, Blender 4.3 is easier than ever to integrate in studio pipelines. Deploying Blender in Production GEOMETRY NODES SO MUCH IT DOESN’T FIT THIS TITLE For Each Element Zone. Gizmos. Support for Grease Pencil. Packed Bakes. Custom Warnings. Geometry Names. New Nodes. Simon Thommes offers a delightful sneak peek into the abundance of Geo Goodies packed into this Blender update! Overview of everything new in Geometry Nodes by Simon Thommes from Blender Studio. EFFORTLESS ITERATION WITH FOR EACH ELEMENT A new type of loop zone is available now! The For Each Element zone will make it easy to iterate over elements of a geometry in parallel, making repetitive processes a breeze to handle. Read Details GET HANDS-ON You can now add gizmos to node groups! This means you can edit the inputs to a node tree right in the 3D viewport – no need to dive into the node editor or modifier stack. It’s a game-changer for a more intuitive and hands-on experience! Modifier Interaction Nodes Interaction Although built-in nodes aren’t using this power yet, the future looks bright. Advanced users will benefit now, and soon everyone will enjoy Blender’s growing procedural capabilities! Read Details GEOMETRY NAMING MADE EASY Introducing the Set Geometry Name node, a new node designed to enhance your workflow. This node allows for the easy assignment of names to geometries, with automatic initialization based on object and collection names. Geometries can now be named with the new Set Geometry Name node. Read Details GEO NODES MEETS GREASE PENCIL Geometry Nodes now work smoothly with Grease Pencil data, breaking it down into layers with curves and custom attributes. Updated nodes can now handle Grease Pencil data seamlessly, processing each layer separately. Fan art by Daniel Martinez Lara of “Magical Cat” by Threadwood, drawn using Grease Pencil converted into mesh on-the-fly using Geometry Nodes for a “claymation” effect. Read Details NEW UTILITY NODES Hash Value Hashes various types into an integer. Useful to generate stable randomness. Integer Math Provides building block support for integer operations WARNINGS UI Introducing the Warning Node, which allows custom warnings to be added to node groups, allowing dynamic messaging directly on the interface. BE ALERT The display of warnings in the Geometry Nodes modifier has been reorganized. They are now in a panel and ordered by severity and alphabetically. Read Details OTHER GEO GOODIES Bake node in use PACKED BAKES Bakes created with the Bake node or Simulation zone can now be packed into the .blend file. MORE GEOMETRY NODES Convert Grease Pencil to Curves Convert Curves to Grease Pencil Merge Layers node for Grease Pencil layers Improved node timings for better accuracy Skip checkbox is now hidden in Simulation Zone Sculpt mask made accessible in node tools What the community is creating with Geometry Nodes Error diffusion image dithering using #geometrynodes🤷♂#b3d pic.twitter.com/Q6T0129rr8 — higgsas (@higgsasxyz) September 17, 2024 Posing with my ClayPencil (Just Grease Pencil Geo nodes) parts already modeled, a lot of possibilities for more complex chars. Geo can have Shapekeys. A week after the Blender 4.3 release ClayPencil will be public, Stay tuned! #Blender #B3D #ClayPencil #GreasePencil #2D pic.twitter.com/nGhfzjlSLx — Daniel Martínez Lara 🔶 (@_pepeland_) November 18, 2024 After Lunch I spent a couple hours to figure out how improve my bike chain setup. Now it works for pretty small bending radii. Not sure it's the most efficient, accurate or robust solution, but it seems to work! #b3d #geometrynodes pic.twitter.com/w2hyp0dodB — Riccardo Bancone (@RiccardoBancone) November 16, 2024 #GeometryNodes on Bluesky on mstdn.social on X What’s Next? Get a glimpse of what’s to come to Geometry Nodes (spoiler alert: hair dynamics and physics!). Read Blog Post Blender is and will always be free, forever. Releases are possible thanks to donations by the community. Donate Monthly Donate Once GREASE PENCIL GREASE LIGHTNIN’ The Grease Pencil engine was completely rewritten for better performance and to remove deeper limitations. Pencils down up. Group Hug Organize your layers into Layer Groups! A new way to keep your Grease Pencil objects tidy. They allow for easy toggling of visibility, locking, and onion skinning for all the layers in the group. Additionally, they can be color-coded using color tags, similar to Collections in the Outliner. Read Manual Filling Great A new Fill Gradient tool is here! Found in Edit Mode below the Interpolation tool, it lets users change the fill gradient (set in material settings) by selecting fill strokes and click-dragging with the Gradient tool. Read Manual Four eraser brushes available through the asset shelf. No Trace Left Behind The eraser has been rewritten to allow strokes to be “cut”. Instead of just deleting points, it now correctly solves for the intersections of the eraser’s edge and creates new points on the stroke. The Soft eraser also does this but for multiple levels of transparency radiating outwards from the center of the eraser. Read Manual MORE GREASE PENCIL Conversion & Compatibility Deprecated Features Python API Migration Guide INDUSTRY READY All Blender releases this year align with the VFX Reference Platform 2024, making integration and maintenance in studio pipelines easier. VFX Platform See All Library Changes BRUSH UP YOUR WORKFLOW With over a hundred default brushes and an improved interface, Blender 4.3’s new brush workflow makes management, customization, and sharing across projects easier than ever. Asset shelf in the 3D Viewport while in mesh Sculpt mode. Asset shelf in the 3D Viewport while in Grease Pencil Draw mode. BRUSH = ASSET Brushes are now assets, making them easy to save, share, and reuse through the Asset Browser. Organize custom libraries and access your favorite brushes across projects. Read Manual Duplicate, save, and edit medatada of brush assets. AND MORE Voxel Size operator now works in relative mode by default Lasso tools now have optional stroke stabilization A polyline gesture can now be finished with double-click Sculpt mask is now accessible in node tools Mask from Boundary operator added to modify mask values based on mesh or face set islands USER INTERFACE USER INTERFACE Dock-on Dock-off Introducing: Area Docking! Tear an area into its own (now neatly named) window, or dock it back into another area. Lots to Say Tooltips now provide much more information than before. Images display their resolution and color space, movies show their duration, and fonts include previews. To improve accessibility, tooltips now have dedicated style settings for font size, weight, and shadows. Clarity A new theme option allows for enhanced highlights around an area when the mouse is over it, for better accessibility. Non-active areas can also have a custom border color. MORE UI Nodes Added support for line separators in built-in nodes. Hover on panel headers show their description. Node Groups: new default width option. String sockets can now use placeholders instead of labels, to allow wider input fields. Inserting nodes with link-drag-search is much more convenient now. Cursors macOS: New “Hand”-style mouse cursors added for dialog and docking interaction. New cursors for VSE handles. Keymap File Browser: Assign mouse buttons 4 and 5 to navigate back/next. Spreadsheet Support for selecting the viewed geometry from the instance tree. Status Bar Status bar cleared during text input and other modal operations. Compact layout thanks to Improved spacing. New SVG icons. Show screen operation keymaps info. General Improved Color Picker layout. Wider data-block search lists. Dialogs: Change the cursor on hover when a dialog can be dragged. Timeline (and similar editors) hide the scroll-bars when they are sized very short. Maximum UI resolution scale increased to 6x. Prevent tooltips from changing size on zoom. Viewport color picking in Rendering mode now returns the linear color value, without view transform applied. The Auto-Keying button is now colored red when active. Windows with a single editor now have descriptive titles. Better padding all over the place. File Browser thumbnail file type icon moved to corner of item bounds. Color sampling performance improvements. Hit size for resizing areas increased. Menus Recent Files: New option to clear only items not found. Recent Files: Items not found are no longer removed automatically. “Save Incremental” menu item moved down in its section to reduce accidental selection. Text “Widget Label” Text Style removed. Text weights change correctly if selecting a UI font with non-regular weight. ARMED FOR THE FUTURE Blender 4.3 runs natively on Windows on Arm, thanks to integration by Qualcomm, which recently joined the Development Fund. Learn More Watch Announcement by Qualcomm BRINGING STRIPS TOGETHER Blender 4.3 is another fantastic release for the Video Sequencer, with numerous performance improvements and features you won’t want to live without. CONNECTED STRIPS Connect strips for easy selection and transformation. Hold Alt to select and transform only one strip while keeping the connection. Quickly toggle connection using Ctrl+Alt+C. Movie strips are automatically connected to their sound strip. Snapping in Sequencer Preview. OH SNAP! Sequencer Preview now supports snapping, with source points at corners and origins of all selected, visible strips. Targets include preview boundaries, center, and other strip corners/origins. FASTER EVERYTHING Color Balance is several times faster. Saturation and Multiply strip color controls are several times faster (multi-threaded now). Tonemap modifier is 10-15 times faster (multi-threaded luminance estimation, color conversion optimizations). Faster processing of alpha-over strip at the bottom of strip stack. Various parts of image processing related to color transformations (particularly for float/HDR images) are faster now. Drawing the timeline has been improved for complex timelines: Faster drawing of the channels list. Faster drawing of waveforms and animation curve overlays. Faster drawing of retiming keys. Other optimizations. Multi-threaded video proxy downscaling. Thumbnails now draw much faster. Faster and more consistent thumbnail cache. EVEN MORE SEQUENCER Thumbnails on strips are now enabled by default Sound: Added support for sub-frame adjustments Preview: Show toolbars by default Modifiers: Move modifier-specific settings first, Mask settings after Preview: Change default tool to Box Select Timeline: Box Select and Tweak tools combined GET THE ARTWORK Blender splash artwork source files are available for you to play with, not only for Blender 4.3 but for previous versions as well. Download Splash File Blender 4.3 splash artwork by Blender Studio BUT WAIT, THERE’S MORE Vulkan Backend On Windows and Linux, it is possible to use Vulkan to render the user interface (experimental) Sculpt Entering sculpt mode is about 5 times faster. Brush evaluation is about 8 times faster for mesh sculpting. Animation Properties Editor: Added Action selectors for data-blocks. Motion Paths: New theme entry to drive the color of the lines before/after the current frame. Keyframing: When inserting keys, all other keys get deselected leaving only the newly created keys selected. Bones Eyedropper: Properties where a bone can be chosen now have an eyedropper button that allows to pick bones from the 3D Viewport or the Outliner. Core It is now possible to force rename elements. Copy Material to Selected now sets the Link type of the selected objects to match those of the active object. Cycles Linux: Support for hardware accelerated ray-tracing using HIP-RT. Volume Scatter: New phase functions. Oren-Nayar BSDF is now energy-presserving and accounts for multiscattering, matching matches OpenPBR. Principled BSDF: New Diffuse Roughness input. macOS: AMD and Intel GPU support from Metal backend has been removed. Panoramic Camera: Support for cylindrical projection mapping. NVIDIA: Performance and memory optimizations for B-Spline curves when using OptiX. NVIDIA: Minimum driver required is now version 495.89 AMD: Support for Vega in Cycles AMD HIP backend has been removed. EEVEE Add toggle for Fast GI Approximation, similar to Cycles. Objects with volume probe visibility turned off now cast shadow properly during lightprobe volume baking. Modifiers Bevel: New option to use custom attributes as bevel weights. USD Added support for exporting Point Clouds Material purposes are now selectable during import Improved handling of @asset@ paths inside USD files More efficient export of animated attribute data when unchanged from prior frames. glTF Import: Enable Draco mesh compression Import: Fixed importing Vertex Color on point/edges Export: Set UDIM material names with tile number. Export: Manage Quaternion & Matrix attribute types for custom attributes. Export: Better logging Export: Enable exporting joint leaf at tail of leaf bones. Hook UI: distinct import & export draw code Several bug fixes Python API Handlers: Added new blend_import_pre and blend_import_post called before/after any linked-data-related operation. Grease Pencil Python API migration guide Data-Blocks: New rename function New functions for Curves Added bpy.app.python_args to support calling Python in an environment matching Blender’s Python environment. uiLayout.template_search() now supports a custom text label Breaking changes Text Editor “Wrap Around” is now enabled by default UI improvements to Find panel Plus hundreds of bug fixes, code cleanups and refactors. See the full list of changes. CREDITS List of developers that contributed to Blender 4.3 Blender is a community project. Learn more on how you can contribute to Blender. Splash artwork and Geometry Nodes demo files by Blender Studio. Features recap video by Jonathan Lampel from CGCookie, Harry Blends, Christopher 3D, and Aidy Burrows from Creative Shrimp. Huge thanks to everyone involved ❤ The Blender team. November, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42190863",
    "commentBody": "Blender 4.3 (blender.org)232 points by antome 14 hours agohidepastfavorite106 comments xyst 10 hours agoIf you are remodeling your home, strongly suggest using Bonsai (previously Blender BIM) add-on to model and plan. In addition to getting my layout just right down to the ft, and using current materials (ie, hardwood, cabinet placement). I modeled out HVAC ducts and overall system. I had planned on doing major renovations (including plumbing, electric, networking) and would have modeled those as well since the walls would be exposed. But ultimately, due to changing political atmosphere, decided to put these on hold. It does have a learning curve but would have been made collaboration with at architects much easier (exchanging ifc files). reply smusamashah 9 hours agoparentIt looks like its for people with engineering backgrounds. For me something like Sketchup is lot easier to plan a mockup. https://extensions.blender.org/add-ons/bonsai/ reply xyst 4 hours agorootparentDownside with sketchup is that it likely has to translate IFC into its own format. Could result in loss. Might be easier up front but lose out on sharing/communicating your designs to different types of people. reply Joel_Mckay 7 hours agorootparentprevBlender needs additional add-ons for auto-generated floor plans to 3D scenes. https://github.com/grebtsew/FloorplanToBlender3d The learning curve on your own is like a kick to the berries... However, we found these courses, and some after market assets libraries make it workable. Should wait for the $19.99 deals to take these courses, and pay attention to the stated Blender version during instruction: https://www.udemy.com/course/blendertutorial/ (low poly model skills) https://www.udemy.com/course/blender-animation-rigging/ (some overlapping material, more rigging) https://www.udemy.com/course/blender-3d-sculpting-course/ (stuff is out of date in this one, but if you can tolerate the kids antics... one may learn a lot about retopo and non-human sculpts.) Textures: https://blendermarket.com/products/sanctus-library-addon---p... (Awesome... can bake wood, metal, brick, tile, plastics etc.) https://tinynocky.gumroad.com/l/tinyeye (Fantastic totally underrated eye tool) Assets: https://blendermarket.com/products/poly-haven-asset-browser (assets are free on their site too) https://static.makehumancommunity.org/mpfb/releases/release_... (free low poly random people) Would also recommend looking at speed sculpts for tips about how to fix humanoid meshes: https://www.youtube.com/@BranSculpts/playlists For advanced users, there are also working rudimentary ZSpheres/Metaballs support similar to ZBrush, fabric quad stitching, and volume aware re-topo process tools: https://bartoszstyperek.gumroad.com/l/ahwvli?layout=profile https://www.youtube.com/watch?v=a0GW8Na5CIE (how to get blender workflows to work on lower end GPUs) Rigging tools: https://blendermarket.com/products/auto-rig-pro (UE5 export helps) Went through several grand worth of plugins this year for a project, and these paths proved useful. Best of luck, YMMV... =3 reply haunter 6 hours agoparentprevFor the average person Sweet Home 3D is a much better choice http://www.sweethome3d.com/ reply diggan 3 hours agorootparentDifferent use case, as far as I can tell. To put it into web programming terms, Bonsai is for making the backend/structure, while Sweet Home 3D looks more like HTML/CSS/frontend. reply xyst 4 hours agorootparentprevRight - if you only care about interior design. Might be overkill. reply rnewme 9 hours agoparentprevHow does it compare to Revit, for smaller basic projects like you mentioned (residential house/simple office, with hvac etc)? reply xyst 4 hours agorootparentI am not a professional architect or engineer (in the construction space), so I haven’t tried it. The subscription pricing turns me away. Seems like it has most of the same offerings as Bonsai just by browsing the marketing materials (support for IFC). Not sure if it’s native IFC or translated to their proprietary format, likely the latter. With that said, seems like it’s more or less the same learning curve (ignoring price) reply oever 3 hours agoparentprevIs it possible to model thermal performance of a building with Bonsai? reply xyst 1 hour agorootparentI haven’t explored it myself, but at least the IFC spec allows you to define thermal properties of materials. https://ifc43-docs.standards.buildingsmart.org/IFC/RELEASE/I... I suspect it’s something that’s not offered out of the box though. reply wolframhempel 11 hours agoprevBlender, in my view, is one of the best arguments to develop native desktop applications over web view wrappers like Electron. Everything is responsive and instant, you can open new 3D view windows by just sliding them open at 60 frames a second, performance is stable even with multiple windows open and the download size is a reasonable 334MB. reply aurareturn 10 hours agoparent>Blender, in my view, is one of the best arguments to develop native desktop applications over web view wrappers like Electron. Different class of products. For Electron apps, often it's the choice of having a desktop app or not having one at all. reply daemin 8 hours agorootparentI would prefer they do not make an electron app and just have people use the web-app contained in a browser like it was meant to be. reply aurareturn 7 hours agorootparentSo don't download the desktop electron app? What's the problem? reply daemin 7 hours agorootparentWeb versions don't work with all browsers, they also want you to use the app, and heavily promote using the app, even though the app is just a web page in a browser that's not your browser. If they want you to use an installed app then they should just build a native app that can be installed. reply zztop44 7 hours agorootparentBut what’s stopping you just using the web page (in a browser that it works with)? Yes, they want you to use the app, but you can still do what you want. reply swiftcoder 5 hours agorootparentThere is usually critical missing functionality from the web version (mostly around native filesystem access). Also, most sites that offer an Electron app paster annoying banners advertising this fact across their web version... reply daemin 6 hours agorootparentprevI have done that in the past, though it was more that certain features were just broken in a browser rather than the whole website not working. If I needed to run another entire browser just for this website I wouldn't bother. reply McDyver 8 hours agorootparentprevIn my opinion, the actual problem is the focus on making everything web first. > For Electron apps, often it's the choice of having a desktop app or not having one at all. Maybe it would be better to have none at all. Having an electron app is an excuse to say \"we already have a desktop app, so we won't make a native one\". reply flohofwoe 3 hours agorootparentThe reason why Electron has won is because native platforms couldn't be arsed to agree on a standardized native UI framework integrated into the platforms (and not just Microsoft, Apple and Google (via Android), but also 'Linux on the desktop' which is still the same fragmented mess as it ever was (arguably worse after Wayland will replace X11 any moment now). Electron's success is strictly to blame on the desktop platform owners/maintainers dropping the ball, consistently and repeatedly for the last 3 decades (Microsoft's UI framework 'evolution' is especially hilarious, who in their right mind would write an application against a widget library that's going to be deprecated in 3 years). reply aurareturn 8 hours agorootparentprevIn my opinion, the actual problem is the focus on making everything web first. I don't think this is a problem. Many class of products require a web version over a desktop one. The desktop one is a very nice bonus. reply rounce 8 hours agorootparentprevI wonder if this relationship will invert as wasm and web guy and their tooling matures, to the point where these types of applications where drawing performance is a must are being targeted desktop first and then thrown through a OSFA wasmification step to run on the web. reply flohofwoe 11 hours agoparentprevIt notably doesn't use any 'standard' UI framework though, which is frowned upon by some people just as much as using Electron (but for different reasons - usually missing accessibility features and not looking consistent with 'system applications'). reply jbggs 10 hours agorootparentI think that's the smoking gun for why most UIs are terrible. it is definitely cheaper to use a framework, but maybe, sometimes, doing something yourself (for 25+) years is worth it reply bzzzt 3 hours agorootparentprevAs if there's still a consistent option. It doesn't matter if you use Microsoft, Apple or an OSS desktop: all offer multiple UI toolkit options with varying amounts of consistency, maturity and hardware support. reply InDubioProRubio 10 hours agorootparentprevShould be separated out and reused by many Electron apps in my opinion reply flohofwoe 9 hours agorootparentTo bear clear, Blender is not an Electron app (reading my comment again it could be read as such I guess). Blender is a native application sitting directly on the operating system's 3D API and window system (with the UI being rendered through the 3D API, not through operating system widgets). reply exe34 8 hours agorootparentit could probably work well under webgl... reply swiftcoder 5 hours agorootparentThe WebGL API is sadly two decades behind state-of-the-art for modern rendering, and even the will-be-ready-for-adoption-any-day-now WebGPU spec is ~5 years behind. Between that and the overhead of running the whole thing in WASM+Javascript, I don't think we'll see a pro-level DCC tool running on the web anytime soon. reply KeplerBoy 7 hours agorootparentprevIt's not html/css/js though. reply cultofmetatron 11 hours agoparentprev> and the download size is a reasonable 334MB. I remember when it was 2 MB!! reply ideasman42 10 hours agorootparentI remember downloading it from my library on a floppy disk (hoping the librarian wouldn't hear it the floppy drive working - since it wasn't allowed). :) It's in fact still quite small if additional 3rd party libraries are disabled. It's possible to make a much smaller download that removes OpenCollada, FFMPEG, OpenEXR, OpenImageDenoise, OpenVDB ... etc. However it's a hassle to distribute a second version at a time when the current size is manageable for most users. reply cageface 9 hours agoparentprevOf course there are still categories of app that should be native. That doesn’t mean there isn’t a place for things like Tauri and Electron too. reply lukan 10 hours agoparentprevElectron apps can be very responsive as well. But a 3D designer is definitely in the category of needing all the performance you can get. So yes, native makes sense here. reply raincole 9 hours agoparentprev3D DCCs are very different from todo lists. reply geenat 11 hours agoparentprevIt's not a web view reply flohofwoe 11 hours agorootparentThat's what the parent is saying, I also had to read twice though ;) reply demarq 9 hours agoparentprevPlease check out plasticity, this “electron bad” argument is pretty outdated and never evidenced. There is absolutely nothing that says browsers can’t do 60fps with multiple 3d views. reply divan 8 hours agorootparent\"electron bad\" argument will always be valid. I wish people could \"see\" what actually happens in the app they running. Electron is essentially a hidden browser that runs a web app. It doesn't matter if it can or can not do 60fps. Installing yet another copy of the browser just to run that small web app is just wrong. reply aniviacat 7 hours agorootparentI assume most people who dislike Electron dislike Tauri too, even though it doesn't need to ship a browser. reply flohofwoe 9 hours agorootparentprevThere definitely are a couple of annoying details. For instance visual mouse lag (when dragging items around) in WebGL and WebGPU can be higher than in a well written native application (just one or two additional frames of latency make a huge difference). Then there's WebGL2 being stuck on a GPU feature set from ca 2 decades ago, and WebGPU starting at a feature set from about a decade ago (and not being quite ready yet anyway). Then there's also annoying differences between browsers when capturing the mouse - which is needed for camera controls (all browsers show some sort of popup while the mouse is captured, with Safari even shifting the canvas around). TL;DR: it's possible in theory, but can be very annoying when actually trying to do it, mostly because most web APIs are badly designed (WebGL2 and WebGPU are notable exceptions, but they still lag far behind native 3D APIs). reply countrymile 10 hours agoprevBlender really is a gem, for beginners there really is little reason to learn anything else, fully featured, free and runs off a USB or shared drive, can run on pretty old hardware. For examples of what teenagers in the UK are making with Blender see these: https://news.ycombinator.com/item?id=42113898 https://younganimator.uk/submissions/16f0edf9-4fff-43d5-b82b... https://younganimator.uk/submissions/96059d6c-796e-485f-9582... reply pcblues 8 hours agoparentThey were gorgeous. reply hannofcart 7 hours agoprevBlender's success is a lot due to how welcoming they are of new contributors. 15 years years back, this was the first big OSS project that I contributed to. Since I was a greenhorn, the patch I submitted (there weren't 'PR's back then :) ) was, to say the least, rather shoddy. Ton Roosendaal helped me patiently on the IRC (I think they were on Freenode back then, not fully sure), guiding me through the changes needed to make it acceptable to merge it. I learnt a ton just hanging out in the dev channel. Good times. It was a great product even back then but it's amazing how much more awesome it has gotten since then. Great work Blender team. reply stevage 11 hours agoprevIt's really puzzling (but extremely welcome) that Blender continues to be such an open source success story. Seems rare for such complex pieces of software in a niche space to get that level of development. I wonder what the secret is. reply greenknight 11 hours agoparentWhat really kickstarted their development... was the introduction of the Blender Development Fund -- https://fund.blender.org/ They made tiers, made it simple and easy, and promoted it. Before the fund really was pushed... they were getting about 5,000 USD per month to develop it... Now it is sitting at 215,000 USD per month. More money = More developers. More Developers = Better product. Yes it didnt happen overnight that increase, but it was slow and steady. reply jasode 7 hours agorootparent>What really kickstarted their development... was the introduction of the Blender Development Fund [...] >More money = More developers. There was actually another \"kickstart\" before that kickstart. Blender's open-source timeline has a very unusual history in that it was a commercial product funded by €4.5 million in VC capital. Those original investors lost their money by selling in a \"down round\" to a 2nd set of investors. Those later investors also then lost their money by selling back the source code for a discount of 100k EUR to today's non-profit Blender organization. https://docs.blender.org/api/htmlI/x115.html One of the reasons (but not the only reason) that other examples of open-source projects like ... Gimp lagging Photoshop, or FreeCAD not being as polished as Fusion360/SolidWorks ... is those tools never had millions in investor money paying the salaries of 50 developers to kickstart them. E.g. FreeCAD has a non-profit fund but it doesn't attract the same mindshare as Blender did in 2002: https://www.google.com/search?q=freecad+non-profit+fund Just because Blender started a fund doesn't mean any open-source project can also just \"start a development fund\" and attract the same level of donations. Blender has some extra history and circumstances in the timeline of \"cause & effect\" that a random open-source project can't easily replicate. Blender circa ~2002 had a level of mindshare + evangelism + momentum that most open-source projects do not have. Those ingredients have to be there first to help attract donations to the fund. reply dirkc 1 hour agorootparentMy takeaway from this is that there was a community of people willing to collectively pay 100k EUR and I'd say that was the biggest contributor to it's success - a large group of people seeing the value of the shared good. I don't have examples at hand to point at, but I feel like there is/was several open source projects that did have the initial VC money, but fizzled out after the money was spent specifically because they got to a fairly polished point without really having a community reply stevage 5 hours agorootparentprevAh, that's really interesting history, thank you. And confirms I'm not crazy - I had vaguely thought long ago that Blender cost money. reply diggan 8 hours agorootparentprev> What really kickstarted their development... was the introduction of the Blender Development Fund -- https://fund.blender.org/ Before that, I'd say open sourcing the project is what really gave it a second wind. If I remember a talk correctly by Ton Roosendaal, the company behind Blender development went bankrupt and development stopped for a while, as it was closed-source at that point. Eventually, he started a fundraiser to get funds to re-acquire Blender and open sourcing it (the goal being 100K EUR or something) which was successful and made Blender into the open source project we know today :) reply prox 10 hours agorootparentprevAnd apparently only 2% percent (according to their ad) donates. I wish people would be more in the “giving back” spirit. If you can afford a designer coffee, throw that money to the Blender Fund! I donate yearly, and its worth it. reply flohofwoe 9 hours agorootparentI guess one very important part is that there are now pretty big companies which depend on Blender and have an interest in Blender's continued development. reply The_Colonel 9 hours agorootparentRight, that's exactly what separates Blender from e.g. GIMP. reply Tomte 8 hours agorootparentprevThe more natural way for individuals is probably https://studio.blender.org/welcome/ reply stevage 5 hours agorootparentprev2% is a pretty high rate. It's pretty standard for 1% of any userbase to contribute in any way (including through content) to it. reply iamgopal 11 hours agorootparentprevtell me more or write a blog post. reply flohofwoe 11 hours agoparentprevAutodesk monopolizing the 3D tooling ecosystem and having entered an aggressive 'customer milking phase' via an overpriced subscription model about a decade ago played a very important part. reply swiftcoder 5 hours agorootparentYeah, I think this is definitely a good part of the driving force behind Blender's more recent successes. That said, Adobe has been doing pretty much the same thing in all their spaces, and it hasn't spurred open-source competition in the same way (though plenty of proprietary competitors have been boosted by it) reply exitb 11 hours agoparentprevAt the same time, I'd expect it should have its Linux-moment by now and eat the world, but it doesn't seem to be the case for high profile productions. reply flohofwoe 11 hours agorootparentIn a way, Blender has eaten the world, it's just very hard to replace an existing and highly customized 3D production pipeline across hundreds or thousands of seats in an existing business. But I bet everyone has created an Autodesk exit-strategy by now and is waiting for an opportune moment to realize it. Using Blender for a video game production is entirely normal now, but was unthinkable 15 years ago, and starting a new game company which depends on Autodesk tools instead of using Blender is quite foolish tbh. reply psychoslave 9 hours agorootparentprevNot sure what you mean with Linux-moment exactly, but Blender is sitting rather in a niche need compared to Linux which sit just above the hardware. So in that sense, Blender can’t have a Linux moment: it won’t be deployed in most servers, it won’t be embedded in a mission to Mars… reply exitb 8 hours agorootparentAt a certain point in time Linux became the best platform available to base wide variety of projects on, so the industry followed and made it a de facto standard. It seems, at least superficially, that Blender has all the capabilities needed for wide industry adoption, but it doesn't seem to happen. Obviously talking about industries related to video production, gamedev etc. reply flohofwoe 9 hours agorootparentprevI guess 'Linux moment' as in 'first they ignore you, then they laugh at you...'. Both Linux and Blender have been going through those stages. reply DonHopkins 8 hours agoparentprevOutstandingly excellent leadership who attracts and inspires an extremely skilled dedicated international team. Ton Roosendaal is the open secret, who's earned all that great credit, reputation, and loyalty over the decades. BLENDERHEADS - Ep.07 https://www.youtube.com/watch?v=u8A59Jtluxw Enjoy the seventh episode of Blenderheads, a series about the people behind the Blender project. In this episode, we follow the process around Blender's showcase named Project Gold, by ‪@BlenderStudio‬ . The editor and director –documentary maker Maaike Kleverlaan– works embedded in the Blender headquarters to cover the activities and conduct interviews. reply xiaomai 11 hours agoprevWhat's the best way to get started with Blender these days? I'm mostly interested in making art and possibly even 3d printing some stuff (do these skills overlap at all?). reply gebar 9 hours agoparentI am a 3D (jewelry) artist & goldsmith who specializes in 3D printable models and sculptures, 99% done in Blender. You might want to check out my Youtube channel for some free tutorials: https://www.youtube.com/@PhialoDesign Or classes on Skillshare: Here is one of my most successful classes, a complete workflow \"From Sketch to (printable) Model\": https://skl.sh/3219zVE (from 2020 but still 100% doable with current Blender version!) Or my \"Blender for Absolute Beginners\" class that is a bit more recent, a thorough introduction to Blender and creation of a simple 3D printable bird :) https://skl.sh/3xw8S77 (Both links give you a one month free trial of Skillshare, and you can watch the intro video without logging in) You may also buy my beginners course directly here: https://gesa-pickbrenner-s-school.teachable.com/p/learn-blen... Cheers :) reply tritiy 11 hours agoparentprevDonut tutorial https://www.youtube.com/watch?v=B0J27sf9N1Y&list=PLjEaoINr3z... As for 3d printing there are other software which is simpler if you need to do simpler things. But you can not go wrong in learning Blender for 3d printing as well. reply KeplerBoy 11 hours agorootparentI'd say that depends a lot on what you want to print. Some people might be able to use Blender for technical parts, but for most people traditional CAD software would be the better choice here. reply skykooler 10 hours agorootparentCAD is great when you want to make a nice parametric model that you can edit as requirements change. But for a simple one-off part, I can often whip one up in Blender in a third the time it would take me to dimension everything in a CAD sketch. reply KeplerBoy 9 hours agorootparentYou and I must make very different parts. Unfortunaley even my one-off 3D prints are never one-offs. I almost always have to go back and tune a dimension or two. reply countrymile 10 hours agoparentprevI've been running Blender training for 17 years and Grant Abbitt is what we normally suggest: https://www.gabbitt.co.uk/course-filter/beginner reply salviati 11 hours agoparentprevThis post from three months ago discusses the donut tutorial (a famous blender course) and another one I've never heard about. https://old.reddit.com/r/blender/comments/1eekomd/beginner_d... I think to get an expert answer on the topic you'll need to listen to what a beginner has to say, so I believe the link I provided is a good source to look into. reply klysm 11 hours agoparentprevDepends on what you want to 3D print. If you’re going for functional parts, then you probably want a CAD package rather than something like blender. If you want to 3D print art than by all means blender is your friend reply Tomte 11 hours agoparentprevCG Boost‘s apple basket course (free) or their Blender Launchpad course (cheap). reply timonoko 10 hours agoparentprevBest way is to steal somebody else's model and improve it to your taste. That is the main use of Blender in 3D-printing, because parametric modelling is not the best property in Blender. Although it is almost feasible to define OpenSCAD model and ask AI to redefine in blender-python. Hey ChatGPT, how you define an empty box in Blender Python? import bpy empty = bpy.data.objects.new(\"Empty_Box\", None) empty.empty_display_type = 'CUBE' empty.empty_display_size = 1.0 bpy.context.collection.objects.link(empty) reply timonoko 10 hours agorootparentIt seems to be \"empty\" in name only. Lets try again. ChatGPT: The OpenSCAD statement difference(){ cube(11); cube(10); } creates a shape by subtracting a smaller cube (10x10x10) from a larger cube (11x11x11). Here's how you can create the same effect in Blender using its Python API: import bpy import bmesh # Create the larger cube bpy.ops.mesh.primitive_cube_add(size=11, location=(0, 0, 0)) cube1 = bpy.context.object # Create the smaller cube bpy.ops.mesh.primitive_cube_add(size=10, location=(0, 0, 0)) cube2 = bpy.context.object # Add a Boolean modifier to subtract the smaller cube from the larger cube mod_bool = cube1.modifiers.new(name=\"Boolean\", type='BOOLEAN') mod_bool.operation = 'DIFFERENCE' mod_bool.object = cube2 # Apply the Boolean modifier bpy.context.view_layer.objects.active = cube1 bpy.ops.object.modifier_apply(modifier=mod_bool.name) # Remove the smaller cube from the scene bpy.data.objects.remove(cube2) reply pavlov 9 hours agoprevThe \"Geo Nodes Meets Grease Pencil\" demo video is so cool. Makes me want to download Blender again just to play with this 2.5D effect. I don't think there's any other open source project that does release notes with such amazing quality as Blender. For that matter, very few commercial software companies reach this level of quality in their product websites. Note how the Blender page doesn't break scrolling, doesn't fluff up everything with useless adjectives like \"delightful\" and \"beautiful\", etc. reply hi_hi 11 hours agoprevFrom a beginners perspective, a good mouse is crucial for working in these kind of tools. It's touched on at the start of the donut tutorial (linked elsewhere) but I just wanted to re-iterate it here. Now, I haven't done serious 3d work for many years, and recently tried getting into Blender again on my Mac laptop with a Magic mouse. It was a horrible experience. Can anyone recommend a good mouse you can buy nowadays (so not something like the original SGI workstation mouse that had 3 buttons and no scroll wheel, unless those are still readily available...I loved them!) that works well in these programs. I remember the key differentiator being the scroll wheel which also acts as the middle button. Getting this wrong can make for a frustrating experience as it will zoom the view while trying to middle click. reply Kaijo 10 hours agoparentOne thing that at least doubled my modelling efficiency was my acquisition of a multi-button gaming mouse with a twelve-button thumb grid. In Blender I have that mapped mostly to the numpad which enables 3D navigation at the speed of thought, and without me having to move my other hand away from the cluster of most-often-used hotkeys at the bottom-left of the keyboard. You also get five functions out of the MMB which also can really speed up working in the Node Editor. But. I'm left-handed. The only suitable mouse I can find like this is a Razer Naga Left-Handed Edition, and I don't like the build quality. It's too light and small for my hand, and on the two specimens I've used, the MMB is a little bit glitchy (occasionally registering scrolling the wrong way, and click turns into push-left or push-right). So I wouldn't be without it, but I wish there were better options. reply adrian_b 5 hours agoparentprevI have continued to use 3-button Logitech mice many years after they were no longer manufactured, because, especially in EDA/CAD programs, I valued the ability of making gestures with the middle button as far more useful than a scroll wheel. Later, I have used various mice and also other alternative pointing devices, e.g. trackballs, track points and touch pads, but in the last few years I have settled on using a small Wacom Intuos tablet and stylus as the pointing device, instead of a mouse, with the tablet configured in its \"Relative\" mode, not in its default \"Absolute\" mode. In the \"Relative\" mode, the stylus behaves exactly like a mouse. In my opinion, a good stylus is much better than any mouse. It is much more comfortable, due to the natural position of the hand. It is much faster, because it is extremely light and it does not touch the tablet. A minute movement of the hand would move the cursor instantaneously from one corner to another. It is more accurate than any mouse, as you can easily draw or write freehand with it. Because the stylus is very light, I can keep it between the fingers when typing on the keyboard (while touch typing with all fingers), so unless I want to type a long text, when I drop the stylus on the tablet, the transitions of the right hand between keyboard and pointing device are much faster than with a mouse. The Wacom stylus has 3 buttons (one being the tip of the stylus). They can be programmed for any function. I prefer the tip to be left click, the next button as the right click and the last button as the double left click. The functions can be changed at any moment, so if you want within a program to have one button as middle click, you can do that instantaneously. There are a few buttons on the tablet (which has the same size as a mouse pad), so you could also put middle click or any other function there. reply gebar 9 hours agoparentprevI have found a simple 3-button-mouse (so LMB, RMB and MMB/clickable scroll wheel) to be absolutely sufficient for Blender. I tried the Space Navigator for a few months. But honestly, the navigation in Blender is so straightforward, I sold it again. Now I use this somewhat anatomical three button mouse from logitec: https://www.amazon.de/dp/B07W4DGC27?ref=ppx_yo2ov_dt_b_fed_a... What I wouldn't recommend is one of those Apple mouse types, where everything is touch. In general, touchpads do not do well with Blender. And the recommendation further below, getting a gaming mouse and mapping the numpad to it, is very smart! reply 1jss 6 hours agoparentprevI switched to an Evoluent vertical mouse for its ergonomics, but the feature I like the most is the middle click button! It has 3 buttons for left, middle and right click, a vertical scroll wheel and two navigation buttons for back and forward (that I never use). Find it really comfy to use in Blender and elsewhere. reply pcblues 8 hours agoparentprevI'm too old to remember the mappings for 20 button mice, but I've found the lowly Logitech G305 useful because it has a hardware button to toggle between DPI settings. That makes it useful without using any other key to change the speed of the mouse. Other than that, it has four buttons, and the scroll wheel clicks and goes left and right. reply diggan 8 hours agorootparentAnother mouse I'm currently using for 3D stuff is Logitech G502, which also has a DPI toggle switch but more importantly, it has a \"soft aim\" button which temporarily lower the sensitivity while you hold that button. Great for precision stuff in 2D/3D arts, although I think the original use case is for snipering in video games or something like that. Nothing beats using a Wacom tablet or similar though, even for boring 3D stuff. Not sure why, but I just end up with ridiculous precision compared to any mouse. reply pcblues 7 hours agorootparentI think it has to do with the precision of the hand wrapped around a pen/paintbrush with a lifetime of experience. The precision of effectively painting with an orange can not be matched to painting with a stick :) (Edit follows:) Also having the side of the hand on the canvas creates the ability to by so much more accurate. You have minute movements of the fingers rather than painting with your forearm. reply regularfry 6 hours agoparentprevI'm quite fond of Steelseries gaming mice for this sort of thing. You want the resolution. But also it's worth giving a graphics tablet a try and see if that works for your workflow. I find it much easier for sculpting. reply akerr 9 hours agoparentprevI really like the MX Anywhere 3S (with BetterMouse for macOS instead of Logi Options+ to configure gestures like middle click move to pan). It’s closer to a Magic Mouse in functionality/simplicity but more ergonomic and USB-C on the front. reply HelloNurse 9 hours agoparentprevThe key differentiator between mice is RSI. Try them for a few days and figure out whether they hurt or not. reply TOMDM 12 hours agoprevBlender continues to be an inspiration and aspiration for open source. Huge congrats and thank you to everyone who contributed. reply eblanshey 8 hours agoprevLike others have mentioned, Blender has become quite the successful open-source story. They used to be riddled with bugs and UX issues, much like FreeCAD was. Yesterday FreeCAD released v1 of their software, and they seem to be on the same redemption path as Blender. It's too bad their v1 release didn't gain much traction on here, as more people ought to give FreeCAD another whirl. The improvements there are massive. And it's the only proper parametric CAD software available on Linux. reply amelius 7 hours agoparentTo what extent can Blender replace FreeCAD for mechanical engineering purposes? reply cultofmetatron 7 hours agorootparentI would say avoid it. blender is an excellent MESH modeler but that puts it fundamentally at odds with being a good parametric modeler. a parametric modeler's base primitives are based in deformations on solid objects. mesh modelers are just vertices connected by line segments where 3 form a face. servicable if you're just doing simple objects for a 3d printer but disastrous if you need precision. reply amelius 7 hours agorootparentI don't understand why precision would be an issue? Is it not possible to fix the position of vertices to sub-micron precision? I know that Blender is used more in the movie industry. But what if I wanted to make, say, an animation of some cartoon character that gets shredded in a gearbox? What program would I use? reply rounce 4 hours agorootparentA curve in a parametric CAD program will have an internal representation which is perfectly smooth. As rather than being than a set of straight lines (edges) connected by vertices it is instead a mathematical description of a curve which has infinite resolution. For your animation example Blender would be the appropriate tool to use as you are doing stuff that requires flexibility of form rather than precision. reply amelius 4 hours agorootparentAha, so it is a bit like bitmap versus vector graphics in 2d painting programs. reply rounce 1 hour agorootparentYeah somewhat, there’s also the thing where mesh models can potentially have no thickness (eg. a single polygon) as well as gaps in the mesh whereas this is (usually) impossible in the case of a parametric model. reply regularfry 6 hours agorootparentprevRigging. The assembly bits in FreeCAD just haven't been great historically, and the Ondsel assembly layer is very new. If you want to visually check for clashes I can see how someone might prefer to just import a bunch of STLs into Blender, rig them up, and wiggle them about. reply rounce 7 hours agorootparentprevWhy? They’re fundamentally different applications for different purposes. reply amelius 6 hours agorootparentI was thinking that since Blender has physics simulation, and it also has nice video renderings, that would be two great reasons to use it for mechanical designs with moving parts, for example. But I don't have much experience in designing parts. I like SolveSpace, but it becomes slow for medium/large designs. I know FreeCAD has a lot of problems with stability and UI consistency, so I avoided it. reply rounce 5 hours agorootparentFreeCAD has more rigorous simulation features - FEM/FEA, mechanical assembly, CAM path generation and simulation, and robotics to name a few - out of the box which makes sense as it’s for engineering rather than art, and there are additional addons for CFD and sheet metal available among many others. The recent 1.0 update brought some major UI/UX improvements, though if you’re coming from other software you’ll find the Ribbon addon to be extremely helpful to feel comfortable. I think it gets a lot of over the top criticism given there are more people working on just the Autodesk CAD kernel than the entirety of FreeCAD and its dependencies. The rate of improvement is gradually accelerating and its already a big jump from where it was a few years ago. reply masteruvpuppetz 10 hours agoprevLove Blender's interface. I wish MS Excel was built like this. Dragging the border to create new viewports, customizable views, ease of binding keyboard shortcuts to functions.. FA-BU-LO-SO reply flohofwoe 10 hours agoparentYour statement is remarkable insofar as I can remember that Blender was famous for having an absolutely terrible UX (IIRC 2.8 was the version where it really started to improve). reply Kuraj 8 hours agorootparentThat is true. In my opinion the introduction of the command palette was the game changer, but of course it's more than just that. Today you can really say the UI is nice and discoverable. reply gebar 9 hours agorootparentprevToday it really has one of the best UI I have ever seen in any program. reply zerop 8 hours agoprev [–] Is it possible to generate code in LLM and create animation using Blender. I want to create Animation for educational video content. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Blender 4.3, released on November 19, 2024, brings significant updates, including EEVEE's Light and Shadow Linking, a new Metallic BSDF node, and Gabor Noise texture node.",
      "Enhancements include interactive multi-pass compositing, improved UV editing, and significant updates to Geometry Nodes and Grease Pencil for better performance and new tools.",
      "The release also features user interface improvements, Video Sequencer enhancements, Vulkan backend support, and remains free, supported by community donations."
    ],
    "commentSummary": [
      "Blender 4.3, particularly with the Bonsai add-on, is recommended for home remodeling due to its detailed planning capabilities.",
      "The software is praised for its open-source model, strong community support, and development fund, contributing to its success and continuous improvement.",
      "Users highlight Blender's versatility in art and 3D printing, and suggest tutorials like the donut tutorial for beginners to overcome its learning curve."
    ],
    "points": 232,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1732077552
  },
  {
    "id": 42187766,
    "title": "Open Riak – open, modern Riak fork",
    "originLink": "https://github.com/OpenRiak",
    "originBody": "The OpenRiak Project Promoting an open, modern Riak",
    "commentLink": "https://news.ycombinator.com/item?id=42187766",
    "commentBody": "Open Riak – open, modern Riak fork (github.com/openriak)203 points by amarsahinovic 22 hours agohidepastfavorite69 comments masleeds 20 hours agoRiak has been maintained through the post-basho years by engineers at some of its larger customers (disclaimer - including myself). The focus has been on trying to improve the stability of the database when subject to complex failure scenarios under stressful load, with minimal need for urgent operator intervention. The focus has been on keeping those existing operators happy rather than seeking out new users. Evolution of the product since basho has been slow but significant. The project now has support from Erlang Ecosystem Foundation, and we're looking to invest some effort over the next few months explaining what we've done, and to start to articulate what we see as the future for Riak. So if you're interested watch this space. It is expected to remain a niche product though. However, it may still find a home for those demanding specific non-functional requirements, with an acceptance of some functional constraints. reply pton_xd 18 hours agoparent> we're looking to invest some effort over the next few months explaining what we've done, and to start to articulate what we see as the future for Riak. So if you're interested watch this space. Where's that going to be posted? I'm not a Riak user but I am interested in hearing what others are doing in regards to improving failure scenarios in distributed systems. reply masleeds 18 hours agorootparentWhen we get stuff together there will be a link from our discussions page - https://github.com/orgs/OpenRiak/discussions. reply jamesblonde 20 hours agoparentprevMetastability is an under-rated system property for databases and systems software, in general. reply mst 6 hours agorootparentDid you mean 'not remaining in metastable failure' ? I was under the impression that once you learned how to pet the cat forwards (the non-triviality of getting to that point being an oft cited adoption barrier, but still), Riak was really quite excellent at recovering from things going wrong, which would fit with that being what you were trying to say. Or maybe my guess has gone completely wide, in which case please do break out the small words and crayon drawings and explain what you actually meant :) reply a2800276 9 hours agorootparentprevI'm not aware of any definition of metastability that would be a desirable property of a system. Vaguely speaking, in my understanding metastability in physics, chemistry and electronics refers to a state that has not yet fully settled and the final state is therefore undeterminable / random. Could you elaborate your understanding of the term? I can see it related to concepts like eventual consistency, just unclear of how it would be considered a positive characteristic. I'd have thought that a deterministic outcome would be important in a database system? reply remram 14 hours agorootparentprevWhat does metastability mean in this context? I've only seen it used to mean \"appears stable but not actually stable\", eg systems that resist small perturbations but never return to nominal after bigger disturbance (like cold boot). Did you mean \"stability\"? reply 7qW24A 14 hours agorootparentMarc Brooker’s blog on the topic is good: https://brooker.co.za/blog/2021/05/24/metastable.html reply jtuple 14 hours agoprevThis really hits home and makes me happy to see on the HN front page. Nearly 10 years later and I still consider my time working on Riak at Basho the highlight of my career. After leaving, my original plan was to found \"Basho 2.0\" after my non-compete expired. But, unexpected personal/family hardships in 2015-2018 made big-tech money the better choice for awhile, and Cloud/competitors continued to chip away at the market. Often stil regret not taking that path. But, happy to see technology I'm very fond of still living on and providing value to the world. reply _russelldb 5 hours agoparentI'm super happy to see YOU jtuple!!! I haven't seen or heard anything from you in years. I'm sorry to hear about personal/family hardships you suffered. I loved working for Basho in that era. I hope things are going well for you these days. No need to reply and get all mushy if you don't want to, I'm just really happy to see you posting and hear that you're out there. reply freerobby 20 hours agoprevI led a migration from Mongo to Riak at Shareaholic about 12 years ago: https://www.slideshare.net/slideshow/migrating-to-riak-at-sh... It was successful at first, but ultimately we traded one set of problems for another (how novel, I know). In particular, I underestimated the pain of troubleshooting the database itself. Riak was a new product, we were a small team that had never run anything on BEAM, and ultimately we lost too many days debugging and trying to make sense of Erlang stacktraces. The Basho folks were great, and to this day I appreciate how quickly they fixed a number of bugs for us. But ultimately it wasn't enough -- we found problems faster than they could be patched. reply tibbar 21 hours agoprevI've never met an engineering team that used Riak, but it is used heavily as an example technology in Kleppmann's 'Designing Data Intensive Applications'. (I would say, informally, it's usually the example of the \"other way\" as opposed to other more well-known databases.) This does make me wonder what became of it, why it didn't take off. reply macintux 21 hours agoparentSpeaking as a former tech evangelist/engineer at Basho, there were a few significant challenges. Riak is horribly unfriendly as a database: no SQL, it exposes eventual consistency directly to the developer, it’s relatively slow, and Erlang is a fairly unusual language. While you can run Riak on a single server, you’d have to really want to. Its strength is the ability to scale massively, but not many projects need that scale, and by the time you do, you’re probably already using some friendlier database and you’d rather make that one work. reply mrweasel 10 hours agorootparentI joined a company that had some investment in Basho and managed to sell Riak as the data store for a large client. It never really worked out, not enough of the SREs had be properly trained on Riak, the developers hated it because getting help and support was somewhat difficult, especially in an emergency. In the end more and more data was offloaded to MariaDB, until one day the last remaining data couldn't justify the cost of the Riak cluster. I think we swapped out an eight node Riak cluster for two largish MariaDB database (one being a hot-standby). For one of the other clients it was the exact same scenario, only we had been contracted in to help run the Riak cluster, which we didn't do well. Once they had migrate of it, to Oracle I think, the client left. To me it always felt like it was just the wrong tool for that particular job. Someone really wanted to be able to jump on the NoSQL hype and sell something. They picked Riak, because it honestly looked really good, and probably was, compared to MongoDB, CouchDB or whatever else happened to float around at the time. It just wasn't the right tool for the problems it was applied to. reply btilly 20 hours agorootparentprevBack in 2011 I was working on a project that involved Riak. The difficulty and slowness for doing stuff corresponding to basic SQL operations was certainly a giant strike against it, and helped sink that project before it was released. reply p_l 19 hours agorootparent> corresponding to basic SQL operations Ohhh, this brings memories of developers hitting the wall... Between different SQL databases! Back in 2016 I was delegated at work to do ops on a project that had big data ambitions in Threat Intelligence space. Part of how they intended to support that was Apache Phoenix, an SQL database backed by HBase, running on top of Hadoop that also provided object storage (annoyingly through WebHDFS gateway). Constant problems with hung Phoenix queries and instability of Hadoop in entirety led me to propose moving over to PostgreSQL, which generally went quite well... Except several cases of \"basic SQL operations\" that turned to have wildly different performance compared to Phoenix and most importantly, to MySQL in MyISAM mode, like doing SELECT (*) on huge tables. Fun times, got to meet a postgres core team member thanks to it. reply anotherjesse 20 hours agorootparentprevhttps://howfuckedismydatabase.com/nosql/ this infamous comic is about riak reply rubiquity 18 hours agorootparentThat could also very well be about CouchDB which implemented indexes/views as MapReduce functions. reply senderista 16 hours agorootparentBack in the day we had a CouchDB MapReduce view (on Cloudant) which took a full month to rebuild (while an angry customer was waiting). The I/O inefficiency was absolutely off the charts. reply binary132 21 hours agorootparentprevI wonder if some of these issues could be addressed sanely in an extension to the functionality reply macintux 18 hours agorootparentWe were working on ways of making it easier (such as CRDTs to reduce the amount of work developers had to do to leverage eventual consistency), but these were pretty challenging problems to solve. One of our biggest disappointments: we had plans to add a way to enforce strong consistency leveraging (IIRC) something akin to multi-paxos, but couldn't get it to work. reply jtuple 15 hours agorootparentTBH, we shipped fully working strong consistency in 2014. It just had a limited feature set, was disabled by default, and was never promoted/marketed since it didn't fit the direction the new CEO/CTO was pushing. The engineering exodus around that time sorta killed the project though, and we never were able to do the big follow-up work to make it really shine. (Disclaimer: Former Basho Principal Engineer, primary author of strong consistency work, lead riak_core dev from 2011-2015) I think another 18 months would have been enough too. But it just wasn't the right environment after the hostile take-over / leadership transition. reply NickM 14 hours agorootparentThere were customers happily using strong consistency in production, but somehow the idea that it wasn’t “finished” kept getting repeated over and over by management. I was well on my way to solving the biggest rough edge (tombstone reaping in SC buckets) but then I got pulled off to work on the infamous “data platform” and never got to finish that work :-( reply binary132 2 hours agorootparentprevIf you don’t mind my asking, how did you find working with Erlang in such a context? I’ve always been curious about it, but these days it seems like most people are mostly interested in Elixir for REST APIs and I am content with my existing tools for that purpose. However, there are clearly places where OTP and other Erlang functionality would shine and I’ve always thought it might be fun to keep in my back pocket. reply masleeds 10 hours agorootparentprevI'm not sure though for how much longer it will continue to make sense for the project as-is to continue to roll riak_ensemble forward as part of future releases. As there are no contributors who have direct direct experience or knowledge of using it in production, so it is hard to claim it as being a supported part of the product in any real sense. I apologise if we do eventually cut it. Having worked through the code when chasing unstable tests, I developed an appreciation for the quality of the work. reply macintux 15 hours agorootparentprevMy apologies for misremembering. I’m glad you chimed in to correct the record. reply binary132 16 hours agorootparentprevthat sounds like a painful session reply cmrdporcupine 20 hours agorootparentprevThing is, Cassandra became and remained popular, with similar aspects (though in JVM instead of Erlang, so). Though it had a couple years head start when there really no other options for people wanting that kind of kit. reply mst 6 hours agorootparentI remember somebody whose competence I rate highly talking about ending up picking Cassandra over Riak (at a point where Riak was technically decently mature) simply because of availability of ops expertise - basically his take seemed to be that Basho support was solid but they hadn't succeeded at cultivating people he could *hire* to handle as much as possible in house before calling in the gurus. (I can't, of course, speak to the truth of this, only that over a couple decades of knowing the dude in question and working with him on and off he had sufficient Clue that I expect he did put in the effort before coming to that conclusion) reply wbl 16 hours agorootparentprevI feel building a threat intelligence product on Cassandra is a bit on the nose. What's next, calling the TCB Palladium? reply 0xbadcafebee 20 hours agoparentprevI worked on a team that built a massive, high-performance internal service based on Riak. There are many things I learned from that system. Here is the best takeaway I can offer: It does not matter what your technology is, or how theoretically superior it is. Getting it to actually work well \"in production\" is a whole separate thing than simply designing it and writing code. When it's a very small system, it will look like it's doing great. As it gets bigger, the seams will start to burst, and you will find out that promises and theory don't always match reality. In the end, while its aims are great, it takes a whoooooole lot of work to smooth out the bumps in such a system. You need experts in that technology to address bugs in a timely manner. You need developers versed in the system to properly build apps utilizing it. You need competent operators to build, orchestrate, operate and maintain the whole thing. All of that is made easier by using simple technology that everybody knows, that there's a huge support community for, professional services for, etc. A technology like MySQL or Postgres etc, has the corporate, development, support, etc to make it easy to work with at any scale. A little janky at times, limited, but dependable, predictable, controllable. A small bespoke system with a small support community and virtually no corporate support is, comparatively, a hell of a lot more difficult/costly to support and harder to make work reliably. reply red_hare 20 hours agoparentprevMy old team used Riak in production for time series data in a real-time system. Our code was in Clojure, and we just wrapped the Java client. The conflict resolution was a steep learning curve, but overall, it was kind of nice (coming from Mongo). But man, Clojure stack traces wrapping Java stack traces wrapping Erlang stack traces in a Kafka consumer... I wish that hell on no one. reply mst 6 hours agorootparentThis feels like it needs a \"yo dawg, I heard you liked stack traces\" meme. Also bourbon. Probably *lots* of bourbon. reply rmetzler 10 hours agoparentprev> I've never met an engineering team that used Riak I was part of a recent cloud migration. Part of on-prem (though unfortunately not migrated by my team) were this very first Riak Cluster I saw in production. The engineering team used it as \"kind of S3\" for images, with 3 to 5 PHP scripts providing an interface to Riak and imageMagic. It seemed to me like a good abstraction and I think the migration to S3 was mostly painless. Other than that I only had contact with Riak at university around 15 years ago, when we tested cluster setups of several NoSQL databases and tried to manually introduce faults to see if they could heal. Riak passed our test at that time, MongoDB didn't. reply bojo 20 hours agoparentprevI'm pretty sure Stripe was a heavy user of this for a while. They used it due to their write-heavy system, if I recall. I fondly remember writing a Go driver for it. Was a good experience: https://github.com/riaken/riaken-core reply veyh 20 hours agoparentprevWe used Riak at $dayjob at around 2014-2017 (iirc). I don't exactly remember it fondly. It was slow and unreliable. You could make it freeze/crash with the wrong SOLR query. (I was pretty good at that...) reply masleeds 20 hours agorootparentThe SOLR part has now been retired from the last few releases. Current development has been focused on improving the flexibility of secondary indexes. There was some funky stuff achieved by some users using overloaded 2i terms and distributed processing of regular expressions against those terms - the aim is now to make this more flexible to the modern developer using the language of projected attributes and filter expressions (ala DynamoDB). There's also some active work to both replicate-to and full-sync (i.e. reconcile with) external OpenSearch clusters. The primary goal for OpenRiak is stability under load/failure as a K/V store - so the ultra-flexibility of in-built SOLR querying has been sacrificed in the move towards that aim. Anything that can do harm is to be offloaded or constrained. reply encoderer 21 hours agoparentprevInscrutable erlang stack traces definitely played a part. They were horrible. reply m00x 11 hours agoparentprevCompanies would rather use something like dynamodb than self-host riak. You get an army of Amazon code monkeys to help you if something goes wrong, and it's a click away. reply amarsahinovic 22 hours agoprevLightning Talk: Introducing OpenRiak - Nicholas Adams: https://www.youtube.com/watch?v=0GLBsBeM4Kc reply isoos 21 hours agoprevBasho team was very kind to open source contributions in ~2011-12: I've written an open source Riak client in Dart, and they had sent me t-shirts (the quality ones that are rare today). Nice treats for a fun project :) reply carterschonwald 22 hours agoprevCool! I never used it but really liked the engineers I met who worked at basho on risk. AFAIK, they basically had an engineering dream team until their last ceo had them go hard in certain directions that didn’t pan out. reply sitkack 20 hours agoparentThanks! reply chadd 22 hours agoprevI used Riak for a project back in 2012, the app that became the Whisper App, and as a huge Erlang fanboy, I was so excited about it. But it was incredibly unreliable at scale, and my colleague and I spent a week of sleepless nights under incredible personal and business pressure - as the servers got busier and busier - ripping it out. Still love vector clocks, though, and have fond memories of the Basho team presenting at Erlang Factory reply amanj41 21 hours agoparentVector clocks are very cool. Having read through how they were initially used in Riak, I was blown away that such an implementation could scale. I guess this is why Cassandra took a different approach? reply tibbar 21 hours agorootparentVector clocks are certainly cool but fundamentally premised on the idea of having multiple 'live' versions of a value at once. Amazon's original Dynamo paper required conflict resolution at the application level, which is a very strange framework to build applications on. (Notably DynamoDB has moved away from this, I believe to Last Write Wins.) Cassandra takes the latter approach by default as well, I believe. reply amanj41 20 hours agorootparentyes there's that idiosyncrasy, as well as client ideally needing to read the previous clock from the DB before writing an update for that key unless it's ok with the write being viewed as concurrent. Plus the extra memory overhead to store the clocks in the client. reply datadeft 8 hours agoprevFor a long time Riak was my favorite key value store. It went into production without any significant issue in several companies and was running without interruption despite the face of hardware issues. I think it stability was due to the fact of combining great technologies like LevelDB and Erlang. I wish it was a bit more popular. reply tptacek 19 hours agoprevAs someone who has used Riak in anger once in his career and who has a blossoming interest in FoundationDB I'd love someone to contrast the two systems. My knee-jerk reaction --- which I'm calling out as such! --- is that FDB has decreased the relevance of systems like Riak. reply nemothekid 2 hours agoparentI spent a lot of time with NoSQL systems in the early 2010s and I think what “killed” them was processors and networks getting so much faster that Postgres “just worked” for more and more use cases. There are systems I’ve built in the past with 20+ Cassandra nodes and tens of thousands of ops that were originally built on MySQL/Postgresql but migrated to Cassandra because the performance/cost of the SQL systems was just to high. Now those performance requirements can be handled cheaply with 1 or 2 beefy PostgreSQL databases. The level of scale you need today to make put up with something like Cassandra is much higher while yesteryear it felt like every startup was falling over once they found pmf reply masleeds 19 hours agoparentprevI would tend to agree, perhaps a decade ago it was easier to define the uniqueness of Riak, and now there are alternatives that offer similar guarantees. So the relevance of Riak is not as obvious. Also as we focus on stability on OpenRiak going forward, that means reducing some of the capability that may have made Riak stand-out in the scale-out space. The preference going forward is to do fewer things, but do those things predictably well. There will be differences between Riak and FoundationDB, and I hope those differences are sufficient to make Riak interesting, and allow it to continue to occupy a small niche in the world of databases. reply amerine 15 hours agoprevI still use my RICON pint glass and wear my RICON jacket Basho gave everyone at RICON almost every week. My favorite conf swag ever reply vosper 21 hours agoprevWho would this be for in 2024? I remember evaluating Riak back in 2011 or so for an analytics solution, but ended up going with a more traditional OLAP database that was a much better option. It's hard for me to imagine where Riak would be a good option given how many choices we have today for various data stores. reply EwanToo 20 hours agoparentIt's realistically for the handful (dozens at most?) of very large Riak implementations where it would be enormously expensive to rewrite the application running on top of it. For example, the UK NHS Spine messaging system which has been building on Riak for 10 years https://riak.com/posts/press/nhs-launches-upgraded-it-backbo... reply nicholas-adams 15 hours agoparentprevI think that in the time since 2011, things have changed more than a bit. As my employer provides Enterprise Grade Riak Support (and, of course, OpenRiak support), I'm under NDA and cannot really share names. However, I can share that that there are quite a few places that use Riak. Here are a few off the top of my head: - the biggest online betting company in the world - one of Japan's largest e-commerce sites - a large Hungarian bank - one of China's largest electronic manufacturers - arguably Asia's largest or second largest messaging platform - a significant Indian online-documentation provider - one of the largest US insurance providers - an Australian app analytics provider - a European telephone services provider - one of the world's largest travel sites - an Asian-based credit-card fraud detection service - a number of start ups in various industries - me - I do my crypto taxes using a 5 node Riak cluster running on Raspberry Pi's In the Basho era (up to early 2017), Riak may have only been targetted to larger players but now, when it comes to areas such as in-house data sovereignty, compliance (e.g. GDPR), the flexibility, speed and reliability Riak now provides plus being free to run, people from individuals to corporates are starting to wake up and see the advantages. (edited in an attempt to improve list formatting) reply brcmthrowaway 1 hour agorootparentCrypto taxes? You do automated crypto trading? reply sitkack 20 hours agoparentprevIt is a fault tolerant massively scalable key value store capable of handling hundreds of terabytes of data. What are these options you are thinking of? The only thing that comes to my mind is Aerospike and possibly ScyllaDB. reply tptacek 16 hours agorootparentFoundationDB seems like the obvious example? But they're not strictly comparable in anything but scale, right? FDB is ACID. reply senderista 16 hours agorootparentprevIf cloud is an option, DynamoDB? reply felixgallo 21 hours agoparentprevRiak isn’t remotely like OLAP. What was your use case? reply ramon156 21 hours agorootparentThink they meant OLTP reply p_l 14 hours agoprevA question possibly answered elsewhere, but did openriak include only Risk KV, or also other projects like CS & TS? reply masleeds 6 hours agoparentThere are forks being maintained of Riak CS and Riak TS by TI Tokyo - https://github.com/ti-tokyo. The focus of the OpenRiak community for the moment is on Riak KV only. reply p_l 4 hours agorootparentThank you. I am tempted to try playing with the entire stack, used to pine for it back in 2011 or so :) reply KameltoeLLM 5 hours agoprev\"mOdERn\" (stupidest buzzword ever; for me: modern == stupid NIH rewrite) reply binary132 21 hours agoprevIt’s actually kinda silly how exciting this is to me reply remram 14 hours agoprevWhat's a Riak and is there really no better link for this news? reply sriram_malhar 14 hours agoparenthttps://en.wikipedia.org/wiki/Riak reply indulona 7 hours agoprev [–] it's always great idea to build your company on software written in some obscure niche language... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "OpenRiak is a modern fork of the Riak database, maintained by engineers from its major customers, focusing on enhancing stability in complex failure scenarios.",
      "Supported by the Erlang Ecosystem Foundation, OpenRiak remains a niche product but may attract users with specific non-functional requirements.",
      "The OpenRiak community is currently concentrating on Riak KV, with other forks being maintained by TI Tokyo, and plans to share future developments soon."
    ],
    "points": 203,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1732048170
  },
  {
    "id": 42190395,
    "title": "Webvm: Virtual Machine for the Web",
    "originLink": "https://github.com/leaningtech/webvm",
    "originBody": "WebVM This repository hosts the source code for https://webvm.io, a Linux virtual machine that runs in your browser. Try out the new Alpine / Xorg / i3 graphical environment: https://webvm.io/alpine.html WebVM is a server-less virtual environment running fully client-side in HTML5/WebAssembly. It's designed to be Linux ABI-compatible. It runs an unmodified Debian distribution including many native development toolchains. WebVM is powered by the CheerpX virtualization engine, and enables safe, sandboxed client-side execution of x86 binaries on any browser. CheerpX includes an x86-to-WebAssembly JIT compiler, a virtual block-based file system, and a Linux syscall emulator. Enable networking Modern browsers do not provide APIs to directly use TCP or UDP. WebVM provides networking support by integrating with Tailscale, a VPN network that supports WebSockets as a transport layer. Open the \"Networking\" panel from the side-bar Click \"Connect to Tailscale\" from the panel Log in to Tailscale (create an account if you don't have one) Click \"Connect\" when prompted by Tailscale If you are unfamiliar with Tailscale or would like additional information see WebVM and Tailscale. Fork, deploy, customize Fork the repository. Enable Github pages in settings. Click on Settings. Go to the Pages section. Select Github Actions as the source. If you are using a custom domain, ensure Enforce HTTPS is enabled. Run the workflow. Click on Actions. Accept the prompt. This is required only once to enable Actions for your fork. Click on the workflow named Deploy. Click Run workflow and then once more Run workflow in the menu. After a few seconds a new Deploy workflow will start, click on it to see details. After the workflow completes, which takes a few minutes, it will show the URL below the deploy_to_github_pages job. You can now customize dockerfiles/debian_mini to suit your needs, or make a new Dockerfile from scratch. Use the Path to Dockerfile workflow parameter to select it. Local deployment From a local git clone Download the debian_mini Ext2 image from https://github.com/leaningtech/webvm/releases/ You can also build your own by selecting the \"Upload GitHub release\" workflow option Place the image in the repository root folder Edit config_github_terminal.js Uncomment the default values for CMD, ARGS, ENV and CWD Replace IMAGE_URL with the URL (absolute or relative) for the Ext2 image. For example \"/debian_mini_20230519_5022088024.ext2\" Build WebVM using npm, output will be placed in the build directory npm install npm run build Start NGINX, it automatically points to the build directory just created nginx -p . -c nginx.conf Visit http://127.0.0.1:8081 and enjoy your local WebVM Example customization: Python3 REPL The Deploy workflow takes into account the CMD specified in the Dockerfile. To build a REPL you can simply apply this patch and deploy. diff --git a/dockerfiles/debian_mini b/dockerfiles/debian_mini index 2878332..1f3103a 100644 --- a/dockerfiles/debian_mini +++ b/dockerfiles/debian_mini @@ -15,4 +15,4 @@ WORKDIR /home/user/ # We set env, as this gets extracted by Webvm. This is optional. ENV HOME=\"/home/user\" TERM=\"xterm\" USER=\"user\" SHELL=\"/bin/bash\" EDITOR=\"vim\" LANG=\"en_US.UTF-8\" LC_ALL=\"C\" RUN echo 'root:password'chpasswd -CMD [ \"/bin/bash\" ] +CMD [ \"/usr/bin/python3\" ] Bugs and Issues Please use Issues to report any bug. Or come to say hello / share your feedback on Discord. More links WebVM: server-less x86 virtual machines in the browser WebVM: Linux Virtualization in WebAssembly with Full Networking via Tailscale Mini.WebVM: Your own Linux box from Dockerfile, virtualized in the browser via WebAssembly Reference GitHub Pages deployment: Mini.WebVM Crafting the Impossible: X86 Virtualization in the Browser with WebAssembly Talk at JsNation 2022 Thanks to... This project depends on: CheerpX, made by Leaning Technologies for x86 virtualization and Linux emulation xterm.js, https://xtermjs.org/, for providing the Web-based terminal emulator Tailscale, for the networking component lwIP, for the TCP/IP stack, compiled for the Web via Cheerp Versioning WebVM depends on the CheerpX x86-to-WebAssembly virtualization technology, which is included in the project via NPM. The NPM package is updated on every release. Every build is immutable, if a specific version works well for you today, it will keep working forever. License WebVM is released under the Apache License, Version 2.0. You are welcome to use, modify, and redistribute the contents of this repository. The public CheerpX deployment is provided as-is and is free to use for technological exploration, testing and use by individuals. Any other use by organizations, including non-profit, academia and the public sector, requires a license. Downloading a CheerpX build for the purpose of hosting it elsewhere is not permitted without a commercial license. Read more about CheerpX licensing If you want to build a product on top of CheerpX/WebVM, please get in touch: sales@leaningtech.com",
    "commentLink": "https://news.ycombinator.com/item?id=42190395",
    "commentBody": "Webvm: Virtual Machine for the Web (github.com/leaningtech)190 points by thunderbong 16 hours agohidepastfavorite50 comments apignotti 12 hours agoHello again HN. Lead dev of WebVM and CTO of Leaning Technologies here. Happy to answer any question from the community. reply doctorpangloss 12 hours agoparentI've heard about CheerpJ and your Flash solution too. You guys pump out a lot of pretty sophisticated web middlewares. Supposing people were fully aware of your technology, which application do you think would have the highest yield to be ported to your portfolio of middlewares? reply apignotti 11 hours agorootparentWe envision virtualization of corporate internal apps to be of the most interesting markets in the short-medium term. CheerpJ has been very successful in this segment for quite some time already. CheerpX can be seen as an alternative to Citrix, at least for some use cases. In the longer term we plan to get unmodified Docker containers to work with CheerpX, including exposing REST APIs to the local Web app. We have also internally speculated about a marketplace-like system to allow immediate conversion of traditional client-side apps to Web apps. This would be intended for the long tail of client software vendors that have not yet adopted Web-first distribution methods. reply NikhilVerma 8 hours agorootparentBeing able to run docker containers on the web will immediately unlock so many usecases for my work! Right now I've been trying to get a Python sentence parser to run in the browser but it requires a lot of the ecosystem (Pytorch and such). Which is not trivial to compile to WASM. reply apignotti 8 hours agorootparentThat would fit our vision for a new generation of Web apps with traditional server-side payloads running client-side, with lots of positive impacts on user privacy and operational costs. The main difficulties right now are two: * Most docker containers are 64-bit, while CheerpX currently only support 32-bit x86 code * Due to CORS limitations it is not currently possible to downloaded layers from repositories such as Docker Hub The first limitation will be eventually fixed, the second one will require a specialized repository, a proxy, or co-operation from the existing repositories. reply JoshTriplett 10 hours agoparentprevThis is really impressive! You mention having a \"virtual block-based file system\". How easily can people integrate other devices into the VM, including custom devices? For instance, if someone had a different network filesystem they wanted to use, which could be tunneled over WebSocket or WebTransport, how easily could they integrate that into this? What's the equivalent of virtio here? It looks like the client-side bits are largely proprietary? Is CheerpX the primary thing you consider to be your competitive advantage, or is it more about all the layers you're putting on top of that for things like Flash and Java and Oracle Forms and converting local apps to web apps? It'd be nice to be able to build atop the underlying VM and extend it for all sorts of purposes, the way KVM has been open enough to become a focal point for the modern cloud, and have just the enterprise-y bits and \"convert your local app to a web app\" framework running on top of that being proprietary. reply apignotti 10 hours agorootparentWe don't currently expose an API to integrate custom block devices, although it could be possible. We provide an integrated backend based on HTTP byte ranges that can work on any standard compliant HTTP server. See here for more info: https://cheerpx.io/docs/guides/File-System-support#block-dev... CheerpX is indeed proprietary, but free-to-use for individuals and open source projects. We do want to see the community building on top of our tech. Our Flash product is build on top of CheerpX and the official Flash player plugin that needs to be licensed separately from Harman/Adobe. Java support is provided by a completely different product called CheerpJ, although lots of the ideas and parts of the JIT engines are shared with CheerpX. At this stage we believe that keeping CheerpX proprietary serves our growth plans the best, this could of course change over time as adoption increases and we build further added-value products on top. On the other hand the WebVM integration is FOSS since it doubles as an extensive sample of what can you build with our technology. reply JoshTriplett 10 hours agorootparent> We don't currently expose an API to integrate custom block devices, although it could be possible. I was thinking more about arbitrary custom devices, like custom network drivers (e.g. to connect to a server-side virtual network rather than tailscale) or custom filesystem drivers at either the block or FS layer. > At this stage we believe that keeping CheerpX proprietary serves our growth plans the best, this could of course change over time as adoption increases and we build further added-value products on top. I'd be very interested in chatting with you about our respective products' future plans. reply apignotti 9 hours agorootparent> I'd be very interested in chatting with you about our respective products' future plans. Sure, you can find me and the rest of the team on Discord: https://discord.leaningtech.com reply Qem 7 hours agoparentprevCan it be installed and run offline as a WebApp? reply apignotti 6 hours agorootparentNot right now, WebVM/CheerpX support large disks (1GB+) to be able to run complete distributions. This an advantage of the solution, but it also means that it cannot run completely offline unless we provide a way to pre-download all this data. This does not seem a practical solution. Fundamentally, a real VM running a real OS needs lots of data. reply James_K 15 hours agoprevSad to see Linux turning into a web app. I thought they'd be smart enough to avoid this sort of trend. Hope they continue to support the desktop version. reply 8n4vidtmkvmk 14 hours agoparentUnless Linus himself is behind this project, I don't think you have anything to worry about. reply johnisgood 10 hours agorootparentWho is going to take Linus' place were something happen to him (hopefully not!), or is it going to be chaos? reply noobface 13 hours agoparentprevPoe's law in full effect. reply athrowaway3z 14 hours agoparentprevI'm waiting for the IPhone app. reply mmh0000 14 hours agorootparentMay your wish be granted: https://apps.apple.com/us/app/ish-shell/id1436902243 reply balder1991 12 hours agorootparentThis is great in comparison to nothing at all, but honestly it’s quite slow since it emulates a different architecture. reply odo1242 11 hours agorootparentIf you’re interested, a-Shell hits a different part of the tradeoff curve where it can run at native speeds and has most of the important built in utilities like ssh, but it’s hard to install additional native software because of iOS restrictions (though python stuff are ok) reply sharperguy 9 hours agoparentprevLinux was just an electron app this whole time anyway. reply kristopolous 12 hours agoparentprevAnd a SPA no less. Pity reply baudaux 11 hours agoparentprevIn https://exaequos.com, I have designed a new Unix-like OS with kind of microkernel architecture reply rahimnathwani 15 hours agoprevPrevious discussion with 632 points: https://news.ycombinator.com/item?id=40940225 reply abixb 9 hours agoprevStudent here, and this has to be the most satisfying 20 minutes I spent cloning a GitHub repo in a while. Here are a few things I encountered which might probably help others: # Version mismatch: The install version of node.js I had (v12.22.9) was too old to meet the project's requirements (>= 18.13). To fix that, I used nvm to install the right Node.js version (>18.0) # Version check before install: Used the following to check version to verify before proceeding ahead with building it: node -v npm -v It ran successfully, and I've been playing around with built-in Python3 and C scripts and pushed the modified repo to my personal GitHub. Thank you, dev! reply apignotti 8 hours agoparentThank you, appreciated. reply smusamashah 9 hours agoprevSome other VMs that run in browser https://copy.sh/v86/ https://bellard.org/jslinux/ https://jamesfriend.com.au/pce-js/ (https://github.com/jsdf/pce) https://www.pcjs.org/ (lots of hardware and OSes) (https://github.com/jeffpar/pcjs) https://infinitemac.org/ (https://blog.persistent.info/2023/03/infinitemac-dot-org.htm...) https://jamesfriend.com.au/projects/basiliskii/BasiliskII-wo... https://jamesfriend.com.au/pce-js/pce-js-apps/ reply maxloh 9 hours agoparentAlso: https://ktock.github.io/container2wasm-demo/ reply zb3 6 hours agorootparentHoly cow, this is the first time I see x86_64 and AArch64 in the browser! reply benchmarkist 15 hours agoprevNetworking over websockets with tailscale is a nice touch. reply jeswin 12 hours agoprevVery cool project, and great for many usecases. In absolute performance, I find it about 16x slower than CPU [1], but it'd still be super fast compared to the full Linux GUIs we were using comfortably in the 90s/2000s on slower CPUs. Might work for sandboxing user code as well. [1]: My utterly naive benchmark completes in 9s - time seq 1 50000000wc -l reply punnerud 11 hours agoprevIt also supports Tailscale (or the other way around). If you use SSH from Tailscale Admin it’s actually WebVM you see: https://labs.leaningtech.com/blog/webvm-virtual-machine-with... reply apignotti 11 hours agoparentCheerpX uses Tailscale, not the other way around. The Tailscale SSH client is built on the same Go stack (compiled to Wasm) used by CheerpX uses. reply DeathArrow 11 hours agoprevCool, starting with this we might see Linux being implemented in Javascript. /joking reply tertle950 12 hours agoprevRemember when the web was just for displaying documents? Yeah, me neither reply takemetoearth 11 hours agoparentDoesn't sound very monetizable. reply sudo_bang_bang 14 hours agoprevCan I run WebVM inside WebVM? reply TuringTest 1 hour agoparentIt's penguins all the way down reply DeathArrow 11 hours agoprevCool, starting with this we might see Linux being implemented in Javascript. reply zb3 14 hours agoprevNote this repository doesn't include source for CheerpX, which is the main component of WebVM. reply maxloh 13 hours agoparentYeah, the project is merely an open-source wrapper to promote the underlying proprietary library. reply Uptrenda 14 hours agoprevI think its cool but am I the only one that doesn't really think the web is a good platform for this? Like, VM stuff needs to be as close to bare metal as possible to be fast. There's special processor features that make virtualisation work well. It's amazingly efficient now. You can vastly over-allocate your hardware between VMs with good modern virtualisation software because they're so damn efficient that they only utilise CPU and memory when they need it. That means they're not just like allocated 4 GBs if you allocate 4 GB to the VM. All these little optimisations... just aren't going to work well in a browser. I mean, I can already see that alpine (an extremely lightweight VM that I've had boot instantly with vagrant and other such stuff) is slow here. It's a cool hack. I just prefer things to be practical... reply solarkraft 13 hours agoparentThere are better approaches to virtualization when that’s your only goal, but this is on the web, which has value in itself. reply takemetoearth 11 hours agoparentprevNo, a lot of people think this. You just won't find most of them here. reply z3t4 11 hours agoprevIs there a server if you want to run your own WebSockets as a transport layer? reply apignotti 10 hours agoparentYou can use the self-hosted version of the Tailscale control plane, called Headscale, but you need our own fork since the upstream version has various issues. We plan to merge these fixes upstream soon. For more info, see the last few paragraph of the Networking section from our latest blog post: https://labs.leaningtech.com/blog/webvm-20#private-networkin... reply westurner 15 hours agoprev/? \"webvm\" jupyter: https://www.google.com/search?q=%22webvm%22+jupyter - \"Is WebVM a potential solution to \"JupyterLite doesn't have a bash/zsh shell\"?\" https://news.ycombinator.com/item?id=30167403#30168491 : - [ ] \"ENH: Terminal and Shell: BusyBox, bash/zsh, git; WebVM,\" https://github.com/jupyterlite/jupyterlite/issues/949 And then now WASM containers in the browser FWIU reply lofaszvanitt 13 hours agoprevWho needs this in the browser and for what purpose? reply odo1242 11 hours agoparentThis company makes a tool to performantly run Linux code in the web browser and uses it to emulate Flash / Java, among other things. This is a test demo of the tool. reply bouncycastle 10 hours agoprevImagine the performance boost if you removed the browser reply colesantiago 15 hours agoprev [–] duplicate: https://news.ycombinator.com/item?id=40940225 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WebVM is a Linux virtual machine that operates within a web browser using HTML5 and WebAssembly, utilizing the CheerpX virtualization engine to support x86 binaries.",
      "It offers a server-less, client-side environment with networking capabilities enabled through Tailscale, a VPN network, making it accessible without traditional server infrastructure.",
      "WebVM is open-source under the Apache License 2.0, with specific restrictions for organizational use, and can be deployed by forking the repository, enabling GitHub Pages, and customizing Dockerfiles."
    ],
    "commentSummary": [
      "WebVM, developed by Leaning Technologies, is a virtual machine designed for web use, with potential applications in virtualizing corporate apps and plans to support Docker containers.",
      "CheerpX, a proprietary component, is crucial for WebVM's offerings, providing support for Flash and Java, although WebVM itself is open-source.",
      "Despite its inability to run offline due to large data needs, WebVM supports Tailscale for networking, and there is user interest in the feasibility and performance of web-based virtual machines."
    ],
    "points": 190,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1732070723
  },
  {
    "id": 42193549,
    "title": "Bluesky is ushering in a pick-your-own algorithm era of social media",
    "originLink": "https://www.newscientist.com/article/2456782-bluesky-is-ushering-in-a-pick-your-own-algorithm-era-of-social-media/",
    "originBody": "Comment and Technology Bluesky is ushering in a pick-your-own algorithm era of social media More than 20 million people have joined Bluesky, a social network that gives you fine-grained control over what you see and who you interact with. I think it is the future of social media, says Chris Stokel-Walker By Chris Stokel-Walker 19 November 2024 Bluesky sign-ups continue to grow Anna Barclay/Getty Images As a technology reporter, I like to think I’m an early adopter. I first signed up to the social network Bluesky around 18 months ago, when the platform saw a small surge in users disaffected by Elon Musk’s approach to what was then still called Twitter. It didn’t stick. Like many, I found the lure of Twitter too strong, and let my Bluesky account wither, but in recent weeks I have returned – and I am not alone. With Musk continuing to transform his social platform, now called X, at the same time as taking a role in US president-elect Donald Trump’s upcoming government, the Xodus has begun. Bluesky has gained 12 million users in two months, and has just surpassed 20 million users. This time I intend on sticking around – and I think others will, too. Read more How does ChatGPT work and do AI-powered chatbots “think” like us? Advertisement In large part, that’s because I want a social media experience without being bombarded by hate speech, gore and pornographic videos – all of which users of X have complained about in recent months. But I’m also big on Bluesky because I think it signals a shift in how social media works on a more fundamental level. Social media algorithms – the computer code that decides what each user is shown – have long been a point of contentious debate. Fears of disappearing down “rabbit holes” of radicalisation, or being trapped in “echo chambers” of consensual, sometimes conspiratorial, viewpoints, have dominated scientific literature. The use of algorithms to filter information has become the norm because chronologically presenting information from followers creates a confusing morass for the average user to process. Sorting and filtering what is important – or likely to keep users engaged – has become key to the success of platforms like Facebook, X and Instagram. Sign up to our The Daily newsletter The latest science news delivered to your inbox, every day. Sign up to newsletter But control of these algorithms also gives you a big say in what people read. One of the bugbears many users have with X is its “For you” algorithm, which under Musk has seen commentary by and about him seemingly shoved into users’ timelines, even if they don’t directly follow him. Bluesky’s approach isn’t to ditch algorithms – instead, it has more than the average social network. In a 2023 blog post, Jay Graber, Bluesky’s CEO, outlined the ethos of the platform. Bluesky promotes a “marketplace of algorithms”, she wrote, instead of a single “master algorithm”. Read more Mind-reading AI recreates what you're looking at with amazing accuracy In practice, this means that users can see posts by people they follow on the app, the standard view Bluesky defaults to. But they can equally opt to see what’s popular with friends, an algorithmically-dictated selection of posts that your peers enjoy. There are feeds specifically for scientists, curated by those working in the field, or ones to promote Black voices, which are often thinned out by algorithmic filtering. One feed even specifically promotes “quiet posters” – users who post infrequently, and whose views would otherwise be drowned out by those who share every opinion with their followers. This menu of options allows Bluesky to serve two purposes, bridging the past era of social media and the future one. The platform has the potential, once it reaches a critical mass of users, to act as the “de facto public town square”, as Musk dubbed Twitter before he purchased it. Bluesky arguably is the only remaining such square, given X has shifted to exclude many mainstream voices, and competitors like Threads choose to shy away from promoting politics and current affairs. But Bluesky also allows you to tailor the app to your needs – not only through feeds, but other elements like starter packs of recommended users to quickly get involved in individual niches, or blocking tools to quieten unruly voices. There are still hitches, undoubtedly. Finding the right feed for you can be tricky, while creating your own is even more complicated, requiring third-party tools. But the ability to get the full view of public conversation, then to drill down into smaller debates within clusters and communities of that broad swathe of society, is exciting. It’s a model of a new social media where users, not big companies or enigmatic individuals, are in charge of what they see. And if Bluesky continues to add users, it could become the norm. So come and join me – I’m @stokel.bsky.social. Chris Stokel-Walker is a freelance technology journalist Topics: Elon Musk / Social Media Advertisement",
    "commentLink": "https://news.ycombinator.com/item?id=42193549",
    "commentBody": "Bluesky is ushering in a pick-your-own algorithm era of social media (newscientist.com)175 points by amichail 6 hours agohidepastfavorite185 comments grishka 2 hours ago> The use of algorithms to filter information has become the norm because chronologically presenting information from followers creates a confusing morass for the average user to process. Can't disagree more. Call me old-fashioned but I hate any algorithms at all meddling with what I see. If I follow someone, I want to see their posts, all of them, without exceptions. If I don't follow someone, I only want to see their posts if they were knowingly reposted by someone who I do follow. If I want some posts filtered from my feed, I'll set up word filters myself, thank you very much. It's a recurring theme in the modern IT industry that \"the average user\" can't be trusted to take their own responsibility. It's sometimes taken as an indisputable truth, even. Why does this keep happening? What can I do to put an end to this? reply aeturnum 2 hours agoparentIt's true, of course, that the \"chronological timeline\" is an obvious and straightforward default, but I think you are being unfair to the position you are critiquing. Many (90%+ I would say but the exact proportion doesn't matter for this) people do not have the time to process every social media post from every person they are connected to. They are only going to see N \"posts\" (videos, texts, questions, etc) per time unit (day / week / bathroom break). It is 100% genuinely and obviously worse to, if someone only sees...3 posts on your social network for those posts to be [someone complaining about commute, breakfast photo, angry election post] as opposed to [wedding announcement, request for a resource the user has, a close friend sharing something exciting that the user hasn't seen]. Telling users that you are showing them less interesting stuff because \"they happened in chronological order\" is a bad answer. Of course social media companies do a bad job at this! They push high-conflict high-engagement content into our feeds because it makes them more money. But I think the problem of \"there is a lot going on and you would like a machine to help you prioritize how to process things\" is genuinely one of the pressing problems of our age and I get so frustrated when people downplay it. There is more stuff happening in my social world than I have time to fully process - that's just true. I am not interesting in living such a small life that I have time to fully engage with every single happening - I would like a machine to help me. reply kristofferg 23 minutes agorootparentYou know people are lost in the woods when the they use terms like “100% genuine and obvious. Your personal preferences are not universal and people are not downplaying it the need for controlling feeds. They are frustrated that control of feeds are taken from them from paternalistic profit-driven product managers et al. reply HumblyTossed 1 hour agorootparentprev> Many (90%+ I would say but the exact proportion doesn't matter for this) people do not have the time to process every social media post from every person they are connected to. Correct, but really don't want to. I want to open the app and get the pulse of what is happening in that moment. Not 8 hours ago. Not 4 weeks ago. Right now. reply SoftTalker 2 hours agorootparentprev> Many ... people do not have the time to process every social media post from every person they are connected to. Then they are following too many people. Decades ago, a professor at school quipped \"if you can't keep up with your news feed using 'more' to read the spool then you follow too many newsgroups\" reply aeturnum 1 hour agorootparentI wholly and completely disagree with this and think it's an unethical belief to hold. If you are under the impression that you are perfectly up to date with every detail of every person in your life you are either deeply misguided or dismissive of the inner lives of folks around you. reply amonith 26 minutes agorootparentIsn't the point of the comment above to not even want to be up to date with every detail of people that are objectively not that important in your personal life? Not to decrease the social media usage because you feel you're up to date but to do it because it's unnatural and pointless? reply tifik 2 hours agorootparentprev> I would like a machine to help me. Cool. I would not. It would be nice to have that option. reply aeturnum 1 hour agorootparentYah, 100% - I agree that the chronological timeline should be a default feed alogirthm on every service. reply horsawlarway 2 hours agorootparentprevI genuinely don't understand the desire to engage with so many folks on such a superficial level. Like - if the only way you're going to know about someone's wedding is from a social feed... you aren't friends with that person, you're just acquaintances. Most folks do a great job at informing you of the things that are relevant in your relationship with that person when you... talk to them. > I am not interesting in living such a small life that I have time to fully engage with every single happening - I would like a machine to help me. You think that life is small... but I think yours is utterly dehumanizing. You aren't interested in engaging with individuals, you seem to just want their life's highlights thrown at you repetitively until you've burned yourself out on them, like slamming the oxytocin button for your brain without actually doing anything nearly so drab as actually talking to someone. --- I think my reaction to your comment is driven by your idea that friendships and human interactions are formed over big events (like weddings or exciting happenings). I'd argue fairly strongly that they're driven instead by precisely the small, boring, daily things you're not at all interested in: Commutes. Meals. Emotional responses to small things (politics or not). I find it distasteful to think you're friends with someone when you only give a shit about the big exciting news they have to share. That's not friendship, it's a weird twisted form of paparazzi/voyeurism. You don't want to know them, you just want their life's highlights presented to you... --- Emotional response aside - Hard disagree on > It is 100% genuinely and obviously worse to, if someone only sees...3 posts on your social network for those posts to be [someone complaining about commute, breakfast photo, angry election post] as opposed to [wedding announcement, request for a resource the user has, a close friend sharing something exciting that the user hasn't seen]. Telling users that you are showing them less interesting stuff because \"they happened in chronological order\" is a bad answer. reply aeturnum 1 hour agorootparent> I genuinely don't understand the desire to engage with so many folks on such a superficial level. That's fine! I am not asking you to understand that desire. I'm asking you to understand it's a genuinely held desire that people actually want. We can (and will) have different preferences and live in the same society. That's a fine thing. You have a totally fine and healthy preference for how you manage your own social life, but you are mistaking that preference for a universal standard about how everyone should best manage their social lives. That is the thing I am critiquing. You are allowed to do what you want and I support you! But so often people describe the fact that their preferences are not \"the standard\" and imply that the balance would be better for everyone - without considering that different people want different things. Edit: We could also have a discussion about \"what is the ideal social model for society\" - but that is a different conversation with different claims than the one we are having now. If you are trying to talk about how you think our current society sucks by attacking my points about the benefits of how social media algorithms interact with us - I think you are coming at me in a confused way. Even if a version of life where we all had smaller social circles and all had less information coming at us was healthier (totally possible!) - that's not the world I find myself living in. I would like tools to help me live in the world I find myself in and I find it distressing that so many fellow tech workers think that's immoral somehow. P.s. I think you're being quite rude to me and I don't appreciate it. reply ziddoap 2 hours agorootparentprev>You think that life is small... but I think yours is utterly dehumanizing. The person posted (barely) 3 paragraphs. Like, less than 10 sentences. Seems pretty hasty to label their life \"utterly dehumanizing\" from that. Your whole next paragraph is drawing a lot of (frankly, quite rude) conclusions based on nothing. You've read so much into their short comment that you've created an entire fictional person, and then got angry at the fictional person you created. Looking at their comment and your reply, I would say they have a healthier approach to socializing on the internet than you appear to. reply horsawlarway 1 hour agorootparentEh - anger isn't the same as disgust or confusion. And it's not really pointed at the above poster explicitly, it's pointed at the culture that results from the attitude that human interactions should be prioritized on the scale of \"entertain me\" by a digital algorithm, and that that's a good thing. And while you might wish it's fan fiction... it's the very real reason we see things like nation-wide social media bans by age. Calls to reduce or reform social media in general. And a huge number of negative social outcomes since the advent of that style of social media. It's really, really hard to argue that form of media consumption is healthy. Or appropriate. reply ziddoap 55 minutes agorootparent>Eh - anger isn't the same as disgust or confusion. Okay, you're disgusted or confused at the fictional person you created. >And it's not really pointed at the above poster explicitly, It certainly seems like it is very explicitly pointed at the poster you replied to considering you directly quote their opinion and then, based on that opinion, say that their life is \"utterly dehumanizing\". >attitude that human interactions should be prioritized on the scale of \"entertain me\" This is not what the parent poster said. reply dfabulich 2 hours agoparentprevSocial media apps need users, and they need users to return and re-engage. The data is clear that even very basic algorithmic feeds get better engagement, presumably by showing users better stuff than whatever happens to be newest. You can't possibly do anything to \"put an end to this.\" Twitter and Bluesky both allow you to see a chronological feed, though you have to jump through some hoops to get to it. Just use that. reply grishka 1 hour agorootparent> Social media apps need users, and they need users to return and re-engage. And this is where the goals of the platforms and their users are at odds with each other. > Just use that. The problem is that while I can \"just use that\", which of course I do, the mere presence of an algorithmic timeline, let alone as the default option, still substantially shapes the way people post and share. People post differently when they expect interactions from outside of their usual network vs when they don't. I had my tweets get uncomfortably popular on several occasions, presumably because the algorithm decided so, and I didn't enjoy that. Then there's also the problem that some people you follow will use the algorithmic feed and will repost things from there. Again, this wouldn't happen if it didn't exist, and it's not something I can influence with my choices. What I want is for content to spread organically again. I want the platform to be a dumb pipe between me and the people I follow. I don't want it to have any agency whatsoever. And I don't want \"influencers\" to be possible. reply dfabulich 38 minutes agorootparent> And this is where the goals of the platforms and their users are at odds with each other. They can be, but they usually mostly aren't. Showing people what they like is the best way to get them to come back. I think you need to accept that what you want is different from what most people want. > I want the platform to be a dumb pipe between me and the people I follow. I guess your only hope would be to make it illegal, worldwide, to provide algorithmic feeds. Hacker News uses an algorithmic feed, and that's why we're here talking. https://news.ycombinator.com/newest exists but it's not very good. You can also browse Reddit chronologically https://www.reddit.com/new/ but, seriously, don't bother. So, as long as someone can do algorithmic feeds, someone will, and people will use it, even you, because algorithmic feeds are just better than chronological feeds. > I don't want \"influencers\" to be possible. This one is truly hopeless. We've had influencers at least as long as we've had the written word. reply grishka 1 minute agorootparent> Showing people what they like is the best way to get them to come back. There are different usage scenarios of social media. You seem to imply that people use it for entertainment, and yes, the companies themselves sure make them optimized for that. But I want to use social media for staying up to date on my friends' lives and nothing else. Most existing platforms actively resist this use case because it doesn't grow metrics. > I guess your only hope would be to make it illegal, worldwide, to provide algorithmic feeds. Well, at least I'm working on two fediverse projects. There are no algorithms on the fediverse. You see posts from the people you follow, in the order in which they were posted, and nothing else. > We've had influencers at least as long as we've had the written word. That's different. Those \"influencers\" always became such organically, because people voluntarily spread their \"content\". This is vastly different from the platform itself stepping in and non-consensually shoving this content into millions of faces because its black-box algorithm said so. tanjtanjtanj 1 hour agorootparentprevTwitter’s (X) “following” feed is not a purely chronological feed. I will often see tweets from people I follow on “For You” that don’t show up in the other feed. It also tries really hard to direct you over to the For You feed silently at any chance it can get. Also among followers it will surface tweets that it thinks will drive engagement and show/not show retweets based on algorithm. reply 015a 2 hours agorootparentprevMeth producers need users, and they need users to return and re-engage. The data is clear that even a small amount of meth introduced into a community generates higher return on investment, presumably by giving its users a high that's better than not being high. You can't possibly do anything to \"put an end to this\". reply 015a 1 hour agoparentprevThe reason social media apps use more complex global discovery algorithms (over a chronological feed) is because chronological feeds always run out of content. That's literally the only reason. At some point, some team at some gigacorporation invented the \"hours spent with us\" KPI, and tasked their hundred reports to increase it. It turns out, it doesn't matter how many people complain, if the \"hours spent with us\" KPI keeps going up. \"But users prefer algorithmic feeds\": There's no evidence of this. The KPI is measuring an increase in hours spent with the app; it is not scientific-method A/B testing a preference between two options. Even if an app could do this, what does \"preference\" mean? You could measure how many users pick one experience versus another, but I've never found an app that, if it offers both experiences, durably and reliably saves your choice for a chronological feed between re-launches. Also: Maybe I want both experiences, at will. Hours spent in one experience versus the other? This is not communicating a preference; if I choose spending an hour driving during my commute to one job, versus ten minutes walking to another, have I revealed a preference for a longer driving commute? Obviously not. You can ask users directly: And users may actually reveal their preference that social media never existed at all because your company isn't actually delivering value to the world [1]. Oops. Uh, don't run that survey again, bury it, make sure shareholders don't find out. All social media is trash, and should not be consumed by anyone who has even an ounce of self-respect. Honestly: HackerNews is in that bucket, but at least its not as bad as most platforms. [1] https://fortune.com/well/article/nearly-half-of-gen-zers-wis... reply prophesi 2 hours agoparentprevYep. I'm not on social media, but even with Youtube it can be frustrating that subscriptions are \"personalized\" by default; you otherwise have to click the notification button again and select \"All\" for all their videos to show in your subscriptions tab. I'm at least glad the option is there to not let it be determined by an algorithm. When I was on Instagram, they introduced a chronological feed, but that view hid Stories for some reason. Tinfoil hat theory is that it's to show users prefer their personalized feeds full of ads when in reality they made the chronological view frustrating to use as people use stories more than regular posts these days. reply intended 2 hours agoparentprevThere seem to be two different issues in your point here. First is algorithms to select content for users. This is often an issue because the algorithm is designed to maximize time on site, which results in content that pressed emotional buttons and engages the fight or flight reflex built into us. The other issue is that users can’t be trusted to use a tool correctly. I don’t think this last point is wrong, but I don’t think it links to your primary point. reply terminalbraid 2 hours agoparentprevWhat's an example of being shown content from a pool that does not involve an algorithm? reply anon7000 2 hours agoparentprevI actually disagree. (I agree that engagement-driven algos are cancer though. And that they developed for money reasons, not to help users. So maybe I agree with you actually lol) I never used Twitter back in the day. I’m trying out BlueSky and not sure what my account should be. I could post software stuff, eg a career related account. I could post pictures from around the city. I could post my personal political thoughts. Or maybe hobby-related, like board games. But if I’m following someone who’s respected in the career, I’m expecting career content, not random political thoughts. If someone is following me, I want to be able to post more personal content, and more random stuff. Unless it’s a personal friend, I probably don't need to see everything they post! So I don’t necessarily want a chronological timeline. Custom algos like BlueSky has are pretty interesting. “Here are all the developer posts” and “keep the political posts over here out of your main feed” reply SoftTalker 2 hours agorootparentIt's pretty easy: if someone posts a lot of stuff you find uninteresting, stop following them. reply EasyMark 2 hours agoparentprevI would like multiple knobs “must see > # likes”, “intermingle follows level” “limit to ## posts from one person in a day” and similar common sense settings. I don’t need AI algos picking for me reply pessimizer 2 hours agoparentprev> chronologically presenting information from followers creates a confusing morass for the average user to process. This was simply a lie press released by Facebook, and endlessly repeated uncritically. Facebook became Facebook with a chronological feed. It began to manipulate the feed because it was profitable and the government didn't object. That confused the hell out of people for years, when they couldn't figure out why their aunt posted something that never showed up. And after that, social media transformed into something other than keeping track of your family and friends because of the paid injections of crap. reply DennisP 2 hours agoparentprevLetting users pick the algorithm seems like a good way to give them responsibility. And the article says Bluesky still has a simple subscription feed as the default. reply nerdjon 2 hours agoparentprevI think the problem with social media is there is just a lot of noise, and generally had a discoverability issue. I would like to be recommended people that I want to follow (and have what I put out be recommended to people). I mean do we really need to remember the Facebook posts that people were making 10ish years ago that really was pointless? That being said, that's the power of having some choice in the matter. If you don't want it, you don't need to use it. Both can be perfectly valid ways to consume social media content. reply spoonjim 2 hours agoparentprevThe two problems with your approach are: 1) There is always some post you'd be interested in that you won't see because it's outside your social network. Different people have different preferences on how much of this they want to see. 2) When you follow someone, you're probably interested in some aspect of what they post and not others. For example, if there's a biologist who is also a big Pittsburg Steelers fan, you're probably not interested in both types of posts. reply ZeroCool2u 3 hours agoprevI used a starter pack that focused on NLP/LLM academics and researchers in industry that tend to publish and talk about their work on Twitter, but have moved to BlueSky. It really does feel like a breath of fresh air. It's the content I want to casually browse when I'm on the subway or the ferry with a lot less rage baiting and without bots and spam. Plus, it has that nice chronological feature in the default algorithm that really focuses on recent news, which was always my issue with Threads. reply tbalsam 2 hours agoparentDo you have a link to said starter pack? Have been having trouble finding a good one of those specifically in this vein.I fear the recent US election is going to kill it, though. How? Aren’t there a lot more people migrating to Bluesky now in light of Musk’s antics on X? reply kps 2 hours agorootparentLook at some of the comments here. If Bluesky has the reputation of being centered around US politics, it won't be the choice of those with other interests. reply danielbln 50 minutes agorootparentI've got my wors filters set up so that I never see an ounce of US politics. It's wonderful. reply rsynnott 2 hours agorootparentprevThat sort of ultra-rapid growth can be difficult to absorb, though so far it seems to be going fairly well. reply intended 2 hours agorootparentprevMarket power and a compliant government / political party. reply Vaslo 2 hours agorootparentprevLol who told you that? X is vibrant and big ad companies are coming back. I personally created an account on Bluesky not to support it but to ensure leftists cannot create an echo chamber. reply bee_rider 52 minutes agorootparentWhat do you mean by “cannot create an echo chamber?” The whole pitch of the site is build-your-own-filter functionality. reply BadHumans 2 hours agorootparentprevMusk has the president-elect in his pocket and Musk hates BlueSky. The goal is to be the only source of propaganda and let right-wing conspiracies fly unchecked on social media. reply cmxch 9 minutes agorootparentThat’s a bit reductive. What says that Bluesky doesn’t have their flavor of propaganda that suits their preferences? That is, what makes the Bluesky perspective more valid other than just being on a platform that hates Twitter? About anything that is currently legal and permitted on Twitter seems to be specifically prohibited by design (and can’t be opted out of wrt moderation) in Bluesky. If the Mastodon case is any guide, there will be a great effort to ensure that nothing wrt software can conflict with it. reply DFHippie 2 hours agorootparentprevTrump talked continuously about weaponizing the government, how it was done to him (\"They are investigating/prosecuting my crimes and I am a politician now! Disgraceful!\"), and how he was going to do it good and hard to those who opposed him. Now Musk is the de facto vice president, so anyone who annoys Musk or gets in the way of his amassing more wealth and power is a potential target. You don't have to have any case. Just drown the startup in legal bills. reply pm90 2 hours agoprevBluesky is a refreshing addition to social media. Many users say it reminds them of “old twitter”. I didn’t use twitter so Im not sure what that means. But compared with other social medias? No ads. Auth using your domain. Choose your own timeline algo. Its amazing! I am worried about the commercial aspects though. I am willing to pay them a subscription if they just ignore ads altogether. The fact that all of it is oss (the protocol, and the implementation!) does give me hope that they won’t turn into an ad infected slop. reply bee_rider 57 minutes agoparentWhat is the business model, anyway? reply rsynnott 2 hours agoprevDecades later, those people who spent hours on their USENET killfiles (despite the name, killfiles weren't just blocklists - fancier clients supported quite sophisticated scoring - https://en.wikipedia.org/wiki/Kill_file) are finally getting their day in the sun. (I don't necessarily disagree that this is the future, but it is quite funny that the \"bring your own algorithm\" approach was basically forgotten about for about 25 years, and then revived...) reply TheOtherHobbes 2 hours agoparentI don't think it was forgotten, it was considered irrelevant by FB/Meta and TwX because they were more interested in collecting information about social graphs and demographics so they could sell ads and influence. So they invented (copied) following/friending individuals, which was the opposite of USENET's newsgroup (topic) system and also (sadly...) a proxy for social status. In reality there's always been huge interest in topic media. Reddit sort-of owned the space for non-real-time posting, and Twitter got some way there with lists for news and the breaking hashtags. BlueSky feels somewhere between those - still short posts (bad...), more topic than social graph, but not so obsessed with clout chasing and status. reply mattferderer 4 hours agoprevI preferred lists as the only usable way to use Twitter, even before the takeover. I like how Bluesky has improved the functionality of your own feeds & being able to share them. If I recall, Elon was promising something similar when he bought Twitter but I don't believe that ever happened. It will be interesting to see how Bluesky is able to continue operating when it needs to generate a profit though. I'm curious what their plans are. The need for profit on social media platforms often results in loss of quality & user experience. reply m3kw9 3 hours agoparentI used to use lists but find that the info there diverges and gets noisy overtime, they don’t have a one click way to remove people from lists, so I use them less than I should reply PurpleBison 3 hours agoprevI've been using Bluesky casually for the past week and, as someone who was never a regular Twitter user, I don't see the point of using these kinds of social media websites. Sure, Bluesky is definitely less toxic than Twitter, but I still haven't found a way for it to add value to my life. reply mtlynch 3 hours agoparentIt's fine that you don't get value from it, but how does that add to the conversation? There are an infinite number of activities available to humans. Some people will find some of those activities enjoyable, and others will not. Is there value in joining a conversation about every activity to declare that you don't enjoy it personally? A more constructive way of engaging might be to say how it might add value to your life if it were different in some way. Or you could warn others that it's harmful in ways they don't recognize. But just an unqualified, \"I don't see the point of that,\" is not so helpful. reply hwbehrens 3 hours agorootparentI partially agree, and I'm generally in favor of doing our part to help dang and keep the level of conversation high. With that being said, I read the parent post slightly differently. I took their post as an implicit request that commenters share their own experiences and how they receive value from these services. A bit like the nuance between the statements \"This is pointless\" vs \"I don't see the point\", where the latter has something of an implicit (yet). reply assanineass 3 hours agorootparentprevRoasted reply PurpleBison 2 hours agorootparentprevThanks for you advice on writing Internet comments. reply mtlynch 7 minutes agorootparentYou should review the site guidelines, as you've only made two comments on HN, and both of them violate the community guidelines: >Be kind. Don't be snarky. Converse curiously; don't cross-examine. Edit out swipes. >Please don't post shallow dismissals, especially of other people's work. A good critical comment teaches us something. reply tbrownaw 3 hours agoparentprevIf people you know are there, it's an alternative to sms or phone calls or whatever. If people sharing your profession or hobbies are there, it's a way to hobnob or talk shop. If you follow a bunch of reporter-type people, it's an alternative to the newspaper. The actual site itself is mostly irrelevant, except for how easy or hard or makes it to do specific things. reply dpkonofa 3 hours agoparentprevIt's not for casual users. The only way it adds value to your life is if you use it more than casually. It's a symbiotic thing. The more info you feed it, the more valuable the information you receive on it will be to you. reply jazzyjackson 2 hours agorootparentAlso it was a revelation for me as a nontwitterer to go barhopping with a friend who tweeted his itinerary and we met up with a couple of strangers that followed him online. Like, oh, social media can actually lead to social interaction, amazing. Probably not typical experience but a few people I know would never willingly give up on such a powerful networking tool in their niche. reply the__alchemist 3 hours agoparentprevI remember thinking of it as \"Facebook status updates, without the rest of [the old] Facebook\" reply intended 2 hours agoparentprevMaybe it’s not useful for you, which would itself be a cool result. How comes ? What works, what doesn’t? reply rsynnott 2 hours agoparentprev> I've been using Bluesky casually for the past week and, as someone who was never a regular Twitter user, I don't see the point of using these kinds of social media websites. Yeah, if you didn't like old/pre-Musk Twitter, you're probably not going to like Bluesky; as far as the user is concerned it's a slightly refined version of the same thing. reply danielbln 48 minutes agorootparentDisagreed, block lists, word filters and starter packs turn it into whatever I want it to be, and of all the things it's not X, I tell you that. reply Barrin92 2 hours agoparentprev>but I still haven't found a way for it to add value to my life. I think one of the most useful cases of these sites is looking for conversations by people on a subject you are interested in but don't have a lot of real life connection to. For example I was really interested in China studies, so I found a list of Sinologists. Just reading what they write, what sources they recommend and \"listening in\" on it is a very good way to get exposed to all kinds of stuff you wouldn't even know to search for. And there's lots of fields like this. Maybe you are interested in abstract expressionist art. What's the chances you know a lot of scholars unless you are one? These networks of really interesting people is I think where the value is in platforms like this. reply tootie 3 hours agoparentprevThe only value I got from twitter that I now get from bluesky is following prominent people and institutions. Journalists, scientists, engineers, economists or whatever interest group you align with. It's a great way to keep up with interesting things they are doing or thinking. I rarely if ever get engaged with conversations or debates. reply XorNot 3 hours agoparentprevI don't use any of these services actively, but we do all end up being passive Twitter and maybe in the future Bluesky consumers. That said I've done my usual due diligence and created an account very obviously under my IRL name now to hold down the username. reply giancarlostoro 4 hours agoprevI really dont understand why we cannot just go back to chronological as a default. This is how I use X/Twitter, and anything else that lets me just go chronological. reply coldpie 4 hours agoparentIt's because the majority of users are being fed more content than they can consume, whether that's through a large count of follows or global search results or a discovery tab. In that case, you need some method by which to decide what subset of that content to show to the user. Chronological (\"show me the latest 50\") is one option, but is it the best, for however one defines \"best\"? The people running these things seem to think it is not the best, for however they define \"best\", so we see the various discovery algorithms and all their associated pros & cons. reply johannes1234321 3 hours agorootparent> majority of users are being fed more content than they can consume That is a group of users. Another group of users follows only few active others and therefore sees only little content, but the platforms wants to show them something new all the time, to keep the platform \"relevant\" (in order to show more ads) This then of course ignore the fact that they probably purposely follow only few. reply MadcapJake 3 hours agorootparentClearly the service is not designed for people to only engage with a few folks, it's meant to be a zeitgeist firehose. If you're only following a few people it's like using a spreadsheet for tracking household frozen pizza inventory. reply yamazakiwi 2 hours agorootparentThe tool might be more sophisticated than you need but following only a few people is totally fine and should not be overrun with algo content just to promote ad revenue to the platform owner. Maybe the people you want to follow are only on said platform so you are required to consume it that way. reply dpkonofa 3 hours agorootparentprevThis is a spot-on, although incredibly weird, analogy for it. It only works if you use it. You get out what you put in. reply KoftaBob 3 hours agorootparentprev> That is a group of users. It's the majority of users. Those who \"follows only few active others\" are a very small subset. reply pessimizer 2 hours agorootparentThis is a result of the algorithm. It also forces people who would prefer to only be following updates from an intimately curated group to have to pick what they've explicitly taken the time to select out of a pile of crap. reply giancarlostoro 3 hours agorootparentprevI think one issue I see on Facebook is, it went from being very personal, to just being a mix of other social media norms. Which adds noise. If Facebook had a \"Show me only relevant personal things\" timeline, I'd use it. They used to let you define a custom timeline, where you group x number of friends, it was much nicer than the standard since I could weed out people depending on what type of content I wanted. I've stopped using FB for a while now though. reply j2kun 3 hours agorootparentprevThe \"information overload\" problem always seems like a problem invented by the creators of these platforms to project on their users and justify coercive behavior. reply mjcl 4 hours agoparentprevGood news! Bluesky does default to chronological, but also provides other options. reply swatcoder 3 hours agoparentprevChronological is ideal for personal feeds -- family and friends, maybe some professionals and curators you follow, your preferred brands, your local public services, maybe keyword/topic subscriptions, etc. A few hundred or a few thousand explicit subscriptions with output sized to match how often you check your feeds and how much you care about missing things when you don't. Like maybe you, that's all I want, so it feels like chronological should just be the default option that all this algorithm and trending business is nonsense. I just want a nice aggregation of the information I know I want from the sources I personally know, appreciate, and can contextualize. But \"at scale\" you end up with a lot of users who are more interested in idle discovery, seeing what their peers are seeing so they can talk about it, etc -- as well as platform maintainers hearing the siren call of advertising and paid placement as way to offset the high costs of maintaining a multimedia network for millions upon millions of users. Together, this becomes the wind behind algorithmic feeds and paid visibility features, because the algorithmic feeds are something users actually enjoy and breaking away from chronological feeds opens tons of revenue opportunities in an expensive and intensely competitive business. I no longer expect to find my kind of service from any platform that's positioned for the global mainstream. The winds are always going to take that somewhere else, even if it looks promising today. reply MisterBastahrd 2 hours agorootparentThere's a Discover tab. If you want to discover things, use it. Tiktok follows a similar approach for their secondary feeds. I don't want to discover anything on my personal feed unless it comes from one of the sources that I have chosen to follow, and I want information relayed to me in the order in which it is posted. For Discover, I couldn't care less. reply M04R_PYL0N5 4 hours agoparentprevAgreed. They kind of just overthought the experience to try to game engagement and clicks. Chronological should be the default, anything else should be up to the user but I know that doesn't quite make money for the apps the same way... reply Spivak 4 hours agorootparentTIL priority inbox is trying to keep me in Gmail longer. I could understand feeding people rage-bait content as a method of false engagement but these are people you followed. Most liked/boosted/retweeted among the people you want updates from seems ideal. reply ruined 4 hours agorootparentedit:n/t reply JumpCrisscross 3 hours agorootparentA better analogy would have been spam filters. reply sameoldtune 4 hours agoparentprevI agree, but some people use social media to follow 1000s of other users. Some kind of “hot right now” or “high engagement since you last logged on” setting might be nice for them. reply garciasn 4 hours agorootparentI think it's super interesting you believe the social companies care about what is 'nice for the user' as opposed to what is nice for the advertisers, audience/data brokers, and the investors. The reason algorithmic ordering is so common is because that's what gives the most runway for advertising, behavior manipulation/tracking, and its downstream financial effects. reply JumpCrisscross 4 hours agorootparent> reason algorithmic ordering is so common is because that's what gives the most runway for advertising You’re both right. Algorithmic feeds boost engagement, both by surfacing the most-engaging content and removing the burden of trimming one’s follow list, and also aids in serving ads. (Both by making them easier to sneak in and in the same engine that surfaces engaging organic content being useful for serving engaging ads.) reply ziddoap 3 hours agorootparentprevAn experience that is super shitty for the user isn't going to result in any users. If you are trying to take users away from twitter, you're going to focus on some 'nice for the user' things (or, at least, 'nicer than twitter for the user'). Like most things in life, this isn't a binary choice (user or advertiser). They're going to try to optimize for both, striking a balance. reply notpushkin 3 hours agorootparentYes. However, an experience that’s okay for the user but also super addictve will result in a lot of users. reply garciasn 3 hours agorootparentprevMarketshare comes first, then revenue optimization comes later. reply jt_b 3 hours agorootparentprevThe pattern can be useful for multiple parties, for different reasons, some nefarious. Some users are definitely interested in higher \"signal\" content, especially when you follow enough accounts that consuming even a small fraction of the content isn't feasible. reply vehemenz 3 hours agorootparentprevThis is basically right, but if there's a takeaway from Twitter/X's decline it's that users will only tolerate so much and that platform inertia has its limits. reply JoshTriplett 3 hours agoparentprevIn addition to all the other reasons social networks are incentivized to feed you content that maximizes how long you spend on the site/app: I think some of this comes from a combination of social networks that mirrored real-world networks (and thus create social incentives to follow people you might otherwise not want to), social networks on which people post a firehose of content, and Fear Of Missing Out (FOMO). Some people use Facebook as a primary means of keeping in touch with family. Some people's Facebook networks mirror their family-and-friends networks. It's socially awkward to unfollow your relatives, even if you don't particularly want to see what they post, or can't deal with the volume they post. But it's not socially awkward for Facebook to notice what you do and don't engage with, and try to show you more of what you engage with, regardless of who you follow. If you treat following someone on X, or Fediverse, or Bluesky, as nothing more or less than a means of seeing what they post, then you can carefully and selectively choose who you follow, such that your chronological timeline is a manageable amount of content. You can choose, for instance, to not follow people who post a massive amount of content, or whose content you mostly don't want to see. You can make lists for people whose posts you might want to sample from time to time and not read all of. You can rely on other people you do follow to repost things that are interesting. But if you're following so many people, or such high-volume people, that your chronological timeline is a firehose you can't possibly read all of, then an algorithmic timeline becomes more tempting. reply ideashower 3 hours agoparentprevIt does default to chronological, though? reply alwayslikethis 4 hours agoparentprevA problem is that your sources may have substantially different flowrates. One source can fill up the feed by posting a lot, which is a problem with RSS if you use it to subscribe to any high volume blog. reply eddieroger 4 hours agoparentprevAside from the usual \"because everyone has different preferences and more people prefer it this way,\" a lot of what happens on social media is ephemeral, and to many people there is little value to go back and see things that happened a while ago versus something happening right now with higher engagement. It's the difference of seeing what happened versus wanting to be part of it. reply dragonwriter 2 hours agorootparentBut “chronological” feeds are typically newest-first, so “people don't want to go back to what happened a while ago” isn’t really an argument against them. reply dawnerd 3 hours agoparentprevWell good news, there’s Mastodon for that and there’s increasing interoperability with threads and Bluesky via bridges. reply LordRishav 4 hours agoparentprevWhat is meant by chronological here? Do you mean you follow some people and your Home page just arranges all the posts by those you've followed chronologically? Because that is what Mastodon does. And while I personally prefer it to be this way, this won't work for the user who just wants to see the type of posts they like, not necessarily the people they like. The recent exodus of American and Brazilian people from X is thus divided into those who chose Mastodon and those who chose Bluesky, with the latter having a much larger number. Make of what you will. Best would still be RSS feeds and everyone having their own blog. Just saying. reply nemomarx 3 hours agorootparentI think blue sky has a default chronological following feed too? Twitter pushing for you instead of following is kinda notably distinct. reply dragonwriter 2 hours agorootparentTwitter. Facebook, TikTok, all push a For You; Threads doesn't even have a pure following feed,... I think Bluesky/Mastodon are the outliers here, not Twitter. reply ok123456 4 hours agoparentprevshowing you posts that maximize your use of the service is considered \"growth hacking\" reply BipolarCapybara 3 hours agoparentprevBecause then your feed is flooded with news channels or posters that tweet every other minute. reply schnable 4 hours agoparentprevSo that it's different when you open the app every 15 minutes. reply JoshTriplett 3 hours agorootparentThis is, simultaneously, the reason why social networks want to use non-chronological timelines, and exactly the reason to use chronological timelines: so that it discourages perpetual usage. On Fediverse, I can open the page, read the things that are new since the last time, and close it. reply dragonwriter 2 hours agorootparentprevIf you follow a decent number of active accounts, a chronological feed is different, too, especially if it is like Twitter/Bluesky (and unlike Facebook) where responses are the same kind of item as top-level posts. reply AlienRobot 3 hours agoparentprevhttps://www.youtube.com/watch?v=zgA4GzRsldI reply sharkjacobs 2 hours agoprevBluesky is a nice experience, exactly the way other social media sites were five or ten or fifteen years ago, when they were still focusing on user acquisition and were still paying employee salaries with VC money. If we get a few good years of Bluesky before it turns that's not bad, I'll take that, but I feel like the turn is inevitable, right? reply dkobia 3 hours agoprevAdvertising incentivizes engagement driven content amplification which is usually best manifested in outrage unfortunately. On Twitter (X), Instagram, TikTok, it seems any minute signal (view, like, scroll, linger) algorithmically retunes your posts to maximize engagement, which is the root of all the problems. reply tking8924 1 hour agoparentI do personally wish the role of ad based monetization models were included in these conversations more often. With both traditional media and social media the conversation tends to blame ideology for their shortcomings but in reality it's just, as you noted, a bad incentive model. They aren't ideological, they're just maximizing the amount of your attention they can capture because that maximizes the amount of ad revenue they can bring in. There are certain people in certain, specific, situations that have a strong enough ideological stance to make a decision based on that ideology, counter to the one they're incentivized to make. But the majority of the people in the majority of situations are going to make the incentivized choice. If you want to really change something, you have to change the incentives. reply PittleyDunkin 3 hours agoparentprevI can't speak to twitter and instagram, both of which seem to be terribly confused about what sort of content I like, but this works very well for narrowly tailoring TikTok videos to be content I appreciate. reply intended 2 hours agoprevThere is one constant pattern for media. Old Media centralizes. New media decentralizes. New media becomes old media. I’m tempted to say that the only rule is that information networks with humans on it tend to centralize. I have no idea why, or how to explain the behavior, and I’m pretty sure this has happened since print came into existence. If you have the term or field that research would come under, do share. (economics ? media economics? Information x?) reply pessimizer 2 hours agoparentThis isn't a pattern. Old media was at least two or three orders of magnitude less centralized than new media, and this was legally enforced by restricting media ownership. After Clinton deregulated media, it centralized. That's it. So the pattern is if you let extremely wealthy people accumulate without limit, they will. reply xena 3 hours agoprevCustom feeds are really cool. I made a custom feed that shows every time someone said \"sneak peak\" instead of \"sneak peek\" on a livestream: https://bsky.app/profile/stealthmountain.xeiaso.net/feed/sne... It's currently running either under my desk or in the living room on my homelab Kubernetes cluster. It's a fun little thing to look into every so often to get a vertical slice of humanity. reply warunsl 2 hours agoparentCustom feeds really intrigue me. If I understand the API right, if you are an implementer of a custom feed, you need to expose an endpoint that the bluesky server can hit whenever a user wants to load content for that custom feed. And the endpoint that you implement will return the results. Does this mean that the implementer of this feed will have to take into account the network costs? What I am trying to get at is that if you implement a custom feed, you need to be aware that you are potentially looking at hitting your data caps on your internet provider if a lot of folks start consuming that feed. Do I understand this right? I am aware that there are services that let you create custom feeds. But they are mostly for simple compositions like a feed for the following set of words and/or set of people, etc. reply MostlyStable 3 hours agoprevIf one of the options is \"only show me content by people that I have explicitely followed/subscribed to\", then I might be interested. I completely understand why social media companies need to have some kind of an algorithm. Without one, when you first join, your feed would be completely empty and I'm sure that user retention after the first visit would be near zero. I do not understand _at all_ why it isn't at least an option to, at some point, decide I only want to see content from people I have actively selected. reply fckgw 3 hours agoparentThat's the default feed option. There is no \"for you/trending\" algorithm on Bluesky. reply dragonwriter 3 hours agorootparent“Discover”, “What’s Hot Classic”, and “Popular with Friends”, of the Bluesky-provided feeds (there may be others) are different versions of that concept, but there is not one shoved down your throat like on many other social media apps. reply dpkonofa 3 hours agorootparentprevNot as a single feed. The \"Discover\" and \"What's Hot Classic\" feeds duplicate that behavior, if I'm not mistaken. reply dragonwriter 3 hours agoparentprev> If one of the options is \"only show me content by people that I have explicitely followed/subscribed to\", then I might be interested. Yes, like most social media, Bluesky has a “Following” feed available (and, unlike many that always start on a different feed, with Bluesky Following can be — and I think is initially — the default feed it opens to.) reply arcatech 3 hours agoparentprevYes, the following feed is the default. reply scinadier 2 hours agoparentprev>>> I do not understand _at all_ why it isn't an option to, at some point, decide I only want to see content from people I have actively selected. Probably because browser bookmarks exist. reply dpkonofa 3 hours agoparentprevI've been on Bluesky for a bit now and this is probably its biggest upside. The \"followed\" feed is the default and there's options for a \"Discover\" feed to see related but not subscribed posts and then there's also a \"What's Hot Classic\" feed which attempts to replicate the old Twitter feed where popular posts are listed with those of relevance to you bumped a bit. I'll throw out a warning, though, to make sure you tune your settings unless you're ok with seeing buttholes and other risque images. I don't have any followers and it's constantly showing me accounts that post all kinds of sexual content by default. It's probably closer to the supposed \"free speech\" Twitter is claiming but without all the Nazi stuff. reply fldskfjdslkfj 2 hours agoprevI never understood why something like YouTube doesn't allow you to at least control the level of discovery. reply cube00 2 hours agoparentThey control the level because they need users to remain on the site for as long as possible. They'll do a better job of that then if the user has control. You might have a better experience if you could set your own level of discovery but your session could also be shorter and that's unacceptable to an advertising company that needs your eyeballs for as long as possible. Just ask Facebook why I could never see only my friends posts and then twisting the knife by showing me \"suggested\" click bait junk before it had even exhausted the posts available from my friends. reply caekislove 2 hours agoprevI'm old enough to remember when the media tried to astroturf Threads like this. reply hnpolicestate 57 minutes agoparentRemember when HN tried to astroturf Lemmmy? reply josephd79 1 hour agoprevFAD.. will end up like threads and clubhouse. reply zzzeek 3 hours agoprevcan folks share some tech starter packs here that are preferably not all LLMs? python / web stuff / databases / systems design / hardware etc? reply Super_Jambo 3 hours agoprevI'm having a crack at making feeds that filter by topic and location using LLM. The current test feed is here: https://bsky.app/profile/super-james.bsky.social/feed/uk-pol... But I'm in the middle of a big re-work so it'll get a lot better when I finish that. reply nathias 3 hours agoprevI want to own my data, algo, and instance. reply haunter 3 hours agoparentSo basically a blog reply fsflover 2 hours agorootparentOr Mastodon. reply kspacewalk2 3 hours agoparentprevAnd hosted for free, no ads. And a pony. reply m3kw9 3 hours agoprevI use lists but generally I stick to for you and use my head to filter posts that are likely propaganda and stuff I know what they are trying to do. Once you use twitter enough you develop a filter that by pass a lot of bs and makes the whole thing enjoyable reply hntempacct99 3 hours agoprev [–] From what I have seen using Bluesky this isn't true at all. It's brutally censored, even more than Twitter was in 2021. Or are there other relays and appviews I can use that aren't? Is there a comprehensive list of Bluesky infrastructure that isn't run by Bluesky themselves (excluding a PDS)? Or is it totally centralized for now? reply nick_ 3 hours agoparentWhat brutal censorship have you observed? reply throwaboutbsky5 3 hours agorootparenthttps://bsky.app/profile/sallgrover.bsky.social https://x.com/salltweets/status/1857595757882188086 Sall Grover is the creator of a woman-only social app in Australia that was taken to court over that sex exclusivity. Posted a few controversial statements to test the atmosphere and this is the result. reply kspacewalk2 3 hours agorootparentJudging purely from those Tweets, Sall is a troll who was correctly booted off a platform that is trying to improve the quality of discussions. reply happytoexplain 3 hours agorootparentprevThis is just the common case where it's a thing one could express a morally honest opinion about, even if it's emotional or negative, but is instead expressed curtly for the purpose of encouraging hatred broadly. I.e. it's the exact definition of trolling (and specifically, group-hatred by intrinsic qualities like sexual feelings, race, etc, which is understandably the most commonly moderated type of trolling). I'm not going to go so far as to say that all platforms must moderate that type of content, but it is of course a decision that falls within the realm of reason for any given platform. So, it seems dishonest to spit on it as \"censorship\" (ever more, \"brutal censorship\"), assuming you are agreeing with the GP. reply tedajax 3 hours agorootparentprevnext [5 more] [flagged] zb3 2 hours agorootparentYes, I want to decide what I want to read, and I don't care what you call \"transphobia\", especially since facts are often labeled as such. reply unclad5968 3 hours agorootparentprev\"censorship is fine as long as it's happening to people I don't agree with\" reply tedajax 3 hours agorootparentGenerally speaking, in real life, people tend to get kicked out of places for being bigots. reply intended 2 hours agorootparentprevYeah I’m done with this dishonesty. Perhaps you aren’t being dishonest, but this argument is the tip of the spear to justify harm of others. I’ve been a mod. I hate the fact that my only option is to silence. But by all that is holy I’m going to use all that I can when someone is using dishonest, malicious, malformed and malign arguments. I have seen what happens when trolls run unchallenged. ——- The great thing is that no two moderators will come to the same decision on a case, because context matters. There is almost certainly a community where X type of content is welcome. Why not go there ? reply okeuro49 3 hours agorootparentprevhttps://bsky.app/profile/realbabylonbee.bsky.social reply kps 3 hours agorootparentI think that's in line with pick-your-own — Bluesky has the concept of ‘labelling service’ (with Bluesky as a/the default labeller) and client actions based on those labels (hide/warn/show). If that's all that's happening, the really bad part is contributing to the perception that Bluesky is just a left-Gab (and if that's what you want, there are perfectly good Mastodon cliques already). There used to be a US-politics labeller, of value to non-Americans, but it seems to have fallen over. reply vehemenz 3 hours agorootparentprevCensorship and moderation aren't the same thing. Cmon folks, this comes up once a month on HN. reply steveoscaro 3 hours agorootparentThis is such a weird logical hoop that that so many people are eager to jump through. reply happytoexplain 3 hours agorootparentIt's a pretty understandable semantic argument, where tons of people are going to be irrationally biased in whichever of the two directions suits them on a given example. I.e. it's not really \"weird\", is my point. reply PittleyDunkin 2 hours agorootparentSorry, which two directions? Surely you can have more than two distinct opinions on how to best handle moderation. Which is a fatal flaw to the twitter \"community notes\" feature, too. reply ilikehurdles 2 hours agorootparentprevThis is just a thought-terminating cliche. reply vehemenz 2 hours agorootparentIt seems like it doesn't take much to terminate thought for you. If you want to suggest that moderation and censorship are the same—two concepts with obviously differing senses in English—take a stab at making the argument instead of just asserting it in, ironically, a cliche. reply PittleyDunkin 3 hours agorootparentprevIt looks like you can just click through the content warning. reply wulfstan 3 hours agorootparentYou can also just turn it off globally by turning off the \"Intolerance\" setting on the Bluesky Moderation account - visit @moderation.bsky.app and set it up how you want. reply jayd16 3 hours agorootparentYeah, I just made an account to test this very thing. As a brand new user it was easy to find. Makes you wonder how earnest the complaints are. reply anderber 3 hours agorootparentprevYou can still see the content, right? So it's just a label, it seems. reply gr__or 3 hours agorootparentprevThere is a cultural divide on where you stand wrt transphobia. The default appview is indeed not down with it, where Twitter is ofc very down with it. The protocol is ambivalent towards it, so if you seek hate, you could host your own. I'm very fine (happy even) with the bsky team not being invested in that side of history. reply aighto 1 hour agorootparentYes, and much of it comes down to sexism. Anyone who looks at Levine and thinks something along the lines of wearing a dress, must be a woman rather than that is obviously a man in a dress has deeply sexist ideas about how women should present themselves. You can label this as \"transphobia\" if you like but that's just a tacit acknowledgement that the \"trans\" belief system is built upon sexist principles. reply gigatree 3 hours agorootparentprevAnd what if you just espouse the normal view that all of history & 97% of people currently on earth hold? reply dpkonofa 3 hours agorootparentWhich view is that? I highly doubt that there's any view that fits the criteria you just posited. reply happytoexplain 2 hours agorootparentprevWhatever you mean, you can probably write about it without encouraging hatred for groups of people based on qualities that, by themselves, are harmless, like whether they feel male or female, are biologically male or female, are gay or straight, white or black, etc etc. reply gigatree 44 minutes agorootparentThe problem is that an overwhelming amount of the left label statements not meant to be hateful as hateful, specifically: “there are two genders”. Everything has been reduced to “hate”, to the point that it actively muddies the waters wrt actual hatred. reply jhp123 7 minutes agorootparentwhy would you say \"there are two genders\"? You haven't heard of nonbinary gender?[0] [0] https://en.wikipedia.org/wiki/Non-binary_gender zb3 2 hours agorootparentprevSeems that if I write that \"X that feels Y is X and not Y\", then I'm apparently encouraging hate. But the real point is that I feel descriptions should belong to the person describing, not the person described.. How is this hate? reply Quinner 2 hours agorootparentprevYou can stay on twitter. reply iamdbtoo 3 hours agorootparentprevSo the victim here is the Babylon Bee and not the trans person they are mocking? reply zb3 2 hours agorootparentThe victim of censorship is the Babylon Bee, I don't see that trans person being censored. reply spacephysics 3 hours agoparentprevThey had a massive amount of reports that they can hardly keep up with. Their “safety” team will be costly and grow very large! https://www.theverge.com/2024/11/17/24298790/bluesky-moves-d... reply josefresco 3 hours agorootparent> Bluesky Safety team posted Friday that it received 42,000 moderation reports in the preceding 24 hours (versus 360,000 in all of 2023). This sounds more like an attack then a byproduct of a growth bump. reply pessimizer 2 hours agorootparentIt's mass reporting. It worked on old twitter, and it works on new old twitter. reply happytoexplain 3 hours agoparentprevWow, \"brutally censored\"? This is the first I've heard. What are you referring to? (I don't use Bluesky). reply hntempacct99 3 hours agorootparentCheck what the default moderation service is flagging. I understand that the standard Bluesky site moderates however they want, and that's fine, but this is a decentralized network right? So a productive discussion is to discuss what other relays and appviews are currently running where the users can pick and choose that algorithm, as content exclusion is perhaps the single most important part of any content algorithm, and the defaults on standard Bluesky are pretty locked down. reply happytoexplain 2 hours agorootparentThis? https://bsky.app/profile/moderation.bsky.app These looks like reasonable defaults - frankly I'm a bit delighted they are configurable. A lot of these are things most social platforms would outright ban without an opt-out. I think it makes sense to start medium-narrow and let users broaden it (not to mention it's kind to new users - though I understand that kindness is a bit dead in our culture currently, since it's been falsely accused of being mutually incompatible with having hard, real conversations). And I do get the pros and cons - I get the argument about starting broad and making the user narrow it down. But, specifically, I think \"brutally censored\" is pretty dramatic. reply rsynnott 2 hours agoparentprev> From what I have seen using Bluesky this isn't true at all. It's brutally censored, even more than Twitter was in 2021. Where are you getting that from? Do you mean blocklists? Like, you are not required to use blocklists. They are not even the default; you have to affirmatively use them. reply PittleyDunkin 3 hours agoparentprev> It's brutally censored, even more than Twitter was in 2021 ??? You can literally post porn on twitter. You could in 2021, too. Pretending it was censoring people seems asinine. reply tootie 2 hours agoparentprevPersonally, I'm 100% on board with heavy moderation. I think it's a complete myth that unfettered free speech will make for a useful platform. Spam, abuse, disinformation, hate speech. They all make the platform less valuable. reply zb3 2 hours agorootparent*as long as it fits my views reply tootie 2 hours agorootparentI think this isn't as hard to solve as you may think. There is definitely gray area to all of them, but there's plenty of stuff that is obviously unacceptable. On X right now, it is common to find straight up Nazi propaganda flourishing. That is within the bounds of free speech per the first amendment, but it's almost universally (excluding the actual nazis) derided as hate speech. I don't think banning Nazis is an impossible task nor is it a slippery slope. Getting it exactly right is impossible, but there's plenty you can do without controversy and a robust appeal process could mitigate any gray areas. https://www.nbcnews.com/tech/social-media/x-twitter-elon-mus... reply zb3 1 hour agorootparentIt is a slippery slope because there's a disagreement as to who qualifies as one. We might agree on a given definition, but then it might turn out that the person doing the moderation would regard majority of the population as Nazi, which would be ridiculous and'd actually obscure the actual tragedies that happened... reply hnpolicestate 3 hours agoparentprevBluesky is just another r/politics. Irrelevant to everyone but an extreme fringe minority of Western liberals. I'm surprised how much I see posts about it on HN. reply jazzyjackson 2 hours agorootparent> an extreme fringe minority of Western liberals We're on a silicon valley forum my guy, it's as western as it gets before wrapping around and becoming east again! reply kubb 3 hours agoparentprevFor anyone wondering, the \"brutal censorship\" is that a post making fun of a trans person is hidden (but can be clicked on and viewed) and flagged as intolerance. reply seneca 3 hours agoparentprev [–] Agreed. I spent about 10 minutes on bsky before deciding it was a blatant, seemingly intentional, echo chamber and abandoned any interest. reply vehemenz 3 hours agorootparentIt was in the beginning, but it's gotten better as more people have joined. Of course, some people just claim \"echo chamber\" when there's not enough political extremism, which seems like a false equivalence. reply happytoexplain 3 hours agorootparentprev [–] 10 minutes?? That sounds like only enough time to see \"too many\" opinions you disagree with and \"not enough\" that you agree with, which is the shallow definition of \"echo chamber\" (and probably the de facto definition at this point, unfortunately). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bluesky, a social network with over 20 million users, provides personalized control over social media algorithms, allowing users to choose their content.",
      "Unlike platforms like X (formerly Twitter), where algorithms dictate content, Bluesky offers a \"marketplace of algorithms\" for users to select feeds based on interests.",
      "This approach empowers users to shape their social media experience and could set a new industry standard, despite challenges in finding or creating the right feed."
    ],
    "commentSummary": [
      "Bluesky is launching customizable algorithms for social media, enabling users to select how they view content, either chronologically or through algorithmic sorting.",
      "The introduction of these features sparks a debate on whether algorithms improve or diminish user experience, with opinions divided between the need for user control and the advantages of curated content.",
      "Bluesky aims to balance user preferences and engagement by offering a chronological default feed while allowing for personalized content viewing options."
    ],
    "points": 175,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1732107595
  }
]
