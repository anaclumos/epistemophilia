[
  {
    "id": 42208383,
    "title": "Llama 3.2 Interpretability with Sparse Autoencoders",
    "originLink": "https://github.com/PaulPauls/llama3_interpretability_sae",
    "originBody": "I spent a lot of time and money on this rather big side project of mine that attempts to replicate the mechanistic interpretability research on proprietary LLMs that was quite popular this year and produced great research papers by Anthropic [1], OpenAI [2] and Deepmind [3].I am quite proud of this project and since I consider myself the target audience for HackerNews did I think that maybe some of you would appreciate this open research replication as well. Happy to answer any questions or face any feedback.Cheers[1] https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticit...[2] https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.04093[3] https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.05147",
    "commentLink": "https://news.ycombinator.com/item?id=42208383",
    "commentBody": "Llama 3.2 Interpretability with Sparse Autoencoders (github.com/paulpauls)497 points by PaulPauls 22 hours agohidepastfavorite71 comments I spent a lot of time and money on this rather big side project of mine that attempts to replicate the mechanistic interpretability research on proprietary LLMs that was quite popular this year and produced great research papers by Anthropic [1], OpenAI [2] and Deepmind [3]. I am quite proud of this project and since I consider myself the target audience for HackerNews did I think that maybe some of you would appreciate this open research replication as well. Happy to answer any questions or face any feedback. Cheers [1] https://transformer-circuits.pub/2024/scaling-monosemanticit... [2] https://arxiv.org/abs/2406.04093 [3] https://arxiv.org/abs/2408.05147 foundry27 19 hours agoFor anyone who hasn’t seen this before, mechanistic interpretability solves a very common problem with LLMs: when you ask a model to explain itself, you’re playing a game of rhetoric where the model tries to “convince” you of a reason for what it did by generating a plausible-sounding answer based on patterns in its training data. But unlike most trends of benchmark numbers getting better as models improve, more powerful models often score worse on tests designed to self-detect “untruthfulness” because they have stronger rhetoric, and are therefore more compelling at justifying lies after the fact. The objective is coherence, not truth. Rhetoric isn’t reasoning. True explainability, like what overfitted Sparse Autoencoders claim they offer, basically results in the causal sequence of “thoughts” the model went through as it produces an answer. It’s the same way you may have a bunch of ephemeral thoughts in different directions while you think about anything. reply stavros 18 hours agoparentI want to point out here that people do the same: a lot of the time we don't know why we thought or did something, but we'll confabulate plausible-sounding rhetoric after the fact. reply mdp2021 10 hours agorootparent/Some/ people bullshit themselves stating the plausible; others check their hypotheses. The difference is total in both humans and automated processes. reply catskul2 1 hour agorootparentEveryone, every last one of us, does this every single day, all day, and only occasionally do we deviate to check ourselves, and often then it's to save face. A Nobel prize was given for related research to Daniel Kahneman. If you think it doesn't apply to you, you're definitely wrong. reply mdp2021 1 hour agorootparent> occasionally Properly educated people do it regularly, not occasionally. You are describing a definite set of people. No, it does not cover all. Some people will output a pre-given answer; some people check. reply stavros 10 hours agorootparentprevHow are you going to check your hypotheses for why you preferred that jacket to that other jacket? reply mdp2021 9 hours agorootparentDo not lose the original point: some systems have a goal to sound plausible, while some have a goal to say the truth. Some systems, when asked \"where have you been\", will reply \"at the baker's\" because it is a nice narrative in their \"novel writing, re-writing of reality\", some other will check memory and say \"at the butcher's\", where they have actually been. When people invent explicit reasons on why they turned left or right, those reasons remain hypotheses. The clumsy will promote those hypotheses to beliefs. The apt will keep the spontaneous ideas as hypotheses, until the ability to assess them comes. reply DSingularity 9 hours agorootparentprevIs that example representative for the LLM tasks for which we seek explainability ? reply stavros 9 hours agorootparentAre we holding LLMs to a higher standard than people? reply f_devd 8 hours agorootparentIdeally yes, LLMs are tools that we expect to work, people are inherently fallible and (even unintentionally) deceptive. LLMs being human-like in this specific way is not desirable. reply stavros 8 hours agorootparentThen I think you'll be very disappointed. LLMs aren't in the same category as calculators, for example. reply f_devd 6 hours agorootparentI have no illusions on LLMs, I have been working with them since og BERT, always with these same issues and more. I'm just stating what would be needed in the future to make them reliably useful outside of creative writing & (human-guided & checked) search. If an LLM provides an incorrect/orthogonal rhetoric without a way to reliably fix/debug it it's just not as useful as it theoretically could be given the data contained in the parameters. reply LoganDark 13 hours agorootparentprevThe split-brain experiment is one of my favorites! https://www.youtube.com/watch?v=wfYbgdo8e-8 reply btbuildem 6 hours agorootparenthttps://en.wikipedia.org/wiki/Peace_on_Earth_(novel) reply sinuhe69 13 hours agorootparentprevNot in math. reply TeMPOraL 12 hours agorootparentYes in math. Formalisms come after casual thoughts, at every step. reply mdp2021 9 hours agorootparentIt's totally different: those formalisms are in a workbench, following a set of rules that either work or not. So, yes, that (math) is representative of the actual process: pattern recognition gives you spontaneous ideas, that you assess for truthfulness in conscious acts of verification. reply sinuhe69 9 hours agorootparentprevWhat is a casual thought that you cannot explain in math? reply TeMPOraL 8 hours agorootparentThat question makes no sense. You can explain anything in math, because math is a language and lets you define whatever terms and axioms you need at a given moment. (Whether or not such explanation is useful for anything is another issue entirely.) reply worldsayshi 7 hours agorootparentCan you explain how intuition led you to try a certain approach? reply TeMPOraL 6 hours agorootparentIs it enough if I hand-wave it with probability distributions, or do you want me to write out adjacency search in a high-dimensional space? reply snthpy 14 hours agoparentprevA{rt,I} imitating life I believe that's why humans reason too. We make snap judgements and then use reason to try to convince others of our beliefs. Can't recall the reference right now but they argued that it's really a tool for social influence. That also explains why people who are good at it find it hard to admit when they are wrong - they're not used to having to do it because they can usually out argue others. Prominent examples are easy to find - X marks de spot. reply jamesemmott 7 hours agorootparentI wonder if the reference you are reaching for, if it's not the Jonathan Haidt book suggested by a sibling comment, is The Enigma of Reason by the cognitive psychologists Hugo Mercier and Dan Sperber (2017). In that book (quoting here from the abstract), Mercier and Sperber argue that reason 'is not geared to solitary use, to arriving at better beliefs and decisions on our own', but rather to 'help us justify our beliefs and actions to others, convince them through argumentation, and evaluate the justifications and arguments that others address to us'. Reason, they suggest, 'helps humans better exploit their uniquely rich social environment'. They resist the idea (popularized by Daniel Kahneman) that there is 'a contrast between intuition and reasoning as if these were two quite different forms of inference', proposing instead that 'reasoning is itself a kind of intuitive inference'. For them, reason as a cognitive mechanism is 'much more opportunistic and eclectic' than is implied by the common association with formal systems like logic. 'The main role of logic in reasoning, we suggest, may well be a rhetorical one: logic helps simplify and schematize intuitive arguments, highlighting and often exaggerating their force.' Their 'interactionist' perspective helps explain how illogical rhetoric can be so socially powerful; it is reason, 'a cognitive mechanism aimed at justifying oneself and convincing others', fulfilling its evolutionary social function. Highly recommended, if you're not already familiar. reply snthpy 5 hours agorootparentThank you. That's exactly the idea and described much more eloquently. I probably heard it through the Sapolsky lecture from a sibling comment but that captures it exactly. Bookmarked. reply omgwtfbyobbq 12 hours agorootparentprevI think Robert Sapolsky's lectures on yt cover this to some degree around 115. https://youtu.be/wLE71i4JJiM?feature=shared Sometimes our cortex is in charge, sometimes other parts of our brain are, and we can't tell the difference. Regardless, if we try to justify it later, that justification isn't always coherent because we're not always using the part of our brain we consider to be rational. reply snthpy 5 hours agorootparentYes that was probably it because I rewatched that recently. Thanks! reply mdp2021 10 hours agorootparentprevAlready before Galileo we had experiments to determine whether ideas represented reality or not. And in crucial cases, long before that, it meant life or death. This will be clear to engineers. «Reason» is part of that mechanism of vetting ideas. You experience massive failures without it. So, no, trained judgement is a real thing, and the presence of innumerable incompetent do not prove an alleged absence of the competent. reply shshshshs 12 hours agorootparentprevPeople who are good at reasoning find it hard to admit that they were wrong? That’s not my experience. People with reason are.. reasonable. You mention X and that’s not where the reasoners are. That’s where the (wanna be) politicians are. Rhetoric is not all of reasoning. I can agree that rationalizing snap judgements is one of our capabilities but I am totally unconvinced that it is the totality of our reasoning capabilities. Perhaps I misunderstood. reply Hedepig 10 hours agorootparentThis is not totally my experience, I've debated a successful engineer who by all accounts has good reasoning skills, but he will absolutely double down on unreasonable ideas he's made on the fly he if can find what he considers a coherent argument behind them. Sometimes if I absolutely can prove him wrong he'll change his mind. But I think this is ego getting in the way, and our reluctance to change our minds. We like to point to artificial intelligence and explain how it works differently and then say therefore it's not \"true reasoning\". I'm not sure that's a good conclusion. We should look at the output and decide. As flawed as it is, I think it's rather impressive reply mdp2021 10 hours agorootparent> ego getting in the way That thing which was in fact identified thousands of years ago as the evil to ditch. > reluctance to change our minds That is clumsiness in a general drive that makes sense and is recognized part of the Belief Change Theory: epistemic change is conservative. I.e., when you revise a body of knowledge you do not want to lose valid notions. But conversely, you do not want to be unable to see change or errors, so there is a balance. > it's not \"true reasoning\" If it shows not to explicitly check its \"spontaneous\" ideas, then it is a correct formula to say 'it's not \"true reasoning\"'. reply Hedepig 43 minutes agorootparent> then it is a correct formula to say 'it's not \"true reasoning\"' why is that point fundamental? reply snthpy 5 hours agorootparentprevPeople with reason ... sound reasonable. I think some prominent people on X who are good at reasoning from First Principles will double down on things rather than admit their mistake. The other very prominent psychological phenomenon I have observed in the world is \"Projection\", i.e. the phenomenon of seeing qualities in other people that we have ourselves. I guess it is because we think others would do what we would do ourselves. Trump is a clear example of this - whatever he accuses someone else off, you know he is doing. Point here being that this doubling down on bad reasons in order to not admit my mistakes is something I've observed in myself. Reason does indeed help me to try and overcome it when I recognise it but the tricky part is being able to recognise it. reply fragmede 4 hours agorootparentprevThe smarter a person is, the better they are at rationalizing their decisions. Especially the really stupid decisions. reply briffid 13 hours agorootparentprevJonathan Haidt's The Righteous Mind describes this ín details. reply snthpy 5 hours agorootparentThanks reply benreesman 16 hours agoparentprevA lot of the mech interp stuff has seemed to me like a different kind of voodoo: the Integer Quantum Hall Effect? Overloading the term “Superposition” in a weird analogy not governed by serious group representation theory and some clear symmetry? You guys are reaching. And I’ve read all the papers. Spot the postdoc who decided to get paid. But there is one thing in particular that I’ll acknowledge as a great insight and the beginnings of a very plausible research agenda: bounded near orthogonal vector spaces are wildly counterintuitive in high dimensions and there are existing results around it that create scope for rigor [1]. [1] https://en.m.wikipedia.org/wiki/Johnson%E2%80%93Lindenstraus... reply txnf 15 hours agorootparentSuperposition code is a well known concept in information theory - I think there is certainly more to the story then described in the current works, but it does feel like they are going in the right direction reply drdeca 15 hours agorootparentprevWhere are you seeing the integer quantum Hall effect mentioned? Or are you bringing it up rather than responding to it being brought up elsewhere? I don’t understand what the connection between IQHE and these SAE interpretability approaches is supposed to be. reply benreesman 14 hours agorootparentPardon me, the reference is to the fractional Hall effect. \"But our results may also be of broader interest. We find preliminary evidence that superposition may be linked to adversarial examples and grokking, and might also suggest a theory for the performance of mixture of experts models. More broadly, the toy model we investigate has unexpectedly rich structure, exhibiting phase changes, a geometric structure based on uniform polytopes, \"energy level\"-like jumps during training, and a phenomenon which is qualitatively similar to the fractional quantum Hall effect in physics, among other striking phenomena. We originally investigated the subject to gain understanding of cleanly-interpretable neurons in larger models, but we've found these toy models to be surprisingly interesting in their own right.\" https://transformer-circuits.pub/2022/toy_model/index.html reply Onavo 18 hours agoparentprevHow does the causality part work? Can it spit out a graphical model? reply fsndz 18 hours agoparentprevI stopped at: \"causal sequence of “thoughts” \" reply benchmarkist 17 hours agorootparentInterpretability research is basically a projection of the original function implemented by the neural network onto a sub-space of \"explanatory\" functions that people consider to be more understandable. You're right that the words they use to sell the research is completely nonsensical because the abstract process has nothing to do with anything causal. reply HeatrayEnjoyer 15 hours agorootparentAll code is causal. reply benchmarkist 15 hours agorootparentWhich makes it entirely irrelevant as a descriptive term. reply mdp2021 9 hours agorootparentprev\"Servers shall be strict in formulation and flexible in interpretation.\" reply jwuphysics 18 hours agoprevIncredible, well-documented work -- this is an amazing effort! Two things that caught my eye were (i) your loss curves and (ii) the assessment of dead latents. Our team also studied SAEs -- trained to reconstruct dense embeddings of paper abstracts rather than individual tokens [1]. We observed a power-law scaling of the lower bound of loss curves, even when we varied the sparsity level and the dimensionality of the SAE latent space. We also were able to totally mitigate dead latents with an auxiliary loss, and we saw smooth sinusoidal patterns throughout training iterations. Not sure if these were due to the specific application we performed (over paper abstracts embeddings) or if they represent more general phenomena. [1] https://arxiv.org/abs/2408.00657 reply PaulPauls 17 hours agoparentI'm very happy you appreciate it - particularly the documentation. Writing the documentation was much harder for me than writing the code so I'm happy it is appreciated. I furthermore downloaded your paper and will read through it tomorrow morning - thank you for sharing it! reply Eliezer 16 hours agoprevThis seems like decent alignment-positive work on a glance, though I haven't checked full details yet. I probably can't make it happen, but how much would someone need to pay you to make up your time, expense, and risk? reply curious_cat_163 21 hours agoprevHey - Thanks for sharing! Will take a closer look later but if you are hanging around now, it might be worth asking this now. I read this blog post recently: https://adamkarvonen.github.io/machine_learning/2024/06/11/s... And the author talks about challenges with evaluating SAEs. I wonder how you tackled that and where to look inside your repo for understanding the your approach around that if possible. Thanks again! reply PaulPauls 17 hours agoparentSo evaluating SAEs - determining which SAE is better at creating the most unique features while being as sparse as possible at the same time - is a very complex topic that is very much at the heart of the current research into LLM interpretability through SAEs. Assuming you already solved the problem of finding multiple perfect SAE architectures and you trained them to perfection (very much an interesting ML engineering problem that this SAE project attempts to solve) then deciding on which SAE is better comes down to which SAE performs better on the metrics of your automated interpretability methodology. Particularly OpenAI's methodology emphasizes this automated interpretability at scale utilizing a lot of technical metrics upon which the SAEs can be scored _and thereby evaluated_. Since determining the best metrics and methodology is such an open research question that I could've experimented on for a few additional months, have I instead opted for a simple approach in this first release. I am talking about my and OpenAI's methodology and the differences between both in chapter 4. Interpretability Analysis [1] in my Implementation Details & Results section. I can also recommend reading the OpenAI paper directly or visiting Anthropics transformer-circuits.pub website that often publishes smaller blog posts on exactly this topic. [1] https://github.com/PaulPauls/llama3_interpretability_sae#4-i... [2] https://transformer-circuits.pub/ reply curious_cat_163 14 hours agorootparentThanks! reply samstevens 13 hours agoprevI’m really excited to see some more open SAE work! The engineering effort is non trivial and I’m going to check out your dataloading code tomorrow. You might be interested in an currently in-progress project of mine to train SAEs on vision models: https://github.com/samuelstevens/saev reply monkeycantype 15 hours agoprevThank you for posting this PaulPauls, can I please ask a wacky question that I have about mech.interp. ? we know that when we use a metric to measure humans, we game the metric, I wonder about future ai, gaming mech.interp. for simplicity let me propose a 2d matrix to encode tokens, and hope my understanding of neural networks is good enough for this to be a valid question I understand that higher dimensions have a lot more 'space', so accidental collisions are easily avoided, but 2d makes my example easier to explain. if we had the following tokens mapped in 2d space Apple 1a Pear 1b Donkey 2a Horse 2b it becomes impossible to understand if the neurons 1,2,a,b, all excited represents apple and horse or donkey and pear? I wonder if a future, vastly more competent AI overseeing its own training might use this form of ambiguity as means to create a model in which the weights are selected so the potential for encoding collisions exist, so that it is possible to deceive an mech.int. observer as to the true thoughts of the model, essentially enabling the ai to think in euphemisms? reply Majromax 2 hours agoparentWhat you propose is a harder AI safety scenario. You don't need a 'vastly more competent AI overseeing its own training' to elicit this potential problem, just a malicious AI researcher, looking for (e.g.) a model that's racist but that does not have any interperable activation patterns that identifiably correspond to racism. The work here on this Show HN suggests that this kind of adversarial training might just barely be possible for a sufficiently-funded individual, and it seems like novel results would be very interesting. reply vivekkalyan 13 hours agoprevThis is great work! Mechanistic interpretability has tons of use cases, it's great to see open research in that field. You mentioned you spent your own time and money on it, would you be willing to share how much you spent? It would help others who might be considering independent research. reply PaulPauls 6 minutes agoparentThank you, I too am a big believer and enjoyer of open research. The actual code has clarity that complex research papers were never able to convey to me as well as the actual code could. Regarding the cost I would probably sum it up to round about ~2.5k USD for just the actual execution cost. Development cost would've probably doubled that sum if I wouldn't already have a GPU workstation for experiments at home that I take for granted. That cost is made up of: * ~400 USD for ~2 months of storage and traffic of 7.4 TB (3.2 TB of raw, 3.2 TB of preprocessed training data) on a GCP standard bucket * ~100 USD for Anthropic claude requests for experimenting with the right system prompt and test runs and the actual final execution * The other ~2k USD were used to rent 8x Nvidia RTX4090's together with a 5TB SSD from runpod.io for various stages of the experiments. For the actual SAE training I rented the node for 8 days straight and I would probably allocate an additional ~3-4 days of runtime just to experiments to determine the best Hyperparameters for training. reply yangwang92 5 hours agoprevNice！ You did what I wanted. Have you tried to train SAE for vision encoder and language encoder? I am working on this idea. May we work together, let me initial an issue. reply imranhou 9 hours agoprevComing from a layman's perspective, a genuine question regarding: \"Implements SAE training with auxiliary loss to prevent and revive dead latents, and gradient projection to stabilize training dynamics\". I struggle to understand this phrase \"to prevent and revive \", perhaps this is simple speak to those that understand the subject of SAEs, but it feels a bit self contradictory to me, could anyone elaborate? reply PaulPauls 41 minutes agoparentJust bad wording from me, trying to combine too much information in 1 sentence. The auxiliary loss is supposed to prevent dead latents from occuring in the first place - therefore \"prevent dead latents\" - and it is also supposed to revive the latents that are already dead - therefore \"revive dead latents\". Now that I review that sentence again I see that I used 2 verbs on the same subject that could be interpreted differently depending on the verb. Me culpa. I hope you still gained some insights into it =) reply versteegen 9 hours agoparentprevA latent that is never active and hence doesn't (seem to) represent anything. A loss term to reduce the occurrence of that, and if it does happen, push it back to being active sometimes. reply imranhou 9 hours agorootparentSo basically preventing dead latents from occurring and whenever they do occur to possibly reviving them through the use of auxiliary loss term in the loss function? Thanks btw reply moconnor 8 hours agoprevFind a latent for the Golden Gate bridge and put a Golden Gate Llama 3.2 on HuggingFace. This will get even more attention and love, more so if you include link to a space to chat with it! Also, you didn't ask for suggestions but putting some interesting results / visualizations at the top of the README is a very good idea. reply JackYoustra 21 hours agoprevVery cool work! Any plans to integrate it with SAELens? reply PaulPauls 18 hours agoparentNot sure yet to be honest. I'll definitely consider it but I'll reorient myself and what I plan to do next in the coming week. I also planned on maybe starting a simpler project and maybe showing people how to create the full model of a current Llama 3.2 implementation from scratch in pure PyTorch. I love building things from teh ground up and when I looked for documentation for the Llama 3.2 background section of this SAE project then the existing documentation I found was either too superficial or outdated and intended for Llama 1 or 2 - Documentation in ML gets outdated so quickly nowadays... reply jaykr_ 21 hours agoprevThis is awesome! I really appreciate the time you took to document everything! reply PaulPauls 18 hours agoparentThank you for saying that! I have a much, much harder time documenting everything and writing out each decision in continuous text than actually writing the code. So it took a look time for me to write all of this down - so I'm happy you appreciate it! =) reply enterthedragon 11 hours agoprevThis is amazing, the documentation is very well organized reply Carrentt 13 hours agoprevFantastic work! I absolutely love all the documentation. reply batterylake 14 hours agoprevThis is incredible! PaulPauls, how would you like us to cite your work? reply PaulPauls 24 minutes agoparentThank you very much! I included a section at the bottom that provides a sample bibtex citation. I didn't expect this much attention so I didn't even bother with a License but I'll include a MIT license later today and release 0.2.1 reply S-Kaenel 9 hours agoprev [–] Amazing research!! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses a personal side project focused on replicating mechanistic interpretability research on proprietary Large Language Models (LLMs), inspired by recent work from companies like Anthropic, OpenAI, and DeepMind.",
      "The author is seeking feedback and engagement from the HackerNews community, indicating a collaborative and open approach to their research.",
      "The project is linked to recent academic publications, suggesting it is grounded in current scientific discourse and advancements in the field of AI interpretability."
    ],
    "commentSummary": [
      "Llama 3.2 Interpretability with Sparse Autoencoders is a project by PaulPauls that aims to replicate mechanistic interpretability research on proprietary Large Language Models (LLMs).- The project utilizes Sparse Autoencoders (SAEs) to explore the causal sequence of \"thoughts\" in models, drawing parallels to human reasoning.- This open research, available on GitHub, has initiated discussions on interpretability, human reasoning, and the challenges associated with evaluating SAEs, with PaulPauls welcoming community feedback."
    ],
    "points": 497,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1732221476
  },
  {
    "id": 42212644,
    "title": "A Marble Madness-inspired WebGL game we built for Netlify",
    "originLink": "https://5-million-devs.netlify.com/",
    "originBody": "Hello HN! We’re a small creative studio specializing in real-time 3D experiences. Netlify approached us to design and build an interactive experience to celebrate reaching 5 million developers.Inspired by the classic game Marble Madness, we created a gamified experience where users control a ball through playful, interactive levels. The goal was to blend marketing content with the look and feel of a game to engage users.The app is built with Three.js [1], using our custom render pipeline and shaders, and uses Rapier for physics simulation [2]. The 2D content is overlaid on the WebGL view using CSS 3D transforms for a seamless integration with the 3D view.We’d love for you to try it out and share your thoughts![1] https:&#x2F;&#x2F;threejs.org[2] https:&#x2F;&#x2F;rapier.rsEDIT: More info on this project here: https:&#x2F;&#x2F;www.littleworkshop.fr&#x2F;projects&#x2F;5milliondevs&#x2F;",
    "commentLink": "https://news.ycombinator.com/item?id=42212644",
    "commentBody": "A Marble Madness-inspired WebGL game we built for Netlify (5-million-devs.netlify.com)294 points by franck 6 hours agohidepastfavorite144 comments Hello HN! We’re a small creative studio specializing in real-time 3D experiences. Netlify approached us to design and build an interactive experience to celebrate reaching 5 million developers. Inspired by the classic game Marble Madness, we created a gamified experience where users control a ball through playful, interactive levels. The goal was to blend marketing content with the look and feel of a game to engage users. The app is built with Three.js [1], using our custom render pipeline and shaders, and uses Rapier for physics simulation [2]. The 2D content is overlaid on the WebGL view using CSS 3D transforms for a seamless integration with the 3D view. We’d love for you to try it out and share your thoughts! [1] https://threejs.org [2] https://rapier.rs EDIT: More info on this project here: https://www.littleworkshop.fr/projects/5milliondevs/ iandanforth 3 hours agoNice! My favorite challenge was avoiding the glowing white dots along the path. reply CSMastermind 2 hours agoparentYeah, they really should be some kind of powerup so you're incentivized to hit them. Right now learning about the company feels like a penalty which I doubt was the intent. Also for anyone who hits a dot and is confused how to get out of the information screen - you just press the arrows. I tried escape and clicking for longer than I would care to admit before I realized this. Super cool idea though. reply hailpixel 2 hours agoparentprevI also went for 0% speedrun challenge. reply AndrewStephens 57 minutes agoprevThis game is way better than it needs to be for a quick burst of advertising. Not only is the implementation fantastic, with perfect controls, but the level design is also great. I really enjoyed the multiple routes and the fact you can skip most of the advertising displays. It seems such a shame that this isn't a full game. Removing the advertising and adding more complex levels with puzzles would make for a perfect little distraction. reply bhaney 2 hours agoprevI think this is the first time I've ever seen an online game correctly tell me to use WARS keys for movement. Big props for handling non-qwerty layouts. Great job optimizing too. Runs totally smooth on my 2012 macbook and its decrepit HD 4000 iGPU, which is no small feat for web-games these days. reply guigui 2 hours agoparentThank you! As an azerty user, I know the pain of using websites that are only designed for US keyboards. If anyone's wondering, the getLayoutMap method from the Keyboard API is what we're using to handle international keyboard layouts. https://developer.mozilla.org/en-US/docs/Web/API/Keyboard/ge... reply guigui 5 hours agoprevI'm one of the developers who worked on this project. Happy to answer any questions. More info on the project here: https://www.littleworkshop.fr/projects/5milliondevs/ reply ianbooker 4 hours agoparentGreat work, it really captures the feeling of Marble Madness. Its maybe to deep of a thought, but I really fancy the spin of the marble, something that the original was not conveying as fancy as your version does. reply dole 33 minutes agorootparentAs a MM fan, I wanted to second this. Great work, engaging enough to make me finish it and wish there were more mechanics like the catapults and enemies. reply guigui 4 hours agorootparentprevThank you. The physics engine we're using (Rapier) really does most of the work to make the spin of the marble look realistic. But we spent quite some time tweaking the controls to make them as enjoyable as we could. reply forth_throwaway 3 hours agorootparentThe marble has the perfect amount of friction, I'm able to \"drift\" around corners which feels really nice. reply worldsayshi 3 hours agorootparentprevYou did a great job. The physics seems about as enjoyable as those of Rocket League and they must've have spent a considerably larger effort. reply khernandezrt 3 hours agoparentprevHow long till people start speed running this? reply nogridbag 2 hours agoparentprevThis was really fun. Right before this little hole on the second level, my macbook started running a bit slow and the collision detection somehow sucked the marble into the floor. https://imgur.com/ZANb1cT reply guigui 2 hours agorootparentPhysics collision bugs are more likely to occur during frame rate drops. We’ve tried to address this by implementing an auto-respawn mechanism to handle cases where the ball gets stuck inside a collider, but it seems this sometimes fails. Anyway, thank you for playing! reply runnr_az 4 hours agoparentprevThat’s really lovely and polished. Nice job… reply guigui 4 hours agorootparentThanks for playing! Glad you enjoyed it. reply suyash 3 hours agoparentprevNice work, love to know more about the technical bits: what framework did you use for 3D scene, objects? how did you handle camera movements to track the ball? What library for sound as that was a nice touch. How did you do physics? Thanks! reply guigui 2 hours agorootparentThanks. The WebGL rendering is based on Three.js. We're using Rapier for the physics simulation, and Howler for the audio. Our game engine is responsible for all the controls and updating things like the camera position (which follows the position of the ball at every frame). reply doctorpangloss 2 hours agorootparentWhy did you choose to do things this way instead of using Unity WebGL? It's okay if the reason is, \"because we make websites and the programming we know is Javascript\" or whatever. It doesn't have to be about some objective comparison, like optimization or whatever, which isn't going to be true or necessarily matter anyway. reply franck 2 hours agorootparentUnity WebGL is not supported on mobile and we needed the experience to be playable on both desktop and mobile browsers. However, mobile browsers will be supported with Unity 6 web exports, still experimental currently AFAIK, but that should become a viable option soon. reply doctorpangloss 2 hours agorootparent> Unity WebGL is not supported on mobile. Hmm... Unity WebGL has worked correctly on Mobile Safari since 2013. Support has probably been flawless since around 2019. It has been supported in all the ways that matter for a long time. reply vunderba 1 hour agorootparentI wasn't aware of that. The Unity 6 Preview announcement from just this year had a lot of stuff around iOS and Android browser support: From the article: Android and iOS browser support has arrived With Unity 6 Preview. Now, you can run your Unity games anywhere on the web, without limiting your browser games to desktop platforms. https://unity.com/blog/engine-platform/unity-6-preview-relea... reply franck 1 hour agorootparentprevI should have said that it's not officially supported. For client work, we prefer not to choose an engine that may not work on a few devices and which we have no ability to fix. reply proee 3 hours agoparentprevHow many people worked on this project and how long did it take to develop? Nice job! reply guigui 3 hours agorootparentBesides people from the Netlify team who helped write the content and worked on some back-end aspects, the design and development of the game took around 8 weeks for a team of two. reply itronitron 5 hours agoparentprevThere is a glitch on the momentum level, where the marble gets stuck behind a wall after dropping into a hole. reply paol 5 hours agorootparentI got stuck in the spiral slide on the same level. I got the impression framerate glitches are affecting the collision detection (common physics implementation pitfall). I could be wrong though. Still, very cool. Too cool to waste on marketing in fact :) reply franck 5 hours agorootparentYeah, sometimes the ball does some crazy things due to the way collision detection works. We tried to optimize and avoid most of the issues but it can happen. reply franck 5 hours agorootparentprevThere is code in place to respawn the ball if we detect that it's stuck inside a block or wall, which can occur due to frame drops during the physics simulation. I'll try to reproduce this issue. Thanks for reporting it! reply weightedreply 33 minutes agorootparentI ran into the same infinite respawn. Here's where it happened to me: https://imgur.com/RFxyl1Q reply tux3 1 hour agorootparentprevI seem to have dropped through a wall while taking an elevator on the elevation level, it keeps respawning me in the void, so that was a game over =) reply pbronez 3 hours agorootparentprevI hit the same thing. It killed me while I was dropping into the hole, then respawned me into the block, ending my game. reply jetbalsa 5 hours agoparentprevadd a built in timer and I bet people would speed run this thing reply guigui 5 hours agorootparentThanks for playing. Actually, your time is displayed once you finish the experience (there are 5 levels in total). During the project, we discussed adding a speed-run mode but ultimately had to drop this feature due to time constraints. However, we intentionally included some shortcuts in the level design with that intent in mind. reply dylan604 4 hours agorootparentthe purpose of the game was to force marketing upon the players. a speed run version would defeat the dwell time of the marketing on the screen. i'm sure the marketing department would not be a fan reply bobfunk 53 minutes agorootparentFor what it's worth our head of marketing was the one asking for a speedrun mode, but it just couldn't make the cut in terms of scope :) reply maroonblazer 3 hours agorootparentprevPerhaps make it such that once you've completed it at 'normal' speed all the marketing messages are disabled, enabling the speedrun. reply jetbalsa 4 hours agorootparentprevand physics bugs, there are a few edge spots you can clip into and really send yourself flying! reply yossi_peti 3 hours agoparentprevHow is the gameplay related to the information? The connection seems pretty contrived to me. reply franck 3 hours agorootparentThe glowing line represents a timeline of Netlify's milestones that you have to follow in order to discover their journey. No particular reason for the physics-based gameplay except to have a bit of fun. reply asadm 33 minutes agoprevI wish I had code access to this, I would make a multiplayer mode using https://docs.joinplayroom.com/usage/threejs (my project) reply pissflaps 4 hours agoprevThat was enjoyable. I wasn't at all interested in any of the \"Netlify facts\" but it was fun to push the marble around and I'm impressed by how smooth the experience was. Well done! reply deskr 1 hour agoprevIs there a page somewhere that tells me what netlify is on a technical level? All I see it marketing speak and I can't make sense of it. reply 6figurelenins 55 minutes agoparentTurnkey web hosting. Drag and drop your build folder, get a URL. Or point it to your git repo. reply Sn0wCoder 1 hour agoparentprevhttps://docs.netlify.com/platform/what-is-netlify/ reply Lukas_Skywalker 46 minutes agoprevThat's quite fun! I didn't know about \"Marble Madness\", but it reminded me of Cuboro [1], a (hardware!) toy that consists wooden blocks and allows people to create quite complicated marble runs that look very very similar to this game. [1]: https://de.wikipedia.org/wiki/Cuboro reply solatic 5 hours agoprevAny chance this could get open-sourced? This seems like a great example of a lot of stuff for which there are few tutorials currently. reply diggan 4 hours agoparent> This seems like a great example of a lot of stuff for which there are few tutorials currently. Not OP but, what exactly you feel like is missing tutorials? It's a nice little polished experience, but I don't think there is anything particularly innovative or difficult to build with the resources that exists today. Or is there something in particular that looks/seems difficult from what they shared? reply thih9 4 hours agorootparentI disagree, I think the \"nice little polished experience\" is the difficult part. In practice building something like this with resources that exist today can still mean a stream of issues specific to a given platform, browser, library, programming language, IDE, issues related to a combination of any earlier two and a yak that needs shaving[1]. Meanwhile this project is described as[2]: > fully optimized for both desktop and mobile browsers, with user controls and UI components tailored for each device, ensuring intuitive navigation and interaction across all platforms. If this process was easy and well documented, Netlify wouldn't hire an external agency. [1]: https://en.wiktionary.org/wiki/yak_shaving [2]: https://www.littleworkshop.fr/projects/5milliondevs/ reply diggan 3 hours agorootparent> I disagree, I think the \"nice little polished experience\" is the difficult part. Right, I agree, most of the time will be spent in the polish. But is there really no resources out there on how to polish? Assuming there isn't, what would you want a tutorial to contain to make it apply to a wider audience, as polish is typically hyper-specific to the project. > If this process was easy and well documented, Netlify wouldn't hire an external agency. Companies don't typically hire external agencies because something is difficult for them to do per se, but more that it would be wasteful for them to spend the time building something like that instead of focusing on things core to the business. FWIW: I'm asked parent about this in order to see if there are actual gaps in the available resources today for what parent wanted to do, hence the question to specify what exactly they're looking for. I guess \"how to polish\" is a valid answer, but again, there are resources out there to help understand how to approach that. reply solatic 2 hours agorootparentprevYou can't really compare the depth of resources that exists for something like React versus something like Three, which has a bunch of toy examples but no fully coherent experiences. Companies like Figma have shown that there is a huge appetite for solutions built on top of Canvas or WebGL, but if you don't have the privilege of working for one of these companies that built up lots of proprietary building blocks from scratch, it's much more difficult to get started. reply guigui 5 hours agoparentprevNo plans to open-source at the moment, but we intend to share some behind-the-scenes info in the future. reply cyberlimerence 3 hours agoprevBeautiful work, well done. It also made me remember a game I played a long time ago called Ballance[1]. Weird how memory works like that. [1] https://en.wikipedia.org/wiki/Ballance_(video_game) reply bambax 1 hour agoprevThis is absolutely amazing! Very well executed, congrats!! I was absolutely terrified at first that falling off would have me start again from the beginning, so I was very careful. Once I did fall and come back where I was I grew bolder which made it more fun. Maybe that should be advertised somewhere. (I'm still unsure what Netlify exactly is or what it does but this will make me want to find out!) reply adzm 4 hours agoprevThis was actually very fun, great work here! There was a PC game called hamsterball that I really enjoyed a long time ago; this brought back memories. reply NaolGBasaye 3 hours agoprevReally smooth, clean work. Didn't get a chance to monitor memory and network, but seemed pretty light, kudos! reply johtso 4 hours agoprevThe controls don't seem to work for me. AWSD, only up and down work, unless I try to go diagonally, then it just gets \"stuck\" moving forever. Arrow keys no directions work unless I hold multiple keys down at once, then it also gets stuck moving. M1 macbook pro, Arc browser reply guigui 4 hours agoparentThat's weird, the desktop controls are supposed to work regardless of your keyboard layout. Are you able to play with your arrow keys (which are also supported)? reply nusl 35 minutes agoprevThis is fun but it's buggy. I randomly glitch out and get forced to respawn :( reply DecoySalamander 46 minutes agoprevHave you considered using R3F for this project? Also curious, why did you implement a custom renderer pipeline? reply vunderba 1 hour agoprevNice job! You just need to have it so if the marble exceeds a certain Y velocity threshold that it shatters and a dust pan appears to sweep it up. reply maniacwhat 3 hours agoprevInteresting. This consistently crashes my chrome browser whenever I get to the first glowing white checkpoint. But it's not like any crash I've seen before, the page reverts to a google search result I was on this morning. And the whole page is flickering white. That tab was closed long ago, but it seems something in this gets back to that state in memory, maybe a buffer overflow somewhere or something? While on the google search result, the music from the game is still playing. If I open a new tab, the title of this tab changes from the google title to the netlify one, and vice versa if I change back. reply flanbiscuit 3 hours agoprevI've been enjoying the quality and aesthetics of your studio's work. I would love for you to build a complete game or a longer experience, rather than only for marketing. I love where art and programming intersect. I would love to be able to create experiences like this myself one day. reply guigui 3 hours agoparentThank you for the kind words. We’re glad you enjoy our work! We’d love to create a longer game someday, but making a living as a small indie studio in the gaming world is definitely challenging. Never say never, though! reply Jeremy1026 5 hours agoprevVery nice way to show off some of the history of Netlify while making it fun. Congrats on 5m. reply JohnMakin 1 hour agoprevI’ve heard people claim they beat the original marble madness, but I don’t believe them. reply techtalsky 1 hour agoparentThere's proof! https://www.youtube.com/watch?v=vskQsSJ_IJg&ab_channel=hirud... Watching a playthrough I was surprised to see I'd never seen anyone get past level two. reply pizzafeelsright 4 hours agoprevI am very curious how the physics feel realy-wordly for the most part at the mathematical level. Are there existing algos that define the gravitational pull of the \"Facts\" spots or was there a lot of tweaking? The 45 degree rotation does require more dual input than I care for which makes me wonder if that is a design choice. reply franck 4 hours agoparentThe physics engine we are using is Rapier 3D which does a lot of the heavy lifting, even though we had to tweak a lot the physics properties of the ball and surfaces in order to get something that felt right. For hotspots specifically, we implemented the magnet-like effect with custom code (by applying a force that pushes the ball toward the center and slowing it down at the same time) as there is no attractor primitive in Rapier. The dual input is indeed a consequence of our isometric-view design choice, which I agree may not be the easiest way to control the ball. But the 45 degree angle just looks cooler in our opinion. reply mrighele 4 hours agoparentprev> The 45 degree rotation does require more dual input than I care for which makes me wonder if that is a design choice. Well they say \"A Marble Madness-inspired WebGL game\" so there is not much choice about the rotation [1] [1] https://en.wikipedia.org/wiki/Marble_Madness reply coldpie 4 hours agorootparentThe NES Marble Madness port (and probably others) had a choice of control schemes, where the D-Pad is either mapped directly to the screen (Down is down) or mapped at a 45 degree angle (Down is down-right). I never could wrap my head around the latter, but I can see the benefit given the stage layout mostly uses 45 degree paths. reply egypturnash 1 hour agorootparentThe second scheme works pretty well if you can turn the controller 45º. I never had an NES but I am pretty sure some of the isometric games I played on the c64 and Amiga had this as an option. reply pizzafeelsright 4 hours agorootparentprevLooking through the linked docs I see the physics frameworks. Of all the programming I find the 3D gaming to be the most complex and unattainable at my current knowledge or intelligence level. reply munchler 2 hours agoparentprevFWIW, you can mentally remap the keys to partially eliminate dual input. E.g. Pressing down and right together as a single input moves the marble southeast. This considerably simplifies game play for me. reply dustinsterk 1 hour agoprevThis is great! Fantastic work. You should port a game like this to steamdeck, it would be a huge hit! reply Jean-Papoulos 3 hours agoprevAlright, alright. Where are the speedrunning leaderboards for this ? reply wizzwizz4 3 hours agoparentThere are definitely some neat skips available. reply kolleraa 4 hours agoprevGreat fun and very polished! I'd love to try this with accelerometer based controls. reply GistNoesis 4 hours agoprevGot stuck in a re-spawn loop : some collision detection failed in level 4 at the start of a standard 30° incline, the ball fell through, and was re-spawned at the same place resulting in falling again, locking me in loop. reply davidwparker 2 hours agoprevCute game. A lot more forgiving than Marble Madness though. Needs a good enemy marble to come crack you and hammers too. reply burnt-resistor 4 hours agoprevThe ASDF controls are absolutely backwards while the cursor controls work. reply qingcharles 2 hours agoparentIt's WASD on QWERTY. reply Helmut10001 3 hours agoprevThank you so much. This is a great game for my 6yo, it made his day! He loved it. We spent months looking for web games for 6yo without ads, dark patterns and distracting details, but this was the first one that really fit. I wish there would be more similar games. reply dylan604 3 hours agoparentHow is this without ads? It's sole purpose was to be a marketing tool and interrupts the game play with...ads. reply hugs 2 hours agorootparentThey possibly could/should add the qualifier of obnoxious ads. Many mobile ads are extremely obnoxious. Especially the ones that hide the \"X\" or place several \"X\"s in the interface so it's hard to tell where exactly to tap to get the ad to go away. This game is pretty classy and subtle in comparison. Kind of how Hacker News itself is an ad for YCombinator. reply dylan604 2 hours agorootparentit's cute how you've come to accept the less obnoxious ads as okay. it appears the obnoxious ads have served their purpose. reply wizzwizz4 3 hours agorootparentprevThese are ads in the original sense (to advertise something), rather than the modern sense (obnoxious attention-grabbing multimedia presentations); and all of them are skippable, save the victory screen. I do find it quite ironic, though, that the best ad-free video game I've seen this week is an advertisement. reply Helmut10001 1 hour agorootparentThis is exactly how I meant it. From the perspective of my 6yo, who can't really read yet or know what Netlify is, it was also never perceived as an Ad, but a game. reply muragekibicho 1 hour agoprevThere's a glitch in the velocity level. My marble keeps respawing and the game is stuck in a refresh loop reply biomcgary 1 hour agoparentOn the last level I ran into one of the raised blocks and tunneled into the cubes. I could still roll around inside, lol, but there was no way out. reply rc_mob 4 hours agoprevIts been a long time since someone built something like this for the web. Praystation was a long time ago. No one has really followed up since then. reply kodablah 2 hours agoprevQuestion unrelated to the game specifically - how does Netlify quantify the number of developers? How does it know/guess that it is 5 million? reply pininja 4 hours agoprevSuper fun. Loved the rubbery sound design. I’d want to watch the CEO host a speed run stream. reply guigui 3 hours agoparentActually, we used a recording of a basketball for the sound of the marble bouncing. It wasn’t our original intention, as we initially imagined the ball to have more of a metallic quality. However, the rubbery effect kinda works, I guess. :) reply pooper 1 hour agoprevthis gives me an idea... what if you keep everything about the game the same but change it to hjkl navigation for vim learners? reply noctane 4 hours agoprevGreat work. Are there any plans to convert some of the code to OSS? reply aliwoto 3 hours agoprevHello, really amazing work, well done! Just one curious question: have you made the background music yourself? if not, can I know the name? reply guigui 3 hours agoparentThank you! The entire soundtrack was composed by this artist: https://www.pond5.com/artist/avifauna reply pjmlp 2 hours agoprevNice game, although I would expect to be able to use the gyroscope on mobile devices. reply guigui 2 hours agoparentThe issue with using the gyroscope is that the Device Orientation API requires websites to request user permission first (at least on iOS Safari), which we feared might drive away too many visitors. reply pjmlp 41 minutes agorootparentIt could be a game setting, no need to be by default. reply Lerc 3 hours agoprevNice, I did manage to get a respawn point where it immediately died causing a loop. On Elevation. Still have no idea what netlify is or does. reply chenbin74851 1 hour agoprevI like it. I will share it with my friends. reply nightowl_games 4 hours agoprevGreat game! On mobile, feels like the joystick is a bit too sensitive. Ie: I move my finger a tiny amount and the ball goes flying. What's your company called? reply guigui 4 hours agoparentThanks! Agreed that the mobile experience can sometimes be a little difficult compared to the desktop version. It was hard to get right, and we may have set the ball speed a bit too high. Our studio is called Little Workshop. You can find more info about us here: https://www.littleworkshop.fr reply ivanjermakov 3 hours agoprevSpeedrun any% category coming soon? reply tempworkac 5 hours agoprevCould you explain more on: > The 2D content is overlaid on the WebGL view using CSS 3D transforms for a seamless integration with the 3D view. Maybe a simple example of this with code? reply guigui 5 hours agoparentWe use the CSS3DRenderer from Three.js which automatically applies CSS transforms to DOM elements. More info here: https://threejs.org/docs/index.html?q=css3D#examples/en/rend... And some code samples from the Three.js website: https://threejs.org/examples/?q=CSS3D#css3d_periodictable https://threejs.org/examples/?q=CSS3D#css3d_molecules reply dylan604 3 hours agoprevdoes anybody remember the name of the board game that you turned knobs on the outside of the box to tilt on the x/y axis to drive the marble around the board? reply avisser 3 hours agoparentPretty sure it was Labyrinth. reply dylan604 3 hours agorootparentThanks. Although, just searching for Labyrinth board game brings up things not what I was thinking about, but it was the correct name and got me there. Here's what I was thinking of (at least in type): https://www.sunnywood.net/product/60-hole-labyrinth/ reply shimonabi 58 minutes agoprevIt's buggy. I got stuck in the blue \"pool\" in the last stage, under one of the green ramps. reply Keyframe 4 hours agoprevi got a few questions: - you say it's built with three.js but you also use rapier. How does that work / integrate? I see one is JS frontend thing, the other rust engine - how did you design levels, with what? reply franck 4 hours agoparentThe rendering engine is using Three.js which is a WebGL library. The physics/collision detection code is using Rapier through a WebAssembly module available on npm [1], which means that it can be used on the web even though it's originally written in Rust. The levels were built inside the Unity Editor, then exported to FBX, then went through a pipeline based on Blender python scripting that optimized their geometry, assigned materials and exported them to GLTF (the final format that we load in the browser). [1] https://rapier.rs/docs/user_guides/javascript/getting_starte... reply Keyframe 3 hours agorootparentthanks for answering! Interesting you used unity for level layout. Interested to hear the advantage here. Considering you already use Blender down the pipe, how come you haven't used Blender for it or any other dcc app lile maya, max, whatever? reply franck 2 hours agorootparentThe main draw of the Unity Editor for us is how it auto-reloads assets, like 3D models, as soon as the asset file is updated. So the workflow is having your DCC app open in which you model things and export assets from, and Unity Editor to design your level where every model is always up-to-date. This is not possible with Blender because it contains all models inside a single .blend file, so assets must be manually re-imported each you change them. There is a Link feature in Blender but in my experience it's not as good as what Unity does out of the box. reply ivanjermakov 3 hours agorootparentprevHow did you assemble Rapier colliders from GLTFs? reply franck 2 hours agorootparentNothing complicated, we simply have initialization code that parses the GLTF scene on startup by iterating over the children of a specific group, and creating Rapier colliders for each of them (Triangle Mesh Colliders to be specific, in order to allow things such as curved ramps). Since their geometry is very simple, we can use directly the rendering geometry for the collider geo. reply skydhash 3 hours agoparentprevNot the dev, but Rapier has a JavaScript binding through WASM. And you can design the levels with a 3d tool like Blender, then script out the animation. reply markatkinson 4 hours agoprevThis is fantastic!. Time to start speed running! reply butz 2 hours agoparent4 minutes and 31 seconds, any%. Need to do some better routing. 4 minutes and 21 seconds after few more attempts. 3 minutes and 59 seconds. Sub 4 is good enough for me :) reply yhprum 1 hour agorootparentMy competitive side got the better of me and spent too long playing this :| 3 minutes 33 seconds after ~10 attempts routing is definitely fun, I enjoyed figuring out which bounce pads to take --- 3:20 after some more tries, I think sub 3 is possible 3:14 even with the game glitching me into the abyss on the last level :( reply markatkinson 4 hours agoparentprev4 minutes 58 seconds. reply whitehexagon 59 minutes agorootparentI'll post my slowest time at the bottom of the stack: 5 minutes and 59 seconds didnt work on FF, and only 1/4 screen on brave index-dffbfc39.js:4603 expected expression got ? Anyway great fun, and much easier than what I remember of the Amiga version, very forgiving controls, thanks and well done. reply 725686 4 hours agoprevIt needs more gravity for the marble to roll down slopes better. reply ivanjermakov 3 hours agoparentSeems like the ball is 0.5m in diameter, if you treat single wireframe texture tile as 1m. Gravity seems to be correct for the ball of this size (although linear dampening aka air resistance is quite high). I think this is intentional, since higher gravity/smaller ball would significantly raise difficulty. reply eddyvinck 3 hours agoprevPretty fun simple little game! reply yazzku 1 hour agoprevI'm here just for the game. If you move fast enough, you can glitch the ball against the slides and get stuck. reply fragmede 5 hours agoprevThat was fun, thanks! reply blendertom 3 hours agoprevThis is amazing! reply pbronez 3 hours agoprevI got stuck in the second chapter. Went into a tube leading to a spiraling green slide and my ball reset... to the interior of the block. Oh well, cool project. reply franck 3 hours agoparentThanks and sorry you got stuck, we made our best effort to prevent these situations from happening but apparently they still do. reply Traubenfuchs 4 hours agoprevOn iOS, sometimes it scrolls the page and pressing and holding opens a weird right click magnifier and releasing it a share option on the top left? Those jarring little things seem to just never disappear from modern browser games. Beyond that it‘s amazingly fluid. reply guigui 4 hours agoparentWe tried to implement a workaround for that [1], but for some reason it still shows up from time to time. I really wish iOS Safari gave developers a way to disable these gestures! [1] https://discourse.threejs.org/t/iphone-how-to-remove-text-se... reply Traubenfuchs 3 hours agorootparentAlso there are spots from which you can fall that lock you in a revival loop. https://imgur.com/a/L5PV0HX I am very sure scrolling is related to popups when you roll over popup points. reply bobfunk 2 hours agoprevNetlify CEO here. I spotted Little Workshop when I saw https://equinox.space/ on Hacker News and noticed it was running on Netlify. Loved the fluidness, speed and art direction of a game running directly in the browser and working smoothly on my phone. Immediately thought of them when we started thinking about a 5 million developer celebration and reached out. Love the result :) reply MattSayar 35 minutes agoparentI knew I recognized the feel of this project. It did quite well on the front page [0]. Hopefully you and others keep hiring them so we can keep enjoying their work! [0] https://news.ycombinator.com/item?id=40113013 reply guigui 1 hour agoparentprevWe’re very grateful for the opportunity to create this experience! Huge thanks to you and the Netlify team for supporting innovative campaigns like this one. reply trollbridge 4 hours agoprev [–] Well, I'd love to know what Netlify does, but... #1. I could not find pricing anywhere. #2. The \"ROI calculator\" steered me to enter in my name, e-mail, and phone number. I don't want to sign up to get spam from a salesman just to find out the basics about some tool or platform. #3. Wikipedia's page for Netlify has a content warning that the content appears to be an ad brochure, but at least it said this: \"Netlify is a remote-first cloud computing company that offers a development platform that includes build, deploy, and serverless backend services for web applications and dynamic websites. The company enables building, deploying, and scaling websites whose source files are stored in the version control system Git and then generated into static web content files served via a content delivery network.\" Still have no idea what Netlify does (beyond what I can already do with git with a few clicks), or if it's right for our team, or if we can even afford it. The Marble game was quite fun, however... #4. The main thing that stuck in my mind from the little \"milestones\" about Netlify was that they changed their logo. This may seem significant to the Netlify team, but is completely irrelevant to the rest of us. #5. The second thing was that they \"bought Squirrel, an open source\"... it is rather dystopian to hear that someone \"bought\" an open source platform. Since we have a few Netlify people posting here, please feel free to correct my ignorance or point me in the right direction. reply throwaway77385 4 hours agoparentUsed Netlify back in the day (prior to Cloudflare pages / workers sites). The experience was largely smooth. HOWEVER, pricing was both opaque and prone to explode without warning, with little to no way of setting billing limits. Ultimately, that was too risky for the kind of small-ish projects I'm running. They had the Netlify CMS for a while, which I quite liked. But that's gone now. Be interesting to know what their USP is over CF Pages. reply fastball 4 hours agorootparentNetlify is Vercel before Vercel. reply hunter2_ 2 hours agorootparentNetlify being related to Gatsby and Vercel being related to Next.js. reply markerz 4 hours agoparentprev [–] > The company enables building, deploying, and scaling websites whose source files are stored in the version control system Git and then generated into static web content files served via a content delivery network. That’s the meat of it. It’s Heroku for statically generated websites or websites that can run as lambdas. Pretty limited but very fast for those purposes cause everything is handled by edge servers rather than primary data center servers. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A creative studio developed an interactive game for Netlify to celebrate reaching 5 million developers, inspired by the classic game Marble Madness.- The game utilizes Three.js for 3D graphics and Rapier for physics, integrating 2D content with CSS 3D transforms to create a seamless experience.- The studio invites users to try the game and provide feedback, highlighting the blend of marketing content with a game-like experience."
    ],
    "commentSummary": [
      "A WebGL game inspired by Marble Madness was developed for Netlify to commemorate reaching 5 million developers, combining gameplay with marketing content.- The game utilizes Three.js for 3D graphics and Rapier for physics, integrating 2D content through CSS 3D transforms.- Feedback is encouraged to enhance the user experience."
    ],
    "points": 295,
    "commentCount": 144,
    "retryCount": 0,
    "time": 1732271516
  },
  {
    "id": 42213663,
    "title": "Salmon return to lay eggs in historic habitat after dam removal project",
    "originLink": "https://www.opb.org/article/2024/11/17/salmon-return-to-lay-eggs-in-historic-habitat-after-largest-dam-removal-project-in-us-history/",
    "originBody": "In The News St. Helens investigation Your school's report card 🏫 Owyhee Canyonlands Bird flu hits NW farms Superabundant dispatch 'OPB Politics Now' Science & Environment Salmon return to lay eggs in historic habitat after largest dam removal project in US history By HALLIE GOLDEN (Associated Press) Nov. 17, 2024 4:15 p.m. Less than a month after four towering dams on the Klamath River were demolished, hundreds of salmon made it into waters they have been cut off from for decades to spawn in cool creeks A giant female Chinook salmon flips on her side in the shallow water and wriggles wildly, using her tail to carve out a nest in the riverbed as her body glistens in the sunlight. In another moment, males butt into each other as they jockey for a good position to fertilize eggs. These are scenes local tribes have dreamed of seeing for decades as they fought to bring down four hydroelectric dams blocking passage for struggling salmon along more than 400 miles (644 kilometers) of the Klamath River and its tributaries along the Oregon-California border. THANKS TO OUR SPONSOR: Become a Sponsor Now, less than a month after those dams came down in the largest dam removal project in U.S. history, salmon are once more returning to spawn in cool creeks that have been cut off to them for generations. Video shot by the Yurok Tribe show that hundreds of salmon have made it to tributaries between the former Iron Gate and Copco dams, a hopeful sign for the newly freed waterway. “Seeing salmon spawning above the former dams fills my heart,” said Joseph L. James, chairman of the Yurok Tribe. “Our salmon are coming home. Klamath Basin tribes fought for decades to make this day a reality because our future generations deserve to inherit a healthier river from the headwaters to the sea.” FILE - Excess water spills over the top of a dam on the Lower Klamath River known as Copco 1 near Hornbrook, Calif. Gillian Flaccus / AP The Klamath River flows from its headwaters in southern Oregon and across the mountainous forests of northern California before it reaches the Pacific Ocean. The completion of the hydroelectric dam removal project on Oct. 2 marked a major victory for local tribes. Through protests, testimony and lawsuits, the tribes showcased the environmental devastation caused by the dams, especially to salmon, which were cut off from their historic habitat and dying in alarming numbers because of poor water-quality. There have been lower concentrations of harmful algae blooms since the dam removal, Toz Soto, fisheries program manager with the Karuk Tribe, said during a press conference after the dams came down. In October, the water temperature during the day was an average of 8 degrees Celsius (14 degrees Fahrenheit) cooler compared to the same month over the last nine years, according to the Klamath River Renewal Corporation, the nonprofit entity created to oversee the project. THANKS TO OUR SPONSOR: Become a Sponsor “All in all, the fish that came up this year were really healthy,” Soto said. “I didn’t see fish with bacterial infections and things like that, so water temperature’s already having an impact on the fishes’ health.” The number of salmon that have quickly made it into previously inaccessible tributaries has also been encouraging. Experts have counted 42 redds, or salmon egg nests, and have tallied as many as 115 Chinook salmon in one day in Spencer Creek, which is above the former J.C. Boyle dam, the furthest upstream of the four removed dams, said Mark Hereford with the Oregon Department of Fish and Wildlife. “They’re showing us where the good habitat is; they’re showing us where there’s a lack of habitat,” said Barry McCovey Jr, director of the Yurok Tribal Fisheries department. “So we can use these fish to inform us as river managers, as scientists, where restoration needs to take place.” FILE - A view shows the Copco 1 Dam in Hornbrook, Calif., Sunday, Sept. 17, 2023. Haven Daley / AP Power company PacifiCorp built the dams to generate electricity between 1918 and 1962. But the structures halted the natural flow of the waterway that was once known as the third-largest salmon-producing river on the West Coast. They disrupted the lifecycle of the region’s salmon, which spend most of their life in the Pacific Ocean but return to the chilly mountain streams to lay eggs. At the same time, the dams only produced a fraction of PacifiCorp’s energy at full capacity, enough to power about 70,000 homes. They also didn’t provide irrigation, drinking water or flood control, according to Klamath River Renewal Corporation. McCovey said the return of so many salmon happened faster than he had expected and makes him hopeful for the future of the river. “Out of all the milestones that we’ve had, this one to me is the most significant,” he said. “It feels like catharsis. It feels like the right path.” ___ Associated Press reporter Sophie Austin contributed to this report. THANKS TO OUR SPONSOR: Become a Sponsor THANKS TO OUR SPONSOR: Become a Sponsor 📨 Daily news in your inbox Sign up today for OPB’s “First Look” – your daily guide to the most important news and culture stories from around the Northwest. Email Please leave this field blankSign up Related Stories Fish biologists collaborate to track pioneering Klamath River salmon Chinook salmon are spawning in streams above four former dam sites on the Klamath River in numbers that are astounding biologists. Now, a network of tribes, agencies, university researchers, and conservation groups is working together to track the fish as they explore the newly opened habitat. Nov. 3, 2024 Salmon return to Klamath Basin in Oregon after more than a century The Oregon Department of Fish and Wildlife spotted the fish on a tributary above the former J.C. Boyle Dam site, just north of the California border. Several more soon followed it. Oct. 18, 2024 Salmon swim freely in the Klamath River for 1st time in a century after dams removed For the first time in more than a century, salmon are swimming freely along the Klamath River and its tributaries near the California-Oregon line Oct. 7, 2024 Tags: Salmon, Klamath River, Dams",
    "commentLink": "https://news.ycombinator.com/item?id=42213663",
    "commentBody": "Salmon return to lay eggs in historic habitat after dam removal project (opb.org)219 points by gmays 5 hours agohidepastfavorite145 comments netcraft 4 hours ago>Less than a month after four towering dams on the Klamath River were demolished, hundreds of salmon made it into waters they have been cut off from for decades to spawn in cool creeks Do we understand the mechanisms of this \"genetic memory\" (my words, no idea if its accurate or if there is a better word for it)? Butterflies knowing where to fly even though it was their grandparents that last did it - eels traveling thousands of miles to breed in a place theyve never seen - countless bird migrations - even something as simple as how it takes a human baby 12-18 months to walk but many animals walk as soon as they are born. I would love to understand better how this knowledge is inherited reply joe_the_user 8 minutes agoparentIf some salmon group had been simplistically \"programmed\" to go up these waters, they would have been trying and failing to go up the river during the entire time the dam was there and so likely wiped out as a group/subspecies. It seems like the fish would have to have had some kind of way to test if the river lead to adequate spawning grounds. And if they had that, they wouldn't really need any memory of any given river. reply tokai 3 hours agoparentprevLooking at salmon research literature I found a study[0] with the following conclusion: This study provides convincing empirical support for fine-scale local selection against dispersal in a large Atlantic salmon meta-population, signifying that local individuals have a marked home ground advantage in reproductive fitness. These results emphasize the notion that migration and dispersal may not be beneficial in all contexts and highlight the potential for selection against dispersal and for local adaptation to drive population divergence across fine spatial scales. Seems like it might simply be that they go where they adapted to thrive. [0] https://www.science.org/doi/full/10.1126/sciadv.aav1112 reply tsimionescu 1 hour agorootparentThat doesn't really explain how they know to find this place, decades after the last time any member of their species visited it. It explains why evolution selected for this behavior, but the more interesting part is how it happens in an individual salmon. reply RaftPeople 16 minutes agorootparentThis is total and complete speculation, but possibly some sort of genetic or epigenetic driven system favoring some sort of chemical gradient/fingerprint unique to each river, maybe? reply idunnoman1222 1 hour agorootparentprevClearly, the story that salmon go back to spawn and their birth pool is not 100% true reply kranner 4 hours agoparentprevThere could be an environmental feature they prefer in that spot. Edit: the article mentions lower concentration of harmful algae and a cooler temperature. reply joseda-hg 4 hours agorootparentBut then how are they aware of those conditions Also, the preference usually is more on the side of where they're born vs optimal proper placement reply chmod775 3 hours agorootparentNice water flows downstream, terminates in the ocean. They simply follow it back upstream. reply jagged-chisel 3 hours agorootparentI’m with you on this. Found some tasty water? Swim towards it. It gets tastier the further we go? Keep going. reply monknomo 2 hours agorootparentYeah, no need to make this complicated. reply Angostura 40 minutes agorootparentprevThanks, I was bang my head on this one, until you suggested a nice simple solution reply ssnistfajen 34 minutes agoparentprevMy 100% speculation is emergent behaviour from the brain itself. Same way human interactions have remained largely the same over thousands of years. Also, we don't notice the salmon that swam up dead ends elsewhere. reply conradev 3 hours agoparentprevThe book Bird Sense by Tim Birkhead covers birds’ magnetic sense in Chapter 6. Research has demonstrated that seabirds have a magnetic map and compass that they use to navigate home, but it doesn’t discuss how this knowledge is inherited. I believe Salmon use a similar mechanism, but it might be supplemented with chemical signatures. For Salmon, it’s possible that they genetically inherit the capability but learn the location at birth. reply Aurornis 3 hours agorootparent> seabirds have a magnetic map and compass that they use to navigate home, but it doesn’t discuss how this knowledge is inherited. It’s not something that was decided by one ancestor and then inherited by everyone else. It was something that certain birds had a tendency to prefer. Those birds thrived and reproduced at a higher rate, while birds without that preference presumably found less suitable homes. It’s just natural selection and normal genetic variance. Some offspring every year will be born with slightly difference preferences due to the influence of various genetic differences. Some of those differences will be more beneficial for finding a good “home”, others less so. There was a recent report of a very confused penguin showing up on a beach far from their normal habitat. Apparently this happens every once in a while. Those cases did not win the genetic lottery (though hopefully it made it back to a more suitable climate) reply s1artibartfast 2 hours agorootparentFor animals like seabirds, a big part of the location could be non-genetic, as birds have different home roosts. I would add that there can be many local maxima, so it isnt always about finding less suitable homes. Birds of the same species can have different homes. reply shkkmo 3 hours agorootparentprevSalmon do use magnetic senses to navigate the oceans as well, but it is an acute sense of smell (among other things) that allows (most of) them to return to the headwaters of their birth. reply idunnoman1222 1 hour agorootparentNone of those salmon were born there because the Damn was in the way reply BurningFrog 2 hours agoparentprevIt's not a genetic memory. They return to the place they were born. This is probably based on the \"scent\" of that place, and maybe other factors. Some percentage either accidentally or deliberately go up a different river, which is how the species spreads. That's very likely who this story is about. reply EasyMark 2 hours agorootparentI think the point is if they \"return to the place they are born\" then why would they go back to the waterways freed up by destroying this damn. Clearly they have some heirachy in where they prefer to spawn and this place is at or near the top, or they would have opted to return to where they were born reply BurningFrog 58 minutes agorootparentMost of them go back to their birthplace, but some end up elsewhere. If that's a \"deliberate\" evolutionary strategy or just that 100% navigation success doesn't happen is unknowable. reply piuantiderp 33 minutes agorootparentprevMight just be some kind of salinity thing. Upstream -> Less minerals dissolved reply jncfhnb 3 hours agoparentprevHuman babies physically cannot walk. It’s not merely a knowledge check. Pretrained brain modules aren’t the most surprising thing. Humans have plenty of pre trained behaviors, some of which kick in a while after birth. reply DFHippie 3 hours agorootparent> Human babies physically cannot walk. It’s not merely a knowledge check. They physically cannot walk, but they also don't know how to. We know this because they need to practice and acquire skill. If they are deprived of opportunity to learn but their body continues to mature, their mature body does not give them the mature skill. reply Retric 2 hours agorootparentPractice itself is an instinctual behavior. Evolution isn’t limited to direct methods, as long as it works that’s enough. reply MisterBastahrd 2 hours agorootparentprevI never crawled. My parents were worried, they went to doctors who assured them that I was mostly alright, and then one day, I got up and started walking. I saw the exact same behavior with my ex-gf's sister's son, who we took in after he was in foster care from birth. The child had clearly not been engaged with properly... the back of his head was bald because he was always on his back in a cramped bassinette and at 11 months he hadn't even learned to turn over. Within 3 months of being with us, he was walking. reply shkkmo 3 hours agoparentprev> Do we understand the mechanisms of this \"genetic memory\" I don't think there is particular evidence for \"genetic\" memory here. The salmon were already further down river, they just kept swimming upstream. While most salmon do return to the place of their birth, a small percentage always stray, which is how salmon are able to colonize new habitats and survive things like ice ages. reply jimnotgym 1 hour agorootparentExactly that. They also need the right kind of gravel to spawn in. The kind you find in mountain streams. Glad they are doing well. reply Aurornis 3 hours agoparentprev> Do we understand the mechanisms of this \"genetic memory\" (my words, no idea if its accurate or if there is a better word for it)? It’s not actually a memory that gets encoded in genes. It’s a tendency to behave in certain ways as influenced by combinations of genes Ancestors who had the same tendencies, drives, and preferences would have some similar behaviors, resulting in some of them going toward the same places. So not an actual memory that gets inherited, more like personality traits (but in a more general sense) that lead to similar outcomes. There is a field of epigenetics which studies heritable changes in cells that occur without DNA alteration, but these signals are much simpler than memories and not a mechanism for carrying memories across generations. A lot of pseudoscience has been written around epigenetics right now so you have to be careful about where you source info on this. reply snowwrestler 3 hours agoparentprevThe simplest answer in this specific case is that there is no genetic memory involved, and salmon will just swim upstream into any fresh water stream they come across. reply lawlessone 2 hours agorootparentCould be very very simple. Swim until you can't anymore? Swim until the current is very weak? Swim until the water smells/tastes nice? Someone could probably simulate these and see which matches reality the most. reply locallost 40 minutes agoparentprevThe story about eels is especially fascinating. I was told in my fishing course they can even get across small patches of land to continue on their journey. I did not bother to fact check it though. reply astura 4 hours agoparentprev> how it takes a human baby 12-18 months to walk but many animals walk as soon as they are born. This is because humans are born with, comparably, extremely immature brains. The animals that can walk after birth are born with more mature brain development than humans are born with, so they are capable of walking. https://www.livescience.com/9760-study-reveals-infants-walk.... reply evilduck 3 hours agorootparentIt's not completely brain development, look up the stepping reflex in human babies. Humans are just as neurally pre-wired to walk as foals are on day one but we're also born long before we're anywhere near strong enough to do it, it takes at least another 6 months of physical growth and strengthening out of the womb before babies even try. reply netcraft 4 hours agorootparentprevsure - but how did a horse foal learn how to walk within an hour of their legs being in contact with the ground? Or even for human babies, how are they hard wired to search for milk or even breathe? reply gherkinnn 4 hours agorootparentHumans and horses don't share the same evolutionary pressures. A foal gets eaten if it can't walk right away, we don't. Evidently our super brains are worth all the hassle. Unsatisfactory answer, maybe. reply gambiting 4 hours agorootparentprev>>or even breathe The same way your heart \"knows\" how to beat - it's a lower level function that happens without your conciousness. That's why people who are brain dead still live and breathe and swallow and digest and their hearts livers and kidneys still do their job. >>how are they hard wired to search for milk The ones who didn't died, to put it bluntly. Obviously not human babies, this evolutionary step happened long long time before the earliest hominids. reply JumpCrisscross 3 hours agorootparent> same way your heart \"knows\" how to beat - it's a lower level function that happens without your conciousness Heart cells in a Petri dish will happily beat away. reply s1artibartfast 2 hours agorootparentprevThat isn't an explanation of how it works. This is kinda like explaining how a car works with \"you fire and replace engineers until it moves\". reply netcraft 3 hours agorootparentprevtotally - but to be clear the question I have is more like \"where in the body is this knowledge encoded (for lack of a better term)\" Do you have neurons in your brain that are pre-wired for these things? Is that encoded in your DNA? Like physically how is it inherited and the selective pressures applied? reply zamfi 3 hours agorootparentYes, yes, and you got it. Largely it’s DNA that controls development of neurons/muscles/etc. that mediate nursing, walking, and so on. On selective pressures: human babies that aren’t born with the ability to nurse, or foals born without the ability to walk—because their in-utero development didn’t allow it—historically don’t survive, and thus don’t reproduce. reply detourdog 3 hours agorootparentprevI think it's a chemical structure reacting to an energetic stimulus. reply MrMcCall 3 hours agoparentprevI know it's going to sound like a bunch of hooey, but information really is the most intrinsic element of all aspects of this universe, especially when it comes to life. The life force is a thing that is interrelated with our physical bodies, but is not the physical body. It's just like the zen concept of \"Not 2, not 1\". Our minds have the same relationship with our brains. They're not separate, they're not the same; they're interrelated. That we can't \"see\" the other side of the connection with our science is due to our science being built with our physical world's constituents (matter & energy), thus those other dimensions are immeasurable with our science's tools. Rupert Sheldrake speaks of this when he says that the genetic code's protein construction genes do not and can not account for the resulting organism's shape. That coordinated construction requires a separate guiding force. That interrelationship is similar to the \"memory\" that creatures such as salmon have, which is intrinsic to their entire being, not just their physical body, which is only half of our being's totality. reply snowwrestler 3 hours agorootparentWhether or not this is “true,” it’s not explanatory. Someone asked how a thing works, and the answer above is essentially just restating that it does in fact work, for some ineffable, immeasurable reason. So while interesting to think about, it’s not a useful response to the question. reply MrMcCall 2 hours agorootparentWe understood that Mercury's orbit was wrong per Newton's laws long before Einstein came along to explain to us why. Whether or not something is true is always the beginning of a scientific exploration. reply snowwrestler 2 hours agorootparentOf course, but if we don’t know how something works, it’s ok to just say “we don’t know yet.” There may in fact be physical, measurable mechanisms that govern these types of animal behavior. Just like there was a physical, measurable explanation for Mercury’s orbit. reply MrMcCall 2 hours agorootparentYes, but it was Einstein's imagination that provided the theoretical framework that allowed the longstanding physical measurement to line up. If his imagination was limited by Newton's laws, he would have never come up with GR. If he had said that mass causes time to vary, he would have been laughed out of the room, with much ad hominem shouting. What I'm saying here is that we need to push beyond our current scientific paradigms to find out how these inexplicable corner cases actually work. As well, I do realize that the depth of exploration required will be further than most people are willing to plumb, which is demonstrated by the in-their-feelings reactions to my ideas. reply cruffle_duffle 2 hours agorootparentprevAnd now dark matter is throwing a wrench in Einstein’s stuff. Like Newton’s laws, Einstein’s stuff gets is mostly right (impressively so, even!) but breaks inside black holes and doesn’t seem to exactly line up with what we observe about what keeps galaxies in tact. And I’m sure whatever we discover that “solves” for dark matter will eventually start showing cracks as well, prompting another deep inquiry into the nature of our universe. Good times. reply MrMcCall 1 hour agorootparent5/6ths of the universe's matter is missing, or thereabouts. That fact aligns with there being six vibrationally distinct dimensions in our 3-space (our physical dimension being just one of them, our soul inhabiting its counter-dimension, all things in our universe having been created in pairs). The matter/energy from each dimension are distinct, so we can't detect the others using instruments made with ours, yet -- somehow, I don't know how -- the mass combines to contribute to the gravitational inertia that keeps the galaxies from flying apart. That said, when we slam particles together at high enough energies, we do see crossover (briefly) in the form of anti-particles. I couldn't begin to explain the mechanisms behind this, but the structure can be known to seekers of compassionate existence. This is also a hint to the solution to the question of why, after the Big Bang, we don't have an anti-matter left; the answer is that it's where it is, but that we can't detect it with our current tech (or maybe any tech, for all I know). The universe was made to be known by we human beings, we being the information processors designed to work in harmony with this information-theoretic universe, which is fully queryable by a suitable trained mystic. A Sufi Murshid (teacher) lived his entire life in a single town that consisted of a single pair of roads that met in the center of town. Late in his life, he stated that, he \"knew the stars of the Milky Way better than he knew his town\". (A love-consumed mystic remains conscious as our souls leave our bodies when we sleep. What is called astral travel is not limited by our physical body's speed laws; it is bounded only by the \"speed of thought\".) Sufi stories are glimpses of corner cases meant to spur us to push past our \"known\" boundaries. We need to get this world at peace before we can explore our advanced abilities. As Louis Armstrong said, \"If lots more of us loved each other, man, this world would be a gasser!\" reply hiatus 1 hour agorootparent> A Sufi Murshid (teacher) lived his entire life in a single town that consisted of a single pair of roads that met in the center of town. Late in his life, he stated that, he \"knew the stars of the Milky Way better than he knew his town\". (A love-consumed mystic remains conscious as our souls leave our bodies when we sleep. What is called astral travel is not limited by our physical body's speed laws; it is bounded only by the \"speed of thought\".) Do you have any suggested material/resources where I can learn more? reply MrMcCall 40 minutes agorootparentThis appears to only be in German: https://zwwa.de/ But this site has a few different languages, selectable in the upper-right corner of the page: https://mihr.com/ Note that the bulk of the teachings are about self-evolution via transmuting our vices into their corresponding virtues. It is that transformation that unlocks our ability to consciously travel during sleep. The key to all such teachings is that becoming consumed by compassion is the real goal; all else is just added benefit. As Steel Pulse put it so eloquently so long ago, \"Love is the golden chord that binds all commandments.\" It is also the scaffolding that boosts our abilities to their greatest height; but, in reality, the spiritual path is really about stripping away our selfish ego-nature that impedes our realizing our full potential. Peace be with you. reply lupusreal 1 hour agorootparentprev> our soul Is there any empirical test for such things? reply idunnoman1222 1 hour agorootparentAnd they’re never will be > without faith, God is nothing > If there was proof in God, you would have to worship him. That’s not the world we live in. reply cruffle_duffle 1 hour agorootparentprevThere may well never be. Not everything about our existence is knowable. An uncomfortable fact, indeed. reply EasyMark 2 hours agorootparentprevwhat you're saying is basically untestable and that's why most scientific minded people only talk about such things over beers or dismiss it entirely. It's not unlike religion or crystals. I mean we can't necessarily disprove them as they are based mostly on faith in an untestable conclusion. reply roughly 1 hour agorootparentprev> Rupert Sheldrake speaks of this when he says that the genetic code's protein construction genes do not and can not account for the resulting organism's shape. That coordinated construction requires a separate guiding force. Of course there's a separate guiding force. It's the biochemical environment around the cell. Cells operate on chemical signals they receive from their environment and generate the same; these cause cells to differentiate themselves based on their genetic code, which where the resulting organism's shape comes from. This isn't some kind of mystery, we know how this works, and matter & energy are indeed sufficient to explain it. reply Aurornis 3 hours agorootparentprev> Rupert Sheldrake speaks of this when he says that the genetic code's protein construction genes do not and can not account for the resulting organism's shape. > That interrelationship is similar to the \"memory\" that creatures such as salmon have, which is intrinsic to their entire being, not just their physical body, which is only half of our being's totality. This is all pseudoscience and borderline religious thinking. Rupert Sheldrake and others pushing this line of thinking are not grounded in reality or science. I’m surprised this is the most upvoted sub comment at the time I’m responding. Is pseudoscience like this really becoming so pervasive that comments like this pass as good information? reply MrMcCall 3 hours agorootparentWell, when your science explains where the 5/6ths of the missing matter in the universe is, or where the \"dark energy\" is, I'm all ears. Also, you can try to explain how individual proteins arrange themselves into bilaterally symmetrical, organ-infused organisms of astounding complexity, using only protein recipes. I know you can't explain it, but that doesn't mean you won't try. There is the known, the unknown, and the unknowable. For many, entire branches of the unknown are unknowable because they refuse to expand their criteria for how they evaluate the facts. Sherlock Holmes' father had a quote to the effect about once you have eliminated the possible, all that's left is the impossible (bad paraphrase, I know). reply Aloisius 2 hours agorootparentThat’s beyond bad paraphrasing - that's the polar opposite of the original. When you have eliminated the impossible, whatever remains, however improbable, must be the truth. reply abid786 3 hours agorootparentprevThis is a bunch of pseudoscience that isn't proven by anything at all and isn't peer reviewed either reply MrMcCall 3 hours agorootparentAnd your proof that it's wrong is ... ? That would make your counterargument a pseudo-counterargument, no? It's just reaching into one's feelings/nether-regions and blabbering out some words. You don't even have a sensible counter-theory, right? reply hackeraccount 3 hours agoprevAnimal behavior usually has a weird combination of inborn instinct and learned behavior. The one I've read about that stuck with me was dam building by beavers. Some part of the behavior is driven by a dislike of the sound of running water. Someone did an experiment with speakers playing the sound of running water and the beavers near the speakers would attempt to cover them with sticks and mud. In my head I'm imaging that sound is like nails on a chalkboard to beaver. reply snowwrestler 2 hours agoparentInstinct shows up locally as emotion. An individual animal acts based on their emotional state, and their emotional state is governed by a set of rules deep in their brain of which they are not conscious, many of which are set by birth. This is true of humans as well. We each make food selections based on what tastes good. We seek particular sexual partners because it feels good. We protect and raise kids because it makes us feel good to do so. This causes all sorts of evolutionarily weird side effects like people treating pets like kids in order to access the same emotional state as parenting. Or beavers covering speakers with mud and sticks. reply mathgradthrow 2 hours agorootparentevolution uses whatever hook it can find to tune behavior. Brains of sufficient complexity have to learn, you can't fit even enough information in DNA to manually wire up a brain, and its hard enough to guess how a barin will end up being wired. you can attach a squirrels optic nerve to their auditory cortex and they'll learn to see. (I may have the animal wrong). You can grow a brain completely inside out that will function. Instincts are deterministic, but learned behaviors. reply athenot 2 hours agoparentprev\"Dislike\" may be an anthropomorphism. Perhaps it's more of an opportunity for the beavers, since dams are their habitat and provide a food source for them. reply EasyMark 2 hours agorootparentyep it could be just as likely that they enjoy building the dam whenever they hear water. seems much less stressful on the system reply ASalazarMX 1 hour agorootparentEvolution doesn't mind how it feels, it only matters if it's effective at adaptation. It could be that running water in their homes stresses them as much as it stresses us, albeit for different reasons. The running water speaker experiment was done in dry land, and beavers are very wary of going out of the water because of their predators, yet they risked working over the speakers. reply grouseway 1 hour agoparentprevMaybe that's a thing, but here's a video of a pet beaver making a \"dam\" out of stuffies and other household objects. https://www.youtube.com/watch?v=-ImdlZtOU80 reply ics 3 hours agoparentprevI like the sound of running water from a fountain. But if I hear it inside, I assume there’s a leak and I go looking for it to fix. Maybe the beavers just need to visit the zen garden. reply finnh 2 hours agorootparentNot a beaver, but close: an otter wreaked repeat havoc in the Sun Yat-sen botanical garden in Vancouver, eating many valuable koi: https://vancouversun.com/news/local-news/the-godf-otter-part... reply duxup 4 hours agoprevLots of discussion about salmon memory and such, but is it possible this is just Salmon finding \"hey this is a great spot\"? It is hard to imagine salmon not being flexible to some extent, and still surviving. reply Hilift 2 hours agoparentSalmon are also making a resurgence in some areas where storm water runoff is being controlled and filtered. A chemical in tires to prevent cracking is lethal for Chinook and Steelhead, so keeping that out of watersheds could create huge population increases due to the amount of eggs. \"6PPD-quinone, that is deadly to coho salmon at extremely low concentrations and is often found in urban streams. Stormwater run-off from roads kills both juvenile and adult coho within a matter of a few hours. Even stormwater diluted to a mixture of just 5 percent highway runoff still killed juvenile coho, the new research found.\" https://www.fisheries.noaa.gov/feature-story/roadway-runoff-... reply ajsnigrutin 4 hours agoparentprevYep... also add the \"let's go up the river as far as we can, and we'll find a nice spot somewhere over there\". reply detourdog 3 hours agorootparentSometimes young Moose from the north run past the mating grounds through excitement. They end up in my neighborhood for the season and then run back north. reply ant6n 4 hours agorootparentprevWell, maybe this „find a nice spot“ search function is the „memory“ that’s encoded in genetics. reply shkkmo 3 hours agorootparentIt's already been established that a sense of smell is vital in Salmon's ability to return to the headwaters of their birth. I'm not aware of any \"genetic\" component, it is simply that Salmon remember the smell of where they were born and most salmon try and return. The feat is amazing and there are many \"instinctual\" behaviors involved, but no evidence that there is a genetic heritage from a specific headwaters is important in returning to that headwaters. This \"genetic memory\" talk is just uninformed people jumping to conclusions and spreading speculation as fact. reply ant6n 45 minutes agorootparentI’m merely proposing a mechanism for how it could be possible to have salmon return to the same spot after several generations, if that actually does happen. The idea would be that a salmon could be genetically predisposed to follow a certain path, perhaps preferring the smell of a certain combination of chemicals, thus encoding the location. It means the „memory“ would be encoded via genetics as a result of genetic combination and mutations, and the „encoding“ would essentially just be selection. It’s just speculation on how this genetic memory idea could work without actually encoding memories on genes. reply ph4 2 hours agoprevI'm lucky enough to have a salmon-bearing stream on my property here in the northwest. They are an extremely inspiring species to watch through their lifecycle. Tenacious. reply aesch 3 hours agoprevI read a fascinating article on this dam removal last week! https://hakaimagazine.com/features/the-other-side-of-the-wor... The article tells both sides of the story of the dam removal in as fair a way as I think is possible. Many of the locals were against it and there was a strong advocacy group that fought for it, including a tribal constituency. I came away from the article feeling I understood both sides better but with less certainty about what was the right choice. reply s1artibartfast 1 minute agoparentNot a bad article, all things considered, but I do think it gives a shallow treatment to reasons of the objectors to dam removal. How many people are impacted and how? Will they lose their businesses, jobs, and life savings? The closest it comes is talking about the spotted owl, where 30,000 people lost their loverhoods without compensation due to an environmental regulation that not only failed to deliver, but was doomed from the start. What are the parallels here? reply willsmith72 2 hours agoparentprevPeople will believe and fight for literally anything, surely thousands of years of con men has taught us that. The fact that this guy with a whopping 4 generations in the area doesn't agree means next to nothing to me. reply s1artibartfast 2 hours agorootparentThe point isnt that some guy doesnt agree, it is the ideas and information they are communicating. reply calibas 2 hours agoparentprevA lot of the local opposition to dam removal is because of this guy specifically. Here's his article on why the toxic cyanobacteria that was in the former reservoirs is actually good for the river: https://www.siskiyou.news/2024/01/24/blue-green-algae-in-cop... It pretends to be a regular news site, and even \"scientific\", to the point where it fooled Google and his site was often at the top of search results. He was also aggressively promoting his articles on Facebook. The guy is confusing green algae with bacteria. He's also ignoring the fact that the kind of blue-green \"algae\" in question, Microcystis aeruginosa, isn't the nitrogen-fixing kind. He has no clue was he's talking about, but that doesn't stop him, and he's unfortunately a major source of \"knowledge\" (confusion and misinformation) for the locals here. reply aliasxneo 2 hours agoparentprev> Resistance to dam removal on the Klamath is emblematic of the profound mistrust of official narratives that increasingly leads to such upside-down outcomes as survivors of climate disasters denying climate change, or rural communities accusing the wildfire fighters who protect their homes of deliberately setting the fires. Reservoir Reach is a place where, if KRRC is using helicopters to prep for dam removal, it must make sure the public knows that the choppers aren’t carrying out black ops against American sovereignty on behalf of the United Nations. The author seems to have developed quite a strong bias about the area. reply marssaxman 1 hour agorootparentWhat about that statement sounds biased to you? To my ears, that is a plain spoken description of the culture of the area, compatible with what I have observed myself over the years. reply s1artibartfast 41 minutes agorootparentIt seems like it is painting with an overly broad brush, condemnation by anecdote, and characterization by the negative extremes. But then again, I have my own priors, which probably bias me to thinking these people have legitimate reasons to distrust authorities who view their lives as expendable. reply proee 3 hours agoprevI'm surprised we could never engineer a proper salmon \"elevator\" to bypass the damn. Given the price of removing the damn, there seems to be a huge budget for creating some sort of high-tech Robo-elevator to scoop the fish out and drive them way upstream in a robo-vehicle. Maybe a giant net that lies at the base of the damn, and periodically lifts out of the water to catch the fish and automate the transportation of them to ideal next step drop. reply lizknope 1 hour agoparentFish ladders have been around for centuries. They have mixed results. I went to Bonneville Dam on the Columbia River between Washington and Oregon and it was kind of like being in an aquarium watching the fish swim upstream. They seemed to get tired and would float backwards with the water current and then start swimming again against the current. https://en.wikipedia.org/wiki/Fish_ladder They also had a salmon hatchery literally right after the dam. But they had some stats showing that it also had mixed effectiveness. reply nwsm 3 hours agoparentprevThings like that have been developed https://www.youtube.com/watch?v=2z3ZyGlqUkA reply blackeyeblitzar 3 hours agoparentprevThere are definitely ones that work very well. Ladders with just the right water flow and step sizes, chutes that whisk away fish in a tube, etc. As dumb as it sounds one of the more effective methods is having fish collect into concentrated tanks that are then trucked upstream to the right spot. reply proee 3 hours agorootparentThen why couldn't they make that work for this damn? I'm assuming there must be other motivating factors for removing the damn. reply patall 3 hours agorootparentOne is surely sediment erosion. All the small gravel that would usually end up in the delta being collected behind the dam. I.e like here: https://en.wikipedia.org/wiki/Restoration_of_the_Elwha_River... reply davidw 3 hours agorootparentprevMost of the Klamath dams were pretty old and not all that useful for other things like electricity generation or flood control, IIRC. I drove through Klamath Falls yesterday... that storm is sure going to give the river a lot of water to work with. Yeesh. reply patall 3 hours agorootparentIn the article it says energy for 70k households. Not saying you are wrong but that is substantial. reply calibas 1 hour agorootparentIt could power 70k homes at full capacity. Important note, it wasn't at full capacity, and the entire county of Siskiyou is only about 40k people (not homes). Our power company, Pacific Power, said they didn't need the dam at all, and it would cost more to maintain than it was worth. reply blackeyeblitzar 3 hours agorootparentprevA lot of the dam removal pressure is activist not scientific - people who have come to believe dams are evil because they aren’t natural. Some of it is cultural - with upstream tribal lands where people cannot practice traditional life or activities like fishing without returning fish each season. Some of it is practical - we don’t do a good job maintaining old dams and new replacement projects are expensive. But I do worry that the new dam removal movement is sacrificing renewable energy and flood control and navigable rivers for little gain, when they could find solutions that keep the dams and help upstream environments. Well designed ladders work efficiently. Fish don’t have to over exert themselves, make jumps (actual leaps to the next step) no bigger than they would naturally (with no dam), and have lots of resting spaces across the ladder where they can regain energy in gentle waters before continuing swimming and jumping upstream. They slowly gain elevation moving across spacious concrete tiers until they reach either a natural release point upstream enough that the strong flow into the dam doesn’t take them, or they end up in a hatchery. I feel like hatcheries are underrated. Sure the upstream habitats are not the same without the fish and associated ecosystem. But if you have the right equipment, staffing, funding, and all that (basically a good government) the hatcheries could be made to churn out more fish than would be naturally possible. That’s because the trip upstream naturally is hard and many fish won’t make it anyways. reply cruffle_duffle 1 hour agorootparentKeeping the dam isn’t a ‘scientific’ decision because science doesn’t make decisions—it just tells us what might happen: more fish, less renewable energy, changes to flood control, etc. The real decision is about trade-offs, like how much we value fish versus clean energy, upstream ecosystems versus downstream economies, or cultural traditions versus infrastructure costs. Calling dam removal ‘activist’ implies the push to keep it isn’t. But keeping the dam is just as much about advocacy—it’s about prioritizing things like renewable energy or flood control. Neither side is more ‘scientific’ than the other; they’re both driven by values. Science helps us understand the stakes, but humans decide what matters most. That’s why this stuff gets so messy. reply MostlyStable 1 hour agorootparentThank you. So many people confuse their own values with science. Science might say \"If you take action X, thing A increases\" and a person who values thing A hears \"Science says we should take action X\". That is not correct. Science informs you about the impacts of your actions (imperfectly), and it is a social/cultural/political (and most definitely not a scientific) discussion which of those impacts we actually prefer. reply cruffle_duffle 50 minutes agorootparentThank you—this is exactly the point. People confuse their own values with science, and ‘follow the science’ rhetoric only makes it worse. Science might say, ‘If you take action X, thing A increases,’ but deciding whether to take action X involves weighing A against everything else we care about—values, costs, benefits, and human experience. COVID was a perfect example of this. Policies like isolating grandma in a nursing home or pulling kids out of school for two years were framed as ‘following the science,’ but they ignored entire fields of science and vast parts of the human experience. Loneliness has measurable health consequences—science shows it can kill. So do we isolate grandma to protect her from COVID, or risk her dying of loneliness? Similarly, the science of childhood education tells us that pulling kids from school harms them for life. These are real trade-offs, rooted in human values, not just science. And to be frank, that entire discussion was shut down completely. The entire decision making process was incredibly one-sided and myopic. The same applies to dams. Decisions about whether to keep or remove them aren’t just ‘science versus activism.’ Both sides are informed by science, but they’re also driven by emotion, lived experience, and the values people hold. Science doesn’t tell us what to do—it gives us information about potential outcomes. What we choose depends on how we weigh those outcomes and whose priorities matter most. When rhetoric like ‘keep the dam = science, remove the dam = activism’ takes over, it oversimplifies these deeply human decisions and turns them into unnecessary battles. At the end of the day, it’s not ‘us vs. them’—it’s all of us trying to navigate complex trade-offs in a way that reflects the full spectrum of what matters to humans. reply sophacles 58 minutes agoparentprevBut why? The cost of upkeep for the dams compared to the amount of utility they provided was already too high to preserve the dam. Adding this sort of mechansim would only add to the cost of upkeep, making the preservation of the dam an even worse proposition. reply jibbit 1 hour agoprevfor the past few years i've been watching the salmon return to a spot in the uk they've not been to for over 200 years. i had no idea growing up there that these were salmon spawning grounds, then some wiers were removed. such a wonderful thing to see. i don't think it's memory! reply EasyMark 2 hours agoprevThis is only tangential but with more solar and nuclear, more and more projects like this will become possible. reply thrance 2 hours agoprevI have a guy in my family who worked to remove dams over a small tributary river of the Seine, in Normandy, France. It took him several years to remove the 300+ dams, the oldest ones being easily 150 years old. The very first year after his work was completed the salmons came back. Now he works in the environmental police, and is often called to handle cetaceans getting lost in the Seine delta. People freak out because it is an unusual sight nowadays, but he told me this is just a return to how things were. They are stories of dolphins swimming as far back as Paris in the past centuries. I guess this means we're doing something right, I hope one day we'll be rid of this poisonous brown opaque water flowing through our cities. I really hope one day to be able to see this \"clear water\" my grandpa told me he learned to swim in. reply spencerflem 42 minutes agoparentI do too - thank you so much to your relative for their important work. Sadly, it seems like things are mostly going in the opposite direction reply alecco 3 hours agoprevThere are systems to allow salmon to go over dams. From ladders to cannons. I hope they are right about this dam not needed for flood prevention. Spain just lost hundreds of people and suffered billions in damages because these kinds of policies. reply Rygian 2 hours agoparentGetting fish ladders to work where they exist, or built where they are lacking, is not an easy feat either. And the dam removals in Spain have nothing to do with receiving 770 mm of water in one single day. None of the removed dams would have protected an area that was planned to get flooded when the works of the 1960s were done. https://www.reuters.com/fact-check/map-shows-existing-river-... reply everyone 4 hours agoprevI'm most curious about how the salmon found it so fast.. Did their instincts predispose them to go there, if they were in the area? or was there some physical trace they were following? or is there some weird lamarkian genetic memory thing going on? .. In fact do we know now salmon normally navigate 1000's of miles back to their spawn location? reply blackeyeblitzar 3 hours agoparentThey swim towards where they feel water flowing from. They keep going until their bodies are breaking down. Fish at the spawn location often have rotting bodies, even as they still live - losing color and with their flesh changing consistency. reply everyone 3 hours agorootparentBut what about at the start when they are in the ocean and water isn't flowing from anywhere in particular? + How do they end up in one particular river and not another? reply blackeyeblitzar 3 hours agorootparentThat part is not really known. Various things have been suspected like memory of magnetic fields, salinity, temperature patterns, odors, etc. Basically they may be memorizing those on the way out and end up coming back to the same shoreline. From there it’s following upstream water pressure (which is how salmon ladders induce them to follow the ladder). reply 2OEH8eoCRo0 3 hours agorootparentprevDo rivers have a smell? Animals have a keen sense of smell and the volume of rivers is enormous. Seems like a random-walk sniffing for rivers would be effective. reply ImHereToVote 4 hours agoprevI thought that project already had salmon spillway weirs. reply buildsjets 4 hours agoparentFish ladders and spillway weirs are fish killers that impose a decimation on the salmon population at each elevation change. Dams destroy the estuary and natural wetland environments that salmon need to reproduce. Dams reduce water flow and silt over gravel beds. Dam impoundments cause stream and river temperatures to rise, suffocating fish. Dam removal is not just obstacle removal, it is habitat restoration and rehabilitation. reply duxup 4 hours agorootparentI feel like that's just a block of true-ish text but doesn't address the actual comment. Nothing you said talked about salmon spillway weirs. reply Enginerrrd 55 minutes agorootparentprevYeah the dams also tend to regulate flows in the river system which doesn't allow natural cycles of peaks and valleys to help regulate parasites. reply blackeyeblitzar 3 hours agorootparentprevThis isn’t exactly true. Fish ladders and weirs shouldn’t be grouped together like this. Many hatcheries have a weir salmon cannot cross and a ladder as the alternative path the fish take by feeling the flow of water across the ladder and going upstream. The ladders lead to hatcheries where the fish reproduce. And new tiny fish are efficiently raised in protected tanks and later released to go back downstream. In other words, the weir and ladder are a combination to make the hatchery work, and not substitutes for each other. Also ladders can work very well. There are many badly designed ones but the good ones basically let every fish move upstream. reply bbarnett 4 hours agorootparentprevMuch of what you said is an exaggeration, for where a habitat disappears with a dam, different habitats appear. But regardless, the point is that salmon were still breeding there. The \"return\" is an unwarranted claim, for they never stopped coming and spawning. reply SalmonSnarker 3 hours agorootparentSalmon were not still breeding there, this is the first return in over 100 years. October of this year: > a fall-run Chinook salmon was identified by ODFW’s fish biologists in a tributary to the Klamath River above the former J.C. Boyle Dam, becoming the first anadromous fish to return to the Klamath Basin in Oregon since 1912 when the first of four hydroelectric dams was constructed, blocking migration. https://www.dfw.state.or.us/news/2024/10_Oct/101724.asp reply ruined 4 hours agorootparentprev>salmon were still breeding there. The \"return\" is an unwarranted claim, for they never stopped coming and spawning. let's read \"Less than a month after four towering dams on the Klamath River were demolished, hundreds of salmon made it into waters they have been cut off from for decades\" what does that mean \"salmon are once more returning to spawn in cool creeks that have been cut off to them for generations.\" \"salmon, which were cut off from their historic habitat\" \"salmon that have quickly made it into previously inaccessible tributaries\" reply dylan604 3 hours agorootparentso...you're saying that the salmon are able to access places they haven't been able to access? that's like you're trying to tell us that the damn dam was what was preventing it. it's like the dam being removed was the reason for these salmon to gain access to the spots. i'm still confused. /s reply soco 4 hours agorootparentprevDifferent habitats of algae and mud, so I'll agree of course better than nothing while also very far from the previous quality. reply InDubioProRubio 4 hours agorootparentprevNot dams impose climate change that destroys all things. reply timdiggerm 4 hours agoparentprevThey don't work all that well compared to an open river. reply astura 4 hours agoparentprevThis article is better at explaining environmental issues the dam caused https://www.bbc.com/future/article/20240903-removing-the-kla... reply lupusreal 4 hours agoprevHow do they know? I thought salmon always return to the same river, so a river no salmon come from won't get any returning, but I guess a certain percent are adventurous? reply ivandenysov 4 hours agoparentIf all salmon returned to the same river then there would be only one river with Salmon spawning. Maybe they do have perfect memory, but a certain percentage of them get carried to other rivers by birds of prey who want to have Salmon in THEIR river reply InDubioProRubio 4 hours agorootparentActually- its birds like ducks eating the eggs and a percentage of eggs surviving the ingestion and being shit out into a new river reply optimalsolver 3 hours agorootparentThe elegant beauty of Nature. reply AlotOfReading 4 hours agoparentprevThe river wasn't entirely inaccessible to salmon, the dams just prevented access to the upper lakes and river segments. reply shkkmo 3 hours agoparentprevMost salmon do, but a small percentage always stray. If you think about it, it is kinda an obviously necessary behavior given that many current salmon habitats were not present during the last ice age. reply silexia 4 hours agoprev [16 more] [flagged] toomuchtodo 4 hours agoparent> Power company PacifiCorp built the dams to generate electricity between 1918 and 1962. But the structures halted the natural flow of the waterway that was once known as the third-largest salmon-producing river on the West Coast. They disrupted the lifecycle of the region’s salmon, which spend most of their life in the Pacific Ocean but return to the chilly mountain streams to lay eggs. > At the same time, the dams only produced a fraction of PacifiCorp’s energy at full capacity, enough to power about 70,000 homes. They also didn’t provide irrigation, drinking water or flood control, according to Klamath River Renewal Corporation. The dams were providing immaterial value, and that power generation can trivially be replaced. Chesterton's Fence reminder. reply Loic 4 hours agoparentprevhttps://klamathrenewal.org/the-project/ Take a look at the MW and age of the dams they removed. They were relatively small (big like 4 wind turbine for one) and at the end of life. The owners clearly stated that it was not economically feasible to keep upgrade them to meet the current regulations. Yes they can provide base load, but I would be more nuanced than \"not serious about climate change\". reply nyargh 4 hours agoparentprevThe US is a net energy exporter and just the columbia river main stem system generates enough power for washington, idaho and oregon 10 times over. Hydroelectric is not green, has a limited service life and even has significant greenhouse gas emissions from its reservoirs. Go troll somewhere else. reply mapt 3 hours agorootparent\"More than enough power\" There's no such thing anymore. Dams are more important than they were in decades past. Existing dams are by far the easiest and cheapest source of on-demand peaking power. 10GW that you're not doing anything with, can provide load balancing for 20GW-50GW of wind and solar somewhere else. Perhaps half the country away and in a different grid entirely through the use of HVDC transmission lines. That's not to say that the ecological consideration is nonexistent. It's not that difficult to build dams that make small sacrifices in cost and throughput to permit fish navigation. We just didn't care about that _at all_ a hundred years ago. reply Cthulhu_ 4 hours agorootparentprevHow come the reservoirs have significant greenhouse gas emissions? I did a curious and googled it, it looks like the reservoirs have a lot of dissolved co2 and methane (probably from natural decomposition?) but it's released when the water is churned through the turbines and spillways. Never really considered that, but then, wouldn't that also happen naturally with lakes and rivers? Anyway the article I read says they're looking into capturing it somehow, which would reduce the emissions a bit. reply toomuchtodo 4 hours agorootparentSubmerged biomass decomposing. https://news.agu.org/press-release/greenhouse-gas-emissions-... https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GB00... reply Cthulhu_ 4 hours agoparentprev> At the same time, the dams only produced a fraction of PacifiCorp’s energy at full capacity, enough to power about 70,000 homes. They also didn’t provide irrigation, drinking water or flood control, according to Klamath River Renewal Corporation. It kind of implies that these dams weren't really important to begin with, which makes the tradeoff a lot easier. Second, they were built in the early to mid 1900s, at this point they had to decide between repairing, replacing, or demolishing them; it looks like they weren't worth repairing or replacing. It also looks like they had to make adjustments to allow the salmon to go upstream (according to a quick browse of https://en.wikipedia.org/wiki/Klamath_River_Hydroelectric_Pr...) regardless, legal requirements. Anyway, your comment deals in some absolutes / false dichotomies and generalizations; climate change is not fixed by keeping these dams around. Climate change is also but one problem, there's other causes for mass extinction - e.g. spawning grounds being inaccessible due to dams. There's many different renewable energy sources, hydroelectric being a big one, but they have to be built in a way that doesn't disrupt nature, which wasn't yet a thing when these were built. TL;DR, you can have both hydroelectric dams AND access to spawning grounds. reply saalweachter 4 hours agorootparent> It kind of implies that these dams weren't really important to begin with... On a second reading, I don't think you were actually saying \"it was pointless to build them to being with\", but apropos my initial misreading -- It's worth noting that US per capital electric usage has increased ~10 fold since 1950; when these dams were built, 70,000 homes probably would have been 700,000. reply BeetleB 2 hours agoparentprevHydroelectric dams are great as long as they displace someone else. The oldest continuously habited place in North America (for 10K years) is now submerged because of a dam. Not only was it a loss of a lot of archaeological value, but real people who were living there had to be displaced. (Also a loss in the natural world - the most voluminous waterfall in N America was submerged there as well). reply rsynnott 4 hours agoparentprevThese are small, old dams, and peaked at 169MW production, while having significant undesirable impact. reply buildsjets 4 hours agoparentprevYou are clearly not from here. Dam removal in the PNW enjoys widespread bipartisan support from both lefties and righties. I know many conservative outdoorsmen who know EXACTLY how much damage these dams have done to their life and livelihood. Many of the dams being removed are obsolete and produce negligible amounts of electricity, but their removal quickly results enhancing and preserving local fishing, hunting, and other outdoor recreational activities for current and future generations of outdoorsmen. reply silexia 1 hour agorootparentI was born in Washington state, raised here, and currently farm in Eastern Washington. Every single farmer and person I know wants more dams and more irrigation. I have never met a Republican who supports dam removal. reply timbit42 4 hours agoparentprevTheir ponds emit CO2 though. reply astura 4 hours agoparentprev [–] Garbage take full of misinformation and lies. This was a project spearheaded by the local indigenous tribes, not the \"far left.\" It's been a major cause the tribes have been fighting for literally a century. These dams were not providing very much electricity in their heyday and were at the end of their life. They needed either significant maintenance or destruction and maintenance most likely would have been more costly. Their presence caused several environment issues. Because of all of the above, dam removal was/is a generally popular project with a lot of people, not just the \"far left.\" reply silexia 1 hour agorootparent [–] I am Choctaw myself. I recognize the tribes are socialist institutions meant to divide rather than unite now. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Following the demolition of four dams on the Klamath River, hundreds of salmon have returned to spawn in areas that were previously inaccessible, marking a rapid ecological recovery.",
      "This event is a significant victory for local tribes who advocated for the dam removal to restore the river's ecosystem, highlighting the project's success in improving water quality and salmon health.",
      "The project is noted as the largest dam removal in U.S. history, with the swift return of salmon exceeding expectations and providing optimism for the river's future."
    ],
    "commentSummary": [
      "Salmon have returned to their historic spawning grounds on the Klamath River following the removal of four dams, prompting discussions on their navigation methods after decades of being cut off.- The dam removal was part of a broader ecological restoration effort, supported by local tribes and environmental groups, aimed at restoring natural habitats.- The dams were outdated and provided limited electricity, making their removal a feasible option for enhancing the river's ecological health."
    ],
    "points": 219,
    "commentCount": 145,
    "retryCount": 0,
    "time": 1732282032
  },
  {
    "id": 42215126,
    "title": "Amazon to invest another $4B in Anthropic, OpenAI's biggest rival",
    "originLink": "https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS VIDEO INVESTING CLUB PRO LIVESTREAM Search quotes, news & videos WATCHLIST SIGN IN AI AGE AI INSIGHTS AI EFFECT Amazon to invest another $4 billion in Anthropic, OpenAI's biggest rival PUBLISHED FRI, NOV 22 20249:14 AM ESTUPDATED 3 HOURS AGO Hayden Field @HAYDENFIELD KEY POINTS Amazon on Friday announced it would invest an additional $4 billion in Anthropic, the artificial intelligence startup founded by ex-OpenAI research executives. The new funding brings the tech giant's total investment to $8 billion, though Amazon will retain its position as a minority investor. Amazon Web Services will also become Anthropic's \"primary cloud and training partner,\" according to a blog post. In this article AMZN Follow your favorite stocks CREATE FREE ACCOUNT AnadoluAnadoluGetty Images Amazon on Friday announced it would invest an additional $4 billion in Anthropic, the artificial intelligence startup founded by ex-OpenAI research executives. The new funding brings the tech giant's total investment to $8 billion, though Amazon will retain its position as a minority investor, according to Anthropic, the San Francisco-based company behind the Claude chatbot and AI model. Amazon Web Services will also become Anthropic's \"primary cloud and training partner,\" according to a blog post. From now on, Anthropic will use AWS Trainium and Inferentia chips to train and deploy its largest AI models. Anthropic is the company behind Claude — one of the chatbots that, like OpenAI's ChatGPT and Google's Gemini, has exploded in popularity. Startups like Anthropic and OpenAI, alongside tech giants such as Google, Amazon, Microsoft and Meta, are all part of a generative AI arms race to ensure they don't fall behind in a market predicted to top $1 trillion in revenue within a decade. Some, like Microsoft and Amazon, are backing generative AI startups with hefty investments as well as working on in-house generative AI. The partnership announced Friday will also allow AWS customers \"early access\" to an Anthropic feature: the ability for an AWS customer to do fine-tuning with their own data on Anthropic's Claude. It's a unique benefit for AWS customers, according to a company blog post. In March, Amazon's $2.75 billion investment in Anthropic was the company's largest outside investment in its three-decade history. The companies announced an initial $1.25 billion investment in September 2023. Amazon does not have a seat on Anthropic's board. News of Amazon's additional investment comes one month after Anthropic announced a significant milestone for the company: AI agents that can use a computer to complete complex tasks like a human would. Anthropic's new Computer Use capability, part of its two newest AI models, allows its tech to interpret what's on a computer screen, select buttons, enter text, navigate websites, and execute tasks through any software and real-time internet browsing. The tool can \"use computers in basically the same way that we do,\" Jared Kaplan, Anthropic's chief science officer, told CNBC in an interview last month, adding it can do tasks with \"tens or even hundreds of steps.\" Amazon had early access to the tool, Anthropic told CNBC at the time, and early customers and beta testers included Asana, Canva and Notion. The company had been working on the tool since early this year, according to Kaplan. In September, Anthropic rolled out Claude Enterprise, its biggest new product since its chatbot's debut, designed for businesses looking to integrate Anthropic's AI. In June, the company debuted its more powerful AI model, Claude 3.5 Sonnet, and in May, it rolled out its \"Team\" plan for smaller businesses. Last year, Google committed to invest $2 billion in Anthropic, after previously confirming it had taken a 10% stake in the startup alongside a large cloud contract between the two companies. Don’t miss these insights from CNBC PRO Warren Buffett's Berkshire Hathaway takes a stake in Domino's Pizza Wall Street is gearing up for an M&A boom under Trump. These companies could be targets Inflation report shows market could have a 'recipe for disaster' heading into new year, says economist Morningstar names cheap stocks in a sector that ‘deserves a place in everybody’s portfolio’ These 2 active ETFs have outperformed the S&P 500 this year, last year and over 5 years MORE IN AI EFFECT Nvidia nearly doubles revenue on strong AI demand Kif Leswing 3 HOURS AGO Snowflake rockets 32%, its best day ever, after earnings beat Hayden Field Business spending on AI surged 500% this year to $13.8 billion, says Menlo Ventures Hayden Field READ MORE Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=42215126",
    "commentBody": "Amazon to invest another $4B in Anthropic, OpenAI's biggest rival (cnbc.com)193 points by swyx 2 hours agohidepastfavorite93 comments cainxinth 1 hour agoThey certainly need the money. The Pro service has been running in limited mode all week due to being over capacity. It defaults to “concise” mode during high capacity but Pro users can select to put it back into “Full Response.” But I can tell the quality drops even when you do that, and it fails and brings up error messages more commonly. They don’t have enough compute to go around. reply sbuttgereit 40 minutes agoparentHmmm... I wonder if this is why some of the results I've gotten over the past few days have been pretty bad. It's easy to dismiss poor results on LLM quality variance from prompt to prompt vs. something like this where the quality is actively degraded without notification. I can't say this is in fact what I'm experience, but it was noticeable enough I'm going to check. reply jmathai 8 minutes agorootparentNever occurred to me that the response changes based on load. I’ve definitely noticed it seems smarter at times. Makes evaluating results nearly impossible. reply 55555 35 minutes agorootparentprevSame experience here. reply jmathai 10 minutes agoparentprevI’ve been using the API for a few weeks and routinely get 529 overloaded messages. I wasn’t sure if that’s always been the case but it certainly makes it unsuitable for production workloads because it will last hours at a time. Hopefully they can add the capacity needed because it’s a lot better than GPT-4o for my intended use case. reply el_benhameen 10 minutes agoparentprevInteresting. I also find it frustrating to be rate limited/have responses fail when I’m paying for the product, but I’ve actually found that the “concise” mode answers have less fluff and make for faster back and forth. I’ve once or twice looked for the concise mode selector when the load wasn’t high. reply neya 32 minutes agoparentprevI am a paying customer with credits and the API endpoints rate-limited me to the point where it's actually unusable as a coding assistant. I use a VS Code extension and it just bailed out in the middle of a migration. I had to revert everything it changed and that was not a pleasant experience, sadly. reply square_usual 24 minutes agorootparentWhen working with AI coding tools commit early, commit often becomes essential advice. I like that aider makes every change its own commit. I can always manicure the commit history later, I'd rather not lose anything when the AI can make destructive changes to code. reply htrp 24 minutes agorootparentprevControl your own inference endpoints. reply nowahe 31 minutes agoparentprevI've had it refuse to generate a long text response (I was trying to concise a 300kb documentation to 20-30kb to be able to put it in the project's context), and every time I asked it replied \"How should structure the results ?\", \"Shall I go ahead with writing the artifacts now ?\", etc. It wasn't even during the over-capacity event I don't think, and I'm a pro user. reply mikeocool 1 hour agoprevCurious if anyone knows the logistics of these cloud provider/AI company deals. In this case, it seems like the terms of the deal mean that Anthropic ends up spending most of the investment on AWS to pay for training. Does anthropic basically get at cost pricing on AWS? If Amazon has any margin on their pricing, it seems like this $4B investment ends up costing them a lot less, and this is a nice way to turn a cap ex investment into AWS revenue. reply B4CKlash 2 minutes agoparentThere's also another angle. During the call with Lex last week, Dario seemed to imply that future models would run on amazon chips from Annapurna Labs (Amazon's 2015 fabless purchase). Amazon is all about the flywheel + picks and shovels and I, personally, see this as the endgame. Create demand for your hardware to reduce the per unit cost and speed up the dev cycle. Add the AWS interplay and it's a money printing machine. reply tyre 1 hour agoparentprevYes exactly. This was the brilliance of the original MSFT investment into OpenAI. It was an investment in Azure scaling its AI training infra, but roundabout through a massive customer (exactly what you’d want as a design partner) and getting equity. I’m sure Anthropic negotiated a great deal on their largest cost center, while Amazon gets a huge customer to build out their system with. reply wcunning 1 hour agorootparentThat’s honestly one of the hardest things in engineering — identifying not just a customer to drive requirements, but a knowledgeable customer who can drive good requirements that work for a broader user base and can drive further expansion. Anthropic seems ideal for that, plus they act as a service/API provider on AWS. reply whatshisface 1 hour agorootparentprevThis explanation makes no sense, I could be AWS' biggest customer if they wanted to pay me for it. Something a little closer could be that the big tech companies wanted to acquire outside LLMs, not quite realizing that spending $1B on training only puts you $1B ahead. reply raverbashing 1 hour agorootparentYes but Amazon is not making extra money with you being their biggest customer With Anthropic yes reply whatshisface 1 hour agorootparentAnthropic is getting $4B in investment in a year where their revenue was about $850M. Even if Amazon had bought them outright for that much, they would not be ahead. The fact that everybody keeps repeating the claim that Amazon is \"making money\" makes this appear like some kind of scam. reply vineyardmike 1 hour agorootparentThese sort of investments usually also contain licensing deals. Amazon probably gets Anthropic models they can resell “for free”. The 850M revenue is Anthropic’s, but there is incremental additional revenue to AWS’s hosted model services. AWS was already doing lots of things with Anthropic models, and this may alter the terms more in amazons favor. Are they actually making money? I don’t know, investments aren’t usually profitable on day one. Is this an opportunity for more AWS revenue in the future? Probably. reply surgical_fire 1 hour agorootparentprevIt appears to be a scam because it sort of is. AI needs to be propped up because the bug tech cloud providers they depend on need AI to be a thing to justify their valuations. Tech is going through a bit of a slump where all things being hyped a few years ago sort of died down (crypto? VR? Voice assistants? Metaverse?). Nobody gets very hyped about any of those nowadays. I am probably forgetting a couple of hyped things that fizzled out over the years. Case in point, as much as I despise Apple, they are not all-in the AI bandwagon because it does nothing for them. reply vineyardmike 1 hour agorootparentGo look at earnings reports for big tech companies. AI is definitely driving incremental revenue. Apple is definitely on the AI bandwagon, they just have a different business model and they’re very disciplined. Apple tends not to increase research and investment costs faster than revenue growth. You’ll also notice rumors that they’re lowering their self driving car and VR research goals. reply surgical_fire 54 minutes agorootparent> Go look at earnings reports for big tech companies. AI is definitely driving incremental revenue. Yes. Which proves my point. reply herval 29 minutes agorootparentprev> Case in point, as much as I despise Apple, they are not all-in the AI bandwagon because it does nothing for them. not sure if you've been paying attention, but AI is literally _the only thing_ Apple talks about these days. They literally released _an entire generation of devices_ where the only new thing is \"Apple Intelligence\" reply staticman2 17 minutes agorootparentIs Apple investing in AI as much as Google, Meta, Microsoft, and xAI? If not they are not \"all in\". reply herval 2 minutes agorootparentThey don’t disclose it, but I’d imagine so. They also admit to being a couple of years late, so they’re accelerating (as per their last earnings call) eitally 1 hour agoparentprevI am not privy to specific details, but in general there is a difference between investment and partnership. If it's literally an investment, it can either be in cash or in kind, where in kind can be like what MSFT did for OpenAI, essentially giving them unlimited-ish ($10b) Azure credits for training ... but there was quid pro quo where MSFT in turn agreed to embed/extend OpenAI in Azure services. If it's a partnership investment, there may be both money & in-kind components, but the money won't be in the context of fractional ownership. Rather it would be partner development funds of various flavors, which are usually tied to consumption commits as well as GTM targets. Sometimes in reading press releases or third party articles it's difficult to determine exactly what kind of relationship the ISV has with the CSP. reply chatmasta 14 minutes agoparentprevSupermicro is currently under DOJ investigation for similar schemes to this. The legality of it probably depends on the accounting, and how revenue is recognized, etc. It certainly looks sketchy. But I’m sure there’s a way to do it legitimately if their accountants and lawyers are careful about it… reply aiinnyc 1 hour agoparentprevOne hand washes the other. reply jatins 1 hour agoprevAnthropic gets a lot of it's business via AWS Bedrock so it's fair to say that Amazon probably has reasonable insight into how the Claude usage is growing that makes them confident in this investment reply swyx 1 hour agoparent> gets a lot of it's business via AWS Bedrock can you quantify? any numbers, even guesstimates? reply mediaman 54 minutes agorootparentOne source [1] puts it at 60-75% of revenue as third-party API, most of which is AWS. [1]https://www.tanayj.com/p/openai-and-anthropic-revenue-breakd... reply peppertree 1 hour agoprevAnthropic should double down on the strategy of being the better code generator. No I don't need an AI agent to call the restaurant for me. Win the developers over and the rest will follow. reply rtsil 1 hour agoparent> Win the developers over and the rest will follow. Will they really? Anecdotal evidence, but nobody I know in real life knows about Claude (other than it's an ordinary first name). And they all use or at least know about ChatGPT. None of them are software engineers of course. But the corporate deciders aren't software engineers either. reply staticman2 45 minutes agorootparentMost people I know in real life have certainly heard of ChatGPT but don't pay for it. I think someone enthusiastic enough to pay for the subscription is more likely to be willing to try a rival service, but that's not most people. Usually when these services are ready to grow they offer a month or more free to try, at least that's what Google has been doing with their Gemini bundle. reply hiq 16 minutes agorootparentI'm actually baffled by the number of people I've met who pay for such services, when I can't tell the difference between the models available within one service, or between one service or the other (at least not consistently). I do use them everyday, but there's no way I'd pay $20/month for something like that as long as I can easily jump from one to the other. There's no guarantee that my premium account on $X is or will remain better than a free account on $Y, so committing to anything seems pointless. I do wonder though: several services started adding \"memories\" (chunks of information retained from previous interactions), making future interactions more relevant. Some users are very careful about what they feed recommendation algorithms to ensure they keep enjoying the content they get (another behavior I'm was surprised by), so maybe they also value this personalization enough to focus on one specific LLM service. reply peppertree 50 minutes agorootparentprevConsumers don't have to consciously choose Claude, just like most people don't know about Linux. But if they use an Android phone or ever use any web services they are using Linux. reply croes 41 minutes agorootparentprevMaybe the software engineers should talk to the deciders then. reply fullstackwife 1 hour agoparentprevI also don't understand the idea of voice mode, or agent controller computer. Maybe it is cool to see as a tech demo, but all I really want is good quality, at reasonable price for the LLM service reply ianmcgowan 1 hour agoparentprevI mean, look at Linux and Firefox! reply peppertree 52 minutes agorootparentPretty sure most frontend developers use Chrome since it has better dev tools. And yes everyone uses Linux most just don't know it. reply ripped_britches 1 hour agorootparentprevLegendary comment, bravo reply fariszr 1 hour agoprevThis makes sense in the grand scheme of things. Anthropic used to be in the Google camp, but DeepMind seems to have picked up speed lately, with new “Experimental” Gemini Models beating everyone, while AWS doesn't have anything on the cutting edge of AI. Hopefully this helps Anthropic to fix their abysmal rate limits. reply n2d4 1 hour agoparent> Anthropic used to be in the Google camp I don't think Anthropic took any allegiances here. Amazon already invested $4B last year (Google invested $2B). reply fariszr 1 hour agorootparentAFAIK they used Gcloud to run their models. reply aliasxneo 1 hour agoprev> Amazon Web Services will also become Anthropic’s “primary cloud and training partner,” according to a blog post. From now on, Anthropic will use AWS Trainium and Inferentia chips to train and deploy its largest AI models. I suspect that's worth more than $4B in the long term? I'm not familiar with the costs, though. reply devjab 1 hour agoparentI’ve been impressed with the AI assisted tooling for the various monitoring systems in Azure at least. Of course this is mainly because those tools are so ridiculously hard to use that I basically can’t for a lot of things. The AI does it impressively well though. I’d assume there is a big benefit to having AI assisted resource generation for cloud vendors. Our developers often have to mess around with things that we really, really, shouldn’t in Azure because operations lacks the resources and knowledge. Technically we’ve outsourced it, but most requests take 3 months and get done wrong… if an AI could generate our network settings from a global policy that would be excellent. Hell if it could handle all our resource generation they would be so much useless time wasted because our organisation views “IT” as HRs uncharming cost center cousin. reply ipaddr 1 hour agoprevI' m not sure how they make it back. The guardrails in place are extremely strict. The only people who seem to use it are a subset of developers who are unhappy with OpenAI. With Bard popping up free everywhere taking away much of the general user crowd and OpenAI offering the mini model always free and limited image generation / expensive model. Then you have to do it yourself crowd with llama. What is their target market? Governments? Amazon companies?There free their offers 10 queries and half of them need to be used to get around filters I don't see this positioned well for general customers. reply staticman2 1 hour agoparentThe Guardrails on Claude Sonnet 3.5 API are not stricter than Openai's guardrails in my experience. More specifically, if you access the models via API or third party services like Poe or Perplexity the guardrails are not stricter than GPT4o. I've never subscribed to Claude.ai so can't comment on that. I have no experience with Claud.ai vs ChatGPT but it's clear the underlying model has no issue with guardrails and this is simply an easily tweaked developer setting if you are correct that they are stricter on Claude.ai. (The old Claude 2.1 was hilariously unwilling to follow reasonable user instructions due to \"ethics\" but they've come a long way since then.) reply dragonwriter 1 hour agorootparent> The Guardrails on Claude Sonnet 3.5 API are not stricter than Openai’s guardrails in my experience. Both Gemini and Claude (via the API) have substantially tighter guardrails around recitation (producing output matching data from their training set) than OpenAI, which I ran into when testing an image text-extraction-and-document-formatting toolchain against all three. Both Claude and Gemini gave refusals on text extraction from image documents (not available publicly anywhere I can find as text) from a CIA FOIA release Not sure if they are tighter in other areas. reply staticman2 1 hour agorootparentI just asked GPT4o to recognize a cartoon character (I accessed it via Perplexity) and it told me it isn't able to do that, while Claude Sonnet happily identified the character, so this might vary by use case or even by prompt. reply msp26 1 hour agorootparentprevI've had a situation where Claude (Sonnet 3.5) refused to translate song lyrics because of safety/copyright bullshit. It worked in a new chat where I mentioned that it was a pre 1900s poem. reply staticman2 1 hour agorootparentI've translated hundreds of pages of novel text via Sonnet 3.5. But I did it where I have system prompt access and tell it to act as a translator. reply ipaddr 1 hour agorootparentprevMy comment was purely about Claud.ai which is where general customers would go. reply staticman2 1 hour agorootparentI don't know if Claude.ai or ChatGPT are even profitable at this stage, so they might not particularly want general customers. reply loandbehold 1 hour agoparentprevClaude is the best model for programming. New generation of code tools like Cursor all use Claude as the main model. reply petesergeant 45 minutes agorootparent> Claude is the best model for programming This week. reply iLemming 15 minutes agorootparentAnecdotally, Claude seems to hallucinate more during certain hours. It's amusing to watch, almost like your dog that gets too bored and stops responding to your commands - you say \"sit\" and he looks at you, tilts his head, looks straight up at you, almost like saying \"I know what you're saying...\" but then decides to run to another room and bring his toy. And you'd be wondering: \"darn, where's that toughest, most obidient and smart Belgian malinois that just a few hour ago was ready to take down a Bin Laden?\" reply square_usual 19 minutes agorootparentprevIt has held this position since at least June. The Aider LLM leaderboards [1] have the Sonnet 3.5 June version beating 4o handily. Only o1-preview beat it narrowly, but IIRC at much higher costs. Sonnet 3.5 October has taken the lead again by a wide margin. 1: https://aider.chat/docs/leaderboards/ reply GaggiX 32 minutes agorootparentprevIt has been for the last several months now. reply reubenmorais 1 hour agoparentprevWith Claude on Bedrock I can use LLMs in production without sending customer data to the US. And if you're already on AWS it's super easy to onboard wrt. auth and billing and compliance. reply JamesBarney 1 hour agoparentprevClaude api use is already as high as openai. I believe that market will grow far more over time than chat as AI gets embedded in more of the applications we already use. reply atsaloli 1 hour agoparentprevI am in Operations. I use it (and pay for it) because the free version seemed to work best for me compared to Perplexity (which had been my go-to) and ChatGPT/OpenAI. reply hamburga 1 hour agoparentprevGovernment alone could be huge, with this recent nonsense about the military funding a “Manhattan project for AI” and the recently announced Pentagon contracts. reply liquidise 1 hour agoprevCan someone with familiarity in rounds close to this size speak to their terms? For instance: i imagine a significant part of this will be “paid” as AWS credits and is not going to be reflected as a balance in a bank account transfer. reply swyx 1 hour agoprevrelated coverage - https://www.anthropic.com/news/anthropic-amazon-trainium - https://www.aboutamazon.com/news/aws/amazon-invests-addition... - https://techcrunch.com/2024/11/22/anthropic-raises-an-additi... reply OceanBreeze77 1 hour agoparentWhat's the difference between trainium and the AWS bedrock offering? reply newfocogi 1 hour agorootparentAWS Trainium is a machine learning chip designed by AWS to accelerate training deep learning models. AWS Bedrock is a fully managed service that allows developers to build and scale generative AI applications using foundation models from various providers. Trainium == Silicon (looks like Anthropic has agreed to use it) Bedrock == AWS Service for LLMs behind APIs (you can use Anthropic models through AWS here) reply lasermike026 58 minutes agoprevDoes anyone know how they are going to make money and turn a profit one day? reply thornewolf 8 minutes agoparentLLM inference is getting cheaper year over year. It often loses money now, it may eventually stop losing money when it gets cheap enough to run. - But surely the race to the bottom will continue? Maybe, but they do offer a consumer subscription that can diverge from actual serving costs. /speculation reply pknerd 1 hour agoprevRival? They kick you out after a few messages and ask you to come back later. Gpt doesn't do that reply Etheryte 13 minutes agoparentAnecdotal experience, but as far as I've played around with them, Claude's models have given me a better impression. I would much rather have great responses with lower availability than mediocre responses available all the time. reply KaoruAoiShiho 42 minutes agoprevI know that if Nvidia did this lots of people on twitter would be screaming about fraud and self-dealing. reply danvoell 1 hour agoprevGreat move. The value to easily deploying content, code, anything digital to AWS is immense. reply bfrog 41 minutes agoprevMcAfee like investing reply blibble 1 hour agoprevwhat does this say about their internal teams working on the same thing? reply seydor 1 hour agoprevGotta protect those H100s from rusting reply yieldcrv 50 minutes agoprevits a better experience, prints out token responses faster, and doesn't randomly 'disconnect' or whatever ChatGPT does I hope they're also cooking up some cool features and can handle capacity reply uneekname 1 hour agoprevAs someone who doesn't really follow the LLM space closely, I have been consistently turning to Anthropic when I want to use an LLM (usually to work through coding problems) Beside Sonnet impressing me, I like Anthropic because there's less of an \"icky\" factor compared to OpenAI or even Google. I don't know how much better Anthropic actually is, but I don't think I'm the only one who chooses based on my perception of the company's values and social responsibility. reply valbaca 1 hour agoparent> or even Google > Last year, Google committed to invest $2 billion in Anthropic, after previously confirming it had taken a 10% stake in the startup alongside a large cloud contract between the two companies. reply uneekname 1 hour agorootparentWell, there you go. These companies are always closer than they seem at first glance, and my preference for Anthropic may just be patting myself on the back. reply rafark 1 hour agorootparentBut why though? Claude is REALLY good at programming. I love it reply mossTechnician 1 hour agoparentprevPersonally, I find companies with names like \"Anthropic\" to be inherently icky too. Anthropic means \"human,\" and if a company must remind me it is made of/by/for humans, it always feels less so. E.g. The Browser Company of New York is a group of friendly humans... Second, generative AI is machine generated; if there's any \"making\" of the training content, Anthropic didn't do it. Kind of like how OpenAI isn't open, the name doesn't match the product. reply derefr 27 minutes agorootparent> Anthropic means \"human,\" and if a company must remind me it is made of/by/for humans Why do you think that that's their intended reading? I had assumed the name was implying \"we're going to be an AGI company eventually; we want to make AI that acts like a human.\" > if there's any \"making\" of the training content, Anthropic didn't do it This is incorrect. First-gen LLM base models were made largely of raw Internet text corpus, but since then all the improvements have been from: • careful training data curation, using data-science tools (or LLMs!) to scan the training-data corpus for various kinds of noise or bias, and prune it out — this is \"making\" in the sense of \"making a cut of a movie\"; • synthesis of training data using existing LLMs, with careful prompting, and non-ML pre/post-processing steps — this is \"making\" in the sense of \"making a song on a synthesizer\"; • Reinforcement Learning from Human Feedback (RLHF) — this is \"making\" in the sense of \"noticing when the model is being dumb in practice\" [from explicit feedback UX, async sentiment analysis of user responses in chat conversations, etc] and then converting those into weights on existing training data + additional synthesized \"don't do this\" training data. reply noirbot 1 hour agoparentprevYea, even if they're practically as bad, there's value in not having someone like Altman who's out there saying things about how many jobs he's excited to make obsolete and how much of the creative work of the world is worthless. reply apwell23 1 hour agorootparentor that 'AI is going to solve all of physics' or that 'AI is going to clone his brain by 2027' . PG famously called him 'Michael jordan of listening' , i would say he is 'Michael jordan of bullshitting' reply thisiscrazy2k 55 minutes agorootparentUnfortunately, that position is already held by Musk. reply MichaelZuo 1 hour agorootparentprevI don’t think he ever said or even implied any percentage of ‘creative work of the world is worthless’? A lot less valuable then what artists may have desired or aspired to at the time of creation, sure, but definitely with some value. reply staticman2 58 minutes agorootparentDoesn't he basically troll people on Twitter constantly? reply Der_Einzige 47 minutes agoparentprevFunny, I use Mistral because it has 'more\" of that same factor, even in the name! They're the only company who doesn't lobotomize/censor their model in the RLHF/DPO/related phase. It's telling that they, along with huggingface, are from le france - a place with a notably less puritanical culture. reply maxclark 1 hour agoprev [–] Is this really a $4B investment, or credits on AWS? AWS margins are close to 40%, so the real cost of this \"investment\" would be way less than the press release. reply swyx 1 hour agoparent [–] https://techcrunch.com/2024/11/22/anthropic-raises-an-additi... > \"This new CASH infusion brings Amazon’s total investment in Anthropic to $8 billion while maintaining the tech giant’s position as a minority investor, Anthropic said.\" reply mef 1 hour agorootparentwhether cash or credit, it's all going right back to AWS reply mistrial9 1 hour agorootparentprev [–] ok but how much cash, really.. looks ambiguous. ps- plenty of people turning a blind eye towards rampant valuation inflation and \"big words\" statements on deals. Where is the grounding on the same dollars that are used at a grocery store? The whole thing is fodder for instability in a big way IMHO reply Etheryte 9 minutes agorootparent [–] I don't really see any ambiguity? If the reporting is accurate, the whole $4B is cash. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Amazon has announced a significant $4 billion investment in Anthropic, an AI startup founded by former OpenAI executives, increasing its total investment to $8 billion.- Amazon Web Services (AWS) will serve as Anthropic's primary cloud and training partner, offering AWS customers early access to customize Anthropic's Claude chatbot with their data.- This investment highlights the competitive nature of the generative AI market, with Anthropic recently unveiling AI agents for complex tasks and Google having previously invested $2 billion in the company."
    ],
    "commentSummary": [
      "Amazon is investing $4 billion in Anthropic, a competitor to OpenAI, to address capacity issues and enhance performance for Anthropic's Pro service.",
      "The investment is expected to involve AWS (Amazon Web Services) credits, potentially lowering Amazon's actual expenditure, and includes using Amazon's chips for AI model training.",
      "This strategic move supports Amazon's goal to increase AWS revenue and expand its artificial intelligence capabilities."
    ],
    "points": 193,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1732292717
  },
  {
    "id": 42210689,
    "title": "Autoflow, a Graph RAG based and conversational knowledge base tool",
    "originLink": "https://github.com/pingcap/autoflow",
    "originBody": "autoflow Introduction An open source GraphRAG (Knowledge Graph) built on top of TiDB Vector and LlamaIndex and DSPy. Live Demo: TiDB.AI Documentation: Docs Features Perplexity-style Conversational Search page: Our platform features an advanced built-in website crawler, designed to elevate your browsing experience. This crawler effortlessly navigates official and documentation sites, ensuring comprehensive coverage and streamlined search processes through sitemap URL scraping. You can even edit the Knowledge Graph to add more information or correct any inaccuracies. This feature is particularly useful for enhancing the search experience and ensuring that the information provided is accurate and up-to-date. Embeddable JavaScript Snippet: Integrate our conversational search window effortlessly into your website by copying and embedding a simple JavaScript code snippet. This widget, typically placed at the bottom right corner of your site, facilitates instant responses to product-related queries. Deploy Deploy with Docker Compose (with: 4 CPU cores and 8GB RAM) Tech Stack TiDB – Database to store chat history, vector, json, and analytic LlamaIndex - RAG framework DSPy - The framework for programming—not prompting—foundation models Next.js – Framework shadcn/ui - Design Contact Us You can reach out to us on @TiDB_Developer on Twitter. Contributing We welcome contributions from the community. If you are interested in contributing to the project, please read the Contributing Guidelines. License TiDB.AI is open-source under the Apache License, Version 2.0. You can find it here.",
    "commentLink": "https://news.ycombinator.com/item?id=42210689",
    "commentBody": "Autoflow, a Graph RAG based and conversational knowledge base tool (github.com/pingcap)186 points by jinqueeny 16 hours agohidepastfavorite28 comments xianshou 3 hours agoI ask \"what is TiDB\" in the demo as suggested, and it takes 2 minutes to start responding in the midst of a multi-stage workflow with several steps each of graph retrieval, vector search, generation, and response combination. Each of these is individually cool, but it strikes me as tragic that so much effort has been put into an intricate workflow and beautifully crafted UI only to culminate in a completely useless hello-world example, which after 5+ minutes of successive querying and response-building concludes with a network error. I could use this to build exactly what I need...after stripping out 80% of the features to make it streamlined and responsive. Why isn't that minimal version the default? reply striking 1 hour agoparentIt appears to be much faster on more specific questions (like the ones that are suggested after you ask it \"what is TiDB\"). I got a response in about 40s on the question \"How does TiDB's cloud-native design enhance its scalability and reliability compared to traditional MySQL databases?\" Also, what's wrong with a nice UI? It appears to mostly be components from https://ui.shadcn.com/. Is there something wrong with good frontend craft, especially for a demo where you're trying to sell something? It seems like something that is being offered as a self-contained tool that's easy for end users to play with, which isn't going to be the minimal version. I'm sure you could build something that suits your needs exactly, but it would be hard for someone else to predict your exact needs, and there's a decent chance everyone needs or wants a slightly different set of features, and that those things may not make for the most ideal demo. I am personally far from the typical profile of an AI booster, but I can't help but say something about what I feel is a middlebrow dismissal. reply andai 2 hours agoparentprevWhat would you remove? reply silversmith 7 hours agoprevIs this wholly self-hostable? I'd be curious to run something like this on a home server, have some small model via ollama slowly chew through my documents / conversations / receipts / .... and provide a chat-like search engine over the whole mess. reply manishsharan 3 hours agoparentHere is how I am implementing something close to what you mentioned. In my setup, I make sure to create a readme.md at the root of every folder which is a document for me as well as LLM that tells me what is inside the folder and how it is relevant to my life or project. kind of a drunken brain dump for the folder . I have a cron job that executes every night and iterates through my filesystem looking for changes since the last time it ran. If it finds new files or changes, it creates embeddings and stores them in Milvus. The chat with LLM using Embeddings if not that great yet. To be fair,I have not yet tried to implement the GraphRAG or Claude's contexual RAG approaches. I have a lot of code in different programming languages, text documents, bills pdf, images. Not sure if one RAG can handle it all. I am using AWS Bedrock APIs for LLama and Claude and locally hosted Milvus reply j45 2 hours agorootparentWondering if you have tried AnythingLLM, and if so what you thought of it. reply manishsharan 2 hours agorootparentI have not .. but this seems to be something I must try. reply thawab 9 hours agoprevThanks a lot, this is the first time i saw a RAG using DSPy. I wanted to know about the expected cost. A few days ago fast graphrag compared their implementation with Microsoft: > Using The Wizard of Oz, fast-graphrag costs $0.08 vs. graphrag $0.48 — a 6x costs saving that further improves with data size and number of insertions. reply visarga 13 hours agoprevI'd love to see a GraphRAG browser that collects the pages I visit automatically. reply _flux 11 hours agoparentMany years ago there used to be a Firefox extension (..or might have even been a Mozilla one..) that would store all the pages I visit. I recall its name was Breadcrumbs but I could be misremembering. Space is cheap, or at least affordable if one would exclude videos, which are probably technically more difficult to archive anyway, but sometimes one remembers having seen content that is never to be found again. I think it would be useful to have just a personal basic search engine on that kind of contents, but possibly a RAG or even a fine tuned LLM would be even cooler. Actually, e.g. Firefox could do that at least for its bookmarks and tabs, though it already does provide the function for tagging bookmarks. And I think there's probably an extension for searching tabs' contents.. reply irthomasthomas 4 hours agorootparentNot identical but I started building a smart bookmark tool that stores the content in vectors and sqlite dB and hosts them in GitHub issues with labels managed by the ai. Check it: https://undecidability.com and code lives at https://github.com/irthomasthomas/label-maker It's a bit rough but there is a working cli. It uses local jina embeddings model but openai logprobs to determine when to create new labels. reply fire_lake 10 hours agorootparentprevGiven how personal browsing history can be this is a great use case for local LLMs. I would love for Mozilla to deliver on this. reply jumping_frog 3 hours agorootparentBuilding personal assistant could be beneficial to Mozilla based on how much we do online. I would like to track changes to my beliefs based on how I came across new information. In future, the AI could automatically shorten paragraphs in essays about topics or terms I am already aware of while keeping new concepts introduced in it full expanded so that I grok them better. reply gazreese 7 hours agorootparentprevI need this so much, someone please build it ASAP. This would be so useful! reply TiredOfLife 9 hours agorootparentprevThe original version of read it later (now Mozilla owned Pocket) had that option. but then removed that option because it went against their commercial interests. reply monkeydust 7 hours agorootparentPocket is good. I use it across all my devices, simple and works for me but do wonder if they could or should do more with the data they collect from me which is all the things I really care about. reply 3abiton 5 hours agorootparentWhat's the selling point for it though? I don't get it? reply jpt4 1 hour agoparentprevLocal archiving tool I've been testing: webchiver.com reply m-s-y 5 hours agoparentprevI’d love to see a brain interface so that all these pages we visit can instantly become available to our own non-ai in-brain all-human reasoning. reply TiredOfLife 9 hours agoparentprevAccording to HN and Reddit that would be spyware and and you are wrong for wanting that. reply stogot 9 hours agorootparentOnly if it’s turned on by default and uploaded to the cloud. Privacy and user choice are what these readers want reply TiredOfLife 5 hours agorootparentThat's exactly what Recall is: offline and fully customizable, but HN/Reddit went mad over it. reply woodson 4 hours agorootparentThey got mad because you got Recall in an update, no matter whether you wanted it or not, and after another update you couldn’t uninstall it anymore. No choice. reply kristjansson 11 hours agoprevFYI the 'StackVM' link that pops up appears to show all inbound messages. https://stackvm-ui.vercel.app/tasks/3710e8d2-fb66-4274-9f78-... reply sykp241095 10 hours agoparentHi, this link is currently for demo purposes. With the help of StackVM, we can DEBUG a RAG retrieval flow step by step and reevaluate the retrieval plan. reply kristjansson 3 hours agorootparentSure, security expectations for a demo are ~0, but “everyone can see everyone else’s inputs” is surprising even by demo standards reply asabla 12 hours agoprev [–] Oh, this looks pretty well made. Since it's using nextjs and shadcn/ui, I wonder if they also used v0 to generate components. Has anyone any experience with TiDB? Haven't heard about it before this post reply datadeft 2 hours agoparent [–] Yes I have some experience with TiDB. It is pretty amazing actually. They came up with a novel way of distributing data across nodes and having strong consistency while also maintaining great performance. We are recommending it to some of our clients who are looking for an easy scaling option with MySQL (TiDB is MySQL compatible on the connector level.) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Autoflow is an open-source Knowledge Graph called GraphRAG, utilizing TiDB Vector, LlamaIndex, and DSPy, featuring a perplexity-style conversational search and a website crawler for sitemap URL scraping.",
      "It can be deployed using Docker Compose, requiring 4 CPU cores and 8GB RAM, and includes a tech stack of TiDB, LlamaIndex, DSPy, Next.js, and shadcn/ui.",
      "Autoflow allows users to edit the Knowledge Graph for accuracy and offers an embeddable JavaScript snippet for integrating a conversational search window on websites."
    ],
    "commentSummary": [
      "Autoflow, a tool based on Graph RAG (Recurrent Attention Graph), has a complex workflow that some users find slow and cumbersome, especially for basic queries.",
      "While the user interface is well-designed, there are calls for a more streamlined version to cater to users who may not need all its intricate features.",
      "Discussions highlight interest in self-hosting, personal data management, and the potential use of local LLMs (Large Language Models) to improve personal browsing history management, emphasizing privacy and user choice."
    ],
    "points": 186,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1732243334
  },
  {
    "id": 42211280,
    "title": "Amazon S3 now supports the ability to append data to an object",
    "originLink": "https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/",
    "originBody": "Amazon S3 Express One Zone now supports the ability to append data to an object Posted on: Nov 21, 2024 Amazon S3 Express One Zone now supports the ability to append data to an object. For the first time, applications can add data to an existing object in S3. Applications that continuously receive data over a period of time need the ability to add data to existing objects. For example, log-processing applications continuously add new log entries to the end of existing log files. Similarly, media-broadcasting applications add new video segments to video files as they are transcoded and then immediately stream the video to viewers. Previously, these applications needed to combine data in local storage before copying the final object to S3. Now, applications can directly append new data to existing objects and then immediately read the object, all within S3 Express One Zone. You can append data to objects in S3 Express One Zone in all AWS Regions where the storage class is available. You can get started using the AWS SDK, the AWS CLI, or Mountpoint for Amazon S3 (version 1.12.0 or higher). To learn more, visit the S3 User Guide.",
    "commentLink": "https://news.ycombinator.com/item?id=42211280",
    "commentBody": "Amazon S3 now supports the ability to append data to an object (amazon.com)184 points by notgiorgi 14 hours agohidepastfavorite65 comments simonw 13 hours agoWrote some notes on this here: https://simonwillison.net/2024/Nov/22/amazon-s3-append-data/ Key points: - It's just for the \"S3 Express One Zone\" bucket class, which is more expensive (16c/GB/month compared to 2.3c for S3 standard tier) and less highly available, since it lives in just one availability zone - \"With each successful append operation, you create a part of the object and each object can have up to 10,000 parts. This means you can append data to an object up to 10,000 times.\" That 10,000 parts limit means this isn't quite the solution for writing log files directly to S3. reply jiggawatts 11 hours agoparentWow, I'm surprised it took AWS this long to (mostly) catch up to Azure, which had this feature back in 2015: https://learn.microsoft.com/en-us/rest/api/storageservices/u... Azure supports 50,000 parts, zone-redundancy, and append blobs are supported in the normal \"Hot\" tier, which is their low-budget mechanical drive storage. Note that both 10K and 50K parts means that you can use a single blob to store a day's worth of logs and flush every minute (1,440 parts). Conversely, hourly blobs can support flushing every second (3,600 parts). Neither support daily blobs with per-second flushing for a whole day (86,400 parts). Typical designs involve a per-server log, per hour. So the blob path looks like: \"{account}/{path}/{year}/{month}/{day}/{hour}_{servername}.txt\" This seems insane, but it's not a file system! You don't need to create directories, and you're not supposed to read these using VIM, Notepad, or whatever. The typical workflow is to run a daily consolidation into an indexed columnstore format like Parquet, or send it off to Splunk, Log Analytics, or whatever... reply ak217 7 hours agorootparentMicrosoft did this by sacrificing other features of object storage that S3 and GS had since the beginning, primarily performance, automatic scaling, unlimited provisioning and cross-sectional (region wide) bandwidth. Azure blob storage did not have parity on those features back in 2015 and data platform applications could not be implemented on top of it as a result. Since then they fixed some of these, but there are still areas where Azure lacks scaling features that are taken for granted on AWS and GCP. reply cedilla 9 hours agorootparentprevIf I need to consolidate anyway, is this really a win for this use case? I could just upload with {hour}_{minute}.txt instead of appending every minute, right? reply jiggawatts 7 hours agorootparentConsolidation is for archival cost efficiency and long-term analytics. If you don't append regularly, you can lose up to 59 minutes of data. reply zaphirplane 7 hours agorootparentprevIn all fairness. Shipping unreliable features for unreliable services is a lot easier reply sofixa 9 hours agorootparentprev> Wow, I'm surprised it took AWS this long to (mostly) catch up to Azure, which had this feature back in 2015: Microsoft had the benefit of starting later and learning from Amazon's failures and successes. S3 dates from 2006. That being said, both Microsoft and Google learned a lot, but also failed at learning different things. GCP has a lovely global network, which makes multi-region easy. But they spent way too much time on GCE and lost the early advantage they had with Google App Engine. Azure severely lacks in security (check out how many critical cross-tenant security vulnerabilities they've had in the past few years) and reliability (how many times have there been various outages due to a single DC in Texas failing; availability zones still aren't the default there). reply kochie 11 hours agorootparentprevAWS ranks features based on potential income from customers. Normally there’s a fairly big customer PFR needed to get a service team to implement a new feature. reply jiggawatts 11 hours agorootparentI always found it strange that AWS seems to have 2-3x as many products or services as Azure, but it has these bizarre feature gaps where as an Azure user I think: \"Really? Now? In this year you're finally getting this?\" (Conversely, Azure's low-level performance is woeful in comparison to AWS and they're still slow-walking the rollout of their vaguely equivalent networking and storage called Azure Boost.) reply bradleyjg 5 hours agorootparentLow level performance is underselling the issues. Blob storage is not infinitely scalable. That means it’s just not the same thing as s3. reply WaxProlix 11 hours agorootparentprevI've only used azure a little bit, and mostly liked it - but I'd love to know what kinds of things you're referring to here (mostly on AWS only, so probably I don't even know what I'm missing out on). reply jiggawatts 8 hours agorootparentWhat Azure has that from what I've seen AWS does not: Resource Groups that actually act like folders, not just as special tags. Resources with human-readable names instead of gibberish identifiers. Cross-region and cross-subscription (equiv. to AWS account) views of all resources as the default, not as a special feature. Single pane-of-glass across all products instead of separate URLs and consoles for each thing. E.g.: a VM writing to an S3 bucket dedicated to it are \"far apart\" from each other in AWS consoles, but the equivalent resources are directly adjacent to each other in an Azure Resource Group when viewed in its Portal. Azure Application Insights is a genuinely good APM, and the Log Analytics workspace it uses under the hood is the consistent log collection platform across everything in Azure and even Entra ID and parts of Microsoft 365. It's not as featureful as Splunk, but the query language is up there in capability. Azure App Service has its flaws, but it's by far the most featureful serverless web app hosting platform. Etc... reply withinboredom 7 hours agorootparentDon’t forget, you don’t pay for a stopped vm in azure! You only pay while it is running. This makes things like dev environments much more affordable, since you won’t be paying for nights/weekends. reply twisteriffic 8 hours agorootparentprevKusto is wonderful. I'd love to be able to use outside of log analytics. reply omeid2 12 hours agoparentprevNot directly, but enough to write once every hour for more than a year! reply simonw 12 hours agorootparentYeah, or I guess log rotation will work well - you can write 10,000 lines to one key and then switch to a new key name. reply santiagobasulto 12 hours agoparentprevThis will require some serious buffering. reply electroly 13 hours agoprevThe original title is \"Amazon S3 Express One Zone now supports the ability to append data to an object\" and the difference is extremely important! I was excited for a moment. reply teractiveodular 13 hours agoprevFor comparison, while GCS doesn't support appends directly, there's hacky but effective workaround in that you can compose existing objects together into new objects, without having to read & write the data. If you have existing object A, upload new object B, and compose A and B together so that the resulting object is also called A, this effectively functions the same as appending B into A. https://cloud.google.com/storage/docs/composite-objects#appe... reply pclmulqdq 13 hours agoparentColossus allows appends, so it would make sense that there's a backdoor way to take advantage of that in GCS. It seemed silly of me that Google didn't just directly allow appends given the architecture. reply CharlieDigital 7 hours agoparentprevThere are some limitations[0] to work around (can only compose 32 at a time and it doesn't auto delete composed parts), but I find this approach super useful for data ingest and ETL processing flows while being quite easy to use. [0] https://chrlschn.dev/blog/2024/07/merging-objects-in-google-... reply vdm 11 hours agoparentprevS3 can also do this https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPa... reply thinkharderdev 7 hours agorootparentIt's similar but no really the same thing. It has to be done up front by initiating a multi-part upload to start. The parts are still technically accessible as S3 objects but through a different API. But the biggest limitation is that each part has to be >5MB (except for the final part) reply new_user_final 10 hours agorootparentprevIt's totally different thing and requires special way to initiate multi-part uploading. reply vdm 7 hours agorootparenttotally different how? reply sureIy 12 hours agoprevIt's crazy to me that anyone would still consider S3 after R2 was made available, given the egress fees. I regularly see people switching to R2 and saving thousands or hundreds of thousands by switching. reply JonoBB 11 hours agoparentFor the most part I agree, but we have found that R2 does not handle large files (hundreds of GB or larger) very well. It will often silently fail with nothing being returned, so it’s not possible to handle it gracefully. reply dragonwriter 2 hours agoparentprev> It's crazy to me that anyone would still consider S3 after R2 was made available, given the egress fees. If your compute is on AWS, using R2 (or anything outside of AWS) for object storage means you pay AWS egress for “in-system” operations rather than at the system boundary, which is often much more expensive (plus, you also probably add a bunch of latency compared to staying on AWS infra.) And unless you are exposing your object store directly externally as your interface to the world, you still pay AWS egress at the boundary. Now, if all you use AWS for is S3, R2 may be, from a cost perspective, a no brainer, but who does that? reply remus 10 hours agoparentprevDepends a bit on your use case. If you've got lots of other infra on AWS and you don't need to store that much data then the downside of using another platform can outweigh the cost savings. reply compootr 11 hours agoparentprevdoesn't s3 have 'free' (subsidized!) transfer to other products like ec2 though? it might look better to businesspeople that \"well, we're doing this processing on ec2, why not keep our objects in s3 since it's a bit cheaper!\" reply mjlee 8 hours agorootparentS3 has free data transfer within the same region. reply lijok 10 hours agoparentprevPeople still use S3 because doing business with Cloudflare is a liability. reply mxuribe 5 hours agorootparentGenuinely curious what you meant by this? reply bobnamob 9 hours agorootparentprevElaborate? reply anentropic 9 hours agoparentprevThere are reasons like this: https://github.com/cloudflare/terraform-provider-cloudflare/... reply YetAnotherNick 11 hours agoparentprevIn most cases S3 data is not directly exposed to the client. If the middleware is EC2, then you need to pay the same egress fee, but you you will have lower latency with S3, as EC2 shares the same datacenter as S3. reply crest 7 hours agoparentprevJust wait until Cloudflare decides you can afford to be on a different plan. reply ChrisArchitect 13 hours agoprevPlease fix title: Amazon S3 Express One Zone now supports the ability to append data to an object reply supermatt 9 hours agoprevThis doesnt seem very useful for many cases, given that you NEED to specify a write offset in order for it to work. So you need to either track the size (which becomes more complex if you have multiple writers), or need to first request the size every time you want to do a write and then race for it using the checksum of the current object... Urghhh. reply taeric 8 hours agoprevI'm curious on the different use cases for this? Firehose/kinesis whatever the name seems to have the append case covered in ways that I would think has fewer foot guns? reply styx31 12 hours agoprevI am surprised it was not supported until now? How does it compare to azure blob append (which exists for years)? I have been using azure storage append blob to store logs of long running tasks with periodic flush (see https://learn.microsoft.com/en-us/rest/api/storageservices/u...) reply xkqd 12 hours agoparentI know the whole point of cloud services is to pick and choose, but in general I wouldn’t express “outrage” or scoff when comparing Azure to AWS. I recommend Azure to the smallest and leanest of shops, but when you compare functionality matrices and maturity Azure is a children’s toy. To compare the other way, Azure write blocks target replication blob containers. I consider that a primitive and yet they just outright say you can’t do it. When I engaged our TPM on this we were just told our expectations were wrong and we were thinking about the problem wrong. reply styx31 12 hours agorootparentI did not want to express any outrage (even sarcastically), just surprise and the fact that I don't know very well the AWS offer. > Azure write blocks target replication blob containers I am sorry but what does it mean? The goal of my question was about what are the differences between the two solutions: I know HN is a place where I can read technical arguments based on actual experience. reply merek 13 hours agoprevThis is specifically for S3 \"Express One Zone\" reply exac 13 hours agoprevI wonder what the implications for all the s3-like APIs is going to be. reply insomniacity 12 hours agoparentGood point - plus the conditional writes recently announced. Did anyone else implement that? reply 100pctremote 11 hours agorootparentMinIO actually supported conditional writes a year before S3 reply from-nibly 12 hours agoprevLogs are a terrible usecase for this. Loki already existed and it uses the cheaper more highly available s3 reply kylegalbraith 13 hours agoprevI got excited until I saw the one zone part. That is a critical difference in terms of cost. reply juancampa 13 hours agoparentIsn't it cheaper than normal S3 though? > S3 Express One Zone delivers data access speed up to 10x faster and request costs up to 50% lower than S3 Standard [0] The critical difference seems to be in availability (1 AZ) [0] https://aws.amazon.com/s3/storage-classes/express-one-zone/ reply rrampage 13 hours agorootparentStorage cost is much higher. Express One Zone is $0.16 per GB while standard S3 is $0.023 per GB ( https://aws.amazon.com/s3/pricing/?nc=sn&loc=4 ) reply datatrashfire 13 hours agorootparentprevIngress pricing is indeed cheaper. POST is $0.005 per thousand requests on standard and $0.0025 on express one. Egress and storage however are more expensive on express one than any other tier. For comparison, glacier (instant), standard and express are $0.004, $0.023 and $0.16 per GB. Although slight, standard tier also receives additional discounts above 50 TB. reply Maxious 13 hours agorootparentprev> Compared to S3 Standard, even though the new class offers 50% cheaper request pricing—storage is almost seven times more expensive. The house always wins https://www.vantage.sh/blog/amazon-s3-express-one-zone reply crest 8 hours agoprevI wonder at which point they'll admit they've added back all the complexity of a hierarchical filesystem. reply wood_spirit 11 hours agoprevWill be exciting to see what adaptations are needed and how performance and cost changes for delta lake and iceberg and other cloud mutable data storage formats. It could be really dramatic! S3 is often used as a lowest common denominator, and a lot of the features of azure and gcs aren’t leveraged by libraries and formats that try to be cross platform so only want to expose features that are available everywhere. If these days all object stores do append then perhaps all the data storage formats and libs can start leveraging it? reply klysm 13 hours agoprevThis sounds like a thin wrapper over an underlying object store reply thecleaner 13 hours agoprevI don't understand the bashing in the comments. I image this is a tough distributed systems challenge (as with anything S3). Of course AWS is charging more since they've cracked it. Does anybody know if appending still has that 5TB file limit ? reply msoad 13 hours agoprevDoes it really work for livestreams? Can I stream read and write on the same video file? That is huge if true! Edit: oh it’s only in one AZ reply jrpelkonen 6 hours agoparentLivestream is not usually done by writing to/reeding from a single media file. Instead, the media is broken into few second long segments. The current set of segments are indicated by a HLS and/or DASH manifest, which is updated as new segments appear. reply andrewstuart 12 hours agoprevThere are many, many S3 compatible storage services out there provided by other companies. Most of them cheaper, some MUCH cheaper. reply orthoxerox 11 hours agoparentBut most of them are compliant with the standard S3 API. If I use AWS SDK to write data to my on-prem Ceph/Minio/SeaweedFS/Hitachi storage, I want this SDK to support the concept of appending data to an object. reply chx 13 hours agoprevIf you want to know the differences between Express One Zone and normal, check https://www.vantage.sh/blog/amazon-s3-express-one-zone this blog post. I had no idea this even existed. tl;dr: it's x7 expensive. reply water9 11 hours agoprevIncredible breakthrough. What will they come up with next the ability to remove data from an object? It’s clear that not working from home is really working out for them reply andrewstuart 10 hours agoprevAmazon has no right to do this - it no longer owns the S3 standard and should respect the ecosystem and community. S3 has stagnated for a long time, allowing it to become a standard. Third parties have cloned the storage service and a vast array of software is compatible. There’s drivers, there’s file transfer programs and utilities. What does it mean that Amazon is now changing it. Does Amazon even really own the standard any more, does it have the right to break the long standing standards? I’m reminded of IBM when they broke compatibility of the PS/2 computers just so it could maintain dominance. reply maryndisouza 8 hours agoprev [–] This is a fantastic addition to Amazon S3 Express One Zone! The ability to directly append data to existing objects opens up new possibilities for real-time data processing applications. Whether it's continuously adding log entries or appending video segments in a media workflow, this feature will streamline workflows and improve efficiency for many use cases. It's great to see AWS continuing to innovate and make data management even more flexible and user-friendly. Excited to see how this feature enhances the scalability of applications across various industries! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Amazon S3 Express One Zone now supports appending data to existing objects, eliminating the need for local storage and benefiting applications like log-processing and media broadcasting.",
      "This feature is available in all AWS Regions and can be accessed using the AWS SDK, CLI, or Mountpoint for Amazon S3 (version 1.12.0 or higher).",
      "The update is particularly advantageous for applications that require continuous file updates, enhancing efficiency and reducing storage overhead."
    ],
    "commentSummary": [
      "Amazon S3 introduces a new feature allowing data to be appended to objects, but it is limited to the \"S3 Express One Zone\" bucket class, which is costlier and less available than the standard tier.",
      "The feature supports up to 10,000 append operations per object, making it less ideal for direct log file writing, especially when compared to Azure's similar functionality available since 2015.",
      "This development has sparked discussions on the practicality of S3's new feature due to its limitations and higher costs compared to other cloud storage solutions."
    ],
    "points": 184,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1732250799
  },
  {
    "id": 42212992,
    "title": "A “meta-optics” camera that is the size of a grain of salt",
    "originLink": "https://cacm.acm.org/news/a-camera-the-size-of-a-grain-of-salt-could-change-imaging-as-we-know-it/",
    "originBody": "NEWS Architecture and Hardware A Camera the Size of a Grain of Salt Could Change Imaging as We Know It The “meta-optics” camera is 500,000 times smaller than comparable imaging devices. By Logan Kugler Posted Nov 19 2024 Credit: Getty Images Share Twitter Reddit Hacker News Download PDF Print Join the Discussion View in the ACM Digital Library From conventional lenses to metasurfaces Better endoscopes, smartphone cameras, telescopes Further Reading When it comes to cameras, size matters, but not in the way you think. Any time a new smartphone is released, it is easy to drool over the latest, greatest, and biggest features that allow you to take even more stunning selfies composed of even more megapixels. However, in the world of cameras, smaller cameras could end up having a far greater impact on the world at large—and enable a ton of positive applications in society—than the next iPhone camera. Work from researchers at Princeton University and the University of Washington is pointing the way. A team of researchers from both institutions has published work that uses innovative methods and materials to create a “meta-optics” camera that is the size of a single grain of salt. The ultracompact camera system developed by researchers at Princeton University and the University of Washington relies on a technology called a metasurface, which is studded with 1.6 million cylindrical posts and can be produced much like a computer chip. The meta-optics camera is the first device of its kind to produce full-color images that are equal in quality to those produced by conventional cameras, which are an order of magnitude larger. In fact, the meta-optics camera is 500,000 times smaller than conventional cameras that capture the same level of image quality. The approach the researchers used to create this meta-optics camera’s small form factor is a huge deal. They used nano-structures called “metasurfaces” and novel approaches to hardware design to build a meta-optics camera far superior to past efforts, as well as implementing unique AI-powered image post-processing to create high-quality images from the camera. Their work is impressive on its own for breaking through past limitations of meta-optics imaging devices. Yet it is also notable because it opens the door to the creation of extremely small cameras that can create high-fidelity images for a range of industries and use-cases (for instance, by enabling the use of less-invasive medical imaging without compromising image quality). This work also unlocks the science-fiction-like possibilities of turning entire surfaces into cameras made up of thousands of such devices, and launching high-quality, ultra-light telescopes into space. Here’s how they did it—and why it could change the world of imaging as we know it. From conventional lenses to metasurfaces All camera designers and engineers, no matter the type(s) of cameras they design, share the same challenge: they want to make their cameras as compact as possible while still allowing it to record as much light as possible. Smartphone cameras present a great example of the trade-offs inherent in solving this challenge. Each new smartphone packs more computational firepower into smaller and thinner frames, to the point where the newest generations of smartphones look positively futuristic. However, smartphone cameras are still obviously large and obtrusive on otherwise sleek smartphone frames because camera designers are packing more and more lenses into them so they can take higher-quality pictures. This means researchers are always on the hunt for ways to compress more optical power into smaller form factors, said Ethan Tseng, a researcher at Princeton who was part of the team that produced the salt-grain-sized meta-optics camera. “Metasurfaces have emerged as a promising candidate for performing this task,” Tseng said. A metasurface, Tseng explained, is an artificial, man-made material that allows us to affect light in unique ways. It is an ultrathin, flat surface just half a millimeter wide and studded with millions of cylindrical posts, which are called “nano-antennas.” These nano-antennas can be individually tuned by researchers to shape light in certain ways so that, together, they are capable of producing images just like standard refractive glass lenses—but in a device that is much, much smaller. “Using metasurfaces enables us to open a large design space of optics that we only hardly were able to access before with conventional refractive optics,” said Felix Heide, a Princeton professor who is the senior author of the study that produced the salt-grain-sized meta-optics camera. With a standard refractive lens, you can only really shape the surface of the lens and vary the material to get better results. However, with metasurfaces, researchers are able to modulate light at the sub-wavelength level, said Heide. In the salt-grain-sized camera, the research team was able to create a single metasurface that has more light-steering power than a traditional lens, dramatically reducing the overall size of the camera while still achieving similar results. The meta-optic lens itself is 0.5 millimeters in size, while the sensor is 1 millimeter in size, making the entire camera much, much smaller than traditional lenses. The researchers did not invent the concept of using metasurfaces for cameras, but they did determine how to make the approach work in a way that was actually useful in the real world. Meta-optics cameras have been designed before, but none of them can produce images that are of sufficient quality to deploy for imaging use cases. “Existing approaches have been unable to design a meta-optics camera that can capture crisp, wide-field-of-view full-color images,” said Tseng. The research team’s work changed that. Their meta-optics camera is the first high-quality, polarization-insensitive nano-optic imager for full-color, wide field-of-view imaging. “We addressed the shortcomings of previous meta-optics imaging systems through advances in both hardware design and software post-processing,” said Tseng. To do that, the researchers used artificial intelligence to address two challenges: lens design and image processing. First, the team used novel AI optimization algorithms to design the nano-antennas on the actual metasurface. Simulating the optical response of a metasurface and calculating the corresponding gradients can be quite computationally expensive, Tseng said, so the team created essentially fast “proxies” for metasurface physics that allowed them to compute how to design the metasurface very quickly. Then, a physics-based neural network was used to process the images captured by the meta-optics camera. Because the neural network was trained on metasurface physics, it can remove aberrations produced by the camera. “We were the first to treat the metasurface as an optimizable, differentiable layer that can perform computation with light,” said Heide. “This made it possible to effectively treat metasurfaces like layers in optical neural networks and piggyback on the large toolbox of AI to optimize these layers.” Finally, the metasurface physics simulator and the post-processing algorithm were combined into a single pipeline to fabricate the actual meta-optic camera, and then to reconstruct the images it captures into high-quality, full-color images. This innovative combination of hardware and software means that the researchers’ meta-optics camera produces images that could actually be used in real-world contexts, like medical imaging. “Only combined with computation were we able to explore this design space and make our lenses work for broadband applications,” said Heide. Better endoscopes, smartphone cameras, telescopes The potential real-world applications of the research are vast. The most obvious one is medical imaging, which directly benefits from cameras that are as small as possible so as not to be invasive. “We are very excited about miniaturized optics in endoscopes, which could allow for novel non-invasive diagnosis and surgery,” said Heide. Ultra-compact endoscopes powered by a meta-optics camera could even image regions of the body that are difficult to reach with today’s technology. Another major area of interest for using meta-optics cameras—or cameras that incorporate meta-optics techniques—is consumer hardware. The ability to design cameras and lenses that are an order of magnitude smaller than those in devices today opens up exciting possibilities across smartphones, wearables, and augmented reality (AR) and virtual reality (VR) headsets. Your smartphone screen or the back of your phone itself could become a camera, says Heide. Wearables could bake high-quality cameras right into the surfaces of, say, eyeglasses. Or, VR headsets could become dramatically lighter and sleeker, leading to higher adoption and greater use of these devices on the go. Drones also could benefit from significantly smaller cameras. All drones require cameras of some type to perform their work, whether for military purposes like reconnaissance or civilian ones like order delivery. Much smaller cameras would result in far lighter drones that consume far less battery power, said Tseng. In fact, with a breakthrough like the meta-optics camera, the very nature of cameras can be rethought entirely. “Our tiny cameras have also recently allowed us to rethink large cameras as flat arrays of salt-grain cameras—effectively turning surfaces into cameras,” said Heide. Larger metasurfaces could even replace the lenses needed for telescopes, making it not only easier to build them but also to send more powerful lenses into space. While researchers are still in the early stages of brainstorming and engineering potential real-world applications for meta-optics cameras, the way in which metasurfaces are produced has them excited. “Metasurfaces are especially interesting because they can be made using the same mature technology used to produce computer chips,” said Tseng. Today’s computer chips are produced on wafers, and each wafer contains hundreds of identical copies of the chip. Metasurfaces are produced in an identical way, which holds the promise of greatly reducing the individual cost per metasurface produced, he said. Not to mention, while the exact materials used to make metasurfaces vary, the researchers used a silica wafer for their mounting surface and silicon nitride for their nano-antennas. Both materials are compatible with today’s semiconductor manufacturing techniques that pump out computer chips. This means going from sophisticated computer chips to meta-optics cameras might be easier than we think. If so, the picture for how to use these devices in many different industries could get much, much clearer. Further Reading: Sharlach, M., Researchers shrink camera to the size of a salt grain, Princeton University, Nov. 29, 2021, https://engineering.princeton.edu/news/2021/11/29/researchers-shrink-camera-size-salt-grain Smith, A., Researchers develop tiny camera the size of a grain of salt – and it could turn your phone into one big camera, The Independent, Dec. 9, 2021, https://www.independent.co.uk/tech/camera-grain-salt-tiny-princeton-washington-university-b1973070.html Tseng, E. et al. Neural nano-optics for high-quality thin lens imaging, Nature Communications, Nov. 29, 2021, https://www.nature.com/articles/s41467-021-26443-0 About the Authors Logan Kugler is a freelance technology writer based in Tampa, FL. He is a regular contributor to Communications and has written for nearly 100 major publications. Share Twitter Reddit Hacker News Download PDF Print Join the Discussion Submit an Article to CACM CACM welcomes unsolicited submissions on topics of relevance and value to the computing community. You Just Read A Camera the Size of a Grain of Salt Could Change Imaging as We Know It View in the ACM Digital Library ©ACM 0001-0782/24/10 DOI 10.1145/3695862 Advertisement Advertisement Join the Discussion (0) Become a Member or Sign In to Post a Comment Sign In Sign Up The Latest from CACM Explore More News Nov 21 2024 Detecting/Explaining Industrial Hacks R. Colin Johnson Artificial Intelligence and Machine Learning News Nov 20 2024 Security via AI Mark Halper Artificial Intelligence and Machine Learning News Nov 14 2024 The AI Spy Karen Emslie Artificial Intelligence and Machine Learning Shape the Future of Computing ACM encourages its members to take a direct hand in shaping the future of the association. There are more ways than ever to get involved. Get Involved Communications of the ACM (CACM) is now a fully Open Access publication. By opening CACM to the world, we hope to increase engagement among the broader computer science community and encourage non-members to discover the rich resources ACM has to offer. Learn More",
    "commentLink": "https://news.ycombinator.com/item?id=42212992",
    "commentBody": "A “meta-optics” camera that is the size of a grain of salt (acm.org)169 points by rbanffy 7 hours agohidepastfavorite106 comments bhaney 6 hours ago> produce full-color images that are equal in quality to those produced by conventional cameras I was really skeptical of this since the article conveniently doesn't include any photos taken by the nano-camera, but there are examples [1] in the original paper that are pretty impressive. [1] https://www.nature.com/articles/s41467-021-26443-0/figures/2 reply roelschroeven 5 hours agoparentThose images are certainly impressive, but I certainly don't agree with the statement \"equal in quality to those produced by conventional cameras\": they're quite obviously lacking in sharpness and color. reply queuebert 6 minutes agorootparentTiny cameras will always be limited in aperture, so low light and depth of field will be a challenge. reply neom 5 hours agorootparentprevconventional ultra thin lens cameras are mostly endoscopes, so it's up against this: https://www.endoscopy-campus.com/wp-content/uploads/Neuroend... reply jvanderbot 3 hours agorootparentJust curious, what am I looking at here? reply neom 3 hours agorootparentmy education is on the imaging side not the medical side but I believe this: https://www.mayoclinic.org/diseases-conditions/neuroendocrin... + this: https://emedicine.medscape.com/article/176036-overview?form=... - looks like it was shot with this: https://vet-trade.eu/enteroscope/218-olympus-enteroscope-sif... reply dylan604 2 hours agorootparentprevThere's one of those Taboola type ads going around with a similar image that suggests it is a close up of belly fat. Given the source and their propensity for using images unrelated to topic, so not sure if that's what it really is. reply card_zero 4 hours agorootparentprevI wonder how they took pictures with four different cameras from the exact same position at the exact same point in time. Maybe the chameleon was staying very still, and maybe the flowers were indoors and that's why they didn't move in the breeze, and they used a special rock-solid mount that kept all three cameras perfectly aligned with microscopic precision. Or maybe these aren't genuine demonstrations, just mock-ups, and they didn't even really have a chameleon. reply derefr 44 minutes agorootparentThey didn't really have a chameleon. See \"Experimental setup\" in the linked paper [emphasis mine]: > After fabrication of the meta-optic, we account for fabrication error by performing a PSF calibration step. This is accomplished by using an optical relay system to image a pinhole illuminated by fiber-coupled LEDs. We then conduct imaging experiments by replacing the pinhole with an OLED monitor. The OLED monitor is used to display images that will be captured by our nano-optic imager. But shooting a real chameleon is irrelevant to what they're trying to demonstrate here. At the scales they're working at here (\"nano-optics\"), there's no travel distance for chromatic distortion to take place within the lens. Therefore, whether they're shooting a 3D scene (a chameleon) or a 2D scene (an OLED monitor showing a picture of a chameleon), the light that makes it through their tiny lens to hit the sensor is going to be the same. (That's the intuitive explanation, at least; the technical explanation is a bit stranger, as the lens is sub-wavelength – and shaped into structures that act as antennae for specific light frequencies. You might say that all the lens is doing is chromatic distortion — but in a very controlled manner, \"funnelling\" each frequency of inbound light to a specific part of the sensor, somewhat like a MIMO antenna \"funnels\" each frequency-band of signal to a specific ADC+DSP. Which amounts to the same thing: this lens doesn't \"see\" any difference between 3D scenes and 2D images of those scenes.) reply gcanyon 2 hours agorootparentprevGiven the size of their camera, you could glue it to the center of another camera’s lens with relatively insignificant effect on the larger camera’s performance. reply cliffy 4 hours agorootparentprevCamera rigs exist for this exact reason. reply dylan604 2 hours agorootparentwhat happens when you go too far from trusting what you see/read/hear on the internet? simple logic gets tossed out like a baby in the bathwater. now, here's the rig I'd love to see with this: take a hundred of them and position them like a bug's eye to see what could be done with that. there'd be so much overlapping coverage that 3D would be possible, yet the parallax would be so small that makes me wonder how much depth would be discernible reply baxtr 5 hours agoparentprevAlso interesting: the paper is from 2021. reply Intralexical 5 hours agoparentprev> Ultrathin meta-optics utilize subwavelength nano-antennas to modulate incident light with greater design freedom and space-bandwidth product over conventional diffractive optical elements (DOEs). Is this basically a visible-wavelength beamsteering phased array? reply itishappy 4 hours agorootparentYup. It's also passive. The nanostructures act like delay lines. reply mrec 2 hours agorootparentInteresting. This idea appears pretty much exactly at the end of Bob Shaw's 1972 SFnal collection Other Days, Other Eyes. The starting premise is the invention of \"slow glass\" that looks like an irrelevant gimmick but ends up revolutionizing all sorts of things, and the final bits envisage a disturbing surveillance society with these tiny passive cameras spread everywhere. It's a good read; I don't think the extrapolation of one technical advance has ever been done better. reply andrepd 6 hours agoparentprevHow does this work? If it's just reconstructing the images with nn, a la Samsung pasting a picture of the moon when it detected a white disc on the image, it's not very impressive. reply nateroling 6 hours agorootparentI had the same thought, but it sounds like this operates at a much lower level than that kind of thing: > Then, a physics-based neural network was used to process the images captured by the meta-optics camera. Because the neural network was trained on metasurface physics, it can remove aberrations produced by the camera. reply Intralexical 5 hours agorootparentI'd like to see some examples showing how it does when taking a picture of completely random fractal noise. That should show it's not just trained to reconstruct known image patterns. Generally it's probably wise to be skeptical of anything that appears to get around the diffraction limit. reply brookst 5 hours agorootparentI believe the claim is that the NN is trained to reconstruct pixels, not images. As in so many areas, the diffraction limit is probabalistic so combining information from multiple overlapping samples and NNs trained on known diffracted -> accurate pairs may well recover information. You’re right that it might fail on noise with resolution fine enough to break assumptions from the NN training set. But that’s not a super common application for cameras, and traditional cameras have their own limitations. Not saying we shouldn’t be skeptical, just that there is a plausible mechanism here. reply neom 4 hours agorootparentwe've had very good chromatic aberration correction since I got a degree in imaging technology and that was over 20 years ago so I'd imagine it's not particularly difficult for name your flavour of ML. reply Intralexical 1 hour agorootparentprevMy concern would be that if it can't produce accurate results on a random noise test, then how do we trust that it actually produces accurate results (as opposed to merely plausible results) on normal images? Multilevel fractal noise specifically would give an indication of how fine you can go. reply alexpotato 4 hours agoprevYears ago I saw an interview with a futurist that mentioned the following: \"One day, your kids will go to the toy store and get a sheet of stickers. Each sticker is actually a camera with an IPv6 address. That means they can put a sticker somewhere, go and point a browser at that address and see a live camera feed. I should point out: all of the technology to do this already exists, it just hasn't gotten cheap enough to mass market. When economies of scale do kick in, society is going to have to deal with a dramatic change in what they think 'physical privacy' means.\" reply brokensegue 2 hours agoparentI'm very skeptical this technology already exists. Maybe if you vastly change the meaning of \"sticker\" reply Workaccount2 2 hours agorootparent\"PCB-with-onboard-battery-and-adhesive-backing-icker\" reply petra 1 hour agoparentprevMaybe it's possible but i can't i seem to think of an energy harvesting Method that would fit that system without direct sunlight. reply gatkinso 4 minutes agoprevAll kinds of exciting implications for small cameras and lens assemblies in VR/AR reply 1024core 14 minutes agoprev> The meta-optics camera is the first device of its kind to produce full-color images that are equal in quality to those produced by conventional cameras, which are an order of magnitude larger. In fact, the meta-optics camera is 500,000 times smaller than conventional cameras that capture the same level of image quality. That would make them 6 orders of magnitude larger. reply Nevermark 25 minutes agoprevWow. Given the tiny dimensions, and wide field, adding regular lenses over an array could create extreme wide field, like 160x160 degrees, for everyday phone cameras. Or very small 360x180 degree stand-alone cameras. AR glasses with a few cameras could operate with 360x160 degrees and be extremely situationally aware! Another application would be small light field cameras. I don't know enough to judge if this is directly applicable, or adaptable to that. But it would be wonderful to finally have small cheap light field cameras. Both for post-focus adjustment and (better than stereo) 3D image sensing and scene reconstruction. reply mwigdahl 6 hours agoprevChalk another one up for Vernor Vinge. This tech seems like it could directly enable the “ubiquitous surveillance” from _A Deepness in the Sky_. Definitely something to watch closely. reply KineticLensman 6 hours agoparentAlso the scatterable surveillance cameras used in his other great novel, 'The Peace War' [0]. Although IIRC they were the size of seeds or similar. [0] https://en.wikipedia.org/wiki/The_Peace_War reply EdwardCoffin 4 hours agorootparent3 or 4 mm in diameter, according to a scene in chapter 6, big enough to have similar resolution to that of a human eye, according to Paul, but able to look in any direction without physically rotating. In chapter 13 the enemy describes them as using Fourier optics, though that seemed to be their speculation - not sure whether it was right. reply ben_w 6 hours agoparentprevI've been interested in smart dust for a while; recently the news seems to have dried up, and while that may have been other stuff taking up all the attention (and investment money), I suspect that many R&D teams went under government NDAs because they are now good enough to be interesting. reply arethuza 6 hours agoparentprevI wonder if someone tried to build a localizer how small they could actually be made? PS It's \"Vernor\" reply cmpb 4 hours agorootparentThe other side to the localizers is the communication / mesh networking, and the extremely effective security partitioning. Even Anne couldn't crack them! It's certainly a lot to package in such a small form reply mwigdahl 6 hours agorootparentprevThanks, I typed that on my phone and it \"fixed\" it for me without me noticing. reply gcanyon 2 hours agoparentprevOr Rudy Rucker’s Postsingular, where the “orphidnet” utility fog enables universal perception/visualization. reply 12907835202 6 hours agoparentprevI haven't read deepness in the sky but it's interesting how wrong alot of scifi got this. Cameras are always considerably bigger than grains of sand reply cmpb 4 hours agorootparentWell, Deepness is set a few thousand years in the future, so we've got some time to work on it. reply ep_jhu 5 hours agoprevEveryone here is thinking about privacy and surveillance and here I am wondering if this is what lets us speed up nano cameras to relativistic speeds with lasers to image other solar systems up close. reply TeMPOraL 5 hours agoparentThank you! It's been a while since I've heard anyone talk about the Starshot project[0]. Maybe this would help revitalize it. Also even without aiming for Proxima Centauri, it would be great to have more cameras in our own planetary system. -- [0] - https://en.wikipedia.org/wiki/Breakthrough_Starshot reply hindsightbias 2 hours agorootparentGilster writes about it every few months https://www.centauri-dreams.org/2024/01/19/data-return-from-... reply skandinaff 5 hours agoparentprevwe would also need a transmitter of equivalent size to send those images back. also an energy source reply Workaccount2 2 hours agorootparentJust do round trip! reply sangnoir 56 minutes agorootparentWe'll need even bigger[1] breakthroughs in propulsion if it's going to be self-propelling itself back to Sol at relativistic speeds. 1. A \"simpler\" sci-fi solution foe a 1-way trip that's still out of our reach is a large light sail and huge Earth-based laser, but his required \"smaller\" breakthroughs in material science reply DCH3416 3 minutes agorootparentWell if you can propel something forward you can propel it backwards as well. I'm assuming some sort of fixed laser type propulsion mechanism would leverage a type of solar sail technology. Maybe you could send a phased laser signal that \"vibrates\" a solar sail towards the source of energy instead of away. reply SoftTalker 49 minutes agorootparentprevAs well as a way around Newton's Third Law. reply sangnoir 36 minutes agorootparentI meant to say the \"simpler\" (but still very complicated) solar sail approach was for a one-way trip. On paper, our civilization can muster the energy required to accelerate tiny masses to relativistic speeds. A return trip at those speeds would require a nee type of science to concentrate that amount of energy in a small mass and use it for controlled propulsion. reply pizza234 6 hours agoprev> as well as implementing unique AI-powered image post-processing to create high-quality images from the camera. They're not comparable, in the intuitive sense, to conventional cameras. reply Etheryte 6 hours agoparentAre they not? Every modern camera does the same thing. Upscaling, denoising, deblurring, adjusting colors, bumping and dropping shadows and highlights, pretty much no aspect of the picture is the way the sensor sees it once the rest of the pipeline is done. Phone cameras do this to a more extreme degree than say pro cameras, but they all do it. reply PittleyDunkin 5 hours agorootparentTo point out the obvious, film cameras don't, nor do many digital cameras. Unless you mean modern in the sense of \"cameras you can buy from best buy right now\", of course. But that isn't very interesting: best buy has terrible taste in cameras. reply sega_sai 5 hours agorootparentThere are a lot of steps like that provided you want an image that you want to show to the user (i.e. Jpeg). You do have somehow merge the 3 Bayer filter detections on rectangular grid, which involves interpolation. You do have to subtract some sort of bias in a detector, possibly correct for different sensitivity across the detector. You have to map the raw 'electron counts' into Jpeg scale which involves another set of decisions/image processing steps reply PittleyDunkin 1 hour agorootparentThere is clear processing in terms of interpreting the raw sensor data as you're describing. Then there are blurrier processes still, like \"denoising\" and \"upscaling\", which straddle the line between bias-correction and alteration. Then there's modification of actual color and luminance as the parent was describing. Now we're seeing full alterations applied automatically with neural nets, literally altering shapes and shadows and natural lighting phenomena. I think it's useful to distinguish all of these even if they are desired. I really love my iPhone camera, but there's something deeply unsettling about how it alters the photos. It's fundamentally producing a different image you can get with either film or through your eyes. Naturally this is true for all digital sensors but we once could point out specifically how and why the resulting image differs from what our eyes see. It's no longer easy to even enumerate the possible alterations that go on via software, let alone control many of them, and I think there will be backlash at some point (or stated differently, a market for cameras that allow controlling this). I've got to imagine it's frustrating for people who rely on their phone cameras for daily work to find out that upgrading a phone necessarily means relearning its foibles and adjusting how you shoot to accommodate it. Granted, I mostly take smartphone photos in situations where i'd rather not be neurotic about the result (candids, memories, reminders, etc) but surely there are professionals out there who can speak to this. reply kristjank 5 hours agorootparentprevHuh, I like your comment. It's such a nice way of pointing out someone equating marketability to quality. reply stevenae 5 hours agorootparentprevPro cameras do not do this to any degree. Edit: by default. reply vlabakje90 5 hours agorootparentThe cameras themselves might not, but in order to get a decent picture you will need to apply demosaicing and gamma correction in software at the very least, even with high end cameras. reply gyomu 4 hours agorootparentRight, and the point ppl are making upthread is that deterministic signal processing and probabilistic reconstruction approaches are apples and oranges. reply oasisaimlessly 1 hour agorootparentIt's trivial to make most AI implementations deterministic; just use a constant RNG seed. reply cubefox 5 hours agorootparentprev\"AI-powered image post-processing\" is only done in smartphones I believe. reply CharlesW 1 hour agorootparentNot anymore. DSLR makers are already using AI (in-camera neural network processing) for things like upscaling and noise removal. https://www.digitalcameraworld.com/reviews/canon-eos-r1-revi... \"The Neural network Image Processing features in this camera are arguably even more important here than they are in the R5 Mark II. A combination of deep learning and algorithmic AI is used to power In-Camera Upscaling, which transforms the pedestrian-resolution 24.2MP images into pixel-packed 96MP photos – immediately outclassing every full-frame camera on the market, and effectively hitting GFX and Hasselblad territory. \"On top of that is High ISO Noise Reduction, which uses AI to denoise images by 2 stops. It works wonders when you're pushing those higher ISOs, which are already way cleaner than you'd expect thanks to the flagship image sensor and modest pixel count.\" reply foul 6 hours agoprevHow would someone detect sensors so small? How would someone excrete an array of these cameras if ingested? reply thfuran 5 hours agoparentIf you eat something the size of a grain of salt that isn't digestible, excreting it poses no problem. reply kaimac 5 hours agoparentprevyou could detect the supporting electronics with a nonlinear junction detector but they are not cheap reply burnt-resistor 4 hours agoprev(2021) The real story: https://light.princeton.edu/publication/neural-nano-optics/ reply curiousObject 6 hours agoprevIf that’s true, maybe it would allow you to put a 10,000 camera array (100x100) on a smartphone, and do interesting things with computational imaging? reply bhaney 6 hours agoparentSome rough numbers: The paper says that reconstructing an actual image from the raw data produced by the sensor takes ~58ms of computation, so doing it for 10,000 sensors would naively take around ten minutes, though I'm sure there's room for optimization and parallelization. The sensors produce 720x720px images, so a 100x100 array of them would produce 72,000x72,000px images, or ~5 gigapixels. That's a lot of pixels for a smartphone to push around and process and store. reply fragmede 6 hours agorootparent72,000*72,000* say, 24 bits per color * 3 colors, equals ~43 GiB per image. edit: mixed up bits and bytes reply bhaney 5 hours agorootparentCareful with your bits vs bytes there reply fragmede 5 hours agorootparentedited, thanks! reply jajko 6 hours agoparentprevSensor size is super important for resulting quality, that's why pros still lug around huge full frame (even if mirrorless) cameras and not run around with phones. There are other reasons ie speed for sports but lets keep it simple (also speed is affected by data amount processed, which goes back to resolution). Plus higher resolution sensors have this nasty habit of producing too large files, processing of which slows down given devices compared to smaller, crisper photos and they take much more space, even more so for videos. That's probably why Apple held to 12mpix main camera for so long, there were even 200mpix sensors available around if wanted. reply taosx 6 hours agoprevThat's a nice innovation that I'm not that happy about, as there would be even less privacy... Maybe on the other side it's good news as ppl are usually their best selves when they are being watched. reply krunck 3 hours agoparentThe watchers would be able to blackmail/control anybody who engages in private activities that they don't want to be public. So who watches the watchers? And who watches them? No. Privacy is 100% required in a free society. reply hypeatei 6 hours agoparentprevUnless you're in your own home, I think it's basically a guarantee at this point that you're being recorded. Could be CCTV, trail cameras, some random recording a TikTok, etc... reply mandmandam 6 hours agoparentprev> ppl are usually their best selves when they are being watched. I don't think that view holds up. A, it very much depends on who is watching, what their incentives are, and what power they hold. And B, it also depends on who is being watched - not everyone thrives under a microscope. Are they the type to feel stifled? Or rebellious? reply Scarblac 6 hours agorootparentAlso, whose definition of \"best self\" are we using, that of the person being watched or of the person controlling the camera? reply ninalanyon 5 hours agoparentprevThat will only hold while being watched is rare. See Clarke and Baxter's Light of Other Days for an examination of the consequences of ubiquitous surveillance. reply dmitrygr 26 minutes agoprevGrains of rice are pretty big, and the images they demonstrate are NOT that impressive. There are cameras you can BUY right now whose size is 1x1x2 mm (smaller than a grain if rice) which produce images that compare. Here is one example: https://www.digikey.com/en/products/detail/ams-osram-usa-inc... It is pretty easy to interface with too - i did it with a pi pico microcontroller: https://x.com/dmitrygr/status/1753585604971917313 reply eurleif 14 minutes agoparentThe OP describes them as the size of a grain of salt, not a grain of rice. reply roflmaostc 6 hours agoprevThis is no news? Has been published in 2021. Also here https://news.ycombinator.com/item?id=29399828 reply jdalgetty 6 hours agoprevThis won’t be good for society. reply whynotminot 6 hours agoparentHow will it be all that different than the ubiquitous imaging we have now? reply timdiggerm 6 hours agorootparentYou can sometimes find a hidden camera today. reply rapnie 5 hours agorootparentprevThe rayban metaglasshole comes to mind. Now its just journalists who fool people in the street with AI face recognition tricks, and its all still fun and games. But this is clearly a horror invention, merrily introduced by jolly zuck, boss of facelook. reply hk__2 6 hours agorootparentprevIt will be the same, but worse. reply delegate 6 hours agoprevFirst thought that came to mind - insect-sized killer drones. I guess that's the informational context we are in right now. reply cjameskeller 3 hours agoparentThe Air Force was already publicly talking about such things in 2009: https://m.youtube.com/watch?v=_5YkQ9w3PJ4 reply Dma54rhs 6 hours agoparentprevYou would still have to power the thing and store the data etc. This is just about the lense. reply ripe 6 hours agoprevMaybe I'm being too skeptical, and certainly I am only a layman in this field, but the amount of ANN-based post-processing it takes to produce the final image seems to cast suspicion on the meaning of the result. At what point do you reduce the signal to the equivalent of an LLM prompt, with most of the resulting image being explained by the training data? Yeah, I know that modern phone cameras are also heavily post-processed, but the hardware is at least producing a reasonable optical image to begin with. There's some correspondence between input and output; at least they're comparable. reply mahoho 5 hours agoparentI've seen someone on this site comment to the effect that if they could use a tool like dall-e to generate a picture of \"their dog\" that looked better than a photo they could take themselves, they would happily take it over a photo. The future is going to become difficult for people who find value in creative activities, beyond just a raw audio/visual/textual signal at the output. I think most people who really care about a creative medium would say there's some kind of value in the process and the human intentionality that creates works, both for the creator who engages in it and the audience who is aware of it. In my opinion most AI creative tools don't actually benefit serious creators, they just provide a competitive edge for companies to sell new products and enable more dilettantes to enter the scene and flood us with mediocrity reply guerrilla 6 hours agoprevThis seems like it's going to ne a serious problem for privacy... not that anyone cares. reply nachox999 3 hours agoparentIt is possible to create realistic images and videos with AI, making anyone do anything. Whether a photo or video is real or not will soon be impossible to distinguish, and it won't matter to those who want to cause harm reply casey2 2 hours agoprevIs it possible to make an orbital death laser with this? reply dcreater 6 hours agoprevNo link in the article to the actual paper? reply hk__2 6 hours agoparentIt’s in the \"Further Reading\" section at the bottom: https://www.nature.com/articles/s41467-021-26443-0 reply d--b 5 hours agoprevThere is some optics thing that looks cool, but it doesn't say how the image is actually recorded. Then there is the whole \"neural\" part. Do these get \"enhanced\" by a generative AI that fills the blur based on the most statistically likely pixels? The article is pretty bad. reply xg15 6 hours agoprevI don't want a camera the size of a grain of salt! At least not while surveillance capitalism and creeping authoritariarism are in full swing... reply nachox999 3 hours agoparentFor those who want to cause harm (discredit), they don't need a real photo; AI is enough reply rapnie 6 hours agoparentprevJust file a complaint with the United Nations Ethics Czar. Oh.. wait. reply kaimac 5 hours agoprevNot a single mention of the obvious privacy concerns in the article reply jansan 6 hours agoprevCan we agree that in the field of cameras we surpassed science fiction? I can remember watching a TV series as a child where a time traveler went back to the 80s and some person told him that everything is about miniaturization. Then he pointed to a little pin on the time traveler's jacket, which was actually a camera, and said: \"This little pin for example could one day hold a full video camera\", which seemed a bit ridiculous at that time. reply rippeltippel 6 hours agoprevIt's interesting how they mention beneficial impacts on medicine and science in general, but everyone knows that the first applications will likely be military and surveillance. reply BiteCode_dev 6 hours agoparentAnd since it's AI improved, all of th will hurt people because of hallucinations. I don't trust human to avoid taking shorcuts once the tech is available, it's too convenient to have \"information\" for so cheap, and less costly to silence the occasional scandal. reply meiraleal 6 hours agoparentprev> but everyone knows that the first applications will likely be military and surveillance. military, surveillance and porn reply cubefox 5 hours agoprevThe article was published in 2021. Why do they repost this as \"news\" three years later? reply api 5 hours agoprevThis kind of thing -- that humans can do today with current technology -- is why if an ET intelligence that could travel interstellar distances wanted to observe us we would never know unless they wanted us to know. Their probes could be the size of sand grains, maybe even dust. Maybe not quite sophons, but not much better as far as our odds of finding anything. I suppose there would have to be something larger to receive signals from these things and send them back (because physics), but that could be hanging out somewhere we'd be unlikely to see it. Yet another Fermi paradox answer: we are looking for big spacecraft when the universe is full of smart dust. reply ashoeafoot 1 hour agoprev [–] as an optic scientists i would protest my work being lumped zogether with the psychedelics of AIchemists reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers at Princeton University and the University of Washington have created a \"meta-optics\" camera, remarkably small at the size of a grain of salt, and 500,000 times smaller than traditional cameras.",
      "This camera utilizes a metasurface with 1.6 million cylindrical posts to capture high-quality, full-color images, potentially transforming fields like medical imaging, consumer electronics, and space telescopes.",
      "The metasurfaces are manufactured using techniques akin to computer chip production, which could lower costs and broaden the range of applications for these ultra-compact cameras."
    ],
    "commentSummary": [
      "A \"meta-optics\" camera, as small as a grain of salt, can capture full-color images, though some question its image quality compared to traditional cameras.",
      "The technology employs subwavelength nano-antennas and AI-driven post-processing to improve image quality, but images may still lack sharpness and color.",
      "The camera's tiny size and potential uses in fields like medicine and military raise privacy concerns and highlight its ongoing relevance and impact since its 2021 publication."
    ],
    "points": 169,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1732275593
  },
  {
    "id": 42211689,
    "title": "Story of the two thousand stolen Playdate handhelds",
    "originLink": "https://podcast.play.date/episodes/s01e31/",
    "originBody": "Episode 31: True Crime Edition Tuesday, November 19, 2024 • 54 minutes Earlier this year, our Financial Controller, Jen, realized our Playdate inventory was 2,000 units short. How did that eventually lead us to a Circle K in North Las Vegas, and just how much should you tip for a roofing consultation, anyway? Buckle up, because we are going for a ride—in Magnum P.I.'s cool car. Show Notes Cabel’s GDC Talk Magnum, P.I. Circle K Froster Episode page with photos Episode transcript Read the transcript A pallet of Playdates addressed to Ship Fusion, dropped off in a Circle K parking lot in the desert. Another photo of the Circle K parking lot courtesy of Ship Fusion, showing where the pallets were mis-delivered. The newly-constructed Circle K across from Ship Fusion, as documented by Jen and Kyle. Two small Dr. Pepper flavored Circle K Frosters, a necessary mystery-solving expense. A tidy stack of Playdate cartons mercilessly ditched, dumped, abandoned! A less-than-tidy stack of Playdate cartons mercilessly ditched, dumped, abandoned! A closer look at the tidy stack; some of the cardboard was a little rumpled, but the Playdates were unharmed. Wouldn't it be funny if this was a Cracker Barrel? Listen Now",
    "commentLink": "https://news.ycombinator.com/item?id=42211689",
    "commentBody": "Story of the two thousand stolen Playdate handhelds (play.date)169 points by textadventure 12 hours agohidepastfavorite91 comments gambiting 9 hours agoThe thing that surprised me the most about it is that FedEx didn't just pay them the 400k for lost shipment. They had all the proof that it was lost, all that Fedex had was a signature of someone who doesn't even work at their fulfilment centre. Even after their \"higher ups\" got involved all that FedEx could do was \"huh, sucks to be you I guess?\" Does freight shipment not have insurance? What's going on here? reply InsideOutSanta 9 hours agoparentThis reminds me of the recent story where an Uber courier stole two MacBooks, there was no signature, CCTV showing no delivery, and Apple was just like \"our carrier has completed the requested investigation, and no further action will be taken by Apple.\" reply Maxious 9 hours agorootparentBack in the old days Apple Security was ruthless about tracking down lost hardware https://www.theguardian.com/technology/2011/sep/01/apple-sta... These days you have to beg them to take their proprietary prototype hardware back https://www.youtube.com/watch?v=QgeEHdAmJDg reply madeofpalk 8 hours agorootparentI mean, I'm sure Apple's still the same for their own stuff. But retail is a whole different beast. reply matwood 7 hours agorootparentprevI followed that story and something seemed off. Not that the person was lying, but something felt left out. Apple and Uber know driver who the order was given to and the police are involved but they also haven’t been able to do anything? Just seems odd. I’ve had trade ins go missing and after a short investigation Apple has always credited me. reply yard2010 8 hours agorootparentprevWhen you're the one making the rules, enforcing them and judging who breaks them, you can basically do whatever you want as long as you pay taxes :) reply actionfromafar 7 hours agorootparentAs long as you properly plan your taxes to 0. reply suprfsat 8 hours agorootparentprevAnd the jury's still out on that last bit. reply kawsper 8 hours agorootparentprevWouldn't the right course of action in that case be to issue a chargeback and let Apple and Uber fight it out? reply actionfromafar 7 hours agorootparentYes, but it can be quite burdensome if your Apple account has any value to you. (Say, if you are an app developer for instance, or aren't quite sure which services you have registered with an apple email address, etc.) reply agos 8 hours agorootparentprevI wonder if this tunes changes once lawyers are involved reply johnnyanmac 7 hours agorootparentgiven that those mac's would have been 4-5k? I sure as heck would have at least consulted a lawyer. I don't want to just lazily have my package stolen in real time. if no one else is seeking justice, I will at least poke the blind woman a bit. reply michaelt 7 hours agoparentprev> all that Fedex had was a signature of someone who doesn't even work at their fulfilment centre. [...] What's going on here? Basically a lot of global logistics runs on trust. If a driver is delivering a pallet to the FooCorp warehouse, he doesn't get given a copy of the FooCorp org chart, or get an example signature to compare against the signature they're given, or get given a map or a secret password or anything like that. He just pulls up to the building that says FooCorp on it, says \"got a delivery for FooCorp\", they let him in and he accepts any name and signature from whoever is near the door. reply gambiting 7 hours agorootparent>>If a driver is delivering a pallet to the FooCorp warehouse, he doesn't get given a copy of the FooCorp org chart, or get an example signature to compare against the signature they're given, or get given a map or a secret password or anything like that. Obviously. But if there is 400 grand on the line, you'd think someone would actually check(when the claim is made). The receiver would say \"you have a signature from person X. Person X doesn't actually work here\". Fedex then says \"ok, prove it\" - and then the receiver does, in whatever way is legally acceptable. Edit: in fact, let me add a bit more - if the shipment was delivered to the right address just signed by someone who didn't actually work there then sure, I think FedEx would be in the clear. But they delivered the parcel to the wrong place - the fact that it was signed for by someone is almost irrelevant, it's the same as having no signature at all. reply gus_massa 6 hours agorootparentWhen I get a package from Mercado Libre here in Argentina they ask for my national ID number [1], take a photo of the number in the door and when it's expensive enough (USD$100?) they ask for a secret word that they send the previous day bay email. For USD$400,000 I'd expect them to do a DNA analysis :) . [1] I'm not sure how they check the number, because they accept the ID number of anyone in the family and even the full time cleaner of the building. They type it and the app takes a second to show a green check. I guess it's a fuzzy list to prevent obvious scams. reply gambiting 6 hours agorootparentHere in the UK Amazon gives you a code that you have to give to the driver for more expensive parcels specifically for this reason. Again doesn't seem like an outrageous thing to implement for a 400k delivery. reply joezydeco 5 hours agorootparentI've ordered expensive things from Amazon to my office in the US and they did the whole code ritual... the driver just dumped it at the door and ran. It's all for show. reply gambiting 4 hours agorootparentOver here I was definitely told by Amazon drivers that unless they type in the code it won't mark the parcel as delivered. The same with GPS trace - unless they are within ~100m of the destination it won't let them actually mark it as delivered. Maybe your driver didn't care about having missing parcels at the end of the day, but the system is strict about it(again, over here, maybe Amazon US is different). reply joezydeco 2 hours agorootparentIt must be, because I got the \"delivered\" message and found it sitting outside our door on the floor. reply michaelt 6 hours agorootparentprevBetter controlled delivery services presumably exist for things like delivering cash to banks, drugs to pharmacies and suchlike. Hell, there are food delivery companies where the customer gets a secret code they have to give to the driver. As far as FedEx is concerned, though? They were not contracted to get any specific name or signature. They were contracted to deliver to a certain address. The GPS says the driver went to the right address, to within the accuracy of a GPS trace. The driver got a name and signature at that time. The driver marked the delivery as successful in their computer. Maybe they call up the driver and say \"Did you deliver to the wrong address?\" and the driver says \"I don't think so, but I make a lot of deliveries so I can't say for sure.\" Sure, in reality the pallet was dropped off in the parking lot across the road from Ship Fusion, instead of at Ship Fusion. But FedEx's records say everything is in order. To be clear I'm not saying this is good, I'm just saying it's normal. reply kruador 2 hours agorootparentIf you look at the map, Shipfusion is 4350 North 5th Street, North Las Vegas. The Circle K is literally next door, on the intersection of North 5th Street and East Craig Road. See https://maps.app.goo.gl/Wz6iNaViGf3ToM1U6. I think what happened here is that the FedEx driver was in such a hurry to drop off that he saw a bunch of people apparently outside Shipfusion, stopped and said 'Hey, delivery for you', rather than going and finding the proper entrance to the loading dock. And to be fair, that's the corner with the company logo on it - looking at Google Streetview I can't see a logo at the back of the building where the loading dock is! And to be even more fair to the driver, they are often given utterly unrealistic amounts of time to drive to the next drop, or to complete the drop. They can only complete their rounds by cutting corners. reply ndiddy 1 hour agorootparentprevThe article you're commenting on mentions that FedEx didn't deliver the pallets of handhelds to the warehouse, but instead left them at an unrelated construction site next door. This is clearly FedEx's fault, they didn't even deliver to the correct address. reply michaelt 1 hour agorootparentSure, FedEx shouldn't have delivered to the wrong address. I'm just saying it's totally normal for the names and signatures to go unchecked when the goods are handed over. The driver can't validate someone is a real employee before handing over the delivery. There's no mechanism for that. reply RateMyPE 6 hours agorootparentprevEverything in the world runs on trust. It's human nature. It's kind of overwhelming when you stop and really think about it. reply steve918 9 hours agoparentprevThis isn't surprising at all if you have every had an interaction with Fedex. reply brk 4 hours agorootparentFWIW, I've had really good interactions with FedEx for the most part. Including for Apple hardware, where I had a FedEx driver pull into the end of my long driveway, wait 5 seconds and then leave. They marked the signature-required MacBook Air as \"undeliverable, nobody home\", while I was in fact home and waiting for the delivery. Called the local FedEx hub and they sent the driver back to me. reply gambiting 4 hours agorootparentSounds like you had a pretty bad experience with FedEx then. reply therein 6 hours agorootparentprevEvery time I bought an electric unicycle, it got stolen at South San Francisco branch. It is totally reproducible, happened 3 times. One time I found some higher up at FedEx on LinkedIn and sent them an InMail. The regional manager really pushed this issue to highest priority for the branch. Branch managers got CC'ed in, drivers, people working at the warehouse; it was an all hands on deck situation. They reassured me that they have cameras everywhere and it couldn't be stolen. It was stolen. They don't know who did it. FedEx is terrible. reply raybb 8 hours agoparentprevWould playdate be able to sue FedEx or take them to small claims court or do you sign something when you use FedEx that says you can't sue them for XYZ? reply pbhjpbhj 7 hours agorootparent'you cannot sue us for not doing the only thing you are paying us for (delivering your goods)' sounds like an inconscionable clause. Surely any worthwhile legal system would make such clauses illegal. Otherwise many scams (fake invoicing, for example) would be essentially legal as long the perp buried a clause in a contract. reply sweetjuly 8 hours agorootparentprev$400k is quite a bit more than they typically let you pursue in small claims court :) reply johnnyanmac 7 hours agorootparentIn this economy? Don't worry just give it 20 years ;) reply whazor 7 hours agoparentprevShipment insurance is normally an optional add-on. IMO, if the shipper doesn’t get it, it is on them. It is nicer for the shipper to decide the value and pay the corresponding price for that. Because you need to know the replacement value of that lost item. This is dependent on all kinds of factors. reply gambiting 7 hours agorootparentIs there any scenario where someone would knowingly decide not to take insurance on a 400k shipment? What would be the reason for doing so? In this case the shipper is the company behind the Playdate, so it seems weird to me they wouldn't insure their own stock. But maybe there's a good reason why this isn't done? reply sakjur 7 hours agorootparentI’m not sure about Playdate, but for a larger company, it’ll be beneficial to stop paying fpr the insurance after a while — the insurance companies bank on the difference between risk tolerance for individuals and in groups. Your liquidity might not be able to survive an event that is fairly insignificant in the context of a larger group (say your house burning down — devastating for you, but zooming out to a city-level it happens occasionally). Larger companies can develop a higher risk tolerance in-house and eliminate the middle hand, and that’ll let them siphon the excess to their shareholders rather than the insurance company. reply soneil 7 hours agorootparentprevIt's not unusual to effectively self-insure for shipping. If insurance costs you more than lost shipments, it stops making sense to pay for insurance. If insurance costs less than lost shipments, it stops making sense for the insurer to offer you insurance at that rate. Insurance works when the loss is disruptive to your cashflow. If I can't afford to absorb that loss right now, it may be worth finding an insurer that can. If I can afford to absorb that cost, it's almost always better to leave my money working for me, than working for the insurer. If you're shipping in volume, self-insurance almost always works out better. You can bet amazon don't insure their shipments to you. If they take the potential cost of insurance, stick it in a pot, and dip into it when there's a shipping issue - that pot should never run dry. If that pot runs dry, it means insurers are operating at a loss, and they're not going to stay in business long. reply brk 4 hours agorootparentprevPackage insurance is more of the exception than the norm, even for stuff like that. The insurance cost is proportionate to the value you claim, and it's not cheap (not like 10%, but still, for $400K, might be $1,000?). That cuts into margins, especially when you consider that you are expecting a global freight company to be able to globally deliver a basic freight item. reply supermatt 9 hours agoprevI wanted to buy a playdate when they first came out, but unfortunately they weren't shipping to my country. Now they do, so I just placed an order 15 mins ago and my partner just received a call from the bank to verify that it wasn't a fraudulent transaction. She just asked me - what is this \"play date\" you just sent $300 to? Oh dear. :D reply Toorkit 7 hours agoparentThen you say \"it's a handheld gaming device\" and the matter is settled. reply johnnyanmac 7 hours agorootparentThen she asks why you spend $300 for a Gameboy when you already have a PS5 and Switch, and at this point you just get the guest room ready for yourself tonight. reply aa-jv 7 hours agorootparentprevThen you spend days on end winding the crank, endlessly .. reply beAbU 7 hours agoparentprevThis comment fells too reddit esque for me, as if it's crafted to solicit upvotes or \"facebook up, hit your lawyer and delete the gym\" style comments. - Why is your partner getting the call from the bank when you placed the order? - If it's a shared account, why would you not forewarn your partner about this transaction? If I'm about to buy pay for something big from our joint account, I sure as hell let my partner know about it ahead of time. - If none of the above applies, then a simple \"it's a portable gaming console that I've been yearning after for ages that I finally ordered earlier today\", and 9/10 times that should settle the matter. reply throwaind29k 7 hours agorootparentIt does feel very reddit-esque the comment, because it seems a tad like an interesting yarn to farm attention without having content of substance. reply johnnyanmac 7 hours agorootparentHN tells personal stories all the time. I think they just wanted to tell a little joke about how suspicious the word \"PlayDate\" sounds out of context. I don't think it's so egregious to warrant the 3rd degree. If anything, the overscrutinizing on an innocent story feels more reddit-esque to me. reply supermatt 4 hours agorootparentIts actually 100% true! It happened no more than a minute before I made the post. I am nowhere near funny enough to make that stuff up :D reply johnnyanmac 3 hours agorootparentI found it cute. I'm sorry some people are so cynical. I put it as simple as this: what is the maximum harm I have for believing this story vs just chucking and moving on? I see zero risk so trying to join this odd callout fake culture is more disruptive than leaving it be. reply KomoD 2 hours agorootparentprevIt's not that serious. reply TYPE_FASTER 5 hours agorootparentprevImagine a Beowulf cluster of Playdates reply romanhn 7 hours agoprevInteresting choice to go with a PI who's focused on recovery rather than criminal convinctions. Given the lack of sophistication in this operation, I suspect recovery would have happened either way, and the thieves might have faced some actual consequences. As is, they didn't lose anything other than the stolen items and will likely continue to capitalize on similar opportunities in the future. reply Terr_ 9 hours agoprevHuh, so a half-baked crime of opportunity, as opposed to a sophisticated operation. Still unclear on how the delivery managed to get put (or taken) to the wrong side of the road at a construction site. Fedex mistake? Trickery by thief? Misdirection by thief that took them from loading-dock? reply josephg 9 hours agoparentAlmost certainly just a fedex mistake taking them to the wrong lot. Happens all the time. And, Hanlon's razor - never attribute to malice what can be explained by stupidity. Its pretty easy to imagine construction workers just signing for everything that arrives, and only afterwards figuring out that the address is wrong. reply Shank 8 hours agoprevI suppose the key insight is that mandatory device registration really saved them. Everyone loves the concept of an entirely open device that doesn’t require this, but if Panic didn’t have registration, it would’ve been impossible to locate the devices, and end up being a $400k write off. reply Hackbraten 9 hours agoprevLove this quote in particular: > Thanks so much for listening, and please don’t steal our Playdates. Because we will find you. reply LeonidasXIV 9 hours agoparentAllegedly. reply Cthulhu_ 8 hours agorootparentI wonder where to put it though. > Thanks so much for alledgedly listening > please don’t alledgedly steal our Playdates. > please don’t steal our alledged Playdates. > Because we will alledgedly find you. reply actionfromafar 7 hours agorootparentAllegedly because we will find you. reply deely3 3 hours agorootparentAllegedly because we will allegedly find allegedly you. reply kuschku 8 hours agoprevI summarized the transcript into an article: https://gist.github.com/justjanne/1e1254bf207c4d98862e040136... Not sure if it helps anyone else, but for me it made the story a lot easier to grasp. I wrote most of it by hand, using an LLM just for a rough outline which I then manually rewrote line by line, streamlined, removed hallucinations, double-checked all quotes, reordered and added images and links. reply deely3 3 hours agoparentHonestly original article is way more interesting and nuanced. I'm afraid LLM version is too short and while technically correct definitely feels like dry list of random facts from transcription. reply kuschku 2 hours agorootparent> original article is way more interesting and nuanced Oh I don't disagree in the slightest, after all it's basically an interview full of personal experiences and anecdotes. But HN already has a problem with people commenting without reading the article, even if that article is relatively short. With an hour-long podcast episode or a transcript stuffed with filler words and partial sentences it's even worse. > I'm afraid LLM version is too short and while technically correct definitely feels like dry list of random facts from transcription Well, the LLM summary was a lot more thrilling and entertaining than my version. Sadly, it was also wrong and full of hallucinations. reply anymouse123456 7 hours agoparentprevThat was amazing. I wasn't up for spending an hour listening to the podcast, but a few minute reading the article you created were well worth it. Thank you! reply jatins 8 hours agoprevwait FedEx just delivers 400k worth of stuff without any KYC or OTP verification?? reply Spooky23 7 hours agoparentUPS pays their drivers very well. FedEx… does not. I’ve had them “deliver” a bunch of PCs to a dumpster. Or drop off a laptop to a garbage can in a Manhattan office. How do I know? The courier took a picture to document the delivery. reply anymouse123456 7 hours agorootparentFedEx employees in our region are consistently grumpy, unhelpful, and extremely abusive with packages. We got video of one FedEx guy kicking retail Apple Computer boxes off the back of his big rig (destroyed 2x iMacs). Our UPS drivers are consistently cheerful, helpful and considerate with the packages they carry. The contrast grows dramatically if you ever suffer some misfortune that requires a phone call into corporate. FedEx is reliably crabby, unhelpful and actively belligerent. These two companies are a perfect illustration of what happens when you make it obvious to your staff how you feel about them. reply johnnyanmac 6 hours agorootparentprevI supposed this begs the obvious question I'll ask: why? It seems like this is an easy way to make sure any and all Fedex employess are looking to jump ship to UPS or any other competitor. reply Spooky23 5 hours agorootparentUPS drivers are teamsters and are pretty committed to the company. You need to work your ass off in distribution centers and other places before you get in a truck. When FedEx was falling behind by not having ground service, they bought another network (RPS iirc) and built out a network of contracted providers. In my area, it was the companies that did newspaper delivery. Some of them are ok, most of them suck. I think the residential delivery is all contracted, but business services are FedEx badged people in some scenarios. reply joezydeco 5 hours agorootparentprevUPS drivers are unionized. FedEx drivers at the local stage are a network of third party companies that contract to FedEx and put their logo on their trucks. Look at the grey fine print by the driver door. reply Cthulhu_ 8 hours agoparentprevI get the impression business-to-business shipping is a lot more informal... somehow. reply RantyDave 9 hours agoprevStrange things are afoot at the circle k. reply markovs_gun 8 hours agoprevImagine being the thief in this case and getting stuck with an entire pallet of these weird indie handhelds that you can't fence because nobody knows what they are just hanging out on your garage. reply actionfromafar 7 hours agoparentIt seemed it was a planned thing, so probably not a big surprise. reply markovs_gun 7 hours agorootparentThat's not the vibe I got. For me it seemed like FedEx showed up, said they had a pallet of electronics, and the guy signed for it as a crime of opportunity. Then someone else stole some later. reply vlaaad 8 hours agoprevnext [9 more] [flagged] nkrisc 8 hours agoparentThanks. I don’t have time to listen and reading a transcript of casual conversation is awful. reply kuschku 8 hours agoparentprevHere's a better version, with images, in the style of a traditional print newspaper article: https://gist.github.com/justjanne/1e1254bf207c4d98862e040136... Not sure if it helps anyone else, but for me it made the story a lot easier to grasp. I wrote most of it by hand, using an LLM just for a rough outline which I then manually rewrote line by line, streamlined, removed hallucinations, double-checked all quotes, reordered and added images and links. reply forrestthewoods 8 hours agoparentprevThis is getting downvoted but I actually quite appreciated it. Story is interesting enough I'll listen to the whole podcast! I can't do that for every single podcast link that I come across. There's not enough time in the year much less the day. reply Wowfunhappy 8 hours agorootparentIt's AI generated, how do I know if it's accurate? It could be making up the whole thing. reply kuschku 6 hours agorootparentThat's why you should combine AI and manual work. AI is great to get a rough template for a story, or to rephrase sentences. Take a look at the revision history of my summary (linked elsewhere in this thread). In the end I had to replace all the quotes and rewrite half of the paragraphs to make sure everything's factually correct, but the end result still has much better tone and phrasing than anything I could've written without AI. reply Wowfunhappy 3 hours agorootparentYour summary is fine. That's different from what the OP of this thread posted. reply nkrisc 7 hours agorootparentprevIf that concerns you, read the actual transcript. I honestly don’t care how accurate this is because beyond casual interest right now, I’m likely never going to think about this again. reply traverseda 8 hours agorootparentprevYeah, hn will just downvote anything with the word ai-generated in it blindly. reply piqufoh 9 hours agoprevnext [7 more] [flagged] jna_sh 9 hours agoparentSource for the arrest? I listened to this the other day and I don’t recall an arrest being mentioned. The podcast in fact covers that the police weren’t very much interested, which is why they got a PI involved, who got the thieves to “return” (dump in an adjacent car park) the devices simply by asking questions. reply saagarjha 9 hours agorootparentI’m guessing the AI hallucinated it. reply gambiting 9 hours agorootparentYou know what, until your comment I haven't even considered that someone just copy pasted the entire transcript into ChatGPT and asked for a summary. It sucks - and I see that happening everywhere actually, especially in facebook groups, people are trying to be \"helpful\" by just copying output from ChatGPT or Gemini, but more often than not it's just completely wrong. reply saagarjha 9 hours agorootparentI can’t say for sure but the thing I thought was suspicious was someone saying “The episode provides an in-depth look at the challenges Panic faced during this ordeal and the measures taken to resolve the situation”. People who actually read the content and offer TL;DRs typically wouldn’t include statements like these that are basically just fluff. reply piqufoh 7 hours agorootparentprevYeah - I don't have an hour to listen to the podcast or read the transcript. I got an AI to summarise the article and it saved me the time, I thought someone else might appreciate the summary (and it appears they did). Perhaps next time I'll add TL;got-an-llm-to-do-it or something reply bmalum 9 hours agoparentprevnext [2 more] [flagged] throawayonthe 9 hours agorootparentthe TLDR seems to be incorrect reply bmalum 9 hours agoprev [–] Surprised by HN again. 1st Place „Story“, wanted to read because no headphones with me, ending up reading a podcast transcript, and TL,DR out. reply voidUpdate 9 hours agoparentI read it all just now, I quite enjoyed it. Its not my preferred format to read but it wasn't bad reply cr3ative 9 hours agoparentprevThe transcript was a good read, I also don’t prefer audio. Admittedly I can skim read quite well though. reply nkrisc 8 hours agorootparentI found it a miserable read. It’s a pain to get through all the tic words and aborted sentences, I gave up very soon after starting. reply saagarjha 9 hours agoparentprev [–] I read it for lunch yesterday. It wasn’t that bad. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Two thousand Playdate handheld gaming devices were stolen, and FedEx has refused to compensate the $400,000 loss, citing a signature from an unauthorized individual.- This incident underscores significant issues in logistics and delivery verification, highlighting a pattern of inadequate responses from companies in similar situations.- The case emphasizes the importance of device registration as a potential method for recovering stolen goods."
    ],
    "points": 169,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1732258435
  },
  {
    "id": 42209272,
    "title": "What's Next for WebGPU",
    "originLink": "https://developer.chrome.com/blog/next-for-webgpu",
    "originBody": "Chrome for Developers Blog What's next for WebGPU Stay organized with collections Save and categorize content based on your preferences. François Beaufort Corentin Wallez Published: November 21, 2024 The WebGPU specification is always evolving, with major companies like Google, Mozilla, Apple, Intel, and Microsoft meeting weekly to discuss its development. The most recent GPU for the Web working group meeting offered a glimpse into the main objectives and features planned for the next iteration of WebGPU. This blog post explores some of the key takeaways from the meeting. Reaching candidate recommendation status A major focus of the meeting was to discuss the progress of Milestone 0 and to finalize the issues that need to be addressed before it can reach candidate recommendation status for the W3C. This is the next step in the standardization process, and it comes with stronger guarantees of stability and intellectual property protection. There was general agreement among the meeting participants that they are no blockers and that these issues can be resolved in a timely manner, paving the way for the W3C candidate recommendation of WebGPU. Prioritizing new features Participants at the meeting also prioritized new features. They started with a list of feature requests compiled from feedback from developers, implementers, and stakeholders. After discussion, the following key WebGPU features for AI were identified: Subgroups and subgroup matrices: Let application use fast local communication between GPU threads, and take advantage of fixed-size matrix multiplication hardware next to shader cores. See the subgroups proposal. Texel buffers: Provide a more efficient way to store and access small data types, like 16-bit or 8-bit values, in a portable way. This is important for some ML image processing algorithms. See the texel buffer slides. UMA buffer mapping: Improve data upload performance by reducing or eliminating copies and synchronization overhead. See the spec issue 2388. Also under consideration and prioritization are the following WebGPU features to unlock new kinds of rendering algorithms: Bindless: This highly anticipated feature proposal is a prerequisite for most leading-edge rendering algorithms because they need scene-wide information. Bindless lets shaders use an unlimited number of resources, including textures, when compared to the relatively strict limits currently. Multi-draw indirect: Lets previous computations on the GPU created multiple draws instead of just one with drawIndirect previously. It is an important capability for GPU-driven rendering like for GPU culling of objects. See the pull request 2315. 64-bit atomics: Either in buffers or textures, it is necessary for doing \"software rasterization\" on the GPU, by bundling the depth-test and writing of a 32-bit payload in a single atomicMax operation. See the issue 4329. To enhance WebGPU's capabilities and integration with the broader web platform, the following WebGPU features have been discussed: Compatibility mode: This mode aims to enable WebGPU to run on a wider range of devices, including those that only support OpenGL ES 3.1. See the compatibility mode proposal. WebXR: Allows the existing WebXR Layers module to interface with WebGPU by providing WebGPU swapchains for each layer type. See the WebGPU/WebXR Integration slides. Canvas2D: Creates better interoperability between Canvas 2D and WebGPU, addressing both performance and ergonomics problems. This WebGPU Transfer proposal would allow having access to text and path drawing in WebGPU, and being able to apply WebGPU rendering to Canvas 2D. The meeting also featured presentations and discussions on efforts to improve WGSL tooling and libraries. One notable initiative is the development of WESL (WGSL Extended Shading Language) which aims to provide a community-driven set of extensions to WGSL. You can find more information in the raw meeting notes. Thoughts This meeting highlighted the importance of collaboration between the WebGPU working group, developers, and the broader graphics community in shaping the future of WebGPU. The working group is actively seeking feedback on the proposed features and is committed to working with developers to ensure that WebGPU meets their needs. The next evolutions of WebGPU promise to be a significant step forward, unlocking new possibilities for web graphics and empowering developers to create even more immersive and engaging web experiences for AI.",
    "commentLink": "https://news.ycombinator.com/item?id=42209272",
    "commentBody": "What's Next for WebGPU (chrome.com)166 points by mariuz 20 hours agohidepastfavorite155 comments jms55 16 hours agoBindless is pretty much _the_ most important feature we need in WebGPU. Other stuff can be worked around to varying degrees of success, but lack of bindless makes our state changes extremely frequent, which heavily kills performance with how expensive WebGPU makes changing state. The default texture limits without bindless are also way too small for serious applications - just implementing the glTF PBR spec + extensions will blow past them. I'm really looking forward to getting bindless later down the road, although I expect it to take quite a while. By the same token, I'm quite surprised that effort is being put into a compatibility mode, when WebGPU is already too old and limiting for a lot of people, and when WebGL(2) is going to have to be maintained by browsers anyways. reply Animats 16 hours agoparent> Bindless is pretty much _the_ most important feature we need in WebGPU. Other stuff can be worked around to varying degrees of success, but lack of bindless makes our state changes extremely frequent, which heavily kills performance with how expensive WebGPU makes changing state. Yes. This has had a devastating effect on Rust 3D graphics. The main crate for doing 3D graphics in Rust is WGPU. WGPU supports not just WebGPU, but Android, Vulkan, Metal, Direct-X 12, and OpenGL. It makes them all look much like Vulkan. Bevy, Rend3, and Renderling, the next level up, all use WGPU. It's so convenient. WGPU has lowest common denominator support. If WebGPU can't do something inside a browser, then WGPU probably can't do it on other platforms which could handle it. So WGPU makes your gamer PC perform like a browser or a phone. No bindless, no multiple queues, and somewhat inefficient binding and allocation. This is one reason we don't see high-performance games written in Rust. After four years of development, WGPU performance has gone down, not up. When it dropped 21% recently and I pointed that out, some people were very annoyed.[1] Google pushing bindless forward might help get this unstuck. Although notice that the target date on their whiteboard is December 2026. I'm not sure that game dev in Rust has that much runway left. Three major projects have been cancelled and the main site for Rust game dev stopped updating in June 2024.[2] [1] https://github.com/gfx-rs/wgpu/issues/6434 [2] https://gamedev.rs/ reply jms55 16 hours agorootparent> This is one reason we don't see high-performance games written in Rust. Rendering is _hard_, and Rust is an uncommon toolchain in the gamedev industry. I don't think wgpu has much to do with it. Vulkan via ash and DirectX12 via windows-rs are both great options in Rust. > After four years of development, WGPU performance has gone down, not up. When it dropped 21% recently and I pointed that out, some people were very annoyed.[1] Performance isn't most of the wgpu maintainer's (who are paid by Mozilla) priority at the moment. Fixing bugs and implementing missing features so that they can ship WebGPU support in Firefox is more important. The other maintainers are volunteers with no obligation besides finding it enjoyable to work on. Performance can always be improved later, but getting working WebGPU support to users so that websites can start targeting it is crucial. The annoyance is that you were rude about it. > Google pushing bindless forward might help get this unstuck. Although notice that the target date on their whiteboard is December 2026. The bindless stuff is basically \"developers requested it a ton when we asked for feedback on features they wanted (I was one of those people who gave them feedback), and we had some draft proposals from (iirc) 1-2 different people\". It's wanted, but there are still major questions to answer. It's not like this is a set thing they've been developing and are preparing to release. All the features listed are just feedback from users and discussion that took place at the WebGPU face to face recently. reply jblandy 12 hours agorootparentWGPU dev here. I agree with everything JMS55 says here, but I want to avoid a potential misunderstanding. Performance is definitely a priority for WGPU, the open source project. Much of WGPU's audience is very concerned with performance. My team at Mozilla are active contributors to WGPU. For the moment, when we Mozilla engineers are prioritizing our own work, we are focused on compatibility and safety, because that's what we need most urgently for our use case. Once we have shipped WebGPU in Firefox, we will start putting our efforts into other things like performance, developer experience, and so on. But WGPU has other contributors with other priorities. For example, WGPU just merged some additions to its nascent ray tracing support. That's not a Mozilla priority, but WGPU took the PR. Similarly for some recent extensions to 64-bit atomics (which I think is used by Bevy for Nanite-like techniques?), and other areas. WGPU is an open source project. We at Mozilla contribute to the features we need; other people contribute to what they care about; and the overall direction of the project is determined by what capable contributors put in the time to make happen. reply jms55 11 hours agorootparent> But WGPU has other contributors with other priorities. For example, WGPU just merged some additions to its nascent ray tracing support. That's not a Mozilla priority, but WGPU took the PR. Similarly for some recent extensions to 64-bit atomics (which I think is used by Bevy for Nanite-like techniques?), and other areas. Yep! The 64-bit atomic stuff let me implement software rasterization for our Nanite-like renderer - it was a huge win. Same for raytracing, I'm using it to develop a RT DI/GI solution for Bevy. Both were really exciting additions. The question of how performant and featureful wgpu is is mostly just a matter of resources in my view. Like with Bevy, it's up to contributors. The unfortunate reality is that if I'm busy working on Bevy, I don't have any time for wgpu. So I'm thankful for the people who _do_ put in time to wgpu, so that I can continue to improve Bevy. reply kookamamie 13 hours agorootparentprev> implementing missing features so that they can ship WebGPU support in Firefox Sounds like WGPU, the project, should be detached from Firefox? To me the priority of shipping WGPU on FF is kind of mind-boggling, as I consider the browser irrelevant at this point in time. reply brookman64k 10 hours agorootparentJust to avoid potential confusion: WebGPU and WGPU are different things. reply slimsag 3 hours agorootparent(a) WebGPU -> Specification or browser API (b) WGPU, gfx-rs/wgpu, wgpu.rs -> Rust crate implementing WebGPU (c) wgpu -> the prefix used by the C API for all WebGPU native implementations. (d) 'wgpu' -> a cute shorthand used by everyone to describe either (a), (b), or (c) with confusion. reply pjmlp 7 hours agorootparentprevOutside of the browser the answer is middleware. WGPU has to decide, either stay compatible with WebGPU, and thus be constrained by the design of a Web 3D API, or embrace native code and diverge from WebGPU. reply slimsag 2 hours agorootparentThis is the right answer^ But even more, the level at which WebGPU exists (not too high level, not too low level) necessitates that if a native API graphics abstraction sticks with the WebGPU's API design and only 'extends' it, you actually end up with three totally different ways to use the API: * The one with your native 'extensions' -> your app will only run natively and never in the browser unless you implement two different WebGPU rendering backends. Also won't run on Chromebook-type devices that only have GLES-era hardware. * The WebGPU browser API -> your app will run in the browser, but not on GLES-era hardware. Perish in the verbosity of not having bindless support. * The new 'compatability' mode in WebGPU -> your app runs everywhere, but perish in the verbosity of not having bindless, suffer without reversed-z buffers because the underlying API doesn't support it. And if you want your app to run in all three as best as possible, you need to write three different webgpu backends for your app, effectively, as if they are different APIs and shading languages. reply adrian17 38 minutes agorootparent> The WebGPU browser API -> your app will run in the browser, but not on GLES-era hardware. Perish in the verbosity of not having bindless support. Note, regarding \"GLES-era\": WGPU does have a GLES/WebGL2 backend; missing WebGL1 is unfortunate, but at least it covers most recent browsers/hardware that happens to not have WebGPU supported yet. (and there's necessarily some added overhead from having to adapt to GLES-style api; it's especially silly if you consider that the browser might then convert the api calls and shaders _again_ to D3D11 via ANGLE) reply littlestymaar 12 hours agorootparentprevThe irrelevant browser is the one paying developers to build wgpu though… reply adastra22 12 hours agorootparentprevIs WGPU even a Mozilla project? I think he is just saying that those paid developers (paid by Mozilla) have that priority, and everyone else is volunteer. Not that WGPU is a Firefox project. reply kookamamie 11 hours agorootparentThanks, I checked the WGPU project's roots and you're right - it's not Mozilla's project, per-se. reply jblandy 11 hours agorootparentprevYes, this. reply z3phyr 55 minutes agorootparentprevAnother reason is that exploratory programming is hard by design in Rust. Rust is great if you already have a spec and know what needs to be done. Most of the gamedev in my opinion is extremely exploratory and demands constant experimentation with design. C/C++ offer fluidity, a very good and mature debug toolchain, solid performance ceiling and support from other people. It will be really hard to replace C++ in performance/simulation contexts. Security takes a backseat there. reply jblandy 12 hours agorootparentprevThere have been a bunch of significant improvements to WGPU's performance over the last few years. * Before the major rework called \"arcanization\", `wgpu_core` used a locking design that caused huge amounts of contention in any multi-threaded program. It took write locks so often I doubt you could get much parallelism at all out of it. That's all been ripped out, and we've been evolving steadily towards a more limited and reasonable locking discipline. * `wgpu_core` used to have a complex system of \"suspected resources\" and deferred cleanup, apparently to try to reduce the amount of work that needed to be done when a command buffer finished executing on the GPU. This turned out not to actually save any work at all: it did exactly the same amount of bookkeeping, just at a different time. We ripped out this complexity and got big speedups on some test cases. * `wgpu_core` used to use Rust generics to generate, essentially, a separate copy of its entire code for each backend (Vulkan, Metal, D3D12) that it used. The idea was that the code generator would be able to see exactly what backend types and functions `wgpu_core` was using, inline stuff, optimize, etc. It also put our build times through the roof. So, to see if we could do something about the build times, Wumpf experimented with making the `wgpu_hal` API use dynamic dispatch instead. For reasons that are not clear to me, switching from generics to dynamic dispatch made WGPU faster --- substantially so on some benchmarks. Animats posts frequently about performance problems they're running into, but when they do it's always this huge pile of unanalyzed data. It's almost as if, they run into a performance problem with their code, and then rather than figuring out what's going on themselves, they throw their whole app over the wall and ask WGPU to debug the problem. That is just not a service we offer. reply ossobuco 2 hours agorootparentHe's reporting a 23% drop in performance and seems to have invested quite some time in pinning down what's causing it, plus he's provided a repro repository with benchmarks. I honestly don't get your annoyed response; any OSS project wishes they had such detailed bug reports, and such a performance regression would concern me very much if it happened in a project I maintain. reply Animats 5 hours agorootparentprevThis is in reference to [1]. [1] https://github.com/gfx-rs/wgpu/issues/6434 reply flohofwoe 8 hours agorootparentprevTbf, tons of games have been created and are still being created without bindless resource binding. While WebGPU does have some surprising performance bottlenecks around setBindGroup(), details like that hardly make or break a game (if the devs are somewhat competent they'll come up with ways to workaround 3D API limitations - that's how it's always been and always will be - the old batching tricks from the D3D9 era still sometimes make sense, I wonder if people simply forgot about those or don't know them in the first place because it was before their time). reply adastra22 12 hours agorootparentprevTbh I was annoyed reading it too as an open source developer. The people you are talking to are volunteering their time, and you weren’t very considerate of that. Open source software isn’t the same support structure as paid software. You don’t file tickets and expect them to be promptly fixed, unless you do the legwork yourself. reply raincole 15 hours agorootparentprevAs far as I know, Unity doesn't support bindless either. However thousands of Unity games are released on Steam every year. So it's safe to say performance isn't the main (or major) reason why Rust gamedev isn't getting much traction. reply Animats 15 hours agorootparentThat limits Unity's scene size. See [1]. [1] https://discussions.unity.com/t/gpu-bindless-resources-suppo... reply efnx 16 hours agorootparentprevAuthor of Renderling here. Thanks for the shout out Animats! Bindless is a game changer - pun intended. It can’t happen soon enough. Just curious, what are the three major projects that were cancelled? I also want to mention that folks are shipping high performance games in Rust - the first title that comes to mind is “Tiny Glade” which is breathtakingly gorgeous, though it is a casual game. It does not run on wgpu though, to my knowledge. I may have a different definition of high performance, with lower expectations. reply Animats 15 hours agorootparent> What are the three major projects that were cancelled? Here are some: - LogLog Games [1]. Not happy with Bevy. Not too unhappy about performance, although it's mentioned. - Moonlight Coffee [2]. Not a major project, but he got as far as loading glTF and displaying the results, then quit. That's a common place to give up. - Hexops. [3] Found Rust \"too hard\", switched to Zig. Tiny Glade is very well done. But, of course, it's a tiny glade. This avoids the scaling problems. [1] https://news.ycombinator.com/item?id=40172033 [2] https://www.gamedev.net/blogs/entry/2294178-abandoning-the-r... [3] https://devlog.hexops.com/2021/increasing-my-contribution-to... reply slimsag 3 hours agorootparentIt's crazy you've cited Hexops as an example: 1. It's a game studio not a project (CEO here :)) 2. It's very much still alive and well today, not 'cancelled' 3. We never even used WebGPU in Rust, this was before WebGPU was really a thing. It is true that we looked elsewhere for a better language for us with different tradeoffs, and have since fully embraced Zig. It's also true that we were big proponents of WebGPU earlier on, and have in recent years abandoned WebGPU in favor of something which is better for graphics outside the browser (that's its own worthwhile story).. But we've never played /any/ role in the Rust gamedev ecosystem, really. reply z3phyr 52 minutes agorootparentI think the future is getting rid of all the APIs and driver overhead, compile directly to GPU compute and write your own software renderers in a language targeting GPUs (Could be Zig) reply adastra22 12 hours agorootparentprevTiny glade isn’t tiny on the rendering side. It does gorgeous, detailed landscapes. reply pjmlp 7 hours agorootparentIndeed, they also do most of the stuff custom. reply littlestymaar 12 hours agorootparentprevNone of those are “major projects” by any definition of the word though. And none of the three has anything to do with wgpu's performance. Rust for game engine has always been a highly risky endeavor since the ecosystem is much less mature than everything else, and even though things have improved a ton over the past few years, it's still light-years away from the mainstream tools. Building a complete game ecosystem is very hard and it's not surprising to see that Rust is still struggling. reply ladyanita22 9 hours agorootparentprevI thought WGPU only supported WebGPU, and then there were translation libraries (akin to Proton) to run WebGPU over Vulkan. Does it directly, internally, support Vulkan instead of on-the-fly translation from WebGPU to VK? reply flohofwoe 8 hours agorootparentWGPU (https://wgpu.rs/) is one of currently three implementations of the WebGPU specification (the other two being Google's Dawn library used in Chrome, and the implementation in WebKit used in Safari). The main purpose of WebGPU is to specify a 3D API over the common subset of Metal/D3D12/Vulkan features (e.g. doing an 'on-the-fly translation' of WebGPU API calls to Metal/D3D12/Vulkan API calls, very similar to how (a part of) Proton does an on-the-fly translation of the various D3D API versions to Vulkan. reply ladyanita22 6 hours agorootparentYou're describing the WebGPU spec and its different implementations. OP claimed WGPU had native support for VK, DX and others. But as far as I know, WGPU just supports WebGPU being translated on the fly to those other backends, with the obvious performance hit. If I'm wrong, I'd be interested to know, as this would make WGPU a more interesting choice for many if, in reality, the code was native instead of translation. Edit: https://docs.rs/wgpu/latest/wgpu/#backends it seems they indeed support native code in almost every backend? reply flohofwoe 4 hours agorootparentI don't understand the question... Those three WebGPU implementation libraries are compiled to native code (they are written in Rust or C/C++), and at least WGPU and Dawn are usable as native libraries outside the browser in regular native apps that want to use WebGPU as a cross-platform 3D API. Yet still, those native libraries do a runtime translation of WebGPU API calls to DX/Vk/Metal API calls (and also a runtime translation of either WGSL or SPIRV to the respective 3D backend API shading languages) - in that sense, quite similar to what Proton does, just for a different 'frontend API'. reply pjmlp 7 hours agorootparentprevWithin the constrains of the browser sandbox and 10 year old hardware, which is when its design started. reply flohofwoe 6 hours agorootparentI think it's more about low-end mobile GPUs which WebGPU needs to support too (which is also the main reason why Vulkan is such a mess). The feature gap between the low- and high-end is bigger than ever before and will most likely continue to grow. reply pjmlp 5 hours agorootparentI am yet to see anyone deliver in WebGL something at the level of Infinity Blade that Apple used to demo OpenGL ES 3.0 capabilities of their 2011 iPhone model, a mobile phone GPU from almost 15 years ago. Unless we are talking about cool shadertoy examples. reply flohofwoe 3 hours agorootparentThat's more a business problem than a technical problem. Web games are in a local maximum of minimal production cost (via 2D assets) versus maximized profits (via free-2-play), and as long as this works well there won't be an Infinity Blade because it's too expensive to produce. reply pjmlp 2 hours agorootparentYeah, but then what do we want this technology for, besides visualisations and shadertoy demos? Streaming solves the business case, with native APIs using server side rendering. reply flohofwoe 1 hour agorootparentAt least it breaks up a chicken-egg problem, and the most interesting use cases are the ones that nobody was expecting anyway. > Streaming solves the business case, with native APIs using server side rendering. And yet history is littered with the dead husks of game streaming services ;) reply pjmlp 37 minutes agorootparentThe chicken egg problem caused by killing Flash games, and that nowadays no one cares about the browser, because everyone doing Flash moved into mobile phones or Steam, with better APIs? Game Pass, GeForce Now, are doing alright. Stadia failed, because Google doesn't get games industry. reply astlouis44 2 hours agorootparentprevTry this Unreal Engine 5 WebGPU demo: https://play.spacelancers.com/ reply pjmlp 40 minutes agorootparentDemo, not game. Shadertoy is full of impressive demos. reply miloignis 3 hours agorootparentprevTiny Glade? https://store.steampowered.com/app/2198150/Tiny_Glade/ reply pjmlp 2 hours agorootparentWhere is the URL for the game on the browser? reply klysm 15 hours agorootparentprevThe tone of the thread was perfectly fine until you made a passive aggressive comment reply ribit 4 hours agoparentprevQuick note: I looked at the bindless proposal linked from the blog post and their description of Metal is quite outdated. MTLArgumentEncoder has been deprecated for a while now, the layout is a transparent C struct that you populate at will with GPU addresses. There are still descriptors for textures and samplers, but these are hidden from the user (the API will maintain internal tables). It's a very convenient model and probably the simplest and most flexible of all current APIs. I'd love to see something similar for WebGPU. reply nox101 16 hours agoparentprev> The default texture limits without bindless are also way too small for serious applications I'm not disagreeing that bindless is needed but it's a bit of hyperbole to claim the texture limits are too small for serious applications given the large list of serious graphics applications that shipped before bindless existed and the large number of serious graphics applications and games still shipping that don't use them. reply jms55 15 hours agorootparentIt's partly because WebGPU has very conservative default texture limits so that they can support old mobile devices, and partly it's a problem for engines that may have a bunch of different bindings and have increasingly hacky workarounds to compile different variants with only the enabled features so that you don't blow past texture limits. For an idea of bevy's default view and PBR material bindings, see: * https://github.com/bevyengine/bevy/blob/main/crates/bevy_pbr... * https://github.com/bevyengine/bevy/blob/main/crates/bevy_pbr... reply elabajaba 15 hours agorootparentprevThey're talking about the 16 sampled texture binding limit which is the same as webgl2. If you look at eg. the list of devices that are stuck with that few texture bindings they don't even support basic GL with compute shaders or vulkan, so they can't even run webgpu in the first place. reply Animats 14 hours agorootparentprevYes. If you're stuck with that limitation, you pack up related textures into a big texture atlas. When you enter a new area, the player sees \"Loading...\" while the next batch of content is loaded. That was the state of the art 15 years ago. It's kind of dated now. reply jblandy 12 hours agoparentprevThe nice thing about WebGPU's \"compat mode\" is that it's designed so browsers don't have to implement it if they don't want to. Chrome is really excited about it; Safari has no plans to implement it, ever. I agree that compat mode takes up more of the WebGPU standard committee's time than bindless. I'm not sure that's how I would prioritize things. (As a Mozilla engineer, we have more than enough implementation work to do already, so what the committee discusses is sort of beside the point for us...) What would be really helpful is if, once the bindless proposalgets merged into the spec repo , a contributor could start adapting what WGPU has now to match the proposal. Implementation experience would be incredibly valuable feedback for the committee. reply pjmlp 12 hours agoparentprevIt only goes to show the limitations of browser 3D APIs, and the huge mistake some folks do for native games using it instead of a proper middleware engines, capable of exposing modern hardware. reply jms55 11 hours agorootparentI don't necessarily disagree. But I don't agree either. WebGPU has given us as many positives as it has negatives. A lot of our user base is not on modern hardware, as much as other users are. Part of the challenge of making a general purpose engine is that we can't make choices that specialize to a use case like that. We need to support all the backends, all the rendering features, all the tradeoffs, so that our users don't have to. It's a hard challenge. reply pjmlp 11 hours agorootparentBasically the goal of any middleware engine, since the dawn of time in the games industry. reply modeless 13 hours agoparentprevYou don't have to settle for the default limits. Simply request more. reply jms55 11 hours agorootparentWe do when there available, but I think the way browsers implement limit bucketing (to combat fingerprinting) means that some users ran into the limit. I never personally ran into the issue, but I know it's a problem our users have had. reply modeless 11 hours agorootparentThat makes sense. I bet the WebGPU WG would be interested in hearing about that experience. They might be able to make changes to the buckets. reply adastra22 12 hours agorootparentprevYeah I went down the rabbit hole of trying to rewrite all our shaders to work on webgpu’s crazy low limits. I’m embarrassed to say how long I worked that problem until I tried requesting higher limits, and it worked on every device we were targeting. The default limits are like the lowest common denominator and typically way lower than what the device actually supports. reply wwwtyro 16 hours agoprevAny word on when it'll be supported on Linux without a flag? reply pjmlp 7 hours agoparentWhen they see a business value on GNU/Linux Desktop WebGPU support for Chrome, note that other Linux kernel based systems from Google already support it. reply nox101 15 hours agoparentprevMy guess is when someone one who cares about Linux submits a pull request to support it. reply vetinari 7 hours agorootparentSo that's how it worked with MacOS and Windows? Color me surprised. But bth, Google doesn't seem to care about Android either. Chrome supports it on Snapdragons and that's it. Do you have Xclipse GPU? Like, I don't know, Samsung's current flagship line Galaxy S24 does? Too bad, not good enough. reply macawfish 18 hours agoprevLinux support please! reply tkzed49 16 hours agoprevI wish there were a good way to profile WebGPU code. I've seen this (very useful) article[1] on setting up PIX, but I'm ambitious. I want to see everything from draw call timings to flamegraphs of shader code. Right now I feel like the only way to write efficient WebGPU code is to deeply understand specific GPU architectures. I hope some day there's a dev tools tab that shows me I'm spending too much time sampling a texture or there's a lot of contention on my atomic add. [1]: https://toji.dev/webgpu-profiling/pix.html reply jms55 16 hours agoparentTimestamp queries will give you essentially time spans you can use for profiling, but anything more than that and you really want to use a dedicated tool from your vendor like NSight, RGP, IGA, XCode, or PIX. > Right now I feel like the only way to write efficient WebGPU code is to deeply understand specific GPU architectures. I hope some day there's a dev tools tab that shows me I'm spending too much time sampling a texture or there's a lot of contention on my atomic add. It's kind of the nature of the beast. Something that's cheap on one GPU might be more expensive on another, or might be fine because you can hide the latency even if it's slow, or the CPU overhead negates any GPU wins, etc. The APIs that give you the data for what you're asking are also vendor-specific. reply pjmlp 12 hours agoparentprevCan forget about it. Developer tooling for debugging 3D Web APIs has been a continuous request since WebGL 1.0, from 2011. Until now the only thing that ever came out of it was SpectorJS and it shows its age. For a while Firefox did have a debugger, that they eventually removed from developer tools. You are left with writing the application twice, so that can make use of modern native debugging tools for graphics programming. reply hit8run 53 minutes agoprevDon't know for you but for me the next thing for WebGPU is uninstalling Google Chrome. Manifest v3 my ass. reply vFunct 18 hours agoprevI could use more 2-d line support for CAD applications as well as font drawing. reply jsheard 16 hours agoparentFont drawing is very much out of scope for a 3D API, that's something you (or a library) would implement on top of WebGPU. Likewise for line drawing, the API may expose some simple line primitives that the hardware supports natively, but if you want anything fancier then you're expected to do it yourself with shaders which is already possible in WebGPU. The 3D API is just meant to be a minimal portable abstraction over common low-level hardware features, composing those features into higher level constructs is intentionally left as an exercise for the user. reply lmariscal 9 hours agorootparentI mean, they are extending support for Canvas2D, which from what I believe would allow for easier text rendering. reply Lichtso 4 hours agoparentprevThere are libraries building on top of WebGPU for that, like: https://github.com/linebender/vello and (shameless plug): https://github.com/Lichtso/contrast_renderer reply flohofwoe 8 hours agoparentprevWebGPU does have line primitives of course, but only the type of lines that's supported in the underlying 3D APIs (e.g. 1-pixel wide). Since the whole purpose of WebGPU is to provide a thin API over the common feature set of D3D12, Metal and Vulkan that's totally expected though. reply nox101 16 hours agoparentprevAsking for lines is like asking for your CPU to support macros. The GPU is low-level, lines are high level. You build the high level on top of the low-level with libraries etc... reply wffurr 14 hours agoparentprevThe Canvas2D interop feature should make it very easy to draw text to a 2D canvas and then pull that into WebGPU as a texture. reply Waterluvian 18 hours agoparentprevSame. I do a ton of 2D map stuff and it’s always quite uncomfortable to do in shaders or very slow in a Canvas context. The last time I tried with pixi.js the problem was smoothly zooming polygons with a constant width border thicker than hairline. Doing that was basically just generating new textures on every frame. reply efnx 16 hours agorootparentJust out of curiosity, what is uncomfortable about writing shaders, in your opinion? reply skywal_l 11 hours agorootparentdebugging reply sakesun 16 hours agoprevI'm curious why supporting ML algorithms is important inside browser ? Will my machine be improperly utilized by the sites I visit ? reply Grimblewald 15 hours agoparentIt can absolutely be used for bad, and I know many sites do use it for bad. However, it does good as well, and I think it's important to develop but also it should come with similar permission requests that use of microphone or camera do. I do research and develop ANN's for data analysis within chemistry. Making it possible for less tech literate people to visit a site, select a model, load their dataset, and get answers, is quite handy. The best part is because I can use their hardware to do it all, it all stays private, no one has to upload any sensitive research data etc. and I don't have to ship to various devices etc. I know if they have a mainstream updated browser they can likely use the tool. No endless requests for help, no mystery issues to solve, things just work. reply chrismorgan 15 hours agorootparent> it should come with similar permission requests that use of microphone or camera do. Such permissions requests have been associated I think exclusively with input and output, not computation. reply sakesun 12 hours agoparentprevLot of downvote for my curiosity reply worik 16 hours agoprevThey say: \"This is the next step in the standardization process, and it comes with stronger guarantees of stability and intellectual property protection.\" I understand stability, and in the general sense I see that people feel they need to protect their IP, but in this specific case what is meant by \"intellectual property protection\"? reply vasilvv 16 hours agoparentW3C generally requires Working Group participants to provide IPR licensing commitments for the spec in question [0]. As far as I understand, higher level of specification maturity implies stronger level of obligations, though the specifics of what specifically changes when were never clear to me. [0] https://www.w3.org/policies/patent-policy/#sec-Requirements reply choonway 16 hours agoparentprevI think it means protection from intellectual property claims in the future. reply astlouis44 2 hours agoprevMy team (third party) has developed WebGPU support for Unreal Engine 5, along with an asset-streaming system that solves the large download, multi-gigabyte game in your webpage issue by only loading in what a player needs to see at any given moment. It's constantly loading and unloading data, which helps tremendously with memory management. WebGPU is going to usher in a new era of web games, the biggest benefit being compute shaders which have never before been possible inside the browser. DISCLAIMER - Will only work on a Windows device running Chrome or a Chromium browser. Mac and iOS isn't well supported yet. Space demo - https://play.spacelancers.com/ Forest demo - https://play-dev.simplystream.com/?token=bd4ca6db-522a-4a73-... reply iknowstuff 2 hours agoparentAw neither works on iOS 18.2 reply bobajeff 1 hour agorootparentNor Linux. So basically Webgpu is just another graphics API for targeting Windows. Yay. reply astlouis44 1 hour agorootparentprevYeah WebGPU isn't officially enabled by default on Safari yet, but that's actually not what's stopping these demos from working. Once we mobile optimize, they should work well. reply brcmthrowaway 19 hours agoprevBeing spun off to a different company. reply stackghost 17 hours agoprevHonest question: Can someone explain to me why people are excited about WebGPU and WASM and similar technologies? To me, one of the greatest things about the web is that the DOM is malleable in that you can right click -> view source -> change things. This is dead in an era where the server just sends you a compiled WASM dll. It seems to me that the inevitable result of things like WASM and WebGPU will be \"rich media web 4.0 applications\" that are just DRM, crypto miners, and spyware compiled so that they're more difficult to circumvent, and delivered via the browser. An excuse to write web apps with poor performance because \"well the user just needs a stronger GPU\". It seems like an express train back to the bad old days of every website being written in flash. I honestly cannot see the upsides of these technologies. Is it gaming? Why would I want to play a 3D game in my fucking browser of all places? That's a strict downgrade in almost every way I can think of. Why would anyone want that? Is it \"AI\"? Why would I want to run an LLM in the browser, I could just run it natively for better performance? All I can see and have seen over the last several years is a steady parade of new technologies that will make the internet (and in some cases the lives of every day people) objectively worse while enriching a handful of big tech douchebags. Why are we going down this path? Who is asking for this stuff? Why the fuck would I want to expose my GPU to a website? reply Flux159 17 hours agoparent> Why would I want to play a 3D game in my fucking browser of all places? To provide users a way to instantly play a game without having to download all assets at once. Give developers a potential way to avoid app store royalties of up to 30% on desktop or mobile. With wgpu in rust, you can also target WebGPU as a shared 3d runtime that will run across OS's natively rather than having to target Vulkan, Metal, and DirectX. > Why would I want to run an LLM in the browser, I could just run it natively for better performance? What about users who don't know how to download a model and run it locally? I would argue this is the vast majority of users in the world. Also, this specific use case is probably not going to be generalized with WebGPU yet due to model sizes, but rather other APIs like the Prompt API in Chrome which will use Gemini Nano embedded into the browser (assume it will eventually get standardized). https://developer.chrome.com/docs/ai/built-in-apis I agree with you that WASM and WebGPU will be used for adware, targeting, and spyware - but if you don't want to use them, you should disable them in your browser settings - there's definitely value add for other users even if you can't see any benefits. reply stackghost 17 hours agorootparent>To provide users a way to instantly play a game without having to download all assets at once There's a reason QuakeLive didn't catch on and it's because streaming resources to the player makes for awful UX. >What about users who don't know how to download a model and run it locally? Those users also don't know how to compile MS Word from source but they have been getting along just fine with installers. reply dartos 14 hours agorootparentThe ability to paste a URL into a browser and have that be everything you need to do to play a game is pretty compelling for many kinds of games. That’s what made flash games so big back in the day. reply pjmlp 12 hours agorootparentURLs can also be mapped into native applications, it is a matter of the right OS. https://developer.android.com/training/app-links reply dartos 4 hours agorootparentThat’s a different flow though. Pasting URL into browser, then having access to game Vs Pasting url in browser Get link to install app Install app Navigate to the original url again. Technical capabilities don’t always change user experience reply pjmlp 3 hours agorootparentApplication streaming, no need for installation before using. reply int0x29 29 minutes agorootparentThat needs a hell of a lot of sandboxing before I get anywhere near it. Which sounds like a good use for WASM and WebGPU. reply MindSpunk 13 hours agorootparentprevBrowsers will never run games that aren't toys or use very simple assets in a way that doesn't completely suck. High quality assets need gigabytes of data. You either require users to download all the assets upfront (the thing we're trying to avoid) or streaming the assets dynamically. You end up having to re-implement steam to keep a local copy of the assets on the client device yourself, expect browsers to do the same to manage caching the gigabytes of data transparently, or design your game around a very slow storage device or use tiny assets. Flash games worked because they fit very nicely into the 'tiny assets' category. reply skydhash 17 hours agorootparentprev> To provide users a way to instantly play a game without having to download all assets at once No need for the web in that case, which is inefficient. You can do with like those 1MB installers and stream those assets. > but if you don't want to use them, you should disable them in your browser settings Which the majority won't. People don't even go in their phone settings, apart from connecting to WiFi and changing their wallpaper. reply zztop44 16 hours agorootparentI don’t want to download a random executable from some unknown source. However, I trust the browser sandbox. reply skydhash 16 hours agorootparent> I don’t want to download a random executable from some unknown source Why would you do that? --- There's few applications that warrant having direct access to the GPU and other devices. And for those, a native app would be a much efficient way (for the user). reply text0404 15 hours agorootparentyeah but users don't care about technical efficiency, they care about having seamless experiences that aren't interrupted by long downloads, app/context switching, and loading screens. reply skydhash 14 hours agorootparentWhich the web doesn't provide. Try opening Figma and Sketch at the same time or Mail.app and Gmail. Google Doc is closer to Wordpad than Libreoffice. reply pjmlp 12 hours agorootparentprevApplication streaming sorts that out, with much better tooling for 3D development. reply bigstrat2003 58 minutes agoparentprevI agree. Trying to shove everything into the browser is absolutely stupid. Native apps are better than doing things in the browser in almost all cases. But that's not trendy, so people don't chase after it. reply peutetre 13 hours agoparentprev> Can someone explain to me why people are excited about WebGPU and WASM and similar technologies? WebAssembly brings all languages to the browser. Why shouldn't I be able to use C#, Rust, Go, Dart, Python, or whatever else in browser? WebAssembly brings better performance. That's what Webamp found: https://jordaneldredge.com/blog/speeding-up-winamps-music-vi... And what Amazon found: https://www.amazon.science/blog/how-prime-video-updates-its-... And what Google found: https://web.dev/case-studies/google-sheets-wasmgc Why make things perform worse when they can perform better? Why shouldn't I be able to make an application that compiles to the Windows, macOS, and Linux desktops and also to the browser? This one does: https://bandysc.github.io/AvaloniaVisualBasic6/ reply dahart 17 hours agoparentprevFor that matter, why would you expose your CPU to a website? Or your monitor? It could show you anything! ;) Maybe you are not be aware of the number of good web apps that use some WebGL under the hood? You might be using office applications in your browser already that use WebGL when it’s available, and the reason is it makes things faster, more responsive, more scalable, and more efficient. Same would go for WebGPU. There’s no reason to imagine that the web will do bad things with your resources that you didn’t ask for and don’t have control over. There have been hiccups in the past, but they got fixed. Awareness is higher now, and if there are hiccups, they’ll get fixed. reply stackghost 17 hours agorootparent>There’s no reason to imagine that the web will do bad things with your resources that you didn’t ask for and don’t have control over. The web is like this right now. Why would things magically become a utopia? reply fulafel 12 hours agorootparentprev> There’s no reason to imagine that the web will do bad things with your resources that you didn’t ask for and don’t have control over. Read some security update news from browser vendors and vulnerability researcher posts. There's some weak signals about vendors acknowledging the difficulty of securing the enormous attack surface of browsers built on unsafe foundations, eg MS \"enhanced security mode\" and Apple \"lockdown mode\". reply skydhash 17 hours agorootparentprevI don't mind the browser using the GPU to speed up graphical operations. But I do mind random sites and apps going further than that. Native apps have better access, but there's an higher selection criteria than just opening a URL. reply koolala 15 hours agoparentprevIt's interesting to me you don't like any WebGL websites. I remember first trying Minecraft Java in the browser and it was awesome. Runescape! I grew up playing Runescape! How could anyone not want games like Runescape to exist?!? reply bigstrat2003 1 hour agorootparent> How could anyone not want games like Runescape to exist?!? I mean, I wouldn't say I don't want it to exist. But Runescape is one of the shittiest, most boring games I've ever played. It's not exactly a strong argument for why we should run stuff in the browser reply do_not_redeem 17 hours agoparentprevWhy do you think wasm is harder to circumvent? The only way to instantiate a wasm module in the browser is through (drum roll) javascript. Install noscript if you're worried. The days of view source -> edit are basically over anyway due to every site's 1MB+ minified JS blobs. > Why would I want to run an LLM in the browser, I could just run it natively for better performance? Why would you try out a new app in a sandboxed browser, when instead you could give it complete access to your entire computer? reply stackghost 17 hours agorootparentIf the app can run arbitrary code on my GPU it's not exactly sandboxed, is it? reply do_not_redeem 17 hours agorootparentAre you launching Chrome with --disable-gpu-sandbox? If not, it's sandboxed. reply stackghost 16 hours agorootparentIf websites can run compute shaders on my hardware, that's not a sandbox. reply crazygringo 16 hours agorootparentSandboxing is about preventing code from accessing data it's not supposed to. Data like files or memory belonging to other tabs or other processes. Or data streams like your webcam or microphone. Data outside of its, well, sandbox. So how are compute shaders accessing data they're not supposed to? How do you think they're escaping the sandbox? reply do_not_redeem 16 hours agorootparentprevIt seems like you're just making up your own definitions now because you don't like the tech. What do think a sandbox is, exactly? And what do you think Chrome's GPU sandbox does, if it's not a sandbox? reply tedunangst 16 hours agorootparentprevIf websites can run JavaScript on your hardware, is that not sandboxed? reply stanleykm 17 hours agoparentprevwhy would you want to expose your cpu to a website? why would you want to expose your keyboard to a website? why would you want to use a website at all? why don’t you go outside, read a book, look at a duck, and pick a flower instead? do we exist just to complain? is this meaning? reply lukaqq 17 hours agoparentprevYou should try Chillin(https://chillin.online), browser-based video editor. Powered by WebGL, WASM, and WebCodecs, Chillin provides a full suite of video editing capabilities on the web. Trust me, Chillin is smoother than most native video editors, even on mobile. I believe Chillin can leverage WebGPU to bring even more powerful rendering features. Yes, running LLMs on the web may not have significant advantages due to the speed limitations, but other models, such as those for bg removal, speech-to-subtitles, and translation, could become practical and efficient thanks to WebGPU. reply 01HNNWZ0MV43FF 17 hours agoparentprevUsers wanted an app sandbox. HTML documents were sort-of like an app sandbox. Evolution is now adding an app sandbox to HTML. There is little we can do to resist it. I don't like it either - I hate HTML. reply crabmusket 12 hours agoparentprevFigma, my dude reply doctorpangloss 18 hours agoprev [–] Whom is WebGPU for, besides Unity? reply dahart 17 hours agoparentResponding to your pre-edited comment. > whom is WebGPU for? […] it’s a huge rigamarole. Where is this comment coming from? WebGPU enables compute shaders, and there are applications in anything that uses a GPU, from ML to physics to audio to … you name it. What is making you think game engines would be the only users? I bet a lot of companies are looking forward to being able to use compute shaders in JS apps and web pages. > Godot has had zero development for WebGPU support. Why would Godot be an indicator? I love Godot and their efforts, but it’s less than 1% of game engine market share, and a much smaller less well funded team. Of course they’re not on the bleeding edge. Unity is closer to 30% market share and is actively engaging with WebGPU, so it seems like you’re downplaying and contradicting a strong indicator. reply doctorpangloss 17 hours agorootparentI edited my comment. > WebGPU enables compute shaders, and there are applications in anything that uses a GPU, from ML to physics to audio to … you name it. I know. If you have to go through a giant product like Unity for example to use WebGPU because Apple will essentially have its own flavor of WebGPU just like it has its own flavor of everything, is it really cross platform? Does Apple support Vulkan? No. It was invented for middlewares! Apple has a flag to toggle on WebGPU on iOS today. I know dude. What does that really mean? They have such a poor record of support for gamey things on Mobile Safari. No immersive WebXR, a long history of breaking WASM, a long history of poor WebGL 2 and texture compression support. Why is this going to be any different? reply dahart 17 hours agorootparentI’m still not sure what the point is. WebGPU is an API, is that that you mean by middleware? What’s the issue? Apple will do their own thing, and they might not allow WebGPU on Safari. What bearing does that have on what people using Linux, Windows, Firefox, and Chrome should do? And where exactly is this cross platform claim you’re referring to? reply CharlesW 14 hours agorootparent> Apple will do their own thing, and they might not allow WebGPU on Safari. Safari has WebGPU support today, albeit behind a feature flag until it's fully baked. https://imgur.com/a/b3spVWd Not sure if this is good, but animometer shows an Avg Frame time of ~25.5 ms on a Mac Studio M1 Max with Safari 18.2 (20620.1.16.11.6). https://webgpu.github.io/webgpu-samples/sample/animometer/ reply flohofwoe 6 hours agorootparentThe demo is doing a setBindGroup per triangle, so not exactly surprising since this is a well known bottleneck (Chrome's implementation is better optimized but even there setBindGroup is a surprisingly slow call). But since both implementations run on top of Metal there's no reason why Safari couldn't get at least to the same performance as Chrome. reply doctorpangloss 17 hours agorootparentprevThe issue is, it's likely that a company with $2 BILLION spent on product development and a very deep relationship with Apple, like Unity, will have success using WebGPU the way it is intended, and nobody else will. So then, in conclusion, WebGPU is designed for Unity, not you and me. Unity is designed for you and me. Are you getting it? reply dahart 3 hours agorootparentIt seems like you’ve jumped to and are stuck on a conclusion that isn’t really supported, somewhat ignoring people from multiple companies in this thread who are actively using WebGPU, and it’s not clear what you want to have happen or why. Do you want WebGPU development to stop? Do you want Apple to support it? What outcome are you advocating for? Unity spends the vast majority of its money on other things, and Unity isn’t the only company that will make use of WebGPU. Saying nobody will have success with it is like saying nobody will succeed at using CUDA. We’re just talking about compute shaders. What is making you think they’re too hard to use without Apple’s help? reply jms55 16 hours agorootparentprev> The issue is, it's likely that a company with $2 BILLION spent on product development and a very deep relationship with Apple, like Unity, will have success using WebGPU the way it is intended, and nobody else will. Not really. Bevy https://bevyengine.org uses WebGPU exclusively, and we have unfortunately little funding - definitely not $2 billion. A lot of the stuff proposed in the article (bindless, 64-bit atomics, etc) is stuff we (and others) proposed :) If anything, WebGPU the spec could really use _more_ funding and developer time from experienced graphics developers. reply doctorpangloss 12 hours agorootparentI agree with you, Bevy is a worthy cause. Why are people downvoting? The idea of Great High Performance Graphics Effortlessly on All Platforms is very appealing. It is fundamentally empathetic. It is an opium to game developers whose real antagonist is Apple and Nintendo, and who want a more organic journey in game development than Unity Learn. Is it a realizable goal? Time will tell. Everyone should be advocating for more focused efforts, but then. Are you going to say, Bevy is better than Godot? It’s subjective right? Open source efforts are already spread so thin. An inability to rally behind one engine means achieving 2013’s Unity 5 level of functionality is years away. Looking at it critically, in fact much effort in the open source ecosystem is even anti games. For example emulators used to pirate Nintendo Switch games have more mature multiplatform graphics engine implementations than Godot and Bevy do. It would be nice if that weren’t true, you might tell me in some sense that I am wrong, but c’mon. It’s crazy how much community effort goes into piracy compared to the stuff that would sincerely benefit game developers. WebGPU is authored by giant media companies, and will have purposefully hobbled support by the most obnoxious of them all, Apple - the one platform where it is kind of impractical to pirate stuff, but also, where it is kind of impractical to deliver games through the browser. Precisely because of the empathetic, yet ultimately false, promises of WebGPU. reply dahart 3 hours agorootparentWhy are you bringing up Godot again? Are you worried Godot will be left behind or unable to compete with Unity? Are you working on Godot? Why are you focused exclusively on games? What are the ‘false promises’ of WebGPU, and why do you think it won’t deliver compute shaders to every browser that supports it, like it says? I’m just curious, I get the sense there’s an underlying issue you feel strongly about and a set of assumptions that you’re not stating here. I don’t have a lot invested in whether WebGPU is adopted by everyone, and I’m trying to understand if and why you do. Seems like compute shaders in the browser will have a lot of interest considering the wild success of CUDA. Are you against people having access to compute shaders in the browser? reply doctorpangloss 2 hours agorootparent> What are the ‘false promises’ of WebGPU That you can write a \"compute shader\" once, and it will run \"anywhere.\" This isn't the case with any accelerated compute API, so why is WebGPU going to be different? Reality will be Chrome Windows Desktop WebGPU, Chrome Android (newish) WebGPU, Mobile Safari iOS 18 WebGPU, iPad WebGPU, macOS Safari WebGPU, macOS Chrome WebGPU, iOS default in app browser WebGPU, Instagram and Facebook in app browser WebGPU... This isn't complicated! If that's reality, I'd rather have: Apple Compute Shaders for Browser. Windows Chrome Compute Shaders for Browser. Android Chrome Compute Shaders for Browser. Because I'm going to go through a middleware like Unity to deal with both situations. But look at which is simpler. It's not complicated. > I’m trying to understand if and why you do. I make games. I like the status quo where we get amazing game engines for free. I cannot force open source developers to do anything. They are welcome to waste their time on any effort. If Bevy has great WebGL 2 support, which runs almost without warts everywhere, even on iOS, for example, it makes no sense to worry about WebGPU at all, due to the nature of the games that use Bevy. Because \"runs on WebGPU\" is making-believe that you can avoid the hard multiplatform engine bits. Engines like Construct and LOVE and whatever - 2D games don't need compute shaders, they are not very performance sensitive, use the browser as the middleware, and the ones that are, they should just use a huge commercial game engine. People have choices. reply dahart 1 hour agorootparent> That you can write a \"compute shader\" once, and it will run \"anywhere.\" Can you post a link to that quote? What exactly are you quoting? reply Eiim 16 hours agorootparentprevYou haven't substantiated why nobody else could make use of WebGPU. Are Google the only ones who can understand Beacons because they make $300B/year? GPU is hard, but it doesn't take billions to figure out. reply raincole 15 hours agorootparentprevThere are already open source projects making use of WebGPU, e.g. wgpu. reply asyx 10 hours agorootparentprevApple submitted Metal as a web spec and they turned this into WebGPU and Apple got everything they asked for to avoid apple going rogue again. The fear that Apple of all companies is going to drop WebGPU support is really not based in reality. reply flohofwoe 8 hours agorootparentprev> because Apple will essentially have its own flavor of WebGPU Apple's WebGPU implementation in Safari is entirely spec compliant, and this time they've actually been faster than Firefox. reply rwbt 8 hours agorootparentI wish Apple made a standalone Webgpu.framework spinning it off of WebKit so that apps can link to it directly instead of having to link to Dawn/wgpu. reply flohofwoe 6 hours agorootparentSounds like an interesting idea at first until the point where they will probably create a Swift/ObjC API around it instead of the standard webgpu.h C API, and at that point you can just as well use Metal - which is actually a bit less awkward than the WebGPU API in some areas. reply rwbt 5 hours agorootparentMaybe it makes more sense as a community project. Not sure how difficult it'd be to extract it from Webkit... reply nox101 15 hours agoparentprevThe number 1 use of WebGL is Google Maps by several orders of magnitude over any other use. At some point they'll likely switch to WebGPU making it the number 1 use of Google Maps. Google went over what this enables when they shipped it. Lots of features including being able to highlight relevant roads that change depending on what you searched for. https://www.youtube.com/watch?v=HrLyZ24UcRE Apple maps and others also use it reply pornel 6 hours agoparentprevI'm new to GPU programming, and WebGPU and Rust's wgpu seem pretty nice to me (except the bindings!). The API is high-level enough that it's easy to learn. It hasn't grown deprecated parts or vendor-specific extensions yet. reply lukaqq 17 hours agoparentprevWeb video editor(https://chillin.online), we are eagerly looking forward to the WebGPU API maturing and being extended to all major browsers, enabling faster rendering, bringing more effects, and facilitating the rendering and editing of 3D assets. reply edflsafoiewq 17 hours agoparentprevDevs in the future? There was a long time between when WebGL2 released and when it finally worked \"everywhere\" too. reply AshleysBrain 18 hours agoparentprev [–] Our browser based game engine Construct (https://www.construct.net) supports rendering with both WebGL and WebGPU. reply doctorpangloss 17 hours agorootparent [–] When I visit a Construct 3 Ultra Pixel Survive on Mobile Safari on the latest production iOS with either WebGPU enabled and disabled, I only see black: https://www.construct.net/en/free-online-games/ultra-pixel-s... Which Construct 3 game should I try on Mobile Safari? I know there are other game engines. Supporting Mobile Safari is very very hard. It has its own flavor of everything. I would never speak in absolutes about some web standard and how it will work on Mobile Safari. reply itishappy 17 hours agorootparentRuns great on Android! reply doctorpangloss 17 hours agorootparentThen you agree that WebGPU isn't for Construct, it's for Unity. reply itishappy 16 hours agorootparentNo. How does that follow? reply nox101 15 hours agorootparentprev [–] You realize that WebGPU has not shipped in Safari right? There's a reason it's still behind a developer flag. It's not finished. reply doctorpangloss 13 hours agorootparent [–] Construct 3 doesn’t work with WebGPU disabled either. I’m sure it has official support for Mobile Safari, just not official enough to work when someone visits a Construct 3 experience. I’m not dunking on the engine. It’s just to say, well this is what graphics programming in browser is: making shit work on mobile Safari. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The WebGPU specification is under active development, with major tech companies like Google, Mozilla, Apple, Intel, and Microsoft collaborating weekly.- A recent meeting aimed at advancing WebGPU to candidate recommendation status for the World Wide Web Consortium (W3C) and discussed new features such as subgroups, texel buffers, and 64-bit atomics.- The meeting highlighted the importance of collaboration and feedback, focusing on enhancing WebGPU's capabilities for web graphics and artificial intelligence (AI)."
    ],
    "commentSummary": [
      "WebGPU is focusing on adding features like bindless to enhance performance by minimizing frequent state changes, although implementation may take time.- Current limitations, such as small default texture limits, restrict WebGPU's use in serious applications, posing challenges for Rust game developers.- Despite these constraints, WebGPU is considered promising for advanced graphics and compute capabilities in web applications, with potential uses in gaming and machine learning, though platform adoption remains a concern, particularly with Apple's unique implementations."
    ],
    "points": 166,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1732227537
  },
  {
    "id": 42210553,
    "title": "Tailwind CSS v4.0 Beta 1",
    "originLink": "https://tailwindcss.com/blog/tailwindcss-v4-beta",
    "originBody": "Tailwind CSS v4.0 Beta 1 Date Thursday, November 21, 2024 Adam Wathan @adamwathan About eight months ago we open-sourced our progress on Tailwind CSS v4.0. Hundreds of hours of fixing bugs, soul-crushing backward compatibility work, and troubleshooting Windows CI failures later, I’m excited to finally tag the first public beta release. As I talked about when we published the first alpha, Tailwind CSS v4.0 is an all-new engine built for performance, and designed for the modern web. Built for performance — full builds in the new engine are up to 5x faster, and incremental builds are over 100x faster — and measured in microseconds. Unified toolchain — built-in import handling, vendor prefixing, and syntax transforms, with no additional tooling required. CSS-first configuration — a reimagined developer experience where you customize and extend the framework directly in CSS instead of a JavaScript configuration file. Designed for the modern web — built on native cascade layers, wide-gamut colors, and including first-class support for modern CSS features like container queries, @starting-style, popovers, and more. There’s so much more to say, but everything you need to get started is in the new beta documentation we published today: Get started with Tailwind CSS v4.0 Beta 1 → Start building and help us bullet-proof this thing for the stable release early in the new year.",
    "commentLink": "https://news.ycombinator.com/item?id=42210553",
    "commentBody": "Tailwind CSS v4.0 Beta 1 (tailwindcss.com)157 points by creativedg 16 hours agohidepastfavorite98 comments kookamamie 10 hours agoRead some of the Getting Started documentation, for fun. First step: > Installing with Vite What the hell is Vite? Oh, it seems to be something you need to install via npm. I cannot help but to ask - why do we require npm, or Vite, for something that looks like it should \"help\" with CSS? Why is the webdev such a shitshow, that seems to get ever deeper into its own weird rabbit hole? reply deergomoo 10 hours agoparentI agree with you in the general case, but many people are going to be using Tailwind as part of a build step, and many of those people will be using Vite to do that build. Two options down is the option to install the CLI, which does not require any other build tools. As for why it requires npm: I dislike the Node ecosystem as much as the next guy, but it’s a tool used for web development. Where else are you gonna install it from? reply jph00 9 hours agorootparentnpm is for designed primarily for js dev. Many (most?) sites aren't written in js -- they're written in Java, C#, Python, PHP, etc. Thankfully, tailwind is available directly as a downloadable binary. Unfortunately the link in their beta docs is buried, and also broken -- the correct link is here: https://github.com/tailwindlabs/tailwindcss/releases/ reply simonsarris 2 hours agoparentprevTo answer your question sincerely, Tailwind is a series of utility classes that make it easy to author CSS within HTML (pages or components). Back in the day we had many libraries to do this, the most popular was called Bootstrap. It looks like this: https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootst... You'll notice that Bootstrap CSS includes every single utility class within Bootstrap. This is fine, but it could be a lot smaller - reducing page load times for clients and data transfer for servers - if the CSS included by a library was solely the CSS used by one's site, and no more. This is why we web developers use an npm/vite build step generally, even if our out is very plain static HTML/CSS. \"Well why can't I just use Tailwind like I do with bootstrap? I just want to play around with it and not have to learn all that build stuff.\" You can: https://tailwindcss.com/docs/installation/play-cdn You'll also notice, if you search for Bootstrap today, that it expects one is using npm and webpack, parcel, or vite. reply jamie_ca 1 hour agorootparentAlso: When served from a CDN like your link above, popular CSS like Bootstrap had a decent chance of _already_ being in your browser's cache, so despite downloading more initially you'd come across sites using the same version that meant _no_ delay downloading CSS for it. reply c-hendricks 1 hour agorootparentIsn't this outdated information? https://httptoolkit.com/blog/public-cdn-risks/ > Most importantly: cached content is no longer shared between domains. This is known as cache partitioning and has been the default in Chrome since October 2020 (v86), Firefox since January 2021 (v85), and Safari since 2013 (v6.1). That means if a visitor visits site A and site B, and both of them load https://public-cdn.example/my-script.js, the script will be loaded from scratch both times. reply hu3 1 hour agorootparentprev> You'll also notice, if you search for Bootstrap today, that it expects one is using npm and webpack, parcel, or vite. It doesn't? Most of my clients and websites I saw just use the bundle. https://getbootstrap.com/docs/5.0/getting-started/introducti... The docs don't expect any complicated tooling. reply lenkite 10 hours agoparentprevGenerally use the standalone tailwind CSS CLI tool for keeping things lean. Sadly the download link from the \"Getting Started\" page https://github.com/tailwindlabs/tailwindcss/releases/tag/v4.... is broken. Shows that the \"bloated\" way of doing things is now the norm. reply 0x6c6f6c 6 hours agorootparentThis is a feature for projects already using Node and Vite (which is quite popular these days). It's also not a requirement. The binary still exists, you can run it standalone. reply zaphodias 6 hours agoparentprevWhile I don't disagree (broadly speaking) with you about the complexity of webdev, it looks like you don't have a full understanding of what Tailwind is: a (CSS) code generator. It happens to be written in JS so it can be integrated as part of existing build steps or bundling. That's the rationale behind it. :) It's build-time, not something client-side. reply asteroidburger 10 hours agoparentprevSoftware is required to \"help\" with CSS. Only seems reasonable that you'd need to install said software. Installing the Tailwind package provides the Tailwind software. So when you build your website that uses the `block` class, that class actually exists somewhere - because you pulled it in with the installation via the package manager. Surely we can agree that distributing shared libraries via a package manager is a good practice, no? reply kookamamie 10 hours agorootparentCSS is text. What's wrong with copying the required CSS stylesheets over and including (importing) them from your own stylesheets? Surely we don't need a \"package manager\" and a \"build chain\" for this? Then again, I'm a person that writes any web-related code (HTML, JS, CSS, etc.) by hand. Where did we take the left-turn of not understanding how things work on the actual tech level? These tools hide all of the actually required steps to npm-infested bloat. reply ahussain 9 hours agorootparentThe number of tailwind classes is so large now that copy/pasting the whole set into your project would mean a huge bundle size. Part of that the tailwind package is doing is making sure that only the relevant tailwind classes are included. reply dotancohen 9 hours agorootparentSo Vite builds a custom style sheet based upon the Tailwind classes that are actually used in the project? reply agos 9 hours agorootparentyes, exactly reply macguillicuddy 10 hours agorootparentprevx86 ASM is text but we don't build software by copying and pasting it. While the underlying thing we ship is CSS, that doesn't mean we can't add tooling layers to make it easier or more efficient. While it's totally allowed to write by hand, most frontend web developers I know are very comfortable with NPM, and indeed having dependencies consumed from a package manager is often preferred. reply mablopoule 8 hours agorootparentprev> Surely we can agree that distributing shared libraries via a package manager is a good practice, no? At one point, long ago, you could just download a file, reference it in your index.html, and use it without ever having to worry about updating this package. It had its flaws, but it also had many advantages compared to having an external dependency that might conflict with your version of Typescript, or being highjacked by bad actors. I don't diss the concept of package managers at all, but there are lots of case where vendoring an external package is preferable than adding it as a dependency. reply jarek83 8 hours agoparentprevWebdev is a shitshow so investors money could burn quicker. Huge potential for companies that can come in and transform the stack by throwing away the whole \"modern\" JS stack and use things like Turbo or LiveView reply mablopoule 8 hours agorootparentAgree, and the most maddening thing is that the platform is honestly better than ever. Doing your layout in CSS is so much easier now than it was 10 years ago, thanks to CSS variable, containers queries, and widely supported flexbox & grid. It's the NPM ecosystem above that's being stupid, and believe that adding more tools and more processing is a good thing. reply phist_mcgee 7 hours agorootparentprevThis is such a biased take. There's many ways to build a website, you just don't like the most popular way because you're not comfortable with the technology and ethos. That's ok, but don't go around spouting nonsense about about investors throwing their money away on js projects. reply hamishwhc 10 hours agoparentprevBecause if you are working with CSS, there’s a decent chance you’ll be using npm and/or Vite, and it’s convenient to integrate your tools to save yourself time. Don’t like it, pick another installation method, like the standalone CLI build option, that needs neither. reply Leonelf 8 hours agoparentprevAre you just lazy? Apart from not being up to date with common ecosystems (which I'd expect when doing anything web), look at the Getting Started: Getting started ->Installing with Vite ->Installing with PostCSS ->Installing the CLI ->Upgrading from v3 At the bottom of the CLI paragraph it says \"You can also download standalone builds of the new CLI tool from GitHub for projects that don’t otherwise depend on the Node.js ecosystem.\" They just put the most common stuff at the top, because that's what most devs will use. reply ssijak 9 hours agoparentprevWhat is the issue? Except you not being up to date with the ecosystem and complaining about it? Why do I need a car key and worry about gasoline? I can just hop on a horsie and ride away! reply dotancohen 9 hours agorootparentSo Vite is a security measure against opportunistic theft? Theft of what? Hey, I yearn for the days of good car analogies too. But the car key is not an essential part of the car experience, rather a necessity borne of the distrust of follow man. reply astrofinn 9 hours agoparentprevLooks like you are not really familiar with how Tailwind works. And if you don't understand it of course Vite or a build step will look weird. But I suggest first getting familiar with the thing you are discussing to avoid adding misguided comments to the discussion that really don't add any value. reply dotancohen 9 hours agorootparentThough you are of course correct, I see no problem with somebody reading about technologies that they themselves don't use here on HN. I personally read HN to get familiar with new technologies that I may find useful in my work and private life. I first heard of OpenAI and Anthropic here. reply Timber-6539 9 hours agoparentprevAside from that the website also has a guide to install it with postcss, a tool I've never heard of before (forgive my intentional ignorance). reply thiht 7 hours agoparentprev> What the hell is Vite? That’s on you. Vite is the de facto standard bundler today and one of the best things to happen to the frontend ecosystem in the last few years. Without even using it, do yourself a favor and check it out. reply notRobot 12 hours agoprevI've never had as much fun doing front-end web stuff as I've had since I've picked up tailwind. reply davidsainez 11 hours agoparentI resisted using it for a long time, it just seemed too much like read-only code for me. But I've been building my latest project with it, and I have never iterated on the front end more quickly. It is great fun. reply iKlsR 7 hours agorootparent80/20 is the greatest \"thing\" of all time. You critique it from the outside but when you're in, oh man. reply notsylver 11 hours agoparentprevi've been using it for... years? and it still feels like magic when i use it. v4 fixes my only real issue with it which was the config file feeling so detached and rough compared to everything else. reply caustic 9 hours agoparentprevIs tailwind better than CSS modules, and if so then why? reply agos 8 hours agorootparentthey serve different purposes. CSS modules is a (great) mechanism for providing isolation, very useful in the context of today's component-oriented front end development. Tailwind provides some (quite good) building blocks for a design system, such as a palette, a sizing scale, values for shadows and rounded corners... and a vast set of classes to apply them. The set of classes covers basically every CSS feature, making it possible to slap everything in the class attribute of your HTML elements and never or almost never have to deal with CSS files. In other words, CSS modules is a solution for those who love CSS and needed just a way to not deal with naming clashes without resorting to BEM. Tailwind is a tool for those who very much would like to avoid writing CSS reply EasyMark 3 minutes agorootparentMaybe I should revisit this stuff. I’ve been fighting anything other than my own custom CSS classes for a while. It’s beginning to look at bit dated and I have no interest in rewriting them for the few sites that I support (family members and a couple of charities). I am no web guru and much prefer my hobby/day job of embedded stuff that you’ll never see on a GUI. I keep hearing great stuff about tailwind though. reply vendiddy 10 hours agoparentprevI feel the same. I tried many css solutions over the years and always made a mess. Tailwind is by far the first time I'm productive with styling. I'm impressed with everything they've built. reply yieldcrv 12 hours agoparentprevthis is the honeymoon phase, wait until your package manager and transpiler is out of date, and your progressive web app framework is out of date, and your typescript version isnt compatible with the upgrade yet, and tailwinds isnt either or it is but none of the documentation is yet, but you have to upgrade because your CI/CD cant run your version of node anymore and now you have 100 flags across 5 configuration files and dont know which one to change to make everything work, but changing it might break the ability for a random dependency to compile, and even if you get that to work it turns out your project doesnt render anymore reply quantadev 11 hours agorootparentGood rant! That's kind of what web-development seems like sometimes. Almost every part of the technology stack is there to fix some other part of the stack that never was ideal to begin with. TypeScript being the best example. It only exists because JavaScript sucks, and JavaScript only ever existed because in 1994ish some guy failed to get Java to run in the browser, and the only reason we're building apps in what was originally a \"Document Rendering\" system (HTML) was also not by design either but just an accident of history. One train wreck after another. reply agos 8 hours agorootparentno, it's not a good rant. this is the billionth \"web dev is too complicated!\" rant that is present under every. single. thread. about any technology vaguely related to front end development. reply quantadev 28 minutes agorootparentI just started HN a few months ago, so I haven't been around long enough to realize there's certain topics the HN aficionados have grown bored with. My apologies for the disruption. reply lucsky 2 hours agorootparentprevI haven't yet reached the end of the comments, but I'm also fully expecting at least one \"Electron is a cancer that is eating my computer and the reason why my wife left me\". reply phist_mcgee 7 hours agorootparentprevThis site leans heavily backend it seems. There's a strain of patent smugness that comes with a lot of comments about frontend on this site and it's quite disheartening to see it. You rarely see frontend devs disparaging backend devs for their tech choices. reply notRobot 3 hours agorootparentprevI've been using it for a long time now and the honeymoon period hasn't ended. I do understand what you're saying, but luckily I keep it pretty simple with a standalone tw executable and no node.js or complicated frameworks, so I have been able to avoid these pitfalls. https://tailwindcss.com/blog/standalone-cli reply norman784 9 hours agorootparentprevWhat do you think are the best option to avoid that? I was wondering what could I do with a project that I have that is a long lived code base, so we have some legacy parts. Looking at the alternatives I was considering Vite with React and Vue plugins (and try to migrate my code so it works by default without any other configuration in Vite) or try Astro (because I have Vue and React) with their default configuration, but still not sure what would be the best option. If it where just for me I would rather use React like framework (while I don't like that much React) but at least bun supports JSX out of the box and they seems to be working towards having a bundler integrated. reply inopinatus 10 hours agorootparentprevIt’s a standalone executable. reply BillyTheKing 11 hours agorootparentprevor just use bun? used to also face all these issues - but with bun it's mostly gone for now (and for newer projects) - hope it stays that way! reply assimpleaspossi 10 hours agorootparentOr just use CSS. Never had any of those issues. reply azangru 10 hours agorootparentAlways the right choice! reply yieldcrv 11 hours agorootparentprevI use bun where I can, and I’ve upgraded old projects to it successfully the main project I work on uses nx, sst, and pnpm, it’s all tightly coupled. this project is fine for now. just different. reply devjab 10 hours agoparentprevI don’t work on the frontend very often, I think the last time I did so was when bootstrap was “the tailwind”, but when I recently had to cover for one of our react developers I got to work with it. I genuinely didn’t think there was a difference in my work flow between the two. Looking up meta language magic in the official documentation and then using it through brute-force development. Maybe I’m missing something, more likely I’m just a terrible frontend developer. The latter is true either way. These days I’ll use HTMX and let a LLM do the CSS. Unless I specifically have to work on something which faces customers. So I’m wondering if there is some trick to tailwind I’m missing? reply throwup238 10 hours agorootparentBootstrap is so much worse because of all the nested CSS styles. One of the big benefits of utility classes is that they’re relatively isolated to one element (with a few exceptions like tailwind’s group classes). This has a lot of benefits like eliminating spooky action at a distance and making elements so independent they can just be copy pasted between Tailwind projects. If you’re brute forcing Tailwind then you need to study CSS fundamentals. The vast majority of its utility classes are essentially 1:1 translations of CSS properties with some syntax sugar and DX improvements. Translating them to custom classes is a mechanical process even without @apply. There are even browser extensions like Windy that can rip HTML elements with arbitrary CSS to Tailwind classes. reply devjab 7 hours agorootparentProbably, I’m just sharing how it feels to use it when you work on frontend once a year. reply atsjie 14 hours agoprevThey are switching from sRGB to OKLCH. First time I heard of OKLCH tbh. Anyone know if that is part of a wider adoption trend or is Tailwind pioneering here? Looking at the examples it does seem to offer some advantages; but was primarily surprised that they now use it as a default. reply kevinsync 14 hours agoparentOKLCH is a curve that makes everything look nicer lol https://abhisaha.com/blog/interactive-post-oklch-color-space... reply Brajeshwar 12 hours agoparentprevOKLCH is widely supported in all modern browsers.[1] My easiest winning point with getting teams to start with OKLCH is that we can 'programmatically' control the color shades and tints by tinkering with numbers/values. To the designers (who don't write codes), I tell them that they can now focus on choosing the key colors (primary, accent, etc.) and then let CSS do the magic. A good friend maintains a tool to tinker with various shades/tint of colors with OKLCH https://colorcolor.in 1. https://caniuse.com/?search=oklch reply CGamesPlay 13 hours agoparentprevFirst, note that all colors in sRGB can be losslessly converted to OKLch (but not the other way around), so I don't know if this actually changes the colors or not. The reason to use OKLch is so that when using CSS color mixing (via animations, gradients, etc), they look \"better\", as indicated by the sibling comments. reply block_dagger 14 hours agoparentprevAccording to this article [1] OKLCH accounts for biases in human perception of color brightness. Seems useful, but I don't know the answer to your question. [1] https://evilmartians.com/chronicles/oklch-in-css-why-quit-rg... reply bryanrasmussen 13 hours agoparentprevOKLCH has the main advantage of LCH, which is that the numbers make sense, meaning that if you have two colors that are the same lightness they will look like they are the same lightness, because the numbers make sense you can now do programmatic color manipulation - increase lightness by 5 etc. that in the past would have been too difficult to really do (so people would instead just have variables giving the different rgb values and switch them in) OKLCH just basically exists because there is a hue change from blue to purple in LCH when the lightness goes less which does not match how humans think of these colors (supposedly, don't know if there is any cultural difference) So in OKLCH the lighter blue does not look purple like it does in LCH, it looks like lighter blue. reply CharlesW 14 hours agoparentprev> Anyone know if that is part of a wider adoption trend or is Tailwind pioneering here? It's part of a wider adoption trend. https://bottosson.github.io/posts/oklab/#oklab-implementatio... reply chrismorgan 9 hours agoparentprevOklch is a polar colour space corresponding to the rectangular Oklab, designed as a perceptual colour space, matching how people see things, rather than how display technology works. There are three things to consider here: 1. Colour interpolation, as used in things like linear-gradient() and color-mix(). The default of sRGB is not particularly good; choosing Oklab or Oklch instead will normally improve things. Oklch is an okay default these days. I reckon that for most applications, Oklab is better. Oklch tends to be too vibrant in the most extreme cases like #ff0000 → #0000ff. Here’s a simple demo you can play with: data:text/html,div{width:20em;height:2em;display:flex;align-items:center;justify-content:center;background:linear-gradient(in var(--space) to right,var(--from),var(--to));}document.querySelectorAll(\"input\").forEach(e=>e.onchange())sRGBOklabOklch 2. Specifying colours that are beyond the sRGB gamut. This is what they say in https://tailwindcss.com/docs/v4-beta#modernized-p3-color-pal...: “taking advantage of the wider gamut to make the colors more vivid in places where we were previously limited by the sRGB color space”. Frankly, this is seldom actually advisable. For most purposes, sRGB is quite enough, thank you: go beyond it, and your colours will be too vivid. In photographs it makes sense, but for colour palettes used with blocks of colours and such, it’s normally a bad idea. Yet if you are trying to go beyond sRGB, meh, nothing wrong with writing it in Oklch. Though color(display-p3 ‹r› ‹g› ‹b›) may be easier to reason about— 3. Specifying general colours. I’ll be blunt. Perceptual colour spaces are horrible at this. With things like HSL and LCH (and even display-p3 colours), you get values in a known range (e.g. 0–100%, 0–360°, 0–1), and they all work. With things like Oklch, do you know how thin the range of acceptable values is? Just try working with yellow, I dare you, or anywhere at the edges of what things are capable of. As a human, you cannot work with these values. You have to treat them as opaque, only to be manipulated by colour-space-aware tools. To take the most extreme example, take #ffff00, which https://oklch.com/ says is approximately oklch(0.968 0.21095439261133309 109.76923207652135)… whoops, you already slipped out into Rec2020 space. And it goes approximating it to #ffff01, anyway. And fairly minor adjustments will take it out of any current (or probable!) gamut. Just look at the diagrams, see how slim the area of legal values is at this extreme. Perceptual colour spaces are really good for some things: interpolation, and some data visualisation palette things. But beyond that, you’re in a digital world, and a limited world at that, and they’re actually quite difficult to work with, and you should normally stick with #rrggbb. Especially if you have any interest in working near the maximum of any colour channel. As an example, look at a place oklch() is used in that document: --color-avocado-100: oklch(0.99 0 0); --color-avocado-200: oklch(0.98 0.04 113.22); --color-avocado-300: oklch(0.94 0.11 115.03); --color-avocado-400: oklch(0.92 0.19 114.08); --color-avocado-500: oklch(0.84 0.18 117.33); --color-avocado-600: oklch(0.53 0.12 118.34); I don’t know how they chose those numbers, but there’s no low-order polynomial curve there; they seem entirely arbitrary choices. Not a good showing for Oklch. The “modernized P3 color palette” shown is just as bad: all of L, C and H seem arbitrarily chosen. If you’re going to do it like that, you’re completely wasting your time putting the numbers in a perceptual colour space. To be clear: Oklch is way better than RGB for some things, but it’s downright terrible for some purposes (often because you do actually care about the display technology, rather than a theoretical model of how vision works), and a lot of the claimed benefits for some purposes don’t hold up to real analysis. reply seanwilson 4 hours agorootparentAny thoughts on what color space you'd prefer to use? I'm working on a Tailwind palette creator (https://www.inclusivecolors.com/, it's based around letting you visualise the curves you mentioned), which uses HSLuv right now to avoid problem 2 in your list. HSLuv has perceptual uniformity as well (which is great at helping you pick colors that contrast) but it doesn't support P3 unfortunately, similar to Google's HCT color space. I'm not seeing much demand for P3 at the moment though. Most designers I've spoke to don't know it exists. reply going_north 9 hours agoprevCSS first configuration is a good change! It seems like it would makes it easier to combine tailwind with regular CSS files which still uses the same design tokens. This is useful e.g. when creating a site with a component architecture where the components are styled with CSS, but some of the content comes from CSS or markdown. reply renerick 4 hours agoprevAfter some experimenting I found that Tailwind works best with hybrid approach. The essay[1] is correct, that often you have one-of-a-kind blocks, like headers, footers, etc. that you can just fully implement with TW utilities, reducing cognitive load for class naming and structuring. But for more reusable elements, like buttons, links, inputs, popups, it's just much nicer to have them (and their variations) behind simple `btn btn-blue btn-small`. Even when using component-based approach, it's just much simpler to manage all those various states as explicit classes, rather than re-implementing CSS cascade in JS to determine how to combine corresponding TW utilities. I feel like if Tailwind Labs would be less radical in their marketing strategy, it'd receive a bit less of a backlash. Maybe :) And I do agree, that moving to better integration with native CSS is a step in the right direction, very curious about future of v4 [1]: https://adamwathan.me/css-utility-classes-and-separation-of-... reply croisillon 9 hours agoprevi never used Tailwind but i can't wait for the release video clip for the curious: - v2 https://www.youtube.com/watch?v=3u_vIdnJYLc - v3 https://www.youtube.com/watch?v=TmWIrBPE6Bc reply emmacharp 5 hours agoprevI invite (for fun and learning!) anyone here still thinking native CSS provides no efficient solution to the problems Tailwind may have solved 5 years ago to challenge me: Bring up any said problem and I'll give you an efficient, robust, fast, simple and maintainable way to solve it in pure, native CSS (maybe even with further advantages!). The time has come to embrace good ol' CSS again! Heheh. reply thiht 3 hours agoparentI'm a new user of Tailwind, but there are 5 points where I believe Tailwind is wayyy more efficient than CSS: - layout: flex and grid are great solutions, Tailwind just makes them extremely easy to use. Flex and grid are barely unusable without first defining at least 4 or 5 helper classes, more likely a dozen. Tailwind just provides them and makes them easy to use. I never could remember the flex properties, I learned the Tailwind classes I needed in like half a day of practice - mobile first design: Tailwind strongly encourages mobile first design if you want to do responsive design. It scared me at first because media queries are hard to manipulate in CSS. I've never made a responsive design page faster than with Tailwind, it's insane how easy it is - dark mode: as simple as `dark:xxx`. The default and dark properties being close together is awesome. As with responsive design, Tailwind prefixes make it so easy to do what's painful in plain CSS - defaults: having sane, overridable defaults for everything (sizes, colors, whatever) makes the code infinitely more consistent. You barely even need to think about real values, just how it looks, and you have less risk of defining slightly different classes for no reason - no cascading: CSS cascading makes it hard to reason about styles, and easy to break unrelated components by mistake. It's solved with CSS modules (or even with BEM), but Tailwind just eliminates the problem CSS has the solutions, it just doesn't make them easy to use. reply emmacharp 1 hour agorootparent1. I don't think I understand why you think flex & grid are barely unusable... without a dozen classes..? Maybe an example would clarify this perception for me. 2. You barely need media queries anymore for responsive design. Auto-adapting flex or grid containers and the use of clamp() reduced 90% of my media query usage. I usually use like 1 to 4 media queries in project (for master layout and responsive nav, for example). And there's a nice side effect to this type of intrinsic design : layout and type is way easier to optimize for every viewport! Mobile first design is really easy to approach in this way. 3. You can apply a dark mode by only switching a theme file with design tokens inside. Way lighter, way easier to read, does not clutter your codebase. 4. As a corollary to the previous point, the defaults you are talking about can be CSS custom properties defined in a theme file as I think, Tailwind 4 will do. You could then use the exact same defaults in native CSS. 5. Simple rules about CSS usage, enforced by Stylelint for example, can prevent the vast majority (and maybe all!) of the cascading problems you mention. Here are some rules and a Stylelint config I posted in a reply below: https://ecss.info. I've got a good track record with teamwork and this config. CSS solutions may be really easy to use when approached properly. I'm happy to follow up with examples and/or reply to subsequent questions and thoughts! reply Vinnl 5 hours agoparentprevOk, my problem is that I don't know just by reading the styles that other contributors wrote, whether they are still relevant, or if the HTML they applied to was changed or removed. They didn't write clean atomic commit, and I can't get them to adhere to some sort of convention. The other problem is that I don't have a designer and tend to make things ugly given full freedom, but I do want things to have their own visual identity. reply emmacharp 5 hours agorootparentGreat questions/problems! 1. I address the problem of relevance by inserting the CSS right into the component with a link tag. If the component is not used, the CSS isn't either. A positive side effect of this technique is that you always have an easy access to the CSS by following the link in your editor of choice. As for the relevance of the rules inside the component, the said component should be light/simple enough that this isn't harder that glancing a minute (max!) at both files. 2. You can use Stylelint to ensure adherence to specific rules. I have developed a config aiming to prevent these kinds of problems. You can find rules, guidelines and a link to the Stylelint config (still beta) at https://ecss.info. 3. A CSS theme file with custom property design tokens is sufficient. As I understand, Tailwind 4.0 made the switch to CSS tokens. You can thus use the same naming convention for your tokens in native CSS. As complementary advantages you get future-proof code, no build step, and a lot less unused CSS (for instance, Tailwind's own homepage sports 85% unused CSS!). I'm happy to elaborate further or respond to any subsequent questions you may have! reply seanwilson 15 hours agoprevTailwind threads usually include comments and questions that are answered in the documentation, so here's some useful links for people that haven't used Tailwind before. A core part of Tailwind is that you reuse styles by using a templating system vs using custom CSS classes e.g. you have a button.html file that contains the styling for your buttons for reuse, so you don't have to keep repeating the same utility classes everywhere. https://v2.tailwindcss.com/docs/extracting-components#extrac... > It’s very rare that all of the information needed to define a UI component can live entirely in CSS — there’s almost always some important corresponding HTML structure you need to use as well. > For this reason, it’s often better to extract reusable pieces of your UI into template partials or JavaScript components instead of writing custom CSS classes. @apply (e.g. .btn { @apply py-2 px-4 bg-indigo-500 text-white }) is only meant for reusing styles for simple things like a single tag and is generally recommended against because you should use templates instead: https://v2.tailwindcss.com/docs/extracting-components#extrac... > For small components like buttons and form elements, creating a template partial or JavaScript component can often feel too heavy compared to a simple CSS class. > In these situations, you can use Tailwind’s @apply directive to easily extract common utility patterns to CSS component classes. Inline styles can't be responsive and can't target hover/focus states e.g. there's no inline way to write \"text-black hover:text-blue py-2 md:py-4 lg:py-5 lg:flex lg:items-center\" and the CSS equivalent is very verbose. https://v2.tailwindcss.com/docs/utility-first#why-not-just-u... > But using utility classes has a few important advantages over inline styles: > Designing with constraints. Using inline styles, every value is a magic number. With utilities, you’re choosing styles from a predefined design system, which makes it much easier to build visually consistent UIs. > Responsive design. You can’t use media queries in inline styles, but you can use Tailwind’s responsive utilities to build fully responsive interfaces easily. > Hover, focus, and other states. Inline styles can’t target states like hover or focus, but Tailwind’s state variants make it easy to style those states with utility classes. Opinion: As utility classes are quick to type and change, and it's easy to find where you need to edit to change the styles of a component, it's an order of magnitude quicker to iterate on designs compared to CSS that's scattered across files and shared between pages in hard to track ways. CSS specificity and cascading aren't often used, and mostly just cause complexity and headaches so are best avoided. Tailwind-style vs classic CSS is similar to composition vs inheritance in OOP with similar pros/cons, where complex inheritance is generally discouraged now in OOP languages. Yes, the Tailwind approach is going against the standard CSS approach, but CSS wasn't initially designed for highly custom responsive designs so it's not surprising if its \"best practices\" don't fit everywhere. Also, Tailwind is really for custom responsive UI and website designs. If your site is mostly Markdown documents and you don't need a custom design or complex mobile/desktop styling, the above isn't going to make any sense and plain CSS or something like Bootstrap is likely a better choice. reply bryanrasmussen 12 hours agoparentHow does Tailwind handle responsive design issues better than CSS? Assuming of course CSS where the devs know how to use clamp. reply throwup238 12 hours agorootparentYMMV but I find ‘lg:…’ to be much better than ‘@media (min-width: 992px) { … }’ at the expense of repeating ‘lg:’ every class. Since most components don’t need too many responsive classes per breakpoint, this is a net benefit most of the time (and nothing is stopping devs from combining utility classes and custom css with media queries). Like GP said, this kind of specificity is impossible in inline styles altogether which is relevant when not using CSS-in-JS in React et al. reply emmacharp 6 hours agorootparentThe clamp() CSS function (and auto-adapting grids) can reduce media query usage to a really loooow minimum. Nowadays I use media queries only for mobile nav and maybe layout grid adaptation. Like 2 to 4-5 media queries per project maximum. I think that's what the comment you're responding to meant. reply teaearlgraycold 15 hours agoparentprevI see tailwind as a new form of CSS built for the component age. It’s not a framework or design system. reply sodapopcan 14 hours agorootparentYes! This sums it up perfectly. To the latter point, it's more so a tool for building design systems that comes with a [very large and thorough] default implementation. reply conradludgate 13 hours agorootparentprevI prefer scoped css, eg svelte or react with CSS modules. This allows one to closely pair the styles with the component, but still separate out the styling from the html (I cannot stand tailwind/inline syles) reply foretop_yardarm 9 hours agorootparentMy favourite way too and fwiw not mutually exclusive with Tailwind (in case anyone was wondering). reply tuzemec 10 hours agorootparentprevI see it as a way of granular styling, because there's no cascading. And IMO works great if you style each element (or component) individually. But the moment you need to style real html and not some kind of component structure - you have to look for something else. If I switch to \"old man yelling at the sky\" mode I would say that's an example of a nice concept (utility classes) taken way too far. reply that_guy_iain 15 hours agoprevI'm really curious as to why they felt the need to work on improving performance.[1] Were people complaining it was too slow? Were the performance improvements just the benefit of refactoring they did for other reasons? Considering the build happens once during your build phase, taking under half a second doesn't seem like something I would even bother looking at. So it just jumps out to me, so I'm just curious. [1] https://tailwindcss.com/docs/v4-beta#new-high-performance-en... reply dcre 14 hours agoparentI think a lot of the optimization work was for dev time and it carried over to build time. I listened to a podcast interview with Adam Wathan where I think he said they just got really into shaving nanoseconds. They have a very successful business and I think they just enjoyed doing this work. On the other hand I do think even if you’re cutting from 50ms to 5ms (both low numbers in absolute terms) there are often new unforeseen workflows that become possible once you can do that operation so frequently that it’s free. You could do it on every keystroke. reply joshdavham 13 hours agorootparent> there are often new unforeseen workflows that become possible once you can do that operation so frequently that it’s free. You could do it on every keystroke. That’s really interesting to think about actually! What kinds of practical things do you think could be enabled by an extremely fast tailwindcss? reply LaundroMat 12 hours agorootparentLive reloading reply mattigames 12 hours agorootparentprevLLM trying design using tailwind, stuff like: try to recreate this bitmap image using tailwind utility classes; the iteration speed for that kind of tasks depend on such speed. reply emmanueloga_ 13 hours agoparentprevI think the rewrite aimed to simplify installation and configuration. TW 4.0 switched from PostCSS to LightningCSS [1], and Rust-based tools tend to offer a simpler setup via single binaries (in contrast to the complexity of typical JS-based tools). Moving from pure JS to Rust also brings performance gains. Even if the main goal wasn't \"performance,\" it's a nice side effect worth highlighting. -- 1: https://lightningcss.dev/ reply ripley12 14 hours agoparentprevI absolutely care about things that take hundreds of milliseconds for my builds. The faster I can build, the faster I can iteratively try new things things out. The goal should be to be able to see changes instantly after you make them, and Tailwind is usually just 1 part of a build pipeline. Bret Victor's Inventing on Principle is probably the best demonstration of why this matters; in the first 10 minutes he shows a very concrete example of an insight that would not have occurred without instant feedback https://www.youtube.com/watch?v=EGqwXt90ZqA reply that_guy_iain 14 hours agorootparent> I absolutely care about things that take hundreds of milliseconds for my builds. The faster I can build, the faster I can iteratively try new things things out. The goal should be to be able to see changes instantly after you make them, and Tailwind is usually just 1 part of a build pipeline. When we're talking about ms the build is not the thing that is affecting your ability to try new things out. It was already so fast it was near instant. reply ripley12 13 hours agorootparent1. Tailwind is just 1 part of a build system, and it all adds up 2. Watch Inventing on Principle if you haven't already. Pushing build times into milliseconds or sub-milliseconds enables new capabilities reply that_guy_iain 12 hours agorootparentIf you want to try new things out and move fast with your CSS, you're not building the entire system. It's a partial build and that was already faster than you would even be able to react. Your browser refreshing would take longer. reply asimpletune 14 hours agoparentprevDespite tailwind 3 being very fast there are situations where it can be a little slow and it adds up. I’m personally really glad they’re tackling performance in v4. reply sodapopcan 14 hours agoparentprevNo idea if anyone was complaining, but if building in CI it shaves off some pennies (or more?) no? reply teaearlgraycold 13 hours agorootparentGiven its popularity this can actually get into measurable carbon emission savings. Not world changing. But maybe a few international flights over the lifespan. reply Sesse__ 10 hours agorootparentThe carbon cost of a bloated CSS framework isn't the build time, it's the billions of times the resulting CSS has to be parsed and applied on the web pages where it's used. reply yurishimo 6 hours agorootparentHave you examined the build artifacts of a site using tailwind? The size of the CSS is often smaller than the same styling in \"normal\" hand written CSS. There are a dozen blog posts from large engineering orgs that have confirmed that switching to Tailwind slimmed their builds down. Shopify has been a big proponent of Tailwind and they are absolutely obsessed with shipping as little code as possible because of the scale they operate at. Also, browsers are insanely good at parsing and applying CSS. You need hundreds of thousands of unique selectors before the browser takes more than fraction of a second to parse and render an entire CSS file. https://www.trysmudford.com/blog/i-spent-a-day-making-the-we... reply Sesse__ 6 hours agorootparentYes, I have. I work on browser CSS performance. reply sam_goody 11 hours agoprevLooks good. I am glad that they support container-queries[1]. But all the real goodness is in contain[2] (which sounds the same but is not related). Tailwind really needs to support contain. I am disappointed by their statement that there will be no major additions, because contain is really that important - both from a dev perspective, and from a performance perspective. It belongs in v4 from the start. [1]: https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_contain... [2]: https://developer.mozilla.org/en-US/docs/Web/CSS/contain reply hanneskt 8 hours agoparentLooks like the contain utility has been added here [1] and is also available in v4. [1]: https://github.com/tailwindlabs/tailwindcss/pull/12993 reply barrenko 12 hours agoprev [3 more] [flagged] cambaceres 10 hours agoparent [–] What does that mean? reply nXqd 10 hours agorootparent [–] bros keep eating ... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tailwind CSS v4.0 Beta 1 was released on November 21, 2024, introducing a faster engine and a unified toolchain.- This version emphasizes a CSS-first configuration, catering to modern web development needs with features like native cascade layers and wide-gamut colors.- Users are encouraged to explore the beta documentation to contribute to the preparation for the stable release expected early next year."
    ],
    "commentSummary": [
      "Tailwind CSS v4.0 Beta 1 has been released, introducing features like the OKLCH color space and improved performance, which has sparked discussions in the web development community.",
      "The release has prompted debates about the complexity of modern web development tools such as Vite and npm, with some users questioning their necessity and others defending their efficiency.",
      "Tailwind CSS is praised for its utility classes that simplify CSS within HTML, offering benefits for responsive design and reducing CSS file sizes, similar to but more optimized than Bootstrap."
    ],
    "points": 157,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1732241546
  }
]
