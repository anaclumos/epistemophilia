[
  {
    "id": 42022151,
    "title": "Sleep regularity is a stronger predictor of mortality than sleep duration (2023)",
    "originLink": "https://academic.oup.com/sleep/article/47/1/zsad253/7280269",
    "originBody": "Skip to Main Content Advertisement Journals Books Search Menu Menu Sign in through your institution Navbar Search Filter SLEEP This issue SRS Journals Clinical Neuroscience Neuroscience Sleep Medicine Books Journals Oxford Academic Mobile Enter search term Search Issues More Content Advance Articles Supplements Editor's Choice Virtual Issues Virtual Roundtables Abstract Supplements Subject Basic Science Circadian Disorders Cognitive, Affective and Behavioral Neuroscience of Sleep Insomnia Neurological Disorders Sleep Across the Lifespan Sleep and Metabolism Sleep Disordered Breathing Sleep Health and Safety Submit Author Guidelines Instructions for Reviewers Submission Site Open Access Options Additional Resources Self-Archiving Policy Why publish with this journal? Purchase About About SLEEP Editorial Board Alerts Dispatch Dates Permissions Advertising & Corporate Services Journals Career Network Reprints and ePrints Sponsored Supplements Media Kit Journals on Oxford Academic Books on Oxford Academic SRS Journals Issues More Content Advance Articles Supplements Editor's Choice Virtual Issues Virtual Roundtables Abstract Supplements Subject All Subject Expand Expand Basic Science Circadian Disorders Cognitive, Affective and Behavioral Neuroscience of Sleep Insomnia Neurological Disorders Sleep Across the Lifespan Sleep and Metabolism Sleep Disordered Breathing Sleep Health and Safety Browse all content Browse content in Submit Author Guidelines Instructions for Reviewers Submission Site Open Access Options Additional Resources Self-Archiving Policy Why publish with this journal? Purchase About About SLEEP Editorial Board Alerts Dispatch Dates Permissions Advertising & Corporate Services Journals Career Network Reprints and ePrints Sponsored Supplements Media Kit Close Navbar Search Filter SLEEP This issue SRS Journals Clinical Neuroscience Neuroscience Sleep Medicine Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Article Navigation Close mobile search navigation Article Navigation Volume 47 Issue 1 January 2024 Article Contents Abstract Introduction Methods Results Discussion Acknowledgments Disclosure Statement Data Availability ReferencesArticle Navigation Article Navigation Journal Article Editor's Choice Sleep regularity is a stronger predictor of mortality risk than sleep duration: A prospective cohort study Daniel P Windred, Daniel P Windred Turner Institute for Brain and Mental Health, School of Psychological Sciences, Faculty of Medicine, Nursing and Health Sciences, Monash University , Melbourne, VIC , Australia Corresponding Author: Daniel P. Windred, 18 Innovation Walk, Monash University, Clayton, VIC 3800, Australia. Email: daniel.windred@monash.edu; https://orcid.org/0000-0002-7461-7498 Search for other works by this author on: Oxford Academic Google Scholar Angus C Burns, Angus C Burns Turner Institute for Brain and Mental Health, School of Psychological Sciences, Faculty of Medicine, Nursing and Health Sciences, Monash University , Melbourne, VIC , Australia Division of Sleep and Circadian Disorders, Brigham and Women’s Hospital , Boston, MA , USA Program in Medical and Population Genetics, Broad Institute , Cambridge, MA , USA Center for Genomic Medicine, Massachusetts General Hospital , Boston, MA , USA https://orcid.org/0000-0001-6336-5531 Search for other works by this author on: Oxford Academic Google Scholar Jacqueline M Lane, Jacqueline M Lane Division of Sleep and Circadian Disorders, Brigham and Women’s Hospital , Boston, MA , USA Program in Medical and Population Genetics, Broad Institute , Cambridge, MA , USA Center for Genomic Medicine, Massachusetts General Hospital , Boston, MA , USA Department of Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital and Harvard Medical School , Boston, MA , USA https://orcid.org/0000-0001-6101-2855 Search for other works by this author on: Oxford Academic Google Scholar Richa Saxena, Richa Saxena Division of Sleep and Circadian Disorders, Brigham and Women’s Hospital , Boston, MA , USA Program in Medical and Population Genetics, Broad Institute , Cambridge, MA , USA Center for Genomic Medicine, Massachusetts General Hospital , Boston, MA , USA Department of Anesthesia, Critical Care and Pain Medicine, Massachusetts General Hospital and Harvard Medical School , Boston, MA , USA https://orcid.org/0000-0003-2233-1065 Search for other works by this author on: Oxford Academic Google Scholar Martin K Rutter, Martin K Rutter Centre for Biological Timing, Division of Endocrinology, Diabetes and Gastroenterology, School of Medical Sciences, Faculty of Biology, Medicine and Health, Manchester Academic Health Science Centre, University of Manchester , Manchester , UK Diabetes, Endocrinology and Metabolism Centre, NIHR Manchester Biomedical Research Centre, Manchester University NHS Foundation Trust , Manchester , UK Search for other works by this author on: Oxford Academic Google Scholar Sean W Cain, Sean W Cain Turner Institute for Brain and Mental Health, School of Psychological Sciences, Faculty of Medicine, Nursing and Health Sciences, Monash University , Melbourne, VIC , Australia https://orcid.org/0000-0002-8385-1550 Search for other works by this author on: Oxford Academic Google Scholar Andrew J K Phillips Andrew J K Phillips Turner Institute for Brain and Mental Health, School of Psychological Sciences, Faculty of Medicine, Nursing and Health Sciences, Monash University , Melbourne, VIC , Australia Corresponding Author: Andrew J. K. Phillips, 18 Innovation Walk, Monash University, Clayton, VIC 3800, Australia. Email: andrew.phillips@monash.edu. https://orcid.org/0000-0003-1156-7056 Search for other works by this author on: Oxford Academic Google Scholar Sean W. Cain and Andrew J. K. Phillips contributed equally to this manuscript. Author Notes Sleep, Volume 47, Issue 1, January 2024, zsad253, https://doi.org/10.1093/sleep/zsad253 Published: 21 September 2023 Article history Received: 02 June 2023 Revision received: 13 September 2023 Published: 21 September 2023 Corrected and typeset: 17 October 2023 PDF Split View Views Article contents Figures & tables Video Audio Supplementary Data Cite Cite Daniel P Windred, Angus C Burns, Jacqueline M Lane, Richa Saxena, Martin K Rutter, Sean W Cain, Andrew J K Phillips, Sleep regularity is a stronger predictor of mortality risk than sleep duration: A prospective cohort study, Sleep, Volume 47, Issue 1, January 2024, zsad253, https://doi.org/10.1093/sleep/zsad253 Select Format Select format .ris (Mendeley, Papers, Zotero) .enw (EndNote) .bibtex (BibTex) .txt (Medlars, RefWorks) Download citation Close Permissions Icon Permissions Share Icon Share Facebook Twitter LinkedIn Email Navbar Search Filter SLEEP This issue SRS Journals Clinical Neuroscience Neuroscience Sleep Medicine Books Journals Oxford Academic Mobile Enter search term Search Close Navbar Search Filter SLEEP This issue SRS Journals Clinical Neuroscience Neuroscience Sleep Medicine Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Abstract Abnormally short and long sleep are associated with premature mortality, and achieving optimal sleep duration has been the focus of sleep health guidelines. Emerging research demonstrates that sleep regularity, the day-to-day consistency of sleep–wake timing, can be a stronger predictor for some health outcomes than sleep duration. The role of sleep regularity in mortality, however, has not been investigated in a large cohort with objective data. We therefore aimed to compare how sleep regularity and duration predicted risk for all-cause and cause-specific mortality. We calculated Sleep Regularity Index (SRI) scores from > 10 million hours of accelerometer data in 60 977 UK Biobank participants (62.8 ± 7.8 years, 55.0% female, median[IQR] SRI: 81.0[73.8–86.3]). Mortality was reported up to 7.8 years after accelerometer recording in 1859 participants (4.84 deaths per 1000 person-years, mean (±SD) follow-up of 6.30 ± 0.83 years). Higher sleep regularity was associated with a 20%–48% lower risk of all-cause mortality (p60 000 individuals, we found that people with less regular sleep patterns have a higher risk of premature mortality, and that sleep regularity is a stronger predictor of mortality risk than sleep duration. These findings were robust with detailed control for confounding factors, providing evidence that sleep regularity is a key index of human health and potentially a more important marker of health than sleep duration. Introduction A large body of research has demonstrated that both subjective and objective estimates of average sleep duration are associated with mortality risk [1–9]. Several meta-analyses of subjective sleep duration have associated both short and/or long sleep duration, outside an approximate range of 7–9 hours, with higher risks for all-cause mortality [1–5]. Recently, large studies of objective sleep duration have confirmed these associations of sleep duration with all-cause mortality [6–9]. These findings are supported by results showing associations between sleep duration and many other dimensions of health [10]. Maintaining optimal sleep duration is the central focus of current sleep health guidelines [11]. Recent evidence, however, indicates that sleep regularity, defined as the day-to-day consistency of sleep–wake timing, is a stronger predictor of some health outcomes than average sleep duration [12, 13]. Studies with longitudinal measures of sleep regularity have found associations between irregular sleep and adverse cardiometabolic outcomes [12, 14–17], epigenetic aging [18], depressed mood [19], and lower quality of life [13]. People with irregular sleep patterns are exposed to irregular patterns of environmental stimuli, including light, and may have irregularly timed behaviors, such as physical activity and meals. This unstable timing of both stimuli and behaviors leads to disruption of circadian rhythms, with downstream negative health effects [20]. While mortality risk has been associated with self-reported sleep regularity [21], this relationship has not been investigated prospectively in a large cohort with objective sleep data. Furthermore, the relative importance of sleep duration compared with sleep regularity for mortality risk is not known. We aimed to assess the relationship of objectively measured sleep regularity with risk for all-cause mortality, and mortality from cardiometabolic causes and cancer, in a large cohort (N = 60 977) who wore accelerometer devices for 1 week. We also assessed whether sleep regularity was a stronger predictor of mortality risk than sleep duration. We used data from our previously reported assessment of sleep regularity in UK Biobank participants [22], where we extracted Sleep Regularity Index (SRI) scores. This metric assesses day-to-day similarity in sleep patterns, accounting for irregularity due to fragmented sleep, napping, and variable sleep onset and offset timing. Methods Overview A baseline cohort of approximately 502 000 participants aged between 40 and 69 years were recruited to the UK Biobank between 2006 and 2010 [23]. Participants completed an initial assessment at the time of recruitment, designed to capture information across a broad range of health and lifestyle factors, through questionnaires and physical measurements. Assessment centers were located to capture a range of socioeconomic, ethnic, and urban–rural distributions within the UK population. From this baseline cohort, 103 669 participants wore Axivity AX3 devices (Axivity, Newcastle upon Tyne, UK) on their dominant wrist for 7 days under free-living conditions between 2013 and 2016. Devices were tri-axial, and logged accelerometer data at 100 Hz. Invitations to participate and devices were distributed via post. Written informed consent was obtained and all data collection was conducted in accordance with the Declaration of Helsinki. See Supplementary Methods S1.1 for links to protocol and consent documents. Sleep regularity and sleep duration Sleep regularity was assessed using the SRI [24], a metric that compares the similarity of sleep patterns from one day to the next. The SRI calculates the average concordance in sleep–wake state of all epoch pairs separated by 24 hours (see Supplementary Methods S1.7 for SRI calculation formula). An SRI of 100 represents perfectly regular sleep–wake patterns, and zero represents random patterns. SRI scores were derived in this cohort in our previous work [22], and these scores were used in all analyses presented here. SRI scores were calculated using epoch-by-epoch sleep–wake state across each participant’s 1-week recording. Sleep–wake state was estimated using “GGIR”, a validated, widely used open-source R package for estimating sleep–wake from accelerometer data [25, 26], and “sleepreg”, an R package developed by our group to accurately calculate SRI scores from “GGIR” output [22]. The “sleepreg” package uses sustained inactivity data to account for naps, fragmented sleep, and large periods of wake during sleep in its calculation of SRI scores, which could not be achieved using “GGIR” summary output alone. This method accounted for device non-wear, and excluded instances of miscalculated sleep onset and offset. It also allowed patterns including more than one sleep episode within a 24-hour period to be accurately represented. Valid SRI scores were calculated in participants with at least 120 hours (i.e., 5 days) of 24-hour-separated epoch pairs after non-wear removal and exclusion of miscalculated days. Further detail is included in Supplementary Methods S1.6-8, and all scripts used in generating SRI scores are included in the “sleepreg” package, which is freely available on GitHub [https://github.com/dpwindred/sleepreg]. Sleep duration was calculated on a daily basis for each individual, as the duration of sustained inactivity between daily sleep onset and sleep offset times estimated by GGIR. Daily sleep durations were extracted in the same study days used to calculate SRI scores, and participant-level average sleep duration was calculated across these days. Similarly, participant-level average mid-sleep timing was calculated from daily mid-sleep, defined as the clock time halfway between sleep onset and sleep offset. Intraindividual variability in sleep onset and offset timing was calculated as the standard deviation of daily sleep onset and offset times for each individual. Mortality records Mortality data were received from NHS Digital (England) and NHS Central Register (Scotland). Records include date of death and primary cause of death, diagnosed in accordance with the ICD-10 [27]. Records from June 2013 to March 2021 were included. Cardiometabolic mortality was defined according to ICD-10 diseases of the circulatory system, or endocrine and metabolic diseases (I05-I89, E00-E90). Predominant circulatory causes of death were ischemic heart disease (I20-I25); cerebrovascular diseases (I60-I69); other heart disease (I30-I52); diseases of the arteries, arterioles, and capillaries (I70-I79); and hypertensive diseases (I10-I15). Predominant endocrine and metabolic causes of death were diabetes mellitus (E10-E14); metabolic disorders (E70-E90); and obesity (E65-E68). Cancer was defined as any cause of death by malignant or benign neoplasms (C00-C97, D10-48). Predominant causes of death by cancer were malignant neoplasms of digestive organs (C15-C26); respiratory and intrathoracic organs (C30-C39); lymphoid and hematopoietic tissue (primary; C81-C96); and breast (C50). Covariates Average physical activity was defined as average device acceleration across the recording, after exclusion of low-quality data and periods of non-wear, as described in previous work [28], and was derived from the same accelerometer records used to estimate sleep–wake state. Additional covariates were collected during an initial assessment between 2006 and 2010, including: self-reported ethnic background; employment status; yearly household income; Townsend Deprivation Index (average material deprivation of a participant’s residential location); weekly social activities; frequency of social visits; smoking status; urban or rural postcode; rotating shift work status; prescription of medication for hypertension or cholesterol; diagnosis of cancer, diabetes, or vascular conditions; body mass index (BMI); cholesterol ratio; frequency of depressed mood, unenthusiasm/disinterest, tenseness/restlessness, tiredness/lethargy; and visitation to a general practitioner or psychiatrist for mental health concerns. See Supplementary Methods S1.2-3 for detailed descriptions of covariates. Statistical analyses SRI and sleep duration were split into quintiles. Hazards of mortality were estimated for each of the top four SRI and sleep duration quintiles compared to their respective lowest quintiles, which were hypothesized to have the highest mortality risk. This approach allowed for unspecified non-linearity in sleep/mortality relationships. Hazard ratios for all-cause and cause-specific mortality were estimated using Cox proportional hazards models and competing-risks proportional sub-hazards models [29]. All Cox models used time since accelerometer recording as the timescale. Sleep variables were included as predictors of mortality risk in three separate Cox models, as follows: model 1: SRI, model 2: sleep duration, and model 3: SRI plus sleep duration. Each of these three models were implemented as both “minimally adjusted” and “fully adjusted”, for a total of six models. Minimally adjusted models included age, sex, and ethnicity; and fully adjusted models were additionally adjusted for physical activity, employment, income, deprivation, social activities, social visits, smoking status, urbanicity, shift work status, and use of medication for cholesterol or hypertension (see Supplementary Methods S1.5 for model syntax). All covariates were selected as potential confounders of sleep/mortality relationships. Additional covariates in the fully adjusted models were also possible mediators of sleep/mortality relationships. Akaike Information Criteria was used to compare models 1 and 2 (equivalent SRI-only and sleep duration-only models). Likelihood ratio test was used to compare models 1 and 3 (nested hierarchical models). See Supplementary Results S2.2 for further detail on model comparisons. Fully adjusted models 1–3 were subject to further adjustments for sleep timing, and baseline physical and mental health (see Supplementary Results S2.4-6). Each of the following variables were individually added to fully adjusted models 1–3: mid-sleep timing, cancer diagnosis, diabetes diagnosis, vascular conditions, BMI, cholesterol ratio, depressed mood, unenthusiasm/disinterest, tenseness/restlessness, tiredness/lethargy, and visitation to a GP or psychiatrist for mental health concerns. The association between SRI and sleep duration was tested using linear regression, by inclusion of sleep duration as a linear, quadratic, and cubic predictor of SRI in three individual models. Model fit was compared between each of the three models using ANOVA (F-test). Non-linear relationships were tested due to the hypothesis that more irregular sleep would be associated with both shorter and longer sleep, primarily due to sleep restriction and associated recovery sleep. Results Participant characteristics Our analyses included 60 997 participants with valid SRI scores. The SRI distribution was negatively skewed, had a median (IQR) of 81.0 (73.8–86.3), and ranged from 2.5 to 98.5, as described in our previous work [22]. Sleep duration had mean (±SD) of 6.77 ± 1.00 hours, and ranged from 0.48 to 13.49 hours. The mean (±SD) follow-up period between accelerometer recording and study endpoint (March 21, 2021) was 6.30 ± 0.83 years. All-cause mortality rate was 4.84 deaths per 1000 person-years, with 1859 all-cause deaths, 377 by cardiometabolic causes, and 1092 by cancer. Mean (±SD) time to mortality was 5.96 ± 1.32 years. Participants in the final sample of 60 997 were 62.8 ± 7.8 years of age, 55.0% female, 97.2% white ethnicity, 60.8% employed, 7.3% shift workers, and had median income range £31000–51 999, Townsend deprivation index score of −1.79 ± 2.77, ≥1 weekly social activities in 72.9%, social visits most commonly experienced weekly (36.2%), 83.6% from an urban postcode, 36.4% previous and 6.3% current smokers, and mean (±SD) physical activity of 28.4 ± 8.1 milli-g across weekly recordings. Descriptive statistics of each covariate across SRI and sleep duration quintiles are provided in Table 1. Number of deaths by all-causes, cardiometabolic, cancer, and other causes are provided in Tables 2 and 3 for each SRI and sleep duration quintile. Intraindividual variability in sleep onset and offset timing across SRI quintiles are provided in Supporting Information Supplementary Results S2.7. Table 1. Open in new tab Descriptive statistics for participants, grouped according to sleep regularity and sleep duration percentilesSleep regularity percentile 40%–60% 60%–80% 80%–100% Sleep duration percentile 40%–60% 60%–80% 80%–100% 0%–20% 20%–40% 0%–20% 20%–40% Age (years) M ± SD 63.14 ± 7.8 62.71 ± 7.83 62.34 ± 7.81 62.52 ± 7.72 63.17 ± 7.75 63.19 ± 7.84 62.24 ± 7.89 62.24 ± 7.78 62.79 ± 7.74 63.43 ± 7.61 Range 43.89–78.72 43.59–79.00 43.69–78.84 43.50–78.20 44.10–78.88 43.92–79.00 43.50–78.88 43.69–78.84 43.90–78.61 44.10–78.20 Sex (% male, N) 49.58 (6048) 46.23 (5640) 43.67 (5318) 42.68 (5203) 42.24 (5164) 57.51 (7015) 47.65 (5813) 42.75 (5215) 39.15 (4776) 37.33 (4554) Ethnicity (% white, N) 95.39 (11 573) 96.55 (11 734) 97.27 (11 819) 97.92 (11 907) 98.76 (12 033) 94.58 (11 474) 96.89 (11 770) 97.55 (11 874) 98.07 (11 922) 98.81 (12 026) Physical activity (milli-g) M ± SD 25.93 ± 7.97 27.68 ± 7.85 28.70 ± 7.80 29.42 ± 7.98 30.11 ± 8.08 28.65 ± 8.46 29.33 ± 8.23 28.93 ± 8.01 28.20 ± 7.75 26.74 ± 7.63 Range 4.83–67.81 6.48–69.41 6.46–69.21 7.91–68.61 8.05–68.64 5.69–68.64 7.34–69.41 4.83–67.23 6.46–68.64 5.52–69.21 Employment status (% employed, N) 58.33 (7052) 62.26 (7546) 62.98 (7625) 62.07 (7522) 58.45 (7108) 60.81 (7360) 63.8 (7735) 63.73 (7726) 59.93 (7266) 55.82 (6766) Household income bracket (M ± SD, range 1–5) 2.65 ± 1.17 2.82 ± 1.16 2.91 ± 1.16 2.97 ± 1.14 2.94 ± 1.14 2.78 ± 1.17 2.92 ± 1.16 2.93 ± 1.15 2.87 ± 1.15 2.79 ± 1.14 Townsend deprivation index M ± SD −1.33 ± 3.02 −1.68 ± 2.82 −1.84 ± 2.74 −1.96 ± 2.64 −2.14 ± 2.54 −1.4 ± 3.00 −1.69 ± 2.81 −1.87 ± 2.72 −1.94 ± 2.66 −2.06 ± 2.60 Range −6.26–9.89 −6.26–8.94 −6.26–9.16 −6.26–10.46 −6.26–10.1 −6.26–10.46 −6.26–9.89 −6.26–9.89 −6.26–9.41 −6.26–10.1 Social visits (M ± SD, range 1–7) 5.15 ± 1.17 5.19 ± 1.12 5.18 ± 1.09 5.20 ± 1.08 5.14 ± 1.08 5.13 ± 1.16 5.16 ± 1.12 5.16 ± 1.10 5.20 ± 1.08 5.22 ± 1.08 Social activities (% >1 weekly, N) 70.29 (8553) 72.18 (8785) 73.68 (8965) 73.65 (8965) 74.56 (9105) 72.11 (8779) 74.21 (9040) 73.8 (8993) 73.05 (8892) 71.19 (8669) Smoking % previous, N 39.10 (4751) 36.92 (4491) 36.29 (4412) 35.90 (4367) 33.84 (4127) 39.01 (4748) 36.46 (4433) 35.56 (4329) 35.71 (4343) 35.29 (4295) % current, N 9.46 (1149) 6.90 (839) 5.95 (723) 4.77 (580) 4.22 (514) 8.59 (1045) 6.59 (801) 6.15 (749) 5.16 (628) 4.78 (582) Urbanicity (% >10 000 population, N) 86.14 (9444) 83.88 (9317) 83.38 (9308) 82.85 (9288) 81.78 (9222) 85.23 (9409) 83.79 (9339) 83.51 (9337) 83.37 (9357) 82.09 (9137) Shift work status (% shift workers, N) 10.37 (1252) 8.46 (1024) 7.00 (847) 5.80 (702) 5.12 (622) 9.55 (1154) 7.51 (910) 6.82 (826) 6.71 (813) 6.14 (744) SRI M ± SD 62.03 ± 9.54 75.45 ± 1.97 80.95 ± 1.32 85.22 ± 1.2 90.20 ± 2.11 72.56 ± 12.32 78.85 ± 9.99 80.58 ± 9.49 81.25 ± 9.17 80.65 ± 9.69 Median (IQR) 65.10 (58.4–68.9) 75.62 (73.8–77.2) 80.99 (79.8–82.1) 85.22 (84.2–86.3) 89.80 (88.5–91.6) 74.59 (65.9–81.5) 80.66 (74.2–85.8) 82.45 (76.2–87.1) 82.98 (77.3–87.5) 82.63 (76.2–87.3) Range 2.46–71.65 71.65–78.56 78.56–83.16 83.17–87.31 87.32–98.53 9.18–97.23 6.05–97.29 9.82–98.19 16.57–97.59 2.46–98.53 Sleep duration (h) M ± SD 6.25 ± 1.29 6.69 ± 1 6.86 ± 0.88 6.95 ± 0.81 7.07 ± 0.74 5.32 ± 0.72 6.33 ± 0.16 6.82 ± 0.13 7.29 ± 0.15 8.07 ± 0.47 Median (IQR) 6.32 (5.5–7.1) 6.70 (6.1–7.3) 6.86 (6.3–7.4) 6.95 (6.4–7.5) 7.07 (6.6–7.6) 5.54 (5.1–5.8) 6.34 (6.2–6.5) 6.82 (6.7–6.9) 7.28 (7.2–7.4) 7.94 (7.7–8.3) Range 0.65–13.03 0.51–13.49 0.48–11.64 0.59–12.74 3.98–10.8 0.48–6.02 6.02–6.59 6.59–7.05 7.05–7.56 7.56–13.49Sleep regularity percentile 40%–60% 60%–80% 80%–100% Sleep duration percentile 40%–60% 60%–80% 80%–100% 0%–20% 20%–40% 0%–20% 20%–40% Age (years) M ± SD 63.14 ± 7.8 62.71 ± 7.83 62.34 ± 7.81 62.52 ± 7.72 63.17 ± 7.75 63.19 ± 7.84 62.24 ± 7.89 62.24 ± 7.78 62.79 ± 7.74 63.43 ± 7.61 Range 43.89–78.72 43.59–79.00 43.69–78.84 43.50–78.20 44.10–78.88 43.92–79.00 43.50–78.88 43.69–78.84 43.90–78.61 44.10–78.20 Sex (% male, N) 49.58 (6048) 46.23 (5640) 43.67 (5318) 42.68 (5203) 42.24 (5164) 57.51 (7015) 47.65 (5813) 42.75 (5215) 39.15 (4776) 37.33 (4554) Ethnicity (% white, N) 95.39 (11 573) 96.55 (11 734) 97.27 (11 819) 97.92 (11 907) 98.76 (12 033) 94.58 (11 474) 96.89 (11 770) 97.55 (11 874) 98.07 (11 922) 98.81 (12 026) Physical activity (milli-g) M ± SD 25.93 ± 7.97 27.68 ± 7.85 28.70 ± 7.80 29.42 ± 7.98 30.11 ± 8.08 28.65 ± 8.46 29.33 ± 8.23 28.93 ± 8.01 28.20 ± 7.75 26.74 ± 7.63 Range 4.83–67.81 6.48–69.41 6.46–69.21 7.91–68.61 8.05–68.64 5.69–68.64 7.34–69.41 4.83–67.23 6.46–68.64 5.52–69.21 Employment status (% employed, N) 58.33 (7052) 62.26 (7546) 62.98 (7625) 62.07 (7522) 58.45 (7108) 60.81 (7360) 63.8 (7735) 63.73 (7726) 59.93 (7266) 55.82 (6766) Household income bracket (M ± SD, range 1–5) 2.65 ± 1.17 2.82 ± 1.16 2.91 ± 1.16 2.97 ± 1.14 2.94 ± 1.14 2.78 ± 1.17 2.92 ± 1.16 2.93 ± 1.15 2.87 ± 1.15 2.79 ± 1.14 Townsend deprivation index M ± SD −1.33 ± 3.02 −1.68 ± 2.82 −1.84 ± 2.74 −1.96 ± 2.64 −2.14 ± 2.54 −1.4 ± 3.00 −1.69 ± 2.81 −1.87 ± 2.72 −1.94 ± 2.66 −2.06 ± 2.60 Range −6.26–9.89 −6.26–8.94 −6.26–9.16 −6.26–10.46 −6.26–10.1 −6.26–10.46 −6.26–9.89 −6.26–9.89 −6.26–9.41 −6.26–10.1 Social visits (M ± SD, range 1–7) 5.15 ± 1.17 5.19 ± 1.12 5.18 ± 1.09 5.20 ± 1.08 5.14 ± 1.08 5.13 ± 1.16 5.16 ± 1.12 5.16 ± 1.10 5.20 ± 1.08 5.22 ± 1.08 Social activities (% >1 weekly, N) 70.29 (8553) 72.18 (8785) 73.68 (8965) 73.65 (8965) 74.56 (9105) 72.11 (8779) 74.21 (9040) 73.8 (8993) 73.05 (8892) 71.19 (8669) Smoking % previous, N 39.10 (4751) 36.92 (4491) 36.29 (4412) 35.90 (4367) 33.84 (4127) 39.01 (4748) 36.46 (4433) 35.56 (4329) 35.71 (4343) 35.29 (4295) % current, N 9.46 (1149) 6.90 (839) 5.95 (723) 4.77 (580) 4.22 (514) 8.59 (1045) 6.59 (801) 6.15 (749) 5.16 (628) 4.78 (582) Urbanicity (% >10 000 population, N) 86.14 (9444) 83.88 (9317) 83.38 (9308) 82.85 (9288) 81.78 (9222) 85.23 (9409) 83.79 (9339) 83.51 (9337) 83.37 (9357) 82.09 (9137) Shift work status (% shift workers, N) 10.37 (1252) 8.46 (1024) 7.00 (847) 5.80 (702) 5.12 (622) 9.55 (1154) 7.51 (910) 6.82 (826) 6.71 (813) 6.14 (744) SRI M ± SD 62.03 ± 9.54 75.45 ± 1.97 80.95 ± 1.32 85.22 ± 1.2 90.20 ± 2.11 72.56 ± 12.32 78.85 ± 9.99 80.58 ± 9.49 81.25 ± 9.17 80.65 ± 9.69 Median (IQR) 65.10 (58.4–68.9) 75.62 (73.8–77.2) 80.99 (79.8–82.1) 85.22 (84.2–86.3) 89.80 (88.5–91.6) 74.59 (65.9–81.5) 80.66 (74.2–85.8) 82.45 (76.2–87.1) 82.98 (77.3–87.5) 82.63 (76.2–87.3) Range 2.46–71.65 71.65–78.56 78.56–83.16 83.17–87.31 87.32–98.53 9.18–97.23 6.05–97.29 9.82–98.19 16.57–97.59 2.46–98.53 Sleep duration (h) M ± SD 6.25 ± 1.29 6.69 ± 1 6.86 ± 0.88 6.95 ± 0.81 7.07 ± 0.74 5.32 ± 0.72 6.33 ± 0.16 6.82 ± 0.13 7.29 ± 0.15 8.07 ± 0.47 Median (IQR) 6.32 (5.5–7.1) 6.70 (6.1–7.3) 6.86 (6.3–7.4) 6.95 (6.4–7.5) 7.07 (6.6–7.6) 5.54 (5.1–5.8) 6.34 (6.2–6.5) 6.82 (6.7–6.9) 7.28 (7.2–7.4) 7.94 (7.7–8.3) Range 0.65–13.03 0.51–13.49 0.48–11.64 0.59–12.74 3.98–10.8 0.48–6.02 6.02–6.59 6.59–7.05 7.05–7.56 7.56–13.49 Table 1. Open in new tab Descriptive statistics for participants, grouped according to sleep regularity and sleep duration percentilesSleep regularity percentile 40%–60% 60%–80% 80%–100% Sleep duration percentile 40%–60% 60%–80% 80%–100% 0%–20% 20%–40% 0%–20% 20%–40% Age (years) M ± SD 63.14 ± 7.8 62.71 ± 7.83 62.34 ± 7.81 62.52 ± 7.72 63.17 ± 7.75 63.19 ± 7.84 62.24 ± 7.89 62.24 ± 7.78 62.79 ± 7.74 63.43 ± 7.61 Range 43.89–78.72 43.59–79.00 43.69–78.84 43.50–78.20 44.10–78.88 43.92–79.00 43.50–78.88 43.69–78.84 43.90–78.61 44.10–78.20 Sex (% male, N) 49.58 (6048) 46.23 (5640) 43.67 (5318) 42.68 (5203) 42.24 (5164) 57.51 (7015) 47.65 (5813) 42.75 (5215) 39.15 (4776) 37.33 (4554) Ethnicity (% white, N) 95.39 (11 573) 96.55 (11 734) 97.27 (11 819) 97.92 (11 907) 98.76 (12 033) 94.58 (11 474) 96.89 (11 770) 97.55 (11 874) 98.07 (11 922) 98.81 (12 026) Physical activity (milli-g) M ± SD 25.93 ± 7.97 27.68 ± 7.85 28.70 ± 7.80 29.42 ± 7.98 30.11 ± 8.08 28.65 ± 8.46 29.33 ± 8.23 28.93 ± 8.01 28.20 ± 7.75 26.74 ± 7.63 Range 4.83–67.81 6.48–69.41 6.46–69.21 7.91–68.61 8.05–68.64 5.69–68.64 7.34–69.41 4.83–67.23 6.46–68.64 5.52–69.21 Employment status (% employed, N) 58.33 (7052) 62.26 (7546) 62.98 (7625) 62.07 (7522) 58.45 (7108) 60.81 (7360) 63.8 (7735) 63.73 (7726) 59.93 (7266) 55.82 (6766) Household income bracket (M ± SD, range 1–5) 2.65 ± 1.17 2.82 ± 1.16 2.91 ± 1.16 2.97 ± 1.14 2.94 ± 1.14 2.78 ± 1.17 2.92 ± 1.16 2.93 ± 1.15 2.87 ± 1.15 2.79 ± 1.14 Townsend deprivation index M ± SD −1.33 ± 3.02 −1.68 ± 2.82 −1.84 ± 2.74 −1.96 ± 2.64 −2.14 ± 2.54 −1.4 ± 3.00 −1.69 ± 2.81 −1.87 ± 2.72 −1.94 ± 2.66 −2.06 ± 2.60 Range −6.26–9.89 −6.26–8.94 −6.26–9.16 −6.26–10.46 −6.26–10.1 −6.26–10.46 −6.26–9.89 −6.26–9.89 −6.26–9.41 −6.26–10.1 Social visits (M ± SD, range 1–7) 5.15 ± 1.17 5.19 ± 1.12 5.18 ± 1.09 5.20 ± 1.08 5.14 ± 1.08 5.13 ± 1.16 5.16 ± 1.12 5.16 ± 1.10 5.20 ± 1.08 5.22 ± 1.08 Social activities (% >1 weekly, N) 70.29 (8553) 72.18 (8785) 73.68 (8965) 73.65 (8965) 74.56 (9105) 72.11 (8779) 74.21 (9040) 73.8 (8993) 73.05 (8892) 71.19 (8669) Smoking % previous, N 39.10 (4751) 36.92 (4491) 36.29 (4412) 35.90 (4367) 33.84 (4127) 39.01 (4748) 36.46 (4433) 35.56 (4329) 35.71 (4343) 35.29 (4295) % current, N 9.46 (1149) 6.90 (839) 5.95 (723) 4.77 (580) 4.22 (514) 8.59 (1045) 6.59 (801) 6.15 (749) 5.16 (628) 4.78 (582) Urbanicity (% >10 000 population, N) 86.14 (9444) 83.88 (9317) 83.38 (9308) 82.85 (9288) 81.78 (9222) 85.23 (9409) 83.79 (9339) 83.51 (9337) 83.37 (9357) 82.09 (9137) Shift work status (% shift workers, N) 10.37 (1252) 8.46 (1024) 7.00 (847) 5.80 (702) 5.12 (622) 9.55 (1154) 7.51 (910) 6.82 (826) 6.71 (813) 6.14 (744) SRI M ± SD 62.03 ± 9.54 75.45 ± 1.97 80.95 ± 1.32 85.22 ± 1.2 90.20 ± 2.11 72.56 ± 12.32 78.85 ± 9.99 80.58 ± 9.49 81.25 ± 9.17 80.65 ± 9.69 Median (IQR) 65.10 (58.4–68.9) 75.62 (73.8–77.2) 80.99 (79.8–82.1) 85.22 (84.2–86.3) 89.80 (88.5–91.6) 74.59 (65.9–81.5) 80.66 (74.2–85.8) 82.45 (76.2–87.1) 82.98 (77.3–87.5) 82.63 (76.2–87.3) Range 2.46–71.65 71.65–78.56 78.56–83.16 83.17–87.31 87.32–98.53 9.18–97.23 6.05–97.29 9.82–98.19 16.57–97.59 2.46–98.53 Sleep duration (h) M ± SD 6.25 ± 1.29 6.69 ± 1 6.86 ± 0.88 6.95 ± 0.81 7.07 ± 0.74 5.32 ± 0.72 6.33 ± 0.16 6.82 ± 0.13 7.29 ± 0.15 8.07 ± 0.47 Median (IQR) 6.32 (5.5–7.1) 6.70 (6.1–7.3) 6.86 (6.3–7.4) 6.95 (6.4–7.5) 7.07 (6.6–7.6) 5.54 (5.1–5.8) 6.34 (6.2–6.5) 6.82 (6.7–6.9) 7.28 (7.2–7.4) 7.94 (7.7–8.3) Range 0.65–13.03 0.51–13.49 0.48–11.64 0.59–12.74 3.98–10.8 0.48–6.02 6.02–6.59 6.59–7.05 7.05–7.56 7.56–13.49Sleep regularity percentile 40%–60% 60%–80% 80%–100% Sleep duration percentile 40%–60% 60%–80% 80%–100% 0%–20% 20%–40% 0%–20% 20%–40% Age (years) M ± SD 63.14 ± 7.8 62.71 ± 7.83 62.34 ± 7.81 62.52 ± 7.72 63.17 ± 7.75 63.19 ± 7.84 62.24 ± 7.89 62.24 ± 7.78 62.79 ± 7.74 63.43 ± 7.61 Range 43.89–78.72 43.59–79.00 43.69–78.84 43.50–78.20 44.10–78.88 43.92–79.00 43.50–78.88 43.69–78.84 43.90–78.61 44.10–78.20 Sex (% male, N) 49.58 (6048) 46.23 (5640) 43.67 (5318) 42.68 (5203) 42.24 (5164) 57.51 (7015) 47.65 (5813) 42.75 (5215) 39.15 (4776) 37.33 (4554) Ethnicity (% white, N) 95.39 (11 573) 96.55 (11 734) 97.27 (11 819) 97.92 (11 907) 98.76 (12 033) 94.58 (11 474) 96.89 (11 770) 97.55 (11 874) 98.07 (11 922) 98.81 (12 026) Physical activity (milli-g) M ± SD 25.93 ± 7.97 27.68 ± 7.85 28.70 ± 7.80 29.42 ± 7.98 30.11 ± 8.08 28.65 ± 8.46 29.33 ± 8.23 28.93 ± 8.01 28.20 ± 7.75 26.74 ± 7.63 Range 4.83–67.81 6.48–69.41 6.46–69.21 7.91–68.61 8.05–68.64 5.69–68.64 7.34–69.41 4.83–67.23 6.46–68.64 5.52–69.21 Employment status (% employed, N) 58.33 (7052) 62.26 (7546) 62.98 (7625) 62.07 (7522) 58.45 (7108) 60.81 (7360) 63.8 (7735) 63.73 (7726) 59.93 (7266) 55.82 (6766) Household income bracket (M ± SD, range 1–5) 2.65 ± 1.17 2.82 ± 1.16 2.91 ± 1.16 2.97 ± 1.14 2.94 ± 1.14 2.78 ± 1.17 2.92 ± 1.16 2.93 ± 1.15 2.87 ± 1.15 2.79 ± 1.14 Townsend deprivation index M ± SD −1.33 ± 3.02 −1.68 ± 2.82 −1.84 ± 2.74 −1.96 ± 2.64 −2.14 ± 2.54 −1.4 ± 3.00 −1.69 ± 2.81 −1.87 ± 2.72 −1.94 ± 2.66 −2.06 ± 2.60 Range −6.26–9.89 −6.26–8.94 −6.26–9.16 −6.26–10.46 −6.26–10.1 −6.26–10.46 −6.26–9.89 −6.26–9.89 −6.26–9.41 −6.26–10.1 Social visits (M ± SD, range 1–7) 5.15 ± 1.17 5.19 ± 1.12 5.18 ± 1.09 5.20 ± 1.08 5.14 ± 1.08 5.13 ± 1.16 5.16 ± 1.12 5.16 ± 1.10 5.20 ± 1.08 5.22 ± 1.08 Social activities (% >1 weekly, N) 70.29 (8553) 72.18 (8785) 73.68 (8965) 73.65 (8965) 74.56 (9105) 72.11 (8779) 74.21 (9040) 73.8 (8993) 73.05 (8892) 71.19 (8669) Smoking % previous, N 39.10 (4751) 36.92 (4491) 36.29 (4412) 35.90 (4367) 33.84 (4127) 39.01 (4748) 36.46 (4433) 35.56 (4329) 35.71 (4343) 35.29 (4295) % current, N 9.46 (1149) 6.90 (839) 5.95 (723) 4.77 (580) 4.22 (514) 8.59 (1045) 6.59 (801) 6.15 (749) 5.16 (628) 4.78 (582) Urbanicity (% >10 000 population, N) 86.14 (9444) 83.88 (9317) 83.38 (9308) 82.85 (9288) 81.78 (9222) 85.23 (9409) 83.79 (9339) 83.51 (9337) 83.37 (9357) 82.09 (9137) Shift work status (% shift workers, N) 10.37 (1252) 8.46 (1024) 7.00 (847) 5.80 (702) 5.12 (622) 9.55 (1154) 7.51 (910) 6.82 (826) 6.71 (813) 6.14 (744) SRI M ± SD 62.03 ± 9.54 75.45 ± 1.97 80.95 ± 1.32 85.22 ± 1.2 90.20 ± 2.11 72.56 ± 12.32 78.85 ± 9.99 80.58 ± 9.49 81.25 ± 9.17 80.65 ± 9.69 Median (IQR) 65.10 (58.4–68.9) 75.62 (73.8–77.2) 80.99 (79.8–82.1) 85.22 (84.2–86.3) 89.80 (88.5–91.6) 74.59 (65.9–81.5) 80.66 (74.2–85.8) 82.45 (76.2–87.1) 82.98 (77.3–87.5) 82.63 (76.2–87.3) Range 2.46–71.65 71.65–78.56 78.56–83.16 83.17–87.31 87.32–98.53 9.18–97.23 6.05–97.29 9.82–98.19 16.57–97.59 2.46–98.53 Sleep duration (h) M ± SD 6.25 ± 1.29 6.69 ± 1 6.86 ± 0.88 6.95 ± 0.81 7.07 ± 0.74 5.32 ± 0.72 6.33 ± 0.16 6.82 ± 0.13 7.29 ± 0.15 8.07 ± 0.47 Median (IQR) 6.32 (5.5–7.1) 6.70 (6.1–7.3) 6.86 (6.3–7.4) 6.95 (6.4–7.5) 7.07 (6.6–7.6) 5.54 (5.1–5.8) 6.34 (6.2–6.5) 6.82 (6.7–6.9) 7.28 (7.2–7.4) 7.94 (7.7–8.3) Range 0.65–13.03 0.51–13.49 0.48–11.64 0.59–12.74 3.98–10.8 0.48–6.02 6.02–6.59 6.59–7.05 7.05–7.56 7.56–13.49 Table 2. Open in new tab Risk of all-cause, cardiometabolic, cancer and other-cause mortality, according to sleep regularity quintiles (model 1) Model Percentile All-cause Cardiometabolic Cancer Other-cause N (%) HR [95% CI] N (%) HR [95% CI] N (%) HR [95% CI] N (%) HR [95% CI] Minimal SRI (0%–20%) 565 (0.93) — 130 (0.21) — 294 (0.48) — 140 (0.23) — N = 60 780 SRI (20%–40%) 389 (0.64) 0.72 [0.63 to 0.82]*** 79 (0.13) 0.65 [0.49 to 0.86]** 230 (0.38) 0.81 [0.68 to 0.97]* 80 (0.13) 0.60 [0.46 to 0.79]***SRI (40%–60%) 322 (0.53) 0.62 [0.54 to 0.71]*** 61 (0.1) 0.54 [0.40 to 0.73]*** 204 (0.34) 0.75 [0.63 to 0.89]** 56 (0.09) 0.44 [0.33 to 0.61]***SRI (60%–80%) 284 (0.47) 0.54 [0.47 to 0.63]*** 49 (0.08) 0.43 [0.31 to 0.60]*** 182 (0.3) 0.66 [0.55 to 0.80]*** 50 (0.08) 0.39 [0.29 to 0.55]***SRI (80%–100%) 288 (0.47) 0.52 [0.45 to 0.60]*** 55 (0.09) 0.45 [0.33 to 0.61]*** 176 (0.29) 0.61 [0.50 to 0.73]*** 56 (0.09) 0.41 [0.30 to 0.56]*** Full SRI (0%–20%) 434 (0.93) — 96 (0.21) — 231 (0.49) — 106 (0.23) — N = 46 721 SRI (20%–40%) 296 (0.63) 0.80 [0.69 to 0.93]** 61 (0.13) 0.78 [0.56 to 1.08] 170 (0.36) 0.84 [0.69 to 1.02] 65 (0.14) 0.80 [0.58 to 1.09]SRI (40%–60%) 249 (0.53) 0.75 [0.64 to 0.88]*** 50 (0.11) 0.72 [0.51 to 1.01] 155 (0.33) 0.83 [0.67 to 1.02] 44 (0.09) 0.63 [0.44 to 0.90]*SRI (60%–80%) 226 (0.48) 0.72 [0.61 to 0.84]*** 45 (0.1) 0.69 [0.48 to 1.00]* 139 (0.3) 0.77 [0.62 to 0.95]* 39 (0.08) 0.60 [0.41 to 0.88]**SRI (80%–100%) 221 (0.47) 0.70 [0.59 to 0.83]*** 41 (0.09) 0.62 [0.42 to 0.91]* 137 (0.29) 0.76 [0.61 to 0.94]* 42 (0.09) 0.66 [0.46 to 0.94]* Model Percentile All-cause Cardiometabolic Cancer Other-cause N (%) HR [95% CI] N (%) HR [95% CI] N (%) HR [95% CI] N (%) HR [95% CI] Minimal SRI (0%–20%) 565 (0.93) — 130 (0.21) — 294 (0.48) — 140 (0.23) — N = 60 780 SRI (20%–40%) 389 (0.64) 0.72 [0.63 to 0.82]*** 79 (0.13) 0.65 [0.49 to 0.86]** 230 (0.38) 0.81 [0.68 to 0.97]* 80 (0.13) 0.60 [0.46 to 0.79]***SRI (40%–60%) 322 (0.53) 0.62 [0.54 to 0.71]*** 61 (0.1) 0.54 [0.40 to 0.73]*** 204 (0.34) 0.75 [0.63 to 0.89]** 56 (0.09) 0.44 [0.33 to 0.61]***SRI (60%–80%) 284 (0.47) 0.54 [0.47 to 0.63]*** 49 (0.08) 0.43 [0.31 to 0.60]*** 182 (0.3) 0.66 [0.55 to 0.80]*** 50 (0.08) 0.39 [0.29 to 0.55]***SRI (80%–100%) 288 (0.47) 0.52 [0.45 to 0.60]*** 55 (0.09) 0.45 [0.33 to 0.61]*** 176 (0.29) 0.61 [0.50 to 0.73]*** 56 (0.09) 0.41 [0.30 to 0.56]*** Full SRI (0%–20%) 434 (0.93) — 96 (0.21) — 231 (0.49) — 106 (0.23) — N = 46 721 SRI (20%–40%) 296 (0.63) 0.80 [0.69 to 0.93]** 61 (0.13) 0.78 [0.56 to 1.08] 170 (0.36) 0.84 [0.69 to 1.02] 65 (0.14) 0.80 [0.58 to 1.09]SRI (40%–60%) 249 (0.53) 0.75 [0.64 to 0.88]*** 50 (0.11) 0.72 [0.51 to 1.01] 155 (0.33) 0.83 [0.67 to 1.02] 44 (0.09) 0.63 [0.44 to 0.90]*SRI (60%–80%) 226 (0.48) 0.72 [0.61 to 0.84]*** 45 (0.1) 0.69 [0.48 to 1.00]* 139 (0.3) 0.77 [0.62 to 0.95]* 39 (0.08) 0.60 [0.41 to 0.88]**SRI (80%–100%) 221 (0.47) 0.70 [0.59 to 0.83]*** 41 (0.09) 0.62 [0.42 to 0.91]* 137 (0.29) 0.76 [0.61 to 0.94]* 42 (0.09) 0.66 [0.46 to 0.94]* * p10 million hours of actigraphy data in 60 977 UK Biobank participants, we found that more regular sleep was a significant predictor of lower risk of all-cause mortality. Participants in the top four quintiles (SRI 71.6–98.5) had a 20%–48% lower risk of all-cause mortality compared to those with SRI scores in the bottom quintile (SRI7.56 hours, whereas previous studies showing an association of mortality with long sleep have tended to use more extreme cutoffs (e.g. >9 hours or the highest possible response self-report category). We therefore would not necessarily expect to see a heightened risk of mortality in this upper quintile. Our study advances previous literature by directly comparing sleep duration and sleep regularity as predictors of mortality risk. Our findings demonstrate that sleep regularity is generally a stronger predictor of mortality risk than sleep duration. These findings align with other studies showing sleep regularity to be a stronger predictor than sleep duration of other health outcomes [12, 13]. We propose that these findings emerge because sleep regularity is a more direct proxy for circadian disruption, which is known from experimental studies to have very broad adverse effects on physiology [38]. In animal studies, circadian disruption induced by light patterns that contribute to irregular sleep–wake behavior is known to cause premature mortality [39] and purposeful disruption of the circadian clock with light causes profound cardiovascular disease [40]. Prospective studies of people with irregular sleep have demonstrated links with cardiometabolic health [12, 14–17]. We note that while sleep regularity is a more direct measure of circadian disruption, sleep duration may in part also be capturing aspects of circadian disruption. Short and long sleep duration may influence the timing of light exposure, nutritional intake, and physical activity, which impact both central and peripheral circadian rhythms. Higher risk of mortality by cardiometabolic causes was associated with both irregular sleep and short sleep duration in our study. These findings are consistent with experimental and epidemiological evidence linking sleep regularity and duration with cardiometabolic health [4, 8, 12, 21, 41–46]. Experimental evidence in humans indicates that cardiometabolic risk factors, including arterial blood pressure, inflammatory markers, and insulin sensitivity are altered with reduced sleep duration [41], and under experimental conditions that disrupt circadian rhythms [42]. Epidemiological studies show that the risks of cardiovascular events and mortality are higher in short sleepers [4, 8, 43, 44] across both objective and subjective measures of sleep duration. For sleep regularity, epidemiological studies using actigraphy have linked more variable sleep timing and duration with incident fatal and non-fatal cardiovascular disease [45] and metabolic abnormalities [46], and SRI-measured sleep regularity with cardiometabolic risk factors [12]. Subjective sleep regularity has also been linked with cardiovascular mortality [21]. While sleep duration and regularity independently predicted cardiometabolic mortality risk in our study, we also found that regularity was not a significant predictor of cardiometabolic mortality in models that also included duration, after adjustment for physical activity, smoking status, shift work status, and sociodemographic and lifestyle factors. This result suggests that different aspects of sleep behaviors (duration vs. regularity) may differentially contribute to cardiometabolic mortality risk, possibly due to different effects of homeostatic and circadian disruption on cardiometabolic factors [41, 42]. We found that irregular sleep predicted higher risk of mortality by cancer, whereas short sleep duration did not. Similar to all-cause mortality, this relationship may be ultimately driven by circadian disruption. Several lines of evidence have linked circadian disruption with cancer, including: (1) studies implicating gene expression and metabolic rhythms in cancer initiation and progression [47]; (2) animal studies showing irregular sleep–wake rhythms and circadian disruption cause increased tumor progression and metastasis [48–51] and cancer-induced inflammation [52]; and (3) epidemiological evidence implicating light exposure at night in higher risk of incident breast [53–55], thyroid [56], and pancreatic cancer [57], and shift work exposure in breast [58], prostate [59], lung, and skin cancer [60]. In contrast, epidemiological evidence indicates that sleep duration is only linked with higher cancer mortality risk in a small substrata of long sleepers (>9 hours), but not in short sleepers [1, 43, 61], which is consistent with our findings for short sleepers. Notably, the relationship we found between irregular sleep and higher cancer mortality risk remained robust in individuals without a preexisting cancer diagnosis, further supporting the hypothesis that irregular sleep and circadian disruption influence cancer initiation, progression, and eventual mortality. There are several limitations in this study. First, the single week of data collected for each individual provides only a snapshot of their sleep–wake patterns, and future work should collect sleep–wake data over a longer timeframe and include multiple weekend-weekday transitions. It is nevertheless interesting that even a snapshot of sleep behaviors is predictive of mortality for a follow-up period of several years. Second, accelerometer recordings did not occur simultaneously with collection of baseline covariates, and some of these covariates may not remain temporally stable within each individual. Third, our findings are within an older age group of mostly homogeneous ethnicity, and should be replicated across other cohorts, including cross-culturally. Fourth, our fully adjusted models contain variables that potentially have both confounding and mediating effects (e.g. smoking status). The true strength of the sleep regularity-mortality relationship therefore likely lies between our minimally adjusted models, which contain only non-mediating covariates, and our fully adjusted models. Finally, we acknowledge the correlational nature of our findings. Sleep regularity may be both a cause and marker of premature mortality risk. One key determinant of a high SRI score is regular timing of sleep onset and offset. In our study, people with the top 20% of SRI scores went to sleep and awoke within approximately 1-hour windows on the majority of their study days. By contrast, people with the bottom 20% of SRI scores went to sleep and awoke within approximately 3-hour windows. Aiming to fall asleep and wake up within 1-hour windows each day may therefore be a feasible strategy for improving SRI, especially for individuals who keep only one main sleep episode per day. We note that awakenings during the night and fragmented sleep patterns also contribute to low SRI scores, and should therefore also be taken into consideration. In summary, our findings challenge the long-standing assumption that sleep duration is the most important index of sleep for human health. The preponderance of research on sleep and health has focused on sleep duration as the primary predictor. The resultant frameworks for public health have consequently tended to focus on sleep duration as the key target. Our results confirm an important role for sleep duration in predicting mortality, but reveal that sleep regularity is an even stronger predictor. Fortunately, sleep regularity may also be an easier dimension to target through interventions. Due to both psychosocial and biological reasons, extending sleep duration can be challenging to achieve in practice. Interventions to extend sleep that do not directly schedule when participants can sleep have only small effects on sleep duration [62]. Asking people to maintain more similar sleep times between days to improve sleep regularity, rather than devoting a greater proportion of the day to sleep, may ultimately be a more feasible strategy. Acknowledgments The authors acknowledge Alex Russell and Phillip Chan for their help in procuring and implementing high-performance computing resources to derive Sleep Regularity Index scores. Disclosure Statement Financial Disclosure: AJKP and SWC have received research funding from Delos and Versalux, and they are co-founders and co-directors of Circadian Health Innovations PTY LTD. SWC has also received research funding from Beacon Lighting and has consulted for Dyson.. Nonfinancial Disclosure: none. Data Availability The data underlying this article are available in the UK Biobank repository and can be accessed upon application (https://www.ukbiobank.ac.uk/). References 1. Gallicchio L , Kalesan B. Sleep duration and mortality: a systematic review and meta‐analysis . J Sleep Res. 2009 ; 18 ( 2 ): 148 – 158 . doi: 10.1111/j.1365-2869.2008.00732.x Google Scholar Crossref Search ADS PubMed 2. Cappuccio FP , D’Elia L , Strazzullo P , Miller MA. Sleep duration and all-cause mortality: a systematic review and meta-analysis of prospective studies . Sleep. 2010 ; 33 ( 5 ): 585 – 592 . doi: 10.1093/sleep/33.5.585 Google Scholar Crossref Search ADS PubMed 3. Liu T-Z , Xu C , Rota M , et al. . Sleep duration and risk of all-cause mortality: a flexible, non-linear, meta-regression of 40 prospective cohort studies . Sleep Med Rev. 2017 ; 32 : 28 – 36 . doi: 10.1016/j.smrv.2016.02.005 Google Scholar Crossref Search ADS PubMed 4. Yin J , Jin X , Shan Z , et al. . Relationship of sleep duration with all‐cause mortality and cardiovascular events: a systematic review and dose‐response meta‐analysis of prospective cohort studies . J Am Heart Assoc . 2017 ; 6 ( 9 ): e005947 . doi: 10.1161/JAHA.117.005947 Google Scholar Crossref Search ADS PubMed 5. Jike M , Itani O , Watanabe N , Buysse DJ , Kaneita Y. Long sleep duration and health outcomes: a systematic review, meta-analysis and meta-regression . Sleep Med Rev. 2018 ; 39 : 25 – 36 . doi: 10.1016/j.smrv.2017.06.011 Google Scholar Crossref Search ADS PubMed 6. Wang Y-H , Wang J , Chen S-H , et al. . Association of longitudinal patterns of habitual sleep duration with risk of cardiovascular events and all-cause mortality . JAMA Netw Open . 2020 ; 3 ( 5 ): e205246 – e205246 . doi: 10.1001/jamanetworkopen.2020.5246 Google Scholar Crossref Search ADS PubMed 7. Han H , Wang Y , Li T , et al. . Sleep duration and risks of incident cardiovascular disease and mortality among people with type 2 diabetes . Diabetes Care. 2023 ; 46 ( 1 ): 101 – 110 . doi: 10.2337/dc22-1127 Google Scholar Crossref Search ADS PubMed 8. Zhao B , Meng Y , Jin X , et al. . Association of objective and self‐reported sleep duration with all‐cause and cardiovascular disease mortality: a community‐based study . J Am Heart Assoc . 2023 ; 12 ( 6 ): e027832 . doi: 10.1161/JAHA.122.027832 Google Scholar Crossref Search ADS PubMed 9. Liang YY , Ai S , Xue H , et al. . Joint associations of device-measured sleep duration and efficiency with all-cause and cause-specific mortality: a prospective cohort study of 90 398 UK biobank participants . J Gerontol . 2023 ; 78 ( 9 ): 1717 – 1724 . doi: 10.1093/gerona/glad108 Google Scholar Crossref Search ADS 10. Chaput J-P , Dutil C , Featherstone R , et al. . Sleep duration and health in adults: an overview of systematic reviews . Appl Physiol Nutr Metab. 2020 ; 45 ( 10 ): S218 – S231 . doi: 10.1139/apnm-2020-0034 Google Scholar PubMed OpenURL Placeholder Text 11. Hirshkowitz M , Whiton K , Albert SM , et al. . National Sleep Foundation’s sleep time duration recommendations: methodology and results summary . Sleep Health . 2015 ; 1 ( 1 ): 40 – 43 . doi: 10.1016/j.sleh.2014.12.010 Google Scholar Crossref Search ADS PubMed 12. Lunsford-Avery JR , Engelhard MM , Navar AM , Kollins SH. Validation of the sleep regularity index in older adults and associations with cardiometabolic risk . Sci Rep. 2018 ; 8 ( 1 ): 14158 . doi: 10.1038/s41598-018-32402-5 Google Scholar Crossref Search ADS PubMed 13. Trivedi R , Man H , Madut A , et al. . Irregular sleep/wake patterns are associated with reduced quality of life in post-treatment cancer patients: a study across three cancer cohorts . Front Neurosci. 2021 ; 15 ( 700923 ): 1 – 10 . doi: 10.3389/fnins.2021.700923 Google Scholar OpenURL Placeholder Text 14. Zuraikat FM , Makarem N , Redline S , Aggarwal B , Jelic S , St-Onge M-P. Sleep regularity and cardiometabolic heath: is variability in sleep patterns a risk factor for excess adiposity and glycemic dysregulation ? Curr Diab Rep. 2020 ; 20 ( 8 ): 1 – 9 . Google Scholar Crossref Search ADS PubMed 15. Makarem N , Zuraikat FM , Aggarwal B , Jelic S , St-Onge M-P. Variability in sleep patterns: an emerging risk factor for hypertension . Curr Hypertens Rep. 2020 ; 22 ( 2 ): 1 – 10 . Google Scholar Crossref Search ADS PubMed 16. Fritz J , Phillips AJK , Hunt LC , et al. . Cross-sectional and prospective associations between sleep regularity and metabolic health in the Hispanic Community Health Study/Study of Latinos . Sleep. 2021 ; 44 ( 4 ). doi: 10.1093/sleep/zsaa218 Google Scholar OpenURL Placeholder Text 17. Wong PM , Barker D , Roane BM , Van Reen E , Carskadon MA. Sleep regularity and body mass index: findings from a prospective study of first-year college students . Sleep Adv . 2022 ; 3 ( 1 ): zpac004 . doi: 10.1093/sleepadvances/zpac004 Google Scholar Crossref Search ADS PubMed 18. Carskadon MA , Chappell KR , Barker DH , et al. . A pilot prospective study of sleep patterns and DNA methylation-characterized epigenetic aging in young adults . BMC Res Notes . 2019 ; 12 ( 1 ): 1 – 5 . Google Scholar Crossref Search ADS PubMed 19. Pye J , Phillips AJK , Cain SW , et al. . Irregular sleep-wake patterns in older adults with current or remitted depression . J Affect Disord. 2021 ; 281 : 431 – 437 . doi: 10.1016/j.jad.2020.12.034 Google Scholar Crossref Search ADS PubMed 20. Vetter C. Circadian disruption: what do we actually mean ? Eur J Neurosci. 2020 ; 51 ( 1 ): 531 – 550 . doi: 10.1111/ejn.14255 Google Scholar Crossref Search ADS PubMed 21. Omichi C , Koyama T , Kadotani H , et al. .; J-MICC Study Group . Irregular sleep and all-cause mortality: a large prospective cohort study . Sleep Health . 2022 ; 8 ( 6 ): 678 – 683 . doi: 10.1016/j.sleh.2022.08.010 Google Scholar Crossref Search ADS PubMed 22. Windred DP , Jones SE , Russell A , et al. . Objective assessment of sleep regularity in 60 000 UK Biobank participants using an open-source package . Sleep. 2021 ; 44 ( 12 ). doi: 10.1093/sleep/zsab254 Google Scholar OpenURL Placeholder Text 23. Sudlow C , Gallacher J , Allen N , et al. . UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age . PLoS Med. 2015 ; 12 ( 3 ): e1001779 . doi: 10.1371/journal.pmed.1001779 Google Scholar Crossref Search ADS PubMed 24. Phillips AJK , Clerx WM , O’Brien CS , et al. . Irregular sleep/wake patterns are associated with poorer academic performance and delayed circadian and sleep/wake timing . Sci Rep. 2017 ; 7 ( 1 ): 3216 . doi: 10.1038/s41598-017-03171-4 Google Scholar Crossref Search ADS PubMed 25. Migueles JH , Rowlands AV , Huber F , Sabia S , van Hees VT. GGIR: a research community–driven open source R package for generating physical activity and sleep outcomes from multi-day raw accelerometer data . J Measure Physic Behav . 2019 ; 2 ( 3 ): 188 – 196 . doi: 10.1123/jmpb.2018-0063 Google Scholar Crossref Search ADS 26. van Hees VT , Sabia S , Jones SE , et al. . Estimating sleep parameters using an accelerometer without sleep diary . Sci Rep. 2018 ; 8 ( 1 ): 1 – 11 . Google Scholar Crossref Search ADS PubMed 27. World Health O . International statistical classification of diseases and related health problems . 10th rev. ed. Geneva: Geneva : World Health Organization ; 1992 . Google Scholar OpenURL Placeholder Text 28. Doherty A , Jackson D , Hammerla N , et al. . Large scale population assessment of physical activity using wrist worn accelerometers: The UK Biobank Study . PLoS One. 2017 ; 12 ( 2 ): e0169649 . doi: 10.1371/journal.pone.0169649 Google Scholar Crossref Search ADS PubMed 29. Austin PC , Lee DS , Fine JP. Introduction to the analysis of survival data in the presence of competing risks . Circulation. 2016 ; 133 ( 6 ): 601 – 609 . doi: 10.1161/CIRCULATIONAHA.115.017719 Google Scholar Crossref Search ADS PubMed 30. Fischer D , Klerman EB , Phillips AJK. Measuring sleep regularity: theoretical properties and practical usage of existing metrics . Sleep. 2021 ; 44 : 1 – 16 . Google Scholar Crossref Search ADS 31. Katamreddy A , Uppal D , Ramani G , et al. . Day-to-day variation in sleep duration is associated with increased all-cause mortality . J Clin Sleep Med. 2022 ; 18 ( 3 ): 921 – 926 . doi: 10.5664/jcsm.9664 Google Scholar Crossref Search ADS PubMed 32. Zuurbier LA , Luik AI , Hofman A , Franco OH , Van Someren EJ , Tiemeier H. Fragmentation and stability of circadian activity rhythms predict mortality: the Rotterdam study . Am J Epidemiol. 2015 ; 181 ( 1 ): 54 – 63 . doi: 10.1093/aje/kwu245 Google Scholar Crossref Search ADS PubMed 33. Guida JL , Alfini AJ , Gallicchio L , Spira AP , Caporaso NE , Green PA. Association of objectively measured sleep with frailty and 5-year mortality in community-dwelling older adults . Sleep. 2021 ; 44 ( 7 ). doi: 10.1093/sleep/zsab003 Google Scholar OpenURL Placeholder Text 34. Leroux A , Xu S , Kundu P , et al. . Quantifying the predictive performance of objectively measured physical activity on mortality in the UK Biobank . J Gerontol A Biol Sci Med Sci. 2021 ; 76 ( 8 ): 1486 – 1494 . doi: 10.1093/gerona/glaa250 Google Scholar Crossref Search ADS PubMed 35. Feng H , Yang L , Ai S , et al. . Association between accelerometer-measured amplitude of rest–activity rhythm and future health risk: a prospective cohort study of the UK Biobank . Lancet Healthy Longev . 2023 ; 4 ( 5 ): e200 – e210 . doi: 10.1016/s2666-7568(23)00056-9 Google Scholar Crossref Search ADS PubMed 36. Zhong G , Wang Y , Tao T , Ying J , Zhao Y. Daytime napping and mortality from all causes, cardiovascular disease, and cancer: a meta-analysis of prospective cohort studies . Sleep Med. 2015 ; 16 ( 7 ): 811 – 819 . doi: 10.1016/j.sleep.2015.01.025 Google Scholar Crossref Search ADS PubMed 37. Wang C , Bangdiwala SI , Rangarajan S , et al. . Association of estimated sleep duration and naps with mortality and cardiovascular events: a study of 116 632 people from 21 countries . Eur Heart J. 2019 ; 40 ( 20 ): 1620 – 1629 . doi: 10.1093/eurheartj/ehy695 Google Scholar Crossref Search ADS PubMed 38. Baron KG , Reid KJ. Circadian misalignment and health . Int Rev Psychiatry. 2014 ; 26 ( 2 ): 139 – 154 . doi: 10.3109/09540261.2014.911149 Google Scholar Crossref Search ADS PubMed 39. Hurd MW , Ralph MR. The significance of circadian organization for longevity in the golden hamster . J Biol Rhythms. 1998 ; 13 ( 5 ): 430 – 436 . doi: 10.1177/074873098129000255 Google Scholar Crossref Search ADS PubMed 40. Martino TA , Oudit GY , Herzenberg AM , et al. . Circadian rhythm disorganization produces profound cardiovascular and renal disease in hamsters . Am J Physiol Regul Integr Comp Physiol. 2008 ; 294 ( 5 ): R1675 – R1683 . doi: 10.1152/ajpregu.00829.2007 Google Scholar Crossref Search ADS PubMed 41. Tobaldini E , Costantino G , Solbiati M , et al. . Sleep, sleep deprivation, autonomic nervous system and cardiovascular diseases . Neurosci Biobehav Rev. 2017 ; 74 : 321 – 329 . doi: 10.1016/j.neubiorev.2016.07.004 Google Scholar Crossref Search ADS PubMed 42. Chellappa SL , Vujovic N , Williams JS , Scheer FA. Impact of circadian disruption on cardiovascular function and disease . Trends Endocrinol Metab. 2019 ; 30 ( 10 ): 767 – 779 . doi: 10.1016/j.tem.2019.07.008 Google Scholar Crossref Search ADS PubMed 43. Tao F , Cao Z , Jiang Y , et al. . Associations of sleep duration and quality with incident cardiovascular disease, cancer, and mortality: a prospective cohort study of 407,500 UK biobank participants . Sleep Med. 2021 ; 81 : 401 – 409 . doi: 10.1016/j.sleep.2021.03.015 Google Scholar Crossref Search ADS PubMed 44. Kronholm E , Laatikainen T , Peltonen M , Sippola R , Partonen T. Self-reported sleep duration, all-cause mortality, cardiovascular mortality and morbidity in Finland . Sleep Med. 2011 ; 12 ( 3 ): 215 – 221 . doi: 10.1016/j.sleep.2010.07.021 Google Scholar Crossref Search ADS PubMed 45. Huang T , Mariani S , Redline S. Sleep irregularity and risk of cardiovascular events: the multi-ethnic study of atherosclerosis . J Am Coll Cardiol. 2020 ; 75 ( 9 ): 991 – 999 . doi: 10.1016/j.jacc.2019.12.054 Google Scholar Crossref Search ADS PubMed 46. Huang T , Redline S. Cross-sectional and prospective associations of actigraphy-assessed sleep regularity with metabolic abnormalities: the multi-ethnic study of atherosclerosis . Diabetes Care. 2019 ; 42 ( 8 ): 1422 – 1429 . doi: 10.2337/dc19-0596 Google Scholar Crossref Search ADS PubMed 47. Fekry B , Eckel-Mahan K. The circadian clock and cancer: links between circadian disruption and disease Pathology . J Biochem. 2022 ; 171 ( 5 ): 477 – 486 . doi: 10.1093/jb/mvac017 Google Scholar Crossref Search ADS PubMed 48. Filipski E , Delaunay F , King VM , et al. . Effects of chronic jet lag on tumor progression in mice . Cancer Res. 2004 ; 64 ( 21 ): 7879 – 7885 . doi: 10.1158/0008-5472.CAN-04-0674 Google Scholar Crossref Search ADS PubMed 49. Kettner NM , Voicu H , Finegold MJ , et al. . Circadian homeostasis of liver metabolism suppresses hepatocarcinogenesis . Cancer Cell . 2016 ; 30 ( 6 ): 909 – 924 . doi: 10.1016/j.ccell.2016.10.007 Google Scholar Crossref Search ADS PubMed 50. Hadadi E , Taylor W , Li X-M , et al. . Chronic circadian disruption modulates breast cancer stemness and immune microenvironment to drive metastasis in mice . Nat Commun. 2020 ; 11 ( 1 ): 3193 . doi: 10.1038/s41467-020-16890-6 Google Scholar Crossref Search ADS PubMed 51. Khan S , Yong VW , Xue M. Circadian disruption in mice through chronic jet lag-like conditions modulates molecular profiles of cancer in nucleus accumbens and prefrontal cortex . Carcinogenesis. 2021 ; 42 ( 6 ): 864 – 873 . doi: 10.1093/carcin/bgab012 Google Scholar Crossref Search ADS PubMed 52. Lawther AJ , Phillips AJK , Chung NC , et al. . Disrupting circadian rhythms promotes cancer-induced inflammation in mice . Brain Behav Immun Health . 2022 ; 21 : 100428 . doi: 10.1016/j.bbih.2022.100428 Google Scholar Crossref Search ADS PubMed 53. Rybnikova N , Haim A , Portnov BA. Artificial light at night (ALAN) and breast cancer incidence worldwide: a revisit of earlier findings with analysis of current trends . Chronobiol Int. 2015 ; 32 ( 6 ): 757 – 773 . doi: 10.3109/07420528.2015.1043369 Google Scholar Crossref Search ADS PubMed 54. Sweeney MR , Nichols HB , Jones RR , et al. . Light at night and the risk of breast cancer: findings from the Sister study . Environ Int. 2022 ; 169 : 107495 . doi: 10.1016/j.envint.2022.107495 Google Scholar Crossref Search ADS PubMed 55. Lai KY , Sarkar C , Ni MY , Cheung LW , Gallacher J , Webster C. Exposure to light at night (LAN) and risk of breast cancer: a systematic review and meta-analysis . Sci Total Environ. 2021 ; 762 : 143159 . doi: 10.1016/j.scitotenv.2020.143159 Google Scholar Crossref Search ADS PubMed 56. Zhang D , Jones RR , James P , Kitahara CM , Xiao Q. Associations between artificial light at night and risk for thyroid cancer: a large US cohort study . Cancer. 2021 ; 127 ( 9 ): 1448 – 1458 . doi: 10.1002/cncr.33392 Google Scholar Crossref Search ADS PubMed 57. Xiao Q , Jones RR , James P , Stolzenberg-Solomon RZ. Light at night and risk of pancreatic cancer in the NIH-AARP diet and health studylight at night and pancreatic cancer . Cancer Res. 2021 ; 81 ( 6 ): 1616 – 1622 . doi: 10.1158/0008-5472.CAN-20-2256 Google Scholar Crossref Search ADS PubMed 58. Wang F , Yeung K , Chan W , et al. . A meta-analysis on dose–response relationship between night shift work and the risk of breast cancer . Ann Oncol. 2013 ; 24 ( 11 ): 2724 – 2732 . doi: 10.1093/annonc/mdt283 Google Scholar Crossref Search ADS PubMed 59. Rao D , Yu H , Bai Y , Zheng X , Xie L. Does night-shift work increase the risk of prostate cancer? A systematic review and meta-analysis . Onco Targets Ther . 2015 ; 8 : 2817 – 2826 . doi: 10.2147/OTT.S89769 Google Scholar PubMed OpenURL Placeholder Text 60. Dun A , Zhao X , Jin X , et al. . Association between night-shift work and cancer risk: updated systematic review and meta-analysis . Front Oncol. 2020 ; 10 : 1006 . doi: 10.3389/fonc.2020.01006 Google Scholar Crossref Search ADS PubMed 61. Stone CR , Haig TR , Fiest KM , McNeil J , Brenner DR , Friedenreich CM. The association between sleep duration and cancer-specific mortality: a systematic review and meta-analysis . Cancer Causes Control. 2019 ; 30 : 501 – 525 . doi: 10.1007/s10552-019-01156-4 Google Scholar Crossref Search ADS PubMed 62. Baron KG , Duffecy J , Reutrakul S , et al. . Behavioral interventions to extend sleep duration: a systematic review and meta-analysis . Sleep Med Rev. 2021 ; 60 : 101532 . doi: 10.1016/j.smrv.2021.101532 Google Scholar Crossref Search ADS PubMed Author notes Sean W. Cain and Andrew J. K. Phillips contributed equally to this manuscript. © The Author(s) 2023. Published by Oxford University Press on behalf of Sleep Research Society. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Topic: cancer serotonin uptake inhibitors mortality sleep sleep duration Issue Section: Sleep, Health, and Disease Download all slides Supplementary data Supplementary data zsad253_suppl_Supplementary_Material - docx file Comments 0 Comments Comments (0) Add comment Close comment form modal I agree to the terms and conditions. You must accept the terms and conditions. Add comment Cancel Submit a comment Name Affiliations Comment title Comment You have entered an invalid code Submit Cancel Thank you for submitting a comment on this article. Your comment will be reviewed and published at the journal's discretion. Please check for further notifications by email. Advertisement Citations Views 50,265 Altmetric More metrics information Metrics Total Views 50,265 44,414 Pageviews 5,851 PDF Downloads Since 9/1/2023 Month: Total Views: September 2023 408 October 2023 1,625 November 2023 13,490 December 2023 5,440 January 2024 5,526 February 2024 2,942 March 2024 3,055 April 2024 3,445 May 2024 3,486 June 2024 1,813 July 2024 1,753 August 2024 1,882 September 2024 3,058 October 2024 2,342 Citations Powered by Dimensions 21 Web of Science Altmetrics × Email alerts Article activity alert Advance article alerts New issue alert Subject alert Receive exclusive offers and updates from Oxford Academic See also Commentary Consistency is key: sleep regularity predicts all-cause mortality Citing articles via Web of Science (21) Google Scholar Latest Most Read Most Cited Association between slow wave sleep and blood pressure in insomnia Enhanced Delta-gamma Phase Amplitude Coupling during Phasic REM Sleep in isolated REM Sleep Behavior Disorder Is Slow Wave Sleep the Key to Elevated Blood Pressure in Insomnia Patients? Accelerometer-Derived Sleep Metrics in Adolescents Reveal Shared Genetic Influences with Obesity and Stress in a Brazilian Birth Cohort Study Maternal depressive symptoms in and beyond the perinatal period: Associations with infant and preschooler sleep More from Oxford Academic Clinical Medicine Clinical Neuroscience Medicine and Health Neuroscience Science and Mathematics Sleep Medicine Books Journals Looking for your next opportunity? Academic Gastroenterologist (3-309-1197) Baltimore, Maryland Advanced Gastroenterologist White Plains, New York Assistant/Associate Professor New Haven, Connecticut Assistant/Associate Professor New Haven, Connecticut View all jobs Advertisement Advertisement close advertisement Advertisement About SLEEP Editorial Board Author Guidelines Facebook X (formerly Twitter) Contact Us Purchase Recommend to Your Librarian Advertising and Corporate Services Journals Career Network Online ISSN 1550-9109 Print ISSN 0161-8105 Copyright © 2024 Sleep Research Society About Oxford Academic Publish journals with us University press partners What we publish New features Authoring Open access Purchasing Institutional account management Rights and permissions Get help with access Accessibility Contact us Advertising Media enquiries Oxford University Press News Oxford Languages University of Oxford Oxford University Press is a department of the University of Oxford. It furthers the University's objective of excellence in research, scholarship, and education by publishing worldwide Copyright © 2024 Oxford University Press Cookie settings Cookie policy Privacy policy Legal notice Close Close This Feature Is Available To Subscribers Only Sign In or Create an Account Close This PDF is available to Subscribers Only View Article Abstract & Purchase Options For full access to this pdf, sign in to an existing account, or purchase an annual subscription. Close",
    "commentLink": "https://news.ycombinator.com/item?id=42022151",
    "commentBody": "Sleep regularity is a stronger predictor of mortality than sleep duration (2023) (oup.com)360 points by yamrzou 20 hours agohidepastfavorite221 comments Calavar 19 hours ago> We aimed to assess the relationship of objectively measured sleep regularity with risk for all-cause mortality, and mortality from cardiometabolic causes and cancer, in a large cohort (N = 60 977) who wore accelerometer devices for 1 week. They look at one week of sleep data and then check mortality records about 10 to 15 years later. It's hard to argue a causative effect between one week of bad sleep and death potentially 10+ years out. Obviously there's an implication that people with terrible sleep regularity in that one week snapshot had terrible sleep regularity chronically, which in turn had a causative effect on mortality, but we have to make a couple of deductive jumps to get to that conclusion. I'd really like to see the same study with longer term sleep data. reply benrapscallion 17 hours agoparentThat’s because this is in UK Biobank, a cohort of >500K Brits and collecting actually in such a large cohort is a miracle, let alone for multiple days. All thanks to the people who volunteered into the study. Would it be nice to have even more? Sure. But at that scale, patterns start to emerge. reply makeitdouble 14 hours agorootparentAn issue could be how people choose that week. As you point out that's a multi day commitment, and if part of the volunteers either adjust the timing of the experiments to match specific weeks (e.g. parents choosing school vacations), or adjust their schedule accordingly, what is measured becomes fundamentally different in nature to what measuring longer periods would bring. I'm with you on how we don't have a choice regarding to the quality of the study, it's just crazy hard to get any data at scale. But we can look at it as a very flawed \"best of what we can do\" and not take the patterns too seriously. reply iamacyborg 9 hours agorootparentprevThere’s a health research programme currently underway in the UK that’s looking to recruit up to 5 million people. I believe they’re currently at around the 1 million mark. https://ourfuturehealth.org.uk/ reply simmerup 8 hours agorootparentI almost volunteered for this but was concerned that they were liberally using the NHS branding to disguise that they’re private reply sho_hn 11 hours agorootparentprevNote the study had 60977 samples, not >500K. reply benrapscallion 9 hours agorootparentI was referring to the size of UK Biobank. Yes, only a fraction participated in actigraphy. The same is true of imaging, blood biomarkers, etc. reply tossandthrow 9 hours agorootparentprevAnd that's fair - it should just thoroughly be discussed and qualified not to release false research. reply benrapscallion 6 hours agorootparentHow is this false research? Most other studies rely on self-reported measures of sleep. This is using objective measures. reply styyyaaa 16 hours agorootparentprevYes. Any statistics buffs here who can tell me: Is 500k brits for 1 week as good as 5k brits for 100 weeks. Effectively with so much data aren't you getting a superposition anyway. reply kitchi 14 hours agorootparentA superposition of? In statistical mechanics there's a concept of \"ensemble average\" and its provable that if you have a system, the average state of the system over say a 100 realisations (\"ensemble average\") run for 1 second each, is equal to the the average of one system run for a 100 seconds - under some assumptions of course. I don't know enough a about human biology to make a statement about whether any of those assumptions will hold true, but maybe someone else will. reply sfink 30 minutes agorootparentThat's not the scenario here. An ensemble average requires those 1 second periods to be randomly sampled from the overall runtime. You can't just measure the first second after starting for all samples, or even the 7th second after starting, and say anything statistically sound about the average of the whole runtime. The weeks that were measured in this study were not random. How far from random is the big question. reply ratedgene 13 hours agorootparentprevI would also say it's geospecific. Like if there is a nationwide trauma of sorts (politics, etc) it could influence a group to not be representative of the population at large. reply pletnes 11 hours agorootparentprevHumans age, so obviously not. Interesting analogy, though reply styyyaaa 11 hours agorootparentprevI mean the data lacks any person's sleep history, but if there is a cohort of bad sleepers, as a group you get a collective pseudo-history (because altough study is done at the same time they are in different life stages) that is no-one's history but might track the average well for that cohort AND take into consideration more than a week. In the similar way sampling the height of 1000 of 1M people will give you a good estimate of the average. reply nerdponx 55 minutes agoparentprevI would expect that P(bad sleep this weekbad sleep chronically) > P(bad sleep this weeknot bad sleep chronically). It's a question of inferential power: what is the minimum detectable effect size with such an indirect measurement? reply PittleyDunkin 19 hours agoparentprev> It's hard to argue a causative effect between one week of bad sleep and death potentially 10+ years out. Statistically—absolutely, I agree with you, but controls and sample sizes can always be improved. Narratively—it's also not difficult to see: \"gunk builds up in brain; gunk requires regular removal; sleep removes gunk; stable sleep removes gunk better than unstable sleep\" It's difficult to blame people for emotionally attaching more to the latter than the former. reply geysersam 19 hours agorootparentIt seems easier to see: \"some people have lots of difficulties in their lives that makes them have an irregular sleep schedule; some people have lots of difficulties in their lives that makes them die early\" reply kimixa 18 hours agorootparentI agree - an obvious connection would be \"Lower income jobs tend to have less control over their schedule\", be it shift workers, hourly service jobs or similar. There also may be links to worse healthcare due to lack of insurance in those jobs. It might just be another \"Poorer people don't live as long\" correlation. reply sahmeepee 16 hours agorootparentThe source data is from the UK so it has nothing to do with the health insurance attached to specific jobs. Healthcare is covered via general taxation in the UK. reply geysersam 2 hours agorootparentInterestingly enough, even in countries with tax funded healthcare people with lower income and socioeconomic status tend to receive worse healthcare (fewer expensive interventions, etc). reply kimixa 10 hours agorootparentprevI'm aware of the UK system, having been born and lived there for over 25 years :) I didn't note that the data was from UK persons, but the general point likely remains, if not as strongly. While true health insurance isn't attached to jobs in the same way, there's still uneven access - the town I grew up in closed it's local surgery a decade or so ago, so it's about a 30 minute drive, or over 2 hours on a rather indirect bus. Not everyone has a car, or can afford to take pretty much an entire day off work to get there. Assuming you can actually get an appointment, too. Richer areas often are better served, and richer people have better access to transport and time flexibility. And my job came with BUPA membership, which can also make some things easier, it's not the hard barrier to any care in the same way as the American system. And while the paper said that it corrected for Socioeconomic status, knowing the /scale/ (and possible error) in that correction relative to the claimed meaningful difference would be useful in studies like this. It feels like the sort of correction should be detailed more than just saying they did it. But I guess the paper isn't really claiming causation, merely correlation in that it's a predictor of mortality. Though many commenters here seem to assume. reply kaonwarb 17 hours agorootparentprevThey at least tried: \"Results were adjusted for age, sex, ethnicity, and sociodemographic, lifestyle, and health factors.\" reply lazyasciiart 16 hours agorootparentI wonder if lifestyle or sociodemographic includes having specifically young children. That seems like a specific known temporary sleep disruption. reply rcyeh 15 hours agorootparentInteresting, I initially thought this factor was probably negligible, because the cohort was aged \"62.8 ± 7.8 years\" (either before or after the 7-year observation period). I still think it's small, but perhaps not negligible. I estimate that some 2-4% of the group could be heavily involved in caring for grandchildren, sorting themselves into the irregular category. It's well-known that school-aged children pass flu to grandparents [2], and then grandparents die, just in time for the 7-year post-birth observation window. The absolute death rate due to flu is 10^-4 to 10^-3 per year, which would be visible on the paper's mortality time course chart. Estimation details: * some 20% are grandparents (and very few are parents) of young children [0] * 20-50% of grandparents care for grandchildren regularly [1] So maybe 4-10% of the cohort as an upper bound. If the birth rate is 12 per 1000 per year, and babies cause sleep disruption for about two years to two persons, then that's about 4% also, but perhaps mostly not the same age group. [0]: https://assets.publishing.service.gov.uk/media/5eeb975b86650... [1]: https://www.ageuk.org.uk/latest-news/articles/2017/september... [2]: https://www.sciencedirect.com/science/article/abs/pii/S00255... Going to sleep now. reply EVa5I7bHFq9mnYK 15 hours agorootparentprevThe cogort was 40-69 yo. reply jdietrich 12 hours agorootparentprevAlso: \"people in poor health don't sleep well\". That's really hard to fully control for, because a lot of problems will affect mortality at sub-clinical levels that don't satisfy diagnostic criteria and won't appear on your medical records. reply hackernewds 18 hours agorootparentprevI would presume that they somehow controlled for this most obvious confounding factor. almost every experiment I find that they are naysayers (as important as they are) that assume someone just simply plotted a chi-squared distribution. reply mcmcmc 17 hours agorootparent> presume Okay reply njtransit 17 hours agorootparentprevThe issue is that almost all human behavior is correlated and, even if you have an easy-to-see [sic] method of action, e.g. “brain gunk”, that doesn’t automatically negate a nearly-infinite set of other possible causes from correlated behavior. Just a random example: those with high stress probably sleep poorly. You can think of a number of possible explanations that link high stress to shorter lifespans: more likely to commit suicide, more fat retention, less time for healthy activities, etc. reply danparsonson 19 hours agorootparentprevSure, but things that make narrative sense can easily fall down when examined - that's why the statistics are important and not just a rubber stamp. See for example https://en.m.wikipedia.org/wiki/Regression_toward_the_mean reply meindnoch 18 hours agorootparentprev99% of people die from causes other than \"brain gunk\". reply ArnoVW 10 hours agorootparentI think the parent is talking about Alzheimers, which is the accumulation of brain gunk. Alzheimer's is the fifth-leading cause of death among Americans age 65 and older. reply 7e 18 hours agorootparentprevBrain gunk could accelerate those causes, no? reply saalweachter 17 hours agorootparentEh, you gotta be careful with One Weird Trick medicine. Once you get past painfully obvious problems, everything not currently easily fixed by modern medicine tends to be \"well it's really complicated\" sets of problems; there's not one Cancer, your blood pressure can be elevated for many reasons, syndromes like chronic fatigue are almost certainly a mix of dozens of problems binned together by common symptoms but will have different causes and treatments. Anyone saying they have one treatment to fix dozens of problems is a huckster, and trying to come up with a medical Theory of Everything to explain large swaths of disease is your origin story for how you become a huckster. reply TZubiri 16 hours agorootparentThe difference between panaceas and this is that the idea for extending life isn't to add something, but not to fuck an essential part of life. If you think of life as the careful coordination of millions of parts, then there will be a million of things that if you fuck up you will die. reply lanstin 15 hours agorootparentWhich is the model that matches acceleration of death rates as we age. reply TZubiri 36 minutes agorootparentThe reduction of death rates which is not due to panaceas but to fixing specific causes of death, one by one, yes. reply seba_dos1 18 hours agorootparentprev...or those causes could accelerate brain gunk buildup. reply TZubiri 16 hours agoparentprev\"Obviously there's an implication that people with terrible sleep regularity in that one week snapshot had terrible sleep regularity chronically, which in turn had a causative effect on mortality\" You got it. \"but we have to make a couple of deductive jumps to get to that conclusion\" This is always the case unless you assume reality is as big as what you can perceive. \"I'd really like to see the same study with longer term sleep data.\" You can say this of literally 100% of the studies, it will never be enough. I understand it when authors put this at the end of a study because they want more funding and because their subject is all they think about. But for reasonable human beings you gotta make a common sense jump and allocate resources to other subjects. Yes, regular sleep has good effects on health, the burden of proof of that was already 0, this study is a nice added touch, there will be no double dessert, move on. reply Calavar 15 hours agorootparentEvery deductive leap comes with uncertainty in establishing a causal chain. I don't I'm being overly reductive about the nature of evidence when I say that. There is a specific question that needs to be addressed: With a one week window into sleep habits, are we selecting for people who are chronically poor sleepers, with those poor sleep habits leading to disease? OR Are we selecting for people who are chronically ill for other reasons and those chronic illnesses prevent them from getting regular sleep? For example, people with sleep apnea have terrible sleep, and they have lower life expectancy than the general population. However, the cause of that lower life expectancy is not the poor quality of sleep; it's the cardiac effects of abnormal breathing over a long period of time. If a person with decades of excellent sleep habits developed sleep apnea in the last 5 to 10 years of their life, the accelerometer will capture their irregular sleep and the death registry will capture their early deaths. That doesn't mean poor sleep habits killed them. There are many other such chronic illnesses that can be confounded this way. Heart failure. Obstructive lung disease. Dementia. All can lead to irregular sleep. Add them together and you've captured a large segment of the population with ~10 years left to live. The authors address this relationship in the conclusion and supplementary materials, but they appear to approach it entirely from the framework of poor sleep being causative of cardiovascular disease. Well yes, there's evidence that poor sleep can cause cardiovascular disease, but it's also well established (as I explained) that it can happen the other way around. If you want to cement a full chain of causality, you need a longer time window. Capture a young population with a low burden of chronic disease, show that poor sleep habits came first (i.e. within a certain age window), then cardiovascular disease, and then shorter lifespan. That would be the ideal data, even if difficult to acquire. reply TZubiri 36 minutes agorootparentToo much word. Sleep good, regular sleep good. No more think, more do. reply mewpmewp2 16 hours agorootparentprevIt is easy to say that good quality sleep is good for a person. But what if I literally never get that type of sleep. What should I do? How concerned should I be? How much focus and effort should I put into this? Should I take the Ambien I'm prescribed or should I try to go for the best natural path - because I really, really don't have good experience with Ambien. And it's almost like it's the best modern medicine can offer. reply TZubiri 1 hour agorootparentHow to fix is an entirely different topic. My bet is no drugs, and have faith in your agency. reply baxtr 12 hours agoparentprevFWIW it’s mentioned in the study explicitly as a limitation: > There are several limitations in this study. First, the single week of data collected for each individual provides only a snapshot of their sleep–wake patterns, and future work should collect sleep–wake data over a longer timeframe and include multiple weekend-weekday transitions. It is nevertheless interesting that even a snapshot of sleep behaviors is predictive of mortality for a follow-up period of several years. reply joe_the_user 14 hours agoparentprevThe other problem is that irregular sleep may correlate with lower income, which correlates with a lot of things that lower life expectancy. reply ErigmolCt 9 hours agoparentprevYep, longer tracking period would better establish if consistent irregularity (or lack thereof) impacts mortality risk. reply bjornsing 10 hours agoparentprevAnother hypothesis that seems plausible to me is that what we’re really measuring when looking at sleep is mental health, and mental health is a strong predictor of mortality 10-15 years into the future. reply bloqs 9 hours agorootparentYou are already correct, but the actual factor here is trait Neuroticism (negative emotion personality dimension), which is very roughly speaking a combination of genetic factors and your ACE score. ACE score is shockingly good at predicting mortality, having one over 6 makes people die on average 20 years earlier. reply bjornsing 8 hours agorootparentYeah that’s kind of what I’m after: ACEs mess up people’s sleep and cause early deaths. That makes sleep patters a strong predictor for all cause mortality, but the causal relationship is really ”ACEs cause both poor sleep and early death”. reply thimabi 16 hours agoprevI found the best way to ensure sleep regularity is by having a fixed wake-up time. If you don’t have one and you begin to set your alarm clock to early hours, the first weeks doing so will feel like hell. But eventually all will be worth it. The sleepiness during the day will disappear and, at night, your body will naturally want to get some rest early. Another strategy is to fill your day with deep work and exercise, so as to ensure maximum tiredness at night. Nowadays, my sleeping patterns are mostly regular thanks to these protocols. Every day about 9pm, I feel sleepy, and my bed begins to look very enticing. I follow this pattern for my daily well-being, but it’s nice to know it reduces my mortality risk, too. reply xeromal 13 hours agoparentI think you have a key component in good sleep but there are 2 more imo. The first is that phones and light break our natural sleepiness triggers and the second is that we don't exercise our bodies enough because of our sedentary lifetstyle. I used to struggle with terrible insomnia and I still get bouts of it time to time but I've also found it's related to my laxing my 3 rules. If I have a good workout about 5x a week, I turn off my phone and the lights 30 minutes before bed and take a good long shower in the dark and I have a regular alarm set at 7am that goes off 7 days a week, I'm almost guaranteed a good nights sleep every night. It also had the added affect of ridding me of my night terrors and sleep walking I used to have frequently but I'm 90% certain that was correlated to me using my phone in bed and it causing my brain to enter a weird state where it never really turned off. reply duggan 9 hours agorootparentI'm going to \"yes, and\" this with just one more thing that may seem obvious: almost any amount of alcohol is enough to throw that pattern out of whack. The older I get the more reluctant I am to make the trade. Few glasses of wine with friends is hard to pass up though :) reply Starlevel004 8 hours agorootparentprevI do none of those things and have great sleep. reply ErigmolCt 9 hours agorootparentprevShutting off your phone and lights early is especially smart. It lets your mind and body transition smoothly into rest mode reply xeromal 4 hours agorootparentI think it makes the biggest difference out of my habits. I kind of picked up on it when I realized that I never struggled to sleep when I went camping out in the woods. My body would instantly fall into sleepiness when it got dark out because I had no artificial light keeping me up as well as no easy access entertainment like TV reply lazyasciiart 16 hours agoparentprevI did that for years with jobs that required it and didn’t find that worked for me. reply seer 12 hours agorootparentI think I effectively switched from a night owl to an early bird by having something I _really_ wanted to do in the morning that was actually physical (martial arts classes). I remember the exact day my body switched - I was going in the early morning and my body was not approving of my initiative. Every morning was hell - but then one morning early spring we had a training outside - sunrise, early leaves on the trees in the park, we all got quite sweaty some of the guys started to take our tops off and it looked like a scene from a martial arts movie. I remember thinking this looks so cool - we’re training like our forefather used to. Next day I woke up exact time I needed no grogginess whatsoever. Have been an early bird ever since, almost 10 years now. reply karmonhardan 8 hours agorootparentOlder people tend to get up earlier. There is a special place in hell for the Baby Boomers who insisted on a 7:30 start time for our office (though, of course, as the guy who opened every morning, I had to be there earlier). The anxiety that I developed went away when I left and could get up when I wanted to. There are many, many monrings where I lie in bed at 7:30 and think, \"I would have had to be a Bukowski quote by now.\" reply thimabi 16 hours agorootparentprevThat’s a bummer! Have you found out why that didn’t work? Perhaps something like caffeine or allergies was interfering with your sleep? If haven’t found out, I wholly recommend doing a sleep study if you can. It can shed some light on any obstacles you may be experiencing. I’ve never done one myself, but I know people who got enduring benefits from the insights of their studies. reply bitwize 16 hours agorootparentIn my case it's probably a combination of undiagnosed ADHD and \"revenge bedtime procrastination\". reply jschrf 15 hours agorootparentCame here to post this. I remember once when going back on ADHD meds, a very smart doctor adding 5mg of Dexedrine. At 5pm. I asked him why he would prescribe a stimulant so late in the day. His reply was simply \"it's so you can think of one thing instead of all the things\". Worked for me. reply tomcam 15 hours agoparentprevThat method nearly wrecked my life and didn’t change my sleep patterns reply makeitdouble 14 hours agoparentprevFor people failing for a long time at fixed wake-up schedules or still feeling like shit after months of doing it: giving up is fine, and it frees you to explore what works for you. It might change depending on the seasons, and you might more or less sleep depending on what you're doing at that time. Waking up at the same damn hour everyday to deal with my kid's school was an utter pain for years, and I got in a better health and shape once I could adjust depending on my daily condition. It still have a set of fixed alarms, but regularly ignore the first ones as needed, and only wake no-matter-what for the last one for my job. I heard from other coworkers doing the same, and it was a game-changer for most of us. reply RMPR 10 hours agorootparent> For people failing for a long time at fixed wake-up schedules or still feeling like shit after months of doing it: giving up is fine, and it frees you to explore what works for you. Another piece of anecdata from someone who used to be like this for years. I first noticed that regular alarm sounds annoyed me and eventually I would get used to turning it off and going back to sleep (that is until I HAVE to wake up). I then figured that if I set up an alarm with a song that I like, it would make waking up more enjoyable. Which I eventually did. The first few weeks, I enjoy waking up and in a sense look forward to it, but after a certain amount of time not only I get used to it and the cycle continue, but I also can't stand that song anymore (RIP rolling in the deep, chainsmoking, ...). I randomly stumbled upon the app sleep for Android that has a feature I didn't know I needed, putting a playlist as an alarm sound (I shuffle it of course). Now every morning, waking up is an adventure, and more often than not I end up singing along. Now months in, I haven't failed to wake up even once. And I don't have any alarms on Sunday, yet I still wake up without it. That is with the caveat that I know I need between 7:30 and 8h of sleep, and I stop all screens by 10 (night time feature of Android is very helpful in this regard). Except my ebook reader than I use without backlight. reply ErigmolCt 9 hours agorootparentprevFinding what feels natural can sometimes be a matter of listening to those instincts rather than pushing against them reply clvx 15 hours agoparentprevGet a german shepherd dog. They will wake you up at the same time everyday whether you like it or not. reply thimabi 7 hours agorootparentI don’t know about a German shepherd dog specifically. But my experience with dogs is that they will wake me up whether I like it or not… whenever they feel like it, even in the middle of the night. That can be severely disrupting. reply ErigmolCt 9 hours agorootparentprevHaha, so true—whether it’s a German Shepherd or a cat, you’re definitely getting a \"reliable alarm clock\" that doesn’t care about weekends or your sleep schedule! reply t-3 15 hours agoparentprevAlarms aren't foolproof though. I often shut them off in my sleep or sleep through them, so I have to keep the phone on the other side of the room AND change the alarm every week or two to prevent adaptation, and that still doesn't help if I can't fall asleep for whatever reason and end up sleeping through due to sheer exhaustion. What I've found that actually works well is alcohol; if I'm not tired at sleep-time, take a swig of 130 proof absinthe and I'll be asleep before long. reply thimabi 7 hours agorootparent> What I've found that actually works well is alcohol; if I'm not tired at sleep-time, take a swig of 130 proof absinthe and I'll be asleep before long. I’d be wary of relying on alcohol to sleep, because the relaxation that it offers is somewhat distinct from a good night’s sleep. Alcohol has been known to disrupt “REM sleep”, thus making your sleep phases inconsistent. In the long run, it might leave you with even poorer sleep quality. reply internet101010 14 hours agorootparentprevI set an additional alarm one hour before I need to wake up specifically so that I can get the feeling of going back to sleep. It has really helped. reply dchftcs 13 hours agorootparentprevI have found that vibrating alarms on wrist watches to be very effective. For over 5 years I've been setting a vibration alarm on my watch and another backup alarm on my phone. I never slept through the vibration alarm. Granted YMMV reply globular-toast 9 hours agorootparentprevSleep for Android has some neat modes to try to prevent this. Even just the fact it colours the snooze button green and the dismiss button red is incredibly thoughtful and works well. My just woken up brain can somehow understand \"red bad\". If you're on iOS the alarms are caveman style. My partner sets like 20 alarms on her phone, it's hilarious. reply left-struck 13 hours agoparentprevI agree with this except for the early part. At least personally waking up at about 8 works for me, that way I’m not waking up before sunrise in winter which makes me unhappy quite reliably. A lot of people are weirdly proud of how early they wake up, and I’ve literally been shamed for waking up late, called lazy etc in a casual sense but that’s nonsense. I just work later. reply hombre_fatal 14 hours agoparentprevMost people have to wake up at a fixed time for work and school their entire lives. I don't think that's what sleep hinges on, else everyone would have good regularity and we'd take it for granted. It's your behavior and attitude towards going to bed. reply ErigmolCt 9 hours agoparentprevI think a solid rhythm is important! reply Starlevel004 16 hours agoparentprevThis is a fantastic way to give yourself an early heart attack. reply lazyeye 14 hours agoparentprevOne thing I've done is create a \"color clock\" using a smart bulb that changes color based on a daily schedule. So at 8:00pm the bulb has a dim orange glow, this changes to dim red glow at 9pm and then turns off at 10pm (sleep time). Its a really nice relaxing way of ensuring a regular sleep pattern (no longer clock watching etc). reply m463 16 hours agoparentprevwhat helped me was to use an alarm clock. but to go to sleep! reply thimabi 8 hours agorootparentIn my case, having a bedtime alarm would only leave me anxious and frustrated if I went to bed and failed to sleep at that time. But I have tried something similar: setting an alarm to decompress before sleeping. No phone, no TV — just some quiet music playing or some books to read. I still set this night alarm, but it is much easier to ignore it than to ignore my body’s natural tendency to lose steam after waking up early and having done so much during the day. reply colechristensen 15 hours agoparentprevI do much better with no alarm clock and being in situations where i don’t need one. Allowing myself to naturally wake up and being in situations where variable times to naturally wake up are ok are very much better for my brain and overall health. reply snvzz 16 hours agoparentprevI do a version of this where what I have is a fixed wake-up time WINDOW. The window is 90m, to account for 90m sleep cycle. I set the alarm to whichever multiple of 90 falls within the wake up window. As a result, I sleep somewhere with 7h to 7h30 consistently, with the odd ~6h sleep day or ~9h in special circumstances (being sick). reply Izkata 25 minutes agorootparentThere's an Android app called \"Sleep\" where you set an alarm window and put your phone on the corner of your bed, and the alarm only goes off when it detects you're a little active already and not in deep sleep (using the accelerometer to detect movement). If it doesn't detect enough activity it'll go off regardless at the end of the window. It has a bunch of other features as well (rating how well you slept and detecting patterns to give advice, a snore detector, etc) but that alarm is the one I use it for. reply ttoinou 16 hours agoparentprevHave you ever heard the saying “correlation is not causation” ? Couldnt it be that you already had a good enough health to push you to adopt a better sleep schedule ? reply apwell23 16 hours agoparentprevfor me the best way is to not worry about sleep at all. After many decades of abusive relationship with sleep. I've had enough. If it comes it comes, if not then thats ok too. I stop reading this 8 hr nonsense or fearing an early death from not sleeping. whatever. reply noduerme 19 hours agoprevI wonder how this translates if the irregularity is by choice. I can certainly see why people working back to back night and day shifts, or otherwise frequently being at a sleep debt that was compensated for later, would die younger. But as a freelancer, my sleep schedule is more or less my own. I go to sleep when I'm tired and usually try to sleep as long as I want to. Sometimes that's 11pm-10am, sometimes 6am-11am. It can oscillate throughout the week, but I try to average 16 hours of sleep in any given 48 hour period. Maybe this is incredibly unhealthy, but I've believed for a long time that it's kept me younger and healthier than being forced into a sleep rhythm that isn't what my body wants. reply atomicnumber3 18 hours agoparentAs someone whose natural sleep cycle seems to be closer to 26-28 hours, and whose preferred sleeping hours in a 24-hour cycle are 6am to 2-3pm... I commend you. Sincerely, someone forced into a \"normal\" schedule by kids school start times and, well, everything else too, I suppose. reply noduerme 18 hours agorootparentHah. I've often thought that my actual rhythm is for a 25-26 hour day. I was going to write that but it seemed like too much to explain. As I go to bed later and later, eventually I find myself awake past sunrise, which is usually the day when I'll intentionally have a short sleep and snap back to an early bedtime that night. Kids would definitely screw up this aspect of my lifestyle lol. But kids give you immortality, and here I am just wondering if I'm gaining or losing a couple years. reply deprecative 18 hours agorootparentJust as a secondary data point. Without medication I have about a 32:12 hour cycle. Up for at least 30 hours and sleeping for about 12. I can function on 5 but I'm grumpy about it for a while. Always been that way. Completely inverted as well. Up all night and my body tries to convince me to sleep during the day but I'm just not able to for another twelve hours. I got fired from several jobs because of it. My folks weren't understanding so I genuinely believed that I was willingly staying up and well... Needless to say I have no professional network and most of my friends thought I was a massive flake and those connections fell apart, too. I wouldn't wish this on anyone. Even diagnosed with insomnia I still feel massive guilt about not being able to sleep like a normal person. I just ruin everyone's plans around me. If I need to be up I have to hope my medication works (when employed) and if it does I need an hour to wake up enough to feel safe doing anything major like cooking or driving. Sorry for the ramble. Appropriately I've been up for like 34 hours and am hoping these OTC meds kick in. Hope y'all have a great weekend. reply noduerme 14 hours agorootparentHey, I feel that. Lots of people think I'm a flake. And I use the word \"inverted\" frequently to describe my state to my friends/girlfriend/clients when it gets too far outside social norms ;) Just a piece of unsolicited advice: I learned to make a virtue out of it. I'm a solo software dev and I have to maintain big pieces of code that run 24/7. Well, my virtue is that I've been available 24/7 to my clients for the last 20 years. And one of the results of that has been that they've never abandoned me and gone to larger companies to deal with software issues. My own schedule is so variable, it doesn't really matter if I'm asleep or awake or what time zone they're in; if it's not urgent, it goes into my inbox, but if it's urgent, I usually answer the phone immediately. Part of this has been adding layers of support forms so I don't have to wake up to every phone call. But the people who have my cell number get through right away. The result of that is that I basically get paid $300 every time I have to wake up, which is soul-soothing enough to prevent me from being angry. And the rest of the time I can sleep whenever I feel like it. Being on a 32:12 hour cycle could have massive rewards. Clients are extremely appreciative, especially if you break your sleep for something important. Like, don't be afraid to tell your clients about your sleep cycle. Getting through admitting that was probably the biggest breakthrough of my career. My girlfriend loves that I'm still up working and make her breakfast at 4am when she's headed for work some days, or make her dinner when she comes home at night others. It's always a surprise, I tell her. You just have to find people who appreciate the energy you bring. Yes, the \"straight world\" of people with 9-5 jobs and kids absolutely abhors this lifestyle and thinks it's irresponsible and flakey. But then again, they don't get paid $300 for waking up in the middle of the day ;) My friends (and girlfriends) are lyft drivers, waiters, coders, night shift workers, and other people who spurn daylight society. We are legion. reply authorfly 9 hours agorootparentFor a bit of alternate perspective, I had a happy life until a night shift worker moved in next to me, and began setting off alarms, cooking and leaving at variable times while experimenting with their sleep. They explained how they could sleep through everything. Completely oblivious to why the building wanted them to leave(I'm not saying this is the same for you). Maybe that is what this study is capturing. reply willy_k 12 hours agorootparentprevHave you tried magnesium l-threonate, sub-milligram melatonin, or CBN? The first two have been very effective for me and the latter seems promising, but I don’t have that much trouble getting to sleep so I’m curious if they’re effective for someone with real difficulties. reply Clamchop 16 hours agorootparentprevI relate so much. I'm on some heavy hitting medication to manage it which no doubt will have their own consequences for me. Feels like there's no winning. reply metadat 15 hours agorootparentYes, what are the \"heavy hitting meds\" for sleep, please? reply mrandish 17 hours agorootparentprevJust curious what meds you take for this. reply dmurray 18 hours agorootparentprevExperiments where people aren't told what time it is end up with them gravitating towards a 25-26 hour day. So you're totally normal. Wikipedia [0] criticises these experiments and says they didn't account for electric light, which apparently lengthens the cycle. So to rephrase, people with any access whatsoever to electric light favour a 25-26 hour day. You're still totally normal, but it explains you may have better outcomes with annoying interventions like \"no artificial light in the last few hours of the evening\". [0] https://en.m.wikipedia.org/wiki/Circadian_rhythm reply vundercind 16 hours agorootparentI actually experimented with that. I allowed myself two candles, which I found was enough to read by and plenty for navigating my house. Soon, all night time electric lighting seemed batshit crazy. Dozens of times too bright. Of course we all can’t fucking sleep. It’s so wildly much brighter than necessary. I gave it up after a few weeks because it’s pretty much impossible to keep up if you’re ever around other people in the evening (and I have a family, so…) and they’re not entirely on-board, or just need to get stuff done at night because life is busy (again: I have a family) and everything about modern scheduling and activities assumes you can do things for hours after sun-down. But, the experience did convince me that 95% of “night owls” and serious trouble sleeping are just the obvious and natural consequence of crazy-bright nighttime lighting and hyper-stimulating electronic home entertainment (which all also emits light, so I was shunning that stuff after dark too during my experiment). We have a sleeplessness epidemic? Gee I wonder if it’s because we light up our houses like a carnival and then put a world’s fair x100 at our finger tips. Like, yes, of course it’s that. I’ve since discovered that low-dose weed gummies also get the job done with no side effects (aside from allowing me an hour or so of giggly TV watching right before bed, if I want it—oh no, what a tragedy), for me, and are far more compatible with modern life. Kinda lame to have to medicate my way out of the human body and mind of course not being able to cope with what we all do to them after dark, through. I’ve used the analogy on here before, but imagine some 18th century emperor or king complained to his physician that he’d been lighting up his palace and grounds every night as bright possible and hosting a weeks-long 24/7 festival featuring the world’s finest entertainers (including the rather lewd sorts), intellectuals, travelers and philosophers, jesters and players, and, well, for some reason he’s having trouble falling asleep at a decent hour. LOL fucking yeah, dude, no wonder. But we do that and then go “man I wonder if maybe I need a better pillow or to get more vitamin D” or whatever. Seriously? reply noduerme 14 hours agorootparentWeed gummies used to work like a charm for me, up until my late 30s, but in the past few years even the low-THC ones leave me extremely groggy the next day. Although I get a great sleep, it isn't worth it anymore for how long it takes me to come back to full function. I keep them around for emergencies when I need to go to sleep. I keep my lights very low in the hours before I go to bed, and use candles, and mostly read books rather than electronic devices. But one thing I'd point out is that - I live fairly far north. At this time of the year, there is very little direct sunlight at all. Whole days can be as dark as 6am, and the sun will be setting before 5pm soon. There is a balance, in the winter, when without bright electric light you could never really wake up. And sometimes it feels like living on a space station, because if you're a night owl that's the only light you get. I've had modestly good results from using a \"happy lamp\" during the darkest days. Just pointing out that - although I agree with you about clamping down on carnival lights at night - there is an opposite extreme which can cause people to go into a sort of seasonal hibernation. reply wrs 18 hours agorootparentprevMany years ago, at college, I stayed in town for a summer rather than going back home or elsewhere. I had a coding job with no fixed hours (and working in a windowless basement), and no friends in town. So I tried the “sleep when I want” experiment. IIRC I settled into about a 26-hour cycle. reply left-struck 13 hours agorootparentprevI used to do this as well sort of, my schedule would shift by a few hours every day. I found that it had a significant impact on my mental health when I was sleeping through the day though, and it made it difficult to participate in various aspects of society like work, education and in person social things. I was eventually able to “fix” it by taking melatonin as a supplement to fix a short term schedule and being super anal about not letting people pressure me into playing games late or staying out late. I basically reframed sleeping hygiene as a primary health concern and that worked for me. Anyway if your schedule doesn’t impact you in a negative way then go for it reply artdigital 17 hours agorootparentprev> I've often thought that my actual rhythm is for a 25-26 hour day To be fair, we're all like this. The average length of Circadian Cycle is closer to 25h than 24h when isolated: https://www.circadiansleepdisorders.org/info/cycle_length.ph... reply OJFord 16 hours agorootparentI was all ready to agree because I always feel I want >24h, but that link is actually correcting earlier studies finding 25h to say no it is more like 24h on average (once isolated from screens)? reply deergomoo 17 hours agorootparentprevI really hoped I’d grow out of wanting to sleep ~3-11am but it’s just never happened. No matter how tired I am during the day, I get a second wind around 8pm and have to force myself to go to bed at a reasonable hour during the work week because I won’t get sleepy, even if I stay away from screens. Unfortunately I really enjoy night time, so I regularly completely fail at that task. I can’t remember the last time I woke up feeling rested, and yet here I am on Hacker News at 1:48am. At least I can sleep in tomorrow. reply gexla 13 hours agorootparentI think everyone gets a second wind in the late hours. I used to have a schedule where I would start nodding off at 8PM and had to sleep, then wake up within 10 minutes of 4AM the next morning. My whole day was on such a routine, I never had to check the time. I could tell by temperature, sun, what I was doing, how I felt (hungry, tired) what time it was. I would be at my highest energy for the first hours after waking up. Then my energy would plunge during midday. Then it would build back up leading into the evening until I couldn't keep my eyes open anymore. Maybe 4 to 8PM I would be at good energy until I hit the abrupt cliff. I guess I was sort of a morning AND night person. I had to train my body to get on this schedule. reply lucb1e 16 hours agorootparentprevGetting up at 11 sounds workable though. I have a colleague that regularly starts at 11, may be having sleep issues (they mentioned something but I don't know if that's always the reason), nobody seems to mind. My problem is that, during holidays, 13:37 seems to somehow be a very common time for me to get out of bed (suspiciously often around that minute, making me think it's a bias rather than coincidence)... so more like going to bed when the sun and birds would otherwise get annoying to fall asleep with People speak of teenagers having a different sleep cycle but I'm now suspecting that, rather than that you'd grow out of your body's schedule, it's just that you don't complain to your toddler and expect them to understand and shift your job of entertaining them to later in the day. Same story at work; also a factor most teenagers don't have in the same way. So you suck it up and fall into a new rhythm that kinda works too reply deergomoo 5 hours agorootparentFor me I think the only way I could make it work would be to be self employed and work a shorter day. My job wants me online from 9am, and my wife keeps a “normal” schedule so starting an 8 hour work day at 11 would seriously cut into our time together in the evening. reply noduerme 14 hours agorootparentprevLooking back, I think that I was chronically exhausted as a teenager. I was always on 5 or less hours of sleep. At that age, you have the metabolism to wake up and have a full day without feeling sleepy. But I've always wondered about why militaries deprive their recruits of sleep, except right before battle, or why school systems do - and I think it's simply that people are more malleable when they're chronically tired. Yes, you can get used to it, and your adrenaline will still kick in when necessary. But you're not performing as a fully cognizant human being. Teenagers, however, don't need to be fully cognizant of anything anyway... so it all works out. reply noduerme 14 hours agorootparentprevIf you're going 3-11, that's 8 hrs... I'm the king of getting a second wind, but it's usually alcohol-driven. Curtailing food and booze about 3 hrs before sleep, watching a little Mentour Pilot or reading a book, I'll conk out before I planned to. Avoiding the second wind is a discussion in itself. reply deergomoo 5 hours agorootparentProblem is the rest of my life is incompatible with waking up at 11. My job wants to me available from 9am and even if that wasn’t the case my wife—perfectly understandably—would not be too happy with me working until 19:30 or 20:00 as that would massively cut into our time in the evenings. Hence why I wish I’d grown out of it. +1 for Mentour Pilot, love his videos reply vundercind 15 hours agorootparentprevMethods that work for me, a lifelong “natural” (lol, nope) night owl: 1) No nighttime lighting brighter than a single-digit count of candles. No glowing screens after dark, either. Within a couple days I was no longer a “night owl”. Go figure, it was all fake, all those years. 2) A few mg of THC edible 90 minutes before I want to be asleep. reply deergomoo 5 hours agorootparentI have found that, when I do succeed in forcing myself into bed at a normal time, I can “brute force” tiredness quicker by reading on my Kobo with low brightness. Honestly the part I struggle most with is actually just going to bed, there’s a strong psychological aspect to it. As for your two points, they’re not super practical for me, at least not for a big chunk of the year. From now until about February it goes dark around 5pm, so that would effectively mean no lights, video games, movies/TV, or browsing on work days. And while weed will succeed in knocking me the hell out, it’s illegal for recreational use here, and very difficult to get a medical prescription, so it’s not something I have regular access to. reply willy_k 11 hours agorootparentprevDo you still get noticeable REM sleep with the THC? In my own experience, albiet with higher dosage and smoked so significantly shorter half life, is that I have subjectively instant sleep latency but I do not dream (unless I take magnesium, in which case I have anxiety-type dreams that I can remember very brief flashed of). Do you notice anything in the morning, like maybe a dull sort of haze? reply vundercind 5 hours agorootparentThey work great for me, good sleep quality and no grogginess. I’d tried prescription sleep aids in the past and did have issues with grogginess. Between that, needing timing to be pretty precise for the grogginess not to be even worse, and having to choose between “glass of wine” and “be able to sleep”, those had enough down-sides that it didn’t really work out (the “still feel shitty in the morning” bit was definitely the worst part) reply MengerSponge 17 hours agorootparentprevDo you drink any caffeine? reply deergomoo 16 hours agorootparentUsually just one cup of coffee on weekday mornings, but in general caffeine does not seem to perk me up, keep me awake, or have any noticeable effect besides jump starting my digestive system. I could (and have done, in the past) have a coffee at 2am and go to sleep around 3 no problem. In my adult life I have ranged between 5-6 cups a day when I used to go into the office and none at all during periods where I just fell out of the habit of making any. reply noduerme 13 hours agorootparent>> besides jump starting my digestive system lol. Same here. Keeping your digestive system on track with your sleep cycle deserves a whole separate discussion. For those of us who've discovered coffee and cigarettes, usually the sight of one or the other is enough to get ya goin' reply deergomoo 5 hours agorootparentMy buddy refers to it as “the strike” heh. First coffee (or cigarette for him) of the day, you better be near a bathroom. reply colechristensen 15 hours agorootparentprevI’ve been in this situation and the problem is basically that the various internal clocks aren’t getting good quality information to keep them in sync. Light, food, and exercise and the timing of them have a big impact. reply pyeri 16 hours agoparentprevAfter I quit my day job and started freelancing, I actually saw my sleep schedule worsen. It partly had to do with the initial lack of projects and clients, and other challenges of freelancing itself, but what I observed was that my day job's grind or regular schedule also helped my sleep schedule. However, I eventually overcame those challenges and created my own schedule to work with, and my sleep cycle became better then onwards. reply coolandsmartrr 15 hours agorootparentHow did you fill your schedules despite your initial lack of projects? reply fastball 13 hours agoprevMy problem with regular sleep is that the sum of \"good night sleep\" + \"energy for the day\" is > 24 hours. Realistically I think it is about 26 hours on average. If I sleep for a normal amount (7-8 hours) I will generally have about 18 hours of energy. So if I just sleep when I'm tired my schedule is constantly shifting. If I go to bed when not tired I just stair at the ceiling for hours, which feels like a waste. reply seer 12 hours agoparentYou might need stronger environmental queues for your body to wake up. The body has both internal and external data to know when you have to wake up - lights, noise levels etc. If you set up your bedroom “too well” - quiet, light blocking roller blinds etc, then your body can only rely on the internal clock. I used to have that but since moving to another place without those “niceties” suddenly my body quite easily finds “the correct time” every day. Also you can experiment with this on long flights to get rid of jet lag. After I land, if I spend the first night drinking and fall asleep, I effectively ruin my internal clock for the night, and then the body has only the environmental queues. Wake up in the morning and my clock is effectively reset. Might not feel great for the day but suffer zero jet lag as I start waking up in the morning at the “correct” time even though I’ve flown halfway across the planet. reply beautron 13 hours agoparentprevThis matches my experience. I think I have a 25 hour circadian rhythm, which has me always wanting to stay up one hour later than the night before. reply neom 19 hours agoprevInteresting, I have a DEC2 mutation and don't need much sleep, but I do sleep very consistently. When I was first working through this discovery, I asked the dr how it was that people who have this have no adverse effects when folks always say not sleeping long enough is bad for your health and he said \"well, we don't really know that's true\" and kinda shrugged it off. reply magicalhippo 19 hours agoparentI recall watching a documentary on Discovery way back, where they followed some multiday ultramaraton competitors. They'd draw blood samples, measure vitals and have them answer mental acuity questionnaires each time they stopped for food or sleep. The competitors limited themselves to power naps, max 30 minutes though often just ten minutes. The scientist found that all the physical adverse effects of sleep deprivation were negated by the short power naps. Though IIRC the mental acuity did drop some as the event progressed. Been at least 15 years since I saw the program so might misrember some parts, but I clearly recall the physical effects of the power naps. Keep in mind these were quite fit athletes though. reply fsckboy 18 hours agorootparent>The scientist found that all the physical adverse effects of sleep deprivation were negated by the short power naps. we don't even know if the \"brain cleaning so, longterm, you don't get dementia\" hypothesis is true, and here you/they are saying \"we know it's false for these people\". nah, fam. reply magicalhippo 18 hours agorootparentBy physical I meant primarily the body. As I mentioned the cofnitive fuctions did start to suffer somewhat. Though they had quite short naps. reply jorvi 17 hours agorootparentprev> The scientist found that all the physical adverse effects of sleep deprivation were negated by the short power naps. Though IIRC the mental acuity did drop some as the event progressed. I assume you mixed these up? There's two well-known extreme sleep schedules (Uberman 6x20m or Dymaxion 4x30m) that let you subsist on two hours of sleep a day, because you drop into REM sleep immediately. This however only clears up \"brain fog\". If you would do exercise on these sleep schedules, you would make little progress because muscle repair happens outside of REM sleep. reply magicalhippo 17 hours agorootparentPretty sure I did not. But it was related to blood pressure, stress markers in blood and such as I recall it. reply makeitdouble 18 hours agorootparentprev> power nap I assume they weren't drinking coffee at every stop, do you remember what they did before napping ? (the \"power\" part) reply magicalhippo 18 hours agorootparentPower nap meaning getting up after 10-15 minutes, 30 minutes max. The idea being that after 30 minutes other processes in the body kick in, so you feel much worse if you woke up after an hour. That said, they usually did get some food, a bar or some hot stew IIRC. Though can't recall if they did that before or after the nap. reply foota 18 hours agoparentprevI realize this is far off, but I'm hoping that someday I can get gene therapy to get this. reply styyyaaa 16 hours agorootparentThe assumption is what you do when awake those extra hours is more productive (or fun of whatever you are aiming for...) than asleep. Sleeping you has the disadvantage it cannot keep score. Wake you has \"I completed X today and Y yesterday\". Advocate for sleeping you! reply mklepaczewski 7 hours agoparentprevDo you work out? How often, how long and what type of exercises do you do? reply neom 44 minutes agorootparent5 days a week, Velodrome 1 hour high intensity. 49/14 + :30 each way to work. I'm tall and skinny. reply swyx 18 hours agoparentprevwhen you say dont need much sleep, are we talking 6 hours or something more out thre like 4 hours or less? reply neom 18 hours agorootparentI go to sleep between 1am and 1:30am naturally and wake up around 6/:30am naturally. Allergy season makes me kinda lethargic, but doesn't change how much sleep I need. 4/5hrs is ok, less than 4 is not ok. I also oddly don't get jet lag. reply swyx 18 hours agorootparentits a blessing. i think i survive well on 4-5hrs (incl wake up naturally) but def feel suboptimal compared to 7-8hrs. reply exe34 19 hours agoparentprevalso things are recommended for the average population, not the individual - with \"conditions\"/superpowers like yours, it's absolutely the individual that matters, not the mean. reply computator 15 hours agoprevWhenever I see a study about something that affects longevity, I want to know how much of a difference it makes expressed as a number I can relate to. If you were able to switch from a highly irregular sleep schedule to a very regular schedule, would you live 18 hours longer on average, or 1.5 months, or 5 years? This would be a way to decide how much attention and effort one should to devote to the numerous studies about things that affect mortality. reply alister 15 hours agoparentLooking at the graph in the study, it looks like 0.972 fraction of the very regular sleepers are alive after 7.8 years and 0.945 of the highly irregular sleepers. The difference is 0.027, or in other words 2.7% more of the highly irregular sleepers have died off after 7.8 years. It might be significant in the statistical sense, but it looks like a pretty small difference to me. I don't know to translate that into a statement like: If you were able to switch from a highly irregular sleep schedule to a very regular schedule, you would live __(x days)__ longer on average. With some hand-wavy reasoning I arrived at something like 10 days longer over a period of 10 years. I.e., a very small amount on average. I'd welcome someone with a statistics background to do a real calculation. reply julianeon 15 hours agoparentprevA confounding factor example would be someone who sleeps regularly and someone who sleeps irregularly, two people who both live the same amount of years... but the irregular sleeper lives their last 10 years with greatly impaired capacity after a stroke. [Note: some of my work involves dealing with elderly people and \"impaired after a stroke\" is an extremely real, and common, thing.] These 'fuzzy' conclusions may be the best we can do. reply snitzr 17 hours agoprevSleep is like drinking water. No one says, \"I'm gonna get dehydrated right now and then drink a lot more water later so it's OK.\" But people do this for sleep. You need to sleep right when you're tired, because being tired is a stress that will need more time to heal the longer it goes. Sleep timing is underrated but it is just as important as quality and amount. reply OJFord 16 hours agoparent> No one says, \"I'm gonna get dehydrated right now and then drink a lot more water later so it's OK.\" Err yeah they do? That's 'a night out', 'going out for drinks', 'night on the town', etc. Sort of get the point, but not a brilliant analogy ;) reply xeromal 13 hours agorootparentI think the analogy holds if you consider all these behaviors as negative behaviors which they are. But you're right, people do say it so he's wrong. You could rephrase it as going on a bender and not eating/drinking as healthy is probably just as bad as lack of regular sleep. I've walked myself into a circle. You're right reply Uncouple4063 16 hours agoparentprev> Sleep regularity was a stronger predictor of all-cause mortality than sleep duration, by comparing equivalent mortality models, and by comparing nested SRI-mortality models with and without sleep duration (p = 0.14–0.20). These findings indicate that sleep regularity is an important predictor of mortality risk and is a stronger predictor than sleep duration. Not totally disagreeing with you, but this indicates that it is _more_ important than the latter two when concerned with all-cause mortality. reply ChrisArchitect 17 hours agoprevRelated: How to Train Yourself to Go to Sleep Earlier https://news.ycombinator.com/item?id=42016904 reply matwood 19 hours agoprevOne of the best things I ever did was start going to bed and waking up at roughly the same time every day regardless of weekend or weekday. reply hackernewds 18 hours agoparentin what ways was it one of the best things? reply matwood 16 hours agorootparentI wake up feeling rested everyday around the same time without an alarm. I also go to sleep fairly easily. The yo yo of late weekends and spending part of the week trying to get back to an earlier time left me tired much of the time. reply xeromal 13 hours agorootparentprevIrregular weekend sleeping seems to have a knockon effect of hurting weekday sleeping. Just do it the same every time. The weekend shouldn't be different reply FuckButtons 19 hours agoprevAh, fuck. reply hinkley 10 hours agoparentMy reaction was slightly more humorous: Baymax: Oh no. reply CoastalCoder 17 hours agoparentprevYou beat me to it. reply colordrops 19 hours agoparentprevMy exact same sentiment. My day got a little darker after reading this. reply hypeatei 19 hours agoprevDoes anyone else consistently get 6 to 7 hours of sleep no matter what? It doesn't feel optimal and I feel great on the rare days I get 8 hours, but I can function just fine on less. Also cannot easily fall back asleep if woken up which is really frustrating. reply almost_usual 17 hours agoparentThis used to be a problem for me until I stopped drinking alcohol. I used to think it was related to caffeine or work stress but cutting out drinking seems to have fixed all my sleep issues. reply Toorkit 19 hours agoparentprevOn my days off, I can sleep 12 hours if I don't set an alarm. And I could easily do more, if I didn't force myself out of bed. reply derwiki 16 hours agorootparentAmazing! Without Advil PM I can’t sleep more than 7. Really goes to show how diverse sleep habits/requirements are from person to person. reply Toorkit 16 hours agorootparentHah, I blame my job. Different shift every few days, always tired. I'd love to try a fixed sleep schedule, but sometimes I don't get home until 2am, other times I have to get up at 7:30am. reply xpl 17 hours agoparentprev> Does anyone else consistently get 6 to 7 hours of sleep no matter what 6 hours is too little IMO. If I sleep 6 hours for more than a few days in a row, I feel like shit and need caffeine to wake up properly. Then I end up sleeping 10-12 hours on my days off to compensate (and to take a break from caffeine). It doesn't feel healthy. So, I try to get at least 8 hours consistently. That way, I don't need caffeine at all and function just about fine. reply acidburnNSA 18 hours agoparentprevI'm this 100%. Waking up at 5am on a Saturday is such a pain because my brain just won't let me go back to sleep. reply rtsil 18 hours agoparentprevI'm a zombie if I have less than 8 hours, that's the absolute minimum. I drank a lot of coffee for years even though that had diminishing returns, until I realized it just concealed my exhaustion instead of helping, so I stopped. Of course, spending 12-14 hours a day facing brights screens do not help. reply xeromal 13 hours agoparentprevI've found that 7 is about ideal for me as a 35 year old male. Anything more or less seems to cause more issues reply jambutters 15 hours agoparentprevSame, let me know if you find a solution. My brain just runs wild if I ever wake up leading me to not fall back asleep reply p1esk 18 hours agoparentprevI feel like shit any time I get less than 8 hours of sleep. Usually I get 9 or 10. However a day nap of 45-60 min does help a lot when I’m at a deficit. reply AmVess 18 hours agoparentprevI usually get 6 hours dead on. Rarely 7 or more. I'm asleep less than a minute after going to bed, and have such a regular sleep duration that I never have to set an alarm. In fact, I haven't used an alarm clock at all in 20+ years. Well perhaps, less than a dozen times. I am fully awake and ready to go a few minutes after waking up, too. No coffee needed. I do occasionally take an hour nap here and there, but that is also rare. reply nomilk 15 hours agoprevCurious to learn which lifestyle factors correlate with extremely regular sleep patterns. Of my friends who have very regular sleep patterns, they tend to be very stable, long termists, career focussed, good in relationships, at least moderately sociable, fit and healthy (although in no way obsessed) and well rounded. Kinda good at everything but not extreme in any way. I'd guess those factors alone would have a noticeable effect on longevity. reply flemhans 13 hours agoprevI sleep when I feel tired and wake up when my body naturally wakes up (never an alarm). However it results in irregularity, although somewhat aligning with a 28-hour cycle. I wonder if that's good or bad. reply princearthur 12 hours agoparentIf you can't help it, this is called Non-24-Hour Sleep-Wake Disorder (N24SWD) [1]. If it's just because you're not exposed to natural light a whole lot, circadian drift is \"normal,\" though my recollection is that ~25 hours is more common. I guess it gets diagnosed if and when you complain to a doctor that you can't help it. [1] https://en.wikipedia.org/wiki/Non-24-hour_sleep–wake_disorde... reply andsoitis 17 hours agoprevMy Oura ring establishes a consistent sleep rhythm and nudges me when it is approaching. reply styyyaaa 15 hours agoparentI read that with a lower case O at first and was intrigued! reply 4b11b4 16 hours agoprevJust consider yourself to not live long, that is the reality anyway reply EVa5I7bHFq9mnYK 15 hours agoprevWhen I was younger I went to clubs every Friday and Saturday, dancing till morning, then continuing the exercises at home. Guess that takes a toll. reply cryptozeus 16 hours agoprevOh man I took my sleep for granted last month, have been working late and waking up at strange times. Sometimes sleep at 3 am and wake up at 7am then sleep at 11pm and wake up at 6am. Eventually my body totally crashed, interestingly I had a panic attack and heart attack type symptoms at 2am in the morning. Had to rush to ER. reply codr7 18 hours agoprevWell my sleep is certainly regularly fucked up, so I guess that means I'm dying eventually. reply syspec 14 hours agoprevWhat about people with very young children? Like infants? reply left-struck 13 hours agoparentWell obviously they would have severely impacted sleep? What are you trying to imply? reply xeromal 13 hours agoparentprevConsider it a sacrifice. The cause being noble doesn't make it any healthier reply dghughes 15 hours agoprevCat owners are screwed. reply worstspotgain 19 hours agoprevI wish some of the sleeping/eating studies covered the options \"sleep when I'm tired\" and \"eat when I'm hungry.\" When remote work is an option, it'd be nice to know the health opportunity cost of RTO. Sadly the cohort is too hard to study outside of nursing home residents. reply fatnoah 19 hours agoprevGreat, another source of anxiety to make it harder to maintain a consistent sleep schedule. reply __turbobrew__ 16 hours agoparentI know this will not help you, but stressing about sleep is a vicious cycle which does not help. I used to be much more anxious about sleep — and life in general — but I have leaned that if I don’t sleep that is fine. Life goes on, the body gets what the body needs, and I should focus on living instead of sleeping. Being active, spending time with friends, going outside for walks, laughing, and taking the world as it comes to me has done more for my sleep than reading studies or buying stuff (outside of a tempurpedic matress and solid oak frame). reply dakiol 19 hours agoparentprevAgree. Every month there’s some new “discovery” about what can make your life shorter. It makes me anxious and definitely doesn’t help at all. I cannot simply manage so many variables. I try to do a little bit of everything, to be in the middle point of everything as much as I can; that’s my strategy. reply guerrilla 11 hours agorootparentI think this is a framing problem. You need to let go of the outcome and focus on what you can do. You can't control the result, but you can do the best that you're capsble of in your current situation, which includes not over-extending yourself. reply benbojangles 16 hours agoprevAs somebody with Non-24 sleep wake disorder i concur. reply throw7 16 hours agoprevvery cool that dst has been killing us all this time. reply kiwijamo 15 hours agoparentI live in an area that observes DST and since most devices does the changeover automatically I've never noticed the switch. Only notice when other people start moaning about it. Wonder if it affects certain people more than others. reply left-struck 13 hours agorootparentIt deeply affects me. I’ve lived in places that have it and currently live where there is no DST. If my state implemented it I would try to negotiate with my workplace to keep the same actual hours so my schedule doesn’t change, if they refused I would consider moving, job or state. reply xeromal 13 hours agoparentprevIt would still happen with or without DST. The winter and summer phases of sunlight adjust people's sleep schedules. It's just more dramatic with DST reply m3kw9 15 hours agoprevThere is likely 100 factors that affects longevity and having a 3% better is pretty huge just for one factor reply alliao 16 hours agoprevwhen I worked in UK doing shift work work gave me bonuses and pamphlets on the health costs for working through the graveyard shift regularly. The management initially wanted us to do weekly rotations of 3 shifts we tried for a month and it was undeniably sure fire way to die early. So we decided we'd rather do monthly shifts, it worked mostly ok except heading into winter I didn't get to see the sun for a while. nowadays I still do some kind of on-call hours, and I think being on call should pay a butt load more honestly. reply lucb1e 16 hours agoparentSubtitling: \"Graveyard shift is a work shift running through the late hours of the night through the early hours of the morning, typically from midnight until 8 am.\" I was wondering if you actually did some job on a graveyard at night or if there was a second meaning there. Good I looked that up... reply tayo42 18 hours agoprevAfter having a kid Ive wondered how all these sleep disruption means you'll die studies could actually be true. You'd think evolution would have taken care of it at some point reply devmor 17 hours agoparentAs far as evolution is concerned, you've already had the kid - you passed on your genes. If you survive long enough to make sure your kid reaches maturity, that's all that matters there. Evolution can't select for anything that happens significantly past the birth of your progeny. reply lostlogin 16 hours agorootparent> As far as evolution is concerned, you've already had the kid - you passed on your genes A second child would help with that too. reply xeromal 13 hours agorootparentSure but I doubt lack of sleep effects mortality greatly in childbearing years. Assuming you make it to 40-50, that's it. Now you can die whenever and have succeeded reply georgeburdell 16 hours agorootparentprevAs a parent, you’re underselling the impact of the help of grandparents and siblings on fertility reply rgrieselhuber 16 hours agorootparentprevSure it can. Knowledge, skills, and wisdom passed on to children and grandchildren well into adulthood significantly increases their chances of successfully passing on their genes to future generations, attract quality mates, and reduce stressors that can be passed down through subsequent generations. reply CardenB 17 hours agorootparentprevlong enough to pass on genes also includes living long enough to make sure your child survives, so there is that as well. reply tayo42 17 hours agorootparentprevMaybe, but do other mammals suffer from this? Maybe it woukd have been taken care of with some early human-ish species with shorter life spans or something. reply spazbob 18 hours agoprevThis feels like one of those points that go back to our recent history vs the last 100,000 years of evolution. Before artifical light, alarm clocks, jobs, screens with blue light etc - we presumably woke and slept fairly consistently, no doubt tied partly to sunlight and temperature. Some people probably slept more than others, as they always have, but I wouldn't be surprised if there was a lot more consistency. Disclaimer: I know nothing, just interested reply nojs 17 hours agoprevI think a “correlation is not causation” cautionary note is important here - there are a lot of things that cause fragmented sleep, and also cause death. Stress and underlying disease are two obvious ones. reply sahmeepee 16 hours agoparent\"Finally, we acknowledge the correlational nature of our findings. Sleep regularity may be both a cause and marker of premature mortality risk.\" reply greatpostman 19 hours agoprevThis is probably correlated with so many other cofounding factors, like employment/stress/substance abuse reply slfnflctd 18 hours agoparentRegardless of that reality, circadian rhythms have been extensively studied, and there is more evidence than just this study to support the claim they're making. Patterns and routines are generally beneficial as a rule. That being said, there is a lot of diversity amongst us, and I'm quite sure that when you factor in (epi-)genetic variation - particularly in the short to medium term - there are some unexpected advantage/cost ratios to wildly different strategies. reply yamrzou 19 hours agoparentprev> Results were adjusted for age, sex, ethnicity, and sociodemographic, lifestyle, and health factors. reply anonym29 19 hours agorootparentWhew, glad we covered all 6 of the known possible confounding variables. reply nativeit 19 hours agorootparentYou are willfully taking more away from that than was stated. Such a fallacy is a favorite trope among science denialists, and those who would distort the objectivity of research for misinformed (and/or dishonest) ends. I don’t know what, if any, specific motivations you have, but I think it’s worth pointing out. reply danparsonson 18 hours agorootparentTheir point was, there are other confounding variables, and therefore the parent comment doesn't negate the grandparent comment. I agree, and I am absolutely not a science denialist. reply geysersam 16 hours agorootparentprevWhat do you mean? The commenter just pointed out (in a joking manner) that, even if some confounding factors were taken into account, the result might still be caused by other confounding factors. That's a serious critique, not a fallacy. reply l33t7332273 16 hours agorootparentprevSuch a critique is also a favorite refrain among science professionals reply dmead 14 hours agoprevim checking hacker news in between astrophotography subs. looks like i'm in danger reply wayoverthecloud 17 hours agoprevShit, I do sleep 8 hrs a day but regularity is out of the question. Fuck reply tomcam 15 hours agoprevI am so fucked reply Hashex129542 18 hours agoprev [–] - reply Buttons840 18 hours agoparentPersonal experiences cannot disprove a large scale statistical observation like this. reply Hashex129542 17 hours agorootparenti forgot the rule that I don't post against the wind in internet. So I can avoid arguments with random peoples at online. reply hackernewds 18 hours agoparentprev [–] your anecdata is not data. this almost seems like a troll post reply Hashex129542 17 hours agorootparent [–] i forgot the rule that I don't post against the wind in internet. So I can avoid arguments with random peoples at online. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A study in the journal \"Sleep\" indicates that sleep regularity is a more significant predictor of mortality risk than sleep duration.- Analysis of data from over 60,000 UK Biobank participants showed that consistent sleep patterns reduced all-cause mortality risk by 20-48%.- The research challenges traditional health guidelines that emphasize sleep duration, highlighting the importance of maintaining regular sleep-wake times for better health."
    ],
    "commentSummary": [
      "A study with 60,977 participants found that sleep regularity is a stronger predictor of mortality than sleep duration, suggesting irregular sleep patterns may indicate chronic sleep issues.",
      "The study's short data collection period has led to criticism, with experts calling for longer-term data to better establish the link between sleep regularity and mortality.",
      "Despite its limitations, the study emphasizes the importance of maintaining a regular sleep schedule for overall health."
    ],
    "points": 360,
    "commentCount": 221,
    "retryCount": 0,
    "time": 1730499803
  },
  {
    "id": 42022282,
    "title": "Nvidia to join Dow Jones Industrial Average, replacing Intel",
    "originLink": "https://www.cnbc.com/2024/11/01/nvidia-to-join-dow-jones-industrial-average-replacing-intel.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS VIDEO INVESTING CLUB PRO LIVESTREAM Search quotes, news & videos WATCHLIST SIGN IN TECH Nvidia to join Dow Jones Industrial Average, replacing rival chipmaker Intel PUBLISHED FRI, NOV 1 20245:22 PM EDTUPDATED FRI, NOV 1 20246:01 PM EDT Kif Leswing @KIFLESWING KEY POINTS Nvidia is replacing Intel in the Dow Jones Industrial Average, a shakeup that reflects a massive change in the semiconductor industry. Nvidia shares have gained more than 170% this year, while Intel has lost over half its value. The last change to the index came in February, when fellow tech giant Amazon was added to the Dow. In this article NVDA DOW Follow your favorite stocks CREATE FREE ACCOUNT CEO of Nvidia, Jensen Huang, speaks during the launch of the supercomputer Gefion, where the new AI supercomputer has been established in collaboration with EIFO and NVIDIA at Vilhelm Lauritzen Terminal in Kastrup, Denmark October 23, 2024. Ritzau ScanpixMads Claus RasmussenVia Reuters Nvidia is replacing rival chipmaker Intel in the Dow Jones Industrial Average, a shakeup to the blue-chip index that reflects the boom in artificial intelligence and a major shift in the semiconductor industry. Intel shares were down 1% in extended trading on Friday. Nvidia shares rose 1%. The switch will take place on Nov. 8. Also, Sherwin Williams will replace Dow Inc . in the index, S&P Dow Jones said in a statement. Nvidia shares have climbed over 170% so far in 2024 after jumping roughly 240% last year, as investors have rushed to get a piece of the AI chipmaker. Nvidia's market cap has swelled to $3.3 trillion, second only to Apple among publicly traded companies. Companies including Microsoft, Meta , Google and Amazon are purchasing Nvidia's graphics processing units (GPUs), such as the H100, in massive quantities to build clusters of computers for their AI work. Nvidia's revenue has more than doubled in each of the past five quarters, and has at least tripled in three of them. The company has sginaled that demand for its next-generation AI GPU called Blackwell is \"insane.\" With the addition of Nvidia, four of the six trillion-dollar tech companies are now in the index. The two not in the Dow are Alphabet and Meta. While Nvidia has been soaring, Intel has been slumping. Long the dominant maker of PC chips, Intel has lost market share to Advanced Micro Devices and has made very little headway in AI. Intel shares have fallen by more than half this year as the company struggles with manufacturing challenges and new competition for its central processors. Intel said in a filing this week that the board's audit and finance committee approved cost and capital reduction activities, including lowering head count by 16,500 employees and reducing its real estate footprint. The job cuts were originally announced in August. The Dow contains 30 components and is weighted by the share price of the individual stocks instead of total market value. Nvidia put itself in better position to join the index in May, when the company announced a 10-for-1 stock split. While doing nothing to its market cap, the move slashed the price of each share by 90%, allowing the company to become a part of the Dow without having too heavy a weighting. The switch is the first change to the index since February, when Amazon replaced Walgreens Boots Alliance . Over the years, the Dow has been playing catchup in gaining exposure to the largest technology companies. The stocks in the index are chosen by a committee from S&P Dow Jones Indices. WATCH: Nvidia leaps and bounds ahead of AMD WATCH NOW VIDEO03:17 Nvidia is leaps and bounds ahead of AMD on the AI story, says Susquehanna's Christopher Rolland Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=42022282",
    "commentBody": "Nvidia to join Dow Jones Industrial Average, replacing Intel (cnbc.com)352 points by koolba 20 hours agohidepastfavorite195 comments anonu 7 hours agoThe DJIA is a dinosaur and irrelevant. Its price-weighted because that was easier to calculate than a market-cap weighted fund when the thing was created ~140 years ago. Which means that high dollar value stocks have more weight in the index which doesn't really make sense. But worse, how we talk about the index, even today on radio and TV, in terms of nominal points up or down is completely ridiculous. Just yesterday \"the Dow Jones was up 100 points\". Ok - no reference value, no percentage change: means nothing to me. And we wonder why we are financially illiterate in this country. reply stackghost 5 hours agoparentI agree that nowadays the Dow is somewhat obsolete, but: The DJIA is not meant to be viewed in isolation but in the context of other indexes like DJTA, the idea being that if industrial companies are doing well but the railroads or shipping aren't, it will be reflected in those respective averages diverging, hinting at deeper economic problems. Also they adjust for stock splits now. reply ninth_ant 2 hours agorootparentEven with this objective, using the share prices and not the market cap of the companies involved creates enormous skew. For example UNH and especially GS take up the largest individual components despite not being being amongst the largest companies, giving their share movement disproportionate weight in the “index”. Perhaps it made sense 140 years ago but today it belongs in the rubbish. reply JJMcJ 3 hours agorootparentprevMost news reports also give the percentage change as well as the point difference. That began about 20 years ago. reply bluGill 6 hours agoparentprevThe dow was useful in pre computer times when a reborter could calculate it every few minutes. The s&p500 Is a better indicator but that took hours by hand and so was only done overnight. now that we have computers and index you want can be done in milliseconds and so the dow needs to die. reply makeitdouble 4 hours agorootparentIn this day and age we could skip all of that and report on the actual average of all the stocks in the exchange, instead of only a fraction. reply david-gpu 4 hours agorootparentSP500 represents something like 85% of market capitalization. Adding a few more thousand stocks would make little difference, especially when you consider how heavily correlated they are. Just compare SPY with VT -- nearly the same thing. reply dmoy 48 minutes agorootparentMinor nit: SPY and VTI are 80-85% the same thing VT includes a ton of non-US stocks and is significantly different reply david-gpu 30 minutes agorootparentThank you. I misremembered. reply devoutsalsa 2 hours agorootparentprevYou just described a total market fund like VTI :) reply e_y_ 48 minutes agorootparentAs I understand it, they don't own every stock. Right now VTI has about 3700 stocks out of the ~4300 on US publicly traded markets (excluding OTC). But it's more representative than the S&P 500 and similar. reply pjfin123 1 hour agoparentprevAnother advantage of a price-weighted stock index is that it's easier to track with a real portfolio because they don't need to rebalance as frequently. To replicate the performance of the DJIA you just have to buy one share of each stock and hold them, you only need to place trades when a company is added or removed from the index. With a market cap weighted index you have to make more frequent trades every time a company does a buyback or issues new shares. reply hanklazard 2 hours agoparentprevre: up XX number of points, I haven't bothered for many years, but that sounds like NPR's \"All Things Considered\" or \"Marketplace\". Always drove me nuts too and does nothing to help people understand financial markets. reply jacobjjacob 1 hour agorootparentEveryone reports it this way. It used to bother me, but I think it does provide some interesting information vs the % day-to-day. Because % is relative to the previous day, number of points isn’t. Personally I prefer the % reply pjfin123 46 minutes agoparentprev> Just yesterday \"the Dow Jones was up 100 points\". Ok - no reference value, no percentage change: means nothing to me. This always really bugged me reply EVa5I7bHFq9mnYK 2 hours agoparentprevMaybe splits and reverse splits weren't a thing back then, so price really meant something? reply manojlds 2 hours agoparentprevHow is the second point anything to do with DJIA and this news? reply KronisLV 2 hours agoprevThe past few years have been pretty unfortunate for Intel, even though I think they could have been a pretty decent player in the market. Their new Core Ultra chips have pretty okay performance and good energy efficiency, their P/E core design seems to make sense, even their entry into the dedicated graphics segment seemed like a good thing for the average consumer. But the 13/14th gen issues were a pretty major hit, I do have an Intel Arc card that I got for a really nice price (cheaper than similar AMD/Nvidia GPUs) but it's not without its own share of issues even with pretty decently developed drivers, people seem to have taken the Core Ultra being a bit of a sidegrade pretty badly and the pricing doesn't always seem all that competitive when it comes to CPUs (even the motherboards seem more expensive when compared to AMD). What a bummer. That said, when it comes to the consumer segment, even AMD doesn't seem to be doing all that well, for example their net revenue for gaming is down 69%: https://ir.amd.com/news-events/press-releases/detail/1224/am... reply insane_dreamer 17 hours agoprevIntel's CEO wanted to buy NVidia 20 years ago. Unfortunately for Intel, the board of directors killed the proposal as \"too expensive\" at $20B. Granted no one could see the future of AI on GPUs, but think of the ROI on that investment. reply ipsum2 17 hours agoparentNvidia would've acquired the culture of Intel, so it wouldn't be the company it was today if that happened. reply BeetleB 2 hours agorootparentIndeed. A friend of a friend of a friend was in the room when the decision was made. The reason for not buying: Huang won't be a team player and is hard to work with. So yes, they would have tried to integrate it into the rest of Intel. reply thijson 13 minutes agorootparentJust look at what happened to Altera under Intel. Preacquisition it was similar in size to Xilinx. Now it's just a shadow of its former self, latest few quarters shows it in the red. The person they have in charge of it. Sandra Rivera was formerly the head of HR. From 2019 to 2021, Rivera was Intel’s chief people officer, leading the company’s Human Resources organization worldwide. I heard a rumor that Jensen wouldn't agree to the acquisition unless he became CEO of the combined entity. reply insane_dreamer 17 hours agorootparentprevNot sure that would have mattered once LLMs emerged. NVidia's success is mostly a matter of being the only major advanced GPU player at the time when LLMs exploded onto the scene. That won't be the case for long. The big players are already heavily investing in high performance AI chips to wean themselves off a dependance on NVidia. reply kiratp 17 hours agorootparentNvidia’s success is due to a decade+ of investment into their software stack (CUDA and friends), built on a world view that more software can and should be massively parallel. They were a key cause for LLMs being a thing in the first place. reply BobbyJo 16 hours agorootparentThis. I worked in HPC between 2010 and 2013, and people were trying to compete with NVidia in the GPGPU space even back then, seeing the way the wind was blowing. Compute bandwidth, and FLOPS/watt has steadily been more and more important the last 15 years, even before AI. NVidia has continued to stay ahead because every alternative to CUDA is half baked trash, even when the silicone make sense. As a tech company, trading $$ for time not spent dealing with compatibility bugs and broken drivers pretty much always makes business sense. reply ghaff 6 hours agorootparentNvidia did a lot of things right. But it sure didn't hurt that the crypto bubble gave way to LLMs with almost perfect timing. Luck favors the prepared and all that but Nvidia also would have had trouble timing market changes better. reply FredPret 48 minutes agorootparentNvidia did ride a perfect storm of gaming + bitcoin + AI. But AMD and others could have done the same, had they been better at riding that wave. There’s a reason it was Nvidia who won out. reply toasterlovin 4 hours agorootparentprevYeah, exactly, it has never been clear that we would find a use for parallel computation outside of niches like graphics and HPC. And in fact most of the history of computing has been evidence of the opposite: useful computation has been predominantly serial in nature. But here we are now. reply neurostimulant 8 hours agorootparentprevWasn't Intel's commercial compiler also dominated HPC space around that time period? I remember back then people would use cuda and icc for hpc stuff. reply rbanffy 7 hours agorootparentI believe their compilers still dominate in CPU-bound workloads. reply otabdeveloper4 11 hours agorootparentprev> half baked trash > dealing with compatibility bugs > broken drivers Describes my experience trying to use CUDA perfectly. We have a long way to go and we haven't even started yet. reply initplus 11 hours agorootparentGiven that experience, think about what state the alternatives must be in! reply disgruntledphd2 10 hours agorootparentTo be fair, Cuda has improved a lot since 2014 or so. I messed up my Linux box multiple times trying to install Cuda but the last time it was just apt install and maybe setting ld library and it all just worked. reply yieldcrv 9 hours agorootparentprevIt’s pretty bad. just when you think you can order AMD chips since there is no shortage, and use a translation layer and have a cheap AI datacenter, it turns out AMD is fumbling the ball at every step of the way reply bottled_poe 4 hours agorootparentIt’s interesting. They have had plenty of time and resources available to mount solid competition. Why haven’t they? Is it a talent hiring problems or some more fundamental problem with their engineering processes? The writing has been on the wall for gpgpu for more than 10 years. Definitely enough time to catch up. reply the__alchemist 2 hours agorootparentprevHave you tried Vulkan Compute or OpenCL? I recommend evaluating this relatively, vice compared to an ideal. reply llm_trw 15 hours agorootparentprevMore than a decade. I was using it as soon as it came out in 2007 and my dinky desktop workstation was out performing the main frame in the basement. reply mullingitover 14 hours agorootparentprev> a world view that more software can and should be massively parallel. Twenty years ago I was thinking we'd be speccing machines in kilocores by now. reply epcoa 14 hours agorootparenthttps://en.wikipedia.org/wiki/Connection_Machine reply Keyframe 10 hours agorootparentprev4090 has 16384 cuda cores, so there's that! reply atq2119 9 hours agorootparentNvidia's marketing is misleading. Those \"cuda cores\" are more SIMD lanes than cores. Number of SMs is a more appropriate equivalent to CPU core count. reply dahart 3 hours agorootparentAre you sure that isn’t what @mullingitover meant? > Number of SMs is a more appropriate equivalent to CPU core count. What do you mean by this? Why should an SM be considered equivalent to a CPU core? An SM can do 128 simultaneous adds and/or multiplies in a single cycle, where a CPU core can do, what, 2 or maybe 4? Obviously depends on the CPU / core / hyperthreading / # math pipelines / etc., but the SM to CPU-core ratio of the number of simultaneous calculation is in the double digits. It’s a tradeoff where the GPU has some restrictions in return for being able to do many multiples more at the same time. If you consider an SM and a CPU equivalent, then the SM’s perf can exceed the CPU core by ~2 orders of magnitude — is that the comparison you want? If you consider a GPU thread lane and a CPU thread lane equivalent, the the GPU thread lane is slower and more restricted. Neither comparison is apples to apples, CPUs and GPUs are made for different workloads, but arguing that an SM is equivalent to a CPU core seems equally or more “misleading” when you’re leaving out the tradeoff. I’d argue that comparing SMs to cores is misleading and that it makes more sense to compare chips is by their thread counts. Or, don’t compare cores at all and just look at the performance in, say, FLOPS. reply Dylan16807 46 minutes agorootparent> An SM can do 128 simultaneous adds and/or multiplies in a single cycle, where a CPU core can do, what, 2 or maybe 4? https://images.nvidia.com/aem-dam/Solutions/geforce/news/rtx... An SM is split into four identical blocks, and I would say each block is roughly equivalent to a CPU core. It has a scheduler, registers, 32 ALUs or FPUs, and some other stuff. A CPU core with two AVX-512 units can do several integer operations plus 32 single-precision operations (including FMA) per cycle. Not 2 or 4. An older CPU with 2-3 AVX2 units could fall slightly behind, but it's pretty close. That doesn't factor in the tensor units, but they're less general purpose, and CPUs usually put such things outside the cores. I would say an SM is roughly equivalent to four CPU cores. reply dahart 25 minutes agorootparentYeah I totally forgot to consider CPU SIMD. Brain fart. The other comment corrected me too, you’re both right. When I said 2 or 4, I was thinking of the SISD math pipe and not AVX instructions. Yes, considering CPU SIMD, maybe comparing a CPU core to a CUDA warp makes some sense in some situations. The peak FLOPS rate is still so much higher on Nvidia though, that the comparison hardly makes sense. So yeah like I and the other commenter mentioned, it depends entirely on what comparison is being made. reply atq2119 3 hours agorootparentprevA single Zen5 core can do 32 single precision FMAs per clock. That's using SIMD, but so is Nvidia for all intents and purposes. Those \"cuda cores\" aren't truly independent: when their execution diverges, masking is used pretty much like you'd do in CPU SIMD. A lot of the control logic is per-SM or perhaps per-SIMD unit -- there are multiple of those per SM. You could perhaps make a case that it's the individual SIMDs which correspond to CPU cores (that makes the flops line up even more closely). It depends on what the goal of the comparison is. reply Keyframe 6 hours agorootparentprevyou're right reply otabdeveloper4 11 hours agorootparentprev128 and 512 cores are normal now, so not that far off. reply TrainedMonkey 9 hours agorootparentprevI think this undersells the story. NVidia's success is built on innovating and scaling for 20 years. Vastly oversimplifying it: - CUDA conception in 2006 to build super computers for scientific computing. - CUDA influencing CPU designs to be dual purpose, with major distinction of RAM amounts (for scientific compute you need a lot more RAM compared to gaming) - Crypto craze driving extreme consumer GPU demand which enabled them to invest heavily into RND and scale up production. - AI workload explosion arriving right as the crypto demand was dying down. - Consistently great execution, or at least not making any major blunders, during all of the above. reply no_wizard 1 hour agorootparentIt’s more no major blunders + no real major competition, they have not consistently “executed great”, it’s dumb luck + no extremely stupid decisions. It doesn’t mean they didn’t make a bunch of mistakes its that when they did there was no competition to to realistically turn towards, and they fixed a lot of their mistakes. reply deelowe 16 hours agorootparentprevAs someone who's intimately familiar with both cultures, I'm convinced Intel would have killed any innovation Nvidia had going for it before it had a chance to really take off. Management at the two could not be more opposite if they tried. reply grogenaut 16 hours agorootparentprevIntels 3rd wave management by MBA would have ruined nvidia way back then forcing them into something dumb and blocked all of the R&D they did since since that's what they also did at intel. reply astrange 13 hours agorootparentprevOther companies are capable of making big GPUs; they aren't the only TSMC customer. Intel themselves have perfectly fine GPUs. Their issue is that their management never allocates enough space in their chips to let them perform well. Nvidia's advantage is that they have by far the most complete programming ecosystem for them. (Also honestly… they're a meme stock.) reply DeathArrow 11 hours agorootparentOther companies don't have CUDA. reply KaoruAoiShiho 16 hours agorootparentprevThis seems to have cause and effect backwards. LLMs emerged because of Nvidia. reply dagmx 16 hours agorootparentLLMs absolutely did not emerge because of NVIDIA. you’re the one imho who is mistaking correlation with causality. The first transformer models were developed at Google. NVIDIA were the card du jour for accelerating it in the years since and have contributed research too, but your statement goes way too far reply llm_trw 15 hours agorootparentI was around the space before Alexnet came out. Without NVidia the last 15 years of AI would not have happened. Deep learning was only possible because you could do it on NVidia cards with Cuda without having to use the big machines in the basement. Trying to convince anyone that neural networks could be useful in 2009 was impossible - I got a grant declined and my PhD supervisor told me to drop the useless tech and focus on something better like support vector machines. reply aledalgrande 2 hours agorootparentI was not a PhD, but I studied both around that period in university and can confirm that neural networks were only seen as a kind of theoretical plaything. All the libraries were pushing SVMs. reply dagmx 15 hours agorootparentprevI was around then too and people forget that AMD also had GPU compute APIs. The difference is AMD killed theirs to use OpenCL and NVIDIA kept CUDA around as well. reply llm_trw 15 hours agorootparentJust like how ROCm is supposed to be competitive today and it isn't unless you have an army of grad students to look after your data center cards. I tried using AMD Stream and it lacked documentation, debugging information and most of the tools needed to get anything done without a large team of experts. NVidia by comparison could - and was - used by single grad students on their franken-stations which we build out of gaming GPUs. The less we talk about the disaster that the move to opencl was the better. reply dagmx 15 hours agorootparentI agree ROCm was a mess and only recently is even really usable. That said ROCm is quite a recent thing borne out of acknowledging that OpenCL 2 was a disaster. OpenCL 1 had a reasonable shot and was gaining adoption but 2 scuppered it. reply llm_trw 13 hours agorootparentDid you ever train anything past mnist on AMD? I try every five years or so and promise myself never again each and every time. reply dagmx 12 hours agorootparentNot with Rocm since I’ve moved my personal stack to NVIDIA (for rendering) and Macs for day to day use. I did write quite a bit of OpenCL prior to that on Intel/AMd/NVIDIA, both for training and for general rendering though, and did some work with Stream before then. reply llm_trw 12 hours agorootparentWas it OpenCL1? That's the only one I hadn't tried out for AMD GPUs. Everything else I have and can say with absolute certainty that you spend more time fighting the hardware than you did writing code. Cuda by comparison JustWorks^tm. reply dagmx 12 hours agorootparentBoth 1 and 2. I haven’t done much with 3 as OpenCL is effectively a dead api at this point. 1 was definitely a lot easier to work with than 2. CUDA is easier than both but I don’t think I hit anything I could do in CUDA that I couldn’t do in OpenCL, though CUDA of course had a larger ecosystem of existing libraries. reply llm_trw 11 hours agorootparentDamn, I missed the best one. Thanks for the trip down memory lane. reply BobbyJo 16 hours agorootparentprevI think their point is that easily accessible hardware capable of supporting the research is the reason the research has come as far as it has, and I would tend to agree. At the very least, GPUs keeping the PCI ecosystem going has played a major role in allowing the specialized accelerator market to flourish. reply dagmx 16 hours agorootparentBut that could apply to any of the GPU manufacturers. CUDA made for an easier ecosystem but if it didn’t exist it would have been any of the other APIs. The first transformer models didn’t even use CUDA and CUDA didn’t have mass ecosystem inroads till years later. I’m not trying to downplay NVIDIA but they specifically mentioned cause and effect, and then said it was because of NVIDIA. reply KaoruAoiShiho 16 hours agorootparentFirst transformer != LLM. Imagine a world where you had to use AMD or CPUs, there would be no AlexNet, there would be no LLMs. Nvidia seeded universities with gifted hardware accelerators for a over a decade. Nvidia built the foundations for modern ML on which transformer lies, it's just one step on a long road to LLMs. reply dagmx 15 hours agorootparentAMD had GPU compute APIs at the time as well. They also used to contribute GPUs (albeit in much smaller quantities). They just ended up killing them in favor of OpenCL which then withered on the vine. NVIDIA absolutely contributed to the foundation but they are not the foundation alone. Alexnet was great research but they could have done the same on other vendors at the time too. The hardware didn’t exist in a vacuum. reply physicsguy 13 hours agorootparentOpenCL itself is kinda fine, but the libraries never existed because every academic from postdoc up could get a free NVidia card. You literally filled a form out and they sent you it. reply dagmx 12 hours agorootparentI agree the library situation for OpenCL never stood up. Khronos tried at the start but then lost steam. It’s one of the projects I think the Khronos group mishandled the most unfortunately. reply DeathArrow 10 hours agorootparentDo they have a project that achieved success? reply dagmx 4 hours agorootparentHere’s a list of projects https://www.khronos.org/ If I were to categorize the successful ones: glTF, KTX, SPIR-V, OpenGL and its variants, WebGL People will say Vulkan but it has the same level of adoption as OpenCL, and has the same issue that it competes against vendor specific APIs (DX and Metal) that are just better to use. It’s still used though of course as a translation target but imho that doesn’t qualify it as a success. OpenCL was and is a failure of grand magnitude. As was colada. reply echoangle 10 hours agorootparentprevOpenGL, Vulkan, WebGL? reply KaoruAoiShiho 14 hours agorootparentprevAlexnet could not have been done on AMD, nope. reply dagmx 14 hours agorootparentPlease, this is just revisionism. There’s nothing inherent to AlexNet that relied on NVIDIA hardware or even software. It was just what happened to be available and most straightforward at the time. To say it wouldn’t have been possible on AMD is ludicrous and there is a pattern to your comments where you dismiss any other companies efforts or capabilities, but are quite happy to lay all the laurels on NVIDIA. The reality is that multiple companies and individuals got us to where we are, and multiple products could have done the same. That's not to take away from NVIDIA's success, it's well earned, but if you took them out of the equation, there's nothing that would have prevented the tech existing. reply hnlmorg 10 hours agorootparentThe crux of the issue is this: > It was just what happened to be available and most straightforward at the time AMD made better hardware for a while and people wanted OpenCL to succeed. The reason why nvidia became dominant was because their competitors simply weren’t good enough for general purpose parallel compute. Would AI still have happened without CUDA? Almost certainly. However nvidia still had a massive role in shaping what it looks like today. reply EVa5I7bHFq9mnYK 2 hours agorootparentprevMany people here are heavily invested in NVDA stock, which certainly clouds the judgement. reply BobbyJo 14 hours agorootparentprev> The first transformer models didn’t even use CUDA and CUDA didn’t have mass ecosystem inroads till years later. I graduated college in 2010 and I took a class taught in CUDA before graduating. CUDA was a primary driver of NN research at the time. Sure, other tools were available, but CUDA allowed people to build and distribute actually useful software which further encouraged the space. Could things have happened without it? Yeah, for sure, but it would have taken a good deal longer. reply daedrdev 16 hours agorootparentprevSurely intel would have killed off CUDA reply Shorel 9 hours agorootparentprevYou are completely ignoring the impacts of Covid and GPU cryptocurrency mining on Nvidia profits. It could have been AMD/ATI profiting from such random events, as they were the ones that financed AI development. reply EVa5I7bHFq9mnYK 2 hours agorootparentBTW, when GPUs were used for Bitcoin mining (up until year 2013, obsoleted by spacialized chips after that), AMD chips were used exclusively, because they had much better integer math performance, compared to Nvidia cards, which focused on floating point performance. reply DeathArrow 11 hours agorootparentprevNvidia will make general purpose GPUs while the AI players will make ASICS. I guess most data center will prefer GPUs so customers can run whatever AI model they need. reply kylebenzle 2 hours agorootparentprevAnd when the LLM hype dies NVDA will be the hardest to fall. reply albertop 13 hours agorootparentprev100% on target. The Intel culture was a big contributor to Intel fall. reply princearthur 4 hours agorootparentprevI doubt they would have actually merged operations. The NIH at Intel is so strong that it would have virtually annihilated it otherwise. reply hinkley 12 hours agorootparentprevAnd the danger of Nvidia buying Intel is the same. See also MD/Boeing reply DeathArrow 11 hours agorootparentNvidia buying Intel would mean some damn fast CPUs. I'd like to see that. reply homebrewer 10 hours agorootparentAnd most likely the death of Intel being one of the largest contributors to the Linux kernel. reply ipsum2 11 hours agorootparentprevNvidia buying Intel would be doing Intel a great service. reply insane_dreamer 4 hours agorootparentDoes nvidia contribute anything to OSS? Its drivers are closed, that much I know. Whereas intel is the top corp contributor to the Linux kernel reply DeathArrow 11 hours agorootparentprevCan you expand on what is the Intel culture and why it would hurt growth? reply pixelpoet 11 hours agorootparentSome of my favourite reading on this topic is Matt Pharr's retrospective on ISPC: https://pharr.org/matt/blog/2018/04/30/ispc-all reply toasterlovin 4 hours agorootparentSecond this. A great read in general and, to the extent that it’s accurate, very instructive about dysfunction at Intel. reply metadat 15 hours agoparentprevAnd if the Intel board jumped on every expensive acquisition novelty that came to the table, how does that go? Everyday I look at stocks and wish I'd known to buy the ones that went up today. Overall, pretty uninteresting and uninsightful, unless you have Doc Brown's DeLorean with the Mr. Fusion upgrade (BTTF II). Even then, would it actually be good if you built your own reality to such an extent? I'm sure life's weird for the UHNW's, unclear if it's actually better. We're all still stuck on the same planet, our kids and kid's kids will all face the same struggles. Even still, today is a pretty interesting temporal location to occupy! reply mrpippy 15 hours agorootparentNot gonna say they jumped on every one, but…McAfee, Havok(!), Infineon’s wireless business, Mobileye, etc reply metadat 15 hours agorootparentYeah, McAfee.. https://youtube.com/watch?v=bKgf5PaBzyg metadat slowly saunters off into the nearby foggy embankment, disappearing, Homer Simpson style reply lotsofpulp 6 hours agorootparentprev> Everyday I look at stocks and wish I'd known to buy the ones that went up today. You can’t compare yourself to the board of the premiere microchip manufacturing company (at the time). They should have more information than you and they are paid to be making more informed decisions (obviously they can be wrong too). reply asah 4 hours agoparentprevEvery big techco was the target of numerous buyout offers along the way. Yahoo! tried to buy Google and Facebook got any number of offers. It makes sense when the growing company doesn't have a path forward (e.g. YouTube's bandwidth costs) or the price is truly crazy (e.g. WhatsApp). It's not clear to me why Instagram sold out to Facebook for $1B. reply wellthisisgreat 12 minutes agoparentprevAI wouldn’t have had a future in its current form. Or maybe Intel would be partitioning off Nvidia like the memory unit. reply DeathArrow 11 hours agoparentprevBefore AI Nvidia made some nice money from the crypto craze. Also, during pandemics and after, but before AI, there was a severe shortage of Nvidia GPUs. reply trhway 9 hours agorootparent>during pandemics and after, but before AI, there was a severe shortage of Nvidia GPUs. you'd think that such multi-year shortage of a product would be used as an opportunity by other players to jump in and make great money. Yet that major machinery of capitalism is failing here. reply Applejinx 7 hours agorootparentImplies that 'that major machinery of capitalism' is an assumption. If it doesn't happen even under optimal conditions, you've learned something about the assumptions you've been taught as axioms. reply throwaway48476 9 hours agoparentprevAMD at one point wanted to buy Nvidia too. reply Shorel 9 hours agoparentprevIntel involvement would have killed any initiative that would lead to what Nvidia is now. So, the ROI would have been much, much lower. reply bogwog 16 hours agoparentprevImagine where we'd (not) be if that happened. Acquisitions/consolidation kill innovation. reply Animats 17 hours agoprevThe \"Dow Jones Industrial Index\" isn't that industrial any more. Six of the companies are financial. Only about half actually run factories. reply dehrmann 12 hours agoparentAnd MTV doesn't play music. The DJIA is curated slice of the American economy with a goofy weighting. reply wodenokoto 4 hours agorootparentFor what it's worth, they have MTV MEA running in my local gym and it plays nothing but music all the time. reply nextworddev 18 hours agoprevThis is how the dump begins. Onto the index huggers and pension funds reply khuey 18 hours agoparentMajor index funds don't follow the DJIA. $DIA has 5% of $SPY's assets under management. reply bhouston 17 hours agorootparentDIA has ~$35.62 billion, IYY has ~$2.5B in the DJIA. There are only 30 companies in it (~$12T total value), so it averages $1.1B per company. But the S&P 500 index funds has somewhere around ~$2.5T invested, but that is spread across 500 companies (~$40T total valuation), so it averages $5B per company. Thus you are correct the S&P 500 is more influential in terms of index fund reallocations, but only roughly 5x more influential. reply tripletao 12 hours agorootparentThe S&P 500 is cap-weighted, so the average value per company is meaningless. Nvidia is currently 7% of the index, while the smallest member is 0.01%. Since Nvidia was already a part of the S&P 500 (and other similar indices) prior to its big run, those index investors generally profited from its rise. New flows into those funds do help prop it up, though. The DJIA is a weird historical relic, and there's little reason for anyone to buy a fund tracking it. It's possible that those who did anyways will end up holding a tiny fraction of the bag due to this change, but it's not a big effect. reply staticman2 14 hours agorootparentprevThe dow has nothing to do with index funds. The dow industrial average is price weighted. The s&p 500 is market weighted. No competent index would follow the dow- it doesn't even make sense conceptually, it has zero relation to the economics of the companies when you use price weighting, and 30 companies is a stupid low number. There is, as far as I can tell, zero point to the dow, it's a completely useless tracker that is reported on because people talk about it because it's reported on. reply dkrich 12 hours agorootparentThis may make sense in theory but in reality is wrong. The DJIA has historically had a very close correlation to the S&P. Plus there is an inescapable psychological aspect to the Dow that is unique. When the Dow is off 1500 points it hits much differently than the S&P falling 200 points. reply dubcanada 6 hours agorootparentI have a feeling that is because every company in the DJIA is also in S&P, and most are very heavily weighted in S&P. Rather then companies within the DJIA doing as good as companies in the S&P. reply bhouston 13 hours agorootparentprev> The dow has nothing to do with index funds I am not sure what point you’re making but there is ~$38B invested in index funds (the ones I mention in the previous post) that track the DJIA. Granted it is a fraction of the index funds which track the S&P 500. reply sooheon 16 hours agorootparentprevPut another way, IYY and DIA combined holdDJAI ? reply MengerSponge 16 hours agoparentOnly if they can make it French too reply jameslk 19 hours agoprevDJIA is not taken very seriously outside of mainstream media it seems. The news loves to quote big drops or increases to the DJIA due to the index being price weighted. “Wow the news is saying the DJIA dropped 1000 points today! That’s huge.” Nah, that’s like 2% This seems like another media opportunity about a nothing burger event for an index that has lost relevance, when other indices like the S&P 500 and QQQ have already incorporated NVIDIA a while ago. They’re just playing catch up. reply stocknoob 18 hours agoparent100%. It’s a good example of how the media wants to entertain, not inform. The DJIA is a price-weighted index and doesn’t track dividends, yet is somehow supposed to reflect investor sentiment. reply JumpCrisscross 2 hours agorootparent> It’s a good example of how the media wants to entertain, not inform It’s a good example of the media being split into two populations. The free media, which is filler for ads. And media one pays for. The financial press isn’t really headlining this story; it’s on CNBC. reply bradleyjg 18 hours agorootparentprevthe media wants to entertain, not inform The media wants to not go bankrupt. It’s the customers that decide whether and how that’s possible. reply thimabi 18 hours agorootparent> The media wants to not go bankrupt. I’d say that depends. Many media organizations operate without profits — or even incurring losses — in order to serve the public good, promote an ideology, engage in activism, spread misinformation, etc. reply hackernewds 17 hours agorootparentspread misinformation? which major media operations are operating at a loss how for the public good (discounting state run) and how? reply thimabi 16 hours agorootparent> spread misinformation? Yes, several state-run and private media organizations operate without profits or at a loss in order to spread misinformation. > which major media operations are operating at a loss how for the public good (discounting state run) and how? Sorry, I was unable to properly understand your question. In any case, I never said the organizations that are not primarily concerned with chasing profits are “major” ones. I believe that applies best to lesser-well-known entities, like those who live on social media or the blogosphere. Though there are some instances of major state-run companies, even on TV and on the radio, that operate in a similar fashion. Also, I am surely not equating the spread of misinformation with serving the public good — these are just distinct objectives that may be sought by media organizations, rather than avoiding bankruptcy. reply pessimizer 16 hours agorootparentprevThe Guardian has turned a profit in maybe 4 of the last 30 years. reply dredmorbius 16 hours agorootparentprevAs one example, the Chicago NPR affiliate, WBEZ, acquired the Chicago Sun-Times as what was widely described as a \"no-cash\" deal.[1] A rather more truthful description is that WBEZ was paid $61 million (largely through philanthropic support) to take over a long-ailing newspaper, as described by the city's other long-ailing paper, the Chicago Tribune: \"Chicago Sun-Times becomes nonprofit newspaper with $61 million in backing as WBEZ merger closes\"Of newspapers operating at a loss or as philanthropies, there are the Baltimore Banner,[2] The Guardian,[3] and ProPublica,[4] which all operate as non-profits, relying on a mix of advertising, subscriptions, and philanthropy. The privately-held, for-profit Washington Post is a for-profit paper that's operated at a loss for years, and this before losing ~10% of its subscribers due to recent editorial decisions.[4][5] There are numerous propaganda institutions (usually labeled as \"think tanks\") promulgating various ideologies or interests, with the Atlas Network being amongst the largest and most influential:________________________________ Notes: 1. See for example the WSJ's coverage: \"Chicago Public Media to Acquire Chicago Sun-Times, Creating a Nonprofit Local-News Powerhouse\"archive/paywall: . 2. A non-profit newspaper established in 2022: . 3. A slightly dated take on 2016 turmoil at The Guardian: \"Everything you need to know about the Guardian’s giant bust-up\" (2026-5-18)and Wikipedia's entry on the Scott Trust Limited which underwrites the paper: . 4. ProPublica was established as a 501(c)(3) in 2007, with funding from the Sandler, Knight, MacArthur, and Ford foundations, along with the Pew Charitable Trusts, Carnegie Corporation, and Atlantic Philanthropies: . 5. \"The Washington Post publisher disclosed the paper lost $77 million last year. Here’s his plan to turn it around\" (2024-5-23)6. \"Washington Post cancellations hit 250,000 – 10% of subscribers\" (2024-10-29)reply bradleyjg 7 hours agorootparentThat’s just a different set of customers (“patrons”). reply dredmorbius 5 hours agorootparentNo. For most of the listed orgs, it's a different business organisation (not-for-profit rather than shareholder-based), the organisations aren't intended to run an operating profit, and the content is available to far more than just those who pay directly for access. Mind that in the case of ad-supported print media, the principle customers (the advertisers) weren't identical with the set of readers. But access was largely limited to those who subscribed directly, bought a newsstand copy, or could access a copy obtained by either method. In either case the operation was generally intended to run a profit. There have also been free papers, either supported entirely by advertising (frequently \"entertainment weeklies\"), or published as propaganda organs for a given organisation, frequently religious or political. reply refulgentis 17 hours agorootparentprevOr it's a time-honored way, neutral, way to communicate the impact of business news. YMMV, in my experience not based not on facts, but how willing I am to catastrophize things in order to make things I don't like simple to blame on someone. reply rty32 7 hours agoparentprevThis could be very wrong, but I thought DJIA is relevant mostly due to WSJ? Most publications care about S&P 500, but in WSJ they always quote DJIA first (for obvious reasons) and maybe mention S&P 500. reply anonu 6 hours agorootparentThe Dow Jones corp used to own the index and the newspaper prior to the mid 2000s. Now, News Corp runs the WSJ and the index is mostly owned and run by S&P. So there is a historical connection and probably some legacy ownership today. reply Terr_ 19 hours agoparentprev> DJIA is not taken very seriously outside of mainstream media it seems AFAIK its only real utility is if you want to make very long term comparisons over the years against old values of itself, where its long baseline may be valuable compared to other metrics that don't stretch as far. Everything else is just flim-flam for getting views/clicks or comforting very old viewers with something that is a familiar staple. reply Apocryphon 19 hours agoparentprevDo any index funds track it? reply phil21 18 hours agorootparentQuite a few, pretty much every major ETF provider should have one. I recall Vanguard not, and a quick google seems to confirm that. I do wonder why? The one I know of off the top of my head is DIA. reply staticman2 14 hours agorootparentThe dow is price weighted. Vanguard, being a sort of non profit, probably doesn't want people investing in an obviously horrible index that should have gone away decades ago. reply Ekaros 10 hours agorootparentThey don't care about people investing in losing bets. But they care about wasting resources on running stuff that does not sell. And not having a plus followed by big number and percentage sign in column next to name is bad seller. In the end how funds are marketed and presented is important part to understand. Also it is better for them to sell actively managed fund with bigger number on it than index with negative one. reply quickthrowman 2 hours agorootparentprev> I recall Vanguard not, and a quick google seems to confirm that. I do wonder why? It’s a crappy price weighted benchmark with 30 stocks invented over 100 years ago, that’s why Vanguard doesn’t offer a fund. The largest DJIA ETF is small potatoes, SPY’s average daily volume is higher than DIA’s total assets under management. VOO tracks the S&P 500 and VTI tracks the US total market, both of these are much better, more diversified options for equity investing. reply worstspotgain 18 hours agoparentprev> nothing burger event for an index that has lost relevance This argument has been around since time immemorial. The right way to think of it is more like a country club or a who's who, rather than a survey or a directory. As for the news at hand, it's really more about Intel than Nvidia. Sic transit gloria mundi. reply lotsofpulp 6 hours agorootparentIntel hasn’t been in the country club for 10+ years. reply worstspotgain 5 hours agorootparentIf you exclude its pre-Athlon Nvidia-like era, Intel's market cap earlier this year was only ~20% below its 2020ish peak. It has engineers. It might yet surprise. Nvidia hasn't proven it can turn a ridiculous amount of capital into a matrix-multiplication moat. reply httpz 12 hours agoprevNobody should take Dow Jones Industrial Average seriously. It's only relevant because it's been around for 139 years. It only tracks arbitrarily selected 30 large companies and it's not even weighted by market cap. It was made this way because 139 years ago we didn't have computers and someone had to manually calculate the average. reply knowitnone 19 hours agoprevSo you can just replace underperforming/flailing companies and still call it an average? reply kristopolous 19 hours agoparentYes that's how it's always worked https://en.m.wikipedia.org/wiki/Historical_components_of_the... reply tedsanders 19 hours agorootparentAccording to this, the original DJIA was 11 stocks, comprising 9 railroad companies, 1 steamship company, and 1 telegraph company. Capital was deployed very differently 140 years ago. reply dredmorbius 18 hours agorootparentIt was and wasn't. Those were the telecoms, transport, and infrastructure companies of their day. They connected the industrial and agricultural output of the United States in much the way Apple, Amazon, Boeing, Verizon, and Walmart (all current components of the DJIA) do today. That said, yes, it's interesting to watch how the components and industrial sectors represented change over time. The first DJIA proper (26 May 1896) featured cotton oil, sugar, tobacco, gas & coke (coal), cattle feed, electrical utility, lead, railroads, leather, rubber, and a holding company (trust) largely engaged in utilities and transportation, and dropped later the same year, along with US Rubber. Changes to the average have been a consistent feature to its origins. Think of those which aren't directly comparable to modern concerns (e.g., oil & gas, electric utilities) as raw materials (mining and ag), transport and logistics, and food (or feed). reply RobRivera 18 hours agoparentprevIts not a market average, its an average of members curated by an arbitrary set of rules. Many exist. This is how it has always been. reply xnx 18 hours agoparentprevDJIA is kind of a joke index. SP500 is much better. reply millipede 18 hours agorootparentThe Dow Jones index serves a different purpose: it's meant to be easy to update without the use of a computer. It's easy to update and publish the index since the math is a lot easier, and doesn't involve pulling in hundreds of quotes and trying to tabulate a weighted average. It's not that useful now that we have computers, but in the early 1900s it was a reasonably good approximation of a market cap using fast math. reply staticman2 14 hours agorootparentIndex funds didn't exist until 1975 while the Dow was formed in 1896. So I suppose it didn't really matter if the Dow was tracking things well, it wouldn't have impacted your investments. reply dredmorbius 11 hours agorootparentIt was still useful to have a general idea of how stock markets were valued, particularly in light of the events on and about 29 October 1929, a/k/a Black Tuesday. Valuation of equities markets themselves has a profound impact on the monetary and financial systems as a whole. On which point, John K. Galbraith's The Great Crash: 1929 (1954) remains an excellent history of those events (and notes the DJIA's value frequently), as well as a general primer on equities and investments, and how they may go wrong. reply dkrich 11 hours agorootparentprevI love that so many people keep saying this so confidently yet if anyone bothered to look they’d see that the correlation between the two is nearly perfect over any timeframe including the past five years reply bityard 18 hours agorootparentprevYes, it is entirely symbolic and has been for decades. The only purpose it really serves is differentiating those who know a little bit about stock markets and those who don't. It's annoying that serious media outlets continue to publish it, alongside valid indexes, implying that it has anywhere near the same level of legitimacy. Also, I don't like it. reply mrtransient 17 hours agorootparentprevMay I know why you consider it a joke? I obviously have no much clue about it all reply sooheon 16 hours agorootparentIf you want to estimate the stock market, would you rather: 1. sum 500 of the biggest companies by size (price * n shares). or 2. have WSJ editors select 30 companies by any criteria they see fit, but you don't get to see the size of the companies, only the share price. reply dredmorbius 11 hours agorootparentNote that the DJIA also changes constantly over time (as discussed elsewhere in this thread), as do the weights assigned to the individual companies constituting it. The way that the DJIA changes isn't the same as an index of, say, the n most highly capitalised equities might (Fortune 5, 10, 20, S&P 500, etc.). reply irjustin 18 hours agorootparentprevWonder how much DJIA had to give to get them to list. reply tippytippytango 11 hours agoprevA cautionary tale for Nvidia to not become the next Intel. reply markus_zhang 19 hours agoprevDoes that mean it's gonna top... reply guerrilla 11 hours agoprevI've been assuming that Intel will eventually catch up, because that's the world we grew up in, but that's not going to happen, is it? reply ulfw 12 hours agoprevMay I suggest a merger of intel with Boeing? Two back-in-the-day engineering driven future-forward companies and today run into the ground by Quarterly Reports And Nothing Else Matters culture and meandering around. They'd be a lovely cultural fit. reply m3kw9 15 hours agoprevOk so why switch? reply jruz 7 hours agoprevShit the top is in, time to sell NVDA and buy INTC. Was fun ride. reply Havoc 4 hours agoparentThat’s assuming intel can sort itself out. I’m not entirely convinced reply chillingeffect 19 hours agoprevSymbolic of the times. The new wizzy guy edges out the old school lumbering giant. reply mananaysiempre 19 hours agoparentFor some value of “new”, anyway. I remember attending a talk on how GPU programming was cool and how GPGPU was the next big thing—in the mid 2000s, as a middle schooler. I don’t believe CUDA was a thing yet (looking at Wikipedia, the first release might have been several months later?), instead Cg and NV_fragment_program4 were the new hotness. reply kelnos 19 hours agoparentprevEh, not really? Nvidia is 31 years old, that's hardly new or wizzy. Sure, Intel is a bit older at 56, but I feel like Nvidia has been around more than long enough. reply the_clarence 6 hours agoprevWhy do we care about the dow jones again? reply talldayo 20 hours agoprev [–] > Intel shares were down 1% in extended trading on Friday. Nvidia shares rose 1%. Riveting reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nvidia will replace Intel in the Dow Jones Industrial Average on November 8, indicating a significant shift in the semiconductor industry.",
      "Nvidia's market capitalization has soared to $3.3 trillion, driven by a 170% surge in shares this year, largely due to high demand for its AI GPUs.",
      "Intel is experiencing challenges such as declining stock value, job cuts, and increased competition, contrasting with Nvidia's recent growth and success."
    ],
    "commentSummary": [
      "Nvidia is replacing Intel in the Dow Jones Industrial Average (DJIA), highlighting Nvidia's growth in AI and GPU demand and Intel's recent challenges.",
      "This change has sparked debates about the DJIA's relevance, as critics argue its price-weighted nature favors high-priced stocks, making it less representative of the market.",
      "Many consider the S&P 500 a more accurate market indicator, questioning the DJIA's current practical utility despite its historical significance."
    ],
    "points": 352,
    "commentCount": 195,
    "retryCount": 0,
    "time": 1730500995
  },
  {
    "id": 42023089,
    "title": "My Time Working at Stripe",
    "originLink": "https://jondlm.github.io/website/blog/leaving_stripe/",
    "originBody": "Leaving Stripe 2024-10-28 I did the thing you’re not supposed to do. I quit my job at Stripe without another lined up. Before I left I pondered deeply how I wanted to leave. The thought of leaving my job was terrifying yet energizing in ways I didn’t expect. I found myself up in the middle of the night thinking deeply. I wrote my long-form thoughts as an internal blog post. I rewrote them. I read them to my wife who helped me refine them into something closer to what I really care about: telling our stories more truly. I’d like to share my story more broadly now. Why? Because perhaps there is someone out there wondering if they’re struggling alone. I’m scared to share this publicly. I’m afraid of how it’ll make me look. I’ve been sitting on this post for months now questioning my motives. At some point you just have to hit the “fuck it” button. I care about honesty. Hell, one of my core values is “humbling honesty”. I suppose this is a small way to embody that. Here’s what I shared internally without any edits: Hi. I’m Jon. I’ve been an engineer at Stripe for almost four years. My last day will be on Friday August 30th. Lately I’ve found myself increasingly drawn toward honest writing. I’d like to offer you the story of my time at Stripe that won’t be found in shipped emails. Let’s start at the beginning. It was a late Summer afternoon in my home office in Portland Oregon. I was sitting at my desk pouring over my IDE setup to make sure I’d put my best foot forward for my imminent phone screen. TypeScript all configured? Check. Minimal testing framework installed? Mmmhmm. Tmux working? Yup. Tastefully muted terminal colors on display? Like a peacock. I could feel my palms sweating as I watched the clock slowly tick toward 3pm. I was nervous as hell. I’d already had a successful career as a software engineer for the better part of a decade. I’d initiated and driven a company-wide migration to Kubernetes. I’d authored a component library used by hundreds of engineers. When my previous company had been bought by AT&T I was given one of the highest retainer packages to keep me around. None of that seemed to matter in the build up to my interview. I was about to be closely judged and I was terrified of being found lacking. My impression of Stripe was astronomically high. I considered it an outstanding company of refined craft, attention to detail, and engineering excellence. I always got ridiculously excited when I’d see a new product launch on Hacker News. I had listened to Patrick on podcasts and admired him as an industry visionary. The word “revered” feels fitting. I carried all those swirling impressions with me as 3pm hit. My heart was beating so strongly I could feel it in my head. I tried to take some deep breaths but it didn’t help. My mind was racing over every little detail. “I’ll join about 30 seconds after the hour to make sure he knows I’m on-time but not too eager.” I joined the call and my interviewer joined shortly after. He introduced himself and we settled into the usual ritual of a phone screen. I told him I already had my own environment set up and I shared my screen. “Surely he’ll take note of my meticulous programming setup.” I thought. He gave me a coderpad link. I started to read through the question. I could already feel my mind moving like molasses. On the surface the question seemed straightforward enough. It presented a simple data structure and asked me to do some filtering based on a set of constraints. “No problem!” I thought. I fired up vim and got to work. “Since they’re so dedicated to quality I should make sure my types are airtight.” I started modeling my solution with types first. “Hmm, well this question needs dynamic keys in the objects so that’ll mean I need an indexed type. Thank god I remember how to do those without looking them up. Don’t want to appear weak.” Things were moving rather smoothly despite my sweat soaked armpits. I silently resented my wife’s decision to unilaterally give up anti-perspirant due to chemicals. I got to a reasonable spot to try out my code. I hit save on the file. My little fsnotify script detected the file change, ran a TypeScript check, then executed my code. Error. Shit. As I read the error carefully, I realized I didn’t immediately know the solution. I felt a soupy darkness creep further in on the edges of my mind. I fumbled around for a couple minutes trying to fix it correctly. I was trapped, caged by my own need to appear competent. I could feel my mind racing from idea to idea trying to find a way out. Eventually I looked at the clock and realized we were well into the interview. I had to find a pressure relief valve before the whole thing ran off the rails. I did the unthinkable. I added a //@ts-ignore comment to bypass the compiler. “Uh heh, if this were real code obviously I’d figure out the problem. I’ll just bypass this one error and move on for the sake of time.” I said. I continued down the increasingly treacherous road toward solving the problem. Another error. Another one. More ignore comments. Shit shit shit. At that point I was hardly present at all. I was falling violently down a hole of self consciousness that felt like a threat to my very existence. I knew my code was close to working but my mind was gone. I was done for and I knew it. He kindly ended the interview and asked if I had any questions. I bumbled some out but I knew my verdict. Unqualified. Failure. We ended the call and I let my head sink into my hands, but only for a moment. An idea sprang to mind. What if I got my code working really quickly and found a way to send it to him? I hopped over to Chrome and googled his name. All I could find was his Twitter account. At that point I had nothing to lose so it was trivial to solve the problem. Within minutes I threw together a public gist with my working code and DM’d him: Don’t feel like you need to message me back. I just hate leaving a problem unresolved like I did. The code still isn’t ideal but I at least wanted to reach out and send that to you. Thanks again! I nervously hit the send button and walked away from the computer dejected. Within a couple days I got the email I was expecting. Rejected. Eventually I mustered the courage to try interviewing again six months later. I got the job. I was elated. I was extremely proud of the fact that I had gotten a job at Stripe. I even made a little web animation thing to share with my coworkers letting them know where I’d be headed next. I was the first hire onto the brand new JS Infra team. I hit the ground running trying to prove myself. It took me a long time to find a project where I could clearly demonstrate business value. Before I found that project I constantly struggled with something else: writing. Writing was difficult for me when I came to Stripe. I had never really written a serious project proposal in my career working at smaller companies. My writing chops felt woefully underdeveloped and the impression only grew as I read through well-crafted ships every day. I remember a sinking feeling in my chest as I would try to work up the courage to share a link to a document I had agonized over. Worse still was seeing little avatar badges on an incomplete document I was editing. Allowing my unpolished work to be seen was terrifying. I didn’t want anyone to know I struggled. I didn’t want them to know how much their opinions mattered to me. I’ll circle back to writing in a bit. About a year into working at Stripe I found my first really impactful project. I worked tirelessly to migrate Dashboard development onto a new JS bundler. I sped up development by about 10x. It was a smashing success. I remember the moment when my buddy Slacked me saying that my shipped email had been one of Patrick’s tabs in a Friday Fireside. I hadn’t attended because I was too busy trying to work more. My email admired by a billionaire? Bliss. The thought of Patrick noticing me felt unbelievably good. Finally I was proving myself at the company of my dreams. I was on cloud nine. Until I had my next one on one with my boss. She shared with me some concerns about my communication and project management. She, and apparently others she gathered feedback from, felt like I hadn’t done a good enough job pulling other engineers into the project. She spoke of how proud she was of the outcome but that it didn’t excuse my less than ideal methods. Performance review time came and I got a partially meets expectations (PME). “I know it’s probably hard to get this rating after such a successful project. I want to assure you you’re on track toward a solid review next cycle as long as you address the feedback around project management and keep delivering quality stuff like this.” My boss said to me in another one on one. After the call I crawled into my closet and quietly cried. It was one of the few times I let myself. I felt ashamed and humiliated. I told my buddies, who referred me to Stripe, about my rating and they were both shocked. They offered kind words but it all felt hollow. Everyone assured me that a PME is a normal occurrence and that they were sure I’d bounce back. It still hurt. Worse still was feeling bad for being affected so deeply by it. “Damn it, Jon, shrug it off and get back to work” I told myself. I found comradery with one of my old friends on my team. He and I went a long way back and had worked together at a previous employer. We had eaten lunch together for years in a physical office. I allowed myself to be just a little vulnerable with him about how much I was struggling to get back in the saddle. I found our raw conversations to be a small source of life. I trusted him in a way I didn’t trust my boss. I knew that he cared about me no matter how I was performing at work. Having his avatar beside mine on an unfinished google doc felt invigorating. Together we quickly put together a solid proposal for the next phase of the JS bundler project that we both believed in. With him in my court I felt less afraid to present the proposal to my boss. I presented it to her and sought feedback. I could intuitively tell she wasn’t impressed. Over time I was able to sus out how she really felt. She thought we should shelve the work in favor of something else more pressing but she was reluctant to say it directly. Eventually I shelved the project proposal in favor of another JS modularity project. She applauded the decision publicly. The whole experience left me feeling dizzy. Regardless, I put my head down and got to work on the new project. In hindsight the pivot ended up being the right call. I don’t know why my boss wasn’t more straightforward. I never talked directly to her about it. We’re more alike than not. She was promoted into a more visible role and I got a new manager. I really liked my new manager. He did something right out the gate that suggested he might be a different kind of manager. He sought vulnerability. During one of his early meetings at a team onsite he asked the team, “for this meeting I’d like us to try and introduce ourselves a little differently. If you’re comfortable, I’d like us to try and be 10% more vulnerable than we normally would in a work setting.” I remember feeling a mix of anxiety and excitement rise in my chest. I sat pondering what I would share. I decided to go for more than 10%. I shared about how my marriage had almost collapsed a couple years prior and a taste of how painful it was. Some of my coworkers shared deeper things I’d never heard in a work setting. It was awkward. It was beautiful. That memory springs forth from my imagination as I consider my favorite moments at Stripe. It wasn’t my project successes. It wasn’t my shipped email being mentioned by Patrick. It was getting to know the people I work with a little more closely. Time passed and my new project was another slam dunk. Review time came and I got an exceeds expectations. I was shocked in a good way. More surprising still was how quickly the jolt of motivation from the rating passed. It felt good to be rewarded for my performance but I was increasingly aware of how little I cared for the recognition. Still more time passed and then came the depression. I found myself increasingly demotivated in all aspects of my life. I could hardly even muster the energy to play video games (my usual haunt). Some evenings I would literally sit and stare at a wall. My sleep went to shit. I was stunned. I couldn’t recall ever being depressed as an adult. I’d had slumps before but this felt different. I’d experienced something like that as a young teenager but doesn’t time heal all wounds? Apparently not for me. I felt increasingly helpless in the presence of a faceless enemy. I started noticing that “run” weeks for me often left our backlog larger than when I’d started. The feeling of letting my team down sunk in deeper. It sunk into a place I felt I had no control over. I tried cutting out distractions. I DNS blocked Hacker News and Twitter. I stopped reading google news. That helped a bit but even that failed eventually. I managed to stay off them but I was still depressed. It was extremely difficult because I really loved my team and the work I was doing. I’d get little moments where my mind felt hydrated. I would briefly get into the flow. Code, documents, and Slack replies to tough questions would stream out of me. It became more and more rare and I knew it. All my tactics were failing. I had to do something more serious. I did the unthinkable. I told my team I was struggling with depression and took some time off. I remember the feeling in my body leading up to the meeting. My stomach was churning. I felt queasy. “Am I really doing this? Am I really this weak?” were a small sampling of the thoughts running through my head. By that point another possibility had started to take root alongside the others. Perhaps weakness hides the path to what I’m searching for. My team was extremely supportive. I felt such care and understanding from them it was the second time I let myself shed a tear about something at work. That time I didn’t have to crawl into the closet. I went back to work. Months passed leading up to the last couple weeks. I’ve found myself again unable to sleep and rest properly. By now I’ve seen the cycle enough to know that another round of relief and invigoration would be only temporary. I’m left with an alternative that excites and terrifies me. It’s time to step without knowing what comes next. I’ll end with a poem that’s haunted me this year. It’s by the late John O’Donohue from his book To Bless the Space Between Us: The mind of time is hard to read. We can never predict what it will bring, Nor even from all that is already gone Can we say what form it finally takes; For time gathers its moments secretly. Often we only know it’s time to change When a force has built inside the heart That leaves us uneasy as we are. Perhaps the work we do has lost its soul, Or the love where we once belonged Calls nothing alive in us anymore. We drift through this gray, increasing nowhere Until we stand before a threshold we know We have to cross to come alive once more. May we have the courage to take the step Into the unknown that beckons us; Trust that a richer life awaits us there, That we will lose nothing But what has already died; Feel the deeper knowing in us sure Of all that is about to be born beyond The pale frames where we stayed confined, Not realizing how such vacant endurance Was bleaching our soul’s desire. Thank you for listening to my story. Love, Jon de la Motte",
    "commentLink": "https://news.ycombinator.com/item?id=42023089",
    "commentBody": "My Time Working at Stripe (jondlm.github.io)321 points by jondlm 18 hours agohidepastfavorite292 comments kev009 0 minutes agoOf course we only see one side of the story here, but as written the original manager sounds like a maniac. Letting someone complete a major and well received project and then sniping backward at how they did it with a performance review reeks of incompetent tyrant. Where the hell was the feedback and mentoring when it mattered? If you are worth your salt as a manager your reports should know exactly where they are going into performance reviews and that process should just be a formality to make the HR department whole. Unfortunately, a lot of dipshits end up in management because once you make it here it is a lot harder (ironically) to get performance reviewed out. reply DeathArrow 10 hours agoprev>he asked the team, “for this meeting I’d like us to try and introduce ourselves a little differently. If you’re comfortable, I’d like us to try and be 10% more vulnerable than we normally would in a work setting.” I remember feeling a mix of anxiety and excitement rise in my chest. I sat pondering what I would share. I decided to go for more than 10%. I shared about how my marriage had almost collapsed a couple years prior and a taste of how painful it was. Some of my coworkers shared deeper things I’d never heard in a work setting. It was awkward. It was beautiful. I'd rather not have my manager forcing me to do group therapy. I owe the company some work hours and they own me money. If I want to have personal relations with someone from my work, befriend somebody, share personal things, that is my choice. My personal life is not the company business. Of course I wouldn't say all that to the manager, but 'I'll put him on the list of people I should be careful about, and fake some confessions. reply cpursley 9 hours agoparentIt’s pretty nuts, putting out your dirty laundry in public is a surefire way to self sabotage - especially in a work setting. In American culture, there’s a lot of “positive self help” talk about being open about these things. But the reality is - it actually just provides drama and ammunition to people who might not have your best interests at heart. Some things are best kept between your close trusted friends and family or therapist, no matter how trendy the culture is about being “open”. reply ValentinA23 2 hours agorootparentThis happened to me. The result: - secret slack room to help me overcome my depression (wasn't invited of course, I just spotted it several times while looking at my colleagues' screens) - lowest paid member in the team, while also being one of if not the most productive. 12x: verified. 30x: speculated if I had control over the architecture. - 70h/week, no vacations, for 3 years. No paid overtime of any form. - Double bind situation where I was used as a punching ball between the CTO and the #2 employee. - Excluded from secret negotiations to obtain a raise. You know what I did ? I quitted, cold turkey. That's when the CEO started to hate on me, feeling betrayed. The #2 left shortly after confiding he wanted to punch the CTO in the face. After realizing hiring someone to replace me wasn't cutting it, they distributed overtime to everybody on the team. Result: unionization. One year after my departure I received a call from the COO telling me he was willing to hire me back at the condition I would behave (since I was thrown at every hot issue, semantic propagation tied in their mind the idea I was also problematic). He was let go shortly after and replaced by a manager from the holding company (if you know what I mean). I haven't worked in 6-7 years. Most colleagues who left have had a similar empty period. reply Tainnor 5 hours agorootparentprev> no matter how trendy the culture is about being “open” I've noticed this trend in recent years to be \"open about mental health\", etc. And I think by itself this is great, mental health is important. But invariably, especially in management/HR settings (or in the entertainment industry), it is done in an incredibly shallow way. Mental health is hard. Just talking openly isn't magically going to fix things and may even initially make things worse. It may release feelings that need to be carefully managed e.g. by a therapist. Obviously, no company would ever be able to provide that. Instead, they're just looking for \"I started running every morning and was able to fix my depression\" kind of stories. reply whatshisface 2 hours agorootparent\"I used to have anxiety over being a good developer, but then I realized that it was about hitting story points, not learning Rust. Now I deliver business value regularly and have been able to cancel my appointments with my therapist.\" \"Being on-call 24/7 helps me feel like I'm a part of something, fulfilling a deep want for belonging that being kicked off the Girl Scout troop created in me.\" \"I've always lacked a father figure. When you told me to spend less time browsing HackerNews and more time on Jira, I felt like I was part of a real family.\" reply c0balt 2 hours agorootparentThat reminds me heavily of krazams's \"I Have Delivered Value... But At What Cost?\", https://www.youtube.com/watch?v=DYvhC_RdIwQ reply TrainedMonkey 9 hours agorootparentprev> Some things are best kept between your close trusted friends and family or therapist, no matter how trendy the culture is about being “open”. This makes total sense if you have a therapist and / or have friends you can be vulnerable with (and know how to). But there are undoubtedly those who do not have that. For them the risk of career sabotage vs a change of improved quality of life might be worth considering. I don't know where the breakdown is, probably against mandatory sharing in the work setting on the account of an average workspace being pretty adversarial. However, if I could make a call for my younger self I would choose to share. reply biorach 3 hours agorootparent> For them the risk of career sabotage vs a change of improved quality of life might be worth considering The risk is way too high reply Imustaskforhelp 7 hours agorootparentprevI also agree with this sentiment but at the same time I think it boils down to how you interpret these things , for op it was one of the greatest things but to somebody else , this makes boss not likeable. I think this shows that the boss was going out of his way which I \"personally\" appreciate. I don't know what people expect managers / boss to do. Somebody on team expects their boss to X and somebody else on that same team expects their boss to be anything but X. If I ever become a manager , I am probably taking this boss's approach , I think its good to have some talks like this. I am also much inspired by the likes of charles schwab and dale carnegie's how to win friends and influence people so probably there is a bias here reply thunky 6 hours agorootparent> I don't know what people expect managers / boss to do What they do should be related to the company work, otherwise they're overstepping. They're not social workers. They have to respect the privacy and independence of their coworkers. > If I ever become a manager , I am probably taking this boss's approach Please don't. Coworkers can obviously have social relationships but they should grow organically without being \"managed\". You know the best way for that to happen is? Let them work together. They'll build their own relationships. They don't need your help. If they don't work together enough for this to happen naturally then they're strangers to each other anyway and that's ok too. reply dymk 6 hours agorootparentprevIf you were my boss and did this I'd be going to my skip manager to complain. reply captnObvious 5 hours agorootparentIt sounded like an optional request not a command. I’ve managed and I’ve been managed and the relationship ships I’ve built along the way are deep enough to have conversations like these. We’re doing life with this people it’s not a dystopian novel about capitalism. If you have a genuine person as your boss moments like these help form friendships, and don’t feel like a dark Dilbert comic. Friendships increase team communicability which bolster project success. reply Brian_K_White 1 hour agorootparentAn \"optional request\" in a public group setting like that is not optional. You were non-optionally forced to respond to the challenge one way or another. If you don't share, then you're the guy who opted not to share. It turns off the touchy feely types who like it and don't understand the problem and can be \"not a team player\" fodder for anyone who wants it. The actual respectful and considerate way is not to put people in such positions in the first place. Like asking something from someone where it would be some unusual imposition or favor (like something that would be ok to ask a friend or family but not a mere aquaintance) and telling them it's ok if they say no. That is an empty statement. You were not supposed to put them in the position where they had to say no. You can allow for people to not be robots without putting other people into awkward positions they didn't deserve to be put in. It's definitely favoring some people at the expense of others. Like the gp comment, now they not only have to do their normal job, they also have to generate bs for their manager. The manager should be the one who has to figure out how to work with their different people, not the other way around. That is the managers explicit role that they supposedly get paid more for and what gives them the authority to manage and judge anyone else. That's their actual job rather than coding or whatever. Instead, they are making their team members all conform to them, on top of their actual jobs which are supposed to be something other than managing people. reply dymk 3 hours agorootparentprevA manager asking you to do something is never “just” an optional request reply watwut 29 minutes agorootparentI said no to things and world did not ended. You absolutely can say no. If you can't, you need to change the team or job. reply hhh 3 hours agorootparentprevThey absolutely can be. You can push back against your manager. reply twelve40 2 hours agorootparentStill, you'd rather save your push back privileges - which come with a cost - for something actually important and work-related. Not for random crap like that, being put on the spot randomly in a personal way. reply elygre 3 hours agorootparentprevThank you for this comment. I know the US is different from where I live, but the distrust shown in other comments just made me very depressed. reply Tainnor 1 hour agorootparentThis has nothing to do with it. I'm not from the US and I find this deeply unprofessional. reply sigseg1v 5 hours agorootparentprevI mean, I am a manager and if I saw another manager doing this type of exercise I would probably report it because it seems extremely inappropriate and they would probably want a heads up that we have a manager trying to be a wannabe therapist rather than trying to foster a safe and productive team environment. reply DeathArrow 8 hours agorootparentprevIt depends on your objectives. You can use those \"confessions\" to play the scenario to your advantage. reply Nashooo 6 hours agorootparentprevIn what way is this ever providing someone with 'ammunition' and how is this self-sabotage? What are you afraid someone would do with this kind of information? Reading this comment and others feels like people have read a bit too much Sun Tzu and are treating what could be collegiality as warfare. reply whutmuffin 1 hour agorootparentMaybe you’ve never worked at a company like this. Did you not see the note about his perf review, which had nothing to do with his performance (the actual outcome) but instead had random negative feedback? Like not involving coworkers enough? What if your “confession” indicated that you have a hard time opening up and involving others? Couching your feedback in a personal insecurity makes it far less likely it will be challenged since the person already has a complex about it. reply thdhhghgbhy 3 hours agorootparentprevYou haven't been in corporate long enough bub. reply wkjagt 8 hours agoparentprevPost: \"If you’re comfortable, I’d like us to try and be 10% more vulnerable\" This comment: \"I'd rather not have my manager forcing me to do group therapy.\" How did \"if you’re comfortable\" become \"forcing me\", and \"try and be 10% more vulnerable\" become \"group therapy\"? reply kroolik 6 hours agorootparentIts not difficult to imagine a person saying 'i have nothing i want to share here' be stigmatised in future. As a person who doesnt want to work together and be part of the company culture. reply dzikimarian 6 hours agorootparentOr just share regular story you would normally share, when introducing yourself in work setting? I'm 100% sure nobody would care. reply XorNot 5 hours agorootparentI'm not sure anyone wants to share the work-safe marginally funny story they use around new people as the chaser to someone discussing how they got clean off drugs or recently dealt with a miscarriage or something. Like will anyone care? Probably not but it's an incredibly awkward situation to toss people into when they're at work. reply Tainnor 5 hours agorootparent> how they got clean off drugs which would probably even work okay because everyone loves and applauds a success story or something that isn't your fault, but imagine someone sharing their ongoing drug addiction. People who think that workplaces are \"safe\" spaces for personal issues should try envisioning what the reaction to that would be. reply dzikimarian 5 hours agorootparentNow let's switch to close family setting. It would be OK to discuss drug addiction with your parents. It would not be OK to discuss steamy night with your wife with them. It's almost like adult people can understand environment and make conscious decisions what is acceptable and what is 10% they can bend the rules if they want to switch the mood a little. Honestly it's hilarious how supposedly emotionally mature people panic and over-analyse in this thread. reply Tainnor 5 hours agorootparent> Almost like adult people can understand environment and make conscious decisions what is acceptable [...] Apparently not because in TFA, the author shared personal facts about their divorce and other people shared even \"deeper\" things they'd \"never heard in a work setting\". Quote: \"it was awkward\". These aren't things that belong in a workplace context (sure, if you're going out for drinks after work maybe you can volunteer such things - but not during a workplace meeting!). reply watwut 27 minutes agorootparentTo be fair, I did seen people discuss divorce, health and even alcoholism in workplace. reply Nashooo 6 hours agorootparentprevIt's also not difficult to imagine the opposite? Feels a bit like a strawman fallacy. reply stego-tech 5 hours agorootparentprevBecause bosses exist on a scale. For the sake of the argument, let’s use American letter grades. The boss mentioned above would be a “B” boss - good intentions, but a forced execution that shifts more of the burden to employees. I don’t necessarily mind these types, but it can be incredibly off-putting for those of us who work to live, rather than live to work. Above that you have the “A” bosses, who invite that level of vulnerability by repeatedly displaying their trustworthiness to you and the team. I’ve had a few of these bosses before, and they’re amazing; sadly, they rarely last long in the face of “maximum profit” corporate cultures that demand everyone be a replaceable cog in a machine. Okay, so at what point does that become “forced group therapy?” The C-D-F bosses. C-rank bosses will demand vulnerability because they mistake it for honesty or loyalty. Good employees pick up on this and will placate them with some useless trivia nugget but otherwise have mediocre to bad rapport with said boss. D-grade bosses are even worse. They’ll push and pull and drag and yank something suitably vulnerable from everyone, and even try to solutionize whatever was shared with the team. This is (in my subjective experience) the most common form of boss, born of MBA coursework and saddled with buzzword-laden articles of “how to be a better leader” from magazines no C-Suite would be caught dead reading. Finally is the F-grade boss, who literally just wants to know who is going to need replacing imminently. Don’t get me wrong, this is something every other grade is silently doing as well (hence the “forced group therapy” metaphor), but F-grade bosses are bad enough to be overt about it. Got a coworker who mentions they’re trying to have a baby? They’ll make sure said coworker never has a positive review again, so it’s easier to replace them once they’re pregnant. All of this is to say that if YOU are a boss, and an employee is being vulnerable with you in an unprompted or non-coerced manner, you have won their trust and you better respect that. But if you’re forcing them to be vulnerable somehow, you’re actually harming the trust relationship in the long run and will make your staff wary of your motives. Prove they can trust you, and they’ll open up on their own. reply jondlm 1 hour agorootparentThis is great. I absolutely agree that the best leaders gently invite and lead with appropriate vulnerability. They also don't mind if people say \"no\" either with words or body language. reply insane_dreamer 2 hours agorootparentprevA suggestion like that can some across as not optional. See the famous “wear bling” scene in Office Space. reply seanhandley 1 hour agorootparent“Pieces of flair” reply hosteur 8 hours agorootparentprevProbably because of the power asymmetry. reply molochai 6 hours agorootparentPower asymmetry and also group pressure. There is still plenty of tacit pressure to share despite the \"optional\" caveat. Someone who says directly \"I don't like this, no thank you\" even in very deferential words will garner some degree of negativity, either from the manager or the group. Same for someone who shares too much and winds up sobbing, or someone who shares a vulnerability inconsequential in comparison to what others share. reply twelve40 2 hours agorootparentI don't even know how I would react if i was caught off-guard with a stupid question like that out of the blue with everyone watching. On a good day, I would probably laugh it off and say something obviously idiotic like \"my vulnerability is that i work too hard\" to indicate that I don't find this amusing. On a bad day, caught off-guard, I might actually take the bait, \"become vulnerable\" and blurt out something far more stupid in front of everybody. reply jondlm 1 hour agorootparentprevI know I won't be able to convince everyone here, but I didn't feel pressured to share anything deeply personal. Some people on the team didn't. They weren't punished or looked down on. We were a tight-knit team before and after that event. reply baxtr 6 hours agorootparentprevThe thing is - and the post actually showcases this - that \"10% more vulnerable\" is super abstract. What does that even mean? Seems like it triggers people to share very deep stuff. reply kyleee 3 hours agorootparentPeople are very weird. 10% more vulnerable to me would be a story about “I’m not very good at painting but I like it” or “I have 6 toes on my left foot”. As opposed to divorce, bipolar, etc. reply HPsquared 6 hours agorootparentprevThe difference between \"0-10%\" and \"10-11%\" reply dymk 6 hours agorootparentprevA manager asking a teammate to do something in front of a bunch of other people - even \"only if you're comfortable\" - always puts them in a position where they feel they have to do the thing. Come on, this is basic social behavior. reply bell-cot 5 hours agorootparentprevRead his description a couple more times, while pretending that you're a therapist. The \"wait, what the ?\" stuff can give you a better sense of his emotions and mental state, if you don't balk at the objective counterfactuals. reply Aurornis 3 hours agoparentprevIn recent years I had a manager who read a lot of pop-psychology and self-help books. His 1:1s turned into pseudo therapy sessions where he tried to assume the role of therapist. He and I did not get along because I politely changed the subject back to work and the office every time he tried to get me to “open up” about my childhood, my home life, my fears and anxieties, and other topics. I thought he was a weird outlier, but I joined an invite-only Slack for management a while ago where I’d estimate 1/4 of the topics in the #1-on-1s channel are from managers trying to psychoanalyze their team or act as therapists. The good news is that the Slack is very good about correcting these people about their role and what is appropriate for work. The bad news is that these new managers are getting the idea from somewhere that they need to be some type of therapist and patriarch of the team. It feels like a weird iteration of new-age management philosophies that breed in B-list business books and social media like LinkedIn. reply d0gsg0w00f 1 hour agorootparentI'm a serious work focused person too and my manager is a little on the mushy side. Exasperating for me, but the younger talent on the team eats it up so it works for him in that respect. I'm still not sure what percentage of it is an act. reply jondlm 1 hour agorootparentprevThat's rough. Sorry to hear you were pushed. reply serial_dev 8 hours agoparentprevI wouldn’t necessarily be worried about them using it against me later, just the fact that they want to force me to open up and be pretend buddies is annoying. It’s a work relationship. It could be just me enjoying remote work, but I am not looking to be friends with them 4000 km away. I’m too old for this, I know all my work friendships faded away in a couple of years and even in those years we focused on work and chat about career. Don’t force me to share personal things with everyone. I’ll share it if I feel like it with the ones that I feel like it. reply tegling 7 hours agoparentprevI'm thinking maybe this is rather to present an opportunity for the team to put forward their expectations, pain points and desires. Being 'vulnerable' for me in a workplace setting means perhaps admitting that I really don't appreciate people bikeshedding over items I'm submitting for review (rather than being actually constructive) or that I feel the git work flow don't attribute fairly (hiding my contributions). Maybe others feel the same and we could do something about it? And those were only on individualistic side. To evolve the team and team play I'd think this a good time to bring up that I'm actually hating responding to support inquiries and rather have Pete doing more of that. And that I think Sue is doing amazing work with Figma and I'd really like to learn from her. Wouldn't this type of 'vulnerable' make sense in a team? reply sbochins 5 hours agoparentprevYea, this was really disturbing. Your work and manager already have a lot of power over you. It seems really nefarious to ask employees to bring in vulnerabilities about themselves from outside of work. I don’t buy the team building nonsense, which will probably used as the “explanation” here. reply porter 7 hours agoparentprevParaphrasing Seth Godin: do you want your heart surgeon greeting you before surgery by authentically telling you about the big fight he just had with his wife this morning? reply dzikimarian 5 hours agorootparentHow often do you meet your heart surgeon for let's say 2-3 years, a few times a week? reply christophilus 5 hours agoparentprevNot gonna lie. When these sorts of situations have arisen in the past, I find it really tempting to troll. These prompts are such BS that they just invite a BS response. “Whoo. OK. I’ve never talked about this, but here goes. When I was 12 I used a Hello Kitty doll to kill a man. Wow. Feels great to finally get that off my chest.” reply pavel_lishin 6 hours agoparentprevWe had a team building exercise, that I wasn't able to attend. The person running it decided that a great ice-breaker would be for everyone to name their greatest fear. I'm a parent. You can probably pretty easily infer what my greatest fear is. Would I share that? Fuck no! Not only am I not particularly willing to get that intimate with a bunch of strangers, I also don't want to be a huge fucking downer. What I do want is to recognize that while you can speed-run friendship & bonding to some degree, it's fucking inappropriate to try to do so at work. It's work. I'll make friends if I make friends, but otherwise I'm out at 5, physically, mentally and emotionally. reply dnissley 5 hours agorootparentIs it something bad happening to your kids? That seems perfectly appropriate to share reply pavel_lishin 4 hours agorootparentI don't fucking want to think about that during a fucking team building exercise. And I don't think anyone else wants to hear that, either. reply chinathrow 3 hours agorootparentNeither would I. I can only think of this question having any place in such a situation if you suffix it by \"at work\" and exclude the obvious things such as getting fired or the company going bust. reply Aeolun 8 hours agoparentprevTo be fair, 10% more vulnerable than “I won’t talk about my private live at work,” is still “I won’t talk about my private life at work.” I find it strange that the same people that ask you “How are you doing?” Without expecting an answer, are so into this kinda fake buddy buddy thing. reply aaronbrethorst 2 hours agoparentprev10% more vulnerable? Ok, I’ll make up a story about how some weekends I keep working because I love this job and team so danged much! A team isn’t a family. A company isn’t a family. reply soneca 10 hours agoparentprevWell, the manager asked only for “10% more vulnerable”. That seems pretty far from “being forced into group therapy” in my perception. It was the author and colleagues that decided to overstate, and seemingly did not regret. Groups of humans sometimes have connections like these, whatever the environment they are in. I am sure other similar meeting with the same prompt only generated people saying they have two cats, they I am anxious about learning a new programming language or that they do not quite understand Stripe business model reply sensanaty 9 hours agorootparent10% \"more vulnerable\" for me in a work setting would amount to something like \"the eggs I made this morning were a bit overcooked\", because I don't share anything vulnerable about myself at work. Especially not with managers. I have a sneaking suspicion that wouldn't be an acceptable answer in that meeting, however. reply SoftTalker 2 hours agorootparentprev\"I think your question is overly invasive and inappropriate in a professional work setting, and I'm not going to answer it.\" Is that more than 10%? Another option would be what Norm Macdonald did when Larry King asked him \"what's something nobody knows about you\" and he answered \"I'm a deeply closeted homosexual.\" Just go over the top. reply Tainnor 10 hours agorootparentprev> Well, the manager asked only for “10% more vulnerable”. And they have absolutely no right to do that. \"Being vulnerable\" is not part of any job description. reply random42 6 hours agorootparentMaking high performance teams is part of their job description though, and there is research to indicate that teams with strong social bonds perform better. reply tbrownaw 1 hour agorootparentCorrelations tend to disappear when one of the correlated things starts to get used as a tunable. Related: [Goodhart's Law](https://www.oxfordreference.com/display/10.1093/oi/authority...) reply Tainnor 2 hours agorootparentprevYeah but you don't form strong social bonds through awkward team exercises. The best team I worked on was super high performing and we did end up sharing personal stuff on our spare time eventually (and some of us are still friends years later). That all happened organically. What didn't work, however, was the office manager's increasingly awkward attempts to force socialising. When at some point we raised the issue (politely!) that this was often distracting, he became offended and lashed out. We later also found out that he was basically spying on us and reporting back to HQ. It's one of the reasons why I'm incredibly wary of people who try too hard to be my \"buddy\" at work, especially if they're not peers. reply wkjagt 8 hours agorootparentprevThat's why they said \"if you're comfortable\". It was totally optional. I don't understand your strong reaction to this. reply echoangle 5 hours agorootparentAsking a question in front of a group, bur adding a \"if you're comfortable\", doesn't really make the problem go away. There's always a pressure to answer, even if you say that answering is optional. It's not a horrible violation but it's still kind of weird and unnecessary in a professional context. reply DJHenk 6 hours agorootparentprevBecause you cannot freely say no to that. They have authority over you. Even if they promise it won't affect their opinion of you, you never know if that is actually true. So you are forced to play along. reply Tainnor 6 hours agorootparentprev\"Are you planning to have children? You don't have to answer\" reply baumy 4 hours agorootparentprevI promise you I mean this sincerely, and not in a memey internet insult kind of way - are you perhaps autistic? I ask because this is exactly the type of comment one of my siblings would make, and she is on the spectrum. She wouldn't be able to understand the social dynamics at play here, whereas for other folks it's just common sense. \"Optional\" is not really optional in some social settings, and that's something autistic folks struggle with being aware of, much less understanding. The activity may be optional in the most pedantic sense; you won't be struck down by lightning if you decide not to participate. But there will be consequences. All of which is another reason why this whole exercise is an awful idea, considering that the number of autistic folks in software engineering is probably a bit higher than the population baseline. reply Nimitz14 4 hours agorootparentKind of hilarious to make this suggestion while simultaneously revealing yourself to not understand nuance or commonsense. reply baumy 3 hours agorootparentPlease, elaborate. What nuance and commonsense do you feel I'm not understanding? reply fragmede 7 hours agorootparentprevYou are not your code, but putting my code up to be torn apart in a PR definitely feels like being vulnerable to me. It's a good thing for it to be torn apart and scrutinized, so as to make the product better, which is what we're all after, but it still touches a nerve when the reviewer is an ass about it. reply Tainnor 6 hours agorootparentThis is definitely not what anyone would think of when prompted to \"be vulnerable\". reply 4hg4ufxhy 9 hours agorootparentprevWhat does 10% even mean? My food delivery order forgot the fries and it was upsetting? I'm pretty sure this is not what they are expecting. reply shpx 7 hours agorootparentIt means whatever you feel like it means given how you feel in the moment/situation. It's not a task you can fail. reply echoangle 5 hours agorootparent> It's not a task you can fail. Really? If you are too honest, I am pretty sure you can fail spectacularly. Therapy is confidential for a reason. reply yunohn 7 hours agorootparentprevI’ve heard of exactly these “vulnerability sharing” exercises from colleagues, but luckily have not experienced it myself. Anyone starting to over-share becomes very problematic, because it forces everyone to conform. One of my colleagues didn’t feel like sharing anything personal, and he was silently outcast for the rest of the offsite and it continued to bite him in the ass for the rest of his time in that team. The examples were very similar to those mentioned - people talking about their divorces, sex life, etc. I don’t think it’s fair to force colleagues to share such stories with each other. reply varispeed 8 hours agorootparentprevThis is pure simple abuse. Manager uses position of power to potentially injure his co-workers. Playing \"I am somewhat therapist myself\" in the workplace should be sackable offence. reply insane_dreamer 2 hours agoparentprevI think the better approach would be for the manager to share something themselves, to demonstrate that a bit of vulnerability is okay, but not even ask if anyone else wants to say anything and certainly not make people feel they should. In other words lead by example so that if someone feels like they want to open up they can and their manager will be supportive. reply namaria 5 hours agoparentprevPutting aside the trite \"we're cool here\" corporate routine, it sounds to me like a hack of a manager gave someone the opening to trauma dump on their co-workers and some by-standers got treated to some drive by group theory. Awful all around. reply mywacaday 7 hours agoparentprevI work in manufacturing and we have a weekly safety slide that normally presents a safety based work scenario or learning, this week's one was based on how to be safe while out trick or treating and one of the guys said he didn't appreciate being told in how to behave outside of work hours. I have have to admit that his attitude caught me off guard but I understand where he is coming from even if the intention comes from a good place finding the line between work and home can be a grey area. Saying that I don't thinkmi would appreciate being ambushed with being asked to provide a personal anecdote reply elygre 3 hours agorootparentLovely. Sounds like a real team player. reply d0gsg0w00f 1 hour agoparentprevWhen these things come up I just share work vulnerabilities like, \"Yeah, that project I pushed for was a terrible idea in hindsight.\" Or \"I'm really struggling with this project because I don't know if anyone cares about it\". That way I tick the participation box without making it therapy. reply ruthmarx 7 hours agoparentprev10% more vulnerable than normal is open to interpretation. You don't need to lie or put anyone on a list, it's just an attempt to foster persona relations between a team, the idea being with a little empathy and knowledge of each other people might be more likely to resolve future differences amicably. reply Tainnor 3 hours agorootparentIt's funny how people in this thread are arguing that the 10% isn't a big deal when we have TFA describing how they shared personal details about their divorce upon hearing this prompt, and that other people apparently shared even more intimate things. There are ways of lighten the mood that are decidedly less abusive, such as \"tell us something funny about yourself\" or \"what is your hobby?\". You don't have to like such questions either but at least they're not open invitations for trauma dumping. reply tharkun__ 6 hours agorootparentprevSo now you have a bunch of people that freely overshared and that would freely overshare anyhow, you have people that invented something like \"my cat just died\" and everyone is now doling out condolences and asking how they feel for weeks even though they never even had a cat and they secretly hate the boss and the one guy that openly said they don't do stuff like that at work and they're now shunned by everyone. Worked out great, didn't it? Now the above is probably a bit over top but it shows the typical American company culture effect. Going through this at work right now since we got bought by Americans and more and more fakeness is coming in. reply spencerflem 5 hours agorootparentImo you're being a bit dramatic. I find it nice to work with real people, I'm work from home right now and its odd to me just how little I know about the people I ostensibly share 8 hours a day with. reply tharkun__ 5 hours agorootparentSee I think there is a huge difference between naturally getting to know your coworkers and this group pressure sharing exercise. It's completely fine to get to know your coworkers. But let the oversharing ones (the extroverts) congegrate together naturally. We had someone like that on the team. Was likeable enough but wouldn't stop talking about everything in their life. That was their choice tho and I am fine to listen but I'm not \"sharing back\". And then there were introverts. Some that spoke very very little and some that were in the middle. They randomly shared some things from time to time in quieter moments with less people around and when they felt comfortable. Very real people and it was nice to get to know the a bit more. I'm more on the introvert side there and if you group pressure me into this sort of thing I will remember that for a very very very long time. As in we are never gonna be friends. I'm still sour about what an extrovert HR person did 4 years ago and I don't speak to them any longer. They are also showing from time to time that they haven't changed and they'd be the type to use whatever came out in an oversharing session against you. Now tell me to \"report them to HR\". reply spencerflem 4 hours agorootparentI'm on the introvert side too fwiw, I just feel like inventing a dead cat is a pretty out there response for the situation, which was a gentle push to be a little more open than usual for those that feel comfortable. Definitely can be taken too far in either direction. Without any place for chit chat being set up for it though, at least in my company, the 'natural' amount of sharing is basically zero, which feels like not enough for me. reply tharkun__ 4 hours agorootparentIt's not a gentle push though. It's setting up a group pressure situation that is definitely gonna make introverts feel uncomfortable (by a probably extroverted person that may genuinely not understand what they're even doing in that regard as it'd feel natural for them). Now I understand that with remote work there may not be enough space for chit chat. But there are other ways than a forced group pressure situation to set that up. For example, we decided as a team when Covid WFH started to basically use our daily 15 min stand-up for whatever. Sometimes we just did stand-up and we're out of there after 5 min coz nobody felt like talking. Sometimes it'd be a half an hour of just talking about whatever. We also have a dedicated time each week blocked off on the calendar for it as well and when we have ad hoc working session calls we get in some chit chat here or there as well. It's not all just business but it can be. RE cat: see this is the internet and it's an anonymous forum. I used that example because our cat actually just died a few weeks ago. You thought it was over the top. Interesting indeed. reply spencerflem 3 hours agorootparentI thought feeling compelled to make up a story like a dead cat would be over the top, I'm so sorry that actually happened. I don't quite get the difference between a blocked off time for chit chat and the way the manager asked for it here but totally, being forced like you have to say something you don't want to sucks. reply tharkun__ 3 hours agorootparentFeeling compelled to make up a story is part of the whole group pressure thing. Either you overshare i.e. share something that actually happened but that you would never have wanted to share without the group (and boss) pressure situation or you make something up (like many people in the comments here are saying). I guess the difference is the same what I mentioned about stand-up changing: The team decided to do that. It wasn't a manager saying that's how we do things now. EDIT: I also just realized you gave out condolences. See if I was in the situation from the article I might not be able to quickly come up with something fake and I'd instinctively just mention the cat thing (coz I don't want to be the guy that gets shunned). And then the next few weeks are gonna be very uncomfortable coz people will keep mentioning it. reply superultra 3 hours agoparentprevI am the kind of guy who wears his heart on his sleeve. I grew up in a familialy dishonest context so being open and transparent has been my default. That attitude is a liability at work and I’ve learned the hard way. We’ve all heard the phrase don’t shit where you eat. In modern workplace cadence that means don’t use your workplace as your source of therapy or friends or certainly family. The way we present yourselves at work has to intrinsically be a performance because that’s how money works. Find another place to express emotions and thoughts and find friends. That place should not be the workplace. reply jondlm 52 minutes agorootparentSorry to hear you've been burned by sharing. reply 946789987649 3 hours agoparentprevI had a manager do something like this but in a slightly better way. Each week we took it in turns doing our \"life story\". Some did it funny, some did it really personal and serious, some more of a CV, and some a mix of them all. You could really decide how you wanted to do it, and people got to know you a bit better. reply neilv 1 hour agoparentprev> Of course I wouldn't say all that to the manager, but 'I'll put him on the list of people I should be careful about, Both reasonable. > and fake some confessions. Could you find some other way to handle it, without lying? reply Tainnor 10 hours agoparentprevI agree and this wasn't the only red flag in the post for me. The reverence for a company that does, in the end, \"just\" process payments or the part about \"a billionaire liked my project\"... I feel like there's something rather unhealthy about that kind of mindset. All the best to the author of the post, hopefully they can find more meaningful work. reply whstl 4 hours agorootparent> The reverence for a company that does, in the end, \"just\" process payments It is funny how the more unimportant the company is, the more this kind of reverence is expected. For me there was a stark contrast between two jobs I had: Consultancy where I worked on software for local government that millions literally depend on? Keep your work-life 100% separate. Marketing startup that spammed Google with crazy amounts of SEO-spam? Lots of instructions from HR to ask about personal matters in 1-on-1 meetings, weekly 2h mandatory confraternization followed by free beer, random CEO speech about his favorite topic for 1h every month. reply tbrownaw 25 minutes agorootparentAt least with those specific examples, it could just as easily be startups being inexperienced and more susceptible to silly fads. reply jondlm 50 minutes agorootparentprevYour red flags are absolutely merited. I won't deny that. reply dabeeeenster 6 hours agoparentprevIf you did this in the UK you can be guaranteed a meeting with HR. Absolutely mind blowing. reply kyleee 2 hours agorootparentA meeting with HR too, talk about adding insult to injury reply rdtsc 4 hours agoparentprevAbsolutely. This has only downsides for employees and only upsides for employers. Don’t fall for this! reply thdhhghgbhy 3 hours agoparentprevI'd go further, the situation as described was gross. A kind of peer pressure invasion of privacy for all employees involved. reply scruple 4 hours agoparentprevYeah, I would just instinctively make something up. Maybe everyone else does that, too, how would any of us know? I really don't like this trend of employers trying to act like we're all friends or one big happy family. It only serves to blur the very real boundaries of power in favor of the employer. Engaging with it in earnest feels like self-exploitation to me. reply nvarsj 7 hours agoparentprevIt sounds like something straight out of Silicon Valley. I get they are trying to create a sense of psychological safety for people, but this is a bit much. reply varispeed 8 hours agoparentprev> I’d like us to try and be 10% more vulnerable than we normally would I would have reported manager to HR for this and if they didn't take action I would take company to tribunal. This is pure simple coercion into sharing personal data that then other workers could use against you. I can't see the world where sharing personal stuff at work is ever appropriate, let alone be subjected to peer pressure to share it. reply Imustaskforhelp 7 hours agorootparentThis isn't and shouldn't be the typical response to such things. The boss had also become vulnerable asking you to be 10% more vulnerable. I think I would respect it more than harsh cold calculating boss. And also the chinese gospel's taking its effect here , the post above this paraphrased it differently and this has taken it to completely different level. Nobody is forcing you with a gun to tell things or else you are fired the boss just said , \" Hey if you are comfortable , lets try to be a little vulnerable \" and if you didn't like it you could've said \" hey boss , I appreciate your gesture but I really really wouldn't like to share it as it would be a violation of my privacy , hope you understand \" This would probably be my approach if I was one of those people who hated boss asking him such thing but I don't. I think I like it. reply jondlm 47 minutes agorootparentMy boss shared first and was very much clear about it being optional. People bring a lot of context into these conversations and make a lot of assumptions. There is a lot of nuance here and that's tough to sit with. reply ahajxjxjxbx 3 hours agorootparentprevTrust is earned. A person you don’t know immediately asking you to be vulnerable is predatory. Commonly seen in cults, frats, etc but at least you opt in to those (and can opt out). Agreeing to work with a new manager is putting you in a vulnerable enough state. Build that trust over time - you likely don’t even need to hear about coworkers failed marriages. reply varispeed 1 hour agorootparentprev> The boss had also become vulnerable asking you to be 10% more vulnerable. That's really irrelevant. It's like taking your pants down, showing your privates and assuming your coworkers will do the same. Absolutely unacceptable especially when it comes from position of power, where workers may think that if they don't engage they'll get fired or won't get promotion. It's coercion. > the boss just said , \" Hey if you are comfortable , lets try to be a little vulnerable \" and if you didn't like it you could've said \" hey boss , I appreciate your gesture but I really really wouldn't like to share it as it would be a violation of my privacy , hope you understand \" This is never innocent. If someone's life depends on the job, they'll comply as they don't want to second guess if their refusal will lead to them end up being homeless. They'll share details they don't want to and get injured. reply wtfparanoid 8 hours agorootparentprevwhat kind of dystopian workplace do you go to where people are looking to use things against you? The jobs I stick with are the ones where colleagues have got each others back. All this zero-sum game playing toxicity is what ruins work for the collaborative pro-social people. Find a work place that doesn't reward that kind of behaviour. reply varispeed 5 hours agorootparent> what kind of dystopian workplace do you go to where people are looking to use things against you? Any corporate environment. You'll get stabbed in the back sooner or later. > where colleagues have got each others back. That doesn't sound healthy if people have to get each others back - usually means there are some bad actors that you'll need to cover up for. Do the job, be polite and leave ego at home. There is no need for politics, gossip and other meaningless activities. > work for the collaborative pro-social people. That sounds dystopian to me. I prefer to have social life outside of work, with actual friends that I choose, not HR. reply FpUser 3 hours agorootparent>\"That sounds dystopian to me. I prefer to have social life outside of work, with actual friends that I choose, not HR.\" This. Companies doing social thing is nothing but trying to fool employees into feeling like they owe to company. reply runarberg 8 hours agorootparentprevI don’t know whether you are American or not, but I find it ironic that on a forum who’s demographic skews American, this sentiment of anti-individualism at the workplace is so widely shared. America is supposed to be this super-freedom, super-individualistic place. Except when you are a worker at a workplace. Then you are supposed to spend at least 40 hours a week (except you are supposed to want to spend more time than that; and most do [except they don’t want to]), you are supposed to leave your politics behind, don’t talk about politics at work, you are supposed to do everything your boss tells you without question, and certainly don’t bring in your morals. Your workplace is a dictatorship (in a freedom loving country) and you are not supposed to complain about it. You are supposed to like your work, so you are not even entitled to your feelings. And suddenly your workplace is supposed to be this communist utopia, where every worker has a task, and the work is supposed to make them happy, and the whole workplace has a comradery of peers. Except you are all making money for your boss (or worse, your shareholders) and only keep a tiny portion of what you produce, but you are not supposed to complain and definitely not supposed to unite in bargaining for better, only unite in being exploited (except you aren’t supposed to see it that way). The American workplace is probably as far away from the American dream as it possibly could be. reply ryanackley 7 hours agorootparentFirst of all, you should be aware that Stripe was founded by two guys from Ireland and it's based in San Francisco. This is the epicenter of the American political left. This is the side of American politics that cares about worker's rights, treatment, etc. Ignoring that, your characterization isn't accurate at all for American software companies in my experience. I'm an American and I've worked overseas in Australia. In addition, most of the current technical team that I am currently on is based in the UK. If anything, I've found other western cultures to be more discouraging of openly challenging management decisions. However, the differences are very very small. Working for a UK/Australia/German company is exactly the same as working for an American company. The day-to-day culture depends a lot on the people who run the company rather than their nationality. Trust me, there are jerks everywhere in the world. reply dist-epoch 8 hours agorootparentprevSuggesting people share 10% more of some vague stuff is not coercion by any definition of that word, especially since there is no way to prove how much % you shared. reply olivermuty 12 hours agoprevWhat you are experiencing is the symptom of working in a large company where no matter how well you perform, you can not significantly move the needle yourself. Coupled with the fact that, as you yourself pointed out, there is a literal endless amount of work to do, forever. This is also due to the nature of the company being so big. All companies always has work to do, and no one is ever «done», but in a giga-enterprise all meaningful deadlines and deliveries sort of tangentially rounds down to zero in terms of impact. I almost burned out from this myself working in Microsoft. I was succeeding in my work by most metrics, but I am motivated more by my work being MEANINGFUL and having impact more than anything. That is almost impossible to achieve in any large enough company. Jumped off to be a startup CTO and life started smiling again instantly. Take time away from work, but not too much time. Comments such as «it takes years» can be true if you have ground yourself down to a nub, but trying (and being ok with failing) to do some work that lets you feel like you mean something and contribute back to society is an understated and important part of the healing process. Good luck! reply DeathArrow 10 hours agoparent>What you are experiencing is the symptom of working in a large company where no matter how well you perform, you can not significantly move the needle yourself. You can't move the needle unless you are part of management and learn how to do politics. Large companies mean lots of politics being done. And you want to position yourself on the part of the winning team. Also, people skills always trumps technical skills. Being aware of that helped me immensely on my career, much more than anything else I could have done. reply gorgoiler 4 hours agorootparentI agree and this is good advice that works well. My counterpoint is that if you are very good at: finding things that could be better or tools that should exist but don’t, and that if materialized would solve multiple problems at once instead of just one; and explaining why that’s important in a way that everyone can buy into and dealing with naysayers who’ll want to cut you down and doomers who will say it’ll never work; and have the focus, discipline and grit to make it through a 10k LoC project and land it on the other side while ekeing out code review along the way and not tripping over loose cables and uneven steps of your initial design / architecture / model that was either flawed from the outset or not malleable enough to fix after a month of entrenchment into your code; and finally once you’re ready, to then go over the entire codebase and lift the old code up into your new way of doing things so that there is no split brain between the old way and your new tool: then you can make it as a 10x engineer. It is exhausting but so is running up and down a football field, cycling laps around France, or serving tennis balls at 90mph for three hours at a time. Taking the elite-coder staff-engineering path isn’t for the faint hearted. reply zeroonetwothree 44 minutes agorootparentprevI wouldn’t say people skills “always” trump technical skills. Some projects require strong technical chops such that no amount of people skills will be sufficient. If everyone thought this way we wouldn’t have made so many of the recent major tech advances. Now given a fixed level of technical skills it’s obviously always better to have more people skills, perhaps that’s what you meant. reply Agingcoder 9 hours agorootparentprevThis is an extremely important piece of advice. To drive change in a large org, you will need support, which will either come from management, or people on the ground - this is purely people skills. Just writing amazing code and shipping it isn’t enough. reply Imustaskforhelp 7 hours agorootparenthmm surprising because I had mentioned how to win friends and influence people book by dale carnegie in some other comment on this post and one of the most sticky / remembered lines of this book is that even in a highly technical post , much of the success (approx 70% IIRC) can be because of having people skills. People skills matter , But I suppose in large organisations. I don't think people skills matter \"that much\" in open source (other than documentation and hey not messing up like attomatic and being a little bit nicer in general) Hell we should all try to be a little nicer in general , not just in open source. reply Aeolun 8 hours agorootparentprevWhile this is true, I find myself depressed by the idea of exchanging coding for herding everyone in the right direction. I’ve been trying to make that switch as it’s the only career prospect I have, but… reply mattgreenrocks 5 hours agorootparentI’m in early 40s and going thru a minor midlife crisis around this. Feeling that I’d wasted time in my 20s building technical prowess only to see myself still be viewed as mostly fungible. I think my future is moving towards consulting and building my own products. If people want to fight each other to get a seat at the table, be my guest. I see it as a waste of life. I can already build significant projects on my own, and it’s long overdue for me to reap all of the profit that those efforts generate. reply zeroonetwothree 43 minutes agorootparentprevMany companies have parallel IC tracks. And even though higher level ICs do need to “herd” more you’re still doing a fair amount of technical work (maybe at least until director equivalent). reply Aeolun 34 minutes agorootparent> maybe at least until director equivalent Hence the issue xD I’m at the top of what my track offers, but it feels silly to stay in the same place for the next 30 years. reply AbstractH24 6 hours agorootparentprevI can relate Would love to hear how others have solved this reply mattgreenrocks 3 hours agorootparentIn some sense, you don’t solve it. You are correct to perceive you are at a crossroads. And the hard part is knowing that either choice may foreclose some possibilities on the other path. There’s still plenty of time to change gears, or move back and forth between the two choices. There are also other paths; see my comment above. Important part is to not ignore the discomfort. I suggest reading Charity Majors’ blog posts around leadership vs engineering roles. reply jondlm 40 minutes agoparentprevThank you for the kind words! Thankfully I'm not ground down to a nub. I've found a lot of support over the last few years outside of work. reply zeroonetwothree 47 minutes agoparentprevI work at a much larger company than Stripe and I’ve definitely “moved the needle” myself. It’s all about finding high leverage projects to work on. reply teaearlgraycold 23 minutes agorootparentThere are absolutely a lucky few at even the biggest mega-corps that are seeing their work make huge impacts. But most people won't find those projects or there won't be space in them. reply keiferski 12 hours agoparentprevYeah this is why I sometimes miss the jobs I had in my early 20s, working at bakeries/ice cream shops/etc. Obviously the pay wasn’t great and the working conditions subpar, but there is a genuine psychological benefit to making a simple thing, giving it to people, then finishing work at the end of the day and being done, without an ongoing to-do list, sprints, daily meetings, and all the other requirements of contemporary white collar jobs. It’s led me to wonder how one could structure a knowledge work job in a similar way. The tough part conceptually is how to make progress long-term while still only keeping your focus on a day at a time, max. reply AbstractH24 5 hours agorootparentI’ve had a chance recently to moonlight doing the work I did the first half of my 20s almost a decade after I left it for tech. Despite its dead end nature, I’m seriously thinking about returning to it. The work amounts to super overpaid (thanks unions) retail using antiquated software that it modernized could eliminate at least half the jobs, but since there are union staffing minimums and salaries the employer has no incentive. There’s something nice about helping customers for a couple hours, doing some other mindless work which could be done in half the time if automated, and then going home. And the group of folks in the industry are mostly great to be around. Particularly after nearly 5 years of working from home in roles that are increasingly more isolated and less collaborative. reply foota 12 hours agorootparentprevI couldn't find it after searching, but I remember reading about a company where they just relaxed and got done what they got done. To me, this would be the ideal workplace. reply sharkweek 11 hours agorootparentWith any form of investor expecting a return I dunno how possible this is, but I’ve worked for one family owned business (non-tech industry but had a small tech team) and it was super relaxed like you mention. No investors, no board of directors, just a woman and her son who owned the business. They wanted to grow it but were very reasonable about it. That being said the salary was probably 20-30% below market. At the time I wanted to make more money so I chased that for a while instead. reply whstl 4 hours agorootparentI often find that even investors can be reasonable. Most of the problems in tech are of the employees own creation. Unreasonable requirements and unreasonable timelines, often coming out of thin air for no reason other than some middle manager's political interests. It is super hard to fight against it, and it's even harder to demonstrate how forcing tech to work more than necessary is bad for business. Small businesses deal with it by being small. Big companies are just chaotic complex systems where you don't have much of a choice. reply AbstractH24 5 hours agorootparentprev20-30% lower on a hourly basis or total compensation? reply duggan 10 hours agorootparentprevI’ve heard similar stories, but I think they tend to have some runaway hit cash cow in their rearview enabling it. Companies that can kind of do anything and it’s fine. Like Google with AdWords. Whatever they do, AdWords goes to work every day and pays the bills. Other companies get high on their own supply and invent things like “Holocracy” and claim it is responsible for their success. reply varispeed 8 hours agorootparentprev> finishing work at the end of the day and being done You can try to find your own thing that you'll focus on after work. After 5 I would switch off any company equipment, phone etc. Make sure that there is never an expectation that you could be available outside of your work hours. Feel free to forget anything you worked on after 5, you'll catch up next morning. Some workplaces will be against that and make fuss. Find another job then and if it is not possible, just deliver as little as you can without being sacked. If manager is unhappy, but not unhappy enough to let you go, then you are doing great. reply keiferski 8 hours agorootparentThat's definitely possible, but in my experience it's not the same thing as actually being done when you're working at a bakery or wherever. Then you don't need to turn off your phone, avoid company messages, think about what project you have to do tomorrow. You don't need to care about the job at all until your next shift starts. reply vladvasiliu 8 hours agorootparentI work in IT and I don't need to think about that project that's running late or whatever. Yeah, I have to turn off my work laptop, but that's more akin to taking off my apron if I worked in a bakery. Sure, it sometimes happens that I'm working on some interesting project, and I may find myself thinking about it after my workday is done. But it's because I generally love what I do. It wouldn't surprise me that, say, a pastry chef would possibly think about combining ingredients in some different way, too. I think the only job where you absolutely wouldn't have to think about it is if you don't have any kind of agency and only do what you're told. I'm not sure that I would enjoy that kind of job. reply vladvasiliu 8 hours agorootparentprevIndeed. And I think this is both a case of managing expectations (if you're always available, people will learn to expect it) and also of realizing what is and isn't expected of you. Where I work, a normal workday is 9 AM to 6 PM with an hour lunch. There's some flexibility as to the actual hours, I usually do 8-5, others do 10-7. Yet, some people always put in very long hours for some reason, complete with gulping down a sandwich in front of their computer for lunch. I'm not sure what gives them the impression they have to do it, since it's clearly not expected from our common higher-ups. reply globnomulous 4 hours agorootparent> some people always put in very long hours for some reason, complete with gulping down a sandwich in front of their computer for lunch. I'm not sure what gives them the impression they have to do it, since it's clearly not expected from our common higher-ups. This is just how I like to work and approach problem solving. I can take breaks when I make the effort, but, when work starts, I rarely think about anything except climbing whatever mountain I find myself on, helping others at my company, or exceeding my own (or others') expectations. That intensity helps me think deeply and solve problems effectively. There are people in my company who do good work but draw boundaries clearly and sharply. I respect that. I just love what I do and find it interesting and absorbing partly because I love the intensity of striving and struggling, constantly refining, improving, growing, and exploring. My company generously rewards good work, too. That also helps; my effort never feels wasted, unacknowledged, or unrewarded. reply vladvasiliu 3 hours agorootparentI don't think you're talking about the same thing as GP. I've had spells like you describe, and I did enjoy them. And I don't know about you, but I never felt like this was expected of me by the company, so there was no pressure involved. I think that this actually being my choice made all the difference. This seems quite distinct from what is discussed in this thread, which is people feeling some kind of pressure to work long hours, or to be available after work hours when they're supposed to be doing personal stuff. And, usually, the reasons for being called up don't seem to be some genuinely exciting problem that it feels invigorating to tackle. Instead, it's typically some form of TPS report that needs pushing around. reply varispeed 5 hours agorootparentprev> Yet, some people always put in very long hours for some reason Guilty. The reason was living with abusive partner and not being brave enough to end the relationship. Things were bad, but not bad enough to do something about it. reply cobertos 12 hours agoparentprev> motivated more by my work being MEANINGFUL Do you have any guiding star for what work gives you meaning? I thought getting closer to the users/customers would help me. After all, software is for humans. But I found the social aspects of that, handling bugs, setting expectations for features, was tiresome and not meaningful. The only happiness I did find was when I saw my client/users flourish without me with the software I made tailored as best as it could be for the money. But that doesn't make a business unfortunately... reply anal_reactor 11 hours agorootparentWork satisfaction is a meme. Yes, there are people satisfied with their work, but realistically, this is not going to happen to you. Stop giving a fuck about your work, slack off as much as possible, and look for coworkers with whom you can build real human connection, instead of corporate NPCs. reply 9dev 10 hours agorootparentI don’t know if that’s sustainable. We’re all stuck with this work-thing being one third of our life for the foreseeable time, so spending 33% of like, everything I’ll ever experience on something I don’t give a fuck about seems like both a lot of dull time and a bad decision overall. reply throw_pm23 8 hours agorootparent\"We're all stuck with this...\" I think you are rationalizing your choices by thinking everything does the same. It is not true, some are not stuck anywhere, some choose to live a simpler life that needs less money, fewer compromises, etc. etc. the world is a big place. reply 9dev 7 hours agorootparentSure. I was specifically talking to people browsing hacker news, which have a high probability of working a job in the capitalist economy. Of course there are other ways of life, but the predominant part of humanity does not choose those. So I still think it’s appropriate to talk about ”all of us“ in this case. reply anal_reactor 8 hours agorootparentprevWork from home exists for a reason reply immibis 9 hours agorootparentprevWell, your choices are that or death - no matter whether you give it bare minimum effort or a lot of effort. reply Aeolun 8 hours agorootparentYou you actually feel fulfilled giving anything minimum effort? Like, I’m not going to work overtime for free or anything, but the time I do work gets my best effort. reply mattgreenrocks 3 hours agorootparentCynicism is a cope for not doing the work. Minimum effort is exhausting for the long term. reply cobertos 1 hour agorootparentprevHow is making this human connection w/ coworkers done remotely? Is it possible? I'm struggling with that. Is it reasonable to accept/leave a job based on no interesting connections? reply scott_w 9 hours agorootparentprevIf I could pick a comment to explain a “self-fulfilling prophecy,” it would be this one right here. reply morning-coffee 4 hours agorootparentExactly... if you want to be miserable at work, by all means, \"slack off as much as possible\". I think we forget the big picture that \"no one gets out alive\" and that it's up to each of us to spend the time we have in a way that makes us happy. If you can afford to not spend 1/3 of your time at a job in order to fund the remaining 2/3 of your existence, lucky you! But if not, choosing to slack off at a job you hate is essentially choosing to throw 1/3 of your life away being miserable. (Not judging if that's your choice, but its baffling to me.) reply zeroonetwothree 41 minutes agorootparentI agree completely. I’ve had periods where I just slacked off and I was miserable, leading to a negative feedback loop. The only way to break out was ironically to actually work more, which led to more enjoyment of it. Cognitive dissonance is powerful. reply stogot 51 minutes agorootparentprevAnd it makes your colleagues miserable because they have to carry the load or slack off too, which inevitably means the whole team is disfunctional and should be let go reply scott_w 1 hour agorootparentprevThat’s my opinion, too. I also find that by taking this attitude, you guarantee that you’ll only end up working jobs that confirm your worldview. I have friends who think my work experience is a wild fluke. reply DeathArrow 10 hours agorootparentprevYou still have to give people the impression you do a terrible good job and that you are working your ass out. reply anal_reactor 8 hours agorootparentThat's the tricky part reply nicbou 11 hours agorootparentprevI would say that anything that is opposed to Marx’s concept of alienation would fit the bill. Work should feel impactful and your decisions should lead to some tangible output. It helps to be close to your users, or to build software that has a clear purpose. It’s good to work on a small team where your input has a clear influence on the quality pf the product. reply hinkley 10 hours agoparentprevOr sometimes the needles you can move seem unimportant to the org, even if they have objective value. You get a few pats like OP but no real recognition. Though maybe that’s why you can move those needles; nobody is guarding them jealously. reply lifeisstillgood 10 hours agoparentprevIt’s taking me a long time to learn these lessons - thanks for the tip reply varispeed 8 hours agoparentprevPeople have wrong priorities when working in big corporations. If you want to make an impact you should work towards creating your own business and working in big corporation is just a mean to achieve that. Focus on growing your bank account, good credit history, live lean and in your free time nurture the idea you have, make steps to make it happen. In some jurisdictions you need to check your employment contract as company may feel entitled to your idea you are developing outside of work hours. Ensure you don't have any such stipulations in the contract. reply DeathArrow 10 hours agoprevI think the author had unrealistic expectations from working at a large company. He expected doing something meaningful that will change the world. He expected to be applauded like a hero for his efforts. But things don't work like that. If you work your ass out, don't expect more than a pat on the back. Managers won't care about your efforts, your mental state or your sleepless nights. They care about looking good in front of their superiors. So it's better to do just that is expected for you, enough to get a good evaluation. If you come out with a brilliant idea that might help the company a lot don't just do it. Find some allies in the higher hierarchy, explain to them what is their advantage, do a POC or MVP, then let the top management know in a public meeting. That way you get a lot of credits and applause for doing great things for the company and fighting the good fight. reply ta_1138 2 hours agoparentThis story of his would have fir just fine in a much smaller Stripe, back when you could actually know everyone, and Patrick interviewed every programmer. A time where making a difference wasn't all that hard, as everyone fit in one cafeteria in the Mission. It was always a pretty competitive place, with a lot of smart people working a lot of hours, and a culture of attention to detail that left many people with impostor syndrome. There were pretty good expectations of being nice to each other: No infrastructure team giving your request the cold shoulder, because that was just not OK. So people working really hard and burning out to try to meet every growing expectations was common. The post also had the other weakness of the culture: A lot of management changes, along with a culture of performance among managers that would be fit for Amazon. So a manager might change teams, and the person that used to get exceed expectations would end up with a PIP with the next manager, often by surprise. You can imagine what it does to morale to tell someone how they are the most helpful person they've worked with, and then see them gone two weeks later. It was a great place to work in many ways, but the negative parts took their toll. So, if anything, the story showed me that even though it's been many years, a lot of Stripe is still recognizable. reply d0gsg0w00f 1 hour agoparentprevMy big company strategy is to troll internal chats/forums for things that seem like interesting problems. Do something to fix them or make them suck a little less. Always be building solid, interesting things and make sure that if someone, for some reason finds them, that they would say \"wow, this is cool, who made this?\" Then I casually share links to those things in the right context. If I get a 10% hit rate then I'm happy. Do this all day every day between assigned projects 9-5 and you'd be amazed at the network you can build outside your own team. Keeps the options open for lateral movement. reply lmm 9 hours agoparentprevThe flip side of that is that if companies make \"passion\" a hiring criterion, they'll end up hiring people who care about making a difference, which may serve them poorly if the position doesn't actually fit that. reply Aeolun 8 hours agorootparentOf course not. They got 4 years of fantastic work out of the guy before he burned out. That serves them just fine. The only requirement is that you have no morals. reply stitched2gethr 4 hours agorootparentI don't see it that way. He wanted to work there. They paid him for his time and from his account all of the passion was his own. I'm not speaking against those who just want to do what's required, but is it really wrong to hire someone who is passionate about the work? reply Aeolun 29 minutes agorootparentSorta, yeah. Working for a largeish enterprise I actively select against too much passion. Those people will come in, go WTF constantly for about half a year, spend another half year doing less and less, before eventually leaving for greener pastures. I mean, I get them, as I went through the same thing. reply varispeed 8 hours agoparentprev> a brilliant idea that might help the company a lot don't just do it. Never do it. It will never end up benefitting you, even if you find \"allies\" - who might even stab you in the back and take credit for the idea and then let you go once they get a promotion. If you have a brilliant idea, keep it to yourself, try to think of a way it could be used outside of the company and if it is strong enough start your own business based on the idea. Otherwise forget it. Never do more than you are asked to do. reply zeroonetwothree 49 minutes agorootparentThis has not been my experience. I’ve seen people rewarded for ‘brilliant ideas’ (though really more like brilliant execution, let’s be honest, ideas are worthless in their own). Now could they have more reward starting their own company? Perhaps. But it’s certainly more risky. In a regular job if your idea doesn’t pan out you haven’t lost much, maybe you get a slightly smaller bonus that year or something. reply hansvm 2 hours agorootparentprevIt's not \"more\" work; it's \"different\" work. I quite enjoy low-level optimizations, and all I have to do to work on that all day instead of coordination, meetings, and drudgery is convince people a few levels up the chain that the cost savings are huge and that the extra speed enables cool new features. It's corporate America, so I'm going to be unceremoniously fired eventually anyway, but in the meantime I might as well enjoy myself, impress my coworkers, snag a promotion or two, and get a \"made a cool new thing increasing profits $XX million/yr\" line item on my resume. > start your own business Probably eventually, but starting a business is very different from being at a place big enough that I can profit my salary many times over just from faster code. If I start one, I'll write fast code there too, but \"fast\" isn't a business idea by itself, and I don't see anything wrong with doing a good job for whoever happens to be writing my paycheck. reply varispeed 1 hour agorootparent> the cost savings are huge and that the extra speed enables cool new features. Been there. The savings were never passed down, but you could always enjoy photos of boss's new sports car or their month's trip to Borneo to \"recharge\" and think of new challenges. Ah sorry! I got an iPod once as a thank you. > \"made a cool new thing increasing profits $XX million/yr\" That may backfire. Nobody likes a new kid on the block that has tricks up his sleeve that could jeopardise someone senior's career. reply DeathArrow 8 hours agorootparentprevThere might be ideas that work only in that particular context and you can't implement on your own outside of that. Otherwise, I agree with you. >Never do it. It will never end up benefitting you, even if you find \"allies\" - who might even stab you in the back and take credit for the idea and then let you go once they get a promotion. That's why I said to get allies from the higher ups. Not your boss or your peers. Preferably business people. And you don't need to give them all the details. Just let them know if would make them look good, have their approval and use them as a shield. reply Aeolun 8 hours agorootparentprevI mean, brilliant ideas can still be fun to work on. Who cares if you don’t get appropriately rewarded for saving the company $50k/month, it was time well spent. As opposed to working on that pointless crud thing your boss likes. reply margalabargala 3 hours agorootparentI agree with you. But not everyone will. Some people have jobs to pay for their lives outside of work, and that's it. And that's fine. I take the stance of, the job I do for 8 hours 5 days a week might as well be something I actually enjoy doing above and beyond the financial incentive. Working on something I think is really neat is enjoyable, and I'm lucky to work in an industry doing things not so far from what I would spend my time doing anyway if I were retired. reply varispeed 1 hour agorootparentprev> Who cares if you don’t get appropriately rewarded for saving the company $50k/month, it was time well spent. If you are that great, it is much better to contribute to Open Source projects than to someone's already fat pockets. reply Aeolun 27 minutes agorootparentI’m not sure how you see that happening unless your company allows it? reply whstl 4 hours agorootparentprevI'd say work on brilliant ideas if they're good for your resume. Yeah, I know this annoys the senior devs, but after 20 years on this grind I totally get why people do it. reply abc-1 11 hours agoprevGreat blog, but reading it gave me deep secondhand anxiety and even exhaustion. It’s strikingly clear this person is a people pleaser to potentially unhealthy levels, maybe due to low self-esteem or something else, and it could be contributing to the depression. Legitimately, therapy might be the right call here. But on the positive side, they do seem like a good person who wants to build good things. That’s commendable. reply Jgrubb 6 hours agoparent100% agree. Please OP consider therapy or at least a next job that you can't give too much of a shit about. Care about the quality of the work you're shipping because it's fun. Be a human but keep your personal life personal. People pleasing, approval seeking makes you the battery in somebody else's flashlight. You will always find yourself used up by others. Don't give others control of your joy. reply mndgs 6 hours agoprevSorry for a probably unpopular opinion here, and let me generalize a bit: Gen Z all the way... (Saying this makes me feel a bit old, I guess I'm certainly am than the author). On a serious note, in my book there are hints of perfectionism right from the start of the story (fonts dimming, wait 30 sec to join the meeting..). And too much fragility in personal attidudes. S/he is probably a relatively young idealistic person, early in the career. Such people often don't last long, if they can't change inside and take manager's or corporate shit. One needs at least some \"fuck it\" attitude to preserve one's dignity. Your performance review does not and cannot define you as a person. Else you're likely to end up disappointed and/or emotionally exhausted. And that's what I see here. reply nunez 2 hours agoparentPut yourself in their shoes. They idolized a company that is known for top-shelf engineering talent and, more importantly, brutally hard interviews...and they just got an opportunity to interview with them. You have no idea what the interviewee will \"no hire\" them over, so you assume they want perfection because obviously. You're deathly afraid of making any mistakes because not getting the job you've been dreaming about for years is not an option, so spiraling straight into the ground after your first minor oops makes total sense. Once you've gotten the dream job, you now have to work even harder to keep it because, shit, have you seen the engineering talent in this place? Getting a \"failing\" perf review from your management sends this anxiety into overdrive, so you work harder to prove your worth. Some people barely last under these conditions. Others will gladly torch everything and everyone in their life to succeed, whatever that means. reply Brian_K_White 1 hour agorootparentnext [2 more] [flagged] BonoboIO 33 minutes agorootparentEveryone is searching for validation and often times we seek it on the outside and often in the wrong place. Companies don’t care about you, they care about the work you do … and if you don’t bring less to the table … surprise. But everyone has to learn it the hard way, some earlier than others. reply kelnos 36 minutes agoparentprevThe author mentions they have nearly 10 years of experience in the field, so assuming they went to college and got their first programming job soon thereafter, that puts them around 30-32, so fairly solidly millennial, if maybe on the younger side of it. Not that this matters; I'm always dubious of generalizations based on made up things like \"generations\". I definitely got people-pleasing vibes, and I agree the whole \"making my screen look neat and tidy\" and \"engineering my arrival time to the meeting\" bits were too much. If I had an interviewer who cared about or was impressed by those \"metrics\", I would consider that pretty shallow. Based on that first performance review they got, my feeling is they had a bad manager. I agree with the manager that communication is a crucially important part of work, but you don't give someone a low rating when they completed an otherwise wildly successful project. That's just cruel and demoralizing. You can attack the communication problem without demotivating your employee and possibly hurting their career progression. Whenever a poor performance review is a surprise to the employee, that alone is a management failure. Don't wait until review time to discuss problems; bring them up immediately and try to address them. Maybe by the time the review comes up, the problem will be gone. And if it's not, the employee won't be surprised by the reasons behind a less-than-ideal review. reply eddd-ddde 4 hours agoparentprevEverything in life needs a healthy amount of \"fuck it\" attitude. The second I learnt this and started living by it my happiness shot up and my anxiety went down. Who cares if you mess up, you have to stop living by your ideas of other people's ideas of you. reply _proofs 2 hours agorootparenti feel similarly and have had an adjacent experience with anxiety. the more i got to know myself, and the more i've accepted myself over the years, the more i've found myself self-advocating and validating my own existence (to myself) -- i do not need to prove or justify it to others, and i have been working on keeping this mantra alive. a large part of this experience has been overcoming things by applying a healthy dose of, \"fuck it -- this is for me.\" [1] [1] obviously one can suggest there is an element of hedonism or selfishness inherent in the attitude, but i think we can appreciate framing it in the context of not using this attitude or mantra to justify being self-destructive, or harmful -- that is not the point. it's more about applying it in a way, that combats the mundane insecurities i've faced and experienced in a range of extremes, which otherwise get in the way of personal growth. reply nunez 2 hours agorootparentprevI agree, but many people in this world are raised by parents for whom \"fuck it\" is not an option. Escaping that kind of environment is very difficult. reply cloverich 1 hour agorootparentIts funny coming from the other side, where fuck it is all you know. Of course balance is the answer and change is hard. If i ever can id love to incorporate this into my hiring criteria as a kind of diversity, because as a fuck it person i find i pair quite well with those who cant do that; and likewise working at jobs with too many of either type the flaws (and mono culture) become so significant its hard to stomach. reply waitasec 2 hours agoparentprevNo need to generalize, or assume. > I’d already had a successful career as a software engineer for the better part of a decade. OP's resume suggests they've been a professional engineer for over a decade. I agree with your points about perfectionism and idealism, but they are not isolated to Gen Z. reply phyrex 1 hour agorootparent\"Better part of a decade\" means \"more than half a decade\", so definitely not a decade yet reply schmichael 1 hour agoparentprevThe author appears to be solidly millenial from their resume. As an Elder Millennial myself I hope HN and our industry is free of ageism against young and old. Comments like yours don’t help. reply kelnos 31 minutes agorootparentI wouldn't call that ageism; attitudes change over time, and people early in their career haven't yet learned \"how the game is played\" so to speak. They will have some emotional attitudes about work that don't always align with reality, which in some cases can be harmful for their mental health and career progression. This is just a consequence of being inexperienced, and in this case it correlates closely with age. I do agree that bucketing people by \"generation\" isn't helpful. Just saying \"it seems like this person is early in their career\" is sufficient, and is the actual relevant bit. reply wkjagt 7 hours agoprevThere's a lot of comments on the \"10% more vulnerable\" part of the post in here that amount to \"I just owe the company work hours, and they owe me money\", and a strong aversion to showing anything resembling human connection going slightly further than \"how are you\", \"fine\". One commented even called the question by the manager \"abuse\", and suggested the manager should be fired. But if on most days we're spending most of our time with these people, what do we gain from automatically disqualifying them from human connection just because they work at the same place as us? And let's not pretend that the manager in this story forced anyone to do anything. reply sensanaty 5 hours agoparentThere's a power differential when management suggests you do something. Even if they genuinely, 100% for real would not care if you didn't share, the power balance is still at play there, which makes it dicey regardless of anything else. I actually like my manager, he's a chill guy and I've had some deep talks with him a few times (after work hours!), but if he pulled a stunt like this during work hours I'd 100% tell him that it's a completely inappropriate thing to do. I know that he doesn't mean ill or harm by it, but how is the new hire who's anxious to be starting the job supposed to react to it? How can you expect them to say no without feeling like \"not being a team player\" or whatever other stupid cliche HR types like to use? > But if on most days we're spending most of our time with these people, what do we gain from automatically disqualifying them from human connection just because they work at the same place as us? Some people just don't care to make people from work anything more than an acquaintance. They have their own social lives which they invest infinitely more energy into than their work lives, and that's completely fine. I personally can't remember the last time I've spoken to any of my ex-colleagues after they or I left the job, I definitely wouldn't call any of them friends. I still hung out with them at the bar on Friday evenings after a tough sprint, we had a good working relationship, we just weren't friends. I have actual friends who I'd rather spend the time with for that. reply jarjoura 1 hour agorootparentThe goal with any of these icebreakers is to encourage the team to be humble and approach work related problems in a thoughtful manner. It's easy for someone to assume you are asked to open up about non-work related stories, but it's never the intention. For example, talking about how your Aunt passing away being the reason you got into software engineering, is not really going to help the team in any meaningful way. However, it is quite valuable to share a story about how a coworker left you a huge unfinished project before abruptly going on vacation, and how you struggled to pick up the slack. You can use that story about the pain and frustration you felt and how you learned the value of having your work in a proper hand-off state when you eventually plan to take extended time away. Don't ever feel like you need to talk about your hobbies, or family, or even your love of cats or dogs. In time you will build your work friends and connect with people who share some your interests, but it will never be forced, and a few of them may become lifelong friends. reply progbits 3 hours agorootparentprev> Some people just don't care to make people from work anything more than an acquaintance. I've also been in a situation where I tried but realized behind the work persona they were assholes I would never be friends with. I've considered quiting at that point but decided to just not socialize with them. Work interactions are fine. It's a bit sad because in previous job I got along really well with most coworkers but such is life. I have enough friends and if it ever becomes a problem at work I'll just leave. reply molochai 6 hours agoparentprevI have personal experience with a manager who acted similarly intrusively, seeming to want to blur lines between professional and personal. They later critiqued my (supposed) emotional state and motivations in a professional review. \"Force\" may not be accurate as in strapping me into the group until I talk, \"abuse\" may not be accurate as in an active misuse of power -- but there are negative consequences for non-participation. I'm not the least bit opposed to connecting with peers. I've extracted two cherished friends of 10+ years from prior jobs. I just require it to occur organically and with peers, separate from groups led by the person signing my paychecks and writing my annual reviews. reply Brian_K_White 1 hour agoparentprevIt is abuse and the manager forced everyone to respond either by participating or by refusing to participate or by lying or by exposing that they actually have literally nothing interesting to say. If you don't get that, then you are probably out their making life 5% hell for half the people around you, in blissful ignorance thanks to the effort they all put in to catering to your inconsiderate interactions. reply AbstractH24 5 hours agoparentprevUnpopular opinion, but this is the problem with work from home. Reduces people to output. And if that output can be done cheaper by someone in another country or automated away, why even hesitate to change out the things that generate the output anymore than a lightbulb? reply yunohn 7 hours agoparentprevI’ve heard of cases where not sharing vulnerability in such a setting led to others (who did) being offended and subsequent othering. While arguing the manager needs to be fired is a bit too far, you do need to consider the power differential and how it forces conformance from everyone. reply kyleee 1 hour agorootparentYes, if someone shares they were abused as a child, it becomes much more fraught to tell everyone you messed up two poached eggs while making brunch last weekend and you are not quite the chef you hope to be. That’s why you have to volunteer to go first, because then you have plausible deniability about the expected level of vulnerability. reply fnordpiglet 11 hours agoprevThese symptoms are a classic sign of burn out. One thing I notice in your writing is you’re very tied up in things having meaning and mattering in some specific way. This itself can lead to burnout because if everything must matter you must be emotionally invested in everything. But you can care without it mattering to you. You can do a good job without being totally invested in everything about it. You can love what you do without it having significance in every detail. In a complex job with a fast pace, a fair amount of tech debt around every edge, a relentless pace of innovation happening, and - yes - growth, there’s too much to be invested in everything. It doesn’t have to matter that much. The parts you really care about, the craft and quality of your work, your relationships, mentoring and growing the people around you, seeing things get better one piece at a time, and a few things - they can matter. But everything can’t. And even those you have to at some very deep level realize don’t matter really. Stripe doesn’t really exist in this world. It’s a shared fiction to help frame the interactions between you and a few people you actually interact with in a day. The real truth is the only thing that happens in your days is you type on a computer and talk to a few people. It actually doesn’t matter in any meaningful way what you typed or some higher purpose around humbling honesty or exothermic curiosity or PMEs or whatever stories we tell ourselves to create some sort of reality out of the fiction. The only important things you really do is how you shape the lives of the people you interact with, and how you shape your own life. Burnout is hard. Adopt a daily meditation practice. Let your mind heal by letting go of meaning and practice enjoying the moment you’re in with whomever you’re with, but most especially yourself. The joy will come back faster the faster you let go of things needing to matter or have deeper meaning, especially when those things are a fiction like a company or a career or any of the other small and big lies we’ve been told and we reinforce to ourselves daily. I know I’ve been there man, and I know exactly - exactly - the sensations and experiences you describe. It gets better, but I think once you get there it never totally goes away and it’s easier to slip back. FWIW I don’t think burnout is the same as depression. I’ve felt both and burnout is different. It’s that loss of ability to engage - which overlaps with depression - but usually doesn’t come with the thoughts of hopeless despair and desire for life to be over. It’s just more of a deadness and inability to initiate what you think you should want to do but can’t, and it pervasively impacts everything. It gets better. reply ryandrake 3 hours agoparentThe way I conquered burnout was by having a life outside of work that isn't computer related. In my early twenties, the only notable attribute of my life was work and computers. I'd wake up in the morning, go to work programming things, then get home and either do programming side projects or play computer games, then I'd go to sleep and repeat it the next day. I had no hobbies, no friends or acquaintances outside of the people I met at work, and nothing to drive me besides the artificial goals at work. Recipe for burnout and it constantly happened. Eventually, I got married, had a family, found things to do in the community, and do plenty of sports and hobbies that involve shutting off the computer. Even though my jobs after my 20s have been even more soul-crushing than the ones I had earlier, I've never had a hint of burnout. When I'm done with work, I shut off the computer and that's it. I'll go build a piece of furniture, or repair my car, spend time with my kid, anything that doesn't involve office politics, two week sprints, or status reports. Just having a life outside of work has done wonders for my mental health. No meditation or spirituality needed! reply wtroughton 3 hours agoparentprev“A shared fiction in the world to help frame interactions between people”. You sir are a natural philosopher. I think what’s universally important is to keep active and engaged in what we’re doing. Some of us have to tell fictions that what we’re doing has meaning, whilst others just know what brings meaning to their life. reply 3vidence 9 hours agoparentprevCommenting to save this for myself. Of the many many hours I've spent on this website, this comment probably made me feel more relaxed than anything else. Please continue your kind and helpful words on the internet:) reply morning-coffee 4 hours agoparentprevVery well said and spot on. Thanks for posting. reply replwoacause 6 hours agoparentprevVery insightful comment and it describes quite accurately what my experience with burnout has been. Thank you reply jalopy 6 hours agoparentprevGreat advice. What books/resources do you suggest to follow more on this path of meditation (possibly also stoicism)? reply Mindjolt 3 hours agoparentprevCommenting because I want to revisit this at a future point of time reply rnts08 5 hours agoparentprevThis is spot on. Thank you for pointing it out. reply dionian 4 hours agoparentprevI second this. Well put reply Gooblebrai 9 hours agoparentprevVery good write-up, thanks! reply alexpetralia 5 hours agoparentprevBingo. reply kelvinjps10 2 hours agoparentprevGreat comment reply guitheengineer 12 hours agoprevI’ve seen this pattern repeated so many times that I feel like it can be generalized: when your mental health collapses nothing else holds value, it really doesn’t matter if you achieved your dream job, got all the prestige and income you initially desired, being mentally healthy is the basis of the pyramid. Something I learned based on that is to really prioritize it! Even if someone considers their career to be everything, realize that spending some of your earnings seeking professional help (therapy) is even a cheap investment considering that if you break and have to quit, you’re going to lose hundreds of thousands, and to recover it will suck (I’ve heard of people trying to quit tech altogether after burning out) Sneaking something else related to mental health: sleep should be #1 he",
    "originSummary": [
      "Jon de la Motte shares his personal experience of leaving his job at Stripe without having another position secured, highlighting the mix of fear and excitement in making such a decision.",
      "Despite a successful career at Stripe, Jon faced challenges with depression and motivation, leading him to take time off for mental health.",
      "He is inspired by a poem to embrace change and hopes to connect with others who might feel isolated in their struggles."
    ],
    "commentSummary": [
      "The author shares their experience at Stripe, dealing with a challenging work environment and a demanding manager, which contributed to anxiety and burnout.",
      "This experience underscores the significance of mental health, setting boundaries, and the influence of corporate culture on personal well-being.",
      "The author advocates for prioritizing mental health, seeking therapy, and maintaining a work-life balance to mitigate burnout."
    ],
    "points": 321,
    "commentCount": 293,
    "retryCount": 0,
    "time": 1730508282
  },
  {
    "id": 42021237,
    "title": "Linux on Apple Silicon with Alyssa Rosenzweig [audio]",
    "originLink": "https://softwareengineeringdaily.com/2024/10/15/linux-apple-silicon-alyssa-rosenzweig/",
    "originBody": "Linux on Apple Silicon with Alyssa Rosenzweig By SEDaily Podcast Tuesday, October 15 2024 Audio Player 00:00 00:00 00:00 Use Up/Down Arrow keys to increase or decrease volume. Podcast: Play in new windowDownload Subscribe: RSS Asahi Linux is a project that aims to port Linux to Apple Silicon chips, which use a custom ARM-based architecture. The project is fundamentally important given the popularity of Apple Silicon Macs, and it’s also a heroic effort because Apple Silicon is an entirely undocumented platform. Alyssa Rosenzweig is a well-known computer scientist who describes herself as a graphics developer passionate about software freedom. She is currently a contractor at Valve where she develops open source software to improve Linux gaming. Alyssa is also a contributor to Asahi Linux and works on reverse-engineering the Apple M1 GPU, among other contributions to the project. Alyssa joins the podcast to talk about reverse engineering hardware, Asahi Linux, new advances in gaming on Asahi, and more. Sean’s been an academic, startup founder, and Googler. He has published works covering a wide range of topics from information visualization to quantum computing. Currently, Sean is Head of Marketing and Developer Relations at Skyflow and host of the podcast Partially Redacted, a podcast about privacy and security engineering. You can connect with Sean on Twitter @seanfalconer. Please click here to see the transcript of this episode. Sponsorship inquiries: sponsor@softwareengineeringdaily.com SEDaily Sponsors Are you among the 65% of developers who still hard-code secrets in source code? Storing machine and infrastructure secrets in code, unencrypted env files, or messaging apps can make your business more vulnerable to leaked secrets and data breaches. Bitwarden Secrets Manager offers a super straightforward solution to this problem—it prevents secret leaks by making it easy to manage and deploy machine and infrastructure secrets all from one secure location. Bitwarden is unique because it’s open source, end-to-end encrypted, and can be easily deployed into your existing environments with a robust CLI, SDKs, and out-of-the-box integrations like Kubernetes, GitHub, and Ansible. Start a free trial today at bitwarden.com/secrets! If slow QA processes bottleneck you or your software engineering team and you’re releasing slower because of it, you need to check out qawolf.com/sed. QA Wolf gets engineering teams to 80% automated end-to-end test coverage and helps them ship 2x faster by reducing QA cycles from hours to minutes. With over 100 5-star reviews on G2 and customer testimonials from Salesloft, Drata, Autotrader, and many more, you’re in good hands. Go to qawolf.com/sed to see if they can help you squash the QA bottleneck. Twitter POPULAR Linux on Apple Silicon with Alyssa Rosenzweig 13.7k views Rust and C++ with Steve Klabnik and Herb Sutter 416 views Why We Switched from Python to Go 134 views The Big Changes in Python 3.13 with Łukasz Langa 72 views TypeScript ESLint with Josh Goldberg 46 views Software Daily Subscribe to Software Daily, a curated newsletter featuring the best and newest from the software engineering community. Exclusive Articles VMware Tanzu GemFire and Next-Generation Real-Time Application Development Uber’s LedgerStore and its Trillions of Indexes with Kaushik Devarajaiah GraphQL vs. REST: What Are They, and Which Is Better for You? Cloud Engineering Building Chess.com with Jay Severson Mastodon with Eugen Rochko AWS re:Invent Special: PartyRock Generative AI Apps with Mike Miller Business and Philosophy Startup Investing with George Mathew KubeCon Special: Docker with Justin Cormack Software Architecture with Josh Prismon Greatest Hits Hardening C++ with Bjarne Stroustrup Surviving ChatGPT with Christian Hubicki Special Episode with George Hotz Popular Linux on Apple Silicon with Alyssa Rosenzweig Rust and C++ with Steve Klabnik and Herb Sutter Why We Switched from Python to Go The Big Changes in Python 3.13 with Łukasz Langa Slack Data Platform with Josh Wills Hackers Making React 70% faster with Aiden Bai of Million.js Cross-functional Incident Management with Ashley Sawatsky and Niall Murphy SDKs for your API with Sagar Batchu Data Hyperscaling SQL with Sam Lambert Spring AI and Java in 2024 Iceberg at Netflix and Beyond with Ryan Blue",
    "commentLink": "https://news.ycombinator.com/item?id=42021237",
    "commentBody": "Linux on Apple Silicon with Alyssa Rosenzweig [audio] (softwareengineeringdaily.com)270 points by tosh 22 hours agohidepastfavorite215 comments jsheard 20 hours agoI haven't listened to this podcast yet so I don't know if this comes up, but a particularly scary part of running a custom OS on Apple Silicon machines is that the internal speakers temperature is regulated in software. The Asahi devs have had to painstakingly reverse engineer and reimplement the safety DSP that macOS uses on each device, and add some safety margin, because if they get it wrong they could literally destroy the speakers (and IIRC at least one of their own MacBooks did have its speakers sacrificed along the way). I wonder if there will be a similar issue with the displays when Asahi gets around to supporting HDR on machines equipped with FALD mini-LED backlights (or XDR, as Apple calls it). HDR displays usually regulate their brightness to keep the panel from getting too hot, and if Apple does that in software too then Asahi will need to replicate it. reply neya 11 hours agoparentAs someone who builds speakers commercially, there aren't any devices that will actually monitor a speaker's temperature. I think what you mean is perhaps, wattage? The speaker's temperature arises from the coil, which is what moves to produce sound. In large speakers, I've seen manufacturers attach some sort of sensors pointing at the moving coil to produce an estimate, but for the speakers used in the Macbooks, they are so thin that this would be a near impossible feat. Maybe Apple uses an estimation from the DSP based on the watts supplied vs time. But, certainly, you cannot attach any sort of device to the coil without making it distort or non-functional. I know that Apple like to supply the speakers with slightly more power (Watts) than they can deal with to get them loud, but I haven't seen nor heard of this temperature monitoring so far. I couldn't find anything related to it either. Please share citations, if you have. reply mcpherrinm 10 hours agorootparentThe temperature is modelled based on how much power is put through it. Power, not temperature is measured. https://github.com/AsahiLinux/speakersafetyd has a readme and the code reply rmu09 10 hours agorootparentprevI don't know what apple is doing, but in theory, it should be possible to measure ohmic resistance of the coil, and infer temperature via PTC coefficient of coil material. The effect isn't very big at temperature differences you could tolerate in a loudspeaker coil though (~0,5%/K). reply BoingBoomTschak 8 hours agorootparentprevYou're probably right. I looked at Neumann (\"Independent soft clip, peak and thermo limiters for woofer and tweeter; Woofer excursion limiter; thermo limiter for the electronics and amplifiers\") and Genelec (https://www.genelec.com/key-technologies/protection-circuitr...) for info - as they're the reference in the monitoring world - and my guess is that it's implemented like Apple, via some simulation function from time-integrated wattage and frequency to bool (limit on/off). Apple isn't the first to use advanced limiters to get the most of loudspeakers, if I remember well, this trick is what allowed Devialet to make the Phantom what it is. Which is why loudspeaker measurements should _always_ include something like this: http://0x0.st/XG1y.png reply userbinator 18 hours agoparentprevThe fact that speaker temperature even needs to be considered is something that boggles the mind. I can't find any other references elsewhere on the Internet. Are you sure it's not DC blocking that they've cheaped out on somehow? It somewhat reminds me of this: https://news.ycombinator.com/item?id=7205759 reply jsheard 18 hours agorootparenthttps://github.com/AsahiLinux/speakersafetyd#some-background... They do it because it lets them drive the speakers much louder than they could do safely with a simple power limiter. Apparently there are amplifier chips which can do that smart regulation internally but Apple decided to do it in software for whatever reason. Maybe they can squeeze out more sound quality with a more complex DSP that way, I doubt it's cost-cutting given their margins. reply TaylorAlexander 12 hours agorootparentCertainly I can see the appeal of implementing control in the silicon I own rather than buying a vendor's chip and having to deal with their supply chain, their sales team, their space claim on my board, etc. reply adgjlsfhk1 17 hours agorootparentprevIt's probably a combo of cost-cutting and control. If you use a hardware chip, the chip itself only costs cents, but space is at a premium, and there assembly costs and reduced flexibility (you have to lock in your design well in advance). reply userbinator 16 hours agorootparentCost-cutting, control, malicious compliance, and more importantly, plausible deniability. Apple has been caught sabotaging third-party repair multiple times and this sort of design is totally in line with that strategy. Making the design overly complex and fragile (and spending extra $$$ in the process), just to make it harder to correctly use, seems to be a common theme. They will of course come up with plausible arguments why their design is \"better\", but in reality it's only better for them. https://news.ycombinator.com/item?id=36926276 https://news.ycombinator.com/item?id=24955071 reply reaperman 15 hours agorootparent> Apparently there are amplifier chips which can do that smart regulation internally but Apple decided to do it in software for whatever reason. > Apple has been caught sabotaging third-party repair multiple times and this sort of design is totally in line with that strategy. Using fewer specialized hardware chips to dynamically regulate speaker temperatures and implementing it in software seems like it would be more repair-friendly, not less. Unless you meant this in some more general sense that doesn't include repair, per se? reply JoshTriplett 14 hours agorootparentprevI've worked with PC laptop manufacturers before, and they do exactly the same thing. In every case, it's either \"how can we take really cheap speakers with limited placement options and make them sound decent, to save costs\", or \"how can we take slightly better speakers and slightly better placement options and make them sound like surround sound so our audio can be a selling point\". It doesn't require malice or conspiracy to wind up with a closed proprietary design that makes up for cheap flawed hardware with software workarounds. It's the default if you don't have the opposite as an key value and make a deliberate effort to achieve it. reply josephg 11 hours agorootparentYeah - the same thing has been happening for well over a decade with phone cameras. I bought a fancy full frame dslr camera recently and the image quality is incredible. But the sensor on my camera is 30x larger than my phone’s sensor (or something like that). And good lenses are massive - my arms get a workout from shooting. It really puts into relief how weird it is that we get such good quality out of phone cameras. They’re almost as much generative AI images as they are actual photos. reply kaonwarb 15 hours agorootparentprevOf all of those reasons, malicious compliance and plausible deniability seem the least likely to me. With and from what, exactly? I get being upset at Apple for e.g. 30% take on in-app purchases. But what exactly would they be trying to do by making it complex to control their speakers? reply userbinator 15 hours agorootparentBut what exactly would they be trying to do by making it complex to control their speakers? Making it harder for alternative OSes to use their hardware without accidentally damaging it, further cementing macOS' position as the only OS to be trusted. It's similar to their war against third-party repair shops by deliberately making it difficult to replace parts, even with genuine ones --- see my second link about the iPhone 12 camera. This is the automotive equivalent of adding sensors and ECU firmware that detects if the engine is using the manufacturer's proprietary oil, and subtly changing the parameters to decrease power and fuel economy, increase emissions, and/or shorten the life of the engine if it isn't. Then blaming third-party repair shops when customers complain. reply josephg 11 hours agorootparent> Making it harder for alternative OSes to use their hardware without accidentally damaging it, further cementing macOS' position as the only OS to be trusted. If apple wanted to kill asahi linux, they wouldn't have had to lift a finger. Its the opposite - Apple engineers have needed to do several small things to keep the door open to easily run 3rd party operating systems on their computers. Remember, a modern mac has essentially identical hardware as an ipad. Apple has locked down the entire boot process and uses digital signatures all through the boot chain. They actively changed how macs boot to make sure things like asahi linux can run on their computers at all. I don't think they deserve special praise for that. But it does make it a stretch to claim their speakers were intentionally designed hurt linux-on-mac. If they wanted to stop asahi linux, they had plenty of much easier, much more effective ways to do so. reply m4rtink 11 hours agorootparentSounds like \"be glad they gave you some bones from the table\" instead, you know, the company providing the actual proper means for users to reliably run whatever on the hardware they bought, not just the manufacturers blessed OS. Sometimes I wonder if it really makes sense to spend so much time to do the work Apple should have done in the first place & with no guarantee it will even work after a firmware upgrade or on the next model. Spending the same effort on more open platforms or open hardware efforts might be wiser in the long term. reply doix 11 hours agorootparentOn one hand I agree with you, on the other hand I'm happy they are taking the effort to do this because it will reduce e-waste. When those MacBooks no longer receive updates, they can get a second life thanks to this work. reply josephg 10 hours agorootparentprevYes, I did say in my comment that I don’t think they deserve special praise for making a computer a computer. I have complicated feelings about it all too. On one hand, I wish my Apple devices were more open: I think the App Store tax is anticompetitive abuse of a monopoly. I have an expectation of actually owning the things I buy, and I don’t feel that way about the software ecosystem on my iPhone. On the other hand, I adore the security and cross-application sandboxing that iOS brings. I wish Linux had more ways to sandbox applications from one another - since it’s clear that “trust all computer programs” is an insanely idiotic policy in 2024. And “implicitly trust all code” is de facto how Linux, npm, cargo, etc all run today. My comment was in response to the idea that the speakers are maliciously designed to hurt asahi Linux, which just seems obviously wrong given how much power Apple has over their devices. They could completely kill asahi Linux with a flick of the wrist if they ever want to. There’s no way that’s the reason for their complex speaker setup. reply userbinator 8 hours agorootparentMy comment was in response to the idea that the speakers are maliciously designed to hurt asahi Linux, which just seems obviously wrong given how much power Apple has over their devices. They could completely kill asahi Linux with a flick of the wrist if they ever want to. There’s no way that’s the reason for their complex speaker setup. If they killed off asahi Linux, they would face far more backlash than if they just made it harder for them (with ostensible reasons why), while keeping it strictly worse than macOS. reply imwillofficial 12 hours agorootparentprevConsidering how well Apple speakers perform for their size, I would say you’re making a huge leap not supported by evidence. They make and sell more speakers than any other company on earth and are routinely praised for their quality to size ratio. reply ruthmarx 15 hours agorootparentprev> Apple has been caught sabotaging third-party repair multiple times and this sort of design is totally in line with that strategy. Making the design overly complex and fragile (and spending extra $$$ in the process), just to make it harder to correctly use, seems to be a common theme. This, and the nannying nature of their OS is why I could never have a mac as a primary machine. I'm always slightly mind-boggled at the amount of technical people that do. But then I guess many just live in an IDE which works fine. reply musicale 17 hours agorootparentprevI wish they had a hardware power limiter, because I am pretty sure a buggy Boot Camp audio driver damaged my MacBook Pro speakers by overdriving them. Also I note that it took them more than a decade to fix the bug where left-right balance would drift. reply alpaca128 9 hours agorootparentThis was indeed a known problem with the 2016 Macbook Pro, the driver initially liked to destroy the speakers. reply nicoburns 13 hours agorootparentprevApple has a very long history of implementing hardware drivers \"in software\". Steve Wozniak famously did this for a floppy disk drive for one of Apple's early computers. I think it's just a different, integrated, approach to hardware a software development. If you're doing things custom anyway, then why add an extra chip? reply bboygravity 12 hours agorootparentWhy add an extra chip? Because: 1. Software is more likely to fail at protection with worse consequences when it does (fire, damaged goods, warranty claims). Not just now, but also the future updates. 2. It eats away at the resources that are intended for the user. In other words: it makes the machine slightly slower for the user, for no good reason to the user. 3. You can do things that are impossible in laptop OS software. It gives redundancy, even if the OS freezes you can still make sure the speaker doesn't overheat. If it's implemented in a seperate chip. Also there is real time ability, etc. 4. It makes the OS and drivers much much simpler, which is important if you want to support other OSes on the same laptop. Advantages, for Apple, to do it in software: 1. Software upgrade is easier and cheaper (assuming they never ever fail). 2. Cheaper. 3. You can keep competing OS'es off of your hardware, because it's too hard to write drivers for your secret sauce closed source drivers that include functionality that is \"preventing parts from frying themselves\". reply Arch-TK 18 hours agoparentprevThe control loop is handled by the OS? What if the (relatively complex and therefore much more likely to crash) OS crashes? Why would there not at least be some kind of basic thermal throttling implemented in hardware as a fallback? Oh wait, it's Apple, never mind. reply kccqzy 18 hours agorootparentIt's managed by the Linux kernel communicating with a user space daemon (speakersafetyd). If the user land crashes or if the user space daemon is too slow the kernel can still fall back to a ridiculously low limit that will not damage the speakers for any audio. If the kernel crashes, well you get no audio in that case. IIRC the reason they couldn't do it completely in the kernel was because the temperature model uses floating point which is not allowed in the kernel. reply userbinator 17 hours agorootparentIf the kernel crashes, well you get no audio in that case. More likely to be repeating audio, whatever was last in the buffer. reply wbl 16 hours agorootparentprevA floating to fixed conversion is possible but would take some painstaking numerical analysis. reply userbinator 18 hours agorootparentprevIn the late 2000s I remember hearing of PC laptops which if left in the BIOS setup would overheat and shut down due to the fan control being exclusively done by OS drivers, so this sort of \"planned fragility\" isn't exclusive to Apple. reply Narishma 13 hours agorootparentI had one such laptop. It was always a crapshoot whether Windows updates (the big ones where it rebooted one or more times) would succeed or not before the laptop overheated and shut down. The reason was that the BIOS didn't do any power management and would always run the CPU at maximum power with the fans blowing like a jet engine. It was only once Windows (or Linux) was fully booted that they would take over and do proper power management. But the Windows updates were so slow that they would often spend too much time in that pre-boot environment leading to overheating and shutdown. reply Arch-TK 8 hours agorootparentprevIt's not the late 2000s anymore, this is a flagship product. reply ranger_danger 17 hours agorootparentprevHow do you know it was intentional? reply userbinator 16 hours agorootparentNo one thinks \"let the fans stay off and overheat\" is a good idea unless they were convinced to do that against their common sense. Other manufacturers' models either had the expected automatic fan control via EC firmware, or defaulted to the fans being on some intermediate speed. reply unit149 19 hours agoprevReverse engineering the first bit of Apple's flagship M1 CPU, which was implemented in the original iPhone released back in '08, through an ARM-based architectural analysis, and integrating it into Linux kernel system calls is an extreme measure. This reduplication of the original dump, checking its hex value, then altering it to see if the application is functional. Doing x,y, and z, then seeing if it works inside VM hypervisor-space. https://softwareengineeringdaily.com/wp-content/uploads/2024... reply chubs 18 hours agoprevWhat would be possible if everyone interested in Linux-on-macOS chipped in $5 each month? I wonder if this could fund a big enough team to make this super-polished. reply ajdude 18 hours agoparentI definitely respect them for everything they're doing, and I really want to like this project, but my very first exposure to this project was when someone linked to it on HN, I clicked the link, and the website told me that since it detected that I was from HN I am not welcomed and it refused to show me the article... it just left a really weird taste in my mouth. I don't know if they still do that but that's the first thing I think of every time I see Asahi mentioned or I think about giving it a try. reply sliken 11 hours agorootparentThat was the result of an previous thread on asahi. It was the only time I've been embarrassed by the HN community and it's treatment of people contributing to open source. reply raffraffraff 9 hours agorootparentLink to back story? This is the first I've heard of that. reply bschwindHN 17 hours agorootparentprevJust based on the quality of the comments that comes from HN towards the Asahi project, I don't blame them for doing that. reply christophilus 17 hours agorootparentI dunno. To me, the comments seem 99% positive every time this project is discussed here. What are you referring to? reply virgildotcodes 17 hours agorootparentI think there are a small minority of people who consistently want to guess at the identity of Asahi Lina and make comments about her gender. reply Bitnotri 16 hours agorootparentI don't think it's that hard to find out, given the discussion that was already held here on the topic reply not2b 13 hours agorootparentThat discussion and similar discussions that seems to come up regularly was probably the reason they don't want HN traffic, and it's unfortunate the the moderators here haven't worked harder to shut that stuff down. The gender of their developers just isn't relevant to the quality of their work. reply raffraffraff 9 hours agorootparentThat's really surprising. I've been on HN a while now and I've never noticed that type of comment. The reason I'm on HN at all is that it reminds me of the \"old internet\" where people posted links and discussed stuff. You're always gonna get someone saying stuff you don't like, but banning all of HN considering how decent the 99.9% is.. is odd. It's not 4chan. reply Elinvynia 9 hours agorootparenthttps://news.ycombinator.com/item?id=42014188 Well, you just didn't look hard enough, this is from literally the last thread about Asahi before this one. Pretty much every single post about Asahi Linux on HN has transphobic comments. reply asbonn 2 hours agorootparentThanks for linking my comment. Would you mind explaining exactly why you disagree with it? reply esperent 8 hours agorootparentprevThat's from a newly made account (green name), and it's flagged so it's not visible to most users (including me). There's no forum in the world with open sign ups that can prevent a few assholes from joining. reply fastball 13 hours agorootparentprevCan you find me a single HN comment on this topic that states (or even implies) that the gender of a developer is relevant to the quality of their work? Because I haven't seen anything like that, so you seem to be attacking a strawthem. reply fastball 13 hours agorootparentprevAnyone who actually cares about Asahi Linux knows who Asahi Lina is. If it is actually intended to be a secret it is one of the worst kept secrets I can think of. reply bschwindHN 11 hours agorootparentprevMaybe it has changed since the project was launched, and moderation has been applied. I remember lots and lots of \"what's the point?\" and \"it's a shame your efforts are wasted\" sentiments, and \"apple is just going to snap their fingers and shut this down one day\". They've already thought about these things and it's still what they chose to work on, so I can understand them blocking a contributing source of those comments. That being said, I agree the more recent threads have more positivity than negativity so that's good. reply dymk 17 hours agorootparentprevHonestly, I'd want to donate to them more for that reply AI_beffr 21 hours agoprevits insane to me that people are working so hard on reverse engineering apple silicon. like, the diagrams are right there in cupertino. it just seems like such a waste. its like during some kind of economic depression there are people starving and struggling while a bunch of food and resources are just sitting around not being used. existential grid-lock. reply robbiewxyz 21 hours agoparentThis definitely sucks. I feel similarly about e.g. the jailbreaking community: I appreciate the work they do and at the same time I very much wish it wasn't necessary. If Apple and other companies like them were a little less greedy we could have far more nice things for free and Alyssa and other brilliant engineers could go work on other great projects. Also if regulators were a little more vigilant and a little less corrupt. Someday. reply WD-42 18 hours agorootparentTo me what makes it suck even more is the fact that Apple has no qualms exploiting FOSS themselves. BSD, WebKit, their new “game porting toolkit”. And look what they provide in return. It’s so gross. reply musicale 17 hours agorootparentApple does develop WebKit, Swift, and clang/llvm, as well as other things like open source ML models, but I understand what you're saying. One important lesson I've learned regarding open source is that companies absolutely love it when you work for them for free. Something I've learned about Apple is that one of their primary businesses is selling hardware with proprietary software running on it. reply WD-42 17 hours agorootparentAgree, at least WebKit can be used outside of Apple. They still did KHTML dirty though. Your point about working for free is right on the money. I get that asahi is probably intellectually stimulating to work on, but I couldn’t do it knowing I am essentially enriching a company that doesn’t have public benefit in mind. reply musicale 17 hours agorootparentClang can definitely be used outside of Apple, and can even compile Linux. Swift technically can be used anywhere, though it is largely driven by Apple and laughs at backward compatibility. The people I know at Apple actually do have public benefit in mind. They believe in things like usability, privacy, accessibility, sustainability, etc. They don't want to replace you with intrusive AI (yet). And personally I like Apple's products and am using one right now. Unfortunately all large companies tend to turn into monsters that do their best to grind up resources - natural or human - in order to turn them into money. And the designer of the iPhone regrets turning us into zombies - that was not an intended effect. reply ruthmarx 14 hours agorootparent> And the designer of the iPhone regrets turning us into zombies - that was not an intended effect. People were already zombies. They just swapped out television for smart phones. reply matheusmoreira 14 hours agorootparentprev> companies absolutely love it when you work for them for free Which is why everyone should AGPLv3 their code. reply runjake 21 hours agoparentprevI think it only fuels the possibility that Apple would open up the architecture documentation where it otherwise wouldn't if you didn't have people diligently reverse engineering it. Something similar to this happened in the early days of the iPhone, with the iPhone Dev Team. Initially, iPhone \"apps\" were going to be web pages, but then these reverse engineers came along and developed their own toolchain. Apple realized they had to take action and their \"webpages as an app\" strategy wasn't going to work. reply hu3 21 hours agorootparentThat's a rather incomplete, revisionist and rose tinted glasses view of the history of native vs web apps in iPhone. A much more plausible theory is that Apple likes their 30% app store commission from big players. > https://www.apple.com/newsroom/2023/05/developers-generated-... \"App Store developers generated $1.1 trillion in total billings and sales in the App Store ecosystem in 2022\" People forget the only thing fueling big corps is profit. reply runjake 20 hours agorootparentI was there, writing apps with the DevTeam's toolchain before Apple ever released theirs. Were you? Also, I assume you haven't read the Steve Jobs biography, which discusses this and contradicts your point. One positive outcome of your comment is that it reminded me I still have the 2008 book, \"iPhone Open Application Development\" by Jonathan Zdziarski. That was a nice walk down memory lane. https://www.amazon.com/iPhone-Open-Application-Development-A... reply chrisoverzero 20 hours agorootparentprevSure, but at the time that Apple made the decision, they had $0.0 trillion in billings and sales. reply hu3 20 hours agorootparentA decision which changed once, you know, they saw the income potential. reply refulgentis 20 hours agorootparentI was there, part of a small community writing apps pre-SDK. Neither, I, nor anyone else, can promise you it wasn't just a simple $ calculation. That being said, literally every signal, inside, outside, or leaked, was that apps / public SDK, if it existed meaningfully before release, had to be accelerated due to a poor reaction to the infamous \"sweet solution\", web apps. I agree its logically possible, but I'd like to note for the historical record that this does not jive with what happened, at the time. Even setting that aside, it doesn't sound right in the context of that management team. That version of Apple wasn't proud of selling complements to their goods, they weren't huge on maximizing revenue from selling music or bragging about it. But they were huge on bragging about selling iPods. reply hu3 20 hours agorootparentThanks. I appreciate your information. Always nice to know how things started. reply reaperducer 21 hours agorootparentprevThat's a rather incomplete, revisionist and rose tinted glasses view of the history of native vs web apps in iPhone. As someone who built one of the first web apps featured by Apple, I can say that your view, too, is incomplete and revisionist. A much more plausible theory Theories are not necessary. Apple was very up-front about its trajectory with the iPhone at launch. reply hu3 20 hours agorootparentup-front at launch doesn't prevent changing their minds when looking at world record revenue. what makes you think it was set in stone? reply kergonath 19 hours agorootparent> what makes you think it was set in stone? It was not. But you got contradicted by people who actually remember what happened. It is fairly well documented, and was common knowledge even at the time. Jobs was initially sold on Web Apps for several reasons, and the state of iPhoneOS 1 and its internal APIs was very precarious and not stable enough for serious third-party development. Again, this was known at the time thanks to the jailbreak community, and it has been explained in details over the years by people who left Apple since then, and Jobs himself in Isaacson’s biography. When they pivoted towards the AppStore model, there was no predicting how big it would become, or even if it would be successful at all. The iPhone was exceeding expectations, but those were initially quite modest. It was far from world-record revenue. reply sschueller 20 hours agorootparentprevApple would close down their macos just like iOS if they could get away with it so they can get their 30 percent on apps installed . However since their moat is now filling with European soil this is not something they will attempt at this point IMO. reply GeekyBear 19 hours agorootparentApple literally designed a new boot loader for Macs that allows the computer's owner to install and run an unsigned OS without having it degrade the security of the system when booted into the first party OS. That didn't happen accidentally. reply bqmjjx0kac 19 hours agorootparentBut perhaps it happened not out of the goodness of their hearts, but for unfathomable reasons like warding off antitrust lawsuits. reply nicoburns 11 hours agorootparentMy guess would be that it was personally advocated for by someone who has enough influence within Apple to make it happen. Possibly someone on the hardware team, as I hear that the people developing the Apple Silicon processors run linux on them while they're in development. This used to be one of the best things about Apple when Steve Jobs was still running the company: you'd get a bunch of features that a purely profit-focussed \"rational business\" would never approve, just because Steve wanted them. And I suspect Apple still has some of that culture. reply GeekyBear 17 hours agorootparentprevGiven that the boot liader design predates any antitrust action against this Apple, you'll have to find a different conspiracy theory. reply panick21_ 18 hours agorootparentprevOn the internet it seems antitrust law can just be used to explain every. Antitrust actually has a pretty strict legal definition. And not a lot of thing fall into that. And if it Antitrust did apply, it would apply far more to the IPhone. It would take an outright legal revolution in the definition of antitrust for this to be even a remote possibility, and frankly that is not happening. reply bqmjjx0kac 16 hours agorootparentNot all lawsuits need to be legitimate. They just need to be plausible and expensive to influence corporate decision-making. reply panick21_ 4 hours agorootparentApple has plenty of money and lawyers. If they are 100% sure they win, they have no reason to be afraid of such a lawsuit. By your logic, Apple could also be sued for not doing it. See how your logic doesn't add up? reply musictubes 16 hours agorootparentprevThis is tiresome. They cannot lock down the Mac without losing one of its biggest markets, software development. It was mentioned at a WWDC 5 or 6 years ago I think that software developers were the largest of the professional groups using Macs. You can’t have a development environment that doesn’t allow for the downloading, writing, and installation of arbitrary code. As long as Apple wants people to develop software for its platforms and/or sell to web developers, Android developers, scientific computing, etc. they will not lock down the Mac. reply sschueller 5 minutes agorootparentI don't buy that for one second. If they cared for software developers they would have never moved the ESC key into the touchbar back when that was a thing. Also if they really cared, they would operate/manage brew and integrate it better than have people spend their free time to make apple's products more usable for software developers. reply qubitly 21 hours agorootparentprevExactly! It’s like, Apple never budges—until someone reverse engineers it. Maybe Asahi can finally give them a nudge reply kergonath 19 hours agorootparent> It’s like, Apple never budges—until someone reverse engineers it. Could you give 3 examples of this? Because I cannot think of many. reply robgibbons 16 hours agoparentprevI have to agree with this take, as much as I appreciate the indelible hacker spirit of jailbreaking closed hardware and making it available to freedom loving individuals... A huge, vocal part of me also feels like the entire effort is truly just empowering Apple to sell more closed down hardware and even the freedom loving nerds will buy it because it's sexy. There's no getting around the sexiness of Apple hardware. But there's also no getting around how closed it is. You implicitly support Apple's business model when you buy their hardware, regardless of whether you run Asahi or Mac OS. reply hu3 21 hours agoparentprevSpecially because Apple seems to not care much about the project even after current progress. m3 support still not there (let alone m4) because things broke. Which is expected from Apple, they are just doing their thing and improving their products. If they cared they would have at least hired these people by now. It wouldn't make a dent in their budget. reply Twisell 20 hours agorootparentM3 and M4 is not there because the Asahi Teams have a roadmap and stick to it. They don't want to leave M1/M2 half botched before moving on to the next gen that will ultimately support more features. If you are not happy with the pace go on and contribute, but don't invent false issues. reply ksynwa 19 hours agorootparentYou are misreading the comment. It is indicting Apple, not the Asahi team, for not caring. If Apple cared and hired the Asahi folks and provided them with help, they would probably be able to churn out drivers faster. reply concerndc1tizen 19 hours agorootparentApple does not want it. reply ryukoposting 18 hours agoparentprevMy wife's 2017 MBP has gotten so dog-slow since Apple dropped support for it that it can't handle more than 3 Chrome tabs at a time now. The reality of Apple products is that the manufacturer sees them as eminently disposable. As early ARM macbooks age, the ability to run something, anything that isn't MacOS will be an asset. Otherwise, they're all landfill fodder. reply bschwindHN 17 hours agorootparentMy 2014 MBP is still going strong and can handle more than 3 browser tabs, not sure what's up with your wife's machine but that doesn't sound normal. You can also trade it back in to get recycled, no one should be straight up throwing computer hardware in the trash. reply ryukoposting 15 hours agorootparentMore than half of \"recycled\" e-waste just gets exported to developing countries without environmental regulations where it either gets burned or buried. The only sustainable thing you can do with a bad laptop is fix it or part it out, but for all my years taking apart fragile electronics, is it really worth the effort to take apart a device that was intentionally designed to be difficult or impossible for the owner to repair? reply Reason077 14 hours agorootparentprevThe last few macOS updates have really been killing performance on Intel Macs. Your 2014 is probably safe because it'll still be running an older macOS. reply jcul 10 hours agorootparentprevI have an old google nexus 7 tablet that I recently installed uboot and postmarketos on. I can ssh to it, run X apps over ssh, print directly from it. It's pretty cool. I also have a really old iPad 2. It works perfectly HW wise, screen, wifi etc. But is effectively a paper weight due to software. I am logged into it from my old Apple account, that was only ever used for this tablet. I have the username and password but cannot log in as I don't know the security questions, so I can't reset the device or try to install apps. I even phoned apple but they said there's nothing they can do. It pains me to just dump a perfectly good piece of hardware. reply ryukoposting 6 hours agorootparentWe have an iPhone 6 in a similar boat. It's my wife's old phone, and it'd be totally fine as a test phone for my work, but there doesn't seem to be a way to factory reset it without a passcode she doesn't know. reply cheesycod 20 hours agoparentprevFor many people, the Apple Silicon GPU is an interesting problem to solve given that the firmware is loaded by the bootloader and all and its actually generally easier to interact with than say NVIDIA while having decent perf. Also GPUs in general are really complex beasts involving IP from tons of companies in general. Would not be surprised if even Apple doesn't have the full schematics... reply vlovich123 20 hours agorootparent> and its actually generally easier to interact with than say NVIDIA while having decent perf I’m pretty sure that Turing and newer work the same way. The drivers basically do nothing but load the firmware & do some basic memory management if I recall correctly. reply WD-42 19 hours agoparentprevHonestly I can only imagine it's because the team enjoys the challenge. Once they get bored/fed up, goodbye Linux on Macs. If you think you want to run Linux, don't buy hardware from a company that views it as a threat to their business model, simple as that. reply jvanderbot 19 hours agorootparentYou're just stating the problem the parent content was upset about. It's all well and good to state the facts and say \"face reality\" but in this case we all apparently know that it's a fragile state of affairs. reply kaba0 19 hours agorootparentprev> If you think you want to run Linux, don't buy hardware from a company that views it as a threat to their business model, simple as that. Show me any hardware that is 100% \"libre\"? Even the pinephone itself has plenty of closed source blobs running as firmware. reply beeflet 19 hours agorootparentIt doesn't have to be 100% libre. This is about booting any OS you want in the first place. If you take some random windows laptop off the shelf, it will boot linux (and continue to do so in the future) because they have to support UEFI. If you take a \"linux\" friendly vendor off the shelf, you may even have coreboot or something on-board. But with this apple situation there is no guarantee the next generation of hardware won't lock you out, or they might push out an OTA firmware update that locks you out. It's like porting linux to the playstation or something. reply WD-42 19 hours agorootparentprevFalse dichotomy. You know very well there is a spectrum on which Apple resides on nearly the opposite side of pine phone. reply kaba0 9 hours agorootparentDoes it? reply WD-42 59 minutes agorootparentListen to the podcast. reply acheong08 17 hours agoparentprevSeriously, Macs would be so much more attractive if Apple just straight up supported Linux reply tim-- 17 hours agorootparentIn a roundabout way, Apple tried this in the x86 era with OpenDarwin, and there was no interest for an Apple led open source operating system. Apple does not have a reason to support any other operating system. Apple engineers do however both officially and unofficially support Asahi, so there's that. reply linguae 16 hours agorootparentI’ll go back a little further: at one point before Apple purchased NeXT, Apple had its own version of the Linux kernel called Mklinux (https://en.m.wikipedia.org/wiki/MkLinux). reply WD-42 16 hours agorootparentprevOh please. OpenDarwin lasted what, 2 years? The people running it realized their efforts were merely going towards enriching OSX, it was not a truly open source effort. reply moffkalast 21 hours agoparentprevIf people wonder why some of us don't like Apple, this is the fundamental philosophy why. It's not about the M series, it's been their modus operandi since time immemorial. It's like if Microsoft owned x86 and nobody could run anything on it but Windows. And people would like it because it's a \"cohesive ecosystem\" or whatever. reply rched 20 hours agorootparentI'm not sure that's really the same thing. Apple doesn't own ARM and the main issue here seems to be the GPU no? Is this much different from how things work with Nvidia? I guess the difference is that Nvidia provides drivers for Linux while Apple does not. As far as I know Nvidia Linux drivers aren't open source either though. reply beeflet 19 hours agorootparentThe point is that apple acts as both the source of hardware and software. Your analogy is not applicable because you can't run apple's OS on generic third-party ARM hardware. reply rched 15 hours agorootparentBut isn’t this whole thread about running Linux on Apple hardware? I haven’t seen anyone in this thread complaining that they can’t run macOS on non Apple hardware. reply moffkalast 20 hours agorootparentprevNvidia is not much better, but they do only make one component and generally ensure compatibility. If Nvidia went full Apple, their cards would have a special power connector for the Nvidia PSU, a custom PCIe express lane that only works with Nvidia motherboards, which also requires Nvidia RAM sticks and only boots NvidiaOS. And also most of the software that would run on it would be blocked from running on other OSes because fuck you that's why. Also if you tried running NvidiaOS in a VM, they would sue you. It's still profoundly weird to me that nobody can run Safari outside MacOS, even for testing. At least the EU has strong armed them into dropping thunderbolt ports now, so we have that minor interoperability going for us, which is nice. reply trimethylpurine 19 hours agorootparentYou left out that they would also cost ~double per unit of performance. And that when Nvidia claims to be better for graphics and video, they can back those claims (albeit unfairly, some might say), whereas Apple marketing appears to avoid any price/value comparisons. So, I guess, even when you're dressing Nvidia up to sound ugly for a hypothetical, they still sound better than Apple. reply kaba0 19 hours agorootparentprevAre we living in the same world? Nvidia only recently started caring about Linux (due to profit obviously, it turns out servers don't run anything else nowadays). May I remind you of the famous `--my-next-gpu-wont-be-nvidia` flag in Linux compositor? Meanwhile, apple literally went out of their way to make secure boot for third-party OSs possible. reply jasomill 16 hours agorootparentConversely, Nvidia provides first-party Linux support for most of the hardware they sell, and Apple goes out of their way to make booting third-party OSes on the majority of hardware they sell (read: all non-Mac devices) all but impossible. reply kaba0 11 hours agorootparentExcept for the M-line where they went out of their way to make it possible in a secure way.. reply almostgotcaught 21 hours agoparentprevYou think that's bad? Imagine how much churn there is because NVIDIA doesn't have open source drivers. I'll actually do you one better: part of my PhD was working around issues in Vivado. If it were open source I could've just patched it and moved on to real \"science\" but that's not the world we live in. And I'm by far not the only \"scientist\" doing this kind of \"research\". reply esjeon 13 hours agoparentprevYeah, I agree. I do respect the effort itself, but it always feels like a huge waste of talent. Imagine what could have been created with all that time and energy. I believe the best solution to proprietary hardware is not to buy them in the first place. Buy from vendors who are more open/supportive, like Framework and Thinkpad. This helps those vendors keep supporting open source. reply talldayo 21 hours agoparentprevA recurring theme you'll encounter across most of Apple's products is that any feature that forces first-party Apple software to compete on fair terms with other products is conspicuously missing. reply tonymet 20 hours agoparentprevblame the lawyers. any effort to share specs would be an implicit license. reply robbiewxyz 20 hours agorootparentI'm not familiar with this. Suppose Apple released docs under an \"as is\" type disclaimer like is so common in the open source community: would doing so potentially come back to bite them? reply FooBarBizBazz 20 hours agoparentprevI get what you mean. I'm glad that they're doing this; it's great that the best laptop hardware is going to run Linux before long; it's a fun endeavor -- but when you zoom way out and take the philosophical view, yeah, it seems silly that it should be necessary, in the same that way it feels absurd that impressive and courageous feats in battle should have actually needed to happen. reply kaba0 19 hours agoparentprevIs it really a far comparison? Apple has a proper bootloader capable of secure booting 3rd party OSs. What part of the open-source ecosystem was built differently? It just so happened that after possibly even more painstaking reverse engineering, the responsible hardware vendor later realized that server machines are de facto linux, and they better support it properly. Like, that Intel chip working that good was not achieved differently, we just like to wear rose-tinted glasses. reply asdasdsddd 21 hours agoparentprevwhats the point of reverse engineering this again? reply linguae 21 hours agorootparent1. Even if one loves macOS, Apple doesn't support its hardware forever. Being able to run an alternative operating system on unsupported hardware helps extends that device's useful life. My 2013 Mac Pro is now unsupported, but I could install Linux on it and thus run up-to-date software. 2. Some people want to use Apple's impressive ARM hardware, but their needs require an alternative operating system. reply risho 21 hours agorootparentprevif you want an arm laptop with incredible specs, incredible build quality, incredible battery life and incredible performance that runs linux what other option is there? reply jrockway 20 hours agorootparentYeah, the M4 is apparently the fastest CPU on single-core benchmarks. If you want a fast laptop, you have to get it. Not being forced to use Mac OS would be nice. reply beeflet 19 hours agorootparentprevYou could wait a decade reply tokinonagare 20 hours agorootparentprevJust run Linux inside a VM. Problem solved. reply Philpax 19 hours agorootparentThis is not a comparable user experience to running Linux natively for a variety of reasons, but the most obvious one is the relatively limited graphics acceleration. That's pretty important for a variety of use cases! reply baq 21 hours agorootparentprevmacOS sucks. It does a disservice to the greatest laptop hardware package ever made. reply fragmede 21 hours agorootparentprevSome people really like the hardware but can't stand the software, and have the skills to do something about it. reply Teever 21 hours agorootparentprevIt is an inspirational demonstration of the hacker spirit and a way for the individuals involved to both expand their technical abilities and demonstrate them to prospective employers. I personally consider it very inspirational though I recognize that I will probably never be able to undertake such a difficult task. I can imagine that it is very inspirational to the next generation of extremely proficient and dedicated teens who want to master software development and explore leading edge hardware. reply umanwizard 21 hours agorootparentprevTo run Linux on MBPs reply dougall 19 hours agorootparentprevIt's fun :) reply SG- 17 hours agoprevthe negative comments in this thread are frankly disappointing especially for a place called \"hacker news\". like Linux doesn't have roots in reverse engineering and continued reverse engineering and people here constantly \"advocating\" for open source drivers from likes of Nvidia instead of the close source binary blobs. yet here someone makes great effort and most comments are negative Nancy's asking why it's being done or bringing up support issues with newer hardware revisions from a 1-3 person outfit that everyone said would be impossible to do. reply ddingus 16 hours agoparentThis effort is fantastic! I just ignore the drama and appreciate being able to learn something about the intense Apple environment. reply imiric 21 hours agoprevGodspeed to the Asahi team, but as much as I envy the performance and efficiency of Apple silicon, I could never depend on a small group of hackers to reverse engineer every part of a closed system and to maintain it in perpetuity so that I can run free software on it. As brilliant as this team is, and as much progress as they've made, fighting against a trillion-dollar corporation that can decide to shut it down at any moment is a sisyphean endeavor. Spending thousands of dollars on that bet is a hard sell, even for tech nerds. Not to mention that you'd be supporting a corporation that has this hostile stance towards their customers to begin with. Meanwhile, other x86 and ARM manufacturers are making substantial improvements that are shortening Apple's lead. You're not losing much by buying a new CPU from them in 2024 or 2025, but you gain much more in return. Most importantly, the freedom to run any software you choose. reply Wowfunhappy 20 hours agoparentAren't tons of Linux drivers for x86 laptops based entirely on reverse engineering? Maybe even most of them? I haven't used Linux seriously in almost two decades, but that's my memory. reply zeusk 20 hours agorootparentMost of the x86 platform (ACPI) is well defined and openly accessible (not free but open). There’s still some offenders (Surface, HP, Broadcom) that introduce quirks that break sleep and some HID accessories but most of it works out of the box. ARM has been the Wild West for a while but they’re going in the right direction with device trees et al. Apple however doesn’t have to care about the “wider” ecosystem since they left x86 for their own silicon and tighter integration from bottom up allows for some really nice end user experience. I still think it’s much better to use the VM infrastructure and just run Linux as a guest. Mac as a platform is very end user friendly as-is unlike Windows. reply pzmarzly 18 hours agorootparentDevice Trees are becoming an old thing now. With ARM SystemReady, most devices need to have UEFI, SMBIOS and ACPI. Only the SystemReady IR (IoT) variant is defined as using Device Trees. https://github.com/ArmDeveloperEcosystem/systemready-guides?... reply zeusk 17 hours agorootparentMicrosoft is the only one pushing for ACPI on ARM, but as you said they have hefty weight in that area. I don't think it is right for the platform, but if it works who am I to say. reply binkHN 20 hours agorootparentprevYou've been out of the game for too long; almost every major hardware vendor has at least one or two people that are regularly submitting patches to the Linux kernel. My ThinkPad work computer running Linux is a major thing of joy; in many ways it performs more reliably on Linux than it does on Windows. reply kaba0 19 hours agorootparentEven though, there are no lack of issues that are simply not cared about. E.g. thermal throttling for ThinkPads is a very annoying problem the best solution to is simply a python script that just periodically overwrites a memory location. reply RVuRnvbM2e 11 hours agorootparentIt isn't nearly so dire. The modern solution for this is https://gitlab.freedesktop.org/upower/power-profiles-daemon reply tucnak 9 hours agorootparentit's using dbus how modern can it be? :-) reply bravetraveler 20 hours agorootparentprevWe are well beyond the days of NDISWrapper, most of the kernel contributions come from hardware manufacturers or integrators reply imiric 20 hours agorootparentprevI'm not sure if \"tons\" is accurate, but some of them are, yes. And most of them are not great IME. Not discrediting the talented programmers who work on them, it's just the nature of supporting hardware in the dark, without any support from the manufacturer. Though this was more common during the early days of Linux, and nowadays many manufacturers offer some kind of support. reply renewiltord 20 hours agorootparentprevOpenChrome far exceeded Unichrome Windows drivers in performance. But things have changed. Modern engineers prefer “official” software. I understand why. Systems are more complex now. reply jasoneckert 20 hours agoparentprevWhile I don't feel I have enough information to comment about the likelihood that Apple would try to stop the Asahi project, those who are knowledgable are of the opinion that they would not. However, as a Mac Studio M1 owner that has used Asahi as a daily driver for software development since the first release (originally Arch, later Fedora), I can confidently say that I could care less. By running the software I want to run far faster than macOS could on the same hardware, Asahi has saved me countless hours and made me far more productive. And I'm incredibly grateful for this tangible benefit, regardless of what happens in the future. reply 7e 18 hours agorootparentYou claim that your apps are faster on Linux on an M1 than macOS on the M1; can you add more detail? Which apps, and did you run benchmarks? I find it hard to believe that Apple hasn't optimized apps to be faster on their own OS and hardware. reply jasoneckert 18 hours agorootparentIn short, all apps run faster - I have some more detail in this blog post (which also includes a link to my original blog post with additional performance details): https://jasoneckert.github.io/myblog/fedora-asahi-remix/ I suspect this is primarily due to Linux being a more performance-optimized OS compared to macOS, which seems to have introduced a great deal of bloat over the years. reply 2OEH8eoCRo0 20 hours agorootparentprevThey won't stop Asahi with a frontal assault, they'll stop it by churning out new chips every year until the work to support them all is unsustainable. reply WD-42 19 hours agorootparentNot sure why you are being down-voted. We're already seeing this with the team saying they won't work on M3 yet because they aren't even close to done with M2... reply SG- 17 hours agorootparentactually they're saying they're close to getting M3 support and the big thing that has prevented work on it as the lack of a M3 Mac Minis. reply kaba0 19 hours agorootparentprevHow is that apple's fault, nor any form of \"deliberate attack\"? Like, come on, neither of the parties are malicious, especially not for the sake of it. reply WD-42 19 hours agorootparentI didn’t say it was anyone’s fault. It’s the reality of a closed system. reply vlovich123 20 hours agoparentprev> As brilliant as this team is, and as much progress as they've made, fighting against a trillion-dollar corporation that can decide to shut it down at any moment is a sisyphean endeavor Apple historically cares very little about Linux on Mac whereas it seems like you’re talking about the non-Mac product lines. Indeed, they go out of their way, if I recall correctly, to make it possible and easier in the first place. reply imiric 20 hours agorootparentI wouldn't describe leaving the bootloader unlocked as \"going out of their way\" to make all of this possible. Clearly, if just booting another kernel would be sufficient, running Linux on their machines should be easy. Yet none of this is. \"Going out of their way\" would at the very least be providing documentation and support so that reverse engineering their hardware wouldn't be necessary. Also, what's not to say that they will decide to lock the bootloader just as they do on all their other devices? What does Apple practically gain by leaving it unlocked? If they're doing it as a gesture of good faith to their users, they're doing an awful job at it. Doing it so they can sell a negligible amount of machines to a niche group of hackers also doesn't make business sense. Depending on the good will of a corporation that historically hasn't shown much of it to the hacker community is not a great idea. reply Retr0id 20 hours agorootparentThe changes made were not as simple as not-setting a lock fuse bit. Making the bootloader unlockable in a way that didn't compromise their existing security model did require going out of their way. The status-quo for previous \"apple silicon\" bootchains (iphone, ipad, etc.) was not built this way. Even T2 macs had no way to boot custom firmware on the T2 chip, without exploits. Sure, they could've done way more, but evidently they'd rather not lock down the platform completely. reply sys_64738 20 hours agorootparentprevApple does new hardware bring up using the Linux kernel. reply yjftsjthsd-h 20 hours agorootparentIf they don't release that code to the public, what good does it do? (Also, if they are only doing a temporary in-house version for initial hardware work, they can do all kinds of ugly hacks that wouldn't really be good for upstream use any anyway.) reply skybrian 20 hours agoparentprevDon't let worries about future hardware get in the way of using gear that works in the present. If future Macs aren't supported for some reason, that doesn't break your current hardware, and you can buy different hardware next time. There are people running Linux on abandoned hardware from companies that went out of business, and that's okay. reply amelius 19 hours agorootparentNo, because from your perspective you are supporting the wrong company. Competition is good for consumers, especially if you're a minority group. reply skybrian 16 hours agorootparentI don't think boycotting Apple because they don't support open source enough is likely to have much effect? Maybe there are other companies you'd prefer to support, but it's still only going to work if lots of other people buy stuff from them, too. reply amelius 14 hours agorootparentIt's not boycotting, just voting with your wallet ... reply skybrian 13 hours agorootparentI don't see any difference. This \"vote\" is a rounding error unless lots of people do it, which is not going to happen without organization. The effect on you matters and the effect on the vendor is ignorable. reply jjtheblunt 20 hours agoparentprev> Not to mention that you'd be supporting a corporation that has this hostile stance towards their customers to begin with. Is the \"this\" in that sentence your previous paragraph of concern that Apple will purposefully break AsahiLinux? reply jorvi 17 hours agoparentprev> You're not losing much by buying a new CPU from them in 2024 or 2025, but you gain much more in return That's just false. Performance-per-watt, AMD, Intel, Nvidia and Qualcomm still get smoked by comparable M-series chips. On desktops the equation is different because CPUs and GPUs can go ham with the wattage. reply arnaudsm 19 hours agoparentprevIndeed, the Apple Silicon hype stemmed from the terrible Intel gen at the time (unlike AMD), and because Apple monopolized the TSMC N5 node for months. Most of that lead is gone, x86 is cheaper, open, and as battery friendly now. reply Philpax 19 hours agorootparentFrom https://news.ycombinator.com/item?id=42016931, it looks like M4 is very much still in the lead. It's also not just the CPU: the laptops themselves are simply phenomenal. I have never had a x86 laptop that's this much of a genuine pleasure to use (ignoring macOS, which is my least-favourite operating system). Some of the newer Windows laptops are ahead, but they're still worse in at least one metric. reply elcritch 18 hours agorootparentNot to mention Mac laptops are some of the only laptops with non 1080p or non 16:9 aspect ratio. Microsoft also has forced makers to drop the old S3 sleep modes which set x86 laptops back decades regarding sleep and power management. reply arnaudsm 8 hours agorootparentThat was true 5 years ago. Today, many flagship Ultrabooks have 3k+ screens and 16:10 ratios. I'm typing this from my Lenovo yoga 7 that does. reply metabagel 13 hours agorootparentprevMy AMD Ryzen Framework Laptop 13 running Ubuntu 22.04 is a joy to use. Huge difference between that laptop and the first generation Intel Framework Laptop 13. I don’t notice the fan running on it much at all. reply arnaudsm 18 hours agorootparentprevThat's an unconfirmed leak. I only compare chips that are available reply hybrid_study 20 hours agoparentprevSadly, this is the exact reason why I hold back trying Asahi and run the chance of liking any of it :-( Recently saw someone wondering why no one has tried building a laptop with as much quality as an Apple? A special version of Linux to run on such a laptop would offer more long-term commitment and maybe pull in more adoption. reply wslh 20 hours agoparentprevI am genuinely looking forward to a Dell XPS 13 or Lenovo X1 Carbon which is fanless and have the battery duration and performance of the Apple Macbook Air. reply somat 20 hours agorootparentWhat is the difference in battery life between linux on and macos on the Apple M1? That is, I would be surprised if linux on the M1 had close to macos levels of battery life. My theory being the better battery life on the M1 is more due to the tight integration between the OS and the hardware power profiles than the power profiles themself. reply wslh 20 hours agorootparentI’m sure Apple has some unique tricks when it comes to energy efficiency, but I haven’t seen the same level of optimization in other operating systems. Apple’s energy management is just another competitive advantage, offering a level of sophistication that sets it apart technically and strategically. Just add the Mx chips to the equation. reply nextos 18 hours agorootparentHonestly, I don't think this is true. I had identical efficiency on my Intel MacBook Air running Linux, compared to OS X. Both ran out of battery +/- 10 minutes of each other on the same hardware. The only distinct advantage is Safari, which is heavily optimized for efficiency. But a lightweight Linux desktop, with fewer active services, can compensate for that. reply wslh 7 hours agorootparentAnd, do you primarily use Google Chrome? You lost me a bit with \"...but a lightweight Linux desktop\" as it seems to counter your point. When discussing battery efficiency, I think it’s key to consider real-world tools like VSCode, which many of us use regularly. Chrome, especially, can drastically alter power usage across different operating systems. reply nextos 0 minutes agorootparentI use Firefox on Linux, but used Safari on Mac. My point was that this gave some efficiency advantage to my MacBook Air running OS X, compared to the same machine running Linux (and Firefox). Safari is a little marvel of energy efficiency. This was countered, to some extent, by a simple barebones X11 setup on the Linux side. Otherwise, my setup was identical on both sides. Linux tends to be regarded as energy inefficient because it ships with defaults that prioritize performance on desktops, workstations and servers. Some simple tweaks with udev rules can make a big difference. talldayo 20 hours agorootparentprevI'd be surprised if MacOS could match the efficiency of Linux. MacOS relies on a hybrid kernel architecture that emulates a variety of different APIs that aren't used or integrated fully. The simple act of running software on MacOS is deliberately somewhat inefficient, which is a perfectly fine tradeoff for a desktop OS that's not intended for server or edge applications. The fundamental hardware of Apple Silicon is very efficient but I don't think MacOS is inherently optimized any better than the others. My experience installing Linux on Intel and PowerPC Macbooks tended to increase their battery quite noticeably. reply wslh 20 hours agorootparentWell, if in 2021 you took your MacBook Air M1 (8GB) out on a Friday, downloaded movies, watched them, browsed the internet, did some casual development, and came back late Sunday without needing to charge it, I’d be impressed. reply andrewmutz 20 hours agorootparentprevIs the Linux support not there yet? How close is it? reply chickenzzzzu 20 hours agorootparentIt's the hardware that's the problem, not Linux support. Simply, the hardware manufacturers don't make fanless, thin, light, performant, power efficient laptops. reply hnuser123456 20 hours agorootparentWhat about fanless chromebooks? reply chickenzzzzu 20 hours agorootparentYup that's a good start! It proves that a company other than apple can do something fanless. Probably they're plastic-y, but they are thin, light, and fanless. Power efficiency and performance are likely not good, but, at least google doesn't deliberately obfuscate their hardware like Apple does. Instead, they just let everything that's not ChromeOS fester, since they're trying to make money. But anyone who wants to start a business selling Alpine on Chromebooks can ;) reply racked 16 hours agoparentprevExactly this. All that work, for reverse-engineering a handful of laptops made by a company whose only desire is to lock its users into its ecosystem and ergonomics. Even more demotivating is that the M1 and the M2 are already superseded. Similarly, I completely do not understand the popularity of Apple's laptops in this community. Endemic mindless fanboyism. reply mixmastamyk 20 hours agoparentprevI’m with you in spirit, but most of the work has already been done. Purchased hardware won’t change. Firmware updates could be held if needed as well. Another team could take a crack at it. Worse case you restore macos and possibly sell at a medium loss. That said I’m still waiting. reply pmarreck 20 hours agoparentprev> other x86 and ARM manufacturers are making substantial improvements that are shortening Apple's lead x86 has fundamental issues that I believe prevent it from ever achieving the MIPS per watt efficiency of anything from ARM. I mean... the newest M4 laptop will have a 24 hour battery life. That exceeds anything remotely possible in the same laptop form factor but with x86 by nearly an order of magnitude. So now you're talking just ARM. Linux has been compilable on ARM for a while now, so where are the competing ARM laptops that are anywhere close to the power of Apple's version of ARM? I do get what you're saying though (I'm definitely a Linux fan and have a Linux Framework laptop), but I wish it wasn't an x86 laptop because its battery life is crap (and that is sometimes important). reply imiric 19 hours agorootparent> the newest M4 laptop will have a 24 hour battery life. That exceeds anything remotely possible in the same laptop form factor but with x86 by nearly an order of magnitude. Honestly, why is this such an appealing feature? Are you often away from an outlet for 20+ hours? I use 6+ year old laptops that last 4 hours at most on a single charge, even after a battery replacement. Plugging them in every few hours is not a big inconvenience for me. If I'm traveling, I can usually find an outlet anywhere. It's not like I'm working in remote places where this could even be an issue. Then there's the concern about fan noise and the appeal of completely silent computers. Sure, it's a bit annoying when the fan ramps up, but again, this is not something that makes my computers unbearable to use. And finally, the performance gap is closing. Qualcomm's, AMD's and Intel's latest chips might not be \"M4 killers\" (or M3, for that matter), but they're certainly competitive. This will only keep improving in 2025. reply Philpax 19 hours agorootparentIt's not that these are must-haves: it's that it removes any such anxiety about these to begin with. I can take the laptop with me wherever I'm going, and not have to worry about charging it, or that the heat will make it unbearable to use on my lap, or that the fan will be noticeable. It means I can work at a cafe, in a car, on a bus, on a train, on a flight without power, and not have to worry. And these things compound, as the other poster mentioned: 24 hours of light use means longer heavy use, which actually does matter to me. I often move around while using the MacBook because it's good to change up my (physical) perspective - and it means I can do that without the charger while hammering every core with rustc. Once you see that better things are possible, it's very hard to go back to the comparatively terrible performance-per-watt and heat generation of equally powerful x86 laptops. reply imiric 8 hours agorootparentI'm not saying that these things are not appealing. I would certainly like to enjoy them as well. But there are always compromises. With Apple, you first have to pay whatever exorbitant price they want to charge you. The Air is relatively affordable, but bump up the specs on a MBP and your eyes will start to water. That's fine. You get what you pay for, right? Except now you're stuck in their walled garden and have to pay the same tax for overpriced accessories, software, and anything else they decide to charge you for. OR you go the Asahi route and live with the stress and uncertainty of depending on a small team of contributors to keep your machine running. I already experience this stress with Linux on supported hardware, and couldn't imagine what it would be like if the hardware had to be reverse engineered for this to work. That said, I'll probably end up buying a used MacBook Air just to follow the progress of Asahi. If nothing else I'll have a more informed opinion the next time this discussion is brought up. reply elcritch 17 hours agorootparentprev> Once you see that better things are possible, it’s very hard to go back Yeah, there’s something a bit freeing about being able to go all day or more without charging. Just not needing to think about power or charging when you’re busy focusing on other things. I’m glad other manufacturers got a bit of pressure to catch up as well. Now people come to expect laptops to just run for days at a time without charging. reply pmarreck 14 hours agorootparentprevWith all due respect I think this is a \"640k is enough for everyone\" problem, in the sense that you don't realize what something enables because you're simply so used to not having it: 1) Internet cafes that removed outlets to encourage laptop people not to squat :) 2) Airports where you can sit anywhere you want instead of just near an outlet 3) Airplanes when the power doesn't work (has happened more than once in my experience) 4) Cars, trains, subways, buses 5) My boat, sometimes I like to work from it for a change of pace 6) Don't have to hunt for an outlet when my fam is at the grandparents' house 7) I can work on my deck without dragging a power cord out to the table 8) I can go to a pretty overlook and watch the sunset while working 9) Conference rooms, don't have to deal with the hassle 10) Libraries, same thing. I can sit outside or in the stacks (quieter there) instead of only in the reading room (those tables are the only ones with power, in my local library) 11) Power outs and just other situations where you lose power for a time 12) It's extra juice to power your phone off of (or anything else) I'm certainly forgetting more examples. reply imiric 8 hours agorootparentSome of those things would be nice, sure, but it comes down to the question of whether they're worth trading for using a computing platform tightly controlled by single corporation. macOS is a deal-breaker for me, whether I use it in a cubicle or on a boat. So the only alternative is Asahi Linux, which has its own set of drawbacks. Clearly we must have different priorities, which is fine. Just don't think that yours are somehow superior to mine. reply erik_seaberg 17 hours agorootparentprevOver years, I've worn laptops (with sealed-in batteries) down to three-ish reliable hours. There are never enough power outlets (or AC vents or even seats at the table) for a big meeting. That's a problem for a very long meeting format like a war room or a promo committee. reply skydhash 17 hours agorootparentThat's why you throw a couple of multi outlets in the drawer or a cabinet in that room. reply erik_seaberg 15 hours agorootparentTech companies wire rows of desks for laptops and big monitors, but I think it'd be hard to find a meeting room where you could safely plug in more than a dozen 140 W chargers. reply littlecosmic 19 hours agorootparentprev24hrs of web/office is a work day of something more intense, or a few hours of something crazy. reply talldayo 20 hours agorootparentprev> so where are the competing ARM laptops that are anywhere close to the power of Apple's version of ARM? Better question: where are the incentives for them to make it? Apple is pretty much the only company with an outstanding architectural license to design ARM cores, and the best off-the-shelf ARM core designs don't even compete with 5-year-old x86 ones. If you're a company that has Apple-level capital and Apple-tier core design chops, you might as well embrace RISC-V and save yourself the ARM licensing fee. That's what Nvidia does for many of their GPU SOCs. If SoftBank offered ARM licenses under more attractive terms, there would be genuine competition for good ARM CPUs. Given that Apple has a controlling stake in SoftBank, I wouldn't hold out faith. reply GeekyBear 19 hours agorootparent> Apple is pretty much the only company with an outstanding architectural license to design ARM cores Many other companies have done this to great effect in the past, but in recent years it has become more common to just license one of ARM's own stock cores, instead of designing your own from scratch. This follows a period where companies like Qualcomm and Samsung were still trying to roll their own core designs from scratch, but ending up with designs that were slower and less power efficient than the cheaper stock cores you could license from ARM. reply IshKebab 19 hours agorootparentprevApparently Qualcomm's new ARM laptops are pretty close. However I think ARM platforms tend to be way less open source friendly than x86, at least on mobile. Maybe the laptops are better because they have to run Windows and Microsoft probably wouldn't put up with the shit vendors pull on Android. I don't know. reply pmarreck 13 hours agorootparentExcept that ARM just cancelled Qualcomm's chip design license, so... oopsy reply jojobas 11 hours agorootparentWe might not know if it's a valid cancellation for another few months and many millions of lawyer dollars. I wouldn't be surprised if Arm is strongarmed into playing hardball with Qualcomm by Apple itself. reply throwaway984393 20 hours agoprevWhat they've been able to accomplish in such a short time is nothing short of amazing, and I applaud them for their efforts. That said, I've been using Asahi for a month, and I'm ditching it. Maybe in a year or two it'll be stable, but for now it's got too many bugs and unsupported features. A lot of the problems come down to Wayland and KDE/Gnome, as you literally have to use Wayland. But there's plenty of other buggy or challenging parts that all add up to a very difficult working experience. One of the biggest challenges I see is support for hardware and 3rd party apps. Not only do all the apps need to support this slightly odd Arm system, but so do hardware driver developers. I never realized before just how much of a Linux system works because most people had an incredibly common platform (x86_64). Even if Linux on Mac became incredibly popular, it would actually take away development focus on x86_64, and we'd see less getting done. (This kind of problem is totally common among Linux laptops, btw; there's a ton of hardware out there and Linux bugs may exist in each one. Adding a new model doesn't add to the number of developers supporting them all. If anything, the Mac is probably benefited by the fact that it has so few models compared to the x86_64 world. But it's still only got so many devs, and 3rd party devs aren't all going to join the party overnight) reply Philpax 19 hours agoparentYeah, I can definitely see this being an issue going forward for quite some time. The existence of non-Apple ARM devices should hopefully lead to general interest in addressing these issues, but there's so much hardware and software out there, and only so many devs with the time, interest and access to fix them. On the other hand, I suspect people will start making choices for their hardware/software that maximise compatibility, as they already do for Linux x86. (\"Don't buy NVIDIA if you want functioning Wayland\", etc.) It'll be tough, but things will hopefully get better over time. reply timeon 17 hours agorootparent'Non-Apple ARM' is still a bit like second class citizen, at least on Arch. But at least there is option to compile things by yourself. reply imiric 19 hours agoparentprevI don't get why you were downvoted to oblivion. A perspective from someone who actually used Asahi is very valuable, so thanks for sharing. You're definitely right that having a usable system is not just about supporting first-party hardware. Linux on its own is a huge mess of different components that all somehow need to work together, and it's a miracle of engineering that it works as well as it does, even on well-supported hardware. I can't imagine how difficult it must be getting all of this to work on hardware that requires reverse engineering. It seems practically impossible to me. reply kccqzy 18 hours agorootparentOn HN downvotes are usually because of disagreement. OP's experience doesn't match mine: I have used Asahi for quite a bit longer than OP and I have experienced no serious bugs. But then again I only use those software that's available in the distribution or those that can be compiled by me. So naturally I don't deal with incompatible third party software. reply imiric 8 hours agorootparent> OP's experience doesn't match mine That's great. Is your experience somehow more valid then? Downvoting because of disagreement is asinine to begin with. Burying opinions that contribute to the discussion does nothing but perpetuate the hive mind. reply kccqzy 4 hours agorootparentI used to think that's asinine too. So I asked in https://news.ycombinator.com/item?id=36673613 It turns out everyone else said downvoting because of disagreement is fine. reply imiric 2 hours agorootparentI'm aware that this is what happens. I can still think it's asinine. reply grahamj 18 hours agoprev [–] For those who didn't get my joke, she commonly dresses up as a witch at XDC :) https://lwn.net/SubscriberLink/995383/34dc5950cab5e739/ reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The SEDaily Podcast features Alyssa Rosenzweig discussing the efforts to port Linux to Apple's ARM-based chips, known as Apple Silicon, through the Asahi Linux project.",
      "Alyssa Rosenzweig, a graphics developer, provides insights into the reverse-engineering of the Apple M1 GPU and advancements in Linux gaming.",
      "The task is challenging due to the lack of official documentation on Apple Silicon, making the project noteworthy for its technical complexity and potential impact on Linux users."
    ],
    "commentSummary": [
      "Running Linux on Apple Silicon is difficult due to Apple's proprietary hardware, which includes unique features like software-regulated speaker temperature.",
      "The Asahi Linux team is making progress in reverse-engineering these systems, but concerns about hardware damage and maintaining compatibility with Apple's updates persist.",
      "The project aims to extend the lifespan of Apple hardware and offer alternatives to macOS, highlighting the tension between Apple's closed ecosystem and the open-source community's goals."
    ],
    "points": 270,
    "commentCount": 215,
    "retryCount": 0,
    "time": 1730493013
  },
  {
    "id": 42024727,
    "title": "Weird Lexical Syntax",
    "originLink": "https://justine.lol/lex/",
    "originBody": "Oct 31st, 2024 @ justine's web page Weird Lexical Syntax I just learned 42 programming languages this month to build a new syntax highlighter for llamafile. I feel like I'm up to my eyeballs in programming languages right now. Now that it's halloween, I thought I'd share some of the spookiest most surprising syntax I've seen. The languages I decided to support are Ada, Assembly, BASIC, C, C#, C++, COBOL, CSS, D, FORTH, FORTRAN, Go, Haskell, HTML, Java, JavaScript, Julia, JSON, Kotlin, ld, LISP, Lua, m4, Make, Markdown, MATLAB, Pascal, Perl, PHP, Python, R, Ruby, Rust, Scala, Shell, SQL, Swift, Tcl, TeX, TXT, TypeScript, and Zig. That crosses off pretty much everything on the TIOBE Index except Scratch, which can't be highlighted, since it uses blocks instead of text. How To Code a Syntax Highlighter It's really not difficult to implement a syntax highlighter. You could probably write one over the course of a job interview. My favorite tools for doing this have been C++ and GNU gperf. The hardest problem here is avoiding the need to do a bunch of string comparisons to determine if something is a keyword or not. Most developers would just use a hash table, but gperf lets you create a perfect hash table. For example: %{ #include%} %pic %compare-strncmp %language=ANSI-C %readonly-tables %define lookup-function-name is_keyword_java_constant %% true false null gperf was originally invented for gcc and it's a great way to squeeze out every last drop of performance. If you run the gperf command on the above code above, it'll generate this .c file. You'll notice its hash function only needs to consider a single character in in a string to get a collision free lookup. That's what makes it perfect, and perfect means better performance. I'm not sure who wants to be able to syntax highlight C at 35 MB per second, but I am now able to do so, even though I've defined about 4,000 keywords for the language. Thanks to gperf, those keywords don't slow things down. The rest just boils down to finite state machines. You don't really need flex, bison, or ragel to build a basic syntax highlighter. You simply need a for loop and a switch statement. At least for my use case, where I've really only been focusing on strings, comments, and keywords. If I wanted to highlight things like C function names, well, then I'd probably need to do actual parsing. But focusing on the essentials, we're only really doing lexing at most. See highlight_ada.cpp as an example. Demo All the research you're about to read about on this page, went into making one thing, which is llamafile's new syntax highlighter. This is probably the strongest advantage that llamafile has over ollama these days, since ollama doesn't do syntax highlighting at all. Here's a demo of it running on Windows 10, using the Meta LLaMA 3.2 3B Instruct model. Please note, these llamafiles will run on MacOS, Linux, FreeBSD, and NetBSD too. The new highlighter and chatbot interface has made llamafile so pleasant for me to use, combined with the fact that open weights models like gemma 27b it have gotten so good, that it's become increasingly rare that I'll feel tempted to use Claude these days. Examples of Surprising Lexical Syntax So while writing this highlighter, let's talk about the kinds of lexical syntax that surprised me. C The C programming language, despite claiming to be simple, actually has some of the weirdest lexical elements of any language. For starters, we have trigraphs, which were probably invented to help Europeans use C when using keyboards that didn't include #, [, \\, ^, {, |, }, and ~. You can replace those characters with ??=, ??(, ??/, ??), ??', ??, and ??-. Intuitive, right? That means, for example, the following is perfectly valid C code. int main(int argc, char* argv??(??)) ?? That is, at least until trigraphs were removed in the C23 standard. However compilers will be supporting this syntax forever for legacy software, so a good syntax highlighter ought to too. But just because trigraphs are officially dead, doesn't mean the standards committees haven't thought up other weird syntax to replace it. Consider universal characters: int \\uFEB2 = 1; This feature is useful for anyone who wants, for example, variable names with arabic characters while still keeping the source code pure ASCII. I'm not sure why anyone would use it. I was hoping I could abuse this to say: int main(int argc, char* argv\\u005b\\u005d) \\u007b printf(\"hello world\"); \\u007d But alas, GCC raises an error if universal characters aren't used on the specific UNICODE planes that've been blessed by the standards committee. This next one is one of my favorites. Did you know that a single line comment in C can span multiple lines if you use backslash at the end of the line? //hi\\ there Most other languages don't support this. Even languages that allow backslash escapes in their source code (e.g. Perl, Ruby, and Shell) don't have this particular feature from C. The ones that do support this too, as far as I can tell, are Tcl and GNU Make. Tools for syntax highlighting oftentimes get this wrong, like Emacs and Pygments. Although Vim seems to always be right about backslash. Haskell Every C programmers knows you can't embed a multi-line comment in a multi-line comment. For example: /* hello /* again */ nope nope nope */ However with Haskell, you can. They finally fixed the bug. Although they did adopt a different syntax. -- Test nested comments within code blocks let result3 = {- This comment contains {- a nested comment -} -} 10 - 5 Tcl The thing that surprised me most about Tcl, is that identifiers can have quotes in them. For example, this program will print a\"b: puts a\"b You can even have quote in your variable names, however you'll only be able to reference it if you use the ${a\"b} notation, rather than $a\"b. set a\"b doge puts ${a\"b} JavaScript JavaScript has a builtin lexical syntax for regular expressions. However it's easy to lex it wrong if you aren't paying attention. Consider the following: var foo = /[/]/g; When I first wrote my lexer, I would simply scan for the closing slash, and assume that any slashes inside the regex would be escaped. That turned out to be wrong when I highlighted some minified code. If a slash is inside the square quotes for a character set, then that slash doesn't need to be escaped! Now onto the even weirder. There's some invisible UNICODE characters called the LINE SEPARATOR (u2028) and PARAGRAPH SEPARATOR (u2029). I don't know what the use case is for these codepoints, but the ECMAScript standard defines them as line terminators, which effectively makes them the same thing as . Since these are Trojan Source characters, I configure my Emacs to render them as ↵ and ¶. However most software hasn't been written to be aware of these characters, and will oftentimes render them as question marks. Also as far as I know, no other language does this. I was able to use that to my advantage for SectorLISP, since it let me create C + JavaScript polyglots. javascript syntax highlighting //¶` ... C only code goes here ... //` That's how I'd insert C code into JavaScript files. c syntax highlighting //¶` #if 0 //` ... JavaScript only code goes here ... //¶` #endif //` And that's how I'd insert JavaScript into my C source code. An example of a piece of production code where I did this is lisp.js which is what powers my SectorLISP blog post. It both runs in the browser, and you can compile it with GCC and run it locally too. llamafile is able to correctly syntax highlight this stuff, but I've yet to find another syntax highlighter that does too. Not that it matters, since I doubt an LLM would ever print this. But it sure is fun to think about these corner cases. Shell We're all familiar with the heredoc syntax of shell scripts, e.g. catPublic Function SomeFunction() As String #ElsePublic Function SomeFunction() As String #End If Perl One of the trickier languages to highlight is Perl. It's exists in the spiritual gulf between shells and programming languages, and inherits the complexity of both. Perl isn't as popular today as it once was, but its influence continues to be prolific. Perl made regular expressions a first class citizen of the language, and the way regex works in Perl has since been adopted by many other programming languages, such as Python. However the regex lexical syntax itself continues to be somewhat unique. For example, in Perl, you can replace text similar to sed as follows: my $string = \"HELLO, World!\"; $string =~ s/hello/Perl/i; print $string; # Output: Perl, World! Like sed, Perl also allows you to replace the slashes with an arbitrary punctuation character, since that makes it easier for you to put slashes inside your regex. $string =~ s!hello!Perl!i; What you might not have known, is that it's possible to do this with mirrored characters as well, in which case you need to insert an additional character: $string =~ s{hello}{Perl}i; However s/// isn't the only weird thing that needs to be highlighted like a string. Perl has a wide variety of other magic prefixes. /case sensitive match/ /case insensitive match/i y/abc/xyz/e s!hi!there! m!hi!i m;hi;i qr!hi!u qw!hi!h qq!hi!h qx!hi!h m-hi- s-hi-there-g s\"hi\"there\"g s@hi@there@ yo s{hi}{there}g One thing that makes this tricky to highlight, is you need to take context into consideration, so you don't accidentally think that y/x/y/ is a division formula. Thankfully, Perl makes this relatively easy, because variables can always be counted upon to have sigils, which are usually $ for scalars, @ for arrays, and % for hashes. my $greeting = \"Hello, world!\"; # Array: A list of names my @names = (\"Alice\", \"Bob\", \"Charlie\"); # Hash: A dictionary of ages my %ages = (\"Alice\" => 30, \"Bob\" => 25, \"Charlie\" => 35); # Print the greeting print \"$greeting\"; # Print each name from the array foreach my $name (@names) { print \"$name\"; } This helps us avoid the need for parsing the language grammar. Perl also has this goofy convention for writing man pages in your source code. Basically, any =word at the start of the line will get it going, and =cut will finish it. #!/usr/bin/perl =pod =head1 NAME my_silly_script - A Perl script demonstrating =cut syntax =head1 SYNOPSIS my_silly_script [OPTIONS] =head1 DESCRIPTION This script does absolutely nothing useful, but it showcases the quirky =cut syntax for POD documentation in Perl. =head1 OPTIONS There are no options. =head1 AUTHOR Your Name=head1 COPYRIGHT Copyright (c) 2023 Your Name. All rights reserved. =cut print \"Hello, world!\"; Ruby Of all the languages, I've saved the best for last, which is Ruby. Now here's a language whose syntax evades all attempts at understanding. Ruby is the union of all earlier languages, and it's not even formally documented. Their manual has a section on Ruby syntax, but it's very light on details. Whenever I try to test my syntax highlighting, by concatenating all the .rb files on my hard drive, there's always another file that finds some way to break it. def `(command) return \"just testing a backquote override\" end Since ruby supports backquote syntax like var = `echo hello`, I'm not exactly sure how to tell that the backquote above isn't meant to be highlighted as a string. Another example is this: when /\\.*\\.h/ options[:includes] <<arg; true when /--(\\w+)=\\\"?(.*)\\\"?/ options[$1.to_sym] = $2; true Ruby has a << operator, and it also supports heredocs (just like Perl and Shell). So I'm not exactly sure how to tell that the code above isn't a heredoc. Yes that code actually exists in the wild. Even Emacs gets this wrong. Out of all 42 languages I've evaluated, that's probably the biggest shocker so far. It might be the case that Ruby isn't possible to lex without parsing. Even with parsing, I'm still not sure how it's possible to make sense of that. Complexity of Supported Languages If I were to rank the complexity of programming languages by how many lines of code each one takes to syntax highlight, then FORTH would be the simplest language, and Ruby would be the most complicated. 125 highlight_forth.cpp 266 highlight_lua.cpp 132 highlight_m4.cpp 282 highlight_csharp.cpp 149 highlight_ada.cpp 282 highlight_rust.cpp 160 highlight_lisp.cpp 297 highlight_python.cpp 163 highlight_test.cpp 300 highlight_java.cpp 166 highlight_matlab.cpp 321 highlight_haskell.cpp 186 highlight_cobol.cpp 335 highlight_markdown.cpp 199 highlight_basic.cpp 337 highlight_js.cpp 200 highlight_fortran.cpp 340 highlight_html.cpp 211 highlight_sql.cpp 371 highlight_typescript.cpp 216 highlight_tcl.cpp 387 highlight_kotlin.cpp 218 highlight_tex.cpp 387 highlight_scala.cpp 219 highlight.cpp 447 highlight_asm.cpp 220 highlight_go.cpp 449 highlight_c.cpp 225 highlight_css.cpp 455 highlight_swift.cpp 225 highlight_pascal.cpp 560 highlight_shell.cpp 230 highlight_zig.cpp 563 highlight_perl.cpp 235 highlight_make.cpp 624 highlight_ruby.cpp 239 highlight_ld.cpp 263 highlight_r.cpp Funding llamafile is a Mozilla project who sponsors me to work on it. My work on open source is also made possible by my GitHub sponsors and Patreon subscribers. Thank you for giving me the opportunity to serve you all these last four years. Since you've read this far, I'd like to invite you to join both the Mozilla AI Discord and the Redbean Discord servers where you can chat with me and other people who love these projects. twitter.com/justinetunney github.com/jart Written by Justine Tunney jtunney@gmail.com",
    "commentLink": "https://news.ycombinator.com/item?id=42024727",
    "commentBody": "Weird Lexical Syntax (justine.lol)256 points by jart 8 hours agohidepastfavorite106 comments pdw 5 hours agoSome random things that the author seem to have missed: > but TypeScript, Swift, Kotlin, and Scala take string interpolation to the furthest extreme of encouraging actual code being embedded inside strings Many more languages support that: C# $\"{x} plus {y} equals {x + y}\" Python f\"{x} plus {y} equals {x + y}\" JavaScript `${x} plus ${y} equals ${x + y}` Ruby \"#{x} plus #{y} equals #{x + y}\" Shell \"$x plus $y equals $(echo \"$x+$y\"bc)\" Make :) echo \"$(x) plus $(y) equals $(shell echo \"$x+$y\"bc)\" > Tcl Tcl is funny because comments are only recognized in code, and since it's a homoiconic, it's very hard to distinguish code and data. { } are just funny string delimiters. E.g.: xyzzy {#hello world} Is xyzzy a command that takes a code block or a string? There's no way to tell. (Yes, that means that the Tcl tokenizer/parser cannot discard comments: only at evaluation time it's possible to tell if something is a comment or not.) > SQL PostgreSQL has the very convenient dollar-quoted strings: https://www.postgresql.org/docs/current/sql-syntax-lexical.h... E.g. these are equivalent: 'Dianne''s horse' $$Dianne's horse$$ $SomeTag$Dianne's horse$SomeTag$ reply autarch 3 hours agoparentPerl lets you do this too: my $foo = 5; my $bar = 'x'; my $quux = \"I have $foo $bar\\'s: @{[$bar x $foo]}\"; print \"$quux\"; This prints out: I have 5 x's: xxxxx The \"@{[...]}\" syntax is abusing Perl's ability to interpolate an _array_ as well as a scalar. The inner \"[...]\" creates an array reference and the outer \"@{...}\" dereferences it. For reasons I don't remember, the Perl interpreter allows arbitrary code in the inner \"[...]\" expression that creates the array reference. reply layer8 1 hour agoparentprev> actual code being embedded inside strings My view on this is that it shouldn’t be interpreted as code being embedded inside strings, but as a special form of string concatenation syntax. In turn, this would mean that you can nest the syntax, for example: \"foo { toUpper(\"bar { x + y } bar\") } foo\" The individual tokens being (one per line): \"foo { toUpper ( \"bar { x + y } bar\" ) } foo\" If `+` does string concatenation, the above would effectively be equivalent to: \"foo \" + toUpper(\"bar \" + (x + y) + \" bar\") + \" foo\" I don’t know if there is a language that actually works that way. reply epcoa 7 minutes agorootparent> \"foo { … That should probably not be one token. > My view on this is that it shouldn’t be interpreted as code being embedded inside strings I’m not sure exactly what you’re proposing and how it is different. You still can’t parse it as a regular lexical grammar. How does this change how you highlight either? To the lexer it is a special string, it has to know how to match it. I might be being dense but I’m not sure what’s formally distinct. reply panzi 1 hour agorootparentprevIndeed in some of the listed languages you can nest it like that, but in others (e.g. Python) you can't. I would guess they deliberately don't want to enable that and it's not a problem in their parser or something. reply Tarean 1 hour agorootparentAs of python 3.6 you can nest fstrings. Not all formatters and highlighters have caught up, though. Which is fun, because correct highlighting depends on language version. Haskell has similar problems where different compiler flags require different parsers. Close enough is sufficient for syntax highlighting, though. Python is also a bit weird because it calls the format methods, so objects can intercept and react to the format specifiers in the f-string while being formatted. reply layer8 1 hour agorootparentprevEven when nesting is disallowed, my point is that I find it preferable to not view it (and syntax-highlight it) as a “special string” with embedded magic, but as multiple string literals with just different delimiters that allow omitting the explicit concatenation operator, and normal expressions interspersed in between. I think it’s important to realize that it is really just very simple syntactic sugar for normal string concatenation. reply panzi 1 hour agoparentprevIs this a bash-ism? \"$x plus $y equals $((x+y))\" reply jonahx 1 minute agorootparentThis works in \"sh\" as well for me. reply layer8 1 hour agoprevThe author may have missed that lexing C is actually context-sensitive, i.e. you need a symbol table: https://en.wikipedia.org/wiki/Lexer_hack Of course, for syntax highlighting this is only relevant if you want to highlight the multiplication operator differently from the dereferencing operator, or declarations differently from expressions. More generally, however, I find it useful to highlight (say) types differently from variables or functions, which in some (most?) popular languages requires full parsing and symbol table information. Some IDEs therefore implement two levels of syntax highlighting, a basic one that only requires lexical information, and an extended one that kicks in when full grammar and type information becomes available. reply legobmw99 48 minutes agoparentI’d be shocked if jart didn’t know this, but it seems unlikely that an LLM would generate one of these most vexing parses, unless explicitly asked reply layer8 18 minutes agorootparentGiven all the things that were new to the author in the article, I wouldn’t be shocked at all. There’s just a huge number of things to know, or to have come across. reply kazinator 5 hours agoprevI don't think it's easy to write a good syntax coloring engine like the one in Vim. Syntax coloring has to handle context: different rules for material nested in certain ways. Vim's syntax higlighter lets you declare two kinds of items: matches and regions. Matches are simpler lexical rules, whereas regions have separate expressions for matching the start and end and middle. There are ways to exclude leading and trailing material from a region. Matches and regions can declare that they are contained. In that case they are not active unless they occur in a containing region. Contained matches declare which regions contain them. Regions declare which other regions they contain. That's the basic semantic architecture; there are bells and whistles in the system due to situations that arise. I don't think even Justine could develop that in an interview, other than as an overnight take home. reply kazinator 4 hours agoparentHere is an example of something hard to handle: TXR language with embedded TXR Lisp. This is the \"genman\" script which takes the raw output of a manpage to HTML converter, and massages it to form the HTML version of the TXR manual: https://www.kylheku.com/cgit/txr/tree/genman.txr Everything that is white (not colored) is literal template material. Lisp code is embedded in directives, like @(do ...). In this scheme, TXR keywords appear purple, TXR Lisp ones green. They can be the same; see the (and ...) in line 149, versus numerous occurrences of @(and). Quasistrings contain nested syntax: see 130 where ` ... ` contains an embedded (if ...). That could itself contain a quasistring with more embedded code. TXR's txr.vim\" and tl.vim* syntax definition files are both generated by this: https://www.kylheku.com/cgit/txr/tree/genvim.txr reply __MatrixMan__ 5 hours agoprevThis was a fun read, but it left me a bit more sympathetic to the lisp perspective, which (if I've understood it) is that syntax, being not an especially important part of a language, is more of a hurdle than a help, and should be as simple and uniform as possible so we can focus on other things. Which is sort of ironic because learning how to do structural editing on lisps has absolutely been more hurdle than help so far, but I'm sure it'll pay off eventually. reply mqus 4 hours agoparentHaving a simple syntax might be fine for computers but syntax is mainly designed to be read and written by humans. Having a simple one like lisp then just makes syntactic discussions a semantic problem, just shifting the layers. And I think an complex syntax is far easier to read and write than a simple syntax with complex semantics. You also get a faster feedback loop in case the syntax of your code is wrong vs the semantics (which might be undiscovered until runtime). reply __MatrixMan__ 3 hours agorootparentJury's out re: whether I feel this in my gut. Need more time with the lisps for that. But re: cognitive load maybe it goes like: 1. 1 language to rule them all, fancy syntax 2. Many languages, 1 simple syntax to rule them all 3. Many languages and many fancy syntaxes Here in the wreckage of the tower of babel, 1. isn't really on the table. But 2. might have benefits because the inhumanity of the syntax need only be confronted once. The cumulative cost of all the competing opinionated fancy syntaxes may be the worst option. Think of all the hours lost to tabs vs spaces or braces vs whitespace. reply dartos 2 hours agorootparentI think 3 is not only a natural state, but the best state. I don’t think we can have 1 language that satisfies the needs of all people who write code, and thus, we can’t have 1 syntax that does that either. 3 seems the only sensible solution to me, and we have it. reply __MatrixMan__ 2 hours agorootparentI dunno, here in 3 the hardest part of learning a language has little to do with the language itself and more to do with the ecosystem of tooling around that language. I think we could more easily get on to the business of using the right language for the job if more of that tooling was shared. If each language, for instance did not have it's own package manager, its own IDE, its own linters and language servers all with their own idiosyncrasies arising not from deep philosophical differences of the associated language but instead from accidental quirks of perspective from whoever decided that their favorite language needed a new widget. I admire the widget makers, especially those wrangling the gaps between languages. I just wish their work could be made easier. reply skydhash 1 hour agorootparentI really like the Linux package managers. If you're going to write an application that will run on some system, it's better to bake dependencies into it. And with virtualization and containerization, the system is not tied to a physical machine. I've been using containers (incus) more and more for real development purposes as I can use almost the same environment to deploy. I don't care much about the IDE, but I'm glad we have LSP, Tree-sitter, and DAP. The one thing I do not like is the proliferation of tooling version manager (NVM,..) instead of managing the environment itself (tied to the project). reply drewr 3 hours agorootparentprevI don't understand your distinction between syntax and semantics. If the semantics are complex, wouldn't that mean the syntax is thus complex? reply SuperCuber 1 hour agorootparentlisp's syntax is simple - its just parenthesis to define a list, first element of a list is executed as a function. but for example a language like C has many different syntaxes for different operations, like function declaration or variable or array syntax, or if/switch-case etc etc. so to know C syntax you need to learn all these different ways to do different things, but in lisp you just need to know how to match parenthesis. But of course you still want to declare variables, or have if/else and switch case. So you instead need to learn the builtin macros (what GP means by semantics) and their \"syntax\" that is technically not part of the language's syntax but actually is since you still need all those operations enough that they are included in the standard library and defining your own is frowned upon. reply skydhash 58 minutes agorootparentprevMost languages' abstract machines expose a very simple API, it's up to the language to add useful constructs to help us write code more efficiently. Languages like Lisp start with a very simple syntax, then add those constructs with the language itself (even though those can be fixed using a standard), others just add it through the syntax. These constructs plus the abstract machine's operations form the semantics, syntax is however the language designer decided to present them. reply fanf2 3 hours agoparentprevLisp has reader macros which allow you to reprogram its lexer. Lisp macros allow you to program the translation from the visible structure to the parse tree. For example, https://pyret.org/ It really isn’t simple or necessarily uniform. reply nlitened 4 hours agoparentprevI am surprised to hear that structural editing has been a hurdle for you, and I think I can offer a piece of advice. I also used to be terrified by its apparent complexity, but later found out that one just needs to use parinfer and to know key bindings for only three commands: slurp, barf, and raise. With just these four things you will be 95% there, enjoying the fruits of paredit without any complexity — all the remaining tricks you can learn later when you feel like you’re fluent. reply __MatrixMan__ 4 hours agorootparentThanks very much for the advice, it's timely.It's not so much the editing itself but the unfamiliarity of the ecosystem. It seems it's a square-peg I've been crafting a round hole of habits for it: I guess I should use emacs? How to even configure it such that these actions are available? Or maybe I should write a plugin for helix so that I can be in a familiar environment. Oh, but the helix plugin language is a scheme, so I guess I'll use emacs until I can learn scheme better and then write that plugin. Oh but emacs keybinds are conflicting with what I've configured for zellij, maybe I can avoid conflicts by using evil mode? Oh ok, emacs-lisp, that's a thing. Hey symex seems like it aligns with my modal brain, oh but there goes another afternoon of fussing with emacs. Found and reported a symex \"bug\" but apparently it only appears in nix-governed environments so I guess I gotta figure out how to report the packaging bug (still todo). Also, I guess I might as well figure out how to get emacs to evaluate expressions based on which ones are selected, since that's one of the fun things you can do in lisps, but there's no plugin for the scheme that helix is using for its plugin language (which is why I'm learning scheme in the first place), but it turns out that AI is weirdly good at configuring emacs so now my emacs config contains most that that plugin would entail. Ok, now I'm finally ready to learn scheme, I've got this big list of new actions to learn: https://countvajhula.com/2021/09/25/the-animated-guide-to-sy.... Slurp, barf, and raise you say? excellent, I'll focus on those. I'm not actually trying to critique the unfamiliar space. These are all self inflicted wounds: me being persnickety about having it my way. It's just usually not so difficult to use something new and also have it my way. reply nlitened 1 hour agorootparentTo be fair, I am not a \"lisper\" and I don't know Emacs at all. I am just a Clojure enjoyer who uses IntelliJ + Cursive with its built-in parinfer/paredit. reply xenophonf 2 hours agorootparentprevI never bothered with structural editing on Emacs. I just use the sentence/paragraph movement commands. M-a, M-e, M-n, M-p, M-T, M-space, etc. reply TomatoCo 4 hours agoprevI think my favorite C trigraph was something like do_action() ??!??! handle_error() It almost looks like special error handling syntax but still remains satisfying once you realize it's an || logical-or statement and it's using short circuiting rules to execute handle error if the action returns a non-zero value. reply wslh 4 hours agoparentDid you choose the legacy C trigraphs over || for aesthetic purposes? reply pwdisswordfishz 6 hours agoprev> Of all the languages, I've saved the best for last, which is Ruby. Now here's a language whose syntax evades all attempts at understanding. TeX with its arbitrarily reprogrammable lexer: how adorable reply fanf2 3 hours agoparentLisp reader macros allow you to program its lexer too. reply skydhash 55 minutes agorootparentYou can basically define a new language with a few lines of code in Racket. reply skitter 6 hours agoprevAnother syntax oddity (not mentioned here) that breaks most highlighters: In Java, unicode escapes can be anywhere, not just in strings. For example, the following is a valid class: class Foo\\u007b} and this assert will not trigger: assert // String literals can have unicode escapes like \\u000A! \"Hello World\".equals(\"\\u00E4\"); reply mistercow 3 hours agoparentI also argue that failing to syntax highlight this correctly is a security issue. You can terminate block comments with Unicode escapes, so if you wanted to hide some malicious code in a Java source file, you just need an excuse for there to be a block of Unicode escapes in a comment. A dev who doesn’t know about this quirk is likely to just skip over it, assuming it’s commented out. reply ivanjermakov 5 hours agoparentprevI have never seen this in Java! Is there any use cases where it could be useful? reply layer8 32 minutes agorootparentJavac uses the platform encoding [0] by default to interpret Java source files. This means that Java source code files are inherently non-portable. When Java was first developed (and for a long time after), this was the default situation for any kind of plain text files. The escape sequence syntax allows to transform [1] Java source code into a portable (that is, ASCII-only) representation that is completely equivalent to the original, and also to convert it back to any platform encoding. Source control clients could apply this automatically upon checkin/checkout, so that clients with different platform encodings can work together. Alternatively, IDEs could do this when saving/loading Java source files. That never quite caught on, and the general advice was to stick to ASCII, at least outside comments. [0] Since JDK 18, the default encoding defaults to UTF-8. This probably also extends to javac, though I haven’t verified it. [1] https://docs.oracle.com/javase/8/docs/technotes/tools/window... reply susam 4 hours agorootparentprevI don't know about usefulness but it does let us write identifiers using Unicode characters. For example: public class Foo { public static void main(String[] args) { double \\u03c0 = 3.14159265; System.out.println(\"\\u03c0 = \" + \\u03c0); } } Output: $ javac Foo.java && java Foo π = 3.14159265 Of course, nowadays we can simply write this with any decent editor: public class Foo { public static void main(String[] args) { double π = 3.14159265; System.out.println(\"π = \" + π); } } Support for Unicode escape sequences is a result of how the Java Language Specification (JLS) defines InputCharacter. Quoting from Section 3.4 of JLS : InputCharacter: UnicodeInputCharacter but not CR or LF UnicodeInputCharacter is defined as the following in section 3.3: UnicodeInputCharacter: UnicodeEscape RawInputCharacter UnicodeEscape: \\ UnicodeMarker HexDigit HexDigit HexDigit HexDigit UnicodeMarker: u {u} HexDigit: (one of) 0 1 2 3 4 5 6 7 8 9 a b c d e f A B C D E F RawInputCharacter: any Unicode character As a result the lexical analyser honours Unicode escape sequences absolutely anywhere in the program text. For example, this is a valid Java program: public class Bar { public static void \\u006d\\u0061\\u0069\\u006e(String[] args) { System.out.println(\"hello, world\"); } } Here is the output: $ javac Bar.java && java Bar hello, world However, this is an incorrect Java program: public class Baz { // This comment contains \\u6d. public static void main(String[] args) { System.out.println(\"hello, world\"); } } Here is the error: $ javac Baz.java Baz.java:2: error: illegal unicode escape // This comment contains \\u6d.^ 1 error Yes, this is an error even if the illegal Unicode escape sequence occurs in a comment! reply ivanjermakov 2 hours agorootparentI wonder if full unicode range was accepted because some companies are writing code in non-english. reply susam 5 hours agoprev> Every C programmers (sic) knows you can't embed a multi-line comment in a multi-line comment. And every Standard ML programmer might find this to be a surprising limitation. The following is a valid Standard ML program: (* (* Nested (**) *) comment *) val _ = print \"hello, world\" Here is the output: $ smlML didn’t have single-line comments, so same level of surprising limitation. It is not quite clear to me why the lack of single-line comments is such a surprising limitation. After all, a single-line block comment can easily serve as a substitute. However, there is no straightforward workaround for the lack of nested block comments. > I’ve never heard someone refer to C as “expressive”, but maybe it was in 1972 when compared to assembly. I was thinking of Fortran in this context. For instance, Fortran 77 lacked function pointers and offered a limited set of control flow structures, along with cumbersome support for recursion. I know Fortran, with its native support for multidimensional arrays, excelled in numerical and scientific computing but C quickly became the preferred language for general purpose computing. While very few today would consider C a pinnacle of expressiveness, when I was learning C, the landscape of mainstream programming languages was much more restricted. In fact, the preface to the first edition of K&R notes the following: \"In our experience, C has proven to be a pleasant, expressive and versatile language for a wide variety of programs.\" C, Pascal, etc. stood out as some of the few mainstream programming languages that offered a reasonable level of expressiveness. Of course, Lisp was exceptionally expressive in its own right, but it wasn't always the best fit for certain applications or environments. > And what bearing does the comment syntax have on the expressiveness of a language? Nothing at all. I agree. The expressiveness of C comes from its grammar, which the language parser handles. Support for nested comments, in the context of C, is a concern for the lexer, so indeed one does not directly influence the other. However, it is still curious that a language with such a sophisticated grammar and parser could not allocate a bit of its complexity budget to support nested comments in its lexer. This is a trivial matter, I know, but I still couldn't help but wonder about it. reply dahart 3 hours agorootparentFair enough. From my perspective, lack of single line comments is a little surprising because most other languages had it at the time (1973, when ML was introduced). Lack of nested comments doesn’t seem surprising, because it isn’t an important feature for a language, and because most other languages did not have it at the time (1972, when C was introduced). I can imagine both pro and con arguments for supporting nested comments, but regardless of what I think, C certainly could have added support for nested comments at any time, and hasn’t, which suggests that there isn’t sufficient need for it. That might be the entire explanation: not even worth a little complexity. reply masfuerte 2 hours agorootparentAFAIK, C didn't get single line comments until C99. They were a C++ feature originally. reply dahart 2 hours agorootparentOh wow, I didn’t remember that, and I did start writing C before 99. I stand corrected. I guess that is a little surprising. ;) Is true that many languages had single line comments? Maybe I’m forgetting more, but I remember everything else having single line comments… asm, basic, shell. I used Pascal in the 80s and apparently forgot it didn’t have line comments either? reply masfuerte 2 hours agorootparentThat's my recollection, that most languages had single line comments. Some had multi-line comments but C++ is the first I remember having syntaxes for both. That said, I'm not terribly familiar with pre-80s stuff. reply pklausler 1 hour agorootparentprev> Fortran 77 lacked function pointers But we did have dummy procedures, which covered one of the important use cases directly, and which could be abused to fake function/subroutine pointers stored in data. reply kragen 2 hours agoparentprevThis is not just true of Standard ML; it's also true of regular ML. reply gsliepen 3 hours agoparentprevWell there is one way to nest comments in C, and that's by using #if 0: #if 0 This is a #if 0 nested comment! #endif #endif reply fanf2 3 hours agorootparentExcept that text inside #if 0 still has to lex correctly. (unifdef has some evil code to support using C-style preprocessor directives with non-C source, which mostly boils down to ignoring comments. I don’t recommend it!) reply dahart 2 hours agorootparent> Except that text inside #if 0 still has to lex correctly. Are you sure? I just tried on godbolt and that’s not true with gcc 14.2. I’ve definitely put syntax errors intentionally into #if 0 blocks and had it compile. Are you thinking of some older version or something? I thought the pre-processor ran before the lexer since always… reply fanf2 1 hour agorootparentThere are three (relevant) phases (see “translation phases” in section 5 of the standard): • program is lexed into preprocessing tokens; comments turn into whitespace • preprocessor does its thing • preprocessor tokens are turned into proper tokens; different kinds of number are disambiguated; keywords and identifiers are disambiguated If you put an unclosed comment inside #if 0 then it won’t work as you might expect. reply dahart 35 minutes agorootparentAh, I see. You’re right! reply irdc 5 hours agoprevI’d be interested to see a re-usable implementation of joe's[0] syntax highlighting.[1] The format is powerful enough to allow for the proper highlighting of Python f-strings.[2] 0. https://joe-editor.sf.net/ 1. https://github.com/cmur2/joe-syntax/blob/joe-4.4/misc/HowItW... 2. https://gist.github.com/irdc/6188f11b1e699d615ce2520f03f1d0d... reply pama 5 hours agoparentInterestingly, python f-strings changed their syntax at version 3.12, so highlighting should depend on the version. reply irdc 5 hours agorootparentIt’s just that nesting them arbitrarily is now allowed, right? That shouldn’t matter much for a mere syntax highlighter then. And one could even argue that code that relies on this too much is not really for human consumption. reply pansa2 5 hours agorootparentAlso, you can now use the same quote character that encloses an f-string within the {} expressions. That could make them harder to tokenize, because it makes it harder to recognise the end of the string. reply rererereferred 5 hours agoprevIn the C# multiquoted strings, how does it know this: Console.WriteLine(\"\"\"\"\"\"); Console.WriteLine(\"\"\"\"\"\"); Are 2 triplequoted empty strings and not one \"Console.WriteLine(\" sixtuplequoted string? reply yen223 5 hours agoparentIt's a syntax error! Unterminated raw string literal. https://replit.com/@Wei-YenYen/DistantAdmirableCareware#main... reply Joker_vD 5 hours agorootparentAh, so there is no backtracking in lexer for this case. Makes sense. reply Joker_vD 5 hours agoparentprevIf the opening quotes are followed by anything that is not a whitespace before the next new-line (or EOF), then it's a single-line string. I imagine implementing those things took several iterations :) reply ygra 5 hours agoparentprevThe former, I'd say. https://learn.microsoft.com/en-us/dotnet/csharp/programming-... For a multi-line string the quotes have to be on their own line. reply pansa2 5 hours agoprev> TypeScript, Swift, Kotlin, and Scala take string interpolation to the furthest extreme of encouraging actual code being embedded inside strings. So to highlight a string, one must count curly brackets and maintain a stack of parser states. Presumably this is also true in Python - IIRC the brace-delimited fields within f-strings may contain arbitrary expressions. More generally, this must mean that the lexical grammar of those languages isn't regular. \"Maintaining a stack\" isn't part of a finite-state machine for a regular grammar - instead we're in the realm of pushdown automata and context-free grammars. Is it even possible to support generalized string interpolation within a strictly regular lexical grammar? reply aphantastic 4 hours agoparent> Is it even possible to support generalized string interpolation within a strictly regular lexical grammar? Almost certainly not, a fun exercise is to attempt to devise a Pumping tactic for your proposed language. If it doesn’t exist, it’s not regular. https://en.m.wikipedia.org/wiki/Pumping_lemma_for_regular_la... reply fanf2 3 hours agoparentprevComplicated interpolation can be lexed as a regular language if you treat strings as three separate lexical things, eg in JavaScript template literals there are, `stuff${ }stuff${ }stuff` so the ${ and } are extra closing and opening string delimiters, leaving the nesting to be handled by the parser. You need a lexer hack so that the lexer does not treat } as the start of a string literal, except when the parser is inside an interpolation but all nested {} have been closed. reply yen223 6 hours agoprevselect'select'select is a perfectly valid SQL query, at least for Postgres. Languages' approach to whitespace between tokens is all over the place reply metadat 3 hours agoprev> The languages I decided to support are Ada, Assembly, BASIC, C, C#, C++, COBOL, CSS, D, FORTH, FORTRAN, Go, Haskell, HTML, Java, JavaScript, Julia, JSON, Kotlin, ld, LISP, Lua, m4, Make, Markdown, MATLAB, Pascal, Perl, PHP, Python, R, Ruby, Rust, Scala, Shell, SQL, Swift, Tcl, TeX, TXT, TypeScript, and Zig. A few (admittedly silly) questions about the list: 1. Why no Erlang, Elixir, or Crystal? Erlang appears to be just at the author's boundary at #47 on the TIOBE index. https://www.tiobe.com/tiobe-index/ 2. What is \"Shell\"? Sh, Bash, Zsh, Windows Cmd, PowerShell..? 3. Perl but no Awk? Curious why, because Awk is a similar but comparatively trivial language. Widely used, too. To be fair, Awk, Erlang, and Elixir rank low on popularity. Yet m4, Tcl, TeX, and Zig aren't registered in the top 50 at all. What's the methodology / criteria? Only things the author is already familiar with? Still a fun article. reply Yasuraka 1 hour agoparentTiobes's index is quite literally worthless, especially with regards to its stated purpose, let alone as a general point of orientation. I'd wish that purple would stop lending it any credibility. reply dakiol 2 hours agoprevWouldn’t be possible to let the LLM do the highlighting? Instead of returning code in plain text, it could return code within html with the appropriate tags. Maybe it’s harder than it sounds… but if it’s just for highlighting the code the LLM returns, I wouldn’t mind the highlighting not being 100% accurate. reply trashburger 2 hours agoparentWould be much slower and eat up precious context window. reply SonOfLilit 4 hours agoprevJustine gets very close to the hairiest parsing issue in any language without encountering it: Perl's syntax is undecidable, because the difference between treating some characters as a comment or as a regex can depend on the type of a variable that is only determined e.g. based on whether a search for a Collatz counterexample terminates, or just, you know, user input. https://perlmonks.org/?node_id=663393 C++ templates have a similar issue, I think. reply swolchok 3 hours agoparent> C++ templates have a similar issue TIL! I went and dug up a citation: https://blog.reverberate.org/2013/08/parsing-c-is-literally-... reply fanf2 3 hours agoparentprevI think possibly the most hilariously complicated instance of this is in perl’s tokenizer, toke.c (which starts with a Tolkien quote, 'It all comes from here, the stench and the peril.' — Frodo). There’s a function called intuit_more which works out if $var[stuff] inside a regex is a variable interpolation followed by a character class, or an array element interpolation. Its result can depend on whether something in the stuff has been declared as a variable or not. But even if you ignore the undecidability, the rest is still ridiculously complicated. https://github.com/Perl/perl5/blob/blead/toke.c#L4502 reply layer8 24 minutes agoparentprevHow could a search for a Collatz counterexample possibly terminate? ;) reply playingalong 4 hours agoprevNice read. I guess the article could be called Falsehoods Programmers Assume of Programming Language Syntaxes. reply notsylver 6 hours agoprevAs soon as I saw this was part of llamafile I was hoping that it would be used to limit LLM output to always be \"valid\" code as soon as it saw the backticks, but I suppose most LLMs don't have problems with that anyway. And I'm not sure you'd want something like that automatically forcing valid code anyway reply dilap 5 hours agoparentllama.cpp does support something like this -- you can give it a grammar which restricts the set of available next tokens that are sampled over so in theory you could notice \"```python\" or whatever and then start restricting to valid python code. (in least in theory, not sure how feasible/possible it would be in practice w/ their grammar format.) for code i'm not sure how useful it would be since likely any model that is giving you working code wouldn't be struggling w/ syntax errors anyway? but i have had success experimentally using the feature to drive fiction content for a game from a smaller llm to be in a very specific format. reply notsylver 3 hours agorootparentyeah, ive used llama.cpp grammars before, which is why i was thinking about it. i just think it'd be cool for llamafile to do basically that, but with included defaults so you could eg, require JSON output. it could be cool for prototyping or something. but i dont think that would be too useful anyway, most of the time i think you would want to restrict it to a specific schema, so i can only see it being useful for something like a tiny local LLM for code completion, but that would just encourage valid-looking but incorrect code. i think i just like the idea of restricting LLM output, it has a lot of interesting use cases reply dilap 2 hours agorootparentgotchya. i do think that is a cool idea actually -- LLMs tiny enough to do useful things with formally structured output but not big enough to nail the structure ~100% is probably not an empty set. reply skrebbel 6 hours agoprevThis was a delightful read, thanks! reply jim_lawless 4 hours agoprevForth has a default syntax, but Forth code can execute during the compilation process allowing it to accept/compile custom syntaxes. reply llm_trw 6 hours agoprevI've done a fair bit of forth and I've not seen c\" used. The usual string printing operator is .\" . reply kragen 6 hours agoparentRight, c\" is for when you want to pass a literal string to some other word, not print it. But I agree that it's not very common, because you normally use s\" for that, which leaves the address and length on the stack, while c\" leaves just an address on the stack, pointing to a one-byte count field followed by the bytes. I think adding c\" in Forth-83 (and renaming \" to s\") was a mistake, and it would have been better to deprecate the standard words that expect or produce such counted strings, other than count itself. See https://forth-standard.org/standard/alpha, https://forth-standard.org/standard/core/Cq, https://forth-standard.org/standard/core/COUNT, and https://forth-standard.org/standard/core/Sq. You can easily add new string and comment syntaxes to Forth, though. For example, you can add BCPL-style // comments to end of line with this line of code in, I believe, all standard Forths, though I've only tested it in GForth: : // 10 word drop ; immediate Getting it to work in block files requires more work but is still only a few lines of code. The standard word \\ does this, and see \\ decompiles the GForth implementation as : \\ blk @ IF >in @ c/l / 1+ c/l * >in ! EXIT THEN source >in ! drop ; immediate This kind of thing was commonly done for text editor commands, for example; you might define i as a word that reads text until the end of the line and inserts it at the current position in the editor, rather than discarding it like my // above. Among other things, the screen editor in F83 does exactly that. So, as with Perl, PostScript, TeX, m4, and Lisps that support readmacros, you can't lex Forth without executing it. reply mananaysiempre 6 hours agoparentprevCounted (“Pascal”) strings are rare nowadays so C\" is not often used. Its addr len equivalent is S\" and that one is fairly common in string manipulation code. reply petesergeant 3 hours agoprev> Perl also has this goofy convention for writing man pages in your source code The world corpus of software would be much better documented if everywhere else had stolen this from Perl. Inline POD is great. reply kragen 2 hours agoparentPerl and Python stole it from Emacs Lisp, though Perl took it further. I'm not sure where Java stole it from, but nowadays Doxygen is pretty common for C code. Unfortunately this results in people thinking that Javadoc and Doxygen are substitutes for actual documentation like the Emacs Lisp Reference Manual, which cannot be generated from docstrings, because the organization of the source code is hopelessly inadequate for a reference manual. reply ygra 5 hours agoprevAs for C#'s triple-quoted strings, they actually came from Java before and C# ended up adopting the same or almost the same semantics. Including stripping leading whitespace. reply mcphage 6 hours agoprevAt one point there was an open source project to formally specify Ruby, but I don’t know if it’s still alive: https://github.com/ruby/spec Hmm, it seems to be alive, but based more on behavior than syntax. reply IshKebab 4 hours agoprevI don't understand why you wouldn't use Tree Sitter's syntax highlighting for this. I mean it's not going to be as fast but that clearly isn't an issue here. Is this a \"no third party dependencies\" thing? reply jart 1 hour agoparentI don't want to require everyone who builds llamafile from source need to install rust. I don't even require that people install the gperf command, since I can build gperf as a 700kb actually portable executable and vendor it in the repo. Tree sitter I'd imagine does a really great highly precise job with the languages it supports. However it appears to support fewer of them than I am currently. I'm taking a breadth first approach to syntax highlighting, due to the enormity of languages LLMs understand. reply lupire 5 hours agoprev> You'll notice its hash function only needs to consider a single character in in a string. That's what makes it perfect, Is that a joke? https://en.m.wikipedia.org/wiki/Perfect_hash_function reply keybored 6 hours agoprevMeanwhile NeoVim doesn’t syntax highlight my commit message properly if I have messed with \"commit cleanup\" enough. The comment character in Git commit messages can be a problem when you insist on prepending your commits with some \"id\" and the id starts with `#`. One suggestion was to allow backslash escapes in commit messages since that makes sense to a computer scientist.[1] But looking at all of this lexical stuff I wonder if makes-sense-to-computer-scientist is a good goal. They invented the problem of using a uniform delimiter for strings and then had to solve their own problem. Maybe it was hard to use backtick in the 70’s and 80’s, but today[2] you could use backtick to start a string and a single quote to end it. What do C-like programming languages use single quotes for? To quote characters. Why do you need to quote characters? I’ve never seen a literal character which needed an \"end character\" marker. Raw strings would still be useful but you wouldn’t need raw strings just to do a very basic thing like make a string which has typewriter quotes in it. Of course this was for C-like languages. Don’t even get me started on shell and related languages where basically everything is a string and you have to make a single-quote/double-quote battle plan before doing anything slightly nested. [1] https://lore.kernel.org/git/vpq3808p40o.fsf@anie.imag.fr/ [2] Notwithstanding us Europeans that use a dead-key keyboard layout where you have to type twice to get one measly backtick (not that I use those) reply pwdisswordfishz 5 hours agoparent> The comment character in Git commit messages can be a problem when you insist on prepending your commits with some \"id\" and the id starts with `#` https://git-scm.com/docs/git-commit#Documentation/git-commit... reply keybored 5 hours agorootparentSee \"commit cleanup\". There’s surprising layers to this. That the reporter in that thread says that git-commit will “happily” accept `#` in commit messages is half-true: it will accept it if you don’t edit the message since the `default` cleanup (that you linked to) will not remove comments if the message is given through things like `-m` and not an editing session. So `git commit -m'#something' is fine. But then try to do rebase and cherry-pick and whatever else later, maybe get a merge commit message with a commented \"conflicted\" files. Well it can get confusing. reply shawa_a_a 5 hours agoparentprevThe comment character is also configurable: git config core.commentcharThis is helpful where you want to use use say, markdown to have tidily formatted commit messages make up your pull request body too. reply keybored 4 hours agorootparentI want to try to set it to `auto` and see what spicy things it comes up with. reply kragen 5 hours agoparentprev> Maybe it was hard to use backtick in the 70’s and 80’s, but today[2] you could use backtick to start a string and a single quote to end it. That's how quoting works by default in m4 and TeX, both defined in the 70s. Unfortunately Unicode retconned the ASCII apostrophe character ' to be a vertical line, maybe out of a misguided deference to Microsoft Windows, and now we all have to suffer the consequences. (Unless we're using Computer Modern fonts or other fonts that predate this error, such as VGA font ROM dumps.) In the 70s and 80s, and into the current millennium on Unix, `x' did look like ‘x’, but now instead it looks like dogshit. Even if you are willing to require a custom font for readability, though, that doesn't solve the problem; you need some way to include an apostrophe in your quoted string! As for end delimiters, C itself supports multicharacter literals, which are potentially useful for things like Macintosh type and creator codes, or FTP commands. Unfortunately, following the Unicode botch theme, the standard failed to define an endianness or minimum width for them, so they're not very useful today. You can use them as enum values if you want to make your memory dumps easier to read in the debugger, and that's about it. I think Microsoft's compiler botched them so badly that even that's not an option if you need your code to run on it. reply ygra 5 hours agorootparent> Unfortunately Unicode retconned the ASCII apostrophe character ' to be a vertical line Unicode does not precribe the appearance of characters. Although in the code chart¹ it says »neutral (vertical) glyph with mixed usage« (next to »apostrophe-quote« and »single quote«), font vendors have to deal with this mixed usage. And with Unicode the correct quotation marks have their own code points, making it unnecessary to design fonts where the ASCII apostrophe takes their form, but rendering all other uses pretty ugly. I would regard using ` and ' as paired quotation marks as a hack from times when typographic expression was simply not possible with the character sets of the day. _________ ¹ 0027 ' APOSTROPHE = apostrophe-quote (1.0) = single quote = APL quote • neutral (vertical) glyph with mixed usage • 2019 ’ is preferred for apostrophe • preferred characters in English for paired quotation marks are 2018 ‘ & 2019 ’ • 05F3 ׳ is preferred for geresh when writing Hebrew → 02B9 ʹ modifier letter prime → 02BC ʼ modifier letter apostrophe → 02C8 ˈ modifier letter vertical line → 0301 $́ combining acute accent → 030D $̍ combining vertical line above → 05F3 ׳ hebrew punctuation geresh → 2018 ‘ left single quotation mark → 2019 ’ right single quotation mark → 2032 ′ prime → A78C ꞌ latin small letter saltillo« reply tom_ 2 hours agorootparentprevSee also: https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html reply kragen 2 hours agorootparentThis is an excellent document. I disagree with its normative conclusions, because I think being incompatible with ASCII, Unix, Emacs, and TeX is worse than being incompatible with ISO-8859-1, Microsoft Windows, and MacOS 9, but it is an excellent reference for the factual background. reply keybored 5 hours agorootparentprev> That's how quoting works by default in m4 and TeX, both defined in the 70s. Good point. And it was in m4[1] I saw that backtick+apostrophe syntax. I would have probably not thought of that possibility if I hadn’t seen it there. [1] Probably on Wikipedia since I have never used it > Unfortunately Unicode retconned the ASCII apostrophe character ' to be a vertical line, maybe out of a misguided deference to Microsoft Windows, and now we all have to suffer the consequences. (Unless we're using Computer Modern fonts or other fonts that predate this error, such as VGA font ROM dumps.) I do think the vertical line looks subpar (and I don’t use it in prose). But most programmers don’t seem bothered by it. :| > In the 70s and 80s, and into the current millennium on Unix, `x' did look like ‘x’, but now instead it looks like dogshit. Emacs tries to render it like ‘x’ since it uses backtick+apostrophe for quotes. With some mixed results in my experience. > Even if you are willing to require a custom font for readability, though, that doesn't solve the problem; you need some way to include an apostrophe in your quoted string! Aha, I honestly didn’t even think that far. Seems a bit restrictive to not be able to use possessives and contractions in strings without escapes. > As for end delimiters, C itself supports multicharacter literals, which are potentially useful for things like Macintosh type and creator codes, or FTP commands. I should have made it clear that I was only considering C-likes and not C itself. A language from the C trigraph days can be excused. To a certain extent. reply kragen 3 hours agorootparentI'd forgotten about `' in Emacs documentation! That may be influenced by TeX. C multicharacter literals are unrelated to trigraphs. Trigraphs were a mistake added many years later in the ANSI process. reply croisillon 6 hours agoprev [–] Glad to see confirmed that PHP is the most non weird programming language ;) reply rererereferred 4 hours agoparentI recently learned php's heredoc can have space before it and it will remove those spaces from the lines in the string: $a = <<<EOL This is not indented but this has 4 spaces of indentation EOL; But the spaces have to match, if any line has less spaces than the EOL it gives an error. reply alganet 2 hours agoparentprev [–] There are two types of languages: the ones full of quirks and the ones no one uses. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A developer learned 42 programming languages to create a syntax highlighter for a project called llamafile, highlighting the complexity and diversity of programming languages.",
      "The syntax highlighter, optimized using C++ and GNU gperf, focuses on strings, comments, and keywords, employing finite state machines for efficiency.",
      "Sponsored by Mozilla, the project encourages community engagement through Mozilla AI and Redbean Discord servers for further discussion."
    ],
    "commentSummary": [
      "The article examines the unique syntax features of various programming languages, particularly focusing on string interpolation and comment handling.- It discusses the challenges of syntax highlighting, mentioning tools like Vim, and the complexity of handling nested comments in languages such as C and Standard ML.- The text explores the evolution of syntax in languages like Java and C#, the role of Unicode, and the potential use of Large Language Models (LLMs) for syntax highlighting."
    ],
    "points": 256,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1730533526
  },
  {
    "id": 42024246,
    "title": "Rewrite It in Rails",
    "originLink": "https://dirkjonker.bearblog.dev/rewrite-it-in-rails/",
    "originBody": "Rewrite it in Rails 01 Nov, 2024 In 2022 I started building an application for creating Customs declarations. After evaluating some options and writing code in various programming languages and frameworks such as .NET/F#, Go, Rust, React, I eventually decided to write it in Ruby on Rails. Why? Because I was already familiar with Rails and because of that I could literally go 10x faster than I was with any of the aforementioned options that were quite new to me. I told myself that this was temporary, a proof of concept and that later on, I would rewrite it in a different language that wouldn't let me shoot myself in the foot as easily as I tend to do with Ruby. The Rails application however quickly grew in functionality and my colleagues really liked the application. I could quickly build features that would make them more productive and prevent a ton of things that could go wrong (and often did initially go wrong, until I wrote some code to prevent it). Meanwhile, I started building \"version 2\" of the application. I chose Rust as the language for the backend and SvelteKit for the frontend. Initially, the new version looked great, was blazing fast, but only had about 10% of the required features. As Rust doesn't really have anything comparable to Rails, I ended up having to do a lot of plumbing instead of writing business code. After a while, it became obvious that I could never write a feature complete version 2 without completely freezing version 1. So version 2 got thrown in the bin. Maybe this experience taught me something: rewriting something seems to take about as long as writing the original thing in the first place. Was this the end of the story? Absolutely not. Having experienced the joy of Rust, with beautifully typed code and blazing fast performance, the feeling of confidence you get when something compiles without errors, I knew I had to have Rust in the application. Despite over four years with Ruby, I still regularly wrote code that had issues at runtime. Mostly issues with null values and unhandled exceptions. After each deploy, I would closely watch the error reporting tools and quickly fix any issues that would occur in production. Rust just seemed like a necessity with its lack of null and making it easy for you to handle all errors. And I always enjoy learning new things and finding better ways of writing software. Besides Ruby on the backend, I was also getting a bit frustrated with frontend in Rails, having logic and markup spread across different files. Svelte just made that so much easier with related logic grouped in a single file. The decision was thus ti slowly write new parts of the frontend in SvelteKit and backend in Rust. Then slowly rewrite the existing Rails parts as well and eventually substitute Rails completely. This idea looked to be off to a great start. It didn't take long to move all the \"index\" pages with lists of records and filters to the new stack. And it was blazing fast and (at least the Rust part) barely had any issues at runtime. Deployment got a bit messy with a lot of rules for which routes would go to Rails, Rust or SvelteKit, but hey, it would only be temporary until everything was rewritten and awesome. However, as time went on, the Rails parts slowly got useful new features and the existing features got deeper and more refined. I performed various refactors to fix wrong assumptions I made very early on. Having a well designed database schema would benefit any version of the application, so this was good. I decided not to spend any more time updating the Rails UI as it would be replaced soon™ anyway. This went on for months and months, and the business was successful enough that we could afford to employ a full time developer. This would be great and would surely speed up the migration to Rust domination. It is good to dream big, I guess. And a big dream it surely was. In reality, I spent a lot of time writing abstractions and mechanisms for accessing the database, to ensure that users could only see the data they are allowed to see, keeping track of all changes, search and filters and using the type system to enforce it all. Then I realized I would also need background jobs, a pub/sub system for pushing updates to the client, e-mail capabilities, OAuth 2.0 support for connecting with external services, SMTP and POP, object storage, PDF generation and more. While there are plenty of Rust libraries available for doing all of these, selecting the right one and wiring everything together was a lot of work. We were having a lot of discussions avout technical implementation details, what ORMs for Rust might be useful, when Svelte 5 would come out and how much better it would be, and so on. After probably a good year of writing Rust and Svelte, it dawned on me that our users didn't benefit from any of this. Sure, parts of the application were now very sleek and blazing fast, but to be honest, those parts were never an issue to begin with. The most used pages were still running Rails and they often did 10x more than the current state of the \"beta version\" of that same page, written in the new stack. With days and often weeks on end being spent on adding absolutely no direct value, I had to keep telling myself that it was an investment, and that it would pay off in the long run. And then I called BS on myself. Some investments are just dumb and will not work out, no matter how hard you want things to succeed. I was getting stressed because our users needed stuff that I wasn't building for them. And I was getting more stressed just thinking about the sheer amount of stuff that still had to be (re)written in Rust. I was still the only person on the team maintaining the Rails app, the rest was only familiar with Rust and Svelte. The application became rather complicated, with many changes to be made in multiple places if you just wanted to make a small change. There was only one reasonable decision to be made. We had to Rewrite it in Rails. Well, not the whole thing, just the parts that were now running on the \"new\" stack. And the rest of the team would have to start learning Rails, I guess, whether they would like it or not. Even though in my opinion, this was the only good decision, it was still a very hard one to make. After all, we had spent so much time and wrote so much code, only to just throw it all away? And even potentially alienate the people on the team who may not even like Ruby or Rails? Luckily, the team seemed to understand the decision and was open to learn new things. Well, I wouldn't blame them if they would end up not enjoying it at all, but hopefully they'll give it an honest try at least. What makes Rails so good then, that it is apparently better than Rust of all things and Svelte with TypeScript, the darling of web developers everywhere? Perhaps being the foundation for several hugely successful companies, who still contribute to it as much as ever, counts for something. Rails has lots of batteries included and the ecosystem consists of many mature and stable libraries to complement it. It let me focus on building the product instead of writing all of the supporting code underneath. And due to its opinionated design and conventions, it is often very concise compared to other languages and frameworks. Was the attempt at using different languages and frameworks a waste then? I personally don't think so. First of all, I've learned a lot. I always enjoy learning new things and learning a new programming language or framework often gives me a lot of new insights and ideas that make me a better programmer in any language (or at least I'd like to think so). More importantly, I learned that writing a web application is all about making tradeoffs. There are a lot of decisions to be made and no choice will be perfect. You either have to accept the flaws of your chosen framework or do the investment of writing things yourself. But you have to consider whether that investment will really give you a competitive advantage and whether it will really pay off over time. In this case, I think it was a great decision to go back to building everything in Rails. Rewriting a lot of stuff back to Rails was frankly rather trivial and it reduced a lot of stress for me. What's more, these last weeks I've felt very productive again, building new features, improving existing ones, updating the UI, fixing stupid bugs. I also really enjoy working more closely with my colleagues who use the application all day, every day and figuring out how I can make the product better for them. But what about the downsides of Ruby and Rails then? Well, those are simply things to take into account when writing code. Let me give my opinion here. Have many issues at runtime? Test more. Does it turn into unmaintainable spaghetti code after a while? Only if you let it. It's typically caused by developers, not programming languages or frameworks. There are plenty of ways to nicely organize your code and you should probably spend more time refactoring. Rails doesn't scale? I think that if you can't scale Rails, you probably can't scale anything else either. Rails is slow? It is fast enough, and there are plenty of ways to \"cheat\" with Turbo and caching. Fun fact: the Rust stack wasn't noticably faster for our users. On Rails, the most heavy page has a P95 duration of 338 ms. There is of course room for improvement but it's plenty snappy. Interestingly, all of the things that used to annoy me about Ruby and Rails now annoy me much, much, much less. I have accepted that there is no perfect language or framework. You just have to know its strengths and weaknesses and deal with them. Luckily, there are some really good frameworks out there for people who want to focus on building a good product. I think Rails is probably one of the best, but there are a lot of other long-lived frameworks with lots of active contributors that are probably just as good. It might still be worth it to choose a more exotic language, framework or to dive in and write your own. If only for the opportunity to learn these kinds of lessons on your own. 169",
    "commentLink": "https://news.ycombinator.com/item?id=42024246",
    "commentBody": "Rewrite It in Rails (dirkjonker.bearblog.dev)178 points by WuxiFingerHold 13 hours agohidepastfavorite179 comments pech0rin 10 hours agoRails (and possibly Django and Laravel) are just light-years ahead of any other stack for building web apps. They have dealt with all the tedium, know all the requirements, and actually get out of your face when building an application. I have been developing web apps for 15 years now. I have tried Meteor (back in the day), Remix, Nextjs, Node w/ Express, etc. Always talking about how much better they are. But in my mind web dev is a solved problem. The js stuff is mainly just developers wanking off, driven by a bunch of dollars from big companies. Systems stuff, deployment infra, etc. is great for stuff like Rust & Go, but shoehorning into web dev makes no sense. I would love to just move on from this debate but it seems thats going to never be possible. reply gepardi 6 minutes agoparent“… the rest is developers wanking off …” Hahaha this made me chuckle out loud. reply alanfranz 10 hours agoparentprevI must say that even if it’s not a trendy stack, I find Spring Framework/Spring Boot to be the best all-around web application stack. Very easy integration with the tons of java libs you have around that can do virtually anything, a statically typed language which can help in a lot of situations (just data binding by convention via Jackson is great since forever, while it’s an afterthought in dynamically typed languages). reply t-writescode 10 hours agorootparentI've been incredibly pleased with Ktor and Kotlin, as well. JVM underneath, robust libraries, easy to use infra, using Exposed for the ORM which is mostly (genuinely) a joy. The best thing I think Rails still has is ActiveAdmin. Everything else, I really can take or leave. reply Tainnor 9 hours agorootparent> The best thing I think Rails still has is ActiveAdmin. Everything else, I really can take or leave. ActiveAdmin gets you off the ground very quickly, but is also extremely inflexible (and, IIRC, poorly maintained). The last time I worked on an ActiveAdmin backend we had to use all sorts of weird hacks to build the interface the way our backoffice team needed it to be. reply dajonker 8 hours agorootparentprevIn another company I had to completely replace ActiveAdmin with \"vanilla\" Rails abstractions as it was quite buggy and caused Rails to hang indefinitely as soon as you edited any ruby file. The admin part also turned out to be the most used part of the app, so it probably made sense to replace it in any case, but the bugginess at the time was more than I could handle. reply michaelmior 6 hours agorootparentprevI think part of the problem may be that a lot of people are familiar with old versions of Java before things like lambdas and anonymous interface implementations were possible. Writing Java without some of those things can be pretty painful. reply egeozcan 10 hours agorootparentprevI did write a lot of Java but never called myself a Java developer, and in my current company there are a lot of hardcore Java devs who used to swear by the Spring stack (some even liked JSF!), and a lot of them are moving things to Quarkus (https://quarkus.io/). No idea why, just an observation. reply royjacobs 10 hours agorootparentThey are moving there because it's new and shiny, not because it's better. Spring and Spring Boot are incredibly productive and give you a ton of stuff out of the box, whereas some people actually enjoy writing a lot of plumbing code as a distraction/challenge from boring business code. Those people will migrate to \"lean\" frameworks because it will give them the opportunity to write more low-level plumbing code. reply ffsm8 9 hours agorootparent> They are moving there because it's new and shiny, not because it's better. I highly doubt that. The real reason is likely more about the GraalVM, which spring hasn't supported until very recently. (And still only with caveats) reply royjacobs 9 hours agorootparentThat sort of proves my point, though. Spring support for GraalVM is ongoing and will likely be quite useable and mature out of the box. The people jumping to Quarkus or Micronaut are more eager to chase the new shiny and are willing to spend the time debugging not-yet-mature stacks. Edit: To be clear, the reply mentions fanboyism but we are talking about the maturity of a stack. Spring has been around for 20 years and is not going anywhere. Quarkus just reached 5. reply ffsm8 9 hours agorootparentThe GraalVM is significantly more performant. Calling that \"chasing the new shiny\" redefines the meaning of the expression, which has historically been about side-grades for unclear advantages. Especially considering that Springs support isn't full yet, and quarkus has been around for 5yrs now. They're both production ready stacks though, really strange to have spring fanboyism in 2024 reply pjmlp 9 hours agorootparentprevJSF can be a great experience when coupled with the right framework like PrimeFaces. reply egeozcan 7 hours agorootparentMy impression after many years of working with JSF: PrimeFaces or not, it's a lot of server-side state making it hard to work on the front-end and an unnecessary burden on the server. I have also some nice things to say, and I'm not going to make a detailed analysis here but IMHO, it's really not worth the tradeoff. reply pjmlp 7 hours agorootparentAgreed, and yet the celebration of stuff like Blazor Server, Phoenix, among others show that some folks really like this approach. Personally the experience with JSF and Web Forms, is what makes me appreciate any framework that exposes the underlying browser stack, instead of pretending it is something it is not. reply egeozcan 10 hours agoparentprevFrom my experience, these frameworks are like the express lane for kicking off a project because there are no big decisions to make. But once you’re knee-deep in Rails, Django, or Laravel and need to do something off the beaten path, things can get dicey. Why? Because you didn’t write the glue code yourself, so there's a gap in understanding, not really a technical roadblock (most of the time!). My point is, if you dive into these frameworks and actually learn the guts, you'll save yourself tons of time coding by, well, reading code. Now, do I actually do this? Absolutely not. Rewriting everything is way too much fun, and I live for the thrill of trying new things, even if it makes zero business sense. reply whamlastxmas 3 hours agorootparentWhat’s an example of a feature for a Laravel site where not fully understanding the mechanisms of Laravel under the hood would make that existing code get in the way of building the new feature? Genuine question. reply egeozcan 3 hours agorootparentLuckily I have a real life example from a customer project I was involved as a consultant: Integrating a custom identity provider with multi-factor authentication. This was from many years ago though. They also used to have some performance problems in some naively implemented middleware and needed to deep-dive into how things actually work before being able to optimize. And commands - I was doing the integration with our software and their command queue was always causing problems with stuck jobs until they read the implementation. The specific problem with Laravel as a product is (was?) that the docs are too beginner oriented and perhaps \"you don't need to know what's under the hood\" mindset is exactly what's causing this. I got to know many experienced PHP developers who got really frustrated because of the ELI5 style docs. You can't always rely on the docs too, however excellent they may be - some game developers read code from game engines to optimize, some web developers read code from their web frameworks. But, some change their whole stack when they get frustrated, and I argue that it makes little to no business sense to do so. Just understand what you are working with and deeply. reply Chiron1991 52 minutes agorootparentI can't speak for the other frameworks, but with Django this would have not been a problem at all. In Django, most \"batteries included\" features really just are 1st party plugins, i.e. you can choose to not use the builtin authentication stack and bring your own. All of this is officially supported and well documented, e.g. https://docs.djangoproject.com/en/5.1/topics/auth/customizin... reply pjmlp 9 hours agoparentprevAs someone that used AOLServer, was in a startup doing something like Rails but in 1999 with Tcl, I don't really agree. The Rails demo wasn't that appealing to me, other than showing the difference on how lucky one might get regarding adoption and spotlight being in the right place. Nowadays I still don't see a value, and rather go for Spring or ASP.NET. By the way, the founders of that Tcl based startup, went on to create OutSystems, which is one of the few successful RAD tools for Web development at enterprise scale, and Portuguese success stories in IT world. reply ridruejo 7 hours agorootparentTcl for web development was great and AOLServer ahead of its time reply rockyj 10 hours agoparentprevI do not agree. Rails is absolutely great in terms of features and productivity, but will fail you elsewhere i.e. it is not the silver bullet for webdev. Do not take my opinion, look at data - https://www.youtube.com/watch?v=Qp9SOOtgmS4 If you are using Rails for anything where you are not absolutely sure of how many users or RPS you will have, you are just saving money in launch time but spending more on servers. reply BoumTAC 10 hours agorootparentShopify is built using Ruby on Rails, they successfully handle enormous traffic spikes during Black Friday sales without issues. So I think we're good with performance. reply rockyj 9 hours agorootparentEverything can scale if you throw enough servers at it. Of-course Shopify scales, they even spent time and money to build a JIT on top of Ruby. As a smaller company, does everyone have the time and money to spend on servers or optimising the language to this extent? reply bnferguson 8 hours agorootparentThat's the nice thing! You don't need to optimise the language and build a JIT as a smaller company, Shopify already did that for you. Just like Google did for Javascript, which lead to Javascript having any performance at all (which lead to node being a thing). Also remember that Shopify didn't start out making billions. They started as a small side project on a far, far slower version of Ruby and Rails. Same with GitHub, same with many others that are either still on Rails or started there. You can optimise things later once you actually have customers, know the shape of your problem and where the actual pain points are/what needs to be scaled. To me, I care a ton about performance (it's an area I work in), but there's not a lot of sense in sacrificing development agility for request speed on things that may not matter or be things people will pay for. Especially when you're small. reply chucke 8 hours agorootparentprevNo, they only have time for features and productivity, which is, as you pointed out earlier, what rails is good at. reply lentil 8 hours agorootparentprevSmaller companies have less traffic, need less expensive servers, and have no need to spend money optimising the language. They can focus on that when they make billions of dollars, like Shopify does. reply axelthegerman 8 hours agorootparentAnd in the meantime just passively benefit from the OSS improvements along the way reply ksec 2 hours agorootparentprev>So I think we're good with performance. >On Rails, the most heavy page has a P95 duration of 338 ms. There is of course room for improvement but it's plenty snappy. I guess everyone will have different opinion on P95 at 338ms. The great thing is that we are getting cheaper CPU Core price and YJIT. As long as this trend continues, the definition of Fast Enough will cover more grounds for more people. reply dajonker 26 minutes agorootparentThere's lots of tricks you can do, such as preloading pages when the users hovers over the link. This makes even a \"slow\" page load of 400ms feel pretty much instant to a human. reply biorach 9 hours agorootparentprev> you are just saving money in launch time but spending more on servers. But... That's kind of the whole point reply wutwutwat 8 hours agorootparentprevthe \"rails doesn't scale\" myth has been debunked for a decade now and anyone with the slightest clue about performance can scale a rails app using the same techniques that you'd use to scale any other language and framework yjit and fibers have made things even better, and plenty more is coming I'm sure reply CPLX 9 hours agorootparentprev> just saving money in launch time but spending more on servers That seems like an amazingly good trade off, even if it were true which I am not sure about. reply timeon 6 hours agorootparentIt is. On the other hand this kind of mindset is the reason why there is climate change. reply CPLX 3 hours agorootparentI’m pretty sure the decision to use Ruby On Rails instead of another platform for building my web applications is not in fact the reason why there is climate change. reply hit8run 9 hours agorootparentprevIf RPS become a problem you can probably switch to JRuby. I did exactly this 15 years ago for EUs biggest software company and it worked out quite well. reply rockyj 9 hours agorootparentHave you seen the latest state of JRuby development or seen the posts of its development team. If you are comfortable with the idea of your main runtime being dependent on the time / finances / availability of around 5 (or less) core developers I would say you are ok with JRuby. reply ethagnawl 10 hours agoparentprevI came through to note that most of this applies to Django, too. I've historically preferred RoR but with the tremendous growth of Python in the last 10+ years, Django has become a more practical choice. ML and data science devs are already familiar with Python and, with the Django docs being as excellent as they are, these folks can be productive in a very short amount of time -- should they need to be. I've seen this firsthand on multiple projects. Also, along with the author's case for the path of least resistance, the Django framework results in fewer \"decisions\" (arguments) about application structure than using a less opinionated library or micro-framework. The Django/FLOSS community is also much more active than I was expecting it to be (Rails bias, probably) and has been very pleasant to interact with. I only wish Django had Rails-like generators and in-built data seeding (e.g. rake db:seed). reply appplication 3 hours agorootparentDjango has been an absolute pleasure to work with. Contrast it to flask, which is a complete footgun factory. I agree 100% on there being (honestly, massive) value in reducing decision points in the project. Very few devs truly have the experience needed to see around the corners in their designs and architecture, and Django represents the culmination of decades of learnings on what works, and works well. Overall, I’ve been very impressed with the product and maintainers. There are so few OSS projects that rise or the level of quality that Django has managed to achieve. Now if only they could sort out type hints ;). reply tomwphillips 8 hours agoparentprevI agree. Unfortunately every team I’ve worked in hasn’t seen the light and prefers FastAPI/SQLAlchemy/Pydantic (before FastAPI it was Flask). My theory is that the initial learning curves are different: with FastAPI it’s quick and easy. You barely have to read anything. Django has a steeper learning curve. There’s a lot of reading involved. Type hints aren’t a big thing in Django, but they are in FastAPI, and the average full stack dev seems to like them. Later on it’s totally different of course. With FastAPI you’re building it all from scratch, and it’ll be much worse than the Django solution. reply allendoerfer 8 hours agorootparentType hints are were the whole Python ecosystem is going, so using them is more integration at a deeper level than using an integrated framework, which is not relying on them. SQLAlchemy was historically a much better ORM than Django's. It's layered architecture combined with Alembic does make a difference. I still agree that using the integrated thing anyway is probably the right way to do it if you are working in a team. I also think Django should just adopt these components and we would not have the discussion in the first place. reply tomwphillips 2 hours agorootparent> Type hints are were the whole Python ecosystem is going I see what you’re saying, but a lot of Python users, especially those who have been using it pre-3, would say that this is unfortunate. > I also think Django should just adopt these components and we would not have the discussion in the first place. Oof, such a monoculture sounds terrible to me! reply jonatron 7 hours agorootparentprevI think SQLAlchemy vs Django ORM was a 2007 blogging topic: https://www.b-list.org/weblog/2007/sep/04/orm-wars/ reply allendoerfer 7 hours agorootparentWhile it is not Django's responsibility to unite the Python ecosystem, continuing to rely on a tool a sizeable share of the community deems inferior to a popular alternative will keep these discussions open and results in the fragmentation OP is talking about. Now of course it is not Django's responsibility to unite the Python ecosystem in the first place and they can value other factors and arguments as they see fit. Although this very thread shows that there might have been something to it. reply cloverich 4 hours agorootparentA corollary is the debate itself leads to a waste of effort that multiplies across all users. I use Rails only in anger, but to see literally nobody bike shed on the ORM is pretty amazing. Seems like you use Active Record or you write SQL and either way move on with life. reply Volrath89 2 hours agoparentprevDotnet is also very good for building web apps, it also includes static typing and a huge ecosystem. I agree that web dev is basically a solved problem, I don’t know why stuff like next.js exists reply timkofu 1 hour agoparentprevDjango. Plus with Django and Py03, one can write the service layer in Rust(if you’re into that). It also enables building an interface to Python’s data tools. reply keb_ 2 hours agoparentprevI've been working on a Rails codebase for a few years now. The biggest downside coming from TypeScript codebases is that lack of static typing and the immature static typing ecosystem in Ruby. I know DHH publicly denounced strong typing, but if you're coming from a language with it, Ruby & Rails is a hard sell. reply awongh 10 hours agoparentprevI like the rails dev experience, but it feels like they haven’t come up with nice new front end solutions that an integrated framework like nextjs has done. One specific example is images and srcset. If you are doing a react front end anywhere in your app (not even an SPA) interfacing with the asset pipeline doesn’t have a canonical solution. This just makes it feel behind the times, since what I always liked about rails was the “one right way” that was always reasonably sane. So it feels like nowadays you need to trade user functional front end app things, like load time and LCP for dev experience in rails. Sad. reply gardenhedge 8 hours agoparentprevI disagree. Rails is fine but just a differential approach. I prefer a standalone fronted that talks to an api layer. Web Frontend for me is remix. Mobile is react native and api layer could be any language you're productive in, eg. Java,.net or javascript. reply TomK32 12 hours agoprevI'm working with Rails for 17 years now, still love it and still prefer it even for the frontend. It give you all the options to separate your code without the head of your files becoming a unnecessary long list of `includes` like I see in the angular app I have to work on for a client. I don't see a disadvantage in splitting logic and markup for the frontend, it allows you to test the logic independently and normally when you hunt a bug you should know whether it's in the markup or the logic. Tracking down a bug thus becomes more easy. I've written my fair share of far too long ruby methods, 200 lines and more, but those are truly my fault. With a more granular unit test regime those can be prevented. reply Etheryte 8 hours agoparentYou know what else allows you to avoid having that long list of includes? Making everything global. I'm not sure if I see this as a benefit. Imports can take up a lot of space, yes, but they also make it explicit how everything ties together. If they're in your way, you can always have your IDE collapse them. reply axelthegerman 8 hours agorootparent> but they also make it explicit how everything ties together So does the autoloader used in Rails, it's called convention over configuration. There are only very few places it will look for the code (and in production everything gets preloaded too) So unless you want your class User actually in iMeanThisUser.ext or ../../../a/random/path/that/makes/no/sense/user.ext you shouldn't need explicit configuration reply nbittich 11 hours agoprevRust is a language built by extremely smart people, unfortunately their focus is more on type theory and sparing few allocations than building something useful and coherent for blue collar devs like me. When I read a blog post on rust, I literally don't understand half of it, although I'm working with rust since 2020. reply Expurple 8 hours agoparentCould you share your examples of incoherence in Rust? I actually find it a small, well-designed and very coherent language. At least, when compared to huge mainstream languages. E.g., traits cover the use cases of both \"abstract classes\" and \"concepts\" in C++, or both \"abstract classes\" and \"protocols\" in Python. And there are no inconsistencies regarding using exceptions or returning errors by value (panics exist, but they are used as assertions). There's no separate ternary operator, which is redundant when you have an if-else expression. reply hu3 3 hours agorootparent> Could you share your examples of incoherence in Rust? just a nit. I think they used a narrow definition of coherent, for blue collars, which in web often imply mundane CRUD and agency apps. I wouldn't expand the discussion to a brother definition about a general incoherency in the language. And certainty wouldn't bring comparisons to C++ when replying to their message. reply nbittich 7 hours agorootparentprevAsync vs non async code. It's basically two different languages imho. generics are also a big pain to work with, specially along with lifetimes. I have fun working with rust now that I am good enough at it, but I kept my objectivity in the process of learning it. reply WuxiFingerHold 13 hours agoprevGreat real world article. Everybody should keep the bottom line \"under their pillow\": >> Interestingly, all of the things that used to annoy me about Ruby and Rails now annoy me much, much, much less. I have accepted that there is no perfect language or framework. You just have to know its strengths and weaknesses and deal with them. reply bubblyworld 12 hours agoparentA little gem of an article, thanks for posting. It's nice to see a pragmatic take on rewriting for a change. Feels like a good primer to read before considering a rewrite myself haha reply TomK32 12 hours agorootparentRegarding rewrites, I'd love to have my code in such a shape that I could just copy over the business logic, maybe even put it into an independent library. The rest is just a user/api interface after all. reply karmakaze 5 hours agoparentprevFor context that came after trying this: > Meanwhile, I started building \"version 2\" of the application. I chose Rust as the language for the backend and SvelteKit for the frontend. Since it started out as Rails in v1, the post is to beware abandoning a framework that's working for you. reply dajonker 6 hours agoparentprevYes this was indeed the most important thing for me to realize. I am now much happier, not constantly thinking \"if only I had Rust enums!\" reply ozim 11 hours agoparentprevWhen people nag about frameworks I usually think that have their “perfect” world idea they don’t want compromise. Lots of times it is not having full understanding of the framework and why’s. But it is much more efficient working with the framework, knowing its limitations. Just spending time really understanding tools one work with. reply DeathArrow 11 hours agoprevWriting large web applications in Rust isn't going to happen fast. Large applications written in Rails won't run fast, might be ridden with bugs and might be hard to maintain and extend. Choosing something like Java, C# or Go might not sound sexy or cool, but with get the job done as fast as Rails while running almost as fast as Rust. That choice will also generally mean less bugs and a having a project that is easy to maintain and extend. reply 0xblinq 11 hours agoparentYou can't be seriously saying using \"Go\" (a low level systems language) will be as productive as using Rails (a very high level framework intended for CRUD applications). Despite how much you might hate Rails, that's like saying you'll build a blog faster with C++ than with WordPress. reply jb1991 10 hours agorootparentGo, a language with garbage collection and a compiler that does only an average job at optimizing machine code, is now considered low level? reply pjmlp 9 hours agorootparentIt is as high level as C, unless you consider all non ISO extensions part of the language, in which case, any language can have similar extensions. There are more compilers by the way. reply jb1991 8 hours agorootparentLast I checked, C was not garbage collected. reply pjmlp 8 hours agorootparentSo what? Here is a kernel writen in a GC enabled system programming language. https://people.inf.ethz.ch/wirth/ProjectOberon/Sources/Kerne... Or do you rather have one in Go? https://www.phoronix.com/news/TamaGo-Bare-Metal-Go-ARM reply jb1991 8 hours agorootparentOK, so it sounds like you are confirming an answer to my question, that Go, a language with garbage collection and a compiler (at least its most popular one) that does only an average job at optimizing machine code, is now considered low level. reply pjmlp 7 hours agorootparentyeah, it has always been as low level as something like Cedar in Xerox PARC, and just as high level. Or just as low level and high level as Interlisp-D at Xerox Parc. As plenty of other languages. If you want actual low level code, that is Assembly, and even that depends if the underlying CPU is a pure hardware implementation, or makes use of microcode and is for all practical purposes a mini-interpreter/JIT in hardware. reply jb1991 6 hours agorootparentThat’s a fine analysis, but most people have a more generally accepted definition of what low-level means in a context of modern systems programming languages. I suppose you have provided an alternate definition, but it’s not really in the context of what most are thinking about in this arena. It’s a bit like a podcast I listened to recently where someone suggested any language with a proper algebraic sum type can be considered a dynamic typed language in some ways. I mean, if you get philosophical about definitions and lean heavily mainly on historical context, it’s possible to say just about anything. reply pjmlp 3 hours agorootparentAnd some people call themselves engineers after a bootcamp, instead of actually earning a Software Engineering accreditation. \"C Is Not a Low-level Language Your computer is not a fast PDP-11\" https://queue.acm.org/detail.cfm?id=3212479 Naturally those people will also vouch that an article on the ACM, written by a one of the GCC Objective-C and GNUStep early contributors, and nowadays key researcher on CHERI project isn't something to take into consideration. reply 59nadir 10 hours agorootparentprevGo is not a low-level systems language. Even using the more flexible modern definition of \"low-level\" it's not a low-level language. That's like saying OCaml is low-level; it's not even close. reply yxhuvud 9 hours agorootparentGo is not a low-level systems language, but it is a low-level web site builder language. reply pjmlp 9 hours agorootparentprevGreat given that OCaml seems good enough to write hypervisors and unikernels. reply unit149 10 hours agorootparentprevIn terms of Go or C, even Rust which is one of the only of the two used in kernel development. Migrating is in line with the principles of the CRUD methodology, whether it takes place by means of a compiler or a junior developer parsing assembly line-by-line. reply loktarogar 11 hours agoparentprev> Large applications written in Rails won't run fast, might be ridden with bugs and might be hard to maintain and extend. With care, none of that is necessarily true. I think that's an overgeneralisation. I've worked with many large, bug free, stable and easy to modify rails apps. I've also worked with messy rats nests rails apps, and go apps, etc. It can be true that some languages/frameworks make it easier to get into that state, but all have a chance to make that a possibility, or other trade offs that make it difficult to build with for other reasons. With discipline you can make anything work. reply t-writescode 10 hours agorootparentI think the biggest difference for me is that when there's a rat's nest of a Rails application, refactoring it becomes the hell of type errors and writing tests that, frankly, a compiler would catch. And with sufficiently large applications, those issues really start to spread through the whole program - or can. addendum: I'd love to know why I'm getting downvoted here. I definitely speak from experience, here. reply cageface 9 hours agorootparentThis is my experience as well. Rails makes it incredibly easy to get a web app off the ground but as it grows and adds more features and complex business logic it becomes much more difficult to maintain than a codebase with static type checking. reply loktarogar 8 hours agorootparentprevI guess my point was that, with discipline, it's not a foregone conclusion that a Rails app is going to be a rats nest. The lows can be pretty low for sure. But it's one of the tradeoffs for the framework and one that can be mitigated with thought and effort. reply PaulRobinson 10 hours agoparentprevYou can write large applications in Rails that run fast, have low bug counts and are not hard to maintain or extend. There are many examples in the wild. Rails is quicker to build in than Java, C# or Go for most functionality needed in most web applications for similar skilled developers. In my experience, up to 3x quicker. I’ve worked on large monoliths and SOAs, and worked alongside Java teams. I even introduced Go to a Rails shop because I believed what you did. Turns out the real problems were MVC isn’t right for everything, and we weren’t name-spacing with enough rigour. Fix those in small and targeted areas, and Rails is just fine. reply jkman 4 hours agorootparentAs a dev that hasn't touched Ruby in his time yet, could you expand on the productivity differences you've seen between Java/C# and ruby devs? I totally buy that a language like ruby can be more productive, but a 3x difference is wild. Have you found that RoR is really just that much more productive than say, Spring or Asp.NET? reply cloverich 4 hours agorootparent(Ruby -> Rails) As one that switched into Rails after a decade of web dev, IME its the batteries included nature. You can't compare Rails to Go or Typescript. You compare Rails to language + all the standard libraries people use. The trick with Rails is _most_ of those choices are baked into the framework, so e.g. 25 rails dev's will know _mostly_ the same exact set of libraries walking in the door. Personally I absolutely hate it, and regret taking a job in it. Its the first language ecosystem that made me realize yes, apparently I really do care a lot about the language(s) I work in (had prior exp in php, python, java, Go, and Typescript, and enjoyed most of them). But I wouldn't knock anyone for using it, especially if they can hire experienced teammates in it or generally find the approach (typeless, convention over configuration) appealing. It's definitely a great choice for a web product if the team(s) will stick with it over time. I'd happily manage a team using it at this point, I just don't want to write it anymore. reply n_ary 10 hours agoparentprev> Large applications written in Rails won't run fast, might be ridden with bugs and might be hard to maintain and extend. I have never used BaseCamp, but I believe their products are well written and decent, hence with adequate care, good architecture and maintenance, it should be possible. reply tinco 11 hours agoprevThe reason NextJS and similar tech like SvelteKit are so popular amongst JS developers is that they alleviate the problem of Node.JS not having a great backend development experience. Building a backend in Typescript is like building it in C#, but without the advantages of a properly designed language. The experience of building a backend in Rails is miles away. Node.JS developers are starting to realize it's not just about having the ecosystem and the tooling, it's also about the framework being a holistic solution to building out your application. That's what NextJS and SvelteKit help with, and that's what Rails revolutionized back in 2006. reply josephg 11 hours agoparent> Building a backend in Typescript is like building it in C#, but without the advantages of a properly designed language. I like typescript. In what way isn't it a \"properly designed language\", compared to C#? reply jamil7 10 hours agorootparentWell it’s bolted on for a start so introduces overhead from the get go. Its type system is also severely hamstrung and complicated by JS compatibility. It doesn’t really compare to Rust, Swift, OCaml etc. I do think it’s a huge improvement on JS though. reply josephg 10 hours agorootparentYou say that, but its type system has some wonderful feature that those other languages lack. In what way is it hamstrung? It seems better than C# to me! For example, C# doesn’t support parameterised enums like typescript and rust do. Once you start using parameterised enums, you seriously can’t go back. It’s a killer feature. But even then, typescript goes further. All of these languages let me make a Color enum with red, green and blue variants. But as far as I know, only typescript will let me write a function which takes a strict subset of those variants. In typescript I can have a function that only accepts red or blue - and passing green (or something that might be green) would be a compilation error. Typescript also lets you make another type with a superset of another type’s variants. Eg type Foo = Color“yellow”. In rust if I want to change a function’s signature to make one of the parameters optional, that’s a breaking api change since all existing callers need to wrap the parameter in Some(val). But in typescript, I just change the parameter’s type from T to Tnull and everything works. Again, in what way is it hamstring? I’ll grant that JavaScript doesn’t have quite the performance of C# (though modern runtimes are pretty impressive!). But typescript itself seems great. reply neonsunset 5 hours agorootparent> C# doesn’t support parameterised enums like typescript and rust do To be fair, you rarely miss them with pattern matching and records. In a similar vein to a sibling comment: var animal = ... Console.WriteLine(animal switch { Animal.Dog(_, var cats) => $\"Chased {cats} cats\", Animal.Cat or Animal.Bat => \"No cats chased\", _ => \"Unknown animal\" }); abstract record Animal(string Name) { public record Dog(string Name, int CatsChased): Animal(Name); public record Cat(string Name): Animal(Name); public record Bat(string Name): Animal(Name); } There are many libraries to further enhance the experience, provide additional exhaustiveness analysis, etc. reply timeon 5 hours agorootparentprevCan TS enums carry values like Option ? reply josephg 2 hours agorootparentSure! Though the normal way to write Option is simply Tnull. reply gardenhedge 8 hours agorootparentprevDoes TS have parameterised enums? Can you give an example? reply MrJohz 7 hours agorootparentIt's not an explicit feature, but a combination of arbitrary records, sum types, and type narrowing: type Animal ={ species: \"dog\", name: string, catsChased: number }{ species: \"cat\", name: string } const animal: Animal = ... // Available in all cases console.log(animal.name) if (animal.species === \"dog\") { // Can access dog attrs here console.log(animal.catsChased) } // ERROR: catsChased only exists for dog and not for cat animal.catsChased reply creata 10 hours agorootparentprevWhile it is thoughtfully designed, its type system has been designed under the severe constraint that it must be retrofittable on typical JavaScript code. reply josephg 10 hours agorootparentYour comment doesn’t advance the conversation in any way. It just restates the claim that typescript is worse in some unspecified way. Can you give any examples of ways in which those constraints have resulted in a worse language? reply creata 8 hours agorootparent> Your comment doesn’t advance the conversation in any way. That's oddly rude. I thought it might be useful to someone (not necessarily you) to bring up the fact that TypeScript is subject to a very different pressure than most other mainstream languages. That has obviously resulted in a very different type system, e.g., one with much better support for structural typing than most mainstream languages. > It just restates the claim that typescript is worse in some unspecified way. I did not \"restate that claim\". In fact, I was careful not to claim that TypeScript is \"worse\" than any other language, because that's a meaningless word without more context. > Can you give any examples of ways in which those constraints have resulted in a worse language? Again, saying that it's \"worse\" or \"better\" without context is meaningless. reply pjmlp 10 hours agorootparentprevWhich if it was easy, instead of someone like Anders Hejlsberg leading a language design team, anyone would do it. reply creata 8 hours agorootparentI didn't say it was easy. It obviously wasn't. reply wiseowise 10 hours agorootparentprevIn a way where you can’t admit that you like JS, because it’s not cool. reply 0xblinq 11 hours agoparentprev> That's what NextJS and SvelteKit help with, and that's what Rails revolutionized back in 2006. Are you seriously comparing \"being able to execute code on the backend\" with Rails? Or Django? Or Laravel? There's no way in this world Next/Remix/etc are helping in any way to write code on the backend. Where's the support for database access? migrations? ORM? queues? scheduled jobs? validations? translations? authentication? authorization? The only thing Next.js is helping with is pushing people towards Vercel's platform, that's it. reply DeathArrow 11 hours agoparentprevIf it comes to C# vs Typescript, I'll happily chose C#, at least for backend. For the FE I would prefer Typescript over Javascript. reply rajamaka 10 hours agorootparentWhy? I don't really understand the point of declaring a preference if there's accompanying explanation. reply pjmlp 9 hours agorootparentprevI wish, unfortunately in the golden age of SaaS vendors, many seem to have Vercel deals in place, so Next.js with Typescript it is. reply lionkor 9 hours agoprevI'm a little confused about the comments here saying Rust is not great for web backends. I've had the opposite experience, building mostly small services like beampaint.com and probablyup.net. Maybe the complexity goes way up once you try to build a bigger app? I mean surely it does,but Rust is great at giving you confidence where you need it when you have large code bases reply keybored 8 hours agoparentRight, wouldn’t the difficulty go up drastically as soon as you need more framework-territory things? Because the established frameworks have them out of the box but the Rust framework is unlikely to (due to age). Imagine going from just two of those missing things to six. reply physicsguy 11 hours agoprevI’m the same with Django. I’ve found that perf issues were much more often caused by the DB queries than by Django itself. reply sureglymop 11 hours agoparentI think almost always the DB query will take the longest but: I've seen too many engineers not even do basic optimizations such as creating indexes. Here's a list of easy/low hanging fruit optimizations that are often just forgotten: Creating indexes in the db, delegating more logic to the db than doing it in code (e.g. using pipelines in mongodb, partial indexes in postgres, etc.), turning on brotli/zstd compression for all or most requests, caching objects or requests in memory (e.g. with redis), setting appropriate cache control headers, frontend stuff such as not rendering thousands of objects as DOM nodes and instead using canvas (recently saw this in a map application). reply VMtest 47 minutes agorootparentcreating indexes will lock the tables if not mistaken, the problem about database is that many of us just don't study enough to make sense out of it reply jimnotgym 9 hours agorootparentprevI have seen the opposite too, a db full of indexes that grew very large and was very slow on inserts! Understanding that an index is only useful on columns that you want to search might be a start! reply jeppester 11 hours agoprevWe recently started testing AdonisJS as a TS alternative to rails. I'd recommend anyone who'd be interested in \"Rails, but in TS\" to give it a go. It feels much more similar to rails (or Laravel) than the most popular TS stacks, and also has \"batteries included\" instead of leaving you to decide on every part of your stack (or trust a template from a third party). reply hanifc 3 hours agoparentI've been curious about AdonisJS, and I think I'll give it a try thanks to your comment. Recently, I've been fascinated by the \"batteries included\" back end frameworks of Rails, Laravel, and Django, but I've written TypeScript for pretty much my entire career, and it's just easier to keep it that way than to switch now. What are your thoughts so far in your testing? Could you compare it to any of those other frameworks I mentioned? reply oftenwrong 4 hours agoprevI once worked for a startup that used Rails for building their web app. I would say around 50% of all development was in Rails. For things that demanded more performance or correctness on complex tasks, there were some Java services and some Postgres sprocs. Our user-friendly and featureful website was a big part of our appeal. A lot of the non-Rails functionality was the unseen \"secret sauce\". I think this approach worked well. Most of what you need for a web app is commodity functionality. Correctness is not even particularly important in many cases. The Rails ecosystem has a lot of useful tools that make it easy. For example, there are multiple options for automatically adding a comprehensive admin panel. This let our non-technical admins manually accomplish CRUD tasks in the web app's domain. This allowed us to avoid overengineering. We could implement something that covered 80% of cases, and manually handle any edge cases or things that happened infrequently. An example is locking user accounts when a person left a customer's company; they could just email us and an admin would manually go in and check the \"locked\" box for the user. This allowed us to move very fast on new web app features, and focus on other aspects of the system. There were downsides, of course. One of the big pain points was upgrading Rails itself, and dealing with dependency hell at times with various libraries. You also have to know many things about how Rails works, which involves a lot of quasi-mysterious and implicitly-performed things. For example, I recall frequently coming across function calls, and then I would look for the function definition, only to find that it was a function that doesn't exist in the code and was generated at runtime by Rails. reply dajonker 1 hour agoparentThe downside of using more than one programming language (not counting javascript) is that it makes everything more complex. Development, testing, deployment, issue tracking, etc. Plus it forces people to either specialize in one part of the application or learn two things in depth. reply samjbobb 4 hours agoprevThis was a great read. Thanks to the author for the vulnerability. In this thread and others, HN does a lot of talking about the fixed properties of these languages and frameworks. Something I don’t see much is: choose the tool that you and your team are most familiar with. It’s relatively easy to learn a new language, but it takes a long time to learn an ecosystem. Learning that ecosystem takes time away from focusing on your customer. I recently started a new web app and tried to use Django because of the batteries-included framework “should” have been the right tool for the job. But I haven’t used Django in 15 years and it was slow going. After a few weeks of slow going, I rewrote the whole thing in Typescript in a week. Not because of any general argument that TS is better, but because it’s the ecosystem I know well. Not by choice, but as a function of the jobs I’ve had. I’d like to suggest that it’s okay to choose the tool you’re most familiar with, because you’ll move faster, write better code, and frankly web apps can be written in any of these languages. reply 0xblinq 12 hours agoprevUsing Rust for a CRUD application is a terrible choice. Of course it won't work. It's like using Rails to write a database or a DNS server. You're just trying to use the wrong tool for the job. reply oneshtein 9 hours agoparentIn my experience, CRUD applications in Rust are good: they are easy to write and test. :-/ Why you think that Rust is a wrong choice for a simple or a complex CRUD application? Can you explain, please? reply JoshTriplett 11 hours agoparentprevIt's not a terrible choice. It's not necessarily wildly better than alternatives, but it's not wildly worse, either. Among other reasons to consider it: if you want to run on a FaaS platorm or container setup, Rust has much faster cold start time and makes for smaller container sizes. Among reasons to not consider it: if you want to iterate rapidly by editing text and near-instantly hot-reloading your code, Rust web frameworks aren't really there yet. They will be one day, but they're not yet. reply elcritch 10 hours agorootparent> Among other reasons to consider it: if you want to run on a FaaS platorm or container setup, Rust has much faster cold start time and makes for smaller container sizes. Faster start time maybe, but I’m not sure about smaller containers. Rust is terrible with binary sizes in my admittedly limited experience. reply oneshtein 8 hours agorootparentI just compiled and checked: - sqlx binary: 200+ crates, 5.4Mb (after strip), 27 kb per crate on average. - zero2prod last example: 400+ crates, 14Mb (after strip), 35 kb per crate on average. Binary is statically linked, so Docker container is very small too. reply eYrKEC2 5 hours agoparentprevIf you don't like being paged at night, then Rust for CRUD apps is fantastic -- having worked with Python, c#, and nodejs backends, I'll take Rust backends every day of the week. reply jb1991 10 hours agoprevIt’s fascinating to me to see that nowhere in this thread, at the time of writing, is mentioned what used to be the darling of web development, a language that seduced many ruby developers to crossover: Clojure. The times have changed. reply frou_dh 10 hours agoparentTo do Clojure web development was to noodle around stitching together various libraries piecemeal, i.e. the same thing that everyone who resists proper batteries-included web frameworks does. reply pjmlp 9 hours agoparentprevThose folks are now bragging about Elixir, or maybe they jumped again into Gleam now. reply duggan 10 hours agoparentprevI think Elixir/Phoenix may be seducing more of them these days. reply keybored 11 hours agoprevThis is a nice enough article considered as an experience report. It’s tempting for me to focus too much on the title (was it a rewrite in an interesting sense? no, it was like going on a five-year trip across Asia and returning to your hometown to marry your high school sweetheart[1]). It doesn’t seem like the author learnt that much. They know Rails and Rails is good for making webapps? Makes sense. [1] For the author. The rest of the team didn’t know Rails. And there was no one on the team that knew that rewriting parts in “blazing fast”-lang would work. reply dajonker 9 hours agoparentYeah, it is maybe a bit of a clickbaity title, but I thought it was relevant enough. I tried Rewriting it in Rust and ended up not finding it worth the cost. There was some rewriting in Rails, but indeed, not very much. I did learn that things wouldn't necessarily be any better if I were to use language/framework X instead of Y. Edit: I guess I also learned that Rails does so much for you, so you can focus on building a good product. Just gotta make sure you use it as it wants to be used as much as possible. reply crabsand 10 hours agoprev\"I couldn't horseshoe a hard systems language to replace a web framework for a CRUD app written with soft language, so I had to go back\" Rails is very good until you need to go off rails. Even in that case rewrites must be partial and not to aim to fully replace the original. Most of the time rewrites are a waste of time. This article could be true if you s/Rust/Ruby and tell the story in the opposite direction. reply pmdr 4 hours agoprevRewrite it in any framework that _doesn't_ make you dependent on platforms waiting to milk you dry when usage takes off. reply simonpantzare 10 hours agoprevLaravel with Filament is super productive also. Django used to be my goto for 15 years but the frontend infra, Livewire, and flexibility of Filament compared to django-admin are hard to beat. reply block_dagger 10 hours agoprevReact plus Rails is my favorite modern combo. Development bliss. https://github.com/shakacode/react_on_rails reply hit8run 11 hours agoprevThis article is kind of the story of my life… I launched a SaaS app written in Go in 2013 coming from Rails and I had to figure out everything on my own. Migrations, database layer, asset pipeline, deployment, validations, logging, payments and so many more things that rails just give you. reply Rzor 11 hours agoparentThat experience has likely made you a more skilled developer, though. reply juliogreff 7 hours agorootparentI'm going through the same path right now, building my own product, and while I do indeed feel like it taught me tons, I don't want lessons, I want to get a business off the ground! reply 0xblinq 11 hours agoparentprevYou're comparing a systems oriented barebone low level programming language to a very (maybe the most) high level framework on top of a very (maybe the most) high level language. I'm not a fan of Go, but that's not the thing to blame here. Using Go for a \"CRUD\" application is a terrible decision from the get go. reply neonsunset 11 hours agorootparentGo is not a low level language. It does pretend to look like C but places at the other end of abstraction spectrum, even if it doesn’t seem that way. reply hit8run 11 hours agorootparentprevI’m not blaming and I fully knew what I was getting myself into. In the end it worked out and successfully launched etc. Just took me longer and had a learning curve that made me a better developer too. But had I wanted to just launch quickly from a business perspective Rails would have been the way as I already knew it. I still to this day like the simplicity and speed of Go. Like every year or two I come back to Go and try to get the dev experience for my own framework right with Rails in the back of my mind but it’s tough and more of a boilerplate than a framework. reply BoorishBears 11 hours agoprevI want to try Rails but I can't downgrade away from static typing for my personal projects. I typically want to spin them out quickly and not worry about writing tests: static typing adds way too much value in that scenario. I looked into Ruby's static typing attempts but the picture painted was grim. reply cloverich 3 hours agoparentIMHO if you like static types, I suspect you won't like Rails. Its the intentional opposite of them in many ways. I made the swap you're contemplating with similar reservations and absolutely regret it. Rails is definitely great but IME it appeals to a particular kind of developer, in the same way hard-core functional languages, strict types, or e.g. Go's' flavor of error handling each appeal to other groups. I've found those higher level patterns extend beyond mere fashion and more into some kind of persistent style preferences that's not easily changed. I dipped into the functional world and did not like it. Now I feel similarly about Rails. There are just so many times I want to offload my mental model into types, and that concept doesn't really exist in Rails. And it doesn't seem to bother most of the folks I work with, and I suspect it has more to do with how my brain works under the hood compared to theirs. I definitely think Rails can be worth trying, but if you're the type to be evaluating types from the outset, I'd wager that's the exact kind of person that won't like what they get into. reply Alifatisk 9 hours agoparentprevSadly, that is the reality of Ruby, no excuses. Introducing static typing to Ruby is a huge task considering the way the language is designed. The attempts you've seen is probably Sorbet (sorbet-runtime) and RBS. In my opinion, RBS would be an option for me if they didn't force me to work in a separate file just for type declarations. There is one gem that is interesting, it reminds me a lot of jsdoc. It's called rbs-inline https://rubygems.org/gems/rbs-inline reply chucke 8 hours agorootparentFYI rbs-inline is maintained by the rbs creator, and will eventually be merged into mainline, once the inline syntax alternatives are set in stone. All to address the main complaint you mentioned above. reply Alifatisk 6 hours agorootparentI am so glad you mentioned this, this got me excited! reply xiphias2 8 hours agorootparentprevIt's not really sad. Optional static typing has advantages and disadvantages and there are enough dynamic programming languages that have it (JS, Python for example). I think the problem with static typing is that it changes the whole ecosystem too much to be considered just optional, even if it's backwards compatible (like in Python). Just as an example I'm using lambdas in fasthtml in Python that don't allow type specifications for adding routes and fasthtml keeps on spitting out warnings that I didn't specify the type and doesn't handle variables in lambda functions correctly. reply Alifatisk 6 hours agorootparent> Just as an example I'm using lambdas in fasthtml in Python that don't allow type specifications for adding routes and fasthtml keeps on spitting out warnings that I didn't specify the type and doesn't handle variables in lambda functions correctly. That's a very interesting yet valid downside I didn't think of reply steve_gh 10 hours agoprevThis: \"Have many issues at runtime? Test more. Does it turn into unmaintainable spaghetti code after a while? Only if you let it. It's typically caused by developers, not programming languages or frameworks.\" reply t-writescode 10 hours agoparentAnd when the code becomes spaghetti, types will make it far easier to trust that the de-spaghettiing of it is safe. reply yxhuvud 9 hours agoparentprevYes. I'd much rather work with developers that know how to untangle and fix spaghetti, than those that have their hands held so steady by the environment that when the spaghetti does show up they don't know how to get out of it. reply sealeck 6 hours agorootparentI'd rather use a tool that has footguns X,Y as opposed to one without them so that when (separate) footgun Z arises in both languages the developers are more used to handling lots and lots of bugs? reply ilrwbwrkhv 10 hours agoprevRust is fine. The worst is TypeScript and Node.js. A lot of startups are making their code bases in TypeScript. And it is an unbearable mess. I recently consulted with a company where I rewrote their app in Rails. And now it's much simpler, much better, much more extensible, and far more enjoyable for the developers to work on it. reply alpaca128 9 hours agoparentAs a Rust dev it feels like I can't rely on the Typescript compiler at all. If you don't spell everything out for it, it might silently fail to infer the type of a variable and suddenly the function returns a number array instead of a string and the compiler will be just fine with it. Not to mention runtime still has all the problems of JS. reply kristiandupont 9 hours agorootparent>[...] suddenly the function returns a number array instead of a string and the compiler will be just fine with it. The type checker is quite forgiving, but that does not sound like something it would just be fine with? The TS type system is not sound which has obvious tradeoffs. I prefer it over Rust for most things because Rust feels like a Comic-book-guy type person looking over my shoulder going \"Akshually, you didn't tell me what the lifetime of this string is\", even if it is trivially obvious. I acknowledge that there are situations where that is very valuable but in my opinion it's not generally superior. reply duggan 10 hours agoparentprevAre there persistent ways in which you think Typescript codebases end up a mess? Developers have to invent too much themselves vs Rails? Something deeper? reply smrtinsert 11 hours agoprevNot getting this one. Programmer writes app in framework he knows experiences productivity? reply ridruejo 10 hours agoprevSomewhere, DHH is reading this and smiling… reply michaelteter 11 hours agoprevImagine if the author had tried Phoenix and Elixir with its amazing OTP foundation… reply zem 11 hours agoparentthe elixir ecosystem doesn't have the \"library for anything you could possibly need\" benefit that you find with rails (and probably django too) reply flymaipie 10 hours agorootparentAs an elixir dev I am wondering what exactly you’re missing? reply zem 10 hours agorootparentmost recently, maximum matching over a graph (edmond's blossom algorithm) reply flymaipie 9 hours agorootparentThat actually sounds like a perfect use case for a Rust NIF in Elixir, since graph matching can be a performance issue reply biorach 8 hours agorootparentprev> maximum matching over a graph (edmond's blossom algorithm) Ruby/Rails has this? reply zem 2 hours agorootparentruby does indeed, as do python and javascript. here's the ruby library: https://github.com/jaredbeck/graph_matching reply josefrichter 7 hours agorootparentprevThis claim is greatly exaggerated reply andrewstuart 9 hours agoprevThere's this weird conceit that Ruby On Rails somehow has a monopoly on developer experience, that Ruby On Rails gives developers pleasure that is unique and uncopyable - a unique and utterly amazing and incomprehensibly awesome developer experience that no other language or framework can replicate. Rails developers smugly assert that Ruby on Rails is more productive than anything else out there. I call BS on the \"Ruby On Rails is the very best by far\" conceit. Ruby On Rails has a monopoly only on the perception that its developer experience and productivity is off the charts. Ruby On Rails is just another language and framework. There's literally nothing that makes it different or particularly special compared to anything else. Developers everywhere with Python or nodejs or TypeScript or C# or Golang have a developer experience that is just as great, without the high handed conceit that what they are doing is amazingly better than anything else. reply frou_dh 8 hours agoparentSEGA used to have an advert, \"To be this good takes AGES\". Maybe Rails doesn't have a monopoly on having 20 years of continuous development refining it, but that's certainly not the norm. reply neonsunset 11 hours agoprevPlease don’t. You pay 100x for each line of code and get nothing in return over ASP.NET Core or Vert.X. Pick any fast compiled language you like, don’t contribute to global warming. reply duckmysick 8 hours agoparentCould you explain your comment more? The way I understand it is: - I have two servers with the exact same hardware - one runs a Ruby on Rails web app - the other runs a Vert.X web app - both apps have similar features to the end users - if I run both of them for a month, the server running the Ruby on Rails app will consume hundred times more energy in kWh compared to the server with the Vert.X app - another way to look at it, to serve the same number of visitors, I would need hundred servers running a Ruby on Rails apps and one server running a Vert.X app Is that correct? It hasn't been my experience when I measured the long-term consumptions with energy meters in real-life scenarios. How did you arrive at your number? By comparison, going from resistive heating to a heat pump is a 2.5x-4x jump in efficiency. 100x gains would be nothing short of spectacular. I noticed the best energy savings when switching to Arm CPUs, but that's a separate topic. reply bowsamic 11 hours agoparentprev“Don’t contribute to global warming” is the silliest argument for perf bc you can use it to dismiss basically any human leisure activity. Why is it okay to use a computer to play video games but not make the perf tradeoff with Ruby? reply neonsunset 11 hours agorootparentBecause there is no trade off. Rails does not offer anything over picking Kotlin/C#/F# or even Go offerings. The “productivity” is a little excuse used to hand wave away the criticism of absolutely unacceptable state of Ruby performance in the year of 2024 and all the other problems that it has. reply WA 10 hours agorootparentWhat do you mean? Rails seems to come with authentication, session management, cookie management, an ORM, extensive logging capabilities and a bunch of other things. Go and Kotlin are languages, not frameworks, and they certainly don't include these things. So what are you comparing exactly? reply neonsunset 10 hours agorootparentMy initial comment lists two great options that make choosing Rails an unconditionally short-sighted decision. reply wiseowise 10 hours agorootparentASP.NET - maybe, but you’ve got to be kidding if you suggest Vert.x to be a replacement for RoR. They operate on completely different levels of abstraction (even ignoring nightmare of Reactive code you have to write using Vert.x). reply p2detar 9 hours agorootparentNot a nightmare at all, if you know what you’re doing. There’s a learning curve indeed, I’ll give you that. reply neonsunset 6 hours agorootparentprevMaybe my impression on Vert.X was misplaced. Trying it out left me off with \"Oh, so kind of like ANC minimal API with Rx.NET but faster\". I should revisit this, thank you. JVM ecosystem has other great frameworks like Active-J and Ktor. My main point is, unless that's what you're most productive in, RoR is a really poor choice for both small and large codebases and there are plenty of options that completely eliminate the classes of issues you would otherwise have to spend the time on. Sure, I like using C# (and recently F#) a lot but it wouldn't kill me if I had to use Kotlin/Java/Go/Swift instead. All these have understandable tradeoffs and merits to them. On the other hand, my experience of interacting with advocates of Python, Ruby and Erlang is incredibly poor - it's impossible to have a technical conversation and it seems that their communities live in a bubble of belief that using interpreted junk comes with magical productivity advantages inaccessible to expressive modern compiled languages with rich type systems and excelled back-end frameworks/libraries. reply dajonker 8 hours agorootparentprevSure, I'll bite. Are you a Rails developer maintaining an app that actually has massive performance issues? Luckily, our Rails app is very snappy and runs on hardware equivalent to an average 4 year old laptop. No issues with performance that I can't fix. No exorbitant power usage. Also, as a niche business app, it will never have to scale as much as GitHub does. reply robertjpayne 9 hours agorootparentprevRuby really isn't that slow -- even so most of its \"performance\" problems are due to idling waiting for I/O not actual code. If you care about power consumption of computing you need to be looking at current AI and crypto currencies consumption like Bitcoin… reply bowsamic 11 hours agorootparentprevThe whole article is about how there is a tradeoff, sorry they didn’t investigate your preferred technology but you can’t expect them to try forever. Even if they had tho, are those really “batteries included” offerings like Rails? reply wiseowise 10 hours agorootparentprevYou can argue that game devs also have moral obligation to mitigate impact of their games by better optimizing them. reply bowsamic 9 hours agorootparentOkay but where does it stop? Surely any amount of energy wasted on entertainment is wrong. Why is it okay to waste energy for entertainment but not waste energy for development experience and efficiency? reply chikere232 10 hours agoprev [–] Didn't we make this mistake and learn from it in 2005? I guess not. Ruby, and Rails, was an awful idea then and it's an awful idea now. Even in-browser stuff is moving to typescript to get away from having to write unit tests for basic type errors. But sure, let's dig up this failure and try it again reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The developer initially experimented with multiple programming languages and frameworks, including .NET/F#, Go, Rust, and React, before choosing Ruby on Rails for its familiarity and development speed.",
      "A \"version 2\" of the app was attempted using Rust and SvelteKit, but it was found to be impractical due to missing features and the need for extensive setup, leading to a return to Rails.",
      "The experience highlighted the trade-offs in web application development, with Ruby on Rails offering a mature ecosystem that allows developers to prioritize product development over infrastructure concerns."
    ],
    "commentSummary": [
      "Rails, Django, and Laravel are highly regarded for their efficiency and comprehensive features in web app development, making them popular choices among developers.",
      "Despite the availability of newer technologies like Meteor, Remix, and Next.js, some developers prefer traditional frameworks due to their ease of use and familiarity.",
      "The choice of a web development framework often hinges on specific project requirements and personal familiarity, with alternatives like Spring Boot and Ktor offering different advantages such as integration capabilities and static typing."
    ],
    "points": 178,
    "commentCount": 179,
    "retryCount": 0,
    "time": 1730525140
  },
  {
    "id": 42022649,
    "title": "Direct Sockets API in Chrome 131",
    "originLink": "https://chromestatus.com/feature/6398297361088512",
    "originBody": "Chrome Platform Status const urlParams = new URLSearchParams(window.location.search); if (urlParams.get(\"loginStatus\") == 'False') { alert('Please log in.'); }import {ChromeStatusClient} from \"/static/dist/cs-client.js?v=06ee8b6\"; import {ChromeStatusOpenApiClient} from \"/static/dist/openapi-client.js?v=06ee8b6\"; window.csClient = new ChromeStatusClient( 'djx1OfSsXXu4xGr-QDtrGCR6AlYNKrkP1cZVfM8CU6s6MTczMDU3NDEyMA==', 0); window.csOpenApiClient = new ChromeStatusOpenApiClient();",
    "commentLink": "https://news.ycombinator.com/item?id=42022649",
    "commentBody": "Direct Sockets API in Chrome 131 (chromestatus.com)159 points by michaelkrem 19 hours agohidepastfavorite127 comments modeless 16 hours agoI think a lot of people don't realize it's possible to use UDP in browsers today with WebRTC DataChannel. I have a demo of multiplayer Quake III using peer-to-peer UDP here: https://thelongestyard.link/ Direct sockets will have their uses for compatibility with existing applications, but it's possible to do almost any kind of networking you want on the web if you control both sides of the connection. reply bpfrh 10 hours agoparentYou can also use WebTransport with streams for tcp and datagramms for udp https://developer.mozilla.org/en-US/docs/Web/API/WebTranspor... reply IshKebab 1 hour agorootparentNot peer to peer though presumably? reply modeless 1 hour agorootparentYes and not in Safari yet either. Someday I hope that all parts of WebRTC can be replaced with smaller and better APIs like this. But for now we're stuck with WebRTC. reply typedef_struct 4 hours agoparentprevThis looks to use Web Sockets, not WebRTC, right? I don't see any RTCPeerConnection, and the peerServer variable is unused. I ask because I've spent multiple days trying to get a viable non-local WebRTC connection going with no luck. view-source:https://thelongestyard.link/q3a-demo/?server=Seveja reply modeless 3 hours agorootparentWeb sockets are only used for WebRTC connection establishment. The code that creates the RTCPeerConnection is part of the Emscripten-generated JavaScript bundle. I'm using a library called HumbleNet to emulate Berkeley sockets over WebRTC. The code is here: https://github.com/jdarpinian/ioq3 and here: https://github.com/jdarpinian/HumbleNet. For example, here is the file where the RTCPeerConnection is created: https://github.com/jdarpinian/HumbleNet/blob/master/src/humb... I feel your pain. WebRTC is extremely difficult to use. reply evbogue 1 hour agorootparentprevCheck out Trystero[1], it makes WebRTC super simple to develop with. [1] https://github.com/dmotz/trystero reply ignoramous 13 hours agoparentprev> Direct sockets will have their uses for compatibility with existing applications... In fact runtimes like Node, Deno, Cloudflare Workers, Fastly Compute, Bun et al run JS on servers, and will benefit from standardization of such features. [WICG] aims to provide a space for JavaScript runtimes to collaborate on API interoperability. We focus on documenting and improving interoperability of web platform APIs across runtimes (especially non-browser ones). https://wintercg.org/ reply synctext 10 hours agorootparentThis slowly alters the essence of The Internet, due to the permissionless nature of running any self-organising system like Bittorrent and Bitcoin. This is NOT in Android, just isolated Web Apps at desktops at this stage[0]. The \"direct socket access\" creep moves forward again. First, IoT without any security standards. Now Web Apps. With direct socket access to TCP/UDP you can build anything! You loose the constraint of JS servers, costly WebRTC server hosting, and lack of listen sockets feature in WebRTC DataChannel. NAT puncturing is already solved in our lab, even for mobile 4G/5G. This might bring back the cyberpunk dreams of Peer2Peer... In our lab we bought 40+ SIM cards for the big EU 4G/5G networks and got the carrier-grade NAT puncturing working[1]. Demo blends 4G/5G puncturing, TikTok-style streaming, and Bittorrent content backend. Reading the docs, these \"isolated\" Web Apps can even do SMTP STARTTLS, IMAP STARTTLS and POP STLS. wow! [0] https://github.com/WICG/direct-sockets/blob/main/docs/explai... [1] https://repository.tudelft.nl/record/uuid:cf27f6d4-ca0b-4e20... reply 3np 4 hours agorootparent> By leveraging provider-aware (Vodafone,Orange,Telia, etc.) NAT puncturing strategies we create direct UDP-based phone-to-phone connectivity. > We utilise parallelism by opening at least 500 Internet datagram sockets on two devices. By relying on provider-aware IPv4 range allocations, provider-aware port prediction heuristics, high bandwidth probing, and the birthday paradox we can successfully bypass even symmetric NATs. U mad. Love it! reply Uptrenda 4 hours agorootparentprevHello, I wanted to say I've been working on a peer-to-peer library and I'm very much interested in your work on symmetric NAT punching (which as far as I know is novel.) Your work is exactly what I was looking for. Good job on the research. It will have far-reaching applications. I'd be interesting in implementing your algorithms depending on the difficulty some time. Are they patented or is this something anyone can use? Here's a link to an over-view for my system: https://p2pd.readthedocs.io/en/latest/p2p/connect.html My system can't handle symmetric --- symmetric. But could in theory handle other types of NATs ---- symmetric. Depending on the exact NAT types and delta types. reply ignoramous 1 hour agorootparentI read OP's thesis (which focuses on CGNAT), and one of the techniques discussed therein is similar to Tailscale's: https://tailscale.com/blog/how-nat-traversal-works ...with the help of the birthday paradox. Rather than open 1 port on the hard side and have the easy side try 65,535 possibilities, let’s open, say, 256 ports on the hard side (by having 256 sockets sending to the easy side's ip:port), and have the easy side probe target ports at random. reply noduerme 11 hours agorootparentprevCan you explain further... how does this improve upon websockets and socketIO for node? reply arlort 10 hours agorootparentWithout a middleman you can only use web socket to connect to an http server. So, for instance if I want to connect to an mqtt server from a webpage I have to use a server that supports websocket endpoint. With direct sockets I could connect to any server using any protocol reply dboreham 15 hours agoparentprevWebRTC depends on some message transport (using http) existing first between peers before the data channel can be established . That's far from equivalent capability to direct sockets. reply modeless 14 hours agorootparentYes, you do need a connection establishment server, but in most cases traffic can flow directly between peers after connection establishment. The reality of the modern internet is even with native sockets many if not most peers will not be able to establish a direct peer-to-peer connection without the involvement of a connection establishment server anyway due to firewalls, NAT, etc. So it's not as big of a downgrade as you might think. reply huggingmouth 14 hours agorootparentThat changed (ahm.. will change) with ipv6. I was surprised to see that I can reach residential ipv6 lan hosts directly from the server. No firewalls, no nat. This remains true even with abusive isps that only give out /64 blocks. That said, I agree that peer to peer will never be seemless thanks mostly to said abusive isps. reply theamk 13 hours agorootparentI sure hope not, this will bring in a new era for internet worms. If some ISPs are not currently firewalling all incoming IPv6 connections, it's a major security risk. I hope some security researcher raises boise about that soon, and the firewalls will go closed by default. reply immibis 9 hours agorootparentMy home router seems to have a stateful firewall and so does my cellphone in tethering mode - I don't know whether that one's implemented on the phone (under my control) or the network. Firewalling goes back in the control of the user in most cases - the other day we on IRC told someone how to unblock port 80 on their home router. reply 1oooqooq 13 hours agorootparentprevit kinda of already begun reply modeless 12 hours agorootparentHas there been a big ipv6 worm? I thought that the defense against worms was that scanning the address space was impractical due to the large size. reply 1oooqooq 6 hours agorootparenti don't think they scan the entire space. but even before that there were ones abusing bonjour/upnp which is what chrome will bring back with this feature. reply kelnos 13 hours agorootparentprev> I was surprised to see that I can reach residential ipv6 lan hosts directly from the server. No firewalls, no nat No NAT, sure, that's great. But no firewalls? That's not great. Lots of misconfigured networks waiting for the right malware to come by... reply apitman 13 hours agorootparentprevIPv6 isn't going to happen. Most people's needs are met by NAT for clients and SNI routing for servers. We ran out of IPv4 addresses years ago. If it was actually a problem it would have happened then. It makes me said for the p2p internet but it's true. reply ElijahLynn 4 hours agorootparent\"We are introducing a new charge for public IPv4 addresses. Effective February 1, 2024 there will be a charge of $0.005 per IP per hour for all public IPv4 addresses\" https://aws.amazon.com/blogs/aws/new-aws-public-ipv4-address... reply apitman 3 hours agorootparentYes and setting up a single IPv4 VPS as load balancer with SNI routing in front of IPv6-only instances solves that. Most people are probably using ELB anyway. reply justahuman74 13 hours agorootparentprev> If it was actually a problem It became a problem precisely the moment AWS starting charging for ipv4 addresses. \"IPv4 will cost our company X dollars in 2026, supporting IPv6 by 2026 will cost Y dollars, a Z% saving\" There's now a tangible motivator for various corporate systems to at least support ipv6 everywhere - which was the real ipv6 impediment. Residential ISP appear to be very capable of moving to v6, there are lots of examples of that happening in their backends, and they've demonstrated already that they're plenty capable of giving end users boxes the just so happen to do ipv6. reply apitman 3 hours agorootparentYes and setting up a single IPv4 VPS as load balancer with SNI routing in front of IPv6-only instances solves that. Most people are probably using ELB anyway reply immibis 9 hours agorootparentprevWhat do you mean not going to happen? It's already happening. It's about 45% of internet packets. reply apitman 3 hours agorootparentThe sun is about 45% of the way through its life. reply paulddraper 5 hours agorootparentprevNot happening for 55%. Try to connect to github.com over IPv6. reply remram 4 hours agorootparentIt doesn't work now so it's never going to work? reply apitman 3 hours agorootparentGitHub might work someday. Wide enough adoption that you can host a service without an IPv4 address will never happen. reply sroussey 37 minutes agorootparentHonestly, it could be a feature rather than a bug… reply lifthrasiir 14 hours agorootparentprevNot only that, but DTLS is mandated for any UDP connections. reply modeless 13 hours agorootparentIs that a problem? Again, I'm talking about the scenario where you control both sides of the connection, not where you're trying to use UDP to communicate with a third party service. reply lifthrasiir 13 hours agorootparentI think all three comments including mine are essentially saying the same but in different viewpoints. reply flohofwoe 8 hours agoparentprevThere's also this new WebTransport thingie based on HTTP/3: https://developer.mozilla.org/en-US/docs/Web/API/WebTranspor... I haven't tinkered with it yet though. reply modeless 2 hours agorootparentYeah, not in Safari yet and no peer-to-peer support. Maybe someday though! It will be great if all of WebRTC's features can be replaced by better, smaller-scoped APIs like this. reply mhitza 16 hours agoparentprevLongest Yard is my favorite Q3 map, but for some reason I cannot use my mouse (?) in your version of the Quake 3 demo. reply modeless 16 hours agorootparentInteresting, what browser and OS? reply nmfisher 9 hours agorootparentI can't use mouse either, macos/Chrome. Otherwise, cool! reply mhitza 16 hours agorootparentprevBrave browser (Chromium via Flatpak) on the Steam Deck (Arch Linux) in Desktop mode with bluetooth connected mouse/keyboard. reply modeless 13 hours agorootparentHmm, I bet the problem is my code expects touch events instead of mouse events when a touchscreen is present. Unfortunately I don't have a computer with both touchscreen and mouse here to test with so I didn't test that case. I did implement both gamepad and touch controls, so you could try them to see if they work. reply topspin 15 hours agorootparentprevSame browser on win10. Mouse works after you click in the window and it goes full screen. However, it hangs after a few seconds of game play. Stopped hanging... then input locks up somehow. Switched to chrome on win10, same issue: input locks up after a bit. reply modeless 14 hours agorootparentYeah that issue I have seen, but unfortunately haven't been able to debug yet as it isn't very reproducible and usually stops happening under a debugger. reply mhitza 16 hours agorootparentprevWorks in Firefox, on the same system. reply yesthisiswes 7 hours agoparentprevAwesome demo. I’ve really missed that map it’s been too long. reply nightowl_games 15 hours agoparentprevYeah we use WebRTC for our games built on a fork of Godot 3. https://gooberdash.winterpixel.io/ tbh the WebRTC performance is basically the same network performance as websockets and was way more complicated to implement. Maybe the webrtc perf is better in other parts of the world or something... reply modeless 14 hours agorootparentYeah WebRTC is a bear to implement for sure. Very poorly designed API. It can definitely provide significant performance improvements over web sockets, but only when configured correctly (unordered/unreliable mode) and not in every case (peer-to-peer is an afterthought in the modern internet). reply nightowl_games 14 hours agorootparentWe got it in unreliable/unordered and it still barely moves the needle on network perf over websockets from what we see in north america connecting to another server in north america reply modeless 14 hours agorootparentI wouldn't expect a big improvement in average performance but the long tail of high latency cases should be improved by avoiding head-of-line blocking. Also peer-to-peer should be an improvement over client-server-client in some situations. Not for battle royale though I guess. Edit: Very cool game! I love instant loading web games and yours seems very polished and fun to play. Has the web version been profitable, or is most of your revenue from the app stores? I wish I better understood the reasons web games (reportedly) struggle to monetize. reply windows2020 14 hours agorootparentprevI would say WebRTC is both a must and only worth it if you need UDP, such as in the case of real-time video. reply saurik 12 hours agorootparentprevI mean, the only cases where UDP vs. TCP are going to matter are 1) if you experience packet loss (and maybe you aren't for whatever reason) and 2) if you are willing to actively try to shove other protocols around and not have a congestion controller (and WebRTC definitely has a congestion controller, with the default in most implementations being an algorithm about as good as a low-quality TCP stack). reply modeless 12 hours agorootparentOut-of-order delivery is another case where UDP provides a benefit. reply winrid 15 hours agoparentprevRuns smoother than the Android home screen. :) reply chocolatkey 18 hours agoprevWhen reading https://github.com/WICG/direct-sockets/blob/main/docs%2Fexpl..., it's noted this is part of the \"isolated web apps\" proposal: https://github.com/WICG/isolated-web-apps/blob/main/README.m... , which is important context because the obvious reaction to this is the security nightmare reply crote 13 hours agoparentThat doesn't really make it any better, if you ask me. The entire Isolated Web Apps proposal is a massive breakdown of the well-established boundaries provided by browsers. Every user understands two things about the internet: 1) check the URL before entering any sensitive data, and 2) don't run random stuff you download. The latter is heavily enforced by both Chrome and Windows complaining quite a bit if you're trying to run downloaded executables - especially unsigned ones. If you follow those two basic things, websites cannot hurt your machine. IWA seems to be turning this upside-down. Chrome is essentially completely bypassing all protections the OS has added, and allowing Magically Flagged Websites to do all sorts of dangerous stuff on your computer. No matter what kind of UX they provide, it is going to be nigh-on impossible to explain to people that websites are now suddenly able to do serious harm to your local network. Browsers should not be involved in this. They are intended to run untrusted code. No browser should be allowed to randomly start executing third-party code as if it is trustworthy, that's not what browsers are for. It's like the FDA suddenly allowing rat poison into food products - provided you inform consumers by adding it to the ingredients list of course. reply apitman 13 hours agorootparent> Every user understands two things about the internet: 1) check the URL before entering any sensitive data, and 2) don't run random stuff you download I think you're severely overestimating the things every user knows. reply derefr 10 hours agorootparentprevDoes it help to think of it less as Chrome allowing websites to do XYZ, and more as a PWA API for offering to install full-fat browser-wrapper OS apps (like the Electron kind) — where these apps just so happen to “borrow” the runtime of the browser they were installed with, rather than shipping with (and thus having to update) their own? reply mschuster91 9 minutes agorootparentprev> If you follow those two basic things, websites cannot hurt your machine. Oh yes they can. Quite a bunch of \"helper\" apps - printer drivers are a bit notorious IME - open up local HTTP servers, and not all of them enforce CORS properly. Add some RCE or privilege escalation vulnerability in that helper app and you got yourself an 0wn-from-the-browser exploit chain. reply rad_gruchalski 10 hours agorootparentprevThe last time I used Chrome was about 3 years ago. You have a choice. reply eitland 8 hours agorootparentSomething always breaks my streak, but since last year or so I feel I am down to twice a year or something. reply girvo 13 hours agorootparentprevUnfortunately this is the future. Handing the world wide webs future to Google was a mistake, and the only remedy is likely to come from an (unlikely) antitrust breakup or divestment. reply rad_gruchalski 5 hours agorootparent> Handing the world wide webs future to Google Nobody handed anything to anyone. They go with the flow. The flow is driven by people who use their products. The browser is how Google delivers their products so it’s kinda difficult to blame them for trying to push the envelope but there are alternatives to Chrome. reply troupo 5 hours agorootparent> They go with the flow. The ancient history of just 10-15 years ago shows Google aggressively marketing Chrome across all of its not inconsiderable properties like search and Youtube, and sabotaging other browsers while they were at it: https://archive.is/2019.04.15-165942/https://twitter.com/joh... reply rad_gruchalski 3 hours agorootparentIndeed. There was time I myself used it as my primary browser and recommended it to everyone around. That changed when they started insisting on signing into the account to „make the most out of it” so I went back to Firefox. Since then I stopped caring. I know, virtue signalling. My point is: nobody handed anything over to Google. At the time alternatives sucked so they won the market. But today we have great alternatives. reply bloomingkales 12 hours agorootparentprevI doubt websites as we know it will be what we’ll be dealing with going forward anyways. What is a browser if we just digest all the HTML and spit out clean text in the long run? We handed over something of some value I guess, once upon a time. reply phildenhoff 17 hours agoparentprevInteresting — the Firefox team’s response was very negative, but didn’t (in my reading) address use of the API as being part of an otherwise essentially trusted app (as opposed to being an API available to any website). In reading their comments, I also felt the API was a bad idea. Especially when technology like Electron or Tauri exist, which can do those TCP or UDP connections. But IWA serves to displace Electron, I guess reply nzoschke 16 hours agorootparentI'm hacking on a Tauri web app that needs to bridge to talking UDP protocols literally as we speak. While Tauri seems better than ever for cross platform native apps, it's still a huge step to take to allow my web app access to lower level. Rust toolchain, Tauri plugins, sidecar processes, code gen, JSON RPC, all to let my web app talk to my network. Seems great that Chrome continues to bundle these pieces into the browser engine itself. Direct sockets plus WASM could eat a lot of software... reply 1oooqooq 12 hours agorootparentwith so many multiplatform gui toolkits today, tauri and electron are really bad choices reply montymintypie 12 hours agorootparentWhat's your recommendation? I've tried so many multiplatform toolkits (including GTK, Qt, wxWidgets, Iced, egui, imgui, and investigated slint and sciter) and nothing has come close to the speed of dev and small final app size of something like Tauri+Svelte. reply nzoschke 9 hours agorootparentI've also tried Flutter, React Native, Kotlin multiplatform, Wails. I'm landing on Svelte and Tauri too. The other alternative I dabble with is using the Android Studio, XCode to write my own WebView wrappers. reply bpfrh 3 hours agorootparentWhat did you dislike about kotlin multiplattform? reply 1oooqooq 6 hours agorootparentprevof course dev speed will be better with tauri plus the literal ton of JavaScript transpilers we use today. but for us an inhouse egui pile of helpers allow for fast applications that are closer to native speeds. and flutter for mobile (using neither Cupertino or material) reply cageface 9 hours agorootparentprevThe cross platform desktop gui toolkits all have some very big downsides and tend to result in bad looking UIs too. reply rubymamis 9 hours agorootparentI've built my app[1] using Qt (C++ and QML), and I think the UI looks decent. There's still a long way for it to feel truly native, but I've got some cool ideas. [1] https://get-notes.com/ reply rty32 4 hours agorootparentYou are probably not solving the same problems many other people are facing. Many such applications are accessible on the web, often with the exact UI. They may even have a mobile/iPad version. They may be big enough that they have a design system that needs to be applied to in every UI (including company website). Building C++ code on all platforms and running all the tests may be too expensive. The list goes on. reply rubymamis 2 hours agorootparentI just started prototyping a mobile version of my app (which shares the code as my desktop app) and the result looks promising (still work-in-progress tho). Offering a web app is indeed not trivial. Maybe Qt WebAssembly will be a viable option if I can optimize the binary and users wouldn't mind first long load time (and then the app should be cached for instant load). Or maybe I could build a read-only web app using web technology. Currently, my focus is building a good native application, and I think most of my users care about that. But in the future, I can see how a web app could be useful for more users. One thing I would like to built is a web browser that could load both QML and HTML files (using regular web engine), so I could simply deploy my app by serving my QML files without the binary over the internet. reply cageface 3 hours agorootparentprevThat's definitely one of the best looking Qt apps I've seen. reply rubymamis 2 hours agorootparentThank you! I think Qt is absolutely great. One need to put a little effort to make it look and behave nicely. I wrote a blog post about it[1], if you're interested. [1] https://rubymamistvalove.com/block-editor reply chrismorgan 16 hours agorootparentprev> but didn’t (in my reading) address use of the API as being part of an otherwise essentially trusted app That’s what the Narrower Applicability section is about . It exposes new vulnerabilities because of IP address reuse across networks, and DNS rebinding. reply mmis1000 1 hour agorootparent- It is possible, if not likely, that an attacker will control name resolution for a chosen name. This allows them to provide an IP address (or a redirect that uses CNAME or similar) that could enable request forgery. This is quite trival, not even possible though. DNS server is quite a simple protocol. Writing a dns that reflect every request from aaa-bbb-ccc-ddd.domain.test to ip aaa.bbb.ccc.ddd won't take you even for a day. And in fact this already existed in the wild. reply rty32 5 hours agoparentprevHave isolated web apps/web bundle gained any traction over the past few years? I just realized that this thing existed and there were some discussions around it -- I almost completely forgot this. I did a search, and most stuff come from a few years ago. reply meiraleal 2 hours agorootparentIt is used by chromeOS reply chrisvenum 15 hours agoprevI found this issue indicating a bad idea for end user safety: https://github.com/mozilla/standards-positions/issues/431 reply badgersnake 49 minutes agoprevIt’s pretty clear Google are building an operating system, not a browser. reply jeswin 14 hours agoprevI prefer web apps to native apps any day. However, web apps are limited by what they can do. But what they can do is not consistent - for example, it can take your picture and listen to your microphone if you give permissions; but it can't open a socket. Another example: Chrome came out with an File System Access API [2] in August; it's fantastic (I am using it) and it allows a class of native apps to be replaced by Web Apps. As a user, I don't mind having to jump through hoops (as a user) and giant warning screens to accept that permission - but I want this ability on the Web Platform. For Web Apps to be able to complete with native apps, we need more flexibility Mozilla. [1] [1]: https://mozilla.github.io/standards-positions/ [2]: https://developer.chrome.com/docs/capabilities/web-apis/file... reply 1oooqooq 12 hours agoparentnah. we need even less. i rather webapps because of the limitations. much less to worry about reply hipadev23 11 hours agoprevWhat about WebTransport? I thought that was the http/3 upgrade to WebSockets that supported unreliable and out-of-order messaging reply mmis1000 49 minutes agoparentI think WebRTC data channels will be a good alternative if you want peer to peer connection. WebTransport is strictly for Client-Server architecture only. reply fhdsgbbcaA 15 hours agoprevGreat fingerprinting vector. Expect nothing less from Google. reply mlhpdx 15 hours agoprevI’m excited, and anticipate some interesting innovation once browser applications can “talk UDP”. It’s a long time in the making. Gaming isn’t the end of it — being able to communicate with local network services (hardware) without involving an API intervening is very attractive. reply immibis 9 hours agoparentIndeed. I'll finally be able to connect to your router and change your wifi password, all through your browser. reply lazyasciiart 7 hours agorootparentShhh, you’re giving my parents unrealistic expectations of how much remote tech support I can do. reply arzig 7 hours agoprevThe inner platform effect intensifies. reply Uptrenda 4 hours agoprevI saw this proposal years ago now and was initially excited about it. But seeing how people envisioned the APIs, usage, etc, made me realize that it was already too locked down. Being able to have something that ran on any browser is the core benefit here. I get that there are security concerns but unfortunately everyone who worked on this was too paranoid and dismissive to design something open (yet secure.) And that's where the proposal is today. A niche feature that might as well just be regular sockets on the desktop. 0/10 reply grishka 14 minutes agoprevCan we please stop this feature creep in browsers already? reply bloomingkales 16 hours agoprevCan a browser run a web server with this? reply melchizedek6809 9 hours agoparentSince it allows for accepting incoming TCP connections, this should allow for HTTP servers to run within the browser, although running directly on port 80/443 might not be supported everywhere (can't see it mentioned in the spec, but from what I remember on most *nix systems only root can listen on ports below 1024, though I might be mistaken since it's been a while) reply apitman 13 hours agoparentprevI assume they would limit it to clients. reply Asmod4n 6 hours agoprevThank god they plan to limit this to electron type apps. reply sabbaticaldev 6 hours agoprevso with this I would be able to create a server in my desktop web app and sync all my devices using webrtc reply kureikain 12 hours agoprevThis means that we can finally do gRPC directly from browser. reply Spivak 15 hours agoprevAnything that moves the web closer to its natural end state— the J(S)VM is a win in my book. Making web apps a formally separate thing from pages might do some good for the web overall. We could start thinking about taking away features from the page side. reply remram 4 hours agoparentThis is beyond that, it's more a move to remove the VM than make JS a generic VM. reply revskill 3 hours agoprevThat means we can connect directly to remote Postgres server from web browser ? reply FpUser 3 hours agoprevAll nice and welcome. At what point browser becomes full blown OS with the same functionality and associated vulnerabilities yet still less performant as it sites on top of other OS and goes through more layers. And of course ran and driven by one of the largest privacy invader and spammer of the world reply anilgulecha 2 hours agoparent> At what point browser becomes full blown OS. Happened over a decade ago - ChromeOS. It's also the birthplace of other similar tech.. webmidi webusb Bluetooth etc. reply parweb 17 hours agoprevnext [3 more] [flagged] potwinkle 17 hours agoparentPlease don't paste unedited AI output as a comment to a discussion. reply zzo38computer 17 hours agoparentprevI also think direct sockets can be helpful. (Note: I did not read the article because it does not work on my computer.) Another use would be for extensions (rather than web pages) to implement other protocols (which is related to item 2 in your list, but different). However, I think that many of these things shouldn't need to use a web browser at all. A web browser is a complicated software and using other software would be better if you are able to do so. This includes ping, traceroute, etc, which can already be handled by other programs (and can be used even if you do not have a web browser installed); but these things may be useful on Chromebook, perhaps; or if you have Chrome 131 but cannot use other software for some reason. For example, a service could be available by some other protocols (e.g. IRC), but also provide a web interface; this can then be one of the implementations of the protocol, so that if the web interface is compatible with your computer but the other provided implementations are not compatible (e.g. because you do not have a suitable operating system, or because you don't want to install extra software but you already have Chrome, etc), then it provides an additional interoperability, without needing too much additional complexity. Handling security is necessary, although there are ways to make it securely: Ask the user first to allow it, and allow the user to configure proxies and restrictions on the use (e.g. if it can only access specific addresses or cannot access specific addresses, or to allow or disallow specific port numbers, etc). (If a SOCKS proxy with localhost can be configured, then the user can use separate software to handle this; the web browser will just need to ensure that it is possible to be configured to not block anything, in case the user is configuring it like this in order to implement their own blocking rules.) A server's web pages should ideally include documentation as well, which allows you to find documentation and use other software (or write your own), if you do not have a compatible web browser or if you do not wish to use the web interface. So, I think that it is helpful, although there are some considerations. (The one about documentation is not really one that the authors of web browsers could easily enforce, and is the kind of problem that many web pages already have anyways, and this can't help.) reply pjmlp 7 hours agoprevYet another small step into ChromeOS take over. reply huqedato 10 hours agoprevJust now, when I have only recently switched permanently to Firefox... reply Jiahang 16 hours agoprevnice！ reply tjoff 11 hours agoprevGreat, so now a mis-click and your browser will have a field day infecting your printer, coffee machine and all the other crap that was previously shielded by NAT and/or a firewall. reply jeroenhd 10 hours agoparentAs long as they don't change the spec, this will only be available to special locally installed apps in enterprise ChromeOS environments. I don't think their latest weird app format is going to make it to other browsers, so this will remain one of those weird Chrome only APIs that nobody uses. reply fensgrim 6 hours agorootparent> special locally installed apps in enterprise ChromeOS environments There was https://developer.chrome.com/docs/apps/overview though, so this seems to be a kind of planned feature creep after deprecating former one? \"Yeah our enterprise partners now totally need this, you see, no reasoning needed\" reply troupo 9 hours agoprevStatus of specification: \"It is not a W3C Standard nor is it on the W3C Standards Track.\" Status in Chrome: shipping in 131 Expect people claiming this is a vital standard that Apple is not implementing because they don't want web apps to compete with App Store. Also expect sites like https://whatpwacando.today/ uncritically just include this reply meiraleal 7 hours agoparentExpect Apple claiming this is a not vital standard and Apple is not implementing because they don't want web apps to compete with App Store. Also expect sites like https://whatpwacando.today/ to obviously just include this reply troupo 6 hours agorootparentWhich part of \"is not a w3c standard and not any standards track\" do you not understand? I am not surprised sites like that include Chrome-only non-standards, they've done this for years claiming impartiality reply meiraleal 6 hours agorootparentCry me a river. Apple doesn't need you to defend their strategic and intentional PWA boycott. reply troupo 5 hours agorootparentWhich part of \"is not a w3c standard and not any standards track\" do you not understand? Do you understand that for something to become a standard, it needs two independent implementations? And a consensus on API? Do you understand that \"not on any standards track\" means it's Chrome and only Chrome pushing this? That Firefox isn't interested in this either? Do you understand that blaming Apple for everything is borderline psychotic? And that Chrome implementing something at neck-breaking pace doesn't make it a standard? Here's Mozilla's extensive analysis and conclusion \"harmful\" that Google sycophants and Apple haters couldn't care less about: https://github.com/mozilla/standards-positions/issues/431#is... reply nulld3v 2 hours agorootparentThere are a lot of reasons why people have such extreme differing opinions on this. I for one, am still salty about the death of WebSQL due to \"needing independent implementations\". Frankly put, I think that rule is entirely BS and needs to be completely removed. Sure, there is only one implementation of WebSQL (SQLite) but it is extremely well audited, documented and understood. Now that WebSQL is gone, what has the standards committee done to replace it? Well, now they suggest using IndexedDB or bringing your own SQLite binary using WASM. IndexedDB is very low level, which is why almost no one uses it directly. And it also has garbage performance, to the point where it's literally faster for you run SQLite on top of IndexedDB instead: https://jlongster.com/future-sql-web So ultimately if you want to have any data storage on the web that isn't just key-value, you now have to ship your own SQLite binary or use some custom JS storage library. So end users now have to download a giant binary blob, that is also completely unauditable. And now that there is no standard storage solution, everybody uses a slew of different libraries to try to emulate SQL/NoSQL storage. And this storage is emulated on top of IndexedDB/LocalStorage so they are all trying to mangle high level data into key-value storage so it ends up being incredibly difficult to inspect as an end-user. As a reminder: when the standards committee fails to create a good standard, the result is not \"everybody doesn't do this because there is no standard\", it is \"everybody will still do this but they will do it 1 million different ways\". reply meiraleal 4 hours agorootparentprevWhat part of \"cry me a river\" you didn't understand? Don't go crazy because at least one of the browsers propose things that move the web forward. Geez, you should take a break from the internet. So many \"?\" reply xenator 16 hours agoprevCan't wait to see it working. reply revskill 3 hours agoparentWhy waiting ? What can you do with it ? Can't wait to wait for you. reply hexo 4 hours agoprev [–] Game over for security. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The Direct Sockets API in Chrome 131 introduces direct TCP/UDP connections in browsers, enhancing compatibility with existing applications and enabling new networking possibilities.",
      "While it could simplify use cases compared to WebRTC DataChannel, there are security concerns about potential vulnerabilities in local networks.",
      "Part of the \"isolated web apps\" proposal, this API aims to give web apps capabilities similar to native apps, sparking debate over security and trusted code execution in browsers."
    ],
    "points": 159,
    "commentCount": 127,
    "retryCount": 0,
    "time": 1730504137
  },
  {
    "id": 42022796,
    "title": "Okta – Username Above 52 Characters Security Advisory",
    "originLink": "https://trust.okta.com/security-advisories/okta-ad-ldap-delegated-authentication-username/",
    "originBody": "Okta AD/LDAP Delegated Authentication - Username Above 52 Characters Security Advisory View all security advisories Description On October 30, 2024, a vulnerability was internally identified in generating the cache key for AD/LDAP DelAuth. The Bcrypt algorithm was used to generate the cache key where we hash a combined string of userId + username + password. During specific conditions, this could allow users to authenticate by only providing the username with the stored cache key of a previous successful authentication. Note: A precondition for this vulnerability is that the username must be or exceed 52 characters any time a cache key is generated for the user. Affected products and versions Okta AD/LDAP DelAuth as of July 23, 2024 Resolution This vulnerability was resolved in Okta’s production environment on October 30, 2024. Severity Details The vulnerability can be exploited if the agent is down and cannot be reached OR there is high traffic. This will result in the DelAuth hitting the cache first. Customer Recommendations Customers meeting the preconditions should investigate their org system log for this issue between the period of July 23rd, 2024 to October 30th, 2024. Timeline 2024-07-23 - Vulnerability introduced as part of a standard Okta release 2024-10-30 - Vulnerability discovered internally 2024-10-30 - Vulnerability resolved by switching cryptographic algorithms, from Bcrypt for PBKDF2",
    "commentLink": "https://news.ycombinator.com/item?id=42022796",
    "commentBody": "Okta – Username Above 52 Characters Security Advisory (okta.com)129 points by lopkeny12ko 19 hours agohidepastfavorite67 comments fanf2 18 hours ago> https://man.openbsd.org/crypt > So if the userid is 18 digits, the username is 52 characters, and the delimiters are 1 character each, then the total length of the non-secret prefix is 72, and bcrypt will drop the secret suffix. You aren’t supposed to put more than the salt and the password into trad unix password hashes. reply 3np 5 hours agoparentThat should also mean that ca 50-52 character usernames are likely easily bruteforcable. Which makes the preconditions wider than those stated in the publication. reply fanf2 2 hours agorootparent50 letters is 235 bits which is not at all bruteforceable. reply akerl_ 1 hour agorootparentThey’re saying that if a username is 50 characters, only 2 characters of the password are used to in the cache key. And a 2 character password is very bruteforceable. reply gzer0 15 hours agoparentprevHere's how I see it: Core issue (okta's approach): * They concatenated userId + username + password for a cache key * Used BCrypt (which has a 72-byte limit) * The concatenation could exceed 72 bytes, causing the password portion to be truncated Why this is problematic: * BCrypt is designed for password hashing, not cache key generation * Mixing identifiers (userId, username) with secrets (password) in the same hash * Truncation risk due to BCrypt's limits Password storage should be separate from cache key generation. Use a random salt + appropriate hash function and for cache keys - use HMAC or KDF w/appropriate inputs reply magicalhippo 17 hours agoparentprevPotentially ignorant question, why would they go for bcrypt over say HKDF[1], especially since they mix in public data like the username and potentially userid? [1]: https://datatracker.ietf.org/doc/html/rfc5869 reply ronsor 17 hours agorootparentWhy do we need a KDF for a cache key? Won't a normal cryptographic hash function (or its HMAC variant) suffice? reply fanf2 17 hours agorootparentIf the cache gets leaked, you don’t want any miscreants to be able to bruteforce passwords from the cache keys. reply ronsor 17 hours agorootparentDo we need to put the password in the cache key? reply sebastialonso 12 hours agorootparentCan't believe the answers you're getting. The answer's a big fat NO. If you find yourself in that situation, there's something very incorrect with your design. reply magicalhippo 8 hours agorootparentSo how would you design it instead? reply its-summertime 31 minutes agorootparentkey = anyhash(uuid+username) if (result := cache.get(uuid+username)): if hash_and_equality(password, result.password_hash): return result.the_other_stuff # try login or else fail reply magicalhippo 10 minutes agorootparentSome insight into why this is good and why including the password as input in the derivation of the cache key is terrible would be appreciated. ptcrash 14 hours agorootparentprevIf you want to validate a username/password authn attempt against a cache, then yes the username and password have to be someone in the mix. reply magicalhippo 16 hours agorootparentprevIf the user changes password it invalidates the cache entries automatically, so you avoid stale credentials exploiting the cache At least that's my immediate thought, could be wrong. reply paulddraper 5 hours agorootparentprevIsn't the whole point of using bcrypt is that you can't bruteforce the password? reply a-dub 16 hours agoparentprevwhy would someone in 2024 reach for bcrypt for building a secure hash key? reply heliosyne 13 hours agorootparentBecause bcrypt is still viable. Its cost factor is easily scaled to commodity performance, keeping the attack cost high. The main attack vector these days is GPU-based compute. There, SHA* algorithms are particularly weak because they can be so efficiently computed. Unlike SHA algorithms, bcrypt generates high memory contention, even on modern GPUs. Add in the constraint of broad support, low \"honest use\" cost, and maturity (extensive hostile cryptanalysis), bcrypt stays as one of the better choices even 25 year later. That said, bcrypt's main limitation is it has a low memory-size cost. There are some newer algorithms that improve on bcrypt by increasing the memory-size cost to more than is practical even for FPGA attacks. More importantly, bcrypt didn't actually fail here. The vulnerability happened because okta didn't use it correctly. All crypto is insecure if you use it wrong enough. reply a-dub 8 minutes agorootparentafter all of the headaches in the late 90s/early 2000s with truncating password hash functions, i'm just a little surprised that this sort of thing would still be an issue. i understand performance concerns and design trade offs, but i would expect a secure hashing function in 2024 to do proper message scheduling and compression or return errors when truncations are happening. i suppose 90s culture is hip again these days, so maybe this does make sense? reply mkj 10 hours agorootparentprevIt seemed the best option I could find for a rp2040 microcontroller when I went looking recently? Perhaps not for okta... reply EasyMark 15 hours agorootparentprevokta has been around longer than a year and momentum keeps a lot of companies from changing anything until catastrophe strikes reply pquerna 14 hours agorootparentper2024-07-23 - Vulnerability introduced as part of a standard Okta release This issue is not an \"okta is old\" issue. this was new code written in 2024 that used a password hashing function from 1999 as a cache key. reply marginalia_nu 17 hours agoparentprev> You aren’t supposed to put more than the salt and the password into trad unix password hashes. To be fair, they're basically salting with the userid and username. Still unorthodox to be sure. reply fanf2 17 hours agorootparentThe salt is a separate input to the algorithm that is used differently and usually more restricted than the password. reply njtransit 16 hours agorootparentThe goal of a salt is to prevent lookup attacks. Since the user id is unique to each user, it prevents the use of pre-computed lookup tables like a salt would. reply heliosyne 13 hours agorootparentThe security of salting is twofold. Yes, it defeats the common rainbow table. But if the salt is known, a rainbow table for that salt can be computed. The security of salting depends on the salt being unknown. If the salt is externally known, which the username and userID necessarily are, then the rainbow table for that account can be computed entirely offline, defeating the point of salting. reply MattPalmer1086 8 hours agorootparentThe primary purpose of salting is to prevent precomputation being used to attack all users (e.g. rainbow tables). Even when specific salts are known they have already done this job. Salts are not intended to be secrets. If you want to treat a salt as if it was a private key, that would only provide additional protection for the very specific circumstance where the user hash is compromised, but the corresponding salt was not. reply chiph 4 hours agorootparentprevYou're both right, but are coming at this from different directions. In the past a rainbow table was intended to reveal as many passwords on a system as possible once you got a copy of the passwords. If one of them happened to be a high-value account, great. But maybe access to an ordinary account is good enough for their (nefarious) purposes. It's also possible to build a rainbow table when you already know an account is high-value and have the salt. You can't go download that rainbow table - you'll have to compute it yourself, so the cost to the attacker is higher. But if the account is valuable enough to justify targeting specifically, you'll do it. reply notpushkin 3 hours agorootparentprev> then the rainbow table for that account can be computed entirely offline So you basically bruteforce the password for a specific account before you get the actual hash but after you know the hashing scheme? I don’t see how this helps with any sort of attack though. reply lmz 13 hours agorootparentprevNo. Such a rainbow table is per-username and non reusable. Which is the point of salting. reply heliosyne 12 hours agorootparentNo, rainbow tables are hash-input specific. They're user-specific only if the salt is user-unique. Usernames aren't normally part of the hash input because they're assumed-public knowledge. You can test this for yourself by creating a user account, then editing the master password database and manually changing the username without recalculating the password hash. The password will still work. If the username was part of the hash input, the password would fail. reply lmz 9 hours agorootparentYou complained about them salting using the public username as salt. You asserted that this makes the rainbow table computable offline. I asserted that this didn't matter much for security since the table for H(username || secret) is username specific and not reusable for other usernames. Since precomputed rainbow tables consume quite a bit of space it is rather unlikely that anyone would have such a table stored for any random username. reply vlovich123 3 hours agorootparentYou can still build a rainbow table for a specific username if you want to do a targeted attack. reply heliosyne 7 hours agorootparentprevI didn't complain about anyone. The Okta vulnerability isn't because of public salts. reply marginalia_nu 16 hours agorootparentprevThat's fair. Though you can salt a hash using a function that does not take a distinct salt input by just concatenating the salt with the value. This is a relatively common practice, but of course only works if there is no truncation of the salted input. reply 0x457 17 hours agoparentprevI mean yes overall, but why would put delimeters into hash? you just smash bytes together. reply jrockway 17 hours agorootparentUsername: x@example.xyz Password: .com/!@#$% Concatenated: x@example.xyz.com/!@#$% Username: x@example.xyz.com Password: /!@#$% Concatenated: x@example.xyz.com/!@#$% reply 0x457 1 hour agorootparentBy why does it matter? It's a one-way hash isn't? Also, we're talking about user_id not user_email, so it should be the same length always. Well, unless you're silly and using databases sequence for IDs. reply Dylan16807 16 hours agorootparentprevA delimiter fixes the problem if you're sure the delimiter character can never be inside the username and password. Better would be to prefix the length of each field. Better still would be separately hashing each field and concatenating the results. reply jrockway 10 hours agorootparentPersonally, I like a fixed uint8 or uint16 representing the length of each segment. Then, there are no forbidden characters or quoting required. Maybe I want to have \\0 in my password. reply akira2501 14 hours agorootparentprevI tend to use '\\0' as a delimiter for this reason. reply Dylan16807 14 hours agorootparentYou still need to make sure nulls can't show up, and you need to consider possible truncation scenarios caused by those nulls and make sure they won't cause silent failures at any point. reply akira2501 13 hours agorootparent> You still need to make sure nulls can't show up Which is very easy to do without losing any desired functionality as opposed to delimiters in the ASCII character range. > and you need to consider possible truncation scenarios In particular hashing libraries worth using never have this problem. > and make sure they won't cause silent failures at any point. They literally only need to exist in the data to one function call. Afterwards they are not needed or significant. reply brianshaler 4 hours agorootparentRe the second quote and response: One pattern I bump up against from time to time is the delta between using a perfectly defensible technique for a given use-case (safe delimiters when constructing an input for a specific function) versus a desire to have each decision be driven by some universal law (e.g. \"if you're streaming data between services, using null bytes as delimiters might not be safe if consuming services may truncate at null bytes, so NEVER use null bytes as delimiters because they can be unsafe\") It's not even a matter of one \"side\" being right or wrong. You can simultaneously be right that this is perfectly safe in this use-case, while someone else can be right to be concerned (\"need to consider possible\") because the code will forever be one refactor or copy/paste away from this concatenated string being re-used somewhere else. reply Dylan16807 1 hour agorootparentprev> In particular hashing libraries worth using never have this problem. I'll note that the reason we're here in the first place is that they were using a password hash library with a completely unacceptable API. reply demarq 1 hour agoprevWasn’t there a project posted here that can spot this things automatically. It’s was a fuzzer of some sort reply _hyn3 1 hour agoprevWhy is anyone actually using Okta for anything these days? IMO, better to choose point solutions and combine them. reply jadengis 14 hours agoprevThis is obviously a huge mistake by Okta (for the love of God understand how your crypto functions work before you apply them) but at the same time, a crypto function with a maximum input length that also auto-truncates the data sounds like bad API design. You are basically asking for someone to goof up and make a mistake. It's much better to implement these things defensively so that the caller doesn't inadvertently make a mistake. Especially with a hashing algorithm, because there is no way to verify that the result is correct. reply sebastialonso 12 hours agoparentAgree with the spirit of the argument, but I disagree about the bad design. BCrypt has its trade-offs, you are expected to know how to use it when using it, specially if by choice. It's like complaining about how dangerous an axe is because it's super sharp. You don't complain, you just don't grab the blade section, you grab it by the handle. And reply thiht 7 hours agorootparentIf passing more than 72 bytes to a function makes it silently fail, it IS bad design, especially for a sensitive, security-related function. The first condition in the function should be `if len(input) > 72 then explicitly fail` Not letting people use your API incorrectly is API design 101. To be clear this is not the fault of the bcrypt algorithm, all algorithms have their limitations. This is the fault of bcrypt libs everywhere, when they implement bcrypt, they should add this check, and maybe offer an unsafe_ alternative without the check. reply echoangle 11 hours agorootparentprevIf your crypto library works like an Axe and the methods aren’t prefixed with “unsafe_”, the library is bad. I would expect an exception when a hashing function gets an argument that’s too long, not just dropping of the excess input. Who thinks that’s the best choice? reply mplewis 10 hours agoparentprevPassing something that isn’t a password + salt into bcrypt is the mistake here. reply djbusby 18 hours agoprevConcise write up; not surprising that cache played a part. Can't tell if it's issue with BCrypt or with the state-data going into the key, or combo-cache lookup tho. reply ptcrash 14 hours agoparentI think it's more of a logic problem. I suspect the engineers made a false assumption that bcrypt can hash a trivial amount of data like some other hashing algos. reply Forbo 18 hours agoprevI'm really sick of companies disclosing this shit late Friday afternoon. Go fuck yourselves. Sincerely, Everyone in the industry reply pluc 18 hours agoparentIt's the second time they do that in a few weeks too, _and_ it's not on their security page [1] which promises transparency. [1] https://trust.okta.com/ reply slama 16 hours agorootparentIt is listed on their security advisories page, which you can navigate to from that link: https://trust.okta.com/security-advisories/ reply cyberax 17 hours agoparentprevThank you for your feedback. Next time, we'll disclose it on Saturday evening. -- With Love, Okta. reply hoffs 28 minutes agoparentprevgeee, nobody is targeting you reply chanux 16 hours agoparentprevAlso, are there any repercussions for this kind of stuff? I don't know, fines from the organizations they get compliance certifications from or something. reply _hyn3 1 hour agorootparentNo repercussions, sadly. Those compliance companies are (mostly) all just checking a box. It's (mostly) security theater from people who wouldn't know security if it bit them in the nether regions. Even if that wasn't true, there's probably no box in any compliance regime that says \"Yes, we loudly promulgate our security failures from the nearest rooftop on 10am on a weekday\" (and it's always five o'clock somewhere, right?) If it helps (I know it doesn't), the Executive Branch likes to do this with poor job number revisions, too, lol reply Animats 16 hours agoprev [–] This is written in C, right? reply tedunangst 15 hours agoparentYour rewrite of bcrypt in not C is unlikely to support longer passwords. reply _hyn3 1 hour agorootparentNotC, the language that entire operating systems haven't been written in. reply lanstin 15 hours agoparentprevProbably Java. This is not a memory vulnerability but a protocol vulnerability. reply lelanthran 13 hours agoparentprev [–] > This is written in C, right? What's your point? That rewriting `bcrypt` in something else magically fixes this? AIUI, the issue is that `bcrypt` only uses the first 72 bytes of the input to create a hash. reply drbig 9 hours agorootparent [–] The issue is with the user mistaking bcrypt for a general-purpose digest hashing tool. It's like using a flat-head screwdriver as a hardwood chisel and then the handle breaks off after the third strike. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A vulnerability in Okta's AD/LDAP Delegated Authentication was discovered, allowing potential authentication with only a username if it was 52 characters or longer.",
      "The issue was present from July 23, 2024, and was resolved on October 30, 2024, by changing the algorithm from Bcrypt to PBKDF2.",
      "Okta advises customers to review their system logs for any related issues during the affected period."
    ],
    "commentSummary": [
      "Okta's security advisory reveals a vulnerability where usernames exceeding 52 characters can cause password truncation due to bcrypt's 72-byte limit.",
      "The issue stems from using bcrypt for cache key generation by concatenating userId, username, and password, which is not its intended use.",
      "The advisory recommends separating password storage from cache key generation and suggests using HMAC (Hash-based Message Authentication Code) or KDF (Key Derivation Function) for cache keys, while critiquing bcrypt's design for not managing input length errors effectively."
    ],
    "points": 129,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1730505389
  },
  {
    "id": 42021535,
    "title": "Low-cost, portable device can detect colorectal and prostate cancer in an hour",
    "originLink": "https://medicalxpress.com/news/2024-10-portable-device-colorectal-prostate-cancer.html",
    "originBody": "400 Bad Request Your request has been blocked by our server's security policies. If you believe this is an error, please contact our support team.",
    "commentLink": "https://news.ycombinator.com/item?id=42021535",
    "commentBody": "Low-cost, portable device can detect colorectal and prostate cancer in an hour (medicalxpress.com)122 points by PaulHoule 21 hours agohidepastfavorite36 comments smeej 17 hours agoEven the screening rates in the U.S. would skyrocket. Who doesn't know someone who's put off a colonoscopy? They're really not that bad once you get the hang of them (even the prep), but most people don't do them often enough to get good at them, so they dread them more than is really necessary. As someone who's had enough of them to have a favorite flavor of prep solution (lemon gavilyte-c, which tastes to me like a very mild lemon meringue), a couple tips: 1. Mix the solution with HOT water 12 hours before you need to drink it, and then refrigerate for the 12 hours. This way it doesn't taste salty. 2. For your \"clear liquid\" diet, drink bone broth with avocado oil in it, alternating with some coconut water. Get some protein and fat calories in you so you're not starving and not spiking your blood sugar all over the place. If I add 50% to my regular calorie intake, I'm not even really noticeably hungry. reply tzs 2 hours agoparentI've used the Sutab pills and they were great. They are like a large multivitamin. The number of them is a little annoying (12 taken over about 30 minutes the night before, and another 12 over 30 minutes the next morning). reply forinti 4 hours agoparentprevI had my first one recently and it wasn't really a big deal. I had heard from a few people how horrible the prep was and it really wasn't. It is definitely worth it if you are past 45. reply brandonmenc 15 hours agoparentprevWho even gets colonoscopies anymore? Everyone I know just poops in a box and mails it to a lab nowadays. reply anonzzzies 13 hours agorootparentThen you catch it quite late with current techniques. Hopefully in the near future it will be super accurate and something you can just do at home. reply tzs 3 hours agorootparentprevThose tests have a fairly high false positive rates. Hemorrhoids for example can cause a false positive, and are pretty common in people over 50 in the US. Once you get a positive, even a false positive, health providers tend to try to switch you from an annual poop in a box test to whatever the normal colonoscopy schedule is at your age (e.g., every 10 years unless your previous colonoscopy found something that calls for a shorter interval to the next one). reply bitwize 1 hour agorootparentprevThose aren't highly accurate. I think they just detect blood in the stool, which could come from one of several causes including simple haemorrhoids. If you pop positive on your ColoGuard you're expected to come in for a colonoscopy. reply conception 14 hours agorootparentprevPoop in a box detects pretty far along cancer - bleeding into the gut. Colonoscopy finds it a lot sooner. reply pedalpete 11 hours agorootparentExcept that not enough people are getting colonoscopies, and the cost and time is significantly more (for both the patient and the medical team). In Australia, at 50, everyone is given a poop stick which we send to a lab for results. I think one comes in the mail every 5 years if your previous result was negative. reply CoastalCoder 16 hours agoparentprevMiralax + a cold 2 liter bottle of (green) Mountain Dew, FTW! I find the carbonation masks most of the odd mouth-feel of the solution. And, it's the only occasion on which I let myself indulge in that stuff :) reply nachoab 21 hours agoprevTheranos definitely made us cautious but not every new tech needs to be Theranos 2.0. Microfluidics is legit and widely used, sometimes skepticism just means asking the right questions not shutting every door. reply bawolff 21 hours agoparentIn this case i imagine skepticism just means buying some devices and getting an independent lab to run a blind test. reply bitwize 1 hour agoparentprevI think Elizabeth Holmes was excited about the right stuff, but she wasn't well enough studied in the science to take it beyond the wouldn't-it-be-neat, sci-fi writing phase, and relied on \"fake it till you make it\" to compensate for the fact that her reach far exceeded her grasp. reply idontwantthis 3 hours agoparentprevHow would any company get around the fact that blood is not homogeneous and if you take only a small amount you aren’t getting a representative sample of the person’s blood so you have a random shot of detecting whatever markers you are looking for? reply password4321 19 hours agoprevAnecdata re: colon cancer, colonoscopy, etc. 2023 https://news.ycombinator.com/item?id=38055711 Are colonoscopies worth it? 2022 https://news.ycombinator.com/item?id=33147680 Effect of Colonoscopy Screening on Risks of Colorectal Cancer and Related Death reply melling 18 hours agoparent“Colorectal cancer is the second-most deadly cancer, killing over 1 million people per year around the world” That’s 1 million people every year who don’t need to die because colonoscopies are extremely effective at catching early cancer. reply theshackleford 4 hours agorootparentIn my case, twice. In my mid to late 20’s, and again a few years later when some really early stuff cropped up again. Get your butts probed. Don’t know what else I could say really. reply calmbonsai 16 hours agorootparentprevYou are, charitably, ignorant wrt colorectal cancer diagnostics, treatment, and quality of life. reply randerson 13 hours agorootparentI had precancerous polyps found and removed 7 years ago thanks to a colonoscopy before they caused any problems. I'd most likely be dead right now if not for that procedure. reply calmbonsai 13 hours agorootparentAwesome! Tissue characterization has come so far so fast and saved so many lives. reply melling 5 hours agorootparentprevStrange, considering I got a colonoscopy this summer and found out I had stage 3 colon cancer. Finishing chemo this week. What don’t you think I understand? reply andrekandre 15 hours agorootparentprev> You are, charitably, ignorant wrt colorectal cancer diagnostics, treatment, and quality of life any good info/references for those of us who haven't had any colonoscopies yet? reply estebarb 13 hours agorootparentJust do it, especially if someone in your family had related issues. My mom did it just because, without any symptoms, and they removed around 9 cancerous polyps (in three different colonoscopies, it ended up being a complex case). Accordimg to the doctor she would have been dead 3-6 months later if they didn't have discovered those unbroken polyps on time. reply calmbonsai 12 hours agorootparentYes! This. Also get a proper genomic sequencing. More and more, we're finding colorectal has very strong genetic correlations. It's not as strong as Huntington's or BRCA, but akin to carpel-tunnel with repetitive hand motions or melanoma with UV exposure, a diet that's \"fine\" for most, turns out to be a \"killer\" for a minority. reply amelius 19 hours agoprevWhat are the false positive / false negative rates? reply nograpes 19 hours agoparentIn the full text, they did not appear to study the false positive and false negative rates. Their references to \"sensitivity\" were referring to the fact that they could detect even very small concentrations of the biomarker. This is reasonable for this kind of study. I would expect a false negative / false positive rates in a study on the commercial device. reply freitasm 21 hours agoprev [11 more] [flagged] cyberax 21 hours agoparent??? Microfluidic chemistry is a mainstream thing. It's used for all kinds of processes, even in large-scale drug manufacturing to replace batch processes. reply freitasm 21 hours agorootparentI did not imply this. I believe this needs to be tested by other parties, results checked, etc. Like science. Because claims like this (\"diagnose,\" \"small drops,\" \"quick\") were made before, people ended up in jail for the tactics used trying to protect the lies when they couldn't make it work. reply bawolff 21 hours agorootparentSure same as any medical device. But siezing on the word \"microfluidics\" because theranos used it, makes about as much sense as seizing on the word \"chemistry\" and saying theranos also claimed to use chemistry. reply cyberax 21 hours agorootparentprevSure, but they're not claiming to analyze everything. Cancer marker detection is an active field, and it often involves checking for the levels of several proteins/RNA fragments. What they did, they found a material that allows them to microfluidize the processes. reply Etheryte 20 hours agoparentprevMicrofluidics have been widely used since the 80s, this is like saying a company is a scam just because their sales pitch includes the phrase machine learning. reply readyplayernull 20 hours agorootparentActually the whole history of AI have experienced several hype cycles, with huge and expensive failed projects: https://en.wikipedia.org/wiki/AI_winter reply Etheryte 19 hours agorootparentThis is not really an actually, it's widely and well known. It's also specifically why I said machine learning, not artificial intelligence, the two are not the same thing. reply d4mi3n 21 hours agoparentprev [–] Microfluidics were also key to creating the first line of COVID vaccines. They were also why it was hard to scale up production—turns out machines than can perform microfluidic chemistry aren’t mass produced and take time to make. reply floxy 19 hours agorootparent [–] >turns out machines than can perform microfluidic chemistry aren’t mass produced and take time to make. Can you provide a link to more information about this? I was under the (probably mistaken) assumption that microfluidics were using semiconductor-based lithography processes, and we've figured out how to scale the heck out of that. reply d4mi3n 12 hours agorootparent [–] I can’t find the original article, but Derek Lowe (famously of the Things I Won’t Work With column about exciting chemistry) published a series of articles on the topic around 2021 - 2022. I did find an interview with Derek Lowe where they talk a bit on the topic, it I haven’t listened through all of it to see if they discuss microfluidics: https://www.earwolf.com/episode/where-oh-where-is-the-covid-... EDIT: found it! Here’s the write up where Derek Lowe discusses microfluidics devices that ended up being the bottleneck to producing the original COVID vaccines: https://www.science.org/content/blog-post/myths-vaccine-manu... From the article (there’s more than this): Ah, but now we get back to Step Four. As Neubert says, \"Welcome to the bottleneck!\" Turning a mixture of mRNA and a set of lipids into a well-defined mix of solid nanoparticles with consistent mRNA encapsulation, well, that's the hard part. Moderna appears to be doing this step in-house, although details are scarce, and Pfizer/BioNTech seems to be doing this in Kalamazoo, MI and probably in Europe as well. Everyone is almost certainly having to use some sort of specially-built microfluidics device to get this to happen - I would be extremely surprised to find that it would be feasible without such technology. Microfluidics (a hot area of research for some years now) involves liquid flow through very small channels, allowing for precise mixing and timing on a very small scale. Liquids behave quite differently on that scale than they do when you pour them out of drums or pump them into reactors (which is what we're used to in more traditional drug manufacturing). That's the whole idea. My own guess as to what such a Vaccine Machine involves is a large number of very small reaction chambers, running in parallel, that have equally small and very precisely controlled flows of the mRNA and the various lipid components heading into them. You will have to control the flow rates, the concentrations, the temperature, and who knows what else, and you can be sure that the channel sizes and the size and shape of the mixing chambers are critical as well. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A new affordable and portable device can detect colorectal and prostate cancer within an hour, potentially improving screening rates by offering a more comfortable alternative to colonoscopies.",
      "The device utilizes microfluidics technology to enhance the detection of cancer markers, addressing the limitations of at-home tests that often yield high false positives and detect cancer at later stages.",
      "Despite its promising features, the device requires independent testing to confirm its effectiveness and reliability in medical diagnostics."
    ],
    "points": 122,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1730494966
  },
  {
    "id": 42024342,
    "title": "Cramming Solitaire onto a Nintendo E-Reader card",
    "originLink": "https://mattgreer.dev/blog/cramming-solitaire-onto-a-nintendo-ereader-card/",
    "originBody": "Making a new game from scratch for a forgotten Nintendo peripheral I recently finished making Solitaire for the Nintendo E-Reader. I managed to fit it onto a single card, and it's a pretty full featured version of the game. I'm really happy with how it turned out. I figured I'd talk a bit about how I made it in what turned out to be a long blog post :) What is the E-Reader? The E-Reader is a Game Boy Advance peripheral that Nintendo released in 2002. By scanning cards that have a dot code strip on them, you can load mini games, extra levels, animations and more. The E-Reader and one of its cards I've always really liked the E-Reader and was sad it didn't do too well in America. So I thought maybe I'd take a stab at making games for it myself. Me too, Marge And here is the result Solitaire in its card from If you would like to try it, you can get a card at retrodotcards.com. Tools and docs, starting way back in the past... Where to begin? I remembered there were some old tools and websites about making E-Reader cards from back around when it first came out — twenty years ago! I managed to find Tim Schuerewegen's original site in the Wayback Machine. It had some examples, source code and tools. I also refound CaitSith2's E-Reader site, which thankfully is still up. It also has some tools and information. These tools are the backbone of E-Reader dev. Thanks to Tim and CaitSith2 for making them! They were originally made for Windows, but they were also made multi-platform here. These initial findings were a great start and got me headed down learning how E-Reader applications are programmed. GBATEK also has a section on the E-Reader which also contains lots of useful information. More recently I found AkBKukU's e-reader-dev repo which has also been very helpful. Pick your poison: GBA, NES or ... z80? What kind of E-Reader card should I make? E-Reader cards can come in four broad formats Game Boy Advance applications These are GBA programs written much like if you were making a normal GBA game. The E-Reader simply loads them in then lets them execute on their own for the most part. NES games The E-Reader contains a simple NES emulator, so it is possible to directly put simple NES games onto E-Reader cards. The keyword here is \"simple\", it does not support more advanced NES features. Also the E-Reader has a limit on how many card swipes one application can have. The NES games Nintendo released require 10 card swipes to load! So in the end it is only possible to run early/small NES games. Nintendo used this to release games like Excitebike and Donkey Kong for the E-Reader Excitebike in E-Reader card format Raw binaries Raw E-Reader cards just contain binary data of some kind. Specific games made use of these to add levels, characters, etc. Kind of like a primitive form of DLC. It is up to the specific game to interpret the data as it sees fit. Super Mario Advance 4 released cards like this, adding additional levels, power ups and more for the game Super Mario Advance 4 E-Reader level cards z80 Applications And finally the E-Reader also contains a simple z80 emulator. The z80 is an 8-bit processor that first came out in 1976! It was very successful and found its way into many different computers. I don't believe Nintendo ever used a z80 processor in any of their game consoles. So this choice is an interesting one. I'm sure the z80's simplicity was a big factor here, it's pretty easy to emulate. I have since been informed that the Game Boy and Game Boy Color have CPUs that are similar to the z80. So that might have influenced Nintendo's decision here. I didn't know that, so thanks to those who informed me. Manhole: a simple z80 E-Reader game That means E-Reader apps can be written in z80 assembly. The primary advantage here is z80 apps tend to be quite small. In my experimenting, I found an E-Reader z80 app to be about 30-50% smaller than an equivalent E-Reader GBA app. Nintendo almost entirely went this route with their own cards, I'm guessing to keep the number of swipes needed for an application to a minimum. z80 E-Reader apps I made Solitaire as a z80 application and have become pretty entrenched in this approach. I really like how much smaller the resulting binaries are. But make no doubt about it, z80 assembly is pretty rough. Especially considering you can write a GBA E-Reader card in C. The ERAPI API For z80 games, Nintendo embedded a simple but effective API into the E-Reader that they can take advantage of. Things like creating sprites, playing music, even multiplying and dividing, can all be done through this API. This helps keep card sizes small, as common functionality doesn't need to be packed into the cards, the E-Reader itself will provide it. GBA E-Reader games also can access ERAPI. It's a bit different here and there, but overall it's the same API. As a simple example, here is how to create a sprite using the API ; ERAPI_SpriteCreate() ; e = pal# ; hl = sprite data ld e, #2 ld hl, #my_sprite_data_struct rst 0 .db ERAPI_SpriteCreate ld (my_sprite_handle), hl If you're not familiar with z80 assembly this probably looks bizarre. It is basically the equivalent of int palette_index = 2; int my_sprite_handle = SpriteCreate( palette_index, my_sprite_data_struct ); The ld calls are \"load\", and here we are loading the e register with which palette index we want the sprite to use. The hl register is loaded with a pointer to the information about the sprite (its tiles, colors, frames of animation, etc). The rst 0 and .db ERAPI_SpriteCreate lines are where we actually make the API call. Without getting too deep on how the z80 works, this is a simple function call. When it is done, it will leave the handle to the sprite in the hl register, so we ld (my_sprite_handle), hl to copy that value off into memory for safe keeping. That handle is later used whenever we want to interact with the sprite, such as changing its position. A crippled z80 The E-Reader's z80 emulator is not 100% accurate. Nintendo decided to not support some opcodes and some of the registers. I have also found that some opcodes don't seem to work correctly. Hopefully I'm just using them wrong, but some opcodes just cause the GBA to show a black screen and lock up. The z80 is already a very limited processor, and this makes it even more so. Sometimes E-Reader z80 development is absolutely painful. But hey, the challenge is part of the fun (right?) I often felt like this when trying to implement something Simple things that I usually take for granted like copying one array to another is just so much harder to do in E-Reader z80 assembly. Thankfully I'm starting to get the hang of it. Debugging Another huge challenge was debugging the game. There's no way to log anything, running the game on a Game Boy Advance is a total black box. GBA emulators like mGBA have good debugging features. But this is a z80 emulator running on the GBA's ARM processor. I figured stepping through ARM instructions trying to figure out how z80 instructions worked would be a herculean task, so much so I never even tried. Thankfully I don't think I'll ever need to, more on that below. For my first attempt at creating a debugger, I took z80js, a z80 emulator core created by Molly Howell, and built a small application that would run my binary and log out what the cpu was doing. The output looked like this ... 0B52: call _deck_gfx_render_columna: 17, b: 00, c: 03, d: 08, e: 5c, h: 00, l: 17, bc: 0003, de: 085c, hl: 0017 0B5B: ld b,#0x13a: 17, b: 00, c: 03, d: 08, e: 5c, h: 00, l: 17, bc: 0003, de: 085c, hl: 0017 0B5D: ld c,#0x00a: 17, b: 13, c: 03, d: 08, e: 5c, h: 00, l: 17, bc: 1303, de: 085c, hl: 0017 0B5F: ld hl,(_deck_gfx_cur_column_addr)a: 17, b: 13, c: 00, d: 08, e: 5c, h: 00, l: 17, bc: 1300, de: 085c, hl: 0017 0B62: ld d,#0x00a: 17, b: 13, c: 00, d: 08, e: 5c, h: 08, l: 5c, bc: 1300, de: 085c, hl: 085c 0B64: ld e,ca: 17, b: 13, c: 00, d: 00, e: 5c, h: 08, l: 5c, bc: 1300, de: 005c, hl: 085c 0B65: add hl,dea: 17, b: 13, c: 00, d: 00, e: 00, h: 08, l: 5c, bc: 1300, de: 0000, hl: 085c ... That looks like pure gibberish here in the blog because the lines are too long to fit. Each line contains the opcode the cpu executed, and the state of all the registers at that time. Here is a single line, cleaned up a bit 0B5B: ld b,#0x13a:17, b:00, c:03, d:08, e:5c, h:00, l:17, bc:0003, de:085c,hl: 0017 This ... worked ... I mean it got the job done and I was able to fix bugs by examining this output. But it wasn't very fun. A huge downside to this approach is it wasn't interactive. It just blindly ran the game without allowing any button presses or anything like that. Because of this, I often had to get very creative to get this emulator to run the part of the game I was having troubles with. A proper debugger I used this tracing approach to write most of the game. But towards the end there were two mysterious bugs I just could not figure out. I knew I needed a better solution. I stumbled across the DeZog project, which is a general purpose z80 debugging extension for VS Code. This looked really promising, but then I found ZX81-Debugger. Sebastien Andrivet took DeZog as a basis and made a VS Code extension specifically for writing and debugging ZX81 applications. I really liked ZX81-Debugger right away, what a great tool! Just install it and boom you've got a full fledged ZX81 development environment. I forked its code and started adapting it to work with E-Reader apps. Since both platforms have the z80 processor in common, this turned out to not be as difficult as I thought it would be. After a long weekend of hacking, I surprisingly had an E-Reader debugger running in VS Code! In the end, I was positively floored how quickly I got this working. I am truly standing on the shoulders of giants ... so thank you to everyone who made all of this possible. The E-Reader debugger running in VS Code Here is that image in full size. To get this working I removed most of the ZX81 specific things and then wrote a simple ERAPI emulator. As ERAPI calls come in, the debugger sends them over to my little emulator, which then translates them into a visual GBA screen. A close up of the ERAPI emulator screen output The background is green because I've not added most of the background related API functions to the emulator yet. And below the screen I am dumping out the current state of all the sprites that were created through ERAPI. You can even take a commercial E-Reader game and run it in the debugger. It will disassemble the binary and provide a nice debugging experience. This will be helpful to further figure out more about how E-Reader cards and ERAPI works. An official Nintendo E-Reader card, running in the debugger The colors in that Kirby card are all weird because my ERAPI emulator is super raw and does many things incorrectly. It's drawing the image using the wrong palettes. A lot more work needs to be done. This is just amazing! I never imagined I'd get a developer experience this powerful on a forgotten, 20 year old, Nintendo peripheral. We really live in exciting times sometimes. Ultimately I will open source the E-Reader-Debugger. But as it stands, it's not even alpha quality. I mean it is rough. After I've worked out more kinks, I will throw it up on GitHub. Challenges with the ERAPI API Overall the E-Reader's ERAPI is very useful, and has lots of great stuff to make development easier. But I did find some of the stuff didn't work out. Either because this stuff is buggy, or maybe I just don't yet understand how to use it properly. Hopefully the latter. I really struggled with this when rendering the playfield for Solitaire. Being an old game system, the GBA has limitations when drawing sprites to the screen. You can't put too many on the screen at once or else things like this can happen This video shows Solitaire when I first started working on it. I wanted to see if I could use sprites to draw all the cards. I concluded I couldn't and instead would need to draw the cards into a background. Using backgrounds for graphics like this is a very common tactic on older game systems. And luckily, ERAPI has the function SpriteDrawOnBackground, it seems to be exactly meant for this use case. Using this function, I was able to easily draw my sprites into the background and avoid all graphical glitching ... the first time the playfield was drawn. As it was drawn repeatedly, it seemed like the tiles in video RAM were getting corrupted In this video I was reshuffling and redealing the deck repeatedly. And each time I dealt it out again, graphical glitches would appear. I tried and tried, but I just could not get this to work. I'm not completely sure I wasn't doing something wrong. But ERAPI and the way z80 apps work with the emulator seem pretty straightforward? So I think this function was not meant for rapid use like this. ERAPI has another function, LoadCustomBackground, and it was this one that was a winner for my game. It's a lower level and harder to use function than SpriteDrawOnBackground, but it has never caused graphical glitches on me even once. With this function, it is up to me to figure out which tiles go where to form a background. You need to understand how backgrounds work on the GBA to pull this off. Then once I've figured it all out, I just send it to the GBA in one function call and it appears on the screen. z80 E-Reader apps are kinda script-like I have found the way E-Reader z80 apps work to be pretty interesting. The emulator uses the halt opcode to mean \"draw a frame to the screen\". You can load the a register with how many frames it should draw, accomplishing a very simple way to add waits to your game. Take this little intro animation. I built it like this logoHandle = createSprite(logo); setSpritePosition(logoHandle, 120, 20); footerHandle = createSprite(footer); setSpritePosition(footerHandle, 120, 60) playSystemSound(DRUM_ROLL); for (let i = 0; i < NUM_CARDS_TO_DEAL) { dealOneCard(i); // render one frame to show the newly dealt card halt(1); } playSystemSound(CYMBOL); // wait for 30 frames halt(30) // tear down the logos freeSprite(logoHandle); freeSprite(footerHandle); // let the regular gameplay loop take over from here In the actual game this was done in assembly, I turned it into pseudo-code here to make it easier to read. What I find interesting is I just whipped up a typical game engine loop on the spot, just to deal out the cards. I didn't need to hook all this into a main game loop like you often do with most game development. The entire rest of the game doesn't know or even care that this happens. E-Reader assets You might have noticed my game has music, sound effects, and a Mars-like rocky background. The E-Reader itself has many assets a game can use. This helps keep the dotcode data small. You can do custom graphics (such as the deck of cards in my game) and sounds, but they tend to be quite large and take up precious space. Some of the backgrounds found on the E-Reader Altogether there are over 100 backgrounds, over 800 sounds (both sound effects and music) and over 200 Pokémon sprites stored in the E-Reader's 8MB ROM. If you're an E-Reader fan, you might have noticed Nintendo's mini games tend to reuse the same sound effects, music and often the same backgrounds. This is why. Using them is usually just a simple API call. For example here is how I play the drum roll sound effect ld hl, 755 rst 8 .db ERAPI_PlaySystemSound Crazy assembly syntax aside, this is pretty much just PlaySystemSound(755), where 755 is the drum roll's id. How big can E-Reader apps be? A single E-Reader dotstrip can store 2,192 bytes of data, which is just over 2kb. But there is some overhead with the headers, and error correction, making the actual amount of data I've been able to store on a single strip a bit less than 2kb. This data on the dotstrip is compressed though, which helps a lot. An example dotcode strip A closer view at part of the dotstrip The compression makes the total storage size kind of fuzzy. It really depends on how well your data compresses. For example, here are the tiles for my cards The card graphic tiles for Solitaire I could have saved space by not repeating the same tile over and over again. But this data compressed very well. So well, I just went with it. Deduping these tiles would have made the drawing routines much more complicated, maybe so much so it would have killed any space savings I had achieved. In the end I didn't need to do too much space optimization to keep Solitaire down to two dotstrips (which can be printed onto a single card). I did use up all the space that those two strips alloted me, so any more features would have likely required the game be expanded to three dotstrips, something I really wanted to avoid. Honestly I'm a bit surprised how many strips are needed for some of Nintendo's cards. Based on the game contained inside and my experience writing Solitaire, it seems like some of their games could have been done in fewer dotstrips. But that's just a total guess, I don't really know. The E-Reader and GBA limitations on space The E-Reader itself allows a maximum of 12 strips to be scanned for a single application. Although I've yet to find any application that requires that many. The NES games need 10 The E-Reader waiting for 9 more strips to load Excitebike As for the Game Boy Advance itself, it has 256kb of RAM which is more than enough to store any E-Reader application. The E-Reader will decompress the data into RAM, then execute it. I suppose technically your data could exceed 256kb when decompressed, but realistically I just can't see that happening. More E-Reader cards to come My goal is to make many more E-Reader apps. I'm already deep into development of the next game. I am hoping to build at least 10 apps and have the cards professionally manufactured. Hopefully packaged up in a booster pack, which would be so cool. Why? Why not. I think the E-Reader is really cool and this is a lot of fun. If new E-Reader cards interests you, check out https://retrodotcards.com If you want to try Solitaire out for yourself, you can order a card there. They are free. I will also be posting to Bluesky as I progress, and have also started a subreddit for this, stop on by!",
    "commentLink": "https://news.ycombinator.com/item?id=42024342",
    "commentBody": "Cramming Solitaire onto a Nintendo E-Reader card (mattgreer.dev)118 points by thunderbong 8 hours agohidepastfavorite2 comments juliangoldsmith 6 hours ago [–] Dupe: https://news.ycombinator.com/item?id=42010136 reply dang 11 minutes agoparent [–] Comments moved thither. Thanks! reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A developer created a full-featured Solitaire game for the Nintendo E-Reader, a Game Boy Advance accessory from 2002 that uses dot code cards to load games.",
      "The development involved using the z80 format for its compact size and overcoming challenges with the E-Reader's limited z80 emulator and debugging difficulties.",
      "The developer utilized the E-Reader's ERAPI API and created a debugger in Visual Studio Code to aid in development, successfully fitting the Solitaire game on a single card."
    ],
    "commentSummary": [],
    "points": 118,
    "commentCount": 2,
    "retryCount": 0,
    "time": 1730527497
  },
  {
    "id": 42024661,
    "title": "SmolLM2",
    "originLink": "https://simonwillison.net/2024/Nov/2/smollm2/",
    "originBody": "Simon Willison’s Weblog Subscribe SmolLM2 (via) New from Loubna Ben Allal and her research team at Hugging Face: SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. [...] It was trained on 11 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new mathematics and coding datasets that we curated and will release soon. The model weights are released under an Apache 2 license. I've been trying these out using my llm-gguf plugin for LLM and my first impressions are really positive. Here's a recipe to run a 1.7GB Q8 quantized model from lmstudio-community: llm install llm-gguf llm gguf download-model https://huggingface.co/lmstudio-community/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q8_0.gguf -a smol17 llm chat -m smol17 Or at the other end of the scale, here's how to run the 138MB Q8 quantized 135M model: llm gguf download-model https://huggingface.co/lmstudio-community/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q8_0.gguf' -a smol135m llm chat -m smol135m The blog entry to accompany SmolLM2 should be coming soon, but in the meantime here's the entry from July introducing the first version: SmolLM - blazingly fast and remarkably powerful . Posted 2nd November 2024 at 5:27 am Recent articles W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October - 30th October 2024 You can now run prompts against images, audio and video in your terminal using LLM - 29th October 2024 Run a prompt to generate and execute jq programs using llm-jq - 27th October 2024 open-source 226 ai 878 generative-ai 741 edge-llms 56 llms 737 hugging-face 8 llm 92 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42024661",
    "commentBody": "SmolLM2 (simonwillison.net)108 points by edward 11 hours agohidepastfavorite41 comments echoangle 9 hours agoSemi-related but is there a standard way to run this (or other models from huggingface) in a docker container and interact with them through a web API? ChatGPT tells me to write my own FastAPI wrapper which should work, but is there no pre-made solution for this? reply turblety 9 hours agoparentOllama has built in support [1] for gguf models on huggingface, and exposes a openai compatible http endpoint [2]. You can also just test it out using the cli: ollama run hf.co/unsloth/SmolLM2-1.7B-Instruct-GGUF:F16 1. https://huggingface.co/docs/hub/ollama 2. https://github.com/ollama/ollama?tab=readme-ov-file#start-ol... reply echoangle 8 hours agorootparentThanks, Ollama ist exactly what I was looking for. reply pizza 8 hours agoparentprevIn shell 1: $ docker run --runtime nvidia --gpus all \\ -v ~/.cache/huggingface:/root/.cache/huggingface \\ --env \"HUGGING_FACE_HUB_TOKEN=\" \\ -p 8000:8000 \\ --ipc=host \\ vllm/vllm-openai:latest \\ --model mistralai/Mistral-7B-v0.1 In shell 2: $ curl http://localhost:8000/v1/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"mistralai/Mistral-7B-v0.1\", \"prompt\": \"San Francisco is a\", \"max_tokens\": 7, \"temperature\": 0 }' reply Tostino 5 hours agorootparentLove vLLM for how fast it is while also being easy to host. reply ttyprintk 8 hours agoparentprevHuggingface TGI supports many models and more than one API: https://huggingface.co/docs/text-generation-inference/en/ins... reply exe34 8 hours agoparentprevllama.cpp in a docker container (Google for the gguf version) reply kgeist 9 hours agoprevDoes it support anything other than English? Sadly, most open-weights models have no support for languages other than English, which makes them useless for 75% world's population who don't speak English at all. Does anyone know of a good lightweight open-weights LLM which supports at least a few major languages (let's say, the official UN languages at least)? reply nmstoker 23 minutes agoparentGood point raising just how many don't speak any English. That sounds like a lot of people who could do something to produce/ contribute to a non-English language language model. reply Gathering6678 7 hours agoparentprevI occasionally use the 1.5B and 3B version of Qwen2.5 for translation between English, Chinese and Japanese, of which they seem to do a good job. reply EugeneOZ 9 hours agoparentprev> SmolLM2 models primarily understand and generate content in English. https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct/b... reply jdthedisciple 9 hours agoprevVery interesting. According to their X posts, this meme model \"SmolLm\" beats Meta's new 1B and 3B models across almost all metrics. I wonder how this is possible given that Meta has been in this game for much longer and probably has much more data at their disposal as well. reply stavros 9 hours agoparentUsually, that's because they use a groundbreaking ML method called TTDS, or \"training on the test dataset\". reply jerpint 7 hours agorootparentDo you have proof for this? Why accuse one team and not the other? reply stavros 7 hours agorootparentWhich team did I accuse? I said \"usually\". reply exe34 8 hours agorootparentprevmeta or smol? reply stavros 8 hours agorootparentSmol reply jgalt212 8 hours agorootparentprevI would hope Simon would not fall victim to such shenanigans, and has his own test dataset. reply simonw 5 hours agorootparentMy test dataset is mostly dumb prompts about pelicans. You'll note that I didn't quote their benchmarks in my own post at all, because I didn't want to boost them without feeling confident in what they were stating. I posted about this because my own very limited initial experiments passed a loose vibe check! I'm impressed any time a 1.7GB (or 130MB) model file appears to be able to do anything useful at all. reply stavros 7 hours agorootparentprevI'm not saying either did this, just that that's what most fine tunes tend to do. reply wokwokwok 8 hours agoparentprevYou can be reasonably confident that unless there’s been a significant breakthrough (there hasn’t) if a smaller model beats a larger model it’s either fine tuned for a specific purpose or trained on the test data somehow (ie. fine tuned to have good metrics). To be less snarky, they claim: > These models are built on a meticulously curated high-quality training corpus Ie. Good training data plus a small model beats a bigger model. …but I’m skeptical, when I read: > We observed that performance continues to improve with longer training, even beyond the Chinchilla optimal point. Therefore, we decided to train the 1.7B model on 1 trillion tokens and the 135M and 360M models on 600B tokens, as the performance gains after 400B tokens begin to slow on some benchmarks for these smaller models. So they’re evaluating their models against various benchmarks as they train them and picking the practice that gives the best benchmarks? I dunno. The claim is basically good data > more parameters, but it’s just an observation of “this happened to work for us” rather than something you can usefully take (as far as I can see) and apply to larger models. The claims they actaully make about performance are far more modest than people are making out. The 1.7B model performs better than any other 2B models in their evaluation. Seems nice. Not ground breaking. Not convinced it’s real rather than polluted training data personally. reply thinker567 3 hours agoparentprevThey didn’t say they beat the 3B, their 1.7B outperforms llama3.2 1B thanks to 11T tokens of high quality data (Hugging Face are the ones behind FineWeb dataset that everyone uses now). Btw Qwen2.5-1B also surpasses llama3.2 1B by a large margin so beating it is even more impressive reply moffkalast 8 hours agoparentprevMeta doesn't train on their internal data, at least not for open models. It would be a real PR problem if someone started dumping real Facebook chats out of them. And this is from Huggingface themselves, arguably they have a lot of data as well. reply cpa 9 hours agoprevWhat’s the context size? I couldn’t find it on the model summary page. Tangential: if it’s not on the model page, does it mean that it’s not that relevant here? If so, why? reply cloudbonsai 9 hours agoparent> What’s the context size? SmolLM2 uses up to 8192 tokens. reply oulipo 9 hours agoprevNice! Do you think they could be fine-tuned to implement a cool thing like https://withaqua.com/ ? eg to teach it to do \"inline edits\" of what you say? reply ksri 10 hours agoprevIs there a way to run this in the browser as yet? Transformers js doesn't seem to support this. Is there another way to run this in the browser? reply d_k_f 9 hours agoparentThey linked two examples in another blog post, only the smaller models, though: [135M] https://huggingface.co/spaces/HuggingFaceTB/SmolLM-135M-Inst... [360M] https://huggingface.co/spaces/HuggingFaceTB/SmolLM-360M-Inst... reply Nowado 9 hours agoparentprevProbably ONNX. reply adhamsalama 9 hours agoparentprevMaybe WebAssembly? reply Its_Padar 9 hours agoprevI wonder how one would finetune this reply diimdeep 7 hours agoprevHm, is it too early yet to stop trusting these self published evaluations except 3rd party independent ones ? in other areas, imdb ratings for example are completely meaningless and rigged at this point. reply forrestthewoods 9 hours agoprevIs there a good, small model that can take input images? Or are those all still larger? reply JimDabell 8 hours agoparentI haven’t tried the smaller variants, but I’ve been very impressed with Molmo: https://molmo.allenai.org reply ekianjo 8 hours agoparentprevmoondream fits the bill, but dont expect too much for the performance on image description and all. reply globular-toast 8 hours agoprev [–] Why would I care about this when I can have the entire English Wikipedia on my phone? Really struggling to understand why people are so excited about this stuff. reply terramoto 8 hours agoparentKnowledge doesnt amount to much on the LLM, i think what most are excited about is the artificial reasoning. reply globular-toast 7 hours agorootparentWhat is left for the brain to do? First people let their bodies atrophy. Next it's the mind. Wall-E here we come. reply skeledrew 7 hours agorootparentThere will always be lots to do for those who are motivated to find such things. While the objective value of a thing may change greatly, the subjective value can be kept fairly constant. Just think about all the retro-x enthusiasts. reply nurettin 53 minutes agorootparentprevLLMs have the potential to eliminate a lot of rudimentary tasks. The brain should find better things to do than locating the closest number in two lists. It isn't all doom and gloom. reply tetris11 8 hours agoparentprev [–] \"Hey Wikipedia, at what year did SSD speeds reach RAM speeds of yester-year?\" reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SmolLM2 is a new family of compact language models from Hugging Face, available in sizes of 135M, 360M, and 1.7B parameters, designed to run efficiently on devices.",
      "These models were trained on 11 trillion tokens from diverse datasets, and their weights are released under the Apache 2 license, allowing for broad usage and modification.",
      "Simon Willison shares positive initial impressions and provides instructions for using the models with the llm-gguf plugin, with a detailed blog entry on SmolLM2 expected soon."
    ],
    "commentSummary": [
      "There are multiple ways to run Hugging Face models in a Docker container with a web API, including using FastAPI wrappers or Ollama's built-in support for gguf models with an OpenAI-compatible HTTP endpoint.",
      "Hugging Face TGI supports a wide range of models and APIs, while SmolLM2, which primarily supports English, reportedly outperforms Meta's models due to high-quality training data and can handle up to 8192 tokens.",
      "There is growing interest in small models for browser use and image input, with discussions around the reliability of self-published evaluations and the appeal of LLMs for their artificial reasoning capabilities."
    ],
    "points": 108,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1730532660
  },
  {
    "id": 42027564,
    "title": "Britain's postwar sugar craze confirms harms of sweet diets in early life",
    "originLink": "https://www.science.org/content/article/britain-s-postwar-sugar-craze-confirms-harms-sweet-diets-early-life",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"www.science.org\",cType: 'managed',cRay: '8dc6607ae9113956',cH: 'JpW7FExTjG_t0Cmj.zbOH_D7ozkAFMlFYuJ5b73lW2o-1730574125-1.2.1.1-SSln1pkm6ZwF40WvoVl1WKB0_aYp3ymfgya4EKCvBiA92IKECn8Jk8hU7255kY.k',cUPMDTk: \"\\/content\\/article\\/britain-s-postwar-sugar-craze-confirms-harms-sweet-diets-early-life?__cf_chl_tk=8RXVb7giYQ5DKMLgQK6zN_T6iKGigVGUsBakxwvXtdk-1730574125-1.0.1.1-GdSCSdozlJG1peu0wdPByMdh2LkBVG71c1yb0FOtTac\",cFPWv: 'b',cITimeS: '1730574125',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/content\\/article\\/britain-s-postwar-sugar-craze-confirms-harms-sweet-diets-early-life?__cf_chl_f_tk=8RXVb7giYQ5DKMLgQK6zN_T6iKGigVGUsBakxwvXtdk-1730574125-1.0.1.1-GdSCSdozlJG1peu0wdPByMdh2LkBVG71c1yb0FOtTac\",md: \"ub8NZ.OTrV7DRfHXbhDeIeV1ZNQKFcrODdX_s2KoS2I-1730574125-1.2.1.1-ge1QsYMjKJJVhVADKyuoaQkm448NLXfuiF7igQUeOQdhhbXfeETeZIb5AaTsKG6OOoo.o6SYxWo0cpLajDamGQnsWqwMtFUt.4So_GwWkD.HVLbECZyFz8q27UOxARso82Tn6TC13wZsnLb7lGHXnVdQ_1qNkPWWUim6KYhWxenUt97e1aL1cobxH9hX.uHjca4lHgqM149mOstZP84xrXw3mYaX7Igo2rgRvogehWfkoGm6FxBZOA1jNMZ90qp7WQqcLnVOrocjiti9v8Fq0_SbjtWZmnxbhY0.BFVj5k7_dXhszjSPWNG7EO.556hfuUU1q2QWIex8nFVpx7BXUO8BViHxTEaJABSX_CKP9FoE1z50GoaF5m9ZFBXqDnnjJ_1BWxDhEl8Bat23PnEJRuhBnfQ4vFsgLtGjDAuG57jUfOPxOZeHtjNdS18Esfp0NRjjTaalW8VnhzXogBoGTLubDBbFuSaQHCERQ370l2mirwoNOK1mQrYtUHxP_7uTELq7zDltOSq4uQZWwvHhzih4xKqTq4jJNWhM0ZzhdKjfPv6VaN6comn2f5rHMu3erzMJnV92yr.bx.mbiZ30XFXQPcHrCIg.JJzO7xsaLjaM8pyoU97YqosPvZ4Rcche_uC4Vo37j7WkJyeFDd5KkQYAnF40aXnonnMtCinP02tVyCku0yqJy0x0VoqljI8ydT7zXKV0jS0ePvep5K3nFrU2FehuhY6OnGLTNVN2IzfvXZTP.WK_wJkhrgj7oRCGpLf6p_lg19y6l27D5ToYe6MDqBM3W26b_o5JxV7nubYz9nBjF49890H1b_zU6T8rSg434rdSRf5fR_bwk1jgj35UdqsXSJID4tv8RNieu53hO8iKJRUUfKCDplDvRZJ1HCYnMw7cQKYYD6H_Pf3wsEaVRyVIdU43iZIUhkw5iqNfGIyJG6rHfdOI1Ks0aacJerBl1roRmC223r60Q.GLIXbIlF5obHYdaUTwUFnAP_ymJeLSIQOS4yyWWFSTflc_Emk3Fgg3dBSRafnQckeQbxsMPY1S7gqc4FRTA5GfIiFGTTpbv1b4Ab6kiz2W0Ag1EKkl8AYQmh67runajzU3aAVv.UIsNhLtSXIoVyy74IY24LfupIlwxE48oVJMjsqowLMz12bGrVpc2MUiyOonGFfCs3saExDIxUkAniWDHV9IGdlt74Q58AYXqFGpo0wp2sEWnCmIa8xchkmFOWt.JSf5DPd66DbHtiCBrtBOceRPpswK0Oq0HnDnOP8cjFEXCfkdh94.ATxVYNNv9n9cCBWzYDWFx0VmsryDU3BOc_2S405ee5iQBVWlWLr1pbZnD_Gus8CbwcryMepdyK1knGKVO4uYZjmh.fHP3ygnku7dTqq0Q90kTt5GXGADLX76RXHZnFX.LJINMxMl0O19b9_CFh4HH5eoAFm1RuiniKk1neNDB_MJ91WE3JuDv.gIMxKV5hRderduSb5I99CTf45NIe.X0lQBAKXtpPKZ77ReYyCR04q5.Bnj7qPxQzV0rPBQafL9r4qQDJsQQm9vurQVIU9TJcFRIdYmTLPEb4IYdsvCADkf_RiLl.BLTCU.92crxOOc7jT7RI4gbjbrWGBYXz48.sj_zwdnbEOuxF4TPvmTZR0WGDaAZl_QC.G05oSBooG4bTJp0tRO7n9obFUFqmVnDHRighxsWsX0YxzIIQJVvMVrZn0Q2yN3KfBK6cjwd8TMpQA7k47iYFphvBWPgFdOn6qjWgUhNMiacXeQ.uhHz4p_7jt7R9ru3x6sV2If2v0j27gq_A6ScWxLodx5OSbq6TgqdXohgQT0ybd_11T.cpXIyHArHlDXhUdDV5ztTy9zKVBOv0kdLvmWMg2JEZCgzHWbWFhLmK0kiddFyzZ1iluKDe_Ub.SjbknsJK.7eWDKnggV7olp._3w6lIiF6EPs3TQr.xzs_RU9QL37GaGp2h1LaZcdxhhWB8uJobX2bNBYUc6QP_78DM2ur4d2fAmx6HpaIiEC.YnpBF9DHzrsBs0TKLczyjwxEXSbRy_8HEyhgcQfIZwnF7IFEI7RwlUKL2F4SvwQU6hwmVtlDai2HT_i5ChNXgClxttmar_FFIRXkAsFEHfJa_SCJ.LQm7bELnwbs25J0cu0pCu9o.4zrDsDXmts6zCT4zQluW2H892x2mMVr8bWs96O09HE4WBzAqm7I7Zq9T7ivGTXLIz_YcHudejWoE1ZDNW8NHnUGLck0knkxKR5UJ_CvTsSKqS7f75DynjlDoelULB19WykZ6Xqj7mwDHTpM.UonZ.vyWJZSRUymf25kB48uyW.Gy9iqnQ8hUbRM7I0D_x96jW0pUlb0MqC_K6do3kuH8ouQfwRqJI4YM6zO3hBfIwCdhMbv6Zse741lzDSXs7EptDFmEYQiGJ8pDlwP7CpjpnW7PzfbnWQQZLQyszTwXliJtu56ZzqGeJO0TdIL96EcGty6HUwgdnC2xbgPC8boG7hGj25.yE9RYHp0lcuXifFM9JEeZMwYeyRp3Wwjpud06mRcNUp8.r3Xpujsu9cm1gDb6PaXrD7o.w0FZlKARRnnR9GKop1meBkjX_FCTzAkg0ssG0oIwuk6.JgDld8oOCEheGAewm9yH55znSihuEGPrtCRi0AW8Uey9QtmZilKyjZjjqzPfZyfaRjYA7kjKDJduvmYkmH03ibv7WMPQSNJc_OWL_I35kxOZ9I8.OeiusUEItMCKLA7f9faqh\",mdrd: \"FArBTOHcb3zn_OhXcCk494kMlTbEnVL7.E3ZjYuN2WY-1730574125-1.2.1.1-wxFamcqPvFidjHWbQPt5li_oRNemddr77kHfCGfr6NT4KXXplaIEFuRFltHzG1Byg9HJ1g3ypJiFV.khkP4cKETTwmGjUYPMer4HbFAh_McdxwFa04Iwk8GK.1w26Efh9bNASBJJnqVAOPlQ06B4D8Szk5RVwO94IO7fe7pZPVTIX2HwT7fsLVNbcXHXQrpog4thBsXp9gE5XG_AbgzPpc10DJZbCfSfFad0Hs910rJP9utXEf5EII15_NiLmwMgVdvR5vFUDgszog49wl3wZhMe4nQWIT67q9_vus3XkpfYCFVbIMBYsvf7jZAP.Tbt40uHqyfY3pH0r95guYfDvVnMGSp1Z6AbawGTneGgCOskBn7qPaSTKFXWJwegGeOVctfyTMSKk_5Y1FanyM4a06IpqGItFtz0knzjsicpAqX1k7lR5XmgkrorCKXi3KOJJshOpV4aSnJki59V3LcAKgX6SXHNpo4ikVMzhVfz0ZAiBHfYbxS0BPDWFkD8UpY0XzUjd8n0UM00UVi5fDzbz_wgDMrr8nPa.6skpLLByQIH09jbbXCELLjarD0kRKYnNGnns1QVHtbV7pjAzhRRJ0oix0xe3G2G2QBbH9BoWCdi6uCUNI8bPSuuPgVx8CMH1OWfebCm3xRYJe9O9jXkEQE.qlXVumGRw0tigjGkuquj7sFzIZ1jQTF0d0tIt.gm7r5X2mIPeCYCGC3BJU_cPL4vOzSGCWmVoGBCmSJPk2nR8xzcOgFiqwhHoFh16eXgAcSgcpR67ywvcH8OHDNRj67Ed0VYS20NcVlNVUe6VNR0y7nPOyb07syefRoU_Dr.9c0u4WN94qphP2d8Xm65uCiBklGwFSbWEpdjTWe5UfNk53s4JqTDwQvIAFXYlvowW.CWqQvqRm8XlEBOteP8RVD8ZQGyPfjC.Z9PL2bPbKIaVnpp2GfZhr9fc72ZMU3W1it4Ny1.Gb_F_Rls4nB0uipezkYCxv41RCBG8OH9R.1dJs91rES1aF6xEyomXdnPhmMF2RTdLKTB_Sp2gNOJprZixNcKSYtftTUiKwmjmW5XnNiZn1Gzcbq8QnO607IPUod6TCbAvMpetxDp2iZ3l13sbFdWlLv9JKr68TX7gSJEhvs1CpIoRZyGL5nMRi.v5Bpd.S8A8ZmMVHzJWCwxW5w3jDuGr1L.XqK.tMBbIsyqbND7SGymFPg6nXe4C7cftdvWmv.bJIWMg_tNCDKzi_4L18UlobXJ2LWU092.fPwoqhKoWZWTrUkZU_YSmy6_9h_UvuHS91C_5_6orEi0Nu7UByJDpwrBPik7kNfS81fj8Z2NHofZKT1cBc.ZKiA66XCZBuGCkvA3Y0CrbcNp9b7A0p1qCRfQekZbcp3hfwECFBAAJjr5BKcXkhRaXstOThPricCCJLtDgvO1N4BV6OwT3..pdwCPKCvwcJJ.4mreuKWid.VmIi7gPCdk5S6A4buvgxATD.JW55a5_7aiMPrDEX4uykZ6Ctllo0ZR7XKiIPw3U8BA_Y9arNFZv4x6ytFY3lZEHOmdd5pPH36AIb7ihTzSr5ZbmE152JD8y8ivrheER41JVdvYGS0bw6PCqYyufQK6e.yMwwweaBAniF0BVV9I92Hoxbcxe6tI4.HPdJgdbL.o14HofbneZiNDCBzIacQVoKscVA.X3XixfhLak1fpvfXx1GuA0f.GsPeizuNvXQeOU7YBoPRw5ZgAOFs7qjaLDL0UAUYG3pduNDb8lCWihpWbFXierzrZZpU7ZcUI1qnA01_ye0.jUnmA23iiUGNqNPmHoKcB79DSPmrDvFhp_XlHtdnrrk2tzjPeeg_KrGIYPs7518y_zNM5JP5TyQw7iuwJPneGM20M9o65r9kJBX3pMe7aAwn_4TaRm5DXgwthuezpiH7JVPOSZAdZJ..bTnCdhBYJK3bVnKaqWgIF2oh4.tTbJvWrU44ry48m4fk0vCrXld83gqNT1rXWGUxtqXIxr5go0f.mKCUQhm_I.00YZOIx.Px0NTbUfWp_1YPrKGdNkS3E36Cu0qauGV7z_vP7xJYjGQS3UiNv_yUHlSnZg65..NOjlvA1jwmCh51qEoSJ0oB23SCRcbFcyq1xjTADn6dZqj3TWRz5_LgeyJPBH3ysdaqYoopjkeaB.U3sr0VDcJgCqOLturq4Grf9ZVKV2DQe5r8S2u5id9BZhjBRDGAvwY9FjsJIUjxVuDljYfkwsLV2QC24ZbL6xx07hSomv0UwjMv_SvE6lAuGaG0exuK1uKlM2S6wf2vLCxcZPd1Y66aJjOlGvvlrupSBRaxenXysgVpR0t2AnC.Jq7S2HlSmlAvk6dcoDR7Vir5RtvvFS0zVl2p7G.LT81YFmhYakPi1ENXL3oR9yPlM2xy6fUjsS_OCoxA\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8dc6607ae9113956';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/content\\/article\\/britain-s-postwar-sugar-craze-confirms-harms-sweet-diets-early-life?__cf_chl_rt_tk=8RXVb7giYQ5DKMLgQK6zN_T6iKGigVGUsBakxwvXtdk-1730574125-1.0.1.1-GdSCSdozlJG1peu0wdPByMdh2LkBVG71c1yb0FOtTac\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42027564",
    "commentBody": "Britain's postwar sugar craze confirms harms of sweet diets in early life (science.org)104 points by rbanffy 2 hours agohidepastfavorite36 comments 462436347 31 minutes agoUS sugar consumption declined from 2000-2020 to 1970s levels, while its T2D prevalence only increased: https://news.ycombinator.com/item?id=38094768 And if sugar is so metabolically harmful, where are the RCTs showing this? All I've seen is that outside of a caloric surplus, it isn't especially metabolically harmful, and ironically, even outside of a surplus, saturated fat is much worse: https://diabetesjournals.org/care/article/41/8/1732/36380/Sa... https://link.springer.com/article/10.1007/s00394-015-1108-6 Meanwhile tribes of hunter-gatherers in Africa get 15-80% of their daily calories from honey during certain seasons; why aren't they obese and diabetic? https://www.sciencedirect.com/science/article/abs/pii/S00472... reply nightski 21 minutes agoparentReaders here probably aren't hunter gatherers in Africa though. If you live sedentary lifestyle with an abundance of food you may need to take a different approach to nutrition. Sure it would be ideal if we were all hyper athletes, but the reality is that probably isn't going to happen and I am not sure it's even better holistically. reply schmidtleonard 1 minute agorootparentThe reason to do RCTs and establish causality isn't to generate excuses for a sugar diet, it's to head off bullshit alternative sweeteners that don't fix the problem but advertise like they do. reply 123yawaworht456 19 minutes agoparentprev>US sugar consumption declined from 2000-2020 to 1970s levels, while its T2D prevalence only increased obesity did not decline >And if sugar is so metabolically harmful, where are the RCTs showing this? All I've seen is that outside of a caloric surplus, it isn't especially metabolically harmful https://en.wikipedia.org/wiki/Fructose#Potential_health_effe... >Meanwhile tribes of hunter-gatherers in Africa get 15-80% of their daily calories from honey during certain seasons; why aren't they obese and diabetic? if you are are physically active and don't overeat, you can eat whatever the fuck you want and never get obese. if you are not obese, you will (most likely) never T2D reply erik_seaberg 12 minutes agorootparentJust one cheeseburger is three miles of running. Not only is it very easy to shop and overeat, your body continually encourages it. The only way out is determination not to eat whatever you want. reply omikun 7 minutes agoparentprevTry eating mostly honey and roots and see how much you can over consume. The problem in US is the variety of food and how engineered they are to be hyper palatable. Snacks are designed to pump sugar into the blood stream, with just enough salt, fat, or carbonation (in drinks) to mask just how much sugar is in everything. That's the reason why warm flat soda tastes disgustingly sweet. It's not just sugar, but the amount of it, and how fast it is consumed, and how and when do we expend energy (walking after meals directly consume blood glucose b/c calve muscles don't have a glycogen store) impacts fat buildup and T2D. Check out books by Robert Lustig on the subject. reply sss111 1 hour agoprevIndia saw a similar thing with the sugar craze once the economy opened up in the 1990s— although the diabetes rate has only gone up 2% in three decades. reply FirmwareBurner 53 minutes agoparentSimilar in Eastern Europe after the fall of USSR when western snacks came on the market and we fell pray to advertising after decades of isolation: \"Those sugary western snacks can't be bad for you since they come from the developed west and rich westerners eat them\". Oh boy, if we only knew back then what we know today. Probably why a lot of millennials today don't look very healthy. reply llm_trw 15 minutes agorootparentI imagine it has more to do with the smoking and drinking rates than sugar. I very clearly remember an aunt of mine losing her mind in the 90s about me drinking coke, while smoking when she was pregnant. reply jajko 37 minutes agoparentprev> India ranks second after China in the global diabetes epidemic with 77 million people with diabetes (google) Maybe it went up by 2% only, but it depends what were the actual numbers to start with. I've spent 6 months backpacking all over that country and although food is top notch, the sweets are ridiculously bad, often just distilled sugar with some (rather good) flavoring like safron. I guess when you scorch all your taste buds since early age with all those chillies (its quite common to just eat raw chillies as a side dish to already crazy spicy foods on levels that most westerners going to their local indian restaurants will never experience), then to get any sensation from sweets they have to go over board. reply mmsc 16 minutes agoprevAre there any studies about the harm of a craze in non-sweet diets? While I would generally agree with this \"confirmation\" based on my understanding of diabetes, I wonder if it's actually sugar that is the problem here. For example, what about the massive amount of caffeine in soda, chocolate, and other \"sweet diet\" food? Or, what about just general over-consumption of food in postwar regardless of what it is (which is much more a societal issue than anything else) reply omikun 4 minutes agoparentplenty of studies on caffeine. Chocolate usually comes with loads of sugar unless you mean sugar alternatives? That wouldn't apply to just post war UK though. Also, it's hard to over consume non-sugar so that's not a lot of overlap. Remember carbs break down to sugar as well. reply waihtis 1 hour agoprevYeah, well modern diets also have a wildly imbalanced omega 3-6 ratio, which causes chronic inflammation which in turn is a central driver of diabetes. Funny how they just disregard it and try to pinpoint everything to a single variable. reply christophilus 1 hour agoparentYou’re not wrong. Sugar isn’t the only thing that changed after rations were lifted. Caloric intake in general went way up. Environmental changes came about. Processed foods became mainstream. I just don’t know how you can pinpoint one thing and choose it as the chief villain. reply hn_throwaway_99 36 minutes agorootparentI suggest you read the study in detail. I originally had a similar thought as you, but then seeing how the researchers were able to tease out specific effects based on minor differences in birth timing relative to the end of sugar rationing was strong evidence. reply willy_k 28 minutes agorootparentI don’t see how those are mutually exclusive, they said sugar isn’t the only thing, not that it doesn’t have a significant effect. reply waihtis 59 minutes agorootparentprevIndeed. Its the same idiocy as was/is with cholesterole reply eastbound 44 minutes agorootparentHonestly, when I see the young generation in US, it seems it’s kind of a given than young males must bulk up, under penalty of not being instagrammable enough to find a girlfriend. That sentence may sound terribly superficial… but it’s a reality for a lot of young men. Long story short, bulking up is at odds with ecology, and we ask youngsters to do both. reply ipaddr 32 minutes agorootparentThe kids these days are beanpoles and lack muscle mass. Far cry from the 80s. reply kergonath 20 minutes agorootparentprev> Long story short, bulking up is at odds with ecology, and we ask youngsters to do both. How so? I would think that better nutrition and more physical activity is not at odds with being environmentally conscious, quite the contrary. reply kergonath 34 minutes agoparentprevTo avoid any doubt: there is no scientific basis for “imbalanced omega 3-6 ratio, which causes chronic inflammation which in turn is a central driver of diabetes”. There is no indication that omega-6 in human diet cause inflammation (we don’t eat that much of them, and they are readily metabolised). And as a matter of fact, diabetes is more strongly correlated with sugar than any kind of fat. Actually, the only people I have seen claiming this are conspiracy theorists who jumped on a new boogeyman. reply meiraleal 1 hour agoparentprevSugar raising blood sugar seems like a quite obvious effect but some people argue against it. Edit: it doesn't need much to be smarter than you reply 462436347 21 minutes agorootparentExercise temporarily raises your heart rate and systolic blood pressure, yet avid exercisers have lower RHRs and SBPs; how do you know it isn't the same with carbohydrates, provided fat (especially saturated) is restricted? reply omikun 0 minutes agorootparentAsk an endocrinologist. Robert Lustig has written extensively on the subject. This stuff has already been figured out. Here's one of his famous talks on sugar: https://www.youtube.com/watch?v=gmC4Rm5cpOI&pp=ygUNcm9iZXJ0I... midtake 40 minutes agoparentprevGood luck convincing anyone of that without being called a crackpot, a disinformation bot, or worse. At this point in my life, I keep such knowledge to myself and leave everyone else to the peril of their own incomplete understanding. reply oliwarner 26 minutes agorootparentRigorous scientific study is more effective than luck. reply b800h 1 hour agoprevThis isn't what the paper actually says, of course. Science by press release again. reply Thorrez 23 minutes agoparentHere's a quote from the abstract of the paper: > Using an event study design with UK Biobank data comparing adults conceived just before or after rationing ended, we found that early-life rationing reduced diabetes and hypertension risk by about 35% and 20%, respectively, and delayed disease onset by 4 and 2 years. Protection was evident with in-utero exposure and increased with postnatal sugar restriction, especially after six months when solid foods likely began. Which part of the title isn't supported by the paper? reply fidotron 1 hour agoprev [–] These things still don’t establish actual causation. For example, someone susceptible to later developing diabetes may consume unusually high quantities of sugar when available as a means to deal with some other insufficiency. (Guess where that idea comes from). The problem is not the sugar per se, but the fact that different people respond to the same consumed items completely differently, something that is very inconvenient for those that want to treat everyone the same way. reply kergonath 24 minutes agoparent> The problem is not the sugar per se, but the fact that different people respond to the same consumed items completely differently, something that is very inconvenient for those that want to treat everyone the same way First, the fact that people react differently does not mean that it is not a public health issue. Some people can drink absurd amounts of alcohol and still be functional afterwards. It’s still not a good idea to drink more than a small dose of alcohol regularly. Then, there are dangerous and lethal thresholds for all substances, even for seemingly-tolerant people. The fact that most symptoms take decades to develop does not help. Add the fact that we don’t know why some people are more tolerant and we cannot predict it. Sugar is a problem, at the individual level and even more so because of the burden on society because of public health issues. I am happy to give a gold star to sugar-tolerant people who remain fit and live a long life despite eating tons of the stuff. I am very happy for you. But you are not a proof that sugar is not bad. reply dataflow 59 minutes agoparentprev [–] > The problem is not the sugar per se, but the fact that different people respond to the same consumed items completely differently, something that is very inconvenient for those that want to treat everyone the same way. Haven't diabetes rates been steadily rising until today though? How does one explain that away as \"people respond differently to sugar\"? reply fidotron 52 minutes agorootparentI know this from the simple fact that I respond completely differently to sugar than almost everyone else I know, except gout sufferers. The question is why people are guzzling so much sugar in the first place. The answer is they are malnourished. Post war britain was a particularly bad case they deliberately paper over, but my parents grew up with rationing and never snapped out of it, like many others, which led to many of my generation also being subjected to that diet. It simply fills you up but provides people with my metabolism with no energy at all. reply dataflow 43 minutes agorootparentI'm not disputing that people respond differently to sugar. I'm asking how that explains the diabetes epidemic becoming more and more widespread as time goes on. reply waihtis 50 minutes agorootparentprev [–] Because people eat different types of diets and consume, for example, different ratios of omega fatty acids - which in turn can cause chronic inflammation and diabetes reply dataflow 41 minutes agorootparent [–] I'm not disputing that people respond differently to sugar. I'm asking how that explains the diabetes epidemic becoming more and more widespread as time goes on. reply waihtis 7 minutes agorootparent [–] I see. Diabetes is downstream from chronic inflammation, so something is causing us to be inflammated at scale and that is why diabetes is becoming more commonplace. So differing response to sugar is one factor of N possible affecting variables reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Britain's postwar sugar craze underscores the potential negative impacts of high sugar diets in early life, linking them to health issues like diabetes and hypertension.",
      "Despite a decrease in sugar consumption in the US from 2000-2020, the prevalence of Type 2 diabetes has risen, sparking debate on sugar's metabolic harm versus other factors like lifestyle and food engineering.",
      "Research indicates that restricting sugar intake early in life may lower the risk of developing diabetes and hypertension, though discussions continue on the influence of other dietary components such as omega fatty acids and total caloric intake."
    ],
    "points": 104,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1730566586
  }
]
