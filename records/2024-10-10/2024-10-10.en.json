[
  {
    "id": 41792500,
    "title": "Internet Archive: Security breach alert",
    "originLink": "https://www.theverge.com/2024/10/9/24266419/internet-archive-ddos-attack-pop-up-message",
    "originBody": "Security/ Tech/ Web The Internet Archive is under attack, with a breach revealing info for 31 million accounts The Internet Archive is under attack, with a breach revealing info for 31 million accounts / A pop-up message said the online archive has suffered ‘a catastrophic security breach,’ as its operators say the site has been DDoS’d for days. By Wes Davis, a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020. Oct 9, 2024, 9:26 PM UTC Share this story Photo by Amelia Holowaty Krales / The Verge When visiting The Internet Archive (www.archive.org) on Wednesday afternoon, The Verge was greeted with a pop-up claiming the site had been hacked. Just after 9PM ET, Internet Archive founder Brewster Kahle confirmed the breach and said the website had been defaced with the notification via a JavaScript library. Here’s what the pop-up said: Have you ever felt like the Internet Archive runs on sticks and is constantly on the verge of suffering a catastrophic security breach? It just happened. See 31 million of you on HIBP! HIBP refers to Have I Been Pwned?, a website where people can look up whether their information has been published in data leaked from cyberattacks. HIBP operator Troy Hunt confirmed to BleepingComputer that he received a file containing “email addresses, screen names, password change timestamps, Bcrypt-hashed passwords, and other internal data” for 31 million unique email addresses nine days ago and confirmed it was valid by matching data with a user’s account. A tweet from HIBP said 54 percent of the accounts were already in its database from previous breaches. In posts on his account, Hunt gave further details on the timeline, including contacting the Internet Archive about the breach on October 6th and moving forward with the disclosure process to today, when the site was defaced and DDoS’d at the same time they were loading the data into HIBP to begin notifying affected users. After closing the message, the site loaded normally, albeit slowly. As of 5:30PM ET, the pop-up was gone, but so was the rest of the site, leaving either nothing or a placeholder message saying “Internet Archive services are temporarily offline” and directing visitors to the site’s account on X for updates. Jason Scott, an archivist and software curator of The Internet Archive, said the site was experiencing a DDoS attack, posting on Mastodon that “according to their twitter, they’re doing it just to do it. Just because they can. No statement, no idea, no demands.” Later on Wednesday evening, Brewster Kahle of the Internet Archive confirmed the breach in a post on X: What we know: DDOS attack–fended off for now; defacement of our website via JS library; breach of usernames/email/salted-encrypted passwords. What we’ve done: Disabled the JS library, scrubbing systems, upgrading security. Will share more as we know it. An account on X called SN_Blackmeta said it was behind the attack and implied that another attack was planned for tomorrow. The account also posted about DDoSing the site in May, and Scott has previously posted about attacks seemingly aimed at disrupting the Internet Archive. We’ve reached out to the organization to learn more information. Update, October 9th: Added information from HIBP and BleepingComputer as well as Brewster Kahle’s confirmation of the breach. Most Popular Most Popular The bill finally comes due for Elon Musk The Internet Archive is under attack, with a breach revealing info for 31 million accounts A closer look at Nintendo’s adorable Alarmo clock Casio supersized a classic digital watch to create this retro desk clock Instagram and Threads moderation is out of control Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=41792500",
    "commentBody": "Internet Archive: Security breach alert (theverge.com)1008 points by ewenjo 22 hours agohidepastfavorite545 comments Springtime 15 hours agoJust in terms of privacy, it's worth noting that anyone who has uploaded something on IA already has their email address publicly viewable. This isn't something that commonly known (even judging by comments here) but in the publicly viewable metadata of every upload it contains the uploader's IA account email address. So from a security perspective it's bad but from a privacy perspective a lot of users probably weren't aware of this detail if they've uploaded anything. reply hunter2_ 14 hours agoparentThis raises an interesting question: should email addresses be private? Addresses of buildings aren't private, and they're somewhat analogous as with many computing concepts. (Aside: Before spam filters were quite good, it was typical to avoid scraping of addresses by mild obfuscation, but I think those days are gone, and this is distinct from privacy anyway.) If someone wants to upload and never be found out, then they need to use a throwaway address in any case, lest they be providing their \"private\" address to the administrators of the service without explicitly forbidding further disclosure. If I say something to Alice without demanding that Alice keep it from Bob, then I implicitly don't mind if Alice tells Bob what I said. reply tjoff 13 hours agorootparentWhether the email is considered private or not is completely orthogonal to whether you are allowed / should tie an action to your email. And then again completely orthogonal whether you can/should make that connection public. Even if your email is public information and even if what is uploaded is public information that doesn't imply that the email address behind the account that uploaded that information should be public. reply nerdponx 3 hours agorootparentThe same exact thing goes for physical addresses too. The fact that I live at my address is public knowledge. But the presence of my address in any particular database, mailing list, etc. is not and should not be public knowledge. reply stefs 8 hours agorootparentprevi agree. if \"user contacting another user\" is a feature, there should be the option to (optionally) supply a different email address than your account email or use an online form that keeps your account email hidden. reply slimsag 14 hours agorootparentprevThere is software which is intended to e.g. locate the GitHub profiles of people working at companies, then scrape all public repositories they've contributed to for their email address and the emails of their coworkers - to enable targeted advertising to those individuals. Very common in enterprise sales. With ChatGPT, this can be extended to create emails that look very personal - as if someone has followed all of your work and is genuinely interested in what you are up to - with extremely low effort. And people are already doing this, I already get emails like this today. Should emails be private? I don't know - I personally consider them to be public because I know for a fact mine will eventually be public whether I like it or not. But I am aware AI is out their slurping up every public communication I've ever had, and is likely trying to manipulate me in various ways already today. reply benterix 8 hours agorootparentThis was a problem already before the generative AI era, it just got less expensive. The only way to reduce it is to have two work addresses: one that you rarely check and is exposed to the public, listed on your profile etc., and the real internal one just to get the work done. reply pixl97 5 hours agorootparent>it just got less expensive Quantity is a quality. Add that the AI can profile you and do a decent job spear phishing and you're talking about a sea change. >and the real internal one “Three can keep a secret, if two of them are dead.” There is no such thing as an 'internal' email you communicate to other people outside your company with. It's just an email address. Someone at some point will leak it by accident or malice. reply benterix 18 minutes agorootparent> There is no such thing as an 'internal' email you communicate to other people outside your company with. It's just an email address. Someone at some point will leak it by accident or malice. Sure, so personally I never use it to communicate with people outside. Also, I make sure it's never used to register with external licenses like Docker Desktop etc. as they subscribe me to their spamlist and send the usual semi-personalized messages - but as far as I can tell most of these bigger companies don't sell them outside (for a good reason). Startups, however, will do what they want and will make sure to squeeze the last drop from the info that such-and-such person works and that company and does X. reply Roark66 13 hours agorootparentprevAbout AI slurping all information. I bet one of the first ideas organisations that spy on population had when the recent AI boom happened was: How about we just train our AI on all the intercepted data and just ask it? Is John Smith a terrorist (for our definition of terrorist)? And the AI would reply: Yes he it, he searched on Google where to buy these ingredients that can be used to make explosives. So then they go and figure out some way to \"legally\" arrest the guy and obtain more private info. It looks like the guy was buying the stuff because he's got a plot of land to fertilise and an old car to paint. So they ask the AI again. You said John Smith is a terrorist! And the AI would answer. I'm really sorry, I'm doing my best and I'll endeavour to do better in future. After this the agents ask for another billion $ because clearly they need more VRAM. reply boscovn 3 hours agorootparentprevPersonally I've been using an email aliasing service (simplelogin) and try to use a different alias for every purpose. I don't use it for my git commits but I find that email aliasing services are something to look into not just for privacy concerns but also spam mitigation reply kurisufag 1 hour agorootparentprev>With ChatGPT, this can be extended to create emails that look very personal - as if someone has followed all of your work and is genuinely interested in what you are up to - with extremely low effort. And people are already doing this, I already get emails like this today. shit, now i don't feel like sending e-mails to people i'm actually interested in reply Springtime 14 hours agorootparentprevAn issue is for most sites/services an email has just become a standard authentication method, rather than something that can easily be more unique per account. So any usernames across sites/services that share it identify that user as being the same person (for data broker profiling, doxxing, etc), which is the privacy issue (not the email address per se, unless it perhaps contained one's real name). For contrast truly unique email aliases for example aren't possible on common services like free Gmail*, only things like self-hosting/certain paid email hosts, which makes less feasible for many. So from a privacy perspective while in an ideal world everyone would be able to freely create entirely unique per-account creds we're mostly stuck with the email implementation. * One could create entirely separate accounts but it's high friction and IIRC the same phone number (now a requirement) can only be used for 2-3 accounts. reply StressedDev 12 hours agorootparentProton Mail and iCloud’s hide my e-mail feature allow users to have unlimited e-mail addresses. You can also get unlimited e-mail addresses by running your own e-mail server or using something like Office 365’s business e-mail (costs about $4 per month). reply bossyTeacher 7 hours agorootparentis running your own e mail server a good idea in 2024? Security issues aside, you are at the mercy of the big email providers and whatever rules they want you to follow reply kroltan 6 hours agorootparentFor e-mail addresses as an authentication tool, you don't really need to be able to send emails at all, just receive them, and I think that is pretty feasible to not run afoul of the usual shenanigans. reply rrwo 4 hours agorootparentprevI think the cost of paying for a dedicated email service is worth it. (There are plenty of smaller, privacy-oriented services such as Proton Mail or Fast Mail.) They're better at it than I am, and it means I don't have to fill up my free time maintaining another server. reply bsammon 13 hours agorootparentprev> One could create entirely separate accounts but it's high friction and IIRC the > same phone number (now a requirement) can only be used for 2-3 accounts. I've wondered about this. Every Android/ChromeOS device I've ever bought, I had a new Google account created for it (during setup, instead of using an existing account), and only a few actually had phone numbers (I don't generally use smartphones for telephony). Is \"Google account\" synonymous with \"GMail account\" these days? I've had this idea for an experiment where I get such a device (without a simcard), and see how many times I can iterate the Initialize-Device-With-New-Google-Acct-PowerWash-Repeat cycle, and how many Gmail accounts I would have as a result. reply sureglymop 12 hours agorootparentWhy did you do that? Android doesn't require an account to work. reply exe34 10 hours agorootparentI think it does if you want to install anything from the Play Store. reply gdevenyi 8 hours agorootparentAurora store gets around that reply exe34 7 hours agorootparentthe search doesn't really work does it? you have to search on Google and then click on it to open with aurora. but you're right, it does help! reply gdevenyi 5 hours agorootparentThe search worked for me to find a single app I needed when I was setting up a single-use tablet recently, but I haven't used it hugely beyond that. YMMV reply II2II 6 hours agorootparentprev> This raises an interesting question: should email addresses be private? Addresses of buildings aren't private, and they're somewhat analogous as with many computing concepts. There are several ways to look at that. The organization that I work for considers anything that ties two pieces of information about a person together as private information. That is to say that a person's name is not private and a phone number is not private, but connecting a phone number to a name is private. In one form or another, an email is frequently tied to a name (e.g. the email address is based on their name, or an account record includes both a name and an email address). Another way is to consider how accessible the information is. There was a lot of information that was not considered as private prior to the widespread adoption of the internet. One issue that I remember popping up in the early 1990's involved property (i.e. land) records. Historically, people had to go to a government office to access them but they were publicly available. Since they were publicly available, some governments made them available online. Once they were available online, the barriers to access were removed (e.g. having to physically visit an office) and the ability to abuse that information was vastly increased. All of a sudden, people started considering something that used to be considered as public information as private information. reply KronisLV 11 hours agorootparentprev> This raises an interesting question: should email addresses be private? I sadly don't think that's viable. What might be, in our current world, would be having a mail server/client setup where you can generate random addresses for yourself like Wf1JJUBHLu@domain.com and never re-use an e-mail address, much like with passwords, while being able to see all of the incoming mail in the same place and respond with the corresponding accounts. Then, when your address gets traded around, it'd be fairly obvious (with some basic bookkeeping, e.g. a text field with purpose/URL for why a certain address was created) who is to blame for it and blocking incoming traffic from somewhere would be trivial as well. I do have a self-hosted mail server and there are commands to create new accounts pretty easily, I'd just need to figure out the configuration for collecting everything in one place, as well as maybe make a web UI for automating some of the bits. I wonder if there are any off the shelf solutions for this out there. reply ddoeth 11 hours agorootparentI also have my own mailserver and I don't create new accounts, I have a wildcard filter that drops all emails that come to my domain in my inbox. This is of course only viable when you are the only person using the domain, but I just sign up with a new mail address every time I sign up, for example my hackernews account would be hackernews-acc@xx.com That way I have a clear differentiator for every domain. reply iam-TJ 10 hours agorootparentI do something similar except that I do not allow wildcard reception - I create unique service-identifying user@ for each service I give an address to, and have a simple script that immediately adds that to the Postfix virtual table. That way the SMTP server can reject all unknown user@ without accepting them in the first place - preventing spamming and some types of denial of service through resource starvation. I also apply greylist based on a unique tuple (From, To, client IP address) so on first connection with that tuple valid SMTP clients need to re-deliver the email after a waiting period. Any subsequent delivers are accepted immediately. reply KronisLV 11 hours agorootparentprevThat's a pretty cool approach! I'd only be worried about the risk of leaking the main account address when responding to anything, but it's probably doable with a bit of research, like Postfix catch-all setups seem straightforward enough. reply climb_stealth 10 hours agorootparentFWIW that should just be a matter of using the right configuration and mail client. With Fastmail for example I get to use a catch-all setup with my domain, and respond to whatever email it was sent to. And the other way around as well. Send an email from an arbitrary @domain email address. reply EVa5I7bHFq9mnYK 11 hours agorootparentprevYes, but privacy suffers with this approach, because if one of emails ending in @domain.com is tied to your identity, all are. reply KronisLV 11 hours agorootparentThat's not really my use case, but seems like an important concern for many! At that point, you probably want to use whatever features one of the big providers use, like: https://proton.me/support/aliases-mail Maybe even something that'd sit in front of a mail server that you yourself control, I wonder what the variety of options out there is. reply Sebb767 9 hours agorootparentprevThis is true for someone manually searching for your info, but sufficient to fool spam lists and most data brokers. This really depends on your threat scenario. reply squarefoot 9 hours agorootparentprev> This raises an interesting question: should email addresses be private? Yes and no. Both of them. As any powerful tool, email is going to be abused, like any other alternative would be when it will come one day. Those services allowing creation of dynamic email addresses do their job (until they're banned, that's why I'm not mentioning them), however using them isn't automatic and most people don't even know about their existence. What if we then did upgrade email protocols to reflect current needs wrt privacy and modified existing mail servers so that they could create dynamic addresses when asked by a simple flag? Example: I want to subscribe to a service from company XYZ, however I'm not sure how much I can trust them, therefore, when writing an email or filling a web form I can activate the option to create a new address that is tied to the recipient I'll be writing to, and will work as a dedicated proxy for my real address, that is, every mail I send to the recipient using my real address will be actually sent from the new dynamic address, then all replies to the dynamic address will be routed to my real one, but a field in its headers will always contain either a memo by me (example: \"signup with XYZ\") or the original recipient (example: \"info@xyz_trustuswerenotspammers_yeahsure.com\"). This way one can immediately spot whoever sold their address to others and blacklist them. As said, those services work well but not being built in into mail servers and clients their adoption is quite restricted. I don't see why that function shouldn't be embedded in a new upgraded email protocol as the modification would neither be that hard nor consume any serious resource. I would however expect heavy resistance against the adoption, of course. reply iicc 4 hours agorootparentprev>Addresses of buildings aren't private, and they're somewhat analogous as with many computing concepts. Buildings are analogous to domains, not email addresses. reply figassis 5 hours agorootparentprevIt should, mainly because an email is not just an email, it's a channel to reach otu to you, your internet address. And we know how that is going in your inbox. reply weinzierl 10 hours agorootparentprevThis raises an interesting question: should email addresses be private? GDPR is clear on this and there have been significant fines for revealing email addresses against the will of their owners (e.g. using cc instead of bcc). Not saying this is the ultimate wisdom, just a data point to consider. reply tomjen3 14 hours agorootparentprevIn a world where email costs ten cents to send (per receiver) email addresses need not be private. In our world? They kinda need to for sanity. reply skeeter2020 2 hours agorootparenteven 1/100 of a cent would solve the problem - but create a bunch more! reply makach 14 hours agorootparentprevPr definition the email address is considered as private information and should be protected accordingly. reply fortyseven 14 hours agorootparentprev> should email addresses be private? I dunno. Should your personal phone number be private? Or your home address? Would you be okay if I knew it and shared it with a stranger? Or would you rather be asked permission to share it first? Seems pretty cut and dry to me. Yeah, there's going to be someone out there (there always is) who doesn't care, but I'd wager the majority would be pretty ticked off if you gave those pieces of information out to a rando on the street. reply mjr00 14 hours agorootparentNone of that information is actually private though. Your home address and personal phone number are likely in the public record for any number of reasons, such as ownership records or court filings. Or maybe a Facebook post from 2009 that your mom made. Unless you're one of the 0.00001% of people who do things like rotate your phone number and address annually, it's out there somewhere. But public vs private is a spectrum, not a binary true/false. My phone number is public because I get sales calls from various companies to it. It's annoying, but bearable. But there's a big gap between that and the New York Times putting my name, number and picture on the front page. So your home address and phone number aren't private. But they're also not readily accessible unless someone is really dedicated to finding them, so they're not quite public either. reply harywilke 7 hours agorootparentprevWe used to get these big books delivered to our doorsteps that had your name, your address and your personal phone number. You could pay to opt out. reply the_gorilla 2 hours agorootparentIf I published a list of all name and addresses, that's still different than \"here is harywikle's full name and address\". I imagine you wouldn't be too pleased? reply hunter2_ 1 hour agorootparentThe link between online identity and offline identity is a sacred barrier. And I'm not sure that archive.org breached that particular barrier. reply the_gorilla 1 hour agorootparentThat's the issue I take with the \"phonebook\" defense. It justifies doxing people by collecting and connecting publicly available information online. All the information is out there, it's all on a phone book, your email was published online, and so on, but the end result is clearly bad so something in the process should be handled more carefully. reply chii 11 hours agorootparentprevThe missing part is the action part. An email (or phone number, or address) is an identifier. Asking whether this identifier is public or private misses the important thing, which is the action that can be paird with the identifier. So therefore, there's no universal answer to whether the identifier should be public or private. It's a case by case basis, when paired with an action. For example, i don't want a shop to see me buying condoms, so shops shouldn't get my email address (or phone number). reply amszmidt 14 hours agorootparentprevThere are plenty of countries where all that is public information, back in the day there even used to be a phone book with .. name, phone number, and address. And many countries have this now in digital form. reply slimsag 14 hours agorootparentprevInterestingly, public U.S. state property records will just disclose where you live whether you like it or not. With as little as your name, a home address is trivial to find. reply GeoAtreides 11 hours agorootparentprevPhonebooks were a thing not so long ago... reply mdp2021 11 hours agorootparentAnd they contained data of which people allowed disclosure. When you did not want your information to be published, you informed the telephony provider and the phonebooks would not include it. reply exodust 5 hours agorootparentFor a fee. In Australia at least it cost money not to be listed in the phone book. Numbers were however tied to a property rather than individual personal phones in our pockets. When you think about it, mobile phone technology arrived quickly and caught everyone by surprise. Back in the 80s very few people thought we'd be carrying around \"pocket TV phones\" in such a short time. reply szundi 12 hours agorootparentprevThis question could not be more academic reply steffanA 3 hours agoparentprevThis is bad enough. This alone is a privacy bug/data leak. Theoretically, someone could scrape the pages and compile a list of exposed email addresses. reply spease 2 hours agorootparent> Theoretically, someone could scrape the pages and compile a list of exposed email addresses. I laughed. Oh no! Anyways… The people interested in identity theft are probably too busy figuring out what to do with all the SSNs they stole (not from this breach, but from the annual catastrophic breach of a credit bureau or government repository). And the people who want your email probably already got it from one of the hundreds of other services you have to create an account for now. I’m not really sure if there are circumstances where donating to the internet archive could be held against you and lead to persecution. Maybe in certain Luddite communities? The Amish? But then, how would they know… reply keybpo 5 hours agoparentprevIt's not just uploads but any item that uses the email address as a unique user identifier (I'm not technical enough to explain this clearer but [1]). An email address will be part of the xml in his uploads but also in his profile, which anyone can access by simply changing the url from https://archive.org/details/@foobar to https://archive.org/download/foobar. So, in essence, one just needs to have a registered account, independeltly any uploads made. [1] https://help.archive.org/help/accounts-a-basic-guide-2/ reply rrwo 4 hours agoparentprevOne solution is to use a unique email address for every website, and change the address if the site gets compromised (with the old address getting added to a spam filter). reply steffanA 20 hours agoprevMore details here about the data breach. Stolen database contains 31 million records. https://www.bleepingcomputer.com/news/security/internet-arch... reply ano-ther 19 hours agoparent> the Have I Been Pwned data breach notification service created by Troy Hunt, with whom threat actors commonly share stolen data to be added to the service Do they? Why? reply Maxious 19 hours agorootparentProves they really did hack something. There's other sites where hackers register defacements etc. reply xproot 19 hours agorootparentprevAnyone who buys it or finds it in the wild can also upload it. reply richbell 19 hours agorootparentprevIf Troy authenticates the data, they can use that as an 'endorsement' when trying to sell it. reply ianhawes 15 hours agorootparentThis. Typically HIBP attribution includes the email of the \"submitter\". Various data aggregators will contact them and buy the stolen data. Everybody wins*. * Exceptions apply. reply Thorrez 13 hours agorootparentWhere on HIBP can I see the email of the submitter? reply ramimac 11 hours agorootparentIt's not available in this case, or every case. When available, you can search \"The data was provided by\" in https://haveibeenpwned.com/PwnedWebsites reply RamRodification 6 hours agorootparentprevDoesn't the value drop dramatically if it has already been shared with Troy and the HIBP database? Or is there a time frame where it has been authenticated by Troy but not yet added to the database? reply richbell 6 hours agorootparentI don't think so. Troy isnt publicly sharing the credentials and that's what's valuable — especially having \"exclusive\" access. He blogged or tweeted about this at some point. Sadly, I can't find the link. reply maltris 6 hours agoparentprevMy question is: How did Scott Helme end up with a password hash that features his own name? reply jgrahamc 5 hours agorootparentHe didn't. If you break down that field you see: $2a$ 10$ Bho2e2ptPnFRJyJKIn5Bie hIDiEwhjfMZFVRM9fRCarKXkemA3Pxu ScottHelme 2a = bcrypt, 10 = 2^10 rounds, Bho2e2ptPnFRJyJKIn5Bie is the 22 character salt, hIDiEwhjfMZFVRM9fRCarKXkemA3Pxu is the 31 character hash value, and then there's ScottHelme. Best guess is that the archive.org folks just appended the user name to the stored hash. Maybe once upon a time they didn't have a username column in their table and this was a creative way of adding it. reply mkl 19 hours agoparentprev> The data will soon be added to HIBP My unique-to-archive.org email address is not there yet. reply paulnpace 6 hours agorootparentMany hackers will remove addresses that are obviously unique, including tags, to keep silent which database has been hacked, but it seems inconsistent. I have checked and known my address was in a hack and it isn't there, while other times it is there. I also wonder if they start filtering out by domain, as they see a domain across multiple databases with unique addresses in each database exactly one time. reply nikisweeting 19 hours agorootparentprevI just checked and my unique-to-archive.org email is showing up in the breach as of 2024-08-09. reply SushiHippie 19 hours agorootparentMine isn't, but I've created my account only a week ago, so maybe I've created the account after the breach. EDIT: Should've read TFA more thoroughly, it says the breach happened before the 30th September. And I created my account around the 2nd October reply Funes- 19 hours agorootparentprevMine too. reply mobeigi 15 hours agorootparentprevOut of curiosity, do you use a unique email address for every single service? reply mkl 13 hours agorootparentYes, without exception. I want to know who is leaking/selling my address, and usually stop doing business with those who do. It also makes filtering really easy. People sometimes have strange reactions when I verbally give them an email address with their company name in it, especially when I'm a new customer. All you need is a domain and an email provider that allows catch-all addresses, both of which are easy and cheap. reply dyingkneepad 1 hour agorootparentI always see people claiming they use this strategy, but I never ever ever see people blaming services saying \"this and this company sold my data to spammers\". Where are the name-and-shame people? Have you ever caught anybody doing anything? reply Towaway69 6 hours agorootparentprevI love doing that, when someone asks me for an email address, it’s always their-name@my.domain - always gets strange looks! Edit: even more fun with catch all domains then it’s company-name@spam.my.domain reply pixxel 13 hours agorootparentprevI do the same but use initials and random chars so hackers or employees can’t assume my email addresses for other sites/services. e.g.: hn_t47fb@my.domain reply jenscow 8 hours agorootparentI also use @my.other.domain for websites, so my human contacts won't assume it is me if they see it. reply markgoho 2 hours agorootparentprevis each address truly unique or are you doing something like username+archive@gmail.com, username+facebook@gmail.com, etc. reply systems_glitch 7 hours agorootparentprevYeah we run this on our own Proton Mail whitelabel, and for a few customers who have us manage it, mostly for the filtering aspect, and the occasional customer who has the wrong/mis-spelled address in their system and won't change it. reply buildsjets 15 hours agorootparentprevNot the author but yes, I do. It’s trivially easy so why not? reply nicolas_t 14 hours agorootparentSame here, only issue I’ve ever had was when my email address had the name of the company in it in the format of spamlklcompanyname@domain.com CS people are sometimes confused by that and I’ve been accused of attempting to hack them by a small shop online because of my email. reply qingcharles 13 hours agorootparentMajor SMTP provider refused my email address as login because of this. Luckily my moaning eventually made its way to one of their developers who fixed it. You can't sign up for a Samsung account with the name Samsung anywhere in your e-mail address. Aliexpress another offender. There my email is just spam@domain. reply jmb99 12 hours agorootparentI used ali@domain for aliexpress, which was accepted. reply JCharante 11 hours agorootparentprev\"Are you from corporate?\" is what I often get when I need to give my email to a store associate. reply phantomathkg 14 hours agorootparentprevCurious, how trivially easy is that? reply TheDong 14 hours agorootparentIt's quite trivial. 1. Buy a domain. About $10/year for a .com 2. Buy a /24 ipv4 block with good reputation (maybe like $10k) 3. Get a rack in a nearby datacenter, rack up a BGP-capable router and your servers for redundancy to run email. Takes about $30k initial setup costs if you buy all new, and about $5k initial setup costs if you cut corners and buy used. It'll be $2k/mo after that, so less than the cost of 1 $100 avocado toast per day, quite affordable. 4. Setup your mailserver of choice, such as dovecot + postfix. Enable either a catch-all address, or use recipient_delimiters. The former means \"anything@domain.com\" works, and the latter means \"user-anything@domain.com\" works (assuming your recipiient_delimiters are '). I recommend using a real catchall. 5. Setup your spam setup, this is the hardest part. I have no guidance here. 6. Point your DNS over, setup SPF and DKIM records, test, and off you go! This should all take about 1 to 3 days if you know what you're doing. 7. Find out that some email will go to spam anyway because you're not using one of the big 4 email providers, but it can't be helped, and anyway no one uses email anymore. And after that, for less than $30k/year, you have email with catchall or subadressing support. Nice and easy. You can also pay Fastmail for email and use their \"catchall\" feature https://www.fastmail.help/hc/en-us/articles/1500000277942-Ca... Or Google Apps also has a catchall feature. Then, after you do this, you can simply give internet archive the email address \"internet-archive@mydomain.com\", or generate a random string. If you forget the email you used, you can search your email history for the first email they sent you, and check the To field. reply 2Gkashmiri 14 hours agorootparentHold on. Why do you need a dc rackspace and a /24 just to have your email ? reply TheDong 13 hours agorootparentThis is hacker news, we're all either founders who have 2 billion dollars in (illiquid) stock options, or FAANG employees making 600k/year, what else are we going to do if we want email? Sure, you could pay fastmail $40/year for this, but that's not really the hacker news spirit, and no one on this site knows how to count as low as $40. The real justifications you can give yourself: Shared VPS hosting pretty much all bans email, AWS, DO, etc all have ToS that say \"no email\" as anti-spam measures. Shared IP space will go straight to spam due to people having spammed on it in the past. Buy a /24 to ensure you don't go straight to spam. Rackspace ensures you actually own your email, at least moreso than with other shared hosting, and owning your email is important. reply account42 8 hours agorootparent> Shared VPS hosting pretty much all bans email, AWS, DO, etc all have ToS that say \"no email\" as anti-spam measures. Complete FUD. Here is DO's acceptable use policy: https://www.digitalocean.com/legal/acceptable-use-policy You can see that they explicitly have policies for email hosts. Here is a guide they host on how to setup a mail server: https://www.digitalocean.com/community/tutorials/how-to-run-... They forbid spamming, not all mail. > Shared IP space will go straight to spam due to people having spammed on it in the past. Buy a /24 to ensure you don't go straight to spam. I have had no problems with deliverability to Google from an IP on a shared block. I don't send marketing mails or any other kind of spam though. Microsoft blocks my IP but they are too small (outside businesses) for me to care to give them special snowflake treatment. Deliverability of your own mails is also irrelevant for the original discussion about using unique email addresses for signing up to services - you don't need to be able to send at all for that. reply 2Gkashmiri 9 hours agorootparentprevbeen using racknerd.com vps for last 3 years for running miab. ZERO problems so far. costs around $12/year+domain reply jmb99 11 hours agorootparentprevFor the “least painful” self-hosted email setup, you can’t be hosting on an IP in a subnet that’s ever sent spam, if you want to avoid being blackholed occasionally. This means you can’t have an IP allocated to you by a hosting provider, or a residential ISP, or a “business” ISP, or any cloud provider. That leaves very few options. Note that I am speaking from personal experience here. I have been self-hosting email for over a decade, from the same IP, with (roughly) the same DNS records. Occasionally, for no reason, I will end up on the global spam list for Gmail, Outlook, or iCloud - never more than one at the same time, and never with a discernible reason. The best I can figure is that the IP is allocated to me by a hosting provider that occasionally sends out spam from its subnet (aka any hosting provider that doesn’t block smtp). I have also tried self-hosting a different mail server from a variety of residential IPs in different cities and countries, and ran into the same problem. reply marmaduke 13 hours agorootparentprevIt’s a joke ! You can run an email server off your phone reply squarefoot 9 hours agorootparentNot sure if mobile carriers would allow the required ports to be routed, and the connection is usually behind CGNAT, so you can't accept connections from the outside to receive emails. Many home ISPs however can give you a (mostly) unfiltered public IP that once paired with a dynamic DNS service can be reached from the outside. Once the network part is solved, a small cheap box (*Pi like board, mini PC, etc) can be set up to act as mail server, with firewall rules on the router that don't expose anything else to the outside. reply dgellow 9 hours agorootparentprevSatire reply useless_foghorn 1 hour agorootparentprevI use Bitwarden coupled with AnonAddy (0) for simple and free on demand email alias generation. 0. https://bitwarden.com/help/generator/#username-types reply JCharante 11 hours agorootparentprevI have an even easier approach: - have an iphone/mac w/ icloud+ - go into settings - add custom email - get redirected to login to cloudflare - buy/pick a domain for $12 - icloud+ automatically sets up the MX records on the domain via cloudflare - enable catch-all emails in icloud settings - Done! Takes about 10 minutes & icloud provides the email hosting without any additional fees reply meindnoch 8 hours agorootparentprev1. Register domain on Cloudflare 2. Configure a catch-all forwarding address to your private GMail Done. reply echoangle 14 hours agorootparentprevSome providers allow you to use Alias emails (I think google redirects mail to ia+mymail@gmail.com to mymail@gmail.com), and if you use your own domain, you can just use a catchall redirect and enter a random address (ia@mydomain.com which goes to catchall@mydomain.com). reply beAbU 11 hours agorootparentprev1/ Buy a domain of your choice 2/ Register an account on Migadu.com and pay them $20/year 3/ Configure your domain nameserver with the settings provided by Migadu 4/ Done. reply drsim 14 hours agorootparentprevMany providers support plus addresses like bob+servicename@example.com. Servicename can be anything and doesn’t require any setup. reply duggan 12 hours agorootparentThe +, however is just a comment delimiter. All a service provider or malicious actor has to do is simply not include it when storing or publishing it to evade tracking. Stripping it is not uncommon for services to prevent duplicate accounts. reply ranger_danger 19 hours agorootparentprevHow do they get a hold of all these leaks so fast? reply Aachen 19 hours agorootparentVoluntary sharing, since afaik they don't pay the criminals to get the data. Either the criminals share it directly (fat chance, usually), or someone else bought it and shared it either publicly, privately with HIBP, or privately with someone who then reported it to HIBP How this specific instance unfolded, time will have to tell. The leak may have occurred in 2020 for all we know at this point reply steffanA 19 hours agorootparentThere is a strange dynamic between the threat actors who conduct these breaches and researchers. When not used for extortion and for \"status\" in the hacking community, they share them with researchers (commonly HIBP) to warn people about a site's security and so that site is forced to fix things. Definitely a strange dynamic. reply lazide 15 hours agorootparentA form of ‘counting coup’ I imagine. [https://en.m.wikipedia.org/wiki/Counting_coup] reply crtasm 18 hours agorootparentprev\"Breach date: 28 September 2024\" - I'm assuming they've checked with some recent signups to confirm the timeframe. https://haveibeenpwned.com/PwnedWebsites#InternetArchive reply Funes- 19 hours agoparentprevFriendly reminder to generate a unique password for every account you create so database leaks like this one don't bother you (besides on the site they're used). reply AStonesThrow 19 hours agorootparenthttps://xkcd.com/2176/ reply paulnpace 6 hours agorootparentI think pretty much the same argument for old-world POTS. While nothing was encrypted, nothing was recorded and someone had to physically access the local copper, which in reality provided more privacy than the future (today) where everything is recorded forever and you can bribe, extort, hack, blackmail, or just for fun leak everything recorded. reply voiper1 15 hours agorootparentprevI hadn't seen that one, I love it! reply JohnMakin 19 hours agorootparentprevMFA reply account42 8 hours agorootparent... is not something your should rely on. reply JohnMakin 2 hours agorootparent… but something you should do anyway. Having unique passwords isn’t something you should rely on either. Good MFA practices limits the impact of breaches like this. It isn't an either/or thing, do both. reply haha112 12 hours agorootparentprevI use login with google, idk if it is safe reply 999900000999 20 hours agoprevA pulled an old friends website down from Internet Archive. He's moved on the next stage, but I was glad I was able to put his site back up. It'll be a shame if IA goes down permanently, but we need a decentralized solution anyway. Having a single mega organization in charge of our collective heritage isn't a good idea. reply gabeio 20 hours agoparentI have always thought about this. It would be interesting to have users actually store small amounts of redundant info on a device connected to the internet. Very similarly to what a torrent does but with more peers (more data shards than full copies) and less seeds. And try and keep a huge database for everyone. Obviously open source and it would end up something like tor where they just assist the network with security patches but they don’t actually have any real “control” (admin dashboard control) over the network at large. We already do something smaller but like that with website static file caching, but at much smaller scale. Obviously security implications of this would be very hard but maybe not impossible to overcome. ipfs comes close but it again does more seeds then peers. if anyone knows something like what I'm suggesting, I'd love to hear about it! reply pbhjpbhj 19 hours agorootparentIIRC there were a few storage based projects that popped up using alt coins to encourage people to offer excess storage space for other randos on there internet. The possibility you might be storing illegal content might have been what killed it/them. https://en.wikipedia.org/wiki/Cooperative_storage_cloud gives a few examples, like Filecoin. reply IAmGraydon 11 minutes agorootparentprevAre you, by any chance, named Richard Hendricks? reply xyzsparetimexyz 19 hours agorootparentprevThe main issue that such hosting faces is that it's less efficient and more expensive than just regular centralized servers. reply 999900000999 16 hours agorootparentAnything would be better than the current system where you basically just have one source. Independently ran mirrors all over the world, along with snapshots. Have the occasional fork or two. Say your from a small town in Northern Illinois. If you have 2 TB of image archives from a defunct local newspaper, it might be good for photography forks even if it wouldn't make sense for the main archive. reply Geezus_42 15 hours agorootparentprevThis was a plot line in Silicon Valley. reply max-throat 2 hours agoparentprevThis is why BitTorrent and other P2P solutions were invented, but alas: A. The RIAA, MPAA, and ESA have given these technologies a terrible reputation. B. Nobody likes to seed. Some kind of seeding-based crypto would have been a great incentive if cryptocurrency wasn't also demonized by now. reply aucisson_masque 20 hours agoparentprevIt's called torrent protocol and it doesn't work, no one wants to spend money and bandwidth hosting a god forsaken movie or book that only a handful of people care about. reply squarefoot 19 hours agorootparentNot much money and bandwidth if you aren't on a metered connection. You can share tens of gigabytes or more on a cheap read only flash plugged into into a $25 single board computer that draws way less than a full PC and can be left sitting there near the router. Just limit its bandwidth on the torrent client and you won't even notice it during online gaming. The client can be as small as the Transmission daemon running headless on one of the many Debian based embedded distros: all control through either the web interface or from its client: no monitor, mouse, keyboard etc. just a small cheap box. https://www.friendlyelec.com/index.php?route=product/product... (just an example, as it's way overkill for the task) https://transmissionbt.com/ https://github.com/transmission-remote-gui/transgui reply oxygen_crisis 19 hours agorootparentprevI see 24 seeders for the entire 72-episode run of the 1991 sitcom \"Herman's Head\" which was so poorly rated that it's never seen a home media or streaming release, your premise doesn't hold any water at all. reply pessimizer 18 hours agorootparentPeople are pirating comic books and cookbooks from the 30s; there are a lot of people in this world, if something goes on the web and you tell everyone you put it there, it's pretty much preserved. It's only law enforcement that kills free availability of everything all the time online, for better or for worse. With copyright, as individuals we get to trade all of the wonderful stuff already made (and long paid for) for the flood of minute-old shit and sludge inundating us online constantly. It's a bad trade. Maybe copyright should stop encouraging creativity; the answer to how \"artists\" would get paid post-copyright might be \"who cares, quit if you want.\" We already have Herman's Head, we don't need any more crap. reply sgc 13 hours agorootparentI never thought about UBI and copyright - but as soon as you say that, it is immediately obvious to me that when we have some kind of UBI, copyright should be dramatically reduced. reply tourmalinetaco 13 hours agorootparentCopyright should be reduced in general. 20 years was already excessive for exclusive control over culture, 200 is just absurd. reply sgc 5 hours agorootparentI 100% agree. Just pointing out that UBI changes the discourse on this subject. reply 0x1ch 20 hours agorootparentprevIt does work, when you don't notice it. We need sane limits and permanent seeders. This is why so many regular people get hit with ISP notices, they don't know they've seeded Captain America for the last six months every time they started their PC. reply idle_zealot 19 hours agorootparentYup. If browsers built in support for magnet links and (on desktop) defaulted to seeding with some capped bandwidth then a lot of centralized hosting platforms would become unnecessary. reply kmeisthax 14 hours agorootparentYou can build something very similar with WebRTC. Browsers already have P2P networking capability, it's just not immediately interoperable with BitTorrent clients. Standardizing some sort of BitTorrent over WebRTC bridge and adding it to BT clients would fix this problem. That being said, please do not host content this way. P2P blows away the already thin privacy guarantees that the web provides. Anyone seeding the site gets the IP addresses of everyone on that site, and can trivially correlate that with other sites to build detailed dossiers on, if not individual people, at least households[0] of people. After all, that's how the MAFIAA[1] sent your ISP DMCA scare letters back in the 2000s P2P wars. [0] IPv4 CGNAT would frustrate this level of tracking, but IPv6 is still subnet per subscriber. Note that you can't use individual v6 addresses because we realized very early on that the whole \"put the MAC in the lower 64 bits of the address\" thing was also a privacy nightmare, so IPv6 hosts rotate addresses every hour or so. [1] Music And Film Industry Association of America, a ficticious merger of the MPAA and RIAA in a hoax article reply palata 9 hours agorootparent> You can build something very similar with WebRTC. Isn't that exactly what WebTorrent is? reply idle_zealot 11 hours agorootparentprevI hadn't considered the privacy implications. For this to be workable, you'd need to pair it with near-ubiquitous use of some anonymizing overlay network. reply geraldhh 1 hour agorootparentpreviirc opera browser tried that reply homebrewer 18 hours agorootparentprevI've been seeding some unpopular torrents for ten years (would have done for even longer if I did not change the torrent client a decade ago). \"No one\" is too strong a word, as usual with these absolutist things. reply trinix912 11 hours agorootparentprevIn addition to the costs, I'd say it's also that no one wants to risk getting sued like the IA is getting. reply Timber-6539 14 hours agorootparentprevIf the whole world has bandwidth available for TikTok, it can make the same available for sharing torrent files. reply account42 8 hours agoparentprevAgreed, especially an organziation that has already shown to not always be impartial. reply Simran-B 11 hours agoparentprevA decentralized solution, doesn't that scream internet archive on blockchain? What could go wrong. reply micromacrofoot 4 hours agorootparenttorrents maybe reply ewenjo 22 hours agoprevJust noticed the site now alerts this: > Have you ever felt like the Internet Archive runs on sticks and is constantly on the verge of suffering a catastrophic security breach? It just happened. See 31 million of you on HIBP! reply mewpmewp2 21 hours agoparentJokes on them... I'm already on HIBP countless of times... reply jsheard 21 hours agorootparentIt's all good, as long as you're not in that recent AI Girlfriend breach which exposed a ton of users who were trying to coax it into generating CSAM images. https://x.com/troyhunt/status/1843788319785939422 reply mrkramer 20 hours agorootparent“I went to the site to jerk off (to an adult scenario, to be clear) and noticed that it looked like it [the Muah.ai website] was put together pretty poorly,” the hacker told 404 Media. “It's basically a handful of open-source projects duct-taped together. I started poking around and found some vulnerabilities relatively quickly. At the start it was mostly just curiosity but I decided to contact you once I saw what was in the database.” What a nice guy. reply rpmisms 15 hours agorootparentTrue penetration testing. reply account42 8 hours agorootparentWell, only success with one kind. reply throwaway73583 16 hours agorootparentprevNot sure if you're being sarcastic or not, but pentesting is not a particularly evil activity — and you often have to look at data to see if you actually found something. What is evil is the way that he's ensured that the predators in the dataset will never face any consequences by making the data available to HaveIBeenPwned, making it trivial for predators to protect themselves (the method through which this is possible intentionally left as an exercise for the reader), and making the data available to a news website for...some reason, but it's bound to ensure that the vulnerability will be patched out quickly and no one else will be able to access the data. I find it much more likely that this hacker who sought out a website for uncensored AI erotica isn't actually a good guy, and might even have something to hide within the dataset. Hopefully, I'm wrong and we'll see more of this. reply urbandw311er 8 hours agorootparentDid you miss the joke? Parent poster means penetration as in penetrative sex reply lazide 15 hours agorootparentprevHow would that protect predators? reply nxobject 2 hours agorootparentprevAnd my SSN's probably available for purchase with 9 types of crypto, too. reply to-too-two 15 hours agorootparentprevI'm also on HIBP over 10x. What are we supposed to do? Create a new email address for every service we sign up for? I don't know what the best practice is for keeping our personal data safe anymore. reply perching_aix 11 hours agorootparent> Create a new email address for every service we sign up for? Exactly that, yes! Various services like icloud or proton offer \"hide-my-email\" addresses, or you can use any email service and just leverage a dedicated email aliasing service like SimpleLogin (paid but cheaper). This way your email addresses are always random, and since these are shared services, the fact that it's random doesn't identify you either. In proton's / simplelogin's case, you can even set the display name used and email first, so from the outside it's not going to appear as strange, or have any real limitations. If you think about it, modern email services don't really allow for easily testing if an email address is valid or not, so pretty much the only way your email is ever found out is if you share it on. So never share it on. Always share an alias instead. With automated systems, you may even want to rotate it every so often, so that if there's a leak, you can identify not just who leaked, but also roughly when. Fixed identifiers, like an email address, are terrible, as their lifetime is always significantly longer than whatever context they're being used in for. reply BobbyTables2 15 hours agorootparentprevUsing unique email addresses makes phishing attempts extremely obvious… (No, this official looking email from my bank is fake since it was sent to Grocery@my.domain …) reply wiredfool 10 hours agorootparentI get a ton of \"This is your email administrator -- your email password needs to be reset\" to github@mydomain reply account42 7 hours agorootparentHey at least after they fill your account up with spam they also send you warnings that you are running out of space. reply jmb99 11 hours agorootparentprevTruly unique email addresses and passwords per service is the strongest approach, but there may be alternatives. For instance, Gmail allows address+tag@gmail.com, which will save you from the lowest hanging fruit (block the +tag when it’s compromised to prevent the laziest spam from reaching you). iCloud also allows automatically generating a new email address that forwards to your inbox for a new account when using iCloud Keychain (possibly when using other password managers too, but I haven’t tried). reply DoctorDabadedoo 5 hours agorootparentGmail's +tag (and the .) is nice in theory, but terrible in practice. It's super easy for malicious actors to just drop them and there are a few services out there that simply are not able to work with the +tag, potentially getting you locked you out of your own account. Not gmail's fault, but I would recommend against using it. reply megous 39 minutes agorootparentprevYep. ~300 addresses on my domain, 0 breaches across all of them on HIBP domain search over >6 years. I guess internet security is not as bad these days. :) reply varenc 12 hours agorootparentprev> Create a new email address for every service we sign up for? Yes! Just get a domain and have every email it go to you. Mine is something like “@super-secure-no-viruses.email” reply account42 7 hours agorootparentThere are probably people that would sign up for such a mail. Like urlify.io and other similar URL \"shorteners\". reply lazide 15 hours agorootparentprevPassword manager + unique password per site + 2FA for anything of value. reply mendym 21 hours agoparentprevI assume that if this is a bad actor, then account email/name will be leaked? reply uticus 22 hours agoparentprevIs it a genuine alert, or hacking artifact? Sometimes with friendly / attempt-at-humorous error messages it’s difficult to tell reply jrochkind1 20 hours agorootparentI feel like it's safe to assume the official Internet Archive would not write a \"friendly\"/attempt-at-humurous/unprofessional/confusing/delivered-by-popup message advertising a devastating security breach. Oh also while announcing that nowhere else. Obv an attackers ability to insert a message does imply a breach beyond a DoS. But I am pretty confident that message was not from the IA. reply n_i_k_h_i_l 21 hours agorootparentprevIt's a literal window.alert() reply PLenz 21 hours agorootparentBut was that code placed there by IA or by the malicious party? reply abracadaniel 21 hours agorootparentVerge reports someone has taken credit for an ongoing DDOS against IA. \"An account on X called SN_Blackmeta said it was behind the attack and implied that another attack was planned for tomorrow\" https://www.theverge.com/2024/10/9/24266419/internet-archive... reply dang 21 hours agorootparentOk, let's switch to that link. Thanks! Submitted URL was https://archive.org/. reply silexia 7 hours agorootparentThe verge generally is clickbait, another site choice would have been better. reply varun_ch 10 hours agorootparentprevThis bad actor has videos of them supposedly “ddosing” Spotify by pinging 1.1.1.1 in two terminal windows on their Twitter. Is there any link between them and the real attack or are they just unrelated people claiming credit for it? reply seanw444 21 hours agorootparentprevSounds snarky to me. I'll bet it was the malicious party. reply whimsicalism 3 hours agorootparentprevit wouldn’t be a window.alert if it were IA reply EKSolutions 21 hours agoprevIt looks like someone has compromised one of their subdomains for Polyfill Update: Subdomain seems to be returning normal responses again now. reply Aachen 21 hours agoparentYou mean the IA included some JS polyfill from a subdomain and that's what's compromised / where the alert is coming from? reply mendym 21 hours agorootparentYup. https://news.ycombinator.com/item?id=41792651 reply qnsc 21 hours agorootparentprevyes, \"https://polyfill.archive.org/v3/polyfill.min.js?features=fet...\" is the URL with the malicious code reply Shadow1337 21 hours agorootparentIt looks like it is running the service that was part of the supply chain attacker earlier this year. https://github.com/polyfillpolyfill/polyfill-service/issues/... reply jsheard 21 hours agorootparentThe service was fine, it was the \"official\" hosted instance of the service which was compromised. IA appears to be running their own instance. reply abracadaniel 21 hours agorootparentprevThat was a DNS hack of polyfill.io though right? This looks like it was/is self hosted. reply __jonas 21 hours agorootparentprevYeah I'm getting this exact response from the above URL now: https://sourcegraph.com/github.com/polyfillpolyfill/polyfill... Seems like they self hosted that service reply EKSolutions 21 hours agorootparentprevCorrect. The source subdomain of the popup seems to be hxxps[:]//polyfill[.]archive[.]org reply jrochkind1 20 hours agoparentprevThat would perhaps explain how they managed to inject the JS alert popup, right? reply TZubiri 4 hours agorootparentYeah, but the leak has been confirmed by HIBP, I found my address in there. reply jrochkind1 2 hours agorootparentDOH. I hadn't heard this. reply EasyMark 13 hours agoprevOne of those instances when you really wish curses worked on whoever was pulling this stunt “may you and your descendants suffer the bites of 10000 fleas for 10000 nights as punishment for your misdeeds” reply iamtedd 11 hours agoprevI have had an IA account for a number of years, with a gmail address. Nine months ago, I changed the email address to a masked address using my own domain. Now I find that my gmail address was still stored, and was involved in the breach. Why? I get that they might store change history, but why? BTW, for the current account details, I changed the password to another random string generated by my password manager, and also deleted the masked email address and generated another one, so going forward this sort of thing isn't that much of an issue for me. reply keybpo 5 hours agoparentI have a similar situation, where I signed up with my main account and later changed IA's email to a more private address. It was the first email I checked on HaveIBeenPwned and it doesn't show up in this leak. The other couple IA accounts I have, whose emails and passwords are exclusive to them, they all show in this leak alright. I have no explanation to your situation but this was also my immediate though and I also wanted to give the opposite perspective. reply account42 8 hours agoparentprevIt's also possible that the breach was earlier or going on for longer than reported. reply PenguinRevolver 19 hours agoprevProbably not the best time to say this, but it's surprisingly easy to go through a collection with items and grab every email along with the usernames. https://archive.org/metadata/naturally_a_girl/metadata One way or another, there was going to be someone who would take loads of emails with a username attached to it. A bit intrigued by how the hacker compromised the database and got the passwords. reply fewgrehrehre 17 hours agoparentDamn, I had no idea about this. Definitely would've changed some things had I known that emails were public. This honestly seems like a bit of a design flaw. reply Gingeas 15 hours agoparentprevYeah, they have ignored everyone's concerns about the email thing. https://github.com/internetarchive/iaux/issues/892 reply pityJuke 21 hours agoprevThis thread is looking like it'll be one of the first places this incident will be documented (seems to be on the top of Google). Already there are two new users just for this. reply mendym 21 hours agoparenti see more than 2 reply ewenjo 21 hours agoparentprevYeah, I was looking around, but saw no mention of it anywhere until I realized it just happened. reply quart 21 hours agoparentprevnext [2 more] [flagged] quart 21 hours agorootparentnow internet archive is offline. uh-oh? reply EasyMark 13 hours agoprevThey use bcrypt and I always use a really long password so I’m not gonna freak out over this one for once. reply bjourne 9 hours agoparentAre bcrypt password hashes difficult to crack? I signed up for IA over 10 years ago with a much weaker password than those I use today. reply Tepix 7 hours agorootparentThe difficulty is configurable. You can play around with it at https://bcrypt-generator.com/ I found this, not sure if it's still up-to-date: ◉ PHP's default implementation of bcrypt uses 10 rounds. ◉ Python's bcrypt library uses 12 rounds by default. ◉ Node.js's bcrypt library uses 10 rounds by default. See also: https://gist.github.com/Chick3nman/32e662a5bb63bc4f51b847bb4... reply nicce 7 hours agorootparentprevIf you don't reuse that password anymore, does it matter tho. Some services might use older hashing for older passwords without updating the hash algorithm. But I don't know what is the case here. brypt passwords are very slow to crack. reply marviel 21 hours agoprevhttps://www.reddit.com/r/DataHoarder/comments/h02jl4/lets_sa... I found this reddit thread from /r/DataHoarder about backing up the internet archive particularly interesting, given the circumstances reply numpad0 20 hours agoparent50 PB * $0.014/GB = $0.7M. $0.014/GB is from[1], bare drive cost without chassis, power, or redundancy. 1: https://www.backblaze.com/blog/hard-drive-cost-per-gigabyte/ reply Aachen 20 hours agorootparentHow long does an average hard drive last? You'd have to spend that 700k every that many years (plus the extra bits you mentioned). Quite an operation actually reply everforward 19 hours agorootparentI actually find that fairly tame. For a point of comparison, Wikipedia gets ~$150M in revenue a year, an \"asset rise\" (I presume this is what non-profits call profit?) of ~$15M a year, and is sitting on about a quarter billion in the bank. Not that they want to, but I think Wikipedia could fund this using their current donations if they wanted. Hell, I almost wonder if one of the big storage providers would do it for free if they could do it in their staging environment so they get real traffic. It would be less good than real backups, but extra copies are still extra copies even if they're unreliable. reply Aachen 19 hours agorootparentYou're right, I guess it is tame and achievable so far as organisations go. I was imagining trying to get some friends together to have a decent percentage of the IA backed up, but that seems out of reach based on this napkin math. Not that that is necessarily demotivating, but it's going to depend on a lot of people intuitively seeing the value and keeping up their share reply Intralexical 18 hours agorootparentprevThey should probably consider it, really. A good portion of the text on Wikipedia relies on Wayback Machine links to remain verifiable. If they lose that, I guess the editors might have to comb every page for information which would need to be either resourced or deleted. reply jolmg 19 hours agorootparentprevFor archival, if you use tape, it comes out cheaper (~225k) and ought to last longer (~30 years). reply codemac 19 hours agorootparentprev> How long does an average hard drive last? This is a great question, and a state of the art kind of thing. HDDs are sold with a lifetime drive read/write amount and power cycle warranty, along with usually some environmental operating envelope. read/write relates to the quality/space of the platter, power cycle is usually the actuator & read/write head being reseated/wearing out. Environment is the same as all other devices in a DC. Most folks replace drives when they die (reads/writes stall or return garbage), or when the warranty runs out. Some will pay for a warranty exception, and some will just use the drive outside of warranty. Depending on how you use the drive, what environment it's in, etc changes how much you can push things. I'd say anywhere from 4-8 years, depending on how it's used. In many cases it can be cheaper to have a worse environment for your fleet (thus using less power on hvac) and replace devices more frequently. reply acaiblue44 16 hours agorootparentI'm a new user, is this a good time to plug my project that hopes to put 200 GB on a piece of paper? https://sourceforge.net/u/acaiblue44/blog/2024/09/gigapaper0... reply mdaniel 5 hours agorootparent> I learned that I can't use file i/o in a function outside the main, which is an unspoken rule that no tutorial elucidated. is for sure not true, that would be crazypants reply acaiblue44 1 hour agorootparentI tried for 6 weeks. Eventually, it just stops functioning. The same program and arguments spits out \"segmentation fault\" 33% of the time I run it, with the other 67% working perfectly. The only way I could explain it was that it was in a function outside the main, because when I put the exact same code in the main, compiled and ran, it worked. I have no other explanation. At some point, having too many nested loops and variables causes segmentation faults, whereas less complex code functioned without error. I needed to have certain things performed, and it only functioned in the main. reply remix2000 1 hour agorootparentprevIt's obviously just skill issue reply bigiain 19 hours agorootparentprevThere's also the fact that hard drive capacities keep increasing and increasing significantly faster that the power required, and sooner or later for very long term storage it'd become cheaper to migrate all your data from those 5 year old 4TB drives to more modern 16TB ones. That's assuming you want hot access to the data and don't plan on spinning them down as soon as you've written to them, like you'd do for a cold backup of the whole IA. I remember for a long time (I'm talking 20-ish years back here), every hard drive I bought had double or more the capacity of every drive I'd ever bought previously combined. My first ever 40MB (yes, megabyte) drive got upgraded to an 80MB one, that got updated to a 250MB one, then a 750MB, and then a whopping 2GB drive (how would I _ever_ fill that up???) - and so on. That's slowed down some, but I'm currently starting to think about upgrading my 8TB drives (Raid1 pair) with 20TB drives when the prices start to drop a bit more. reply ajxs 17 hours agorootparentprev> ...or when the warranty runs out. Do people really replace their drives when the warranty runs out? Hard drive manufacturers won't provide data recovery on drives that fail under warranty[1]. It makes more economical sense to just run a drive until it dies. You'll end up paying the price for a new drive either way, but less often if you ignore the warranty expiring. 1: I discovered this myself when a Seagate drive containing some important data failed under warranty. If you're foolish enough to send them a failed drive with data you need recovered (like I was), all they'll do is throw it in the bin and send you a replacement drive. reply bigiain 19 hours agorootparentprevIf this is a backup, you don't need it to be powered up and available 24x7. So the question becomes more like \"how long does an average hard drive last while powered down and still reliably be able to power back up and be read?\". I'm fairly sure that is a lot longer than the single digit years that'd be the probably answer to your question. I wonder if there are useful guidelines for long term storage of powered down hard drives? My gut feel is the major failure modes would be electrolytic capacitor failure, bearings sticking as the lubrication ages, and obseleting of the interfaces. I wonder how hard it'd be to find hardware that'd read my Mac SCSI hard drives from 25 years ago? reply fn-mote 19 hours agorootparent> I wonder how hard it'd be to find hardware that'd read my Mac SCSI hard drives from 25 years ago? Easy… that original Mac is sitting in my basement and it worked like a charm last time it was powered on 4 years ago. reply nephanth 19 hours agorootparentprevFor that purpose you might want to use magnetic tape like they use in long term archival services They are cheaper per Gio, and last significantly longer reply viraptor 19 hours agorootparentprevBackblaze keeps good stats. https://www.backblaze.com/cloud-storage/resources/hard-drive... 1.71% a year failure rate if you care for the hardware as much as they do. reply hsbauauvhabzb 18 hours agorootparentBut that number would increase year on year, a 10 year old drive is far more likely to fail than a 1 year old drive reply viraptor 18 hours agorootparentInternet archive is going for long enough that I'd expect it to stabilise by now. If you replace enough of the drivers, you get a good mix. reply tivert 19 hours agorootparentprev> How long does an average hard drive last? You'd have to spend that 700k every that many years (plus the extra bits you mentioned). Quite an operation actually You'd have to spend a lot more, because with that many drives, you need redundancy now. reply Aachen 19 hours agorootparentTrue, that would be an up front cost. At the same time, the IA is still live. This initial expense can be softened by building up redundancy over some years rather than trying to do everything at once reply tivert 19 hours agorootparent> True, that would be an up front cost. At the same time, the IA is still live. This initial expense can be softened by building up redundancy over some years rather than trying to do everything at once I think with that many drives, you'd be losing them constantly, and I suppose you wouldn't know which ones until later (assuming you're doing an offline backup, if you aren't you have to factor in power costs). reply PostOnce 16 hours agorootparentprevIA stores lots of redundant stuff in 5 file formats and none of them are particularly well-compressed, I think. There are (big) savings to be had, but maybe figuring that out (software dev and compute time) isn't worth it? reply ks2048 18 hours agorootparentprevInteresting to compare their stated drive $/GB to their B2 offering: $6/TB/mo for \"pay-as-you-go\", hard-drive price: $0.014/GB B2 price (12*6/1024): $0.070/GB/year reply alwayslikethis 18 hours agorootparentElectricity, bandwidth, and generally running a business is not free. Also for these pay-as-you-go setups you'd need a considerable amount of free space available on demand. That said, it's not an especially cheap option. Hetzner has storage boxes for EUR 2.5/TB/mo (in fixed 5 and 10TB boxes though) reply ks2048 17 hours agorootparentYeah, I wasn't trying to point out that it's a bad price. I think it's pretty good: same price for two years with all the maintenance. reply nikisweeting 20 hours agoparentprevIt's been tried several times, but it's hard because it's such a massive quantity of data. The IPFS backup never really got off the ground. They have their own backups which I think is good enough for now unless someone plans on donating a few hundred million. reply vincentpants 20 hours agorootparentOh no! I didn't know their IPFS initiative didn't pan out. What happened to it? I am surprised how hard it is to google. I remember interviewing for a role on that team at the archive to help move it to filecoin. Was so happy to hear that the effort was underway to decentralize their datastore. We need this more than ever. reply nikisweeting 19 hours agorootparentThere are people still working on trying to make it happen but it's just a collosal amount of data and filesystems are notoriously hard, so it's very slow going. From my own personal experience doing distributed archiving with no relation to Archive.org, Filecoin/IPFS's UX isn't quite there yet. They still don't let you serve data to the network from a normal filesystem, you have to let their system ingest all of your stuff so you end up double-storing data or you have to give into everything being stored as inscrutable binary blobs. That's why I still haven't integrated ArchiveBox with IPFS/Filecoin/Storj, let my data live in a normal filesystem dammit! reply Aachen 19 hours agorootparent> They still don't let you serve data to the network from a normal filesystem, you have to let their system ingest all of your stuff so you end up double-storing data or you have to give into everything being stored as inscrutable binary blobs. I don't understand this part. What data would you have to give them? Why can't it just live next to your stuff on your OS' filesystem? reply dannyobrien 18 hours agorootparentFor IPFS, I'm fairly sure you can now serve from your normal filesystem, rather than load it into their blockstorage -- or at least the blockstorage has pointers to real data blocks that are part of your existing files (it's the nocopy option[1]; it's marked as experimental, so there may be some sharp edges.) For Filecoin, if you want fast access, you do need to keep a second hot plaintext copy, as well as the sealed Filecoin copy. But that works for the backup case for IA, because the hot copy would be served from the archive's existing infrastructure (and/or a distributed IPFS hot cache) -- you'd just use Filecoin for the proven safe backup. The project to back up IA to Filecoin is still ongoing. The IA dashboard that shows the current state is (perhaps predictably) down at the moment, but it crossed the 1PiB line last year[2], and they've been optimising the onboarding flow recently. [1] https://docs.ipfs.tech/reference/kubo/cli/#ipfs-add [2] https://blog.archive.org/2023/10/20/celebrating-1-petabyte-o... (Disclosure: I work at the Filecoin Foundation/Filecoin Foundation for the Decentralized Web, which partners with the Archive on this project, as well as supporting other Internet Archive backup projects.) reply nikisweeting 18 hours agorootparentNeeding to keep a separate hot copy at 220PiB is already ~$7M/yr, and multiples much more than that if you factor in labor and redundancy. The --nocopy option looks great though, I didn't see it last time I was looking around for an MFS/FUSE solution, I'll try it. I appreciate your effort and I hope the project continues. reply nightpool 19 hours agorootparentprevThey're saying that the client software (the servers that speak the IPFS protocols) has to load the files to be served into their own local storage database, it can't just keep a \"metadata file\" and read the existing files off disk. Presumably somebody could write a client that spoke the IPFS protocol and did this, or fork the main Go or JS one, but until someone does that they're stuck with the software that's already been written reply pshc 18 hours agorootparentprevIPFS is all content-hash-addressed, so my guess is the IPFS service spirits the files away to a (hopefully) immutable store for the sake of sanity. reply pbhjpbhj 20 hours agorootparentprevPerhaps you can persuade Elon that it owns the libs? reply nikisweeting 19 hours agorootparentI don't want Elon anywhere near Archive.org, please don't give him any ideas. There are plenty of other people in the world with money. reply yard2010 17 hours agorootparentYes please, we need this lunatic out of our life, not the other way around reply bunabhucan 18 hours agorootparentprev\"Based on historical records from the first half of the last century, Mr Musk (inventor of the car and the rocket) and President Xi were the most respected and popular individuals on earth.\" reply gaudystead 15 hours agorootparentHistory is written by the winners... reply aryonoco 9 hours agorootparentMaybe in the immediate aftermath, but not long after. King Leopold \"won\" but we now all think he was terrible. reply creer 14 hours agoparentprevBackup / duplication is not an easy project for sure. But meanwhile for now IA is a single organization operating under one legal system. And one technical setup, would be relevant today. That's a major weakness. reply Aachen 20 hours agoprevA few minutes ago (22:48 UTC), I got three emails from HIBP about accounts of mine breached on the Internet Archive. Troy is quick! And I'm surprised the author of that alert() actually had the data as well as followed through Bit of a shame the emails contain an ad for a password manager, saying there's two easy steps to become more secure: Step 1: use our password manager (fair enough), \"Step 2: Enable 2 factor authentication and store the codes inside your [password manager]\" ehh now it's back to 1 factor or am I missing something? Edit: according to https://www.bleepingcomputer.com/news/security/internet-arch... (via https://news.ycombinator.com/item?id=41793669), Troy Hunt / HIBP already received and verified this \"three days ago\" as of yesterday 6pm AoE reply almyk 19 hours agoparentI think it is safer to have 2FA in your password manager than not using 2FA at all. Because even if they got your password, if they don't have access to your password manager they can't login. If you protect your password manager with a yubikey or any other hardware key, then your 2FA inside your password manager is quite secure and convenient. But this is very individual, what your threat model is and how secure you want/need to be. reply Aachen 9 hours agorootparentSee also the considerations mentioned in the sibling thread btw: https://news.ycombinator.com/item?id=41793846 > even if they got your password, if they don't have access to your password manager they can't login. Wouldn't the same argument go for a non-2fa password? What's the difference between a randomly generated 2fa secret and a randomly generated password here? reply account42 7 hours agorootparentAn eavesdropper able to intercept connections could record your password in transit but would only get the current 2FA token which quickly becomes useless. But with TLS eavesdroppers are not a realistic concern for most people so the actual benefit is still questionable. reply nixosbestos 19 hours agoparentprevI was going to disagree with you (and I sort of do about password managers and storing 2FA in them, but I also unlock my password manager with a yubikey). But, doesn't a DB compromise mean that the attacker would have the TOTP seed as well? It can only increase your account security elsewhere, but also not re-using password prevents the IA leak from hurting you elsewhere as well? reply Aachen 19 hours agorootparent> I was going to disagree with you (and I sort of do about password managers and storing 2FA in them Note I'm quoting HIBP's advice from the email they've sent me! I'm absolutely not recommending to store one's 2FA secrets in the same place as the password! Even if one uses 2FA for the password manager, it stops proving \"something you have\" in addition to something you know and you're one unlock away from malware vacuuming it all up. The point of 2FA is to be on a separate device you need to have on hand Of course, the same logic goes for a password manager in the first place, but password reuse is a big enough problem that (for most people's threat model) it seems to be a net positive. 2FA tokens don't have that reuse issue reply tkgally 17 hours agoprevAs of 01:09 GMT on October 10, the Internet Archive is back up. In fact, the Wayback Machine and the book archives are responding more quickly than they did for me a week ago, when I showed the Archive to the students in an online class I teach. I gave the students a homework assignment that involves accessing some old books at the Archive. That assignment is due in about 12 hours, and I was just getting ready to e-mail the students about the outage when I saw that the site is working again. reply divbzero 10 hours agoparentAs of 08:34 GMT on October 10, the Internet Archive is down again. reply tkgally 9 hours agorootparentThanks. I e-mailed my students to let them know. reply lordfrito 18 hours agoprevConfused about this breach... I received a notification from HIBP about this hack, but I don't recall ever creating an account on archive.org (was creating an account there even a thing?). What info does archive.org have on people? Is this info scraped from other websites and stored in the archive.org database? Or is this info related to personal archive.org accounts (as I said I don't recall making an account)? reply floam 18 hours agoparentThey are actual archive.org accounts. Maybe you made an account to upload something, or to check out a digitized book from their library? reply lordfrito 18 hours agorootparentThank you.. was worried at first as I didn't understand the true scope of the breach. For such a vital website, the info gleaned seems relatively harmless (for those of us who don't reuse passwords that is) reply db48x 17 hours agorootparentYea, it is pretty harmless. I suppose someone might be interested in any books you currently have checked out, but beyond that there isn't much. reply Nathans220 21 hours agoprevWhy go for the Internet Archive go for something else not the fucking archive! reply mewpmewp2 21 hours agoparentWe all need our easily accessible decentralized archive of some sort... reply alkonaut 2 hours agoprevDoes IA have much information on users? I’ve been in dozens of these HIBP leaks (including this one) but still none have concerned me, since they were mostly just email/password and nothing else. Does IA store anything sensitive for any users?p physical addresses, credit cards, etc? reply meindnoch 8 hours agoprevHow much of the archive is affected? Could be a targeted effort to tamper with historical records. reply AdmiralAsshat 21 hours agoprevWell this should be fun. Now I'll have to dig through my IA account and remember if I donated to them directly via credit card (and if they stored it), or if it was through PayPal. reply paxys 19 hours agoparentEven if you paid by credit card, there's zero chance they processed the payment themselves. reply zelse 20 hours agoparentprevHaveIbeenpwnd says it was just passwords/usernames/emails, so seemingly not. (My company just got an email from them about the breach and I confirmed I'm in there with a quick search on their website.) reply bigiain 19 hours agorootparentThat's what Troy got sent. It's not necessarily all the attacker took. reply gaudystead 20 hours agoparentprevGood point and thank you for the reminder. Time to go check my email archives... reply KerrAvon 20 hours agorootparentthey use Stripe reply steve_taylor 20 hours agorootparentIf you're a blackhat and you want to be annoying, you can use Stripe tokens to charge your target's customers. The target is the payee, so you won't make any money, but it'll add to the chaos. reply account42 7 hours agoparentprevIf they stored your email from your donation the IA would have already used it to spam you themselves, no attackers needed. reply pentagrama 18 hours agoprevThe reported alert on the site states: > Have you ever felt like the Internet Archive runs on sticks and is constantly on the verge of suffering a catastrophic security breach? It just happened. See 31 million of you on HIBP! But is this an official message from the company? It sounds odd and unprofessional, especially the \"See 31 million of you on HIBP!\" part, which jokingly refers to a huge privacy issue for users. Could it also be that the site was hacked, with hackers posting that message in addition to the data breach and DDoS attack? reply andrelaszlo 18 hours agoparentTroy Hunt's tweet mentions the IA getting breached, defaced AND DDoSed. Here it is, in case you don't want to use that site: >>> Let me share more on the chronology of this: 30 Sep: Someone sends me the breach, but I'm travelling and didn't realise the significance 5 Oct: I get a chance to look at it - whoa! 6 Oct: I get in contact with someone at IA and send the data, advising it's our goal to load within 72 hours 7 Oct: They confirm and I ask for a disclosure notice 8 Oct: I follow up on the disclosure notice and advise we'll load tomorrow 9 Oct: They get defaced and DDoS'd, right as the data is loading into HIBP The timing on the last point seems to be entirely coincidental. It may also be multiple parties involved and when we're talking breach + defacement + DDoS, it's clearly not just one attack.The timing on the last point seems to be entirely coincidental. It may also be multiple parties involved and when we're talking breach + defacement + DDoS, it's clearly not just one attack. It could also be that the attacker has compromised IA communication channels and timed it for maximum dramatic effect and confusion. reply 1oooqooq 13 hours agorootparentvery likely compromised comms. this was coordinated. several archive services hit around the same date. ddosecrets was the first to be hacked, as far as i can tell. span of one week. here's list of suspects i guess https://en.m.wikipedia.org/wiki/List_of_material_published_b... reply gtirloni 18 hours agoparentprevIt's a thankless job to be always begging for donations to keep something working when the Internet at large doesn't value it as much as it should. And now getting targeted like that? I wouldn't judge them if this is an official communication coming from exhausted and frustrated staff. reply appendix-rock 18 hours agorootparentJust a reminder that AI tried pivoting to much more clear-cut legitimate piracy, presumably because they got bored or something, and certainly put ‘donations’ toward that effort. IA is an incredibly valuable resource, but let’s not put them on a pedestal. reply colinsane 18 hours agorootparentheh, if they went 100% \"we're operating our service from international waters and won't be taking any DMCA requests\" i would donate $1000 on the spot (anonymously, of course, but entirely serious). reply Nemo_bis 13 hours agorootparentprevWhat's \"legitimate piracy\"? As a reminder, the scheme was designed to work exactly like typical lending libraries. Publishers were unable to show any harm, and the only evidence available proved they actually benefited from better sales thanks to the Internet Archive. Authors were clearly benefited. https://www.techdirt.com/2024/09/05/second-circuit-says-libr... But I agree, no need to put them on a pedestal. Nobody is perfect. reply dewey 2 hours agorootparent> As a reminder, the scheme was designed to work exactly like typical lending libraries. Wasn't the issue precisely that they removed that limitation and then never added it again? reply nostromo 18 hours agoparentprevThe hackers wrote that. https://www.bleepingcomputer.com/news/security/internet-arch... reply internetter 18 hours agoparentprevThe alert is gone now. It appears the attacker compromised their front end deployment reply driver8_ 16 hours agoprevThat sucks, I was reading my email in the morn and saw the news from haveibeenpwned.com, and I'm indeed effected by it. Consolation is that I used a randomly generated unique password, tried to reset my credentials and see of any 2FA options but the site is overloaded throwing 504s. reply left-struck 12 hours agoparentI’ve been mentioning this a lot lately but it’s also a good idea to use email forwarding services like Firefox relay, icloud/apple “hide my email”, duckduckgo has a free one, simplelogin you can host yourself… In an email breach you can confirm who was breached if you used a unique email, and it also means your actual email remains at least as secure as those services I mentioned reply account42 10 hours agoprevGood. Maybe this will get them to reconsider their website changes that make the IA unusable without javascript. reply Aachen 21 hours agoprevShould we be linking to the site that is very likely to be breached? Could start to host any type of malware until the access can be definitively revoked reply btown 21 hours agoparentThis - dang/mods is there a policy for this? reply abracadaniel 21 hours agorootparentVerge article as possible replacement: https://www.theverge.com/2024/10/9/24266419/internet-archive... reply dang 16 hours agorootparenthttps://news.ycombinator.com/item?id=41793081 reply RGamma 21 hours agoprevLet's hope it was someone dumb enough to be extraditable. reply popcalc 20 hours agoparentNo one gets extradited when the attack aligns with US interests abroad. reply bawolff 20 hours agorootparentWhat weird conspiracy is this? US interests dont involve taking down archive.org reply markus_zhang 20 hours agorootparentThere is no US, there are just a bunch of interest groups. Some interest group definitely wants IA down. I wouldn't be surprised this is a paid attack. reply bigiain 19 hours agorootparentI'd probably believe attribution to either Israel or the MPA with only a little evidence. (I still haven't forgiven Sony for the album on CD I bought with a rootkit on it...) reply markus_zhang 6 hours agorootparentJust curious why Israel? MPA is reasonable though... And a rootkit on CD? Interesting... reply mrguyorama 1 hour agorootparent>https://en.wikipedia.org/wiki/Sony_BMG_copy_protection_rootk... The bad old days before music companies just gave up and started selling un-DRMd mp3 files, and then Spotify solved THAT problem for them. reply LinuxBender 20 hours agorootparentprevJust for completeness sake and my own opinion based on my own witnessing of history, every political party of every government of every country would love to see all the archives gone. It's easier to twist the truth if one can memory hole reports and make the original source go offline or pressure them to change their words. There will always be individuals that archive stories they find interesting, but many stories are uninteresting until people learn what more may have been left out at a later time as part of a much bigger story. That is when the archives become a treasure trove and big archives sites are the first that people turn to for the original reporting. As a generic example, many news sites will redact what they knew to be false after the vast majority saw their misinformation but they can't redact an archive of their twisted truth. The internet has made it a little harder to control a narrative. It was so much easier to control when it was just a few big newspaper publishers that owned the smaller ones and a few big cable companies that owned most of the smaller ones. They would all literally parrot the same lines. Curious to see if they go after archive.is next. reply jrochkind1 20 hours agorootparentprevPeople in other parts of the thread say it's Israel. (Which certainly is \"aligned with US interests abroad\", as the powerful see it anyway). I think it is ridiculous conspiracism, right now anything anyone doesn't like they think Israel is behind it. The crazy rise of conspiracism in our society in general, combined with Israel really is doing some nasty stuff (but not controlling everything you don't like), combined with the latent antisemitism in most conspiracism. And I say this as a strong supporter of and activist on Palestinian rights and liberation. Free Palestine. (But there is no reasonable reason to think Israel is behind an IA hack. Or the fact that your mail came late, or anything else except what they're actually doing which is bad enough. Call your senators and tell them to vote for Bernie's JRD resolutions). reply tdeck 19 hours agorootparentThere are so many well documented awful things IL has done that most people don't know about (many still haven't even heard of the Sde Teiman video) that folks could be spreading the word about instead. It's a shame to see this kind of conspiracy mindset from at least some people who probably mean well. There is no harm in waiting a little bit for facts to emerge. reply jl6 7 hours agoprevDoes the IA publish hashes of its data to a 3rd party, so we could (in principle) verify that nothing has been tampered with? reply n3uman 20 hours agoprevhttps://blog.archive.org/2021/02/04/thank-you-ubuntu-and-lin... \"The Internet Archive is wholly dependent on Ubuntu and the Linux communities that create a reliable, free (as in beer), free (as in speech), rapidly evolving operating system. It is hard to overestimate how important that is to creating services such as the Internet Archive.\" Maybe CUPS? reply adfm 21 hours agoprevThey're hiring, if you're looking for a job. https://www.indeed.com/viewjob?jk=3bb8222ccd9a88ea reply Aachen 20 hours agoparent> Software Engineer, Archiving & Data Services (Remote) [...] Preliminary duties of the role will primarily focus on developing Archive-It That is. Paying over 100k at the lower end of the range for 3y experience as software engineer reply jjice 20 hours agorootparentIt's a non profit. You're probably not choosing to work for the IA for high compensation. reply Aachen 20 hours agorootparentThe undertone was intended to be: that's an insane amount of money, something one with quadruple that amount of experience would maybe earn in a for-profit organisation, but I guess your reaction further proves it's different where you're from reply AlotOfReading 15 hours agorootparentTo put the reaction into context, the individual low income threshold in that area is 105k USD [0]. [0] https://www.hcd.ca.gov/sites/default/files/docs/grants-and-f... reply rsynnott 7 hours agorootparentWhat area, precisely, is '(Remote)'? Why does the Californian government track income information on Remotistan? reply metadat 19 hours agorootparentprevThe way you worded it was confusing to read, I thought it was a complaint about \"only 100k\". Thanks for clarifying your intent. reply worstspotgain 18 hours agorootparentprevThe IA is located in the Inner Richmond, which is a ~ medium income area of SF. Rent alone is ~ $4K, or ~ $60K of your income before taxes. reply tlavoie 15 hours agorootparentThey might be there, but the position was remote-friendly. reply tdeck 19 hours agorootparentprevIt's not high for bay area software jobs; there are new grads who were paid more than that 10 years ago and I assume new grad wages have gone up since. Of course cost of living (particularly rent) and taxes are high there too, but if you don't blow it all on renting a higher-end place or luxuries you can still save a lot. For context someone making less than $105k is classified as \"low income\" in San Francisco. https://www.sfgate.com/local/article/under-100k-low-income-s... reply adfm 20 hours agorootparentprevNot even in the 10th % for the area per https://www.levels.fyi/heatmap/ reply Banditoz 14 hours agorootparentIt says it's remote though, so doesn't seem too bad? reply EasyMark 13 hours agorootparentRight in most southern states in rural areas that would be pretty good and you could enjoy fresh air and nature while working from your back porch and scanning a few acres of land and wildlife, sipping on sweet tea. reply bawolff 20 hours agoprevReporting on security issues is always so terrible. Is it a data breach or is it a DDoS? (Or both). Those are opposite things. One is trying to release secret information one is trying to make the site inaccessible. reply odo1242 20 hours agoparentIt is both. They got attacked by a DDOS after the security breach. reply treesknees 19 hours agorootparentWhich is pretty common. While the org is running around dealing with the DDoS, they're not doing anything to fix their systems. In this case, I can't even get to my account page on IA to change my password. reply Aachen 20 hours agoparentprevThat's like complaining the reporting on the weather forecast channel is so often wrong. This news broke about an hour ago and the IA is down, what witchcraft do you expect news media to practice! Nobody yet has the answers you're looking for, give it some time and log files will be audited and the reporting becomes useful :) reply bawolff 20 hours agorootparentActually figure out what is happening, or at least say how confident they are in what they know. They aren't predicting the future, they are reporting on an ongoing event. reply Aachen 20 hours agorootparent> or at least say how confident they are in what they know This I can very much underwrite. Error bars or rough confidence indicators are missing far too often, also from sites reporting on e.g. benchmark values of hardware they've been testing... such professional organisations yet such basic omissions reply Mr-Hyde 21 hours agoprevhttps://x.com/Sn_darkmeta/status/1844080692772401399?t=j3xDz... Annoying reply Aeolun 20 hours agoparentWhat are they looking for here? Negative karma? reply Mr-Hyde 20 hours agorootparentnext [4 more] [flagged] adastra22 20 hours agorootparentThis makes absolutely no sense. reply steve_taylor 20 hours agorootparentIt makes sense when you look at the age of the commenter's account. reply dumpsterdiver 20 hours agorootparentprevBy \"working idea\" do you mean something that you made up in your head which has no basis in reality, but works for you? Edit: I had only seen the one post on X in which responsibility for the attack was claimed when I made this comment, but looking at the account further they do make many politically motivated comments. With this new insight my comment now seems unnecessarily dismissive because it's not completely unreasonable to suspect false flag attacks when political motivations are being broadcast. To be clear I'm not making any assumptions for this specific case one way or the other, but I am acknowledging that the political speech presented by the attackers does add some merit to your suspicion. reply navigate8310 20 hours agorootparentprevProbably want it wants to purge incriminating documents against a nation state? reply xyst 19 hours agoprevOne of the many benefits of owning my own email server: - I have a catch all setup to forward all emails to specific user on mail server - able to setup adhoc email addresses for each online service (ie, iarch@example.com) - able to claim example.com in haveibeenpwned Now I get breach emails from hibp for the whole domain. Unfortunately, I was exposed in this IA breach reply lolinder 18 hours agoparentIn case anyone would like these benefits but doesn't want to actually run an email server: All you actually need to accomplish this is a domain name and a decent provider. Fastmail is what I use and it's been great for me. reply halJordan 18 hours agorootparentTo be even easier, you can just have Apple or Google hold your domain and provide mail. reply swatcoder 17 hours agorootparentThat's not easier, that's the same but with a worse scale fit. If you need free, you need free. But if you can pay, you want to pay a vendor whose scale is such that you mean something to them while still being mature enough to rely on. This applies to pretty much everything, not just email. With Google and Apple, you service needs are overhead and with Google in particular, your value is entirely in them being able to monitor as much as they legally can about your activity. With Fastmail, Protonmail, etc, you are a customer already and they're invested in making you a bigger happy cuatomer in the future. They have staff that will service your support tickets, you represent profit on their books, and the services they offer you are generally designed for your scale more precisely. reply hackernewds 16 hours agorootparentThey mean getting a Gmail account reply layer8 17 hours agorootparentprevIt’s risky to let your online identity be controlled by a single large provider. Distribute out the services you use as much as possible. Use a different email provider from your domain registrar, and different from the providers of any other online account you have. reply lolinder 17 ho",
    "originSummary": [
      "The Internet Archive experienced a security breach, compromising information for 31 million accounts, including email addresses and other data.",
      "The breach was confirmed by Have I Been Pwned? (HIBP), with 54% of the affected accounts already present in its database from previous incidents.",
      "In addition to the breach, the Internet Archive faced a Distributed Denial of Service (DDoS) attack, leading to service disruptions, and is now working on upgrading its security measures."
    ],
    "commentSummary": [
      "The Internet Archive suffered a security breach, exposing 31 million records, including users' email addresses, which were unknowingly public in metadata.- This incident highlights concerns about email privacy, with discussions on whether email addresses should be treated as private information like phone numbers or home addresses.- The breach is now listed in the Have I Been Pwned database, and users are advised to use unique email addresses for each service and consider email aliasing for enhanced privacy and spam reduction."
    ],
    "points": 1009,
    "commentCount": 545,
    "retryCount": 0,
    "time": 1728507266
  },
  {
    "id": 41797719,
    "title": "Helping wikis move away from Fandom",
    "originLink": "https://weirdgloop.org/blog/why-were-helping-more-wikis-move-away-from-fandom",
    "originBody": "← Blog Why we're helping more wikis move away from Fandom 10 October 2024 · Jonathan Lee Hi! You may have seen that Weird Gloop is now hosting the official League of Legends Wiki. We’ve spent the last couple months working with the Riot folks and the League wiki editors to move it off of Fandom, and turn it into something the players will (hopefully!) really dig. I also love that it got started because one of the Riot guys plays a ton of Old School RuneScape and thinks our wiki is awesome. How cool is that?? I want this to kick off a new era where communities and developers take control from Fandom, and make some really great wikis. We’ve already been doing a bit of this, starting when we helped the Minecraft Wiki leave Fandom, but I think it’s time for me (and the rest of our group) to be more explicit about what we want to do. So if you’re any of these things: A frustrated wiki editor trying to figure out your options A community manager trying to get internal support for an official wiki Someone contemplating making a new wiki I will give you (free, very specific) advice on how to get your wiki off Fandom, and make a kickass wiki somewhere else. We might even be able to host you ourselves. If you think this sounds cool, come talk to me. Why do we actually care? Why ditching Fandom is cool and based What I’m offering How to not turn into Fandom 2.0 (with these 2 simple tricks) Point 1 - wiki communities need to be able to freely leave their host Point 2 - global branding is extremely negative value for wiki farms Why do we actually care? This post (and many others) have done a much better job than I could, explaining from a reader’s perspective why Fandom is bad place to host a wiki, but I thought it might be useful to give my take on it as a long-time wiki editor. I love wikis. I think it’s unbelievably cool that this completely insane idea (“what if we just had a website that anyone can edit?”) doesn’t descend into anarchy, and instead self-organizes into a fun, project-oriented community. I think that despite its flaws, Wikipedia is the single coolest thing the internet has ever done. And wikis on niche topics feel like some of the last remnants of a friendlier, more collaborative, early 2000s web. I loved contributing to wikis, building something with other people, and feeling a sense of ownership (and pride that so many people were using stuff I made). Which is why it’s so concerning that Fandom has taken this wonderful concept and turned it into one of the most dreadful parts of the internet. Being deeply involved with the RuneScape Wiki on Fandom had a huge psychological cost – what wonderful thing did they add today that made our wiki harder to use? Scammy green link ads? Comically bad videos on the top of our most popular pages? Garbage AI-generated Q&A? Ads that take up literally 100% of the content window? I (and so many others) had spent countless nights trying to make the best possible resource for RuneScape, and it was brutal to realize that it didn’t matter how hard we worked or creative we were – our wiki was never gonna be that great, because Fandom was in charge. That sense of ownership and pride…slowly turned into feeling like my passion was being exploited by a company that didn’t want the same things I did. We weren’t the only ones feeling this way, of course – some wiki communities got fed up and moved somewhere independent. But here’s the key thing you need to understand: even when a wiki community unanimously wants to leave, Fandom keeps their copy of the wiki up, even though it no longer has a community. Google remembers years of people searching, linking, and visiting the Fandom wiki URLs, and continues to rank the increasingly stale Fandom results first. Since roughly 85% of a wiki’s traffic comes from Google, it’s nearly impossible for the new wiki to win without fixing this ranking disparity. It’s an extremely draining thing to do – nobody likes to spend their waking hours competing against the thing they helped lovingly build. Historically, independent wikis have had an extremely hard time winning this battle. Most of the traffic stayed on the Fandom wiki, and the independent wikis often fizzled out. This had a chilling effect on the remaining communities, and emboldened Fandom to further prioritize revenue extraction. That’s the key takeaway: if leaving Fandom was easy, they wouldn’t be able to enshittify as much as they have. But don’t lose hope! Google has gotten much friendlier to independent wikis over the last decade. With a large, sustained effort, we were able to recover 95% of RuneScape Wiki traffic within the first year. Why ditching Fandom is cool and based The main advantage of leaving Fandom is likely clear to anyone who’s ever visited one of their wikis without an ad blocker. But there’s more than that! When you have a site that people are happy to go to (instead of something they’re forced to grimace and use), you get all these wonderful secondary effects that are worth mentioning. For starters: on average, moving away from Fandom doubles the number of people editing. I’ve seen the pattern across every wiki we’ve ever moved off Fandom, but here’s a pretty striking graph from OSRS Wiki: It’s incredibly consistent: way more people show up and want to help, when they feel like they’re contributing to something that isn’t taking advantage of them. It’s not a coincidence that OSRS Wiki got really good once we left Fandom in 2018. Once we had way more people wanting to contribute (and the only objective was “make the best possible wiki for the game”) the wiki magically got way better! Crazy! Departing from Fandom has also opened the door for a number of custom technical projects that otherwise would have been downright impossible to implement on the old wiki. In-game item lookup, WikiSync and real-time prices are core parts of our offering now, with hundreds of thousands of users. They’re all made possible by the new flexibility we gained when we took control of the hosting. What I’m offering I think a lot of people would love to get their wiki off Fandom, but it’s extremely not obvious what that even involves, so it’s hard to formulate a plan. I will help you figure out a viable, detailed strategy for you to get your wiki off Fandom, and bring the traffic along. In the next couple weeks, we’ll be posting some general advice on this blog that goes through the main steps and pitfalls involved with leaving Fandom. Most of it should be broadly applicable, but the real power comes from looking at the specifics of your topic (how big is it? does it change frequently? is it a game? are you the rights-holder?) and tailoring the plan to fit. As far as where you host it…there’s plenty of decent options. Wiki hosting is not nearly as hard as Fandom makes it out to be – for example, if you’re the Path of Exile devs and you already host a bunch of PHP web stuff, then hosting the wiki yourself is objectively a really good option. Sometimes Weird Gloop will be the good option for your situation, and being totally honest, sometimes it won’t be. And that’s okay! I want to help communities get away from Fandom, regardless of who’s running the servers. I will say, I don’t think we would ever do a “self-service” thing where you could just sign up and immediately make a wiki. We want to do projects where we get to know the community, and closely support every wiki we host. How to not turn into Fandom 2.0 (with these 2 simple tricks) As we’ve started hosting more wikis besides RuneScape, some people have asked a pretty reasonable question: what’s stopping us from eventually getting enshittified, just like Fandom (or the other wiki farms that eventually sold to Fandom)? From my perspective, there are two key choices that Fandom made that have had major negative consequences for communities. And we’re just going to do the exact opposite on both points. Point 1 - wiki communities need to be able to freely leave their host You can probably tell that I think wiki editors (as opposed to hosts) are the ones who create the vast majority of the value on a wiki. So the premise is simple: If a wiki community is unhappy, and they have a better option somewhere else, they should be able to leave and take their stuff with them. We won’t prop up the old wiki, Weekend-at-Bernie’s style, abusing the dominant Google position that the wiki editors built up while they were on our platform. In my opinion, this is really the only rule that matters. If you have the ability to leave (and take your revenue-driving wiki with you) when things go to shit, then your host has an extremely strong incentive to not let things completely go to shit. There’s a long history of wiki farms vaguely handwaving that they’d agree to something like this, and then backtracking later. So why believe us? It helps that Weird Gloop literally only exists because we were on the losing end of this sort of situation with Fandom back in 2018, and that we have no outside investors or debt (the company’s owned by wiki nerds)…but I don’t think that’s convincing enough on its own. So we’ve been voluntarily entering into agreements with the wikis we host (here’s an example) where we set very clear obligations for what happens if the wiki community wants to go somewhere else (hint: it’s all about the domain). If we ever start going down the same path as Fandom, everyone can just leave! I would love to see other wiki platforms start to do this, because I think it’s the only way you really solve the problem. Point 2 - global branding is extremely negative value for wiki farms If you go to any page on a Fandom wiki, even if you’ve got an ad blocker…you’ll be greeted by an absurd amount of Fandom-related branding: a gaudy sidebar that links to Fan Central (whatever that is), a bunch of other links to wikis that aren’t relevant to you, buttons to follow Fandom on Instagram, TikTok, to take “Fan Quizzes”. The brand strategy seems like it was cooked up by a bunch of market researchers who think that people are fans of…media properties in general? It’s super cringey and totally irrelevant to the people who are on Fandom wiki to, say, look up the stats of a new pickaxe they got. It’s easy to laugh about how bad the branding and identity is, but there’s a bigger issue: the fact that it’s so overwhelmingly branded as “Fandom” (as opposed to, say, the Warframe Wiki) makes it way harder for each of the individual wikis to develop an public identity, because anything they do will be subordinate to the (very loud) global brand. These individual wikis are the only popular thing that Fandom has ever operated, and the focus on global branding makes each individual wiki worse. Our position: the actual wikis should be front and center, because it’s way more important for the wiki itself to have a great reputation, rather than sucking all the oxygen out to make sure people know who owns the platform. We have extremely minimal branding (can you even find it?), and I can’t imagine ever trying to put wikis on subdomains of weirdgloop.org (or anywhere else) unless there were no decent domain options. We don’t actually get anything out of everyday readers knowing who we are. That’s all I’ve got right now. If you liked this and want to talk to me about wiki things, please come say hi – it doesn’t matter if you have a big wiki or a small wiki (or no wiki at all!) – I really just love talking to people about this stuff. Blog · Licensing · Terms of Use · Privacy · Meta Wiki · Contact",
    "commentLink": "https://news.ycombinator.com/item?id=41797719",
    "commentBody": "Helping wikis move away from Fandom (weirdgloop.org)673 points by creatonez 7 hours agohidepastfavorite314 comments citricsquid 4 hours agoAs the person ultimately responsible for the Minecraft Wiki ending up in the hands of Fandom, it is great to see what Weird Gloop (and similar) are achieving. At the time of selling out, the Minecraft Wiki and Minecraft Forum cost tens of thousands of dollars per month to run and so it didn't feel too much like selling out, because we needed money to survive[1]. 15 years later, the internet is a different place, and with the availability of Cloudflare, running high-traffic websites is much more cost effective. If I could do things over again, on today's internet, I like to believe Weird Gloop is the type of organisation we would have built rather than ending up inside Fandom's machine. I guess that's all to say: thank you Weird Gloop for achieving what we couldn't (and sorry to all who have suffered Fandom when reading about Minecraft over the years). [1] That's a bit of a cop out, we did have options, the decision to sell was mostly driven by me being a dumb kid. In hindsight, we could have achieved independent sustainability, it was just far beyond what my tiny little mind could imagine. reply Svip 3 hours agoparentI was approached about a decade ago to combine The Infosphere with then Wikia's Futurama wiki. I asked it was possible to do a no-ads version of the wiki, and while initially they seemed like that might be possible, they eventually said no, and so we said no. So now there are two Futurama wikis online. I still host The Infosphere, haven't checked the Fandom one in years. Fortunately for me, Futurama isn't as popular as Minecraft (for some reason!), so I've been able to pay out of my own pocket. reply Svip 2 hours agorootparentA bit of a follow up to this; after a bit of thought, I am considering reaching out to Weird Gloop. I do not feel I am able to give The Infosphere the care that it deserves. And with Futurama back on Hulu, we are naturally seeing an uptick in activity. We have a very restrictive sign up in place, because I don't have time to moderate it anymore. It keeps the spam down, yes, but also new users away. Note: The reason I'm writing I'm _considering_ reaching out and not just straight up reaching out is because the domain itself has a different owner than me, and I want to make sure they are also approving of this decision. reply babypuncher 21 minutes agorootparentprevThe Infosphere has always been one of the best fan wikis out there, thank you for your hard work (and for not selling out to Fandom) reply ryukoposting 4 hours agoparentprevI remember reading the Minecraft wiki back in the early 2010s, back when Fandom was still Wikia. It would have been much more appealing at the time than it is today - not just for the reasons you list, but because Wikia actually kicked ass in the early 2010s. It was sleek, modern, and easy to use. And today, it isn't. reply epiccoleman 4 hours agorootparentEvery time I wind up on some garbage Fandom page I reminisce about the good old days of Wikia. I remember many a fun night trawling through pages while playing Fallout or Skyrim or whatever - all the information you could ever need, right there at your fingertips. It's an ethos you don't see so much on the modern net. reply mossTechnician 3 hours agorootparentprevWikia is a great example of enshittification - provide great value to users, then take it away from users and hand it to other businesses (eg advertisers), then take it away from businesses too. Will Weird Gloop inevitably suffer the same fate? I hope not. reply diggan 3 hours agorootparent> Will Weird Gloop inevitably suffer the same fate? I hope not. Unless explicitly structured to prevent it, my bet is it will. If it's backed by a for-profit entity, it'll eventually need to turn a profit somehow, and users/visitors are the first to lose their experience at that point. However, if Weird Gloop is a properly registered non-profit with shared ownership between multiple individuals, I'll be much more likely to bet it won't suffer the same fate. I skimmed around a bit on the website to try to get an answer to if it is an non-profit, but didn't find anything obvious that says yes/no. reply cookmeplox 3 hours agorootparentWe're already turning a profit! And there are no third-party investors (or debt) – it's all controlled by wiki people[1] [1] https://meta.weirdgloop.org/w/Weird_Gloop_Limited reply diggan 3 hours agorootparentAw, I take that as it is in fact a for-profit company already. Regardless, I wish you luck for the future! May you not go down the almost inevitable enshittification hole. reply robotnikman 2 hours agorootparentAt least it is a private company though, meaning they are are required to make constant year over year gains for shareholders and investors. They have much more control over where the company goes and how it operates. reply ChadNauseam 1 hour agorootparentpublicly traded companies are not \"required\" to make constant year over year gains for shareholders and investors, that is just what the owners usually decide to tell the company to do. The owners of a privately traded company could decide to, and the owners of a publicly traded company could decide not to. For example, zuckerberg controls 53% of the voting stock of facebook, so whatever zuck says goes and if other shareholders don't like it they can kick rocks. This is pretty much the same situation that people imagine is the case with privately traded companies, even though facebook is obviously publicly traded. reply basicallybones 10 minutes agorootparentThis is not totally accurate. For reference, here is the Wikipedia entry for Dodge v. Ford Motor Co. (1919) (copy and pasted at bottom). https://en.wikipedia.org/wiki/Dodge_v._Ford_Motor_Co. In fact, the relatively new concept of a \"public benefit corporation\" is (at least in part) an effort to allow for-profit entities to pursue goals other than shareholder enrichment. However, some have criticized public benefit corporations as being entities that simply strengthen executive control at the expense of shareholders. https://en.wikipedia.org/wiki/Benefit_corporation About Dodge v. Ford Motor Co.: Dodge v. Ford Motor Co., 204 Mich 459; 170 NW 668 (1919),[1] is a case in which the Michigan Supreme Court held that Henry Ford had to operate the Ford Motor Company in the interests of its shareholders, rather than in a manner for the benefit of his employees or customers. It is often taught as affirming the principle of \"shareholder primacy\" in corporate America, although that teaching has received some criticism.[2][3] At the same time, the case affirmed the business judgment rule, leaving Ford an extremely wide latitude about how to run the company.[citation needed] The general legal position today (except in Delaware, the jurisdiction where over half of all U.S. public companies are domiciled and where shareholder primacy is still upheld[4][5]) is that the business judgment that directors may exercise is expansive.[citation needed] Management decisions will not be challenged where one can point to any rational link to benefiting the corporation as a whole. atomicnumber3 1 hour agorootparentprev\"that is just what the owners usually decide to tell the company to do\" Because the entire system encourages it. The market rewards growth FAR more than it rewards a consistent dividend payout. (See: companies growing 40% YoY command a significfantly higher earnings multiple than those growing 10% YOY). So imo this is a like saying \"people could decide to just invest money and then not seek the best returns possible.\" Also remember these shareholder are seldom John Smith principled human retail investor. It's firms whose entire purpose themselves is to seek maximum return. \"The owners of a privately traded company could decide to\" Meanwhile this DOES actually happen sometimes. See: Valve. We all know there's ways Valve could put up really great growth numbers for about 2-3 years while completely destroying all of the things that make Steam so god damn compelling to users that they can command the same cut as Apple, on an OPEN platform (vs Apple fighting utterly tooth and nail to keep iOS 100% airtight locked down). But they don't. \"For example, zuckerberg controls 53% of the voting stock of facebook, so whatever zuck says goes\" TBC most founders/CEOs are NOT majority voters in their companies. They answer to the board. Most company founders lose voting control. The fact that Zuck is still in control is incredibly unusual and is a testament to how fast Facebook has grown that he's been able to keep hold of the reins. reply Reason077 55 minutes agorootparentElon Musk is another CEO in total control. Although Tesla is a public company and therefore has a board, it’s stacked with Elon’s allies/appointees and answers to him, not the other way around. Despite Elon not being a majority owner of Tesla stock. And when he took over Twitter in 2022, he immediately dissolved the board and fired the executives who were on it. reply mossTechnician 1 hour agorootparentprevShouldn't it be worrying that companies are required to make consistent gains* for shareholders and investors? At some point, a company will naturally reach a market saturation point. * ETA: I meant \"growth\" here, not profit reply lupire 1 hour agorootparentIf it can't generate profit, it's worth more liquidated than operating. Employees should buy out investors if they want to keep operating for their own personal profit. reply xp84 1 hour agorootparent>If it can't generate profit This wasn't exactly the question. The question was about growth. A company could be very profitable without growth (say, they own a mine which produces $40 million worth of ore each year with expenses of $10 million with no end in sight) or can have growth without profit (Open AI is a great example, or for history, the first 5 years of Facebook.) I know most of stock investing is about capital gains and not dividends, but I think GP was saying it's inherently impossible to have growth forever. On a financial level I get why people prefer to invest their money in a stock that goes up rather than one that pays them 8% a year consistently in dividends, but it seems unfortunate that somehow it seems like we aren't allowed to just have sustainable companies that don't depend on infinite growth to stay in business. reply sph 1 hour agorootparentprevs/are/aren't/ required to make constant profit reply adw 1 hour agorootparentprevIt’s a company limited by guarantee, which is the structure you use in the UK for non-charity non-profits. reply cinntaile 3 hours agorootparentprevIf it started that way, I'd say it's less likely to end up \"bad\". Compared to non-profit websites that get sold to ad businesses. reply Imustaskforhelp 3 hours agorootparentprevHow is it making money? reply cookmeplox 2 hours agorootparentWe have services agreements with the League of Legends and RuneScape developers, and we run 1 ad (below-the-fold, not in EU/UK) on the RuneScape wikis. This covers all expenses (including 5 staff) by a pretty healthy margin reply hiatus 1 hour agorootparentprevIt is described in the linked article. > The company primarily relies on three streams of revenue: user donations, serving ads on select Weird Gloop wikis, and a contract with Jagex that includes a fee to cover hosting and administration costs. reply dingnuts 36 minutes agorootparentI didn't see anything in the article about setting up incentives to keep the same thing from happening to Weird Gloop that happened to Fandom, which means the blog post is just empty marketing. The only difference is that Weird Gloop is the little guy. Competition is good! That might be a good enough reason to choose them if you're in the market for wiki hosting! But the moral posturing won't last if they become dominant, unless they set up incentives fundamentally differently than Fandom did, which doesn't seem to be the case. As long as advertising is one of their revenue sources, the user experience will get crappy as soon as the network effects make it hard to leave. The cycle continues. reply 35skill 23 minutes agorootparentDid you read the post? There's a whole section talking about how they are entering into binding agreements that let communities leave (and take the domain) if they have a better option reply madeofpalk 3 hours agorootparentprevCan we flip it? Some companies are explicitly structured to guarantee enshittification. Venture capital/private equity is what causes this. We've been poisoned to believe that websites should exist purely to achieve hyperscale and extract as much money as possible. When you look at the real physical world there are tons of small \"mom and pop\" businesses that are content with being self sustainable without some special corporate structure to legally require that. Maybe websites could be the same? reply SoftTalker 2 hours agorootparentThere are millions of websites like that. They don't show up on the first page of search results, so nobody finds them. reply zellyn 3 hours agorootparentprevThe article explicitly covers this question. Looks like they're setting up explicit legal(?) agreements. One key point is the domain name: minecraft.wiki, for example, not a subdomain of something owned by Weird Gloop. So the wiki can leave if it wants to. reply mossTechnician 3 hours agorootparentDoes that mean that to the users of these wikis, the switching costs[1] of the backend would basically be zero (one day they might just end up on a different server with the same content), while on the administrators' side the switching costs are at a reasonable minimum? [1] a variable in whether something can be enshittified, via https://en.wikipedia.org/wiki/Enshittification#History_and_d... reply Nadya 2 hours agorootparentTo my understanding wikis can take all their data, host it themselves, point the domain to their new hosting, and the move would be entirely invisible to end users if done properly and the quality of the hosting infrastructure wasn't considerably worse. Observant users might notice the removal of any Weird Gloop branding but otherwise the only way people would know if the wiki itself announces the move or performance of the wiki becomes noticeably worse. And Weird Gloop won't do what Fandom does and keep a zombie copy of your wiki online. So you won't be competing with Weird Gloop wiki traffic to reclaim your traffic. In fact, the obligations they agree to forbid it. Reading the Minecraft.wiki Memorandum: https://meta.minecraft.wiki/w/Memorandum_of_Understanding_wi... Upon termination by either party, Weird Gloop is obligated to: - Cease operating any version of the Minecraft Wiki - Transfer ownership of the minecraft.wiki domain to the community members - Provide dumps of Minecraft Wiki databases and image repositories, and any of Weird Gloop's MediaWiki configuration that is specific to Minecraft Wiki - Assist in transferring to the community members any domain-adjacent assets or accounts that cannot reasonably be acquired without Weird Gloop's cooperation - This does not include any of Weird Gloop's core MediaWiki code, Cloudflare configuration, or accounts/relationships related to advertising or sponsorships This sort of agreement means Weird Gloop is incentivized to not become so shit that wiki would want to leave (and take their ad revenue with them) because they've tried to make leaving Weird Gloop as easy as possible. reply mossTechnician 1 hour agorootparentThis is very reassuring. Usually, I assume agreements between different groups will inordinately benefit one party, but this particular agreement sounds like it creates a more level playing field. And besides, it's not like non-profits are exempt from restructuring and becoming worse. There is no silver bullet. reply cookmeplox 2 hours agorootparentprevYeah - it would be on the same domain, so way users access it wouldn't change at all. If any of the wikis we host want to leave, we'd provide them with a database dump. The admins would have to configure all of their own MediaWiki stuff of course, but I figure that's a pretty reasonable switching cost. reply stonemetal12 4 hours agoparentprevThanks(seriously). Fandom may not be great, but you could have said I don't want to foot the bill, turned off the servers and walked away. Then the community would have lost every thing. Leaving it with Fandom gave Weird Gloop something to start with instead starting from scratch. reply beAbU 3 hours agorootparentI can't imagine that this would have happened, like ever. The wiki was basically essential reading prior to starting to play Minecraft, especially in the early days. I think most the crafting recipes were documented by the developers themselves during those days. If they killed the wiki, they would have killed their userbase. reply Dwedit 3 hours agoparentprevAh Cloudflare, where you constantly get captchas for attempting to read a web page. reply whstl 3 hours agorootparentAt least they moved away from Google Captchas, which really hates disabling of 3rd party cookies and other privacy-protection measures. I haven't had a problem with Cloudflare and their new Captcha system since their changed, but I still suffer whenever I see another website using Google Captcha :( reply ChocolateGod 3 hours agorootparentIronically its now easier for robots to solve Google Captchas than it is for humans, as evident by the browser extensions that solve them that exists. reply Dwedit 2 hours agorootparentI used to have a lot of bot spam, but then I mostly foiled them with the world's silliest captcha. Looks like a math problem, but the solution isn't what's required to proceed. reply palunon 2 hours agorootparentprevAFAIK most of those just pay a human in a low income country. reply matt_heimer 3 hours agorootparentprevThat's up to the site owner. For example I configured my osdev wiki (mediawiki based) so that the history and other special pages get the Cloudflare test but just viewing a page doesn't trigger it. OpenAI and other bots were generating way too much traffic to pages they don't need. Blame the bots that are DDOS'ing sites for the captchas. reply kbolino 3 hours agorootparentprevEven better, you can get a captcha before you're allowed to see 404 Not Found. reply theamk 3 hours agorootparentprevCloudflare dropped captchas back in 2022 [0], now it's just a checkbox that you check and it lets you it (or does not). And this mean that my ancient android tablets can no longer visit many cloudflare-enabled sites.. I have a very mixed feelings about this: I hate that my tablets are no longer usable so I want less Cloudflare; but also when I visit websites (on modern computers) which provide traditional captchas where you click on picture of hydrants, I hate this even more and think: move to Cloudflare already, so I can stop doing this nonsense! [0] https://news.ycombinator.com/item?id=33007370 reply eviks 3 hours agorootparentbut there are more user-friendly captchas than the hydrants, which on average could be better that a total block on the tablets? reply theamk 1 hour agorootparenttotal block on _old_ tablets - Android 4.4 specifically, and I am sure many people on HN would be horrified to see those anywhere close to internet. New tablets are fine. As for \"more user-friendly captchas\" - I have seen some of those (like AliExpress' slider) but I doubt they will work as well as hydrants. And with new AI startups (1) slurping all the data on the web and (2) writing realistic-looking spam messages, I am sure anti-bot measures would be more important than ever. reply fwip 2 hours agorootparentprevThe checkboxes are also captchas. reply Washuu 1 hour agoparentprevHey Criticsquid!~ \\(￣︶￣*\\)) It's Azxiana[1]. I hate that MCW ultimately ended up with Fandom in the end. Keeping MCW and the other wikis running smoothly was essentially my one huge passion in my life that I lost after Fandom acquired Curse. No one wanted it to happen that way. Even internally at Curse/Gamepedia we were all devastated when we learned that the company was buying bought out by the rival we were striving to overcome all those years. I am so glad to see after the past few years that the wikis are finally healing and going to places that are better for them. [1] I'm the tech lead/manager that worked on Gamepedia at Curse that administered Minecraft MCW for many years before Fandom bought Curse in December 2018. I'm just writing this here since I figure other readers won't have any idea. ヾ(≧▽≦*)o reply why_at 2 hours agoparentprevOne thing I find interesting about playing video games in modern day is that with the proliferation of Wikis, there is assumed to be some kind of third party guide for every game. Especially in smaller/newer games it seems like developers sometimes don't bother putting necessary information in the game at all because they don't have the person-hours for it. For instance, back when I first played Minecraft in Alpha the only ways to find the crafting recipes was through a wiki, or trial and error. It's nice that it makes development easier, but I wonder if this trend is making it harder for new people to get into video games, since it's hardly obvious if you're not used to it. reply christianqchung 2 hours agorootparentI don't really know how exploratory most games are compared to old Minecraft. Some games like Stardew Valley have certain things that are much easier to do because of third party wikis but I don't think the same is true of a lot of games in the same way it was for Minecraft. reply preciousoo 4 hours agoparentprevYou and your team made(a good portion of) my childhood. I remember spending nights studying all the potion recipes and enchantment odds. Thanks for all you did reply oreally 4 hours agoparentprev> with the availability of Cloudflare, running high-traffic websites is much more cost effective. sidetrack but how does cloudflare make things cost effective? wouldn't it be cheaper if i just hosted the wiki on a simple vps? reply pjc50 4 hours agorootparentCloudflare get the best deals on bandwidth. It will usually be cheaper to serve a terabyte from Cloudflare than to do it yourself: you could probably run the wiki on the free plan! reply account42 3 hours agorootparentPerhaps, but VPS traffic prices are also already a lot better than \"big cloud\" traffic prices, especially if you choose your VPS provider with that in mind. And once your traffic is large enough there are also options where you pay for a fixed pipe instead of a transfer amount. reply diggan 3 hours agorootparentprev> Cloudflare get the best deals on bandwidth. If you want to pay for bandwidth then yeah, CloudFlare is a great option. Otherwise, if you like the experience of not paying per GB/TB, go for a dedicated server with unmetered connection that has the same price every month, regardless. reply KomoD 3 hours agorootparentYou don't need to pay anything to run TBs through Cloudflare, you could use the free plan. Rent VPS or managed hosting or host wherever you want, proxy it with Cloudflare on the free plan, Cloudflare caches it. reply robertlagrant 2 hours agorootparentprevIt's more like: if you have a website that (sometimes) gets a lot of traffic, do you want Cloudflare to cache it and serve it with very few hits to your cheap server, or do you want your compute costs to expand to cope with the requests? reply rjmunro 1 hour agorootparentprevCloudflare don't charge per GB/TB. You get unlimited bandwidth even on their free plan. The problem with paying per GB is that it's in the CDN's interest for you to get a DDOS attack so they can charge you for all the bandwidth. It's in Cloudflare's interest to reduce DDOS attacks and unwanted bot traffic because it costs them bandwidth, not you. reply wpietri 1 hour agorootparentYour point on interest is spot on. I moved a few of my personal websites to AWS's CloudFront and it cost me like a buck a month, way cheaper than maintaining a virtual server to do it. Except that somebody somewhere decided to try their DDOS tool on one of them for a few hours in the middle of the night, and I got a bill for $2541.69. Eventually they credited it, but it was not a fun ride, and decided that I was done using a CDN with misaligned incentives: https://sfba.social/@williampietri/111687143220465824 reply Aachen 1 hour agorootparentprev> it's in the CDN's interest for you to get a DDOS What kind of conspiracy is this? As if anyone charging for bandwidth hopes to get their infrastructure attacked reply citricsquid 4 hours agorootparentprevMore than a decade has passed since then so I am stretching my memory. At peak we were serving in the region of 10 million page views per day which made us one of the most popular websites on the internet (Minecraft was a phenomenon and every Minecraft player needed the wiki). We were probably the highest traffic Wiki after Wikipedia. Nowadays Cloudflare could absorb most traffic because of the highly cacheable nature of it, but at the time, Cloudflare didn't exist, and every request hit our servers. reply owyn 3 hours agorootparentYeah, Wikia in aggregate was in the top 50, maybe a top 20 site at various points. Wikia was built on caching. From my memory, about 99% of page views hit some kind of cache. If that dropped down to 97%, servers started to suffer. It's good to remember that the Fastly CDN company is a spinoff of Wikia, it was developed internally there first. Without that (varnish cache plus lots of memcache) Wikia would not have been able to handle the traffic. Mediawiki is horribly inefficient and one reason why Wikia was attractive as a host was that we had figured out a bunch of tricks to run it efficiently. The default configuration of mediawiki/wikipedia is real bad. Bigger independent wikis just couldn't handle the scale and many of the best independent wikis moved there for that reason. Just as one example, every link/url on a page hits a hook/callback that can call into an extension literally anywhere in the code base, which was several million lines of PHP code. I remember the \"Batman\" page on the DC wiki used to take several minutes to render a new copy if it fell out of the cache. That was one page I used for performance optimization tests. The muppet wiki and the lyrics wiki also had huge performance issues and fixing them was some of the most fun engineering work I've done. Every useful feature had some kind of horrible performance side effect, so it was always a fun puzzle. I also hate landing on a Fandom wiki now but thanks to the actual editors, it's still got some good content. reply Ambroos 4 hours agorootparentprevIf you can run your application on Cloudflare Pages / Workers with Cloudflare's storage/DB things, it really gets dirt cheap (if not free) and very fast. And even without that, Cloudflare's caching CDN is very good, very cheap and very easy. reply bombcar 4 hours agorootparentprevTen years ago bandwidth was expensive. Still is, even if not as much. A simple VPS gets overwhelmed, but a simple VPS behind cloudflare can do quite well. reply thinkmassive 3 hours agorootparents/cloudflare/a CDN/ reply pornel 3 hours agorootparentprevCloudflare caches pages at many many datacenters, often colocated with large ISPs. This lets Cloudflare deliver pages from their local cache over local links (which is fast and cheap), instead of fetching the data every time across the world from wherever the VPS is located. reply Nux 1 hour agoparentprev> At the time of selling out, the Minecraft Wiki and Minecraft Forum cost tens of thousands of dollars per month to run. What kind of decisions got you in that position? Hard to phatom. reply jchw 4 hours agoparentprevIn all fairness, running modest to large MediaWiki instances isn't easy. There's a lot of things that are not immediately obvious: - For anything complex/large enough you have to set `$wgMiserMode` otherwise operations will just get way too long and start timing out. - You have to set `$wgJobRunRate` to 0 or a bunch of requests will just start stalling when they get assigned to calculate an expensive task that takes a lot of memory. Then you need to set up a separate job runner in the background, which can consume a decent amount of memory itself. There is nowadays a Redis-based job queue, but there doesn't seem to be a whole lot of documentation. - Speaking of Redis, it seems like setting up Redis/Memcached is a pretty good idea too, for caching purposes; this especially helps for really complicated pages. Even to this day running a Wiki with an ambient RPS is kind of hard. I actually like MediaWiki because it's very practical and extensible, but on the other hand I know in my heart that it is a messy piece of software that certainly could make better use of the machine it's running on. The cost of running a wiki has gone down over time in my experience though, especially if you are running things as slim as possible. A modest Digital Ocean machine can handle a fair bit of traffic, and if you wanted to scale up you'd get quite a boost by going to one of the lower end dedicated boxes like one of the OVHcloud Rise SKUs. If anyone is trying to do this I have a Digital Ocean pro-tip. Don't use the Premium Intel boxes. The Premium AMD boxes are significantly faster for the money. One trap I also fell into was I thought it might be a good idea to throw this on a hyperscaler, you know, Google Cloud or something. While it does simplify operations, that'll definitely get you right into the \"thousands of dollars per month\" territory without even having that much traffic... At one point in history I actually felt like Wikia/Fandom was a good offering, because they could handle all of this for you. It didn't start out as a bad deal... reply noen 2 hours agorootparentThis is so true. I adopted mediawiki to run a knowledge base for my organization at Microsoft ( https://microsoft.github.io/code-with-engineering-playbook/I... ). As I was exploring self-host options that would scale to our org size, it turned out there was already an internal team running a company wide multi-tenant mediawiki PLATFORM. So I hit them up and a week later we had a custom instance and were off to the races. Almost all the work that team did was making mediawiki hyper efficient with caching and cache gen, along with a lot of plumbing to have shared infra (AD auth, semitrusted code repos, etc) thst still allowed all of us “customers” to implement whatever whacky extensions and templates we needed. I still hope that one day Microsoft will acknowledge that they use Mediawiki internally (and to great effect) and open-source the whole stack, or at least offer it as a hosted platform. I tried setting up a production instance af my next employer - and we ended up using confluence , it was like going back to the dark ages. But I couldn’t make any reasonable financial argument against it - it would have taken a a huge lift to get a vanilla MW instance integrated into the enterprise IT environment. reply bawolff 37 minutes agorootparentMicrosoft did open source a bunch of their mediawiki extensions. https://github.com/microsoft/mediawiki-extensions Last i heard though they were moving off it. reply account42 3 hours agorootparentprevA lot of things should be solved by having (micro)caching in front of your wiki. Almost all non-logged in requests shouldn't even be hitting PHP at all. reply jchw 2 hours agorootparentIn my experience this hasn't been necessary yet on anything I've ran. I know WMF wikis run Varnish or something, but personally I'm trying to keep costs and complexity minimal. To that end, more caching isn't always desirable, because RAM is especially premium on low-end boxen. When tuned well, read-only requests on MediaWiki are not a huge problem. The real issue is actually just keeping the FPM worker pool from getting starved, but when it is starved, it's not because of read-only requests, but usually because of database contention preventing requests from finishing. (And to that end, enabling application-level caching usually will help a lot here, since it can save having to hit the DB at all.) PHP itself is plenty fast enough to serve a decent number of requests per second on a low end box. I won't put a number on it since it is obviously significantly workload-dependent but it would suffice to say that my concerns with optimizing PHP software usually tilt towards memory usage and database performance rather than the actual speed of PHP. (Which, in my experience, has also improved quite a lot just by virtue of PHP itself improving. I think the JIT work has great potential to push it further, too.) The calculus on this probably changes dramatically as the RPS scales up, though. Not doing work will always be better than doing work in the long run. It's just that it's a memory/time trade-off and I wouldn't take it for granted that it always gives you the most cost-effective end result. reply bawolff 31 minutes agorootparentVarnish caching really only helps if the majority of your traffic is logged out requests. Its the sort of thing that is really useful at a high scale but matters much less at a low scale. Application level caching (memcached/redis/apcu) is super important even at a small scale. Most of the time (unless complex extensions are involved or your wiki pages are very simple) mediawiki should be io-bound on converting wikitext -> html (which is why caching that process is important). Normally if db is healthy, db requests shouldn't be the bottle neck (unless you have extensions like smw or cargo installed) reply tempest_ 2 hours agorootparentprevHave any of Intels server offerings been \"premium\" since epyc hit the scene? I just assumed they were still there based on momentum. reply jchw 2 hours agorootparentWith Digital Ocean the cpuinfo is obfuscated so figuring out exactly what you're running on requires a bit more trickery. With that said I honestly assume that the comparison is somewhat older AMD against even older Intel, so it's probably not a great representation of how the battlefield has evolved. That said, Digital Ocean is doing their customers a disservice by making the Premium Intel and Premium AMD SKUs look similar. They are not similar. The performance gap is absolutely massive. reply Arch-TK 3 hours agoparentprevYou say you were a kid when you sold it. I could have sworn you weren't from conversations we had on IRC at the time. Although I most assuredly was a kid. reply citricsquid 2 hours agorootparentI was a teenager at the time. I'm in my mid 30s now, it feels like I was a kid back then. reply fwip 2 hours agorootparentprev\"Kid\" doesn't really have a hard cutoff. When you're 15, 12-year-olds are kids. When you're 30, 20-year-olds are kids. reply jagermo 4 hours agoparentprevholy crap that minecraft wiki is fast now. I actually stopped going to fandom because it was so slow. reply sammy2255 3 hours agoparentprevTens of thousands to run a static webpage? LUL reply misode 3 hours agorootparentThe Mediawiki software is not a static webpage reply tredre3 3 hours agorootparentMediawiki is trivial to cache, though. For all intent and purposes most hits will be cache hits, and thus \"static\" content. I'm also shocked at the tens of thousands per month, it can't possibly be hosting alone. It has to be that the maintainer had a generous salary or something. reply citricsquid 2 hours agorootparentI could have the numbers wrong, archive.org is down otherwise I would check as we shared information publicly at the time. As far as I recall, we weren't taking money from the websites, we were spending on infrastructure alone with more than $10k in spend in the final month before the sites were acquired. I think it is easy to forget how much more expensive running things on the internet was back then along with the unprecedented popularity of Minecraft. Once archive.org is back online, I'll track down numbers. reply bawolff 45 minutes agorootparentprevNot everyone is a professional web hoster with requisite knowledge on how to setup caching properly. Mediawiki involves edits that users expect to propagate instantly to other pages. Sometimes this can easilt result in cache stampedes if not setup carefully. MediaWiki supports extensions. Some of the less well architectured extensions add dynamic content that totally destroies cachability. reply nemothekid 2 hours agorootparentprevSeriously? How does that even make sense to you? The OP had an asset generation 10k+ a month in profit and was so squeezed for cash he had to sell it. Doesn’t it make more sense that a media have site would have been paying through the nose for bandwidth, hence the callout for cloudflare which would have made that cost free? reply Sharlin 3 hours agorootparentprevI have no idea how it works, but given that the read:write ratio is probably 100:1 or more, certainly it could just serve static, prerendered pages straight from the filesystem or something like memcached? reply bawolff 44 minutes agorootparent[Im a mediawiki dev]. Typically people use varnish for that use case. MediaWiki does support serving logged out views from a filesystem cache, but varnish is generally a better idea. There are also some caches out of memcached (mediawiki has \"parser cache\" in memcached which is the part of the page that stays constant between alm users. Typically people use varnish on top of that for the entire page for logged out users) Sometimes people add things to their sites that are incompatible with caching, which will make hosting costs go way up. reply johnklos 2 hours agoprevThis illustrates a problem that I wish more people would see. People, usually businesspeople, consider adding some craptastic thing such as intrusive ads, to make more money. Who doesn't like more money? They add the thing, and revenue goes up! What they don't see is the effect that comes when fewer people visit the site because they're too annoyed to come back over time. They see and take credit for the small increase, but of course they don't take credit for the gradual decline afterwards, a decline that often enough leaves the site making the same or less money than it did before the craptastic ads. If people and companies took the bigger picture in to account, they likely wouldn't do these things. reply xahrepap 16 minutes agoparentI’ve wondered about this wrt public transportation. They keep raising prices, making it less affordable for people. Eventually basically no one is riding, so they … raise prices. It seems needlessly expensive to me to run empty busses. I’d like to see if cheaper transportation can actually make more money. reply jjcm 1 hour agoparentprevIt's the classic question of \"how much of our brand did we sell to achieve this bump in revenue?\". Selling your brand is a very real thing, and I wish more people would take it into account. Brand health correlates with long term health. reply paranoidxprod 4 hours agoprevA few years ago, Path of Exile migrated from the fandom to a new site. GGG (Path of Exile's company) even decided to host the new wiki on their servers (https://www.pathofexile.com/forum/view-thread/3292958)! At this point, the new wiki ranks higher then the old one, but for a time it was an issue. Interesting to see more cases of games wikis leaving Fandom with how horrible the site is, and hopefully this is just the beginning of a trend. reply ykonstant 4 hours agoparentFandom PoE still pollutes the top of Google searches :( reply paranoidxprod 3 hours agorootparentJust tested a bunch and it seems like `path of exile [skill/currency]` usually ranks the Fandom higher while `poe [skill/currency]` ranks the new wiki higher which is why I never noticed (I actually never noticed because I block the PoE Fandom and pin the new wiki on Kagi) reply ykonstant 2 hours agorootparentThat's good info. reply dpbriggs 29 minutes agorootparentprevThere is an extension which automatically redirects you from Fandom to the new wiki. While that's convenient it probably helps Fandom stay near the top. reply dcow 5 hours agoprevThe Runescape wiki is simply amazing. It’s one of the most well built fit for purpose pieces of quality software+content that I have ever come across. It’s clean and crisp visually and well organized at the IA level despite being exactly the type of content problem that resists such attempts by nature. What a solid community. The software doesn't fell clunky, it’s fast and responsive and still feels modern. I can only assume that’s a testament to the quality of mediawiki. I’m glad that it’s getting the attention it deserves. reply sph 4 hours agoparentI have seen cookmeplox, one of the admins of the Runescape wiki, round these parts. Thank you for your work, as a gamer and new Runescape addict. For an MMORPG as massive as OSRS, having a good wiki is crucial and probably the reason why it's seen a resurgence over the past few years. reply cookmeplox 3 hours agorootparentThat's me! I also wrote the blog :) reply candiddevmike 5 hours agoparentprevFactorio and Rimworld have amazing wikis as well. And they're both maintained by the developers AFAIK... reply bombcar 4 hours agorootparentThe Dwarf Fortress wiki https://dwarffortresswiki.org is perhaps the most impressive I've seen, as it maintains namespaces to maintain (and update!) information about particular versions, because many players end up staying on a version for various reasons. reply wpietri 1 hour agorootparentCould you say a bit more about that? Normally I think \"have to support people stuck on old versions\" is something that happens when you're selling enterprise software to insurance companies. This is the first I've heard of it in games. reply dudeinhawaii 1 minute agorootparentOn the Steam platform for instance there is an option (perhaps developer supported) to stay on a certain version of a game. For instance, in the game Mount and Blade: Bannerlord, players notoriously stay 2, 3, or even 10 versions behind in order to maintain compatibility with specific mods or sets of mods (10s or 100s of mods). Eventually, enough of the modders move to the next or latest version and the players gradually move with them. Games with \"always on\" or auto-updaters avoid this. bombcar 1 hour agorootparentprevAs mentioned, some versions of the game introduce breaking concepts that earlier players may not want to deal with (either because it breaks save compatibility, or they don't like the mechanic, etc). Minecraft has this somewhat also, with some people sticking on various versions because of mods, or play style, or combat, etc. For example, one huge change was going from a 2D map to a 3D one, another was how world generation was done. See \"Eras\" here for the big ones: https://dwarffortresswiki.org/index.php/Release_information reply gazook89 1 hour agorootparentprevDF had a massive update probably a decade or more ago that changed the game from 2D to 3D (still represented as 2D z-levels though). With such a change, obviously some people would want to stick with the old version. There have been numerous large updates since then (the game has been in development for 22 years) and with each you get some people that just don’t want to update, either because it might ruin their current games or they prefer to avoid new features etc. Another example is the various Dungeon and Dragons wikis that allow you to toggle between versions, since it has existed for 50 years now. reply Tomte 1 hour agorootparentprevPlayers comfortable with the ASCII graphics version (the one that existed for years) often just paid for Steam release with pretty graphics just to support the brothers. And then kept playing the \"hardcore\" version they are used to. reply munificent 4 hours agorootparentprevI wish the Minecraft wiki did that. I don't tend to play the latest version because I feel like it got overly complex and I get analysis paralysis if I play the latest version. But being on an old version makes navigating the wiki hard. I'm never sure if some content applies to me. Sometimes they say which version a feature was introduced in, but if a mechanic changes, they often just document the latest behavior. reply bombcar 3 hours agorootparentTell me about it; playing GregTech:New Horizons and trying to figure out vanilla mechanics related to 1.7.10 is annoying. All the GTNH specific stuff is on their wiki, but vanilla mechanics are just assumed. reply sph 4 hours agorootparentprevThe good ol' Mediawiki look of the DF wiki reminds me of the underrated, and oft maligned Dungeon Crawl Stone Soup wiki: http://crawl.chaosforge.org/Crawl_Wiki reply csmcg 1 hour agorootparentI haven't played DCSS regularly since probably v0.24 or v0.25, so things may have changed - but if I recall correctly, it was not kept up to date very well, character guides are flat-out wrong, etc... reply Waterluvian 5 hours agoparentprev> I can only assume that’s a testament to the quality of mediawiki. I was curious about this so I poked around both and I think I disagree. Both load very fast for me and are snappy and look pretty nice. The one difference is that the Runescape wiki has a single ad in the sidebar or at the bottom, below the content footer. While the Fandom wikis have 3+ ads, far larger, one of which covers content until interacted with (like being closed). For me, Fandom's ad approach absolutely falls within \"offensively bad,\" while the Runescape ad approach reminds me of early 2000s, \"here's an ad to pay the bills. We've tried to keep it well out of your way.\" So I'd opine that it has less to do with the quality of mediawiki, and more about how much money both Wiki hosts are seeking to gain from the existence of these resources. reply Nadya 5 hours agorootparentTry editing anything on a Fandom wiki and that's where the real differences in experience comes from. Fandom makes it extremely difficult (nigh impossible) to do something as simple as access the page of an image asset. reply Waterluvian 5 hours agorootparentWoof. Yeah, you're right. That was not great. reply wodenokoto 5 hours agorootparentprevFandom runs on media wiki too. reply languagehacker 4 hours agoprevFormer Wikia engineer, here. I left right around when they changed their name to Fandom and kind of saw the writing on the wall. Despite the tremendous amount of information they have at their disposal, they never really saw themselves (or positioned themselves) as more than a low market cap media company. I spent a lot of time in the mid-teens trying to encourage them to be early on AI/NLP kind of stuff and use that to drive new product development. Needless to say, it didn't work out. Imagine the data moat they could have built and monetized, and all without needing to degrade the customer experience. reply rideontime 3 hours agoparentI didn't think I could Fandom being worse than it already is, but imagining it stuffed with AI-generated slop... reply languagehacker 3 hours agorootparentSure, but think about something as low stakes as, \"Does such-and-such a character from my favorite TV show have any siblings\" vs. \"Is it safe to consume XYZ\" Even with the great structured and semi-structured data that Wikis can provide with this like infoboxes and other sort of templates, there were definitely limitations to the tech nearly ten years ago. My experience back then is one of the reasons I'm super skeptical of the long-term value of the AI / LLM trend we're going through right now. reply sushid 10 minutes agorootparentAren't those types of prompts the MOST likely to generate hallucinations? reply snowwrestler 3 hours agorootparentprevIt’s worth remembering that there was AI before generative AI, and there are applications of AI that don’t produce slop, like knowledge graphs and natural language search. Some of that might be called just “machine learning” now. reply wpietri 1 hour agorootparentYeah, it seems like \"AI\" has mostly become a marketing term for generative large models. For the people doing the stuff that is often called \"machine learning\", I see two reactions. Those seeking hype will call it \"AI\" anyhow, and a bunch of those that don't are firmly sticking with \"machine learning\" to avoid the rising backlash. I'm very curious to hear how others are seeing the terms used, though. reply pwdisswordfishz 1 hour agorootparentprevImagining? https://about.fandom.com/news/fandom-launches-new-creator-to... https://old.reddit.com/r/TwoBestFriendsPlay/comments/15vxs2x... reply owyn 3 hours agoparentprevFormer Wikia engineer here too! I also thought there was a lot of potential there. We even invested in some RDF and structured data and NLP projects (second screen, sentiment analysis on comments for detecting flame wars, etc), but for various reasons they just didn't work out beyond hackathons and demos. I think there were a lot of well meaning engineers who wanted to make stuff like that work. Part of the problem is mediawiki itself. A page is literally just text using an awful hacked together xml parser and some regexes to emit HTML. It might look like a database sometimes when it is rendered (and there is Wikidata) but there is no actual structure to it, just a pile of templates made of other templates that people have to tediously wrangle by hand. That it eventually turns into some HTML that you can view is almost an accident. reply languagehacker 2 hours agorootparentOh man, good to see you on here, dude! Yes, extracting the real human-readable text from a Wiki was a lot harder than you'd expect. There was also a question of investment. I think even with some early successes quantified with A/B tests and things like that, there just wasn't the executive or product buy-in to broaden the investment. reply jezzamon 3 hours agoparentprevA data moat of user provided wiki contents? The thing that this article is advocating for the users themselves to own over the hosting site?? Somehow I don't think that is the solution. reply languagehacker 3 hours agorootparentThe licensing on that stuff is complicated, and I haven't looked at it in a while. It does allow you to take your toys and leave, but for those that don't, it would be simple enough to prevent ethical AI scrapers from extracting that content. That's all I mean by data moat in this context. reply Washuu 55 minutes agoparentprevFormer Gamepedia/Wikia/Fandom engineer, I left not too long after Fandom bought out Gamepedia/Curse. You left at a good time. The upper management had no idea what they were doing and were entirely disinterested in the company. Talking with the CEO felt like talking with someone that had no idea what they were doing there. reply EcommerceFlow 5 hours agoprevGoogle giving Fandom powerful rankings bothers me too, since their intrusive ads clearly go against Google ranking factors. Still, I'm glad for some competition. However, even after browsing their site, is contacting them the only way to get something up and running? reply DrillShopper 5 hours agoparentI assume that Fandom pays Google for that placement reply snowwrestler 3 hours agorootparentThey don’t need to. Fandom benefits from being an old and popular site. Google manually adjusts their ranking to prioritize such sites, because they think those sites are what the “average” searcher expects to see come up when they search certain topics. Essentially, Google fears that the average searcher will think Google is broken if certain popular sites don’t come up in their results. reply abound 4 hours agorootparentprevTheoretically, you can't pay for placement on Google without it being labelled an ad. Practically, you can pay SEO experts to help you keep your rankings up. reply niam 4 hours agorootparentprevIf Google were to have the astoundingly poor business sense to secretly allow payment for higher 'organic' search rankings: they'd hopefully at least have the good sense to not blow that secret on a fish as small as Fandom. reply rightbyte 4 hours agorootparentHow so? Fandom seems to have Google ads. We wouldn't be able to prove if Google ranked sites with their ads higher. Google's search ranking is black box. Edit: I guess at great effort you could scrape thousands of sites, not if they remove or add Google ads, and track their rating. I think it is a better assumption to make, that Google puts their profit above luser experience, when it comes to search ranking. reply niam 2 hours agorootparent> We wouldn't be able to prove if Google ranked sites with their ads higher This to me is a different argument, though admittedly reasonable to arrive at through the language of \"paying Google for placement\". > I think it is a better assumption to make, that Google puts their profit above luser experience, when it comes to search ranking. I mean, yes. Though I should hope I needn't preamble any statement aboutwith how cynical I am about their intentions... It's not relevant here because I'm not arguing on the grounds that 'Google would be ethical and kawaii if they didn't accept payment for organic search ranking'--I'm saying that from a business standpoint it wouldn't make sense. reply rightbyte 2 hours agorootparentOk sure I might have misunderstood you. I agree that Fandom is most likely not writing checks or paying directly in other means to Google for increased search rank. reply starkparker 1 hour agoparentprev> However, even after browsing their site, is contacting them the only way to get something up and running? Yes, per this post: > I don’t think we would ever do a “self-service” thing where you could just sign up and immediately make a wiki. We want to do projects where we get to know the community, and closely support every wiki we host. ... > If you liked this and want to talk to me about wiki things, please come say hi[1] 1: https://weirdgloop.org/contact reply TazeTSchnitzel 4 hours agoprevMediaWiki is actually pretty easy to set up on a web server, speaking as someone who's now done it twice. You plop the files into htdocs, make sure PHP is set up, set up vanity URLs if you want to, and then… well, that's it. The final step is to go to the site, fill in the setup form, download the settings file it gives you and upload it. It doesn't even need an external database, it can use SQLite; if email setup is annoying, it doesn't even need that. And it's the most powerful and flexible wiki software out there: if there's something you want a wiki to do, MediaWiki can do it, but it also isn't too bloated out of the box, so you can just install plugins as and when you need them. Thoroughly recommend it. reply giantrobot 2 hours agoparentMaking MediaWiki survive non-trivial amounts of traffic is much harder than simply setting it up. It's not an impossible task for sure but there's no one click performance setting. reply starkparker 1 hour agorootparentSpecifically, managing edge and object caches (and caching for anonymous viewers vs. logged-in editors with separate frontend and backend caches) while mitigating the effects of cache misses, minimizing the impacts of the job queue when many pages are changed at once, optimizing image storage, thumbnailing, and caching, figuring out when to use a wikitext template vs. a Scribunto/Lua module vs. a MediaWiki extension in PHP (and if Scribunto, which Lua runtime to use), figuring out which structured data backend to use and how to tune it, figuring out whether to rely on API bots (expensive on the backend) vs. cache scrapers (expensive on the frontend) vs. database dump bots (no cost to the live site but already outdated before they're finished dumping) for automated content maintenance jobs, tuning rate limiting, and loadbalancing it all. At especially large scales, spinning the API and job queues off altogether into microservices and insulating the live site from the performance impact of logging this whole rat's nest. reply bawolff 27 minutes agorootparentEverything is hard at scale. You have to be pretty big scale before some of that stuff starts to matter (some of course matters at smaller scales) reply robjwells 5 hours agoprev> [This post] (and many others) have done a much better job than I could, explaining from a reader’s perspective why Fandom is bad place to host a wiki, The linked post (at j3s.sh) appears blank to me, so if others have the same problem here’s an archive link: https://archive.ph/kwt1b reply j3s 31 minutes agoparentoop, my bad. it got oomkilled somehow - should be back up now :3 original post is at https://j3s.sh/thought/stop-using-fandom.html reply ryukoposting 4 hours agoparentprevThanks, I had the same problem. reply compootr 4 hours agoparentprevYeah, the site is down (502 status) reply jdoss 4 hours agoprevI play a lot of Path of Exile and one of the best quality of life improvements I did this summer was adding the Fandom Path of Exile wiki URL to my Kagi deny list so it never shows up in search. The official one that is maintained and kept up to date by the game developer poewiki.net/wiki/Path_of_Exile_Wiki was always third or forth on my searches. reply xnorswap 4 hours agoparentYes, despite the poewiki migration being a fairly long time ago now, the fandom wiki still ranks frustratingly highly. The data on it is of course now very outdated and causes confusion for new players. I wonder how much the effect of lots of people having a redirect extension has. If google sees people click on the fandom result and not come back, do they treat it as a good result when in reality people are redirecting to poewiki via the extension? The situation improves every league, particularly since now there are quite a lot of items, skill gems or skill tree node passives/notables missing from the fandom wiki. It's much better than in the past when you could outright search \" poewiki\" and not have the poewiki result anywhere. But it still feels like there's a long way to go, and it's a shame because it further increases the knowledge gap between experienced players who might know to seek out the poewiki, and new players (or very casual players) who might not. It hints also at the power of the \"old web\" and it's historic power over google rankings. reply cubefox 4 hours agorootparentWhy can't they replace the old pages with a link to the new page? Or otherwise remove the contents from the old site? reply xnorswap 3 hours agorootparentThat's considered vandalism of fandom, and probably rightly so. Could you imagine if someone declared a successor to wikipedia and edited all the pages to redirect? Sometimes you just have to put the effort into making the new better, and it's a hard long slog especially against a well funded incumbent. But like all problems in PoE, PoE2 will fix it. ;) reply cubefox 2 hours agorootparentI mean the Wikipedia content arguably belongs more to the Wikipedia community than to the Wikimedia Foundation... Of course it is hardly possible to gain the approval of a majority of editors. > But like all problems in PoE, PoE2 will fix it. ;) Isn't that the game for which Sannikov came up with his new global illumination algorithm? [1] (Apparently yes) [1] https://www.youtube.com/watch?v=3so7xdZHKxw reply yifanl 3 hours agorootparentprevUsually attempts to advertise migration efforts on high visibility wikis away from Fandom will be deleted by Fandom staff. reply duskwuff 2 hours agorootparentAnd they will remove rights from wiki admins who take steps to advertise alternate resources. reply blendergeek 5 hours agoprevCan wikis on weird gloop use their own domain names? I feel like that is the best way to ensure that they can leave and that the host can't keep a zombie version of the wiki that hogs Google search position. reply DandyDev 5 hours agoparentTo they can. See the Minecraft wiki for example: https://minecraft.wiki reply aDyslecticCrow 3 hours agoparentprevFrom the article > (hint: it’s all about the domain). If we ever start going down the same path as Fandom, everyone can just leave! I would love to see other wiki platforms start to do this, because I think it’s the only way you really solve the problem. So yes, the wikis have their own domains for this exact reason. reply blendergeek 53 minutes agorootparentI missed the \"(hint: it's all about the domain)\" or more precisely, I didn't get the hint. I guess I need some things spelled out for me. reply gu5 5 hours agoparentprevCurrently, every wiki they host is on its own domain (besides the meta one) reply tombert 5 hours agoprevFandom is one of my least favorite things now. The site ends up having more ads than the average porn or piracy website, it manages to slow down my relatively beefy laptops without even trying. I love the idea of fan wikis, but Fandom is basically the worst possible implementation of that idea. reply fenomas 4 hours agoparentJust a week or two ago my chrome plugins got temporarily disabled for some reason, and I didn't notice for a day or two... until I happened to check a fandom wiki. Then for about five seconds I thought I'd somehow installed All The Viruses. And ironically, I already hated fandom before I'd seen it without an ad blocker! Just for the large sidebars and ugly flyouts and whatnot. It really feels like a contender for worst site on the internet. reply duxup 5 hours agoparentprevIt's not even accurate at times. I think a lot of the dedicated fans have given up on it. I've seen several that have chunks of straight wrong information. Usually it's stuff where the fan seems to have picked up on something implied in a story, but missed where it is clearly stated that isn't the case ... but then they go and write on fandom and make lots of assumptions from there and fill in other gaps with guesses. reply Starlevel004 4 hours agorootparent> I think a lot of the dedicated fans have given up on it. I've seen several that have chunks of straight wrong information. It's a not-so-open secret that a lot of wikia wikis are not only vandalised but encouraged to be vandalised as to make people move off them. reply mschae23 1 hour agorootparentVandalism on Fandom wikis is counter-productive. It just makes it look more active, to both users and search engines, and so will in turn make it harder for people to find the independent wiki. The best thing to do is just to ignore abandoned Fandom wikis entirely. reply duxup 4 hours agorootparentprevI can understand the urge and frustration level. Just wish there was a more centralized / good alternative to promote rather than just wrecking fandom. reply ziddoap 4 hours agorootparentprev>It's not even accurate at times. I am aware of a few game communities that purposefully poison the fandom version of the wiki with inaccuracies that are non-obvious and time-consuming to verify (so they aren't just auto-reverted). reply shbooms 4 hours agorootparentprevSame here. Prior to my discovery that fandom was bad and a lot of wikis were moving away, I was following so many instances of out dated info in games I was playing due to not realizing that the wiki was no longer maintained since the active contributors had moved elsewhere and updates/patches to the game had rendered the info moot. reply CM30 3 hours agorootparentprevYeah the more dedicated fans have usually gone off to the independent wiki instead, leaving the Fandom one a hellscape of rumours and outdated information. Just compare the versions of Nintendo wikis in the Nintendo Independent Wiki Alliance and their Fandom equivalents for example, and the quality difference is like night and day. Same goes with just about every wiki that has a counterpart that's not on Fandom. reply matheusmoreira 4 hours agorootparentprev> I think a lot of the dedicated fans have given up on it. As someone who once edited those wikis, I certainly hope they did. Who wants to work for free to enrich some private equity firm? reply Dwedit 3 hours agoparentprevFandom is perfectly usable with adblockers and the \"Cleaner Fandom\" userscript. But only with those extensions! reply GoblinSlayer 3 hours agorootparentI just disable javascript and googletagmanager and don't see any ads. The good moment is that Fandom shows static content as opposed to an average web 2.0 SPA. reply yamazakiwi 3 hours agorootparentprevI don't use it out of principle. reply ykonstant 4 hours agoparentprevI have blocked the domain on my browser; this helps with mindless clicking on fandom sites appearing on top of Google searches while the communities have moved on to other wikis. reply jagermo 5 hours agoparentprevagreed. the good thing is, it teaches a new generation why adblockers are great. reply GJim 5 hours agoparentprevWhat are these 'ads' of which you speak? reply ta1243 5 hours agorootparentAssuming you're not on an adblocker, what's really odd is every page has a video about the subject. Not an advert, just a video that you aren't interested in. I don't get it. If I'm looking up a specific year in the star trek universe, say 2381, to see what happened, why would I want 14 minute video on \"a history of star trek\". Then why would I want it again when I check the next year reply homebrewer 5 hours agorootparentFor some reason you're assuming the owners of the website have your interests at heart, and not the interest of their bank account. reply ta1243 4 hours agorootparentSure, but how does serving me a 15 minute video help their bank account? If it were a youtube style video advert I could understand it. reply philipov 4 hours agorootparentPutting autoplaying videos on every page farms their view count and gets the algorithm to show it to more people, which drives ad revenue. It's quite similar to how Fextralife embeds twitch streams to farm viewer counts. reply jorams 2 hours agorootparent> It's quite similar to how Fextralife embeds twitch streams to farm viewer counts. Fextralife notably stopped streaming almost a year ago after Twitch announced that embedded views would no longer be counted. The solution is in the incentive, but unfortunately on the modern internet those generally don't favor the user. reply mschuster91 4 hours agorootparentprevYou watch it, and Google Ads records a veeery long \"user is present on website\" time, which is a boost in SEO - Google ranks how long people spend on a website, hence the \"trend\" of endless waffling around in stuff as basic as cooking recipes, or inline videos that entice the user to spend time on the website. Even if all of it (nowadays including videos) is AI-generated slop. But if the user immediately finds the information and goes back or closes the tab, then the site will get punished for being actually efficient and useful. SEO has ruined the Internet. reply michaelt 3 hours agorootparentprevAs I understand things, video ads produce more $$$ - the advertiser pays more per view, and per click; and the click-through rate is higher. I've heard claims of video ads making 5x more. I assume the irrelevant video is included to give Fandom more video ad space to sell. reply formerly_proven 4 hours agoparentprevIn a nutshell https://en.uesp.net/wiki/Main_Page vs https://elderscrolls.fandom.com/wiki/The_Elder_Scrolls_Wiki (Though UESP has had banner ads for a while now) reply eviks 3 hours agorootparentA big blocking modal with \"Privacy Notice We & our 726 technology partners ask you to consent\"? reply sickofparadox 4 hours agorootparentprevLove the UESP, probably my favorite wiki. reply red-iron-pine 3 hours agorootparentUESP is amazing, and is a great example of what the non-fandom wikis are trying to be reply dvngnt_ 5 hours agoparentprevlaughs in ublock origin reply hbn 4 hours agorootparentUblock doesn't block the AI generated FAQs without manually stepping in, and it certainly won't block all the bad info as the more dedicated and knowledgeable fans move to other wikis. reply tombert 4 hours agorootparentprevI use ublock now too, but it's this really annoying feedback loop; people use ad blockers, making the websites less money, so they add more advertisements for the people who don't have ad blockers, and making the website worse and more likely for them to install an ad blocker etc... I know that running a website isn't free, so I understand the need for ads. Fandom is just a terrible version of it. reply teddyh 4 hours agorootparent> people use ad blockers, making the websites less money, so they add more advertisements for the people who don't have ad blockers I have serious doubts about this step in the spiral. IIUC, people who use ad blockers are still vanishingly few, and therefore the loss of ad impressions should not be that large. reply ARandumGuy 3 hours agorootparentIt's clearly enough of an impact for Google to spend effort killing uBlock on Chrome, and (attempting) to block it on Youtube. Obviously Google is huge, and even a small percentage of users is still a lot of money on the table. reply teddyh 3 hours agorootparentWe can only draw the obvious conclusion: Namely that Google plans to introduce a lot more ads once they have an iron grip on the consumer. If Google did that before Google destroyed ad blockers, regular people would indeed start to use ad blockers. reply ARandumGuy 2 hours agorootparentThat's possible, but I think it's premature to think this is part of some grand plan. What likely happened is that Google estimated the cost to fund a team to shut down ad blockers was less then the money they were losing from ad blockers. Maybe it's part of a larger initiative, but I'd be hesitant to assume that without more evidence. reply card_zero 4 hours agorootparentprevSome sites have a message like \"hey, we can't serve you ads, you must be using an ad blocker, stop that and absorb the advertising as is your duty because we need the money\". But maybe that's just desperation and they aren't losing much to ad blockers anyway. reply teddyh 4 hours agorootparentMany people believe that the loss is great, especially web site owners, which would certainly explain such messages. But lived experience shows at least me that most people don’t even know what an ad blocker is. reply rchaud 1 hour agorootparentprevThese sites aren't adding ads to punish the non-ad blocking users, they're doing it because Google Ads keeps slashing the premiums to keep more of the pie for themselves. reply bee_rider 3 hours agorootparentprevThings aren’t free, but alternative business models to ad-supported have not much of an opportunity to develop. The hope is that the feedback loop you’ve identified will iterate to the point that ad supported content becomes truly unbearable, and eventually enough room will open up that some alternative can develop. reply Drakim 3 hours agorootparentprevThe feedback loop doesn't work like that. You are implying there is some target revenue that the website aims to hit, and if it fails to meet that target it adds more ads. But that's just nonsense, if a website can get more revenue from more ads, they are gonna put more ads right away, they aren't gonna wait until their revenue drops under some magic number before they do. reply the_gorilla 3 hours agorootparentprevThat's a positive feedback loop. Ads are the root of all evil on the internet, and the end of that loop is \"no more ads\". And I don't want to hear shit about the internet dying without ads, in the same thread people are talking about cloudflare serving TBs of data for free or a $4 unmetered VPS. reply cbm-vic-20 5 hours agoparentprevThe \"fan\" in \"Fandom\" means the fan in your computer. reply setopt 5 hours agorootparentAnd the “dom” refers to how it completely dominates that fan. reply preciousoo 3 hours agorootparentOr how they use every square pixel of the dom reply aylons 3 hours agorootparentprevThe dom comes from some of the tame ads... reply razodactyl 5 hours agorootparentprev...I like you. I'm gonna keep you around... hahaha reply dcchambers 4 hours agoprevI love this post. I also LOVE wikis. I have railed against Fandom for years and I have often shared my view on this in the past[^1]. It's an absolute blight on so many beloved game communities at this point. I like this approach much more than the games that have decided to move to another managed/hosted service like https://wiki.gg - which has a very real change of becoming the \"next\" Fandom. Truly independent wikis are the best. [^1]: https://publish.obsidian.md/dakota/Hobbies/Gaming/Gaming+Wik... reply baud147258 4 hours agoparentI skimmed the post linked at [^1], but I have a doubt about that: > Fandom is actually part of the for-profit arm of Wikipedia Are you sure about this? Since Fandom got acquired by private equity in 2018, I don't think Wikipedia has any stake in Fandom anymore reply bawolff 21 minutes agorootparentThey never had a stake at any point. The connection is that 2 of the main people involved originally (jimmy & angela) had a lot of ties to wikimedia, but they were doing wikicities/wikia/fandom as their own thing, not as part of wikimedia. Also long ago there was some minor connections. They briefly shared an office like 15 years ago i think, and they tried to jointly develop a wysiwyg editor back in like 2012 (wikimedia did most of the work i think, but wikia leant a few devs to the project at one point) which eventually became the mediawiki visual editor. Anyways totally separare orgs. reply dcchambers 4 hours agorootparentprevYou're right, that's incorrect on my part. Fandom (well, Wikia) was founded and run by Jimmy Wales for a long time, but there is no official connection with the Wikipedia project/Wikimedia foundation. I will fix that. reply for1nner 4 hours agoprevIt's hard running and managing wikis, and anyone/org/group that does so outside of the auspices of fandom or similar trash-aggregation hosts should be celebrated. Love this for weirdgloop. On a related note, shoutout to liquipedia[1], which has been a great experience for so long (a number of years I refuse to recognize as it would prove I'm old), and I have always feared the possibility of it moving to or becoming a fandom. [1]https://liquipedia.net/ reply lcnPylGDnU4H9OF 1 hour agoprevI've noticed how many wikis for games I look up are on fandom and, probably because of my experience with web development, I take special note of the fact that they're always at game-name.fandom.com (I'm also a bit disappointed, like when I bought Chrono Trigger on Steam and looked up why the cat wouldn't follow me). I don't think I've ever seen a wiki with their branding/style that is not also on their domain. Perhaps some exist which use their software but I've never heard of such a vendor then also demanding a new style to be used, though I guess that's possible. Anyway, I've always doubted it to be an accident that seemingly all of their wikis are hosted on the same domain[1]. Glad to see someone doing good work about that, even if it's just incidental while they solve a different problem. Seeing the official LoL wiki on leagueoflegends.com suggests they don't intend to do the same sort of -- admittedly presumed -- widespread tracking. Regardless, it sounds like the wiki maintainers prefer working with Weird Gloop rather than Fandom and I don't otherwise have a lot of sympathy for Fandom. I have no specific bone to pick with them but I also can't help but feel glad for people who are finding other wiki software vendors. (It's also kind of interesting to see the Minecraft wiki at minecraft.wiki instead of something like wiki.minecraft.com. I guess it's a community project, just noting that Microsoft/Mojang don't seem interested in maintaining it(?). Maybe the community prefers it that way and they're respecting that.) 1: Turns out it definitely is not an accident: https://support.fandom.com/hc/en-us/articles/360021258554-I-... > We can only change the first part of your wiki's URL (i.e. example.fandom.com) - we do not support wikis outside of fandom.com. reply sph 4 hours agoprevA thing that bothers me is that Jimmy Wales, a founder of and arguably the face of Wikipedia, is also the founder and president of Fandom, Inc. (2004–present) I respect the work of Mr. Wales immensely, and I cannot explain how he has allowed his creation to become synonymous with ad-ridden borderline unusable gaming wikis. reply whstl 3 hours agoparentJimmy Wales lost my respect with Wikia itself, even before it was acquired. There was a huge push in Wikipedia in the 2010s to delete content that could be moved into Wikia/Fandom, and a huge amount of quality information was removed. It was clear the goal was to pump views in the money-making website. Then we only saw Wikia becoming Fandom and getting progressively worse. reply marxisttemp 3 hours agoparentprevJimbo is a dyed-in-the-wool libertarian, and as such I find it unsurprising that he has created both a wonderful, decentralized, communitarian project as well as a capitalistic nightmare. Libertarians are essentially anarchists who selectively turn their brains off when they see dollar signs. reply booleandilemma 1 hour agorootparentAre they turning their brains off or on? reply marxisttemp 1 hour agorootparentDepends on whether you think it’s possible for property to be used as a means of curtailing individual liberty, or whether individual liberty is fundamentally rooted in private property. I believe the former, and as such I find libertarianism to be ideologically inconsistent. reply mardifoufs 2 hours agorootparentprevI think you're talking about the other wikipedia founder, no? Jimbo might have some libertarian tendencies too but they haven't been super visible in the way he directed the wiki. But yeah, his involvement with wikia is a huge stain reply marxisttemp 2 hours agorootparentNope, I am talking about Jimbo. From that website he founded: > Wales has previously referred to himself as an Objectivist, referring to the philosophy of writer Ayn Rand in the mid-20th century that emphasizes reason, individualism, and capitalism. reply sph 1 hour agorootparentWhat has objectivism got to do with anarchism? I take offence at being put in the same pile as Rand fanatics. reply marxisttemp 1 hour agorootparentI’m an anarchist too, and I would also take offense to being lumped in with libertarians! I only meant to say that they often seem to have the seeds of an anarchism in some of their thinking e.g. individual liberty and volunteerism, but then immediately embrace contradictory positions due to their inability to critique property. reply vman81 3 hours agoprevHey, as long as they don't have those dark pattern cookie consent forms, I'm a happy camper. The EU should really have specified that accept all/decline all should be a top level choice instead of \"Accept all\" with the alternative being \"learn more\" leading to submenus for every one of the 891 \"partners\". reply thih9 3 hours agoparentThat is already the case: > The GDPR is specific that consent must be as 'easy to withdraw as to give', meaning that a reject-all button must be as easy to access in terms of clicks and visibility as an 'accept all' button. Source: https://en.wikipedia.org/wiki/HTTP_cookie#EU_cookie_directiv... reply jabroni_salad 2 hours agorootparentwhat is required is not the same as what happens in practice. Visit any wiki.gg site and see what they're doing. reply jampekka 43 minutes agorootparentThe law is not enforced. The non-enforcement is largely by design/lobby though. reply backspace_ 5 hours agoprevI have frequently said to myself, \"you know what Fandom needs? More ads\" If I'm looking for a specific piece of info that ends up being on a fandom wiki, it's quite a turn off. reply cozzyd 5 hours agoparentWhenever my phone accidentally opens fandom with Chrome rather than Firefox mobile (with uBO), I wonder how the hell anybody browses the internet on their phone without an ad blocker... reply chongli 5 hours agoparentprevIt gets a lot better with an ad blocker and other annoyance-blockers. The deeper question is whether or not you think it’s worth it. I think many people visit Fandom pages only briefly from a SERP and then take off, like Wikipedia but specific to a game. If that’s the way you use Fandom then it’s probably not worth it. What makes it worth it is if there’s a page specific to a game you like and you spend a good amount of time there reading stuff. That’s a long tail thing though. reply AdmiralAsshat 5 hours agoparentprevMost of the time I don't notice it because I use Firefox w/ uB0 on all platforms. But recently I've been playing some games on Steam and trying to use Steam's browser overlay to cache some guides. Its browser seems to be a chrome fork and does not support any kind of adblocker, unfortunately, and so I've been exposed to just how bad Fandom wikis are without one. reply rchaud 1 hour agoparentprevThat's just par for the course for any online service these days, though. It's not like Netflix, Hulu Spotify are keeping their prices flat. reply pytness 5 hours agoparentprevAlso, the site is really slow. The only thing that the site manages to turn on is the computer fans. reply tjbiddle 3 hours agoprevDecided to give OSRS (Old School RuneScape) another try after more than a decade break from the game. Without their wiki, I don't think I would've continued to play; it's open constantly - incredibly easy to use, very well up to date, and just an all around wonderful resource. Above and beyond what used to exist. reply dpedu 2 hours agoprevI was a user of one of the Fandom wikis this group took over. Moving the information to a better platform is fine. What wasn't fine is how they made every single page on the existing Fandom wiki redirect to a meme page that didn't explain what was happening. This was particularly disruptive because it made every single google result for \" \" invalid as it redirected to this useless page. Fandom has better SEO and the replacement wiki was so new it didn't appear in google results for several weeks. It was extremely annoying. reply forgotpwd16 3 hours agoprevShould mention the pessimistic possibility that Fandom buys WG and those wikis return under their umbrella. An example being Wowpedia forked off WoWWiki in 2010, moved to Curse's Gamepedia in 2013, which, Gamepedia, Fandom (then Wikia) bought in 2018. edit: Seems they moved again recently to wiki.gg. reply Washuu 43 minutes agoparentThe reason behind Fandom buying Gamepedia/Curse is both a blessing and a curse(HAH!) that would require a specific set of circumstances to happen again.[1] Basically during 2018 Curse's owners, Twitch and Amazon, wanted more head count for Twitch and to cut out anything that was not part of Twitch's main mission. The decision at the time from the Twitch CEO was to completely shut down Curse and fire everyone by the end of 2018 even though Curse was a cash positive subsidiary. That would mean turning off every single wiki with no transfer to anywhere else. It would all just be gone. So the director of Curse at the time worked his ass off find a buyer for the company. The final options came down to The Verge, Wikia, and one other that I forgot. Essentially Wikia was the only one that could promise to meet all of the buyout terms and a two year transition period of employee benefits for current employees. I'm not going to call Wikia a savior here, but without any company offering to buy Curse a lot of wikis and jobs may have been lost that December. [1]I signed some NDA about this, but it has been many years and I don't care. reply rbits 3 hours agoparentprevWith Weird Gloop they have agreements that the community can move the wiki away from Weird Gloop if that happens. For example this one is not legally binding yet, but once the minecraft wiki had a legal entity it will be: https://meta.minecraft.wiki/w/Memorandum_of_Understanding_wi... reply yakk0 4 hours agoprevIt think it's been changed, but I believe the Transformers wiki on Fandom started out as a copy of the superior [TFWiki](https://tfwiki.net). TFWiki has been referenced by many official creators and Hasbro designers themselves and has proven to be a great resource. I have no idea what their infrastructure or backup plans are, but I dread the day they go down. reply nullindividual 3 hours agoprevThe Noita wiki moved away from Fandom to noita.wiki.gg due to ads, etc. The Fandom one still exists, of course, but has no community backing and lacks information from the newer updates of the game. Unfortunately the Fandom wiki is still the first link when searching on DDG :-( reply otterpro 3 hours agoprevThe only wiki in Fandom I actually go to is the Vim Tips Wiki (https://vim.fandom.com/wiki/Vim_Tips_Wiki). But how did Vim get in a Fandom in the first place? I hate going to Vim Wiki, even though they have good tips not found anywhere, due to all the things that were mentioned in the article. 50-70% of screen real-estate is filled with ads or distractions. I hope that vim will get its own wiki instead. reply linux2647 3 hours agoparent> But how did Vim get in a Fandom in the first place? It was created back when Fandom was Wikia, back when it was a good place to host a wiki reply rbits 3 hours agoparentprevCheck out BreezeWiki (https://breezewiki.com/). It lets you view any fandom wiki with a much better UI. The Indie Wiki Buddy extension also lets you automatically redirect fandom to BreezeWiki (https://getindie.wiki/) reply kps 4 hours agoprevThere's a browser extension that provides links to Fandom alternatives on various topics: https://github.com/KevinPayravi/indie-wiki-buddy reply rightbyte 3 hours agoprevThere is something fundamental here. It used to be the case that you could form communities around commercial entities. But nowadays it seems to be too many short term profit vultures roaming around looking for targets, to not end up selling out the community. Efficient market I guess. reply Nadya 5 hours agoprevCrazy seeing a weird gloop post in the morning on HN. Cook is very passionate about wikis - as is the rest of the team - and the RS wiki has long been regarded as one of the best gaming wikis on the internet; no contest. If you run a wiki - talk to Weird Gloop. The blog isn't bullshit and they genuinely want to help. I think it's awesome that they're helping more wikis move away from Fandom after the success of the Minecraft wiki moving. They also are running a wiki for Andrew Gower's upcoming game as well. I really hope I hear about other wikis making the move in the near future. Fandom deserves to die out. The RS Wiki is the single website I've whitelisted in my ad blocker. And despite needing ads to cover costs - they made sure to ask the community first about adding them and what alternatives to funding might be possible. It was really a last resort and they are obsessive about making sure the ads are non-intrusive, single banner, not in primary real estate, and not harming the wiki experience. If any ads cause problems they completely pause running ads until the ad host resolves the issue. Although I'm usually signed in - so never see ads anyway as they only show for users who aren't signed in. reply card_zero 4 hours agoparentIf they're non-tracking ads (related to the content of the wiki, instead of the content of the visitor), I could almost like them. reply mdiesel 4 hours agoparentprevThere's also a channel on the rs wiki's discord for reporting bad ads, which Cook responds to very quickly (single digit minutes from the interactions I've seen). reply zellyn 3 hours agoprevRandom question: do you work with the new wikis to create some kind of license that prevents Fandom from scraping future changes back into their version of the wikis? Obviously the technical modifications can't translate, but it seems like it wouldn't be that hard for them to slurp most textual/markup changes back in and make it look like their version of the wiki is still alive… reply nness 4 hours agoprevOut of curiosity, how does weirdgloop pay for wiki hosting? The amount of traffic certainly wouldn't be low... what is stopping them from having to abandon these wikis in the future due to cost pressure? reply onei 4 hours agoparentThere's a recent rough breakdown of costs and funding in [1]. In short, most funding is from ads. I don't think that takes into account funding for the newer Minecraft or LoL wikis, but it'll either be funded by ads or the game devs. [1]: https://meta.weirdgloop.org/w/Forum:Board_Meeting_-_2024-03-... reply card_zero 4 hours agoprevSlightly ad hoc funding (which is probably sensible, spread it around): https://meta.weirdgloop.org/w/Weird_Gloop_Limited Some donations, some ads, and contracts (one so far) with companies that benefit. It all looks very Wikipedia-like. I wonder if the WMF could be persuaded to throw some of their massive pile of cash in this direction, in the public interest? But then Weird Gloop would probably have to be a non-profit. reply sph 4 hours agoparentGiven that Jimmy Wales is president of Fandom, I don't know if that's a good idea for WMF to get involved. reply card_zero 4 hours agorootparentHa! I didn't know that. I'm unclear on whether he actually has any influence at WMF or just serves as a fluffy mascot, but yeah, maybe not such a good idea. reply Aardwolf 4 hours agoprevI found Wikia a great product name which evoked the feeling 'this topic may be too obscure for Wikipedia, but here you can make an entire Wiki about it!', and I never understood why it was changed to 'Fandom' reply lofaszvanitt 1 hour agoprevThe most important aspect of any kind of community thing, if that involves adverts and other income options for the party that owns the platform, is to give back to the contributors. reply Destiner 2 hours agoprevI love when somebody disrupts a hidden market like that. Fandom had terrible UXs for years, but nobody seemed to care enough to make an alternative. I'd assume most users are not engineers/founders, so the opportunity was hidden for a while. In hindsight it makes total sense. reply starkparker 2 hours agoparentThere have been several competitors formed in response to or predating Wikia/Fandom over the years, particularly Gamepedia/Curse Media (which Fandom acquired). Fandom also acquired other game-focused community knowledge resources, like GameFAQs and Giant Bomb. There's also now wiki.gg, which focuses on official wikis run by game developers and was launched after the Gamepedia acquisition by Gamepedia's founder and a former Fandom president. Several wikis are on independent MediaWiki farms like Miraheze or ShoutWiki, and numerous others self-host entirely independently. This Weird Gloop effort seems to be more like wiki.gg, but for community-run wikis rather than gamedev-run wikis — bespoke relationships with communities that want to migrate or relaunch, rather than open sign-ups to a platform like Miraheze or ShoutWiki. reply gregjw 2 hours agoprevWeird Gloop have been doing a great job with the Old School Runescape Wiki for a while now, happy to see them extending that elsewhere. reply dartos 2 hours agoprevThank god. Fandom is the most unusable website I have ever landed on. reply ceroxylon 2 hours agoprevThe same thing is happening to older forums, if you browse without an ad blocker you get ads that try to trigger every emotion all at once, all of them larger than the actual content. Three cheers for weird gloop, JES, and everyone else fighting the good fight. reply zellyn 3 hours agoprevSadly, Fandom still has a lot of search mojo. For instance, when searching for \"minecraft redstone filter bedrock\" I get a link to the Fandom minecraft wiki rather than minecraft.wiki. Hopefully over time, that corrects itself. Also, the Google search results page for that search made me pine for the good old days of Google being 10 real links… reply layer8 4 hours agoprev> I don’t think we would ever do a “self-service” thing where you could just sign up and immediately make a wiki. It’s very useful, however, to have a place where that’s possible, even if that’s currently Fandom. Many wikis wouldn’t exist without that non-barrier to entry. Those that gain traction can then decide to move elsewhere. reply dianliang233 3 hours agoparentThat would be Miraheze [1]. Community funded wiki farm. However it's had some instability such as internal conflict and server issues, but it's better than all the alternatives. [1]: https://miraheze.org/ reply Imustaskforhelp 3 hours agoprevWhat is weird gloop doing exactly ? Is it hosting it on cloudflare / using cloudflare workers or what exactly (because I heard cloudflare being mentioned here) I am all ears because hosting a static site is basically free thanks to github pages / cloudflare pages , but having a site which changes a lot (a wiki can have changes be applied to at an insane rate , though I am not sure if we could use something like git as a wiki I think wikis also allow messages between users ) but is still static can cost a arm and leg reply Capricorn2481 1 hour agoparent> but having a site which changes a lot but is still static can cost a arm and leg How so? Seems like it would be trivial in PHP reply xmprt 2 hours agoprevDid you get in touch with Riot Games to be able to host a subdomain of leagueoflegends.com. If so, it's great that they're also behind this reply xcode42 4 hours agoprevBy the way, you can replace the fandom in the url with breezewiki and get a much more pleasant experience without ads. it's not that much of a difference on desktop, and the layout might debatably be uglier, but it's a godsend on mobile where the search bar doesn't even work half the time for me. reply layer8 4 hours agoparentDoes anyone know of an iOS Safari extension that allows to freely configure such substitutions? reply duxup 4 hours agoprevThese are hosted by weirdgloop.org ... but as far as I can tell without a common known good domain it's hard to know if you're looking at a \"good\" wiki or \"bad\". reply jabroni_salad 4 hours agoparentThere is a browser extension called Indie Wiki Buddy that keeps track of who the best wiki for each game is. And for the ones that do insist on using fandom, it can redirect to breezewiki which is a lite and respectful rehoster. https://getindie.wiki/ reply duxup 4 hours agorootparentVery cool, thank you. reply maverwa 4 hours agoparentprevI'd say if you cannot tell what its hosted at, its \"good\". If it shouts \"fandom\" in your face, its \"bad\". Easy! reply sph 4 hours agoparentprevI mean.. you can use your eyes to tell if it's a good wiki or not. reply duxup 4 hours agorootparentI feel like there's a lot of value when searching when you see a known good domain / would help unseat fandom a great deal. reply bakugo 21 minutes agoprev> For starters: on average, moving away from Fandom doubles the number of people editing Glad to hear I'm not the only one who actively avoids contributing to Fandom wikis because it's effectively doing unpaid labor for a corporation that only cares about making as much money as possible off of said unpaid labor. reply thoma4s 5 hours agoprevVery happy to see the downfall of fandom, on mobile there are times when the whole screen is covered by multiple ads, not to mention the lag... reply erikig 4 hours agoprevWith so many communities interacting on Discord, and given that platform's ephemeral nature, I'd recommend having a module that can summarize highlighted chats and import or append them into the wiki as a stub that needs expansion. Most of the updates I've made on Fandom were of this nature. reply bityard 4 hours agoprevDoes anyone predict Discord might end up going down the same path? reply evanfarrar 3 hours agoprevThey should promise not to become wikimedia board members. That is the main thing that allows fandom to be so bad. reply scoofy 2 hours agoprevJust a shameless plug for https://golfcourse.wiki If you’re into golf, help try to build the most thorough list of courses in the world, accessible to all. reply renewiltord 4 hours agoprevWould be cool to know what extensions you’re using on MediaWiki and how you’ve set it up to maximize performance. These wikis seem really quick to respond. reply cookmeplox 2 hours agoparentThanks! I've been meaning to write up a post that talks about some of the specific tricks we're using. A couple big ones: - Heavy use of Cloudflare Workers to cache ~95% of logged-out pageviews, with a particular focus on doing a lot of edge-side modifications to minimize cache fragmentation - Using the MediaWiki jobrunners to repopulate the parser cache before pageviews are requested, so even when pageviews hit the server, there's a high chance that the core contents have already been computed somewhere - I realized that MediaWiki latency is usually dominated by I/O wait time. For example, some pageviews require thousands of synchronous database/redis cache reads, so the difference between 0.5ms lookup and 0.1ms lookup adds up. So we colocated more of those caches on the same physical machines as the webservers that were reading them, which on average dropped latency by ~40% reply starkparker 1 hour agorootparentIs there a RSS feed on the WG blog? I couldn't track one down, and it looks like a Jekyll site, so I'm not sure if there is one. I don't want to miss that post. reply cookmeplox 1 hour agorootparenthttps://weirdgloop.org/feed.xml should do it reply renewiltord 1 hour agorootparentprevWould love to read that post. Thank you for these tips. I’ll subscribe to your feed and wait for it. reply asl98 4 hours agoprevWhat are people's thoughts on putting wikis on web3 infrastructure reply noman-land 4 hours agoparentDo it. The costs shouldn't be borne by a single entity, they should be spread across the community of users. Onboarding and lag are two big hurdles to overcome, as you will inevitably have to put editing behind a transaction. reply blackeyeblitzar 3 hours agoprevThe ads and videos on fandom are out of control. I get these distractions on top and bottom with a tiny sliver of content in the middle, basically. reply hombre_fatal 4 hours agoprevEveryone complains about Fandom, but it's the only reason 99% of the communities on its site have a wiki. Take a random game like https://endlesslegend.fandom.com/wiki/Endless_Legend_Wiki That game is 10 years old and its wiki was built in the height of its popularity when it had people to build it. The developer moved on, the community moved on. If its wiki weren't on Fandom, then its wiki would depend on some random person paying the bill for eternity for a game they themself moved on from long ago. Yeah, it has ads, but someone has to pay the bill. I'll take the ad-ridden wiki that exists over the idealized one that went offline seven years ago when the interest died out. This becomes a metaphor for the internet in general. reply teddyh 4 hours agoparentIf there actuallt exists a community, they can scare up somebody to host some infrastructure the community depends on. Otherwise the community is dead, and it’s archive.org you should be thanking. reply hombre_fatal 1 hour agorootparentThey can, but they didn't 99% of the time. And archive.org is not a replacement for a website, not even a Fandom wiki. It's horrible to use and you're lucky if it indexes a quarter of what you want, especially on a property as big as a wiki. And it's read-only. On Fandom I can still log in and make improvements. reply layer8 4 hours agorootparentprevArchive.org is awfully slow, and more importantly, the archived pages are not indexed by Google, hence aren’t discoverable. reply Ukv 2 hours agoparentprevWeirdGloop is supposedly profitable despite having only a single, non-intrusive banner ad. It's perfectly possible to run forums/wikis/etc. on even just the free tier of Cloudflare/Oracle OCI. The issue is that Wikia/Fandom, Reddit, etc. subsumed most other alternatives by offering what was for a long time a legitimately convenient and decent-quality service, but now that communities are too locked in to move (due to intentional measures like chang",
    "originSummary": [
      "Weird Gloop is now hosting the official League of Legends Wiki, transitioning it from Fandom to foster better community-driven and developer-friendly wikis.",
      "This move addresses issues with Fandom, such as retaining outdated wiki versions, and benefits from Google's increased support for independent wikis, which helps regain traffic.",
      "The transition can potentially double contributors and allow for custom technical projects, with Weird Gloop providing guidance for those interested in moving their wikis from Fandom."
    ],
    "commentSummary": [
      "Weird Gloop is assisting wikis in transitioning away from Fandom, which is criticized for intrusive ads and poor user experience.- The Minecraft Wiki's move to Weird Gloop exemplifies a successful transition to a more cost-effective and user-friendly platform.- This shift reflects a broader trend of communities seeking alternatives to Fandom, which is seen as prioritizing profit over user experience."
    ],
    "points": 675,
    "commentCount": 314,
    "retryCount": 0,
    "time": 1728559313
  },
  {
    "id": 41793597,
    "title": "Why Gov.uk's Exit this Page component doesn't use the Escape key",
    "originLink": "https://beeps.website/blog/2024-10-09-why-govuk-exit-this-page-doesnt-use-escape/",
    "originBody": "Why GOV.UK’s Exit this Page component doesn’t use the Escape key A very specific question, answered very longwindedly. Published 9 October 2024 Tagged #web-development Contents Why don’t you use Escape? 1. Escape stops the browser from loading pages 2. Escape performs other functions in the operating system 3. Pressing Escape isn’t considered a user interaction Alternative keys Shift was better, but it’s still not perfect Conclusion Bonus: Why does it redirect the user to BBC Weather? Content warning: This blog post references domestic abuse and violence but doesn’t go into specific detail. I’m not an expert in that topic at all, so I may not use the preferred terminology in all instances. Sorry. Last year (oh how time flies), we launched the GOV.UK Design System’s Exit this Page component, or EtP for short. On the surface, it’s a simple component. It’s a big red button that sticks to the top of the screen. If you click it, you’re taken away to BBC Weather. But it does so much more. If you press the ⇧ Shift key on your keyboard three times, you get a visual indicator that you are activating the button. Upon the third press, the page you’re on is blanked out and you’re redirected away to… well, BBC Weather again. It’s intended to be a safety tool. A way for people in unstable, potentially violent, domestic situations to quickly leave the page. But one question that comes up a lot is… Why don’t you use Escape? It’s a decent question. Many other implementations of buttons like this exist, and of the ones that provide a keyboard shortcut, they almost universally use the Escape key: Esc. And it’s literally called Escape. Why wouldn’t you use it for an escape function? Basically, it doesn’t quite work like that. There was a veritable rabbit hole of barriers that ultimately led us towards using a different key. Do rabbit holes normally have barriers, actually? 1. Escape stops the browser from loading pages In virtually all browsers, pressing Escape while a webpage is loading stops the loading process. This is literally the last thing we want to happen for a feature whose sole job is to load a different page from the one you’re on. If we had used Escape, it would be too easy for a user to accidentally cancel the redirection process and end up stuck on the current page, and that’d be a bit naff. 2. Escape performs other functions in the operating system In addition to cancelling loading, we found that the Escape key was used by too many other functions to reliably work for exiting the page. For example, Escape is also used to exit fullscreen media (and on macOS, fullscreen apps), close modal dialogs and popovers (including those generated by native input controls), cancel a dragging interaction, clear a text input, and dismiss notifications. Assistive technologies may also use the Escape key for functionality, and it is also used as part of keyboard shortcuts. All of these take priority over the page’s JavaScript, which often meant needing to press the Escape key more than the requisite three times, creating confusion as to whether the function actually worked as advertised. In the case of macOS and fullscreen apps, it even involved having to wait for the re-windowing animation to finish before the page could begin to intercept Escape key presses again. Very not ideal. 3. Pressing Escape isn’t considered a user interaction Do you remember popup ads? When you’d open up a website and immediately a new window would open purely to shove advertising in your face? How about autoplaying audio and videos? Or when pages begged you to stay when you tried to close them? Or those immediate “do you want notifications from us?” dialogs? These are all the result of people exploiting JavaScript’s mostly-unrestricted ability to do stuff, usually to try and make money. As a result of advertising people being bastards, more and more of what the web platform can do is now being locked behind the requirement of transient activation—that is, the browser will refuse to run pieces of JavaScript unless it is certain that some manner of user interaction took place with the intention of running it. Browsers use a few different heuristics to determine user interaction, including touches, mouse movements, and key presses. If it detects one of these that seems human enough, it will allow transiently activated code to run for a short period afterwards. This can have some unfortunate ramifications on accessibility. As we’ve recently learned on the GOV.UK Design System team, webpage interactions made by some assistive software (in this case, Dragon NaturallySpeaking) are not considered user interactions for the purposes of transient activation, so some JavaScript code will either not work or will work inconsistently for users of that software. Redirecting the user away to another page via JavaScript is one such function that requires user interaction before it will run. And Esc is the only keyboard key that doesn’t count as user interaction for the purposes of transient activation. This meant that if a user tried to use the EtP shortcut immediately after a page loaded, or after a short period having not interacted with the page (for example, if they were busy reading it), the redirection just wouldn’t work. Totally doubleplus ungood. Alternative keys With adequate evidence that Escape wouldn’t work for us, we started looking at other non-typing keys we could use. Testing with the Control key (⌃ Ctrl) worked much better, avoiding virtually all of the pitfalls of Escape, but there were still a few cons to it. Control conflicted with VoiceOver’s default configuration—where pressing Control mutes VoiceOver. Control was also the only key of those we were considering that wasn’t in a physically consistent location on different UK keyboards: While on standalone keyboards the key tends to appear in the far bottom-left, on laptops it is commonly offset inwards to make room for an additional function key. This innocuous difference had the potential to slow down a user’s ability to quickly activate the shortcut, especially if they were using unfamiliar hardware. The Alt or Option keys (⎇ Alt or ⌥ Option), by comparison, were more problematic. Most obviously: It’s named differently on different systems. Guidance and help documentation would get rather more verbose if we had to constantly call it Alt, Option, Opt, the key with ⌥ on it, and so on in every instance. The key’s use for typing letters with diacritics and other ‘alt codes’ meant that the keypresses reported to the browser weren’t always aligned with what physical keys the user had actually pressed, meaning we couldn’t be reliably sure if the user was intending to exit the page or not. Alt/Option, like Escape, also had the issue of being utilised by some browser- and system-level functions, such as a shortcut to access application menus. In Chromium browsers on Windows, for example, pressing Alt would move keyboard focus out of the webpage and to the browser UI—completely removing the user from the webpage context. This prevented subsequent key presses from being detected, and obviously made EtP functionality not work at all. Shift was better, but it’s still not perfect So we landed on ⇧ Shift. Shift works more consistently than Escape did, but still has a bunch of caveats: Its use during normal typing meant needing to be careful about which presses actually counted towards activation. Pressing Shift three times requires many more presses if the Sticky Keys accessibility features is active. I found that it took between 6 and 9 presses with Windows Sticky Keys, depending on speed, and 9 presses with macOS Sticky Keys. macOS’s Slow Keys feature still takes three presses, but they have to be spread across a period of a few seconds to avoid activating the ‘latching’ functionality. The JAWS screen reader in Chromium would register the first Shift keypress twice, though this has potentially been fixed since. Using Shift means that the shortcut is available on touch device’s virtual keyboards. However, these keyboards tend to behave unpredictably compared to their hardware equivalents. I wrote up a massive bug report just about how the Shift key behaves on iOS, part of which subsequently became a bug report. Despite these shortfalls, Shift was ultimately the least flawed of the choices presented to us, with most of its issues being fairly minor and none of them being showstopping. Conclusion So that’s where we are now. In some ways, this is an unfortunate case of technology and web standards working against what would be the ideal user experience. Esc would be the ideal key to use, but it has too many caveats to work consistently. We looked into opening the redirection page in a new tab and automatically closing the previous tab, as some similar tools do, but user research found that this often caused confusion (‘Why doesn’t the back button work?’, ‘Where did my work go?’) and doing that is also subject to transient activation requirements. We would’ve also liked to have had the button overwrite or erase the user’s recent browser history, but we can’t do that either. (And for good reason!) Ultimately, we had to land on the philosophy of doing better, but not aiming for perfection. We could never write a piece of JavaScript that could prevent domestic violence, prying surveillance, or controlling relationships—we can only do what we can do. No one has complained about us using Shift instead of Escape, nor have we received any bug reports about it not working, but it certainly raises an eyebrow when folks hear about it for the first time… As for the other major question we get… Bonus: Why does it redirect the user to BBC Weather? As civil servants, we didn’t want to link to a news service as that could lead to claims of political bias. I also argued (a lot) against linking to the Google homepage—despite this being ridiculously common—because… who actually uses the Google homepage? Google search is right there in your URL bar. Even if you do go to Google’s homepage, the first action you normally take once there is to leave Google’s homepage. If you walked in on someone and they were nervously staring at Google’s homepage, you’d be suspicious as hell. BBC Weather’s homepage is a content-rich page. Users have a reason to be looking at it and to be looking for an extended period of time. It may even default to the user’s location, displaying significantly more personalised content in the process, which is surely less suspicious than a blank, generic search form. Thought this was neat? Why not share a link to this page?",
    "commentLink": "https://news.ycombinator.com/item?id=41793597",
    "commentBody": "Why Gov.uk's Exit this Page component doesn't use the Escape key (beeps.website)355 points by todsacerdoti 20 hours agohidepastfavorite215 comments sambeau 6 hours agoThis smells to me of a team overthinking something so much that they land on something unintuitive. It smells of \"over-fitting\" — a solution way too specific when something general and flexible is needed. Pressing shift three times is clever… but way too clever. Even if you stick a giant popup saying \"hit shift three times to quickly exit\" I'm not sure anyone in a panic will remember—loads of people don't even know which key is shift, especially when there's three buttons on a keyboard that look the same and only two are the same. I've come across people who always use shift-lock and did't realise you could use shift for anything. I'd be interested to know what UX tests they actually did, and who with. If I was going down the press a key three times, I would have gone with pressing any key three times apart from the number keys (plus an info box when you enter the page—\"hit any key 3 times to quickly exit to the weather\"). Most people, I'm sure, would mash the spacebar in a panic but if they missed then it would still work. What I would have preferred to test would be 'mashing'/chording — pressing more than one non-modifier key at the same time, so a user could just smash a load of keys at the same time in a panic. Going to the Weather page is a great idea, though. reply TZubiri 3 hours agoparentHard disagree. First of all. Not using escape key to escape is the standard for almost all applications since the 90s. Do you use escape to close the browser? A tab? your email client? No. All software converged on the idea that a close button was not a good idea, we are left with the actual button as a vestige. Second of all, this software is designed for people in high stress situations where one of their main goals is to avoid detection, they will not only memorize the escape sequence, but they will likely have their finger on the shift key at all times. reply int_19h 2 hours agorootparentUsing the Escape key to close dialogs is the standard for almost all desktop applications since the 90s, though. reply duxup 2 hours agorootparentprevI have my doubts about how sure you are that people in high stress situations will memorize the escape sequence for one website. I think as devs we often think of our site or application as the center of the user's universe, but I don't think users memorize the minutia of our applications like we think / would hope. Also, I actually worked with folks in abusive relationships at one time, their actions are not as predictable as you might hope. reply TZubiri 2 hours agorootparentAh, I misunderstood the scope of the tool. I thought this was a tool that users specifically install in order to browse any content. But instead it seems this is simply a feature so that users that browse gov.uk websites specifically can exit. reply koala_man 2 hours agorootparentprev> one website I'm guessing gov.uk is hoping that this will become some kind of standard, at least for British resources. reply ceuk 2 hours agorootparentprev> I think as devs we often think of our site or application as the center of the user's universe Jakob's law is a thing but I actually think in the case of GDS they are in the fairly rare position of perhaps being able to justify the hubris you speak of slightly. Not only are they directly or indirectly responsible for the UI of a frankly staggering number of online services, they are also one of the most influential bodies - perhaps in the world - when it comes to this sort of thing. reply duxup 2 hours agorootparentMy only concern about setting a standard (beyond the usual process of setting a standard) it's that a standard for what exactly? All the other government sites that ... you don't need this key sequence on? For the user I think that still means asking them to memorize something odd for a very limited use case that you won't think of visiting any other government site. reply cj 2 hours agorootparentprevI think the OP's main point is \"press shift 3 times\" is a very uncommon and unintuitive keyboard shortcut. What do you disagree with? reply Alupis 1 hour agorootparentprev> Second of all, this software is designed for people in high stress situations where one of their main goals is to avoid detection FTFA: `It’s intended to be a safety tool. A way for people in unstable, potentially violent, domestic situations to quickly leave the page.` This is the craziest part of this entire article to me. The UK Government needed to invent a whole design system that included an \"ejection seat\" button in case you're caught looking at UK Government websites? Or does this button exist because one website in particular needed this feature? Over design much? reply SoftTalker 2 hours agorootparentprev> Not using escape key to escape is the standard for almost all applications since the 90s. Really? I always hit \"escape\" when I get a popover on a website, and it often works. Many TUI interfaces use it for \"go back\" or \"exit\" e.g. BIOS settings. reply SilverBirch 2 hours agoparentprevI think it's useful to think about the same way you think about test specificity. Ie, of all the people in the world that hit this page, how many of them are going to need this feature and use it correctly vs. how many don't need this feature and accidentally use it. Using the Escape key is fantastic for \"I needed this feature and it worked\", which is probably 1 in 100,000 users of the page. It's terrible for \"I accidentally used this feature I didn't know about\" and that's the other 99,999. All your other suggestions fail for this reason too - you need a high level of confidence the person really intended to escape. I for example would mash the space bar three times to scroll down. reply monkpit 2 hours agorootparentAlong these lines, in the GitHub discussion they show a graph of the number of times the button was pressed, bucketed by the platform the user was on, which is all utterly useless info. It should be normalized as a percentage of page views at the very least. They’re basically saying “hey we added a big red button and people press it sometimes”. The button could say “fire lasers at my cat” and some amount of people would press it (whether intentional or not). reply amelius 4 hours agoparentprevOur browsers just need a boss-key. reply Suppafly 2 hours agoparentprevHitting shift 3 times happens just by holding the button down too long while typing caps sometimes too. I constantly have sticky keys coming up when inadvertently holding down shift and getting distracted while typing. reply jermaustin1 2 hours agorootparentI've definitely triggered sticky-keys with my shift before, but I can't remember a time it was while typing - potentially while shift + arrow to highlight, though. But it is one of those features that I turn off the second it annoys me 1 too many times. reply Suppafly 36 minutes agorootparentYeah anytime I'm on a fresh install of Windows, it seems to happen pretty quickly and then I turn it off. reply crimsoneer 5 hours agoparentprevYeah, this was my reaction... I wonder if they collect logs of how many people use the triple shift function. I do like GDS' focus on research and service design, but this feels slighly over-engineered from that space. reply monkpit 2 hours agorootparentThe logs are just noise without a way to prove the users’ intention to use the triple-shift feature for its intended purpose. Maybe you could normalize it by listening for triple-shift presses on all pages on the site (not just sensitive ones) and calling that a baseline of accidental events. But, how do we know that events in the baseline are truly accidental? What if users learned the behavior and tried using it on pages where it’s not implemented? There’s just no good way to get analytics on this feature without interviewing users somehow. reply bckygldstn 15 hours agoprevA similar initiative in NZ is Shielded Site [1]. Many large sites (eg The Warehouse [2]) participate by putting an icon at the bottom of their website. When clicked, a modal pops up with domestic abuse resources. There’s a prominent exit button that closes the modal faster than a page navigation or finding the close tab button. Closing the popup returns you to a major website rather than a new tab page. And most importantly, your history contains no evidence you viewed the information. [1] https://shielded.co.nz [2] https://www.thewarehouse.co.nz reply stavros 10 hours agoparentUnfortunately, clicking outside the modal (by far the biggest target to hit) doesn't actually close the modal, you need to click the (relatively small) close button. reply echoangle 14 hours agoparentprevwindow.onload = function(){} Shouldn’t this be addEventHandler? Otherwise, you can only have a single onload callback, right? reply marcosdumay 6 hours agorootparentIt should be addEventHandler if you want to have more than one handler, yes. Otherwise, it's fine. reply fennecfoxy 9 hours agoparentprevAs a Kiwi I miss the ware whare! However I am extremely disappointed to see that the questions section of that starts out gender neutral and then basically does the usual \"if you're a woman being abused by a man...\" There is still no support for male victims of domestic violence, whether the abuser is male or female. :/ it's not hard to cater to all cases, no wonder men don't bother - particularly when it's reported that male victims who resort to calling the police are most often the one handcuffed/detained when they arrive. In before someone comments something that we've all heard before - it's not a competition, both women & men can be helped by the same system, regardless of supposed statistical likeliness, etc. reply pushupentry1219 8 hours agorootparentThis is very fair. I have a close male friend who was the victim of intense domestic violence, physical, emotional and financial manipulation by his ex partner. He talks about how child support staff (like reception for example) are, are not favouring of him. They see DV in his profile and assume he's the perpetrator instantly. He had to explain himself constantly, no doubt reliving trauma when he does. He has been struggling with the courts to gain sole custody of his child. And to top it all off all the posters around these places are, like you say, about women reaching out against their abusive male partners. Which IS an issue and IS statistically more likely. But you make a very good point about these systems being able to help both. reply KMag 6 hours agorootparent> .. women reaching out against their abusive male partners. Which IS an issue and IS statistically more likely. Be careful about your phrasing there. I hope the implied subject on both sides of the \"and\" is different. Women being victims is an issue, and women reaching out is significantly more likely. Women reaching out is (obviously) not an issue, but is statistically more likely. Alternately, women being victims is an issue, but the statistical likelihood of women being victims is unknown, and we have good reason to believe there is significant reporting bias. reply failingslowly 8 hours agorootparentprevThank you, this needs to be repeated whenever this situation arises. reply Throw38495 6 hours agorootparentprevUK minister is trying to close All female prisons. They are already only 4% of prisoners, but that is it enough. So much about accountability. > men can be helped by the same system That is just a misinformation! Calling police if abuser is a female, and you are a male, is a VERY bad idea. Without police you only get some bruises. With police you get escorted in handcuffs in front entire neighbourhood, get fired from job, pay very expensive lawyers, get criminal record and possible prison time! There is no way to fix that, just leave and drop all contact! reply crote 11 hours agoparentprev> There’s a prominent exit button that closes the modal faster than a page navigation or finding the close tab button. I spent about 30 seconds figuring out how to close it. The icon in the top-right? No, that goes to the start page. Perhaps the icon in the top-left? No, that goes to the main menu. Clicking outside the modal, like most other websites? Nope, doesn't work. Turns out the close button is the half-circle at the bottom of the modal, which is exactly the same color as the rest of the modal. It's pretty obvious once you see it, but it took me way too long to find. They should've either placed it in the top-right like literally every other close button ever, or made it bright red so it's impossible to miss. reply frereubu 11 hours agoparentprevThis is a great idea. I can't see the icon on the Warehouse site though - can you point me to it? How do people come to know about what the icon does? reply ClearAndPresent 11 hours agorootparentThe icon is the teal/white circle just in line and to the right of the social media icons at the bottom of the page. I missed it on first glance and would have no idea what it did. reply frereubu 11 hours agorootparentOh. I thought that was a light mode / dark mode button... Unlikely on a retail site I guess, but discoverability feels pretty bad. It's not like you couldn't just write \"suffering from domestic abuse?\" on there because the person doesn't have to click it in situations where that would be risky, and could come back later if they spot it at the wrong time. reply thecatspaw 5 hours agorootparentI think the idea is that you can tell people \"hey, if you're suffering from abuse, you can check a websites footer for this icon to get help\" reply GenerocUsername 4 hours agorootparentThis has probably helped so many people.... In the imaginations of other people reply teruakohatu 11 hours agorootparentprevI had to hunt around to find it. Bottom right, aligned with \"Corporate\" in the footer links. Next to the Facebook icon. reply arp242 16 hours agoprevIdeally this should pre-load the BBC weather page so switching to it is (near-)immediate. Currently it can take a while to load. Replace all DOM and then replace URL should do it. There is also the matter of history; if I load the demo page, click that button, and press \"back\" then I'm on the demo page again. And of course it'll be in the browser history. I have to question how practically useful this is. Ctrl+W or middle click on tab isn't that far off. Or open private window and close that, which is a smart thing to do anyway. Never mind that computers and internet access is ubiquitous enough these days that \"using the family computer\" for this sort of thing isn't really needed in the first place. Overall this seems like a IE5-era solution that's pretty outdated and useless today. Perhaps even worse than useless because the implementation is so-so and protection it offers low. Overall, I'd say telling people to use private windows and teaching then Ctrl+W is probably better. reply seszett 14 hours agoparent> Never mind that computers and internet access is ubiquitous enough these days that \"using the family computer\" for this sort of thing isn't really needed in the first place. I'm just glad you're not in charge of this kind of services because although that might seem like an obvious thing to you, the reality is that the people needing that information the most are the ones who are the least likely to have easy access to a personal device with Internet access. In particular, children and women in dysfunctional, abusive relationships are not very often provided with a smartphone and a data plan by their abusers. I agree that the shift shortcut is unlikely to be of much use, but it's just one available method in addition to the rest. reply graemep 11 hours agorootparentAbused men have similar problems although we are probably less likely to have no internet access restricting and monitoring communications is a common part of abuse. My ex wife did not want me to get a smartphone and, in retrospect, it was because it let me keep in closer touch with family abroad (which is the main reason I have one at all). She also got very upset when I changed the password on my desktop some years previously. reply yreg 9 hours agorootparentprev> I'm just glad you're not in charge of this kind of services Why are you attacking the user instead of just focusing on the argument? reply port19 7 hours agorootparentBad internet habit, just asssume that \"you\" refers to the hypothetical person made of nothing but that one expressed opinion reply xunil2ycom 1 hour agorootparentprevI want to thank you for this comment. I had read the entire article thinking incorrectly about this. I thought it was for people who didn't want to see the material to navigate away, and kept thinking \"just turn your head, close your eyes, hit the back button\". Then I saw your comment and realized I was entirely wrong about how I was thinking about this. I get it now. reply Ntrails 10 hours agorootparentprev> I agree that the shift shortcut is unlikely to be of much use, but it's just one available method in addition to the rest. I don't know how the relevant user is informed about the option/feature, but assuming they're aware it is a positive feature both in terms of thoughtfulness and execution. Be interested to see the stats on how often it gets called reply arp242 8 hours agorootparentprevI think it should be obvious from the full comment that I don't think that doing _something_ for this is useless. Most of my comment is about how this is not actually sufficient to protect people. And \"we need to do something for this\" doesn't mean that this particular feature/button is a good idea. Like I said, telling people to use private windows and teaching them Ctrl+W seems like a better solution to solve the same problem to me. You can have a widget with some basic tips, and you can even show the correct instructions based on the browser the person is using. reply tourist2d 14 hours agorootparentprev> women in dysfunctional, abusive relationships are not very often provided with a smartphone and a data plan This sounds like something which you have no evidence at all for claiming. reply jakkos 12 hours agorootparentI knew a person who was in abusive relationships where the abuser would keep making ridiculous claims that the person was cheating on them, and made them give up having their own phone as \"proof\" that they wouldn't cheat. Of course, the abuser was cheating the whole time. reply guappa 12 hours agorootparentprevThey were kept without computers and internet access: https://www.bbc.com/news/world-europe-19711022 So it does happen, contrary to what you claim. reply seszett 14 hours agorootparentprevI don't have evidence but I do have experience on this. I'm not sure why you would be the quickly dismissive of something that would seem obvious to many. reply Aeolun 13 hours agorootparentprevI mean, it’s a ‘water is wet’ kind of statement. Prisoners aren’t provided a mobile phone and data plan either. reply Moogs 15 hours agoparentprev> Ctrl+W or middle click on tab isn't that far off The point of shift x3 is that it's consistent across keyboard layouts including laptops. I have a laptop where the location of the ctrl key is moved inward to make room for the function key. I frequently hit Fn instead of Ctrl and don't realize what's happening until I look at my keyboard. And that's not when I'm in distress. Same goes for middle click. It's not a consistent interaction. On some laptops you can left click and right click to get a middle click. On my laptop, it's a three finger tap. > Never mind that computers and internet access is ubiquitous enough these days that \"using the family computer\" for this sort of thing isn't really needed in the first place. In a normal situation, this is true, but this is UI design for people in extraordinary situations. Their abuser may have taken their cellphone or other devices and may not have a choice in what computer they use or when they have access to it. Nothing about this prevents private windows or Ctrl+W (assuming they have another window open so it doesn't look suspicious that they're staring at a blank desktop), it just gives victims a quick action they can take to prevent immediate retaliation. reply eviks 12 hours agorootparent> I frequently hit Fn instead of Ctrl and don't realize what's happening until I look at my keyboard. And that's not when I'm in distress. Same goes for middle click. It's not a consistent interaction. Triple Shift that you can only on a single website is worse since you're even less likely to be able to use it in distress Besides, as a site you can try to add typo-similar combinations for your \"hide\" action (like alt+w or win+w) instead of creating a totally different one reply vladvasiliu 12 hours agoparentprev> I have to question how practically useful this is. Ctrl+W or middle click on tab isn't that far off. Or open private window and close that, which is a smart thing to do anyway. Users probably don't want to attract attention by using a private window (which they may or may not think about using), and most browsers I've seen have a distinct appearance when in private mode. Ctrl+W in normal mode has the issue of leaving a trail: Ctrl-Shift-T or similar will bring it back. reply arp242 8 hours agorootparent> Ctrl+W in normal mode has the issue of leaving a trail: Ctrl-Shift-T or similar will bring it back. That also exists with this button: just press \"back\". Even easier. reply erinaceousjones 6 hours agoparentprev> Overall, I'd say telling people to use private windows and teaching then Ctrl+W is probably better. Yes, you should do that as well as understand that, for things like this, where you're providing information for vulnerable people across an entire population, your people are going to span a huge range of technical literacy and you will not be able to reach all of them in time. Give them the big red escape button with the special \"dial 999\" style memorable key combo as well as teach them everything else. But triage and do the \"this solution works for the broadest number of people the quickest\" thing first - the big red button. reply grujicd 11 hours agoparentprevSamsung Magician on Windows uses CTRL+W as a global shortcut and then it doesn't work in browser anymore. That took a while to figure out. reply Toutouxc 8 hours agorootparentThat's completely idiotic and whoever came up with that (apparently it even blocks crouch + walk in some games) should be tarred and feathered. reply rafram 6 hours agoparentprevYou can only replace the URL with another URL on the same domain. Otherwise a site could make itself look like Google and then replace its URL with Google’s, and you’d have no way of knowing that it isn’t Google. reply bjoli 12 hours agoparentprevI think you are underestimating how much being in an abusive relationship or even just poverty in general (poor people are more likely to be abused, so they're double punished) reduces your options and opportunities. This goes for everything. Place where you live. The food that is on offer. Work opportunities, and with that the ability to plan life. Even living large enough to have a private space, like offering your kids an undisturbed place to study or - like in the post - somewhere you can safely report abuse. I have seen it more than once: if someone from a poor family grows up and does really well in school and in college and breaks with the life they had before that is usually not enough. Because when there is time to write a CV the kids from the middle class all had parents that made them do other things. Charity work. Play the trumpet with a youth orchestra that somehow got to play in Carnegie hall. Chemistry camp. Dancing with a youth ballet company at the met. The system is rigged from the start. True meritocracy was never a thing. A feature like this takes a developer a short time to implement, and if it saves someones life or stops abuse it is worth it. reply labster 11 hours agorootparentYour description is exactly meritocracy under the original definition. The second kid has earned all of the merits, and the ones possessing the most documents of merit get ahead. reply int_19h 1 hour agorootparentIt would be meritocracy if both kids had equal opportunity to earn those merits. If the ability to do so is itself gated, it's only meritocratic within the privileged group. reply labster 39 minutes agorootparentNo, equality of opportunity is specifically not needed for a meritocracy. It wasn’t in the original book[1], it didn’t happen in the old Chinese examination system, and it sure doesn’t happen now. Merits are measurements, and society adapts to make those measurements a target. [1]: https://en.m.wikipedia.org/wiki/The_Rise_of_the_Meritocracy reply bjoli 5 hours agorootparentprevFair point. But: the way the modern meritocracy is motivated is that it is a fair system. It is the whole idea of the American dream. Work hard and you can go anywhere. Except some people have to work a lot harder and be a lot smarter. reply lupusreal 8 hours agoparentprevBelieve it or not, a lot of users don't understand the control key and are afraid to touch it because they think it might break their computer. They may not even be able to readily find it on their keyboard since they aren't accustomed to using it, but do tune out and skim over the things on their computer they think they can't understand. reply jandrese 2 hours agoprevIt seems like the shift key is still problematic, especially if it is conflicting with stickykeys. Why not use for example the letter 'q'? You could set it up as a mnemonic that you need to quit quit quit as fast as possible. But for the most part I agree that this is silly and unnecessary. Ctrl-W is a better solution and this would really only make sense if it also scrubbed the site from the browser's history at the same time. In fact this solution is worse because the abuser can just hit the \"back\" button when they see BBC Weather loaded. reply frereubu 12 hours agoprevDoes anybody have any stats for the use of these kinds of buttons? A few of our clients - victim services and honour-based abuse services - ask us to add these kinds of buttons, but I've always wondered they actually get used instead of e.g. people just closing the browser window. The issue for us with adding tracking is that it would slow the interaction which, even if it was only a few milliseconds, isn't something we want to risk. (Or worse, if the JS it breaks and the link doesn't work). I guess it would have to be some kind of post-hoc survey for victims of domestic abuse who've used a site and are now somewhere safe. Edit - thanks to @jdietrich below there are some stats on this link, which shows a correlation between events you'd expect to increase the rush of domestic abuse, such as the Covid lockdowns: https://github.com/alphagov/govuk-design-system/discussions/... I do wonder how they got those stats though. Edit 2 - I'm so glad this got posted! I've been wondering about this for ages and it's really nice to get some evidence for its use. Reading through the comments has also solidified my thinking around \"why don't people just close the browser window\" - many people who use honour-based abuse services are very computer illiterate, don't have time to learn about incognito windows / (CtrlCommand) + W, and can only snatch computer time here and there. Abusers can look back at the browser history, but if the choice is between being discovered on an honour-based abuse website or the chance that the abuser won't look at the history, the second is clearly superior. Edit 3 - I really wonder about the three-press shift keyboard shortcut. Real lack of discoverability, and my worry would be that the lack of consistency across sites would lead to situations where people are on non-gov.uk websites and think that keyboard shortcut would work there too. Although I suppose the fact that the first shift press activates the button in some way does tie it to the presence of the button on screen. Edit 4 - It doesn't seem to be in use on any relevant gov.uk pages. The pilot on the \"check for legal aid\" pages seems to have ended and it's not on the pages about domestic abuse. reply closewith 11 hours agoparentMost probably they're using the sendBeacon method triggered by the visibilitychange event. sendBeacon doesn't delay the unload and asynchronously makes the network request simultaneously. https://developer.mozilla.org/en-US/docs/Web/API/Navigator/s... reply frereubu 11 hours agorootparentThanks, that's really useful to know - I might try and implement something like that on the sites that we run. reply thecatspaw 5 hours agoparentprevcan you expand on what honour based abuse means? reply amiga386 5 hours agorootparenthttps://en.wikipedia.org/wiki/Honor_killing > An honor killing (American English), honour killing (Commonwealth English), or shame killing is a traditional form of murder in which a person is killed by or at the behest of members of their family or their partner, due to culturally sanctioned beliefs that such homicides are necessary as retribution for the perceived dishonoring of the family by the victim. > Methods of murdering include stoning, stabbing, beating, burning, beheading, hanging, throat slashing, lethal acid attacks, shooting, and strangulation. Sometimes, communities perform murders in public to warn others in the community of the possible consequences of engaging in what is seen as illicit behavior > Often, minor girls and boys are selected by the family to act as the murderers, so that the murderer may benefit from the most favorable legal outcome. Boys and sometimes women in the family are often asked to closely control and monitor the behavior of their siblings or other members of the family, to ensure that they do not do anything to tarnish the 'honor' and 'reputation' of the family > Sharif Kanaana, professor of anthropology at Birzeit University, says that honor killing is: \"A complicated issue that cuts deep into the history of Islamic society. .. What the men of the family, clan, or tribe seek control of in a patrilineal society is reproductive power. Women for the tribe were considered a factory for making men. Honor killing is not a means to control sexual power or behavior. What's behind it is the issue of fertility or reproductive power.\" > Nighat Taufeeq of the women's resource center Shirkatgah in Lahore, Pakistan says: \"It is an unholy alliance that works against women: the killers take pride in what they have done, the tribal leaders condone the act and protect the killers and the police connive the cover-up.\" The lawyer and human rights activist Hina Jilani says, \"The right to life of women in Pakistan is conditional on their obeying social norms and traditions.\" > Fareena Alam, editor of a Muslim magazine, writes that honor killings which arise in Western cultures such as Britain are a tactic for immigrant families to cope with the alienating consequences of urbanization. Alam argues that immigrants remain close to the home culture and their relatives because it provides a safety net. She writes that 'In villages \"back home\", a man's sphere of control was broader, with a large support system. In our cities full of strangers, there is virtually no control over who one's family members sit, talk or work with.' Hopefully that expands on it. A rotten culture of \"family values\" that sees women as nothing more than baby factories and keeps them under control at all times, through intimidation, persecution, monitoring, and straight up state-sanctioned killing and blaming of the victim if they try to assert themselves. reply davedx 10 hours agoparentprevYeah the fact that there's no concrete demo beyond the basic JavaScript snippet/demo makes me wonder how well this actually works. I wanted to know how users are informed to press shift repeatedly to use the button? It's weird UX. It does remind me of \"boss keys\" that old DOS games used to have. reply appendix-rock 18 hours agoprev> It’s intended to be a safety tool. A way for people in unstable, potentially violent, domestic situations to quickly leave the page. An upsetting but nonetheless incredibly interesting abnormal UX problem to solve. I appreciate seeing this much thought being put into things like this. reply kranke155 7 hours agoparentGov Uk UX team I believe is doing some of the finest work in the world. reply construct0 10 minutes agoprevTried example. No redirect occurred after 3 SHIFT presses, had to use both ESC and SHIFT to trigger it somehow. The irony. reply layer8 2 hours agoprev> And Esc is the only keyboard key that doesn’t count as user interaction for the purposes of transient activation. It’s pretty weird that pressing the Shift key is considered more of a user interaction than pressing Escape. reply kortilla 18 hours agoprevI’m curious about this history of this. What page are people on that might lead to domestic abuse? What do they use frequently enough that they would learn about this exit functionality rather than just clicking a bookmark bar, closing the tab, or just switching the tab? This seems like such a contrived scenario with a solution that only works for gov uk sites. Why not teach users how to switch or close tabs with keyboard shortcuts? reply kelnos 17 hours agoparent> What page are people on that might lead to domestic abuse? I assume there's a .gov.uk page somewhere that lists resources for people who are in abusive relationships. I imagine if an abusive partner walked in to find you reading that, that might set them off. reply kortilla 14 hours agorootparentSure, but are they going to spend a bunch of time to learn how to use the magic exit button or just press ctrl-w to close the tab? reply youainti 13 hours agorootparentI am highly technical (multiple linux machines at home) and I don't use ctrl-w. I didn't know it was a thing. reply int_19h 1 hour agorootparentI'm mildly surprised because it's been adopted fairly universally for multi-document / multi-tab apps. E.g. most editors with tabs will also use it to close the current document. reply TeMPOraL 11 hours agorootparentprevI do, but only because it's a stupid-ass shortcut I keep triggering on accident. reply thaumasiotes 10 hours agorootparentI don't really mind triggering ctrl-W by accident because ctrl-shift-T will undo the mistake. An accidental ctrl-Q is much worse, because closed incognito windows can't be recovered. reply umanwizard 7 hours agorootparentI think all the major browsers can be configured to prompt before quitting. reply scott_w 13 hours agorootparentprevIf it’s the only tab you have open, it’ll look very suspicious that you’re just staring at the desktop… reply PaulRobinson 12 hours agorootparentprevImagine your abuser “lets you” use the computer for one hour a day. They monitor your browser history. They read your texts, your social media DMs, and browse your search history. They often watch you browsing, save going to the fridge to get a beer or to go to the bathroom. These are the moment where you think about trying to find help. It’s all you think about really: how to get out. How likely are you to know keyboard shortcuts? As a UX designer, would you not want to make a big safe UX button that you need no prior training or experience of, that you can trust to help you get out of a difficult situation. Footsteps. Oh shit. They’re coming back. Is it Ctrl-W? Or Ctrl-V? Oh fuck, he’s nearly in the room. Quick, where’s the tiny little cross to close the window… oh, wait, click that exit page button, or just quickly hit shift a bunch of times. “Oh yes, I was just looking at the weather for tomorrow. I was thinking about whether to put some washing out on the line…” reply eviks 12 hours agorootparentThis scenario is contrived > just quickly hit shift a bunch of times How would you even know about this shortcut you never use anywhere, let alone remember it in a time of stress? reply rsynnott 7 hours agorootparentSee things like https://en.wikipedia.org/wiki/Ask_for_Angela In principle, information about this could be propagated, if it's reliably available on UK govt sites at this point (I'm not sure if it is). reply eviks 3 hours agorootparentThis discussion is about the current practice where a more widely used Ctrl+W is hard to remember, but somehow a niche 3xShift isn't, not a potential future info campaign reply closewith 10 hours agorootparentprev> This scenario is contrived This is a much more realistic user story than 99% you will ever read. reply robertlagrant 9 hours agorootparentprevThe main question is: how do you know to hit shift a load of times? Is that a standard thing being taught to people? reply easton 17 hours agoparentprevAnother example: There’s a page in the iOS settings where you can remove people from your family group and change your password (or do other things you might do if someone was after you). It has a “quick exit” button that kicks you back to the Home Screen, but also completely kills the Settings app so said person wouldn’t know you were on that page if they yoinked your phone. https://support.apple.com/guide/personal-safety/how-safety-c... reply jdietrich 12 hours agoparentprevThe MVP for this component was on the form to start an application for a restraining order. The design team fully explain their rationale and research on the project Github. https://github.com/alphagov/govuk-design-system/discussions/... reply londons_explore 7 hours agoparentprev> This seems like such a contrived scenario Agreed. I suspect the number of people assisted by this button is vanishingly small, and outweighed by the number of people who don't get the information they're looking for because they accidentally click the button and can't find their way back. Or the number of people harmed because the \"exit this page\" UI is on some pages only (for example, it isn't here on HN), and that is even more confusing for users who aren't tech savvy enough to realise its part of the site not the browser and who could come to rely on it. Overall, I think this button is poor UX and shouldn't be used, even on pages with sensitive content that it is intended for. reply zerovox 17 hours agoparentprevThere's some examples (and a pretty sad graph on _when_ users are looking at these resources) on the user research summary: https://github.com/alphagov/govuk-design-system/discussions/... reply rjknight 17 hours agoparentprevI would really like to know whether this feature gets any (non-accidental) use. It's certainly an important problem to solve, and I can see the technical merit in the solution proposed. What I'm left wondering is how this solution is most effectively communicated to the people who need to know about it, such that they're able to make use of it correctly in the critical moments when they need to use it. For obvious reasons there are probably no good statistics on this, but I wonder what the user research was like. reply froggerexpert 18 hours agoparentprev> This seems like such a contrived scenario with a solution that only works for gov uk sites. Why not teach users how to switch or close tabs with keyboard shortcuts? +1. \"Close tab\" is more robust, well-supported and well-known. It seems more likely a user will load an inoccuous page as a decoy, than learn triple-shift is a quick exit. Still, interesting read, to hear the reasoning. Would like to see empirical evidence/user testing. reply scott_w 13 hours agorootparentIf it’s the only tab open, you’ll raise suspicion if your partner walks in to you staring at the desktop reply robertlagrant 9 hours agorootparentI think the point is learning to have two tabs open, one incognito, will work everywhere for all resources, whereas this bespoke interaction needs to be memorised just for this websites. reply oneeyedpigeon 10 hours agorootparentprevIt wouldn't be the desktop, would it? Wouldn't it be an 'empty' browser window? Still just as suspicious, of course, but I wonder if some/all browsers do something special in that case—e.g. default to the home page. They certainly could, as could a plugin. reply scott_w 10 hours agorootparentChrome closes the window on the last tab. It's splitting hairs, however. As you said, it's still raises suspicion which, to a person in a domestic violence situation, is not what they want. reply TheRealPomax 15 hours agorootparentprevvsreply froggerexpert 14 hours agorootparentI understand the happy case. When it works, great. My critiques were on the sad cases: * Presses . Wait why isnt this working? Too late. * Presseson another sensitive site that doesn't implement this. Too late. * Presseson a poorly supported browser, or after the functionality is removed, or after it conflicts with OS-level (it might not today, but who knows about future OS updates) reply PaulRobinson 12 hours agorootparentWe should probably bake it into browser standards then. reply oneeyedpigeon 12 hours agorootparentAbsolutely. This would solve the above problems, plus any problems involving JavaScript bugs that would render the whole thing inactive. Just a shortcut to go to the root of the site seems appropriate. Or maybe sites could configure themselves for a \"safe site\" equivalent if their whole content is a risk. reply kortilla 13 hours agorootparentprevThe timing of those two scenarios is different. Either the abuser walked in while the person was still on the page with the big red button or not. It is not faster to press the big red button or shift 3 times than it is to close a tab. reply logifail 12 hours agorootparent> It is not faster to press the big red button Indeed. Surely Ctrl+W (with a 2nd decoy tab already there and at BBC Weather) is 10x faster than finding and clicking a button on the page you're reading? EDIT: another issue with the Exit This Page as implemented on eg https://www.camden.gov.uk/planning-to-leave-an-abuser - if you open it in a private browsing session, and click it, it sends you to Google, but of course there the first thing you get is the massive cookies pop-up. So wouldn't that be a bit of a red flag to whoever just walked in? :/ reply eviks 12 hours agorootparentprevPartner walks in They see a page changing Black eye reply yakshaving_jgt 10 hours agorootparentOr, perhaps even more likely, abuser stealthily enters the room and silently observes the victim to try to extract more damning information before admonishing (or rather, attacking) them. reply elevatedastalt 18 hours agoparentprevMany possibilities. Something seeking legal help, or an info page about domestic abuse itself, or something around financial literacy. reply rsynnott 7 hours agoparentprev> What page are people on that might lead to domestic abuse? The police, the divorce services, health services pages about contraception, abortion, sexual assault, LGBT youth services, etc etc etc. Think people who are already being abused, mostly. reply frereubu 12 hours agoparentprevThat information is out there, but people in these kinds of circumstances don't always have unrestricted internet time to research it. They might just be able to snatch a few minutes here and here and therefore not know much about how to use browsers etc. This is particularly the case for an honour-based abuse service (forced marriage, honour killings etc) that we work with for example. reply scott_w 13 hours agoparentprevIf it’s the only tab open, switching isn’t an option. Women living under the threat of violence will be very stressed, so won’t be well placed to setup their browser ahead of time. reply vehemenz 16 hours agoprev1. This kind of browsing is more likely to be done on a phone, in private. I find the scenario a bit contrived in 2024. 2. It seems a bit weird to be concerned about UI patterns if you earnestly want this component to do its job. 3. If it's that important, the Escape key event can be added after DOMContentLoaded. Warn content authors to not overuse the component, and it would be fine. You can still have the triple-Shift key event for those cases that they specifically call out. reply FridgeSeal 16 hours agoparentIts entirely plausible that someone in an abusive relationship is a number of mitigating circumstances: - they don’t have a smartphone, or it’s been taken off them - they’re forced to use a desktop because their abuser doesn’t want them to do things in private easily - plausibly mobile has something different entirely, given that this appears to be desktop focused. - They mention escape is intercepted by most browsers to stop loading, if someone is interrupted midway and panics and starts hitting escape, they could plausibly end up _stuck_ on the page they were trying to hide from their abuser. reply thecatspaw 5 hours agorootparentto fix the interrupt issue they could initially load a page with begnign information, and then load the help text afterwards reply frereubu 10 hours agoparentprev1. A large number of people who need this service are likely to be victims of various forms of coercive control. This is a decent, quick summary of what that means in practice (PDF): https://www.leeds.gov.uk/docs/One%20minute%20guides/One%20Mi... 2. I don't understand this comment. Surely this is a perfect example of when you want a component to work as well as possible, including UI research? 3. The mAjor point here is that the functionality of the escape key is ambiguous. It can do various things in various contexts, so you can't rely on people to use it for that, and visitors can't rely on it because it might just e.g. minimise a maximised window on MacOS, leaving the website on-screen. reply zerovox 17 hours agoprevI understand that they couldn't use the Escape key, and so having an alternative makes sense, but I'm not sure as a user how I would ever discover the behavior of pressing \"shift\" three times. reply jdiff 16 hours agoparentEscape might be more intuitive but it's not more discoverable. Shift is used often when inputting information, and the mentioned visual feedback give this behavior an opportunity to be discovered. Having said that, regardless of the key the guidelines on using this pattern say that you should explicitly inform the user of the feature before they first encounter it. https://design-system.service.gov.uk/patterns/exit-a-page-qu... reply changing1999 1 hour agoprevMy only criticism of this approach is that it asks highly sensitive users to learn a critical keyboard shortcut that will not work anywhere else. What will happen if users attempt to triple press \"shift\" on any other surface that doesn't support this? Because that's highly likely. Instead of introducing a new (hidden) shortcut, I would rely on clear visual cues and intuitive (meaning, already common) interactions. E.g. opening the form in a modal; clicking anywhere outside of this modal closes the modal and loads the weather page. The clickable background should be clearly identified as a special feature, e.g. tiled text \"exit page\" all over it. reply duxup 4 hours agoprevI enjoyed reading their thought process. That was a good read. But I agree the end result feels like an over thought process that comes up with something completely counter intuitive that someone would seem to need to trigger at a moments notice. To some extent this seems to be one of those \"well they did something\" solutions that for a lot of work, provides near zero value. reply hamdouni 12 hours agoprevThis makes me laugh \"As a result of advertising people being bastards, more and more of what the web platform can do is ...\" reply oneeyedpigeon 11 hours agoparentIt was refreshingly candid - then I remembered we're reading a government blog where they can say that kind of thing with impunity. reply switch007 10 hours agorootparentIt's a personal blog btw. And there is absolutely no way a UK Gov blog would call Google bastards. reply oneeyedpigeon 9 hours agorootparentI didn't read it as a dig at Google specifically, but I accept your general point is totally correct. reply hbrav 7 hours agorootparentprevThe individual has impunity of the department has impunity? Not sure if you know this, but it might be of interest: in the UK speech, within the House of Commons (maybe the Lords too? I'm unsure) is specifically protected from defamation actions. An MP could stand up and say \"Mr Smith murders kittens in his spare time\" and Mr Smith would have no ability to sue. However, this does not apply to MPs outside of parliament. reply globular-toast 10 hours agorootparentprevWhat makes you think it's a government blog? Looks like a personal blog to me. reply oneeyedpigeon 10 hours agorootparentSorry, I'm not totally sure why I made that assumption. I thought I'd spotted a '.gov' domain, but clearly it's not. I guess some of the writing also implies it (e.g. \"Last year [...], we launched the GOV.UK Design System’s Exit this Page component\") but, of course, this could just be a contractor. reply rsynnott 6 hours agorootparentI _think_ they're an employee of the gov.uk design service? reply eviks 12 hours agoprevThe explanation doesn't make sense without addressing the elephant in the room - why not teach users to use the universal \"tab close\" action via a common shortcut? That one is immediate unlike loading another page reply inejge 12 hours agoparentIf yours is the only tab, \"close tab\" will usually close the whole window, potentially leaving you with an empty desktop. Being caught staring at nothing would be suspicious in the situations where \"exit page\" is supposed to be used. The weather page is comparatively innocuous. (Until the word gets around...) reply eviks 3 hours agorootparent> staring at nothing would be suspicious in the situations where \"exit page\" is supposed to be used. This is actually not true. Having a browser opened leads to the thought of \"let's check the previous page/browser history\" easier (since the browser is right there to remind you) than a situation of \"oh, I've just logged in\" or the activity of doing anything else leads to the thought of having to check a browser reply _qua 7 hours agorootparentprevAnyone who is smart enough to use this weird triple shift key shortcut is intelligent enough to preload a different site in another tab and use the close tab shortcut. I would guess there is almost complete venn diagram overlap between people who can learn this weird shortcut and people who can deal with this threat in any other way using normal browser functions. reply eviks 11 hours agorootparentprevFirst, you wouldn't be staring at nothing, you'd be reopening the browser / opening Solitaire or something But also a better way would be to ask the user to open a second tab (or another app) so that it's not the only tab/app. Still beats remembering a unique shortcut. reply jdiff 4 hours agorootparentTime is of the essence when you're hitting an escape shortcut. That's why this component blanks the page immediately, then loads the decoy, there can be no delay even for the browser to tear down the page as it fetches the next. If you have enough time to just go and open Solitaire, you have no need for an escape button. If you are with someone who cannot know what you are doing, who has appeared suddenly, you are quickly closing what you're doing and, yes, you will be looking at a blank page without some sort of escape mechanism like this. And if it's sudden and unexpected, you might not have been anticipating needing to pop open some decoys. This seems like a complete misunderstanding of the situation. reply eviks 3 hours agorootparentIf time is of the essence, why are you wasting it requiring 3 key presses and a site load? It take longer to do that vs a single shortcut, and is more visible (pages don't load immediately) > If you have enough time to just go and open Solitaire, you have no need for an escape button. You don't have enough time to complete that, you do that not to appear just staring at a blank screen. Activity of opening Solitaire is enough in itself. > who cannot know what you are doing, which is easier achieved when the browser is closed vs. when a browser is opened, since in the latter case it's easier to think about checking \"previous\" browser history > This seems like a complete misunderstanding of the situation. Indeed, so much so that this overengineered-but-underthought solution has none of the supposed benefits under the conditions people come up with to defend it reply jdiff 2 hours agorootparentAt this point I have to assume that this is willful. You are continuing to ignore things that have been addressed by both myself in my last comment and the article. I invite you to read the article more deeply and look into the actual research backing these UI patterns if you are genuinely struggling to understand. reply frereubu 11 hours agoparentprevMany people in abusive situations have very limited opportunity to use computers, and may well not have time to learn about things like \"tab close\" actions. This doesn't stop people who do know about those shortcuts from using them. reply eviks 10 hours agorootparentSo how do you imagine they'll learn about Shift-Shift-Shift??? reply JacketPotato 7 hours agorootparenthttps://design-system.service.gov.uk/patterns/exit-a-page-qu... Government sites that use this component will include a page that explains this feature and how to use it, they have considered this. This is generally used on flows/pages, where the site is walking you through a proccess or a guide. reply frereubu 10 hours agorootparentprevI query that in another comment: https://news.ycombinator.com/item?id=41796257 reply airpoint 13 hours agoprev> BBC Weather’s homepage is a content-rich page. Users have a reason to be looking at it and to be looking for an extended period of time. Most of that rich content is obstructed by them bloody cookie warnings, on first visit. That’s not a very convincing simulation of “I’ve been looking at this page for the last 5 mins!” reply colanderman 13 hours agoparentI often leave cookie popovers unclicked. Sometimes they take an annoying amount of work to decline cookies, and they can be used to cover video ads anyway. reply cwillu 13 hours agoparentprevHmm, I don't get a cookie banner on my browser, even in an incognito window with uBlock turned off. reply oneeyedpigeon 10 hours agorootparentI get one here in the UK, in incognito. It's actually one of the nicest cookie banners you'll ever see—just 75px tall at the top of the page, and it doesn't float so it disappears when you scroll. I recommend at least trying to see it, to appreciate its superiority over all the other cookie banners. reply robertlagrant 9 hours agorootparentI'd rather they just didn't track me. reply JacketPotato 7 hours agorootparentThey're also used for stuff like storing which locations you search for, a pretty important feature. They probably also use them for analytics though. reply razakel 7 hours agorootparentprevThey don't if you're in the UK. reply robertlagrant 6 hours agorootparentI'm in the UK and I get the analytics cookies notice. reply shultays 9 hours agoprevIn virtually all browsers, pressing Escape while a webpage is loading stops the loading process. Whoa, never knew about this or noticed it reply jonathanstrange 9 hours agoparentIt's how people read the New York times. reply YoumuChan 15 hours agoprevShift key is widely used in Eastern Asian input methods to switch between English and Asian scripts. Pressing Shift while holding Alt is the way to cycle through different input methods on windows systems. Using shift key is a decent idea for Latin script users, but is terrible for Asian script users. reply robin_reala 13 hours agoparentThat’s a less likely setup for a GOV.UK user though. reply YoumuChan 3 hours agorootparentYet the gov.uk website about domestic abuse has a Chinese version (among other languages which I imagine also requires different setups): https://www.gov.uk/guidance/domestic-abuse-how-to-get-help.z... I don't think gov.uk would admit that they want to exclude those users. reply nottorp 11 hours agoparentprevWhich Latin script? :) Everyone on the nearby continent has some accented characters and possibly both English and their national keyboard installed. Incidentally, this is a major complaint with smartphone OS designers that only speak English and don't realize there are places where people mix languages daily. That predictive spell checker should be configurable to accept more than one language at a time... reply tuetuopay 3 hours agorootparentAnd there's no need to be to speak some \"obscure\" language (from the point of view of the US-centric designers) to hit this issue. iOS got better at mixed french / english, but it still cannot prevent itself from correcting \"the\" (the english the) to \"thé\" (french for tea). Oh well. reply trollbridge 13 hours agoprevA while ago we did a site for a nonprofit focused on domestic violence. We preloaded Kohl’s (a department store sort of retailer in America) and fiddled with the safety exit button to make sure Kohl’s came up really quickly. If we would have worked on the site longer, I would have a done a rotation of a couple of different stereotypical shopping websites. (Kohl’s was picked by the organisations’s executive director who, unfortunately, had plenty of first hand experience with domestic violence.) reply neilv 18 hours agoprevRelated: https://en.wikipedia.org/wiki/Boss_key reply thih9 3 hours agoprevWhile a weather page sounds good, perhaps something that loads fast would be also a good pick? Then again, the html code shows the button itself as an anchor tag, so it seems easy to customize the target url. reply matteason 2 hours agoparentThe component has some JavaScript which blanks out the page immediately after the button being pressed, so if it takes a while for the browser to load the weather page it shouldn't matter as much https://design-system.service.gov.uk/components/exit-this-pa... reply kayson 17 hours agoprevHow are people expected to know about the Shift key functionality? reply kelnos 16 hours agoparentYeah, it seems a little obscure. Here's a test page with the functionality: https://design-system.service.gov.uk/components/exit-this-pa... One cool thing is when you first hit the shift key once, the \"Exit this page\" button expands vertically, and shows three small circles, one now filled in. So it makes it obvious that hitting the shift key did something related to that button. So if you hit the shift key for any other reason, you'll see something happen. But still, I agree it seems a little hard to discover. reply petepete 9 hours agorootparentThe guidance does cover this in some detail and suggests using an interruption page that explains the behaviour before the risky journey starts. https://design-system.service.gov.uk/patterns/exit-a-page-qu... reply Izkata 15 hours agorootparentprevOut of curiosity I edited the page to put a textarea on it, so I could see what happens when you're typing a sentence and happen to use Shift 3 times: It breaks the button. If the cursor is in the textarea, tapping Shift without any other keys will add 1 circle, but if that wasn't the 3rd one, any additional Shift will remove all the circles and they don't come back. You have to click outside the textarea and hit Shift 4 times to trigger it (the first one doesn't register any circles). It seems like they tried to prevent accidental triggers (if you have 1 or 2 circles and hit anything except Shift they all disappear, and if you hold Shift while hitting another key you don't get any in the first place), but got something slightly wrong. reply kypro 17 hours agoparentprevThat's what I wondered. Presumably services implementing it will add info about using the button before starting the journey, but I'm surprised there's no design system guidance about this. Without that information the button is far less useful. reply YPPH 14 hours agoprevShift is not ideal either. On Microsoft Windows, pressed thrice in quick succession will prompt to activate sticky keys, and divert focus from the web browser. reply Aaron2222 12 hours agoparentIt's five times, not three. reply ascorbic 10 hours agoparentprevThe post covers that reply jstummbillig 10 hours agoprevThis is perfect. Whenever the idea pops up that design/code/system is done because of AI I am mostly confused. Everything is so bad and requires so much though to even get to \"decent\"! Our current standards are so low, because we can not afford higher standards — but when paying attention to the world, anywhere, it does not even take effort to find an instance of a (systemic) design problem that could be fixed. Granted, reconfiguring our system to pay for that is an outstanding issue, but I don't think that's because it requires much fantasy to find things that could be done and that would be appreciated by us and the people around us. reply amiantos 1 hour agoprevIt's fun to read so many people who can't see past their own nose, who declare the scenario contrived and the solution over-engineered, despite having no frame of reference for the need of this button and thus having no ability to properly dogfood the feature, speaking so confidently from their ignorance. Great HN thread. reply mooktakim 5 hours agoprevHow would anyone know that you can use the shift key? Closing the tab/page is just more natural as its something you do all the time. reply jdiff 4 hours agoparentIt's advised in the implementation documentation to add a page explaining it. Shift is also used naturally when inputting information, with the visual feedback inside the button giving an opportunity for discoverability. reply DrBazza 7 hours agoprevIdeally Jira would have something similar so that when you create a new issue and accidentally click somewhere or press escape, it doesn't delete the ticket you've just spent 5 minutes creating. reply RockRobotRock 16 hours agoprevThis is a great idea! How come when I google \"gov uk domestic violence\" none of the govt pages have this button on them? reply andrei-akopian 16 hours agoparentMy first search result was thehotline.org, and it does have a button that redirects to google.com. (But that's a US site) > You can quickly leave this website by clicking the “X” in the top right or by pressing the Escape key twice. And it does have some kind of Escape key functionality. The gov.uk page has some listed hotlines by nation (https://www.gov.uk/guidance/domestic-abuse-how-to-get-help#g...), but none of them are actually using that exact red button: - https://www.nationaldahelpline.org.uk/ uses green bookmark in bottom right and redirects to google.co.uk - https://dsahelpline.org/ has a green area at the bottom right reply tanbog45 9 hours agoprevI make sites for non-profits regularly and have been asked to add exit/escape buttons a few times. There more time Ive spent thinking about the problem and researching solutions the more I think they are a bad idea. 1. Lots - if not most - traffic is from mobile these days. Most people already know the fastest way to exit a page on mobile - the home button/action. Adding anything else is just adding confusion. 2. Unless you are going to great lengths - ie pre loading a page and maybe dropping parts of the dom and dealing with evidence in the history - are you actually doing anything much to help the user exit your site? How motivated/skilled a person are you defending against? 3. If your exit button is just a glorified link or redirect what is the point? It will still be in the history and if they have slow internet they could end up just staring at your site while the redirect loads. 4. For some organisations having such buttons is more about \"showing\" they have it than how useful it actually is to the user. 5. I have tried to push for a page/link to basic internet safety information. Educating visitors would be much better than trying to engineer their personal security day. 6. I've struggled to find good academic/research work on such features. Seems like it would be a good area for a UX researcher but I've not found much actual work. reply int_19h 1 hour agoparentPressing the home button on mobile in this scenario leaves the app open in the background with the page still opened. Worse yet, both Android and iOS show thumbnails of apps in the switcher, and it's an MRU so the last used app will be the first one you see if you bring up the switcher. And bringing up the app switcher is very likely to be the first action the attacker would do to see what the victim was doing just now. reply hengistbury 8 hours agoparentprevI see these points as reasons why it might not be a good idea, but they don't explain why it is a bad idea. Other methods for leaving the site still work. Even if the button isn't the best way to leave the site, if it helps in more cases than it hurts then it's a net benefit. These buttons are essentially panic buttons, and when a person is panicking the big red exit button might end up being the only exit they can find. reply mcculley 3 hours agoprevBeing unable to use the escape key is another reason why web apps will never be as consistent as desktop apps. reply kqr 3 hours agoprevHypothetically, wouldn't an abuser start to find it suspicious when a blank page loads BBC Weather as they enter the room? reply mooktakim 4 hours agoprevIt reminds me of the old lastminute.com (I think) button that would turn the whole front page into an Excel spreadsheet so when the manager walks by, they only see spreadsheets on your screen lol reply globalise83 4 hours agoparentNow that is a real use case! reply Daz1 11 hours agoprev\"Content warning: This blog post references domestic abuse and violence but doesn’t go into specific detail. I’m not an expert in that topic at all, so I may not use the preferred terminology in all instances. Sorry.\" What the hell is this? reply frereubu 10 hours agoparentI guess this is probably a rhetorical question. But if you've been a victim of domestic abuse you may not want to read about it when you think you're just reading about a gov.uk web component, particularly if the abuse was recent and you're still traumatised by it. The author is just trying to be sensitive to that. The language apology-in-advance does feel a bit like overkill though. I'd suggest a generous interpretation is that, given how things often work these days, they don't want people to get caught up in discussions about terminology and just want to focus on the tech. reply rsynnott 6 hours agoparentprevEver watched TV? \"The following programme contains depictions of [whatever]\" Some people, particularly people who've suffered domestic abuse, may not wish to be blindsided by a discussion of it when they think they're reading a technical blog. reply BostonFern 11 hours agoparentprevRead the blog's \"about\" page. reply thaumasiotes 10 hours agorootparentFor reference: > I'm an agender (I use it/its pronouns), asexual, alterhuman robot. I'm also a shapeshifting critter on the internet. This person has absorbed the idea that it's a sin to use natural language to talk about normal phenomena, and the idea that it isn't possible to know what kind of language wouldn't be sinful, but not the idea that maybe that isn't a desirable state of affairs. reply Symbiote 6 hours agoprevI found a live example here, on the page where you can check if you are eligible for legal aid: https://www.gov.uk/check-legal-aid then click \"Start now\" then \"Domestic abuse\" reply imdsm 7 hours agoprevI had a situation where I needed to speak to Police some time ago. An sms about the weather was sent, which allowed me to speak to someone, and then after the call, it took me to bbc weather. It was brilliant and I really commend it. reply joelanman 11 hours agoprevMore info on our pattern here: https://design-system.service.gov.uk/patterns/exit-a-page-qu... reply frereubu 11 hours agoparentThanks. Interesting to note the \"interruption page\" and \"safety content page\" parts, which I think deals with quite a few queries in the comments about how people will know what to do. Also just a note that the first two GOV.UK links under \"Research on this pattern\" don't include live examples any more. reply ykonstant 4 hours agoprevYou can exit the UK Government, but you can never escape. reply tetris11 18 hours agoprevThey're pretty forthcoming for what I assume to be an government agency. I wonder why the gov.uk team are getting so much publicity(?) In the last few years. As much as I love the aesthetic, I'm developing a fear that they'll soon spin off into a startup with some kind of paid model, and that government websites will regress. Irrational fear, I know, but I cant shake off the startup-vibes I'm getting when I read such posts about what is essentially a public service. reply ascorbic 10 hours agoparentThis is all thanks to the GDS, which was formed in 2011 specifically to bring that kind of startup vibe to government. It's even based in Shoreditch, with the startups. A lot of alumni from GDS have gone on to consult with other governments, many of which have launched similar departments. The US equivalent is 18F, which involved collaboration with GDS. https://www.gov.uk/government/organisations/government-digit... https://18f.gsa.gov/ https://gds.blog.gov.uk/2015/01/20/gds-usds/ reply scott_w 13 hours agoparentprevThey’ve done this since the head of the Cabinet Office around 2010 set up a team to improve digital government services. There’s a lot of information published as to their methodologies and their teams present technical topics at conferences. It’s likely part of their efforts to be more transparent, work with other governments and better support departments without having to be in 50 places at once. It’ll also help with recruitment. reply adw 18 hours agoparentprev> As much as I love the aesthetic, I'm developing a fear that they'll soon spin off into a startup with some kind of paid model, and that government websites will regress. gov.uk got started, in part, because the 2009 financial meltdown left a lot of good startup designers and engineers with not enough to do (and made civil service jobs more attractive for a bit!) reply fallingsquirrel 18 hours agoparentprevfwiw this isn't an official gov.uk blog post. I mistook it for one at first too... I only double checked once I stumbled over the \"advertising people being bastards\" line. reply nyanpasu64 15 hours agoparentprevI got to the furry art at the bottom of the page before realizing this was a frontend developer's blog and not the government agency itself. reply hollerith 12 hours agorootparentAh, so you detected differences from the official British governmental furry art. Smart reply tsimionescu 13 hours agoparentprev> when I read such posts about what is essentially a public service. Doesn't it make a lot of sense to be open about how a public service is built and delivered, maybe much more so than any for-pay service in fact? reply caseyy 18 hours agoparentprevCompared to many other countries, UK has a computer science culture that's very open about how technology is used in every day lives, and it invites public participation in new tech. This shows a lot in the government as well as its services like BBC and NHS, and the academia. It's a very broad topic to cover so I'll be terse with evidence/examples only. UK government provides a lot of open data and APIs for the country [0], [1]. They are free and pretty much not throttled. They have a license [2] for a lot of this data which is formal but nearly as free as John Carmack's legendary hacker-friendly \"have fun\" license [3]. There is also a lot of historical Ordnance Survey data and historical legislation data from the National Archives. And of course, you can see the openness in how they have built gov.uk, as blog articles appear on HN about it quite often. There is also a lot of government infrastructure provided to local governments, such as gov.uk Notify [4] or a freely available NHS website CMS (which is why many NHS websites work the same). There is a guide [5] mostly intended for government services but free for others to use on building accessible, secure and quite good-looking websites. Most other governments I lived under are either technically behind UK or they have very advanced tech capabilities in certain branches of the government only (such as the armed forces) but keep it out of the public eye. Ultimately, I think it is the culture of welcoming everyone's participation in technology that makes UK gov so forthcoming and open with their tech and data. Doing this is seen as kind and civilised, which is how governments want to be seen. Of course, there are still areas of improvement in how UK gov provides data, as there always are in everything. Finally, I should mention you can find many BBC technology outreach programmes from the early days of home computing. They are all over YouTube if you search for \"BCC home computing\". There was and continues to be a lot of techno-optimism in the country. It is one of the admittedly not many things that persist from the pre-austerity times. [0] https://www.data.gov.uk [1] https://www.api.gov.uk/index/#index [2] https://www.nationalarchives.gov.uk/doc/open-government-lice... [3] https://github.com/id-Software/DOOM/blob/master/README.TXT (before GPL became popular, id software code was distributed with this readme that said \"Have fun\") [4] https://www.notifications.service.gov.uk [5] https://frontend.design-system.service.gov.uk reply ascorbic 10 hours agorootparentThis isn't really an example of UK culture. 15 years ago, UK gov sites were as bad as everywhere else. Some of the small number of good things that I can credit the Cameron government were a few of these changes, including the establishment of the Government Digital Service and changing \"IT\" education from learning how to use Word, to actually teaching all kids coding, starting in primary school. reply caseyy 10 hours agorootparentIt is culture. The government doesn’t just provide these APIs, people use them. End even if you compare Harvard’s CS50 vs CS courses in the UK, you will see that it’s a lot more oriented around computing in every day life. The BBC home computing shows and their success itself is a bit of a unique phenomenon in the UK. Many other countries had these shows but they never went mainstream, most only attracted viewership of enthusiasts. There is a strong cultural element. reply alephnerd 17 hours agorootparentprev> This shows a lot in the government as well as its services like BBC and NHS, and the academia Salaries play a significant role. Unlike a lot of other countries, private sector salaries for SWEs suck in much of the UK, and gov.uk (in reality part of the Civil Service), GCHQ+MoD, and BBC can pay fairly competitively and give a fairly decent pension compared to private sector gigs. That said, I'd disagree with NHS IT - it's almost entirely outsourced to regional MSPs who suck (and I say this as a former vendor who's helped sell products those guys use in NHS environments) reply rsynnott 6 hours agoparentprev> As much as I love the aesthetic, I'm developing a fear that they'll soon spin off into a startup with some kind of paid model I mean, unless the next PM is Zombie Thatcher, this seems like an excessive level of privatisation. reply ata_aman 16 hours agoprevWould be pretty cool if it also changed the page navigation history to obscure where the user was before visiting bbc weather. If users taking the triple click action are presumed to be in distress, you'd want to remove the ability of the other party to simply click \"back\" and see where they were. reply oneeyedpigeon 10 hours agoparentThat's exactly what it does do — check out [this demo](https://design-system.service.gov.uk/components/exit-this-pa...). Presumably, that requires the [JavaScript History API](https://developer.mozilla.org/en-US/docs/Web/API/History_API), but the whole thing requires JS anyway, so that's no more of a problem. reply VoidWhisperer 16 hours agoprevThis site is flagged by malwarebytes as being compromised for some reason - I'm assuming this is a false positive given that no one else has been having issues reply mrinterweb 13 hours agoprevOn a UK keyboard, you use the Brexit key reply adamrezich 3 hours agoprev> In virtually all browsers, pressing Escape while a webpage is loading stops the loading process. Wow, you learn something new every day! Kinda weird that we got “Backspace to go back” out of web browsers some time ago yet this still exists, though. reply int_19h 1 hour agoparentBackspace was overloaded whenever text input fields on the page were involved, so accidentally pressing that to mean something else entirely and losing data as the result was too common to not address. Escape OTOH was never used for anything else in browsers, as far as I remember. reply ReverseCold 16 hours agoprevWait why not have both esc x3 and shift x3 work? Any of these are \"weird\" keypresses right? reply Moogs 14 hours agoparentThe concern with Esc is that if you hit more than 3 times the user will be stuck on the page. The first 3 presses would trigger the redirect, the 4th press would be intercepted by the browser and stop the page load. reply djtango 14 hours agoprevShould have thought like a vimmer and used caps lock instead reply petesergeant 17 hours agoprevHere it is in action: https://design-system.service.gov.uk/components/exit-this-pa... reply gjsman-1000 15 hours agoprev [–] I don’t understand why this is an either/or. It could be Shift x3 or Esc x1. Tell the user to Shift x3 times, but if they forget or use habit, Esc will still be an option. reply TonyTrapp 10 hours agoparentSomeone might be panicking and press ESC twice \"just to be sure\". Your average user won't know that the second press will cancel the redirection process, inducing further stress and potentially completely closing the opportunity to move away from the page before the abuser sees it. reply tgv 10 hours agoparentprev [–] If you tell people they can use escape, they might press it too soon or repeatedly, preventing the very action they require. Nobody intuitively uses Esc to go to another page, so it's something you really need to be instructed to do. It makes sense to me. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GOV.UK's \"Exit this Page\" component avoids using the Escape key due to its potential to stop page loading and its use in other operating system functions.- The Shift key is used instead for JavaScript redirection, as it provides more consistent functionality despite its limitations.- The component redirects users to BBC Weather to ensure a neutral, content-rich page, aiding users in quickly leaving a page in potentially dangerous situations, such as domestic abuse."
    ],
    "commentSummary": [
      "Gov.uk's \"Exit this Page\" feature avoids using the Escape key to prevent stopping page loading, opting for pressing Shift three times instead.",
      "Critics argue the feature is over-engineered and not user-friendly, suggesting simpler alternatives like teaching users to close tabs with keyboard shortcuts.",
      "The feature is intended for high-stress situations, such as domestic abuse, to quickly hide browsing activity, but its effectiveness and discoverability are under scrutiny."
    ],
    "points": 356,
    "commentCount": 215,
    "retryCount": 0,
    "time": 1728513926
  },
  {
    "id": 41795218,
    "title": "Indian entrepreneur, industrialist, and philanthropist, Ratan Tata, dead at 86",
    "originLink": "https://en.wikipedia.org/wiki/Ratan_Tata",
    "originBody": "Toggle the table of contents Ratan Tata 44 languages العربية অসমীয়া বাংলা Български Deutsch डोटेली Español فارسی Fiji Hindi Français ગુજરાતી 한국어 Hausa हिन्दी Bahasa Indonesia Italiano ಕನ್ನಡ कॉशुर / کٲشُر Kiswahili मैथिली മലയാളം मराठी مصرى Bahasa Melayu Монгол Nederlands 日本語 ଓଡ଼ିଆ ਪੰਜਾਬੀ پنجابی Polski Português Русский संस्कृतम् ᱥᱟᱱᱛᱟᱲᱤ Simple English Svenska தமிழ் తెలుగు ไทย Türkçe Українська اردو 中文 Edit links Article Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version In other projects Wikimedia Commons Appearance move to sidebar hide From Wikipedia, the free encyclopedia This article is about the former chairman of Tata Sons. For the philanthropist and grandfather of Tata, see Ratanji Tata. For the father of J. R. D. Tata, see Ratanji Dadabhoy Tata. This article is currently being heavily edited because its subject has recently died. Information about their death and related events may change significantly and initial news reports may be unreliable. The most recent updates to this article may not reflect the most current information. Please feel free to improve this article (but edits without reliable references may be removed) or discuss changes on the talk page. (Learn how and when to remove this message) Ratan Tata Tata in 2005 Born Ratan Naval Tata 28 December 1937 Bombay, Bombay Province, British India Died 9 October 2024 (aged 86) Mumbai, Maharashtra, India Alma mater Cornell University (BArch) Occupation Industrialist Title Chairman Emeritus, Tata Sons and Tata Group[1] Term1991–2012 2016–2017 Predecessor J. R. D. Tata SuccessorCyrus Mistry (2012–2016) Natarajan Chandrasekaran (2017–present) ParentsNaval Tata (father) Sooni Commissariat (mother) Relatives Tata family AwardsOrder of Australia (2023) Assam Baibhav (2021) Honorary Knight Grand Cross of the Order of the British Empire (2014) Padma Vibhushan (2008) Maharashtra Bhushan (2006) Padma Bhushan (2000) Ratan Naval Tata[a] (28 December 1937 – 9 October 2024) was an Indian industrialist and philanthropist. He served as the chairman of Tata Group and Tata Sons from 1991 to 2012, and he held the position of interim chairman from October 2016 to February 2017.[2][3] In 2008, he received the Padma Vibhushan, the second highest civilian honour in India. Ratan had previously received the Padma Bhushan, the third highest civilian honour, in 2000.[4] Ratan Tata was the son of Naval Tata, who was adopted by Ratanji Tata, son of Jamshedji Tata, the founder of the Tata Group. He graduated from Cornell University College of Architecture with a bachelor's degree in architecture.[5] He joined the Tata Group in 1961 starting on the shop floor of Tata Steel. He later succeeded J. R. D. Tata as chairman of Tata Sons upon the latter's retirement in 1991. During his tenure, the Tata Group acquired Tetley, Jaguar Land Rover, and Corus, in an attempt to turn Tata from a largely India-centric group into a global business. Tata was actually a philanthropist. Tata has invested in over 30 start-ups, primarily in a personal capacity, with some through his investment company.[6][7] Early life and education Main article: Tata family Ratan Tata was born in Bombay, now Mumbai, during the British Raj, into a Parsi Zoroastrian family, on 28 December 1937.[8] He was the son of Naval Tata (who was born in Surat and later adopted into the Tata family), and Sooni Tata (the niece of Tata group founder Jamsetji Tata). Tata's biological grandfather, Hormusji Tata was a member of the Tata family by blood. In 1948, when Tata was 10, his parents separated, and he was subsequently raised and adopted by Navajbai Tata, his grandmother and widow of Ratanji Tata[9]. He had a younger brother Jimmy Tata[10] and a half-brother, Noel Tata, from Naval Tata's second marriage to his stepmother Simone Tata. Tata studied at the Campion School, Mumbai until 8th grade. He then continued his studies at the Cathedral and John Connon School in Mumbai, the Bishop Cotton School in Shimla, and the Riverdale Country School in New York City, from which he graduated in 1955.[11][12][13] After high school, Tata enrolled in Cornell University, from which he graduated with a bachelor's degree in architecture in 1959.[14] While at Cornell, Tata became a member of the Alpha Sigma Phi Fraternity. In 2008, Tata gifted Cornell $50 million, becoming the largest international donor in the university's history.[15][better source needed] Career In the 1970s, Ratan Tata was given a managerial position in the Tata group. He achieved initial success by turning the subsidiary National Radio and Electronics (NELCO) around, only to see it collapse during an economic slowdown.[16][17] In 1991, J. R. D. Tata stepped down as chairman of Tata Sons, naming him his successor. Initially, Tata faced stiff resistance from the heads of various subsidiaries, who had a large amount of operational freedom under the senior Tata's tenure. In response, Tata implemented a number of policies designed to consolidate power, including the implementation of a retirement age, having subsidiaries report directly to the group office, and requiring subsidiaries to contribute their profit to building the Tata group brand. Tata prioritised innovation and delegated many responsibilities to younger talent.[18] Under his leadership, overlapping operations between subsidiaries were streamlined into company-wide operations, with the group exiting unrelated businesses to take on globalisation.[19] Ratan Tata (right) in Bangladesh, 2005 During the 21 years Tata led the Tata Group, revenue grew over 40 times, and profit over 50 times.[16] When he took over the company, sales overwhelmingly comprised commodity sales, but at the end of his tenure, the majority of sales came from brands.[20][21] He had Tata Tea acquire Tetley, Tata Motors acquire Jaguar Land Rover, and Tata Steel acquire Corus. These acquisitions repositioned Tata from a largely India-centric group into a global business, with over 65% of revenues coming from operations and sales internationally.[16] He also conceptualised and spearheaded the development of the Tata Nano car after the grand success of Diesel Tata Indica, which helped put cars at a price-point within reach of the average Indian consumer.[22][18] Tata Motors has since rolled out the first batch of Tigor Electric Vehicles from its Sanand Plant in Gujarat, which Tata has described as to \"fast-forward India's electric dream.\"[23] Upon turning 75, Ratan Tata resigned his executive powers in the Tata group on 28 December 2012. An ensuing leadership crisis over his succession drew intense media scrutiny.[24] The board of directors and legal division of the company refused to appoint his successor, Cyrus Mistry, a relative of Tata and the son of Pallonji Mistry of the Shapoorji Pallonji Group, which was the largest individual shareholder of the Tata group.[25][26] On 24 October 2016, Cyrus Mistry was removed as chairman of Tata Sons, and Ratan Tata was made interim chairman. A selection committee, which included Tata as a member, was formed to find a successor.[27] On 12 January 2017, Natarajan Chandrasekaran was named as the chairman of Tata Sons, a role he assumed in February 2017. In February 2017, Mistry was removed as a director for Tata Sons.[28] The National Company Law Appellate Tribunal later found in December 2019 that the removal of Cyrus Mistry as the chairman of Tata Sons was illegal, and ordered that he be reinstated.[28] On appeal, India's Supreme Court upheld the dismissal of Cyrus Mistry.[29] Tata had also invested in multiple companies with his own wealth. He had invested in Snapdeal – one of India's leading e-commerce websites. In January 2016, he invested in Teabox, an online premium Indian Tea seller,[30] and CashKaro.com, a discount coupons and cash-back website.[31] He had made small investments in both early and late stage companies in India, such as INR 0.95 Cr in Ola Cabs.[32] In April 2015, it was reported that Tata had acquired a stake in Chinese smartphone startup Xiaomi.[33] In 2016, he invested in Nestaway an online real-estate portal that later acquired Zenify to start the online real-estate and pet-care portal, Dogspot.[34][35][36][37] Tata also launched India's companionship startup for senior citizens, Goodfellows, in a bid to encourage intergenerational friendships.\"[38] Philanthropy Tata was a supporter of education, medicine and rural development, and considered a leading philanthropist in India.[39][40][41] Tata supported University of New South Wales Faculty of Engineering to develop capacitive deionisation to provide improved water for challenged areas.[42][43] Tata Hall at the University of California, San Diego (UC San Diego) is a state-of-the-art research facility that was opened in November 2018. The building is named after the Tata Trusts, who donated $70 million to UC San Diego in 2016 to establish the Tata Institute for Genetics and Society (TIGS), which is housed within the building. The Tata Institute for Genetics and Society at UC San Diego is a joint initiative between the Tata Trusts and UC San Diego. It aims to address some of the world's most pressing problems, such as the spread of infectious diseases and the need for sustainable food sources. The research conducted at the institute focuses on a range of topics, including gene editing, stem cell therapy, and disease control.[44] Tata Hall at the UC San Diego is a 4-storey building that is spread over 128,000 square feet and houses research facilities for the biological and physical sciences. The building has laboratories, offices, and meeting spaces that are designed to foster collaboration and innovation among researchers. It is a LEED-certified building; designed to be environmentally sustainable and energy-efficient.[44] Tata Education and Development Trust, a philanthropic affiliate of Tata Group, endowed a $28 million Tata Scholarship Fund that will allow Cornell University to provide financial aid to undergraduate students from India. The scholarship fund will support approximately 20 scholars at any given time and will ensure that the very best Indian students have access to Cornell, regardless of their financial circumstances. The scholarship will be awarded annually; recipients will receive it for the duration of their undergraduate study at Cornell.[45] In 2010 Tata Group companies and Tata charities donated $50 million for the construction of an executive center at Harvard Business School (HBS).[46] The executive center has been named Tata Hall, after Ratan Tata.[47] The total construction costs have been estimated at $100 million.[48] Tata Hall is located in the northeast corner of the HBS campus, and is devoted to the Harvard Business School's mid-career executive education program. It is seven stories tall, and about 155,000 gross square feet. It houses approximately 180 bedrooms, in addition to academic and multi-purpose spaces.[49] The Tata Innovation Center at Cornell Tech is named after Ratan Tata, and mixes academics and industry in a building on the Roosevelt Island campus. The seven-floor structure is meant primarily as a business incubator for students, faculty, and staff, with 70% of the building being commercially leased and 30% devoted to academic space. Tata Consultancy Services (TCS) is a tenant in the space.[50] In 2014, Tata Group endowed the Indian Institute of Technology, Bombay with ₹950 million and formed the Tata Centre for Technology and Design (TCTD) to develop design and engineering principles suited to the needs of people and communities with limited resources.[51][52] Tata Trusts under the chairmanship of Ratan Tata provided a grant of ₹750 million to the Centre for Neuroscience, Indian Institute of Science to study mechanisms underlying the cause of Alzheimer's disease and to evolve methods for its early diagnosis and treatment. This grant was to be spread over 5 years starting in 2014.[53][54] Tata Group, under the leadership of Ratan Tata formed the MIT Tata Center of Technology and Design at Massachusetts Institute of Technology (MIT) with a mission to address the challenges of resource-constrained communities, with an initial focus on India.[55] Board memberships and affiliations Ratan Tata was the interim chairman of Tata Sons. He headed the main two Tata trusts Sir Dorabji Tata and Allied Trusts and Sir Ratan Tata Trust and their allied trusts, with a combined stake of 66% in Tata Sons, Tata group's holding company.[56] He served in various capacities in organisations in India and abroad. He was a member of Prime Minister's 'Council on Trade and Industry' and the 'National Manufacturing Competitiveness Council'. He was on the jury panel of Pritzker Architecture Prize[57] – considered to be one of the world's premier architecture prizes. Over the years, Tata had served on the Cornell University Board of Trustees, personally advising the school's administration in matters of international involvement, particularly regarding projects connected to India. More broadly, Tata had served on the board's Academic Affairs, Student Life, and Development Committees. In 2013 he was named Cornell Entrepreneur of the Year.[58] He was a director on the boards of Alcoa Inc., Mondelez International[59] and Board of Governors of the East–West Center. He was also a member of the board of trustees of University of Southern California, Harvard Business School Board of Dean's Advisors, X Prize[60] and Cornell University. He was a member on the board of International Advisory Council at Bocconi University.[61] He was on the advisory board of Hakluyt & Co, an international consultancy company.[62] In 2013, he was appointed to the board of trustees of the Carnegie Endowment for International Peace.[63] In February 2015, Ratan took an advisory role at Kalari Capital, a venture capital firm founded by Vani Kola.[64] In October 2016, Tata Sons removed Cyrus Mistry as its chairman, nearly 4 years after he took over the reins of the over $100 billion conglomerate, Ratan Tata made a comeback, taking over the company's interim boss for 4 months.[65] On 12 January 2017, Natarajan Chandrasekaran was named as the chairman of Tata Sons, a role he assumed in February 2017.[66] Personal life and death Tata never married and had no children. In 2011, he stated, \"I came close to getting married four times and each time I backed off in fear or for one reason or another.\"[67] However, he was very close to his personal assistant Shantanu Naidu during the later part of his life.[68][69] Tata was admitted to Breach Candy Hospital in critical condition due to dyspnea and was under intensive care from 7 October 2024.[70] He died there at 11:30 p.m. IST on 9 October 2024, at the age of 86 due to age-related issues.[71][72] Following his death, the Government of Maharashtra and the Government of Jharkhand announced a day of mourning. On 10 October, Tata was given a state funeral.[73] His last rites were conducted at the Worli crematorium. He was accorded with military and 21-gun salute during his final rites.[73][74] Mumbai Police delivered a ceremonial guard of honour and his body was wrapped in the Indian flag.[75] Honours and awards President Pratibha Patil presenting the Padma Vibhushan to Ratan Tata, at the Rashtrapati Bhavan, in 2008 Ratan Tata received the Padma Bhushan in 2000 and Padma Vibhushan in 2008, the third and second highest civilian honours awarded by the Government of India.[76] Tata also received various state civilian honours such as 'Maharashtra Bhushan' in 2006 for his work in the public administration in Maharashtra and 'Assam Baibhav' in 2021 for his contribution towards furthering cancer care in Assam.[77] Other awards include: Year Name Awarding organisation Ref. 2001 Honorary Doctor of Business Administration Ohio State University [78] 2004 Medal of the Oriental Republic of Uruguay Government of Uruguay [79] Honorary Doctor of Technology Asian Institute of Technology. [80] 2005 International Distinguished Achievement Award B'nai B'rith International [81] Honorary Doctor of Science University of Warwick. [82] 2006 Honorary Doctor of Science Indian Institute of Technology Madras [83] Responsible Capitalism Award For Inspiration and Recognition of Science and Technology (FIRST) [84] 2007 Honorary Fellowship The London School of Economics and Political Science [85] Carnegie Medal of Philanthropy Carnegie Endowment for International Peace [86] 2008 Honorary Doctor of Law University of Cambridge [87] Honorary Doctor of Science Indian Institute of Technology Bombay [88] Honorary Doctor of Science Indian Institute of Technology Kharagpur [89] Honorary Citizen Award Government of Singapore [90][91] Honorary Fellowship The Institution of Engineering and Technology [92] Inspired Leadership Award The Performance Theatre [93] 2009 Honorary Knight Commander of the Order of the British Empire (KBE) Queen Elizabeth II [94] Life Time Contribution Award in Engineering for 2008 Indian National Academy of Engineering [95] Grand Officer of the Order of Merit of the Italian Republic Government of Italy [96] 2010 Honorary Doctor of Law University of Cambridge [97] Hadrian Award World Monuments Fund [98] Oslo Business for Peace award Business for Peace Foundation [99] Legend in Leadership Award Yale University [100] Honorary Doctor of Laws Pepperdine University [101] Business for Peace Award Business for Peace Foundation [102] Business Leader of the Year The Asian Awards. [103] 2012 Honorary Fellow[4] The Royal Academy of Engineering[4] [104] Doctor of Business honoris causa University of New South Wales [105] Grand Cordon of the Order of the Rising Sun Government of Japan [106] Lifetime Achievement Award Rockefeller Foundation [107] 2013 Foreign Associate National Academy of Engineering [108] Transformational Leader of the Decade Indian Affairs India Leadership Conclave 2013 [109] Ernst and Young Entrepreneur of the Year – Lifetime Achievement Ernst & Young [110] Honorary Doctor of Business Practice Carnegie Mellon University [111] 2014 Honorary Doctor of Business Singapore Management University [112] Sayaji Ratna Award Baroda Management Association [113] Honorary Knight Grand Cross of the Order of the British Empire (GBE) Queen Elizabeth II [114][115] Honorary Doctor of Laws York University, Canada [116] 2015 Honorary Doctor of Automotive Engineering Clemson University [117] Sayaji Ratna Award Baroda Management Association, Honoris Causa, HEC Paris [118] 2016 Commander of the Legion of Honour Government of France [119] 2018 Honorary Doctor in Engineering Swansea University [120][121] 2022 Honorary Doctor of Literature HSNC University [122][123] 2023 Honorary Officer of the Order of Australia (AO) King Charles III [124] 2023 Maharashtra Udyog Ratna Government of Maharashtra [125] In popular culture Mega Icons (2018–2020), an Indian documentary television series on National Geographic about prominent Indian personalities, dedicated an episode to Ratan Tata's contributions.[126] Notes ^ Hindi: [rətən nəʋəl ʈaːʈaː] References ^ \"Tata Sons Board replaces Mr. Ratan Tata as Chairman, Selection Committee set up for new Chairman via @tatacompanies\". Tata. Archived from the original on 24 October 2016. Retrieved 24 October 2016. ^ \"Ratan Tata is chairman emeritus of Tata Sons\". The Times of India. Archived from the original on 11 February 2016. Retrieved 11 January 2016. ^ Masani, Zareer (5 February 2015). \"What makes the Tata empire tick?n;\". The Independent (UK). Archived from the original on 1 June 2016. Retrieved 30 April 2016. ^ a b c \"List of Fellows – Royal Academy of Engineering\". Raeng.org.uk. Archived from the original on 8 June 2016. Retrieved 2 December 2015. ^ \"Leadership TeamTata group\". Tata.com. 28 December 2012. Archived from the original on 1 May 2020. Retrieved 10 January 2022. ^ \"Ratan Tata turns 84: A list of 10 start-ups funded by the industrialist\". Business Today. 28 December 2021. Retrieved 17 August 2023. ^ \"Ratan Tata invests in companionship startup Goodfellows: Why he invested, the business model and other details\". The Times of India. 17 August 2022. ISSN 0971-8257. Retrieved 17 August 2023. ^ Hollar, Sherman. \"Ratan Tata\". Encyclopedia Britannica. Archived from the original on 12 September 2018. Retrieved 12 September 2018. ^ Langley (30 March 2008). \"Ratan Tata rode the tiger economy and now he drives Jaguar\". The Daily Telegraph. Archived from the original on 4 October 2012. Retrieved 31 March 2012. ^ এক্সপ্রেস, বেঙ্গল (8 December 2022). \"দুটা মাত্র ঘর, ব্যাবহার করেন না মোবাইল চিনতে পাচ্ছেন রতন টাটার এই ভাই কে ?\". Bengal Xpress (in Bengali). Retrieved 8 December 2022. ^ \"Ratan Tata goes back to school\". The Times of India. 31 March 2009. Archived from the original on 25 July 2013. Retrieved 31 March 2012. ^ Philip, Handler; Maddy, Handler (June 2009). \"Ratan Tata '59: The Cornell Story\". Cornell University. Archived from the original on 13 March 2018. Retrieved 12 March 2018. ^ \"QUAD Spring 2010\". Issuu. Retrieved 12 March 2018.[permanent dead link] ^ Ruchika Kumari (10 October 2024). \"Ratan Tata Education: From Education to Career Path, Everything You Need to Know About Ratan Tata\". Times Now. Retrieved 10 October 2024. ^ \"Ratan Tata, '62, Becomes Cornell University's Biggest International Donor – C-Suite Spotlight\". 2 December 2020. ^ a b c Majumdar, Shyamal (21 January 2015). \"40 Years Ago... and Now: Ratan Tata increased dare quotient of Tata group\". Business Standard. Archived from the original on 13 May 2016. Retrieved 29 April 2016. ^ \"Ratan Tata and NELCO Crucible – The untold story\". vivifychangecatalyst. 11 October 2013. Archived from the original on 11 October 2016. Retrieved 29 April 2016. ^ a b Christopher, Elizabeth; Deresky, Helen (2012). International Management: Managing Cultural Diversity (Second ed.). Pearson Australia. p. 457. ISBN 978-1-4425-3967-9. Archived from the original on 9 July 2020. Retrieved 30 April 2016. ^ \"The Tata group: Out of India\". The Economist. 3 March 2011. Archived from the original on 5 June 2016. Retrieved 30 April 2016. ^ Aiyar, Shankkar (24 February 2003). \"Ratan's Tata\". India Today. Archived from the original on 31 May 2016. Retrieved 30 April 2016. ^ Goldstein, Andrea (January 2008). \"The Internationalization of Indian Companies: The Case of Tata\". Center for the Advanced Study of India, University of Pennsylvania. p. 36. Archived from the original on 2 June 2016. Retrieved 30 April 2016. ^ \"Interview with Ratan Naval Tata\". Creating Emerging Markets. Harvard Business School. Archived from the original on 26 January 2021. Retrieved 10 December 2017. ^ \"Why Ratan Tata's Tigor rollout is a revolutionary statement\". The Economic Times. 9 December 2017. Archived from the original on 11 December 2017. Retrieved 10 December 2017. ^ Goswami, Ranjit (1 November 2016). \"Can India's mega-conglomerate Tata Sons survive its leadership crisis?\". The Conversation. Archived from the original on 10 January 2022. Retrieved 25 January 2017. ^ \"Cyrus P Mistry to succeed Ratan Tata\". Archived from the original on 25 July 2013. Retrieved 23 November 2011. ^ \"Ratan Tata, India's Corporate Czar, Retires With a $500 Billion Vision\". Bloomberg. Archived from the original on 20 January 2013. Retrieved 27 March 2013. ^ \"Cyrus Mistry Replaced by Ratan Tata as Tata Sons chairman – The Economic Times\". The Economic Times. Archived from the original on 26 October 2016. Retrieved 24 October 2016. ^ a b \"Cyrus Mistry back in the waiting line as Supreme Court stays order allowing him to be reinstated as Tata Sons Chairman\". Business Insider. Archived from the original on 26 January 2020. Retrieved 24 January 2020. ^ \"Tatas making Cyrus chairman wrong decision of lifetime:Supreme Court\". The Times of India. 27 March 2021. Archived from the original on 31 March 2021. Retrieved 31 March 2021. ^ \"Ratan Tata invests in TeaBox, comes on board as advisor\". Business Standard. 27 January 2016. Archived from the original on 10 January 2022. Retrieved 27 January 2016. ^ \"Ratan Tata invests undisclosed amount in online cashback venture CashKaro.com\". Economic Times. 19 January 2016. Archived from the original on 25 May 2016. Retrieved 7 June 2016. ^ \"Decoding Ratan Tata's start-up investments\". Livemint. 2 October 2015. Archived from the original on 7 December 2015. Retrieved 2 December 2015. ^ Flannery, Russell (2 May 2015). \"Ratan Tata Investment Underscores How Xiaomi Defies Odds\". Forbes. Archived from the original on 5 May 2015. Retrieved 4 May 2015. ^ \"Ratan Tata invests in pet care portal\". The Hindu. 4 January 2016. ISSN 0971-751X. Archived from the original on 10 January 2022. Retrieved 28 July 2016. ^ Sahay, Priyanka (4 January 2016). \"DogSpot raises funds from Ratan Tata, others\". Archived from the original on 12 August 2016. Retrieved 28 July 2016. ^ \"Ratan Tata invests in pet care portal DogSpot.in- Business News\". www.businesstoday.in. 4 January 2016. Archived from the original on 7 August 2016. Retrieved 28 July 2016. ^ \"Ratan Tata invests in home rental start-up NestAway\". Business Standard. Press Trust of India. 28 February 2016. Archived from the original on 29 February 2016. Retrieved 28 February 2016. ^ \"Ratan Tata Launches India's 1st Companionship Startup For Senior Citizens\". NDTV. 16 August 2022. Archived from the original on 16 August 2022. Retrieved 16 August 2022. ^ \"You'll Respect These Indians More After You Find Out How Much They Donate To Charity\". ScoopWhoop. 13 May 2015. Archived from the original on 13 November 2017. Retrieved 13 November 2017. ^ \"Tata Trusts\". www.tatatrusts.org. Archived from the original on 16 January 2020. Retrieved 13 February 2020. ^ \"Tata Trusts: A role model for philanthropy\". The Week. Archived from the original on 26 October 2020. Retrieved 15 October 2020. ^ \"UNSW looks to solar-powered desalination to help bust droughts josh\". RenewEconomy. 18 April 2019. Archived from the original on 18 April 2019. Retrieved 18 April 2019. ^ Duong, Cecilia (17 April 2019). \"Solar powered desalination offers hope of a global shift in agriculture\". UNSW Newsroom. Archived from the original on 18 April 2019. Retrieved 18 April 2019. ^ a b Bradley J. Fikes; Gary Robbins (23 October 2016). \"India's Tata gives UCSD $70M in hot area of genetics\". San Diego Union-Tribune. Retrieved 9 October 2024. ^ \"Tata ScholarshipUndergraduate Admissions\". admissions.cornell.edu. Archived from the original on 4 June 2019. Retrieved 16 May 2019. ^ \"Harvard Business School Receives $50 Million Gift from the Tata Trusts and Companies\". 14 October 2010. Archived from the original on 13 January 2017. Retrieved 14 January 2017. ^ \"Tata Hall Dedicated at HBS\". Harvard Magazine. 10 December 2013. Archived from the original on 13 August 2018. Retrieved 6 November 2018. ^ \"HBS Tops Off Tata Hall\". Archived from the original on 18 January 2017. Retrieved 14 January 2017. ^ \"A campus built on philanthropy – Tata Hall\". Harvard Business School -About us. Archived from the original on 22 June 2016. Retrieved 19 June 2016. ^ \"Cornell Tech inaugurates Tata Innovation Center\". ^ \"Tata Centre for Technology and Design\". www.datacentre.iitb.ac.in. Archived from the original on 13 May 2019. Retrieved 2 May 2019. ^ \"IIT-Bombay receives largest ever donation for research and development\". The Times of India. 22 August 2014. Archived from the original on 3 March 2019. Retrieved 2 May 2019. ^ \"Indian Institute of Science, Major benefactors\". iisc.ac.in. Archived from the original on 30 September 2021. Retrieved 30 September 2021. ^ \"Tata grant to IISc materialises five years after promise\". 28 July 2014. Archived from the original on 30 September 2021. Retrieved 30 September 2021. ^ \"Tata Center for Technology + Design\". MIT Innovation Initiative. 28 March 2016. Archived from the original on 6 February 2020. Retrieved 26 January 2019. ^ Jhunjhunwala, Shital (2020). \"Tata Sons and the Mystery of Mistry\". Vikalpa: The Journal for Decision Makers. 45 (3): 170–182. doi:10.1177/0256090920965420. ISSN 0256-0909. ^ \"The Pritzker Architecture Prize Adds Two New Jurors: Kristin Feireiss of Germany and Ratan N. Tata of India\" (PDF). Pritzkerprize.com. Archived (PDF) from the original on 24 September 2015. Retrieved 2 December 2015. ^ \"Ratan Tata named Cornell Entrepreneur of the Year\". Cornell Chronicle. Retrieved 27 January 2023. ^ \"Ratan Tata nominated to the board of Mondelez International\". The Times of India. 3 April 2013. Archived from the original on 3 October 2013. Retrieved 23 March 2014. ^ Ray (1 June 2008). \"Space Prizes: Ratan Tata and Michael Boustridge Join X PRIZE Foundation Board of Directors\". Spaceprizes.blogspot.in. Archived from the original on 8 December 2015. Retrieved 2 December 2015. ^ \"International Advisory Council – Bocconi University\". Archived from the original on 4 August 2016. Retrieved 21 June 2016. ^ \"International Advisory Board\". Hakluyt. Retrieved 16 October 2023. ^ \"Ratan N. Tata Joins Carnegie Board of Trustees\". Washington, DC: Carnegie Endowment for International Peace. 13 September 2013. Archived from the original on 29 November 2018. Retrieved 28 November 2018. ^ Sharma, Samidha (10 February 2015). \"Ratan Tata Turns Advisor to VC Fund\". The Times of India. Archived from the original on 16 August 2018. Retrieved 28 November 2018. ^ \"Ratan Tata's Legacy: How He Made Tata Group A Global Power\". Times Now. 10 October 2024. Retrieved 10 October 2024. ^ \"From Cyrus Mistry's sacking to NCLAT restoring him as Tata Sons head: A chronology of events\". The Indian Express. 18 December 2019. Retrieved 9 October 2024. ^ \"Came close to getting married four times: Ratan Tata\". The Times of India. 14 April 2011. Archived from the original on 30 August 2018. ^ \"Ratan Tata: An unlikely friendship between a magnate and a millennial\". BBC News. 15 March 2020. Retrieved 10 October 2024. ^ \"My bond with seniors behind Goodfellows: Shantanu Naidu\". The Indian Express. 17 August 2022. Retrieved 10 October 2024. ^ \"Ratan Tata in critical condition at Mumbai Hospital, says report\". The Times of India. 9 October 2024. ^ \"Ratan Tata, Whose Indian Business Empire Went Global, Dies at 86\". The New York Times. 9 October 2024. ^ \"Ratan Tata no more; Business Titan dead at 86\". Deccan Herald. 9 October 2024. Retrieved 9 October 2024. ^ a b Fraser, Simon (10 October 2024). \"India holds state funeral for tycoon Ratan Tata\". BBC News. Retrieved 10 October 2024. ^ \"Ratan Tata Death News LIVE Updates: Tata Group Chairman's Last Rites\". Moneycontrol. Retrieved 10 October 2024. ^ \"Ratan Tata's last rites: Wrapped in Indian flag, mortal remains arrive at Worli Parsi CrematoriumWatch\". Livemint. 10 October 2024. Retrieved 10 October 2024. ^ \"Padma Awards\" (PDF). Ministry of Home Affairs, Government of India. 2015. Archived from the original (PDF) on 15 October 2015. Retrieved 21 July 2015. ^ \"Assam CM Announces 'Assam Baibhav' Award To Industrialist Ratan Tata\". The Sentinel. 12 December 2021. Archived from the original on 18 December 2021. Retrieved 18 December 2021. ^ \"Honorary Degree – University Awards & Recognition – The Ohio State University\". Osu.edu. Archived from the original on 10 December 2015. Retrieved 2 December 2015. ^ [1] Archived 20 March 2014 at the Wayback Machine ^ \"Asian Institute of Technology confers doctorate on Ratan Tata\". Asian Institute of Technology. March 2008. Archived from the original on 30 May 2012. ^ \"B'Nai B'Rith International: Past Award Honorees\" (PDF). Bnaibrith.org. Archived (PDF) from the original on 18 December 2015. Retrieved 2 December 2015. ^ \"University of Warwick confers Honorary Doctor of Science on Ratan Tata\". London School of Economics. March 2005. Archived from the original on 25 July 2013. Retrieved 28 June 2011. ^ \"Young engineers should stay back to serve the nation, says Ratan Tata – TAMIL NADU\". The Hindu. 2 July 2006. Archived from the original on 6 October 2008. Retrieved 2 December 2015. ^ \"Ratan Tata wins responsible capitalism awardBusiness Standard News\". Business Standard India. Press Trust of India. 3 December 2006. Archived from the original on 24 November 2015. Retrieved 2 December 2015. ^ \"Ratan Tata becomes an LSE honorary fellow – 2007 – News archive – News – News and media – Home\". Lse.ac.uk. Archived from the original on 8 December 2015. Retrieved 2 December 2015. ^ \"Carnegie Medal for Philanthropy on Ratan Tata\". Carnegie Endowment for International Peace. March 2007. Archived from the original on 8 October 2011. ^ \"University of Cambridge confers doctorate on Ratan Tata\". University of Cambridge. March 2008. Archived from the original on 2 July 2011. Retrieved 28 June 2011. ^ \"Ratan Tata gets smarter by a degree\". Mumbai Mirror. August 2008. Archived from the original on 11 April 2013. ^ \"IIT Kharagpur confers doctorate on Ratan Tata\". Economic Times. March 2008. Archived from the original on 15 July 2012. Retrieved 28 June 2011. ^ \"Singapore confers honorary citizenship on Ratan Tata\". India Today. Indo-Asian News Service. 29 August 2008. Archived from the original on 8 December 2015. ^ \"Singapore Confers Prestigious Honorary Citizen Award on Mr Ratan N. Tata\". www.mom.gov.sg. 2 August 2008. Archived from the original on 30 January 2016. Retrieved 25 January 2016. ^ \"IET Honorary Fellows\". The IET. 2 October 2015. Archived from the original on 8 December 2015. Retrieved 2 December 2015. ^ \"The award\". The Performance Theatre. Archived from the original on 8 December 2015. Retrieved 2 December 2015. ^ \"GBE: Ratan Tata receives one of UK's top civilian honours\". The Economic Times. 5 May 2014. Archived from the original on 20 June 2015. Retrieved 20 June 2015. ^ [2] Archived 24 July 2014 at the Wayback Machine ^ \"Presidenza Del Consiglio Dei Ministri: Collocati A Riposo (Art: 7)\" (PDF). Governo.it. Archived (PDF) from the original on 27 September 2015. Retrieved 2 December 2015. ^ \"Honorary degree 2010 nominations announcedUniversity of Cambridge\". Cam.ac.uk. 1 March 2010. Archived from the original on 8 December 2015. Retrieved 2 December 2015. ^ \"2010 Hadrian Award GalaWorld Monuments Fund\". Wmf.org. 1 October 2010. Archived from the original on 8 December 2015. Retrieved 2 December 2015. ^ [3] Archived 15 April 2014 at the Wayback Machine ^ \"Yale Chief Executive Leadership Institute to Honor Tata Sons Chairman Ratan Tata with \"Legend in Leadership Award\"\". Yale University. September 2010. Archived from the original on 25 September 2011. Retrieved 28 June 2011. ^ \"Pepperdine Confers Honorary Doctor of Laws Degree on Ratan N. Tata\". Pepperdine University. September 2010. Archived from the original on 27 September 2011. ^ \"Seven secure Oslo Business for Peace Awards for 2010ICC – International Chamber of Commerce\". Iccwbo.org. 5 October 2010. Archived from the original on 9 January 2016. Retrieved 2 December 2015. ^ \"Winners of the Asian Awards 2010\". The Times of India. October 2010. Archived from the original on 4 November 2012. ^ George, Lucie (7 August 2012). \"Spotlight on engineeringForeign Office Blogs\". Blogs.fco.gov.uk. Archived from the original on 23 March 2014. Retrieved 2 December 2015. ^ \"Indian industrialist Ratan Tata honorary degreeUNSW Newsroom\". Newsroom.unsw.edu.au. 2 November 2012. Archived from the original on 8 December 2015. Retrieved 2 December 2015. ^ \"Conferment of Japanese Decoration on Mr. Ratan N. Tata, Chairman of Tata Group\". Embassy of Japan in India. 29 April 2012. Archived from the original on 14 July 2017. Retrieved 31 July 2016. ^ \"Ratan Tata receives lifetime achievement award\". Cornell Chronicle. Retrieved 27 January 2023. ^ \"National Academy of Engineering Elects 69 Members And 11 Foreign Associates\". The National Academies of Sciences, Engineering & Medicine. 7 February 2013. Archived from the original on 14 July 2014. Retrieved 19 March 2014. ^ \"Dr. Mukesh Batra, Dr. Mukesh Hariawala, Dilip Surana of Microlabs, Upinder Zutshi of Infinite Computers, Dr. Ravindranath of Global Hospitals, Ratan Tata, Priyanka Chopra Among Others Declared Winners\". Archived from the original on 10 January 2022. Retrieved 17 May 2017. ^ \"EY honors Ratan Tata with life time achievement award\". Ernst & Young. Archived from the original on 24 September 2015. Retrieved 6 August 2015. ^ \"Keynote & Honorees-Commencement Weekend – Carnegie Mellon University\". Cmu.edu. Archived from the original on 2 December 2015. Retrieved 2 December 2015. ^ \"Mr Ratan Tata receives honorary doctorate from SMUNewsSingapore Management University\". SMU. 1 March 2014. Archived from the original on 8 December 2015. Retrieved 2 December 2015. ^ \"BMA to confer Sayaji Ratna Award on Ratan Tata\". The Times of India. 6 April 2014. Archived from the original on 10 April 2014. Retrieved 21 August 2014. ^ \"Touched for being awarded GBE by UK: Ratan Tatabusiness\". Hindustan Times. 1 April 2014. Archived from the original on 18 April 2014. Retrieved 2 December 2015. ^ \"Sir James Bevan presents GBE (Knight Grand Cross) to Ratan Tata – News articles\". GOV.UK. 5 May 2014. Archived from the original on 9 January 2016. Retrieved 7 October 2015. ^ \"Ratan Tata gets honorary doctorate from York University of Canada\". IANS. news.biharprabha.com. Archived from the original on 23 June 2014. Retrieved 22 June 2014. ^ \"2015 SC Automotive Summit & SC Auto Week Agenda\" (PDF). Myscma.com. Archived (PDF) from the original on 4 March 2016. Retrieved 2 December 2015. ^ \"HEC ParisRatan N. Tata receives honoris causa degree from HEC Paris\". Hec.edu. 2 April 2015. Archived from the original on 9 January 2016. Retrieved 2 December 2015. ^ \"Highest French civilian distinction, Commandeur de la Légion d'Honneur conferred on Shri Ratan Tata\". France in India: French Embassy in New Delhi. 18 March 2016. Archived from the original on 4 August 2016. Retrieved 31 July 2016. ^ \"Swansea University Set for New Partnerships in India\". Business News Wales. 3 October 2018. Archived from the original on 10 January 2022. Retrieved 8 April 2020. ^ \"Tata Emeritus Chairman Ratan Tata awarded Honorary Doctorate\". www-2018.swansea.ac.uk. Archived from the original on 10 January 2022. Retrieved 8 April 2020. ^ \"industrialist ratan tata and Governor Bhagat Singh Koshyari attend convocation ceremony of HSNC University in mumbai photos |Photos: रतन टाटांनी वाढवले विद्यार्थ्यांचे मनोबल; विद्यापीठाच्या दीक्षांत समारंभात लावली हजेरी\". Loksatta. 11 June 2022. Retrieved 14 June 2022. ^ Ratan Tata conferred honorary D.Litt. by HSNC university - website of the newspaper Times of India ^ \"Australian Gazette\". 17 March 2023. ^ Banerjee, Shoumojit (19 August 2023). \"Ratan Tata conferred with Maharashtra govt.'s 'Udyog Ratna' award\". The Hindu. Retrieved 9 October 2024. ^ \"Mega Icons Season 2 Episode 2\". Disney+ Hotstar. Archived from the original on 1 November 2020. Retrieved 12 June 2021. Bibliography Naidu, Shantanu (2021). I Came Upon a Lighthouse: A Short Memoir of Life with Ratan Tata. Sanjana Desai. HarperCollins India. ISBN 978-93-90327-52-2. External links Media related to Ratan Naval Tata at Wikimedia Commons Quotations related to Ratan Tata at Wikiquote Ratan Naval Tata's chairman profile at Tata Group Profile at The Guardian Creating Emerging Markets Interview at the Harvard Business School Ratan Tata at IMDb Links to related articles Business positions Preceded by J. R. D. TataChairman of Tata Group 1991–2012 Succeeded by Cyrus Mistry Preceded by Cyrus MistryChairman of Tata Group 2016–2017 Succeeded by Natarajan Chandrasekaran vte Recipients of Padma Vibhushan ArtsEbrahim AlkaziKishori AmonkarPrabha AtreAmitabh BachchanTeejan BaiM. BalamuralikrishnaT. BalasaraswatiS. P. BalasubrahmanyamAsha BhosleNandalal BoseHariprasad ChaurasiaChiranjeeviGirija DeviKumar GandharvaAdoor GopalakrishnanSatish GujralGangubai HangalBhupen HazarikaM. F. HusainZakir HussainIlaiyaraajaSemmangudi Srinivasa IyerBhimsen JoshiAli Akbar KhanAmjad Ali KhanAllauddin KhanBismillah KhanGhulam Mustafa KhanYamini KrishnamurthyDilip KumarR. K. LaxmanBirju MaharajKishan MaharajLata MangeshkarSonal MansinghMallikarjun MansurZubin MehtaMario MirandaChhannulal MishraKelucharan MohapatraRaghunath MohapatraJasraj MotiramBenode Behari MukherjeeHrishikesh MukherjeeRajinikanthRam NarayanD. K. PattammalK. Shankar PillaiBalwant Moreshwar PurandareAkkineni Nageswara RaoKaloji Narayana RaoSatyajit RayS. H. RazaZohra SehgalUday ShankarRavi ShankarV. ShantaramShivkumar SharmaUmayalpuram K. SivaramanM. S. SubbulakshmiK. G. SubramanyanKapila VatsyayanHomai VyarawallaK. J. Yesudas Civil serviceBimala Prasad ChalihaNaresh ChandraT. N. ChaturvediJayanto Nath ChaudhuriSuranjan DasRajeshwar DayalBasanti DeviP. N. DharJyotindra Nath DixitM. S. GillHafiz Mohamad IbrahimH. V. R. IyengarBhola Nath JhaDattatraya Shridhar JoshiAjudhiya Nath KhoslaRai KrishnadasaV. KrishnamurthyP. Prabhakar KumaramangalamPratap Chandra LalK. B. LallSam ManekshawOm Prakash MehraMohan Sinha MehtaM. G. K. MenonBrajesh MishraSumati MorarjeeA. Ramasamy MudaliarSardarilal Mathradas NandaChakravarthi V. NarasimhanBraj Kumar NehruBhairab Dutt PandeGhananand PandeVijaya Lakshmi PanditT. V. RajeswarC. R. Krishnaswamy RaoPattadakal Venkanna R. RaoV. K. R. V. RaoBipin RawatKhusro Faramurz RustamjiHarish Chandra SarinBinay Ranjan SenHomi SethnaArjan SinghHarbaksh SinghKirpal SinghManmohan SinghTarlok SinghLallan Prasad SinghBalaram SivaramanChandrika Prasad SrivastavaT. SwaminathanArun Shridhar VaidyaDharma ViraNarinder Nath Vohra Literature and educationV. S. R. ArunachalamJagdish BhagwatiSatyendra Nath BoseTara ChandSuniti Kumar ChatterjiD. P. ChattopadhyayaBhabatosh DattaAvinash DixitMahasweta DeviJohn Kenneth GalbraithSarvepalli GopalLakshman Shastri JoshiKaka KalelkarDhondo Keshav KarveGopinath KavirajRadheshyam KhemkaKuvempuO. N. V. KurupPrasanta Chandra MahalanobisSitakant MahapatraJohn MathaiKotha Satchidananda MurthyGiani Gurmukh Singh MusafirBasanti Dulal NagchaudhuriBal Ram NandaR. K. NarayanP. ParameswaranAmrita PritamK. N. RajC. RangarajanRaja RaoRamoji RaoHormasji Maneckji SeervaiRajaram ShastriKalu Lal ShrimaliGovindbhai ShroffKhushwant SinghChandeshwar Prasad Narayan SinghPremlila Vithaldas ThackerseyMahadevi VarmaBashir Hussain Zaidi MedicineJasbir Singh BajajB. K. GoyalPurshotam LalA. Lakshmanaswami MudaliarS. I. PadmavatiAutar Singh PaintalKantilal Hastimal SanchetiBalu SankaranV. ShantaVithal Nagesh ShirodkarPrakash Narain TandonBrihaspati Dev TrigunaM. S. ValiathanDilip Mahalanabis OtherSunderlal BahugunaB. K. S. IyengarRambhadracharyaRavi ShankarVishwesha TeerthaJaggi VasudevB. V. Doshi Public affairsL. K. AdvaniMontek Singh AhluwaliaAruna Asaf AliFazal AliAdarsh Sein AnandMadhav Shrihari AneyParkash Singh BadalSikander BakhtMilon K. BanerjiMirza Hameedullah BegP. N. BhagwatiRaja ChelliahChandra Kisan DaphtaryNiren DeC. D. DeshmukhAnthony Lancelot DiasUma Shankar DikshitKazi Lhendup DorjeeGeorge FernandesP. B. GajendragadkarBenjamin GilmanIsmaïl Omar GuellehZakir HusainV. R. Krishna IyerJagmohanLakshmi Chand JainArun JaitleyAditya Nath JhaMurli Manohar JoshiAnerood JugnauthMehdi Nawaz JungAli Yavar JungVijay KelkarHans Raj KhannaV. N. KhareBalasaheb Gangadhar KherAkhlaqur Rahman KidwaiJivraj Narayan MehtaV. K. Krishna MenonHirendranath MukherjeeAjoy MukherjeePranab MukherjeePadmaja NaiduVenkaiah NaiduGulzarilal NandaGovind NarainFali Sam NarimanHosei NorotaNanabhoy PalkhivalaK. ParasaranHari Vinayak PataskarSunder Lal PatwaSharad PawarNaryana Raghvan PillaiSri PrakasaN. G. RangaRavi Narayana ReddyY. Venugopal ReddyGhulam Mohammed SadiqLakshmi SahgalP. A. SangmaM. C. SetalvadKalyan SinghKaran SinghNagendra SinghSwaran SinghWalter SisuluSoli SorabjeeKalyan SundaramSushma SwarajChandulal Madhavlal TrivediAtal Bihari VajpayeeM. N. VenkatachaliahKottayan Katankot VenugopalJigme Dorji WangchuckS. M. KrishnaMulayam Singh Yadav Science and engineeringV. K. AatreSalim AliNorman BorlaugSubrahmanyan ChandrasekharRajagopala ChidambaramCharles CorreaSatish DhawanAnil KakodkarA. P. J. Abdul KalamKrishnaswamy KasturiranganHar Gobind KhoranaDaulat Singh KothariVerghese KurienRaghunath Anant MashelkarG. Madhavan NairRoddam NarasimhaJayant NarlikarRajendra K. PachauriBenjamin Peary PalYash PalI. G. PatelVenkatraman RamakrishnanK. R. RamanathanRaja RamannaC. R. RaoC. N. R. RaoPalle Rama RaoUdupi Ramachandra RaoVikram SarabhaiMan Mohan SharmaObaid SiddiqiE. SreedharanM. R. SrinivasanGeorge SudarshanM. S. SwaminathanNarinder Singh KapanyS. R. Srinivasa Varadhan Social workBaba AmtePandurang Shastri AthavaleJanaki Devi BajajMirabehnKamaladevi ChattopadhyayDurgabai DeshmukhNanaji DeshmukhNirmala DeshpandeMohan DhariaU. N. DhebarValerian GraciasVeerendra HeggadeMary Clubwala JadhavGaganvihari Lallubhai MehtaUsha MehtaSister NirmalaNellie Sengupta SportsViswanathan AnandEdmund HillaryMary KomSachin Tendulkar Trade and industryDhirubhai AmbaniGhanshyam Das BirlaAshok Sekhar GangulyKarim Al Hussaini Aga KhanLakshmi MittalAnil Manibhai NaikN. R. Narayana MurthyM. NarasimhamPrithvi Raj Singh OberoiAzim PremjiPrathap C. ReddyJ. R. D. TataRatan Tata Portal Category WikiProject vte Padma Bhushan award recipients (2000–2009) 2000V. K. AatreAnil AgarwalRam Narain AgarwalSharan Rani BackliwalSwami KalyandevVeerendra HeggadePavaguda V. IndiresanWahiduddin KhanB. B. LalRaghunath Anant MashelkarH. Y. Sharada PrasadRajinikanthBegum Aizaz RasulRadha ReddyRaja ReddyPakkiriswamy Chandra SekharanKaramshi Jethabhai SomaiyaS. SrinivasanRatan TataHarbans Singh Wasir 2001Dev AnandViswanathan AnandAmitabh BachchanRahul BajajB. R. BarwaleBalasaheb BhardeBoyi BhimannaSwadesh ChatterjeeB. R. ChopraAshok DesaiK. M. GeorgeBhupen HazarikaLalgudi JayaramanYamini KrishnamurthyShiv K. KumarRaghunath MohapatraArun NetravaliMohan Singh OberoiRajendra K. PachauriAbdul Karim ParekhAmrita PatelPranAroon PurieB. V. RajuP. BhanumathiSundaram RamakrishnanChitranjan Singh RanawatPalle Rama RaoRaj ReddyUma SharmaL. SubramaniamNaresh Trehan 2002Gary AckermanH. P. S. AhluwaliaPrabha AtreSushantha Kumar BhattacharyyaChandu BordeEugene ChelyshevPravinchandra Varjivan GandhiShobha GurtuHenning Holck-LarsenZakir HussainB. K. S. IyengarF. C. KohliV. C. KulandaiswamyGury MarchukJagat Singh MehtaIsmail MerchantMario MirandaFrank PalloneRamanujam Varatharaja PerumalNatesan RangabashyamMaharaja Krishna RasgotraHabib TanvirK. K. VenugopalNirmal VermaK. J. Yesudas 2003Teejan BaiAmmannur Madhava ChakyarPrabhu ChawlaHerbert FischerJamshyd GodrejColuthur GopalanK. ParasaranB. Rajam IyerShri Krishna JoshiMadurai Narayanan KrishnanRajinder KumarRamesh KumarPurshotam LalSitakant MahapatraBagicha Singh MinhasSubhash MukhopadhyayP. S. NarayanaswamyArcot RamachandranTrichur V. RamachandranKantilal Hastimal SanchetiT. V. SankaranarayananNaseeruddin ShahT. V. R. ShenoyJagjit SinghRam Badan SinghHari Shankar SinghaniaUmayalpuram K. SivaramanNarayanan SrinivasanPadma SubrahmanyamSwapna SundariO. V. VijayanHerbert Alexandrovich Yefremov 2004Thoppil Varghese AntonySoumitra ChatterjeeChandrashekhar Shankar DharmadhikariGulzarSardara Singh JohlM. V. KamathKomal KothariYoshirō MoriGopi Chand NarangGovindarajan PadmanabanPoornima Arvind PakvasaVishnu PrabhakarN. RajamC. H. Hanumantha RaoThiruvengadam Lakshman SankarT. N. SeshagopalanBijoy Nandan ShahiKrishna SrinivasAlarmel Valli 2005Sardar AnjumAndré BeteilleChandi Prasad BhattTumkur Ramaiya SatishchandranMrinal Datta ChaudhuriYash ChopraManna DeyIrfan HabibYusuf HamiedQurratulain HyderTarlochan Singh KlerAnil KohliKiran Mazumdar-ShawMrinal MiriHari MohanBrijmohan Lall MunjalM. T. Vasudevan NairAzim PremjiBalraj PuriSyed Mir QasimA. RamachandranG. V. Iyer RamakrishnaV. S. RamamurthyK. I. Varaprasad ReddyK. Srinath ReddyGirish Chandra SaxenaNarasimaiah SeshagiriMark Tully 2006Jaiveer AgarwalP. S. AppuShashi BhushanGanga Prasad BirlaGrigory Bongard-LevinLokesh ChandraChiranjeeviDinesh Nandini DalmiaTarun DasMadhav GadgilA. K. HangalDevaki JainKamleshwarAbdul Halim Jaffer KhanSabri KhanGhulam Mustafa KhanShanno KhuranaGunter KrugerP. LeelaK. P. P. NambiarNandan NilekaniSai ParanjpyeDeepak ParekhM. V. PyleeSubramaniam RamadoraiN. S. RamaswamyPavani Parameswara RaoRamakanta RathV. ShantaHira Lall SibalBilly Arjan SinghJasjit SinghVijaypat SinghaniaK. G. SubramanyanK. K. TalwarVijay Shankar VyasDušan Zbavitel 2007Javed AkhtarGabriel ChiramelEla GandhiSaroj GhoseV. Mohini GiriSomnath HoreJamshed Jiji IraniGurcharan Singh KalkatN. MahalingamPrithipal Singh MainiTyeb MehtaRajan and Sajan MishraRajan and Sajan MishraSunil MittalRamankutty NairGopaldas NeerajIndra NooyiKavalam Narayana PanickerBhikhu ParekhSyed Mohammad Sharfuddin QuadriV. S. RamachandranTapan RaychaudhuriS. H. RazaJeffrey SachsChandra Prasad SaikiaL. Z. SailoShiv Kumar SarinShriram SharmaManju SharmaT. N. SrinivasanOsamu SuzukiK. T. Thomas 2008Mian Bashir AhmedKaushik BasuShayama ChonaJagjit Singh ChopraRahim Fahimuddin DagarChandrashekhar DasguptaAsis DattaMeghnad DesaiPadma DesaiSukh DevNirmal Kumar GangulyB. N. GoswamyVasant GowarikarBaba KalyaniK. V. KamathInderjit Kaur BarthakurRavindra KelekarAsad Ali KhanDominique LapierreD. R. MehtaShiv NadarSuresh Kumar NeotiaT. K. OommenK. PadmanabhaiahVikram PanditV. RamachandranSushil Kumar SaxenaAmarnath SehgalJasdev SinghShrilal ShuklaP. SusheelaS. R. Srinivasa VaradhanYuli VorontsovSunita WilliamsJi Xianlin 2009Isher Judge AhluwaliaInderjit Kaur BarthakurShamshad BegumAbhinav BindraShanta DhananjayanV. P. DhananjayanRamachandra GuhaShekhar GuptaKhalid HameedMinoru HaraJayakanthanThomas KailathSarvagya Singh KatiyarG. KrishnaR. C. MehtaA. Sreedhara MenonS. K. MisraA. M. NaikSatish NambiarKunwar NarayanNagnath NaikwadiKirit ParikhSam PitrodaC. K. PrahaladGurdip Singh RandhawaBrijendra Kumar RaoBhakta B. RathC. S. SeshadriV. Ganapati SthapatiDevendra TrigunaSarojini Varadappan # Posthumous conferral 1954–19591960–19691970–19791980–19891990–19992000–20092010–20192020–2029 vte Chairmen of Tata Group Jamsetji TataDorabji TataNowroji SaklatwalaJ. R. D. TataRatan TataCyrus MistryNatarajan Chandrasekaran Category vte Tata Group Divisions and subsidiariesInformation technology & engineeringTata Consultancy ServicesTata ElxsiTata Research Development and Design CentreTata Technologies AirlinesAir India Limited Air IndiaAir India ExpressAIX ConnectVistara (51%) SteelTata SteelTata Steel EuropeTata Steel Netherlands AutomotiveTata MotorsTata Motors CarsJaguar Land Rover Daimler CompanyLanchester Motor CompanyChery Jaguar Land RoverTata DaewooTata HispanoTata Hitachi Construction Machinery Consumer & retailTata Consumer ProductsEight O'Clock CoffeeGood Earth TeaKanan DevanTata SaltTetley Tata ChemicalsTata Chemicals Europe British SaltTata Chemicals MagadiTata Swach TrentLandmarkWestside Titan CompanyTanishqFastrackCaratLane Tata DigitalCromāBigBasketTata 1mgTata CLiQTata Neu VoltasVoltas Beko InfrastructureTata PowerGenerationMundra UMPPMaithon Power DistributionTata Power Delhi Distribution LimitedTP Central Odisha Distribution LimitedTP Western Odisha Distribution LimitedNorth Eastern Electricity Supply Company of Odisha (51%) OtherTata Power SolarTata Power SED OtherTata Housing Development CompanyTata Projects Financial servciesTata CapitalTATA AIG Aerospace & defenceTata Advanced SystemsTata Power SED Tourism & travelIndian Hotels Company Limited Ginger HotelsTaj HotelsTajAirVivantaThe PierreThe Newbury Boston Telecom & mediaTata Communications VSNL International CanadaTata Business Support ServicesTata TeleservicesTata PlayTejas Networks Trading & investmentsTata Investment Corp Joint venturesTATA AIG (74%)Tata Hitachi Construction Machinery (40%)Tata Starbucks (50%)Vistara (51%) Former holdingsCMCFavre-LeubaNew India AssurancePiaggio Aerospace (33.3%)Tata CoffeeTata Interactive SystemsTata DocomoTata McGraw-HillTata Oil Mills Company Lakmé CosmeticsTata Steel BSLTata TextilesTayo Rolls SportsJRD Tata Sports ComplexTata Football AcademyTelco Club GroundJamshedpur FCTata Steel Chess Tournament InstitutionsTIFRTIFR, MumbaiNCRA - TIFR, PuneTIFR, HyderabadCAM BangaloreHBCSE BangaloreNCBS Bangalore OtherBhabha Atomic Research CentreHomi Bhabha National InstituteIndian Institute of ScienceThe Energy and Resources Institute TERI School of Advanced StudiesTata Institute of Social SciencesTata Management Training CentreTata TheatreTata-Dhan Academy HospitalsHomi Bhabha Cancer Hospital & Research CentreTata Memorial CentreTata Medical Center TrustsSir Dorabji Tata and Allied TrustsSir Ratan Tata TrustLady Tata Memorial Trust PeopleGroup chairmenJamsetji TataDorabji TataNowroji SaklatwalaJ. R. D. TataRatan TataCyrus MistryNatarajan Chandrasekaran (incumbent) CurrentBoard of Tata SonsNatarajan Chandrasekaran (Chairman)Venu SrinivasanAjay PiramalRalf SpethHarish Manwani Group companiesNoel TataSurya Kant (TCS)Subramaniam RamadoraiT. V. Narendran (Tata Steel)Guenter Butschek (Tata Motors)Thierry Bolloré (JLR) Tata trustsRatan Tata (Chairman)R. K. Krishna KumarVenu SrinivasanAjay PiramalVijay SinghPramit JhaveriPrafulla DesaiMammen Chandy FormerTata familyNaval TataSimone TataRatanji Dadabhoy OtherCyrus MistryPallonji MistryNowroji SaklatwalaR. GopalakrishnanJamshed Jiji IraniIshaat HussainSumant MoolgaokarKarl SlymArdeshir DalalRussi ModyB. MuthuramanF. C. KohliS. JaishankarRajesh Gopinathan OtherBombay HouseRound Oak Steel TerminalTata CentreTata familyTata SonsTCS Amsterdam MarathonTCS NYC Marathon Category Authority control databases InternationalISNIVIAFFASTWorldCat NationalGermanyUnited StatesNorwayKorea OtherIdRefSNAC Retrieved from \"https://en.wikipedia.org/w/index.php?title=Ratan_Tata&oldid=1250483113\" Categories: Recent deaths 1937 births 2024 deaths 20th-century Indian businesspeople 21st-century Indian businesspeople Bishop Cotton School Shimla alumni Businesspeople from Mumbai Businesspeople in metals Businesspeople in steel Carnegie Endowment for International Peace Cathedral and John Connon School alumni Commanders of the Legion of Honour Cornell University College of Architecture, Art, and Planning alumni Foreign associates of the National Academy of Engineering Honorary Fellows of the London School of Economics Honorary Fellows of the Royal Academy of Engineering Honorary Knights Grand Cross of the Order of the British Empire Honorary officers of the Order of Australia Indian industrialists Indian philanthropists Parsi people from Mumbai Parsi people People from Mumbai Recipients of the Maharashtra Bhushan Award Recipients of the Padma Bhushan in trade and industry Recipients of the Padma Vibhushan in trade & industry Riverdale Country School alumni Tata Group people Tata family Hidden categories: Pages with Hindi IPA CS1 Bengali-language sources (bn) All articles with dead external links Articles with dead external links from November 2023 Articles with permanently dead external links Webarchive template wayback links Articles with short description Short description is different from Wikidata Wikipedia pages semi-protected against vandalism Use Indian English from October 2024 All Wikipedia articles written in Indian English Use dmy dates from October 2024 Articles with hCards All articles lacking reliable references Articles lacking reliable references from October 2024 Commons category link is on Wikidata",
    "commentLink": "https://news.ycombinator.com/item?id=41795218",
    "commentBody": "Indian entrepreneur, industrialist, and philanthropist, Ratan Tata, dead at 86 (wikipedia.org)346 points by Brajeshwar 15 hours agohidepastfavorite118 comments wuschel 3 hours agoThere is more to the story than I can tell here, unfortunately, but at least I can write this: During my work for Bentley Motors I was at one of the Geneve Motor Shows in the 2010s. During my stay at the fair, a new (1500 USD) Tata car was introduced. I visited the Tata stand with a friend, looking at their new car, which was quite the contrast in product philosophy, design and target group to the Bentley models. Thanking our host at the Tata stand for our personal tour, I gave him the invitation to return him the favour and show him the Bentley models and stand. To my great surprise later that day Ratan Tata came to the Bentley stand with what appeared to be his family (some male and female family members) - and I was able to show him around. We could not talk much due to the bodyguards and press, but he seemed distinct in demeanor to his sons and entourage. Apart from the colourful and diverse customer group, I met Piech, other industrial magnates of our time, but Ratan managed to retain a humble and human aura. I sympathized with him. reply bongoman42 2 hours agoparentYes, I had the chance to meet him once because he invested in our company and paid a visit. He walked around the office and met folks, was warm and approachable even more so than our own executive team. Massively underrated guy. reply linksnapzz 2 hours agoparentprevDefinitely a contrast with Piech; who had once been described as \"just a monocle and persian cat away from being the villain in a James Bond movie\". reply wanderingmind 14 hours agoprevPeople will talk about his different contribution to economy and philanthropy. However, my favorite is the story where he instructed his hotel employees to treat stray dogs well if they enter the premises. For a man of his wealth and power, to care about a stray dog truly speaks to his humble nature. RIP Sir, you were a crown jewel of the post independence India. https://m.economictimes.com/magazines/panache/ratan-tata-ins... reply aitchnyu 13 hours agoparentOn one hand, stray dogs are a joy for many Indians. For many months the highlight of the day was going to a cafe at 10 pm in middle of work and throwing bones to the assembled dogs. On the other hand, they enter homes and kill babies, attack children and adults and carry rabies epidemics. Right thing to do is to prevent more strays from happening. reply Karrot_Kream 10 hours agorootparentI'm laughing that this is the subthread that generates the most conversation so far on a thread about Ratan Tata's death. reply bravetraveler 9 hours agorootparentSee also: John Wick films, memes. Many signs we're better for animals than ourselves reply blackwateragent 12 hours agorootparentprevA little ignorant here, could you explain a little more on this part. > the highlight of the day was going to a cafe at 10 pm in middle of work Is it 'normal' to have to work late to 10pm, or does it mean second job, etc.? Or was that just a typo and you meant 10am? My gut instinct is 10am, but wanted to ask directly out of curiosity if I was wrong and get a better cultural understanding. reply bongoman42 2 hours agorootparentIts not uncommon in India, especially in offices that service the US time zones. reply thanksgiving 9 hours agorootparentprevI worked for a company with people in India and they were available through like ten am eastern time. That’s about seven thirty pm which isn’t so bad but if there was any problem my manager would not hesitate to reach out to them even at three pm eastern. reply aitchnyu 10 hours agorootparentprevCrunch and 12 hour days are pervasive at tech companies. Also people got meetings with clients or overlap with client timezones (like 2 pm to 11 pm). Hence the office complexes and eateries are still busy at 10 pm. Sorry the \"day\" was confusing. reply k1kingy 12 hours agorootparentprevNight shift/2nd shift is a thing. reply blackwateragent 12 hours agorootparent> Night shift/2nd shift is a thing. Yes, of course and definitely I considered as I mention if he/she meant 2nd job, etc. Just the phrasing threw me off as it says 'day', then '10pm' and it's written as if it is a normal occurrence for the majority of the population (which would imply night shift/2nd shift is normal working times for the majority). Just wanted to get clarification from OP on exactly what was meant to satisfy my own curiosity. reply jannes 10 hours agorootparentMaybe they're working in a call center > Call centre work is typically done overnight to accommodate time zones in the US, UK, and Australia https://en.wikipedia.org/wiki/Call_centre_industry_in_India reply fakedang 12 hours agorootparentprevA lot of people tend to work US/EU hours in India because they're \"takin' yer herbs!!\". reply lazide 11 hours agorootparentprevIt’s pretty typical for many Indians with any sync-up with the US (which is a lot) to work 12 hr day type schedules starting around 10-11am, with a long break in the middle for dinner/family time. Otherwise, it just doesn’t work eh? reply joncrane 5 hours agorootparentprevCan you link to some incidents of stray dogs entering houses and killing babies? reply ericd 5 hours agorootparentNot sure about entering houses, but a friend’s 5 year old just got bitten by a stray dog on the stomach during a month stay in India, she had to go through a course of rabies shots in the stomach. reply jimbob45 12 hours agorootparentprevDo Indians not have animal shelters? Which…now that I think about it, is just hunting with additional steps. For that matter, do Indians hunt? reply abhiyerra 2 hours agorootparentYes, Indians go hunting. Some of my earliest memories were going hunting with my uncle for pheasants in India. reply triceratops 4 hours agorootparentprev> animal shelters... is just hunting with additional steps Explain reply mcmcmc 4 hours agorootparentSince GP won’t, the explanation is a racist assumption that shelter animals would be slaughtered and eaten reply jimbob45 3 hours agorootparentHaha wtf that's not what I was going for at all. I was talking about hunting within the context of animal population control. Shelters perform animal population control with a few added steps. Not sure why everyone thought it was about eating animals when the whole danger of strays is because of a lack of animal population control. reply triceratops 3 hours agorootparentI guess most people think of hunting as a recreational activity, followed by hunting for food. Hunting for population control is usually called culling. Regardless I didn't downvote your original comment. I just found it puzzling. Additionally, animal shelters don't just sterilize animals, they find them new homes. A sterilization-only effort would be more like \"hunting (culling) with extra steps\" reply debarshri 10 hours agorootparentprevThere are shelter in metros. For eg. Yoda in Mumbai https://yoda.co.in/ reply lazide 11 hours agorootparentprevThe Indian subcontinent is so densely populated, very few areas can support hunting. It does happen in some rural areas. Sport hunting is not really a thing though. Most Indians (statistically) are vegetarian, and essentially obligate vegetarian due to the population density (also enforced by religion), though ‘vegetarian’ can include eggs, fish, and sometimes chicken depending on the person, time, etc. Indian Muslims are the notable exception (~13% of the population) and it does cause some serious friction at times - like riots. A favorite rage bait topic is someone killing a cow. It gets people killed pretty often. reply stasi9 11 hours agorootparentStatistically most indians are Non-vegetarians(https://en.wikipedia.org/wiki/Vegetarianism_by_country#India). Various estimates mostly around 20-40%. Statistically i \"think\" more people are killed by terrorist attack then cow vigilantes. reply ignoramous 7 hours agorootparent> more people are killed by terrorist attack then cow vigilantes Cow vigilantes makes it sound like Batman & Robinhood when in fact it is a euphemism for Saffron Terror https://en.wikipedia.org/wiki/Saffron_terrorism reply lazide 10 hours agorootparentprevhttps://www.pewresearch.org/short-reads/2021/07/08/eight-in-... This one says 39% are ‘pure vegetarian’, with 81% ‘limiting meat’ (aka ‘mostly vegetarian’). If just a few percentage points shy of the total non-Muslim population, notably. Which lines up with what I’m saying. And looks like 84 killed in lynchings related to cow killings in this table [https://en.m.wikipedia.org/wiki/List_of_incidents_of_cow_vig...]. I didn’t say a lot died, just that it is a good rage bait topic when someone gets accused of it - and people do get killed over it. Terrorism in India is a big problem, so yes the number of deaths swamp those 12k or so? [https://en.m.wikipedia.org/wiki/Terrorism_in_India] reply addicted 10 hours agorootparentNo comment on the specifics of these issues, but I find the Wikipedia pages illuminating. The “cow lynching” one has details about every incident. Whereas the “terrorist attacks” one simply has summaries. The “cow lynching” is treated as far more important where each incident needs to be explained, but the much more numerous and impactful terrorist incidents are treated as less important. Like Stalin said, 1 death is a tragedy, 1 million deaths is a statistic. reply lazide 9 hours agorootparentAlso, in India a cow killing is both blasphemy/violence against a god figure, and an attack on an important source of renewable protein for the population. Where terrorism is, uh. Business as usual. I remember once when a political party in Bangalore bombed their own headquarters - but got caught doing it. Oops. Within a day or two, the scandal was out of the headlines to be replaced by yet another issue. Trash disposal problems, I think. reply dyauspitr 5 hours agorootparentprevPure vegetarian maybe but eating chicken or fish once or twice a month is still pretty much vegetarian and that’s the vast majority of the population. reply codegeek 2 hours agorootparentThat's not what being Vegetarian means. reply dyauspitr 6 minutes agorootparentIt isn’t but it kinda is. If 95% of your meals are vegetarian then I think the designation holds. renewiltord 2 hours agorootparentprevI’ve been vegetarian all my life because I’ve eaten plants with almost all my meals. One time I was a pure carnivore and eat a meat platter but most of the time I’ll manage at least one olive or something. reply tightbookkeeper 4 hours agorootparentprevVegetarian is in India is often class signaling. reply lotsofpulp 7 hours agorootparentprev> though ‘vegetarian’ can include eggs, fish, and sometimes chicken depending on the person, time, etc. What is the purpose of the word vegetarian if one uses it to mean consuming some animals but not others? Vegan = no animals used to make food Vegetarian = no animals consumed Ovo lacto vegetarian = vegetarian that eats eggs and dairy (I guess additional clarifiers in case someone thinks vegetarian does not include eggs or dairy) Pescatarian = fish + some flavor of vegetatian/vegan Omnivore = eats everything Carnivore = only eats animals reply somethingsright 3 hours agorootparent> What is the purpose of the word vegetarian For most people Vegetarian means no animal is killed - so eggs are ok. In fact, some people claim egg is veg to convince people to consume eggs as protein source. For proper clarity, some people use eggetarian for eggs allowed. Seafood as veg, I guess is more a concept outside India, as consuming seafood is not seen as killing animals. Depending on how the question was interpreted, Vegetarian might mean: May eat outside, but will not cook non-veg at home May eat non-veg only if veg option is not available. Kind Advice for the meat eaters when lunch is brought in office: -- Do not finish the only 2 vegetarian sandwiches because you wanted to see what the impossible burger tastes like. -- Try not finish the cheese pizzas first and then touch the pepperonis later, if there are people who will come to the lunch table later It is for this reason I always say Veg so that there is still some count available for people who need it. Fun Facts: Food packaging (and restaurant menus) in India show \"signal\" of green or red circle to indicate veg or not. Airlines have a menu option of Hindu meal and Veg meal. Hindu meal can have chicken and omelet (or may not). Veg meal could be just salad and fruits. Many Indians pre-order Hindu and get confused when they get chicken. For me it is easier to pre-order Hindu and get cooked food with spices, and I can skip if it is chicken (or ask for veg). [Jain meal is further restricted to veg grown above ground] reply aziaziazi 4 hours agorootparentprevVeganism = no animals \"used\" to make food or whatever (soap, shoes, invivo testing...) Many people here seems to make the confusion that veganism is about the food. It's not, it's about the animals. > What is the purpose of the word vegetarian if one uses it to mean consuming some animals but not others? Languages are not like mathematics, one word can convey many meaning and nothing is stone-defined (dictionaries are only an interpretation of a language). In the Indian context there's a BIG part of the population that have a diet that does not have a definition in the Official Oxford Dictionary. reply lotsofpulp 3 hours agorootparent> In the Indian context there's a BIG part of the population that have a diet that does not have a definition in the Official Oxford Dictionary. How does omnivore not cover most people’s diets in India? (in the context of a discussion about populations of people that avoid eating animals) It seems like Indians use vegetarian to describe frequency or proportion of one’s diet that is animals. Seems like there could be better terminology used to avoid confusion. reply aziaziazi 1 hour agorootparentOmnivore word does cover post people diet and is indeed more accurate, but how is it useful to know that 99.9% of Indians are omnivore? « 40% avoid eating meat most of the time buy may consume it once a month » is more informative but not very practical to repeat each time someone ask your diet. By the way its the same for veganism : most vegans have at least once washed their hand with animal glycerin in a public bathroom soap or used a non vegan cloth washing soap while traveling. Are they less vegan? For some dictionary maybe, but most people don’t care about definition absolutism and prefer focus on the motivations and the results. reply lazide 6 hours agorootparentprevDon’t ask me, I’m just using the terms and definitions locals use. reply keybored 1 hour agoparentprev> For a man of his wealth and power, to care about a stray dog truly speaks to his humble nature. Caring about something is no feat of character or strength when that just means making someone else do it. reply jajko 7 hours agoparentprevThat really isn't that common, IMHO and speaks volumes about his empathy. Its true that my 6-month backpacking experiences are 14 and 16 years old and India is rather a continent on its own, not only population wise but also culturally and geographically so don't want to generalize here. I've seen utter ignorance from ie older girls to stray dogs dying in the middle of the street, while I was reeling from mild shock sitting on the curb. I guess after few years there I would be desensitised too. Cows have it easy there, other animals not so much by western standards. Its true that stray dogs specifically are also a potential threat, the picture ain't black & white. reply talonx 13 hours agoparentprevI came here to comment about the same thing. reply mcint 13 hours agoprevArguably black-bar material. Outside view, it seems like he served a tenure atop a group comparable to Disney, in breadth, scale, and market share across many industries in India, as a magnate. Notably, in the period of massive telecom rollout in India. Largest wholly owned and most advanced subsea fibre network, carrying around 30% of the world’s internet routes - Brave.com's Llama, sourced from Tata sites Note \"30% of the world’s internet routes\" is BGP burden to other operators, but by the same token, a sign of widely distributed control of networks. As an investor, \"first Indian to buy a stake in Xiaomi\" among dozens of startup investments. \"Some senior executives from Xiaomi were quoted saying that they would seek Ratan Tata’s advice on how to expand globally.\" As a philanthropist, perhaps larger still in relevance to seeding, supporting, & growing the hacker community, his philanthropy on US college campuses in tech, biotech & genetics, and scholarships, serve to support more bright, hungry, creative individuals to learn in the US. reply hackernewds 12 hours agoparentI'm finding it really tough to understand your comment across the flipflopping of formats. Here’s a clearer and more concise version of your text: Comment from mcint on HN: From an external perspective, he can be viewed as a magnate who led a group comparable to Disney in terms of breadth, scale, and market share across multiple industries in India. His leadership was particularly notable during the massive telecom rollout in the country. He oversaw the largest wholly owned and most advanced subsea fiber network, responsible for carrying around 30% of the world’s internet routes (source: Brave.com’s Llama, via Tata). While the \"30% of the world’s internet routes\" signifies a burden on BGP for other operators, it also highlights the distributed control of global networks. As an investor, he was the first Indian to acquire a stake in Xiaomi, among many other startup investments. Some Xiaomi executives even noted that they sought Ratan Tata’s advice on global expansion. As a philanthropist, his contributions are even more impactful. He has supported the hacker community, donated to US college campuses in tech, biotech, and genetics, and funded scholarships to foster talented, driven individuals who come to learn in the US. reply burkaman 6 hours agorootparentThis just repeats the whole comment with more words. Please don't copy-paste autogenerated text here unless it adds to the conversation somehow. reply benjaminwootton 12 hours agorootparentprevIs this a GenAI comment responding to a GenAI comment? reply 7thpower 11 hours agorootparentOP was not easy enough to follow to be an LLM, right… right? (OP, I mean this in the kindest of ways) reply MichaelZuo 9 hours agorootparentprevHow is it more concise if it uses roughly the same amount of verbage? reply throwaway10oct 12 hours agoprevThrowaway account here, but I think we should be cautious about hero worship. While it's true that Tata has contributed positively in many areas, there are also significant controversies surrounding his legacy—like leasing coal mines for just 25 paise for 999 years before independence(1), among other issues(2) It's important to consider both the positives and negatives to form a well-rounded opinion. Let's aim to be informed and objective rather than blindly idolizing anyone. (1) https://www.firstpost.com/business/a-tata-coalgate-999-yr-mi... (2) https://www.bhopal.net/the-ugly-face-of-tata/ reply mlnj 10 hours agoparentI too am a big fan of not supporting hero worship. But what we should recognize is the deeds that they perform and the values that they uphold rather than supporting them in everything they do. Having met the guy multiple times growing up he always stood out to me as a very humble man that loved the people and the institution he built. His love for dogs was something that helped me be closer to animals. With regards to the comments about post-independence industrialization, most countries go through that phase where industrialists of the time stand to gain very lucrative opportunities to build value. reply tightbookkeeper 4 hours agorootparentExactly. Contribution ethic vs purity ethic. reply addicted 9 hours agoparentprev> Despite repeated reminders, the company (Tata Steel) didn’t comply with the rules,” the confidential letter reads. > And when threatened with recourse to the law, “the company started submitting the correct royalty. IOW they continued doing what they had always been doing, and ignored a bunch of letters saying they should pay more. When the government took a more serious approach they started paying the royalty they were supposed to under the new rules. Not a great look, but hardly beyond the norm. reply skylurk 3 hours agoparentprev> It's important to consider If I had a penny... reply tightbookkeeper 4 hours agoparentprevEvery prominent person will have controversy. Expressing values means choosing one thing over another. reply yas_hmaheshwari 10 hours agoparentprevI have no doubt that we humans would find fault with Buddha as well, so I am a little intrigued by this criticism (as someone mentioned he would be 9 years old when coal mining incident happened) but not totally surprised But yeah, lets find fault with everyone to form a \"well-rounded opinion\", because that is what we should strive to achieve reply fakedang 12 hours agoparentprevSo Ratan Tata was responsible for leasing coal mines in 1946 when he was 9 years old? At those prices, the guy was a **ing prodigy. This is like aiming for the trees and missing the forest - the corrupt entity here is the Indian government which facilitates such corruption and monopolization and makes it hard for new entrants to compete. reply amarsharma 7 hours agoprevExtremely saddening to hear this, he was an inspiration growing up as a technologist and entrepreneur. Tata does a lot of philanthropy and does a lot of free cancer treatments, aside from all the great industries he has built. Om Shaanti https://www.tata.com/community/health/tata-memorial-centre reply m0llusk 5 hours agoparentHe had a great life. Is dying at 86 really unexpected or sad? It seems to me like an expected end. Exactly how long should he live for his passing to be normalized? reply StackRanker3000 5 hours agorootparentIsn’t it normal and human to be sad about someone’s passing, even if it’s expected? reply stareatgoats 4 hours agorootparentIt is. But it is also perhaps interesting to note that most people seem to rate deaths differently depending on the age of the person. A young person cut down in their prime is a special kind of tragedy, so is the death of toddlers and young children. The passing of an old person, who by all accounts have outlived most other people on earth as far as we can tell, is saddening, especially to the people in the near family. But perhaps not shocking and traumatizing, as the other examples might be. Just observing, not saying that this is how it should be. reply salesynerd 3 hours agorootparentI guess we are in agreement here. As far as I understand, the word shocking is used, in this context, at the unexpected happening of the event; not that the event happened. If I remember the sequence of events correctly, he was admitted to the hospital for a routine checkup and on Monday, his team had released a statement on his behalf that he was recuperating. Another reason that a lot of Indians found this news shocking is because of the value and emotions attached to the name, \"Tatas\". Post India's independence, they were instrumental in helping India industrialized; the other were the Birlas. reply ankit219 9 hours agoprevMany Indians (esp entrepreneurs) don't realise it, Ratan Tata's conduct is the default expectation how the successful and hyped people in India are to behave (not taking a moral stand, rather a factual one) in the society. He was known to be humble and kind and never made headlines for the wrong reasons. Many would do extensive PR, while he stayed away from the limelight. It's remarkably difficult in a country where the heroes are few and get a huge coverage if they want it. Whenever a big brand wants to enter India and (due to FDI rules) has to partner with an Indian brand, invariably they go with Tata (Starbucks, Foxconn). The image of being upstanding, clean, and good at execution is hard earned. reply pphysch 2 hours agoparentWhat does \"default expectation\" mean here? Didn't the Ambanis just have the most opulent wedding of the 21st century? reply stonethrowaway 3 hours agoparentprevIn other words, avoid pathological behaviour. reply ChrisArchitect 13 hours agoprevAn actual news report would be helpful from any of the 15 other submissions OP: Indian tycoon Ratan Tata dies aged 86 https://www.bbc.com/news/articles/cjd5835mp4ko reply screye 10 hours agoprevFor some context, this is the end of an era for the Tata family. They've had an odd pseudo nepotistic passing-on-the-torch framework for 100+ years now. The Tatas adopt promising children in the extended family, instead of passing ownership by birth order. Ratan Tata's father was adopted into the extended family, and Ratan later got adopted by the main line of the family to be groomed into heir apparent. Keeping up with the tradition, Ratan never married or had his own children. Cyrus Mistry, the head of another billionaire family, became one with the Tatas through marriage & mergers. Cyrus was groomed to be the next heir apparent. Unfortunately, he met an untimely death from a car accident in 2022. 2022 marked move away from the century long tradition of keeping it in the family. The new CEO is a self-made man with no relations to the family or the Tatas' shared religion (Parsi). Usually this merit based system would be cause for celebration, but the Tatas hold a paragon-esque reputation among well-run old-money family-owned institutions. It'll be interesting to see if the company loses it's heart as it slowly morphs into a faceless conglomerate. RIP to Ratan. reply mandeepj 9 hours agoparentIt’s notable here - Cyrus was fired from his Chairman position before his untimely death reply Imustaskforhelp 10 hours agoprevAs an Indian , this news is absolutely heartbreaking. Ratan Tata was a gem to India May his soul rest in peace. This news has really taken me a back reply yas_hmaheshwari 10 hours agoparentI am with you. I am personally feeling bad that he died. Can't remember any time in recent history when a person with whom I have no personal connecting died, and it is impacting me so much reply dotps1 6 hours agoprevI'm sure he was an amazing capitalist, but my experience with Tata consulting was the worst. I worked for a division of GE during the Immelt years that outsourced large portions of IT to Tata, and was in charge of the transition. It was a masterclass in waste and inefficiency. Definitely one of the larger nails in the coffin of a former Fortune 5 company. reply lgleason 1 hour agoparentNot to mention all of US jobs that we said \"Ta ta\" to as they went to India. I worked for GE during that time and the US divisions were pretty bad as well. The handwriting was on the wall with the future of the company. I'm sure someone in the C-Suite wrote a Kaizen to shift that inefficiency overseas and probably got a bonus for it as well. reply gautamsomani 5 hours agoparentprevHave same thing to say. I worked for a very large e-commerce company in India, who when they were building their DC, contracted TCS. TCS gave a nightmarish experience on execution and ethics. They were horrible. reply hyggetrold 2 hours agoparentprevThis is all I ever had heard about Tata consulting as well so I have to say it's been an education seeing how sad everyone is to see Ratan Tata pass away. It does make a kind of sense to me now though - if Ratan Tata's goal was to pull money into India, he was massively successful. How he got there might be a different story. But that's just as true of all previous great capitalists as well. reply justmarc 3 hours agoparentprevI for one would love to read about these types of stories, I'm sure many on HN too. I started a thread here https://news.ycombinator.com/item?id=41800114 let's see if people pick up on it. reply uptownfunk 13 hours agoprevOne of the greatest entrepreneurs in the world. I hope they make a movie on him and his life. reply hyperbrainer 13 hours agoprevTata as a company has had near-unrivalled impact on the economy of India. Godspeed. RIP reply Karrot_Kream 10 hours agoprevEven among the diaspora Ratan Tata is well known and loved. A shining gem in a country full of corruption. Om Shanti Sir. reply RigelKentaurus 14 hours agoprevHe was one of the few industrialists who gave capitalism a good name. He was never one to do ostentatious displays of wealth or buy islands. He used his vast wealth and influence in the best possible way. RIP. reply hackernewds 12 hours agoparentThey gauge he would've been the richest person in the world at many points, if not for his generous philanthropy reply codetiger 12 hours agoparentprevWhile the internet is flooded with thousands of posts about his demise, this statement you made is what I 100% agree. reply cosmos_sajal 3 hours agoprevIf there's something everyone should learn from Mr. Tata, \"Be Humble\" reply Gentil 12 hours agoprevI don't think people outside India understand what Tata means. Who Ratan Tata was. He was not some random rich person who did some philanthropy. ~66% of everything that Tata companies make goes back to philanthropy and the people of India. And these days to people where Tata companies operates (I presume). To the TATAs, people are of primary importance. They are a behemoth of a group comparable to Samsung or other big companies/conglomorates. They don't deceive, they don't put money over people. Because their mission IS to the people. You can buy a Tata product with a level of trust that no other brand can provide. Seeing TATA along with a product is more than enough. They are the best definition of capitalism I have ever come across. This is why you don't see Ratan Tatas in billionaire's list or rich people's list. Everything India is because of the Tatas. They single handledly is responsible for building India's foundations. Whether it is in Health, Nuclear, Tech or anywhere. You will see Tata's presence everywhere. I genuinely shed a tear today morning because of his passing. And I am not sure I would do the same for any other business man. Today is indeed a sad day. May he rest in peace. reply pkphilip 12 hours agoparentThe reputation of the Tata group is not as stellar as it was under the leadership of JRD Tata. Nonetheless, it is one of the few large industrial groups in India to go above and beyond their commercial interests to look at the interests of Indians as a whole. I remember the day JRD Tata died. I was still in college then and I had an overwhelming sense of grief over his death. It was a huge loss to lose a man of his caliber and my grief was for the country. With Ratan Tata, it is similar but not on the same scale as JRD. reply Gentil 12 hours agorootparentProbably. I don't think it's possible to please everybody. Also, under Rata Tata, Tata went truly global. I can only presume that it means increased responsibilities & increased issues. JRD Tata didn't go through the same problems as Ratan Tata did. And I don't mean that JRD Tata struggled less. Or more for the matter. The problems were simply different. There is also the fact that we humans are connected 24/7 unlike previous generations. This also means you don't hear the negativity or problems like we do today about someone. This is the norm these days. If you need to find the same for older generations, you really need to dig through it. I say this cos everyone I admire from yester generations come out with a lot more mistakes and issues as I learn more about them. I find it humane. It just makes them human. What I admire about Ratan Tata is that he tried to be better person. As much as he can. And he was one of the nicest human as well. That is enough. :) reply sandeep1998 14 hours agoprevOm Shanti reply hackernewds 12 hours agoparentnext [4 more] [flagged] saagarjha 11 hours agorootparentThat's because it's the moral equivalent of posting \"RIP\" or similar. reply alienonwings 11 hours agorootparentprevThat's \"RIP\" for Indians. reply pknerd 8 hours agorootparentHindus, precisely. reply infocollector 14 hours agoprevhttps://www.reuters.com/world/india/ratan-tata-indias-tata-c... reply __coder__ 12 hours agoprevHe was very humble person , never showed off his wealth, and mostly followed responsible capitalism, these are the reason why many indians see him as an inspiration. RIP sir reply ksampath02 12 hours agoprevMay he rest in peace, his name will be remembered and impact felt for long. reply pkphilip 12 hours agoprevThis is a huge blow. For those not familiar with the history of Tata group of companies and their influence over India, they will not understand the significance of this death. reply 2Gkashmiri 14 hours agoprevAs someone who stays far from politics, ratan tata and tata group in particular are the extraordinary entities who do opposite of enshitification. They dont cut corners (like everyone) and are highly regarded as being genuinely good. Case in point. From a tax perspective, historically reliance indistries was always known for being sneaky around taxes, dodging taxes, finding loopholes and such. Tata OTOH, they are historically regarded as being much more honest and trustworthy in this regard. reply profsummergig 12 hours agoparentTata could have been much, much bigger. But to the extent they could get away with it, they refused to pay bribes to politicians and government employees (they couldn't refuse to pay \"any and all\" bribes, for they would have been shut down completely if they took that posture). reply 2Gkashmiri 9 hours agorootparentyeah. there is a difference between \"Speed money\" and actual bribes for officials to look the other way when you are doing clearly not legal stuff. reply thisisit 7 hours agoparentprevHuh. I wonder what this man “staying away from politics” using the infamous lobbyist Nira Radia for? Oh I remember lobbying and manipulating the telecom licences. It’s okay to like this guy but Tatas are considered saints because not many know their history. reply adityaathalye 3 hours agorootparentHave you heard the Radia tapes? I have, in their entirety. They're out there to listen to, if you want. Fascinating piece of the zeitgeist. My listening suggests he lived up to his legacy of above-board, gentlemanly conduct. Compare Radia's phone calls to Ratan, with her conversations with the rest of the lot. Ratan is a master class in, well class and restraint. I think when you're that big and are so hard-wired into the real economy of a country as huge and populous as India, you can't not \"play the game\", so to speak. The question is do you remain human despite every incentive to turn into a self-serving gluttonous tyrant? The TATAs weren't and aren't saints. They are humans we should have more of, in those positions of wealth and power. reply hinkley 12 hours agoparentprevWho does the torch pass to now that he’s gone? reply mlnj 10 hours agorootparentHe was in a chairman emeritus position for many years now. Since Tata, Cyrus Mistry and Natarajan Chandrasekaran have headed the conglomerate. reply perryizgr8 9 hours agorootparentCyrus Mistry died in a car crash a couple of years ago. reply boilerupnc 13 hours agoprevOm Shanti reply rocode2 12 hours agoparentFor those who don’t know, Om shanti means may he rest in peace. Its usually chanted during cremation as Oooooommmmm Shantiii. Om (the same sound as you chant during yoga sometimes) in this context is like a moment of silence Shanti means peace, in this context, eternal peace. reply shipp02 10 hours agoprevJRD Tata and Ratan Tata are among the most honorable men with pure hearts, never affected by avarice or hatred. Tales of their generosity and kind hearted nature will continue to inspire me. They are shining examples of how capitalists can help uplift society. RIP reply alienonwings 11 hours agoprevOm Shanti reply p2hari 11 hours agoprevom shanti. reply ravins 14 hours agoprevOm Shanti reply 4ggr0 5 hours agoprevRIP, as one says. That being said, it always irks me when people worship \"philantropic\" billionaires. I'm sure he's done good things. But there are no ethical billionaires in capitalism. TATA is a giant corp with a lot of influence, I can't believe that they're a 'honest company which doesn't bribe, pays their taxes and are otherwise clean'. Would probably be the first case of an ethical corporation/billionaire :) Certainly cool that he helped stray dogs and people with cancer, but come on people, see through the corpo propaganda. Sounds the same to me as \"[INSERT US BILLIONARE] has donated 5 milion dollars to [CUTE CAUSE], what a hero! (ignore the atrocious things done by their corps and themselves).\" reply atonse 5 hours agoparentSure but you have to grade on a curve. When you actually see how some of their peers act, compared to them, the Tatas are gems. reply 4ggr0 4 hours agorootparentSure, yeah. My reactionary, immature answer to this is, \"someone who has murdered a single person is not that bad when compared to a serial killer\", but i certainly get what you mean :) reply orochimaaru 4 hours agoparentprevHow exactly do you plan to do business in India without the bribes? In fact after a certain scale you need to bribe in the US too. It’s just that it’s legalized in the US as lobbying and PACs. reply sgaur 3 hours agorootparenthttps://youtu.be/NzxVjUF6NQ0?t=1739 - here's Ratan Tata's answer - \"I want to go to bed knowing I didn't do it\". reply orochimaaru 2 hours agorootparentThat point is subjective. Now, this anecdote is from almost 30 years ago and pretty sure things have changed. But when I went to get my India DL you can go directly to the RTO or you go thru a “driving school”. I went thru the school - why? Because they grease the skids to get the DL. Did I bribe? No. Does the school share “profits” - I’m pretty sure they do. So like everything else with the government in India, if you’re using agents to get your work done you’re technically not bribing. But you are. reply braza 1 hour agoparentprevNot Indian and not personally affective. I came from a society quite similar in terms of cultural e socio-economical background with India and maybe there’s a missing point. I got your point and you’re right in terms of worship philanthropic people with a lot of money. However, there’s one aspect that is overlooked when a person that helped and/or serves as reference for a bunch of people that is the “emotional gratitude” or “sense of reference” that those people has in some peoples mind and no matter how much of objective and maybe correct post-mortem criticism will change people’s mind. At the limit, any public figure can, with little scrutiny, be framed as bad. One example from my country: Ayrton Senna (deceased F1 Driver) even being a millionaire, coming from a very privileged background in 80s in a country that used to have at least 40% of its people below the poverty line. Even not doing (during life) objectively nothing for poor people, a lot of folks (me included) consider the guy as a reference, in some cases as a hero. When he died a lot of folks felt that they lost their “hero” and actually he had a state funeral that no other Brazilian will ever have again [1]. My point is that there’s more nuance and layers around that topic when someone with a lot of money dies, and there’s no right or wrong since we’re al humans. [1] - https://youtu.be/j79qnfHODPk?feature=shared reply insiderinsider 14 hours agoprevOne of the best humans of the world reply oguz-ismail 13 hours agoprev [2 more] [flagged] mcint 13 hours agoparent [–] You have a chance to speak eloquently or clearly against his life, his work, or his impacts. I'm curious to hear a contribution to conversation, but I find it easiest to guess a dislike the idea of very wealthy individuals. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [],
    "points": 346,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1728530957
  },
  {
    "id": 41795561,
    "title": "WASM Is the New CGI",
    "originLink": "https://roborooter.com/post/wasm-is-the-new-cgi/",
    "originBody": "May 4, 2022 Wasm is the new CGI #open-source#programming#tech I read a wonderful twitter thead about CGI and the birth of the web and this triggered a thought I've been kicking around. Wasm is the new CGI And to be clear I don't mean the Common Gateway Interface as a protocol. I mean what CGI and the cgi-bin application model brought to the web. They allowed people to easily write code that makes websites interactive. This shifted the web from an archive of documents to a vast network of applications. It was the first \"web application model\". I think Wasm (WebAssembly) is setup to bring the next \"web application model\" to the industry. Every shift in common web application models that I've observed over the last 20 years has been towards one goal; High performance applications that are easier to build and maintain. I almost titled this essay as \"Wasm is the new serverless\" but \"serverless\" is just the current hot iteration of web application models and not the goal. I'm going to give a history of web application models and then I'm going to talk about what Wasm brings to the table. I'm seeing a convergence of Wasm related technologies maturing that I think will change some of the fundamental constraints of web application development today. When you change the constraints in a system you enable things that were impossible before. From CGI to Serverless There was a time (and honestly still today) where you could throw a script or any executable in a folder named cgi-bin and visit it at a url. The web server would execute the program on demand and return the output of the program to your web browser. This is roughly how the dynamic web worked for it's first decade. Make a request, run a process to respond to the request. It was slow by today's standards as starting a new process (and in some cases parsing scripts too) is a lot of work. The official CGI logo from the spec announcement, yes really FastCGI was developed as a response to the performance problems. It was a new web application model where a long lived processes would listen for CGI requests. Your web server would talk to one or more of these processes instead of invoking a new one for every request. It has its own follies as a lot of applications would be naively ported from the previous model and would leak resources like a sieve. They were built to have their processes terminated after each request and now had to stay alive for a long time. This lead to a long period of transition where languages and frameworks adjusted to the new reality. FastCGI During this time language based web servers would emerge as a convention. (Possibly inspired by the Apache Tomcat era of Java application servers.) These applications would be built around a request/response model, and have different web servers available for different execution strategies. They were usually designed to go behind a \"battle tested\" feature rich web server like Apache or nginx that would isolate the application from slow requests and HTTP details. The general application models were centered around process management with regards to requests. Some of these servers would fork the process on every request, some would use OS or language based threads, others would use an event or reactor models. The Rack web server interface from the Ruby community eventually made into python via the Flask application server and the WSGI specification. To simplify the specification, you receive an HTTP method, a headers hashmap, and a string or stream of input bytes. In response you send a status code, a headers object and a string or stream of response bytes. With all these approaches you have to manage the number of physical or virtual servers you run. With physical servers application servers would often get slow when traffic was high, or have lots of computers sitting idle when traffic is low. With the rise of cloud computing, autoscaling could allow a number of application servers to be based upon CPU or memory load or even the time of day. This allowed you to adapt to changes in traffic over time, turning on machines when you needed them and off when you didn't. Scaling up new computers can take 2-20 minutes depending on many factors of your application and configuration. Additionally it's dependant on the resources available in your cloud hosting region. The introduction of \"Serverless compute\" with Amazon Lambda changed the game. Instead of managing servers you now managed \"functions\". When paired with API Gateway you now had a web server that would guarantee a single processes, isolated CPU and isolated memory for every request. Processes might be reused for up to a couple of hours but would be suspended or destroyed when not in use. This approach removes the concept of servers from the application management and allows AWS to scale up and down based upon request volume in seconds. Amazon started the serverless age of compute with Lambda This of course has its tradeoffs. New processes as we know from CGI are expensive, which lead to a \"cold start\" that some requests observe while scaling up concurrent requests. Each platform has different strategies to mitigate this penalty to varying success. Additionally since processes can be suspended after a response maintaining persistent tcp connections between requests can be troublesome. In practice you can tune database or cache server connections to stay alive (high timeouts on the server, low timeouts on the client) but you need to be more tolerant of reconnecting to external services, or simply reconnect for every request. HTTP based database APIs (see Azure or DynamoDB) are popular in the serverless application model as they tolerate massive numbers of connections and are easier to scale up and down with functions than traditional RDBMs solutions. Another tradeoff is the dedicated CPU and memory that you get with each request. Some workloads thrive in this model, you don't have to manage these resources if they're guaranteed. And you might lower costs and mitigate scaling concerns. Other workloads perform horribly in this model as the CPU and memory go to waste as a single process could be leveraged for a considerable number of requests, or possibly the shared memory of a server model allows for batching or caching that make processing significantly more performant. Anecdotally, I've both brought costs down 90% by moving a CMS based web application to a serverless model, and reduced costs 90% moving an event analytics based service to a server based model. There are many variations of \"Severless\" such as Google Cloud Run or Google Cloud Functions which lets you have a single process to take any number of concurrent requests. They model this with a Docker container but the tradeoffs are mostly the same. (Lambda these days now has docker support too but traditionally supported a zip file with an executable or script.) Lastly, the Rack and WSGI specifications heavily influenced the request/response models we see in serverless environments today. With the request and response tuples becoming function or api signatures. Initially most function services did not provide streaming requests or responses (not to mention websockets or Server Side Events) but this has started to change and is worth exploring as application frameworks are starting to demand it. Wasm on the Server With that context, you might ask; Why on earth are we talking about Wasm? Isn't it for the browser? And I really hope even my mention of that question becomes dated, but I still hear this question quite often so it's worth talking about. Wasm was initially developed to run high perfromant code in the web browser. There's a history that traces to asm.js and other attempts to get code to run really fast in the browser. I'll let webassembly.org speak for itself. WebAssembly (abbreviated Wasm) is a binary instruction format for a stack-based virtual machine. Wasm is designed as a portable compilation target for programming languages, enabling deployment on the web for client and server applications. The Wasm stack machine is designed to be encoded in a size- and load-time-efficient binary format. WebAssembly aims to execute at native speed by taking advantage of common hardware capabilities available on a wide range of platforms What we have today is the ability to compile many languages to Wasm instructions that can be run in both the browser and the server. While running CPU intensive processes in the browser (like Doom) is valuable the isolation and security model that the browser demanded is incredible for server side applications. It's now possible to have a significantly lighter weight form of isolation for untrusted code than a VM or docker container. Web Assembly Logo Additionally since V8 based serverless environments (like Node.js, Cloudflare Workers and Deno) are common, we already have some very mature Wasm execution capabilities thanks to the work in the browser. Wasm native environments are few (Fastly, Shopify and Suborbital) I think we'll see many more in the coming years thanks to advances in tooling. If a Wasm module is instructions for a virtual machine, then you need a virtual machine to execute these instructions. This comes in the form of \"runtimes\" that will take generic Wasm, compile it for your local architecture, and provide it an execution environment. Some of these environments look like POSIX APIs that you'd find on any linux system, and some will do nothing but provide specific functions from the \"host\" system and allow you to execute exported functions in the module itself. These runtimes are available via libraries in many languages and run in process (or threads or anywhere you like). Regardless of your runtime, WebAssembly programs are organized into modules and the VM running the module is called a \"host\". Modules are the unit of deployment, loading, and compilation. A module collects definitions for types, functions, tables, memories, and globals. In addition, it can declare imports and exports and provide initialization in the form of data and element segments, or a start function. \"Memories\" are represented as a uninterrupted contiguous array of bytes; which are allocated by the host at instantiation time, giving each guest module memory isolation. They function as the RAM for your virtual machine. You can provide them empty or pre-fill them with data segments. One of the many effect of how modules are isolated is that you can \"pause\" a module, and save its memory as a data segment. A similar concept to a Snapshot of a virtual machine. You can then start as many copies of the paused module as you like. (As I tell friends, it's like saving your game in an emulator.) The snapshotted module has no extra startup time. The leading utility to perform this is called Wizer and describes the process like so; First we instantiate the input Wasm module with Wasmtime and run the initialization function. Then we record the Wasm instance's state: What are the values of its globals? What regions of memory are non-zero? Then we rewrite the Wasm binary by intializing its globals directly to their recorded state, and removing the module's old data segments and replacing them with data segments for each of the non-zero regions of memory we recorded. If we go back to thinking about our Application Server models; this allows us to have a fresh process but without paying the startup costs of a new process. Essentially giving us CGI without the downsides of CGI. Or in more recent terms, serverless without cold starts. This is how Wasm is the new CGI. Tradeoffs of Wasm Like any tech choice, it does have some tradeoffs. Threads are not a native construct, this forces any blocking operation to host methods. This could mean the host handles the bulk of IO operations, providing wrappers for reading and writing to files or network interfaces, pausing the module as convenient or providing callback handlers. It is possible to build a reactor model (eg, tokio, nodejs) or a blocking model for your application. Until the thread proposal lands the constraint of not being able to have threads moves a significant amount of this design to your execution environment. A process with two threads of execution, running on one processor from Wikipedia Just in Time (JIT) compilation is not possible as dynamic Wasm code generation is not allowed for security reasons. In fact, code itself is not addressable at runtime, which required for traditional JIT compilers as they generate code to replace \"hot paths\" of an interpreted script. This means systems like V8 and CRuby (and many other scripting environments) which rely on a JIT compiler for performance aren't able to run in a Wasm VM or have to abandon their JIT. There are alternative approaches that borrow from JIT. For example a \"pre-jit\" build step that outputs an optimized runtime for your script as been proposed. But they are not in wide use yet. Since Wasm runs in a VM there is a simple interface between a module and its host; Memory. As a result moving data between a Wasm module and its host may require a copy. It is possible to share chunks of memory but depending on how the runtime models memory this may or may not be possible or recommended. (Wasmtime in rust for example uses a vector of bytes which may change out from under you.) As far as I can tell with most run times you cannot have zero copy communication between a module and IO operations. This means streaming data into and out of a Wasm module may be slower than doing it on the host. Wasm VMs however do provide a much higher level of control. Some runtimes can enforce CPU limits by counting CPU instructions (see the Wasmtime fuel concept) which is really cool. All of the VMs are able to limit memory and wall clock time. So if you're looking to control usage limits, it's rather trivial to do. Upcoming features to Wasm such as Interface Types (usable today but not a ratified standard) and module linking (functioning prototypes but no standard approach) help reduce module size and improve IO speed and ergonomics. But they are not standard yet. Wizer currently can interfere with module linking, and while there are custom approaches to solving that problem, there isn't a clear winner. Interface Types provide langue agnostic objects that can move through the Wasm memory boundary without expensive encoding and decoding, today a common approach is to copy JSON into and out of memory. I've hand waved over security but by default Wasm modules only have access to what they're given. It's generally safe to run untrusted Wasm code and the surface area of the VM is quite small. This is not the case with Docker or other isolation models. Timing attacks are possible since the VM will be running translated instructions on the host's hardware, but there are mitigations for such attacks. It is also possible to compile Wasm to a native binary, the surface for attack there is much higher, but is safer than running untrusted native code. (I'm not up on the tradeoffs of this approach.) The Future Already we're seeing the rise of Wasm execution environments. As they (and their dev tools) get more popular, they will drive scripting languages to have Wasm runtimes and \"Wizer like\" preboots. In theory your application could be faster to run in snapshotted copy of your app that resumes on each request than with other models available today. Even on our own computers we could in theory take a CLI written in ruby and ship it in a snapshotted Wasm module that links to a Wasm ruby runtime and have it startup nearly as fast as c++ utility and ensure it only ever operates within the confines of a project directory. Right now I'm seeing enhancements to our existing models and new platforms that are experimenting with what's possible. If we go way back this was your application server. The Jacquard machine (from Wikipedia) The first major enhancement I'm observing is moving \"functions\" to the \"edge\" allowing for compute near your users instead of near your database. Edge functions are a new primitive that application frameworks can take advantage of. (Vercel Edge Functions being the one I'm working on - which is v8 based but shares many principals.) For example next-auth can control access to pre rendered pages based upon JWT login tokens. You're able to get dynamic personalized content that is composed of data cached at the CDN. Another enhancement to existing models (and maybe a natural experiment of a new model) is replacing processes based functions with Wasm based functions for serverless applications. Suborbital (which allows you to build your own Wasm based function execution environment) handles function execution but also allows for innovative chaining of Wasm functions into workflows. Most execution platforms seem to encourage a single module per request ignoring the ability to share memory or quickly invoke multiple modules (even from different languages!). As interface types become standard (very soon as they're part of the Wasm 2.0 spec) I expect to see a \"middleware\" data model (similar to Rack?) emerge. It's worth noting an interesting confluence of technologies is that you are able to execute Wasm inside a Lambda function. You're able to use a snapshotted version of your app on existing infrastructure. While it is an interesting mix of technologies that shouldn't be ignored, I imagine it will disappear eventually. I don't know what the next web application model looks like but I think we're at a tipping point. Wasm improves performance, makes process level security much easier, and lowers the cost of building and executing serverless functions. It can run almost any language and with module linking and interface types it lowers the latency between functions incredibly. When you change the constraints in a system you enable things that were impossible before. All this is very exciting to me and I'm quite eager to help us find out where it goes. Roborooter.com © 2024 Powered by ⚡ and 🤖.",
    "commentLink": "https://news.ycombinator.com/item?id=41795561",
    "commentBody": "WASM Is the New CGI (roborooter.com)231 points by burglins 14 hours agohidepastfavorite270 comments tantalor 4 hours ago> Amazon started the serverless age of compute with Lambda Google App Engine (2008) predates Lambda (2014) by 6 years! reply chubot 3 hours agoparentYeah also heroku and the whole generation of “PaaS” I was never quite sure why we got the name “serverless”, or where it came from, since there were many such products a few years before, and they already had a name App engine had both batch workers and web workers too, and Heroku did too They were both pre-docker, and maybe that makes people think they were different? But I think lambda didn’t launch with docker either reply conradev 5 minutes agorootparentServerless, to me, is purely about efficiency. One way to measure that is the time for a \"cold start\" or \"going from a state where you pay no money to one where you pay money\". These gains in efficiency remove the need for over-provisioning and in many cases allow you to pass these savings onto the consumer (if you want to). Heroku is a few seconds: > It only takes a few seconds to start a one-off dyno process or to scale up a web or worker process. Lambda created Firecracker to be snappier: > The duration of a cold start varies from under 100 ms to over 1 second. I think App Engine is in the same ballpark as Lambda (and predated it). Fly.io uses Firecracker too: > While Fly Machine cold starts are extremely fast, it still takes a few hundred milliseconds, so it’s still worth weighing the impact it has on performance. but WASM is yet an order of magnitude faster and cheaper: > Cloudflare Workers has eliminated cold starts entirely, meaning they need zero spin up time. This is the case in every location in Cloudflare's global network. WASM is currently limited in what it can do, but if all you're doing is manipulating and serving HTML, it's fantastic at that. reply randomdata 2 hours agorootparentprev> I was never quite sure why we got the name “serverless”, or where it came from Serverless refers to the software not being a server (usually implied to be a HTTP server), as was the common way to expose a network application throughout the 2010s, instead using some other process-based means to see the application interface with an outside server implementation. Hence server-less. It's not a new idea, of course. Good old CGI is serverless, but CGI defines a specific protocol whereas serverless refers to a broad category of various implementations. reply bloppe 2 hours agorootparentPedantry police here. I would define serverless to mean that all the hardware is completely abstracted away. For instance, on EC2, you have to pick an instance type. You pick how much memory and compute you need. On a managed kuberenetes cluster, you still have to think about nodes. On a serverless platform, though, you have no idea how many computers or what kinds of computers are actually running your code. It just runs when it needs to. Of course there's still an HTTP server somewhere, though. So, you could run a CGI script on a serverless platform, or a \"serverful\" one. You could even run it locally. https://en.wikipedia.org/wiki/Serverless_computing Per wikipedia: \"Serverless is a misnomer in the sense that servers are still used by cloud service providers to execute code for developers. However, developers of serverless applications are not concerned with capacity planning, configuration, management, maintenance, fault tolerance, or scaling of containers, virtual machines, or physical servers.\" reply chubot 1 hour agorootparentFWIW I agree with you -- serverless does not refer to \"web server\", it refers to \"linux server machine\" (whether it's physical or virtual) You don't care about the specific machine, the OS kernel, the distro, the web server, or SSL certificates when you're doing \"serverless\" And the SAME was true of \"PaaS\" This whole subthread just proves that the cloud is a mess -- nobody knows what \"serverless\" is or that App Engine / Heroku already had it in 2008 :) reply randomdata 1 hour agorootparent> it refers to \"linux server machine\" (whether it's physical or virtual) No, \"server\" most definitely refers to software that listens for network requests. Colloquially, hardware that runs such software is often also given the server moniker (\"the computer running the server\" is a mouthful), but that has no applicability within the realm of discussion here. If you put the user in front of that same computer with a keyboard and mouse controlling a GUI application, it would no longer be considered a server. We'd call it something like a desktop. It is the software that drives the terminology. > nobody knows what \"serverless\" is or that App Engine / Heroku already had it in 2008 :) Hell, we were doing serverless in the 90s. You uploaded your CGI script to the provider and everything else was their problem. The difference back then was that everyone used CGI, and FastCGI later on, so we simply called it CGI. If you are old enough to recall, you'll remember many providers popped up advertising \"CGI hosting\". Nowadays it is a mishmash of proprietary technologies, so while technically no different than what we were doing with CGI back in the day, it isn't always built on literal CGI. Hence why serverless was introduced as a more broad term to capture the gamut of similar technologies. reply bloppe 1 hour agorootparentprevI agree the \"serverless\" is not a good name. But hey, it stuck :/ I also can't come up with one that's significantly better. reply randomdata 2 hours agorootparentprevFor all intents and purposes, when is the hardware not fully abstracted away? Even through the 2010s when running as a server was the norm, for the most part you could throw the same code onto basically any hardware without a second thought. But pedantically, serverless is to be taken literally. It implies that there is no server in your application. reply bloppe 2 hours agorootparentEC2 and managed kubernetes are two examples where you still have to think about hardware. reply randomdata 1 hour agorootparentNot really. The application doesn't care. Hell, many of these modern serverless frameworks are built so that they can run both server and serverless from the very same codebase, so it is likely you can take the same code built to run on someone's MacBook running macOS/ARM and run it on an EC2 instance running Linux/amd64 and then take it to a serverless provider on any arbitrary hardware without any code modification at all! I've been around the web since Perl was the de facto way to build web apps, and it has always been an exceptional situation to not have the hardware fully abstracted away. Typically, if it will run on one system, it will run on any system. The move away from CGI/FastCGI/SCGI to the application being the server was a meaningful shift in how web applications were developed. Now that we've started adopting the server back out of the application in favour of the process-based model again, albeit now largely through propriety protocols instead of a standard like CGI, serverless has come into use in recognition of that. We don't want to go back to calling it CGI because CGI is no longer the protocol du jour. reply Uehreka 2 hours agorootparentprevPaaS, Containerization and Serverless are different concepts. App Engine is PaaS: You provide your app to the service in a runnable form (maybe a container image, maybe not) and they spin up a dedicated server (or slice of a server) to run it continuously. Lambda is Serverless: You provide them a bit of code and a condition under which that code should run. They charge you only when that thing happens and the code runs. How they make that happen (deploy it to a bajillion servers? Only deploy it when it’s called?) are implementation details that are abstracted from the user/developer as long as Lambda makes sure that the code runs whenever the condition happens. So with PaaS you have to pay even if you have 0 users, and when you scale up you have to do so by spinning up more “servers” (which may result in servers not being fully utilized). With Serverless you pay for the exact amount of compute you need, and 0 if your app is idle. reply chubot 2 hours agorootparent> They charge you only when that thing happens and the code runs. That's how App Engine worked in 2008, and it looks like it still works that way: https://cloud.google.com/appengine/pricing Apps running in the flexible environment are deployed to virtual machine types that you specify. These virtual machine resources are billed on a per-second basis with a 1 minute minimum usage cost. This applied to both the web workers and the batch workers It was \"serverless\" in 2008! > spin up a dedicated server (or slice of a server) to run it continuously. Absolutely NOT true of App Engine in 2008, and I'm pretty sure Heroku in 2008 too! reply tantalor 2 hours agorootparentprevI recall you could configure app engine with maximum number of instances you wanted, but you definitely weren't charged if usage was 0. They would start the instances as needed. The fact that lambda would automatically scale to meet whatever QPS you got sounds terrifying. reply junto 13 hours agoprevCan someone explain to me what the difference really is between WASM and older tech like Java Applets, ActiveX, Silverlight and Macromedia Flash, because they don’t really sound much different to me. Maybe I’m just old, but I thought we’d learnt our lesson on running untrusted third party compiled code in a web browser. In all of these cases it’s pitched as improving the customer experience but also conveniently pushes the computational cost from server to client. reply vbezhenar 12 hours agoparentJava and Flash failed to deliver its promise of unbreakable sandbox where one could run anything without risking compromising host. They tried, but their implementations were ridden with vulnerabilities and eventually browsers made them unusable. Other mentioned technologies didn't even promise that, I think. JavaScript did deliver its promise of unbreakable sandbox and nowadays browser runs JavaScript, downloaded from any domain without asking user whether he trusts it or not. WASM builds on JavaScript engine, delivering similar security guarantees. So there's no fundamental difference between WASM and JVM bytecode. There's only practical difference: WASM proved to be secure and JVM did not. So now Google Chrome is secure enough for billions of people to safely run evil WASM without compromising their phones, and you can copy this engine from Google Chrome to server and use this strong sandbox to run scripts from various users, which could share resources. An alternative is to use virtualization. So you can either compile your code to WASM blob and run it in the big WASM server, or you can compile your code to amd64 binary, put it along stripped Linux kernel and run this thing in the VM. There's no clear winner here, I think, for now, there are pros and cons for every approach. reply jasode 5 hours agorootparent>So there's no fundamental difference between WASM and JVM bytecode. There's only practical difference: WASM proved to be secure and JVM did not. There's more to it than just the sandbox security model. The JVM bytecode doesn't have pointers which has significant performance ramifications for any language with native pointers. This limitation was one of the reasons why the JVM was never a serious compilation target platform for low-level languages like C/C++. E.g. Adobe compiled their Photoshop C++ code to WASM but not to the JVM to run in a Java JRE nor the Java web applet. Sure, one can twist a Java byte array to act as a flat address space and then \"emulate\" pointers to C/C++ but this extra layer of indirection which reduces performance wasn't something software companies with C/C++ codebases were interested in. Even though the JVM was advertised as \"WORA Write-Once-Run-Anywhere\", commercial software companies never deployed their C/C++ apps to the JVM. In contrast, the motivation for asm.js (predecessor to WASM) was to act as a reasonable and realistic compilation target for C/C++. (https://blog.mozilla.org/luke/2013/03/21/asm-js-in-firefox-n....) So the WASM-vs-JVM story can't be simplified to \"just security\" or \"just politics\". There were actual different technical choices made in the WASM bytecode architecture to enable lower-level languages like C/C++. That's not to say the Sun Java team's technical choices for the JVM bytecode were \"wrong\"; they just used different assumptions for a different world. reply adamc 4 hours agorootparentAlso, the start-up time for the JVM made running applets very sluggish. Java quickly became a synonym for \"slow\". reply kaba0 3 hours agorootparentYou can’t just compare across decades of software and hardware development. Even downloading native binaries would have been sluggish, as the download would have been slow with those download speeds. reply fastball 7 minutes agorootparentIsn't the cold-start for the JVM still relatively slow, even in [current year]? EDIT: seems like yes[1], at least where AWS Lambda is concerned. [1] https://filia-aleks.medium.com/aws-lambda-battle-2021-perfor... reply adamc 2 hours agorootparentprevBut web pages were not so sluggish, hence people chose them over using applets. reply tromp 3 hours agorootparentprevLack of 64-bit ints didn't help either... reply DanielHB 10 hours agorootparentprev> WASM proved to be secure and JVM did not. It is interesting to ask why that is the case, from my point of view the reason is that the JVM standard library is just too damn large. While WASM goes on a lower-level approach of just not having one. To make WASM have the capabilities required the host (the agent running the WASM code) needs to provide them. For a lot of languages that means using WASI, moving most of the security concerns to the WASI implementation used. But if you really want to create a secure environment you can just... not implement all of WASI. So a lambda function host environment can, for example, just not implement any filesystem WASI calls because a lambda has no business implementing filesystem stuff. > An alternative is to use virtualization. So you can either compile your code to WASM blob and run it in the big WASM server, or you can compile your code to amd64 binary, put it along stripped Linux kernel and run this thing in the VM. I think the first approach gives a lot more room for the host to create optimizations, to the point we could see hardware with custom instructions to make WASM faster. Or custom WASM runtimes heavily tied to the hardware they run on to make better JIT code. I imagine a future where WASM is treated like LLVM IR reply kaba0 8 hours agorootparent> I think the first approach gives a lot more room for the host to create optimizations, to the point we could see hardware with custom instructions to make WASM faster Heh, there were literally CPUs with some support for the JVM! But it turns out that “translating” between different forms is not that expensive (and can be done ahead of time and cached), given that CPUs already use a higher level abstraction of x86/arm to “communicate with us”, while they do something else in the form of microcode. So it didn’t really pay off, and I would wager it wouldn’t pay off with WASM either. reply mshockwave 1 hour agorootparent> Heh, there were literally CPUs with some support for the JVM! Jazelle, a dark history that ARM never wants to mention again reply perching_aix 7 hours agorootparentprev> JavaScript did deliver its promise of unbreakable sandbox Aren't its VM implementations routinely exploited? Ranging from \"mere\" security feature exploits, such as popunders, all the way to full on proper VM escapes? Like even in current day, JS is ran interpreted on a number of platforms, because JIT compiling is not trustworthy enough. And I'm pretty sure the interpreters are no immune either. reply esrauch 7 hours agorootparentI think \"routinely\" is overstating it, billions people are running arbitrary JS on a daily basis and no meaningful number of them are being infected by malware. Browser surface attracts the most intense security researcher scrutiny so they do find really wild chains of like 5 exploits that could possibly zero day, but it more reflects just how much scrutiny it has for hardening, realistically anything else will be more exploitable than that, eg your Chromecast playing arbitrarily video streams must he more exploitable than JS on a fully patched Chrome. reply mmis1000 6 hours agorootparentprevBoth chrome and firefox lock down the javascript that site is running into their own box. By using a standalone process and whatever mechanism system provided. A pwned site alone isn't enough to cause damage. You also need to overcome other layer of defenses (unlike something like flash that can be owned from it's script engine alone) It usually require multi 0 day to overcome all those defense and do anything useful. (And it is also the highest glory in defcon) The browser is surely frequently attacked due to the high rewards. But it also get patched really fast. (As long as you are not using a browser from 10 years ago). reply tightbookkeeper 4 hours agorootparentFlash/applets could have been isolated in a process too, right? reply nox101 2 hours agorootparentyes but no, because they needed access to the OS for various services, all of which would have had to be isolated from the user code. Sun and Adobe woiod never have done this. Chrome did it, Safari and Firefox followed. WASM runs in that environment. Flash/applets ran outside of that environment. they did that precisely to provide services the broswer didn't back then. reply mdhb 7 hours agorootparentprevThere were a bunch of things missing from OPs description around the security considerations of Wasm but it has a lot of other stuff on top of what the browser provides when it’s executing JavaScript. The primary one is its idea of a “capability model” where it basically can’t do any kinds of risky actions (I.e touch the outside world via the network or the file system for example) unless you give it explicit permissions to do so. Beyond that it has things like memory isolation etc so even an exploit in one module can’t impact another and each module has its own operating environment and permission scope associated with it. reply emporas 6 hours agorootparentI was surprised when google has agreed to implement the capabilities model for Chrome. I would guess that asking the user for permission to access the microphone would not sit well with google. In smartphones they own the OS so they can ignore wasm's security model as much as they like. reply mdhb 6 hours agorootparentI feel there’s a bit of a disconnect here between Google’s Ads division who are looking to basically do the bare minimum to avoid getting repeatedly spanked primarily by the EU but also now with talk of a breakup in the US and most other parts of Google who I say this entirely unironically are by far the best of all major options with regards to security in both the browser and their public cloud offerings. I’d even extend that possibly to operating systems as well. ChromeOS is miles in front of anything else out there currently but on mobile Android has historically lagged behind iOS although that gap is close to indistinguishable in 2024. reply themoonisachees 5 hours agorootparentIt is not my intention to be contrarian, but honestly this might be the most incorrect comment I've ever read on hacker news, in several different ways. Sure, some of these might be subjective, but for example chromeOS is Linux with a shiny coat in top, how could it be any better than, well, Linux, let alone miles ahead? reply ewoodrich 2 hours agorootparentChromeOS uses the Linux kernel but unless you enable developer mode (which has multiple levels of scary warnings including on every boot and requires completely wiping the device to enable) everything runs in the Chrome web sandbox or the Android VM. A ChromeOS user isn't apt-get installing binaries or copy/pasting bash one liners from Github. If you enable the Linux dev environment, that also runs in an isolated VM with a much more limited attack surface vs say an out of the box Ubuntu install. Both the Android VM and Linux VM can and routinely are blocked by MDM in school or work contexts. You could lock down a Linux install with SELinux policies and various other restrictions but on ChromeOS it's the default mode that 99% of users are protected by (or limited by depending on your perspective). reply mdhb 1 hour agorootparentEven when you enable “developer mode” which is essentially Debian in a VM the level of care that went into making sure that no matter what happens there you will never suffer a full system compromise is truly impressive. To give you a sense of where they were half a decade ago you can already see that it’s as I described miles in front of anything that exists even today in this video: https://youtu.be/pRlh8LX4kQI When we get to talking about when they went for a total ground up first principles approach with Fuchsia as a next generation operating system that is something else entirely on a different level again. I genuinely didn’t have a hint of irony in my original comment. They are actually that much better when it comes to security. reply silvestrov 10 hours agorootparentprevMost of all the problem with Java Applets was that they were very slow to load and required so many resources that the computer came to a halt. They also took much longer to develop than whatever you could cook up in plain html and javascript. reply gnz11 7 hours agorootparentToo be fair, they were slow to load if you didn’t have the browser extension and correct JRE installed. reply kaba0 8 hours agorootparentprevFunnily enough, wasm also has the problem of “slow to load”. In that vein, a higher level bytecode would probably result in smaller files to transport. And before someone adds, the JVM also supports loading stuff in a streaming way - one just has to write a streaming class loader, and then the app can start immediately and later on load additional classes. reply norswap 4 hours agorootparentprev> WASM proved to be secure and JVM did not. This is an oversimplification — there's nothing about the JVM bytecode architecture making it insecure. In fact, it is quite simpler as an architecture than WASM. Applets were just too early (you have to remember what the state of tech looked like back then), and the implementation was of poor quality to boot (owing in part to some technical limitations — but not only). But worst of all, it just felt jank. It wasn't really part of the page, just a little box in it, that had no connection to HTML, the address bar & page history, or really anything else. The Javascript model rightfully proved superior, but there was no way Sun could have achieved it short of building their own browser with native JVM integration. Today that looks easy, just fork Chromium. But back then the landscape was Internet Explorer 6 vs the very marginal Mozilla (and later Mozilla Firefox) and proprietary Opera that occasionally proved incompatible with major websites. reply skybrian 3 hours agorootparentYes it’s true that there’s more to the story, but also, Java really is more complicated and harder to secure than WASM. You need to look at the entire attack surface and not just the bytecode. For example, Java was the first mainstream language with built-in threading and that resulted in a pile of concurrency bugs. Porting Java to a new platform was not easy because it often required fixing threading bugs in the OS. By contrast, JavaScript and WASM (in the first version) are single-threaded. For JavaScript it was because it was written in a week, but for WASM, they knew from experience to put off threading to keep things simple. Java also has a class loader, a security manager that few people understand and sensitive native methods that relied on stack-walking to make sure they weren’t called in the wrong place. The API at the security boundary was not well-designed. A lot of this is from being first at a lot of things and being wildly ambitious without sufficent review, and then having questionable decisions locked in by backward compatibility concerns. reply eduction 2 hours agorootparentprev> back then the landscape was Internet Explorer 6 vs the very marginal Mozilla Your timeline is off by about five years. Java support shipped with Netscape Navigator 2 in 1995, and 95/96/97 is when Java hype and applet experimentation peaked. Netscape dominated this era. IE6 wouldn’t come out until 2001 and IE share generally wouldn’t cross 50% until 2000 https://en.m.wikipedia.org/wiki/File:Internet-explorer-usage... By the time Mozilla spun up with open sourced Netscape code, Java in the browser was very much dead. You nailed the other stuff though. (Kind of an academic point but I’m curious if Java browser/page integration was much worse than JavaScript in those days. Back then JS wasn’t very capable itself and Netscape was clearly willing to work to promote Java, to the point of mutilating and renaming the language that became JavaScript. I’m not sure back then there was even the term or concept of DOM, and certainly no AJAX. It may be a case of JavaScript just evolving a lot more because applets were so jank as to be DOA) reply Dwedit 2 hours agorootparentprev> JavaScript did deliver its promise of unbreakable sandbox I'm sure there's a big long list of WebKit exploits somewhere that will contradict that sentence... reply EasyMark 3 hours agorootparentprevWhat would you say is the performance difference between say running a qt app as native compiled vs running it in WASM? I’ve always been curious but never tried. I know it would vary based on the application but I’m guessing something that is maybe calculating some Monte Carlo model and then displaying the result or something else along those lines that actually will max out the CPU at times rather than be waiting on human interaction 99%of the time. reply BobbyTables2 5 hours agorootparentprevJavaScript is all fun and games until a type confusion bug in V8 allows arbitrary code execution from a simple piece of JavaScript code… reply abound 5 hours agorootparentSure, and if you find one of those, you can trade it in for $25k or more [1] [1] https://bughunters.google.com/about/rules/chrome-friends/574... reply foobarian 5 hours agorootparentprev> There's only practical difference: WASM proved to be secure and JVM did not. The practical reasons have more to do with how the JVM was embedded in browsers than the actual technology itself (though Flash was worse in this regard). They were linked at binary level and had same privileges as the containing process. With the JS VM the browser has a lot more control over I/O since the integration evolved this way from the start. reply kaba0 8 hours agorootparentprevI would add that most of it was politics. The JVM is not fundamentally insecure the same say as neither is any Turing-complete abstraction like an x86 emulator or so. It’s always the attached APIs that open up new attack surfaces. Since the JVM at the time was used to bring absolutely unimaginable features to the otherwise anemic web, it had to be unsafe to be useful. Since then, the web improved a huge amount, like a complete online FPS game can literally be programmed in just JS almost a decade ago. If a new VM can just interact with this newfound JS ecosystem and rely on these to be the boundaries it can of couse be made much safer. But it’s not inherently due to this other VM. reply pdpi 12 hours agoparentprevUnlike ActiveX, Silverlight, or Flash, it's an open standard developed by a whole bunch of industry players, and it has multiple different implementations (where Java sits on that spectrum is perhaps a bit fuzzier). That alone puts it heads and shoulders above any of the alternatives. Unlike the JVM, WASM offers linear memory, and no GC by default, which makes it a much better compilation target for a broader range of languages (most common being C and C++ through Emscripten, and Rust). > Maybe I’m just old, but I thought we’d learnt our lesson on running untrusted third party compiled code in a web browser. WASM is bytecode, and I think most implementations share a lot of their runtime with the host JavaScript engine. > In all of these cases it’s pitched as improving the customer experience but also conveniently pushes the computational cost from server to client. The whole industry has swung from fat clients to thin clients and back since time immemorial. The pendulum will keep swinging after this too. reply DougMerritt 11 hours agorootparent> The whole industry has swung from fat clients to thin clients and back since time immemorial. The pendulum will keep swinging after this too. Indeed, graphics pioneer and all-around-genius Ivan Sutherland observed (and named) this back in 1968: \"wheel of reincarnation \"[coined in a paper by T.H. Myer and I.E. Sutherland On the Design of Display Processors, Comm. ACM, Vol. 11, no. 6, June 1968)] Term used to refer to a well-known effect whereby function in a computing system family is migrated out to special-purpose peripheral hardware for speed, then the peripheral evolves toward more computing power as it does its job, then somebody notices that it is inefficient to support two asymmetrical processors in the architecture and folds the function back into the main CPU, at which point the cycle begins again. \"Several iterations of this cycle have been observed in graphics-processor design, and at least one or two in communications and floating-point processors. Also known as the Wheel of Life, the Wheel of Samsara, and other variations of the basic Hindu/Buddhist theological idea. See also blitter.\" https://www.catb.org/jargon/html/W/wheel-of-reincarnation.ht... reply justanotherjoe 9 hours agorootparentThat was why i stopped using the word 'tech' to refer to these things. You don't suddenly go back to stop using the wheel after a time, or suddenly think that printing press was a bad idea after all. Those are techs. Many of the things we call techs nowadays are just paradigms. And frameworks are defnitely not 'new technology'. reply artikae 8 hours agorootparentAll it takes for something to be replaced is something that does the job better. You can only really apply your definition in hindsight, after something has stood the test of time. You can't tell the difference between sails and wheels until after the rise of the steam engine. reply wolvesechoes 9 hours agorootparentprev> Many of the things we call techs nowadays are just paradigms More like fads sold to milk even more money from people. reply anthk 4 hours agorootparentprevAlso http://www.winestockwebdesign.com/Essays/Eternal_Mainframe.h... reply pjmlp 8 hours agorootparentprevWasmGC is there no matter what, unless we are talking about an incomplete implementation, also plenty of linear memory based bytecodes since 1958. reply pdpi 6 hours agorootparentWasmGC is a feature you can opt in to, rather than a core feature of the platform. It's more of an enabler for languages that expect a GC from their host platform (for things like Dart and Kotlin). Inversely, other forms of bytecode might have linear memory, but the JVM isn't one of those. For the purposes of OP's question, the memory model difference is one of the key reasons why you might want to use wasm instead of a java applet. reply pjmlp 6 hours agorootparentJVM is one bytecode among many since 1958, no need to keep bashing against it as way to champion WASM. Opt-in or not, it is there on the runtime. reply swsieber 4 hours agorootparentIt seems relevant since we are in a thread asking to compare WASM to java applets. reply Laremere 12 hours agoparentprevWasm has a great benefits over those technologies: - Wasm has verification specification that wasm bytecode must comply to. This verified subset makes security exploits seen in those older technologies outright impossible. Attacks based around misbehaving hardware like heartbleed or rowhammer might still be possible, but you, eg, can't reference memory outside of your wasm's memory by tricking the VM to interpret a number you have as a pointer to memory that doesn't belong to you. - Wasm bytecode is trivial (as it gets) to turn into machine code. So implementations can be smaller and faster than using a VM. - Wasm isn't owned by a specific company, and has an open and well written specification anyone can use. - It has been adopted as a web standard, so no browser extensions are required. As for computation on clients versus serves, that's already true for Javascript. More true in fact, since wasm code can be efficient in ways that are impossible for Javascript. reply kgeist 10 hours agorootparentBtw, is WASM really more secure? JVM and .NET basically have capability-based security thanks to their OOP design together with bytecode verification: if you can't take a reference to an object (say, there's a factory method with a check), you can't access that object in any way (a reference is like an access token). As far as I understand, in WASM memory is a linear blob, so if I compile C++ to WASM, isn't it possible to reference a random segment of memory (say, via an unchecked array index exploit) and then do whatever you want with it (exploit other bugs in the original C++ app). The only benefit is that access to the OS is isolated, but all the other exploits are still possible (and impossible in JVM/.NET). Am I missing something? reply adrian17 6 hours agorootparentAFAIK you’re correct. Also see: https://www.usenix.org/conference/usenixsecurity20/presentat... „We find that many classic vulnerabilities which, due to common mitigations, are no longer exploitable in native binaries, are completely exposed in WebAssembly. Moreover, WebAssembly enables unique attacks, such as overwriting supposedly constant data or manipulating the heap using a stack overflow.” My understanding is that people talking about wasm being more secure mostly talk about the ability to escape the sandbox or access unintended APIs, not integrity of the app itself. reply lifthrasiir 9 hours agorootparentprevFor now, (typical) WASM is indeed more secure than (typical) JVM or .NET bytecodes primarily because external operations with WASM are not yet popular. WASM in this regard has the benefit of decades' worth of hindsight that it can carve its own safe API for interoperation, but otherwise not technically superior or inferior. Given that the current web browser somehow continues to ship and keep such APIs, I think the future WASM with such APIs is also likely to remain safer, but that's by no means guaranteed. reply igrunert 4 hours agorootparentprevWhen discussing security it's important to keep in mind the threat model. We're mostly concerned with being able to visit a malicious site, and execute wasm from that site without that wasm being able to execute arbitrary code on the host - breaking out of the sandbox in order to execute malware. You say the only benefit is that access to the OS is isolated, but that's the big benefit. Having said that, WebAssembly has some design decisions that make your exploits significantly more difficult in practice. The call stack is a separate stack from WebAssembly memory that's effectively invisible to the running WebAssembly program, so return oriented programming exploits should be impossible. Also WebAssembly executable bytecode is separate from WebAssembly memory, making it impossible to inject bytecode via a buffer overflow + execute it. If you want to generate WebAssembly code at runtime, link it in as a new function, and execute it, you need participation from the host, e.g. https://wingolog.org/archives/2022/08/18/just-in-time-code-g... reply ndiddy 3 hours agorootparentThe downside of WASM programs not being able to see the call stack is that it makes it impossible to port software that uses stackful coroutines/fibers/whatever you want to call them to WASM, since that functionality works by switching stacks within the same thread. reply nox101 2 hours agorootparentprevyes you're missing something. Java applets and flash outside of any security and they ran the users code in that insecure environment WASM, in broswers, runs entirely inside a secure environment with no access to the system. js->browser->os+--Flash/java-->os vs wasm->browser->os further. WASM and Js are in their own process with no os acesss. they can't access the os except by rpc to the broswer flash/java tho, ran all user code in the same process with full access to the os reply kgeist 11 hours agorootparentprev>Wasm has verification specification. This verified subset makes security exploits seen in those older technologies outright impossible Both Java and .NET verify their bytecode. >Wasm bytecode is trivial (as it gets) to turn into machine code JVM and .NET bytecodes aren't supercomplicated either. Probably the only real differences are: 1) WASM was designed to be more modular and slimmer from the start, while Java and .NET were designed to be fat; currently there are modularization efforts, but it's too late 2) WASM is an open standard from the start and so browser vendors implement it without plugins Other than that, it feels like WASM is a reinvention of what already existed before. reply flohofwoe 10 hours agorootparentAFAIK the big new thing in WASM is that it enforces 'structured control flow' - so it's a bit more like a high level AST than an assembly-style virtual ISA. Not sure how much of that matters in practice, but AFAIK that was the one important feature that enabled the proper validation of WASM bytecode. reply iainmerrick 10 hours agorootparentI don't think there's any significant advance in the bytecode beyond e.g. JVM bytecode. The difference is in the surface area of the standard library -- Java applets exposed a lot of stuff that turned out to have a lot of security holes, and it was basically impossible to guarantee there weren't further holes. In WASM, the linear memory and very simple OS interface makes the sandboxing much more tractable. reply titzer 8 hours agorootparentI worked on JVM bytecode for a significant number of years before working on Wasm. JVM bytecode verification is non-trivial, not only to specify, but to implement efficiently. In Java 6 the class file format introduced stack maps to tame a worst-case O(n^3) bytecode verification overhead, which had become a DoS attack vector. Structured control flow makes Wasm validation effectively linear and vastly simpler to understand and vet. Wasm cleaned up a number of JVM bytecode issues, such as massive redundancy between class files (duplicate constant pool entries), length limitations (Wasm uses LEBs everywhere), typing of locals, more arithmetic instructions, with signedness and floating point that closer matches hardware, addition of SIMD, explicit tail calls, and now first-class functions and a lower-level object model. reply jeberle 3 hours agorootparentThx for this perspective and info. Regarding \"signedness and floating point that closer matches hardware\", I'm not seeing unsigned integers. Are they supported? I see only: > Two’s complement signed integers in 32 bits and optionally 64 bits. https://webassembly.org/docs/portability/#assumptions-for-ef... And nothing suggesting unsigned ints here: https://webassembly.org/features/ reply flohofwoe 1 hour agorootparentSigned and unsigned are just different views on the same bits. CPU registers don't carry signedness either after all, the value they carry is neither signed nor unsigned until you look at the bits and decide to \"view\" them as a signed or unsigned number. With the two's complement convention, the concept of 'signedness' only matters when a narrow integer value needs to be extended to a wider value (e.g. 8-bit to 16-bit), specifically whether the new bits needs to be replicated from the narrow value's topmost bit (for signed extension) or set to zero (for unsigned extension). It would be interesting to speculate what a high level language would look like with such sign-agnostic \"Schroedinger's integer types\"). reply jeberle 1 hour agorootparentCPU instruction sets do account for signed vs unsigned integers. SHR vs SAR for example. It's part of the ISAs. I'm calling this out as AFAIK, the JVM has no support for unsigned ints and so that in turn makes WASM a little more compelling. https://en.wikibooks.org/wiki/X86_Assembly/Shift_and_Rotate reply Laremere 3 hours agorootparentprevWasm makes no distinction between signed and unsigned integers as variables, only calling them integers. The relevant operations are split between signed and unsigned. https://webassembly.github.io/spec/core/appendix/index-instr... See how there's only i32.load and i32.eq, but there's i32.lt_u and i32.lt_s. Loading bits from memory or comparing them is the same operation bit for bit for each of signed and unsigned. However, less than requires knowing the desired signess, and is split between signed and unsigned. reply kaba0 7 hours agorootparentprevAre they validating code to the same degree though? Like, there are obviously learned lessons in how WASM is designed, but at the same time JVM byte code being at a slightly higher level of abstraction can outright make certain incorrect code impossible to express, so it may not be apples to oranges. What I’m thinking of is simply memory corruption issues from the linear memory model, and while these can only corrupt the given process, not anything outside, it is still not something the JVM allows. reply titzer 3 hours agorootparentWasm bytecode verification is more strict than JVM bytecode verification. For example, JVM locals don't have declared types, they are inferred by the abstract interpretation algorithm (one of the reasons for the afore-mentioned O(n^3) worst case). In Wasm bytecode, all locals have declared types. Wasm GC also introduces non-null reference types, and the validation algorithm guarantees that locals of declared non-null type cannot be used before being initialized. That's also done as part of the single-pass verification. Wasm GC has a lower-level object model and type system than the JVM (basically structs, arrays, and first-class functions, to which object models are lowered), so it's possible that a higher-level type system, when lowered to Wasm GC, may not be enforceable at the bytecode level. So you could, e.g. screw up the virtual dispatch sequence of a Java method call and end up with a Wasm runtime type error. reply bloppe 1 hour agoparentprevWebAssembly has a few things that set it apart: - The security model (touched on by other comments in this thread) - The Component Model. This is probably the hardest part to wrap your head around, but it's pretty huge. It's based on a generalization of \"libraries\" (which export things to be consumed) to \"worlds\" (which can both export and import things from a \"host\"). Component modules are like a rich wrapper around the simpler core modules. Having this 2-layer architecture allows far more compilers to target WebAssembly (because core modules are more general than JVM classes), while also allowing modules compiled from different ecosystems to interoperate in sophisticated ways. It's deceivingly powerful yet also sounds deceivingly unimpressive at the same time. - It's a W3C standard with a lot of browser buy-in. - Some people really like the text format, because they think it makes Wasm modules \"readable\". I'm not sold on that part. - Performance and the ISA design are much more advanced than JVM. reply tptacek 12 hours agoparentprevJava Applets and ActiveX had less-mediated (Applets, somewhat; ActiveX, not at all) access to the underlying OS. The \"outer platform\" of WASM is approximately the Javascript runtime; the \"outer platform\" of Applets is execve(2). reply BiteCode_dev 4 hours agoparentprevWASM is a child of the browser community and built on top of existing infra. Java was an outsider trying to get in. The difference is not in the nature of things, but rather who championed it. reply dspillett 4 hours agoparentprev> Can someone explain to me what the difference really is between WASM and older tech like Java Applets, ActiveX, Silverlight and Macromedia Flash As well as the security model differences other are debating, and WASM being an open standard that is easy to implement and under no control from a commercial entity, there is a significant difference in scope. WebAssemply is just the runtime that executes byte-code compiled code efficiently. That's it. No large standard run-time (compile in everything you need), no UI manipulation (message passing to JS is how you affect the DOM, and how you ready DOM status back), etc. It odes one thing (crunch numbers, essentially) and does it well. reply tsimionescu 12 hours agoparentprevPushing compute to the client is the whole point, and is often a major improvement for the end user, especially in the era in which phones are faster than the supercomputers of the 90s. And otherwise, WASM is different in two ways. For one, browsers have gotten pretty good at running untrusted 3rd party code safely, which Flash or the JVM or IE or.NET were never even slightly adequate for. The other difference is that WASM is designed to allow you to take a program in any language and run it in the user's browser. The techs you mention were all available for a single language, so if you already had a program in, say, Python, you'd have to re-write it in Java or C#, or maybe Scala or F#, to run it as an applet or Silverlight program. reply pjmlp 6 hours agorootparentCLR means Common Language Runtime for a reason. From 2001, \"More than 20 programming tools vendors offer some 26 programming languages — including C++, Perl, Python, Java, COBOL, RPG and Haskell — on .NET.\" https://news.microsoft.com/2001/10/22/massive-industry-and-d... reply tsimionescu 5 hours agorootparentIt's not the same thing though. All of these languages have specific constructs for integrating with the CLR, the CLR is not just a compilation target like WASM is. C++/CLR even has a fourth kind of variable compared to base C++ (^, managed references of a type, in addition to the base type, * pointers to the type, and & references to the type). IronPython has not had a GIL since its early days. I'm sure the others have significant differences, but I am less aware of them. reply pjmlp 3 hours agorootparentAs if WebAssembly doesn't impose similar restrictions, with specific kinds of toolchains, and now the whole components mess. This WebAssembly marketing is incredible. reply pajamaboin 12 hours agoparentprevThis article is about WASM on the server so to answer your question it's different because it's not pushing computational cost from the server to the client. It can, but it doesn't in all cases. That's a huge difference. Others have already commented others (better sandboxing, isolation, etc) reply ranger_danger 1 hour agorootparentIt's amazing how many people don't actually read the article and just start commenting right away. It's like leaving bad amazon reviews for products you haven't purchased. reply flohofwoe 12 hours agoparentprev> untrusted third party compiled code in a web browser. WASM makes that safe, and that's the whole point. It doesn't increase the attack surface by much compared to running Javascript code in the browser, while the alternative solutions where directly poking through into the operating system and bypassing any security infrastructure of the browser for running untrusted code. reply palmfacehn 12 hours agoparentprevThere have also been exploits of Chrome's JS sandbox. For me the greatest difference is that WASM is supported by the browser itself. There isn't the same conflict of interest between OS vendors and 3rd party runtime providers. reply afavour 5 hours agoparentprevThe big conceptual difference is that Flash, ActiveX etc allowed code to reach outside of the browser sandbox. WASM remains _inside_ the browser sandbox. Also no corporate overlord control. reply SkiFire13 12 hours agoparentprevThe replacement for those technologies is arguably javascript. WASM is more focused on performance by providing less abstractions and an instruction set closer to assembly (hence the name). The issue with those older technologies was that the runtime itself was a third-party external plugin you had to trust, and they often had various security issues. WASM however is an open standard, so browser manifacturers can directly implement it in browser engines without trusting other third-parties. It is also much more restricted in scope (less abstractions mean less work to optimize them!) which helps reducing the attack surface. reply 0x457 56 minutes agorootparent> The replacement for those technologies is arguably javascript. WASM is more focused on performance by providing less abstractions and an instruction set closer to assembly (hence the name). That is nonsense. WASM and JS have the exact same performance boundaries in a browser because the same VM runs them. However, WASM allows you to use languages where it's easier to stay on a \"fast-path\". reply sebastianconcpt 6 hours agoparentprevFor starters, in that it gives you memory safe bytecodes computation that aren't coupled with one specific language. reply freetonik 13 hours agoparentprevNot an answer, but I think it’s unfair to group Flash with the others because it was both the editor/compiler and the player were proprietary. I guess same applies to Silverlight at least. reply Kwpolska 12 hours agorootparentThe ActiveX \"player\" (Internet Explorer) was also proprietary. And I'm not sure if you could get away without proprietary Microsoft tools to develop for it. reply mike_hearn 7 hours agoparentprevConceptually, they aren't that different. The details do matter though. WASM on its own isn't anything special security-wise. You could modify Java to be as secure or actually more secure just by stripping out features, as the JVM is blocking some kinds of 'internal' security attacks that WASM only has mitigations for. There have been many sandbox escapes for WASM and will be more, for example this very trivial sandbox escape in Chrome: https://microsoftedge.github.io/edgevr/posts/Escaping-the-sa... ... is somewhat reminiscent of sandbox escapes that were seen in Java and Flash. But! There are some differences: 1. WASM / JS are minimalist and features get added slowly, only after the browser makers have done a lot of effort on sandboxing. The old assumption that operating system code was secure is mostly no longer held whereas in the Flash/applets/pre-Chrome era, it was. Stuff like the Speech XML exploit is fairly rare, whereas for other attempts they added a lot of features very fast and so there was more surface area for attacks. 2. There is the outer kernel sandbox if the inner sandbox fails. Java/Flash didn't have this option because Windows 9x didn't support kernel sandboxing, even Win2K/XP barely supported it. 3. WASM / JS doesn't assume any kind of code signing, it's pure sandbox all the way. reply Starlevel004 10 hours agoparentprevYou can't easily decompile WASM so it makes it harder to block inline ads. reply afiori 8 hours agorootparentYou can alreay compile javascript into https://jsfuck.com/ and you could also very easily recompile the wasm into js. Obsuscation and transpilation are not new in jsland reply IshKebab 12 hours agoparentprevActiveX wasn't sandboxed so it was a security joke. Flash and Silverlight were full custom runtimes that a) only worked with a specific language, and b) didn't integrate well with the existing web platform. WASM fixes all of that. reply tightbookkeeper 4 hours agorootparentBut that’s missing a few steps. First they banned all those technologies saying JavaScript was sufficient, then only later made wasm. There never was a wasm vs applet debate. reply layer8 3 hours agoprevTo expand the premise in the title, to be a true heir to that lineage, I would say that WASM needs to be as easy to host and deploy as PHP applications are (or used to be) on the LAMP stack of any random hosting provider. I suspect that’s not quite the case yet? reply thomastjeffery 3 hours agoparentWASM runs on the browser.. What about hosting do you expect to be different? reply tmpz22 3 hours agorootparentA more accessible toolchain for complete beginners. PHP was literally copy/past code snippets into a file and then upload it to a hosting provider. I don't build for WASM but I'll bet the money in my pocket to a charity of your choice that its harder for a beginner. reply layer8 3 hours agorootparentprevThe article is about WASM on the server, hence the analogy to CGI(-bin) in the title. reply thomastjeffery 2 hours agorootparentI see. My fault for not moving from \"From CGI to Serverless\" to \"Wasm on the Server\". reply fmajid 3 hours agorootparentprevLike Java and JavaScript before it, WASM can also run on Kubernetes clusters and plenty of other non-browser contexts. reply torginus 12 hours agoprevJust in Time (JIT) compilation is not possible as dynamic Wasm code generation is not allowed for security reasons. This sounds.. not right. Honestly,this is an essential feature for allowing workloads like hot reloading code cleanly. I'm quite convinced the alleged security argument is bull. You can hot reload JS (or even do wilder things like codegen) at runtime without compromising security. Additionally, you can emulate codegen or hot reload, by dynamically reloading the entire Wasm runtime and preserving the memory, but the user experience will be clunky. I don't see any technical reason why this couldn't be possible. If this were a security measure, it could be trivially bypassed. Also, WASM bytecode is very similar conceptually to .NET IL, Java bytecode etc., things designed for JIT compilation. I kind of dislike WASM. It's a project lacking strong direction and will to succeed in a timely manner. First, the whole idea is conceptually unclear, its name suggests that it's supposed to be 'assembly for the web', a machine language for a virtual CPU, but it's actually an intermediate representation meant for compiler backends, with high-level features planned such as GC support. It's still missing basic features, like the aforementioned hot reload, non-hacking threading, native interfacing with the DOM (without Javascript ideally), low-overhed graphics/compute API support, low-level audio access etc. You can't run a big multimedia app without major compromises in it. reply bhelx 5 hours agoparentThe statement is correct. Wasm cannot mark memory as executable. It's effectively a Harvard Architecture. The code and memory are split. Furthermore you cannot jump to arbitrary points in code. There isn't even a jump instruction. > I'm quite convinced the alleged security argument is bull. You can hot reload JS (or even do wilder things like codegen) at runtime without compromising security. JIT here is referring to compiling native code at runtime and executing it. This would be a huge security compromise in the browser or in a wasm sandbox. > I don't see any technical reason why this couldn't be possible. If this were a security measure, it could be trivially bypassed. It's not because it's baked into the design and instruction set. You can read some more about how it works here: https://webassembly.org/docs/security/ > Also, WASM bytecode is very similar conceptually to .NET IL, Java bytecode etc., things designed for JIT compilation. Yes, and like with Wasm, the engine is responsible for JITting. But giving the user the power to escape the runtime and emit native code and jump to it is dangerous. reply tombl 3 hours agorootparentwasm has no way to remap writable memory as executable, but you can absolutely call back into javascript to instantiate and link a new executable module, like https://github.com/remko/waforth does. reply bhelx 1 hour agorootparentYes, I understand that you can do anything with imports. But that's not part of the Wasm spec. That's a capability the host has decided to give the module. Of course the person with the most privilege can always open holes up, but that capability is not there by default. reply flohofwoe 11 hours agoparentprev> Just in Time (JIT) compilation is not possible as dynamic Wasm code generation is not allowed for security reasons. Browsers definitely use a form of JIT-ing for WASM (which is a bit unfortunate, because just as with JITs, you might see slight 'warmup stutter' when running WASM code for the first time - although this has gotten a lot better over the years). ...also I'm pretty sure you can dynamically create a WASM blob in the browser and then dynamically instantiate and run that - not sure if that's possible in other WASM runtimes though, and even in the browser you'll have to reach out Javascript, but that's needed for accessing any sort of 'web API'. reply torginus 11 hours agorootparent>Browsers definitely use a form of JIT-ing for WASM I (and the article) wasn't referring to this kind of JIT. I was referring to the ability to dynamically create or modify methods or load libraries while the app is running (like `DynamicMethod` in .NET). Afaik WASM even in the browser does not allow modifying the blob after instantiation. The thing you are referring to puzzles me as well. I initially thought that WASM would be analogous to x86 or ARM asm and would be just another architecture emitted by the compiler. Running it in the browser would just involve a quick translation pass to the native architecture (with usually 1-to-1 mapping to machine instructions) and some quick check to see that it doesn't do anything naughty. Instead it's an LLVM IR analog that needs to be fed into a full-fledged compiler backend. I'm sure there are good technical reasons as to why it was designed like this, but as you mentioned, it comes with tangible costc like startup time and runtime complexity. reply flohofwoe 10 hours agorootparent> Afaik WASM even in the browser does not allow modifying the blob after instantiation. ...not your own WASM blob, but you can build a new WASM blob and run that. > The thing you are referring to puzzles me as well... Yes, compilers emit WASM, but that WASM is just a bytecode (similar to JVM or .NET bytecode but even higher level because WASM enforces 'structured control flow') and needs to be compiled to actual machine code on the client before it can run, and this isn't a simple AOT compilation - in browsers at least (it used to be for a while in Firefox, but that caused issues for large projects like Unity games, which might take dozens of seconds to AOT compile). AFAIK all browsers now use a tiered approach. The WASM-to-machine-code compilation doesn't happen on the whole WASM blob at once, but function by function. For the first time a WASM function is called, a fast compilation will happen which may have slow runtime performance, from then on, 'hot functions' will be compiled with a higher tier backend which does additional optimization, is slow to compile but has better runtime performance - and AFAIK this is also quite similar to how Javascript JIT-ing works. Also from what I understand WASM compilation is more complex than just translating bytecode instructions to native instructions. It's more like compiling an AST into machine code - at least if you want any performance out of it. The only difference to JS might be that WASM functions are never 'de-optimized'. reply torginus 7 hours agorootparentI feel like I need to be a bit more frank > WASM is just a bytecode (similar to JVM or .NET bytecode but even higher level ... Yes, and I think this was a poor engineering choice on behalf of WASM engineering team, instead of using something much closer to actual assembly. And we are grappling with long startup times and lots of compiler infra pushed into the client because of that. > ...not your own WASM blob, but you can build a new WASM blob and run that. another baffling limitation, considering you can modify your C#, Java or even native code at runtime. Unless they are working around some constraint unknown to me, in which case I'd love to know about what it is, they made bad technical decisions in the design. reply flohofwoe 6 hours agorootparent> they made bad technical decisions in the design Considering that the most important design requirement was to have a security model that's good enough for running untrusted code in web browsers at near native performance, I think the WASM peeps did a pretty good job. Your requirements may be different, but then maybe WASM simply isn't the right solution for you (there are plenty of alternatives outside web browsers after all). reply torginus 6 hours agorootparentPNacl also had the same sandboxing requirement, yet had many of the features still missing today from WAsm (threads, 3d graphics API support, access to other native APIs), and it didn't suffer from slow startup times. It had pretty nice and quick uptake considering the tooling was very similar to native toolchains. According to this benchmark (first Google result I found), it was even faster: https://apryse.com/blog/wasm/wasm-vs-pnacl While it might not have been perfect, WASM is yet to catch up in many ways, and some of its limitations might come from its design. reply flohofwoe 6 hours agorootparentI had been working both with NaCl and PNaCl back then, and truth be told, once Google made the switch from NaCl to PNaCl most advantages just disappeared. The compilation of the PNaCl bytecode on start (which was more or less just a subset of LLVM IR) took longer than even the first WASM implementations. PNaCl definitely suffered hard from slow startup times because it ran LLVM for compilation from PNaCl bytecode to native code on startup, and LLVM is slow (I even noticed this compilation process on startup on my absolutely trivial test code). Only the predecessor NaCl didn't suffer from this problem. There was no 'access to other native APIs', PNaCl created its own set of wrapper APIs to access browser features, and while some of those were better than their standardized web API counterparts, some NaCl/PNaCl APIs were worse than the web APIs they replaced - and for the future, PNaCl would have to create more non-standard APIs for every little feature available in browsers, because: Integration with the webpage and Javascript was done via message passing, which was just terrible when compared to how easy and fast it is to call between WASM and JS. The NaCl/PNaCl multithreading feature would have been hit just as hard by Spectre/Meltdown as the SharedArrayBuffer based threading in WASM. Finally, when you look at the PNaCl toolchain versus Emscripten, Emscripten definitely comes out on top because Emscripten was much more concerned about integrating well with existing build systems and simplify porting of existing code, while NaCl/PNaCl had its own weird build system (in old Google NIH tradition). Working with NaCl/PNaCl felt more like working with the Android NDK, which is pretty much the worst developer experience in the world. reply titzer 1 hour agorootparentIt's also worth noting that the NaCl and PNaCl teams were integrated into a large Wasm team at Google and brought their expertise to the project. While we didn't all 100% agree on every decision made in Wasm design, we were intimately familiar with the tradeoffs made by those prior projects. Ultimately the sandboxing requirement of running in-process with the renderer process and integrating with Web APIs like JS dictated hard requirements for security. reply jillesvangurp 12 hours agoprevWASM replaces a language specific vm (javascript) with a general purpose one anywhere javascript vms are currently used. But not exclusively just there. General purpose here means it can run just about anything with a compiler or interpreter for it. Including javascript. So anything, anywhere. Since it is generally implemented as part of the javascript engine, it inherits a lot of stuff that comes with it like sandboxing and access to the APIs that come with it. Standardizing access to that is a bit of an ongoing process but the end state here is that anything that currently can only be done in Javascript will also be possible in WASM. And a lot more that is currently hard or impossible in Javascript. And it all might run a little faster/smoother. That makes WASM many things. But the main thing it does is remove a lot of restrictions we've had on environments where Javascript is currently popular. Javascript is a bit of a divisive language. Some people love it, some people hate it. It goes from being the only game in town to being one of many things you can pick to do a thing. It's been styled as a Javascript replacement, as a docker replacement, as a Java replacement, a CGI replacement (this article), etc. The short version of it is that it is all of these things. And more. reply marcyb5st 11 hours agoparentWhile I don't have a problem with Javascript, I have a problem with the ecosystem around publishing JS for the web. There are so many tools that do more or less the same thing and whose boundaries are unclear. Additionally, when you eventually manage to get everything working it feels brittle (IMHO). For someone that doesn't do that professionally, it is daunting. Nowadays, the few times I need to build something for the web I use leptos which has a much nicer DX and even if it didn't reach 1.x yet, it feels more stable that chaining like 5 tools to transpile, uglify, minify, pack, ... your JS bundle. reply fallous 12 hours agoprevThis article really does remind me of an old Law of Software that we used to invoke: Any sufficiently large and long-lived application will eventually re-implement the entire software stack it runs on, including the operating system.. and it will re-implement it poorly. I'm unsure of the source for this Law, but it certainly proves correct more often than not. reply PoignardAzur 8 hours agoparentThe witty version is known as Greenspun's tenth rule: \"Any sufficiently complicated C or Fortran program contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of Common Lisp.\" The general pattern is called the Inner-Platform Effect. reply fallous 4 hours agorootparentYES! The Inner-Platform Effect is exactly what I was trying to dig up through my fossilized neurons. Thank you. reply anthk 4 hours agorootparentprevAnd a complete TCL spec. reply anonu 1 hour agoprevI like the thought. I also think about how Python losing the GIL. If we can write Python to WASM and maintain multi-threading, then the browser is sort of the new \"Java JRE\"... (to expand on the analogies) reply cheema33 13 hours agoprevI have a different take on this. I think local-first is the future. This is where the apps runs mostly within user's browser with little to no help from the server. Apps like Figma, Linear and Superhuman use this model very successfully. And to some degree Stackblitz does as well. If somewhat complex apps like Figma can run almost entirely within user's browser, then I think vast majority of the apps out there can. Server side mostly is there to sync data between different instances of the app if the user uses it from different locations. The tooling for this is in the works, but is not yet mature. e.g Electric-SQL. Once these libraries are mature, I think this space will take off. Serverless is mostly there to make money for Amazon and Azures of the world and will eventually go the way of the CGI. WASM could succeed as well. But mostly in user's browser. Microsoft uses it today for C#/Blazor. But it isn't the correct approach as dotnet in browser will likely never be as fast as Javascript in the browser. reply llm_trw 12 hours agoparent>Serverless is mostly there to make money for Amazon and Azures of the world and will eventually go the way of the CGI. CGI empowers users and small sites. No one talks about it because you can't scale to a trillion add impressions a second on it. Serverless functions add 10 feet to Bazoz's yacht every time someone writes one. reply mattdesl 12 hours agoparentprevI’m not sure I’d call Figma local first. If I’m offline or in a spotty wifi area, I can’t load my designs. And unless it’s recently changed, if you lose wifi and quit the browser after some edits, they won’t be saved. reply pen2l 7 hours agorootparentA better example than Figma is Rive, made with Flutter. Works well local-first, and syncs with the cloud as needed. Flutter space lends itself very well to making local-first apps that also play well in the cloud. reply curtisblaine 12 hours agorootparentprevThat's intentional: they need you and your data tied to the server to make money. But there's no reason why it couldn't be local first (except the business model), since the bulk of execution is local. Incidentally, I think that's why local-first didn't take off yet: it's difficult to monetize and it's almost impossible to monetize to the extent of server-based or server-less. If your application code is completely local, software producers are back to copy-protection schemes. If your data is completely local, you can migrate it to another app easily, which is good for the user but bad for the companies. It would be great to have more smaller companies embracing local-first instead of tech behemoths monopolizing resources, but I don't see an easy transition to that state of things. reply llm_trw 12 hours agorootparent>Incidentally, I think that's why local-first didn't take off yet Local first is what we had all throughout the 80s to 10s. It's just that you can make a lot more from people who rent your software rather than buy it. reply baq 10 hours agorootparentThe sweet, sweet ARR. Investors love it, banks love it, employees should also love it since it makes their paychecks predictable. It sucks for customers, though. reply OtomotO 12 hours agorootparentprevMore and more reliably. When people have an abo that cannot be quit every month it gives more financial security to the company. Previously people would buy e.g. the creative suite from Adobe and then work with that version for many, many years to come reply curtisblaine 11 hours agorootparentPreviously people would crack CS from Adobe then work with that version for many, many years to come :) reply llm_trw 9 hours agorootparentPreviously amateurs would crack Adobe software and then get a letter telling them they needed to pay or be sued when they went professional. The cracked software was there to onramp teens into users. Adobe has burned this ramp and now no one under 14 uses it any more which is quite the change from when I was 14. reply actionfromafar 11 hours agorootparentprevTrue but do all those peeople now pay $100 a month to Adobe? Hardly. reply auggierose 10 hours agorootparentIf they need what Adobe offers, yes. reply torginus 12 hours agoparentprevHehehe, so the future is how we used to run applications from before the era of the web. reply flohofwoe 12 hours agorootparentExcept with runtime safety, no installation process, no pointless scare popups when trying to run an app directly downloaded from the internet, and trivial distribution without random app store publishing rules getting in the way. In a way - yes - it's almost like it was before the internet, but mostly because other ways to distribute and run applications have become such a hassle, partly for security reasons, but mostly for gatekeeping reasons by the \"platform owners\". reply torginus 11 hours agorootparentApps like these were incredibly common on Windows from the late 90s-early 2010s era. They could do all this (except for the sandboxing thing). You just downloaded a single .exe file, and it ran self-contained, with all its dependencies statically linked, and it would work on practically any system. On MacOS, the user facing model is still that you download an application, drop it in the Applications folder, and it works. reply afiori 8 hours agorootparent> They could do all this (except for the sandboxing thing). The sandbox is very very important, it is the reason I mostly do not worry about clicking random links or pasting random urls in a browser. There are many apps that I would have liked to try if not for the security risk. reply d3VwsX 8 hours agorootparentThe download of a single EXE to keep had a nice side-effect though, that it made it trivial to store (most) apps (or their installers) for future use. Not so sure if in-browser apps can do that (yet?) except maybe by saving an entire virtual machine containing the web browser with the app installed. reply consteval 4 hours agorootparentprevYes and Windows in that time period had massive issues with security and culture. The culture of downloading and running EXEs from the internet quickly caught up to everyone, and not in a good way. Also the \"big idea\" is that those applications aren't portable. Now that primary computers for most people are phones, portable applications are much more important. reply flohofwoe 11 hours agorootparentprev> You just downloaded a single .exe file, and it ran self-contained, with all its dependencies statically linked, and it would work on practically any system. Yeah, but try that today (and even by 2010 that wouldn't work anymore). Windows will show a scare popup with a very hard to find 'run anyway' button, unless your application download is above a certain 'reputation score' or is code-signed with an expensive EV certificate. > On MacOS, the user facing model is still that you download an application, drop it in the Applications folder, and it works. Not really, macOS will tell you that it cannot verify that the app doesn't do any harm and helpfully offer to move the application into the trash bin (unless the app is signed and notarized - for which you'll need an Apple developer account, and AFAIK even then there will be a 'mild' warning popup that the app has been downloaded from the internet and whether you want to run it anyway). Apple is definitely nudging developers towards the app store, even on macOS. reply bigstrat2003 11 hours agorootparentprevExcept worse, because everything has to run in a gigantic web browser even if it could be a small native app. reply adwn 10 hours agorootparentExcept better, because it doesn't only work on Windows, and because I don't invite a dozen viruses into my computer. reply jauntywundrkind 10 hours agorootparentprevEvery native app has to be run in a gigantic special OS when it could be a small webapps running in a medium sized browser. Many many ChromeOS (web based consumer OS) laptops are 4GB of ram. You do not want to try that with any normal OSes. reply dkersten 8 hours agorootparentThat’s because windows is loaded with trash. You can easily run desktop Linux with 4 GB or RAM, and people have been doing it for decades. reply VyseofArcadia 8 hours agorootparentprevBut the browser is running in that gigantic special OS. It's not like the OS magically disappears. reply jauntywundrkind 7 hours agorootparentI've already mentioned ChromeOS as one counter-example. SerenityOS and Ladybird browser forked but until recently had a lot of overlap. LG's WebOS is used on a range of devices, derived from the Palm Pre WebOS released in 2009. The gigantic special OS is baggage which already has been cut loose numberous times. Yes you can run some fine light Linux OS'es in 4GB but man, having done the desktop install for gnome or kde, they are not small at all, even if their runtime is ok. And most users will then go open a web browser anyways. It's unclear to me why people clutch to the legacy native app world, why this other not-connected mode of computing has such persistent adherency to it. The web ran a fine mobile OS in 2009; Palm Pre rocked. It could today. reply VyseofArcadia 5 hours agorootparentI for one don't want to use web apps. I want the speed, convenience, and availability of native apps. I want to use applications that work if the internet isn't. I want to use applications that store my data locally. I want to use unglamorous applications that just work and use a native GUI toolkit instead of torturing a poor, overburdened document display engine into pretending it's a sane place for apps to run. Not to mention, from the perspective of a developer, the relative simplicity of native apps. Why should I jump through all the hoops of distributed computing to, for example, edit a document in a WYSIWYG editor? This is something I could do comfortably on a Packard Bell in 1992. reply consteval 4 hours agorootparentThe Web is portable, operating systems are not. Windows and Mac, being short-sighted, did this to themselves. Nobody can agree on anything, Microsoft is constantly deprecating UI frameworks, and it's not convenient at all to write local apps. It's only JUST NOW we have truly portable UI frameworks. And it's only because of the Web. reply Vampiero 2 hours agorootparentThe only thing that defines portability is everyone adhering to the same standards. You say that the web is portable, but really, only Google's vision for the web is relevant, seeing how they have the final say in how the standards are implemented and evolved. So it's basically another walled garden, only much bigger and not constrained to the CPU architecture and OS kernel. Chromium IS a platform. And indeed many applications that do work on Chrome don't work on Firefox. So we're pretty much back where we started, but the problem is harder to see because Chrome has such a monopoly over browsers that for most intents and purposes, and for most devs, it's the only platform that exists. Everyone is good at multiplat when there's only one plat. reply VyseofArcadia 3 hours agorootparentprevQT has been around for decades. So has GTK. Bindings for whatever language you could possibly want. Runs on whatever OS you want. We've had \"truly portable\" UI frameworks since the late 90s. This has not been an issue for my entire adult life. 20 years ago, I was using desktop applications that ran on Mac OS X, Windows, and *nix with no modifications. They were written in Python, used GTK, and just worked. Web apps are popular because 1) people don't like installing things anymore for some reason and 2) it's easier to justify a subscription pricing model. reply consteval 1 hour agorootparentEven those are not portable because they don't target the #1 personal computer in use - smart phones. reply jauntywundrkind 1 hour agorootparentprevThese are all the views of a fossil. Maybe some truth, historically, but years out of date. Want an offline app? Possible for a long time, build a local-first app. Don't want to build a client-server system? Fine, build an isolated webapps. There's so many great tools for webdev that get people going fast, that are incomparably quick at throwing something together. It's just bias and ignorance of an old crusty complainy world. This is a diseased view, is reprehensible small minded & aggressively mean, and it's absurd given how much incredibly effort has been poured into making HTML and CSS incredibly capable competent featureful fast systems, for shame: torturing a poor, overburdened document display engine into pretending it's a sane place for apps to run The web has a somewhat earned reputation for being overwhelmed by ads, which slow things down, but today it feels like most native mobile apps are 60MB+ and also have burdensome slow ads too. There aren't really any tries to go full in on the web. We have been kind of a second system half measure, for the most part, since Pre WebOS gave up on mobile (since FirefoxOS never really got a chance). Apps have had their day and I'm fine with there being offerings for those with a predeliction for prehistoric relics, but the web deserves a real full go, deserves a chance too, and the old salty grudges and mean spirits shouldn't obstruct the hopeful & the excited who have pioneered some really great tech that has both become the most popular connected ubiquitous tech on the planet, but which is also still largely a second system and not the whole of the thing. The web people are always hopeful & excited & the native app people are always overbearingly negative nellies, old men yelling at the cloud. Yeah, there's some structural issues of power around the cloud today, but as Molly White's recent XOXO talk says, the web is still the most powerful system that all humanity shares that we can use to enrich ourselves however we might dream, and I for one feel great excitement and energy, that this is the only promise I see right now that shows open potential. (I would be overjoyed to see native apps show new promise but they feel tired & their adherents to be displeasurable & backwards looking) https://www.youtube.com/watch?v=MTaeVVAvk-c reply wolvesechoes 9 hours agorootparentprevThere is no other industry that is equally driven by fad and buzzword. Try to hide a simple fact that a whole motivation behind SaaS preaching is greed, and bait users with innovative \"local-first\" option. It is actually kinda funny to read cries about \"enshitiffication\" and praises for more web-based bullshittery on the same site, although both are clearly connected and supporting each other. Good material for studying false consciousness among dev proletariat. reply smolder 12 hours agoparentprevI also support the development of client side applications, but I don't think they should necessarily be run in a browser or sandbox or be bought through an app store, and it's definitely not a new idea. reply consteval 4 hours agoparentprevIt truly depends on the application. If you have a LOB database-centered application that's pretty much impossible to make \"local first\". Figma and other's work because they're mostly client-side applications. But I couldn't, for example, do that with a supply chain application. Or a business monitoring application. Or a ticketing system. reply moi2388 12 hours agoparentprev> Microsoft uses it today for C#/Blazor. But it isn't the correct approach as dotnet in browser will likely never be as fast as Javascript in the browser. Might be true, but both will be more than fast enough. We develop Blazer WASM. When it comes to performance, dotnet is not the issue reply jmull 5 hours agorootparentI thought the problem was the hefty upfront price to pay for loading the runtime. reply noworriesnate 3 hours agorootparentThere's some truth to this, but there's a new way of rendering components on the server and pushing that HTML directly to the browser first. The components render but aren't fully interactive until the WASM comes in. It can make it feel snappy if it doesn't take too long to load the WASM. reply josephg 11 hours agorootparentprevYep. And when wasmgc is stable & widely adopted, apps built using blazer will probably end up smaller than their equivalent rust+wasm counterparts, since .net apps won’t need to ship an allocator. reply csomar 12 hours agoparentprevAt the end of the day, all you are doing is syncing state with the server. In the future, you'll have a local state and a server state and the only server component is a sync Wasm binary hehe. Still, you'll be coding your front-end with Wasm/Rust, so get in on the Rust train :) reply meow_catrix 12 hours agorootparentRust frontend dev is not going to become mainstream, no matter what. reply bryanrasmussen 11 hours agorootparentprevmetaphorically, Rust train does not sound enticing. reply jgord 9 hours agoparentprevI definitely view the browser as an app delivery system... one of the benefits being you don't have to install and thus largely avoid dependency hell. Recently I wrote an .e57 file uploader for quato.xyz - choose a local file, parse its binary headers and embedded xml, decide if it has embedded jpg panoramas in it, pull some out, to give a preview .. and later convert them and upload to 'the cloud'. Why do that ? If you just want a panorama web tour, you only need 1GB of typically 50GB .. pointclouds are large, jpgs less so ! I was kind of surprised that was doable in browser, tbh. We save annotations and 3D linework as json to a backend db .. but I am looking for an append-only json archive format on cloud storage which I think would be a simpler solution, especially as we have some people self hosting .. then the data will all be on their intranet or our big-name-cloud provider... they will just download and run the \"app\" in browser :] reply adrianN 11 hours agoparentprevCGI is alive and well. It’s still the easiest way to build small applications for browsers. reply chgs 7 hours agorootparentNobody talks about it because people who use it just use it and get on with their life. It’s painfully easy to develop and host. However it’s likely that generations who weren’t making websites in the days of Matt’s script archive don’t even know about cgi, and end up with massive complex frameworks which go out of style and usability for doing simple tasks. I’ve got cgi scripts that are over 20 years old which run on modern servers and browsers just as the did during the dot com boom. reply sausagefeet 7 hours agoparentprev> I think local-first is the future. This is where the apps runs mostly within user's browser with little to no help from the server. Apps like Figma, Linear and Superhuman use this model very successfully. The problem is: Figma and Linear are not local-first in the way people who are local-first proponents explain local-first. Both of them require a centralized server, that those companies run, for synchronization. This is not what people mean when they talk about \"local-first\" being the future, they are talking about what Martin Kleppman defined it as, which is no specialized synchronization software required. reply silvestrov 10 hours agoparentprev> Figma can [...] then I think vast majority of the apps out there can This doesn't follow. If Figma has the best of the best developers then most businesses might not be able to write just as complex apps. C++ is a good example of a language that requires high programming skills to be usable at all. This is one of the reasons PHP became popular. reply OtomotO 12 hours agoparentprevI have a different take on this: It depends on what you're actually building. For the business applications I build SSR (without any JS in the stack, but just golang or Rust or Zig) is the future. It saves resources which in turn saves money, is way more reliable (again: money) and less complex (again: money) to syncing state all the time and having frontend state diverge from the actual (backend) state. reply boomskats 12 hours agorootparentI have a different take on this: Business applications don't care about client side resource utilisation. That resource has already been allocated and spent, and it's not like their users can decide to walk away because their app takes an extra 250ms to render. Client-side compute is the real money saver. This means CSR/SPA/PWA/client-side state and things like WASM DuckDB and perspective over anything long-lived or computationally expensive on the backend. reply createaccount99 11 hours agoparentprevThe frontend space is moving away from client-side state, not toward it. reply bryanrasmussen 11 hours agorootparentthe frontend space is always moving in every direction at the same time, this is known as Schrodinger's frontend, depending on when you look at it and what intentions you have - you may think you're looking at the backend. reply jamil7 12 hours agoparentprevI work on an iOS app like this right now, it predates a lot of these newer prebuilt solutions. There are some really nice features of working and building features this way, when it works well you can ignore networking code entirely. There are some tradeoffs though and a big one has been debugging and monitoring as well as migrations. There is also some level of end user education because the apps don’t always work the way they’re expecting. The industry the app serves is one in which people are working in the field, doing data entry on a tablet or phone with patchy connections. reply lagrange77 7 hours agoparentprev> WASM could succeed as well. I would guess WASM is a big building block of the future of apps you imagine. Figma is a good example. reply oscargrouch 8 hours agoparentprevI worked on something in this space[1], using a heavily modified Chrome browser years ago, but I consider I was too early and I bet something in this lines (probably simpler) will take off when the time is right. Unfortunately I got a little of a burnout for working some years on it, but I confess I have a more optimized and more to the point version of this. Also having to work on Chrome for this with all its complexity is a bit too much. So even though is a lot of work, nowadays I think is better to start from scratch and implement the features slowly. 1 - https://github.com/mumba-org/mumba reply curtisblaine 12 hours agoparentprevSome applications are inherently hard to make local-first. Social media and Internet forums come to mind. Heavily collaborative applications maybe too. reply swiftcoder 11 hours agorootparentI feel like social media is one of the main things folks want to be local-first. Own your own data, be able to browse/post while offline, and then it all syncs to the big caches in the sky on reconnect... reply curtisblaine 10 hours agorootparentBut how do you do that without essentially downloading the whole social network to your local machine? Are other people's comments, quotes, likes, moderation signals something that should stay on the server or should be synced to the client for offline use? In the first case, you can't really use the social network without connecting to a server. The second case is a privacy and resources nightmare (privacy, because you can hold posts and comments from users that have deleted their data or banned you, you can see who follows who etc. Resources, because you need to hold the whole social graph in your local client). reply swiftcoder 8 hours agorootparentUsually folks looking for this sort of social network are also looking for a more intimate social experience, so we're not necessarily talking about sync'ing the whole Twitter feed firehose. I don't think it's unreasonable from a resources perspective to sync the posts/actions of mutual followers, and from a privacy standpoint it's not really any worse than your friend screenshotting a text message from you. reply curtisblaine 7 hours agorootparentSure, but they're a tiny fraction of the mainstream users and you can already have that sort of experience with blogging and microblogging. Relevant social networks as the public knows them are hard to develop local-first. Even the humble forum where strangers meet to discuss is really hard to do that way. If it needs centralized moderation, or a relevance system via karma / votes, it's hard. reply curtisblaine 10 hours agorootparentprev(unless you want another paradigm of social networking in which you don't have likes, public follows, replies etc., which won't probably fly because it has a much worse UX compared to established social networks) reply rpcope1 13 hours agoprevSo basically we're reinventing the JVM and it's ecosystem? reply thot_experiment 13 hours agoparentSort of yes, but WASM is designed with a different set of constraints in mind that make more sense when you just want to shove the runtime into your whatever. Sometimes reinventing X with lessons learned is actually a great idea. reply flohofwoe 12 hours agoparentprevIn a way yes, except that WASM supports many more languages (e.g. back when I started to look into running C/C++ code in the browser - around 2010 or so - it was absolutely impossible to compile C/C++ to the JVM, which at the time would have been nice because Java Applets still were a thing - of course WASM didn't exist yet either, but Emscripten did, which eventually led to the creation of WASM via asm.js). reply epistasis 12 hours agoparentprevThe JVM is great and all, but that doesn't mean that it is the be-all end-all of the genre. And having mucked with class loaders and writing directly in JVM assembly in the 2000s as part of programming language classes, I'm not sure that the JVM is even a very high point in the genre. Sure, it allowed a large ecosystem, but holy crap is the whole JVM interface to the external world a clunky mess. For 20+ years I have groaned when encountering anything JVM related. Comparing the packaging and ecosystem of Rust to that of Python, or shudder C++, shows that reinvention, with lessons learned in prior decades, can be a very very good thing. reply singularity2001 12 hours agoparentprevexcept that WASM has a huge classloader / linker problem: It's still very hard to combine two wasm files into one and get the memory merger right. Maybe component model can fix it but it comes with so much bloated nonsense that an adaption in Safari might take forever. reply iainmerrick 10 hours agorootparentIt's a problem for some use cases, but is it really a \"huge\" problem in general? You can't easily publish a library in WASM and link it into another application later. But you can publish it as C++ source (say) and compile it into a C++ application, and build the whole thing as WASM. What are the scenarios where you really really want libraries in WASM format? reply flohofwoe 8 hours agorootparentThe only situation I can think of is a plugin system for native applications, where 'WASM DLLs' would solve a lot of issues compared to native DLLs. But those WASM plugins would be self-contained and wouldn't need to dynamically load other WASM 'DLLs', so that situation is trivial even without the WASM Component Model thingie (which I also think is massively overengineered and kinda pointless - at least from my PoV, maybe other people have different requirements though). reply nilslice 4 hours agorootparentthis is exactly what we created Extism[0] and XTP[1] for! [0]: https://extism.org [1]: https://getxtp.com XTP is the first (afaik) platform of its kind meant to enable an app to open up parts of its codebase for authorized outside developers to “push” wasm plugin code extensions directly into the app dynamically. We created a full testing and simulation suite so the embedding app can ensure the wasm plugin code does what it’s supposed to do before the app loads it. I believe this is an approach to integration/customization that exceeds the capabilities of Webhooks and HTTP APIs. reply fwsgonzo 7 hours agorootparentprevI have to say that yes, it's a PITA. Ever tried to enable exceptions in one part, and disabled in the other? It simply won't load. Or any other option. Really. So many investigations, so much time wasted. reply bhelx 4 hours agorootparentprevI agree that it's a problem and I definitely agree with the concern about component model. But maybe Wasm doesn't need 1-1 replacement of all capabilities in the native world. At least not right now. As someone who mostly uses it for plug-in systems, this hasn't been a big issue for us. reply mlhpdx 12 hours agoparentprevYes, and the .Net CLR, etc. reply palmfacehn 13 hours agoparentprevIf your webserver is already JVM based, there's no context switch between the webserver and the application. Not sure how this would be solved with WASM. reply SkiFire13 12 hours agorootparentThis doesn't make sense, WASM is supposed to run on the client, which is generally a different machine than the webserver, while a context switch is an event that happens within a single machine. reply mlnj 11 hours agorootparentWASM on the server also means that an execution engine that containerizes and runs server code in one of the many languages without the overhead of an entire OS like we do with containers now. reply palmfacehn 11 hours agorootparentprevFrom the article: >Wasm on the Server >Why on earth are we talking about Wasm? Isn't it for the browser? >And I really hope even my mention of that question becomes dated, but I still hear this question quite often so it's worth talking about. Wasm was initially developed to run high performant code in the web browser. reply pjmlp 13 hours agoparentprevYeah, by folks that most likely used to bash Application Servers from early 2000's. Not only JVM, also CLR, BEAM, P-Code, M-Code, and every other bytecode format since UNCOL came to be in 1958, but lets not forget about the coolness of selling WASM instead. reply iforgotpassword 12 hours agorootparentThat's a bit oversimplified. I had this thought too and tried to figure out why this is different, and I think there are some major points. The biggest one is in which order they were built and designed. If we take Java and ask why applets didn't take off since they could do everything WASM offers and more, two things come to mind: it was fucking slow on contemporary machines, and the gui framework sucked. WASM is the complete opposite. The gui framework is HTML/CSS, which despite its idiocy in many places had a long time to mature and we've generally came to accept the way it works. Now we just tacked a powerful VM onto it so we don't need to target slow Javascript. There isn't even a new language to learn, just compile whatever you want to WASM, which means you can use a familiar and mature dev environment. The other point is that WASM is way more open than any of the mentioned predecessors were. They were mostly proprietary crap by vendors who didn't give a shit (flash: security, Microsoft: other platforms) so inevitably someone else would throw their weight around (Apple) to kill them, and with good reason. WASM is part of the browser, so as a vendor you're actually in control regarding security and other things, and are not at the mercy of some lazy entity who doesn't give a damn because they think their product is irreplaceable. reply kaba0 7 hours agorootparentWasm is more open, because we effectively have 1.5 browsers left, and whatever google decides will be the de facto “web standard” everyone should follow. If google were pushing for a slightly revamped jvm/applet model, that would be the standard (as the JVM is as open/standardized as it gets) reply pjmlp 6 hours agorootparentIronically if it was today instead of 2010, Mozilla refusing to adopt PNaCL would hardly matter. reply pjmlp 8 hours agorootparentprevSame premise of many other bytecode formats since 1958, a matter of implementation and marketing. reply singularity2001 12 hours agorootparentprevAny reasonable interaction between WASM and JS/DOM gets postponed seemingly indefinitely though. reply thot_experiment 12 hours agorootparentprevThe coolness of WASM is that I can run WASM on like 99.999% of the targets I care to run code on with zero friction. Everyone (well it's HN so someone is probably on LYNX) reading this page is doing so in a browser with a WASM runtime. That has tremendous value. reply pjmlp 8 hours agorootparentApplies to most bytecode formats, it is a matter of implementation. reply marcosdumay 4 hours agorootparentIt never applied to any web bytecode formats, and applies to very few local local ones (arguably, none). It's just a matter of having everybody agree to install the same interpreter, yes. That never happened before. reply pjmlp 3 hours agorootparentAnother example of lack of computing history. Never happened before, really?!? What examples since 1958 would make you happy? Burroughs, Corvus Systems, IBM, Apple, Unisys, MSR, embedded,.... Probably none of them, I bet. reply thot_experiment 2 hours agorootparentYou're missing the forest for the trees. You already have the bytecode interpreter in front of you and so does everyone else. You are already running it, the difference between \"it's definitely already ru",
    "originSummary": [
      "WebAssembly (Wasm) is revolutionizing web applications by offering high-performance and easily maintainable interactive websites, surpassing the traditional CGI model.",
      "Wasm enables code execution in both browsers and servers, providing a lightweight isolation model for untrusted code and supporting multiple programming languages.",
      "Despite some tradeoffs, such as limited threading and JIT (Just-In-Time) compilation, Wasm's performance and security advantages make it a promising technology for future web development, potentially transforming web development constraints and enabling new possibilities."
    ],
    "commentSummary": [
      "WebAssembly (WASM) is being compared to older technologies like Java Applets and Flash, but it offers enhanced security, efficiency, and operates as an open standard.- Unlike its predecessors, WASM runs within the browser's sandbox, allowing it to execute code in various languages safely and efficiently, not limited to JavaScript.- WASM's versatility and security make it a promising technology for both client and server-side applications, with potential for local-first applications that reduce server dependency by running primarily in the user's browser."
    ],
    "points": 231,
    "commentCount": 270,
    "retryCount": 0,
    "time": 1728535138
  },
  {
    "id": 41793658,
    "title": "You Don’t Know Jack about Bandwidth",
    "originLink": "https://cacm.acm.org/practice/you-dont-know-jack-about-bandwidth/",
    "originBody": "PRACTICE Systems and Networking You Don’t Know Jack about Bandwidth If you are an ISP and your customers hate you, take heart: This is now a solvable problem. By David Collier-Brown Posted Oct 9 2024 Credit: Getty Images Share Twitter Reddit Hacker News Download PDF Print Join the Discussion View in the ACM Digital Library Bandwidth Is How Much, Latency Is How Slow Why Is It Slow? Bad Router Software Good Router Software Fixing the ISP Proof Conclusion References Footnotes Imagine you are a company with a lot of remote employees, and they all hate the local Internet service providers (ISPs). Videoconferences are the worst: People cannot hear each other, they randomly start sounding like Darth Vader, and they occasionally just disappear from the conversation. Or you are a small ISP, and your customers say they hate you. When you talk to your ISP or supplier, they say, “Buy more bandwidth.” When your customers complain to their ISP, they are told the same thing. But when you measure how much bandwidth you are using at the busiest part of the day, it is 30% or 50%. In a sense, the suppliers are right: Bandwidth really boils down to the “diameter of the pipe” between you and your employees or customers. If it is too small your data, connections, and more will really get bogged down. Once you are not filling the pipe, though, when you are only 50% busy, you need to look at the software and see what is keeping your packets from arriving in a reasonable amount of time. For example, I once measured the time to send a “ping” to downtown Toronto from my home office in the suburbs. It took 0.13s to get downtown and back. That is the normal ping time for Istanbul, Turkey, roughly 8,000 km away.a That is a pure software problem, and it is called high latency. Bandwidth Is How Much, Latency Is How Slow If you think of the Internet as a series of pipes, then bandwidth conveys how much data you can send down the pipe. It is the diameter, in Figure 1. Figure 1. Bandwidth versus latency. If you buy a narrow, little pipe, you absolutely will have bad performance: You will not be able to get all your data through it in any reasonable amount of time. You will have starved yourself. If you buy a pipe big enough to contain your data, you are fine. The data will all flow straight down the pipe instead of having to sit around waiting for the previous bytes to squeeze their way through. However, buying a pipe with a larger diameter than you need will not help. Your data will still flow into it without delay, but it will not go any faster. It will not exceed the speed of light. There isn’t a “warp speed” available for the Internet. Why Is It Slow? Latency is how quickly (or slowly) data shows up. The least latency you can have equals the length of the pipe divided by the speed of light. Consider an outgoing Zoom call, comprising a bunch of short snippets of picture and sound, nicely compressed, with a period of time between each, as shown in Figure 2. Figure 2. Latency of a Zoom call. Each little slice of audio and video shoots through the pipe by itself, leaving room for other slices. For a Zoom call to fail, you need a lot of other traffic at the same time, enough to fill up the pipe. If you are working from home, this could mean someone in your family is streaming a movie or uploading photos from a cell phone. That causes contention, shown in Figure 3, where the photos (the big, long slice at the top) elbow their way into the pipe ahead of the smaller Zoom slices. Figure 3. Contention. If you have bad router software, the smaller Zoom slices will have to sit around in a queue (buffering) until the photos finish. Because the Zoom slices are delayed, other people on the Zoom call may see you freeze or stutter. Sometimes you will sound like you are shouting from the bottom of a well. On really bad days, you will just drop out. Bad Router Software In the home, the problem is in the software on the home router, which connects to the wider Internet. The bug is called bufferbloat,1 as described by Phillipa Harrison in the Communications research article, “Buffer-Bloated Router? How to Prevent It and Improve Performance.”2 In this bug, the sender buffers up the photos and as soon as the network is available, sends them all. While doing that, it cannot send the videoconferencing slices, so it delays them and sends them only after the photos complete. That might be fine for computers, but humans discussing something need their words to come in the right order, without delays or interruptions. The photos should share the network fairly with the videoconference and not steal all the bandwidth. Good Router Software In the best of all possible worlds, every router on the Internet would be running software that has the latest changes to minimize latency, usually fq_codel or CAKE. Unfortunately, not everyone can update as often as they would like, and router manufacturers are often years behind on rewriting their proprietary operating systems to include the newest fixes. Some are even back at PIE, a much older predecessor to CAKE. Home modem/router/Wi-Fi boxes are, if anything, worse. They get built, used, and thrown away when they die. They never get updated. The net result is a lot of old, buggy software on the Internet, and a lot of packets sitting in queues twiddling their thumbs as they wait for a chance to be sent. Individuals took control of the problem by adding a smarter router between their home and their ISP’s modem/router/Wi-Fi box. The smart router keeps data flowing as fast as the pipe will allow, sending “I’m having contention” signals that the older software can understand when any of the machines in your home network send too much. Fixing the ISP Adding devices to the home does not scale. If you are a company, you cannot afford to buy a new home router for each of your employees. If you are an ISP, you cannot just tear out painfully expensive core routers to install newer ones. The answer is not to tear out routers but to adapt the techniques used in the home. If your customers hate you, put a device with modern software in the path to your customers, downstream of your routers. Such a device is dedicated only to dealing with buffering, bloat, and contention. That also keeps the price down, because an older single-socket Xeon with eight cores can handle 10Gbps of traffic. The name of the software for the ISP device is LibreQoS (https://libreqos.io/), where QoS means quality of service. It came out from the same team that pioneered the fq_codel and CAKE software that addressed the problem at the home-router end. They viewed the problem from the Internet end and set out to help ISPs and other companies that could not trivially buy more bandwidth, but instead had to make better use of what they had. Their solution is to apply a complete suite of fixes to IP networking, which includes: Fair queuing Bandwidth shaping Active queue management Diffserv handling Ack filtering Network topology awareness Parallel processing Bypassing Linux bridge overheads Built-in round-trip time measurement to show the improvement All these fixes work together to find the highest rate that does not overload the slowest part of the system. That means packets do not have to sit in a queue at the ISP, waiting for a chance to be sent, like the videoconferencing packets waiting at home for an image to finish uploading. The first fixes are fair queuing and control of delay, from fq_codel. CAKE then added Active Queue Management (AQM), which performs the same kind of bandwidth probing that vanilla TCP does but adds in-band congestion signaling to detect congestion as soon as possible. The transmission rate is slowly raised until a congestion signal is received, then reduced. Once reduced, it starts increasing again, continuously searching for the highest rate it can use without causing a queue to build up. The algorithm used is an extension of fq_codel. The fairness component prevents particular flows from being starved of resources by other streams, like our photo example, or by hosts starting large numbers of streams to try stealing as much of the network bandwidth as they can. Diffserv is a mechanism for labeling packets as interactive (high priority) through bulk (low priority), so bulk file transfer can be recognized and kept from delaying videoconferences. Related to this, but not part of diffserv itself, is an algorithm to detect and properly prioritize sparse flows, so they will not be starved. Ack filtering entirely removes packets that are merely carrying an acknowledgment of received data, so long as there are enough non-empty packets flowing the same way to take over that task. Next, we come to LibreQoS itself, which adds components specific to running at the ISP end of the network. The first component is a network topology tree, because an ISP will have many routes to its customers, each with its own maximum rate that must be discovered and processed by its own instance of the flow-shaping code. On top of this sits a hierarchical token-bucket classifier that sorts the incoming packets into the correct CPU core and the correct “shaper” rule. The standard Linux shaper that CAKE uses is single-threaded and tends to run out of capacity above 6Gbps. The LibreQoS shaper is limited only by the number and speed of the CPU cores and Ethernet cards. LibreQoS 1.4 introduced a new fast Ethernet bridge, based on eXpress Data Path (XDP) and extended Berkeley Packet Filter (eBPF), to avoid bottlenecking on the ksoftirqd thread in the kernel. When they were finished with the performance work, the LibreQoS crew added software to measure and manage the improvement: a new low-overhead round-trip time sensor, a packet-capture system, and a suite of real-time programs to measure and visualize the performance of large numbers of routes and customers. This is a mature technology and runs on standard equipment, such as an ISP might already have. As a concrete example, an ISP delivering 1Gbps service plans to its customers and with up to 10Gbps total throughput would need a CPU such as an Intel Xeon E-2388G with eight cores, 16GB of memory, and a dual-port 10G Ethernet card—a very ordinary 1u-sized machine from 2021. Now a company with bad performance can ask its ISP to fix it and point at the software and people who have already used it. If the ISP already knows it has a performance complaint, it can get ahead of the problem by proactively implementing LibreQoS. Proof If you have already solved the problem with LibreQoS, you can see the fix happening in real time. In Figure 4, a customer is downloading Diablo 2 and Diablo 4 at the same time over a 100Mbps link. The link shows as completely saturated in the throughput graph, but the delay is tiny: 1.7ms with peaks to 3.1. Fair queuing keeps the delay below levels that humans or Zoom can detect. Figure 4. Download metrics. It can also be measured from a home office if you are one of the happy people who have a debloated router. Using the Waveform Internet speed test,b my untreated connection to a local ISP scored a “D” (Figure 5) on its report card: not suitable for video gaming, videoconferencing, or even audio-only VoIP calls. And badly is exactly how it behaved until I put a debloated home router in front of the ISP’s device. Figure 5. Waveform Internet speed test. Conversely, that same 150/15Mbps connection with a debloated router earned an “A+” grade, a less-bloated speed report, and in practice has been delivering excellent videoconference performance for several years. Conclusion Bandwidth probably is not the problem when your employees or customers say they have terrible Internet performance. Once they have something in the range of 50 to 100 Mbps, the problem is latency—how long it takes for the ISP’s routers to process their traffic. If you are an ISP and all your customers hate you, take heart. This is now a solvable problem, thanks to a dedicated band of individuals who hunted it down, killed it, and then proved out their solution in home routers. References References 1. Gettys, J. and Nichols, K. Bufferbloat: Dark buffers in the Internet. ACM Queue 9, 11 (2011); https://bit.ly/3MSIWK0. 2. Harrison, P. Buffer-bloated router? How to prevent it and improve performance. Commun. ACM 66, 6 (2023), 73–77; https://bit.ly/4gwfxTB. 3. Høiland-Jørgensen, T. Bufferbloat and beyond: Removing performance barriers in real-world networks. Doctoral thesis. Karlstad University Studies (2018); https://bit.ly/3Xr9PK9. Footnotes a See https://bit.ly/3XBlvde b See https://bit.ly/3XBlvde About the Authors David Collier-Brown is an author and systems programmer, formerly with Sun Microsystems, who mostly does performance and capacity work from his home base in Toronto, Canada. Share Twitter Reddit Hacker News Download PDF Print Join the Discussion Submit an Article to CACM CACM welcomes unsolicited submissions on topics of relevance and value to the computing community. You Just Read You Don’t Know Jack about Bandwidth View in the ACM Digital Library ©2024 Copyright held by owner/author. Publication rights licensed to ACM. DOI 10.1145/3690840 Related Reading Opinion Learning New Things and Avoiding Obstacles Architecture and Hardware News Fugaku Takes the Lead Architecture and Hardware Opinion Prophets, Seers, and Pioneers Computing Profession Practice Bufferbloat: What’s Wrong With the Internet? Architecture and Hardware Advertisement Advertisement Join the Discussion (0) Become a Member or Sign In to Post a Comment Sign In Sign Up The Latest from CACM Explore More News Oct 8 2024 The Games People Play Samuel Greengard Computing Applications BLOG@CACM Oct 3 2024 Leveraging Graph Databases for Fraud Detection in Financial Systems Alex Williams Architecture and Hardware News Oct 2 2024 How Laser Communications Are Improving Satellites Logan Kugler Data and Information Shape the Future of Computing ACM encourages its members to take a direct hand in shaping the future of the association. There are more ways than ever to get involved. Get Involved Communications of the ACM (CACM) is now a fully Open Access publication. By opening CACM to the world, we hope to increase engagement among the broader computer science community and encourage non-members to discover the rich resources ACM has to offer. Learn More",
    "commentLink": "https://news.ycombinator.com/item?id=41793658",
    "commentBody": "You Don’t Know Jack about Bandwidth (acm.org)203 points by sohkamyung 20 hours agohidepastfavorite100 comments thepuppet33r 18 hours agoI have spent hours arguing with someone at my work that the issue we are experiencing at our remote locations is not due to bandwidth, but latency. These graphics are exactly what I've been looking for to help get my point across. People do a speedtest and see low (sub-100) numbers and think that's why their video call is failing. Never mind the fact that Zoom only needs 3 Mbps for 1080p video. reply Roark66 7 hours agoparentThat is why I've been successfully working from home for almost a decade starting on an LTE connection that was 5Mb up and 10Mb down(notice this is small b as in bits). No problem at all... Why because most of the time latency was good. I'm still on the same LTE connection, but everyone kept telling me how my speeds are crap and how I should update to a new LTE cat 21 router. So I got one of more popular models ZTE MF289F. And the speed increased to 50Mb up 75Mb down on a speed test. But all my calls suddenly felt very choppy and the perceived Web browsing was unbearably slow... What happened? Well, the router would just decide every day or so to up it's latency to Google.com from 15ms to 150ms until it was restarted. But that is not all. Even when the ping latency was fine it still felt slower than my ancient tplink lte router... So the zte went into a drawer waiting for the times I'll have time to put Linux on it. And the tplink went back on top of my antenna mast. reply dtaht 4 hours agorootparentIn a lot of cases older is better. See also https://github.com/lynxthecat/cake-autorate for an active measurement tool... reply EvanAnderson 16 hours agoparentprevLatency is a cruel mistress. Had a Customer who was using an old Win32 app that did a ton of individual SELECT queries against the database server to render the UI. They tried to put it on the end of a VPN connection and it was excruciating. The old vendor kept trying to convince them to add bandwidth. Fortunately the Customer asked the question \"Why does the app work fine across the 1.544Mbps T1 to our other office?\" (The T1 had sub-5ms latency.) reply chrismorgan 15 hours agorootparentI was involved in some physical network inventory software a dozen years ago. One team produced some new web-based software for it, for use by field agents and such. The first version we got to review was rather bad in various important areas; my favourite was that search would take over thirty seconds in common real-world deployment environment: it was implemented in .NET stuff that makes server calls easy to do by accident and in unnecessarily blocking fashion, and searched by each of the 34 entity types individually, in sequence; and some .NET session thing meant that if the connection had been idle for ten minutes or something, it would even then need to retry every request that got queued while the session was “stale”, which was all of them. So you ended up with 68 sequential requests, on up to half a second’s latency (high-latency 3G or 2G or geostationary satellite)… so yeah, 30 seconds. They’d only developed it with sub-millisecond latency to the server, so they never noticed this. I don’t think it was a coincidence that the team was US-based: in Australia, we’re used to internet stuff having hundreds of milliseconds of latency, since so much of the internet is US-hosted, so I think Australians would be more likely to notice such issues early on. All those studies about people abandoning pages if they take more than two seconds to load… at those times, it was a rare page that has even started rendering that soon, because of request waterfalls and high latency. (These days, it’s somewhat more common to have CDNs fix the worst of the problem.) reply dietr1ch 6 hours agorootparentI think that latency people grow up with has a huge impact on how careful they are when using the internet. Having gotten my hands on an experimental 128kbps link early on, but later and moving to the countryside with a 56kb-1Mbps really spotty connection made me really appreciate local state as every time things blocked on the internet made it pretty notorious. I'm glad there's a push for synchronized, local-first state now, as roaming around on mobile or with a laptop hopping on wifi can only perform nicely with local state. reply akira2501 14 hours agorootparentprevI test all my web dev on the other side of a 4G modem connected to an MVNO. It forces you to be considerate of both bandwidth and latency as it's about 5-20Mbps in the city with 120ms average latency. It's not at all impossible to design fast and responsive sites and single page applications under these constraints, you just have to be aware of them, and actively target it during the full course of development. reply martyvis 8 hours agorootparentMany years ago I was called into troubleshoot the rollout of a new web based customer management application that was replacing a terminal green screen one. I was flummoxed finding all the developers had only ever tested their app running from workstations on 100Mbps switches where the target offices for this application were connected by 128kbps ISDN lines. I was able demonstrate how each 12kB of their application was going to take 1 second. (It was amazing to see their HTML still full of comments, long variable names, etc). I don't even think they had discovered gzip compression. This was after many millions of dollars had already been spent on the development project. reply slt2021 14 hours agorootparentprevgood for you, you are doing your job very well. part of the reason why modern software is so crappy, is because developers often have thee most powerful machines (MacBook pro like) and don't even realize how resource hungry and crrappy their software at lower end devices reply Escapado 13 hours agorootparentAnother part of the reason is that in every company I have ever worked for some SEO dude insists we need to add literally 10 different tracking scripts and insists they need to load first and consume megabytes of data. At my last gig the landing page was 190kb of HTML, CSS and JS from us, 800kb of images and literally 8mb in vendor scripts we were mandated to load as early as possible. Of course we fought it, of course nobody cared. reply justmarc 13 hours agorootparentTry an e-banking website which literally loads over 25MB of crap just to show the login page. This is by far the worst offender I've seen. Madness. reply huijzer 13 hours agorootparentprevYes MacBook pro with a glass fibre connection with a ping below 10 ms to the server. I’m usually on an old copper line (16 ms ping to Amsterdam) in the Netherlands (130 ms to San Francisco). Some sites are just consistently slow. Especially GitHub or cloud dashboards. My theory is that the round trip to the database slows things down. reply FridgeSeal 11 hours agorootparentGitHub and everything Atlassian deserve to be thrown in the pit-of-shame and laughed at for perpetuity. Jira is so agonisingly slow it’s a wonder anyone actually pays for it. Are the devs who work on it held against their will or something? It’s ludicrous. GitHub gets worse every day, with the worst sin being their cutesy little homegrown loading bar, which is literally never faster than simple reloading the page. reply oefrha 10 hours agorootparentprevThese days people on HN love to advocate sockety frontend solutions like Phoenix LiveView, where every stateful interaction causes a roundtrip, including ones that don’t require any new data (or all required data can be sent in a batch at the beginning / milestones). It’s like they forgot the network exists and is uneven and think everything’s happening in-process. To ward off potential criticism: I know you can mix client-side updates with server-side updates in LiveView and co. I’ve tried. Maintaining client-side changes on a managed DOM, sort of like maintaining a long-lived topic branch that diverges from master, sucks. reply eru 12 hours agorootparentprevIn the Google office we had (perhaps they still have) a deliberately crappy wifi you can connect to with your device, to experience extra latency and latency spikes and random low bandwidth. All in the same of this kind of testing. reply rahimnathwani 3 hours agorootparentprevMultiple requests sent in series (e.g. because each depends on the previous one) is also a problem with web apps. When you're developing locally or testing on a staging server near you, the latency is negligible and you might not notice. Chrome's developer tools allow you to disable caching and choose to simulate a specific network type. Most people know those settings restrict the throughput. But they also increase the request latency. They do this on a request-by-request basis, not at packet level, so it's only an approximation. Still a good test. reply chickenbig 10 hours agorootparentprev> Latency is a cruel mistress. Yes, Bloomberg had fun with latency because of their datacenter locations (about a decade ago they still only had two and a half close to New York). Pages that would paint acceptably in London would be unacceptable in Tokyo as when poorly designed they would require several round trips to render. Once the page rendered there was still the matter of updating the prices, which was handled by separately streaming data from servers close to the markets to the terminals. A very different architecture but rather difficult to test because of the significant terminal-side functionality. reply Hikikomori 10 hours agorootparentprevHad some devs in another country complaining that their database query was taking hours to complete but doing it from a server in the same datacenter took a few minutes. Took some weeks of emails and a meeting or two until they understood that we couldn't do anything, I had to actually say that we couldn't do anything about latency unless they physically move their country closer to us. reply jstanley 10 hours agorootparentCould you replicate the data to their country and let them run queries locally? Could they run their client from your country and operate the UI remotely? There are more options than moving the country! reply Hikikomori 8 hours agorootparentI was the network engineer. It was their server and database, I couldn't solve the latency problem for them. reply rodrigodlu 9 hours agorootparentprevIt's easier to have an replica. Doesn't matter if it's \"realtime\" sync or using CDC, from backups, etc. You can even ask one of these guys to do the setup for you. They'll do in a pinch with a happy face. I know because I did. reply Hikikomori 8 hours agorootparentIt is, but not my problem as a network engineer. We did suggest that though but they refused to believe that we couldn't solve the latency \"problem\". reply martyvis 8 hours agorootparentI have been there many times. While the network guy can solve some of the latency in things like TCP handshakes and use compression and caching with magic black boxes you can't fix the actual application query and acknowledgement requirements that might be there. reply mschuster91 9 hours agorootparentprev> Fortunately the Customer asked the question \"Why does the app work fine across the 1.544Mbps T1 to our other office?\" (The T1 had sub-5ms latency.) That reminds me on the atrocious performance of Apple's TimeMachine with small files. Running backups on SSDs is fast, but cable ethernet is noticeably worse, and even WiFi 6 is utterly disgraceful. To my knowledge you can't even go and say \"do not include any folder named vendor (PHP) or node_modules (JS)\", because (at least on my machine) these stacks tend to be the worst offenders in creating hundreds of thousands of small files. reply dtaht 17 hours agoparentprevspeedtest.net added support for tracking latency under load a few years ago. they show ping during up/dl now. That's the number to show your colleague. However they tend to use something like the 75th percentile and throw out real data. The waveform bufferbloat test does 95% and supplies whisker charts. cloudflare also. No web test tests up + down at the same time, which is the worst scenario. crusader and flent.org's rrul test do. Rathan than argue with your colleague, why not just slap an OpenWrt box as a transparent bridge inline and configure CAKE SQM? reply imp0cat 14 hours agorootparentNext time you need to assess your connection's capabilities, try https://speed.cloudflare.com/ instead of speedtest.net. Much more informative. reply thomasjudge 15 hours agorootparentprevWould you put the \"OpenWrt box as a transparent bridge inline\" between your home router and the cable modem, or on the house side of the home router? reply dtaht 15 hours agorootparentI would replace the home router with an OpenWrt router. reply matheusmoreira 15 hours agorootparentOne of the best things I've ever done. OpenWrt is so good. SQM helps a lot with latency. reply danpalmer 15 hours agoparentprevFunnily enough I have found since moving from London to Sydney that people here are far more understanding of the difference between latency and throughput. Being 200ms from anyone else on the internet will do that to you! reply izacus 9 hours agoparentprevNot just latency, but also jitter. Jitter was the biggest issue we had when broadcasting and streaming video. You don't need a lot of bandwidth, latency can be surivived... but jitter will ruin your experience like nothing else. reply guappa 7 hours agoparentprev\"Never underestimate the bandwidth of a station wagon full of tapes hurtling down the highway.\" -- Andrew S. Tanenbaum reply dtaht 16 hours agoparentprevBITAG published this a while back. https://www.bitag.org/latency-explained.php It's worth a read. reply buginprod 10 hours agoparentprevI dont see how people miss latency. It is the only other number shown on the speed check screen! No curiousity as to why that is there? I mean I bet they do care about litres/100km for their car AND 0-100km accelerarion (and many other stats) reply iscoelho 10 hours agoprevThis article appears to be written from a solid Linux networking background, but not from an ISP networking background. ISPs at scale do not use software routers. They use ASIC routers (Juniper/Arista/Cisco/etc.), for many reasons 1) features 2) capacity 3) reliability. ASIC routers are capable of handling 100-1000x the throughput of the most over-provisioned Linux server (and that may even be an understatement). ASIC routers can also route packets with latency between 750us (0.75ms) and 10us (0.01ms!), complemented by multi-second (>GB) packet buffers. QoS is rarely used at scale, if anything only on the access layer, because transit has become so cheap that ISPs have more bandwidth than they know what to do with. These days, if a link is congested, it's not cost saving, but instead poor network planning. QoS also has very limited benefits at >100G scale. With that said, I feel that this article is definitely missing the full picture. reply davecb 5 hours agoparentHi, Dave here. I recommend using a small box running LibreQoS adjacent to the big router. Large-scale routers based on Application-Specific ICs do a wonderful job, but are hard to change. Having a transparent fix in an inexpensive device now is way better than waiting and hoping that the router vendor can update their ASICs (:-)) I emphasised your problem in a video about the article, at https://vimeo.com/1017926413 reply inemesitaffia 1 hour agoparentprevsome isps are small and buy transit above $10/mbps reply cycomanic 17 hours agoprevI know it's common to say bandwidth casually, but I really wish a Blog trying to explain the difference between data rate and latency would not conflate bandwidth and data rate (one could also say throughput or capacity although the latter is also technically incorrect). The term bandwidth really denotes the spectral width occupied by a signal, and while it is related to the actual data rate, it is much less so nowadays where we use advanced modulation compared to back when everything was just OOK. Coincidentally, the difference between latency and data rate is also much clearer using these two terms. reply zokier 10 hours agoparentits almost as if words have different meaning in different contexts https://en.wikipedia.org/wiki/Bandwidth_(computing) reply ajb 14 hours agoprevThis article is confused, or at best unclear, about how AQM works: \"CAKE then added Active Queue Management (AQM), which performs the same kind of bandwidth probing that vanilla TCP does but adds in-band congestion signaling to detect congestion as soon as possible. The transmission rate is slowly raised until a congestion signal is received,[...]\" This appears to suggest that Cake (an in-network AQM process) takes over some of the functionality of TCP (implemented in the endpoints). What's actually happening is that the AQM provides a better signal to allow TCP to do a better job. The rest of the article is more it less accurate, albeit that it's marketing for one particular tool rather than giving you the level of understanding needed to choose one. The dig at PIE (another AQM) is also a bit misleading, in that their main complaint is not PIE itself but the lack of all these other features they think necessary. If Cake used PIE instead of CODEL I don't think it would be noticeably different. reply dtaht 3 hours agoparentI agree that the phrasing in the article is a bit confusing there! Pie had a severe problem in the rate estimator which was fixed in 2018, in Linux, at least: https://www.sciencedirect.com/science/article/abs/pii/S13891... Pie's principal advantage is that it is slightly easier to implement in hardware, it's disadvantages are that it does tail drop, rather than head drop, and struggles to be stable at a target of 16ms, where codel can go down to us and targets 5ms by default. I haven't really revisited pie since the above paper was published. COBALT in cake is a codel derivative. It is slightly tighter in some respects (hitting slow start sooner), and looser in others (it never drops the last packet in a queue, which fq_codel does.fq_codel scales to hundreds of instances and 10s of thousands of queues, still aiming for 5ms across that target, where it would be easier to essentially DOS that many instances of cake with tons of flows. reply declan_roberts 16 hours agoprevWorking from home has really put a spotlight on the terrible asymmetric upload speeds of most cable internet. I can get 1 gb down but only 50 mb upload. Certain tasks (like uploading a docker image) I cant do at all from my personal computer. The layman has no idea the difference, and even most legislators don't understand the issue (\"isn't 1 gb fast enough?\") reply packetlost 15 hours agoparentThis. I've been fighting AT&T for awhile because they told the FCC (via their broadband maps [0]) that they supply fiber to my condo, so I bought it expecting to get fiber. Well when I finally go to set up internet service, they only offer 50/5 DSL service. Fortunately I can get cable that has usable down speeds but the up is substantially less than 50 with garbage routing. I'm not very happy. [0]: https://broadbandmap.fcc.gov/ reply dsissitka 14 hours agorootparentI'm not sure if it's true but I've heard they take that very seriously. Have you filed a complaint with the FCC? Both times I had to do it things got sorted very quickly. https://consumercomplaints.fcc.gov/hc/en-us/articles/1150022... reply packetlost 4 hours agorootparentI actually did file a complaint, but the FCC rejected it for reasons that I were not communicated. reply silisili 10 hours agorootparentprevGood luck. At one time I was experiencing high ping times and near non existent speed from ATT Fiber to Online.fr's network. I did 80% of the diagnostics for them and provided the details and of course a nudge as to what I felt the issue could be. It's extremely frustrating to be a networking person having to deal with home internet CS. To my surprise, it actually did get to their networking team who replied saying the peer was fine and try again. The problem with that was that it came 8 months later, long after I'd left the area and didn't even have service with them anymore. reply __MatrixMan__ 16 hours agoparentprevI got lucky and fiber became available in my neighborhood around the same time I noticed how painful pushing images over cable was. Hopefully you'll get that option soon too. For the unlucky, maybe we can take advantage of the fact that most image pushes have a predecessor which they are 99% similar to. With some care about the image contents (nar instead of tar, gzip --rsyncable, etc) we ought to be able to save a lot of bandwidth by using rsync on top of the previous version instead of transferring each image independently. reply sneak 14 hours agoparentprevWhy are you building images at home for upload? In my experience, it’s much easier to upload code or commits and build/push artifacts in/from the datacenter, whether manually or via CI. It can be as simple as exporting DOCKER_HOST=“ssh://root@host”. Docker handles uploading the relevant parts of your cwd to the server. I have a wickedly fast workstation but spot instances that are way way faster (and on 10gbps symmetric) are pennies. Added bonus: I can use them from a slow computer with no degradation. reply imp0cat 14 hours agoparentprevYMMV, but this can be usually mitigated by connecting to a machine in a datacentre at work and doing all the stuff there (like building and uploading docker images). reply LoganDark 16 hours agoparentprev> I can get 1 gb down but only 50 mb upload. Certain tasks (like uploading a docker image) I cant do at all from my personal computer. As someone who used to work with LLMs, I feel this pain. It would take days for me to upload models. Other community members rent GPU servers to do the training on just so that their data will already be in the cloud, but that's not really a sustainable solution for me since I like tinkering at home. I have around the same speeds, btw. 1Gb down and barely 40Mb up. Factor of 25! reply latency-guy2 15 hours agorootparentI feel your pain, I haven't been in ML world directly for a few years now but I've done the same exercise multiple times. The worst part is that block compression actually does not help if it doesn't do a significantly good job of compression AND decompression. My use case had to immediately deploy the models across a few nodes in a live environment at customer sites. Cloud wasn't an option for us and fiber was also unavailable many times. The fastest transport protocol was someone's car and a workday of wages. reply LoganDark 13 hours agorootparent> The fastest transport protocol was someone's car and a workday of wages. This is actually the entire premise of AWS Snowball: send someone a bunch of storage space, have them copy their data to that storage, then just ship the storage back with the data on it. It can be several orders of magnitude faster and easier than an internet transfer. Sneakernet really works. https://en.wikipedia.org/wiki/Sneakernet reply fragmede 10 hours agorootparentit would be totally cyberpunk to have a data cafe where you bring your hard drive to upload to the cloud and you'd pay by the terabyte/s. have all day? cheap. need to do it in 30 mins? pay up. reply jrs235 7 hours agoprevBandwidth is the numbers of traffic lanes. Latency (speed) is the determined by the material and it's quality of the lanes (gravel road vs asphalt etc.). Throughput is determined by the number of lanes and the speed that vehicles can travel. reply PeterStuer 11 hours agoprevISP's don't mind customers hating them as long as they don't leave, and in many places they can't because theirs is the only game in town, or there is one other player that screws over their clients in exactly the same way. They have used deep packet inspection and traffic shaping for ages to screw over Over The Top competition to their own services or tier their offerings into higher priced slightly less artificially sabotaged package deals. I realy like what the libreqos people are aiming for, but lets not pretend ISP's are trying to be great and just technically hampered (and yes, I'm sure there are exceptions to this rule). reply kortilla 18 hours agoprev> Now a company with bad performance can ask its ISP to fix it and point at the software and people who have already used it. If the ISP already knows it has a performance complaint, it can get ahead of the problem by proactively implementing LibreQoS. The post was a pretty good explanation about a new distro ISPs can use to help with fair queuing, but this statement is laughably naive. A distro existing is only a baby first step to an ISP adopting this. They need to train on how to monitor these, scale them, take them out for maintenance, and operate them in a highly available fashion. It's a huge opex barrier and capex is not why ISPs didn’t bother to solve it in the first place. reply dtaht 16 hours agoparentWe have seen small ISPs get LibreQos running in under an hour, which includes installing ubuntu. Configuring it right and getting it fully integrated with the customer management system takes longer. We're pretty sure most of those ISPs see reduced opex from support calls. Capex until the appearance of fq_codel (Preseem, Bequant) or cake (LibreQos, Paraqum) middleboxes was essentially infinite. Now it's pennies per subscriber and many just a get a suitable box off of ebay. I agree btw, that how to monitor and scale is a learned thing. For example many naive operators look at \"drops\" as reported by CAKE as a bad thing, when it is actually needed for good congestion control. reply kortilla 13 hours agorootparent> We have seen small ISPs get LibreQos running in under an hour, which includes installing ubuntu. Slapped together as a PoC is different than something production ready. Unless those ISPs are so small they don’t care about uptime, a single Ubuntu box in the only hot path of the network is no bueno. > We're pretty sure most of those ISPs see reduced opex from support calls. I highly doubt this. As someone who worked in an ISP, the things that people call their ISP for are really unrelated to the ISP (poor WiFi placement, computer loaded with malware, can’t find their WiFi password, can’t get into their gmail/bank/whatever). When Zoom sucks they don’t even think to blame their ISP, they just think zoom sucks. There is a tiny fraction of power users who might suspect congestion, but they aren’t the type to go into ISP support for help. > Capex until the appearance of fq_codel (Preseem, Bequant) or cake (LibreQos, Paraqum) middleboxes was essentially infinite. Now it's pennies per subscriber and many just a get a suitable box off of ebay. These tools have been around for a while now. My point is that the ISPs that haven’t done something about this yet aren’t holding out for a cheaper capex option. They are in the mode of not wanting to change anything at all. So this attitude that you only need to tell them “there is an open source thing you can run on an old server that will help with something that isn’t costing you money anyway” is out of touch with how most ISPs are run. The ones that care don’t need their customers to tell them. The ones that don’t care aren’t going to do anything that requires change. reply dtaht 3 hours agorootparentThe users that call less post-libreqos are the gamers & zoomers. I am sorry you are are so down on the lack of motivations care for customer service that many ISPs have. I am thrilled by how much libreqos's customer base cares. reply davecb 5 hours agorootparentprevOn of my _evil hidden agendas_ is making end-users aware that their ISP (Rogers, anyone?) is doing a terrible job, and someone like TekSavvy can solve their problems for them (;-)) reply codesections 17 hours agoprevHow does OpenWRT fair on these metrics? Does it count as a \"debloted router\" is the sense used in TFA? Or is additional software above and beyond the core OpenWRT system needed to handle congestion properly? reply wmf 17 hours agoparentOpenWRT has SQM but you have to enable it. https://openwrt.org/docs/guide-user/network/traffic-shaping/... reply dtaht 16 hours agoparentprevOpenWrt depreciated pfifo_fast in favor of fq_codel in 2012, and have not looked back. It (and BQL) is ever present on all their Ethernet hardware and most of their wifi, no configuration required. It's just there. That said many OpenWrt chips have offloads that bypass that, and while speedier and low power, tend to be over buffered. reply NoPicklez 15 hours agoprevIn my experience higher latency due to bufferbloat occurs when my internet connection is saturated, like the example in the article of downloading a game. However, people can still have latency issues from their ISP even if their connection isn't fully saturated at home. Bufferbloat is just one situation in which higher latency is created. Yes, my Zoom call was terrible BECAUSE I was also downloading Diablo saturating my connection. But my Zoom call could also be terrible without anything else being downloaded if my ISP is bad or any number of other things. As someone who worked in a large ISP, if a customer says their bandwidth is terrible but they are getting their line saturated most ISPs will test for latency issues. Bufferbloat is one of many many reasons why someone's network might be causing them high latency. reply ynik 9 hours agoparentThe really horrible bufferfloat usually happens when the upload bandwidth is saturated -- upload bandwidth tends to be lower so it'll cause more latency for the same buffer size. I used to have issues with my cable modem, where occasionally the upload bandwidth would drop to ~100kbit/s (from normally 5Mbit/s), and if this tiny upload bandwidth was fully used, latency would jump from the normal 20ms to 5500ms. My ISP's customer support (Vodafone Germany) refused to understand the issue and only wanted to upsell me on a plan with more bandwidth. In the end I relented and accepted their upgrade offer because it also came with a new cable modem, which fixed the issue. (back then ISPs didn't allow users to bring their own cable modem -- nowdays German law requires them to allow this) reply jonathanlydall 13 hours agoparentprevIt is true that there is an interplay between bandwidth utilization and latency. However (assuming no prioritisation), if your bandwidth is at least double your video conference bandwidth requirements then a download shouldn’t significantly affect the video conference since TCP tends to be fair between streams. Even when I was on a 10Mb/s line I found gaming and voice was generally fine even with a download. However, if you’re using peer to peer (like BitTorrent), then that is utilizing dozens or hundreds of individual TCP streams and then your video conference bandwidth getting equal amount per all other streams is too slow. Bufferbloat exacerbates high utilisation symptoms because it confounds the TCP algorithms which struggle to find the correct equalibrium due to “erratic” feedback on if you’re transmitting too much. It’s like queuing in person at a government office and not being able to see past a door or corner how bad the queue really is, if you could see it’s bad you might come back later, but because you can’t you stand a while on the queue only to realize quite a bit later you’ll have to wait much longer than you initially expected, but if you’d known upfront it would be bad you might have opted to come back later when it’s more quiet. Most people feel that since they’ve sunk the time already they may as well wait as long as it takes, further keeping the queue long. Higher throughput would help, but just knowing ahead that now’s a bad time would help a lot too. I do wish most consumer ISPs supported deprioritising packets of my choice, which would allow you to download things heavily at low priority and your video call would be fine. reply jiggawatts 18 hours agoprevEven IT professionals can't tell the difference between latency and bandwidth, or capacity and speed. A simple rule of thumb is: If a single user experiences poor performance with your otherwise idle cluster of servers, then adding more servers will not help. You can't imagine how often I have to have this conversation with developers, devops people, architects, business owners, etc... \"Let's just double the cores and see what happens.\" \"Let's not, it'll just double your costs and do nothing to improve things.\" Also another recent conversation: \"Your network security middlebox doesn't use a proper network stack and is adding 5 milliseconds of latency to all datacentre communications.\" \"We can scale out by adding more instances if capacity is a concern!\" \"That's... not what I said.\" reply ozim 4 hours agoparentI can see how increasing resources can be useful as \"try to stop the bleeding RIGHT NOW!\", but yeah if a single user has issues if there is not much other activity that's like doing CPR on a person that is screaming \"stop, my foot is bleeding\" and \"hey man we will try this first, stop screaming\" ;) reply floating-io 14 hours agoparentprevThat is the perfect high-level overview of my previous job. I'm semi-retired now... (edit: I forgot to note that the \"let's not\" part was always overridden by \"You're wrong, this will fix it. Do it!\" by management. Then we would eventually find and fix the actual problem (because it didn't go away), but the cluster size -- and the cost -- would remain because \"No, it was too slow with so few replicas\".) reply ozim 4 hours agorootparentYou know that management get bonus points for blaming developers for increasing it at the end of year when annual budget review comes by. reply dtaht 16 hours agoparentprevI share your pain. I really really really share your pain. reply JohnMakin 17 hours agoparentprevIt’s astounding how many people that work in infrastructure should understand things like this but don’t, particularly network bottlenecks or bottlenecks in general. I’ve seen situations where someone wants to increase the number of replicas for some service because the frontend is 504’ing, but the real reason is because the database has become saturated by the calls from the service. It is possible (a little unlikely, but possible, and the rule with infra at scale is “unlikely” always becomes “certain”) to actually make the problem worse by scaling up there. The number of blank stares I get when explaining things like this is demoralizing sometimes, especially in consulting situations where you have some pig headed engineer manager that thinks he knows everything about everything. reply dtaht 16 hours agorootparentBeing one of the authors of fq_codel, cake and to a small extent, LibreQos, I remain boggled after 15 years of trying, to get these simple points across.. Have some laughs: https://blog.apnic.net/2020/01/22/bufferbloat-may-be-solved-... reply hinkley 13 hours agoparentprev“Just try it anyway.” reply ipython 17 hours agoparentprevAs they say, if you’re getting impatient for your baby to arrive, just get more pregnant ladies together! The cluster of pregnant women make the process move along quicker! /s reply codesections 17 hours agorootparentAnd the \"they\" in question is Warren Buffet, https://nymag.com/intelligencer/2009/06/you_cant_make_a_baby... reply vitus 16 hours agorootparentI thought this was first attributed to Fred Brooks in the 70s. > Brooks points out this limited divisibility with another example: while it takes one woman nine months to make one baby, \"nine women can't make a baby in one month\". https://en.wikipedia.org/wiki/Brooks%27s_law reply fragmede 9 hours agorootparentYou can't take a random 9 women and have a baby in one month, but that makes me wonder, statistically, how many women from the whole population would you need to get to 1 month? 9 women can't make a baby in one month, but given 9 million women, the chances are, one of them are giving birth right now. if I needed a baby tomorrow, what do the statistics say on how many women it would take to have a baby, tomorrow? taking the population of the earth and the birth rate and doing some math, you get around to needing 12,000 women of reproductive age for you to have a baby tomorrow. 12,000 is a lot of women! it's well above Dunbar's number. think about that, next time the 9 women one month baby topic comes up. reply voidwtf 15 hours agoprevThese type of solutions don’t scale to large ISPs, and gets costly to deploy at the edge. It’s also not just about throughput in Gbps, but Mpps. Also, this doesn’t take into account that the congestion/queueing issue might be at an upstream. I could have 100g from the customers local co to my core routers, but if the route is going over a 20g link to a local IX that’s saturated it probably won’t help to have fq/codel at the edge to the customer. reply panosv 17 hours agoprevMacOS has now a built in dedicated tool called networkQuality that tries to capture these variables https://netbeez.net/blog/measure-network-quality-on-macos/ Also take a look at Measurement Swiss Army-Knife (MSAK) https://netbeez.net/blog/msak/ reply dtaht 16 hours agoparentbig fan of flent.org and this tool, written in rust - is coming along smartly. https://github.com/Zoxc/crusader reply globalnode 11 hours agoprevcould i put the appropriate algorithm onto a raspberry pi and put it inline with my cheap router to fix the issue? in theory? reply buginprod 10 hours agoprev130ms latency within a city? Are you using sound or something? Yeah yeah I know thats only 40m ish for sound. reply sandworm101 8 hours agoprev>> For example, I once measured the time to send a “ping” to downtown Toronto from my home office in the suburbs. It took 0.13s to get downtown and back. That is the normal ping time for Istanbul, Turkey, roughly 8,000 km away. Ya. Canada is like that. Lack of choice in ISPs, high costs and horrible uptime performance. reply davecb 5 hours agoparentYup, it was Rogers, which is what my building has, and which I'm stuck with for TV. Other, luckier, people can have Bell (sorta-maybe better) or TekSavvy (definitely better) The terrible performance is spotty: that was a particularly glaring example I detected when everything was failing (:-)) reply sandworm101 4 hours agorootparentI'm with Bell for my home internet. I had no choice. It was either them or a cellular connection. Install went ok, but the kid got a panicked look in his eyes when he realized I knew what \"1.5gb\" actually meant. He had me plugged in using some shoddy cables that couldn't handler more than 100mbps. I fixed it once he left. reply moffkalast 8 hours agoprev> If you are an ISP and your customers hate you So... every ISP that exists then? Networking is one of those fields where the results are just varying shades of terrible no matter how hard you try. reply imp0cat 11 hours agoprevThe Waveform Bufferbloat test page (https://www.waveform.com/tools/bufferbloat) has some recommended routers for mitigating bufferbloat, but the first recommended one (Eero) has apparently already been revised and the new version's capabilities are not as good as they once were. The second one (Netgear Nighthawk) seems to have terrible software and support. So I'm looking for some opinions, what's your experience? Casual googling seems to suggest that the best solution to implement traffic management would either be a dedicated machine with something like OpenWRT or an all in one solution (ie. a Firewalla gold + some AP to provide wifi). reply archi42 8 hours agoparentA router you can flash with a modern OpenWRT is likely a good option. Check the project website and/or forums and/or reddit for recent recommendations. That's what I did in the past. Personally I've moved to OpnSense: Some run it natively on a refurbished low-power SFF hardware (6000 or 7000 series Intel should be fine, or some Ryzen), so even in countries with high electricity costs that's feasible these days. More specifically, I run OpnSense in a qemu/libvirt VM (2C of a E5-2690v4) and do WiFi with a popular prosumer APs. Mind that VMs are likely to introduce latency, so if you try this route, make sure to PCIe-passthrough your network devices to the VM - I was prepared to ditch the VM for a dedicated SFF. reply msla 11 hours agoprevPreviously: It's The Latency, Stupid: http://www.stuartcheshire.org/rants/latency.html reply tonymet 18 hours agoprevThere are three parameters of concern with your ISP. Bandwidth, Latency (and Jitter), and Data Caps. Bandwidth is less of a concern for most people as data rates are over 500mb+ . That's enough to comfortably stream 5 concurrent 4k streams (at 20mbps). Latency and jitter will have a bigger impact on real time applications, particularly video conferencing , VOIP, gaming , and to a lesser extent video streaming when you are scrubbing the feed. You can test yours at https://speed.cloudflare.com/ . If your video is jittery or laggy, and you are having trouble with natural conversation, latency / jitter are likely the issue. Data Caps are a real concern for most people. At 1gbps, most people are hitting their 1-1.5tb data cap within an hour or so. Assuming you are around 500mbps or more, latency & data caps are a bigger concern. reply lxgr 18 hours agoparent> At 1gbps, most people are hitting their 1-1.5tb data cap within an hour or so. Assuming you're talking about consumers: How? All that data needs to go somewhere! Even multiple 4K streams only take a fraction of one gigabit/s, and while downloads can often saturate a connection, the total transmitted amount of data is capped by storage capacities. That's not to say that data caps are a good thing, but conversely it also doesn't mean that gigabit connections with terabit-sized data caps are useless. reply Izkata 17 hours agoparentprev> gaming Gamers tend to have an intuitive understanding of latency, they just use the words \"lag\" and \"ping\" instead. reply daemonologist 13 hours agoparentprevAnother problem around data caps is that even if you have/pay extra for \"unlimited\" data, there's still a point where your ISP will fire you as a customer (or threaten to do so) for using too much data - I've heard of it around 8-10 TB on Comcast for example. Unlike with mobile plans there's no soft cap in the contract, they just decide when you've breached the ToS/AUP and can cut you off at their sole discretion. reply readingnews 18 hours agoprev [–] ACM, come on, stop spreading disinformation. You know well and good nothing travels at the speed of light down the wire or fiber. We have converters on the end, and in fact in glass it is the speed of light divided by the refractive index of the glass. Even in the best of times, not c. I just hate that, when a customer is yelling at me telling me that the latency should be absolute 0, they start pointing at articles like this, \"see, even the mighty ACM says it should be c\". Ugh. reply davecb 4 hours agoparentThat's just a comparison to Star Trek, which is set in an imaginary universe where things _do_ exceed the speed of light. reply anotherhue 18 hours agoparentprevAnd that's before you consider the actual cable length vs the straight line distance. reply lxgr 18 hours agoparentprevIt's a reasonable approximation for most calculations. It seems unfair to call that \"disinformation\". Serialization delay, queuing delay etc. often dominate, but these have little to do with the actual propagation delay, which also can't be neglected. > when a customer is yelling at me telling me that the latency should be absolute 0 The speed of light isn't infinity, is it? reply Izkata 17 hours agorootparentSpeed of light also explained why these emails were limited to a little over 500 miles: http://www.ibiblio.org/harris/500milemail.html So it can be relevant, even as an approximation. reply thowawatp302 17 hours agoparentprev [–] I don’t think you’re going to have an issue with Cherenkov radiation in the fiber and that fiber is not going to be a straight line over a non trivial distance so the approximation is close enough. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Internet Service Providers (ISPs) facing customer dissatisfaction due to poor Internet performance can address the issue of high latency, often caused by outdated router software, rather than insufficient bandwidth.- The problem, known as bufferbloat, can be mitigated using modern software solutions like LibreQoS, developed by the creators of fq_codel and CAKE, which enhance data flow and reduce delays.- Implementing such software allows ISPs to optimize their current infrastructure, improving service quality and customer satisfaction without the need for additional bandwidth investment."
    ],
    "commentSummary": [
      "The discussion on ACM.org clarifies the common confusion between bandwidth and latency, especially in remote work, where video call issues are often wrongly attributed to low bandwidth.",
      "Platforms like Zoom require minimal bandwidth (e.g., 3 Mbps for 1080p video), but latency is the real factor affecting connection quality, as shared by users experiencing internet performance issues.",
      "The conversation also explores challenges faced by Internet Service Providers (ISPs) in addressing latency issues and suggests potential solutions like OpenWrt and LibreQoS to manage network congestion and enhance user experience."
    ],
    "points": 203,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1728514315
  },
  {
    "id": 41792055,
    "title": "Software Engineer Pay Heatmap Across the US",
    "originLink": "https://levels.fyi/heatmap",
    "originBody": "Software Engineer Pay No Data $50,000 – $80,000 $80,000 – $110,000 $110,000 – $140,000 $140,000 – $170,000 $170,000 – $200,000 $200,000 – $230,000 $230,000 – $260,000 $260,000 – $290,000 $290,000 – $300,000 $300,000+ +− Leaflet© OpenStreetMap contributors ⌘ K Benchmark Tool 10th Percentile 25th Percentile Median (50th Percentile) 75th Percentile 90th Percentile Software Engineer Pay Heatmap! (Beta) × Explore the interactive heatmap of total compensation pay ranges across the United States, organized by DMA regions and accompanied by a color-coded legend Click into a region and uncover insights on salary percentiles, breakdown of total compensation components, and top paying companies Send us any feedback! What else would you like to see?",
    "commentLink": "https://news.ycombinator.com/item?id=41792055",
    "commentBody": "Software Engineer Pay Heatmap Across the US (levels.fyi)202 points by zuhayeer 22 hours agohidepastfavorite141 comments Manuel_D 22 hours agoI think the granularity of this map is far too coarse grained. Forks, WA and Seattle are in the same bucket. Same with San Jose, sharing the same bucket as Ukiah. The \"Greater Portland Area\" stretches all the way to the southern border of Oregon. That said, there's still some surprising results. I would have expected NYC to be on par with Western Washington and the Bay Area, but it's significantly less, ~190K vs ~260K. reply ryandrake 21 hours agoparentIncluding Pike County, PA as part of the New York City area is kind of wild, too, from someone who grew up there. If you're making $190K and living in Pike County, you're living like a Sultan of a country with a palace made entirely of gold. reply shmatt 15 hours agorootparentThe pay is for the office location, not home location. And as a tech corp employee in NYC I have multiple coworkers commuting 2h each way 2-3 times a week (including from the Pike County area). Rural PA has become pretty popular for the 4br SFH for sub-$1M crowd reply wbl 21 hours agorootparentprevNot all goods are nontradeable. Flights and lodging at a destination cost the same to everyone as do iPhones etc. reply FredPret 20 hours agorootparentAs an immigrant from the 3rd world -> Canada, I found a surprising number of things fit this bill. Some things have a global market and everyone is paying ~ the same price everywhere. - Meat, to an extent - Any oil-derived product - Electronics - Software - IP - Cars - Clothes Of course there are always local taxes, regulations, and logistical considerations that skew the price this way or that way by 10-30%, but these markets can be pretty efficient. reply jacurtis 19 hours agorootparentWhile I do agree with you, that doesn't really impact salary all too much. Allow me to explain. Taxes, Housing, Transportation, and Insurance is what eats up most of your expenses anyway. Housing varies wildly across the world. Many parts of the US now cost > $3,000 /mo to rent a median home. Most of those homes are probably selling for ~$500k putting your mortgage at current interest rates in the same ballpark of $3k /mo. For somoene making $120k that is 1/3 of their income. For someone making the national average (around $70k) that is 1/2 their total income. Just to mention, those areas (Seattle, San Fran, NYC) where you see $250k-300k salaries, those people are paying much higher than these figures. Probably $5-6k in rent or buying modest homes that just happen to cost $1.25M-1.5M In the US you also can pay $1,000 (or more) /mo for health insurance. Most other parts of the world don't have this expense. As of 2024, the average price of a new car is now $47,000 in the USA. Now not everyone is buying a new car but the used car market swells based on this figure. Then you pay 15-30% in taxes on average for most people here. So You add all that up and probably 2/3-3/4 of your money is gone. I agree with you that things like clothing, consumer goods, software, meat, and oil are largely comparable globally now, but these goods only account for probably 10-20% of a person's monthly expenses. Not to mention retirement. Because all these costs are so high, it means I need to put a larger percent of my paycheck to retirement so I can survive a few years of not working before I die. When basics like healthcare and housing are as high as I outlined, it means more money needs to go to retirement, which is money that can't be spent on these basic goods that you mention. So yes, these goods you call out are in fact comparable around the world. But they only account for a relatively small portion of someone's expenses. The outstanding expenses are the most variable (housing, transportation, insurance, taxes). reply FredPret 19 hours agorootparentBefore I moved from my very cheap birth country to the entirely-not-cheap area known as Toronto, I thought along these lines and was concerned that my standard of living would be the same or lower. However, surprisingly, I found it to go up a lot. - Real Estate did take up a larger % of my budget, but it was nicer - The globalized products are only a small % of the budget but they're relatively much cheaper so I can have more/better/both Of course this doesn't mean expensive cities are perfect or even good. I later moved from Toronto to Alberta and it's night-and-day better. Of course the lifestyle is different and some might not like that. There's a big qualitative angle so you can't really compare these things on a spreadsheet, but more after-tax dollars is almost always better. reply stn8188 19 hours agorootparentprevThis is true but the number of people who commute from Pike County (and the whole tri-state/NY-NJ-PA corner) down to the crowded parts of North Jersey and even NYC is insane. I live just across the river from Pike County and experience the traffic though our small town. reply zuhayeer 21 hours agoparentprevThis is a great point, and something we plan to address. We currently use Nielsen's DMA (Designated Market Area) mappings within the US to separate out regional areas which was used for TV / media market surveys. We happen to use DMA categories for our regional pages on Levels.fyi which is why it was easiest to start with since we already had this data captured. The features can sometimes be a bit off and seem like they're grouped very far and wide (you'll notice there's a bit of Denver within Nevada and its just a vestige of how it used to be categorized), but it still provides a bit of a broader level grouping than something like zip code. We've also been considering using Combined Statistical Areas using population instead, but the benefit with DMAs is that it offers full coverage of the entire US whereas some major tech hubs are still missing from CSAs if relying solely on population. We're planning to create some of our own regional definitions and borders using our own submissions and that should offer some more tighter bounds. This was just a v1, and I think its already resonating with folks. GeoJSON data for the map borders: https://github.com/PublicaMundi/MappingAPI/blob/master/data/... Nielsen DMA regions: https://blocks.roadtolarissa.com/simzou/6459889 reply tangjurine 17 hours agorootparentIs this also using the levels.fyi salary data? If that data is submitted by individuals to a particular company, is it possible to see a lot more detailed heatmap, perhaps down to each address of each company? reply teasp2 22 hours agoparentprevAgreed, the \"Greater Boston Area\" doesn't really reflect the reality, you are mixing Boston Metro (HCOL) with many others in Mass, Vermont, and New Hampshire that can't be compared. reply jdminhbg 21 hours agoparentprev> The \"Greater Portland Area\" stretches all the way to the southern border of Oregon. There may be an off-the-grid remote developer in Steens Mountain somewhere, but there aren't any employers there. reply cvwright 20 hours agorootparentLots of techies out in Bend though reply davidw 20 hours agorootparentThis is accurate, as a techie in Bend. But the map hives off Deschutes county all by itself, and at the same time lumps in Baker City with the \"Greater Portland Area\". That needs some work. reply 0xB31B1B 20 hours agorootparentprevNot nearly as many as there are in portland by a factor of about 25 reply Mountain_Skies 21 hours agoparentprevGiven how many areas are marked as not having enough data, I'm going to guess that the dataset is pretty small, which is why some of the areas had to cover large spaces. reply teqsun 49 minutes agoprevThe areas with the super high TCs seem to have a lower base % and higher stock %. I'd like to know if the derived dollar values are the historic actualized or the current value. Because historically there was a real fortunate time to have your RSUs skyrocket, but now that things are stable/declining is the TC still that high? reply apercu 21 hours agoprevI found it interesting that Madison, WI is on par with Chicago, and pay is better than Milwaukee. Digging in a little it's because global tech companies have offices in Madison (also Epic, but even though I was in Toronto for 20 years and thus out of date, I never knew Epic to be a high paying company), and Milwaukee seems to be regionals (Uline & Kohls - and retail rarely pays top dollar for tech talent). reply mgerdts 18 hours agoparentI’ve commonly heard people referring to Epic as a company that pays quite well for the area. They also have a reputation for being kinda crappy to work for, so I never pursued a position there. All the other jobs and offers I’ve had in the Madison area pay quite poorly compared to remote jobs with companies based in the Bay Area. reply bilsbie 20 hours agoprevFolks if you’re at all senior don’t accept less than 200K. It’s the new 100K. And for the level of value you bring to the table you deserve at least a median house within commute distance of your job. reply daok 20 hours agoparentI'm over 15 years of experience, lost my job, and finding a job at 200k+ took months (Bay Area). I'm lucky and found something, but the market is totally different than it was 3-5 years ago. There is a lot of competition, and there are not a lot of open positions. reply Swizec 19 hours agorootparent> I'm over 15 years of experience, lost my job, and finding a job at 200k+ took months (Bay Area) Contrasting anecdata: I'm over 15 years of experience, started looking while employed, and got a 200k+ job in ~1 month. At an early stage startup in Bay Area. (this year) I wonder what we did differently. My approach focused on why I'm a unique value prop to my target market following the \"What have you achieved for what type of company/project\" positioning statement formula. reply johnnyanmac 19 hours agorootparentToo many factors to really compare. Domain, interview preparation, networks, plain ol' luck. You found a startup as well which is a different mentality than a traditional company. In my comparison, games is still falling in real time and I've found nothing full time for almost a year. But I also randomly got cold called for some part time work that keeps me afloat. reply shepherdjerred 19 hours agorootparentprevI had a similarly easy experience finding a job this year, though I can’t exactly figure out what makes me different from those who struggle reply JoeOfTexas 20 hours agoparentprevThe job market for tech is saturated with all the layoffs. Not many companies are going to pay anywhere close to 200k anymore. reply esalman 19 hours agorootparentThat's my impression as well. Fancy CRUD jobs paying $200k simply does not exist anymore. It's more like 100k is the new 200k, judging by past 3-4 years. reply candiddevmike 19 hours agorootparentprevMaybe parent is trying to push for solidarity. reply lolinder 19 hours agoparentprevImportant missing context for this assertion: Living where? We're here looking at a heatmap of pay across the entire US, and in most of the US it like like refusing anything less than $200k would be insisting that you're in the 90th+ percentile of software engineers in your area, which is for obvious reasons not something everyone can get away with. reply azemetre 18 hours agoparentprevI feel like $180k in base salary in Boston is very hard to achieve. Only FAANG companies seem to pay that amount. reply irrational 19 hours agoparentprevDoesn't that depend on where you live? reply wooque 19 hours agoprevThe map needs better coloring, as most of it is in shades of green that are hard to distinguish. Also \"Not enough data\" should not be colored as the lowest pay range. Also why is Greater Denver Area in the middle of Nevada, and leaking in Wyoming? And some numbers are wrong Missoula Area says $190k median, but linked page says $123k median reply duskwuff 18 hours agoparent> Also \"Not enough data\" should not be colored as the lowest pay range. To a jobseeker, there's very little practical difference between \"the pay for software engineering jobs here is extremely low\" and \"there are no software engineering jobs here\". > Also why is Greater Denver Area in the middle of Nevada...? That's Eureka County. No idea why it's classified as \"greater Denver area\", but its total population is 1,855; it's basically a rounding error. I'd be surprised if more than one or two of them were software engineers, let alone were in this dataset. reply mastazi 16 hours agorootparent> \"there are no software engineering jobs here\". I doubt that's what \"Not enough data\" means. reply drdaeman 22 hours agoprevIs it possible to exclude FAANGs and other large corporations? Mixing smaller companies and soul-draining leviathans all in the same pot does not produce meaningful results. reply ponector 21 hours agoparentSmaller companies are also soul-draining in many cases. Why to exclude only faangs? Let's exclude banks, insurance etc reply ken47 21 hours agorootparentYou're getting caught up on \"soul-draining,\" but FAANG's and adjacent are generally acknowledged to have the highest TC. It would be useful to know what the statistics are with these outlier companies removed. reply q7xvh97o2pDhNrh 21 hours agorootparentThe \"outliers\" are the companies paying these insultingly low salaries for technology development. That's why there's so much low-quality software in the world. FAANG (and a few FAANG-adjacent) companies are the only ones paying close to decent wages, and even they've been making frankly egregious cuts to their protein-bar budgets lately. Let's not sit around manufacturing skewed datasets that give people the wrong idea about what software engineers should get paid. reply ponector 20 hours agorootparentNot everyone who works on faang codebase receive a decent wage. Don't forget an army of cheap external contractors sold to faang by bodyshops. reply romanhn 20 hours agorootparentprevWhile I agree that FAANG data should be present/available, I will say that the only reason those companies are able to pay such amounts are due to their outsized valuations and the market shares they have captured. Vast majority of companies do not bring in per-employee revenues on par with the FAANGs, so it's not realistic to set one's definition of \"decent wages\" at those numbers and expect everyone else to pay them. Lots of high-flying startups made a play for fast growth and paid similarly high comp, with the end result of laying huge numbers of people off when the market demanded accountability. TL;DR: I love being an engineer in the Bay Area, but we truly are a bubble. reply jeffbee 20 hours agorootparentprevThe \"outliers\" employ a quarter million software engineers. You are asking for reality to be warped and censored to suit your weird vibes. reply ken47 14 hours agorootparentNo one called for reality to be censored - simply a toggle that shows the summary stats with and without outliers included. This is the kind of thing folks learn in Stats 101. reply joshuamorton 13 hours agorootparentHow are you defining \"outlier\"? You can obviously exclude the top 25 percentile of salaries, since the app shows breakdowns by percentage, but I doubt levels.fyi has accurate data on the number of employees of each company in a particular region. reply JasserInicide 17 hours agorootparentprevWhy to exclude only faangs? Because they skew the numbers upwards. Most of the market doesn't pay their wages. reply shepherdjerred 19 hours agoparentprevMy job at a smaller company was much more soul sucking than my jobs at larger companies reply linguae 19 hours agoparentprevEven in Silicon Valley, not all large employers pay FAANG rates. In general, older, “enterprisey” companies (think HP, IBM, Cisco, and Intel, to name a few) pay considerably less than FAANG and its peers, which tend to be younger and sell consumer-facing products and services. reply lifeisstillgood 19 hours agoprevSo there is a thing I notice but don’t fully understand. A good example is looking at job positions here in the UK - in the London commuter belt. You can see upwards of 150ukp on offer, commonly 90k. For London based jobs. But often you see jobs out in the commuter belt at around the 45 level. So that’s a double or tripling of the salary for a on hours train journey Now I think of this as being like football (soccer!) teams. There is the division four league where the players are professional but often need a second job just to stay afloat - and there is the ridiculous heights of premier leagues Players in the top of the game are not that much better - look at the stats and it’s maybe 10% more pass completion or shots on goal. But the real issue is the teams - if a team is content in the fourth division, they don’t need to get a Saudi investor to bankroll millions so they can offer huge salaries. They can offer low salaries knowing someone will turn up and only be 90% of the top rated ones. I am not sure I understand why power laws work the way they do, a law firm needs hundreds of A players to service each client, a TV studio or football team only needs a handful because everyone watches the same show. Is every business a SaaS business? Or are there businesses that can get 90% of the talent for 10% of the cost and make it work? reply binary132 19 hours agoprevIt’s very icky to me to think about median pay for the everyday American contrasted with what tech folks are pulling down. We’re not brain surgeons, people. Most people do not have good access to paying work, and expecting them to “learn to code” is literally a sad joke. I’m not cool with this situation just because I’m a part of it. Not clear that there’s much we can do; people just aren’t offering any other meaningful alternatives outside of radically overturning our entire social model, which I suppose it might come to eventually, and probably for the worse for everyone involved. reply vunderba 17 hours agoparentCouple notes: 1. There's a HUGE difference in skill sets between software engineers. Some of us have deep understandings of algorithms, linear algebra, and CS theory, and others know how to style a button with appropriate margins. I'll leave it as an exercise to the reader to decide which of these is more difficult. 2. Inequity exists everywhere, and neurotic self-guilt doesn't really accomplish anything. The bottom line is we're all brought into this world with differing advantages/disadvantages across both nature and nurture. Rather than bemoaning the inherent nature of a free market on a random news board, you'd be better off searching for methods of active altruism, donating to auditable charities (Charity Navigator and GiveWell come to mind), and actively helping those in need. reply binary132 7 hours agorootparentThe fact is that our pseudo-liberal market is extremely dysfunctional and doesn’t offer meaningful opportunity to most people who didn’t get technical degrees. A healthy society cannot run on pure software engineering, and cannot offer meaningful work only to its brightest nerds and most unscrupulous thieves. It’s good and right to notice this, and the solution is not that the nerds and thieves should consider dribbling a few extra scraps to the poors (at this point, everyone else). No, I don’t think knowing how to perform an affine transform justifies a 20x better salary than the common man has access to. The vast majority of folks in tech are not building rocket ships and flying cities. reply qwerpy 19 hours agoparentprevI’m thankful that I happened to end up in this field but I’m not going to feel guilty about it. It is hard work and stressful, and if someone’s going to be “lucky” that they chose the right field why can’t it be me? reply binary132 7 hours agorootparentGuilty is not what I was looking for here although clearly that is what people picked up. I certainly don’t think I bear any responsibility for the present situation, even though I benefit from it. To notice that the situation is quite bad for most people and very good for us is not something that should induce guilt but rather, I think, shame and anger. reply lovecg 19 hours agoparentprevThis sort of pearl clutching among highly paid professionals seems to be unique to software engineers. Lawyers and finance types meanwhile are just laughing all the way to the bank. reply binary132 19 hours agorootparentand we should be more like them, is the thinking? reply lovecg 13 hours agorootparentInstead of attempting to lower the compensation of the field as a whole (which, by the way, the actual capitalist employers will love), you’re welcome to donate half of your salary as you see fit. reply binary132 7 hours agorootparentWhere did you get “we should make less” from what I said? I want more people to have opportunities that don’t suck. reply piuantiderp 17 hours agorootparentprevThey are the most inclined to think they can avoid politics. The other types just deal with it every day reply webninja 4 hours agoprevThe next headline based on this map: “Areas with legalized marijuana are associated with higher software engineer pay.” reply mastazi 21 hours agoprevThe \"Not Enough Data\" should really be a different color, instead it is very close to the lowest bracket which makes it confusing reply jmward01 22 hours agoprevThis is great. I will say though that, unsurprisingly, Puerto Rico isn't part of the dataset. Sites like this could really help make the point to people that the US is bigger than the 50 states. [edit] Alaska and Hawaii aren't on here either so it should have been 'bigger than the continuous 48 states'. reply jonny_eh 22 hours agoparentIncluding other countries like Canada, UK, France, and Germany would certainly be nice. reply darkwizard42 21 hours agoparentprevHelpful tip, the word you are looking for is \"contiguous\" when describing the US states all connected together. Can also use the Lower 48 reply jmward01 19 hours agorootparentI have no idea why I typed that but I will accept the public shame! reply darkwizard42 4 hours agorootparentNo shame intended! Just helping in case you didn't know! reply kalyantm 3 hours agoprevThis is just depressing to look at from a European perspective.... reply jart 3 hours agoparentNow you understand why my ancestors left your continent a very long time ago, and why you'll never leave. reply thunder-blue-3 2 hours agorootparentfor TC? reply joshdavham 20 hours agoprevThis was really cool to look at! But I still don't understand how non-tech people afford to live in SF. Wouldn't they be priced out? reply linguae 20 hours agoparentThere are ways people without FAANG-level salaries are able to afford San Francisco: 1. Purchasing when market prices were lower, or inheriting a home. San Francisco has been expensive for decades, but it didn’t always require FAANG-level salaries to afford purchasing a home there. 2. Living in a rent-controlled unit and avoiding evictions (e.g., if the landlord wants to sell, move in, or redevelop the unit). 3. Qualifying for government-subsidized housing, in the form of either Section 8 (voucher-based housing assistance), below market rate rentals, or below market rate properties for purchase. Many Bay Area municipalities have a local housing authority that provides more information about local subsidized housing programs. 4. Shared living situations, whether it’s with family, friends, or strangers, helps reduce housing costs at the expense of needing to share space with others. I know many people in the Bay Area who wouldn’t be able to afford to live here without some type of shared accommodations. 5. Some employers subsidize housing expenses. For example, some universities in the Bay Area offer housing assistance to tenure-track faculty members, ranging from down payment assistance to zero-interest mortgages. There are some universities that sell homes to faculty and staff at below-market prices, with the stipulation that those properties get sold to other faculty and staff once they are put up for sale. reply ops 18 hours agorootparent6. Sell drugs reply pchristensen 20 hours agoparentprevPeople who bought houses at any point in the last decades are house-rich and have a low tax basis thanks to Prop 13. Otherwise, there's an active, ongoing exodus of non-tech people, as well as increasing cost of living and longer commutes for people who stay. reply joshdavham 15 hours agorootparentThis actually sounds a lot like Vancouver in Canada. reply Aeolun 20 hours agorootparentprevSomeone needs to serve the burgers though. Are you saying those people commute two hours each way? reply linguae 19 hours agorootparentOh, definitely. One of the worst commutes in Northern California is the Altamont Pass, which Interstate 580 traverses. As early as 4:30am there is a long line of cars filled with commuters who live in more affordable Central Valley places like Stockton, Modesto, and Tracy, commuting to either Silicon Valley or San Francisco. These are generally not FAANG software engineers; they do all sorts of other work, such as cleaning offices, preparing food, stocking aisles, teaching students, building Teslas (Tesla has a shuttle that goes from Modesto to its Fremont factory), policing Bay Area communities, fighting fires, and a lot more. reply robotresearcher 2 hours agorootparentThe megacorps also have buses that run to distant towns. People have informal roommate situations midweek and ride the bus home for the WFH+weekend. reply ForHackernews 20 hours agorootparentprevSome people live in their cars during the week. reply johnnyanmac 19 hours agoparentprevThe increasing homeless population says yes. reply dinobones 21 hours agoprevCan levels.fyi stop showing non-liquid pre-IPO startup equity as part of total comp please? You'd be surprised at how difficult it is to get liquidity for that stuff, often there are limits to the amount you can liquidate, some can't sell on private marketplace, some can only sell every once in a blue moon event, etc. This is all without mentioning that the \"valuation\" itself is typically pretty speculative. Levels.fyi is treating this equity the same as public company RSUs, which is not the same at all. reply zuhayeer 20 hours agoparentHow do you think it should be treated? I think at the individual granular data point level adding a tag or note about the equity not being immediately liquid is a good start. But I don't think it'd be a good idea to weigh the stock differently since that can depend on so many things. For example SpaceX and some other private companies do offer regular liquidity and I would consider their equity close to liquid. Appreciate the feedback though, and definitely agree we can work on how we display the data and make it more clear. reply fogleman 20 hours agorootparentAdd another dropdown so we can color code by Base salary only, Stock only, etc. reply neilv 19 hours agorootparentMaybe 3 categories? 1. Salary (straightforward, on regular schedule, and you'll get it) 2. Bonuses and RSUs (various vesting rules, and ways you can never see it) 3. Startup stock and (worse) stock options (probably worthless, vesting rules, and you might need an advisor to make sure you don't exercise and come out with a big negative) reply zuhayeer 20 hours agorootparentprevYeah that make sense, will work on adding for this heatmap reply Aeolun 20 hours agoprevOmg, my employer is on the map xD Guess there’s something to be said for being headquartered in Nashville. It’s a bit sad the pay there seems to easily be twice what they pay in Japan :/ reply et-al 21 hours agoprevThis $263k median in the Bay Area is making me sad. reply onewland 19 hours agoparentI think it's certainly incorrect (having known lots of people on both sides of that number, there are far far more below). Another comment thread suggested that startup equity is being taken at face value, which might justify the number but is totally ridiculous reply 62951413 19 hours agoparentprevOn the other hand, most LinkedIn jobs published last week (e.g. full time/hybrid in SF) seem to be much closer to 180K-200K. reply oceanplexian 1 hour agorootparentThat’s base compensation. They aren’t reporting equity, which is usually at least 1/3 to 1/2 total compensation. reply xyst 20 hours agoprevI used to be in the 90th percentile in my area 2 years ago. Then AIv2 (rebranded as genAI, LLM) pumped with VC money via low interest pushed my TC to 70-75th percentile. I could start chasing the $ again, but at this point I’m nearly financially independent and can almost just say fuck it. reply notesinthefield 20 hours agoprevI zoomed in on my home county and the surrounding ones multiple times and each time got a different result. It showed Franklin county in Columbus Ohio to be Indianapolis. reply cheriot 22 hours agoprevI'm surprised NYC is 190k vs Bay Area at 263k and Seattle at 240k. Maybe there's just more non-tech industry software jobs pulling down the median? reply reducesuffering 21 hours agoparentI believe it's the outsized proportion of FAANMG employees. Bay Area should be obvious but Seattle has gobs of $250k comp Amazon and Microsoft employees, much more than outposts in NYC. reply ryandrake 21 hours agorootparentI wonder how skewed the underlying data is, too. Perhaps they're somehow overcounting people at the higher levels and undercounting people lower on the totem pole. The idea of $250K being a median across all levels -anywhere- is kind of astonishing if true. Yes, we all know a few people making $400K at Facebook who have a vacation home in Aspen and drive two Ferraris (they always tell HN how common it is), but is it really that many to drive the median so high? reply jandrewrogers 20 hours agorootparentI know many SWEs at boring non-FAANG, non-unicorn companies that make around the medians shown here so it seems roughly representative, anecdotally. The competitiveness of the market for good engineers has forced every company to at least pretend to try to compete on compensation. It didn't used to be this way but it has really compressed wages upward because you simply won't be able to hire anyone vaguely qualified otherwise. reply Leherenn 3 hours agorootparentprevNo idea how it is in the US, but if you have a look at Zürich, Switzerland, something like 20% of the data points are from Google, maybe 50% from FAANG. Clearly very heavily biased towards high-paying multinational companies. reply reducesuffering 20 hours agorootparentprevIt’s a selection bias for people that are already interested in compensation and willing to divulge it. If you’re on the site, then it perfectly fits you. But it wouldn’t be representative of all software engineers from a census perspective. reply romanhn 20 hours agorootparentThis is the answer. Most people don't know about levels.fyi. reply jeffbee 20 hours agorootparentprevA person who only makes $400k TC at Meta won't even have a first home in the Bay Area, much less another one in Aspen. reply ac29 19 hours agorootparentThe median home-owning household in the bay area doesnt make anywhere close to $400k/yr. Its expensive, but not that expensive. reply jeffbee 19 hours agorootparentI don't think that's a good way to view it. How about the median home buying household? In my town the median sale price this year has been $1.6 million, considering interest rates and property taxes that amounts to $10k/mo housing costs, which is over half the take-home pay of a $400k gross income in California. This is not even to mention the fact that nobody will lend you a mortgage based on the equity portion of your TC, they will usually only count the salary portion and discount the equity portion. reply reducesuffering 20 hours agorootparentprevPlenty of people with $400k TC at Meta have Bay Area homes. $400k TC doesn’t mean “I just got the job where’s my $2m house.” But over a career (Facebook is 20 years old) it does. reply aprilthird2021 19 hours agorootparentprevNapkin math: $400k * 0.6 = $240k post tax $240k - $36k rent = $204k $204k - $50k annual expenses = $154k So if you just work that job for 3-4 years you'll easily have a heavy down payment for a house, assuming no promo, no raises. If you work the job for 7-8 years you can buy a house (more like a townhouse) all in cash. Not going to be the best house on the market, but it's doable. This is all not considering investing in the stock market, raises, being frugal with your expenses, living with roommates to save up a bit, etc. there's lots of ways to parlay more money than most other people will ever make annually into a home reply jeffbee 18 hours agorootparentYour belief in stationary housing prices is charming. reply aprilthird2021 14 hours agorootparentRead the last part of my comment. I left out many natural ways one's earnings and investments and savings could increase over time also. The math still maths reply Cyclone_ 21 hours agoprevI'm getting the sense there's a heavy sampling bias towards larger companies. I looked at my company which is on 1 metro area and it isn't super accurate. The titles they list don't even match up with what we call them. reply Zaheer 20 hours agoparentWe're working on improving our taxonomy right now and would love some more detailed feedback. Do you mind emailing me at@levels.fyi? reply johnnyanmac 19 hours agoprevSouthern CA was expected, but I chuckled at the containment zone that was Bakersfield. Sometimes reality is funnier than fiction. reply clvx 21 hours agoprevImpressive Missoula, MT has a median higher than many metropolitan areas like Austin. One of the factors of the house market explosion in western MT. In a related note, I was checking for tech meetups (at meetup.com) in Missoula and Bozeman and except for Montana programmers, there's no much there. There are a few slack communities but nothing specific for technologies or other groups. reply bruckie 20 hours agoparentI noticed Missoula as an outlier, too. Anyone have a good explanation? My completely uninformed guess is that a bunch of highly-paid engineers moved there during the pandemic for some reason I don't understand, rather than anything inherent to the tech jobs market in Missoula. If so, why Missoula (vs., say, Jackson Hole)? And if not, is there another plausible explanation? reply jandrewrogers 20 hours agorootparentMissoula has been low-profile fashionable with the tech crowd for a long time. It is basically a slightly smaller Boulder. reply cm2012 20 hours agorootparentprevGas and oil industry reply synergy20 19 hours agoparentprevI'm surprised as well, google did not tell me much there about the high wage either, still have no clue reply nick3443 21 hours agoparentprevYou in bozo too? reply jabroni_salad 21 hours agoprevIt's interesting but I would also recommend checking out the BLS if you are interested in what other locations have to offer. It also has maps of where people are actually employed as well as the pay. https://www.bls.gov/oes/current/oes151252.htm reply Waterluvian 21 hours agoprevWith apologies: a choropleth map, not a heatmap. And the granularity is unfortunately quite too low, but I appreciate that sometimes your geodataset is limiting. But with that pedantry out of the way: it would be awesome to be able to normalize based on cost of living. Not that it’s an excuse: I do find it kind of odd that I do the same job as another remote engineer and yet I’m paid a fraction? But to be fair I don’t have student loans and paid off my house in a few years, so cost of living, cost of education, etc. can reveal practical opportunity even if it enshrouds any definition of fairness. reply daotoad 18 hours agoparentThanks for the pedantry re. choropleth vs heatmap. I learned something today because of it. reply freedomben 21 hours agoprevNot really \"across the US\" because it's only the lower 48. AK, HI, US territories don't appear to be included. We're used to being forgotten so it's not a huge deal, but I figured I'd take this small opportunity to bitch about it :-D If you're curious, SWE pay in AK is pretty low. I'd guess median in the 80k. reply adamhartenz 21 hours agoparentIf I say I have been to McDonald's all across the country, that does not mean I have been to EVERY single McDonald's restaurant. Just various ones across the country reply freedomben 21 hours agorootparent> If I say I have been to McDonald's all across the country, that does not mean I have been to EVERY single McDonald's restaurant. Just various ones across the country Sure, but I would argue that you're speaking imprecisely then and using a cliche or phrase that isn't technically accurate at best, and is actually misleading at worst. And when requested for clarity, you should say, \"well almost all.\" Either that or we have different definitions of what the word \"all\" means[1]. If someone said \"I have been to all the states in the US\" would you expect that they have been to AK and HI? [1]: The MW definition matches my understanding: https://www.merriam-webster.com/dictionary/all reply moosedev 6 hours agorootparentNote the “all” in the example binds to “across”, not “McD’s”. “All McD’s” - quite precise; literally all the McD’s. “McD’s all across the country” - a much looser group, implies various McD’s spanning roughly coast to coast (hence “all across the country”), but not necessarily including the westernmost and easternmost McD’s - just a decent distribution of west-to-east, perhaps with no giant gaps (like missing the whole Midwest). reply ghaff 21 hours agorootparentprevYes. If someone said they had been to all the states I’d expect they had been to Alaska and Hawaii. On the other hand McDs all across the country I’d interpret as having been to a lot of McDs in different states with a wide geographical distribution. reply porridgeraisin 10 hours agorootparentprevYou're only hurting yourself by looking up a dictionary for these sorts of things. Words have varying contextual meaning that can range from the dictionary definition to it's polar opposite. Just take the best possible interpretation and move on. Almost nobody cares about your \"technically accurate\". reply al_borland 21 hours agoprevAnn Arbor gets the call out over Detroit. That's rough... and the pay is higher in Traverse City? I'm so confused by these results. reply justahuman74 22 hours agoprevAre these numbers base salary or TC? reply akavi 22 hours agoparentDefinitely TC reply Mountain_Skies 21 hours agoparentprevThere's a breakdown of the compensation categories with each area. reply zombiwoof 20 hours agoprevSo dumb to pay according to where you live. Fun fact: VCs own lots of corp and residential real estate. They want to drive people to live in their areas, pay more but houses cost more and it’s just a big con reply 01100011 20 hours agoparentIt's not just VCs. Do you think your manager wants their home value to go down? They just paid $2.5 million for that 70 year old box of aluminum wiring, lead paint and asbestos and they sure as hell want to sell it for more when they retire. reply rtpg 20 hours agoparentprevPeople say this as if there’s some conspiracy. But I think the uncoordinated explanation is just that annd top executives often operate in a world where informal quick access to a bunch of powerful people is important. And in that universe it simply makes sense to have people be in the same place. And beyond that, in their eyes… why wouldn’t you want to live in these wonderful cities? Why wouldn’t you want to be in the office and work through things? These are people who self select through working being their life so they barely conceptualize alternatives In the same way you can’t conceptualize why someone would want to be in the office, they can’t conceptualize why you wouldn’t. reply fragmede 19 hours agoparentprevWhat's juvenile is not knowing how prices get set, which leads to an oversimplified world view, which leads to thinking that location-based pay is \"dumb\". To price something, you take what it costs to make the thing, throw that out the window, and make up a number based on vibes. If you're lucky, that number is bigger than what it costs to make it, and the business grows. but if you've made crap, or any of a million other reasons why people aren't buying your stuff, then you have to sell it for less than it costs to make it, and the business might be in trouble. reply conqrr 21 hours agoprevlevels.fyi has been of good use for the industry at providing tools to navigate the incosistencies with leveling across companies. Somewhat similar to what Leetcode did as well (not saying Im happy with the standard). There's a lot more refinement that's needed for levels.fyi data: 1. Data goes stale pretty quickly. Salaries are on a downtrend now and many averages don't reflect it yet. 2. Data is overreported in the few popular reigons and companies. Bay area/FAANGetc 3. Values are inflated with stocks that aren't public companies. 4. Lots of companies are following weird vesting schedule now and that calculation isn't the simplified 4 year average of stock value. reply aprilthird2021 18 hours agoparentThe website makes money by selling salary negotiation, so they have a bit of incentive to inflated salaries a bit reply Mountain_Skies 21 hours agoprevSurprised to see $155k as the median for Montgomery-Selma. It's a very low cost of living area, which in the tech industry is justification to pay low salaries regardless of the low desirability of the area. Going to guess it's mostly defense related developer jobs associated with Gunter. reply aprilthird2021 18 hours agoparent$155k in that part of Alabama gets you a real nice house, big lot, kids in private school, etc. reply jowea 20 hours agoprevIs levels.fyi biased towards high pay jobs? reply jacurtis 19 hours agoparentYes. And extremely biased towards tech jobs. So take their data with a grain of salt. If you use Levels.fyi data in a salary negotiation anywhere outside of SF or Seattle, you will probably get laughed at. reply bravetraveler 9 hours agoprevMap of country, hones in on the west coast. Never change reply derfnugget 20 hours agoprevthis post made me immediately reach out for a raise. im not good at this part of the job. i hate this part of the job. im an engineer. first, last, and most importantly. reply anal_reactor 11 hours agoprevSad European noises. reply renewiltord 21 hours agoprevThis is a nice market where you'll be hired if you're better, so it's also an average quality of engineer map. reply c0nsumer 22 hours agoprev [–] This thing feels weird. In Southeast Michigan the popup is for \"Ann Arbor Area\", yet it stretches from VERY rural areas to Ann Arbor (a wealthy college town), across Detroit, across where all auto companies are, etc. This makes it feel very not-representative nor accurate for the area as a whole. EDIT: Ohh... clicking further, now I see. This is just an ad for a \"salary negotiation\" company. No thanks. reply el_benhameen 22 hours agoparent [–] It may or may not be an ad, but I’ve found levels.fyi to be a valuable source of salary information, much more than a “salary negotiation company”. They’re far more accurate and detailed than, say, Glassdoor or Indeed. reply aprilthird2021 18 hours agorootparent [–] It's not bad to acknowledge that Levels sells salary negotiation, so they are more inclined to skew perception of salaries as higher, so more people look and say \"Oh, I'm underpaid!\" While Glassdoor does not sell this service and thus has no incentive to overhype current salary distributions. I do think it's much more accurate though reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An interactive heatmap is available, showcasing software engineer salary ranges across the U.S., categorized by Designated Market Area (DMA) regions.",
      "The heatmap provides insights into salary percentiles, total compensation details, and highlights top-paying companies, with pay ranging from $50,000 to over $300,000.",
      "Users are encouraged to provide feedback on additional features or data they would like to see included in the heatmap."
    ],
    "commentSummary": [
      "A software engineer pay heatmap reveals that salaries in NYC are notably lower compared to Western Washington and the Bay Area, sparking discussions on regional salary disparities.",
      "Criticisms arise regarding the map's coarse granularity, which groups diverse areas together, and the dataset's accuracy due to limited data and potential biases.",
      "Suggestions for improvement include excluding FAANG (Facebook, Amazon, Apple, Netflix, Google) companies and incorporating cost of living adjustments for a more precise representation."
    ],
    "points": 202,
    "commentCount": 141,
    "retryCount": 0,
    "time": 1728504278
  },
  {
    "id": 41798477,
    "title": "Tenno – Markdown and JavaScript = a hybrid of Word and Excel",
    "originLink": "https://tenno.app",
    "originBody": "Tenno is a web app that lets you create Markdown documents that can include computational cells. You can think of it as a mix of Word and Excel, some sort of \"literate programming\" environment.This is still an early version but I wanted to get some feedback from HN on what could be nice features to add.Check out the Docs and examples page, it has a ton of (in my humble opinion) cool stuff!Why did I build this? I was building some estimation for cloud costs in Google Sheets and I quickly ended up with a mess. I realized that if I wanted to analyze how a certain thing changes wrt to multiple variables by plotting it, I had to create a bunch of copies of data and copy my formulas everywhere... a SWE nightmare!Tenno simplifies this because you can essentially define a function you are interested in, and only the analyzing it using plots that explore various dimensions.BTW, you can also use Tenno to build dashboards by pulling data from APIs, checkout the weather data example.",
    "commentLink": "https://news.ycombinator.com/item?id=41798477",
    "commentBody": "Tenno – Markdown and JavaScript = a hybrid of Word and Excel (tenno.app)185 points by deepmacro 5 hours agohidepastfavorite97 comments Tenno is a web app that lets you create Markdown documents that can include computational cells. You can think of it as a mix of Word and Excel, some sort of \"literate programming\" environment. This is still an early version but I wanted to get some feedback from HN on what could be nice features to add. Check out the Docs and examples page, it has a ton of (in my humble opinion) cool stuff! Why did I build this? I was building some estimation for cloud costs in Google Sheets and I quickly ended up with a mess. I realized that if I wanted to analyze how a certain thing changes wrt to multiple variables by plotting it, I had to create a bunch of copies of data and copy my formulas everywhere... a SWE nightmare! Tenno simplifies this because you can essentially define a function you are interested in, and only the analyzing it using plots that explore various dimensions. BTW, you can also use Tenno to build dashboards by pulling data from APIs, checkout the weather data example. vid 0 minutes agoI did another take on this, that extends Markdown links to include types (like triples, like Semantic Mediawiki) called \"mdld\". It's quite powerful but I left it at a good-enough stage for my uses. https://github.com/vid/mdld reply crazygringo 43 minutes agoprevI love the idea of finding new and better interfaces for spreadsheets, and I applaud this effort! That being said, if you want this to be useful for people in general, not just programmers: - People like WYSIWIG. Markdown and the split-pane view seems to be something only programmers like. So I'd suggest being able to do everything, or mostly everything, directly in the rendered/HTML panel. (Maybe the Markdown panel is for power users only) - This is great for working with individual calculations, but a lot (most?) of spreadsheet use is about applying formulas to whole rows, columns, and tables of values. I see you support basic tables, but they're a huge pain to encode/format/edit in Markdown, and I don't see any ability to support things like 200 rows x 5 columns and do things like calculate sums and averages So I think there's a ton of potential here! But I think WYSIWIG and easy tabular data support are going to be key here for broader usage. While the kinds of programmers this seems aimed at now, are already using Jupyter notebooks and Matplotlib for this kind of thing. (Like, when you describe \"why did I build this\", I don't understand why you didn't just fire up a Google Colab notebook.) reply deepmacro 32 minutes agoparentTables are something I want to tackle and I'd like to do them right because (as you point out) are crucial. I didn't get there yet... I want to make it such that you can code on the left and then hide everything and share that with non technical folks. That's why I did not use a Colab, creating something interactive with it (and I did that quite a lot in the past) it's a bit of a PITA. reply RBerenguel 1 hour agoprevI've been writing something similar that keeps evolving, although computable code blocks and markdown have been in there since v1. Runs locally, saves to LocalStorage and is always in a partially broken state because I add more things than those I fix: https://github.com/rberenguel/weave And a couple recent-ish updates (sadly twitter, because I use it as throw-devlog-there): - https://x.com/berenguel/status/1837917590804451378?s=46&t=jc... - https://x.com/berenguel/status/1799770200310726731?s=46&t=jc... - https://x.com/berenguel/status/1796917242791113118?s=46&t=jc... reply deepmacro 44 minutes agoparentNice work! reply lf-non 3 hours agoprevThis is a great concept. I'd love for this to be a part of a notetaking solution that I can run locally. Curious what your long term plans are. I am also curious if it supports taking (entire) tables as inputs and creating derived tables from them which can then be presented as chart etc. That would be really powerful. reply deepmacro 2 hours agoparentI was trying to get a sense of what people like and find useful. Your suggestions and comments are helping to figure that out. I am planning to add support for entire tables, like one coming from a gSheet, and then present data from it. But not there yet, if you have suggestions, please add them to the feedback form https://forms.gle/A8Q8WAG8zj4sLvwQ7 reply masterj 2 hours agoparentprevIt would be cool if you could run SQL queries against lcoal SQLite or DuckDB instances and generate tables reply james_marks 4 hours agoprevReally nice job, the emphasis on local+live execution is much appreciated. I’ve spent some time working on something like this and ended up in a Turing tarpit, I hope you are able to avoid that fate. The questions I let myself avoid for too long was, who is my user, and what are they trying to accomplish? How technical are they? Once I answered those (on year 2, after running out of money) I built the same capabilities into a very different offering. Still with the goals of local, live, executable docs, but you wouldn’t recognize it. reply deepmacro 4 hours agoparentThanks! That's why I posted this here to see if people would resonate with this or not... I've only worked a couple of weeks on this and I had the same questions! Can I ask what you ended up building? Is there a website for it? reply perpil 40 minutes agoprevNeat, you might also look at https://speedrun.cc for ideas. It's a slightly different space, it's markdown to wrap tools with UI's instead of to plot things, but the way I enable you to prompt the user and run little bits of javascript might be of interest to you. reply pragma_x 3 hours agoprevI'm not knocking it. At first glance this reminds me of a notebook like Jupyter. Was that an inspiration here? Also, how would you say this stacks up in terms of sharing these files and/or running them locally? Thanks. Edit: after seeing this, I kind of wish Jupyter worked with markdown exactly like this. Jupyter's GUI-oriented blocks jammed into my VSCode workspace always felt unnecessarily clunky to me. reply deepmacro 2 hours agoparentThe idea was to have a more linear way to express your thoughts and adding calculations. Another big problem for me was that in excel when you want to create various plots for the same multi-variate problem you have to create a ton of copies of your data and formula. For sharing files, it could be as simple as sharing a text file. You can also imagine a future where you create something and you only share the nice looking html view that is still interactive. Running locally? not a problem, I already tested packaging this into a OSX app and it was like 4MB and has the same functionalities. reply Terretta 41 minutes agorootparentCheck this out: Soulver - https://soulver.app Notepad, meet calculator... Soulver is a natural language notepad calculator app for the Mac, iPad & iPhone. It is a better way to work things out than a traditional calculator, and a more lightweight tool for working through problems than a spreadsheet. Here's an example from a real user: https://x.com/hisaac/status/1355720844929019909 reply skrebbel 2 hours agoprevNice! mad nitpick, I feel like the Euro display filter is weird, as it renders \"12.25€\" whereas I'd expect a Euro value to be displayed \"€12.25\". I've never seen it with the euro sign at the end in my life! Maybe this is country-dependent? Another one, the Editable filter is very cool, but every keypress seems to take focus away from the input, making it rather hard to edit a number in practice. Regardless, I really like this! Of most similar attempts I've seen so far, this seems particularly ergonomic and up my alley. Great job! reply Kydlaw 2 hours agoparentConcerning the currency symbol positioning, yes, it is indeed country-dependent. Countries of the Commonwealth usually place the currency sign in front of the figure, but most european do not, and instead place the symbol after the figure. See the Use section in https://en.wikipedia.org/wiki/Euro_sign reply cocoflunchy 2 hours agoparentprevWhat country are you from? In France it’s always at the end for example reply MrQuincle 51 minutes agorootparentIn the Netherlands it is a prefix, at the beginning. https://nl.m.wikipedia.org/wiki/Euroteken reply deepmacro 2 hours agorootparentprevIn Italy it's at the end, hence my interpretation reply x3ro 2 hours agorootparentprevSame in Germany. Definitely at the end here. reply deepmacro 2 hours agoparentprevThanks! I was always told the dollar is a prefix unit while the euro is not, but perhaps between european countries there are differences in that... Anyway, yes the editable thing is a known problem that could be fixed by switching to a view mode rather than an edit mode. The problem right now is caused by keeping in sync the edit field and the code on the left side. reply devjab 1 hour agorootparentThe dollar sign isn’t a prefix in many EU countries. It’s so weird to learn that currencies go first. Then again, I still don’t understand a single American measurement either so why wouldn’t it be weird? Seriously, I have no idea what a mile or a feet is. I’ve learned from time to time when I needed to, but I always forget again because it’s just not making any sense to me. Or maybe it’s bit weird. Do you guys also put something like “ml” in front of numbers? Would it be: “you can buy 10ml of milk for $2”? reply aejfghalsgjbae 1 hour agorootparentLOL, Americans don't do ml. It would be 1/8 fluid wainscot of milk or something. reply HideousKojima 54 minutes agorootparentI realize making fun of the imperial measurement system is a great internet pastime, but almost everything in America has measurements in both imperial and metric on the packaging. The can of soda I'm looking at says 12 fl oz, and then in parentheses it says 355 ml. reply daft_pink 47 minutes agoprevThis seems really cool although I’m not sure that I would call that a cell reply deepmacro 45 minutes agoparentI was hesitant to call it a variable because most people that only use excel are not going to be familiar with that concept. Suggestion for other names? reply buremba 1 hour agoprevI think this would be great as an evolution of MDX (https://mdxjs.com/). MDX is already pretty popular for documentation and it plays well with React but unfortunately there is no framework that adds interactivity to MDX which will enable use cases like data applications. reply seanw444 53 minutes agoparentThank God for all these banners. Without them, the wars would probably still be going! Oh wait... reply Ennea 5 hours agoprevTenno is the name of the faction in the video game Warframe the player characters are comprised of. Just want to point that out. reply deepmacro 5 hours agoparentCool, didn't know that, but I can assure you it's also the name of a lake close to where I live reply philipov 4 hours agorootparentMake sure to always google a name you want to use before using it, to make sure you're not going to get clobbered by someone else's dominant SEO on the word. reply deepmacro 4 hours agorootparentActually I did some very simple search and most results were about my local lake, so I figured that the context was different enough, we'll see. reply jsnell 3 hours agorootparentThat's personalized search results for you :) That lake would not be ranking high for anyone except the people in the vicinity. reply layer8 4 hours agoparentprevIt’s also the Japanese Emperor. reply sickofparadox 5 hours agoparentprevThis was my first thought, probably will make searching stuff difficult. reply Ennea 4 hours agorootparentShould not be too bad. The two are different enough that you can add some sort of context to your search terms, like searching for \"tenno markdown\". reply yawnxyz 3 hours agoprevReminds me of Ink and Switch's Potluck: https://github.com/inkandswitch/potluck but this has way better DX / is way easier to use reply Remnant44 1 hour agoprevFirst of all, I love this concept! The editable fields within a markdown explainer is really intuitive. What I'd personally like to see is a better data exploration tab or similar - basically some place that makes it easier to view the cells in use in the doc, and edit them. basically a spreadsheet tab ;) reply DonnyV 1 hour agoparentYeah, I think using the same concept you did for the charts, use that for the tabular info. I can imagine using those dash's and bars would eventually get annoying to create a decent size table. reply jnordwick 4 hours agoprevSounds similar to emacs org mode where you have markup and tables can have Elisp formulas in them. reply subsection1h 4 hours agoparentYeah, that's what I was thinking. deepmacro, before you created Tenno, did you try Org? It supports plain text spreadsheets[1] in Org documents whose content is marked up using Org syntax[2], which is a lightweight markup language like Markdown but with many more features. If you did try Org, how was it lacking? [1] https://orgmode.org/manual/The-Spreadsheet.html [2] https://orgmode.org/worg/org-syntax.html reply pdyc 4 hours agorootparenti do my taxes with org spreadsheet i doubt it lacks and you also have the option to get output in html/pdf etc. with babel. reply deepmacro 3 hours agorootparentprevHonestly, no. I heard of it but I never used it. I am sure it has a lot of features that can inspire Tenno, I'll look more into it. One of the main points here was making it more approachable, but it depends on who the audience is. reply majkinetor 3 hours agoprevVery nice. There is no support for table cell expressions though, without which marking it as Excel is wrong. There is VS Code extension I use now: https://github.com/cescript/MarkdownFormula Do you consider adding something like that? Any way to use this locally? Is this going to be FOSS or no? reply deepmacro 2 hours agoparentThanks! Yeah, I kind of want to integrate with gSheets so you can have a place where you can dump the data and then operate on it. Also need to find a nice and easy way to deal with local tables, I'll look into the extension you suggested. reply majkinetor 1 hour agorootparentThe extension allows you to add calculation even between tables by naming them in the comment:| DateAmount| ----------------| 2024-10-011| 2023-09-012|Amount| ---------------------------------| Last[1](#table1!B1)| Total[3](#SUM(table1!B:table1!B))> gSheets I am personally not interested in anything beyond my computer. reply ffsm8 51 minutes agoprevYou might want to add a horizontal split mode and use that by default depending on the resolution of the device (phones/tablets) reply deepmacro 49 minutes agoparentGood idea, thanks, I'd also like to prevent edit mode on mobile so one can only see the HTML rendered part. reply cole_ 3 hours agoprevReminds me of https://evidence.dev/ reply deepmacro 2 hours agoparentnice I did not know about this, thanks for sharing! reply tdba 1 hour agoprevVery cool! I'm working on something similar but a little more wysiwyg and collaborative (think gdocs meets gsheets rather than word meets excel). Let me know if you want to chat - email in profile reply fenomas 3 hours agoprevVery cool! A couple first-blush bits of feedback: - The editable text fields currently lose focus after each keypress - Error handling will be a hard nut to crack, but currently if you, say, add \"a\" to an editable textbox that expects a number, the live component reverts to text - Minimal support for buttons would add a lot for simple interactive charts - something like: ::button label=\"Set x to 5\" x = 5 :: reply mads_quist 1 hour agoprevSorry, but Excel is so great and the most loved software in the world because every user intuitively understands that the cells can be edited \"right where they are\". The markdown abstraction is nice, and probably has many usecases, but it's just not a hybrid of Word and Excel because it introduces an abstract language to describe cells and words. (Anyway, great project engineering-wise!) reply deepmacro 37 minutes agoparentI think having to use a tool different than excel is hard because people have been used to that type of UI for 40 years. The problem I have is that once you go beyond a simple thing it becomes messy and you never know what you are dealing with. The way you present things graphically and they way you organize data to perform computations are tangled, making hard to read IMO. reply emaro 56 minutes agoparentprev> Excel is […] the most loved software Citation needed. reply pzmarzly 4 hours agoprevI opened the docs page, tried editing one of the examples, the page immediately crashed. You may be interested in setting up some error boundaries between your components. Unexpected Application Error! Cannot read properties of null (reading 'alternate') TypeError: Cannot read properties of null (reading 'alternate') at Uh (https://tenno.app/assets/index-y2OkIpP6.js:38:18238) reply deepmacro 4 hours agoparentRight, I did not put a lot of checks in place for such failures. Does it keep happen if you refresh? I've never seen it TBH https://tenno.app/docs reply pzmarzly 3 hours agorootparentYup, still happens. The easiest way to reproduce it to change a title of one of the charts to `title=\"\"` (make sure to type it, not copy-paste). I'm using Safari for macOS if this makes a difference. Unexpected Application Error! Unexpected EOF eval@[native code] parseChartCommand@https://tenno.app/assets/index-y2OkIpP6.js:114:333 reply deepmacro 3 hours agorootparentAh, yes, I just fixed that. While you edit, if the code is not valid now it will prevent from crashing the whole thing (band-aid fix). reply niek_pas 4 hours agoprevI wanted to check this out on my phone, but it is unfortunately impossible to even read the text on the page or look at the examples. reply deepmacro 4 hours agoparentYes, a responsive layout it's something I'll tackle in the future, but I think that in 90% of use cases people should use this on a laptop. Check it out again on a laptop if you can! reply vb-8448 4 hours agoprevHow did it compare with https://observablehq.com/ ? reply deepmacro 4 hours agoparentI'd have to look more into it, one thing for sure is that Tenno does not try to be something like Jupyter. The cells execution order is sorted out for you. reply buremba 1 hour agorootparentObservable also handles the execution order for you. reply yodon 5 hours agoprevYou might be interested in the Handlebars integration[0] Microsoft recently added to their SemanticKernel AI SDK. It's not identical to what you're doing, but there's considerable overlap and possibly some food for thought. [0]https://medium.com/@shijotck/automating-tasks-with-semantic-... reply robertlagrant 4 hours agoprevI can imagine having a secrets store whose contents can be embedded into API calls, either in the URL or in a header, would be pretty useful. And looking further ahead, having a way to authenticate users via Okta etc with the usual gubbins of groups and permissions and personal areas and sharing URLs would no doubt give you uptake in corporate areas. reply brospars 4 hours agoprevGreat demo, reminds me a lot of Jupyter Notebook but the \"inline\" cells are so much better reply gavinray 57 minutes agoprevAbsolutely brilliant. Bookmarked. reply deepmacro 48 minutes agoparentGlad you like, I'll keep you posted for new updates in case you're interested. reply gregwebs 3 hours agoprevNeat little tool. I was looking for Excel in the examples but only saw charts. For Excel + Word I use coda.io. You can also quickly create equations with sliders for variables to satisfy similar needs as Tenno but I haven't tried charting with it yet. reply FredPret 5 hours agoprevVery cool. I've always thought there's a lot of untapped potential in literate programming [0]. There are lots of reports and documents that should be this instead of static Word files. [0] https://en.wikipedia.org/wiki/Literate_programming reply tgv 4 hours agoprevExcel, but linear, without the reference mess. It looks cool for mocking up dashboards. Some form of grid with nested documents could make it more useful. reply deepmacro 4 hours agoparent+1 this! I think writing helps you create a narrative that disambiguates the mess you can create in Excel. It also forces you to think more about what you do. Good suggestions! Thanks reply tgv 1 hour agorootparentEasy on the features, though. If you make it too complex, it can lose its (fairly) unique proposition of simplicity. reply cjonas 5 hours agoprevVet cool! I've always wanted something like this in my note taking (currently use obsidian), but it being a \"webapp\" is kinda a deal breaker. I want something that runs and stores my data locally. reply deepmacro 5 hours agoparentWell it's a web app but it's not using anything in the cloud at the moment, I experimented with packaging it in an app for Mac and works nice. If more people wanted it this could definitely be converted in something running locally where you can store .md files in your computer. reply _1tan 5 hours agorootparentThat would be great! reply deepmacro 5 hours agorootparentGood to hear! There is a form to collect feedback, feel free to add comments in there! https://docs.google.com/forms/d/e/1FAIpQLSf0senRYfIo_034Ukno... reply bshaughn 5 hours agoparentprevYou can do a lot of this in obsidian with the dataview and execute-code plugins! reply deepmacro 5 hours agorootparentDidn't know about this! Will look into it, one nice thing here is that you do not need \"ordering\" like in imperative programming when you define your cells. Is that possible too in Obsidian? Or perhaps it's not useful... reply hiergiltdiestfu 2 hours agoprevThe company security team at my workplace is blocking tenno.app as grayware :D reply deepmacro 2 hours agoparentIt can execute arbitrary JS in your browser using eval(), that could be why. reply Sauravsingh6 3 hours agoprevThank you for sharing Tenno with the HN community! It's always exciting to see new tools that enhance productivity and bridge the gap between document creation and computational analysis. reply hyperbrainer 1 hour agoprevOrg mode in JS? reply btbuildem 5 hours agoprevInteresting effort.. I'd just spin up a Jupyter Notebook instance, but hey. reply deepmacro 5 hours agoparentRight, something that's potentially difficult with Jupyter is sharing something that looks nice with non technical folks where they can easily change values and see the result change immediately without having to execute cells. reply oulipo 5 hours agoprevNice idea, could be a lightweight enough way to create some visualizations reply deepmacro 5 hours agoparentYeah, at the moment I only have line charts but I can add other charts using ChartJS. I also think it could be useful to share some initial prototypes with less technical folks. reply chaosprint 3 hours agoprevinteresting idea. try to understand the dsl for the chart but the doc is not very detailed reply dartos 3 hours agoprevThe lotus will be pleased. reply perftime 5 hours agoprevNice! What about integrations with Google sheets? reply macro-b 5 hours agoparentThat seems quite doable but it would require a backend to store the OAuth reply pdyc 4 hours agorootparentPublic sheets don't require oauth, i am using public sheet as test url to display csv in my tool. reply anthk 2 hours agoprevAh, like a reduced version of Org-Mode. reply jnordwick 1 hour agoparentI hate to be that guy: Org mode did it. (actually, I love pimping emacs) reply ulla7653 5 hours agoprevIf you hate spreadsheets this could be your way out of it... reply hello_computer 4 hours agoprev [–] It would be helpful to have a built-in bignum type. Excel’s usage of float is performant, but it is also a mistake. A complex number type would also be nice to have. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tenno is a web application designed for creating Markdown documents with computational cells, combining features of Word and Excel for \"literate programming.\"",
      "The app is in its early stages and is seeking user feedback on potential features, with a Docs and examples page available to demonstrate its capabilities.",
      "Tenno aims to simplify data analysis and visualization, allowing for function definitions, multi-dimensional plotting, and dashboard creation by integrating data from APIs."
    ],
    "commentSummary": [
      "Tenno is a web application that integrates Markdown and JavaScript, enabling users to create documents with computational cells, akin to a blend of Word and Excel.- It targets \"literate programming\" to streamline complex data analysis and visualization, and is currently in its early development phase, seeking user feedback for enhancements.- The app is compared to Jupyter and Org mode, emphasizing user-friendliness and the ability to share with non-technical users, with potential improvements in WYSIWYG functionality, table support, and local execution."
    ],
    "points": 185,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1728566138
  },
  {
    "id": 41791773,
    "title": "My negative views on Rust (2023)",
    "originLink": "https://chrisdone.com/posts/rust/",
    "originBody": "My negative views on Rust 2023-10-13: Edited based on feedback, but preserves the same content. This is a little summary of my current thoughts on Rust. I wonder whether I’ll look back in five years and see if my views have changed. The Good So that Gentle Reader knows that I’m not entirely biased against the language, there are obviously positive things to appreciate about it: Rust’s macros are very good. They act like Lisp’s macros, unlike Haskell’s. The fact that Rust has type-classes (“traits”) and sum types (“enums”) and pattern matching is very attractive. Making orphans package-wide rather than module-wide is a very good decision. I like its treatment of records. Its standard library does a few things well, like handling strings as UTF-8. Distinguishing mutability has its advantages, it’s easy to see that a function is “morally” pure–if not actually pure–and that’s good for reading. Unsafe and panic The use of unsafe is a little disturbing, because many libraries feature it, and people are tempted to use it on private work projects to quickly get around the language’s limitations. But it’s not much different to using an FFI. I don’t see this is a big downside. panic is a little more bothersome, because Rust libraries go to great pains (with many syntactic tricks like ? and auto-conversions from smaller error types to larger ones) to handle errors explicitly, but then panics unwind the stack to the top of the process, and panics inside a panic don’t run destructors, etc. The overall effect is that, like my three questions of language design, the answer to “how you handle errors” is “at least two, incompatible ways.” Take Sugar? Rust’s use of magical sugar constructs, where the compiler will automatically insert dereferences and copies and drops for you has an initial appealing “it’s all simple underneath” quality to it, but in practice this leads to bad compile errors: The worst kind of compile error is the one where the compiler is complaining about something that it generated for you, rather than something you explicitly wrote. This can be compared with Haskell’s use of monads, that provide syntactic sugar. The more magic you introduce, the harder it is for newbies to learn and to talk about. Fetishization of Efficient Memory Representation I’ve watched people on calls that are a couple years into Rust spend 20 minutes attempting to understand why their perfectly reasonable code doesn’t fit into Rust’s tight memory restrictions. I’ve also been Rust-splained, by people with white in their hair, with an air of misty-eyed revelation, that once you “get” Rust’s memory model of the stack and the heap,1 that things just all fit together wonderfully. There’s nothing wrong with that, but it’s a theme. This touches on another topic I’d like to write about elsewhere: the difference between practice and theory and how users of languages like Rust and Haskell that make big promises and require big sacrifices don’t seem to distinguish the difference. It’s not the technology that’s working poorly, it’s that you’re using it wrongly. In practice, people just want to be able to write a tree-like type without having to play Chess against the compiler. I predict that tracing garbage collectors will become popular in Rust eventually. This is both Rust’s main goal–be like C, no garbage collection,2 but safe–and also its main downside. People waste time on trivialities that will never make a difference. The Rewrite Fallacy I see a lot of “we rewrote X in Rust and it got faster” posts. I think that if you rewrite anything from scratch with performance in mind, you’ll see a significant performance improvement. I’m suspicious of how much Rust itself is needed versus the developers having some performance discipline. Complexity Rust has arrived at the complexity of Haskell and C++, each year requiring more knowledge to keep up with the latest and greatest. Go was designed as the antidote to this kind of endlessly increasing language surface area. Endless libraries re-treading existing patterns (web services, parsers, etc.) in Rust. As a long-term Haskeller, I’ve done more than 15 years of riding a hamster wheel like that. It is fun for a while, but at some point I grew tired of it. This aspect of Rust puts me off. I don’t need another tamagotchi. The “Friendly” Community All new language communities are nice. When things don’t matter much, people have no reason to get mad. As soon as people have a stake in something, that’s when things heat up and tempers come out. You get a stake in a programming language by writing a lot of code in it, or by building a business on it. When you have a stake in how a language works, you’re highly sensitive to changes that will make you do more work than needed, or will limit your goals. I’ve seen this play out for Haskell. Around 2007, when I started with Haskell, the community was friendly as anything, evangelic, open. People praised it for this. Everyone just felt blessed to be able to use this exciting language. Today, it’s more like any other community. What happened? People started using Haskell for real, that’s all. Rust is going through the same thing, much more rapidly. Is it a reason to avoid Rust? No. But a “nice” community isn’t a reason to join an upcoming language, either. Since I wrote this, people are already trying to fork Rust because they’re not happy with the governance of it. This doesn’t mean anything deep other than that people are using Rust now, as stated above, and the “friendly, welcoming” starts to become mixed with more diverse moods and motivations. Async is highly problematic Rust’s choice to exclude a runtime/scheduler blessed and built-in to the language means they had to develop alternative strategies in the language itself. This is not turning out well. Coloured Functions is a powerful metaphor for the incompatibility between async and synchronous functions and the awkward situations that mixing them introduces. Some blog posts have attempted to downplay the situation by focusing on technical aspects of specific syntactic forms. That doesn’t really matter, though, because the reality is much simpler: Async code is easier to use when dependencies are in an async form. People will choose libraries that are async over libraries that are not. However, maintainers that have written good, maintained code, are also resistant to adopt async. One conversation I’ve overhead: Person 1: Another difference between diesel and sqlx is that diesel is not async yet and from looking at the issues it doesn’t seem to be priority yet. Person 2: That sounds like a major issue As the saying goes, the proof of the pudding is in the eating of the pudding. Async introduces long, heated discussions. The problem for Rust is that its users want a runtime, but want the option of not having one. The result is a mess. When combined with iterators, I think understanding such code is quite difficult. Generally, I think Go, Erlang and Haskell are the better choice here for general purpose use. A tracing garbage collector and green threads make programmers more productive for general purpose programming (not systems programming). Which brings me to the next section. As a general purpose language I feel like Rust is self-defined as a “systems” language, but it’s being used to write web apps and command-line tools and all sorts of things. This is a little disappointing, but also predictable: the more successful your language, the more people will use your language for things it wasn’t intended for. This post still offends many who have tied Rust to their identity, but that’s their problem, not mine. Conclusions, if any I won’t be using Rust for any of my own personal projects for the above stated reasons. But it was used at my job at the time of writing, so I felt the need to express myself about it. But I wouldn’t mind using it as a replacement for single-threaded C if I just use the standard library, that might be fun, although I don’t do any embedded work, so I wouldn’t hold my breath. I think that the excellent tooling and dev team for Rust, subsidized by Big Tech, pulls the wool over people’s eyes and convinces them that this is a good language that is simple and worth investing in. There’s danger in that type of thinking. Having done my fair share of C code, there’s nothing new here for me.↩︎ Of course, Rc and Arc are reference counters, which is a form of garbage collection. See The Garbage Collection Handbook: The Art of Automatic Memory Management published in 1996. But most developers have a superficial understanding of garbage collection as a technical subject, and therefore “garbage collection” for them means “tracing garbage collector.” Hence bizarre discussions that pit “garbage collecion versus reference counting.” It’s like saying fruit versus apples.↩︎ © 2021-12-22 Chris Done Read more posts →",
    "commentLink": "https://news.ycombinator.com/item?id=41791773",
    "commentBody": "My negative views on Rust (2023) (chrisdone.com)181 points by rc00 23 hours agohidepastfavorite256 comments jkelleyrtp 21 hours ago\"There are only two kinds of languages: the ones people complain about and the ones nobody uses\". --- Glad to see fluffy negative articles about Rust shooting up the first slot of HN in 20 minutes. It means Rust has made finally made it mainstream :) --- The points, addressed, I guess? - Rust has panics, and this is bad: ...okay? Nobody is writing panic handling code, it's not a form of error handling - Rust inserts Copy, Drop, Deref for you: it would be really annoying to write Rust if you had to call `.copy()` on every bool/int/char. A language like this exists, I'm sure, but this hasn't stopped Rust from taking off - Fetishization of Efficient Memory Representation: ... I don't understand what the point is here. Some people care about avoiding heap allocations? They're a tool just like anything else - Rewrite anything and it gets faster: okay sure, but there are limits to how fast I can make a Py/JS algorithm vs a compiled language, and Rust makes writing compiled code a bit easier. People probably aren't rewriting slow Python projects in C these days - Rust is as complex as C++: ...no, it's not. Rust really hasn't changed much in the past 6 years. A few limitations being lifted, but nothing majorly new. - Rust isn't as nice of community as people think: subjective maybe? People are nice to me at conferences and in discussion rooms. There's occasional drama here and there but overall it's been pretty quiet for the past year. - Async is problematic: Async Rust really is fine. There's a huge meme about how bad it is, but really, it's fine. As a framework author, it's great, actually. I can wrap futures in a custom Poll. I can drive executors from a window event loop. Tokio's default choice of making `spawn` take Send/Sync futures is an odd one - occasionally cryptic compile errors - but you don't need to use that default. I'm unsure why this article is so upvoted given how vapid the content is, but it does have a snappy title, I guess. reply hyperbrainer 21 hours agoparent> Rust is as complex as C++: ...no, it's not. Maybe not yet, but it is heading in that direction; and I only say this because of the absolutely giant pile of features in unstable that seem to be stuck there, but I hope will eventually make its way to stable at some point. > Async Rust really is fine I dunno. Always thought it was too complicated, but as another person pointed out avoiding Tokyo::spawn solves many issues (you said this too, I think). So maybe, not Rust's fault :D reply Diggsey 18 hours agorootparent> Maybe not yet, but it is heading in that direction; It's definitely getting more complex, but C++ has a huge lead haha. C++ is like a fractal in that you can look at almost any feature closer and closer to reveal more and more complexity, and there are a lot of features... Here's a page on just one dark corner of the language: https://isocpp.org/wiki/faq/pointers-to-members and it interacts in interesting ways with all the other corners (like virtual vs non-virtual inheritance) in fun and exciting ways... Also, there are far more ways to cause UB in C++. Rust has a big lead on formalizing what constitutes UB, and even those rules you only need to learn if you are using \"unsafe\", whilst in C++ you don't have that luxury. reply tialaramex 11 hours agorootparent> Also, there are far more ways to cause UB in C++. As well as lots of Undefined Behaviour, C++ also has what its own experts call \"False positives for the question is this a C++ program\" the Ill-Formed No Diagnostic Required features, nothing like these exist in Rust, they're cases where you can write what appears to be C++ but actually although there are is no error or warning from the compiler your entire program has no meaning and might do absolutely anything from the outset. I've seen guesses that most or even all non-trivial C++ invokes IFNDR. So that's categorically worse than Undefined Behaviour. Finally, C++ has cases where the standard just chooses not to explain how something works because doing so would mean actually deciding and that's controversial so in fact all C++ where this matters also has no defined meaning and no way for you to discover what happens except to read the machine code emitted by your compiler, which entirely misses the point of a high level programming language. One of the things happening in Rust's stabilization process is solving those tough issues, for example Aria's \"Strict Provenance experiment\" is likely being stabilized, formally granting Rust a pointer provenance model, something C++ does not have and C23 had to fork into a separate technical document to study. reply googh 11 hours agorootparentMost (if not all) of your posts here on HN boil down to \"C/C++ bad, Rust good\". I wonder what you are trying to achieve by this, but I assure you that this does not do Rust any favor other than giving the impression that the Rust community is obnoxious. reply Ukv 8 hours agorootparentI found their comment an interesting contribution to the discussion, pointing to a couple of specific pitfalls in C++. It's not just an empty \"C/C++ bad, Rust good\", and I don't see why it'd give the impression of an obnoxious community. reply googh 6 hours agorootparentI just wrote that as an observation, and I did not mean to offend the parent. Now, let me explain why I felt that way. First and foremost, the phrase \"undefined behavior\" only applies to C and C++ because the specifications of those languages define it. The statement that Rust has no UB does not make sense because Rust has no specification, and all behavior is defined by the default implementation. For example, C/C++ specifications state that using a pointer after calling \"free()\" on it is UB. But an implementation can make it well-defined by adding a GC and making \"free()\" a no-op. Hence, memory safety is entirely orthogonal to UB. Another example: signed overflow being UB is not a memory safety problem unless the value is used for array indexing. Also, it is possible to enable bounds checking in STL containers (like _GLIBCXX_ASSERTIONS). It seems like that a lot of Rust fans read John Regehr's posts and use \"undefined behavior\" as a boogeyman to throw shade at C/C++. They repeat the same points ad nauseam. It also helps that the phrase \"undefined behavior\" evokes strong emotions (eg., \"nasal demons\"). I see the parent commenter doing this frequently and sometimes[1] even in the C++ subreddit (of all the places!). How is this not obnoxious? Here[2] is another person doing the same, but in a spicier tone. Linked lists and graphs are safe if you have an isoheap allocator (look at Fil-C). You can say that it is moral to endlessly reiterate the problems of unsafe languages, because it could lead to more secure software. But see the reply to my other comment by \"hyperbrainer\"[3] which says that Rust is \"completely\" memory safe, which is entirely wrong[4]. It is hard not to suspect the motives of those who claim to be concerned about memory safety. [1] -https://old.reddit.com/r/cpp/comments/1fu0y6n/when_a_backgro... [2] - https://news.ycombinator.com/item?id=32121622 [3] - I am unable to reply because of the depth. [4] - Rust requires unsafe to do a lot of things which can be done in safe code in a GC'd language. Thus, unsafe is pretty common in Rust than most GC'd languages. If a segfault can literally kill a person, it is absolutely immoral to choose Rust over Java (it does not matter that Rust \"feels\" safer than Java). reply tialaramex 4 hours agorootparentI really am interested in what Safer C++ proposes for [1], but I never found out. Your point [4] is very silly because you're assuming that while the unsafe code implementing a safe Rust interface might be flawed the code implementing a safe Java interface such as its garbage collector (which will often be C++) cannot be. As we'd expect, both these components are occasionally defective, having been made by error prone humans, such flaws are neither impossible nor common in either system. There are indeed even safer choices, and I've recommended them - but they're not Garbage Collected. > First and foremost, the phrase \"undefined behavior\" only applies to C and C++ because the specifications of those languages define it. Nope, those words have an ordinary meaning and are indeed used by Rust's own documentation, for example the Rustonomicon says at one point early on, \"No matter what, Safe Rust can't cause Undefined Behavior\". The purpose there is definitional, it's not a boast about how awesome Rust is, it's a claim that if there is Undefined Behaviour that's not because of the safe Rust, there's a soundness problem somewhere else. > Another example: signed overflow being UB is not a memory safety problem unless the value is used for array indexing This is wrong. Because Signed Overflow is UB the C++ compiler is allowed to just assume it will never happen, regardless of the systemic consequences. What that means is that other IR transformations will always be legal even if they wouldn't have been legal for any possible result of the overflow. This can and does destroy memory safety. Actually it would be weird if somehow the IR transformations always preserved memory safety, something they know nothing about, despite changing what the code does. reply rascul 5 hours agorootparentprev> The statement that Rust has no UB does not make sense because Rust has no specification, and all behavior is defined by the default implementation. It is in the reference. https://doc.rust-lang.org/reference/behavior-considered-unde... reply Ukv 4 hours agorootparentprev> The statement that Rust has no UB does not make sense because Rust has no specification I don't think it was claimed that Rust has no UB in this conversation, only IFNDR. From what I can tell, Rust does document a set of \"behavior considered undefined\" like using unsafe to access misaligned pointers. For practical concerns (\"could code optimization change these semantics?\", \"is this guaranteed to work the same on future compiler versions?\") it seems reasonable to me to call that undefined behavior, and to say that Rust doesn't have much of it. > I see the parent commenter doing this frequently and sometimes[1] even in the C++ subreddit (of all the places!). How is this not obnoxious? Both their comment here and their reddit comment look fine to me. Something like \"C++ sucks, switch to Rust!\" would be annoying, but specific relevant technical comparisons (\"In Rust for comparison the static growable array V isn't dropped when the main thread exits [...]\") seem constructive. > Rust requires unsafe to do a lot of things which can be done in safe code in a GC'd language. Thus, unsafe is pretty common in Rust than most GC'd languages. If a segfault can literally kill a person, it is absolutely immoral to choose Rust over Java (it does not matter that Rust \"feels\" safer than Java). Java does technically have the Unsafe class for low-level unsafe operation and JNI to interoperate with C/C++/assembly. I'd expect that the average Rust program makes more use of unsafe, but largely just because the average Rust program is lower-level (including, increasingly, parts of the Linux and Windows kernels). It's unclear to me whether the same program written in Java or Rust would ultimately prevent more bugs. reply tialaramex 10 hours agorootparentprev> Most (if not all) of your posts here on HN boil down to \"C/C++ bad, Rust good\". I haven't measured but it's easy to say categorically that it's not \"all\" unless somehow my posts about network protocols, aeroplanes, security and psychology among others fall into this vague category. And yes, like Ignaz Semmelweis I can see an obvious improvement to how my profession does what it does and it's infuriating that the response from many other practitioners is \"No, I don't like change, therefore you're crazy for explaining why I should change\" Ignaz Semmelweis died in an asylum. But on the other hand while Ignaz was correct and his proposals would have worked he couldn't explain why because germ theory was only confirmed after he died. Rust isn't in that situation, we know already exactly what the problems are with C++. So that means I can tell you not just that using C++ is a bad idea, but why it's a bad idea. reply googh 9 hours agorootparentMost HN users already use languages with GC which are more memory safe than Rust. People still use C++ either because they are maintaining existing code, or they work in domains where memory safety is not really necessary (games, HFT, ML). Apart from these, C++ is rarely used in the real world. > practitioners is \"No, I don't like change, therefore you're crazy for explaining why I should change\" Who exactly are you referring to here? Your co-workers? LLVM maintainers? or the Linux kernel developers? Please be more precise. reply hyperbrainer 7 hours agorootparentWhat is \"more memory-safe than rust\" supposed to mean? Rust is completely memory-safe. reply tialaramex 6 hours agorootparentOnly safe Rust can guarantee this, and only as a consequence of any unsafe Rust being correct. Most of the popular Garbage Collected languages of course also have a way to escape, in some cases via an \"unsafe\" keyword or magic unsafe package to a language where the same safety rules do not exist, in this sense the difference in Rust is that it's the same language. I'd actually say the more memory safe option would be a language like WUFFS where it pays a high price (generality) to deliver categorically better safety and performance. Most software could not be written in WUFFS but also most of the software which could be written in WUFFS isn't. reply CryZe 21 hours agorootparentprev> Maybe not yet, but it is heading in that direction About 95% of the unstable features lift limitations that most people expect not to be there in the first place. I'm not aware of all too many that aren't like that. reply faitswulff 17 hours agorootparentprev> Maybe not yet, but it is heading in that direction When people say that Rust is complex, they often neglect to differentiate between implementation complexity and developer facing complexity. The implementation complexity is growing in part to support the end user simplicity. I also don't understand why anyone feels the need to know every feature of the language. You can just learn about and use the features that you need. reply jiggawatts 7 hours agorootparentThat only works if you sit in isolation on a mountain and start with no_std and write everything else from scratch, yourself. The majority exist in a community and have to collaborate with others. They have to deal with the code written by others, code which may use any language feature. Every developer doing serious work will trip over every available language feature eventually. reply faitswulff 5 hours agorootparent> Every developer doing serious work will trip over every available language feature eventually. Steve Klabnik: “Just to provide another perspective: if you can write the programs you want to write, then all is good. You don't have to use every single tool in the standard library. I co-authored the Rust book. I have twelve years experience writing Rust code, and just over thirty years of experience writing software. I have written a macro_rules macro exactly one time, and that was 95% taking someone else's macro and modifying it. I have written one proc macro. I have used Box::leak once. I have never used Arc::downgrade. I've used Cow a handful of times. Don't stress yourself out. You're doing fine.“ https://www.reddit.com/r/rust/comments/1fofg43/i_am_struggli... reply dartos 4 hours agorootparentprev> Maybe not yet, but it is heading in that direction I have little doubt that Rust will end up being as complicated as C++ eventually, but a big difference is how explicit and well documented the discusson of new features are. The Rust RFCs provide a ton of context to almost every feature of the lagnuage. I find that historical context extremely helpful when trying to figure out why something is the way that it is. There may be something like that for C++, but I feel like a lot of it is \"you had to be there\" kind of reasons. reply s17n 21 hours agoparentprev> Fetishization of Efficient Memory Representation: ... I don't understand what the point is here. Some people care about avoiding heap allocations? They're a tool just like anything else The point is that dealing with the Rust borrow checker is a huge pain in the ass and for most Rust applications you would have been better off just using a garbage collected language. reply CryZe 21 hours agorootparent> huge pain in the ass Maybe if you structure your code weirdly? I haven't encountered a major borrow checker issue that I couldn't easily resolve in many years. reply delifue 12 hours agorootparentIt's not appropriate to say that \"having trouble with borrow checker means code is wrong\". Sometimes you just want to add a new feature and the borrow check force you to do a big refactor. See also: https://loglog.games/blog/leaving-rust-gamedev/ reply tinrab 10 hours agorootparentI hear this constantly but never see any examples of what they actually mean, or it's coming from misunderstandings of what the language is. I've seen people say how well the async code could be if Rust got a garbage collector. For the borrow checker specifically, I think it's important to understand smart pointers, Cell/RefCells, other primitives, and the fact that you don't have to solve everything with references. reply goku12 6 hours agorootparentprevThe advantage of satisfying the borrow checker isn't all that obvious. The BC is designed to make the program behave well with the fundamental machine model of a common computing device. You may be able to get away with spaghetti code for a new feature in a GC-based language. However my experience is that such technical debt grows over time and you're forced to carry out a massive refactor later anyways. GC isn't going to help you there. You might as well refactor the code in the beginning itself with the guidance of the BC in order to avoid pain in the end. This is why Rust programs have a reputation to run correctly almost always if they compile. And as the other commenter said, the borrow checker isn't all that hard to satisfy. BC complaints are often related to serious memory handling bugs. So if you know how to solve such bugs (which you need to know with C or C++ anyway), BC won't frustrate you. You may occasionally face some issues that are hard to solve under the constraints of the BC. But you can handle them by moving the safety checks from compile-time to runtime (using RefCell, Mutex, etc) or handle it manually (using unsafe) if you know what you're doing. Like the other commenter, I find some of the complaints about programming friction and refactor to be exaggerated very often. That's unfair towards Rust in that it hurts its adoption. reply iknowstuff 21 hours agorootparentprevI haven’t had to „deal with” the borrow checker since like 2018. It’s quite smart reply jkelleyrtp 21 hours agorootparentprevI mean, maybe? If you come into Rust thinking you're going to write doubly-linked lists all day and want to structure everything like that, you're going to have a bad time. But then in python you run into stuff like: ``` def func(list = []): list.append(1) ``` and list is actually a singleton. You want to pull your hair out since this is practically impossible to hunt down in a big codebase. Rust is just different, and instead of writing double-pointer code, you just use flat structures, `Copy` keys, and `loop {}` and move on with your life. reply Izkata 20 hours agorootparentFYI this site doesn't use ``` for code blocks, it uses indentation (two spaces). https://news.ycombinator.com/formatdoc reply mewpmewp2 16 hours agorootparentA bit off topic, but how do people usually write code here or on Reddit, I always find it to be really cumbersome to make sure there's two spaces etc in front of everything? Is there some formatting tool that I'm not aware of that everyone else uses? Because in both forums I keep coming back to edits, and it takes forever to edit some of the things, manually. I feel like I'm being stupid or the UX of all of that is just so terrible. reply steveklabnik 15 hours agorootparentI write it in play.rust-lang.org, then indent using the vim keybindings. I do this even for not rust code. reply satvikpendem 16 hours agorootparentprevI actually use a formatting tool online if on mobile, or just vim if on the computer, which can add two spaces in front of every line. reply Izkata 15 hours agorootparent> or just vim if on the computer I use Firefox + Tridactyl + the native extension, so with the cursor in any text field I can hit Ctrl+i and it pops up a gvim window with the contents of that text field. When you save+quit, it copies the contents back into the field. So glad someone figured out how to do this again once Vimperator died. reply pjmlp 5 hours agorootparentprevCode editor and then paste. reply fiedzia 21 hours agorootparentprev> this is practically impossible to hunt down in a big codebase use linters, they keep getting smarter reply n_plus_1_acc 20 hours agorootparentrustc is a good smart linter reply junon 5 hours agorootparentAnd if you're really wanting to be stringent, clippy exists for more subjective or more expensive lints. And it's also very good. reply s17n 17 hours agorootparentprevI have literally never used a doubly-linked list in my life, and I'm pretty sure that most programmers can say the same thing. As for the example... yeah, Python is pretty terrible (for writing production codebases, I think its a great language for short-lived, one-person projects). Interesting that you mention Python because if you're considering Python and Rust for the same use case that's pretty bonkers, for anything that you might possibly have used Python to do there are many more natural choices than Rust. If you wouldn't have done it in C/C++ ten years ago, you probably shouldn't be doing it in Rust today. reply pjmlp 5 hours agorootparentIf the programmers have a proper degree, they surely have used them, additionally they probably have done it inderectly, depending on what languages they use. reply johnnyanmac 19 hours agorootparentprevYeah. I wouldn't use Rust as a scripting language for that reason. But some critical applications want that enforced correctness and (hopefully) proper performance to be guaranteed if you pass the compiler. I want to eventually join the \"50 engines for every game\" race that is rust gsme engineer development, but I'm sure not going to have the fast iteration part of design be done in Rust. The renderer and all the managers should be absolutely solid, but some parts of games need you to break stuff quickly. reply s17n 1 hour agorootparentEnforced correctness is great. Manual memory management isn't appropriate for most applications (games of course are an example of where it is!) reply XorNot 19 hours agorootparentprevHaving just delved into Rust a little (and then given up and decided to learn Dart/Flutter for more practical applications development - I don't need another language to make command line tools in), this one I did feel while I was going through documentation. The problem is most of the important problems you deal with while programming require heap allocations: i.e. a lot of Rust advice is liable to lead you astray trying to find over-complicated solutions to optimizations you probably don't need up front. So in terms of systems programming, Rust is technically good here - these are all things you'd like to do on low level code. On the other hand if you're making a bunch of web requests and manipulating some big infrequently used data in memory...Box'ing everything with Arc is probably exactly what you should do, but everyone will tell you to try not to do it (and the issue is, if you're like me, you're coding in the \"figure out what and how to do it\" phase not the \"I have a design I will implement optimally\" phase). reply mondobe 19 hours agoparentprev> Rust really hasn't changed much in the past 6 years. Even more importantly than this, Rust has a major emphasis on backwards compatibility. The author mentions a \"hamster wheel\" of endless libraries, but, in Rust, nothing's forcing you to switch to a newer library, even if an old one is no longer maintained. In general, the complexity of your project is completely up to you, and (at least to me) it seems like a lot of the new features (e.g. generator syntax) are trending towards simplicity rather than complexity. reply seethishat 5 hours agoparentprev\"There are only two kinds of languages: the ones people complain about and the ones nobody uses.\" This is a famous quote from Bjarne Stroustrup in defense of C++. Source: https://www.stroustrup.com/quotes.html IMO, it's ironic to see Rust proponents using his quote in defense of Rust (and not crediting him). reply pjmlp 5 hours agoparentprevMaybe I suffer from brain damage due to knowing C++ since 1993, starting with Turbo C++ 1.0 for MS-DOS, however Rust is indeed getting into C++'s complexity. Lets not forget people tend to compare 40 year old language complexity, ampered by backwards compatibility and large scale industry deployment, with one that is around 10 years old, with lots of features still only available on nightly. The Unstable Book has an endless list of features that might land on Rust. reply CryZe 5 hours agorootparentOnly about 1/3 of those are language features. Out of those a ton are #[cfg] related (i.e. being able to more accurately doing conditional compilation), const (lifting limitations with compile time evaluation), documentation, architecture specific additions (like intrinsics) and co. I only see about like 30 or so that are actual proper language additions, some of which are just exploration without even an RFC either, leaving us with about 15 or so, which really isn't that bad. reply pjmlp 5 hours agorootparentEvery single of them needs brain space, regardless of the use. Also any language designer knows that every feature has exponential capacity due to the way it interacts with everything already in use, that is why innovation tokens are a thing in language design. reply spease 3 hours agorootparentI’ll try to keep this comment shorter. :) The thing about Rust abstractions is that they’re a lot more useful and forgiving than C++. Eg: In Rust, I cannot accidentally use an option incorrectly and have the program compile. When it fails to compile, there’s a good chance the compiler will suggest how I could do what I wanted to do. In C++, if I dereference an optional without checking it, I’ve triggered “undefined behavior”. The program will probably just segfault, but it could also return a bogus object of uninitialized memory, but technically it could overwrite my boot sector or call a SWAT team to raid my house, and still be in compliance with the C++ spec. Thus when considering code written in Rust, I mostly need to just consider the happy path. With C++ I need to pedantically check that they used the constructs correctly, and consider many more runtime paths due to how lax the language is. If I see someone dereference an optional without an if-guard, I now need to backtrack through the logic of that entire function to make sure the program doesn’t crash. If I see someone use a destructured value from an Option in Rust, I can rest easy that unless they used unwrap() somewhere, the compiler has done that for me. This scales well for larger abstractions, because if I’m not actively digging into some code I can treat it more as a black box where it interacts with the code I am working with, than as a box of firecrackers that might explode if I do something unexpected with it. reply pjmlp 3 hours agorootparentUse the C++ clippy version, plenty of variants to chose from. Which by way is a good point, even Rust needs its clippy, so not everything is so perfectly designed to make clippy superfluous. reply spease 52 minutes agorootparent> plenty of variants Aaaand you’ve lost me. I don’t want to waste my time either setting up multiple linters or having to drill down into the pros and cons of each. If the C++ community cannot even reach a consensus on which linter it endorses, I imagine it can’t reach a consensus on what it lints, which involves even more decisions. Secondly, both times I’ve tried to roll out or use a linter, I’ve encountered passive or active resistance from the other developers on the team. This resistance went deeper than the linter. On one team they didn’t want to use new language constructs from the last decade, on the other team they explicitly complained about me doing things differently than 15 years ago. In both cases they rejected what I understood to be the core C++ guidelines in favor of writing their own codebase-specific coding guidelines so they could pick and choose the constructs they understood rather than trying to adhere to what might be idiomatic for a particular edition. Unless something is 100% endorsed by the C++ community, it’s absolutely not something that I’m even going to try to champion. I’ve already been flat-out told “no one cares about your opinion” trying to explain how type-safety in C++ can improve readability in code reviews, which I thought was completely noncontroversial. To your second point, the point of linters is to guide code to be more idiomatic; it’s not an issue of language design, but of educating humans in mostly non-functional readability and best practices. reply spease 3 hours agorootparentprevRust is nowhere near C++’s complexity. And most of C++’s language complexity doesn’t come from “large scale industry deployment”, it comes from implementing a feature in a half-assed way, then updating it, but then the old feature needs to be kept around so all the libraries need to deal with two ways of doing things. Then something new and better comes along, and it needs to deal with three ways of doing things. Meanwhile, developers get frustrated with how difficult the abstractions are to use, and end up carving out their own codebase-specific coding standards. On Rust’s side, there’s 10x the emphasis on making things easy to use, so developers converge to consensus on pretty much the same modern style. Just look at project management. Before I even write a project, with C++ I’m hit with choosing between a barrage of build systems and package managers, none of them particularly good. Will I use cmake and Conan? Then I’m stuck writing several lines of boilerplate before I even get started in a weird non-imperative language. In Rust, I type cargo init and I’m ready to go. C++ has basically completely fallen down when it comes to language design, and from what I’ve seen, is simultaneously in denial that things are so horrible for its end users (Bjarne Stroustrup iirc putting out an essay where he claimed “C++ is fine for any project I’m concerned about”) and suddenly trying to rush to (badly) copy features from Rust, and only recently coming to the realization that it really needs to abandon some of its fundamental precepts to stop self-sabotaging by carrying around massive amounts of baggage that nobody should be using anymore. Meanwhile even the White House and other government agencies are saying “please use anything but C or C++”. Because ultimately, no one writes anything close to modern C++, and even modern C++’s memory safety guarantees are painfully minimal in exchange for massive amounts of code complexity (you still have to track every pointer lifetime yourself, and every safety abstraction is opt-in, so you still have to have the expertise to not cut yourself, and every codebase is unique and different in its conventions, which precludes running any kind of static analysis that could rival Rust without significant time investment). It’s just..really bad. The only way Rust will get there is if it falls prey to the same feature hoarding as C++ But Rust already has a deprecation-and-removal process for features, as well as an edition system to provide backwards compatibility for old code, and standardized tooling for linting that’s 10x better at telling you how to refactor than anything I’ve seen with C++. And god help you if you have an error in your code, because the C++ compiler will probably dump you out with a dozen irrelevant errors of dense template code you need to skip over, while with Rust you’ll make a mistake with lifetimes that will generate a text visualization of what you did wrong with the compiler pointing to what you need to change. Plus the Rust program will probably just run the first or second try, whereas the C++ program will segfault for the next ten minutes or so, because all the strictness in Rust means the complexity is more meaningful and less performative. Look at what people are learning in schools today and I bet they’re still starting with new and delete or even malloc and free rather than things like std::make_unique and std::span. The whole C++ ecosystem is basically predicated on nearly everyone having incomplete knowledge and doing something different and having to support or account for infinite combinations of features, whereas Rust has a higher barrier to entry but you can presume that pretty much everybody is using an idiomatic set of constructs for a given edition. Anyway, sorry for beating that over your head. I started with C++ around the time of Visual C++ 6, and to me it’s absolutely shameful at how bad the language has gotten. Whole ecosystems (C#, Java) and subsequent ecosystems (Go, Rust) have arisen in response to how bad people have it programming in C++, and despite two decades of people running away from C++ to create their own general-purpose programming language, so many proponents of the language seem to still be in denial. They’ve simply shifted the goalposts from C++ being the general-purpose programming language to it being a “systems” language to rationalize why the vast majority of developers don’t use it anymore because it refused to evolve. I see people these days comparing it to COBOL, that is, it’s not that anyone wants to use it for the merits of its language design, it simply has incumbency status that means it will be around for a long time. reply pjmlp 3 hours agorootparentEditions only cover syntax changes, and are hardly any different from compiler language switches when semantic and standard library changes are part of the picture. Lets see how complex Rust turns out to be, if it is still around after 30 years, to actually have fair comparison. We can also compare Rust in 2024, with the equivalent C++ version at 10 years old, when C++98 was still around the corner, C++ARM was the actual reference, and in that case the complexity fairs pretty equal across both languages. As for safety, as someone that is a hardcover believer in systems programming languages with automatic resource management, I would rather see C++ folks embrace security than rewrite the world from scratch. After all, Rust is in no rush to drop the hard dependency on LLVM and GCC infrastructure. reply spease 2 hours agorootparentThe big difference (inasmuch as I am aware) is in source compatibility in practice. If I use a header file, as any pre-C++-20 library will (have the major compilers implemented modules yet?), I am SOL. I am specifying a text-import of that library’s code into my code. You’d need an “extern C++11”. As for comparing them at 10 years old, apparent language size might be similar, but in terms of program complexity C++ would be DOA. You’re telling me it takes an equal amount of time to learn these languages, but with Rust I can write code that works on the first try, while with C++ I have to account for data races and memory mistakes at every level of my program? Why do I, a 90s programmer dealing with OSes without process separation and soon the dotcom boom, want to touch that with a 10-foot pole? Java and C# would not exist. There’d be far too little value proposition with an alternative to C++’s memory-unsafety to justify the development of a whole new language. You’d probably see the equivalent of Python and JavaScript (probably named RustScript following the logic of the time). There’d probably be a Go equivalent developed, ie “language that compiles fast and runs almost as fast as Rust that stresses language simplicity”. Language expressiveness and simplicity are at odds with each other and there are uses of both. To be fair, Rust was developed with the last 30 years of programming in mind. But the thing is, memory safety kept on being a central issue of the languages that followed. The next big design issue will probably have more to do with people trying to use LLMs as a first-class programming language. Eg something that’s easy for LLMs to write and humans to read. Or something to do with heterogenous computing resources all sharing the same “program”. However here Rust seems already positioned to do well with its compile-time tracking of asynchronous resource dependencies between threads of computation, and procedural macros that can invoke an external compiler. So I’m not sure that conventional language design is going to change the path it’s been on for the last 30 years until the human side of that interface starts to significantly change. Most of the language design considerations we’re discussing boil down to “make things manageable to humans with limited memory”. If cybernetic augmentation or bioengineering sharply expands those limits, I suppose it could change the direction. Otherwise it feels like things are going to naturally cluster around “complex correct thing” and “simple iterable thing” because those are the two strategies humans can use to deal with complexity beyond the scope they can keep in their head at once. reply csomar 12 hours agoparentprevI was going to write a rebuttal but then I read your comment and it mirrored roughly what I was going to write. > - Rust inserts Copy, Drop, Deref for you: it would be really annoying to write Rust if you had to call `.copy()` on every bool/int/char. A language like this exists, I'm sure, but this hasn't stopped Rust from taking off One improvement here could be the IDE. I don't want to write `let s: String` every time but the IDE (neovim LSP) does show it. It'd be good if I can get the full signature too. > Async is problematic Async Rust is by far the best Async out there. Now when I use other languages I am essentially wondering what the hell is going on there. Is there an executor? Is there a separate thread for this? How/When is this getting executed? Async Rust doesn't execute anything and as a result you can get an idea of how the flow of your program goes (as well as pick an executor of your choice, might not seem important if you are on the Tokio bandwagon but if you are in an envrionment where you need a custom executor and you need to control CPU threads, Rust Async suddenly makes a lot of sense) reply ggregoire 19 hours agoparentprev> I'm unsure why this article is so upvoted given how vapid the content is, but it does have a snappy title, I guess. rust, sqlite, htmx... there is a small list of techs that always get massively upvoted on hn, whatever the content or quality of the article. reply marcosdumay 19 hours agorootparentWell, if the entirely of that list is as awesome as those 3, then it's a good list to be. reply johnnyanmac 19 hours agoparentprev>it would be really annoying to write Rust if you had to call `.copy()` on every bool/int/char. A language like this exists, I'm sure, but this hasn't stopped Rust from taking off Well C++ does the same by default. You need to opt in for deep copies. C++ doesn't drop by default but modern practices like Smart pointers do. >I'm unsure why this article is so upvoted given how vapid the content is, but it does have a snappy title, I guess. Even HN isn't immune to the 90-9-1 rule. reply kedarkhand 3 hours agorootparentWhat is the 90-9-1 rule? reply spease 2 hours agorootparentI’m guessing 90% don’t care, 9% vote, 1% comment? reply throwawaymaths 21 hours agoparentprevI've seen a case where the rust panic handler is used in FFI and this creates a memory leak. reply pclmulqdq 18 hours agoparentprev> Rust has panics, and this is bad: ...okay? Nobody is writing panic handling code, it's not a form of error handling As far as I know, the issue with the panics is that things panic a lot. Times when C or C++ will limp along in a degraded state and log something for you to look at will cause your Rust program to crash. That turns things that are not problems into things that are problems. reply morning-coffee 17 hours agorootparentFirst, things don't panic a lot in my experience writing Rust for the past three years. Second, when things do panic, it indicates a defect in the code that needs to be fixed. Aborting the program with a stack dump is the perfect behavior for seeing the state and the invariant that was violated and then figuring out the fix. Contrast this to C or C++ \"limping along\", usually until a later invariant causes a crash and being further away from and obscuring the true root cause, and we see why C and C++ code is generally still so bug-ridden relatively speaking. Fail-fast is not just a buzz word and program bugs are not recoverable errors. See https://joeduffyblog.com/2016/02/07/the-error-model/ reply pclmulqdq 17 hours agorootparentThese arguments get said a lot and they are all fine in theory, but in practice, all code over a certain size has a tremendous number of latent bugs, even Rust code. At a certain scale, you are virtually guaranteed to be running in a degraded mode of some kind. If the consequences of those latent bugs are operational nightmares, that's a problem. Most people would rather be able to roll in at 10 am to debug a minor issue from logs and traces than get a page at 1 am with 1000 stack traces in it. reply jamincan 4 hours agorootparent> These arguments get said a lot and they are all fine in theory, but in practice, all code over a certain size has a tremendous number of latent bugs, even Rust code. Are you claiming this from direct experience, or do you have some data to back it up? Apparently the rollback rate for Rust code in Android is less than half that of C++. [1] 1. https://security.googleblog.com/2024/09/eliminating-memory-s... reply SkiFire13 7 hours agorootparentprevI wonder how you could realistically end up in those two situations for the same issue though. reply morning-coffee 17 hours agorootparentprevIf your goal is to converge on correctly functioning software, you know, for the benefit of the users of it, then fail-fast can help. If your goal is to optimize the sleep patterns of devops people and make changes to code without testing before releasing it to production, yeah... do what you need to do. :) reply pclmulqdq 16 hours agorootparentYou can have correctly-functioning software when parts of it are operating in a weird way. The complaint I have heard about Rust crashes is that the default behavior is to crash in any situation that could possibly be weird. By the way, the trade you're talking about is great for desktop software (especially for browsers), but server-side software at scale is a bit different. The borrow checker and all the Rust safety stuff is also completely orthogonal to most forms of testing. You don't get to do any less because your language protects you against a specific class of memory-related errors. reply spease 2 hours agorootparentprevThings should not panic a lot, and when I’m going over someone else’s Rust code I will discourage them to use panic at all if it’s socially appropriate. Inasmuch as I am aware, the correct usage of panic is “there is no way to recover from this without leaving the application in an undefined state”. Not “a file I expected to exist wasn’t there” or “the user gave bad input” or “I didn’t implement this feature yet”. But more like “a cosmic ray must have struck the machine or something, because this ought to be logically impossible.” Or pretty much, if you literally don’t see a mechanism in Rust that can pass an error upwards and the program also cannot continue execution without the result being affected, then you panic. That’s a little stricter than what I understand the official guidance is, but not much. If you have something panicking it should be less “I can’t see what’s going on” and more “thank god it stopped itself so it didn’t write bogus data to production.” reply beeflet 19 hours agoparentprev>Rust inserts Copy, Drop, Deref for you: it would be really annoying to write Rust if you had to call `.copy()` on every bool/int/char. A language like this exists, I'm sure, but this hasn't stopped Rust from taking off Is it possible to disable this behavior? I think it might be useful as a learning tool to familiarize myself with the Traits. reply steveklabnik 18 hours agorootparentIt is not. reply lovethevoid 21 hours agoparentprevThis was an oddly defensive and vapid comment. Mostly just handwaving away any views the article brings up, of which at least the article expands on their thoughts. This comment is just \"meh not a bad thing\" repeatedly. Why is this comment being upvoted? reply jkelleyrtp 20 hours agorootparentThe title is inflammatory and yet there are few nuanced takes in the article. It's weird to see it shoot to the top of HN. I think the loglog article is a much better, nuanced, full critique of Rust. https://loglog.games/blog/leaving-rust-gamedev/ The internet is just so full of negativity these days. People upvote titles but don't read articles. Reading about people's views on subjects is useful, but I don't think this one is. reply Shatnerz 19 hours agorootparentInflammatory? \"My Negative Views on X\" is pretty far from inflammatory. It is exactly what the post was, with some positivity sprinkled in as well. reply northhanover 7 hours agorootparentIt’s 2024. If I read some thing I don’t agree with on the internet it IS inflammatory. reply pessimizer 20 hours agorootparentprev> This was an oddly defensive and vapid comment. Even comparatively, next to your own comment? I have no specific idea of what you object to or why, but I have learned that you are upset. reply Ygg2 21 hours agoparentprev> Rust isn't as nice of community as people think It's a numbers game. As the number of people using Rust grows, so does the number of Jerks using Rust. And it's not like the Rust community is a stranger to bullying maintainers of crates for various things. > Async is problematic: Async Rust really is fine. It's... OK. It has a few issues, that hopefully will get fixed, like making Pin from a generic struct into a type of reference. e.g. instead of `Pin` you would write `&pin str`. There is also the coloring problem which is quite a hassle and people are looking into possible solutions. reply api 20 hours agoparentprevIMHO the biggest Rust async annoyance is exactly this: > Tokio's default choice of making `spawn` take Send/Sync futures ... combined with lack of structured concurrency. This means async tasks look like threads in every respect, causing you to end up using Arc and other concurrency constructs all over the place where they ought not be necessary. This harms efficiency and adds verbosity. reply khuey 19 hours agorootparentIt's not too hard to do tokio without Send/Sync futures. See the example in https://docs.rs/tokio/latest/tokio/task/struct.LocalSet.html... It's kind of annoying that the current_thread flavor of the executor doesn't automatically enter a LocalSet and make spawn_local work out of the box but it's easy enough to do at the beginning of your program. reply jyafffyasdfs 17 hours agorootparentprevI use Box::leak without shame. reply api 6 hours agorootparentI just coined a term for this: CRust. This is when your entire program is in unsafe {}. reply Animats 22 hours agoprevMy big problem with Rust is too much \"unsafe\" code. Every time I've had to debug a hard problem, it's been in unsafe code in someone else's crate. Or in something that was C underneath. I'm about 50,000 lines of Rust into a metaverse client, and my own code has zero \"unsafe\". I'm not even calling \"mem\", or transmuting anything. Yet this has both networking and graphics, and goes fast. I just do not see why people seem to use \"unsafe\" so much. Rust does need a better way to do backlinks. You can do it with Rc, RefCell, and Weak, but it involves run-time borrow checks that should never fail. Those should be checked at compile time. Detecting a double borrow is the same problem as detecting a double lock of a mutex by one thread, which is being worked on. reply Const-me 21 hours agoparent> I just do not see why people seem to use \"unsafe\" so much Because it’s impossible to implement any non-trivial data structures in safe Rust. Even Vec has unsafe code in the implementation to allocate heap memory. When you need efficient trees or graphs (I doubt any non-trivial software doesn’t need at least one of them), unsafe code is the only reasonable choice. C++ does pretty much the same under the hood, but that’s OK because the entire language is unsafe. C# has an unsafe subset of the language with more features, just like Rust. However, it runs inside a memory-safe garbage collected runtime. Even List and Dictionary data structures from the standard library are implemented with safe subset of the language. Efficient trees and graphs are also trivial to implement in safe C#, thanks to the GC. reply pcwalton 20 hours agorootparent> When you need efficient trees or graphs (I doubt any non-trivial software doesn’t need at least one of them), unsafe code is the only reasonable choice. To name one example, the AnimationGraph in Bevy is implemented with petgraph, which is built using adjacency lists, and doesn't use any unsafe code in any of the parts that we use. It is very high-performance, as animation evaluation has to be. reply Const-me 19 hours agorootparent> It is very high-performance, as animation evaluation has to be Are you sure evaluating these animations is performance critical? I doubt games have enough data to saturate a CPU core doing that. Screens only have 2-8 megapixels; animated objects need to be much larger than 1 pixel. If you animate bones for skeletal animation that’s still not much data to compute because real life people have less than 256 bones. You don’t need much more than that even if your models have fancy manually-animated clothes. reply refulgentis 18 hours agorootparent> Are you sure evaluating these animations is performance critical? Isn't this obviously true? A key part of UI work is avoiding \"jank\", which commonly refers to skipped frames. > I doubt games have enough data to saturate a CPU core doing that. Got a bit lost here: games? > Screens only have 2-8 megapixels. 4 bytes per pixel, 32 MB/frame. 120 frames / sec = 8 ms/frame. 3.84 GB/second. > animated objects need to be much larger than 1 pixel. Got lost again here. In general, I'm lost. First, there's a weak claim that all performant data structures in Rust must use unsafe code. I don't think the author meant all performant data structures must use unsafe code. I assume they meant \"a Rust data structure with unsafe code will outperform an equivalent Rust data structure with only safe code\" Then, someone mentions a 3D renderer, written in Rust, is using a data structure with only safe code. I don't understand how questioning if its truly performant, then arguing rendering 3D isn't that hard, is relevant. reply Const-me 10 hours agorootparent> Isn't this obviously true? To an extent sure, but we’re talking about low level micro-optimizations. Games don’t animate individual pixels. I don’t think animating 1000 things per frame gonna saturate a CPU core doing these computations, which means the code doing that is not actually performance critical. > Got a bit lost here: games? I searched the internets for “Bevy Engine” and found this web site https://bevyengine.org/ which says “game engine”. I wonder is there another Bevy unrelated to games? > 3.84 GB/second In modern games none of that bandwidth is processed on CPU. Games use GPU for that, which don’t run Rust. > there's a weak claim that all performant data structures in Rust must use unsafe code Weak claim? Look at the source code of data structures implemented by Rust standard library. You will find unsafe code everywhere. When you need custom data structures instead of merely using the standard ones you will have to do the same, because safe Rust is fundamentally limited in that regard. reply steveklabnik 6 hours agorootparent> Look at the source code of data structures implemented by Rust standard library. You will find unsafe code everywhere. This is survivorship bias: one of the criteria back in the day for “should this go in the standard library” was “is it a data structure that uses a lot of unsafe?” because it was understood that the folks in the project would understand unsafe Rust better than your average Rust programmer. These days, that isn’t as true anymore, but back then, things were different. reply refulgentis 1 hour agorootparentprev> I don’t think animating 1000 things per frame gonna saturate a CPU core doing these computations Oh, my sweet summer child. :) > In modern games none of that bandwidth is processed on CPU. Games use GPU for that, which don’t run Rust. So is your claim that OP is making up stuff about running code on the CPU because its a 3D engine? Also, why mention megapixels if you think it's irrelevant? :) > Weak claim? \"_all_ performant data structures in Rust _must_ use unsafe code\" is a long tail reading of the original comment. If that was the intent, it is a weak claim, because we can observe many memory-safe languages and runtimes and have performant data structures. (minecraft was written in Java, years and years ago!) > Look at the source code of data structures implemented by Rust standard library. This is the bailey, which was directly covered in the previous comment. The motte was \"all performant data structures in Rust must use unsafe code\" Here, the bailey, steelmanning as strongly as possible, is \"data structures with unsafe code are more performant than ones without\", which was directly said in the comment you are replying to. In addition to the swapping, this is a picture-perfect replication of the bomber with holes on it meme, as the other reply notes. reply Const-me 20 minutes agorootparent> OP is making up stuff about running code on the CPU because its a 3D engine? I’m not sure CPU-evaluated animations are often critical performance bottlenecks of game engines. > why mention megapixels if you think it's irrelevant? Count of screen-space pixels is a hard upper limit of count of simultaneously visible things on the screen. Animating occluded meshes is pointless. > many memory-safe languages and runtimes and have performant data structures Indeed, because people who designed these memory-safe languages wanted to support arbitrarily complicated data structures implemented using safe subset of these languages. Safe Rust supports arbitrarily complicated code, but requires unsafe to implement most non-trivial data structures. It’s possible to hide unsafe code deep inside standard library, possible to implement safe APIs over unsafe implementations, but still, for people who actually need to create custom data structures (as opposed to consuming libraries implemented by someone else) none of that is relevant. reply dartos 3 hours agorootparentprev> Because it’s impossible to implement any non-trivial data structures in safe Rust This is why I think rust is great for application programmers. Honestly high performance web servers, games, anything in that realm it's pretty good for. Low level systems and high performance data structures are better implemented with direct memory management, but the consumers of those libraries shouldn't have that same burden (as much as possible). Granted, I haven't worked with rust enough to run into issues with other people's unsafe code. reply jltsiren 20 hours agorootparentprevNon-trivial data structures are often just a bunch of arrays/maps with additional semantics. You may consider implementing/using unchecked versions of basic operations for a little bit of additional performance. If you implement (de)serialization yourself, you may need unsafe code. Sometimes the safety of deserialization may be a convenient lie, if checking the invariants fully would be too expensive. And sometimes you may want to expose internal helper functions for various purposes, which may have to be marked unsafe if they can break the invariants. Beyond that, you only need unsafe code for specific kinds of data structures. At least in my experience. reply Const-me 19 hours agorootparent> Non-trivial data structures are often just a bunch of arrays/maps with additional semantics This might be fine for code which consumes data structures implemented by other people. The approach is not good when you actually need to implement data structures in your program. In modern world this is especially bad for a low-level language (marketed as high performance, BTW) because the gap between memory latency and compute performance is now huge. You do need efficient data structures, which often implies developing custom ones for specific use cases. This is required to saturate CPU cores as opposed to stalling on RAM latency while chasing pointers, or on cache coherency protocol while incrementing/decrementing reference counters from multiple CPU cores. Interestingly, neither C++ nor C# has that boundary, for different reasons: C++ is completely unsafe, and safe C# supports almost all data structures (except really weird ones like XOR linked lists) because GC. reply SkiFire13 7 hours agorootparent> You do need efficient data structures, which often implies developing custom ones for specific use cases. You are assuming you can't do this with those vecs/maps. But you can! That's what the \"additional semantics\" are. They will be slightly slower due to often using indexes instead of raw pointer, which requires a bound check and an addition to get the pointer, and sometimes a reallocation, but they won't be that slow. Surely they will be faster than C#, which you claim can implement those same data structures efficiently. You also often get the benefit of better cache locality due to packing everything together, meaning it could even be faster. reply estebank 52 minutes agorootparent> They will be slightly slower due to often using indexes instead of raw pointer, which requires a bound check and an addition to get the pointer, and sometimes a reallocation, but they won't be that slow. This is not a fundamental requirement, though. Assuming arena-like behavior, the index will be constructed/provided by the arena itself, so the bounds check can be safely ellided by-construction. Reallocation cost of the entire arena could be expensive, but if that is a cost you'd want to ammortize down to a new allocation, the arena could be implemented as an extensible list of non-growable arenas: every time an arena is full, you append another. This can be an issue if you don't keep track of deletions/tombstones or can't afford a compaction step to keep memory usage down, but in practice having all of these requirements at once is not as common. reply jltsiren 19 hours agorootparentprevI used to think like that. Then C++11 arrived, and I realized I could get effectively the same performance with containers and move semantics, while spending less effort on writing and debugging the code. If you need an array for your custom data structure, a standard library vector is almost always good enough. Associative arrays are a bit more tricky, but you should be able to find a handful of map implementations that cover most of your needs. And when you need a custom one, you can often implement it on top of the standard library vector. reply Const-me 9 hours agorootparent> I could get effectively the same performance with containers and move semantics When I’m happy with the level of performance delivered by idiomatic C++ and standard collections, I tend to avoid C++ all together because I also proficient with C# which is even faster to write and debug. But sometimes I want more performance. An example from my day job is a multi-step numerical simulation which needs to handle grids of 200M nodes. When processing that amount of data, standard collections are suboptimal. I’m not using std::vector because I don’t need them to grow and I want to allocate these huge buffers bypassing C heap i.e. page-aligned blocks of memory zero initialized by the OS. reply jltsiren 6 hours agorootparentI've been using std::vector with arrays that take up to tens of gigabytes for ~15 years without any real issues. Once the size of the allocation exceeds a few megabytes, any reasonable allocator is going to use an anonymous memory map anyway. And if the array doesn't need to grow, it takes effort to make a standard library vector worse than a manually optimized one. It's just a pointer and two integers, after all. reply Const-me 4 hours agorootparent> it takes effort to make a standard library vector worse than a manually optimized one A simple use case is a 2D array where rows are padded to be multiples of 32 bytes (size of AVX SIMD vectors, saves a lot of implementation complexity because no need to handle remainders) or 64 bytes (saves a tiny bit of performance when parallelizing, guarantees cache lines aren’t shared between rows). When element size is not a power of 2, it’s impossible to implement that RAM layout on top of an std::vector reply almostgotcaught 9 hours agorootparentprev> proficient with C# which is even faster to write and debug. I don't get it - why are we ignoring the fact that C# necessarily implies distributing CLR? NativeAOT doesn't work for everything. reply pjmlp 5 hours agorootparentAs usual the typical argument that if it doesn't cover 100% of everything it isn't good enough. There are plenty of use cases where NativeAOT works perfectly fine, and is getting better with each .NET release. reply almostgotcaught 2 hours agorootparent> As usual the typical argument that if it doesn't cover 100% of everything it isn't good enough. This text > When I’m happy with the level of performance delivered by idiomatic C++ and standard collections, I tend to avoid C++ all together because I also proficient with C# which is even faster to write and debug. very strongly implies that they're completely interchangeable. They're not. It's as simple as that. Sometimes it's hard to keep track of whether I work in software or politics. reply neonsunset 2 hours agorootparent> very strongly implies that they're completely interchangeable Not completely but they are interchangeable. C# at its inception was as much inspired by C++ as it was by Java. Since then, it only further evolved to accommodate far more low-level scenarios and improve their performance. It provides the kind of capabilities you'd usually expect from C++, so this statement holds true. Calling C exports is one `static extern ...` method away, you can define explicit layout for structs (that satisfy 'unmanaged' constraint), you have fixed arrays in structs, stack buffers, pointers and ability to do raw or abstracted away manual memory management, etc. You can mmap device-shared memory and push data into GPU. You can accept pointers from C or C++ and wrap them into Spans and pass those to most standard library methods. reply almostgotcaught 1 hour agorootparent> Not completely but they are interchangeable ...I'll just ask you this extremely simple question: does C# compile/run/whatever-magic-you-think-it-does on targets that aren't in {x32, x86, ARM64}? Does it target RISC-V? Does it target PTX? Does it target AMDGPU? Are you getting the picture? Pre-empting the most low-brow dismissal: these are only niche targets if you've never heard of a company called NVIDIA. reply Const-me 1 hour agorootparent> on targets that aren't in {x32, x86, ARM64}? I have personally shipped embedded Linux software running on 32 bit ARMv7 SoC written mostly in C#. The product is long in production and we’re happy with the outcome. > Does it target RISC-V? Not sure if it’s ready for production, but see that article: https://www.phoronix.com/news/Microsoft-dotNET-RISC-V > Does it target PTX? According to nVidia, the answer is yes: https://developer.nvidia.com/blog/hybridizer-csharp/ > Does it target AMDGPU? Not sure, probably not. reply almostgotcaught 1 hour agorootparent> Not sure if it’s ready for production, but see that article: https://www.phoronix.com/news/Microsoft-dotNET-RISC-V > It's coming from a Samsung engineer, Dong-Heon Jung, who is involved with the .NET platform team and works on it as part of his role at Samsung. answer: no > According to nVidia, the answer is yes: https://developer.nvidia.com/blog/hybridizer-csharp/ > Dec 13, 2017 answer: no > Not sure, probably not. correct, the answer is no. again: this isn't politics, this is software, where the details actually matter. reply Const-me 3 minutes agorootparent> Dec 13, 2017 OK, here’s a newer project which does about the same thing i.e. compiles C# to PTX https://ilgpu.net/ BTW, it supports OpenCL backend in addition to CUDA. Const-me 8 hours agorootparentprevOn AMD64 Windows, .NET 8.0 runtime takes 31.7 MB to download, and 70.7 MB on disk. On ARM64 Linux, the numbers are 28.4 MB and 75.0 MB, respectively. For that amount of data, both internet bandwidth and disk space are rather cheap these days? reply ryandrake 17 hours agorootparentprevThe C++ Standard Library containers are very good and generic, and almost always the right tool to reach for. People are finally giving up on that notion that their own custom linked list is so much better than . My estimate is that out of all the C++ programmers, 1% of them think that the C++ standard containers are not appropriate for their application, and only 1% of that 1% is right, due to their extremely specialized application needs. reply bubaumba 20 hours agorootparentprev>> I just do not see why people seem to use \"unsafe\" so much >Because it’s impossible to implement any non-trivial data structures in safe Rust. Even Vec has unsafe code Hmm.. wasn't memory safety the main selling point for rust? If not the only. Now mix of two languages looks even worst than one complex. Especially taking into account that it can be paired with safe language from long list. Don't know what rust fans are thinking, but from outside it doesn't look very attractive. Definitely not enough to make a switch. Julia looked better at first, but turned out to be just a graveyard of abandoned academic projects. reply pcwalton 20 hours agorootparent> Now mix of two languages looks even worst than one complex. The point is that the vast majority of code doesn't have to be unsafe. Empirically, Rust code has far fewer memory safety problems than non-memory-safe languages. reply TheRoque 19 hours agorootparentprevMemory safety without gc is not the only reason people use rust. It's also nicer to use than C++ for multiple reasons (language features, included package manager, easy to integrate tests...) reply eddd-ddde 20 hours agorootparentprevYou fan wrap unsafe implementations with safe APIs. The point is there's an explicit boundary between unsafe an safe. reply okanat 19 hours agorootparentprevNope it isn't. You just aren't experienced in system programming. Working with hardware is unsafe since it has state that one cannot completely encapsulate in a single program. The entire specific design of a chip isn't available to programmer; only the machine code is. We usually don't know how a processor decides to cache things or switch to kernel permission level. Usually this isn't even the level we're at, OSes have private internals that change behind the programs and they are not accessible from user space. Pressing Ctrl+C to interrupt changes so many things in memory, it would be outright impossible to write programs that handle every single thing. The fundamental / syntactic promise of Rust is providing mechanisms to handle and encapsulate unsafety such that it is possible to construct a set of libraries that handle the unsafety in designated places. Therefore the rest of the program can be mathematically proven to be safe. Only the unsafe parts can be unsafe. Coming from Java or Go or Js or Python angle wouldn't be the same. Those languages don't come with mechanisms to let you to make system calls directly or handle the precise memory structure of the data which is necessary when one is communicating with hardware or the OS or just wants to have an acceptable amount of performance. In C++, the compiler can literally remove your code if you sum or multiply integers wrong or assume the char is signed/unsigned. There is no designated syntax that limits the places possible memory overflow error happen. The design of the language is such that some most trivial oversight can break your program silently and significantly. It is too broad so it is not possible to create a safe and mathematically proven and performant subset with the C and C++ syntax. It is possible with Rust. It is like the difference of chips that didn't have a hardware mechanism to switch between user and kernel mode so everything was simply \"all programs should behave well and no writes to other programs' memory pinky promise\". Rust doesn't leave this just as a possibility. Its standard library is mostly safe and one can already write completely safe and useful utilities with the standard library. The purpose of the standard library is provide you ways to avoid unsafe as much as possible. Of course more hardware access or extremely efficient implementations would require unsafe. However again, only the unsafe parts can cause safety bugs. They are much easier to find and debug compared to C++. People write libraries for encapsulating unsafe so there are even less places that use unsafe. If people are out of their C++ habit, reaching for the big unsafe stick way too often, then they are using Rust wrong. Whatever you do, there will be always a need for people and software that enables a certain hardware mode, multiply matrices fast, allocates a part of display for rendering a window etc. We can encapsulate the critical parts of those operations with unsafe and the rest of the business logic can be safe. reply Const-me 19 hours agorootparent> Those languages don't come with mechanisms to let you to make system calls directly or handle the precise memory structure of the data Here’s a C# library for Linux where I’m doing all these things https://github.com/Const-me/Vrmac/blob/master/VrmacVideo/Rea... As you see from that readme, the performance is pretty good too. reply beeflet 19 hours agorootparentprev> In C++, the compiler can literally remove your code if you sum or multiply integers wrong or assume the char is signed/unsigned. There is no designated syntax that limits the places possible memory overflow error happen. The design of the language is such that some most trivial oversight can break your program silently and significantly. Here is an interesting case of an optimization-triggered bug in Rust code I've heard of: https://www.youtube.com/watch?v=hBjQ3HqCfxs reply pcwalton 20 hours agoparentprev> I'm about 50,000 lines of Rust into a metaverse client, and my own code has zero \"unsafe\". I'm not even calling \"mem\", or transmuting anything. Yet this has both networking and graphics, and goes fast. I just do not see why people seem to use \"unsafe\" so much. I agree. I rarely ever use unsafe, and only as a last resort. Unsafe code is really not needed to achieve high performance. > Rust does need a better way to do backlinks. You can do it with Rc, RefCell, and Weak, but it involves run-time borrow checks that should never fail. I think this will basically turn into provably-correct data structures. Which is possible to do, and I've long thought there should be systems built on top of Rust to allow for proving these correct. But we should be clear that something like Idris is what we're signing up for. Whatever it is, it is assuredly going to be far more complex than the borrow check. We should basically only use such systems for the implementations of core data structures. reply Animats 13 hours agorootparent> I think this will basically turn into provably-correct data structures. That's kind of what I'm thinking. The basic idea is to prove that .upgrade().unwrap() and .borrow() never panic. This isn't all that much harder than what the borrow checker does. If you have the rule that the return value from those functions must stay within the scope where they are created, then what has to be proven is that no two such scopes for the same RefCell overlap. Sometimes this will be easy; sometimes it will be hard. A reasonable first cut would be to check that no such scopes overlap for a specific type, anywhere. That's rather conservative. If you can prove that, you don't need the checking. So it's an optimization. reply tmtvl 7 hours agoparentprevI don't use Rust and I've bounced off trying to learn it 3 times so far, so take this with a mountain of salt, but... I think that Rust having unsafe is fine as there will be edge cases where the compiler can't quite work out whether some code will work fine or not and where the programmer can vouch for it. That's no problem. The issue I have with it is that the unsafe block then gets kinda swallowed by the supposedly safe wrappers around it. And there's no clear auditable trail back to it. I find that a bit surprising as I'd expect it obvious to have some kind of 'uses unsafe' declaration (annotation? I don't know what Rust calls those statements with a number sign and square brackets in front of a subroutine) which would then propagate upwards. A bit like how in Java (the most elegant and refined of all programming languages) you can use an annotation to state that a function 'throws XYZException' which then needs to be propagated up to a point where it can get handled. Not having such a mechanism feels a bit icky to me. It's like if there's spiders crawling out of your ears it's useful to know that they're coming out of your ears so you don't have to wonder 'are those spiders creeping out of my ears? Or out of my nose? Or out of my eyelids?', which would be a bit inconvenient. reply Expurple 5 hours agorootparentThis annotation would be useless, because it would infect ~99% of your codebase. All IO is implemented as unsafe FFI calls or unsafe inline assembly somewhere down the stack. And even \"pure\" types like Box, Option, Vec have a lot of unsafe in their implementations. At some point these standard implementations just have to talk to an allocator which is implemented unsafely (obviously), or they have to construct a slice [1], etc. Would you make an exception for the standard library and why? I think, this just doesn't make sense. It's all unsafe somewhere down the stack. [1] https://doc.rust-lang.org/std/slice/fn.from_raw_parts.html reply pshc 21 hours agoparentprev> Rust does need a better way to do backlinks. You can do it with Rc, RefCell, and Weak, but it involves run-time borrow checks that should never fail. Those should be checked at compile time. It's not clear to me how rustc could detect a dangling backlink in a tree structure at compile time. Seems impossible short of adding proofs to the type system. reply o11c 18 hours agorootparentIf the language actually supported a full set of memory policies it would be quite possible. Unfortunately it only thinks about are unique, an opinionated form of borrowed ownership, and a little bit about shared (weak I think is punted entirely to the library?), and not all other ownership policies can effectively be implemented on top of them. The usual thing approach would be: given two types, P and C (which may be the same type in case of homogeneous trees, but this), with at least the following fields: class P: child1: ChildPointer[C] class C: parent: ParentPointer[P] Then `p.child1 = c` will transparently be transformed into something like: p.child1?.parent = null c?.parent?.child1 = null # only needed if splice-to-steal is permitted; may need iteration if multiple children p.child1 = c p.child1?.parent = p Note that ChildPointer might be come in unique-like or shared-like implementations. ParentPointer[T] is basically Optional[UnopinionatedlyBorrowed[T]]. A have a list of several other ownership policies that people actually want: https://gist.github.com/o11c/dee52f11428b3d70914c4ed5652d43f... reply SkiFire13 6 hours agorootparentIs there some research that shows these are actually safe and can be reasonably checked by a compiler? reply SkiFire13 7 hours agoparentprevIf you're doing graphics then you will have to use `unsafe` to interface with the OS primitives, there's no other way. Backlinks would be \"nice\" but they break fundamental assumptions that the borrow checker does. > Detecting a double borrow is the same problem as detecting a double lock of a mutex by one thread, which is being worked on. Is it being worked on using heuristics or formal methods? reply hyperbrainer 21 hours agoparentprev> I just do not see why people seem to use \"unsafe\" so much. SIMD seems to be a big one. reply Ygg2 22 hours agoparentprev> My big problem with Rust is too much \"unsafe\" code. I hear cargo-geiger is useful identifying such crates. > I just do not see why people seem to use \"unsafe\" so much. Because it's: A) fast (branchless access) B) fast (calling C libs or assembly) C) fast (SIMD) D) people think unsafe Rust is easier Want to write a JSON parser that will gobble up gigabytes per second? Your only way is removing as many branches and use assembly as much as possible. Doubly so on stable! I guess the same goes for making a \"blazingly fast\"™ graphical stack. People that think unsafe is easier, shouldn't be writing unsafe code. Writing unsafe code correctly is like juggling burning chainsaws. Saying that's easier than using chainsaws is moronic at best. EDIT: Consider following, if each of your unsafe {} blocks doesn't contain a // SAFETY: // Detailed explanation of invariants One of the chainsaws just cut off your leg. reply johnnyanmac 18 hours agorootparent>people think unsafe Rust is easier If that's their line of thought, I don't know why they simply don't use C/++. Or even C# with unsafe blocks. I know you said the same, but I wanted to reiterate that. But yes, I can see a few very specific cases where unsafe access is needed. Emphasis on \"few\". I think anything past some fundamentals should be at best a late optimizing step after an MVP is established. I also think, if it's not already there, that crates should be able to identify if it's \"safe\" or \"unsafe\". Same mentality where I'd probably want to rely on safe crates until I need to optimize a sector and then look into blazing fast but unsafe crates. reply ritcgab 21 hours agoparentprevIt's ugly but it's inevitable in some sense. The author should know what they are doing, and `// SAFETY:` comment is a must. reply fiedzia 21 hours agorootparentNo. Nobody is going read those (because most people won't even know that some unsafe is buried 5 layers of dependencies below what they work with). The author should make reasonable effort to prove the code is working correctly (and cannot be abused) by other means if possible. It might be a domain issue, so far all my apps are 100% safe (not counting libraries). reply nneonneo 18 hours agorootparentYou put those safety comments in two places: when you declare an API as being unsafe, and when you use an unsafe API. Not coincidentally, those are the two places in the language that force you to use the “unsafe” keyword. If you declare a safe API that uses unsafe under the hood, it’s on you to ensure that it’s safe to call under any situation, so that callers can call it without worrying that their program is suddenly unsafe. If you can’t guarantee that your API is safe under all circumstances, you need to declare it unsafe and make it the caller’s job to use it safely. reply n_plus_1_acc 20 hours agorootparentprevUnsoundness is considered a bug and should be reported and fixed. reply alfiedotwtf 17 hours agoparentprevI’ve been using Rust almost daily since 2015 and I’ve used unsafe twice - both when interoping with C. I don’t know what fancy things you’re doing with unsafe that you’re seeing it on a daily basis… maybe it’s a you problem reply Yujf 12 hours agorootparentIt is explicitly mentioned that ubsafe is in their dependencies not their own code. reply sebastos 18 hours agoprev\"X is as complex as C++\" is a preposterous statement for all values of X. A lot of people seem to assume that \"C++ is complex\" is referring to how the committee adds new language features every 3 years. The conventional wisdom that C++ is wickedly difficult to learn is NOT about \"oh man, now I need to learn about the spaceship operator?\" C++ is an almost unfathomably bottomless pit. From the arcane template metaprogramming system to the sprawling byzantine rules that govern what members a compiler auto-generates for you, and on to the insane mapping between \"what this keyword was originally introduced for\" and \"what it actually means here in _this_ context, there is no end to it. Keeping up with new language syntax features is an absolute drop in the bucket compared to the inherent complexity required to understand a C++11 codebase, build it (with a completely separate tool that you must choose) and manage its dependencies (with a yet different completely separate tool that you must choose). You don't have to know anything about Rust to know that saying \"Rust has become complex as C++\" is objectively incorrect. reply pjmlp 5 hours agoparentDepends, how much from unstable Rust ends up in Rust during the next 30 years, to match where C++ is today after 40 years of production deployments across the industry. reply christianqchung 17 hours agoparentprevYeah, but in 12 years when reflection is in the language and is widely used, people will probably have to deal with a significantly increased amount of weirdness as a result of that feature. Though all that's to say the complexity gap between rust and C++ is only going to go up. reply hyperbrainer 22 hours agoprevNeeds (2023) > I predict that tracing garbage collectors will become popular in Rust eventually. The use of Rc is already very widespread in projects when people don't want to deal with the borrow checker and only want to use the ML-like features of Rust (Sum types, Option, Error etc.) > Rust has arrived at the complexity of Haskell and C++, each year requiring more knowledge to keep up with the latest and greatest. I wonder when we will see the rise of Haskell like LanguageExtensions in Rust. AFAIK, pretty much everybody uses things like GADT, PolyKinds, OverloadedStrings etc. The most similar thing I can think of Rust right now for is python-like decorator application of things like builder macros using Bon. > Async is highly problematic Agreed. Tokyo is the only reason, I think, anybody is able to use Rust for this stuff. reply tptacek 22 hours agoparentDoes Rc really resolve the core problem this post is talking about, which is that it's really painful to naturally express tree and graph structures in Rust? It feels like I mostly see people talking about building application-layer pointer systems with integers, which would be surprising if (in a single thread, perhaps) you could just Rc your way around the problem. reply umanwizard 21 hours agorootparent> Does Rc really resolve the core problem this post is talking about, which is that it's really painful to naturally express tree and graph structures in Rust No, it doesn't. If you naively express graphs containing cycles with `Rc` you will leak memory, just like you would with `std::shared_ptr` in C++. reply ordu 21 hours agorootparentprev> Does Rc really resolve the core problem this post is talking about, which is that it's really painful to naturally express tree and graph structures in Rust? No, but Gc will not resolve the core problem either. The core problem is that rust forbids two mutable pointers into one chunk of memory. If your tree needs backlinks from child nodes to parents, then you are out of luck. reply tptacek 19 hours agorootparentIn what way am I \"out of luck\"? It's trivial to express a tree, including one with backlinks, in Java. reply jcranmer 18 hours agorootparentRust's mutability rules specifically screw you over here (you can't have two mutable references to the same object, ever); most languages (including Java) don't have those rules. I sometimes wish I could have a mode of Rust where I had to satisfy the lifetime rules but not the at-most-one-mutable-reference-to-an-object rule. reply ordu 18 hours agorootparentprevJava doesn't enforce the rule \"mutable XOR shared\". But if you have a link \"child\" in the parent node, and a link \"parent\" in the child node, then parent.child.parent == parent, and compiler cannot know it. So Rust as the language makes it impossible to do with &-pointers, while standard library of Rust allows it to do with combination of Option, Rc, RefCell but it is really ugly (people above says it is impossible, but I believe it is just ugly in all ways). Like this: type NodeRef = Rc>; struct Node { parent: Option, left: Option, right: Option } So the real type of `parent` field is Option>>. I hate it when it comes to that. But the ugliness is not the only issue. Now any attempt to access parent or child node will go through 2 runtime checks: Option need to check that there is Some reference or just None, and RefCell needs to check that the invariant mut^shared will not be broken. And all this checks must be handled, so your code will probably have a lot of unwraps or ? which worsens the ugliness problem. And yeah, with Rc you need to watch for memory leaks. You need to break all cycles before you allow destructors to run. If I need to write a tree in rust, I'll use raw-pointers and unsafe, and let allergic to unsafe rustaceans say what they like, I just don't care. reply okanat 19 hours agorootparentprevand then your gc will leak it. Rust programs are not only safe but CPU and memory efficient. reply truetraveller 19 hours agorootparentI simple standard mark-and-sweep GC will not \"leak it\". reply hyperbrainer 21 hours agorootparentprevConsidering that there exists a book about building linked lists in Rust[0], I am going to go ahead and say \"Unclear\" That does not matter though. It is easier (though verbose and often unidiomatic), and hence Rc has become really popular, especially with beginners. [0] https://rust-unofficial.github.io/too-many-lists/ reply pcwalton 20 hours agorootparentprevRc does solve the problem, but it often introduces interior mutability, which ends up causing usability problems. That's why at the end of the day adjacency representations (i.e. integers) are often preferred. reply cmrdporcupine 22 hours agorootparentprevSure, Rc/Arc absolutely solves this problem. It's not super idiomatic to go crazy with using it like that, but it's possible/acceptable. Using SlotMap and integer ids, etc. doesn't I think offer any advantage. reply tptacek 21 hours agorootparentI feel pretty comfortable with Rc and Arc, read the \"too many lists\" book, &c. and feel like it is not actually simple to model trees with Rc? What am I missing? I'd love to be convinced I'm wrong about this (I want to like Rust more than I do). reply cmrdporcupine 21 hours agorootparentA tree of Rc/Arc is a tree of references, and is really no different than a Java or Python reference value, except that you'll have to do explicit .clone()s Is it mutability that's tripping you up? Because that's the only gotcha I can think of. Yes, you won't get mutability of the content of those references unless you stick a RefCell or a Mutex inside them. reply tptacek 19 hours agorootparentYes! Mutability is what's tripping me up! That is not a minor detail! reply cmrdporcupine 17 hours agorootparentYou can get something like what you're used to a \"traditional\" language without compiler safeguards by using RefCell and .borrow_mut() on it. That will let you get past the compile-time borrow checks but will do runtime borrow checking and throw panic if more than one borrow happens at runtime. It's verbose, but it's explicit, at least. So: struct Node { parent: Rc>, left: Option>>, right Option>>, } and just off the top of my head it'd be something like { let my_parent = my_node.parent.borrow_mut(); ... do stuff with my_parent ... } ... my_parent drops out of scope here, now others can borrow ... etc. Haven't tried this in compiler my memory might not be right here. reply tptacek 17 hours agorootparentI do know that it's possible, but when people complain about this --- as with this tweet, from a PL theorist: https://x.com/LParreaux/status/1839706950688555086 ... this is what they're talking about. (I know the tweet is about the \"idiomatic\" answer to this problem, which is to replace references with indices into flat data structures). reply cmrdporcupine 7 hours agorootparentI don't have a twitter account and don't read links to paywalled services there, so don't know what the complaint is. Ergonomics aren't great for this type of problem, but it's something I almost never run into. Feels like a cooked up example. I've written tree data structures, etc. and never had much issue. ASTs for compilers, no particular drama. Rust is just making you consider whether you really want to do this, is all. reply nine_k 21 hours agorootparentprevDoesn't SlotMap save RAM and pointer dereferences? reply cmrdporcupine 21 hours agorootparentWhat is a slotmap lookup... if not a pointer dereference, or at least a dereference out of a vector likely on heap... so probably a pointer...? reply nicce 21 hours agoparentprev> Agreed. Tokyo is the only reason, I think, anybody is able to use Rust for this stuff. A lot of problems related to Tokyo can be avoided if you think your code as structured concurrency and avoid using Tokio::spawn. However, too often this is not possible. reply written-beyond 20 hours agorootparentI haven't, yet, run into building rust apps that require highly complex async implementations with lifetimes etc. however excluding those situations I've found it very straightforward and easy to use. I've built some applications with a lot of moving parts, mpsc has always been a life saver. reply hyperbrainer 21 hours agorootparentprevI don't have too much experience with async, but I have noticed a similar pattern. Maybe you are right. reply bsder 20 hours agoparentprev> The use of Rc is already very widespread in projects when people don't want to deal with the borrow checker and only want to use the ML-like features of Rust (Sum types, Option, Error etc.) And the fact that this hasn't caused alarm is kind of an issue. The problem with that is Reference Counting is WAY slower than good Garbage Collectors on modern CPUs. Reference Counting breaks locality, hammers caches and is every bit as non-deterministic as a garbage collector. reply umanwizard 22 hours agoprev> Distinguishing mutability has its advantages I think it's misleading to say that Rust distinguishes mutability. It distinguishes _exclusivity_. If you hold a reference to something, you are guaranteed that nothing else holds an exclusive reference to it (which is spelled &mut). You are _not_ guaranteed that nothing accessible through that reference can change (for example, it might contain a RefCell or other interior-mutable type). Shared references (spelled &) usually imply immutability, but not always. On the other hand, if you hold an exclusive reference to something, you are guaranteed that nothing, anywhere, holds any kind of reference to it. IMO, the fact that exclusive references are spelled \"&mut\", and often colloquially called \"mutable references\", was a pedagogical mistake that we're unfortunately now stuck with. reply beeflet 19 hours agoparent>IMO, the fact that exclusive references are spelled \"&mut\", and often colloquially called \"mutable references\", was a pedagogical mistake that we're unfortunately now stuck with. You couldn't just roll out a change like alias &e to &mut and &s to &, and then have a compiler warning for using the old &mut or &? reply umanwizard 18 hours agorootparentThat’s technically possible, yes, but I really doubt there is any appetite to change basic syntax like that when it’s already so ingrained in everyone’s mind. reply lll-o-lll 22 hours agoprev> People waste time on trivialities that will never make a difference. This is an aha moment as I read it. The complexity of your tools must be paid back by the value they give to the business you’re in. reply johnnyanmac 18 hours agoparentReally depends on the \"difference\". If a male a container crate that is Fer from shippable but gives me knowledge to land a Rust gig, was it a triviality? reply ilrwbwrkhv 18 hours agoparentprevThis is daft logic. Your business itself has no value in the modern world. Your todo saas app is not needed. Of course this argument is faulty, so going one step back, what is valuable is in the eyes of the hacker. For a lot of people, including mine, the triviality is also part of the value chain. reply pcwalton 19 hours agoprevI actually used to agree that Rust generally wasn't good for high-level application code, but working with Bevy has made me change that opinion for certain domains. I simply haven't seen a system that makes automatically parallelizing all application logic (game logic, in this case) feasible, other than Bevy and other Rust-based systems. The trick is that the borrow check gives the scheduler enough information to automatically safely determine which systems can run in parallel, and that's very powerful. It's not that you couldn't do this in C# or whatever--it's that you won't without a system that helps express your intent and enforces that those declarations are up to date. For applications that don't need high performance for the CPU code and aren't systems code, sure, Rust may not be a good choice. I'm not writing the server-side code for low traffic Web sites in Rust either. reply tptacek 18 hours agoparentJust so we're clear, your reference points for \"good for high-level application code\" are systems code and game engines? :) reply pcwalton 18 hours agorootparentGames themselves, not game engines. Systems code, game engines, and games. This isn't meant to be an exhaustive list--it's just the domains I have experience with that Rust was a good fit for. Lest it seem like I'm saying Rust is a good fit for everything I've done, I also worked on Firefox where the UI was JavaScript, and I wouldn't hurry to rewrite that code in Rust. Nor would I want the throwaway stuff I write in Python or Ruby to be Rust. reply tptacek 18 hours agorootparentI'm just saying, games are exactly one of the things I would assume Rust would be a natural fit for. :) reply hollerith 15 hours agorootparent>Rust would be a natural fit for Did you leave out a \"not\" here? reply tptacek 14 hours agorootparentNo, I don't think so? I would have predicted games as a sweet spot for Rust. Most significant game projects are done in C++, and I look at Rust as basically obsoleting C++. reply hollerith 14 hours agorootparentOK. Thanks. My guess was that since almost no one will pay more for a game's having fewer security vulns, there is less benefit to incurring the expense of Rust (takes longer to learn, development speed is slightly less) reply Yujf 12 hours agorootparentIts not just vulnerabilities. In theory you should also get more stability. For example I like to play Civ with a friend, but stopped because about once every 30 minutes one of us would have their game crash. If it was written in Rust, I assume it might be more stable. reply hollerith 5 hours agorootparentThanks. Another consideration that favors Rust, at least for game engines, is https://news.ycombinator.com/item?id=41793725 reply tptacek 13 hours agorootparentprevWhen we talk about Rust being a reasonable choice for things like (say) a CRUD app, memory safety is already table stakes; every mainstream high-level language is memory-safe. Why would you ever pick Rust over Java? The answer to that question will be an even better answer to why you'd write a game in Rust. reply drogus 19 hours agoprevIt's funny when people mention Go as the gold standard of not adding features to the language and Rust as ever-changing when Rust hasn't introduced any major changes in at least 3-4 years and Go introduced a major paradigm shift in how people structure their code (generics). Before you start replying with \"Rust introduced X\" - ask yourself - is X extending an existing feature slightly or does it introduce an entirely new concept? reply senorrib 19 hours agoparentGenerics is hardly a major paradigm shift, and these two languages are worlds apart in amount of features. reply kunley 7 hours agoparentprevI think the author rather refers to \"tamagotchi tooling\" and constant adapting libraries to new trends. It was not that much about language changes per se reply ilrwbwrkhv 18 hours agoparentprevYa the article is a bit fluffy and insubstantial. Surprising it's been voted up so highly on HN. Are people just going off the title? reply dang 22 hours agoprevRelated: My negative views on Rust - https://news.ycombinator.com/item?id=29659056 - Dec 2021 (89 comments) reply jeffreyrogers 20 hours agoprevI learned to program around the peak of object oriented fetishization. Shortly after that came functional programming's moment, and now it seems we are slightly past Rust and other safety focused languages' peak period of hype. All three language families have useful things to offer, but were never the panacea their proponents claimed them to be. \"No Silver Bullet\" continues to be true. reply lacker 21 hours agoprevI like Rust but at the same time I agree with the points here. These things are indeed problems with Rust. Nevertheless, C++ has even worse problems. When your alternative is using C++, that's the time to consider Rust. reply rich_sasha 21 hours agoprevI think to some extent Rust is a victim of its own unreasonable effectiveness. It is great at its narrow niche of memory safe low level programming, but frankly pretty good at lots of other things. But at these other applications some of its design principles get in the way - like the pedantic borrow checker. Languages not used outside their niches don't tend to collect such criticism. Python is a bit like that. It is a top choice for a few things, and ends up used effectively for other things, simply because it is versatile. But then people run into issues with dynamic typing, zero static safety, start slapping type annotations everywhere... and bemoan Python as a bit of a horror show. My use case for Rust is complementing Python, where frankly I don't care about half the complex features, still it's nicer than C or C++ to code in. The complexities of the borrow checker are more like a tax to me. I understand people who are frustrated with it thought, as otherwise they see it as a bit of a perfect language. reply ilrwbwrkhv 18 hours agoparentPeople keep talking about the borrow checker. I have written over 70k lines in Rust so far and it hasn't really been that big of an issue. There are large-ish projects like Zed and Zellij in Rust which seem to be pretty straightforward code as well. I feel it's a bit overblown the who",
    "originSummary": [
      "The author acknowledges Rust's strengths, such as macros, type-classes, and pattern matching, but expresses concerns about its complexity and certain features like \"unsafe\" and \"panic\" that can lead to confusing compile errors.",
      "Rust's focus on efficient memory representation and the lack of a built-in asynchronous runtime are highlighted as challenges, with the author skeptical about its use as a general-purpose language.",
      "Despite Rust's excellent tooling, the author is cautious about investing in it for personal projects, though they might consider it for single-threaded C replacements."
    ],
    "commentSummary": [
      "The discussion centers on Rust's complexity, with some comparing it to C++ and others emphasizing its stability and safety features.- Key concerns include Rust's panic handling, asynchronous programming challenges, and the use of \"unsafe\" code, which can bypass Rust's safety guarantees.- Despite criticisms, Rust is valued for its memory safety and is particularly beneficial in applications like game development, though it is noted for having a steep learning curve."
    ],
    "points": 181,
    "commentCount": 256,
    "retryCount": 0,
    "time": 1728502452
  },
  {
    "id": 41800602,
    "title": "DeskPad – A virtual monitor for screen sharing",
    "originLink": "https://github.com/Stengo/DeskPad",
    "originBody": "DeskPad A virtual monitor for screen sharing Certain workflows require sharing the entire screen (usually due to switching through multiple applications), but if the presenter has a much larger display than the audience it can be hard to see what is happening. DeskPad creates a virtual display that is mirrored within its application window so that you can create a dedicated, easily shareable workspace. Installation You can either download the latest release binary or install via Homebrew by calling brew install deskpad. Usage DeskPad behaves like any other display. Launching the app is equivalent to plugging in a monitor, so macOS will take care of properly arranging your windows to their previous configuration. You can change the display resolution through the system preferences and the application window will adjust accordingly. Whenever you move your mouse cursor to the virtual display, DeskPad will highlight its title bar in blue and move the application window to the front to let you know where you are.",
    "commentLink": "https://news.ycombinator.com/item?id=41800602",
    "commentBody": "DeskPad – A virtual monitor for screen sharing (github.com/stengo)178 points by geerlingguy 2 hours agohidepastfavorite24 comments neilv 1 hour agoThe title could clarify it's for MacOS X. reply neallindsay 55 minutes agoparentLinux users probably already have some weird workflow with X11 virtual buffers to do this. reply marcodiego 1 minute agorootparentXnest is probably enough. I've used it for similar purposes a few times. Don't know the equivalent for Wayland though. reply Devorlon 18 minutes agorootparentprevIt's not exactly the same, but as an alternative to what jauntywundrkind you can use V4L2-Loopback and OBS to create a virtual webcam and use that to share your screen. I find it really handy being able to switch between either just my cam, my desktop or both. reply jauntywundrkind 47 minutes agorootparentprevYeah. On Wlroots or Sway, we can setup virtual displays pretty easily (swaymsg create_output, done). Run wayvnc, and both the other person and yourself connect over vnc or rdp to see what's over there. Available May 2020, https://github.com/any1/wayvnc/issues/7#issuecomment-6256611... reply rcarmo 1 hour agoprevThis is _genius_. I have been using RegionToShare in Windows to share only a section of a widescreen monitor, but didn't have a good Mac equivalent. Now I have something that may well work _just as well_ with Windows inside Parallels (need to try that ASAP, am on the \"wrong\" Mac now). Edit: A quick test shows that yes, the Windows VM sees the additional display just fine--but, alas, Parallels doesn't let me pass _just_ one physical and that virtual display to the VM, so I can't have my \"personal\" portrait monitor unoccupied by Windows... reply rr60 23 minutes agoparent+1 for RegionToShare on windows. It's not perfect but it has made sharing on a 49\" monitor much much easier. reply garysahota93 1 hour agoprevI really like this concept. especially for the use case where I need to share my whole screen, but just want a \"sandbox\" of sorts to share. Typically have gotten around this with a secondary monitor that I share with, but that doesn't work when I'm on the go with my laptop. Will def be using this reply jakelsaunders94 16 minutes agoprevOh lawd I’ve had to say ‘sorry you’ll have to bear with my ultrawide’ during pairing at least 10 times in the last week. You are a lifesaver. reply imzadi 48 minutes agoprevI need this. I have a 49\" monitor and sharing the screen is such a pita reply mmastrac 1 hour agoprevVery cool. Does it require the \"screen recording\" indication to be up the entire time whether screen recording is happening or not? I don't see any info in the repo but I recall some previous solutions would effectively appear to be recording all the time. EDIT: unfortunately it does. But if it's designed for screen sharing, it's probably not a big deal. Unfortunately there's no easy way to mirror on OSX without this, AFAIK. This particular issue is annoying for certain USB-C video adapters that create a virtual screen and mirror it over an arbitrary protocol. reply benjonesutah 19 minutes agoprevHere is a related project I use to share selected content (usually single windows and my iPad) on a projector while teaching: https://github.com/benjones/presenterMode/ reply shmoogy 26 minutes agoprevThank you for this - sharing a window makes drop downs and other things not work. I look forward to trying this out for a better solution. reply ekinertac 1 hour agoprevwhat a briliant idea, most of my meeting i had to share my 4K screen with laptop pals and most of the time i had to zoom so they can see. now it's solved. reply _6GoofyWizard9_ 1 hour agoprevVery nice idea! It would be nice to be able to do the same on Linux and/or Windows, too! reply mcphage 1 hour agoprevAs someone with an ultrawide monitor, this seems like a really neat solution. Thanks for sharing it! reply leptons 49 minutes agoprevWith my 6480 x 3840 (three 4k screens) desktop resolution, in Zoom I just select \"Share a portion of screen\", and I can resize the area that gets shared to something close to a common screen size. reply xahrepap 8 minutes agoparentI used that until we moved to Teams for all video calls. And it doesn’t have that feature :( I’ve looked around for an app like this. But they’re all paid and the security prompts are a little scary. reply delusional 1 hour agoprevWhat an intriguing idea. I wonder if I could do something similar on linux by placing a second monitor on top of my current one with xrandr. reply sadjad 1 hour agoparentOn Linux, you can try Xephyr (https://www.freedesktop.org/wiki/Software/Xephyr/, https://wiki.archlinux.org/title/Xephyr). It's not as nice as DeskPad, but you can basically achieve the same thing. reply nixosbestos 1 hour agoparentprevIt's too easy to just use OBS for this, in my opinion. Add the pipewire display capture, add a filter to crop it to a corner, stretch that container to fit the stage, open the window in the corner. Fairly simple. reply jeanlucas 45 minutes agoprevSo neat! reply transfire 1 hour agoprev [–] Wouldn’t it be nice if we could adjust resolution per window? On a 4k monitor some applications have tiny text and icons, and no way adjust that I can find. reply rcarmo 52 minutes agoparent [–] I just went to Preferences and set the resolution of the virtual display to 1920x1080. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DeskPad is a virtual monitor application designed for seamless full-screen screen sharing, creating a mirrored display within its app window for easy sharing.",
      "Installation options include downloading the latest release or using Homebrew, a package manager for macOS, with the command 'brew install deskpad'.",
      "DeskPad operates like a physical monitor, allowing users to adjust display resolution in system preferences, and highlights its title bar in blue when the cursor moves to the virtual display."
    ],
    "commentSummary": [
      "DeskPad is a virtual monitor tool for MacOS X that facilitates screen sharing by allowing users to share specific areas of their screen, particularly beneficial for large or ultrawide monitors.",
      "It is praised for its convenience, especially for users who need to share screens while traveling or using a single monitor setup.",
      "While some users have suggested the addition of screen recording indicators, the tool is generally regarded as a valuable solution for screen sharing needs."
    ],
    "points": 178,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1728578214
  },
  {
    "id": 41796030,
    "title": "Mozilla fixes Firefox zero-day actively exploited in attacks",
    "originLink": "https://www.bleepingcomputer.com/news/security/mozilla-fixes-firefox-zero-day-actively-exploited-in-attacks/",
    "originBody": "Mozilla fixes Firefox zero-day actively exploited in attacks{ \"@context\": \"https://schema.org\", \"@type\": \"NewsArticle\", \"url\": \"https://www.bleepingcomputer.com/news/security/mozilla-fixes-firefox-zero-day-actively-exploited-in-attacks/\", \"headline\": \"Mozilla fixes Firefox zero-day actively exploited in attacks\", \"name\": \"Mozilla fixes Firefox zero-day actively exploited in attacks\", \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"id\": \"https://www.bleepingcomputer.com/news/security/mozilla-fixes-firefox-zero-day-actively-exploited-in-attacks/\" }, \"description\": \"Mozilla has issued an emergency security update for the Firefox browser to address a critical use-after-free vulnerability that is currently exploited in attacks.\", \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://www.bleepstatic.com/content/hl-images/2021/10/06/Firefox_headpic.jpg\", \"width\": 1600, \"height\": 900 }, \"author\": { \"@type\": \"Person\", \"name\": \"Bill Toulas\", \"url\": \"https://www.bleepingcomputer.com/author/bill-toulas/\" }, \"keywords\": [\"Actively Exploited\",\"CVE-2024-9680\",\"Firefox\",\"Use After Free\",\"Vulnerability\",\"Zero-Day\",\"Security\",\"InfoSec, Computer Security\"], \"datePublished\": \"2024-10-09T13:34:56-04:00\", \"dateModified\": \"2024-10-09T13:34:56-04:00\", \"publisher\": { \"@type\": \"Organization\", \"name\": \"BleepingComputer\", \"url\": \"https://www.bleepingcomputer.com/\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://www.bleepstatic.com/logos/bleepingcomputer-logo.png\", \"width\": 700, \"height\": 700 } } }!function(n){if(!window.cnx){window.cnx={},window.cnx.cmd=[];var t=n.createElement('iframe');t.src='javascript:false'; t.display='none',t.onload=function(){var n=t.contentWindow.document,c=n.createElement('script');c.src='//cd.connatix.com/connatix.player.js?cid=1ffdf4d6-eb53-11e9-b4d2-06948452ae1a',c.setAttribute('async','1'),c.setAttribute('type','text/javascript'),n.body.appendChild(c)},n.head.appendChild(t)}}(document); (new Image()).src = 'https://capi.connatix.com/tr/si?token=de820c7a-cd3f-49f4-9038-04e5790f8d5e&cid=1ffdf4d6-eb53-11e9-b4d2-06948452ae1a'; cnx.cmd.push(function() { cnx({ playerId: \"de820c7a-cd3f-49f4-9038-04e5790f8d5e\" }).render(\"0277831f664a4888be888d4b346dd76c\"); });var freestar = freestar || {}; freestar.queue = freestar.queue || []; freestar.config = freestar.config || {}; // Tag IDs set here, must match Tags served in the Body for proper setup freestar.config.enabled_slots = [];freestar.queue.push(function() { googletag.pubads().setTargeting('section', ['news','security']);}); freestar.initCallback = function () { (freestar.config.enabled_slots.length === 0) ? freestar.initCallbackCalled = false : freestar.newAdSlots(freestar.config.enabled_slots) } ;(function(o) { var w=window.top,a='apdAdmin',ft=w.document.getElementsByTagName('head')[0], l=w.location.href,d=w.document;w.apd_options=o; if(l.indexOf('disable_fi')!=-1) { console.error(\"disable_fi has been detected in URL. FI functionality is disabled for this page view.\"); return; } var fiab=d.createElement('script'); fiab.type = 'text/javascript'; fiab.src=o.scheme+'ecdn.analysis.fi/static/js/fab.js';fiab.id='fi+o.websiteId; ft.appendChild(fiab, ft);if(l.indexOf(a)!=-1) w.localStorage[a]=1; var aM = w.localStorage[a]==1, fi=d.createElement('script'); fi.type='text/javascript'; fi.async=true; if(aM) fi['data-cfasync']='false'; fi.src=o.scheme+(aM?'cdn':'ecdn') + '.firstimpression.io/' + (aM ? 'fi.js?id='+o.websiteId : 'fi_client.js'); ft.appendChild(fi); })({ 'websiteId': 5971, 'scheme': '//' });window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-GD465VRQLD'); NewsFeatured LatestCISA says critical Fortinet RCE flaw now exploited in attacksMicrosoft fixes Word bug that deleted documents when savingInternet Archive hacked, data breach impacts 31 million usersMicrosoft October 2024 Patch Tuesday fixes 5 zero-days, 118 flawsMarriott settles with FTC, to pay $52 million over data breachesUS, UK warn of Russian APT29 hackers targeting Zimbra, TeamCity serversThe affordable course deal helps you study for CompTIA examsFidelity Investments says data breach affects over 77,000 people TutorialsLatest PopularHow to access the Dark Web using the Tor BrowserHow to enable Kernel-mode Hardware-enforced Stack Protection in Windows 11How to use the Windows Registry EditorHow to backup and restore the Windows RegistryHow to start Windows in Safe ModeHow to remove a Trojan, Virus, Worm, or other MalwareHow to show hidden files in Windows 7How to see hidden files in Windows Virus Removal GuidesLatest Most Viewed RansomwareRemove the Theonlinesearch.com Search RedirectRemove the Smartwebfinder.com Search RedirectHow to remove the PBlock+ adware browser extensionRemove the Toksearches.xyz Search RedirectRemove Security Tool and SecurityTool (Uninstall Guide)How to Remove WinFixer / Virtumonde / Msevents / Trojan.vundoHow to remove Antivirus 2009 (Uninstall Instructions)How to remove Google Redirects or the TDSS, TDL3, or Alureon rootkit using TDSSKillerLocky Ransomware Information, Help Guide, and FAQCryptoLocker Ransomware Information Guide and FAQCryptorBit and HowDecrypt Information Guide and FAQCryptoDefense and How_Decrypt Ransomware Information Guide and FAQ DownloadsLatest Most DownloadedQualys BrowserCheckSTOPDecrypterAuroraDecrypterFilesLockerDecrypterAdwCleanerComboFixRKillJunkware Removal Tool DealsCategorieseLearningIT Certification CoursesGear + GadgetsSecurity VPNsPopularBest VPNsHow to change IP addressAccess the dark web safelyBest VPN for YouTube Forums MoreStartup Database Uninstall Database Glossary Chat on Discord Send us a Tip! Welcome Guide HomeNewsSecurityMozilla fixes Firefox zero-day actively exploited in attacks Mozilla fixes Firefox zero-day actively exploited in attacks By Bill Toulas October 9, 2024 01:34 PM 1Mozilla has issued an emergency security update for the Firefox browser to address a critical use-after-free vulnerability that is currently exploited in attacks. The vulnerability, tracked as CVE-2024-9680, and discovered by ESET researcher Damien Schaeffer, is a use-after-free in Animation timelines. This type of flaw occurs when memory that has been freed is still used by the program, allowing malicious actors to add their own malicious data to the memory region to perform code execution. Animation timelines, part of Firefox's Web Animations API, are a mechanism that controls and synchronizes animations on web pages. \"An attacker was able to achieve code execution in the content process by exploiting a use-after-free in Animation timelines,\" reads the security bulletin. \"We have had reports of this vulnerability being exploited in the wild.\" The vulnerability impacts the latest Firefox (standard release) and the extended support releases (ESR). Fixes have been made available in the below versions, which users are recommended to upgrade to immediately: Firefox 131.0.2 Firefox ESR 115.16.1 Firefox ESR 128.3.1 Given the active exploitation status for CVE-2024-9680 and the lack of any information on how people are targeted, upgrading to the latest versions is essential. To upgrade to the latest version, launch Firefox and go to Settings -> Help -> About Firefox, and the update should start automatically. A restart of the program will be required for the changes to apply.Updating FirefoxSource: BleepingComputer BleepingComputer has contacted both Mozilla and ESET to learn more about the vulnerability, how it's being exploited, and against whom, and we will update this post when we receive more information. Throughout 2024, so far, Mozilla had to fix zero-day vulnerabilities on Firefox only once. On March 22, the internet company released security updates to address CVE-2024-29943 and CVE-2024-29944, both critical-severity issues discovered and demonstrated by Manfred Paul during the Pwn2Own Vancouver 2024 hacking competition.Related Articles: Qualcomm patches high-severity zero-day exploited in attacksMicrosoft August 2024 Patch Tuesday fixes 9 zero-days, 6 exploitedMicrosoft October 2024 Patch Tuesday fixes 5 zero-days, 118 flawsIvanti warns of three more CSA zero-days exploited in attacksHackers targeting WhatsUp Gold with public exploit since Augustfreestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_728x90_320x50_InContent_1\", slotId: \"bleepingcomputer_728x90_320x50_InContent_1\" }); Actively Exploited CVE-2024-9680 Firefox Use After Free Vulnerability Zero-Day Bill ToulasBill Toulas is a tech writer and infosec news reporter with over a decade of experience working on various online publications, covering open-source, Linux, malware, data breach incidents, and hacks.Previous ArticleNext Article Comments jmwoods- 22 hours ago Thanks for the heads up.Post a Comment Community RulesYou need to login in order to post a commentNot a member yet? Register NowYou may also like: (adsbygoogle = window.adsbygoogle || []).push({});freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_1\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_1\" });Popular Stories Internet Archive hacked, data breach impacts 31 million usersEuropean govt air-gapped systems breached using custom malwareNew Mamba 2FA bypass service targets Microsoft 365 accountsfreestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_2\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_2\" });Sponsor PostsSee how you can manage shadow IT and reduce your attack surfaceData Theft in Salesforce: Manipulating Public LinksDiscover how to build custom dictionaries in your AD password policyHow open source SIEM and XDR tackle evolving threatsHybrid Analysis Bolstered by Criminal IP’s Comprehensive Domain Intelligence freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_3\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_3\" }); freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_728x90_970x90_970x250_320x50_BTF\", slotId: \"bleepingcomputer_728x90_970x90_970x250_320x50_BTF\" });Follow us:Main SectionsNews VPN Buyer Guides SysAdmin Software Guides Downloads Virus Removal Guides Tutorials Startup Database Uninstall Database GlossaryCommunityForums Forum Rules ChatUseful ResourcesWelcome Guide SitemapCompanyAbout BleepingComputer Contact Us Send us a Tip! Advertising Write for BleepingComputer Social & Feeds Changelog Terms of Use -Privacy Policy - Ethics Statement - Affiliate Disclosure Copyright @ 2003 - 2024Bleeping Computer® LLC- All Rights Reserved Login UsernamePasswordRemember MeSign in anonymously Sign in with TwitterNot a member yet? Register Now$(document).ready(function(e) { $('.articleBody img').not('a>img').not('.contrib_but>img').click(function(e) { e.preventDefault(); $.fancybox({'href' : $(this).attr('src')}); }); }); $(document).ready(function(){ var content = $('.cz-main-left-section'); var sidebar = $('.bc_right_sidebar'); var count = 0; var myTimer; function setEqualContainer() { var getContentHeight = content.outerHeight(); var getSidebarHeight = sidebar.outerHeight(); if ( getContentHeight > getSidebarHeight ) { sidebar.css('min-height', getContentHeight); } if ( getSidebarHeight > getContentHeight ) { content.css('min-height', getSidebarHeight); } } // creating the timer which will run every 500 milliseconds // and will stop after the container will be loaded // ...or after 15 seconds to not eat a lot of memory myTimer = setInterval( function() { count++; if ( $('.testContainer').length == 0 ) { setEqualContainer(); } else { setEqualContainer(); clearInterval(myTimer); } if ( count == 15) { clearInterval(myTimer); } }, 500); $('#pinned').fixTo('.bc_right_sidebar', { bottom: 25, }); $('#more_dd').click(function (e) { e.preventDefault() }); $('.bc_goto_top a').click(function(){ $(\"html, body\").animate({ scrollTop: 0 }, 600); return false; }); jQuery('.bc_login_btn').on('click', function() { jQuery('.bc_popup').fadeIn(\"slow\"); }); jQuery('.bc_popup_close').on('click', function() { jQuery('.bc_popup').fadeOut(\"slow\"); }); });// validate comment box not empty function validate_comment_box_not_empty() {$('#frm_comment_box').submit(function(e) { if($('#comment_html_box').val().length==0) {alert(\"Please enter a comment before pressing submit\");return false; } else {return true; }}); } function cz_strip_tags(input, allowed) { allowed = (((allowed || '') + '') .toLowerCase() .match(//g) || []) .join(''); // making sure the allowed arg is a string containing only tags in lowercase () var tags = /]*>/gi, commentsAndPhpTags = /|/gi; return input.replace(commentsAndPhpTags, '') .replace(tags, function($0, $1) { return allowed.indexOf('') > -1 ? $0 : ''; }); } function cz_br2nl(str) {var regex = //gi; //var pure_str = str.replace(regex,\"\"); var pure_str = str.replace(regex,\"\"); return cz_strip_tags(pure_str,''); } $(document).ready(function(e) { // validate comment box not empty validate_comment_box_not_empty(); // report comment $('#comment-report-other-reason-wrap').css('display','none'); $('.cz-popup-close').click(function(e) { e.preventDefault(); $('.cz-popup').fadeOut(\"slow\"); }); $('.cz-comment-report-btn').click(function(e) { e.preventDefault(); $('.cz-popup').css('height',$( document ).height()+'px'); //var comment_box_report_top = $(this).offset().top; var comment_box_report_top = $(document).scrollTop(); $('.cz-popup-wrapp').css('top',(comment_box_report_top+100)+'px'); $('#comment-id-report').val($(this).attr('data-id')); $('.cz-popup').fadeIn(\"slow\"); }); $(\"input[type='radio'][name='comment-report-reason']\").click(function(e) { if($(this).val()=='Other') { $('#comment-report-other-reason-wrap').css('display','block'); } else { $('#comment-report-other-reason-wrap').css('display','none'); } }); $('.comment-report-submit-btn').click(function(e) { e.preventDefault(); var comment_report_reason = \"\"; var comment_report_reason = $(\"input[type='radio'][name='comment-report-reason']:checked\").val(); if (comment_report_reason=='Other') { comment_report_reason = $('#comment-report-other-reason').val(); } if(comment_report_reason=='') { alert('Please specify reason'); } else { $('.cz-popup-report-submiting').css('display','inline-block'); $.ajax({type: \"POST\", url: 'https://www.bleepingcomputer.com/report-comment/', data: { comment_id: $('#comment-id-report').val(), reason: comment_report_reason }, success: function(data) { $('.cz-popup-report-submiting').css('display','none'); $('.cz-popup').fadeOut(\"slow\"); }}); } }); // report comment $('.cz_comment_reply_btn').click(function(e) { e.preventDefault(); $('#parent_comment_id').val($(this).attr('data-id')); $('#comment_html_box').attr('placeholder','Replying to '+$(this).attr('data-name')); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); $('#comment_html_box').focus(); }); $('.cz_comment_quote_btn').click(function(e) { e.preventDefault(); var quote_comment_html =''; if($(this).attr('data-id')!=undefined && $(this).attr('data-id')!='') { $('#parent_comment_id').val($(this).attr('data-id')); quote_comment_html = $('#comment_html_'+$(this).attr('data-id')).html(); } quote_comment_html = cz_br2nl(quote_comment_html); $('#comment_html_box').val('\"'+quote_comment_html+'\"'); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); $('#comment_html_box').focus(); }); }); function editForm(cid) { $.ajax({ type: \"GET\", url: window.location.href+\"?sa=1\", data: { f: \"e\", cid: cid }, success: function(data) { $('.cz-post-comment-wrapp').html(data);validate_comment_box_not_empty(); } }); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); } $(document).on('click', '.cz-subscribe-button' , function(e) { e.preventDefault(); $.ajax({type: \"POST\", url: window.location.href, data: { a: 'sub' }, success: function(data) { if(data == '1')$( \"li.cz-subscribe-button\" ).replaceWith( ''); } }); }); $(document).on('click', '.cz-unsubscribe-button' , function(e) { e.preventDefault(); $.ajax({ type: \"POST\", url: window.location.href, data: { a: 'unsub' }, success: function(data) { if(data == '1')$( \"li.cz-unsubscribe-button\" ).replaceWith( ''); } }); });$('.cz-print-icon, .cz-lg-print-icon').click(function(e) { e.preventDefault(); var divToPrint = document.getElementById('.article_section'); var mywindow = window.open('','','left=0,top=0,width=950,height=600,toolbar=0,scrollbars=0,status=0,addressbar=0'); var is_chrome = Boolean(mywindow.chrome); mywindow.document.write($( \".article_section\" ).html()); mywindow.document.close(); // necessary for IE >= 10 and necessary before onload for chrome if (is_chrome) { mywindow.onload = function() { // wait until all resources loaded mywindow.focus(); // necessary for IE >= 10 mywindow.print(); // change window to mywindow mywindow.close();// change window to mywindow }; } else { mywindow.document.close(); // necessary for IE >= 10 mywindow.focus(); // necessary for IE >= 10 mywindow.print(); mywindow.close(); } return true; });var loginhash = '880ea6a14ea49e853634fbdc5015a024'; var main_nav_hide_flag = true; var scrollTop =0; var main_nav_hide_timer = ''; function call_main_nav_hide() { if(main_nav_hide_flag && scrollTop >=100) { $('header').addClass(\"nav-up\"); } } var cz_header_pos = $('header').offset().top; $(window).scroll(function() {$('header').each(function(){var cz_top_of_window = $(window).scrollTop()-100; if (cz_top_of_window > cz_header_pos) { $('.bc_goto_top').fadeIn(\"slow\"); } else {$('.bc_goto_top').fadeOut(\"slow\");}}); }); var prevScrollTop = 0; $(window).scroll(function(event){ scrollTop = $(this).scrollTop(); if ( scrollTop$('body').height() - $(window).height() ) { scrollTop = $('body').height() - $(window).height(); } if (scrollTop >= prevScrollTop && scrollTop) { $('header').addClass(\"nav-up\"); } else {if (scrollTop >=100){ $('header').removeClass(\"nav-up\"); main_nav_hide_timer = setTimeout(\"call_main_nav_hide()\",5000);}else{ $('header').removeClass(\"nav-up\"); clearInterval(main_nav_hide_timer);} } prevScrollTop = scrollTop; }); $(document).ready(function(){var bLazy = new Blazy(); $(\".bc_dropdown a\").mouseenter(function(e) { $(this).parent('.bc_dropdown').delay(250).queue(function(){ $(this).addClass('show_menu').dequeue(); bLazy.revalidate(); }); main_nav_hide_flag = false; }); $(\".bc_dropdown\").mouseleave(function(e) { $(\".bc_dropdown\").clearQueue().stop().removeClass('show_menu'); main_nav_hide_flag = true; if (scrollTop >=100) { main_nav_hide_timer = setTimeout(\"call_main_nav_hide()\",5000); } }); $('.bc_dropdown a').each(function(){ if($(this).is(\":hover\")) { $(this).mouseenter(); } }); $('#bc_drop_tab a').hover(function (e) { e.preventDefault() $(this).tab('show') bLazy.revalidate(); });$('#more_dd').click(function (e) { e.preventDefault()});$('.bc_goto_top a').click(function(){$(\"html, body\").animate({ scrollTop: 0 }, 600);return false;});jQuery('.bc_login_btn').on('click', function() { jQuery('.bc_popup').fadeIn(\"slow\"); $('#ips_username').focus(); });jQuery('.bc_popup_close').on('click', function() { jQuery('.bc_popup').fadeOut(\"slow\"); }); }); $(document).mouseup(function (e) { var container = $(\".bc_login_form\"); if (!container.is(e.target) // if the target of the click isn't the container... && container.has(e.target).length === 0 && $('.bc_popup').css('display') =='block') // ... nor a descendant of the container { jQuery('.bc_popup').fadeOut(\"slow\"); } }); if($(window).width()ReporterHelp us understand the problem. What is going on with this comment? Spam Abusive or Harmful Inappropriate content Strong language OtherRead our posting guidelinese to learn what content is prohibited.Submitting... SUBMITvar loadDeferredStyles = function() { var addStylesNode = document.getElementById(\"deferred-styles\"); var replacement = document.createElement(\"div\"); replacement.innerHTML = addStylesNode.textContent; document.body.appendChild(replacement) addStylesNode.parentElement.removeChild(addStylesNode); }; var raf = requestAnimationFrame || mozRequestAnimationFrame || webkitRequestAnimationFrame || msRequestAnimationFrame; if (raf) raf(function() { window.setTimeout(loadDeferredStyles, 0); }); else window.addEventListener('load', loadDeferredStyles);",
    "commentLink": "https://news.ycombinator.com/item?id=41796030",
    "commentBody": "Mozilla fixes Firefox zero-day actively exploited in attacks (bleepingcomputer.com)174 points by timokoesters 12 hours agohidepastfavorite106 comments statusfailed 11 hours agoSeems bad. \"An attacker was able to achieve code execution in the content process by exploiting a use-after-free in Animation timelines. We have had reports of this vulnerability being exploited in the wild.\" See: - NVD page for CVE-2024-9680: https://nvd.nist.gov/vuln/detail/CVE-2024-9680 - Mozilla security advisory: https://www.mozilla.org/en-US/security/advisories/mfsa2024-5... reply btdmaster 9 hours agoparentTicket in Tor Browser: https://gitlab.torproject.org/tpo/applications/tor-browser/-... It seems to be JavaScript-free from the description, which makes it even scarier. Imagine the libwebp decoder bug except embedded media blocking doesn't really work (who blocks CSS?). reply mrob 9 hours agorootparentI block CSS animations: https://news.ycombinator.com/item?id=33223080 I'd be interested to know if it's sufficient to avoid this recent vulnerability. Either way, it confirms my opinion that UI animations are an anti-feature. reply schiffern 8 hours agorootparentAs a uBlock Origin filter (paste in Settings > My Filters): ! No CSS animations ##*,::before,::after:style(transition:none !important;animation-delay:0ms !important;animation-duration:0ms !important) ! No CSS animations (different method) ##*,::before,::after:style(animation-timing-function:step-start !important;transition-timing-function:step-start !important) There's other (often perf heavy) CSS clutter that's nice to get rid of: ! No image filters ##*,::before,::after:style(filter:none !important) ! No text-shadow ##*,::before,::after:style(text-shadow:none !important) ! No box-shadow ##*,::before,::after:style(box-shadow:none !important) ! No rounded corners ##*,::before,::after:style(border-radius:0px !important) No rounded corners is fun. You realize many loading spinners are actually CSS rounded corners! Youtube becomes almost unrecognizable — mercifully — especially if you also revert the new TikTok-inspired font: ! Un-bold Youtube youtube.com##*:style(font-weight:400 !important) reply krackers 1 hour agorootparentprevFirefox doesn't seem to support css animation-timeline, I think this refers to the JS AnimationTimeline API? In that case \"dom.animations-api.timelines.enabled\" flag should control it. reply tempaccount420 5 hours agoparentprevIs this karma for dropping Rust? (please don't explain how Rust actually wouldn't fix this) reply NicolaiS 8 hours agoprevA note for Ubuntu users; if Firefox is installed using `snap` (default) and you run `snap refresh` it will output \"All snaps up to date\" - but this is not true! You have to close firefox, then run `snap refresh` for snap to upgrade firefox... reply guerrilla 8 hours agoparentNot an Ubunutu or snap user but curious, why? reply fhars 8 hours agorootparentThe snap store also complains regularly that it can't updte the snap store because the snap store is running. It is just terrible software overall. reply dspillett 8 hours agorootparentprevIt doesn't update actively running application containers. You don't actually need to stop it before running “snap refresh” though, it'll just be out of date as long as it is kept open. Once the application stops running, next time it is run the updated image will be used. [caveat: I'm not a snap user myself currently, so my information may be inaccurate, take with a pinch of your favourite condiment] reply guerrilla 8 hours agorootparentInteresting. On Arch, Firefox just refuses to keep working after I've updated and requests me to restart it. reply AshamedCaptain 6 hours agorootparentThat is Firefox standalone behavior when it detects its files have been changed and differ from the ones loaded by the current instance. In theory, what snap is doing avoid changing files from a program while it is running. reply guerrilla 6 hours agorootparentYeah, that makes sense. reply joha4270 7 hours agorootparentprevNow, I don't know how your setup looks like, but I don't think anything is distributed as snaps by default on Arch. At least AFAIK its mostly an Ubuntu & derivatives thing. reply bmicraft 6 hours agorootparentprevWell, it was more broken in more interesting ways before they implemented the forced restart reply guerrilla 6 hours agorootparentYeah, I remember. That was fun. haha reply lifthrasiir 8 hours agorootparentprevNeither am I, but seems that snap refreshes can be inhibited programmatically if they may cause some damage when proceeded in the background. So it is technically correct that no snap refreshes can be performed at this point, but the message doesn't clearly state that some refreshes have been inhibited (possibly because there would be tons of them if they are exhaustively listed?). reply KwanEsq 9 hours agoprevThe patch: https://hg.mozilla.org/releases/mozilla-release/rev/d2a21d94... reply palata 9 hours agoprev> The vulnerability impacts the latest Firefox (standard release) and the extended support releases (ESR). Does that mean it impacts Firefox 131.0.+, Firefox ESR 115.16.+ and Firefox ESR 128.3.+? I.e. Firefox 130.0.+ or Firefox ESR 114.+.+ are fine? It's not clear to me when the vulnerability was introduced... reply olejorgenb 8 hours agoparent> This vulnerability affects FirefoxAt the end of the day web browser is just bunch of parsers and compilers working together, and some video/audio That's... an interesting reduction :) I guess it's about as true as saying that the Linux Kernel is a bunch of I/O and a scheduler? reply sunshowers 9 hours agorootparentprevThe problem with writing a browser in C# or Java is that neither of them can provide anywhere close to the level of thread safety that Rust does. reply chii 8 hours agorootparentboth java and c# has thread safety primitives that are also pretty easy to use. E.g., the java concurrency package. reply sunshowers 1 hour agorootparentNo, that's not what I'm talking about. C# and Java have nothing like the Send and Sync traits, and they don't have & and &mut. reply Yoric 5 hours agorootparentprevHaving done concurrency in Java and Rust, my experience is that Rust's concurrency primitives are an order of magnitude better than Java's. I haven't tested C#'s. reply om8 6 hours agorootparentprevThe cool thing about rust is that it forces you to use thread safety primitives. It's called fearless concurrency reply eqvinox 9 hours agorootparentprevServo exists, in Rust. I don't know of any browser engine in C#/Java? Also, modern browsers as a whole outsize entire OSes (sans browser)... reply cesarb 6 hours agorootparent> I don't know of any browser engine in C#/Java? A famous one is HotJava. According to Wikipedia, it was also the first web browser to support Java applets. reply Yoric 5 hours agorootparentIt was also a mess :) reply Ygg2 9 hours agorootparentprev> At the end of the day web browser is just bunch of parsers and compilers working together At the end of the day, OS is just a bunch of command lines being piped together. /sarcasm Sure, you are just missing: rendering, layout, security, network traffic for sockets, low-level control over hardware, writing a decent enough VM, image processing, video playback, music playback, compression, decompression, self-update, decryption, don't forget add-ons people love add-ons, also add-on security and isolation, web edit and debug tools, network analysis tools, etc. You know, little things. reply high_na_euv 8 hours agorootparentWhy would you need to reinvent networking layer instead of just sending http requests via matrue, battle tested lib available in your programming ecosystem e.g from MSFT? Same with crypto, sockets, compression, etc? Video and audio I mentioned. Extensions are tricky, right, but more from privacy standpoint cuz after all you can just expose too much reply michaelt 7 hours agorootparentAll the major browsers came out when Windows XP had substantial market share. So browser vendors couldn't rely on the platform to provide up-to-date SSL support. Or MP3 support. Or MPEG-4 support. Or PDF support. This established the norm that browsers would ship their own video support, their own SSL support, and so on. And Google realised they like the power this gives them - if Google wants to replace HTTP with QUIC or introduce a new video DRM standard, or a new video codec like VP9 - they don't need the cooperation of anyone outside of Google. If Chrome bundles DRM support (allowing it to play Netflix), and its own HTTP/2 stack for speed - are you going to release a browser that's slower and doesn't play Netflix? Doesn't sound like a recipe for big market share. reply Yoric 5 hours agorootparentprevMany of these components have been made part of the ecosystem long after they were introduced in Firefox. Also, the more platform-specific you go for each component, the more you're going to introduce subtle incompatibilities between Firefox running on different versions of Windows or in Firefox for Windows vs. macOS vs. Linux. Also, for a very, very long time, Microsoft had an extremely poor record in terms of security fixes. So what happens when you rely on a Microsoft http library and Microsoft takes a year or two to release a 0-day? There are benefits to this approach, of course, but the costs would have been consequential. reply viraptor 7 hours agorootparentprevBrowsers are using new http features much earlier than they're available in the system libraries. Browsers supported http2 and 3 before they were standardised enough to include in systems. .net http client still can't even tell you about http2 early hints as far as I understand it. It's going to be the same for crypto and compression. Systems don't ship with brotli for example. The battle tested implementations come to the browsers first in many cases - or at least they're battle tested at the point anyone starts including them in .net or Java. reply high_na_euv 5 hours agorootparentSure, not being at the leading edge is a disadvantage, but I guess you could still handle 99.x% of web pages reply Ygg2 7 hours agorootparentprev> Why would you need to reinvent networking layer instead of just sending http requests via matrue, battle tested lib available in your programming ecosystem e.g from MSFT? Because modern browsers are essentially cross-compatible OSes. reply high_na_euv 5 hours agorootparentSo is .net reply heresie-dabord 9 hours agoparentprevFrom the fine article: > Throughout 2024, so far, Mozilla had to fix zero-day vulnerabilities on Firefox only once. > On March 22, the internet company released security updates to address CVE-2024-29943 and CVE-2024-29944, both critical-severity issues Vulnerabilities will be found in everything. Firefox is a fully internationalised application and it is FOSS. The team responsible for Firefox is doing a good job. reply high_na_euv 9 hours agorootparent>Vulnerabilities will be found in everything. Different ratios, different consequences, etc. reply mikae1 10 hours agoparentprevLadybird[1] is switching to Swift[2]. [1] https://ladybird.org [2] https://news.ycombinator.com/item?id=41208836 reply hypeatei 9 hours agorootparentThat's not completely accurate. The plan is to use Swift for \"security critical\" areas like decoding data. It's unlikely core components like the layout/CSS engine will be converted to Swift. reply actualwitch 9 hours agorootparentprevSwift didn't save apple from rce's in blastdoor. reply Ygg2 9 hours agoparentprevSounds like Mozilla should invent a low level language with great safety guarantees, maybe even call it after some form of oxidation process[1]. Then make a browser engine called after a motor[2], and then NOT axe the team responsible for it[3]. I think the last part might be crucial. [1] https://www.rust-lang.org/ [2] https://servo.org/ [3] https://paulrouget.com/bye_mozilla.html reply high_na_euv 9 hours agorootparentIm aware of Rust, but there is C#/Java too, with way bigger ecosystem, community and lower entry level reply Yoric 8 hours agorootparentA long time ago, the possibility of using Java or C# in Gecko (the core of Firefox) was pondered. Java was rejected because of the huge memory requirements and the unpredictable (and sometimes lengthy) garbage-collection pauses. C# was rejected because (at the time) it was too tied to the Microsoft ecosystem and there was no way to get it to build on all the platforms for which Firefox is available. I don't remember garbage-collection pauses being discussed, but they would also be an issue. reply m4rtink 8 hours agorootparentprevIt seems to me that both C#/Java have build their own niches and are hard to impossible to realistically use outside of them, such as to write a web browser. reply aryonoco 8 hours agorootparentprevI think of browsers these days on par with OSes. I mean, they provide a runtime to execute binary code (wasm). They do process management and scheduling. They do a lot of things which up until 15 years ago, we thought bongs to the realm of Operating Systems. And history has shown that when you need to do that kind of low level code, it's nigh on impossible to achieve acceptable results with a garbage collected language. Many people tried, none really succeeded. Hence why Rust was made reply fulafel 10 hours agoparentprevThey already are partly in JS so there's a smooth path. (Wasm isn't safe but could be a building block too) reply flohofwoe 10 hours agorootparentI can't find the link right now but I seem to remember that Firefox already replaced some internal native subsystems with the same code compiled to WASM - or maybe even compiled to WASM and then translated back to C, which basically adds a runtime memory safety layer to unsafe C code at the cost of some performance (I think it was a couple of media codecs, but not sure). Not sure why you think that WASM is less secure than JS though. Even if the WASM heap has internal corruption there's no way for this to do damage outside the WASM sandbox that wouldn't be possible in JS. reply cesarb 6 hours agorootparent> I can't find the link right now but I seem to remember that Firefox already replaced some internal native subsystems with the same code compiled to WASM - or maybe even compiled to WASM and then translated back to C Was it this one? https://hacks.mozilla.org/2021/12/webassembly-and-back-again... Or perhaps this one? https://hacks.mozilla.org/2020/02/securing-firefox-with-weba... reply fulafel 9 hours agorootparentprevIf your browser is running in a wasm sandbox, it's a minor comfort that only your browser gets compromised which contains all your creds, etc. reply flohofwoe 9 hours agorootparentOnly parts of the browser are running in multiple small isolated WASM sandboxes, those WASM sandboxes are isolated from outside world about as well as if they would run in their own process. reply fulafel 9 hours agorootparentCompartments of internally unsafe sandboxes are what we have now, with browsers employing native-code sandboxes and isolated renderer processes etc. It gets leaky. reply moffkalast 9 hours agoparentprev> if it means some perf drop, modern hardware will get it back in X years I think the unfortunate reality is that other browsers will also take advantage of that speed boost, sites will get even more bloated because they can and it will stay unusable for a long long time. reply calyhre 7 hours agoprevIt's fixed in the developer edition 132.0b5 also if you are wondering reply Ennea 6 hours agoparentI was indeed wondering. Thank you. reply rightbyte 6 hours agoprevThis seems quite bad, but how practical is it. Like, the attacker will get write and read access to part or the whole of some other object allocated on the heap, when the memory is reused? Seems hard to do anything useful with. reply jokoon 7 hours agoprevI wonder how many skilled black hats work for Iran, China or Russia. And I can imagine that those countries use front companies to buy exploit. I just hope that those blackhats understand that their discovery might land in the wrong hands. I guess those blackhats don't like authoritarian regimes. reply nullc 9 hours agoprevRegain your ability to sleep at night: https://www.qubes-os.org/ reply fransje26 9 hours agoparentFrom your experience, what are the system requirements needed to use that as comfortably as your daily driver? reply nullc 9 hours agorootparentThey're increased, and some things are just obviously slow at least without extra effort to setup things like gpu pass-through. But is it worth basically turning back the clock on your computer's performance a few years to live in a world where a random click from HN or reddit can't quietly compromise your entire computer? I think so. Probably the biggest thing is to have a lot of ram, because if you're really using the virtualization it's a bit ram inefficient. Many things I expected to be hard or annoying just turn out to be non-issues. Qubes has lots of good automation to make it pretty seamless to use multiple VMs. I was already a fedora user, so I just copied my old home into a new app vm and was instantly productive. Then over time I weaned myself off the monolithic legacy vm into partitioned VMs. reply prmoustache 4 hours agorootparent> \"obviously slow at least without extra effort to setup things like gpu pass-through.\" AFAIK unless you have a desktop computer filled with gpus on pciexpress slots there is no way you can use GPU passthrough on multiple VMs. That kind of defeat the purpose of qubes os no? reply mikedelfino 7 hours agorootparentprev> a world where a random click from HN or reddit can't quietly compromise your entire computer Doesn't Flatpak also solve this? reply krageon 8 hours agorootparentprevIf you anything with a GPU anywhere, you can essentially forget it. Or at least this was the case a few years ago when I briefly toyed with using qubes seriously. reply fransje26 5 hours agorootparentImportant consideration, thank you. Edit: Seems like someone has managed to get CUDA to work, with some effort. https://forum.qubes-os.org/t/nvidia-gpu-passthrough-into-lin... reply creata 9 hours agoparentprevDoes virtualization have that big a security benefit over containers? It's certainly a lot more expensive. reply rwmj 9 hours agorootparentContainers share the same kernel as the host. If you're happy sharing millions of lines of monolithic C between trust domains ... reply creata 8 hours agorootparentYes, but it's a compromise, because I'm not happy spinning up tons of kernels and trying to share access to devices that do not want to be shared, either. You're right that the trusted codebase is huge, but I sincerely do not know how big a problem this is in practice, hence the question. reply nullc 8 hours agorootparentQubes does have answers to the device stuff, like sticking network devices in a network vm, which only talks to a firewall vm, which talks to your other vms. There is a reasonable gui interface where you can just plug devices into particular VMs for other things. In my usage I've never felt the need to share stuff other than the network/sound/storage stuff that qubes make just work. Other devices tend to be just plug them into the particular VM that needs them. YMMV. I would say that perhaps containers could do just as well, or some other technology. The thing qubes brings to the table is that other people are doing most of the heavy lifting to make a usable desktop out of a highly virtualized system. There may be path dependent reasons why qubes approach isn't the best possible... but it doesn't matter because so much stuff just working is worth so much. That the compromise we always make when running a distribution... one could meta-x butterfiles and write your own kernel from scratch, or whatever. Or you can run a system created by others. Their system may have decisions you disagree with or are objectively bad, but they saved you 12 months of tinkering with the dynamic linker-- well worth it. :) For me, the alternative of having my whole laptop compromised by some browser zero day or because a malicious party sent me some malware document was just not viable. I was already carrying two laptops for isolation, and suffering some anxiety from the residual risk. But in my case I've been targeted specifically (due to cryptocurrency bullshit), a friend and former colleague was hit with an astonishingly sophisticated attack that used stuff like BMC vulnerabilities on his web server and then traversal with X11 forwarding and stuff like that all to just break into his desktop. So I'd probably be using qubes today even if I could only move the mouse with my tongue and the computer was slowed down to the speed for a 486sx. But the incorrect belief that it would be that kinda hit really delayed my adoption. It's a hit, it's real, but at least for my usage it was far smoother than I expected. I think right now the only obvious wart I experience is that full screen video stutters pretty badly. So I just don't watch video full screen on the laptop now. There are things that might fix it, but I haven't bothered even trying. There are benefits I didn't expect too. For example, The operating system image in a normal application VM isn't persistent, only your home directory. So you can just scribble all over the OS install in an app vm and it'll go away when you restart it. If you want it to be persistent you change the underlying templatevm. So to get something working I can totally take a chainsaw to my configuration confident I won't get stuck with anything broken. Once I figure out the changes I can apply just the required steps in a template. Another benefit is that updating fedora versions is a riskless breeze--- install a new template vm. shut down your app vms, click to change template. Restart them if some particular app vm is broken, switch it back and worry about it when you have time. reply krageon 8 hours agorootparentprevContainers aren't a security measure, so you'd be comparing a stick of wood to a car in this case. reply sylware 10 hours agoprevuntil the next one... It has been like that for most 'internet software' in the last decades, no light at the end of this tunnel. reply okasaki 10 hours agoprevIt references \"Bug 1923344\" but when I click the link I get \"You are not authorized to access bug 1923344.\" reply pja 10 hours agoparent> It references \"Bug 1923344\" but when I click the link I get \"You are not authorized to access bug 1923344.\" They usually make the bug reports public eventually. reply baq 10 hours agoparentprevThis is a feature. reply loopdoend 10 hours agoprev [–] Fixed many months ago just being made public now, according to the bug tracker. Why a 7 month delay? reply jokoon 7 hours agoparentBecause if you make it public too early, it gives some time for attackers to write exploit to target unpatched versions. Firefox is used in other projects, so the patch needs to spread, and time is needed. reply okasaki 10 hours agoparentprev\"Fixed in Firefox 131.0.2\" which was released 21 hours ago? (https://ftp.mozilla.org/pub/firefox/releases/131.0.2/) reply hannob 10 hours agoparentprevWhat are you talking about? The fix was released today, and FF says they received the report 25 hours before that: https://infosec.exchange/@attackanddefense/11328207943028074... reply Brybry 10 hours agoparentprevI didn't get the ESR 128.3.1 update until yesterday. reply pixelesque 10 hours agoparentprevWhy the need for patch releases then like 128.3.1? reply suprjami 9 hours agorootparent128 is the current ESR: https://whattrainisitnow.com/calendar/ reply gpvos 10 hours agoparentprev [–] Citation needed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mozilla has issued an emergency update for Firefox to address a critical use-after-free vulnerability, identified as CVE-2024-9680, which is currently being exploited.- The vulnerability is located in Animation timelines and allows attackers to execute code using freed memory, posing a significant security risk.- Users are strongly advised to update to Firefox 131.0.2, Firefox ESR 115.16.1, or Firefox ESR 128.3.1 to protect against potential attacks."
    ],
    "commentSummary": [
      "Mozilla has addressed a zero-day vulnerability in Firefox that was actively being exploited, which allowed attackers to execute code through a use-after-free flaw in Animation timelines.- The vulnerability impacted the latest Firefox versions, including the extended support releases (ESR), prompting users to update to the latest version for protection.- This patch highlights the importance of regularly updating software to safeguard against potential security threats."
    ],
    "points": 174,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1728540678
  },
  {
    "id": 41794566,
    "title": "How to make Product give a shit about your architecture proposal",
    "originLink": "https://gieseanw.wordpress.com/2024/10/09/how-to-make-product-give-a-shit-about-your-architecture-proposal/",
    "originBody": "HOW TO MAKE PRODUCT GIVE A SHIT ABOUT YOUR ARCHITECTURE PROPOSAL I had a plumber over the other day. I was worried that my water service line might be leaking. There was a wet patch in the yard and I noticed that one of my sink’s water pressure seemed to be lower than usual. If the line had a pinhole leak in it, it could easily burst into a multi-thousand dollar flood on my hands. The joys of home ownership. The plumber listened to my concerns and then inspected a few things: the piping where the service line enters my home, the pressure regulator, the sink in question, and even my water heater. Afterwards, he told me it’s probably not as bad as I feared, but that I should sit down. He produced a tablet and turned it to face me. “Here are your options,” he started, indicating towards a group of boxes on the screen. “First we have the ‘platinum package’. This includes replacing your pressure regulator with one that is set correctly (apparently pressure regulators can only be set once?), as well as a new water heater, a water softening system, a reverse-osmosis system, an electronic descaler, and a replacement of all your faucet aerators.” The price was conveniently listed below: six easy monthly payments of $2,500 ($2400 if I was a “member”). He patiently answered all of my questions about this package, although it was surprisingly hard to get an answer to “Do you think I really need all this?” From there, we went down the line through the other options. Each cost a little less than the previous, included fewer products, and were more cleverly named than airfare classes; there was ‘premium’ followed by ‘standard’, ‘economy plus’, ‘economy’, and finally ‘band-aid’. I asked more questions, and I was eventually able to negotiate my own “package” consisting of just what I thought I needed. For closure, no, my water service line was not leaking (probably). And that’s how you make product give a shit about your architecture proposals. You are the plumber Your role as a software engineer is to play the plumber to product. The reality of the world is that product holds the money and software development is seen as a cost center to be minimized towards zero. You are selling to them. Let’s walk through an example. How this works Your team successfully launched a new product for your company. It’s awesome at what it does, customers love it so far, but it’s still fairly immature. Product comes to you and says “What our customers need next is the ability to generate reports.” Now, having been intimately involved in the development of the product, you know how the data is stored. It’s a relational database. It’s fantastic at CRUD operations. It can perform some basic aggregation across related tables, but otherwise it’s ill-suited for something like “reporting”. Besides, product tells you that customers would love if their reports could be sorted and filtered based on customer addresses, but customer address information is actually stored in a completely separate system. And this system calls into yours, not vice-versa. And…, and…, and… Before you freak out on product and go into all the reasons you can’t do what they want, take a step back and consider: Product doesn’t give a shit about how your data is stored. Product cares about products Product is looking for a result, not a distributed system design interview whiteboard session Product is human (for now), and they understand that there are engineering considerations. That’s why they came to you in the first place. They came to you for a negotation (more on this in a minute) In other words, product doesn’t give a shit about your architecture proposal. Yet. Channel your inner plumber. (Aside, you’ve earned this “problem”. You designed the system correctly to do what it needed to do in order to be a successful product. The natural order of successful products is to evolve beyond their original infrastructure.) So here’s what you do instead of stuttering through an explanation of “indexes”, “table joins”, and “cardinality”: You show them the “platinum package”. This means that you gather up all the information you need to give product exactly what they want, and then you come back to them with an estimate: six easy monthly payments of $2500. Or, rather, you say “One full time mid-level engineer’s time for 6 months on our team, plus one full time engineer’s time from the Infrastructure team.” Taken aback at such a large estimate (because they were hoping, pretty please, that it would be a single two week sprint. After all, all the data they need is “already in our system”, right?), product utters a single-word question: “Why?” And that is how you get product to give a shit about your architecture proposal. Suddenly they want to know all about it because they know they can’t afford a full year’s worth of engineering work. Now you can (gently) talk to them about the difference between online transaction processing systems (OLTP) and online analysis processing systems (OLAP). Each thing that needs to be done is a line item in your ‘platinum package’ invoice: Define an ETL process to flatten and export our data to Snowflake If you want, you can further break this down into the batch processes that happen, and when, where, and how often they happen, which informs data freshness guarantees Similarly define a process to flatten and export the “customer data” from its system Provision a new Snowflake instance Front-end and backend work to provide the customer a way to specify what data they want and how Translating the customer’s request into Snowflake and back etc. (Saving reports to a document store, emailing them, running them on a schedule, etc. etc. etc.) Be nice During the discussion, you will patiently go into as much detail as product wants about specific line items. You will explain acronyms and technologies. You will explain costs of each to the best of your ability. And then they can turn around and ask you “Do you really think I need all this?”. Ah who am I kidding? Product doesn’t give a flying fuck about your opinion, you dirty code monkey. They’ll ask themselves “Do I really need all this?” And the answer might actually be “yes”. Sure, it’s hard, and expensive, and time-consuming to do, but that’s how businesses make money; by doing the hard, expensive, time-consuming thing for their customers so their customers don’t have to. Granted, that’s only if your company hasn’t yet started confusing “build something our customers need” with “build something that conveniently fits into a 2 week sprint” (and that’s a big “if”). I’m not gonna hire a plumber just to wipe the gunk off my sink faucet. That shit is $87 just for them to ring your doorbell. Anyway, I digress. It’s a negotiation Understandably, product will want to deliver something to their customers sooner than six months, so they’ll reply with things like “What if we put off saving and emailing reports until later?” or “I didn’t realize it would take that long to get these running on a schedule and uploaded to the cloud”, or my favorite, “You are so smart and good looking and totally not a slob that eats lunch at their desk so often that their keyboard has accumulated a mass of crumbs to the point where the shift key sometimes can’t go all the way down, can’t we do everything in the platinum package in two weeks instead of six months?” A surprising number of developers fall for that last one. Like, most of them. Sure, you could “just” murder your database with table-scanning queries that join every single table and hope that you’ve provisioned a beefy enough machine to handle the load “for now”. Just like your plumber could “just” fix a pipe leaking on the floor by shoving a bucket under it and telling you to empty it every week. But you gotta take a stand on quality somewhere. Developers fall into the same trap of thinking in terms of whatever length of time their organization parcels out work into: two week “sprints”, six week “cycles”, what have you. And product is telling them the scope can’t budge. And you know sure as shit you ain’t getting any more team members to handle the load. What’s the only thing that can give at this point? Quality. That’s the iron triangle of software development, and you’re its latest victim. So let me squash the idea of sacrificing quality real quick by asking you a question: What’s worse, delivering something a customer actually hates, or delivering nothing at all? From painful experience, I can tell you that you’ll lose customers if you choose the former. And the former happens when you sacrifice quality. You don’t want a goddamn bucket under your pipe that you have to empty, you want a new pipe. Grow a spine and find a level of quality that you will not compromise on. For example, my plumbers won’t install any off-the-shelf parts. They’ll only install products that they supply themselves because they are confident in the quality. At least I charitably believe that instead of the alternative which is that these products are expensive as heck and yet another way to gouge me on the margins. My point is that quality has some fixed minimum and therefore scope and time are what budges instead. If you make a big enough of a stink you might even get more budget and a bigger team, but if you manage to accomplish that please tell me how. Now back to the show. It’s a negotiation, redux After you’ve decided to be a big strong boy that takes a stand on some bare modicum of quality, we can start the negotiations. When product says things like “What if we put off saving and emailing reports until later?” or “What if we dropped running reports on a schedule?” This is when you start bringing out your ‘standard’ and ‘economy plus’ packages etc. Because you’ve already gone to the trouble of treating these things as line items in an invoice (read: estimated their effort) you can tell product how much their chosen package will cost. You can even give them estimates on when you can deliver these line items, allowing your organization to plan over a longer time horizon with deliverable dates along the way. It gets interesting when product goes a little off-script and says something like “What part of the desired scope can you deliver if I don’t want to pay for an OLAP system?” Then you can get creative. You are negotiating with product on the two facets of the project that will generally change (mentioned in my rant above): scope, and time. If I were jaded (I am), I’d say the only thing that really gets negotiated is scope because for some reason every shop out there has decided that an entire project needs to fit within the arbitrary window of time that constitutes whatever definition of “agile” they’re currently following. If you’re reading this thinking, “No way, management at my company take a longer term view of things in favor of delivering best-in-class products”, then I implore you to include that information in a letter you send to: His Holiness Casa Santa Marta 00120 Vatican City So they can be canonized as the first living saint. In many ways, you are simply helping product to ruthlessly prioritize a list of work so that the team can deliver the highest return-on-investment (ROI) within the confines of the fixed time, and budget points of the iron triangle. If there’s enough good stuff left on the table at the end, you might have accidentally planned ahead for your next project for once. Turning it around If you’re doing your job right, there’s plenty of crap that YOU want to do, but so far we’ve only talked about making the people with the money give it to you when THEY want to do something. So how do you get product to give a shit about your architecture proposal when the tables are turned? Well, your local insulation company will gladly come to you, for free, and spend an hour with really expensive thermal imaging gear to find walls that need more insulation. Free because they know that your home’s “builder grade” insulation made of bits of drywall and discarded Monster Energy cans is literally garbage. They’ll tell you that you’ll save $50 a month in heating and cooling costs if only you could give them $5,000.00 now. You’ll quickly do the math and ask yourself a few questions: am I really that uncomfortable? do I plan on staying at this home for roughly another 8 years (the time it takes before I’ll recover the investment), and if I were going to invest five grand, would it be better spent elsewhere? Wow suddenly you’re the product team. Go take a cold shower. The equation doesn’t change Nothing has changed here except who came up with the idea. Be the insulation company. You still need to come up with line items in an invoice as before. You still need to negotiate time and scope. You still need to make a compelling argument about ROI. Here’s what’s not going to get your proposal greenlit: “I want to add a couple more tests to this code the intern wrote last summer” How the fuck is your company supposed to translate that into ROI? Try this instead: “I noticed that (business critical feature) is under-tested, so I would classify it as high risk of having bugs in it that would be showstoppers. An outage here could cost us big $$$ not just to fix it, but also in customer churn and reputation loss.” And even if your project gets greenlit, it may be after you make some concessions like the project can’t go on for longer than a week, or that you can’t bring in anyone else on it (that’s the negotiation part). You have some petty cash to spend The last point I want to make is that you probably have a little free time to do what you want at your job outside of feature work and chores like keeping third party libraries updated. Google called this “20% time” in the early 2000s (and their engineers called it “120% time”). For the last time, the equation doesn’t change. It’s just that the same person (you) holds the money and proposes the work. You may not need to write out your line items on paper (although it would certainly help); most devs somehow keep that in their head alongside all the movie quotes they endlessly repeat in Slack. Don’t feel obligated to immediately spend your allowance on small things like linting fixes. You can invest it each week towards something bigger like a pub/sub system so that you can pull a new microservice out of your monolith. Just make sure that the rest of the team is onboard with the bigger changes. Summing things up When you are elaborating work for product’s next brilliant feature that less than 1% of customers will ever use, elaborate how to do it right. The cost of time, scope, and budget fall out from there. Let product decide if they want to make that investment. Negotiate with your product team on what you can deliver and when, and never back down on quality. Easier said than done. Harness your inner plumber. Sell that platinum package. It will make dealing with all the shit everyday a little easier. Until next time. Share this: Facebook X Like Loading…",
    "commentLink": "https://news.ycombinator.com/item?id=41794566",
    "commentBody": "How to make Product give a shit about your architecture proposal (gieseanw.wordpress.com)153 points by andyg_blog 17 hours agohidepastfavorite136 comments seanhunter 14 hours agoOne of the most valuable life lessons is you can't get anyone else to care about what you want them to care about basically ever. You need to focus on the things you can control and one of the things you can't control is what someone else is going to care about. So if you want something done and someone else has to agree, you have to figure out how the thing you want coincides somehow with their interests and concerns. Then you explain the thing you want to them in terms of how it advances/affects the interests and concerns of the other person. So in the framing of TFA, product are never ever ever under any circumstances going to give a shit about your architecture proposal (because that is entirely in the domain of your concerns). But they may care about how the architecture is going to prevent them from delivering features that are on the roadmap coming up and how you have a solution that can fix that for example (because now you are in the domain of their concerns). Notice this is not just \"your architecture proposal\", it is how your architecture proposal is going to get them what they want, and if you want to do this you need to think deeply and make sure you really understand what they want, not just what you want. You're not trying to change their mind. You're trying to get what you want by showing them how it will also get them something they want. I'm putting this here because I really wish someone had told me this 25 years ago near the start of my career. reply fallous 13 hours agoparentIt's always a bit disheartening when I see someone from the engineering, product, or marketing sides of the business not understand the most basic of principles that salespeople learn at the beginning of their careers... people mostly make decisions based on needs, and if you don't ask and understand their needs you won't make the sale. Also, the more painful the problem that drives a need the more likely you will make the sale. I had the benefit of learning this before I ever went to University by working sales jobs while in High School, and boy has it made my life easier not only as a programmer but also in nearly any collaboration with co-workers. Don't recite features and benefits, that's just lazily hoping the person you're trying to convince will do your job for you. Take the time to ask enough questions to know and understand their needs. If the thing you're pitching can at least fit, and preferably help solve, some of those needs then you have a good chance of getting them to buy in. If, on the other hand, it doesn't address a need they have then you're going to struggle to convince them... and perhaps that may also be a clue that your solution may not actually be the best way to go. reply frant-hartm 11 hours agorootparentThat's why I hate sales people. Give me all the info on your product, ideally, make it easy to compare to others and I will decide myself. The idea that one can ask me a few questions and give good advice when buying a phone, a car, a house etc.. is just bizarre. Maybe it is not like that in the general population, but it certainly is within technically-minded people. reply wmal 11 hours agorootparentMost people don’t operate this way. Choice is painful and induces anxiety. There’s a high chance of getting buyers remorse even if you chose the „objectively best” model. A good salesperson will make sure the choice process is relatively quick and painless. You will feel good afterwards knowing that all the 125 aspects that differentiate this model from the other ones are not that important. The one you chose runs your favourite apps, integrates well with your car and your home entertainment system. Understanding this and learning how to sell helps in life, incl. negotiating architectural changes with non technical decision makers. reply squarefoot 8 hours agorootparent> A good salesperson will make sure the choice process is relatively quick and painless. The best salesperson isn't the one whose customers are leaving the shop smiling just like a TV advert where buying X or Y will solve every problem in life, but rather the one whose customers leave the shop angry after having purchased this or that product or service, because that is an indicator they were squeezed until just before the point they tell the seller to stick their product somewhere and leave for the competition. Not that I like it, but that is how I see it. reply binary132 7 hours agorootparentwhat makes you think that this is the best way to obtain loyal and more customers? reply ramses0 6 hours agorootparentPrisoners Dilemma vs Iterated Prisoners Dilemma. Low trust == maximize mechanical power and optimization Higher trust == invest time, relationship-building and lower individual transaction profit over a larger volume of profit Few consumer sales interactions fall into the second category. reply binary132 5 hours agorootparentprisoner’s dilemma is a dilemma for a reason: it optimizes the total outcome badly. maybe this is why the pessimization of the modern business cycle everyone loves to cry about always works out that way — we’re all interacting in a commons where trying to screw everyone else as hard as possible is the rule, not the exception. reply squarefoot 3 hours agorootparentprevDepends on the market of course, but scarcity, either natural or artificial, can do wonders. reply konschubert 4 hours agorootparentprevThis isn’t zero sum. reply actinium226 3 hours agorootparentprevOK but don't you hate it when you're trying to sign up for internet service and they're like \"what sorts of things do you do on the computer?\" I know what I need just gimme the 100 MBPs plan! reply pavlov 10 hours agorootparentprevI don’t understand why you’re downvoted. It’s absolutely true that most people don’t like making purchasing decisions by privately comparing spec dumps, even though many programmers enjoy that. reply jkestner 5 hours agorootparentYou're telling me some people out there don't create spreadsheets and a scoring system to compare 10 different ceiling fans before purchase? reply kamarg 5 hours agorootparentAbsolutely not. It should be abstracted out enough that it can be applied to all purchases and not just ceiling fans. Otherwise, you're going to be duplicating effort for the next purchase and you don't want to have to repeat yourself. reply jkestner 4 hours agorootparentI was actually thinking of scaling to a site first, and then expand horizontally to all ceiling-mounted products. Because what's the point if I can't monetize my decision-making system? reply lukevdp 10 hours agorootparentprev\"give all the info on your product\" Exactly except: If you're a CEO, the info you care about is one set of info. If you're a user, the info you care about is different. If you're some other influencer, the info you care about is different again. Everyone wants different sets of info. Good sales is figuring out what that is and giving it to you. reply itake 11 hours agorootparentprevGive me the info by email. I hate it when they try to get me on the phone. reply freedomben 5 hours agorootparentAbsolutely, me too. They push hard for the phone because many of the sales tactics don't work with email because they're based on non-verbal cues and subtle detection/exploitation. For example, many salespeople are trained to closely watch the person's face and identify what lands and what doesn't and dynamically adapt. They also want to be able to pressure you for \"next steps\" or the \"follow up\" meeting. I get it, a call can be a lot more efficient for a discussion. There are certainly legitimate reasons to prefer a call to an email. But it reminds me a lot of big tech companies seizing to things like \"security\" as an ~excuse~ justification for doing things that benefit them. I know the motives aren't pure, and that bothers me. reply n_plus_1_acc 11 hours agorootparentprevI feel the same. I want to make decisions based on facts, not emotional Manipulation. reply roenxi 9 hours agorootparentThat isn't emotional manipulation. It is what would get called \"good communication\" or maybe even \"empathy\" on a slow day. If someone is talking to a salesperson it can't reasonably be seen as manipulation when they explain why said person should buy the thing. That is the point of the conversation. reply walthamstow 9 hours agorootparentprevHow do you obtain objective facts about a product? reply lucianbr 6 hours agorootparentIf I buy a washing machine, it has a fixed known volume, weight capacity, power consumption, noise level. These are objective facts, no? Do you think we can have different opinions on how many kilos of clothes it can take, and be both right? They would come from the manufacturer manual or a spec sheet or something like that. Windows has some objective, known, minimum hardware requirements. Are they open to interpretation? What kind of products are you buying that make you wonder how to get objective facts about them? reply deely3 3 hours agorootparentI have feeling that this is quite an oversimplification. OS like Windows is times and times more complex than washing machine. I don't think that it makes any sense to choose OS by minimum requirements. reply binary132 7 hours agorootparentprevWhy have we developed a system of organizing engineering work that is based on engineers having to sell ideas to their colleagues? reply Loughla 7 hours agorootparentBecause their work impacts other people and therefore requires other people to be on board with it? Unless you're a 1 person company, nothing you do ever only impacts you. reply binary132 5 hours agorootparentIt’s really weird to me that people have such a hard time imagining a world without “scaled agile” and debate over duplo-scale priorities between product managers and nerds every two weeks. In a sane world, these kinds of decisions would not be a pseudo-democratic negotiation. reply kaashif 7 hours agorootparentprevThe fact that you have to explain this is somewhat concerning. Usually people say that people skills are as important for having lots of impact as technical skills, but the bar for people skills is really low sometimes I guess. reply bdowling 3 hours agorootparentprevEven a 1 person company has customers. reply binary132 2 hours agorootparentThe key word here is “colleagues”. reply rkachowski 5 hours agorootparentprevI feel this. The situation would be more obvious if it was framed as \"how to make the organization give a shit about the organization's architectural proposals\" reply datavirtue 7 hours agorootparentprevEconomics. reply vundercind 7 hours agorootparentIn a way, but I have a feeling the conquest of work-life by people who don’t understand the directly-productive work the company does is a result of economics operating in the 1980s—present era of unshackled M&A. It’s a regulatory outcome. reply binary132 7 hours agorootparentprevare you trying to suggest that Darwinian selection is the most efficient way to organize a collective to achieve a goal? reply roenxi 11 hours agoparentprev> One of the most valuable life lessons is you can't get anyone else to care about what you want them to care about basically ever. And, just to echo this, it is quite common to see people go down in flames because of issues they knew about, were told about repeatedly and simply didn't (couldn't?) take an interest in. A situation being important isn't normally enough to get people to change their methods. They generally just do what they always do come hell or high water. A lot of people seem to struggle with this and put it down to stupidity - which is correct, but it is more useful to see that one of the mechanisms is people not being able to do things differently based on how urgent circumstances outside their immediate concerns are. reply gwervc 8 hours agorootparentUrgency isn't a good metric in the corporate world because basically everything is called urgent all the time (spoiler: most of the time it's not). reply pjc50 7 hours agorootparentHence the urgent/important matrix: https://todoist.com/productivity-methods/eisenhower-matrix reply sgarland 6 hours agoparentprevI am slowly and sadly learning this lesson. My customers are devs, and I want so badly for them to care as deeply as I do about RDBMS optimization, normalization, and referential integrity, but by and large it’s a fool’s errand. What has worked to some extent is patiently walking them down the path of what happens if those things don’t happen, e.g. “so no one uses lookup tables or enums, so now every row has X unnecessary bytes, which puts pressure on the buffer pool, and then everyone’s queries slow down, and everyone’s SLOs trend down…” It doesn’t always work; I’ve also had people respond that “they’ll fix it later,” (lolol sure) but it’s had better results than simply explaining why their schema is technically sub-optimal. The absolute worst to deal with have been those who seem to completely lack empathy, and respond flatly with, “fixing that isn’t on our roadmap, and isn’t likely to be,” even when I explain that in X months, my team will be suffering from their decisions. reply freedomben 5 hours agorootparentMaybe you already do, but I've had this exact challenge in the past as well and what has worked best is to send them some SQL queries that do it the \"right\" way. Often times I think they just don't want to deal with the problem: it's hard, it's uncomfortable, and nobody \"up the chain\" will care about it. There's plenty of reason not to want to do it. But giving them the query, or writing the migration for them, often takes care of both one and two. I've even seen this approach ignite a passion for query optimization as it \"clicked\" for them! reply sgarland 2 hours agorootparentI do this when I can, certainly, but there are only so many hours in a day, and I have larger tasks to deal with much of the time. What I’d love is for devs to treat SQL the same as they would their primary language, instead of some mysterious and arcane artifact to be abstracted away and ignored. If I refused to use any data structure other than a dict / hashmap because “it’s good enough,” how do you think that would go over? reply tomjen3 4 hours agorootparentprevOf the items you listed, I only care about referential integrity (or rather, I don't want things to be wrong). Unless we are adding millions of rows on a daily basis, microoptimizations don't make a meaningful impact. Of all the optimizations I have meassured (and I have meassured a fair number), only two types have really moved the needle: do less, and use a better algorithm. If we need to shave a few percentage of the database access, it is more cost effective for us to get a more powerful database server. Assuming that we already have a cache and aren't doing something stupid like not using an index. reply sgarland 2 hours agorootparent> Unless we are adding millions of rows on a daily basis, microoptimizations don't make a meaningful impact. At toy scale (if your entire dataset can easily fit into memory on a small instance; probablyIf we need to shave a few percentage of the database access, it is more cost effective for us to get a more powerful database server. People always say this as though it’s fact, but I’ve yet to see any studies on it, and frankly I doubt its veracity. Take the current RDS pricing for on-demand Postgres, r7g.large vs. r7g.xlarge: in us-east-1, it’s $0.239/hr vs. $0.478/hr. That difference works out to about $2000/yr, for a single-AZ DB of the smallest non-burstable type they have. Since microservices remain all the rage, multiply that by N. You’re telling me it’s not worth the money to spend a few hours reading docs and applying the knowledge? I don’t buy it. Moreover, as someone who enjoys the craft of SWE, it’s physically painful to watch people say “eh, let’s just throw money at it” instead of understanding where their code is slow, and solving it. reply kqr 11 hours agoparentprevGenerally, I've found more success in professional interactions if I approach them not with the intent of convincing the other person of something, but with the intent of learning about their fears, concerns, and desires. It takes a little longer and gets to my desires only obliquely, but I still tend to like the outcome more. reply ahoef 14 hours agoparentprevIndeed. Things I've used before are: \"product speed is declining, see this chart. We must make these optimization to stay acceptable.\" And for example \"We must migrate to this hosting model to keep compute costs down.\" reply dns_snek 6 hours agorootparent> product speed is declining, see this chart. How do you measure product speed in a useful manner? I'd disqualify metrics such as number of tickets, lines of code, and \"story points\", because they either have an undefined relationship with product velocity (e.g. LOC per week), can vary in size too much to be useful (e.g. number of tickets per week), or they're tautological (e.g. story points per week). What's left? reply actinium226 3 hours agorootparentI've noticed no one seems to look at number of PRs per week or average length of time and MR is open. Those seems like they should be good metrics for both how well a team is collaborating and how fast they're moving (perhaps normalized by LOC in the MR, but then again smaller MRs tend to go through faster so maybe LOC is already reflected in the metric). reply hiatus 5 hours agorootparentprevI took it to mean the speed of the product, not developer velocity. reply jaza 13 hours agoparentprevThanks for that nugget of advice! I probably already know it deep down inside, but seldom remember it, and seldom act with that mindset. But yeah, so true. Don't try to make someone give a shit about something that they have no reason to give a shit about. Instead, tell them what's in it for them, then negotiate so there's still something in it for you. reply wfjackson3 3 hours agoparentprevI agree with this. I want to add a product perspective here too. If your product partner routinely tries to understand your suggestions and translate them into the things that matter to themselves and the broader organization, that's a good partner. And on the subject of things that are good to learn early in your career, everyone should know that every business is barely good enough for what it is. If it was materially better at something else that mattered, for example to its market or its economics, it would be a different business. For profit businesses are pretty effective equilibrium finding entities. I used to get really frustrated that people didn't care about improving things that I wanted to improve, until I realized that in most instances those things wouldn't really change any outcome in the business. In the cases where it would really change the business outcome, I realized the company couldn't prioritize effectively. reply pjc50 9 hours agoparentprevThis is small-p politics. In a hierarchical organization, you can give directions to your direct reports. You cannot give directions sideways. You certainly cannot give directions upwards, unless it's for something legally binding like safety. This means you need to ask nicely, to persuade, invite, and probably compromise. It's a very different set of skills. (a \"non hierarchical\" organization has a hierarchy too, but it's more fluid and hard to see) reply lmm 5 hours agorootparentNah. All organisations have relationships, and the ones that appear on the org chart are usually a small subset of them. Actual hierarchies are extremely rare, e.g. even in an officially very hierarchical company there will usually be loops because someone nominally low actually controls someone nominally high. reply pas 7 hours agorootparentprevAlso, for anyone who's new to this, hierarchical organizations have an informal/shadow hierarchy too, usually an extension of the de jure one. reply larsrc 8 hours agoparentprevThis! It's the basis for fruitful negotiations to understand what the other person needs (which is not always what they say at first). Sometimes you can give them something that's cheap for you but valuable for them or vice versa. I can recommend the book \"Getting More\" by Stuart Diamond for valuable insights on how to do this. reply ghaff 8 hours agorootparentThere’s at least overlap with needs but also what their biggest pain points are currently. One way I’ve often heard it put is sell aspirin not vitamins. reply pas 8 hours agorootparentprevAnd one thing that usually is cheap - as an architect - is putting yourself into the other's shoes. understand them, their lingo, their model their immediate context (cash flows, promotion, trends, cultural signifiers, business vision, their own career dreams) ... it's the interface you need to be able to actually model the problem. reply brotchie 13 hours agoparentprevOne of the few strategies that I've seen works is \"I'm going to do it this way, and I'll take all responsibility for it failing.\" and then if it fails, actually take responsibility for the failure. You have to have a certain confidence in your opinion, and you have to be prepared to destroy yourself mentally and physically to deliver if things go off the rails. But once your deliver something that matches your vision, usually worth it. reply beAbU 11 hours agorootparentIn certain adversarial workplaces this basically gives the other party a perpetual license to blame you and your work for any failures, no matter how circumstantial the link. reply pas 7 hours agorootparentdepending on how much weight their voice has it might well worth the risk. (as you gain naysayers but you also deliver results, and that might make certain important folks happy, or at least it will look good on your resume, etc..) reply vundercind 7 hours agorootparentprevHaving watched nearly all my work ever basically be thrown away, usually way before it actually provides enough benefits to have been worth the cost, and for reasons that have nothing to do with the quality of the work: yeah, I definitely don’t care enough about anything I make at work to stick my neck out quite that far. There’s professional, then there’s unhealthy. reply auggierose 10 hours agorootparentprevI don't think that destroying yourself mentally and physically should be part of your toolbox. reply pjc50 9 hours agorootparentprev.. why would you do that? Martyrdom complex? reply keithalewis 12 hours agorootparentprevDoctor, it hurts when I do that. reply workflowsauce 2 hours agoparentprevI think of this as empathy. Step into their frame, look around, and then come back. reply jauntywundrkind 9 hours agoparentprevWorth adding that managers have to report upwards. So you not only have to be able to generate sympathy for someone who presumably knows your product & team well, you have to arm your manager to go upwards with a good story about where your team is spending its efforts that even less technical less intimately connected business operators will need to at least check off on. reply drewcoo 5 hours agoparentprevThere's also the Bernays approach: sell your design as filling some emotional need and convince the stakeholders that they have that need. Automobiles will make you free! This kind if free software is \"free as in beer\" and who doesn't want more beer? My queuing system will make you virile and attractive! Whatever works. reply travisgriggs 13 hours agoprevAh, the contract relationship. Minimize your output while maximizing your ROI. I think this works fine if you plan on being in the software \"service\" industry, kind of like the plumber. It helps if job mobility/demand is high so that when the \"product\" decisions tank the company/product, you just shrug and move on. That's what the plumber would do. In fact, if your customer does the stupid thing, and ends up having to have you come back to pay even more money, hoorah, more money. It's a reverse incentive. Do it enough, and you'll be just like Boeing and the US Government. I've also observed that this type of relationship tends to cause people to optimize short term over mid to long term. Where this deteriorates (imo), is if you're in it for additional reasons other than only the money, but want to build quality things, make the world a better place, yada yada. Or perhaps you like the company and what they do, or something about your job, and actually depend on long term viability. By somewhat strained analogy, imagine the plumber works for a housing co op, and they themselves live there. Suddenly, the plumber becomes a bit more coupled to the choices of \"product\" or the customers. Poor decisions could devalue the neighborhood and your own resale value, or even damage your own property. reply DanielHB 11 hours agoparent> Ah, the contract relationship This is why outsourcing usually goes bad I am from Brazil and I often try to explain people from other countries that if you really want to outsource work you _have_ to build an office in the target country that _really_ works in the same way as the HQ. But that is far more expensive of course. The people in Brazil who end up in those kind of outsourcing \"software factories\" are not the ones most Brazilian product companies want. reply dajonker 8 hours agoprevIn my experience/opinion, if you are mature enough as an engineer(ing team), you should be able to make and implement most technical decisions on your own. Discussing technical changes with non-technical people is usually a giant waste of time. They often don't understand what you want to do in the first place and think it's optional, because why else would you bring it up for discussion? Similarly, don't plan refactoring or architecture changes on the same board as your feature requests. Product managers will think they can change the priorities of this type of work and then they will. The only thing that is useful to discuss in advance is scheduling when you can take the system down for maintenance. For the rest, it's better to ask for forgiveness than to ask for permission: just do the changes that you think are technically necessary to keep everything running smoothly. Measure the outcomes and present them professionally. Of course, with maturity as an engineer, I mean that you will not decide to do crazy, hype driven things like rewriting everything in Rust and then not doing any new features for a year. You have to be able to make trade offs and compromises to keep both sides happy. reply nucleardog 5 hours agoparent> Discussing technical changes with non-technical people is usually a giant waste of time. They often don't understand what you want to do in the first place and think it's optional, because why else would you bring it up for discussion? This bears some—well, all of, the emphasis. I’d come at it even stronger I think. Asking people in non-technical roles to make technical decisions is a complete abdication of the responsibilities of your role. You were hired into engineering to take care of the engineering. That is your role. When’s the last time someone in sales showed up and asked you “This guy wants a 25% discount. Do you want me to give it to him?”. They _may_ show up and say “This guy wants a 25% discount and I’m trying to get a better understanding of our costs. Would we be taking a loss on that? Can you help me understand the costs of delivering service X?” And that’s exactly how engineers should be approaching this. The technical decision is yours, however you probably don’t have the same context everyone else has. You _should_ discuss your decisions with others and consult with them, but that discussion is best had in terms of the impact the decision will have on their area of responsibility… which is not technical decision making. In the situations where engineers are going to product to ask whether we should move to microservices or if this UML diagram makes sense, I’ve always seen engineering looking to pass off the decision so they can pass off the responsibility. I’ve run across this in multiple organizations and it was completely dysfunctional every time. And in every case simply replacing the engineering manager with someone that wasn’t afraid of decisions or responsibility quickly resolved the issue. (Which is probably you if you’re in this situation… if there was engineering above you, you’d be asking them instead.) The discussion to be had should be more along the lines of “FYI, we’re looking to fit in three weeks of additional engineering work this quarter to substantially lower the costs of working on service X going forward.”. Product can discuss their priorities, or even share some information you don’t have yet like “We’ve been discussing axing that service entirely. Our tentative offboarding plan is having everyone off by Q3. Do you think the investment’s worth it?”. If you continually ask someone else to do your job… well, be careful what you wish for. reply wfjackson3 3 hours agorootparentThis is a great response. I would add that this is how any larger project gets evaluated and prioritized. In smaller companies, especially startups, I have seen the friction show up when the company is too small to have any staff dedicated to working architecture and engineering facing engineering systems, but big enough that their past choices are creating friction. At that unique point in time, it's really an organizational issue more than anything. reply dajonker 5 hours agorootparentprevYou hit the nail on the head, it's about responsibility and trust, and this is often lacking in various ways when tech feels like they need permission from product to spend time on anything. reply binary132 7 hours agoparentprevThis is it, right here, but the problem is that in the modern corporate agile model, they want to plan ALL work done through optimizing the backlog with product, and they want ALL of the available hours to go to that work. That causes these teams to have a very difficult time negotiating the right things into their workload, and if you do it by fiat other things necessarily get pushed out. reply llmthrow102 13 hours agoprevMaybe it's the norm, but it's a dysfunctional company if you have engineering that only cares about \"doing things the right way\", product that only cares about \"get the next feature out as soon as possible\", and corporate that just thinks \"minimize software development costs\". And on top of that, you have arbitrary regular deadlines that dictate the flow of work. That points to everyone being focused on their own goals rather than working together to deliver the product that will satisfy customers the best. reply friendzis 13 hours agoparentI can tell from your comment that you are on the engineering side of things. Corporate should focus (among other things) on reducing COGS, product again should focus on delivering customer facing features. The problem are timelines. We all know what technical debt is. You can cut corners and rush a feature out, however at the expense of future velocity. When engineering and product collide, engineering triangle forms and compromise has to be made. The classic triangle is defined as quality -- speed -- cost, however that is more applicable externally. Internally the compromise triangle looks more like \"fixing bugs -- delivering features -- maintaining future level of bugs\". The more the triangle is stretched towards \"delivering features\" the more maintenance suffers. I have seen this coming from engineering far to often. Oh no, product are idiots, they do not understand importance of bug fixing, cleaning of technical debt and maintaining architecture. Believe me, they are roughly as smart as you, you are in the same broad team anyway. Where product fail is at estimation. They lack domain expertise to correctly gauge the impact of rushing a feature on future velocity. They probably understand this relationship, but they cannot quantify the effects. This is where engineering comes: to provide domain expertise. As TFA correctly implies, product does not give a shit about architectural decisions. For all they care it can be held by either literal or metaphorical duck tape. It's the job of engineering to quantify the cost of rushed features. If engineering thinks that product are idiots for rushing n features, then either 1) engineering fails at understanding business goals or 2) engineering fails to convey impact of rushed features. Both are communication problems. Interestingly, the onus is on management to sort internal communication out. reply stavros 9 hours agorootparentIf my job is to prioritize work and understand all the tradeoffs involved in that work, you can be damn sure I'll go understand the tradeoffs. If Product don't understand that technical debt makes things slower, and exactly how much slower it makes them, then they aren't doing their jobs. My current role is \"Director of Product and Technology\", so I have to look after both domains. I have deep knowledge in technology, but if I'm not going around the company asking other departments what the impact of the work they want is (and what happens when they don't get it), I'm just plain bad at the product side of my role. reply IshKebab 12 hours agorootparentprevYeah.. I think the problem is when product dictates what is going to be implemented without asking developers if it's feasible. It happens. At the other end of the scale there are many programmers who are in the habit of making up bullshit technical reasons why something can't (or shouldn't!) be done when the real reason is they just don't want to have to do it. Often they'll resist doing a useful feature because it can't be done perfectly. For example we can't report browser tab memory usage because some memory is shared between tabs so the numbers wouldn't make sense. I used to do that until I had a manager that changed my view. reply friendzis 10 hours agorootparent> I think the problem is when product dictates what is going to be implemented I don't think of that as a problem, that's one of the primary goals of product. Problems (yuuge problems) arise when product also gets to dictate cost/timelines. Sorry, that breaks basic management principles. > I used to do that until I had a manager that changed my view. Small, young teams (e.g. startups) can easily do without management, because communication is unhindered and ad-hoc. The more organization expands and matures, the more communication suffers. That's the primary goal of engineering management - facilitate conversations. When I have a request that is tad too technical I always try to backtrack and ask what's the business goal. I am 99% certain \"display memory usage per tab\" is not the business goal. \"Find resource hungry tabs\" sounds like a good candidate for a business problem. \"Customers\" (e.g. product) tend to be \"helpful\" and provide technical implementation details, diluting the business problem, while engineering tend to fixate on those implementation hints as if they were technical requirements. Ever noticed how technically inept product managers/owners sometimes tend to be good managers? Well, they are either aware of their technical ineptitude or are inept so much that they can't even express technical details and form their requirements as business questions which leaves implementation details open and allows engineering to implement things \"correctly\". It's magical how simply communicating on appropriate abstraction level can lead to awesome results as each team can focus on what they are strongest at. reply pjc50 9 hours agorootparentSome of this is what stackoverflow calls an X/Y problem; someone has a problem, got halfway down a route to a solution, and now is talking to you. It can be quite difficult to dig down into what the actual original problem was, then persuade them to back up and pursue what is in your opinion a better solution. reply hackernewds 10 hours agorootparentprevHow and what did your manager change your view to? reply IshKebab 9 hours agorootparentVery briefly, I resisted implemented features that wouldn't work perfectly. The memory use example was a real one (I was working on a profiler of some complex AI hardware). My boss would say things like \"can we report how much memory this operation uses\", and I would say \"no because some of it is shared with other operations or only live for part of the program run etc.\". He didn't really say anything to change my mind, he just kept asking for things that would be useful to customers and eventually I realised that even if we can't give an answer that makes perfect sense or doesn't work all the time, we can still do better than nothing. Very often something that is roughly right and can be shown some of the time is better than something that doesn't exist at all. It kind of sounds obvious when I put it like this, but you'd be surprised how often you see \"we can't do this very useful thing because \". reply dmead 10 hours agorootparentprevThis is a garbage take. I'll write more later. reply dmead 1 hour agorootparentupdate: i thought that was a good enough MVP and decided not to do it. reply njtransit 13 hours agoparentprevThe issue is that most people are not cross-functional thinkers. Those who are not generally fall prey to the “if you have a hammer, every problem is a nail” fallacy. Engineers want to engineer, PMs want to add features, managers want to “manage”, etc. reply llmthrow102 13 hours agorootparentYou don't need everyone on every team to be a cross-functional thinker, but you need the people who are working cross-functionally to actually think about the big picture and realize they're optimizing for company success and not some arbitrary, often ambiguous goal like \"good engineering\". Those people that are in the decision-making process need to then communicate the result of the decision with their team, and be able to justify the decision. reply njtransit 47 minutes agorootparentBut the people in cross-functional roles are still being drawn from a probability distribution in which most people are unable to think cross-functionally. reply danielmarkbruce 15 hours agoprev> Sure, you could “just” murder your database with table-scanning queries that join every single table and hope that you’ve provisioned a beefy enough machine to handle the load “for now”. Just like your plumber could “just” fix a pipe leaking on the floor by shoving a bucket under it and telling you to empty it every week. This is the actual solution in the vast majority of circumstances. If after x days you realize you've made a terrible mistake, that's a nice problem to have. reply schnable 6 hours agoparentI had the same feeling when I read this. This engineer needs a product person pushing them to rethink their architectures, and shows that it's helpful to have technically minded Product Managers that can call BS on engineering when warranted. reply sneak 14 hours agoparentprevYeah, the whole article seemed “webscale for webscale’s sake”, despite describing a product just a few steps beyond MVP. Overengineering abounds. It’s easy to feature flag something and roll it out only to a fraction of the userbase and see if the database falls over, or if you’re biased toward acronym/resume-driven-development. We’re almost certainly talking about megabytes here, not terabytes. reply et1337 13 hours agorootparentThis line at the end of the article triggered me: > You can invest [your spare time] each week towards something bigger like a pub/sub system so that you can pull a new microservice out of your monolith. reply danjl 14 hours agoprev> The reality of the world is that product holds the money and software development is seen as a cost center to be minimized towards zero. This is only true for mediocre companies. Software development is really a force multiplier, not a cost center. A good optimization for a proprietary app saves time for everyone at the company that uses the app, or for your customers if the app is a product. Also important is that the developers can dramatically alter the cost structure both for the current set of features, as well as the future support and additional features. Product faces outward, helping prioritize features for customers. Development faces inward, minimizing tech debt and future work for the company. Both halves are needed to design and build a good product in a reasonable amount of time. reply shreddit 11 hours agoparentIn a \"perfect\" scenario the product would not require any more development and thus development could be reduced to zero. reply altacc 7 hours agoprevSimpler to define what you want to do as Option B then produce an expensive & time consuming Option A and an unacceptable, risky and business damaging Option C. Then show the Powerpoint to the C-Suite and let them think that going for Option B was their great decision making skills. Everyone leaves happy! reply nucleardog 5 hours agoparentWorks great until someone’s asleep at the wheel and chooses option C! (Speaking for experience watching exactly that happen to someone else… repeatedly.) If there’s one right answer, why even present options A or C? If there’s only one realistic option there’s no decision to make. Just go ahead and do the thing. reply tyleo 7 hours agoprevI haven’t seen this work in my experience. If you are going to threaten to delay a project for 6 months you really need to deliver or you are just burning product’s faith in you. I’ve seen this strategy play out and the result is just a deteriorating trust between product and engineering, “we really needed to wait 6 months for this?” IMO it is a negotiation but the answer to that isn’t “threaten product with huge timelines.” Folks really need to come together and understand which parts of the architecture roadmap *can* be band-aided for now and which can be completed properly within the context of the product’s upcoming needs. reply spo81rty 15 hours agoprevIf you are building software to build a product, you are on the product team. reply bryanrasmussen 14 hours agoparentIF you are building software to build a product, and the company has put you on the product team in their organizational structure, you are on the product team, if they haven't you and they have a problem. reply pjturpeau 9 hours agoprevIf there is no product, there is no need of any architecture. By the way, multiple architectures will give you different product features. So, what do people need in the product drives the functional architecture, while regulation, performance and pricing will drive the technical architecture. Excessive craftsmanship and over-engineering may kill your product as much as over-selling features may kill the project. reply binary_slinger 15 hours agoprevThis is ancillary to the discussion but that was a scummy plumbing company. When I became an homeowner I read up on codes and took a few classes on plumbing, electrical and HVAC. A lot of these companies take advantage of the homeowner not being knowledgeable in those areas. Sometimes these people that come over to your house are sales people and not actual tradespeople. reply linsomniac 6 hours agoparentSlightly related: Water pressure regulators have a typical life-span of 10-15 years. Found that out when a co-worker did battle with some low water pressure. reply mutator 14 hours agoparentprevVery curious to learn about what classes one can take to build up a rudimentary knowledge of plumbing, electrical and HVAC. reply linsomniac 6 hours agorootparentI believe Home Depot runs classes on various DIY type things; I've never taken one but I've seen classes listed when I'm walking in the front door. YouTube and a willingness to experiment will take you a really long way. ElectricianU was really good for learning about the electricals. I've been in this house for a decade now, and I pretty much do everything on it including a down to the studs remodel of a bathroom and kitchen, re-running and adding electrical circuits (replacing and pig-tailing aluminum)... I'm on the fence about whether I'd replace the furnace when it's time, it will need a new intake/exhaust run, but otherwise should be fairly straightforward I'd think. I did just pay for a new roof. Just be patient with it, it will take longer than you'd like and longer than you'd expect, especially if you can only fit in time on weekends to work on projects. I never seem to have much gumption left after work. YMMV related to pulling permits. Our local building dept is really easy to work with as a DIYer. reply pmulv 14 hours agorootparentprevI’ve been a homeowner for a mere two years and I’ve discovered many things wrong with my house. YouTube has a video outlining what I need to fix and how to fix it 90% of the time. Tradespeople get called for the remaining 10%. It’s possible a class with a curriculum and structure would help but YouTube has what you need for ad hoc homeowner issues. reply hinkley 14 hours agorootparentprevBinging old episodes of This Old House will give you about the same level of understanding that a football fan has of his favorite sport. You won’t necessarily be able to fix things but you’ll know something isn’t right. reply eszed 4 hours agorootparentThat's an excellent analogy. I'd push it further to say that there are YouTube channels (like QB School, to stick to football) that can take you to level 2. reply Joker_vD 9 hours agoprev...the parable (I hope it's a parable!) with the plumber is atrocious. You have a suspicion of a leak in the water main, and the plumber instead offers to re-do your whole in-house waterworks while evading the question whether there is actually a leak in the main, or whether our current in-house plumbing is failing. Oh, and also lying about the pressure regulators only being settable once, to boot. Just pay through the nose for all those wonderful new shiny things, come on, you planned on renovating your house in the near future anyhow, right? Why do you propose to us to be like this? reply DeathArrow 11 hours agoprevA plumber is not worried about quality and the future well being of the home owner. Similarly, a developer might not be interested in quality and customer satisfaction, but in doing the things that will yield him better outcomes: a better position inside the company, better payment. reply jblecanard 9 hours agoparentThen you have a mediocre developer not willing to align his interests with those of the company. Cannot end well. reply p0w3n3d 13 hours agoprevSometimes product agrees with Client to do something they do not fully understand. If a software engineer fails to detect this part, product will commit to things that are yet to be specified. Client will start writing letters to Santa but all of them will bind. I had been in such situation and it was nightmare reply devjab 12 hours agoparentI disagree with this take. It’s software engineerings job to figure out, how, to do it, not if it should be done. A plumber isn’t going to refuse adding another bathroom to your house, they’re going to tell you how it can be done and tell you the cost and consequences of each option and then let you decide. A good mantra in anything related to any sort of business is that anything can be done, it’s just a matter of cost. Of course it’s on the business to accept that, and if they can’t, well then you can’t deliver what they want. Which is probably what you meant, but it’s only at that point you should say no. reply jamil7 12 hours agorootparentIf you’re leading an engineering team in a product company you should absolutely be asking why and trying to find the actual problem that’s often buried amongst solutions that are brought to you from the customer or product or sales. This is how you deliver faster, cheaper and simpler solutions. reply devjab 11 hours agorootparentWhat often happens, however, is that when a customer asks for a new bathroom. Engineering will point out that they won’t need it in a year because the customers children at moving out age. Not considering that both the customer and product is well aware of this and their motivation is completely different. Even if your engineering department is quick to accept that, it’ll be a waste of everyone’s time that engineering chose not to trust their own organisation to be good at their jobs. Leading to a rift between software engineering (and IT in general) and the rest of the organisation. This eventually leads to IT not understanding why departments start buying their software systems from 3rd parties, or why nobody in management will listen to them. It’s the job of engineering to point out that adding that bathroom will require a massive rebuild to preserve the integrity of the structure of the house. It’s not their job to tell product that product actually doesn’t want what they are asking for. reply jamil7 11 hours agorootparentI think in the scenario you’re presenting the engineering team has failed to surface the problem or the other teams have failed to accurately present it. I do agree with what you’re saying about engineering teams becoming bottlenecks if their immediate instinct is to derail or mistrust other teams and once the cycle has started it’s hard for the teams involved to break out of this pattern. I do still think there is a responsibility for engineering teams, especially at small companies to look for opportunities to solve problems in a simpler or cheaper way if they can and that might not be obvious to other teams. The customer might have actually just needed a sink or second toilet for example. reply nitwit005 11 hours agorootparentprevPlumbers will absolutely refuse work. You can just go to /r/plumbing and see discussions of it. People in just about every profession get asked to do things that are illegal, unethical, unsafe, or impossible. reply devjab 6 hours agorootparentPlumbers do have the advantage of being third parties though. As a side note, I have no idea what you mean by /r/plumbing. Is that something that I should know? reply skydhash 6 hours agorootparentThe “plumbing” community on reddit. That’s the naming scheme reddit uses. reply pnut 6 hours agorootparentprevThat's how Reddit is organised. reply scott_w 12 hours agorootparentprevDisagree. This position completely disregards our experience and expertise. We need to partner with our Product counterparts to figure out what our customers’ needs are. We must also be clear what our operating constraints are so PMs can make an informed decision on what to prioritise based on a reasonable idea of the costs/benefits. Doing anything less is basically shitting on Product and, if you have that attitude, why do you think you deserve to be treated as an equal in the conversation? reply devjab 11 hours agorootparentWhat makes you think you know more about customer needs than the people working directly with the customers? I think we sort of agree though. I think presenting product with various options and letting them decide includes a lot of what you suggest here. Including working with product. Ultimately though, it’s the job of engineering to deliver what creates business value. Refusing to add a bathroom to a customers house because there are engineering concerns or thinking you are better at spotting customer needs than product is the opposite of that in my opinion. Of course the flip-side or this is that you need an organisation which will accept it when engineering points out that adding a bathroom will be widely expensive because the foundation needs to be reinforced, or maybe the entire house needs to be rebuild. Without that you end up with Boeing. I do think that thinking you know better is unfortunately one of the pitfalls of our profession because we’re so used to working with patterns, but often engineering won’t even be told the full picture. I find it to often be a humongous waste of time if engineering has to be taught why something is actually necessary before they can get on board with it. This is not me saying that forcing engineers to do something they think is a bad idea is the right way to do things. This is me saying that I prefer engineering departments which are cultivated towards delivering value, and not being obstacles you need to “convince”. This is so often the reason software engineering (and IT) in general is disregarded or seen as “them” in organisations, because they are the people who deliver problems rather than solutions. reply scott_w 10 hours agorootparent> What makes you think you know more about customer needs than the people working directly with the customers? I never said I did. What I said was that we should not disregard our own knowledge and experience when working on our products. We should be expected to get a good enough understanding of our customer/user needs to be able to challenge Product prioritisation and also to make our day-to-day decisions better when building out the product. > I think presenting product with various options This wording implies an abdication of responsibility in my opinion. We aren't \"presenting options and letting them decide,\" we're collaborating with our Product counterparts to help them figure out how to prioritise which customer needs we tackle first and how we could address them. On the flip side, our PM can (and should) understand and challenge our technical considerations. In some of the examples given, maybe we can run a restricted set of reports or not allow certain features, or build a PoC for a smaller user subset just to validate the idea. That collaboration needs to be built on a foundation of trust and knowledge of each others' strengths. My manager trusts my technical knowledge and my people-management skills but he'll still challenge my decisions where he may have a different context or point of view. Just as I do with my direct reports. > Refusing to add a bathroom to a customers house because there are engineering concerns or thinking you are better at spotting customer needs than product is the opposite of that in my opinion. > I do think that thinking you know better is unfortunately one of the pitfalls of our profession Since I never said any of these things, I don't see any need to address them. reply bilekas 9 hours agorootparentprev> It’s software engineerings job to figure out, how, to do it, not if it should be done. This isn't true in the explicit sense you've written it at least. How it should be done is very dependent on if what was specifically asked should be done. Client says they want automatic payment processing without users approval. Technical capability, absolutely. Client says they want to disable unsubscribe payments options to retain more subscription revenue. All can be done from a technical standpoint, but its an engineers job to explain to the client that it can't be done that way. reply genezeta 7 hours agorootparentprev> A plumber isn’t going to refuse adding another bathroom to your house Many plumbers will definitely refuse doing some things. They will tell you something like \"we don't do that sort of work\" and tell you to find someone else. reply darkerside 15 hours agoprevThis person doesn't seem to realize that the plumber was selling them a bunch of shit they didn't really need because it was a good way for them to make money. The analogue would be an engineer proposing a gold plated submarine instead of doggy paddling across the lake. This plays into the worst fears of the product manager, that you are engaging in resume driven development that benefits you, your sense of fun, and your personal growth, over what is best for the product and the company. reply EZ-E 13 hours agoparent> This plays into the worst fears of the product manager, that you are engaging in resume driven development that benefits you, your sense of fun, and your personal growth, over what is best for the product and the company I saw someone do exactly that - the problem to solve was simple, but one of the goal of the tech lead was to be able to do a tech talk about the solution, he was just in the company to deliver this project so unsurprisingly it ended up being massively over engineered and a difficult to maintain. To add an attribute in the response of an API you would unironically need to edit 10 files, and this is for a small service. reply et1337 13 hours agorootparentOnly 10 files? Must be nice! reply EZ-E 11 hours agorootparentIt's a small service and we were a small company... reply andyg_blog 14 hours agoparentprevI think I did realize that. However, if you as a homeowner say \"I really super duper don't want any scale buildup OR the minerals to be dissolved in whatever comes out the faucet\" then suddenly the proposal is actually reasonable. Metaphors only go so far. Try to see what I'm really saying here: quality has a cost. Don't shoot yourself in the foot by preemptively reducing quality on account of some ill-conceived notion about how the relationship between product owners and engineering works. reply monkeydust 6 hours agoprevYou need to find common incentives, if they don't exist then try to create them. reply ath3nd 1 hour agoprevI am going to go on a sarcastic rant here. Why should I care if Product cares about my architecture proposal? They are not qualified to judge neither the quality, nor the priority of technical architecture decisions. Those are matters for engineers. Are we doing some \"ask the least qualified person to make the decision for you\"? Why don't we ask random people from the street, while we are at it? I might be coming off too harsh, but the whole premise of involving people who are not technical or mildly technical, into making technical decisions, is hilarious to me. Shall I crash the sales' meeting, and tell em how to approach this important client, or expect someone from legal to wait for me to decide that contract is legally sound? reply dclowd9901 14 hours agoprev> This means that you gather up all the information you need to give product exactly what they want, and then you come back to them with an estimate: six easy monthly payments of $2500. Or, rather, you say “One full time mid-level engineer’s time for 6 months on our team, plus one full time engineer’s time from the Infrastructure team.” But isn't the problem with this whole idea that it outlays a possible sale of a \"bill of goods\" insofar as we don't actually know if a project of this magnitude will actually take the time we say, and require the resourcing it requires? I'm sorry, but I disagree with the entire premise of this post. Product may have the money but money doesn't do much on its own, and that's more an unfortunate artifact of business-school-oriented modern org charting caked with plenty of management self-importance. If they want a product, they'll give me the time _and_ the money and go back to dealing with customers and investors. This post is _exactly_ the reason software development today is a shit show of agile and mid-level managers deciding what tech debt is worth addressing. I daresay product can go away entirely, and I would bet the product would still get built. We have the money and time to build great products. We don't need to sell product on it. We just need to be left alone to do it. reply Jakawao 14 hours agoprevI reference this post again and again when thinking about these \"fun\" discussions. https://news.ycombinator.com/item?id=14070189 reply anonzzzies 14 hours agoprevThe plumber example is quite good, but I prefer the modern car mechanic that connects a computer to a car and reads out things usually even they don't understand which have to be fixed and no one can explain anything but 'engine bad'. Most of the Product people I work with and have worked with collapse easily under technical arguments, but most engineers I work with barely know what they are doing. Only yesterday I (external consultant) asked the tech lead to scp a file from a to b during a workshop zoom where I showed them how to use some new tools and, while he always talked in front of the cto and head product about ssh and scp and his linux chops, he had no clue; he started to download gui tools (windows) and after he was done he still couldn't. I ended up copying the f'ing file to chat (I have no access to their internal systems). This is the guy they trust with core products dev that runs the company. He gets away with it because he talks in difficult tech bla to the cto and product (both MBAs); he keeps using terms from kubernetes (they don't use kubernetes and the guy cannot use docker) and 'things he did in the past' at 'much larger companies' and you can see Product go to his happy place during calls. In the end he lets tech do whatever they want as he understands nothing and gets so much tech babble that he cannot even figure out what to ask. We (small company fixing emergency tech stuff; for this client, it turns out that the emergency is basically their tech department; it's staffed with 100% incompetent people who are decades out of date) hop around a lot and this is very very common; on HN we read about flowers and fairy tales of covering tests, 10x programmers, migrations, kubernetes/containers, git, security etc; in reality however people are sending zip files with dates in them, updating the prod db manually and copy pasting files in whatsapp because they don't understand ssh works (what even IS there to understand ...) and tests? What is that? And these are companies that make more than your startup will ever make statistically. I am going to say that generally the plumber gets his way in the world, the fear of leaks, water or sewage, is enough for people who know nothing about plumbing (it's all pipes!). reply guappa 11 hours agoparentOk but what does this have to do with the topic? reply anonzzzies 11 hours agorootparentThe author argues this is a negotiation and that this is something reasonable and well thinking humans do with eachother; maybe they do in some magical HN dreamed up places, but most of the world is not like that; there is often no negotiation, not between plumber and client, not between car mechanic and client and not between dev and product/client. reply guappa 9 hours agorootparentBut we don't know how most of the world is. It might be that the author of the comment works in a particularly bad place, of which there are many of course. reply est 11 hours agoprevThis is exactly what I did. People like to bargin and negotiate, feel the \"control\". You must start off with an unacceptable price, then talk through \"tradeoffs\". If you plan it right you can get the exact \"tradeoff\" you need. reply scott_w 13 hours agoprevI strongly disagree with this because it infantilises your PM to a ridiculous level. You absolutely should explain (at a high level) what the constraints of your work are. If the PM doesn’t care, that’s a failing on their part. It’s their job to understand what’s possible within given timeframes and it’s your job as an engineer to explain that. If a feature requires major architectural work to achieve, then make clear what that is in terms they can understand. “We need to migrate 30m records affecting 20,000 customers to this new system with 0 downtime” is understandable to most people in tech. It can also help focus minds on what we could change to make progress on or just validate a goal before fully committing. For context, no plumber I know would talk shit about “platinum packages,” they’d just explain what the cause is and what’s making the correct fix so expensive. reply shehjar 15 hours agoprev [–] Be outcome oriented reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "When proposing architecture changes, present them as a comprehensive \"platinum package\" to help product teams understand the complexity and negotiate on scope and time.",
      "Focus on Return on Investment (ROI) and potential risks when suggesting new ideas, ensuring alignment with team goals and priorities.",
      "Use available time to work on improvements that enhance quality and deliver valuable solutions, emphasizing effective negotiation and prioritization."
    ],
    "commentSummary": [],
    "points": 153,
    "commentCount": 136,
    "retryCount": 0,
    "time": 1728522789
  },
  {
    "id": 41792179,
    "title": "Geoffrey Hinton says he's proud Ilya Sutskever 'fired Sam Altman'",
    "originLink": "https://techcrunch.com/2024/10/09/after-winning-nobel-for-foundational-ai-work-geoffrey-hinton-says-hes-proud-ilya-sutskever-fired-sam-altman/",
    "originBody": "IN BRIEF Posted: 10:42 AM PDT · October 9, 2024 Image Credits: Ramsey Cardy/Sportsfile for Collision / Getty Images Maxwell Zeff After winning Nobel for foundational AI work, Geoffrey Hinton says he’s proud Ilya Sutskever ‘fired Sam Altman’ Geoffrey Hinton accepted a Nobel Prize this week, recognizing the foundational work on artificial neural networks that earned him the nickname “godfather of AI.” In a speech Tuesday, Professor Hinton praised one student – alluding to OpenAI’s former Chief Scientist, Ilya Sutskever – for revolting against OpenAI’s CEO. “I was particularly fortunate to have many very clever students – much cleverer than me – who actually made things work,” said Hinton. “They’ve gone on to do great things. I’m particularly proud of the fact that one of my students fired Sam Altman.” Of course, Hinton is referring to OpenAI’s board voting to fire Altman in November 2023. Sutskever delivered the news to Altman via video call, but later said he regretted it. Hinton seems to have celebrated the firing of OpenAI’s CEO as a win for AI safety, which he often advocates for. However, the win was short-lived, as Altman now has more control over OpenAI than before, and he could soon get equity. Topics:AI, TC Related Enterprise Numeric grabs $28M Series A to automate accounting using AI Marina Temkin 3 hours ago In Brief Someone claims to have used AI to apply to 2,843 jobs A reporter used AI to apply to 2,843 jobs Kyle Wiggers 4 hours ago Security Fidelity says data breach exposed personal data of 77,000 customers Carly Page 7 hours ago Latest in AI See More Enterprise Numeric grabs $28M Series A to automate accounting using AI Marina Temkin 3 hours ago In Brief Someone claims to have used AI to apply to 2,843 jobs A reporter used AI to apply to 2,843 jobs Kyle Wiggers 4 hours ago Climate Scope3 starts tracking the carbon footprint of AI Tim De Chant 6 hours ago",
    "commentLink": "https://news.ycombinator.com/item?id=41792179",
    "commentBody": "[flagged] Geoffrey Hinton says he's proud Ilya Sutskever 'fired Sam Altman' (techcrunch.com)148 points by LorenDB 22 hours agohidepastfavorite7 comments apsec112 22 hours agoThis was already submitted, upvoted to the front page and then flagged: https://news.ycombinator.com/item?id=41791692 reply hn_throwaway_99 22 hours agoparentHmm, I can imagine the comments would be a shit show, but I personally am glad I saw this submission. It's something I hadn't seen previously before, and while Hinton's views on AI safety are well-known at this point, I didn't know he had such a strong opinion of Altman. The quote from the article is: \"I’m particularly proud of the fact that one of my students fired Sam Altman.\" Like, that's not subtle, and he said that while accepting his Nobel! I think that alone makes it relevant and newsworthy and not deserving of a flag. Edit: As an aside, has dang or anyone else ever said why HN doesn't support a \"lock comments\" option, instead of just an outright flag? While I definitely think that most of the value I get from HN is from the comments, there are cases like this one, where particularly flame war-inducing topics still have informative value in the article. reply infecto 21 hours agorootparentThere can definitely be cases where interesting links get immediately flagged due to bias but often times mods will pick it up pretty quickly. Locking is an interesting idea though. In this case this techcrunch article is baiting garbage. They cherry-picked a single statement from something that is much more interesting. All this article serves to do is continue the boring regurgitation of pro or against sama. reply 7e 4 hours agorootparentTechCrunch is doing me a service; I was not going to watch the Geoff Hinton video, so I would have remained ignorant of this important bit of news without TC highlighting it. Now I am going to watch the entire video, so it's a win all around. HN censorship, on the other hand, is doing us all great harm. reply hn_throwaway_99 21 hours agorootparentprevLooks like the original article (linked in the top comment) was switched to the full interview of Hinton, so glad dang made that change and unflagged it. reply 7e 4 hours agoparentprevThat post was also heavily censored to protect Sam Altman under the bromide of \"intellectual curiosity.\" reply indigo0086 20 hours agoprev [–] Why is everyone being pulled into this gossip fest around particular CEOS? This is pure yellow journalism inspired hate mongering. Its not even politics which would make sense reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Geoffrey Hinton, a prominent figure in artificial intelligence, was awarded a Nobel Prize for his contributions to artificial neural networks.",
      "In his acceptance speech, Hinton acknowledged Ilya Sutskever, his former student, for influencing the dismissal of OpenAI's CEO, Sam Altman, in 2023, which he considered a win for AI safety.",
      "Despite the initial firing, Sam Altman eventually regained more control over OpenAI, indicating ongoing dynamics in the leadership of AI organizations."
    ],
    "commentSummary": [
      "Geoffrey Hinton's comment about Ilya Sutskever firing Sam Altman during his Nobel acceptance speech has sparked debate on Hacker News regarding the news's relevance and presentation.",
      "The discussion reflects differing opinions, with some users labeling the article as sensationalist, while others value the insight into the complexities of moderating controversial topics."
    ],
    "points": 148,
    "commentCount": 7,
    "retryCount": 0,
    "time": 1728505128
  },
  {
    "id": 41791692,
    "title": "Press Conference: Professor Geoffrey Hinton, Nobel Prize in Physics 2024 [video]",
    "originLink": "https://www.youtube.com/watch?v=H7DgMFqrON0",
    "originBody": "Watch later Back",
    "commentLink": "https://news.ycombinator.com/item?id=41791692",
    "commentBody": "Press Conference: Professor Geoffrey Hinton, Nobel Prize in Physics 2024 [video] (youtube.com)143 points by moose44 23 hours agohidepastfavorite17 comments dang 22 hours agoWe changed the URL from https://techcrunch.com/2024/10/09/after-winning-nobel-for-fo..., which cherry-picks a single detail (doubtless the most sensational) to the video itself. With cases like this, it's helpful to remember that we're trying to optimize HN for one thing, intellectual curiosity [1]. Remembering that often turns borderline calls into easy ones. [1] https://hn.algolia.com/?dateRange=all&page=0&prefix=true&sor... reply 7e 4 hours agoparentRemoving the most curious portion of the video from the headline is not optimizing for intellectual curiosity. I shouldn't have to dive into a half hour long video to get the lede. This is outright censorship, which you're hiding behind a fig leaf, to protect Altman. Ironically, the original headline piqued my curiosity enough to watch the video, and I wasn’t going to before. So censoring the headline probably resulted in fewer views of the video by HN readers, not more. reply stonethrowaway 21 hours agoparentprevThank you dang. I’ll take the downvotes/flags but it really would be beneficial if HN “understood” that we are better off removing sensationalism rather than playing directly into the bait of it. reply ghayes 21 hours agoprevGeoffrey Hinton had an excellent series on neural networks from 2011 for Coursera available here https://m.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6... detailing the fundamentals of machine learning. The series was later wholesale replaced by another led by Andrew Ng of Google. I really adored Geoffrey’s lectures and recommend it to anyone looking to get into the space. It ends with him hinting at the idea of attention networks, but sadly I can’t find any later lectures from him on the topic. reply hintymad 19 hours agoparentI took that course. His language was dense and was quite complex. It was like listening to someone reading out the Goodfellow's Deep Learning book, except that the language was even more dense. I guess it showed Hinton's amazing mental capacity reply rajnathani 12 hours agorootparentI totally agree, as I took that course during University of Toronto undergrad (either 2013 or 2014) when [Sir?] Geoffrey Hinton decided to flip (this is an official term) the course to have it that we watched the Coursera video course of what is linked above, and ask questions in class. It was extremely hard to understand the course material, and I dropped out a few weeks into it after the first unit test. It is probably one of the most awful introductions to neural networks that exists, and that's why the \"go-to\" ML courses are ones by Andrew Ng, Jeremy Howard's fast.ai, and others. But to be fair, the class was very math-heavy in terms of the actual underlying implementation of neural networks, and I seemed to be an exception in that class of about 50 or so students (many seemed from the master's program) who simply could grasp much of the math behind it (I'm sure anyone who understand the course material could implement neural nets at a CUDA level). reply freehorse 22 hours agoprevFull interview: https://youtu.be/H7DgMFqrON0 The particular mention is at ~3:32. edit: the link was initially pointing to a (admittedly not very high quality) techcrunch article. The part mentioned there where GH closes his speech by \"I'm particularly proud of the fact that one of my students fired Sam Altman\" is at ~3:32 of the video in the current link. reply asadm 20 hours agoprevmentioning sama, so petty. reply stonethrowaway 23 hours agoprevnext [3 more] This is what clickbait-by-design looks like. The headline is, minor omission aside that Ilya was Hinton’s student, the entire article. I am sure folks here can find a better use of their time than to dogpile on sama. We have enough of those threads to go around that this one is incredibly below the belt. So please help keep HN relatively drama/snide free, and flag this article. Special thanks to TechCrunch for keeping the quality of their articles high. We couldn’t have done it without you. reply tourist123 22 hours agoparentI think it's unfair to call this drama/snide - it seems pretty justified for academics like Geoffrey Hinton to be upset that a non-profit research lab called OpenAI has been converted to a for-profit company + billions of stock for sama. Seems like a pretty good use of time to talked about how messed up that is. reply 31337Logic 22 hours agoparentprevWhat does it matter that Ilya was Geoffrey's student? I will not flag this. Indeed, quite the opposite - I upvoted. Is it the most scholarly article ever written? Of course not, but that's no reason to flag. I think Sam A. is wholly deserving of all the negative press of late. Articles like this keep the rational debate alive. We're proof of that, right now. reply OutOfHere 20 hours agoprev [–] To give credit where it is due, it is Sam, not Ilya, that brought ChatGPT and its API to the people. reply richerram 14 hours agoparent [–] I am curious to know if Sam has ever contributed to a technical paper, like, I am honestly curious if people like him or Musk ever contribute, formally, to the technical side of things besides publications more on the speculative/descriptive/philosophical side of things. reply OutOfHere 13 hours agorootparentI doubt it, but I do believe that without Sam, OpenAI would be lost fighting \"theoretical safety demons\" with Ilya, as opposed to being an instrument of accelerating technological change. reply richerram 1 hour agorootparentDefinitely, as anyone collaborating with the project like Microsoft or Satya Nadella deciding to help or not Sam Altman... we will see but I hope we as society move towards praising the actual hands-on work more than the great PR. reply bamboozled 6 hours agorootparentprev [–] The more I read about Musk ,the more he just seems to be a conman: https://www.tesla.com/elon-musk Elon Musk co-founded and leads Tesla, SpaceX, Neuralink and The Boring Company. He didn't co-found Tesla, he bought it...that's just one of many examples... He is smart, he has vision, yes, but I really doubt he's a rocket scientist as he likes to pretend. He is smart enough to pay the right people good money though. reply richerram 1 hour agorootparent [–] Yeah no, definitely he is not half as smart as the actual scientits and engineers doing the hard work, yeah he might be a great PR Agent but that's it, as I was saying above I really hope we as society can evolve on to appreciating and praising more and more the actual hands-on work of people in the labs or on the fields rather than the PR agents pretending to know it all. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A press conference video of Professor Geoffrey Hinton, a Nobel Prize winner in Physics 2024, was shared on YouTube, leading to discussions about the balance between sensationalism and substantial content.",
      "The article's headline was altered to reduce sensationalism, prompting debate on intellectual curiosity versus censorship.",
      "Conversations also included Hinton's contributions to neural networks and the influence of tech leaders like Sam Altman and Elon Musk."
    ],
    "points": 143,
    "commentCount": 17,
    "retryCount": 0,
    "time": 1728501966
  },
  {
    "id": 41791708,
    "title": "Command Line Tools I Like (2022)",
    "originLink": "https://rwblickhan.org/newsletters/command-line-tools-i-like-2022/",
    "originBody": "← Newsletters Command Line Tools I Like (2022) Last updated: Wed Jul 17 2024 This is another fairly technical post, and in particular presupposes some enthusiasm for the command line. If that isn’t you, feel free to skip this one! Despite being primarily an iOS developer, I use the command line quite a bit - I guess old habits from my time as an embedded software intern die hard. That said, I like a number of modern command line tools, many written in Rust, which are typically blazing fast and have better command-line interfaces than traditional Unix tools. If you like this list, you might also like Julia Evan’s more comprehensive list! neovim I have a noted love for vim, but when I’m not using an IDE with a vim mode, I’m actually typically using neovim, alias nvim, which is a modern reimplementation of vim with much less technical debt, a scripting engine based on Lua instead of notoriously-idiosyncratic vimscript, and reasonable defaults like syntax highlighting enabled by default. It also has a full implementation of the Language Server Protocol, which enables it to have very rich, Visual Studio Code-esque plugins. fzf fzf is a command-line fuzzy finder; given some input, fzf lets you search through the input with a fuzzy matching search term. One use I find for this is my custom git alias for a fancy branch switcher, git b, which lets me fuzzy-search for branch names when I want to switch branches. It’s also useful on its own! It can helpfully install a replacement for your terminal’s Ctrl+R to fuzzy-search previous commands, as well as a Ctrl+T command to fuzzy-search files in the current directory (although, to be honest, I usually find this option less than effective). bat bat is described as a “cat(1) clone with wings”. cat is technically supposed to be for concatenating text, but more often it’s simply used to print a file to the command line. bat leans into that usage by automatically piping large files into a pager, as well as adding syntax highlighting and git integration. I have cat aliased to bat. exa exa is a modern replacement for ls. Although I do think it has more reasonable defaults than ls, I really only use it for one reason: the pretty colours! I have ls aliased to exa. rg ripgrep, aka rg, is a grep tool; it allows you to efficiently search the full text of all files in a directory using regular expressions. Admittedly, it’s often more convenient to use a real IDE’s search function, but rg works everywhere and is blazingly fast. fd fd is a modern replacement for find. Unlike rg, which searches the full text of files, fd just searches filenames. This is useful in large codebases where you know roughly what a file is named but don’t know what directory it lives in. fd has a more intuitive command-line interface than find and even ignores files in your .gitignore by default! I have find aliased to fd. Notably, some of this behavior is also provided by fzf, but I usually find fd much more effective in actually finding what I want. delta Don’t you wish your command-line git diff was as pretty as Github? Well, now it can be! delta makes git diff output much prettier, with word-level highlighting, line numbers, and an optional side-by-side mode. tldr tldr is a utility that provides community-maintained help pages for command-line tools, meant to complement traditional man pages, which are typically verbose. Instead, tldr provides a quick cheat-sheet for common use cases. I use the tealdeer implementation of tldr. zoxide zoxide is a replacement for cd, inspired by z, that lets you jump around quickly. At a basic level, it can completely emulate the behavior of typical cd. However, you can also give it a fuzzy search term, and it will use a “frecency” algorithm to determine which directory, anywhere on your system, to jump to. I have cd aliased to z, the binary for zoxide. zoxide also has an interactive mode that uses fzf to fuzzy-find recent directory paths. I have that functionality aliased to cdi, though I haven’t gotten in the habit of using it yet. httpie HTTPie is a recent discovery. I don’t need to use curl very often to make HTTP requests, but when I do, it’s always a bit painful to remember the syntax. HTTPie has a much more obvious command-line interface and also built-in support for making HTTPS requests.",
    "commentLink": "https://news.ycombinator.com/item?id=41791708",
    "commentBody": "Command Line Tools I Like (2022) (rwblickhan.org)136 points by yu3zhou4 23 hours agohidepastfavorite64 comments terminaltrove 21 hours agoIf you're looking for more or new tools, we have lots at Terminal Trove and continuously add new ones. https://terminaltrove.com/new/ https://terminaltrove.com/tool-of-the-week/ Every tool added has images/gifs and a quick way to install it. We love this list and sponsored the development of fd which we heavily use ourselves! reply Diti 21 hours agoparentYour website recommends using `nix-env` to install tools. This hasn’t been the recommended way for a while. It should either be installed declaratively, used in a shell without installing, or be installed using `nix profile`. reply terminaltrove 20 hours agorootparentOh right, we didn't know this, most of the tools install guides have nix-env like fd, lychee, gtrash, etc and others we listed which we just use for installation. https://github.com/lycheeverse/lychee?tab=readme-ov-file#nix... https://github.com/sharkdp/fd?tab=readme-ov-file#on-nixos--v... https://github.com/umlx5h/gtrash?tab=readme-ov-file#nixpkgs-... Thanks for the heads up! reply defrost 17 hours agoprevWhether you're a black or white hat or more simply just a grunt pushing stuff out; Pillager (or Gitleaks) is worth having on the sanity checklist https://github.com/brittonhayes/pillager https://terminaltrove.com/pillager/ powerful rules functionality to recursively search directories for sensitive information in files. At it's core, Pillager is designed to assist you in determining if a system is affected by common sources of credential leakage as documented by the MITRE ATT&CK framework. Good for catching those Oops I deployed the company password list again SNAFU's. reply donatj 20 hours agoprevEternal Terminal `et` when we worked from an office where our connection would drop regularly was a life saver. It's like Mosh but less opinionated and doesn't interfere with scrollback. https://eternalterminal.dev/ Probably goes without saying, but for anyone who doesn't know about it, `jq` is life changing, was kind of surprised not to see it. It's a sort of query language for querying JSON blobs. I use it almost every single day. It's indispensable. https://jqlang.github.io/jq/ reply ExtremisAndy 20 hours agoparentI have never heard of “jq”. Oh my goodness. Your comment may have just changed my life. I cannot emphasize enough how many times I have needed a tool like this (and, yes, shame on me for not making a better effort to find one). Thank you! reply dredmorbius 42 minutes agorootparentThe syntax (and theory behind it) are really arcane, but jq is phenomenally powerful. For websites which can publish JSON extracts, piping that through jq to get just what you want and how you want it is an absolute lifesaver. reply wwader 10 hours agorootparentprevAs a heavy jq user and now a days also maintainer i say welcome! happy to help if you run into some problem reply alsothismthelp 5 hours agoparentprevThis is a command line tool which lets you inspect the keys recursively and then use the resulting keypath to query the value. https://github.com/codecando-x/hands reply anyonecancode 17 hours agoparentprevI also like jless: https://github.com/PaulJuliusMartinez/jless For when you aren't manipulating or querying json and just want a nice UI for exploring it. reply darkteflon 16 hours agorootparentCoincidentally, yesterday I decided I needed a JSON TUI and landed on fx (https://github.com/antonmedv/fx), which seems to have come out of the Wave terminal project and looks quite similar to jless. Also uses vim keybindings. I like it so far. reply sandreas 4 hours agoprevHerr is mine.. # dra - download releases from gh devmatteini/dra # bat - modern cat replacement sharkdp/bat # btop - process explorer aristocratos/btop # difftastic - better diff difft;Wilfred/difftastic # eza - modern ls replacement eza-community/eza # fd - find replacement sharkdp/fd # fzf - fuzzy finder junegunn/fzf # gdu - disk usage analyzer similar to ncdu but faster dundee/gdu # jless - json viewer PaulJuliusMartinez/jless # jq - json query tool jqlang/jq # lazydocker - terminal docker management ui jesseduffield/lazydocker # pandoc - document conversion tool jgm/pandoc # pandoc dependency typst/typst # restic - repository based backup tool restic/restic # rg - ripgrep, better grep tool rg;BurntSushi/ripgrep # rga - ripgrep-all, grep for PDF rga;phiresky/ripgrep-all # starship - powerlevel10k replacement starship/starship # tone - audio tagger sandreas/tone # yazi - terminal file manager sxyazi/yazi # zellij - terminal multiplexer zellij-org/zellij # zoxide - modern cd replacement ajeetdsouza/zoxide reply ibash 4 hours agoparentIn addition to restic: rustic https://github.com/rustic-rs/rustic It’s a rewrite of restic in rust, but has a few more quality of life features and a config file for setting params (instead of restic only having cli flags). It’s what changed my backups from something I’d poke at every few days to completely on autopilot. reply sandreas 1 hour agorootparentI'd prefer rededup (technology wise), but it has not many convenience features and a small userbase. Killer feature is backup without providing the decrypt key, but restic has a huge userbase and a more promising future reply Timber-6539 3 hours agorootparentprevThe readme has this note. > Stability rustic currently is in beta state and misses regression tests. It is not recommended to use it for production backups, yet. reply KneeAwn 21 hours agoprevI love these tools. A few more I like are: eget - good for getting these little tools (https://github.com/zyedidia/eget) dust - fd is to find as dust is to du (https://github.com/bootandy/dust) yank - nice to quickly copy things from the command line (https://github.com/mptre/yank) reply blacksmith_tb 19 hours agoparentHmm, yank looks useful on Linux, on macOS there's just pbcopy[1]? 1: https://ss64.com/mac/pbcopy.html reply ashenke 19 hours agorootparentpbcopy puts all stdin in the clipboard, whereas yank gives an interface to pick a subselection of the input reply jftuga 20 hours agoparentpreveget looks great! Thanks for mentioning it. reply netol 22 hours agoprevexa is abandoned. There is now a maintained fork called eza: https://github.com/eza-community/eza reply yu3zhou4 22 hours agoparentI just ran `brew install eza` and I'm overwhelmed with amount of dependencies it installs. Among many others - openjdk, qt, node - what is going on? reply quadhome 22 hours agorootparentYou’re likely running on an old version of MacOS that isn’t able to use the precompiled binaries. So, brew is installing all the dependencies necessary to build eza from scratch. Intel-era Mac? reply yu3zhou4 22 hours agorootparentMacBook Air M2 2022, macOS Sequoia 15.0 reply zbentley 19 hours agorootparentYour Homebrew may not be configured to pull only the runtime dependencies; as others in this thread have mentioned, it's pulling in all those dependencies becauase it's building \"eza\" (or something, perhaps one of \"eza\"'s few transitives) from source, which brings in quite the list, including openjdk as you saw. Homebrew can accidentally end up configured to do this in a number of ways. Some of these may no longer be issues; this list is from memory and should be taken with a grain of salt: - You might be running an outdated homebrew. - You might have homebrew checked out as a git checkout, thus missing \"brew update\" abilities. \"brew doctor\" will report on this. - You might have \"inherited\" your Homebrew install from a prior Mac (e.g. via disk clone or Time Machine), or from the brief transitional period where Homebrew was x86-via-Rosetta on ARM macs, thus leaving your brew in a situation where it can't find prebuilt packages (\"bottles\") for what it observes as a hybrid/unique platform. Tools, including your shell, which install Homebrew for you might install it as the wrong (rosetta-emulated) architecture if any process-spawning part of the tool is an x86-only binary. More details on a similar situation I found myself in are here: https://blog.zacbentley.com/post/dtrace-macos/ - (I'm pretty sure most issues in this area have been fixed, but) you might have an old or \"inherited\" XCode or XCode CLT installation. These, too, can propagate from backups. Removing Homebrew, uninstalling/reinstalling XCode/CLT, and reinstalling Homebrew can help with this. - The HOMEBREW_ARCH, HOMEBREW_ARTIFACT_DOMAIN, HOMEBREW_BOTTLE_DOMAIN, or other environment variables may be set in your shell such that Homebrew either thinks the platform doesn't have bottles available or it shouldn't download them: https://docs.brew.sh/Manpage#environment - Perhaps obvious, but your \"brew\" command might be invoked such that it always builds from source, e.g. via a shell alias. - Homebrew may be unable to access the bottle repository (https://ghcr.io/v2/homebrew/core/), either due to a network/firewall issue or a temporary outage. reply yu3zhou4 13 hours agorootparentThank you for such a comprehensive guide! I’ll try to resolve the issue today with your help reply specialist 8 hours agorootparentprevYour (awesome) comment reminds me... Noob me has had to troubleshoot homebrew. Like keeping laptop and desktop in sync. Like fixing stuff I've somehow borked. So I tried a handful of GUIs (wrappers). Like these two top hits: https://corkmac.app/ https://aerolite.dev/applite But sort of bounced off. Noob friendly homebrew seems like such a great idea. I especially want just one strategy which spans both utils and apps (casks). Versus cobbling together Apple App Store, SetApp, and homebrew. Those GUIs would be even more useful if they spotted and explained the config issues you listed. (I have no idea if \"brew doctor\" suffices.) reply IshKebab 21 hours agorootparentprevBut why would it need all of those dependencies? reply jasonpeacock 21 hours agorootparentBecause most of those are dependencies required to build the actual dependencies. There's (generally) 4 types of dependencies: - Toolchains (frameworks, compilers) - Build (headers and libraries) - Runtime (libraries) - Test (frameworks, headers, libraries) And those dependencies all bring their own dependencies... reply Aaron2222 12 hours agorootparentprevFrom what I can tell, it doesn't: $ brew deps --include-build eza autoconf automake ca-certificates cabal-install certifi cmake ghc ghc@9.8 libgit2 libssh2 llvm@18 lz4 m4 mpdecimal ninja openssl@3 pandoc pkg-config python@3.11 python@3.12 readline rust sphinx-doc sqlite xz zstd reply groby_b 21 hours agorootparentprevBecause all dependency managers at some point devolve to \"install ocean, then boil ocean\". (If you care, \"brew deps--tree\" will tell you.) reply yu3zhou4 21 hours agorootparentbrew deps eza --tree prints: eza └── libgit2 ├── libssh2 │ └── openssl@3 │ └── ca-certificates └── openssl@3 └── ca-certificates reply gitaarik 17 hours agoparentprevYou also have lsd: https://github.com/lsd-rs/lsd reply jasonpeacock 21 hours agoprevI really enjoy `glow`, it makes me smile when I use it: https://github.com/charmbracelet/glow It's a commandline markdown viewer/renderer. reply GNOMES 19 hours agoparentI was previously really excited about this in the past, but uninstalled it due to remote storing of PDF features: https://www.reddit.com/r/commandline/comments/jb4axl/comment... Has this gone away? reply zuntaruk 15 hours agorootparentIts my understanding that these good folks have moved away entirely from their hosted stuff. In the context of glos this was the \"stash\" feature, removed with v2 release. More details can be found here: https://github.com/charmbracelet/charm reply joemi 19 hours agoprevDo the creators/maintainers of these tools ever try to get their improvements merged into the tools they aim to replace? And does it ever happen? For a while I've heard about things like ripgrep and such that seem to be so much better and faster than grep, so why wouldn't those kinds of improvements get brought into grep? (Note: I'm not asking this from a \"down with the old ways!\" perspective, but just out of curiosity. I assume there's a reason people are making separate tools instead of improving the existing ones, I just don't know what that reason is.) reply burntsushi 15 hours agoparentOne of the biggest improvements reported by my users is the smart filtering enabled by default in ripgrep. That can't be contributed back to GNU grep. Also, people have tried to make GNU grep uses multiple threads. As far as I know, none of those attempts have led to a merged patch. There are a boatload of other reasons to be honest as well. And there's no reason why I specifically need to do it. I've written extensively on how and why ripgrep is fast, all the way down to details in the regex engine. There is no mystery here, and anyone can take these techniques and try to port them to another grep. reply arp242 18 hours agoparentprevI've written some tools like this; some released, some not (mostly because unfinished). The problem with \"merging it upstream\" is that it's just a lot of friction. There are also some ideas I have and having my own tool gives me the freedom to explore and experiment without worrying too much what other people think or compatibility. And then there's the language choice, as well as code quality. I don't really want to start a huge discussion about this, but it should be pretty obvious that many people are more comfortable and productive using languages that are not C, and that some of these tools don't have the best C code you can find. A lot of \"Rewrite in Rust\" (or Go, Python, what-have-you) isn't really about the \"Rewrite in Rust\" as such, but rather about \"Rewrite so I can play around with ideas I have\". Also see my comment from a while ago when someone asked \"why do this when $other_tool already exists?\": https://github.com/arp242/elles/issues/1#issuecomment-216855... reply landr0id 19 hours agoparentprevSome algorithm improvements ripgrep uses could be brought into grep, but ripgrep at its core just operates differently. It uses threads by default, assumes unicode, has a completely different regex engine, amongst other things. It could also probably be argued that some things from ripgrep would be pretty difficult to port from Rust to C or C++ safely. burntsushi has a blog post on it here: https://blog.burntsushi.net/ripgrep/ reply 3eb7988a1663 17 hours agorootparentAll of the behind the scenes improvements aside, ripgrep just has significantly better defaults. Recursiveness, ignore binaries, parsing .gitignore, etc. I do not think it would be possible to get grep to alter these configurations. It would break decades of ossified scripts. reply gitaarik 17 hours agoparentprevRipgrep is written in Rust while the original grep in C. So it doesn't make sense to try to combine them. Ripgrep is rather the modern rewrite, and we keep the old grep around for backwards compatibility. reply skdd8 22 hours agoprevI love ranger: https://github.com/ranger/ranger It is a file manager inspired by vim and midnight-commander. reply jpeeler 1 hour agoparentI used to use ranger, but have since switched to yazi[1] for speed and out of the box image support. (Ranger can do the same, but I think you have to set the preview_images_method[2]). [1] https://yazi-rs.github.io [2] https://github.com/ranger/ranger/blob/bd9b37faee9748798f3970... reply nvllsvm 8 hours agoparentprevlf is similar (I switched a system Python version update broke ranger). https://github.com/gokcehan/lf I have it integrated into zsh so the current directory is whatever dir I was in when exiting lf. reply thiht 9 hours agoprevOne rarely mentioned tool I absolutely love is hyperfine: https://github.com/sharkdp/hyperfine It’s a benchmarking CLI tool that can be used as an alternative to `time`. I often use it to detect flacky tests, I run something like `hyperfine —show-output -n=100 'go test ./… -count=1' and it helps me catch tests that fail unreliably reply steph-123 17 hours agoprevI really enjoy `x-cmd` https://github.com/x-cmd/x-cmd A vast and interesting collection of CLI that can then bootstrap lots of other programs / functions in a consistent and structured way (X bootstrap 1000+ tools and your scripts) reply umvi 17 hours agoprevThe problem (for me) with using non standard CLI is that whenever I'm using some other computer (i.e. a VM I spun up, server I'm ssh'd into, etc) said custom tools are no longer available unless I go out of my way to install them and I have to fall back on standard coreutils. So for me it only seems worth custom CLI tooling if you are relatively stable in your work environment. reply petepete 22 hours agoprevxh is a clone of httpie written in Go, it's a little snappier if that's important to you. reply zuntaruk 14 hours agoparentIs this what you're referring to? https://github.com/ducaale/xh Seems like maybe it's written in rust? Still looks slick! reply petepete 7 hours agorootparentOops my bad. Yes that's it. reply jftuga 20 hours agoprevI've recently made a new command line tool: https://github.com/jftuga/DateTimeMate Golang package and CLI to compute the difference between date, time or duration Here is a more detailed announcement: https://news.ycombinator.com/item?id=41058826 reply grakker 19 hours agoprevI've never been a fan of aliasing new commands to coreutils commands. Just use the new name, or make a unique alias. reply ibash 14 hours agoprevduckdb It’s a cli that lets you query anything table-like with sql. csv, excel, parquet, and other dbs all in one comfy sql interface. reply gigatexal 21 hours agoprevZoxide looks really cool reply marcinreal 19 hours agoparentI didn't like frecency in this and similar tools. I would often get put in directories that I didn't want. I wrote my own simple script that just uses recency, and if there's multiple possible matches you get to choose which one you want (though this is configurable). https://github.com/mrcnski/compnav reply BeetleB 17 hours agorootparent> I would often get put in directories that I didn't want. I solved this by combining it with fzf. Get all the directories you've ever visited and pass on to fzf (sorted by frequency). Then do your matching. You can trivially see if the match is taking you where you want. If not it is likely the second or third match. You're no longer constrained to navigating only to the top matched directory. I have this bound to a keystroke in my shell. reply I_complete_me 7 hours agorootparentprevI use dirjump [1] for this. I use zoxide alongside it. [1] https://github.com/imambungo/dirjump reply kstrauser 20 hours agoparentprevI adore it. `z ` is my brain's hardwired shortcut to get back to what I was doing. Pair it with dotenv to automatically set my my shell environment for that project, whatever it is I'm doing at the moment, and it's sooo ergonomic to bounce around between tasks. reply darkteflon 15 hours agoparentprevI used and loved z for years but migrated to zsh-z (https://github.com/agkozak/zsh-z) when MacOS switched the default shell and it became apparent that z wouldn’t be compatible with it. Anyone have a view on whether I should switch from zsh-z (~2k gh stars) to Zoxide (~22k stars)? reply stared 20 hours agoparentprevIt is a simple tool, yet it makes a day-and night difference when traversing directories. reply elmariachi 20 hours agoparentprevIt is! It's saved me quite a bit of typing since I started using it. reply Twelveday 6 hours agoprevI always try to predict what tools are mentioned in these posts and this time I was pretty close to get them all :) I recently started using forgit and find it really usefull without having to change my workflow too much. And instead of tldr i just do `curl cheat.sh/tar` reply AtlasBarfed 21 hours agoprev [–] IS EXA PARSEABLE????!!!??? It's a slowly developing trend, but I also wish that a --json output flag was a part of all cli utility output. Tldr sounds interesting. Man pages are awful for quick reference. At this point it should be possible to collect the statistically ranked most common example usages of commands and provide them, especially if there are very very common associated commands that are piped with them. reply dingnuts 21 hours agoparent [–] I tried out tldr a few years back and in practice tldr never seemed to have what I want in it now, for the same use case, I search for the man page on Kagi, use the LLM \"ask this page questions\" feature to ask the man page how to do what I want, and then ctrl-f with the flags it outputs and read the man page entries for those to ensure no hallucinations. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post highlights modern command line tools, many written in Rust, that enhance efficiency and usability for developers, particularly iOS developers.",
      "Featured tools include Neovim for advanced text editing, fzf for fuzzy searching, bat for syntax-highlighted file viewing, and exa for colorful directory listing.",
      "Other notable tools are ripgrep for fast file content searching, fd for filename searching, delta for improved git diff visualization, tldr for concise command help, zoxide for smarter directory navigation, and HTTPie for user-friendly HTTP requests."
    ],
    "commentSummary": [
      "Terminal Trove provides a collection of command line tools with straightforward installation guides, catering to various user needs.- Popular tools discussed include Pillager for security checks, Eternal Terminal for stable connections, and jq for JSON querying, with alternatives like jless and eza also mentioned.- Users expressed concerns about installation dependencies and the practicality of using non-standard command line interface (CLI) tools across different systems."
    ],
    "points": 136,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1728502051
  }
]
